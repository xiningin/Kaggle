{"cell_type":{"f9169cf0":"code","813eb48b":"code","16281b98":"code","d43a8c88":"code","2691e2ee":"code","5dc3771a":"code","555eb352":"code","24953dc6":"code","7ecb0895":"code","d0baecbe":"code","a12835a7":"code","57decebb":"code","b8ba1375":"code","38f9fe3a":"code","2a30b7fd":"code","cc78217d":"code","982bd2c6":"code","3651e836":"code","0896556f":"code","4566f82e":"code","ef817de6":"code","eb2ecd0f":"code","93f10a25":"markdown","ba694163":"markdown","5c8f80d6":"markdown","dc8a8115":"markdown","5619e923":"markdown","f80fa9ff":"markdown","39a26dd7":"markdown","7f7f9b39":"markdown","036209e8":"markdown","7adbcb3a":"markdown","a42e3c49":"markdown","db372cb9":"markdown","2ffbdf67":"markdown","563bc790":"markdown","b2d0ca3a":"markdown","7ebed118":"markdown","3cbe93ae":"markdown","40dbab33":"markdown","18bb0979":"markdown","1c1ff5af":"markdown","794eb0d6":"markdown","1d5b3409":"markdown","f7b97f89":"markdown","71248a8b":"markdown","27bfdc6b":"markdown","3b167f04":"markdown","3def4b2c":"markdown","900ff625":"markdown","6bb26bd8":"markdown","9288dc84":"markdown","0127c241":"markdown","53d68546":"markdown","63ba8044":"markdown"},"source":{"f9169cf0":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nimport numpy as np\nimport time\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nprint ('modules loaded')","813eb48b":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","16281b98":"class SLR(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(SLR, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training, or enter an integer for how many more epochs to run, then be asked again')  \n            print(' or enter an A to adjust the learning rate. If an A is entered you will be queired to enter a float')\n            print(' values for the new learning rate then be asked to enter an integer for howmany more epochs to run before being asked again')\n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or an integer for the number of additional epochs to run or enter A to adjust the learning rate')\n                ans=input()                \n                if ans == 'H' or ans =='h' or ans == '0': # halt training \n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                elif ans == 'A' or ans == 'a': # user wants to adjust learning rate\n                    lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                    print(' current lr = ', lr, 'Enter a float value for the new learning rate')\n                    ans=input()\n                    lr=float(ans)\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    print('Enter an integer for the number of additional epochs to run then be asked again')\n                    ans=input()\n                    self.ask_epoch += int (ans)\n                    print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)\n                else: # user wants to continue training with current lr\n                    self.ask_epoch += int(ans)\n                    print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","d43a8c88":"class SOMT(keras.callbacks.Callback):\n    def __init__(self, model,  train_thold, valid_thold):\n        super(SOMT, self).__init__()\n        self.model=model        \n        self.train_thold=train_thold\n        self.valid_thold=valid_thold\n        \n    def on_train_begin(self, logs=None):\n        print('Starting Training - training will halt if training accuracy achieves or exceeds ', self.train_thold)\n        print ('and validation accuracy meets or exceeds ', self.valid_thold) \n        msg='{0:^8s}{1:^12s}{2:^12s}{3:^12s}{4:^12s}{5:^12s}'.format('Epoch', 'Train Acc', 'Train Loss','Valid Acc','Valid_Loss','Duration')\n        print (msg)                                                                                    \n            \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:1s}processed batch {1:4s}  training accuracy= {2:8.3f}  loss: {3:8.5f}'.format(' ', str(batch),  acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self,epoch, logs=None): \n        later=time.time()\n        duration=later-self.now \n        tacc=logs.get('accuracy')           \n        vacc=logs.get('val_accuracy')\n        tr_loss=logs.get('loss')\n        v_loss=logs.get('val_loss')\n        ep=epoch+1\n        print(f'{ep:^8.0f} {tacc:^12.2f}{tr_loss:^12.4f}{vacc:^12.2f}{v_loss:^12.4f}{duration:^12.2f}')\n        if tacc>= self.train_thold and vacc>= self.valid_thold:\n            print( f'\\ntraining accuracy and validation accuracy reached the thresholds on epoch {epoch + 1}' )\n            self.model.stop_training = True # stop training","2691e2ee":"class TLC(keras.callbacks.Callback):\n    def __init__(self, model, base_model, epochs,  ask_epoch):\n        super(TLC, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True\n        self.base_model=base_model        \n        \n    def on_train_begin(self, logs=None):        \n        if self.ask_epoch == 0:\n            self.ask=False\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs:\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs' )           \n            self.ask=False\n        if self.epochs == 1:\n            self.ask=False\n        else:\n            if self.base_model !=None and self.base_model.trainable == False:                \n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt, continue \\n'\n                msg=msg + 'or make the base model trainable then asked to enter number of epochs to run then ask again\\n'\n                print (msg, flush=True)\n            else:\n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt \\n'\n                msg=msg + 'or enter the number of epochs to train on then you will be asked again\\n'\n                print (msg, flush=True)\n            \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):        \n        tr_duration=time.time() - self.start_time            \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) \n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask:\n            if epoch + 1 ==self.ask_epoch:                \n                if self.base_model !=None and self.base_model.trainable ==  False:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                    msg=msg + 'or T to train base model\\n'                    \n                else:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                ans=input(msg )\n                if ans == 'T' or ans== 't':\n                    self.base_model.trainable=True\n                    print ('Base Model trainable is now set to ', self.base_model.trainable)\n                    msg='\\n enter an integer for the number of additional epochs to run then ask again\\n'\n                    ans=(input(msg))\n                    \n                if ans == 'H' or ans =='h' or ans == '0':\n                    print ('\\nTraining halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True\n                else:\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                    else:\n                        print ('\\n Training will continue to epoch ', self.ask_epoch, flush=True)","5dc3771a":"class SPREADSHEET(keras.callbacks.Callback):\n    def __init__(self, model, base_model, epochs,  ask_epoch, batches):\n        super(SPREADSHEET, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True\n        self.base_model=base_model \n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.batches=batches        \n        \n    def on_train_begin(self, logs=None): \n        if self.ask_epoch == 0:\n            self.ask=False            \n        if self.ask_epoch >= self.epochs:\n            msg=f'ask_epoch >= epochs, will train for {epochs}  epochs'\n            print_in_color(msg, (0,255,255), (55,65,80))\n            self.ask=False\n        if self.epochs == 1:\n            self.ask=False\n        if self.ask == True:\n            if self.base_model !=None and self.base_model.trainable == False:                \n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt, continue \\n'\n                msg=msg + 'or make the base model trainable then asked to enter number of epochs to run then ask again\\n'\n                print_in_color (msg, (0,255,255), (55,65,80))\n            else:\n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt \\n'\n                msg=msg + 'or enter the number of epochs to train on then you will be asked again\\n'\n                print_in_color (msg, (0,255,255), (55,65,80))\n            \n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:10s}{7:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', '% Improv', 'Duration')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):        \n        tr_duration=time.time() - self.start_time            \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print_in_color (msg, (0,255,255), (55,65,80)) \n        \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        color= (0,255,0)\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        if epoch ==0:\n                pimprov=0.0\n                self.lowest_vloss=v_loss\n        else:\n            pimprov=( self.lowest_vloss- v_loss) * 100\/self.lowest_vloss\n            if v_loss > self.lowest_vloss:\n                color=(245, 170, 66)\n            else:\n                color=(0,255,0)\n                self.lowest_vloss=v_loss\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss') \n        msg=f'{str(epoch+1):^3s}\/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{pimprov:^10.2f}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.ask:\n            if epoch + 1 ==self.ask_epoch:                \n                if self.base_model !=None and self.base_model.trainable ==  False:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                    msg=msg + 'or T to train base model\\n'                    \n                else:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                ans=input(msg )\n                if ans == 'T' or ans== 't':\n                    self.base_model.trainable=True\n                    msg=f'Base Model trainable is now set to {self.base_model.trainable}'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    msg=' enter an integer for the number of additional epochs to run then ask again'\n                    ans=(input(msg))\n                    \n                if ans == 'H' or ans =='h' or ans == '0':\n                    msg=f'\\nTraining halted on epoch {epoch+1} due to user input'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    self.model.stop_training = True\n                else:\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        msg=f'\\nYou specified maximum epochs of as {self.epochs}, cannot train for {self.ask_epoch} '\n                        print_in_color(msg, (244,252,3), (55,65,80))\n                    else:\n                        print ('\\n Training will continue to epoch ', self.ask_epoch, flush=True)\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:10s}{7:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', '% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80)) ","555eb352":"class DWELL(keras.callbacks.Callback):\n    def __init__(self,model,  factor, verbose):\n        super(DWELL, self).__init__()\n        self.model=model\n        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it  \n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights \n        self.verbose=verbose \n        self.best_epoch=0\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate         \n        vloss=logs.get('val_loss')  # get the validation loss for this epoch \n        if vloss>self.lowest_vloss:\n            self.model.set_weights(self.best_weights)\n            new_lr=lr * factor\n            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n            if self.verbose:\n                print( '\\n model weights reset to best weights from epoch ', self.best_epoch+1, ' and reduced lr to ', new_lr, flush=True)\n        else:\n            self.lowest_vloss=vloss\n            self.best_weights=self.model.get_weights()\n            self.best_epoch= epoch\n        ","24953dc6":"class LRA(keras.callbacks.Callback):\n    def __init__(self,model, base_model, patience,stop_patience, threshold, factor, dwell, batches,epochs, ask_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.base_model=base_model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience # specifies how many times to adjust lr without improvement to stop training\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.batches=batches # number of training batch to runn per epoch        \n        self.epochs=epochs\n        self.ask_epoch=ask_epoch\n        self.ask_epoch_initial=ask_epoch # save this value to restore if restarting training\n        # callback variables \n        self.count=0 # how many times lr has been reduced without improvement\n        self.stop_count=0        \n        self.best_epoch=1   # epoch with the lowest loss        \n        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it         \n        self.highest_tracc=0.0 # set highest training accuracy to 0 initially\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n        self.initial_weights=self.model.get_weights()   # save initial weights if they have to get restored \n        \n    def on_train_begin(self, logs=None):        \n        if self.base_model != None:\n            status=base_model.trainable\n            if status:\n                msg=' initializing callback starting training with base_model trainable'\n            else:\n                msg='initializing callback starting training with base_model not trainable'\n        else:\n            msg='initialing callback and starting training'                        \n        print_in_color (msg, (244, 252, 3), (55,65,80)) \n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):\n        stop_time=time.time()\n        tr_duration= stop_time- self.start_time            \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n\n        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n        msg=f'Training is completed - model is set with weights from epoch {self.best_epoch} '\n        print_in_color(msg, (0,255,0), (55,65,80))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print_in_color(msg, (0,255,0), (55,65,80))   \n        \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss')        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if epoch ==0:\n                pimprov=0.0\n            else:\n                pimprov= (acc-self.highest_tracc )*100\/self.highest_tracc\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                self.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                color= (0,255,0)\n                self.best_epoch=epoch + 1  # set the value of best epoch for this epoch              \n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1: # lr should be adjusted\n                    color=(245, 170, 66)\n                    lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1 # count the number of consecutive lr adjustments\n                    self.count=0 # reset counter\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss                                    \n                else:\n                    self.count=self.count +1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if epoch ==0:\n                pimprov=0.0\n            else:\n                pimprov= (self.lowest_vloss- v_loss )*100\/self.lowest_vloss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                self.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                color=(0,255,0)                \n                self.best_epoch=epoch + 1 # set the value of the best epoch to this epoch\n            else: # validation loss did not improve\n                if self.count>=self.patience-1: # need to adjust lr\n                    color=(245, 170, 66)\n                    lr=lr * self.factor # adjust the learning rate                    \n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(self.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}\/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(msg, (0,255,255), (55,65,80))\n            self.model.stop_training = True # stop training\n        else: \n            if self.ask_epoch !=None:\n                if epoch + 1 >= self.ask_epoch:\n                    if base_model.trainable:\n                        msg='enter H to halt training or an integer for number of epochs to run then ask again'\n                    else:\n                        msg='enter H to halt training ,T to train the base_model, or an integer for number of epochs to run then ask again'\n                    print_in_color(msg, (0,255,255), (55,65,80))\n                    ans=input('')                    \n                    if ans=='H' or ans=='h':\n                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n                        print_in_color(msg, (0,255,255), (55,65,80))\n                        self.model.stop_training = True # stop training\n                    elif ans == 'T' or ans=='t':\n                        if base_model.trainable:\n                            msg='base_model is already set as trainable'\n                        else:\n                            msg='setting base_model as trainable for fine tuning of model'\n                            self.base_model.trainable=True\n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='Enter an integer for the number of epochs to run then be asked again'\n                        print_in_color(msg, (0,2555,255), (55,65,80))\n                        ans=input()\n                        ans=int(ans)\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch) \n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80))                         \n                        self.count=0\n                        self.stop_count=0                        \n                        self.ask_epoch = epoch + 1 + self.ask_epoch_initial \n                        \n                    else:\n                        ans=int(ans)\n                        self.ask_epoch +=ans\n                        msg=f' training will continue until epoch ' + str(self.ask_epoch)                         \n                        print_in_color(msg, (0, 255,255), (55,65,80))\n                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n                        print_in_color(msg, (244,252,3), (55,65,80)) ","7ecb0895":"def print_in_color(txt_msg,fore_tupple,back_tupple,):    \n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","d0baecbe":"img_path=r'..\/input\/demo-of-7-useful-custom-keras-callbacks\/Curly\/0.jpg'\nimg=plt.imread(img_path)\nprint ('Image shape is: ', img.shape)\nplt.axis('off')\nplt.imshow(img)","a12835a7":"sdir=r'..\/input\/demo-of-7-useful-custom-keras-callbacks'\nclasslist=os.listdir(sdir)\nlabels=[]\nfilepaths=[]\nfor klass in classlist:\n    classpath=os.path.join(sdir, klass)\n    flist=os.listdir(classpath)\n    for f in flist:\n        fpath=os.path.join(classpath,f)\n        filepaths.append(fpath)\n        labels.append(klass)\nFseries=pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nclass_count=len(list(df['labels'].unique()))\nprint('Number of classes in dataset is ', class_count)","57decebb":"train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\nprint('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\nprint (train_df['labels'].value_counts())","b8ba1375":"img_size = (50,50)\nworking_dir = sdir\nbatch_size= 40\n# calculate test_batch_size and test_step so we go through test files exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=80],reverse=True)[0]  \ntest_steps=int(length\/test_batch_size)\nprint ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\ntrgen=ImageDataGenerator(horizontal_flip=True)\ntvgen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n","38f9fe3a":"def make_model(img_img_size, class_count,lr=.001, trainable=True):\n    img_shape=(img_size[0], img_size[1], 3)\n    model_name='EfficientNetB3'\n    base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n    base_model.trainable=trainable\n    x=base_model.output\n    x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n    x=Dropout(rate=.45, seed=123)(x)        \n    output=Dense(class_count, activation='softmax')(x)\n    model=Model(inputs=base_model.input, outputs=output)\n    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n    return model, base_model # return the base_model so the callback can control its training state","2a30b7fd":"epochs=40 # max number of epochs to run unless training is halted by the user\nask_epoch=5 # set number of epochs to run then be quired to contiue or halt\nlr=.002 # set the optimizer learning rate\ntrainable=True # set base_model trainable\nmodel, _=make_model(img_size, class_count, lr,)\ncallbacks=[ASK(model, epochs,  ask_epoch)]","cc78217d":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","982bd2c6":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, _=make_model(img_size, class_count) # recreate the model - so it starts fresh with new initialized weights\nask_epoch=5 # epoch to query user\nepochs=40  # total epochs to run unless halted by the callback\ncallbacks=[SLR(model, epochs,  ask_epoch)] # instantiate the callback\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","3651e836":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, _=make_model(img_size, class_count) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\ntr_thold=.98 # set training accuracy threshold\nv_thold=.95 # set validation accuracy threshold\ncallbacks=[SOMT(model, tr_thold, v_thold)] # instantiate the callback\n# ****************NOTE - set verbose=0 in model.fit so printouts do not conflict**********************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","0896556f":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# ************NOTE base_model trainable set equal to False in code below when model is created*************\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5\ncallbacks=[TLC(model, base_model, epochs,  ask_epoch)] # instantiate the callback\n\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","4566f82e":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5\nbatches=int(np.ceil(len(train_gen.labels)\/batch_size)) # number of batches in an epoch is number of samples\/batch_size used in generator\n# *****NOTE the SPREADSHEET callback requires the print_in_color function be present\ncallbacks=[SPREADSHEET(model, base_model, epochs,  ask_epoch, batches)] # instantiate the callback\n#************NOTE set verbose=0 to prevent model.fit printout*******************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n\n","ef817de6":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# **********NOTE set an initial large learning rate so as to induce need to reduce it as epochs increase\nmodel, base_model=make_model(img_size, class_count, lr=.05, trainable=True) # recreate the model - so it starts fresh with new initialized weights\nepochs=30 #  total epochs to run unless halted by the callback\nfactor= .5 # if validation loss this epoch>previous epoch new_lr= lr * factor\nverbose=1 # print out when learning rate is adjusted\ncallbacks=[DWELL(model, factor, verbose)] # instantiate the callback\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n\n","eb2ecd0f":"del model # delete the current model\nK.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\nmodel, base_model=make_model(img_size, class_count, trainable=False) # recreate the model - so it starts fresh with new initialized weights\nepochs=40 #  total epochs to run unless halted by the callback\nask_epoch=5 # at the end of the 5th epoch user is queried for input.\nbatches=np.ceil( len(train_gen)\/batch_size) # number of batches in an epoch is number of samples\/batch_size used in generator\n# *****NOTE the LRA callback requires the print_in_color function be present\npatience=1 # if monitored metric ddoes not improve for patience epochs then adjust the learning rate\nstop_patience=4 # halt training if the lr has been adjust stop patience number of times with no improvement in metric being monitored\ntr_thold=.9 # Callback initially monitor training accuracy. If it exceeds the value of tr_thold the callback switches to monitor validation loss\ndwell=True # If lr is adjusted and dwell=True model weights are set to the weights from the epoch with the best metric performance thus far\ncallbacks=[LRA(model, base_model, patience,stop_patience, tr_thold, factor, dwell, batches, epochs, ask_epoch)] # instantiate the callback\n# Note we are doing transfer learning with the base_model initially set as not trainable\n#************NOTE set verbose=0 to prevent model.fit printout*******************\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","93f10a25":"<a id=\"ask\"><\/a>\n# <center>ASK Callback Definition<\/center>","ba694163":"### If you wish to utilize any of these callbacks just copy the appropriate callback definition code.\n### Read the description for the callback that is supplied and see the associated demo for the\n### callback to see how it performs and produces output.\n### if you find any errors please advise of same\n### If you find this notebook of use please consider up voting - thank you","5c8f80d6":"<a id=\"askdemo\"><\/a>\n# <center>ASK Callback Demonstration<\/center>","dc8a8115":"<a id=\"dwell\"><\/a>\n# <center>DWELL Callback Definition<\/center>","5619e923":"### The SOMT callback is useful to end training based on the value of the training accuracy or the validation loss or both.\nThe form of use is callbacks=[SOMT(model, train_thold, valid_thold)] where\n\n* model is the name of your complied model\n* train_thold is a float. It is the value of accuracy (in Percent) that must be achieved by the model in order to conditionally stop training\n* valid_threshold is a float. It is the value of validation accuracy (in Percent) that must be achieved by the model\n  in order to conditionally stop training\n\nNote to stop training BOTH the train_thold and valid_thold must be exceeded in the SAME epoch.   \nIf you want to stop training based soley on the training accuracy set the valid_thold to 0.0.  \nSimilarly if you want to stop training on just validation accuracy set train_thold= 0.0.   \nNote if both thresholds are not achieved in the same epoch training will continue until the value of epochs  \nset in model.fit is reached. For example lets take the case that you want to stop training when the   \ntraining accuracy has reached or exceeded 95 % and the validation accuracy has achieved at least 85%   \nthen the code would be callbacks=[SOMT(my_model, .95, .85)]","f80fa9ff":"<a id=\"somtdemo\"><\/a>\n# <center>SOMT Callback Demonstration<\/center>","39a26dd7":"## train the model ","7f7f9b39":"The TLC callback is a variation of the ASK callback that is useful in the case where you are doing transfer learning.   \nThe form of use is callbacks=[TLC(model, base_model,epochs, ask_epoch)] where\n* model is the name of your compiled model\n* base_model is the name of the base model providing transfer learning. For example in the code below I create a model using the\n  EfficientNetB3 as the base_model. \n   * base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n   * base_model.trainable= False    \n   * x=base_model.output \n   * x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n   * x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n     bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n   * x=Dropout(rate=.45, seed=123)(x)\n   * output=Dense(class_count, activation='softmax')(x) \n   * my_model=Model(inputs=base_model.input, outputs=output) \n   * my_model.compile(Adamax(lr=.005), loss='categorical_crossentropy', metrics=['accuracy'])\n     Note make sure you include base_model.trainable=False so that when queried you can provide an input T to make the base_model trainable when asked\n  \n  \n * ask_epoch is an integer. Lets it is set to 5. In that case at the end of the 5th epoch you will be queried to enter either  \n   T to make the base_model trainable or enter an integer say 4. In that case at the end of the 9th epoch you will be queried again.  \n   If you enter T to make the base_model trainable you will then be asked to enter an integer N say you enter 3.   \n   Then you will train the model for 3 more epochs then be asked to either enter H to halt training or enter an integer  \n   for how many more epochs to run then be queried again.  \n * epochs is the value of epochs you used in model.fit","036209e8":"<a id=\"slrdemo\"><\/a>\n# <center>SLR Callback Demonstration<\/center>","7adbcb3a":"## train_df is  balanced, create train, test and validation generators ","a42e3c49":"## Use transfer learning with EfficientNetB3 model\nThis function creates the model. It is set up for transfer learning. Parameter trainable determines\nif the base model is trainable. The TLC callback can be used to control this state of the base model\nfor those that want to start training with the base model not trainable then after a specified number\nof epochs make the base_model trainable","db372cb9":"The LRA callback is somewhat a combination of the DWELL and SPREADSHEET callback with additional features.  \nThe form of the callback is   \ncallbacks=[LRA(model, base_model, patience,stop_patience, threshold, factor, dwell, batches,epochs, ask_epoch)] where  \n* model is the name of your compiled model\n* base_model is the name of the base_model used for transfer learning. If not doing transfer learning set base_model=None\n* patience is an integer. This callback initially monitors the training accuracy. If the training accuracy fails to improve  \n  after patience number of epochs the learning rate is reduced where new_lr=old_lr * factor.   \n  The training accuracy is monitored until it achieves or exceeds the float value threshold.   \n  At that point the callback now monitors validation loss.  \n  If the validation loss fails to decrease for patience number of epochs the learning rate is adjusted.   \n  The callback always saves the weights for the epoch with the lowest validation loss.  \n  At the conclusion of training the best weights are loaded  into the model.\n* stop_patience is an integer. If stop_patience number of consecutive epochs occur where the learning rate was reduced  \n   but the metric being monitored idi not improve the training is halted.\n* threshold is a float between 0 and 1.0. It is the level the training accuracy must meet or exceed for the callback to   \n   switch to monitoring validation loss.\n* factor is a float between 0 and 1.0. It determines the new learning rate by the formula new_lr=old_lr * factor \n* dwell is a boolean. When dwell=True the callback monitors the metric. If the metric fails to improve on the current epoch    \n   it means you have moved to a point in Nspace(N is the number of trainable parameters of the model) that is NOT as good in  \n   terms of the metric value as was the point for the epoch with the best metric performance. So the callback loads the   \n   model with the weights from that epoch. It then reduces the learning rate and continues training. If dwell=False the  \n   scenario described does not occur. However your model always ends up with the weights for the epoch with the best metric  \n   performance laded in the model.\n* batches is an integer used only for printing purposes. It's value should be the same as train_steps, that is calculated as  \n   batches = number of training samples\/\/batch_size +1. For example if you have 1001 samples and your batch_size is 10  \n   then batches= 10 + 1 = 11.\n* epochs is an integer and is the value of epochs you use for model.fit\n* ask_epoch is an integer. Lets say it is set to a value N. After the model trains for N epochs the user is queired to provide    \n   input. If an integer is entered say M then training will continue for M more epochs and at the end of epoch M + N the    \n   user is queried again. If the user enters another integer the process continues until epochs number of epochs is run.  \n   If the user is doing transfer learning whene the vale of base_model is NOT None at the query you can enter a T. This will    \n   make the base_model trainable. After entering T you will be queried to enter an integer for how many more epochs to run  \n   before you are queried again. At any query you have the option to enter H to halt training.  \n   \n See the LRA callback demo for demonstration of use.","2ffbdf67":"### create a dataframe of the form filepaths(path to the image file), labels (class label of the image file)","563bc790":"<a id=\"import\"><\/a>\n# <center>Import Need Modules<\/center>","b2d0ca3a":"<a id=\"spreadsheetdemo\"><\/a>\n# <center>SPREADSHEET Callback Demonstration<\/center>","7ebed118":"<a id=\"tlcdemo\"><\/a>\n# <center>TLC Callback Demonstration<\/center>","3cbe93ae":"The DWELL callback if useful when training a model and the validation loss on the current epoch exceeds that of the previous epoch.    \nWhen that is the case your model has moved to a point in Nspace(N being the number of trainable parameters) that is less favorable  \nthan that for the previous epoch. What the callback does is to detect this condition and if it occurs it sets the model weights to  \nthose of the  epoch with the lowesy validation loss. It also reduces the learning rate.  \nIf you do not change the learning rate on the next epoch you would end up in the same unfavorable point in Nspace.  \nUse of the callback is of the form    \ncallbacks=[DWELL(model, factor, verbose)] where :\n* model is the name of your compiled model\n* factor is a float between 0 and 1. The new learning rate is given by new_lr= current_lr * factor\n* verbose is a boolean. If verbose is True if the condition occurs, a print out is generated for the current epoch that advises the  \n  model weights have been set to the weights of the epoch with the lowest validation loss and it prints out the reduced learning rate ","40dbab33":"## [1. Import Needed Modules](#import) ##\n## [2. ASK Callback Definition](#ask) ## \n## [3. SLR Callback Definition](#slr) ##\n## [4. SOMT Callback Definition](#somt) ## \n## [5. TLC Callback Definition](#tlc) ## \n## [6. SPREADSHEET Callback Definition](#spreadsheet) ## \n## [7. DWELL Callback Definition](#dwell) ## \n## [8. LRA Callback Definition](#lra) ## \n## [9. Code to Setup Callback Demonstrations](#setup) ## \n## [10. Demonstration of ASK Callback](#askdemo) ## \n## [11. Demonstration of SLR Callback](#slrdemo) ##\n## [12. Demonstration of SOMT Callback](#somtdemo) ##\n## [13. Demonstration of TLC Callback](#tlcdemo) ##\n## [14. Demonstration of SPREADSHEET Callback](#spreadsheetdemo) ##\n## [15. Demonstration of DWELL Callback](#dwelldemo) ##\n## [16. Demonstration of DWELL Callback](#dwelldemo) ##","18bb0979":"<a id=\"lra\"><\/a>\n# <center>LRA Callback Definition<\/center>","1c1ff5af":"<a id=\"spreadsheet\"><\/a>\n# <center>SPREADSHEET Callback Definition<\/center>","794eb0d6":"The SPREADSHEET callback formats the printout used during training in a spreadsheet style. It has the same fuctionality  \nas the TLC callback but prints out the training data in the example format shown below.  \nThe form of use is  callbacks=[SPREADSHEET(model, base_model, epochs, ask_epoch, batches)] where  \n* model is the name of your compiled model\n* base_model is the name of the base model providing transfer learning.  \n  If you are not doing transfer learning set base_model=None.  \n  For example in the code below I create a model using the EfficientNetB3 as the base_model.   \n   * base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n   * base_model.trainable= False    \n   * x=base_model.output \n   * x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n   * x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n     bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n   * x=Dropout(rate=.45, seed=123)(x)\n   * output=Dense(class_count, activation='softmax')(x) \n   * my_model=Model(inputs=base_model.input, outputs=output) \n   * my_model.compile(Adamax(lr=.005), loss='categorical_crossentropy', metrics=['accuracy'])\n     Note make sure you include base_model.trainable=False so that when queried you can provide an input T to make the base_model trainable when asked\n  \n  \n * ask_epoch is an integer. Lets say it is set to 5. In that case at the end of the 5th epoch  \n   you will be queried to enter either T to make the base_model trainable or enter an integer say 4.  \n   In that case at the end of the 9th epoch you will be queried again.  \n   If you enter T to make the base_model trainable you will then be asked to enter an integer N say you enter 3.   \n   Then you will train the model for 3 more epochs then be asked to either enter H to halt training or enter an integer  \n   for how many more epochs to run then be queried again.\n * epochs is the value of epochs you used in model.fit\n * batches is an integer that defines the number of batches that are run per epoch.   \n   This can be calculated as the number of samples in the training set divided by the batch size.  \n   Use batches=np.celing(training samples\/batch_size). For example if your training  \n   set has say 1,000 images and a batch size of 50, then batches would be 20.\n  \n","1d5b3409":"<a id=\"tlc\"><\/a>\n# <center>TLC Callback Definition<\/center>","f7b97f89":"### The function below prints a text message in rgb foreground colors and rgb background colors\n### it is used by some  of the callbacks to provide a easier to see output","71248a8b":"# split df into a train_df, a test_df and a valid df","27bfdc6b":"### The ASK callback is useful when training a model and based on model performance metrics you can elect to halt or continue training.\nOften when you first run a model you are not sure of how man epochs to run. For example you may initially specify 5 epochs in\nmodel.fit and your model is training well so you would really like to run more epochs but to do that you have to start over\nand set epochs to say 15. Conversely you may specify epochs as 100 and your model performance maxes out metric wise at around\n10 epochs. So now you have t0 kill the kernel and rerun everything again with epochs set to 15. The ASK callbacks enables you\nto easily continue training for an additional number of epochs or halt training. The form of use of the callback is\ncallbacks=[ASK(model, epochs, ask_epoch)] where\n* model is the name of your compiled model\n* epochs is the number of epochs you specified in model.fit\n* ask_epoch is an integer. Assume it is set to a value N. When the model is training when it completes the Nth epoch you\n  will receive a printed query asking if you wish to halt train by entering an H, or to enter an integer say for example 5.\n  In that case training will continue up to epoch N+5 at which time you will be queried again. Note however training will\n  always end when epochs number of epochs has been run. Typically make epochs a large number like 100 so it does not end training\n  before you elect to do so.","3b167f04":"<a id=\"slr\"><\/a>\n# <center>SLR Callback Definition<\/center>","3def4b2c":"<a id=\"dwelldemo\"><\/a>\n# <center>DWELL Callback Demonstration<\/center>","900ff625":"### The SLR callback is useful if you wish to change the learning rate during training\nThe callback is a modification of the ASK callback above where when queired you can specify a new learning rate.  \nThe form of use of the callback is callbacks=[SLR(model, epochs, ask_epoch)] where\n* model is the name of your compiled model\n* epochs is the number of epochs you specified in model.fit\n* ask_epoch is an integer. Assume it is set to a value N. When the model is training when it completes the Nth epoch you\n  will receive a printed query asking if you wish to halt train by entering an H, or to enter an integer say for example 5\n  to continue training for 5 more epochs using the current learning rate, or enter A to adjusts the learning rate. If you\n  enter A you will be queired to enter a float value as the new learning rate. After entering the new learning rate you are\n  asked to enter an integer for the number of additional epochs to run then be queired again.","6bb26bd8":"## Below are a series of custom callbacks I have found to be very  useful when training a model.  \n## The use of these callbacks is demonstrated in the code below. To demonstrate I have selected\n## a dataset called Iris of Eye which I copied from Kaggle. It is a small data set that converges quickly\n## and  achieves good training and validation accuracies with short training time, so it is ideal\n## for demonstration of the callbacks.","9288dc84":"<a id=\"somt\"><\/a>\n# <center>SOMT Callback Definition<\/center>","0127c241":"## input an image and get the shape","53d68546":"<a id=\"lrademo\"><\/a>\n# <center>LRA Callback Demonstration<\/center>","63ba8044":"<a id=\"setup\"><\/a>\n# <center>Setup code to demo the callbacks<\/center>"}}