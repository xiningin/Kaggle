{"cell_type":{"9b8f3652":"code","8d148ca8":"code","4ba5ce41":"code","dfa7d918":"code","99386ee4":"code","0d90419a":"code","6e09e463":"code","fa1247cb":"code","ce90879e":"code","6d50f86e":"code","198d8188":"code","feb66b89":"code","3cef4573":"code","311cc26f":"code","c60a2f8c":"code","764d6c33":"code","7cdd8478":"code","8dd1d0c6":"code","217b4550":"code","2cc05d2f":"code","70734078":"code","08d6be2f":"code","1cc6163b":"code","f886a470":"code","d6fcb053":"code","f1611126":"code","0c03a2f1":"code","c73acddc":"code","3c87cd87":"code","3bf37352":"code","cc41b57e":"code","e2c31473":"code","9baeef94":"code","04dfcd6d":"code","45f6b5e0":"code","a221494a":"code","4c8a3f04":"code","782db2be":"code","3957fa08":"code","f5fc2d52":"code","18c4f1e9":"code","c38768c8":"code","f84ef4be":"code","b8023d99":"code","1bbfd8d9":"code","e56da3db":"code","2400cf23":"code","ab18c73e":"code","e38ef4ad":"code","3c250269":"code","80dc53d9":"code","9dd7706a":"code","0ff22d37":"code","e15342b5":"code","27d3a129":"code","6c9a3179":"code","f530409f":"code","451b23e8":"code","dcf4bba9":"code","7d4156f5":"code","61baf0ae":"code","4bd30689":"code","066475d1":"code","bf30b0e7":"code","e8f96f08":"code","e2e26644":"code","d72a7e66":"code","0062d112":"code","640674fb":"code","0da90728":"markdown","a3b9f3f4":"markdown","e0baabf8":"markdown","910efbb4":"markdown","495ad478":"markdown","a7f968f5":"markdown","56fd7e7e":"markdown","1189f134":"markdown","56b4bd29":"markdown","3b0c56b8":"markdown","e0390526":"markdown","38b7c34a":"markdown","049162ba":"markdown","ed8b0a10":"markdown","6139f3f9":"markdown","72ec7a14":"markdown","f4b848a9":"markdown","293c29b8":"markdown","8fd948c8":"markdown","0907f91d":"markdown","3c1f1ad7":"markdown","57488d73":"markdown","e680e1cc":"markdown","a9650ddc":"markdown","4f9762b6":"markdown","e1f59d05":"markdown","2743c186":"markdown","7cb610d9":"markdown","964306f9":"markdown","a194848f":"markdown"},"source":{"9b8f3652":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\nimport warnings\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nwarnings.filterwarnings('ignore')","8d148ca8":"df=pd.read_csv('..\/input\/birds-bones-and-living-habits\/bird.csv')\ndf.head()","4ba5ce41":"df.drop(columns='id', inplace=True)\ndf.info()","dfa7d918":"#visualize where NaN values are located in the dataset\nplt.figure(figsize=(6,5))\nsns.heatmap(df.isna());","99386ee4":"df[(df.isna().cumsum(axis=1).iloc[:,-1])>=2]","0d90419a":"df.dropna(axis=0, thresh=10, inplace=True)","6e09e463":"#drop all NaNs\ndf.dropna(how='any', inplace=True)","fa1247cb":"#how many samples for each class\ndf['type'].value_counts()","ce90879e":"sns.pairplot(df.iloc[:,:-1]);","6d50f86e":"#correlation heatmap\nplt.figure(figsize=(10,7))\nsns.heatmap(df.iloc[:,:-1].corr(), annot=True, fmt='.2f');","198d8188":"df=pd.read_csv('..\/input\/birds-bones-and-living-habits\/bird.csv')  \\\n.drop(columns=['id','ulnal','ulnaw','femw','tibw']).dropna(how='any')","feb66b89":"sns.pairplot(df, vars= df.columns[:-1], hue='type');","3cef4573":"plt.figure(figsize=(7,5))\nsns.heatmap(df.iloc[:,:-1].corr(), annot=True, fmt='.2f');","311cc26f":"#distributions per class\ndef boxes(features):\n    plt.figure(figsize=(12,5));\n    plt.subplot(1,2,1);\n    sns.boxplot(x='type', y=features[0], data=df);\n    plt.title(features[0]);\n    plt.subplot(1,2,2);\n    sns.boxplot(x='type', y=features[1], data=df);\n    plt.title(features[1]);\n\nboxes(df.columns[:2])\nboxes(df.columns[2:4])\nboxes(df.columns[4:6])","c60a2f8c":"#PCA\nsc=StandardScaler()\nx_sc=sc.fit_transform(df.iloc[:,:-1].values)\n\npca=PCA(n_components=2)\nx_pc=pca.fit_transform(x_sc)\n\nplt.figure(figsize=(10,7));\nsns.scatterplot(x_pc[:,0],x_pc[:,1],hue=df['type']);","764d6c33":"pca=PCA()\npca.fit(x_sc)\n\nsns.lineplot(x=np.arange(pca.components_.shape[0])+1, \n             y=pca.explained_variance_ratio_.cumsum());\nplt.ylim(0.0,1.0);\nplt.title('Cumulative explained variance of PCA');\nplt.xlabel('number of principal component');\nplt.ylabel('cumulative sum of explained variance');","7cdd8478":"sns.barplot(x=np.arange(pca.components_.shape[0])+1, y=pca.explained_variance_ratio_, color='grey')\nplt.title('Explained variance of PCA');\nplt.xlabel('Number of principal component');\nplt.ylabel('Explained variance');\n\nprint('Number of Principal Components needed to explain total variance:',\n      pca.components_.shape[0] )\nprint('Cumulative explained variance of first two components:',\n      pca.explained_variance_ratio_[:2].cumsum()[1])\nprint('Individual explained variance of first three components:',\n      pca.explained_variance_ratio_[:3])","8dd1d0c6":"#PCA on raw data\npc=PCA(n_components=2)\nx_dem=pc.fit_transform(df.iloc[:,:-1].values)\nplt.figure(figsize=(10,7));\nsns.scatterplot(x_dem[:,0], x_dem[:,1], hue=df['type']);","217b4550":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples","2cc05d2f":"#use 'elbow' method to find optimal cluster number\ninertia=[]\nfor i in range(1,11):\n    km=KMeans(n_clusters=i, random_state=33)\n    km.fit_predict(x_sc)\n    inertia.append(km.inertia_)\n    \n#inertia plot\nsns.lineplot(range(1,11),inertia);\nplt.title('Inertia');","70734078":"#five clusters\nkm=KMeans(n_clusters=5, random_state=33)\nclusters=km.fit_predict(x_sc)\n\n#Silhouette Graph\n#code from Sebastian Raschka's book 'Python Machine Learning'\nlabels=np.unique(clusters)\nn_clusters=labels.shape[0]\nsils=silhouette_samples(x_sc,clusters,metric='euclidean')\ny_ax_lower, y_ax_upper=0, 0\nyticks=[]\nplt.figure(figsize=(6,5))\nfor i,c in enumerate(labels):\n    cluster_sil=sils[clusters==c]\n    cluster_sil.sort()\n    y_ax_upper +=len(cluster_sil)\n    color=cm.jet(float(i)\/n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper),\n            cluster_sil, height=1.0,\n            edgecolor='none', color=color)\n    yticks.append((y_ax_lower+y_ax_upper)\/2.)\n    y_ax_lower+=len(cluster_sil)\nsilhouette_avg=np.mean(sils)\nplt.axvline(silhouette_avg,color='red', linestyle='--')\nplt.yticks(yticks,labels+1)\nplt.ylabel('Cluster')\nplt.xlabel('Silhouette coefficient')\nplt.show();\n\n#visualize clusters on PCA data\npca=PCA(n_components=2)\ndec=pca.fit_transform(x_sc)\nplt.figure(figsize=(9,6));\nsns.scatterplot(dec[:,0],dec[:,1], hue=clusters);","08d6be2f":"km=KMeans(n_clusters=6, random_state=33)\nclusters=km.fit_predict(x_sc)\n\n#Silhouette Graph\nlabels=np.unique(clusters)\nn_clusters=labels.shape[0]\nsils=silhouette_samples(x_sc,clusters,metric='euclidean')\ny_ax_lower, y_ax_upper=0, 0\nyticks=[]\nplt.figure(figsize=(6,5))\nfor i,c in enumerate(labels):\n    cluster_sil=sils[clusters==c]\n    cluster_sil.sort()\n    y_ax_upper +=len(cluster_sil)\n    color=cm.jet(float(i)\/n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper),\n            cluster_sil, height=1.0,\n            edgecolor='none', color=color)\n    yticks.append((y_ax_lower+y_ax_upper)\/2.)\n    y_ax_lower+=len(cluster_sil)\nsilhouette_avg=np.mean(sils)\nplt.axvline(silhouette_avg,color='red', linestyle='--')\nplt.yticks(yticks,labels+1)\nplt.ylabel('Cluster')\nplt.xlabel('Silhouette coefficient')\nplt.show();\n\n#visualize clusters on PCA data\npca=PCA(n_components=2)\ndec=pca.fit_transform(x_sc)\nplt.figure(figsize=(9,6));\nsns.scatterplot(dec[:,0],dec[:,1], hue=clusters);","1cc6163b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","f886a470":"# create train and test sets, data and labels\nx=x_sc.copy()\n\nenc=LabelEncoder()\ny=enc.fit_transform(df['type'])\nprint ('original classes:',enc.classes_)\n\nx_tr, x_ts, y_tr, y_ts=train_test_split(x,y,random_state=42)","d6fcb053":"#graph class proportions in the dataset\nplt.figure(figsize=(12,5));\nplt.subplot(1,2,1);\nplt.title('class_count');\nsns.countplot(x='type', data=df, color='grey');\nplt.subplot(1,2,2);\ndf['type'].value_counts().plot(kind='pie',autopct='%1.1f%%');\nplt.title('class_proportions');","f1611126":"#dummy classifications\nnp.random.RandomState(seed=7)\nprint('random selection accuracy:' ,\n accuracy_score(y,np.random.choice([0,1,2,3,4,5],(x.shape[0]))))\n\ndum=DummyClassifier(strategy='uniform', random_state=7)\ndum.fit(x,y)\nprint ('random classifier accuracy:',\n      accuracy_score (y, dum.predict(x)))\n\nprint(\"\")\nprint('majority class selection accuracy:' ,\n accuracy_score(y,np.random.choice([2,3],(x.shape[0]))))","0c03a2f1":"#training on the imbalanced data\nlr=LogisticRegression()\nlr.fit(x_tr,y_tr)\nprint('LogistReG accuracy:', accuracy_score(y_ts, lr.predict(x_ts) ))\nprint('LogistReG f1_score:', f1_score(y_ts, lr.predict(x_ts),average='macro' ))\n\nrf=RandomForestClassifier(1000, max_depth=8)\nrf.fit(x_tr,y_tr)\nprint('RandForest accuracy:', accuracy_score(y_ts,rf.predict(x_ts)))\nprint('RandForest f1-score:', f1_score(y_ts,rf.predict(x_ts), average='macro'))","c73acddc":"#first upsampling implementation\n#mere duplicating samples from minority classes\n\nprint('x_tr.shape=', x_tr.shape)\nprint('y_tr.shape=', y_tr.shape)\nprint('')\n\n\n#initializations\nmajority_class=np.argmax(np.bincount(y_tr))\nlength_majority_class = np.max(np.bincount(y_tr))\nadditional_samples=np.array([])\nadditional_labels=np.array([])\n\n\nfor i in np.unique(y_tr):\n\n    if i != majority_class:\n        length_this_class=np.bincount(y_tr)[i]\n        wanted=length_majority_class-length_this_class\n        print('class:',i)\n        print('%d additional samples' % wanted)\n        indexes=np.argwhere(y_tr==i)\n        indexes=np.reshape(indexes, (len(indexes),))\n        choices=np.random.choice(indexes,size=wanted)\n        temp_x=x_tr[choices]\n        additional_samples=np.append(additional_samples,temp_x, axis=None)\n        additional_labels=np.append(additional_labels,np.full(wanted,i), axis=None)\n        \n\n#new samples\nprint('')\nadditional_samples=np.reshape(additional_samples,(int(len(additional_samples)\/6),6))\nprint('new_samples.shape=', additional_samples.shape)\nprint('new_labels.shape=', additional_labels.shape)\n\n#new x_tr --> x_tr1\nflat_x = np.append(x_tr,additional_samples)\nx_columns_number = x_tr.shape[1]\nnew_row_number = int(len(flat_x)\/x_columns_number)\nx_tr1 = np.reshape(flat_x,(new_row_number,x_columns_number))\n\n#new y_tr --> y_tr1\nflat_y = np.append (y_tr, additional_labels)\nnew_shape = (len(y_tr)+len(additional_labels),)\ny_tr1 = np.reshape(flat_y, new_shape)\ny_tr1 = y_tr1.astype('int64')\n\nprint('')\nprint ('x_tr1.shape=',x_tr1.shape)\nprint ('y_tr1.shape=',y_tr1.shape)\nprint('')\nprint('number of samples for each class, before resampling:', np.bincount(y_tr))\nprint('number of samples for each class, after resampling: ', np.bincount(y_tr1))","3c87cd87":"#second upsampling implementation\n#duplicating samples plus generating synthetic ones\n#for every five samples, the fifth will be the average of the previous four\n#the previous four are duplicates of samples present in the original dataset\n\nprint('x_tr.shape=', x_tr.shape)\nprint('y_tr.shape=', y_tr.shape)\nprint('')\n\n\n#initializations\nmajority_class=np.argmax(np.bincount(y_tr))\nlength_majority_class = np.max(np.bincount(y_tr))\nadditional_samples=[]\nadditional_labels=[]\ncounter=0\n\n\nfor i in np.unique(y_tr):\n\n    if i != majority_class:\n        length_this_class=np.bincount(y_tr)[i]\n        wanted=length_majority_class-length_this_class\n        n_new_samples=0\n        indexes=np.argwhere(y_tr==i)\n        indexes=np.reshape(indexes, (len(indexes),))\n        print('class:',i)\n        print('%d additional samples' % wanted)\n        while n_new_samples < wanted:\n            n_new_samples +=1\n            choice_index=np.random.choice(indexes,size=1)\n            choice_sample= x_tr [choice_index]\n            additional_samples=np.append(additional_samples,choice_sample)\n            additional_labels=np.append(additional_labels,i)\n            counter +=1\n            if counter == 4:\n                additional_samples=np.reshape(additional_samples,\n                                              (int(len(additional_samples)\/6),6))\n                new_sample=np.mean(additional_samples[-4:,:],axis=0)\n                additional_samples=np.append(additional_samples,[new_sample])\n                additional_labels=np.append(additional_labels,i)\n                counter=0\n                n_new_samples += 1\n        if n_new_samples > wanted:\n            additional_samples=np.reshape(additional_samples,(int(len(additional_samples)\/6),6))\n            redundant = n_new_samples - wanted\n            additional_samples = additional_samples[:-redundant,:].copy()\n            additional_labels=additional_labels[:-redundant]\n      \n        \n\n#new samples\nprint('')\nprint('new_samples.shape=', additional_samples.shape)\nprint('new_labels.shape=', additional_labels.shape)\n\n#new x_tr --> x_tr2\nflat_x = np.append(x_tr,additional_samples)\nx_columns_number = x_tr.shape[1]\nnew_row_number = int(len(flat_x)\/x_columns_number)\nx_tr2 = np.reshape(flat_x,(new_row_number,x_columns_number))\n\n#new y_tr --> y_tr2\nflat_y = np.append (y_tr, additional_labels)\nnew_shape = (len(y_tr)+len(additional_labels),)\ny_tr2 = np.reshape(flat_y, new_shape)\ny_tr2 = y_tr2.astype('int64')\n\nprint('')\nprint ('x_tr2.shape=',x_tr2.shape)\nprint ('y_tr2.shape=',y_tr2.shape)\nprint('')\nprint('number of samples for each class, before resampling:', np.bincount(y_tr))\nprint('number of samples for each class, after resampling: ', np.bincount(y_tr2))","3bf37352":"#training on the new, balanced data\nlr=LogisticRegression()\nlr.fit(x_tr1,y_tr1)\nprint('LogistReG accuracy:', accuracy_score(y_ts, lr.predict(x_ts) ))\nprint('LogistReg f1-score:', f1_score(y_ts,lr.predict(x_ts), average='macro'))\n\nrf=RandomForestClassifier(1000, max_depth=8)\nrf.fit(x_tr1,y_tr1)\nprint('RandForest accuracy:', accuracy_score(y_ts,rf.predict(x_ts)))\nprint('RandForest f1-score:', f1_score(y_ts,rf.predict(x_ts), average='macro'))","cc41b57e":"#training on the new, balanced and synthetic data\nlr=LogisticRegression()\nlr.fit(x_tr2,y_tr2)\nprint('LogistReG accuracy:', accuracy_score(y_ts, lr.predict(x_ts) ))\nprint('LogistReg f1-score:', f1_score(y_ts,lr.predict(x_ts), average='macro'))\n\nrf=RandomForestClassifier(1000, max_depth=8)\nrf.fit(x_tr2,y_tr2)\nprint('RandForest accuracy:', accuracy_score(y_ts,rf.predict(x_ts)))\nprint('RandForest f1-score:', f1_score(y_ts,rf.predict(x_ts), average='macro'))","e2c31473":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\n\n#dataframes to store experimental results, for later comparison\nresults=pd.DataFrame([], \n    columns=['model', 'parameters','accuracy','precision','recall','F1-score'])\nresults1=pd.DataFrame([], \n    columns=['model', 'parameters','accuracy','precision','recall','F1-score'])\nresults2=pd.DataFrame([], \n    columns=['model', 'parameters','accuracy','precision','recall','F1-score'])\nmajority_vote_results=pd.DataFrame([], \n    columns=['training dataset','accuracy','precision','recall','F1-score' ])","9baeef94":"#Naive Bayes\ngnb=GaussianNB()\ngnb.fit(x_tr,y_tr)\ny_pred=gnb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Gaussian NB', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]\n","04dfcd6d":"#KNN\nknn=KNeighborsClassifier()\nknn.fit(x_tr,y_tr)\ny_pred=knn.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['KNN', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","45f6b5e0":"#SVM\nsvm=SVC()\nsvm.fit(x_tr,y_tr)\ny_pred=svm.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['SVM', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","a221494a":"# SGD\nsgd=SGDClassifier()\nsgd.fit(x_tr,y_tr)\ny_pred=sgd.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['SGD', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","4c8a3f04":"#SGD with different parameters\nsgd_m=SGDClassifier(alpha=0.01, loss='modified_huber', penalty='l1')\nsgd_m.fit(x_tr,y_tr)\ny_pred=sgd_m.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['SGD 2', 'alpha=0.01, loss=modified_huber, penalty=l1', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","782db2be":"# Logistic Regression\nlr=LogisticRegression()\nlr.fit(x_tr,y_tr)\ny_pred=lr.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Logistic Regression', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","3957fa08":"#Decision Tree\ntree=DecisionTreeClassifier(max_depth=5, random_state=42)\ntree.fit(x_tr,y_tr)\ny_pred=tree.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Decision Tree', 'max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","f5fc2d52":"#Random Forest\nrf=RandomForestClassifier(n_estimators=1000, max_depth=7, random_state=42)\nrf.fit(x_tr,y_tr)\ny_pred=rf.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Random Forest', 'n_estimators=1000, max_depth=7', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","18c4f1e9":"#Ada Boost\ntree_ada=DecisionTreeClassifier(criterion='entropy',random_state=1, max_depth=5)\nada=AdaBoostClassifier(base_estimator=tree_ada,n_estimators=1000,learning_rate=0.1, random_state=5)\nada.fit(x_tr,y_tr)\ny_pred=ada.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Ada Boost', 'criterion=entropy, max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","c38768c8":"#Gradient Boosted Tree\ngb=GradientBoostingClassifier(random_state=5, n_estimators=1000, max_depth=4)\ngb.fit(x_tr,y_tr)\ny_pred=gb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults=results.append(pd.DataFrame([['Gradient Boosted Classifier', 'n_estimators=1000, max_depth=4',\n                                      accuracy,precision,recall,f1]],\n                                    columns=list(results.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults.iloc[-1:]","f84ef4be":"results= results.reset_index().drop(columns='index')\nresults","b8023d99":"# majority vote implementation\npred_knn = knn.predict(x_ts) \npred_svm = svm.predict(x_ts)\npred_sgd_m= sgd_m.predict(x_ts)\npred_lr= lr.predict(x_ts)\npred_tree= tree.predict(x_ts)\npred_rf= rf.predict(x_ts)\npred_ada=ada.predict(x_ts)\npred_gb=gb.predict(x_ts)\n\nmode=[]\nfor i in np.arange(y_ts.shape[0]):\n    votes=np.array([pred_knn[i], pred_svm[i], pred_sgd_m[i],pred_lr[i],\n                    pred_tree[i], pred_rf[i], pred_ada[i], pred_gb[i]])\n    bincount=np.bincount(votes)\n    mode=np.append(mode,np.argmax(bincount)) #mode is the most common vote, the majority vote\n\n\nmajority_vote_results=majority_vote_results.append(pd.DataFrame([['original',\n            accuracy_score(y_ts, mode),precision_score(y_ts, mode,average='macro'),\n            recall_score(y_ts, mode,average='macro'),f1_score(y_ts, mode,average='macro')]],\n            columns=list(majority_vote_results.columns)))\nmajority_vote_results.iloc[-1:]","1bbfd8d9":"#Naive Bayes\ngnb1=GaussianNB()\ngnb1.fit(x_tr1,y_tr1)\ny_pred=gnb1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Gaussian NB', 'default', accuracy,precision,recall,f1]],columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]\n","e56da3db":"#KNN\nknn1=KNeighborsClassifier()\nknn1.fit(x_tr1,y_tr1)\ny_pred=knn1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['KNN', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","2400cf23":"#SVM\nsvm1=SVC()\nsvm1.fit(x_tr1,y_tr1)\ny_pred=svm1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['SVM', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","ab18c73e":"#SGD\nsgd1=SGDClassifier()\nsgd1.fit(x_tr1,y_tr1)\ny_pred=sgd1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['SGD', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","e38ef4ad":"#SGD with diferrent parameters\nsgd_m1=SGDClassifier(alpha=0.01, loss='modified_huber', penalty='l1')\nsgd_m1.fit(x_tr1,y_tr1)\ny_pred=sgd_m1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['SGD 2', 'alpha=0.01, loss=modified_huber, penalty=l1', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","3c250269":"#Logistic Regression\nlr1=LogisticRegression()\nlr1.fit(x_tr1,y_tr1)\ny_pred=lr1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Logistic Regression', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","80dc53d9":"#Decision Tree\ntree1=DecisionTreeClassifier(max_depth=5, random_state=42)\ntree1.fit(x_tr1,y_tr1)\ny_pred=tree1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Decision Tree', 'max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","9dd7706a":"#Random Forest\nrf1=RandomForestClassifier(n_estimators=1000, max_depth=7, random_state=42)\nrf1.fit(x_tr1,y_tr1)\ny_pred=rf1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Random Forest', 'n_estimators=1000, max_depth=7', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","0ff22d37":"#Ada Boost\ntree_ada1=DecisionTreeClassifier(criterion='entropy',random_state=1, max_depth=5)\nada1=AdaBoostClassifier(base_estimator=tree_ada1,n_estimators=1000,learning_rate=0.1, random_state=5)\nada1.fit(x_tr1,y_tr1)\ny_pred=ada1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Ada Boost', 'criterion=entropy, max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","e15342b5":"#Gradient Boosted Tree\ngb1=GradientBoostingClassifier(random_state=5, n_estimators=1000, max_depth=4)\ngb1.fit(x_tr1,y_tr1)\ny_pred=gb1.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults1=results1.append(pd.DataFrame([['Gradient Boosted Classifier', 'n_estimators=1000, max_depth=4',\n                                      accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults1.iloc[-1:]","27d3a129":"results1=results1.reset_index().drop(columns='index')\nresults1","6c9a3179":"# majority vote implementation\npred_knn1 = knn1.predict(x_ts) \npred_svm1 = svm1.predict(x_ts)\npred_sgd_m1= sgd_m1.predict(x_ts)\npred_lr1= lr1.predict(x_ts)\npred_tree1= tree1.predict(x_ts)\npred_rf1= rf1.predict(x_ts)\npred_ada1=ada1.predict(x_ts)\npred_gb1=gb1.predict(x_ts)\n\nmode1=[]\nfor i in np.arange(y_ts.shape[0]):\n    votes=np.array([pred_knn1[i], pred_svm1[i], pred_sgd_m1[i],pred_lr1[i],\n                    pred_tree1[i], pred_rf1[i], pred_ada1[i], pred_gb1[i]])\n    bincount=np.bincount(votes)\n    mode1=np.append(mode1,np.argmax(bincount)) #mode is the most common vote, the majority vote\n\n\nmajority_vote_results=majority_vote_results.append(pd.DataFrame([['1st resampled',\n            accuracy_score(y_ts, mode1),precision_score(y_ts, mode1,average='macro'),\n            recall_score(y_ts, mode1,average='macro'),f1_score(y_ts, mode1,average='macro')]],\n            columns=list(majority_vote_results.columns)))\nmajority_vote_results","f530409f":"#Naive Bayes\ngnb2=GaussianNB()\ngnb2.fit(x_tr2,y_tr2)\ny_pred=gnb2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Gaussian NB', 'default', accuracy,precision,recall,f1]],columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]\n","451b23e8":"#KNN\nknn2=KNeighborsClassifier()\nknn2.fit(x_tr2,y_tr2)\ny_pred=knn2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['KNN', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","dcf4bba9":"#SVM\nsvm2=SVC()\nsvm2.fit(x_tr2,y_tr2)\ny_pred=svm2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['SVM', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","7d4156f5":"#SGD\nsgd2=SGDClassifier()\nsgd2.fit(x_tr2,y_tr2)\ny_pred=sgd2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['SGD', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","61baf0ae":"#SGD with different parameters\nsgd_m2=SGDClassifier(alpha=0.01, loss='modified_huber', penalty='l1')\nsgd_m2.fit(x_tr2,y_tr2)\ny_pred=sgd_m2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['SGD 2', 'alpha=0.01, loss=modified_huber, penalty=l1', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","4bd30689":"#Logistic Regression\nlr2=LogisticRegression()\nlr2.fit(x_tr2,y_tr2)\ny_pred=lr2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Logistic Regression', 'default', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","066475d1":"#Decision Tree\ntree2=DecisionTreeClassifier(max_depth=5, random_state=42)\ntree2.fit(x_tr2,y_tr2)\ny_pred=tree2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Decision Tree', 'max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","bf30b0e7":"#Random Forest\nrf2=RandomForestClassifier(n_estimators=1000, max_depth=7, random_state=42)\nrf2.fit(x_tr2,y_tr2)\ny_pred=rf2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Random Forest', 'n_estimators=1000, max_depth=7', accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","e8f96f08":"#Ada Boost\ntree_ada2=DecisionTreeClassifier(criterion='entropy',random_state=1, max_depth=5)\nada2=AdaBoostClassifier(base_estimator=tree_ada2,n_estimators=1000,learning_rate=0.1, random_state=5)\nada2.fit(x_tr2,y_tr2)\ny_pred=ada2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Ada Boost', 'criterion=entropy, max_depth=5', accuracy,precision,recall,f1]],\n                                    columns=list(results1.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","e2e26644":"#Gradient Boosting Tree\ngb2=GradientBoostingClassifier(random_state=5, n_estimators=1000, max_depth=4)\ngb2.fit(x_tr2,y_tr2)\ny_pred=gb2.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred, average='macro')\nrecall=recall_score(y_ts,y_pred, average='macro')\nf1=f1_score(y_ts,y_pred, average='macro')\nresults2=results2.append(pd.DataFrame([['Gradient Boosted Classifier', 'n_estimators=1000, max_depth=4',\n                                      accuracy,precision,recall,f1]],\n                                    columns=list(results2.columns)))\nsns.heatmap(confusion_matrix(y_ts,y_pred),xticklabels= enc.classes_,\n            yticklabels=enc.classes_, annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\nresults2.iloc[-1:]","d72a7e66":"results2=results2.reset_index().drop(columns='index')\nresults2","0062d112":"# majority vote implementation\npred_knn2 = knn2.predict(x_ts) \npred_svm2 = svm2.predict(x_ts)\npred_sgd_m2= sgd_m2.predict(x_ts)\npred_lr2= lr2.predict(x_ts)\npred_tree2= tree2.predict(x_ts)\npred_rf2= rf2.predict(x_ts)\npred_ada2=ada2.predict(x_ts)\npred_gb2=gb2.predict(x_ts)\n\nmode2=[]\nfor i in np.arange(y_ts.shape[0]):\n    votes=np.array([pred_knn2[i], pred_svm2[i], pred_sgd_m2[i],pred_lr2[i],\n                    pred_tree2[i], pred_rf2[i], pred_ada2[i], pred_gb2[i]])\n    bincount=np.bincount(votes)\n    mode2=np.append(mode2,np.argmax(bincount)) #mode is the most common vote, the majority vote\n\n\nmajority_vote_results=majority_vote_results.append(pd.DataFrame([['2nd resampled plus synthetic',\n            accuracy_score(y_ts, mode2),precision_score(y_ts, mode2,average='macro'),\n            recall_score(y_ts, mode2,average='macro'),f1_score(y_ts, mode2,average='macro')]],\n            columns=list(majority_vote_results.columns)))\nmajority_vote_results","640674fb":"display(results)\ndisplay(results1)\ndisplay(results2)","0da90728":"Clearly the third dataset with the synthetic data didn't perform as well. Let's try a majority vote.","a3b9f3f4":"Interestingly, the raw-data PCA scatterplot doesn't look much different than the standardized-data PCA scatterplot. This must be because the data are in similar scales, measurements of length and diameter of bones.\n\nNow, before we start applying machine learning models we will perform a little experiment. Since the data seems hard to separate, and since we already know the class each instance belongs to, it will be interesting to try out unsupervised clustering and see how it works out.","e0baabf8":"Given the 56% accuracy we'd expect from a classifier that only classifies samples in the two majority classes, and classifies data belonging to these two classes correctly, the performance of the two models isn't mindblowing.\n\nTo deal with this, we will resample the training set.\n\nWe will keep the original dataset, but we will also create two balanced ones, and we'll try our models on all three datasets and compare the results. The first balanced dataset will be generated simply by oversampling the minority classes. For the second one, 80% of the additional data will be duplicated samples, and 20% will be generated by averaging samples from the minority classes. (Specifically, four samples will be duplicates of the original entries and the fifth will be the mean of the previous four, continuing this until each class expands and reaches the same number of samples as the majority class)","910efbb4":"These are the rows that have two or more NaN values:","495ad478":"The dataset has six classes, and contains info about six ecological bird groups:\n*Swimming Birds.\n*Wading Birds. *Terrestrial Birds. *Raptors. *Scansorial Birds. *Singing Birds.\n\nEach sample\/bird is comprised of ten measurements, the length and diameter of five bones.","a7f968f5":"Clearly the Gradient Boosted Tree is superior, followed by Random Forest and AdaBoost. Let's implement a majority vote (leaving Naive Bayes out) for comparison","56fd7e7e":"From the 'elbow' method it seems that the optimal cluster number is five, although we know there are six classes. We will try with both five and six clusters.","1189f134":"# Bird Classification\n#### with Clustering Experiment and Two Upsampling Implementations","56b4bd29":"The boxplots reveal that the distributions of the classes often fall within similar ranges. It's hard to identify a feature with high significance. The models we'll later train  probably won't reach high accuracy, as the data don't contain enough information relevant to class discrimination. \n\nThe dataset is about the size of bones of different bird categories, but it seems different categories share similar bone sizes, making our classification task harder.\n\nLet's draw a PCA scatterplot to inspect visually the degree of separability of the data","3b0c56b8":"# Machine Learning","e0390526":"The clustering only seems to work on the 'SO' class, and perhaps a little bit on the 'SW'. We have data that are very hard to cluster, and will probably be hard to classify.\n\nLet's see","38b7c34a":"Here too Gradient Boosted Tree excels on all metrics, followed by AdaBoost andRandom Forest. Let's implement a majority vote (leaving Naive Bayes out) for comparison","049162ba":"In the test set there are no synthetic data. Only the training set contains 20% of synthetic data.","ed8b0a10":"> It surpasses the Gradient Boosted Tree.\n\nLet's try our models on the two resampled training sets","6139f3f9":"Before we start training models, we must consider that the dataset is imbalanced, as shown below","72ec7a14":"### Training Models (on original, imbalanced training set)","f4b848a9":"This will pose problems to our machine learning. To demonstrate this, below is how a classifier that makes random predictions would perform, as well as a classifier that classifies all samples as within one of the two majority classes","293c29b8":"It seems that the best performance on the test set comes from Gradient Boosting Classifier trained on the original, imbalanced training data. \n\nWhereas other models were caused to fail by the imbalanced data, Gradient Boosting Tree and Random Forest didn't meet any problems and in fact, they performed better with the original than with the upsampled training sets.","8fd948c8":"We'll draw a correlation heatmap to identify features that correlate highly with each other. For each pair of such features, we will remove one of them from the dataset, otherwise it's as if we use the same column twice.\n\nWe'll be watching out for correlation coefficients > 0.93, a somewhat arbitrarily chosen threshold, but certainly high enough as not to throw out important information.","0907f91d":"## *Clustering side experiment","3c1f1ad7":"### Training Models (on second resampled training set, with synthetic data)","57488d73":"Now, after we generated the new data and did a quick test with Logistic Regression and Random Forest, let's go through the entire process of model training","e680e1cc":"A random classifier will yield accuracy ~17%.\n\nA classifier that randomly assigns every sample to one of the two majority classes would yield accuracy ~27%, which is about half of the percentage of these two classes' presence on the dataset. If the classifier would learn correctly to distinguish between these two classes, and also classified other samples to the class (from the two majority classes) they're closer to,  accuracy would double at ~54%.\n\nWith that in mind, lets see how two models, trained on the imbalanced dataset, perform.","a9650ddc":"### Preprocessing","4f9762b6":"Since the redundant columns that we'll drop previously contained many of the dataset's NaN values, we will re-import the dataset and first drop these columns before removing NaN values. Now the dataset contains 414 entries instead of 413.","e1f59d05":"In the PCA scatterplot, using two Principal Components, the data don't seem well-separated. Would a higher number of Principal Components alleviate this, or is the data inherently hard to separate? Let's draw a graph of the explained variance of the Principal Components:","2743c186":"The two remaining NaN values turn out to belong to the majority class ('SO'), which contains double and more instances than most of the other classes, so we may just remove it.","7cb610d9":"From the above data and measuments we see that the first two Components account for 92,53% of the total variance, and that adding a third Component would increase explained variance only by an insignificant 4,86%. \n\nTherefore, the problem doesn't lie in the number of Principal Components we chose, but in the inherent data distributions. This was also evident from the boxplots and the kde plots of the pairgrid we previously inspected.\n","964306f9":"### Training Models (on first resampled training set)","a194848f":"Let's erase them"}}