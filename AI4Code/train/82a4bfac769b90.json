{"cell_type":{"9d5c0562":"code","83f9e947":"code","8d37a09c":"code","eeffdd57":"code","4a24b765":"code","c5b7f1af":"code","7e251baf":"code","b7586a2c":"code","96d03894":"code","d5869bd1":"code","19934f64":"code","7db80051":"code","fba12f79":"code","ddce3530":"code","fc51c645":"code","cd2ae37c":"code","3f79c9e7":"code","95271e78":"code","a55bb5fb":"code","b449a137":"code","a09c0b38":"code","81ed0b1c":"code","8d2b44bc":"code","661de9e5":"markdown","77d5a0fe":"markdown","ddb84907":"markdown","7c9dcaaa":"markdown","f7f13e5f":"markdown","c3b64682":"markdown","e568a289":"markdown","e792abe8":"markdown","8b4a0a6f":"markdown","e5f9a770":"markdown","f5ed47e9":"markdown","125be923":"markdown","81a25e64":"markdown","032a5164":"markdown","818e488c":"markdown","b7c32f9f":"markdown","3d8e99e8":"markdown","c41e2e61":"markdown","bfd8c070":"markdown","7c960654":"markdown"},"source":{"9d5c0562":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83f9e947":"#read data from csv file\ndf = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","8d37a09c":"df.head()","eeffdd57":"df.info()","4a24b765":"#Drop id and Unnamed: 32 columns\ndf.drop(['id','Unnamed: 32'],inplace = True, axis = 1)\n\n#Convert Diagnosis from categorical to numeric\ndf.diagnosis = df.diagnosis.map({\"M\":1,\"B\":0})\n\n#Show final info of dataframe\ndf.info()","c5b7f1af":"#create X and Y objects\nX = df.drop([\"diagnosis\"],axis = 1)\nY = df.diagnosis.values.reshape(-1,1)\n\n#create test and train data sets\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state = 0,test_size = 0.2)\n\nprint(\"X_train Shape:\",X_train.shape)\nprint(\"X_test Shape:\",X_test.shape)\nprint(\"Y_train Shape:\",Y_train.shape)\nprint(\"Y_test Shape:\",Y_test.shape)\n\nresult_train = {}\nresult_test = {}","7e251baf":"#This function concatenate test and train scores in a dataframe\ndef prepare_dataframe(test_score_dict,train_score_dict,key,columns):\n    df_test = pd.DataFrame(test_score_dict,index = [\"Test Score\"])\n    df_train = pd.DataFrame(train_score_dict,index = [\"Train Score\"])\n    np_result = np.concatenate([df_test,df_train],axis = 0)\n    df_result = pd.DataFrame(np_result)\n\n    df_result.index = [\"Test Score\",\"Train Score\"]\n    df_result.columns = [key + str(c) for c in columns]  \n    return df_result","b7586a2c":"#Implementing KNN algoritms with different parameters\ndef knn_model(n_neighbors = 5,weights = 'uniform',algorithm = 'auto',p = 2):\n    knn = KNeighborsClassifier(n_neighbors = n_neighbors,\n                               weights = weights,\n                               algorithm = algorithm,\n                               p = p\n                              )\n    \n    accuracies_train = cross_val_score(estimator = knn, X = X_train, y = Y_train, cv = 3)\n    train_score = np.mean(accuracies_train)\n    \n    knn.fit(X_train,Y_train)\n    test_score = knn.score(X_test,Y_test)\n   \n    return train_score, test_score\n       \n","96d03894":"#KNN algorithm implementation with default parameters\ntrain_score,test_score = knn_model()\nresult_train[\"Default-Train\"] = train_score\nresult_test[\"Default-Test\"] = test_score\nprint(\"Mean accuracy of train set:\",train_score)\nprint(\"Mean accuracy of test set:\", test_score) ","d5869bd1":"#KNN algorith implementation with different n_neighbors \nk_list = list(np.arange(1,285))\ntest_score_dict = {}\ntrain_score_dict = {}\n\nfor k in k_list:\n    train_score,test_score = knn_model(n_neighbors = k)\n    train_score_dict[k] = (train_score)\n    test_score_dict[k] = (test_score)\n    \ndf_result = prepare_dataframe(test_score_dict,train_score_dict,\"K = \",k_list)\ndf_result\n","19934f64":"#Plot score of different k values for test and train datas\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.plot(k_list,list(train_score_dict.values()))\nplt.xlabel(\"K Value\")\nplt.ylabel(\"Train Score\")\n\nplt.subplot(1,2,2)\nplt.plot(k_list,list(test_score_dict.values()))\nplt.xlabel(\"K Value\")\nplt.ylabel(\"Test Score\")\nplt.show()","7db80051":"#Results stored in dictionaries\nresult_train[\"K-Best-Train\"] = np.asarray(list(train_score_dict.values())).max()\nresult_test[\"K-Best-Test\"] = np.asarray(list(test_score_dict.values())).max()","fba12f79":"#KNN algorith implementation with different weights \nw_list = ['uniform','distance']\n\ntest_score_dict = {}\ntrain_score_dict = {}\n\nfor w in w_list:\n    train_score,test_score = knn_model(weights = w)\n    train_score_dict[w] = (train_score)\n    test_score_dict[w] = (test_score)\n\nprepare_dataframe(test_score_dict,train_score_dict,\"Weight = \",w_list)","ddce3530":"#Results stored in dictionaries\nresult_train[\"Weight-Best-Train\"] = np.asarray(list(train_score_dict.values())).max()\nresult_test[\"Weight-Best-Test\"] = np.asarray(list(test_score_dict.values())).max()","fc51c645":"#KNN algorith implementation with different algorithm\na_list = ['auto','ball_tree','kd_tree','brute']\n\ntest_score_dict = {}\ntrain_score_dict = {}\n\nfor a in a_list:\n    train_score,test_score = knn_model(algorithm = a)\n    train_score_dict[a] = (train_score)\n    test_score_dict[a] = (test_score)\n\nprepare_dataframe(test_score_dict,train_score_dict,\"Algorithm = \",a_list)","cd2ae37c":"#Results stored in dictionaries\nresult_train[\"Algorithm-Best-Train\"] = np.asarray(list(train_score_dict.values())).max()\nresult_test[\"Algorithm-Best-Test\"] = np.asarray(list(test_score_dict.values())).max()","3f79c9e7":"#KNN algorith implementation with different p\np_list = list(np.arange(1,11))\n\n\ntest_score_dict = {}\ntrain_score_dict = {}\n\nfor p in p_list:\n    train_score,test_score = knn_model(p = p)\n    train_score_dict[p] = (train_score)\n    test_score_dict[p] = (test_score)\n\nprepare_dataframe(test_score_dict,train_score_dict,\"P = \",p_list)","95271e78":"plt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.plot(p_list,list(train_score_dict.values()))\nplt.xlabel(\"P Value\")\nplt.ylabel(\"Train Score\")\n\nplt.subplot(1,2,2)\nplt.plot(p_list,list(test_score_dict.values()))\nplt.xlabel(\"P Value\")\nplt.ylabel(\"Test Score\")\nplt.show()","a55bb5fb":"#Results stored in dictionaries\nresult_train[\"P-Best-Train\"] = np.asarray(list(train_score_dict.values())).max()\nresult_test[\"P-Best-Test\"] = np.asarray(list(test_score_dict.values())).max()","b449a137":"#Implementation of GridSearch\ngrid = {'n_neighbors':np.arange(1,235),\n        'p':np.arange(1,3),\n        'weights':['uniform','distance'],\n        'algorithm':['auto','ball_tree','kd_tree','brute']\n       }\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn,grid,cv=3)\nknn_cv.fit(X_train,Y_train)\n\nprint(\"Hyperparameters:\",knn_cv.best_params_)\nprint(\"Train Score:\",knn_cv.best_score_)\nresult_train[\"GridSearch-Best-Train\"] = knn_cv.best_score_","a09c0b38":"#Results stored in dictionaries\nresult_test[\"GridSearch-Best-Test\"] = knn_cv.score(X_test,Y_test)\nprint(\"Test Score:\",knn_cv.score(X_test,Y_test))","81ed0b1c":"#Result dataframe\ncolumns = [\"Default\",\"K-Best\",\"Weight-Best\",\"Algorithm-Best\",\"P-Best\",\"GridSearchCV\"]\nprepare_dataframe(result_test,result_train,\"\",columns)","8d2b44bc":"#Bar plot for showing result of parameters both train and test datas\nplt.figure(figsize = (12,5))\nX = np.arange(len(result_train))\nax = plt.subplot(111)\nax.bar(X, result_train.values(), width=0.2, color='b', align='center')\nax.bar(X-0.2, result_test.values(), width=0.2, color='g', align='center')\nax.legend(('Train Results','Test Results'))\nplt.xticks(X, columns)\nplt.title(\"Comparing Results of Parameters\", fontsize=17)\nplt.show()","661de9e5":"According to graphs and table above **\"p\"** parameter changes the result of prediction and the best p value seems to be 1 for this dataset. In other words Manhattan Distance is the best distance metric for this dataset.","77d5a0fe":"* Dataframe has 569 entries.\n* Unnamed: 32 column has 569 null value. So it is useless column. It should be dropped.\n* There is one categorical and 32 numeric columns but one of them is id column and it should be dropped.\n* Categorical column is diagnosis column. It is the target column. It would be better if it is converted to int type to avoid any error during implementation of KNN Algorithms.","ddb84907":"* If value of n_neighbors is too low, It can cause of overfitting. Overfitting means that KNN model memorize data instead of learning data.\n* If value of n_neighbors is too high, It can cause of underfitting. Underfitting means that KNN model doesn't learn data efficiently. So probability of false prediction may be high. \n* The result table and graphs shows best value of n_neighbors is 9. So, the best value of n_neighbors value is **9** for now!","7c9dcaaa":"* It is understood from the results that the parameter selection affects the quality of the model which is created with the KNN algorithm.\n* In fact, when the results are examined, it can be said that some parameters have more effect on the result like n_neighbors and p parameters.\n* Considering that the KNN algorithm classifies according to the closest neighbors, it is quite logical that the p and n_neighbors  are more effective because these two parameters are related with calculating nearest neighbors. One of them refers number of neighbors and the other one is related with distance between query point and neighbor.\n* Also it can be said that **auto** option can be preferred as algorithm parameter option. Because it already tries all options and decides the best one.\n* In this kernel, although the parameter effects are examined separately, as a result it is best to try all parameter combinations and find the best combination at once with the GridSearch algorithm.","f7f13e5f":"id and Unnamed: 32 columns were dropped and data type of diagnosis column is int64. So, data set can be splitted as train and test.","c3b64682":"<a id='9'><\/a>\n# GridSearch","e568a289":"<a id='1'><\/a>\n# Load Data and PreCheck","e792abe8":"<a id='3'><\/a>\n# First Prediction With Default Parameters\nDefault parameters of KNN Algorithm are listed below.\n* n_neighbors = 5\n* weights = 'uniform'\n* algorithm = 'auto'\n* leaf_size = 30\n* p = 2\n* metric = 'minkowski'","8b4a0a6f":"<a id='2'><\/a>\n# Preparing Data Set","e5f9a770":"# Introduction\nIn this kernel, parameters of KNN Algorithm are described and effects of these paremeters on result are observed. First prediction is predicted with default parameters and this result is used for comparing. After that, best value of every parameters are found and are discussed their effects on result. <br\/>\n\nFinally, GridSearch algorithm is used to find best values of each parameters. So results can be compared each other in the conclusion part. To purpose of this kernel, understanding parameters of KNN Classifier algorithm and gain experience about hyperparameter tunings.\n\n<font color='blue'>\nContent\n    \n1. [Load Data and PreCheck](#1)\n1. [Preparing Data Set](#2)\n1. [First Prediction With Default Parameters](#3)\n1. [Parameters Description and Observation of Effects](#4)\n    * [n_neighbors](#5)\n    * [weights](#6)\n    * [algorithm](#7)\n    * [p](#8)\n1. [GridSearch](#9)\n1. [Conclusion](#10)","f5ed47e9":"\nSo far, the parameters have been studied separately and the following results have been obtained as best options for parameters.\n* n_neighbors = \"9\"\n* wieght = \"distance\"\n* algorithm = \"auto\"\n* p = \"1\"\n\nOf course, there are some methods to select best parameters for KNN algorithm. One of them is GridSearchCV method. So, a dictionary is prepared with different values of n_neighbors, p, algorithm and weights parameters and it uses by GridSearchCV algorithm to find best values.","125be923":"<a id='10'><\/a>\n# Conclusion","81a25e64":"<a id='5'><\/a>\n## n_neighbors\nn_neighbors parameter is one of the most important parameters for KNN Algorithm. KNN means K - Nearest Neighbors and n_neighbors parameter is this K value. It means that if n_neighbors is selected 3, algorithm finds 3 nearest points and classify the new point according to majority of these 3 points.\n","032a5164":"<a id='8'><\/a>\n# p\n\nDifferent distance metrics are available which use for the tree (like KDTree or Ball Tree). One of them is Minkowski metric. This p parameter is power parameter for Minkowski metric.\n* If p = 1, this is equivalent to using **manhattan_distance(l_1)**\n* If p = 2, this is equivalent to using **euclidean_distance(l_2)**\n* If p > 2, It is **minkowski distance(l_p)**.\n\nTo who wonder what these functions are;\n* EuclideanDistance => sqrt(sum((x - y)^2))\n* ManhattanDistance => sum(|x - y|)\n* MinkowskiDistance => sum(|x - y|^p)^(1\/p)\n","818e488c":"If you look at the table above, you can see that train score of distance a little bit higher than uniform. Changing weight parameter improve train score but there is no effect on test score. This may be due to the structure of the data set  which is reserved for the test. However, according to these results, we can say that the weights parameter has a slight effect on the results.","b7c32f9f":"<a id='4'><\/a>\n# Parameters Description and Observation of Effects\n\nIn this part, each parameter is analyzed separately. Each parameter is tried by using different values and effects of these changes are examined.","3d8e99e8":"* \"Unnamed: 32\" has NaN values. It should be check how many rows have NaN value.\n* \"id\" column keep unique id of each column. So it is useless feature for classification. It would be better if It is dropped.","c41e2e61":"According to result ,changing algorithm parameters didn't change scores. But **auto** algorithm is the best options because it tries all algorithm options to find best one.","bfd8c070":"<a id='6'><\/a>\n## weights\n\nBasically, KNN algorithm uses **uniform** weights. For uniform weight, all neighbors have equal vote on query point. For example; to classify a query point, if five nearest point is selected, there are 3 class1 and 2 class2 in these points then result is class1. The distance of the neighbors are not important for classification.\n\nUnlike uniform weights there is another option which name is **distance**. If distance is selected, the distance of neighbors become important to classification. Nearer neighbors contribute more to the fit. It means that majority isn't enough to classification, distance between query point and neighbors may prevail over the majority.","7c960654":"<a id='7'><\/a>\n# algorithm\n\nAlgorithm parameter sets the algorithm which uses to compute nearest neighbors. There are 3 algorithm but 4 option for this parameter. Algorithms which can be used, are listed below;\n* BallTree\n* KDTree\n* Brute Force\n\nFourth option is auto. Auto option attempt to decide best algorithm for best result.\n"}}