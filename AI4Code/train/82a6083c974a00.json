{"cell_type":{"c0da55ef":"code","648fb844":"code","534fa1e2":"code","18cef844":"code","652acb87":"code","bba8babb":"code","d9447ba7":"code","b060befd":"code","3e7d9067":"code","bb4a0db6":"code","2179fd4b":"code","22b910f4":"code","1c50d737":"code","46a874d9":"code","afeed33f":"code","c03a3516":"code","b007f8d8":"code","eef4c090":"code","246d8604":"code","73f32c1d":"code","de3dda41":"code","8356f8b6":"code","f9aeb764":"code","040e5a81":"code","d66d4790":"code","dd6753f0":"code","6eb77170":"code","ff6039bd":"code","02123aa5":"code","aeb02f67":"code","e9df107c":"code","79b3e805":"code","7dd8c1a5":"code","b4c89964":"code","52c1b3e5":"code","955ca780":"code","2e61393d":"code","6dbfdfef":"code","52fe72d4":"code","4b050d6a":"code","d52a6940":"code","7f9e4d12":"code","3649ad45":"code","ef6b4bb5":"code","0ee7c575":"code","d8bc1381":"code","a7a850ee":"code","736cc5a4":"code","a830812e":"code","68174ee5":"code","2707127c":"code","b52f658d":"markdown","49dae88d":"markdown","33c732c8":"markdown","fbbd6d08":"markdown","064b935f":"markdown","01cf44ed":"markdown"},"source":{"c0da55ef":"#### Importing Libraries ####\n\nimport pandas as pd\nfrom dateutil import parser\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\nimport time","648fb844":"# Reading Data\ndataset = pd.read_csv('..\/input\/app-data\/appdata10.csv')\n","534fa1e2":"\n# Viewing the 10 rows of the Data\ndataset.head(10) ","18cef844":"# Distribution of Numerical Variables\ndataset.describe() ","652acb87":"# First set of Feature cleaning\ndataset[\"hour\"] = dataset.hour.str.slice(1, 3).astype(int)","bba8babb":"dataset[\"hour\"]","d9447ba7":"### Plotting\ndataset2 = dataset.copy().drop(columns = ['user', 'screen_list', 'enrolled_date','first_open', 'enrolled'])","b060befd":"dataset2.head()","3e7d9067":"## Histograms\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(1, dataset2.shape[1] + 1):\n    plt.subplot(3, 3, i)\n    f = plt.gca()\n    #f.axes.get_yaxis().set_visible(False)\n    f.set_title(dataset2.columns.values[i - 1])\n\n    vals = np.size(dataset2.iloc[:, i - 1].unique())\n    \n    plt.hist(dataset2.iloc[:, i - 1], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n# plt.savefig('app_data_hist.jpg')","bb4a0db6":"## Correlation with Response Variable\ndataset2.corrwith(dataset.enrolled).plot.bar(figsize=(20,10),\n                  title = 'Correlation with Reposnse variable',\n                  fontsize = 15, rot = 45,\n                  grid = True)","2179fd4b":"## Correlation Matrix\nsn.set(style=\"white\", font_scale=2)\n\n# Compute the correlation matrix\ncorr = dataset2.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\nf.suptitle(\"Correlation Matrix\", fontsize = 40)\n\n# Generate a custom diverging colormap\ncmap = sn.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","22b910f4":"\n# Formatting Date Columns\ndataset.dtypes","1c50d737":"dataset['first_open'] = pd.to_datetime(dataset['first_open'])\ndataset['enrolled_date'] = pd.to_datetime(dataset['enrolled_date'])","46a874d9":"# Formatting Date Columns\ndataset.dtypes","afeed33f":"# Selecting Time For Response\ndataset[\"difference\"] = (dataset.enrolled_date - dataset.first_open).astype('timedelta64[h]')\nresponse_hist = plt.hist(dataset[\"difference\"].dropna(), color='#3F5D7D')\nplt.title('Distribution of Time-Since-Screen-Reached')\nplt.show()","c03a3516":"plt.hist(dataset[\"difference\"].dropna(), color='#3F5D7D', range = [0, 100])\nplt.title('Distribution of Time-Since-Screen-Reached')\nplt.show()\n","b007f8d8":"dataset.loc[dataset.difference > 48, 'enrolled'] = 0","eef4c090":"dataset = dataset.drop(columns=['enrolled_date', 'difference', 'first_open'])","246d8604":"dataset.head(10)","73f32c1d":"## Formatting the screen_list Field\n\n# Load Top Screens\ntop_screens = pd.read_csv('..\/input\/app-data\/top_screens.csv').top_screens.values\n","de3dda41":"top_screens","8356f8b6":"# Mapping Screens to Fields\ndataset[\"screen_list\"] = dataset.screen_list.astype(str) + ','\n\nfor sc in top_screens:\n    dataset[sc] = dataset.screen_list.str.contains(sc).astype(int)\n    dataset['screen_list'] = dataset.screen_list.str.replace(sc+\",\", \"\")\n\ndataset['Other'] = dataset.screen_list.str.count(\",\")\ndataset = dataset.drop(columns=['screen_list'])","f9aeb764":"# Funnels\nsavings_screens = [\"Saving1\",\n                    \"Saving2\",\n                    \"Saving2Amount\",\n                    \"Saving4\",\n                    \"Saving5\",\n                    \"Saving6\",\n                    \"Saving7\",\n                    \"Saving8\",\n                    \"Saving9\",\"Saving10\"]\ndataset[\"SavingCount\"] = dataset[savings_screens].sum(axis=1)\ndataset = dataset.drop(columns=savings_screens)","040e5a81":"cm_screens = [\"Credit1\",\n               \"Credit2\",\n               \"Credit3\",\n               \"Credit3Container\",\n               \"Credit3Dashboard\"]\ndataset[\"CMCount\"] = dataset[cm_screens].sum(axis=1)\ndataset = dataset.drop(columns=cm_screens)","d66d4790":"cc_screens = [\"CC1\",\n                \"CC1Category\",\n                \"CC3\"]\ndataset[\"CCCount\"] = dataset[cc_screens].sum(axis=1)\ndataset = dataset.drop(columns=cc_screens)","dd6753f0":"loan_screens = [\"Loan\",\n               \"Loan2\",\n               \"Loan3\",\n               \"Loan4\"]\ndataset[\"LoansCount\"] = dataset[loan_screens].sum(axis=1)\ndataset = dataset.drop(columns=loan_screens)","6eb77170":"dataset.head()\n","ff6039bd":"dataset.describe()\n","02123aa5":"dataset.columns\n","aeb02f67":"dataset.to_csv('new_appdata10.csv', index = False)","e9df107c":"dataset = pd.read_csv('new_appdata10.csv')","79b3e805":"# Data Pre-Processing \n\n# Splitting Independent and Response Variables\nresponse = dataset[\"enrolled\"]\ndataset = dataset.drop(columns=\"enrolled\")","7dd8c1a5":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset, response,\n                                                    test_size = 0.2,\n                                                    random_state = 0)","b4c89964":"## Balancing the Training Set\nimport random\ny_train.value_counts()\n\npos_index = y_train[y_train.values == 1].index\nneg_index = y_train[y_train.values == 0].index\n\nif len(pos_index) > len(neg_index):\n    higher = pos_index\n    lower = neg_index\nelse:\n    higher = neg_index\n    lower = pos_index\n\nrandom.seed(0)\nhigher = np.random.choice(higher, size=len(lower))\nlower = np.asarray(lower)\nnew_indexes = np.concatenate((lower, higher))\n\nX_train = X_train.loc[new_indexes,]\ny_train = y_train[new_indexes]","52c1b3e5":"# Removing Identifiers\ntrain_identity = X_train['user']\nX_train = X_train.drop(columns = ['user'])\ntest_identity = X_test['user']\nX_test = X_test.drop(columns = ['user'])","955ca780":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","2e61393d":"#### Model Building ####\n\n# Fitting Model to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l2')\nclassifier.fit(X_train, y_train)","6dbfdfef":"# Predicting Test Set\ny_pred = classifier.predict(X_test)","52fe72d4":"# Evaluating Results\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprecision_score(y_test, y_pred) # tp \/ (tp + fp)\nrecall_score(y_test, y_pred) # tp \/ (tp + fn)\nf1_score(y_test, y_pred)","4b050d6a":"df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","d52a6940":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"SVM Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","7f9e4d12":"# Analyzing Coefficients\npd.concat([pd.DataFrame(dataset.drop(columns = 'user').columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","3649ad45":"#### Model Tuning ####\n\n## Grid Search (Round 1)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC, LinearSVC","ef6b4bb5":"# Select Regularization Method\npenalty = ['l1', 'l2']","0ee7c575":"# Create regularization hyperparameter space\nC = [0.001, 0.01, 0.1, 1, 10, 100, 1000]","d8bc1381":"# Combine Parameters\nparameters = dict(C=C, penalty=penalty)\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","a7a850ee":"## Grid Search (Round 2)\n\n# Select Regularization Method\npenalty = ['l1', 'l2']","736cc5a4":"# Create regularization hyperparameter space\nC = [0.1, 0.5, 0.9, 1, 2, 5]","a830812e":"# Combine Parameters\nparameters = dict(C=C, penalty=penalty)\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 10,\n                           n_jobs = -1)\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters\ngrid_search.best_score_","68174ee5":"# Formatting Final Results\nfinal_results = pd.concat([y_test, test_identity], axis = 1).dropna()\nfinal_results['predicted_reach'] = y_pred\nfinal_results = final_results[['user', 'enrolled', 'predicted_reach']].reset_index(drop=True)","2707127c":"final_results.head(10)","b52f658d":"# Feature Engineering","49dae88d":"# IMPORTING LIBRARIES","33c732c8":"# Building Model","fbbd6d08":"# Saving Results","064b935f":"# Model Tuning","01cf44ed":"# EDA"}}