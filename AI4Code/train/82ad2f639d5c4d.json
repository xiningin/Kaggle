{"cell_type":{"406db292":"code","e36b7e36":"code","8ac75f17":"code","4ecd52b7":"code","ba179d6a":"code","3357cf5f":"code","03f41bc2":"code","210015a2":"code","a1900b31":"code","e859522c":"code","106b5e28":"code","abdebcae":"code","b51f78ae":"code","9ae58a48":"code","fc39635f":"code","10a45e5a":"code","db642c88":"code","a3cfcd71":"code","8c5e03c7":"code","82ae7411":"code","c3069fd1":"code","91385dd5":"code","912d7798":"code","972ca0e5":"code","3b6e67e9":"code","643c6c90":"code","1717ca2f":"code","fe5e4de1":"code","11a47094":"code","a49fcbcc":"code","9cb4770d":"code","799aadff":"code","29929a1a":"code","afd99d80":"code","4e8e6c1e":"code","e68972e8":"code","08bf6802":"code","f9e4b636":"code","7c6db493":"code","a26a66ca":"code","3c01c48a":"code","c69de8ea":"markdown","33c042d8":"markdown","c12ef719":"markdown","308f92f5":"markdown","72575d6a":"markdown","21363870":"markdown","d892b1aa":"markdown","f8e7ada7":"markdown","e2dc22c2":"markdown","e314e2c4":"markdown","6ab12265":"markdown","fbb0d431":"markdown","73e3632f":"markdown","3164c859":"markdown"},"source":{"406db292":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nimport matplotlib as plt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib import pyplot\n# use feature importance for feature selection, with fix for xgboost 1.0.2\nfrom numpy import loadtxt\nfrom numpy import sort\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.inspection import permutation_importance\nimport seaborn as sns\n\n# from keras.utils.np_utils import to_categorical\n# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, BatchNormalization\n# from keras.optimizers import Adam\n# from keras.callbacks import ReduceLROnPlateau\n# from keras.preprocessing.image import ImageDataGenerator\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e36b7e36":"df_training = pd.read_csv(\"..\/input\/elg7186-assignment-1-is-this-an-intrusion\/traindata.csv\")\ndf_testing = pd.read_csv(\"..\/input\/elg7186-assignment-1-is-this-an-intrusion\/testdata.csv\")\n","8ac75f17":"df_training.head()","4ecd52b7":"# pip install imbalanced-learn","ba179d6a":"df_training.head()","3357cf5f":"df_testing.head()","03f41bc2":"df_training.describe()","210015a2":"def initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))","a1900b31":"initial_eda(df_training)","e859522c":"initial_eda(df_testing)","106b5e28":"categorical_train = [var for var in df_training.columns if df_training[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical_train)))\n\nprint('The categorical variables are :\\n\\n', categorical_train)","abdebcae":"categorical_test = [var for var in df_testing.columns if df_testing[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical_test)))\n\nprint('The categorical variables are :\\n\\n', categorical_test)","b51f78ae":"print(df_training[categorical_train].head())\nprint(df_testing[categorical_test].head())","9ae58a48":"for var in categorical_train: \n    \n    print(df_training[var].value_counts())","fc39635f":"for var in categorical_test: \n    \n    print(df_testing[var].value_counts())","10a45e5a":"f,ax=plt.subplots(1,2,figsize=(18,8))\n\nax[0] = df_training['Class'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Attack Or Not')\n\n\n#f, ax = plt.subplots(figsize=(6, 8))\nax[1] = sns.countplot(x=\"Class\", data=df_training, palette=\"Set1\")\nax[1].set_title(\"Frequency distribution of Class variable\")\n\nplt.show()","db642c88":"X_train = df_training.drop(columns={\"Class\",\"ID\"})\ny = df_training[\"Class\"]\nX_test = df_testing.drop(columns={\"ID\"})\n","a3cfcd71":"import category_encoders as ce\nencoder = ce.OrdinalEncoder(cols=['protocol_type', 'service', 'flag'])\n","8c5e03c7":"X_train = encoder.fit_transform(X_train)\n","82ae7411":"X_test = encoder.transform(X_test)\n","c3069fd1":"X_train.head()","91385dd5":"X_train.describe()","912d7798":"cols = X_train.columns","972ca0e5":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n","3b6e67e9":"X_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])","643c6c90":"X_train.head()","1717ca2f":"X_test.head()","fe5e4de1":"corr = df_training.corr()\nsns.set(rc = {'figure.figsize':(25,20)})\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=90,\n    horizontalalignment='right'\n);","11a47094":"import xgboost as xgb\nfrom xgboost import plot_importance","a49fcbcc":"X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_train,y, test_size=0.2, random_state=42)","9cb4770d":"from sklearn.feature_selection import SelectFromModel\nmodel = XGBClassifier(eval_metric =\"logloss\")\nmodel.fit(X_train, y)\n# make predictions for test data and evaluate\ny_pred = model.predict(X_test_val)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test_val, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train_val)\n    # train model\n    selection_model = XGBClassifier(eval_metric =\"logloss\")\n    selection_model.fit(select_X_train, y_train_val)\n    # eval model\n    select_X_test = selection.transform(X_test_val)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test_val, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","799aadff":"from matplotlib import pyplot\nmodel = XGBClassifier()\nmodel.fit(X_train, y)\n# plot feature importance\nplot_importance(model)\npyplot.show()\n","29929a1a":"from hyperopt import STATUS_OK, Trials, fmin, hp, tpe","afd99d80":"space={'max_depth': hp.quniform(\"max_depth\", 1, 40, 1),\n        'gamma': hp.uniform ('gamma', 1,10),\n        'reg_alpha' : hp.quniform('reg_alpha',40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'learning_rate' : hp.uniform('learning_rate', 0.01,0.1),\n        'n_estimators': hp.quniform('n_estimators',1,500,1),\n        'seed': 0\n    }","4e8e6c1e":"\ndef objective(space):\n    clf=xgb.XGBClassifier(\n                    n_estimators =int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train_val, y_train_val), ( X_test_val, y_test_val)]\n    \n    clf.fit(X_train_val, y_train_val,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test_val)\n    accuracy = accuracy_score(y_test_val, pred>0.5)\n   \n    return {'loss': -accuracy, 'status': STATUS_OK }","e68972e8":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 500,\n                        trials = trials)","08bf6802":"print(best_hyperparams)","f9e4b636":"params = {'colsample_bytree': 0.7226597411802128, 'gamma': 6.8303650411637165, 'learning_rate': 0.010018834311388431, 'max_depth': 28, 'min_child_weight': 4,\n          'n_estimators': 91, 'reg_alpha': 42, 'reg_lambda': 0.22610147509152623}","7c6db493":"xgb_clf = XGBClassifier(**params, eval_metric =\"logloss\", random_state = 845).fit(X_train,y)\npred = xgb_clf.predict(X_test)\npred_proba = xgb_clf.predict_proba(X_test)","a26a66ca":"preds=[]\n\nfor i in range(len(pred)):\n    if(pred_proba[i][0]>0.75):\n        preds.append(0)\n    else:\n        preds.append(1)","3c01c48a":"\npredictions = pd.DataFrame()\npredictions[\"id\"] = df_testing['ID']\npredictions[\"Class\"] = preds\n\npredictions.to_csv('submission_xgboost.csv', index=False, header=predictions.columns)\n","c69de8ea":"# XGboost ","33c042d8":"# Exploratry Analysis","c12ef719":"# Fitting Model and Calculate Predictions","308f92f5":"**As in our case the main goal is to prevent intrusions so I decided to use Predict proba as my predictions and deciding threshold**","72575d6a":"# Feature Scaling","21363870":"# Frequency distribution of categorical values ","d892b1aa":"![Screenshot 2021-11-15 045848.png](attachment:be3e8209-a461-4d6b-a687-29a7bb64e4e7.png)","f8e7ada7":"**Find Categorical Values**","e2dc22c2":"# Exploring Categorical features","e314e2c4":"# Data Preprocessing","6ab12265":"**Encoding Categorical Values** ","fbb0d431":"# Compettion Result","73e3632f":"# Read Data into training and testing dataframes","3164c859":"# XGboost Classifier Model to check Feature Importance"}}