{"cell_type":{"a98dfdc7":"code","e35ea368":"code","5bef98d9":"code","d71f4c05":"code","3c3a6b91":"code","ac15666c":"code","b2ecf07d":"code","e6018812":"code","70e0975b":"code","9b0d013a":"code","8c92d8e0":"code","070ba478":"code","76855517":"code","2d3e19ac":"code","23bf5d7e":"code","93474058":"code","bd501894":"code","5047a958":"code","4f3159cb":"code","5694c4e4":"code","6653bad5":"code","0d11dfe1":"code","a82de4d2":"code","b8217b63":"code","98eef01f":"code","10fac08e":"code","ba9e570e":"code","07bab7fd":"code","9df0f2f1":"markdown","11104182":"markdown","2bb9efa3":"markdown"},"source":{"a98dfdc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e35ea368":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport keras_tuner as kt\nfrom tensorflow import keras\nimport math","5bef98d9":"# Load the data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")","d71f4c05":"train.head()","3c3a6b91":"sample_train = train.sample(frac=0.5, random_state=42)","ac15666c":"validation_set = train.sample(frac=0.05, random_state=12)","b2ecf07d":"sample_train.shape","e6018812":"sample_train = sample_train.astype('float')","70e0975b":"# Visualise the classes\nsns.countplot(data=sample_train, x='target')\nplt.show()","9b0d013a":"# Split the data into train and test\nX = sample_train.drop(['id', 'target'], axis=1)\ny = sample_train['target']\n\nX_validation = validation_set.drop(['id', 'target'], axis=1)\ny_validation = validation_set['target']\n\n# Scale the training data\nmms = MinMaxScaler()\nX_scaled = mms.fit_transform(X)\nX_valid_scaled = mms.transform(X_validation)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\nX_val, X_val, y_val, y_val = train_test_split(X_valid_scaled, y_validation, test_size=0.2, random_state=42)","8c92d8e0":"X = sample_train.drop(['id', 'target'], axis=1)\ny = sample_train['target']","070ba478":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)","76855517":"# Build models\ndef build_models(X_train, X_test, y_train, y_test):\n    \n    # 1. Logistic Regression\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_lr = lr.predict_proba(X_train)\n    test_predictions_lr = lr.predict_proba(X_test)\n\n    train_roc_lr = roc_auc_score(y_train, train_predictions_lr[ : , 1])\n    test_roc_lr = roc_auc_score(y_test, test_predictions_lr[ : , 1])\n    \n    print(\"Logistic Regression Model built\")\n    \n    # 3. Decision Tree Classifier\n    dt_clf = DecisionTreeClassifier()\n    \n    # Fit on trainset\n    dt_clf.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_dt = dt_clf.predict_proba(X_train)\n    test_predictions_dt = dt_clf.predict_proba(X_test)\n    \n    train_roc_dt = roc_auc_score(y_train, train_predictions_dt[ : , 1])\n    test_roc_dt = roc_auc_score(y_test, test_predictions_dt[ : , 1])\n          \n    print(\"Deicision Tree Model built\")\n    \n    # 4. Random Forest\n    rf_clf = RandomForestClassifier()\n    \n    # Fit on trainset\n    rf_clf.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_rf = rf_clf.predict_proba(X_train)\n    test_predictions_rf = rf_clf.predict_proba(X_test)\n    \n    train_roc_rf = roc_auc_score(y_train, train_predictions_rf[ : , 1])\n    test_roc_rf = roc_auc_score(y_test, test_predictions_rf[ : , 1])\n    \n    print(\"Random Forest Model built\")\n          \n    # 5. Extra Classifier\n    ext_clf = ExtraTreesClassifier()\n    \n    # Fit on trainset\n    ext_clf.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_ext = ext_clf.predict_proba(X_train)\n    test_predictions_ext = ext_clf.predict_proba(X_test)\n    \n    train_roc_ext = roc_auc_score(y_train, train_predictions_ext[ : , 1])\n    test_roc_ext = roc_auc_score(y_test, test_predictions_ext[ : , 1])\n          \n    print(\"Extra Trees Model built\")\n    \n    # 6. XGBoost\n    xgb_clf = XGBClassifier()\n    \n    # Fit on trainset\n    xgb_clf.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_xgb = xgb_clf.predict_proba(X_train)\n    test_predictions_xgb = xgb_clf.predict_proba(X_test)\n    \n    train_roc_xgb = roc_auc_score(y_train, train_predictions_xgb[ : , 1])\n    test_roc_xgb = roc_auc_score(y_test, test_predictions_xgb[ : , 1])\n    \n    print(\"XGBoost Model built\")\n    \n    # 7. LGBM\n    lgbm_clf = LGBMClassifier()\n    \n    # Fit on trainset\n    lgbm_clf.fit(X_train, y_train)\n    \n    # Make Predictions\n    train_predictions_lgbm = lgbm_clf.predict_proba(X_train)\n    test_predictions_lgbm = lgbm_clf.predict_proba(X_test)\n    \n    train_roc_lgbm = roc_auc_score(y_train, train_predictions_lgbm[ : , 1])\n    test_roc_lgbm = roc_auc_score(y_test, test_predictions_lgbm[ : , 1])\n    \n    print(\"Light GBM Model built\")\n    \n    # Concatenate the train results\n    results_train_arr = np.array([train_roc_lr, \n                                        train_roc_dt, \n                                        train_roc_rf,  \n                                        train_roc_ext,\n                                        train_roc_xgb,\n                                        train_roc_lgbm])\n    \n    # Concatenate the test results\n    results_test_arr = np.array([test_roc_lr, \n                                        test_roc_dt, \n                                        test_roc_rf,  \n                                        test_roc_ext,\n                                        test_roc_xgb,\n                                        test_roc_lgbm])\n    # Concatenate the total results\n    final_results = np.concatenate((results_train_arr.reshape(-1,1), results_test_arr.reshape(-1,1)), axis=1)\n    \n    return final_results","2d3e19ac":"# Call the function\nbaseline_results = build_models(X_train, X_test, y_train, y_test)","23bf5d7e":"baseline_results_df = pd.DataFrame(baseline_results, \n                                   columns = ['Train_ROC', 'Test_ROC'], \n                                   index=['LR', 'DT', 'RF', 'EXT', 'XGB', 'LGBM'])","93474058":"baseline_results_df","bd501894":"# Build the model again and plot the confusion matrix\nlr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\n\n# Make Predictions\ntrain_predictions = lr.predict(X_train)\ntest_predictions = lr.predict(X_test)\n\ncnf_matrix_train = confusion_matrix(y_train, train_predictions, normalize='all')\ncnf_matrix_test = confusion_matrix(y_test, test_predictions, normalize='all')\n\nfigure, ax = plt.subplots(1, 2, figsize=(20,8))\nsns.heatmap(cnf_matrix_train, annot=True, ax=ax[0], fmt='.3g')\nax[0].set_title(\"Train Confusion Matrix\")\nax[0].set_xlabel(\"Predicted Label\")\nax[0].set_ylabel(\"True Label\")\n\n\nsns.heatmap(cnf_matrix_test, annot=True, ax=ax[1], fmt='.3g')\nax[1].set_title(\"Test Confusion Matrix\")\nax[1].set_xlabel(\"Predicted Label\")\nax[1].set_ylabel(\"True Label\")\nfigure.show()\n","5047a958":"sns.countplot(data=sample_train, x='target')\nplt.show()","4f3159cb":"# Hyperparameter Tuning\nlogistic_regression_params = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n                              'tol' : [1e-4, 1e-2, 1e-1],\n                              'C' : [0.25, 0.50, 1.0, 1.5, 2.0],\n                              'class_weight': ['balanced'],\n                              'solver' : ['saga', 'liblinear'],\n                              'max_iter' : [10, 50, 100, 150, 200, 500, 1000]}\n\nlr_clf_2 = LogisticRegression(verbose=5)\nrscv_lr = RandomizedSearchCV(lr_clf_2, logistic_regression_params, n_iter=100, cv=3, verbose=3, scoring='roc_auc', random_state=42, n_jobs=-1)\nrscv_lr.fit(X_train, y_train)","5694c4e4":"cv_results = [(rscv_lr.cv_results_['split{}_test_score'.format(i)]) for i in range(0,3)]\nmean_results = []\nfor result in cv_results:\n    mean_roc_auc = np.mean([val for val in result if not math.isnan(val)])\n    mean_results.append(mean_roc_auc)","6653bad5":"mean_results","0d11dfe1":"# Plot the results of best estimator\nlr_best_estimator = rscv_lr.best_estimator_\n\n\ntrain_predictions_lr = lr_best_estimator.predict_proba(X_train)\ntest_predictions_lr = lr_best_estimator.predict_proba(X_test)\n\ntrain_roc_lr = roc_auc_score(y_train, train_predictions_lr[ : , 1])\ntest_roc_lr = roc_auc_score(y_test, test_predictions_lr[ : , 1])","a82de4d2":"train_roc_lr, test_roc_lr","b8217b63":"train.shape","98eef01f":"# Train the best extimator on the entire dataset\nX_entire = train.drop(['id', 'target'], axis=1)\ny_entire = train['target']\n\nlr_best_estimator.fit(X_entire, y_entire)","10fac08e":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_entire, y_entire)","ba9e570e":"test_scaled = mms.transform(test.drop(['id'], axis=1))","07bab7fd":"lr_predictions = lr.predict(test_scaled)\nlr_submission_arr = np.concatenate((test.id.values.reshape(-1,1), lr_predictions.reshape(-1,1)), axis=1)\nlr_submission_df = pd.DataFrame(lr_submission_arr, columns = ['id', 'target'])\nlr_submission_df.to_csv('.\/lr_submissions_2.csv', index=False)","9df0f2f1":"**Logistic Regression Generalises well**","11104182":"![image.png](attachment:22d8cb2a-e4d7-49e5-b1bb-84409d4120b2.png)","2bb9efa3":"# **Baseline**"}}