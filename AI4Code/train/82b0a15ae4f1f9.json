{"cell_type":{"55c4575a":"code","2199e177":"code","4c7900cc":"code","f43d8f17":"code","be473da2":"code","c4a8a533":"code","dea7312a":"code","07a1107f":"code","0875e908":"code","5a586140":"code","794c0a78":"code","fc0f5bbf":"code","cd308b58":"code","7c30ed8b":"code","9ba8fbe1":"code","9a420bb3":"code","4e12a6c4":"code","be8a9d8c":"code","deb99e6c":"code","ef594758":"code","d3c3d004":"code","4dd52ff9":"code","96e137f7":"code","4f2f8f3a":"code","faa39dde":"code","a9bf4d9a":"code","1cef5d05":"code","19778a0a":"code","8285eb1a":"code","7b603719":"code","1f960d26":"code","29a1183f":"code","cdf4c7d1":"code","8a147872":"code","76c17360":"code","83b872a2":"code","39e6c67b":"code","e8e869f8":"code","8f23682b":"code","d75f79f5":"code","abb4f2ef":"code","03cde964":"code","03ab528d":"code","8425fdf6":"code","4f4490ba":"code","17482fef":"code","7f8b5a13":"code","38530010":"code","d5c6e799":"code","038bf783":"code","f9952092":"code","8a833eb2":"code","3610ff2d":"code","df19efb2":"code","368b6532":"code","a059fc7b":"code","2192af21":"code","f97ab857":"code","f626a50e":"code","86084d34":"code","b5ef14f8":"code","59438244":"code","e7119324":"code","ce66631f":"code","f8c0bffb":"code","c2723e13":"code","2e4d50fe":"code","6d8f2b39":"code","9c2bdab9":"code","7f6454bd":"code","9df66e89":"code","c9cff7b3":"code","6f8cbd91":"code","4b4f938f":"code","8e306b55":"code","4c41a0f8":"code","04fc33c8":"code","ae3bad66":"code","2de93dd6":"code","7b114c52":"code","63a5c4bf":"code","872cdbec":"code","70de53ba":"code","fa283c7f":"code","a6f9af52":"code","5bd68c4b":"code","36e10c3e":"code","127341d8":"markdown","74b38660":"markdown","ff245de1":"markdown","16f3d88e":"markdown","3ffbf7c1":"markdown","162baeb8":"markdown","187bd21b":"markdown","8e734ab6":"markdown","a014dfbe":"markdown","313e6b58":"markdown","bee400f3":"markdown","ba8b0281":"markdown","27eda515":"markdown","327c5c26":"markdown","c21611a9":"markdown","7635d3db":"markdown","08233ff8":"markdown","73b7bef5":"markdown","32c040ef":"markdown","1f8f1fab":"markdown","9642e6f3":"markdown","077b1b2b":"markdown","fb946d5d":"markdown","cd74c833":"markdown","608d97ba":"markdown","9d1377a2":"markdown","017bc928":"markdown","9d678fe3":"markdown","29fa26a0":"markdown","c962fd25":"markdown","2674b64d":"markdown","11d5ed00":"markdown","fe41ead3":"markdown","b49e21af":"markdown","d3e5a81f":"markdown","254c8f3f":"markdown"},"source":{"55c4575a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2199e177":"######### Important Note ###########\n# code is developed in Kaggle as a computation Resource \n# Code works well in the kaggle \n# Kaggle link for this project is provided here\n# https:\/\/www.kaggle.com\/prashanthsheri\/video-assessment","4c7900cc":"# importing the required libraries\nimport cv2\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","f43d8f17":"# copying the required files from Input directory to CPU location\n\n!cp -r \/kaggle\/input\/video-assesment-sheri\/  \/kaggle\/working\/\n\n# pointing the current directory to the data folder location\n!cd video-assesment-sheri\n\n#verification of the current working directory  before starting the execution \n!pwd","be473da2":"# Defining a function for extraction of frames using \"Uniform Time Sampling\" and segmenting video into seconds given by the user\n\ndef uft_frame_extraction(path, fps,vid_seg_time): # vid_seg_time in seconds like 1 minute = 60 seconds\n    \n    vidcap = cv2.VideoCapture(path) # capture the Video from the given path\n    \n    path_split = os.path.split(path)\n    \n    save_dir = os.path.join(path_split[0],\"UTS_frames_fps_{0:d}_seg_{1:d}_sec\".format(fps,vid_seg_time)) #creating a folder for \n    \n    if not(os.path.exists(save_dir)):\n        os.mkdir(save_dir) # create directory to save the frames generated\n    \n    frameRate = 1.0\/fps #it will capture image in every 1.0\/fps seconds\n    success, image = vidcap.read()\n    \n    #intiations for the loop\n    sec = 0\n    frame_count = 1 # frame count to get number of frames captured \n    \n    cur_seg, nxt_seg = 1,1 \n    seg_dir_list = []  # list containing different \n    \n    while success:\n\n        #Video segmentations process\n        # creating directory for every segments of the video and saving the frames init\n        \n        if cur_seg == nxt_seg:\n            \n            save_folder = os.path.join(save_dir,\"video_seg_{:d}\".format(cur_seg))\n            nxt_seg = cur_seg + 1\n            seg_dir_list.append(save_folder)\n            \n            if not(os.path.exists(save_folder)):\n                os.mkdir(save_folder) # create directory to save the frames generated\n        \n        # evaluvating the current segment of the video\n        cur_seg = int(sec\/\/vid_seg_time)+1 \n                                     \n        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n        success,image = vidcap.read()\n    \n        if success:\n            \n            save_path = os.path.join(save_folder, \"timestamp_{0:5.2f}_frame_{1:d}.jpg\".format(sec,frame_count))\n            cv2.imwrite(save_path, image)\n        \n        sec = sec + frameRate\n        sec = round(sec, 3)\n        \n        frame_count = frame_count + 1\n        \n    \n    #vidcap.release()\n    #cv2.destroyAllWindows()\n    \n    print(\"\\n Task: Frames extraction and video segmentaion of frames is completed as per the guidelines \\n Thank You visit again!!!\")\n\n    return  seg_dir_list","c4a8a533":"# Initialisation of feature extraction model using a pretrained DeepNN model\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmdl = VGG16(weights='imagenet', include_top=False)\n\n#get Output layer of Pre0trained model\nm2 = mdl.output\nm3 = tf.keras.layers.Flatten()(m2)\n#m4 = tf.keras.layers.Dense(2000)(m3)\n\n\n#Using Keras Model class\nfea_model = tf.keras.models.Model(inputs=mdl.input, #Pre-trained model input as input layer\n                                    outputs=m3) #Output layer added","dea7312a":"# features extraction process from the given image using pretarined deeplearning model Through  transfer learning approach\n#img_path = \"\/kaggle\/working\/video-assesment-sheri\/UTS_frames_fps_4_seg_60_sec\/video_seg_3\/timestamp_144.50_frame_579.jpg\"\n\ndef img_feature_extraction(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n\n    features = fea_model.predict(x)\n    \n    return features\n# return the features of the image","07a1107f":"# vid_seg_wise feature extraction per given segment of the video created and storing the features into a csv file\n\ndef fea_csv_genration(seg_dir_path):\n    \n    path_list = [] # list of image path in sequence \n    count = 1\n    \n    for dirname, _, filenames in os.walk(seg_dir_path):\n        for filename in filenames:\n            \n            path = os.path.join(dirname, filename) # path extraction from the given vid_seg_directory\n            fea = img_feature_extraction(path) # features extraction for the img path given \n            \n            if count == 1:\n                tf =pd.DataFrame(columns=np.arange(fea.shape[1]))  # instantiate the data frame\n                tf.loc[tf.shape[0]] = fea[0]\n                path_list.append(path)\n            else:\n                tf.loc[tf.shape[0]] = fea[0]\n                path_list.append(path)\n                \n            count = count + 1\n    \n    tf['Img_path'] = path_list\n    \n    csv_dir = os.path.join(os.path.split(os.path.split(seg_dir_path)[0])[0],'fea_csv')\n    if not(os.path.exists(csv_dir)):\n        os.mkdir(csv_dir) # create directory to save the frames generated\n    \n    csv_path = os.path.join(csv_dir,'{}_features.csv'.format(os.path.split(seg_dir_path)[1]))\n    tf.to_csv(csv_path)\n    \n    print(\"Task: Completed \\n feature extraction from seg_dir_path_{}....... completed\".format(seg_dir_path))\n    \n    return  csv_path\n\n# returns the segment features csv path","0875e908":"# a function for calculation of silhouette_score for detection of optimal  number of clusters\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n#when the error reduction\n\ndef sil_elbow(fea_csv_path):\n\n    # Let us check optimal number of clusters-\n\n    fts = pd.read_csv(fea_csv_path).drop([\"Unnamed: 0\",'Img_path'],axis=1)\n    X = fts.values\n    \n    max_num_clusters = 10\n    cluster_range = np.arange(1, max_num_clusters+1) \n    cluster_errors = []\n    sil_score = []\n    \n    for num_clusters in cluster_range:\n\n        clusters = KMeans(num_clusters, n_init = 5)\n        clusters.fit(X)\n        \n        labels = clusters.labels_                     # capture the cluster lables\n        centroids = clusters.cluster_centers_         # capture the centroids\n        cluster_errors.append( clusters.inertia_ )    # capture the intertia\n        \n        if num_clusters == 1:\n            sil_score.append(0)\n        else:\n            sil_score.append(silhouette_score(X, labels, metric = 'euclidean'))  # silhouette_score calculation\n\n\n    error_reduction = np.zeros(max_num_clusters)\n\n    for k in cluster_range:\n        if k==1:\n            error_reduction[0] =  (cluster_errors[0] - cluster_errors[1])*100\/cluster_errors[0]\n        else:\n            error_reduction[k-1] = (cluster_errors[k-2] - cluster_errors[k-1])*100\/cluster_errors[k-2]\n\n\n    # combine the cluster_range and cluster_errors into a dataframe by combining them\n    clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\n    clusters_df['error_reduction_percent'] = error_reduction\n    clusters_df\n\n    #silhouette_score for optimal number of clusters identifcation\n    # optimal number of clusters is detrmined \n    # silhouette_score obtains a global maximum at the optimal k.\n\n    return sil_score,clusters_df\n\n# returns silhouette_score and data required for plotting \"ELBOW\" graph for optimul clusters detection","5a586140":"# function for optimal  number of cluster detection \n\ndef optimul_num_clusters(sil_score,clusters_df):\n    # minimum two cluster are to be made from the each video segment \n    opt_num_clusters = np.argmax(np.array(sil_scores))\n    if opt_num_clusters <= 2:\n        opt_num_clusters = 2;\n    else:\n        opt_num_clusters = opt_num_clusters\n    \n    return opt_num_clusters\n\n# returns optimal number of clusters\n# minimum 2 cluster is chosen  for every video  segment","794c0a78":"# extracting key frames from each clusters  with in every video  segment created above\n\ndef key_frames_csv_extraction(fea_csv_path,opt_num_clusters):\n    \n    df = pd.read_csv(fea_csv_path)\n    \n    fts = df.drop([\"Unnamed: 0\",'Img_path'],axis=1)\n    X = fts.values\n    \n    # Number of clusters\n    kmeans = KMeans(opt_num_clusters)\n    \n    # Fitting the input data\n    kmeans = kmeans.fit(X)   \n    \n    # Getting the cluster labels\n    labels = kmeans.predict(X)\n    \n    hf = df.drop([\"Unnamed: 0\"],axis=1)\n    hf['label'] = labels\n    key_frames_df = hf.groupby('label').nth(0)  # key frame selection methode should be changes write a function for the same\n    \n    #creating a dir for key frame csv files from different segments to be saved\n    key_frame_dir = os.path.join(os.path.split(os.path.split(fea_csv_path)[0])[0],'key_frame_csv')\n    \n    if not(os.path.exists(key_frame_dir)):\n        os.mkdir(key_frame_dir) # create directory to save the frames generated\n    \n    \n    file_nam = (\"_\").join([os.path.split(fea_csv_path)[1].split('.')[0],'key_frames.csv'])\n    key_frame_df_path = os.path.join(key_frame_dir,file_nam)\n    \n    key_frames_df.to_csv(key_frame_df_path)\n    \n    ## intentional gap for clear vision and distinction\n    \n    #creating a dir for saving the csv files after makinng cluster with in the given vid segment\n    cluster_dir = os.path.join(os.path.split(os.path.split(fea_csv_path)[0])[0],'Clustered_frame_csv')\n    \n    if not(os.path.exists(cluster_dir)):\n        os.mkdir(cluster_dir) # create directory to save the frames generated\n    \n    cls_file_nam = (\"_\").join([os.path.split(fea_csv_path)[1].split('.')[0],'Clustered_frame.csv'])\n    clustered_frame_path = os.path.join(cluster_dir,cls_file_nam)\n    \n    hf.to_csv(clustered_frame_path)\n    \n    print(\"Task completed! key frames are extracted from the video segment provided, based on the K-means clustering approach \\\n          \\n files are stored in key_frame_csv folder\")\n    \n    return key_frame_df_path, clustered_frame_path\n\n# returns the key frame csv file path with key frame path inside it","fc0f5bbf":"# Human body key points detection\n#\n!pip install mediapipe\nimport mediapipe as mp\n\nmpPose = mp.solutions.pose\npose = mpPose.Pose()\nmpDraw = mp.solutions.drawing_utils\n\npose_points = ['Nose','Left_eye_inner','Left_eye','Left_eye_outer','Right_eye_inner','Right_eye','Right_eye_outer','Left_ear','Right_ear',\n              'Mouth_left','Mouth_right','Left_shoulder','Right_sholuder','Left_elbow','Right_elbow','Left_wrist','Right_wrist',\n              'Left_pinky','Right_pinky','Left_index','Right_index','Left_thumb','Right_thumb','Left_hip','Right_hip',\n              'Left_knee','Right_knee','Left_ankle','Right_ankle','Left_heel','Right_heel','Left_foot_index','Right_foot_index']\n\ndef body_movements_tracking(image_path): \n    # images which contain only human\n    \n    img = cv2.imread(image_path)\n    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    results = pose.process(imgRGB)\n\n    # print(results.pose_landmarks)\n    vis_key_points = [\"No_Key_Points\"]\n    \n    if results.pose_landmarks:\n        vis_key_points = []\n        for id, lm in enumerate(results.pose_landmarks.landmark):\n            if lm.visibility >=0.75:\n                vis_key_points.append(pose_points[id])\n    \n    else:\n        vis_key_points = [\"No_Key_Points\"]\n    \n    return vis_key_points","cd308b58":"#instalation and Intitation steps for text detection in an Image\n\n# it's a pretrained model for text detection\n# refference link: https:\/\/www.kaggle.com\/prashanthsheri\/free-offline-ocr-text-detection-recognition\/edit\n\n!pip install https:\/\/github.com\/myhub\/tr\/archive\/1.5.1.zip\n    \nfrom tr import *\nfrom PIL import Image, ImageDraw, ImageFont","7c30ed8b":"# Text identification function is defined here\n\ndef text_identification(image_path):\n    print('This image may contain text')\n    img_pil = Image.open(image_path)\n    img = cv2.imread(image_path)\n    rect_arr = detect(img_pil, FLAG_RECT)\n    area_list = []\n    \n    if rect_arr is not None:      #.any()\n        print('This image contain text')\n        for i, rect in enumerate(rect_arr):\n            x, y, w, h = rect\n            area_list.append(w*h)\n\n        total_text_area = sum(area_list)\n        text = \"Yes\"\n        Img_area = img.shape[1]*img.shape[0]\n        Percent_of_text = round(total_text_area *100\/Img_area, 2)\n        \n        text_return_list = [text,Percent_of_text]\n        #[image_path, obj, total_text_area, Img_area, Percent_of_text,[\"No_key_points\"]]\n    else:\n        print('This image not contain any text')\n        text = \"No\"\n        Percent_of_text = 0\n        text_return_list = [text,Percent_of_text]\n        \n        #return_list = [image_path, 'Slide', 0, img.shape[1]*img.shape[0], 0,[\"No_key_points\"]]\n\n    return text, Percent_of_text\n\n#returns presence of text and precentage of text which >=0%","9ba8fbe1":"# intialisation and defing a function for face detection in human being\n\n!pip install mtcnn\n\nfrom matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Circle\nfrom mtcnn.mtcnn import MTCNN\n\n\ndef face_detection(Img_path):\n    \n    pixels = cv2.imread(Img_path)\n    # create the detector, using default weights\n    detector = MTCNN()\n    # detect faces in the image\n    faces = detector.detect_faces(pixels)\n    \n    return faces\n\n#returns faces detected from the imput image","9a420bb3":"# Code adapted from Tensorflow Object Detection Framework\n# https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/object_detection_tutorial.ipynb\n# Tensorflow Object Detection Detector\n#https:\/\/tech.amikelive.com\/node-718\/what-object-categories-labels-are-in-coco-dataset\/   labels in the models and their sequence\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport time\n\nclass DetectorAPI:\n    def __init__(self, path_to_ckpt):\n        self.path_to_ckpt = path_to_ckpt\n\n        self.detection_graph = tf.Graph()\n        with self.detection_graph.as_default():\n            od_graph_def = tf.compat.v1.GraphDef()  #tf.GraphDef()\n            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:   #tf.gfile.GFile\n                serialized_graph = fid.read()\n                od_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(od_graph_def, name='')\n\n        self.default_graph = self.detection_graph.as_default()\n        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n\n        # Definite input and output Tensors for detection_graph\n        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n        # Each box represents a part of the image where a particular object was detected.\n        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n        # Each score represent how level of confidence for each of the objects.\n        # Score is shown on the result image, together with the class label.\n        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n\n    def processFrame(self, image):\n        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n        image_np_expanded = np.expand_dims(image, axis=0)\n        # Actual detection.\n        start_time = time.time()\n        (boxes, scores, classes, num) = self.sess.run(\n            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n            feed_dict={self.image_tensor: image_np_expanded})\n        end_time = time.time()\n\n        print(\"Elapsed Time:\", end_time-start_time)\n\n        im_height, im_width,_ = image.shape\n        boxes_list = [None for i in range(boxes.shape[1])]\n        for i in range(boxes.shape[1]):\n            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n                        int(boxes[0,i,1]*im_width),\n                        int(boxes[0,i,2] * im_height),\n                        int(boxes[0,i,3]*im_width))\n\n        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n\n    def close(self):\n        self.sess.close()\n        self.default_graph.close()\n\n# defing a class for loading and intialising a pretrained model for human detection in an image","4e12a6c4":"model_path = '\/kaggle\/input\/human-pretrain-model\/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\/frozen_inference_graph.pb'\n    #'\/path\/to\/faster_rcnn_inception_v2_coco_2017_11_08\/frozen_inference_graph.pb'\n\nodapi = DetectorAPI(path_to_ckpt=model_path)\n\n# human detetction model and function are defined here\ndef human_detection(image_path):\n    threshold = 0.65\n    \n    img = cv2.imread(image_path)\n    #img = cv2.resize(img, (640, 360))\n\n    boxes, scores, classes, num = odapi.processFrame(img)\n\n    # Visualization of the results of a detection.\n\n    # Class 1 represents human\n    if classes[0] == 1 and scores[0] > threshold:\n        print(\"This image contain Human \")\n        Person = \"Yes\"\n        box = boxes[0]\n        #print(box,'box')\n        #cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n        \n        BBox_area = (box[3]-box[1])*(box[2]- box[0])\n        Img_area = img.shape[1]*img.shape[0]\n        Human_Percent = round(BBox_area*100\/Img_area, 2)\n        \n        #key body points detection\n        Body_key_points = body_movements_tracking(image_path)\n        #text detection in the frame\n        text, text_Percent = text_identification(image_path)\n        \n        num_faces = face_detection(image_path)\n        if len(num_faces)>0:\n            face = \"Yes\"\n        else:\n            face = \"No\"\n            \n        return_list = [image_path, text, text_Percent, Person, Human_Percent,face, Body_key_points]\n        \n    else:\n        text, text_Percent = text_identification(image_path)\n        Person = \"No\"\n        Human_Percent = 0\n        Body_key_points = \"No_key_points\"\n        face = \"No\"\n        return_list = [image_path, text, text_Percent, Person, Human_Percent, face, Body_key_points]\n \n    return return_list","be8a9d8c":"# Step ##---1----##\n# main function code \n\n#given the video file extract frames from the videos with video segmentaion as defined in the problem\nvideo_path = \"\/kaggle\/working\/video-assesment-sheri\/AI_Intro.mp4\" # path to the video to be assessed \nfps = 5  # frames per second\nvid_seg_time = 30 # in seconds, vid segment duration required\nseg_dir_path_list = uft_frame_extraction(video_path, fps, vid_seg_time)","deb99e6c":"#list of segment frames created \n#frames selected per segment are stored in a csv file \nseg_dir_path_list","ef594758":"# Step ##---2----##\n\n# Extracting features  from the frames chosen  from the video in segment wise\nfea_csv_path_list = []\nfor seg_dir_path in seg_dir_path_list:\n    print('\\n \\n',seg_dir_path,'\\n \\n')\n    csv_path = fea_csv_genration(seg_dir_path)\n    fea_csv_path_list.append(csv_path)","d3c3d004":"# csv file path list of features extracted from the frames per segment\nfea_csv_path_list","4dd52ff9":"# Step ##---3----##\n#Key frames extraction process per segment and store the key frames in a csv file\n\nkey_frame_csv_path_list = []\nclustered_frame_path_list=[]\nfor fea_csv_path in fea_csv_path_list:\n    print('\\n \\n',fea_csv_path)\n    sil_scores, clusters_df = sil_elbow(fea_csv_path)\n    print('sil_elbow process completed Successfully')\n    \n    opt_num_clusters = optimul_num_clusters(sil_scores,clusters_df)\n    print('optimul number of cluster extracted')\n    \n    key_frame_path,clustered_frame_path = key_frames_csv_extraction(fea_csv_path,opt_num_clusters)\n    print('key frames csv files is created')\n    \n    key_frame_csv_path_list.append(key_frame_path)\n    clustered_frame_path_list.append(clustered_frame_path)\n    print('Iteration step Completed Sucessfully')","96e137f7":"# Assessment \n# key frames csv path listh in segment wise\nkey_frame_csv_path_list","4f2f8f3a":"# entire frames extracted per segment is stored in this csv files \n# all the frames per segment with their features is stored in this csv files \nclustered_frame_path_list","faa39dde":"# key frame professor action assessment based on the objects detetced in the image\n\ndef frame_assessment(rtrn_list):\n    text = rtrn_list[1]\n    text_p = rtrn_list[2]\n    Person = rtrn_list[3]\n    Human_p = rtrn_list[4]\n    face = rtrn_list[5]\n    #'Nose','Left_eye_inner','Left_eye','Left_eye_outer','Right_eye_inner','Right_eye','Right_eye_outer','Left_ear','Right_ear',\n    #'Mouth_left','Mouth_right'\n    \n    if (Person == \"Yes\" and Human_p > 10 and text == \"Yes\"):\n        \n        if face == \"Yes\":\n            tool = \"Board\"\n            Asmnt = \"Interacting with Students\"\n            \n        if face == \"No\":\n            tool = \"Board\"\n            Asmnt = \"Using Board\"\n        \n    elif (Person == \"Yes\" and Human_p > 10 and text == \"No\"):\n        \n        if face == \"Yes\":\n            tool = \"Board\"\n            Asmnt = \"Interacting with Students\"\n            \n        if face == \"No\":\n            tool = \"Board\"\n            Asmnt = \"Using Board\"\n        \n    elif (Person == \"Yes\" and Human_p <= 10 and text == \"Yes\"):\n        tool = \"Slide\"\n        Asmnt = \"Using Slide\"\n        \n    elif (Person == \"Yes\" and Human_p <= 10 and text == \"No\"):\n        tool = \"Slide\"\n        Asmnt = \"Using Slide\"\n        \n    elif (Person == \"No\" and Human_p == 0 and text == \"Yes\"):\n        tool = \"Slide\"\n        Asmnt = \"Using Slide\"\n        \n    elif (Person == \"No\" and Human_p ==0 and text == \"No\"):\n        tool = \"Slide\"\n        Asmnt = \"Using Slide\"\n    else:\n        tool = \"Null\"\n        Asmnt = \"Null\"\n    \n    return tool, Asmnt","a9bf4d9a":"# Step ##---4----##\n\n# frame assessment code to be included in the main code \n# assessing the frames based on the objects detected in the image\nAssessment_csv_path_list = []\nn_segmts = 0 # number os segments counter\nseg_num_cls = [] #colleting number of segments and number cls per segments\nseg_cls_key_frame_timestamp = [] # time stamp of each key frame \n\nfor key_frame_csv_file in key_frame_csv_path_list: #segment wise call\n    \n    key_df = pd.read_csv(key_frame_csv_file)#csv_file\n    num = int(os.path.split(key_frame_csv_file)[1].split('_')[2]) # video segment number\n    n_segmts = n_segmts +1 # updating the number of segments over the loop iteration\n    \n    fnl_df = pd.DataFrame(columns=['Seg_num', 'label', 'Img_path', 'text', 'text_%', 'Person', 'Human_%', \"face\", 'Body_key_points','Teaching Aid',\"Assessment\"])\n    num_cls_per_seg = key_df.shape[0] # counting the number of clusters per segments\n    \n    seg_num_cls.append({'seg_{}: {}'.format(num, num_cls_per_seg)})\n    \n    icls = 1 #iterator over cluster\n    \n    for Image_path in key_df['Img_path'].values: # call with in segment clusters\n        \n        lst = (os.path.split(Image_path)[1]).split('_') # time stamp extraction from the image path for final assessments \n        sckfts = \"seg_{}_cls_{}_frame_{}_timestamp_{}_seconds\".format(num,icls,icls,float(lst[1]))\n        seg_cls_key_frame_timestamp.append(sckfts)\n        icls = icls + 1 # updating the cls iterative \n        \n        rtrn_list = human_detection(Image_path)\n        \n        tool, Asmnt = frame_assessment(rtrn_list)\n        \n        rtrn_list.append(tool)\n        rtrn_list.append(Asmnt)\n        \n        seg_num = num\n        clust_label = key_df[key_df['Img_path']==Image_path].label.values[0]\n        \n        rtrn_list.insert(0, clust_label)\n        rtrn_list.insert(0, seg_num)\n        #[seg_num, clust_label, image_path, obj, BBox_area, Img_area, Percent_of_image]\n        \n        fnl_df.loc[fnl_df.shape[0]] = rtrn_list\n        \n    \n    # directory for saving the assesment frame csv files \n    asmnt_frame_dir = os.path.join(os.path.split(os.path.split(key_frame_csv_file)[0])[0],'Assessment')\n    \n    if not(os.path.exists(asmnt_frame_dir)):\n        os.mkdir(asmnt_frame_dir) # create directory to save the frames generated\n    \n    kk = (os.path.split(key_frame_csv_file)[1].split('_')[0:3])\n    kk.append('frame_assmnt.csv')\n    file_nam = (\"_\").join(kk)\n\n    asmnt_frame_df_path = os.path.join(asmnt_frame_dir,file_nam)\n    Assessment_csv_path_list.append(asmnt_frame_df_path)\n    fnl_df.to_csv(asmnt_frame_df_path)","1cef5d05":"#primary assessment of the key frames based on object detection methods\n\n# csv files path list - in segmentwise manner\nAssessment_csv_path_list","19778a0a":"# Step ##---5----##\n# sharing the assessment infromation with entire cluster from the key frame\n\ncnts = 1\nSeg_vid_asmnt_df_list = []\nfor i, Assessment_csv_path in enumerate(Assessment_csv_path_list):\n    asmnt = pd.read_csv(Assessment_csv_path).drop([\"Unnamed: 0\"],axis=1)\n    \n    sg_cls = pd.read_csv(clustered_frame_path_list[i]).drop([\"Unnamed: 0\"],axis=1)[['Img_path','label']]\n    \n    la1 = pd.merge(sg_cls, asmnt, how='outer', on='Img_path')\n    \n    #value updatation\n    nc = la1['label_x'].nunique()  #nc number of clusters \n    grps = la1.groupby('label_x')\n    grup_dfs = []\n    \n    for j in np.arange(nc): #loop over nuber of cluster per video segment \n        grpj = grps.get_group(j)\n        nmp = grpj.shape[0] #nmp number of time multiplier or number row in that group\n        \n        h1 = grpj[grpj.columns[:2]] #data frame with ima_path\n        h1.index = np.arange(nmp)\n        #data frame with imputed values from the assesmt cluster frame\n        hu = pd.DataFrame(data=[list(grpj.iloc[0].values[2:])]*nmp, columns=grpj.columns[2:])\n        \n        grp_df = pd.concat([h1,hu],axis=1,ignore_index=True)\n        \n        p1 = list(hu.columns)\n        p2 = list(h1.columns)\n        p1.insert(0, p2[1])\n        p1.insert(0, p2[0])\n        grp_df.columns = p1\n        \n        grup_dfs.append(grp_df)\n        \n    \n    seg_assmnt_df = pd.concat(grup_dfs, axis=0, ignore_index=True)\n    \n    Seg_vid_asmnt_df_list.append(seg_assmnt_df)\n    print('Segment_{} assessment is completed'.format(cnts))\n    cnts = cnts +1\n\n\n#vid segment \nprint('All segments assesments is done \\n  Task completed successfully')\nfull_vid_asmnt_df = pd.concat(Seg_vid_asmnt_df_list, axis=0, ignore_index=True)\n\nfull_vid_asmnt_df.to_csv('\/kaggle\/working\/Complete_video_assesment.csv')","8285eb1a":"# Complete video assessment csv file\n\n# sorting and organising the final assessment csv file into a proper format\ncompl_vid_assmnt_csv = pd.read_csv(\"\/kaggle\/working\/Complete_video_assesment.csv\").drop([\"Unnamed: 0\"],axis=1)\n\nframe_time_stamp = []\nframe_seq_number = []\nfor img_path in compl_vid_assmnt_csv['Img_path'].values:\n    lst = (os.path.split(img_path)[1]).split('_')\n    \n    frame_time_stamp.append(float(lst[1]))\n    frame_seq_number.append(int(lst[3].split('.')[0]))\n    \n\ncompl_vid_assmnt_csv['frame_time_stamp(sec)'] = frame_time_stamp\ncompl_vid_assmnt_csv['frame_seq_number'] = frame_seq_number \n\n#Sorting the frames in sequence \ncompl_vid_assmnt_csv.sort_values(by=['frame_seq_number'],axis=0,ignore_index=True,inplace=True)\n\n# saving the final CSV file\ncompl_vid_assmnt_csv.to_csv('\/kaggle\/working\/compl_sorted_assessment.csv')","7b603719":"#final csv file with assessments made per cluster and presented in a csv file format\ncompl_vid_assmnt_csv.head(50)","1f960d26":"#####_______final step_______#########\n# final format require for submission \n\nmeta_data = pd.read_csv(\"\/kaggle\/input\/video-assesment-sheri\/metadata_AI.csv\")\n\n\nPlaylist_ID =  meta_data.PlayListID[0]\n    \nVideo_ID = meta_data.VideoID[0]\n    \nVideo_Title = meta_data.Title[0]\n\nNumber_of_Segments = n_segmts\nNumber_of_Keyframes = seg_num_cls \nTiming_of_each_keyframe = seg_cls_key_frame_timestamp\n\n\n\nfrom datetime import timedelta\n\ndef vid_duration(video_path):\n    #video_path = \"\/kaggle\/input\/video-assesment-sheri\/AI_Intro.mp4\"\n    cap = cv2.VideoCapture(video_path)\n    fps1 = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    duration = frame_count\/fps1\n    \n    time = timedelta(seconds=duration)\n    vid_durtn = str(time)\n    \n    return vid_durtn\n\n#Total Duration (in hh:mm:ss)\nTotal_Duration = vid_duration(video_path)\n\n\nInstr_dct = dict(compl_vid_assmnt_csv['Person'].value_counts())\n# HH:mm:ss.microseconds format\n\nif ('Yes' in list(Instr_dct.keys())):\n    Instructor_Presence = \"Yes\"\n    time = timedelta(seconds=int(Instr_dct['Yes']))*(1\/fps)\n    Instructor_Presence_duration = str(time)\n    Instructor_Presence_duration\nelse:\n    Instructor_Presence = \"No\"\n    Instructor_Presence_duration = 0\n\n#The total time when the instructor is present (mm:ss) \n\n#Body movement (Y\/N) \n\nkeyp_dct = dict(compl_vid_assmnt_csv[\"Body_key_points\"].value_counts())\n# HH:mm:ss.microseconds format\n\nif sum(list(keyp_dct.values())) > int(keyp_dct['No_key_points']):\n    Body_movement = \"yes\" \nelse:\n    Body_movement = \"No\"\n#Body movement (Y\/N) \n\nTA_dct = dict(compl_vid_assmnt_csv[\"Assessment\"].value_counts())\n\nStime = timedelta(seconds=int(TA_dct['Using Slide'])*(1\/fps))\nUse_of_Slides = str(Stime)\n\nif 'Using Board' in TA_dct.keys():\n    Btime = timedelta(seconds=int(TA_dct['Using Board'])*(1\/fps))\n    Use_of_Board = str(Btime)\nelse:\n    Use_of_Board = \"00:00:00\"\n\n\n#Average fraction of text on slides w.r.t. total slide area\navg_txt = compl_vid_assmnt_csv[compl_vid_assmnt_csv['text']==\"Yes\"][\"text_%\"].mean()","29a1183f":"import json\nfrom collections import defaultdict\nimport pandas as pd\nimport sys,os\nfrom tqdm import tqdm\nfrom urllib import *\nimport argparse\nfrom urllib.parse import urlparse, urlencode, parse_qs\nfrom urllib.request import urlopen\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\n\ndef openURL(url, parms):\n        f = urlopen(url + '?' + urlencode(parms))\n        data = f.read()\n        f.close()\n        matches = data.decode(\"utf-8\")\n        return matches\n\n\ndef load_comments(mat):\n    for item in mat[\"items\"]:\n        comment = item[\"snippet\"][\"topLevelComment\"]\n        comments[\"commentId\"].append(comment[\"id\"])\n        comments['videoId'].append(comment[\"snippet\"][\"videoId\"])\n        comments[\"comment\"].append(comment[\"snippet\"][\"textDisplay\"])\n        comments[\"commentAuthor\"].append(comment[\"snippet\"][\"authorDisplayName\"])\n        comments[\"commentLikesCount\"].append(comment[\"snippet\"][\"likeCount\"])\n        comments[\"commentTimeStamp\"].append(comment[\"snippet\"][\"publishedAt\"])\n\n        if 'replies' in item.keys():\n            for reply in item['replies']['comments']:\n                replies[\"replyId\"].append(reply[\"id\"])\n                replies[\"videoId\"].append(reply[\"snippet\"][\"videoId\"])\n                replies[\"parentCommentId\"].append(reply[\"snippet\"][\"parentId\"])\n                replies[\"replyAuthor\"].append(reply['snippet']['authorDisplayName'])\n                replies[\"reply\"].append(reply[\"snippet\"][\"textDisplay\"])\n                replies[\"replyTimeStamp\"].append(reply[\"snippet\"][\"publishedAt\"])\n                replies[\"replylikesCount\"].append(reply[\"snippet\"][\"likeCount\"])\n\ndef get_video_comments(YOUTUBE_COMMENT_URL,params):\n    url_response = json.loads(openURL(YOUTUBE_COMMENT_URL, params))\n    nextPageToken = url_response.get(\"nextPageToken\")\n    load_comments(url_response)\n\n    while nextPageToken:\n        params.update({'pageToken': nextPageToken})\n        url_response = json.loads(openURL(YOUTUBE_COMMENT_URL, params))\n        nextPageToken = url_response.get(\"nextPageToken\")\n        load_comments(url_response)\n        \n\n        \n        \nyoutube_key = \"AIzaSyCiJMzYkM97Co3WEy5naMWEHA2BkGjjf_8\" #\"AIzaSyB5B6JF8RrcO8Exp6Yuam9NLT69JnK8-xw\"\nYOUTUBE_COMMENT_URL = 'https:\/\/www.googleapis.com\/youtube\/v3\/commentThreads'\n\ncomments = defaultdict(list)\nreplies  = defaultdict(list)\n\ndf = pd.read_csv(\"\/kaggle\/input\/video-assesment-sheri\/metadata_AI.csv\")\nvideo_id = df.VideoID.values[0]\n\n\nparams = {'part': 'snippet,replies',\n                'maxResults': 100,\n                'videoId': video_id,\n                'textFormat': 'plainText',\n                'key': youtube_key\n                }\n\nget_video_comments(YOUTUBE_COMMENT_URL,params) \n\ncomment_df = pd.DataFrame().from_dict(comments)\nreply_df = pd.DataFrame().from_dict(replies)\n\ncomment_df.to_csv(\"comments_data.csv\",index=False)\nreply_df.to_csv(\"reply_data.csv\",index=False)\n\n\ncmnts = list(comment_df.comment.values)\nrply = list(reply_df.reply.values)\n\ncmnts.extend(rply)\ncmnts\nlen(cmnts)","cdf4c7d1":"!pip install vaderSentiment\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()","8a147872":"Positive  = []\nNegative = []\nNutral = []\nClass_vdr = []\nQsn = []\n\nfor cmnt in cmnts:\n    \n    sentence = cmnt\n    vs = analyzer.polarity_scores(sentence)\n    \n    Positive.append(vs[\"pos\"])\n    Negative.append(vs[\"neg\"])\n    Nutral.append(vs[\"neu\"])\n    \n    idx = np.argmax(list(vs.values())[:-1])\n    \n    if idx == 0:\n        cl = \"neg\"\n    if idx == 1:\n        cl = \"neu\"\n    if idx == 2:\n        cl = \"pos\"\n    \n    Class_vdr.append(cl)\n    \n    if \"?\" in cmnt:\n        Qsn.append(\"Yes\")\n    else:\n        Qsn.append(\"No\")\n\ncmnt_vdr_df = pd.DataFrame(columns = ['Comments', \"Positive\", \"Negative\", \"Nutral\", \"Class_vader\", \"Question?\"])\ncmnt_vdr_df[\"Comments\"] = cmnts\ncmnt_vdr_df[\"Positive\"] = Positive\ncmnt_vdr_df[\"Negative\"] = Negative\ncmnt_vdr_df[\"Nutral\"] = Nutral\ncmnt_vdr_df[\"Class_vader\"] = Class_vdr\ncmnt_vdr_df[\"Question?\"] = Qsn\n\n\ncmnt_vdr_df.head()","76c17360":"vdr_PN = dict(cmnt_vdr_df[\"Class_vader\"].value_counts())\nvdr_Qsn = dict(cmnt_vdr_df[\"Question?\"].value_counts())","83b872a2":"Number_of_Total_Comments  = len(cmnts)\nNumber_of_Positive_comments = vdr_PN[\"pos\"]\nNumber_of_Negative_comments = vdr_PN[\"neg\"]\nNumber_of_Nutral_comments = vdr_PN[\"neu\"]\n\nif \"Yes\" in list(vdr_Qsn.keys()):\n    Number_of_questions = vdr_Qsn[\"Yes\"]\nelse:\n    Number_of_questions = 0\n    \n[Number_of_Total_Comments, Number_of_Positive_comments, Number_of_Negative_comments, Number_of_Nutral_comments, Number_of_questions]","39e6c67b":"### The above flair approach will give different results if it works on your note and kaggle cpu,\n#kindly uncomments the cells and run them","e8e869f8":"Playlist_ID","8f23682b":"Video_ID","d75f79f5":"Video_Title","abb4f2ef":"Total_Duration\n# HH_mm_ss_microseconds","03cde964":"Number_of_Segments","03ab528d":"Number_of_Keyframes\n# segment_number_of_keyframes","8425fdf6":"Timing_of_each_keyframe\n# Segment_cluster with the segment _ frame per cluster _ times stamp of the each frame is clearly given below","4f4490ba":"Body_movement\n# this is given in over sense \n# for every cluster sense kindly look into the finall assessments  csv file which shows the body key points \n# if body keypoints present then there are some body moments in the video or else no body moments","17482fef":"Instructor_Presence\n# this is given in over sense \n# for every cluster sense kindly look into the finall assessments  csv file which shows the body key points \n# if perseon is yes then instructor is present in the frame or else there is no instructor in the video","7f8b5a13":"Instructor_Presence_duration\n# HH_mm_ss_microseconds","38530010":"Use_of_Slides\n# this is given in over sense \n# for every cluster sense kindly look into the finall assessments  csv file which shows the body key points ","d5c6e799":"Use_of_Board\n# this is given in over sense \n# for every cluster sense kindly look into the finall assessments  csv file which shows the body key points ","038bf783":"avg_txt\n# this given as percentage ","f9952092":"Number_of_Total_Comments\n# with refeerence to vader approach","8a833eb2":"Number_of_Positive_comments\n# with refeerence to vader approach","3610ff2d":"Number_of_Negative_comments\n# with refeerence to vader approach","df19efb2":"Number_of_Nutral_comments\n# with refeerence to vader approach","368b6532":"Number_of_questions\n# with refeerence to vader approach","a059fc7b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport re","2192af21":"# path = \"\/kaggle\/input\/cmnts-df\/Xg_cmnt_df.csv\"\nXg_cmnt = pd.read_csv(\"\/kaggle\/input\/cmnts-df\/Xg_cmnt_df.csv\")\nXg_cmnt","f97ab857":"Xg_cmnt['Pos_Neg'] = Xg_cmnt['Pos_Neg'].apply(lambda x : 1 if (x==\"POSITIVE\") else 0)\nXg_cmnt.shape","f626a50e":"Xg_cmnt.head()","86084d34":"Xg_cmnt['Pos_Neg'].value_counts()","b5ef14f8":"cmnts = list(Xg_cmnt['Comments'].values)\ncmnts[0:50]","59438244":"cmnts1 = [cmnt.lower() for cmnt in cmnts]\nc1 = [re.sub(\"@\",\"\", cmnt) for cmnt in cmnts1]\n#re.sub(\"\\w+:\/\/\\S+\",\"\", \"@Rahim this course rocks! http:\/\/rahimbaig.com\/ai\")\nc2 = [re.sub(\"\\w+:\/\/\\S+\",\"\", cmnt) for cmnt in c1]\np1 = [re.sub(\"\\w+[.w]\\w+\\.com\",\"\",k) for k in c2]\np2 = [re.sub(\"[.w]\\w+\\.com\",\"\",k) for k in p1]\np3 = [re.sub(\"\\w+\\W+\\.com\",\"\",k) for k in p2]\np4 = [re.sub(\"[\u00f0\u00e2]\\W+\",\"\",k) for k in p3]\ncmnts3 = [re.sub(\"www.\\w+.com\",\"\",k) for k in p4]\n\nl1 = [re.sub(\"n't\",\" not\",k) for k in cmnts3]\nl2 = [re.sub(\"n\u2019t\",\" not\",k) for k in l1]\n\np5 = [re.sub(\"\\W\",\" \",k) for k in l2]\np6 = [re.sub(\"\\d\",\"\",k) for k in p5]\np7 = [re.sub(\"\\s\",\"  \",k) for k in p6]\np8 = [re.sub(\"\\s\\w\\s\",\" \",k) for k in p7]\np9 = [re.sub(\"\\W\",\" \",k) for k in p8]\nlen(p9)","e7119324":"p9[0:50]","ce66631f":"!pip install wordcloud","f8c0bffb":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\n\nfrom wordcloud import WordCloud\nwc = WordCloud()\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport nltk \nfrom nltk.stem import PorterStemmer ","c2723e13":"ps = PorterStemmer()\n\nstpw = stopwords.words(\"english\")\n\n\np10 = []\nfor cmt in p9:\n    \n    tkn = word_tokenize(cmt)\n    wds = [ws for ws in tkn if ws not in stpw]\n    stm = []\n    for w in wds:\n        stm.append(ps.stem(w))\n    \n    p10.append((\" \").join(stm))\n\n\nsen1 = (\" \").join(p10)\n\n\nwc_map = wc.generate(sen1)\nplt.figure(figsize=(20,20))\nplt.imshow(wc_map)","2e4d50fe":"len(p10)","6d8f2b39":"Xg_cmnt[\"cleaned_comments\"] = p10\nXg_cmnt","9c2bdab9":"clnd_cmnts = []\nfor g in Xg_cmnt[\"cleaned_comments\"].values:\n    if len(g)==0:\n        g = \"None\"\n        clnd_cmnts.append(g)\n    else:\n         clnd_cmnts.append(g)\n\n\nXg_cmnt[\"cleaned_comments\"] = clnd_cmnts\n\nXg_fea = Xg_cmnt[Xg_cmnt[\"cleaned_comments\"]!=\"None\"]\n\nXg_fea.index = np.arange(0,Xg_fea.shape[0])\nXg_fea\n","7f6454bd":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n\n\ntknr = tf.keras.preprocessing.text.Tokenizer(num_words=500,split = ' ')\ntknr.fit_on_texts(Xg_fea.cleaned_comments.values)\nX1 = tknr.texts_to_sequences(Xg_fea.cleaned_comments.values)\nX1 = tf.keras.preprocessing.sequence.pad_sequences(X1)","9df66e89":"model = tf.keras.Sequential()\nmodel.add(Embedding(500,100,input_length = X1.shape[1]))\nmodel.add(LSTM(160,dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(2, activation = \"Softmax\"))\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\nprint(model.summary())\n","c9cff7b3":"from sklearn.model_selection import train_test_split\n\ny1 = pd.get_dummies(Xg_fea.Pos_Neg)\n\nX1_train, X1_valid, y1_train, y1_valid = train_test_split(X1,y1, test_size = 0.10, random_state=42)\n\nbs = 32\nmodel.fit(X1_train,y1_train, epochs=10, batch_size = bs, verbose = 1)","6f8cbd91":"model.evaluate(X1_valid, y1_valid)","4b4f938f":"# step  ###______6______###\n# code for visualisation \n\nImg_path = compl_vid_assmnt_csv.Img_path[126]\nimg = cv2.imread(Img_path)\n\nplt.figure(figsize=(20,20))\nplt.grid(False)\n\nplt.imshow(img)","8e306b55":"# step  ###______7______###\n# code for visualisation \n\nmpPose = mp.solutions.pose\npose = mpPose.Pose()\nmpDraw = mp.solutions.drawing_utils\n\n\nimg = cv2.imread(Img_path)\nimgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresults = pose.process(imgRGB)\n\n# print(results.pose_landmarks)\n\nif results.pose_landmarks:\n    mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n    \n    for id, lm in enumerate(results.pose_landmarks.landmark):\n        h, w,c = img.shape\n        #print(id, lm)\n        cx, cy = int(lm.x*w), int(lm.y*h)\n        cv2.circle(img, (cx, cy), 5, (255,0,0), cv2.FILLED)\n\nplt.figure(figsize=(20,20))\nplt.grid(False)\nplt.imshow(img)","4c41a0f8":"body_movements_tracking(Img_path)","04fc33c8":"text_identification(Img_path)","ae3bad66":"img_list =[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/vid-asmnt-test-images'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        img_list.append(os.path.join(dirname, filename))","2de93dd6":"# step  ###______7______###\n# code for visualisation \n\nmpPose = mp.solutions.pose\npose = mpPose.Pose()\nmpDraw = mp.solutions.drawing_utils\n\n\nimg = cv2.imread(img_list[1])\nimgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresults = pose.process(imgRGB)\n\n# print(results.pose_landmarks)\n\nif results.pose_landmarks:\n    mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n    \n    for id, lm in enumerate(results.pose_landmarks.landmark):\n        h, w,c = img.shape\n        #print(id, lm)\n        cx, cy = int(lm.x*w), int(lm.y*h)\n        cv2.circle(img, (cx, cy), 5, (255,0,0), cv2.FILLED)\n\nplt.figure(figsize=(20,20))\nplt.grid(False)\nplt.imshow(img)","7b114c52":"# step  ###______7______###\n# code for visualisation \n\nmpPose = mp.solutions.pose\npose = mpPose.Pose()\nmpDraw = mp.solutions.drawing_utils\n\n\nimg = cv2.imread(img_list[4])\nimgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresults = pose.process(imgRGB)\n\n# print(results.pose_landmarks)\n\nif results.pose_landmarks:\n    mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n    \n    for id, lm in enumerate(results.pose_landmarks.landmark):\n        h, w,c = img.shape\n        #print(id, lm)\n        cx, cy = int(lm.x*w), int(lm.y*h)\n        cv2.circle(img, (cx, cy), 5, (255,0,0), cv2.FILLED)\n\nplt.figure(figsize=(20,20))\nplt.grid(False)\nplt.imshow(img)","63a5c4bf":"# step  ###______7______###\n# code for visualisation \n\nmpPose = mp.solutions.pose\npose = mpPose.Pose()\nmpDraw = mp.solutions.drawing_utils\n\n\nimg = cv2.imread(img_list[6])\nimgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nresults = pose.process(imgRGB)\n\n# print(results.pose_landmarks)\n\nif results.pose_landmarks:\n    mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n    \n    for id, lm in enumerate(results.pose_landmarks.landmark):\n        h, w,c = img.shape\n        #print(id, lm)\n        cx, cy = int(lm.x*w), int(lm.y*h)\n        cv2.circle(img, (cx, cy), 5, (255,0,0), cv2.FILLED)\n\n\nplt.figure(figsize=(20,20))        \nplt.grid(False)\nplt.imshow(img)","872cdbec":"from matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Circle\nfrom mtcnn.mtcnn import MTCNN\n \n# draw an image with detected objects\ndef draw_image_with_boxes(filename, result_list):\n\t# load the image\n\tdata = cv2.imread(filename)\n\t# plot the image\n\tpyplot.imshow(data)\n\t# get the context for drawing boxes\n\tax = pyplot.gca()\n\t# plot each box\n\tfor result in result_list:\n\t\t# get coordinates\n\t\tx, y, width, height = result['box']\n\t\t# create the shape\n\t\trect = Rectangle((x, y), width, height, fill=False, color='red')\n\t\t# draw the box\n\t\tax.add_patch(rect)\n\t\t# draw the dots\n\t\tfor key, value in result['keypoints'].items():\n\t\t\t# create and draw dot\n\t\t\tdot = Circle(value, radius=2, color='red')\n\t\t\tax.add_patch(dot)\n    \n    \n    # show the plot\n    \n\tpyplot.show()","70de53ba":"## Face detection code\nfilename = Img_path\n# load image from file\npixels = cv2.imread(filename)\n# create the detector, using default weights\ndetector = MTCNN()\npyplot.figure(figsize=(20,20))\npyplot.grid(False)\n# detect faces in the image\nfaces = detector.detect_faces(pixels)\n# display faces on the original image\n\ndraw_image_with_boxes(filename, faces)","fa283c7f":"Img_path = compl_vid_assmnt_csv.Img_path[34]\nimg = cv2.imread(Img_path)\n\nplt.figure(figsize=(20,20))\nplt.grid(False)\nplt.imshow(img)","a6f9af52":"img_pil = Image.open(Img_path)\ngray_pil = img_pil.convert(\"L\")\n\nrect_arr = detect(img_pil, FLAG_RECT)\n\nimg_draw = ImageDraw.Draw(img_pil)\ncolors = ['red', 'green', 'blue', \"yellow\", \"pink\"]\n\nfor i, rect in enumerate(rect_arr):\n    x, y, w, h = rect\n    img_draw.rectangle(\n        (x, y, x + w, y + h),\n        outline=colors[i % len(colors)],\n        width=4)\n\nimg_pil","5bd68c4b":"Img_path = compl_vid_assmnt_csv.Img_path[1490]\nimg = cv2.imread(Img_path)\n\npyplot.figure(figsize=(20,20))\nplt.grid(False)\nplt.imshow(img)","36e10c3e":"img_pil = Image.open(Img_path)\ngray_pil = img_pil.convert(\"L\")\n\nrect_arr = detect(img_pil, FLAG_RECT)\n\nimg_draw = ImageDraw.Draw(img_pil)\ncolors = ['red', 'green', 'blue', \"yellow\", \"pink\"]\n\nfor i, rect in enumerate(rect_arr):\n    x, y, w, h = rect\n    img_draw.rectangle(\n        (x, y, x + w, y + h),\n        outline=colors[i % len(colors)],\n        width=4)\n\n\nimg_pil","127341d8":"from flair.models import TextClassifier\nfrom flair.data import Sentence\n\nclassifier = TextClassifier.load('en-sentiment')","74b38660":"# Project Task: Week 2\n\n* Assessment of Instructor Presence and Interaction:\n\n* Time: Identify the fraction of the amount of time the Instructor is \"visible\" over total video length. \n* Also, identify, whether the instructor is present on the whole screen or a part of the video frame (PIP).\n* Interaction of Instructor: Identify body movements (e.g., full-body, face, hands, etc.).\n* Assessment of the use of blackboard, slides:\n\n#####\n\n* Identify whether the instructor has used blackboard or slides or a combination of both in the video. \n* Also, identify the fraction of time each of the teaching aids is used.\n* If instructors have used slides, determine if the instructor has put too much text in one slide.","ff245de1":"### Creatin LSTM and embedding based approach for sentimental analysis","16f3d88e":"## reading data from csv files","3ffbf7c1":"#### Text claening and preprocessing for word vectorisation","162baeb8":"Xg_cmnt_df = cmnt_df[[\"Pos_Neg\",\"Comments\"]]\nXg_cmnt_df.to_csv(\"\/kaggle\/working\/Xg_cmnt_df.csv\", index=None)","187bd21b":"### Key points detection and localisation ","8e734ab6":"# Visualisation Section","a014dfbe":"# Project Task: Week 3\n\n\nYoutube comments and reply sentiments detection and Analysis using Youtube API v3\n\nUsing Youtube Data API v3, extract comments (and replies to comments) for each video.\nClassify comments (and their replies) as one of the categories: neutral comment, positive comment, negative comment, or a question.","313e6b58":"# Project Task: Week 1\n\nVideo Segmentation: \n\n1. The first task is to divide the video into keyframes using Uniform Time Sampling followed by clustering.\n\n Uniform Time Sampling\nIdentify a suitable time interval for each video by analyzing each video individually. Divide the video into multiple equally spaced segments using this time interval. Within each segment, select one or more keyframes by clustering on individual frames in each segment.\nE.g., initially take one minute as a time interval and divide the video into 1-minute segments. Now, within each segment, select one or more keyframes using clustering on video frames.","bee400f3":"# Project Task: Week 4\n\n* Deployment In Sagemaker\n* \n* Take the comment datasets that have been generated from the last tasks\n* Split the data in train and test\n* Use predefined inbuilt xgboost model to train in container\n* Train the model using sagemaker estimator\n* Deploy the model using sagemaker estimator\n","ba8b0281":"### beginning of assessment function and deep learning models for object and text detection from the given images","27eda515":"# Final Results Required after assessment and comment sentiments analysis","327c5c26":"cmnt_df.head()","c21611a9":"Number_of_Total_Comments  = len(cmnts)\nNumber_of_Positive_comments = flair_PN[\"POSITIVE\"]\nNumber_of_Negative_comments = flair_PN[\"NEGATIVE\"]\nNumber_of_Nutral_comments = 0 # for this approach only\n\n\nif \"Yes\" in list(flair_Qsn.keys()):\n    Number_of_questions = flair_Qsn[\"Yes\"]\nelse:\n    Number_of_questions = 0\n    \n[Number_of_Total_Comments, Number_of_Positive_comments, Number_of_Negative_comments, Number_of_Nutral_comments, Number_of_questions]","7635d3db":"# Vader Model Approach for sentimental analysis","08233ff8":"!pip install flair","73b7bef5":"Class = []\nprob = []\nQsn = []\nfor cmnt in cmnts:\n    \n    sentence = Sentence(cmnt)\n    classifier.predict(sentence)\n    rslt = str(sentence.labels[0]).split(\" \")\n    Class.append(rslt[0])\n    prob.append(rslt[1])\n    \n    if \"?\" in cmnt:\n        Qsn.append(\"Yes\")\n    else:\n        Qsn.append(\"No\")\n\ncmnt_df = pd.DataFrame(columns = ['Comments', \"Pos \/ Neg\", \"Probability\", \"Question?\"])\ncmnt_df[\"Comments\"] = cmnts\ncmnt_df[\"Pos \/ Neg\"] = Class\ncmnt_df[\"Probability\"] = prob\ncmnt_df[\"Question?\"] = Qsn","32c040ef":"# Comments Processing Section","1f8f1fab":"### Face detection \n\nFace, Eyes, Mouth detection","9642e6f3":"* The combined output of all tasks may have the following information:\n\n* Playlist ID:\n* Video ID:\n* Video Title:\n* Total Duration (in hh:mm:ss)\n* Number of Segments, Number of Keyframes, Timing of each keyframe\n* Instructor Presence\n* The total time when the instructor is present (mm:ss)\n* Body movement (Y\/N)\n* Use of Slides (mm:ss), Use of Blackboard (mm:ss)\n* Average fraction of text on slides w.r.t. total slide area\n* Number of Total Comments, Number of Positive comments, Number of Negative comments, Number of questions","077b1b2b":"# Flair model based sentiment analysis","fb946d5d":"#### feature vectors extraction from the cleaned comments","cd74c833":"#### ploting word cloud map","608d97ba":"### Test image for visualisation ","9d1377a2":"### Test Images selcted from the similar NPTEL video","017bc928":"### Test Image for validation of the models","9d678fe3":"# AWS Implimentation of Xgboost","29fa26a0":"# Final Assessment CSV File","c962fd25":"# code for execution of the entire functions defined above","2674b64d":"#### loading lib","11d5ed00":"### text sentiment analysis","fe41ead3":"# Final processing  for ouput parameter extraction","b49e21af":"## Text detection code","d3e5a81f":"* Sometimes this flair model may not work due to different version compatible  issues \n* it is working very well in kaggle link given below \n* kindly look into the link and kaggle notebook\n\nhttps:\/\/www.kaggle.com\/prashanthsheri\/extracting-youtube-comments","254c8f3f":"flair_PN = dict(cmnt_df[\"Pos_Neg\"].value_counts())\nflair_Qsn = dict(cmnt_df[\"Question?\"].value_counts())"}}