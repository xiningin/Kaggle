{"cell_type":{"a41e1bd3":"code","2618192a":"code","45d5c520":"code","503ca9fe":"code","6a86d1f4":"code","40ac51b9":"code","9085c196":"code","b5ccd872":"code","b39df5ff":"code","9905f20e":"code","64706d26":"code","e5153e55":"markdown","07b82a2c":"markdown","5a0111df":"markdown","1c4947a3":"markdown"},"source":{"a41e1bd3":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\nimport scipy.stats                # statistics\nfrom sklearn import preprocessing\n\nimport os\nprint(os.listdir(\"..\/input\"))","2618192a":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\n# Print the head of df\nprint(df.head())\n\n# Print the info of df\nprint(df.info())\n\n# Print the shape of df\nprint(df.shape)","45d5c520":"#Scaling the continuos variables\ndf_scale = df.copy()\nscaler = preprocessing.StandardScaler()\ncolumns =df.columns[1:7]\ndf_scale[columns] = scaler.fit_transform(df_scale[columns])\ndf_scale.head()\ndf_scale = df_scale.iloc[:,1:9]","503ca9fe":"sample = np.random.choice(df_scale.index, size=int(len(df_scale)*0.8), replace=False)\ntrain_data, test_data = df_scale.iloc[sample], df_scale.drop(sample)\n\nprint(\"Number of training samples is\", len(train_data))\nprint(\"Number of testing samples is\", len(test_data))\nprint(train_data[:10])\nprint(test_data[:10])","6a86d1f4":"features = train_data.drop('Chance of Admit ', axis=1)\ntargets = train_data['Chance of Admit ']\ntargets = targets > 0.5\nfeatures_test = test_data.drop('Chance of Admit ', axis=1)\ntargets_test = test_data['Chance of Admit ']\ntargets_test = targets_test > 0.5","40ac51b9":"# Activation (sigmoid) function\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\ndef error_term_formula(y, output):\n    return (y-output) * output * (1 - output)","9085c196":"# Neural Network hyperparameters\nepochs = 9000\nlearnrate = 0.3\n\n# Training function\ndef train_nn(features, targets, epochs, learnrate):\n    \n    # Use to same seed to make debugging easier\n    np.random.seed(42)\n\n    n_records, n_features = features.shape\n    last_loss = None\n\n    # Initialize weights\n    weights = np.random.normal(scale=1 \/ n_features**.5, size=n_features)\n    print(weights.shape)\n\n    for e in range(epochs):\n        del_w = np.zeros(weights.shape)\n        for x, y in zip(features.values, targets):\n            # Loop through all records, x is the input, y is the target\n\n            # Activation of the output unit\n            #   Notice we multiply the inputs and the weights here \n            #   rather than storing h as a separate variable \n            output = sigmoid(np.dot(x, weights))\n\n            # The error, the target minus the network output\n            error = error_formula(y, output)\n\n            # The error term\n            #   Notice we calulate f'(h) here instead of defining a separate\n            #   sigmoid_prime function. This just makes it faster because we\n            #   can re-use the result of the sigmoid function stored in\n            #   the output variable\n            error_term = error_term_formula(y, output)\n\n            # The gradient descent step, the error times the gradient times the inputs\n            del_w += error_term * x\n\n        # Update the weights here. The learning rate times the \n        # change in weights, divided by the number of records to average\n        weights += learnrate * del_w \/ n_records\n\n        # Printing out the mean square error on the training set\n        if e % (epochs \/ 10) == 0:\n            out = sigmoid(np.dot(features, weights))\n            loss = np.mean((out - targets) ** 2)\n            print(\"Epoch:\", e)\n            if last_loss and last_loss < loss:\n                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n            else:\n                print(\"Train loss: \", loss)\n            last_loss = loss\n            print(\"=========\")\n    print(\"Finished training!\")\n    return weights\n    \nweights = train_nn(features, targets, epochs, learnrate)","b5ccd872":"# Calculate accuracy on test data\ntes_out = sigmoid(np.dot(features_test, weights))\npredictions = tes_out > 0.5\n#predictions = tes_out\naccuracy = np.mean((predictions == targets_test))\nprint(\"Prediction accuracy: {:.3f}\".format(accuracy))","b39df5ff":"features = train_data.drop('Chance of Admit ', axis=1)\ntargets = train_data['Chance of Admit ']\nfeatures_test = test_data.drop('Chance of Admit ', axis=1)\ntargets_test = test_data['Chance of Admit ']","9905f20e":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n\n# Building the model\nmodel = Sequential()\nmodel.add(Dense(1, activation='softmax', input_shape=(7,)))\nmodel.add(Dense(1, activation='softmax'))\n\n# Compiling the model\nmodel.compile(loss = 'binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()","64706d26":"# Training the model\nmodel.fit(features, targets, epochs=9000, batch_size=1, verbose=0)","e5153e55":"## Basic Exploratory Data Analysis\n\nMore about [preparation and exploratory analysis](https:\/\/www.kaggle.com\/camiloemartinez\/student-admission-clusters).","07b82a2c":"## Objective\nThe idea is to develop a NN to undestand the basic concepts of Deep Learning. Specially the concept of backpropagation.\n\n- Doing a feedforward operation.\n- Comparing the output of the model with the desired output.\n- Calculating the error.\n- Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n- Use this to update the weights, and get a better model.\n- Continue this until we have a model that is good.","5a0111df":"## Short Way Keras","1c4947a3":"## Data Transformations"}}