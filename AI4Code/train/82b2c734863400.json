{"cell_type":{"9f289619":"code","ef81d291":"code","7f22cf7d":"code","ca29a395":"code","07f8ce55":"code","50250f45":"code","26b7ef3a":"code","d4b3c1b4":"code","71a7115c":"code","689908a5":"code","d30493e1":"code","dc3b4b8e":"code","cf642391":"code","05c78c10":"code","ef463c06":"code","4a21c40f":"code","87b52d9a":"code","70431de5":"code","c0fe521a":"code","b099abee":"code","e72f7716":"code","6d04ef71":"code","aa81c806":"code","c4f8f644":"code","4a5cb3ad":"code","9238d893":"code","50ffdebe":"code","6006c1a1":"code","811d9080":"code","c6670d59":"code","7019c978":"code","3f9e06ad":"code","6e724a14":"code","da1f4013":"code","9ce72a41":"code","3ee9bf9d":"code","ccd6d47f":"code","561b7c1c":"code","5e82b418":"code","9bead048":"code","63dfa0b9":"code","38517e28":"markdown","4f16c5f9":"markdown","230f4f87":"markdown","d984711b":"markdown","e7e95d06":"markdown","efeab751":"markdown","75e54e3b":"markdown","fbed3998":"markdown","a82b6bec":"markdown","e583ba3e":"markdown","ef771523":"markdown","3b2b2ece":"markdown","1324d742":"markdown","88f2d85d":"markdown","06a6243f":"markdown","6f7846b7":"markdown","35d348f3":"markdown","de5b21b5":"markdown","1dc2a53f":"markdown","f8e246a7":"markdown","de0537ec":"markdown","075d1051":"markdown","88c46af2":"markdown","08c9b30c":"markdown","925c200d":"markdown","366ef919":"markdown","1c57d233":"markdown","badfbcfd":"markdown","3a3755e5":"markdown","6edfa168":"markdown","ec0343b1":"markdown","3e696195":"markdown","f5401a92":"markdown"},"source":{"9f289619":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef81d291":"!pip install phik\n!pip install prince\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"ticks\", color_codes=True)\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report,roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import TransformerMixin,BaseEstimator\nimport phik\nfrom phik import resources, report\nimport prince","7f22cf7d":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","ca29a395":"cond = df['gender']=='Other'\noccu = len(df['gender'][cond])\nprint('Number of occurrences %d corresponding to %.2f%% of the dataset'%(occu, 100*(np.float64(occu)\/len(df))))","07f8ce55":"cond = (df['bmi']>47) & (df['stroke']==1)\noccu = len(df['bmi'][cond])\nprint('Number of occurrences of BMI>47 and stroke=1 %d corresponding to %.2f%% of the dataset'%(occu, 100*(np.float64(occu)\/len(df))))\n\ncond = (df['bmi']>47) & (df['stroke']==0)\noccu = len(df['bmi'][cond])\nprint('Number of occurrences of BMI>47 and stroke=0 %d corresponding to %.2f%% of the dataset'%(occu, 100*(np.float64(occu)\/len(df))))","50250f45":"cond = (df['avg_glucose_level']<57) & (df['stroke']==1)\noccu = len(df['avg_glucose_level'][cond])\nprint('Number of occurrences of average glucose level <57 and stroke=1 %d corresponding to %.2f%% of the dataset'%(occu, 100*(np.float64(occu)\/len(df))))\n\ncond = (df['avg_glucose_level']<57) & (df['stroke']==0)\noccu = len(df['avg_glucose_level'][cond])\nprint('Number of occurrences of average glucose level <57 and stroke=0 %d corresponding to %.2f%% of the dataset'%(occu, 100*(np.float64(occu)\/len(df))))","26b7ef3a":"df.isnull().sum()","d4b3c1b4":"# from sklearn.impute import KNNImputer\n# imp = KNNImputer(missing_values=np.NaN)\n# df.bmi = imp.fit_transform(df['bmi'].values.reshape(-1,1))[:,0]","71a7115c":"# categ_feat = ['gender','Residence_type','work_type','ever_married','smoking_status']\n# encoder = OrdinalEncoder()\n# result = encoder.fit_transform(df[categ_feat])\n# df_ordinal = df.copy()\n# df_ordinal[categ_feat] = result\n\n\n# dummies = pd.get_dummies(df[categ_feat],drop_first=True )\n# df_tmp = df.drop(categ_feat, axis = 1)\n# df_one_hot = pd.concat([df_tmp,dummies],axis=1)\n# one_hot_columns = df_one_hot.columns","689908a5":"from sklearn.impute import KNNImputer\nfrom sklearn.pipeline import FeatureUnion\n\ndf = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ncateg_feat = ['gender','Residence_type','work_type','ever_married','smoking_status','heart_disease','stroke','hypertension']\nnum_feat = list(set(df.columns)-set(categ_feat))\nnum_feat.remove('id')\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n\nclass RemoveOutliers(BaseEstimator,TransformerMixin):\n    def __init__(self,):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        tmp = X.drop(X[X['gender']=='Other'].index)\n        tmp = tmp.drop(tmp[tmp['avg_glucose_level']<57].index)\n        tmp = tmp.drop(tmp[tmp['bmi']>47].index)\n        return tmp\n\nclass ReturnDataFrame(BaseEstimator,TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X,y=None):\n        return pd.DataFrame(X,columns=self.attribute_names)\n\nclass OneHot(BaseEstimator,TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X,y=None):\n        dummies = pd.get_dummies(X,drop_first=True )\n        df_tmp = X.drop(self.attribute_names, axis = 1)\n        return pd.concat([df_tmp,dummies],axis=1) \n\n\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_feat)),\n    ('imputer',KNNImputer(missing_values=np.NaN)),\n    ('std_scaler',StandardScaler()),\n    ('get_df',ReturnDataFrame(num_feat))\n    ])\n\n\ncat_pipeline = Pipeline([\n    ('selector',DataFrameSelector(categ_feat)),\n    ('encoder',OneHot(categ_feat))\n])\n\n\n\nclass MergeFeatures(BaseEstimator,TransformerMixin):\n    def __init__(self, pip1,pip2):\n        self.pip1 = pip1\n        self.pip2 = pip2\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X,y=None):\n        tmp1 =  self.pip1.fit_transform(X)\n        tmp2 =  self.pip2.fit_transform(X)\n        col1 = list(tmp1.columns)\n        col2 = list(tmp2.columns)\n        cols = col1+col2\n        aux = np.concatenate((tmp1.to_numpy(),tmp2.to_numpy()),axis=1)\n        \n        return pd.DataFrame(aux,columns=cols)\n        ","d30493e1":"preprocessing = Pipeline([\n       ('outliers',RemoveOutliers()),\n        ('preprocess',MergeFeatures(num_pipeline,cat_pipeline))\n])","dc3b4b8e":"sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\nfig = plt.figure(figsize=(12,13))\ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.1, hspace=0.65)\naxes = []\nfor j in range(10):\n    axes.append(fig.add_subplot(gs[int(j\/2),j%2]))\n    \n#Feature 1: age\nsns.kdeplot(data=df, x=\"age\", hue=\"stroke\",\n   fill=True, common_norm=False, palette=\"rocket\",\n   alpha=.5, linewidth=0,ax=axes[0],cut=0,legend=False)\naxes[0].legend(loc='upper left',title='Stroke', labels = ['Yes','No'])\n\n\n#Feature 2: bmi\nsns.kdeplot(data=df, x=\"bmi\", hue=\"stroke\",\n   fill=True, common_norm=False, palette=\"rocket\",\n   alpha=.5, linewidth=0,ax=axes[1],legend=False,cut=0)\n\n\n\n\n#Feature 3: bmi\nsns.kdeplot(data=df, x=\"avg_glucose_level\", hue=\"stroke\",\n   fill=True, common_norm=False, palette=\"rocket\",\n   alpha=.5, linewidth=0,ax=axes[2],legend=False,cut=0)\n\n\n#Feature 4:gender\nsns.histplot(y='gender', hue=\"stroke\",\n            palette=\"rocket\",\n            data=df, ax=axes[3],stat='density',\n            common_norm=False,legend=False)\n\n#Feature 5:hypertension\ntmp_df = df.copy()\ntmp_df['hypertension'] = tmp_df['hypertension'].map(lambda x: {0:'No',1:'Yes'}[x])\n\nsns.histplot(y='hypertension', hue=\"stroke\",\n            palette=\"rocket\",\n            data=tmp_df, ax=axes[4],stat='density',\n            common_norm=False,legend=False)\n\n\n#Feature 6:heart_disease\ntmp_df = df.copy()\ntmp_df['heart_disease'] = tmp_df['heart_disease'].map(lambda x: {0:'No',1:'Yes'}[x])\n\nsns.histplot(y='heart_disease', hue=\"stroke\",\n            palette=\"rocket\",\n            data=tmp_df, ax=axes[5],stat='density',\n            common_norm=False,legend=False)\n\nfor j,feat in enumerate(['ever_married','work_type','Residence_type','smoking_status']):\n    sns.histplot(y=feat, hue=\"stroke\",\n            palette=\"rocket\",\n            data=df, ax=axes[6+j],stat='density',\n            common_norm=False,legend=False)\n\n\nfor j in range(10):\n    which_ax = ['bottom','right'] if j%2==0 else ['bottom','left']\n    for s in which_ax:\n        axes[j].spines[s].set_visible(False)\n    if(j%2==1):\n        axes[j].yaxis.tick_right()\n        axes[j].yaxis.set_label_position(\"right\")\n    axes[j].xaxis.tick_top()\n    axes[j].xaxis.set_label_position(\"top\")","cf642391":"categ_feat = ['gender','Residence_type','work_type','ever_married','smoking_status']\nencoder = OrdinalEncoder()\nresult = encoder.fit_transform(df[categ_feat])\ndf_ordinal = df.copy()\ndf_ordinal[categ_feat] = result\nf,ax = plt.subplots(figsize=(12, 10))\nmask = np.triu(np.ones_like(df_ordinal.corr(), dtype=bool))\nsns.heatmap(df_ordinal.phik_matrix(),mask=mask, annot=True, linewidths=.5, fmt= '.2f',ax=ax)\nplt.show()","05c78c10":"def compute_score(y_train,y_test,y_train_p,y_test_p):\n    test_ac = accuracy_score(y_test,y_test_p)\n    train_ac = accuracy_score(y_train,y_train_p)\n    test_f1 = f1_score(y_test,y_test_p)\n    train_f1 = f1_score(y_train,y_train_p)\n\n    cmte = confusion_matrix(y_test,y_test_p)\n    \n    cmtr = confusion_matrix(y_train,y_train_p)\n        \n    d = {'Dataset':['Test','Train'],'F1':[test_f1,train_f1]}\n    d['Accuracy'] = [test_ac,train_ac]\n    d['Recall'] = [recall_score(y_test,y_test_p),recall_score(y_train,y_train_p)]\n    d['Precision'] = [precision_score(y_test,y_test_p),precision_score(y_train,y_train_p)]\n    d['ROC AUC'] = [roc_auc_score(y_test,y_test_p),roc_auc_score(y_train,y_train_p)]\n    d['False Positives (T-I)'] = [cmte[0,1],cmtr[0,1]]\n    d['False Negatives (T-II)'] = [cmte[1,0],cmtr[1,0]]\n    d['True Positives'] = [cmte[1,1],cmtr[1,1]]\n    d['True Negatives'] = [cmte[0,0],cmtr[0,0]]\n    \n    \n    \n    display(pd.DataFrame(d).head(2))","ef463c06":"def plot_res(results):\n    epochs = len(results['validation_0']['error'])\n    x_axis = range(0, epochs)\n    fig = plt.figure(figsize=(6,4))\n    gs = fig.add_gridspec(1, 1)\n    gs.update(wspace=0.1, hspace=0.65)\n    ax = []\n    ax.append(fig.add_subplot(gs[0,0]))\n#     ax.append(fig.add_subplot(gs[0,1]))\n#     ax[0].plot(x_axis, results['validation_0']['f1'], label='Train',color='red')\n#     ax[0].plot(x_axis, results['validation_1']['f1'], label='Test',color='purple')\n    \n#     ax[0].legend()\n#     ax[0].set_ylabel(r'$F_1$ score')\n#     ax[0].set_title(r'XGBoost $F_1$')\n    \n    \n    ax[0].plot(x_axis, results['validation_0']['error'], label='Train',color='red')\n    ax[0].plot(x_axis, results['validation_1']['error'], label='Test',color='purple')\n    ax[0].set_ylabel('Error')\n    ax[0].set_title('XGBoost Error')\n    \n    ax[0].spines['right'].set_visible(False)\n    ax[0].spines['top'].set_visible(False)\n#     ax[1].spines['left'].set_visible(False)\n#     ax[1].spines['top'].set_visible(False)\n    \n#     ax[1].yaxis.tick_right()\n#     ax[1].yaxis.set_label_position(\"right\")\n    \n    plt.show()\n\n\ndef f1_metric(predt: np.ndarray, dtrain: xgb.DMatrix, threshold=0.5):\n    '''F_1 score.'''\n    y = dtrain.get_label()\n    y_p = (predt > threshold).astype(int)\n    return 'f1', f1_score(y,y_p)","4a21c40f":"df_cols = list(preprocessing.fit_transform(df).columns)\ndf_cols.remove('stroke')\n\n\ndf_clean = preprocessing.fit_transform(df.drop(['id'],axis=1))\nx_train, x_test, y_train, y_test = train_test_split(df_clean.drop(['stroke'],axis=1), df_clean['stroke'], \n                                                    test_size=0.25, random_state=42)\n\n\npars_xgb = {'objective':'binary:logistic',\n            'random_state':7,\n            'scale_pos_weight':1,\n            'use_label_encoder':False,\n            'eval_metric':'error',\n            'learning_rate':0.01,\n            'colsample_bytree': 0.05,\n            'subsample': 0.8,\n            'n_estimators':2000, \n            'reg_alpha': 0.3,\n            'n_jobs':1,\n            'max_depth':10, \n            'gamma':10\n           }\n\n\ndef plot_roc(fpr,tpr,label=None):\n    plt.figure(figsize=(5,4))\n    plt.plot(fpr,tpr,linewidth=2,label=label)\n    plt.plot([0,1],[0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\ndef test_resampling(resampling,x_train, x_test, y_train, y_test, cols = df_cols):\n    \n    pip_res = Pipeline([    \n        ('rsp',resampling), \n            ('xgba',xgb.XGBClassifier(**pars_xgb))])\n    \n    eval_set = [(x_train,y_train)]\n    pip_res = pip_res.fit(x_train,y_train,xgba__eval_metric=f1_metric, \n                         xgba__eval_set=eval_set, xgba__verbose=False)\n\n    scores = pip_res.predict_proba(x_train)[:,1]\n    \n    fpr,tpr,thresholds = roc_curve(y_train,scores)\n    plot_roc(fpr,tpr)\n    \n    scoring = ['accuracy', 'precision', 'recall', 'f1']\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    scores = cross_validate(pip_res, x_train,y_train, scoring=scoring, cv=cv, n_jobs=-1,  return_train_score=True)\n\n    print('Cross-Validation: Accuracy: {:.2f}, Precison: {:.2f}, Recall: {:.2f}, F1: {:.2f}'.format(np.mean(scores['test_accuracy']), np.mean(scores['test_precision']), np.mean(scores['test_recall']), np.mean(scores['test_f1'])))\n\n    \n    compute_score(y_train,y_test,pip_res.predict(x_train),pip_res.predict(x_test))\n\n#     results = pip_res[1].evals_result()\n    \n#     plot_res(results)","87b52d9a":"resampling = SMOTE(random_state=18)\ntest_resampling(resampling,x_train, x_test, y_train, y_test)","70431de5":"resampling = SMOTEENN(random_state=7)\ntest_resampling(resampling,x_train, x_test, y_train, y_test)","c0fe521a":"resampling = SMOTETomek(random_state=7)\ntest_resampling(resampling,x_train, x_test, y_train, y_test)","b099abee":"resampling = RandomOverSampler(random_state=7)\ntest_resampling(resampling,x_train, x_test, y_train, y_test)","e72f7716":"resampling = RandomUnderSampler(random_state=7)\ntest_resampling(resampling,x_train, x_test, y_train, y_test)","6d04ef71":"def update_pars(new_p,pars):\n    for p in new_p.keys():\n        p_new = p.replace('xgba__','')\n        pars[p_new] = new_p[p]","aa81c806":"def test_in_range(x_trainf, x_testf, y_trainf, y_testf, pars,param_test, resampling = SMOTEENN(random_state=7)):\n    \n    pip_res = Pipeline([('rsp',resampling), \n                              ('xgba',xgb.XGBClassifier(**pars_xgb))])\n    \n    new_params = [{'xgba__' + key: el[key] for key in el} for el in param_test]\n    gsearch = GridSearchCV(estimator = pip_res, \n    param_grid = new_params, scoring='f1',n_jobs=-1, cv=5)\n\n    gsearch.fit(x_trainf,y_trainf,xgba__verbose=False)\n    \n    \n    scores = gsearch.predict_proba(x_trainf)[:,1]\n    \n    fpr,tpr,thresholds = roc_curve(y_trainf,scores)\n    plot_roc(fpr,tpr)\n    \n    print('Best score: ',gsearch.best_params_)\n\n    update_pars(gsearch.best_params_,pars_xgb)\n\n    clf_rf = pip_res      \n    clr_rf = pip_res.fit(x_trainf,y_trainf)\n\n    compute_score(y_trainf,y_testf,clr_rf.predict(x_trainf),clr_rf.predict(x_testf))","c4f8f644":"# resampling = RandomUnderSampler(random_state=7)\n# x_tmp, y_tmp = resampling.fit_resample(x_train, y_train)\n# x_train_res = pd.DataFrame(x_tmp,columns=df_cols)\n# y_train_res = pd.Series(y_tmp)\n\nparam_test1 = [\n    {'scale_pos_weight':range(1,10,3)},\n    {'max_depth':range(3,15,2),'min_child_weight':range(1,6,2)},\n    {'gamma':[i\/10.0 for i in range(0,5)]},\n    { 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]}\n]\n\npars_xgb = {'objective':'binary:logistic',\n            'random_state':7,\n            'scale_pos_weight':1,\n            'use_label_encoder':False,\n            'eval_metric':'error',\n            'learning_rate':0.01,\n            'colsample_bytree': 0.05,\n            'subsample': 0.8,\n            'n_estimators':2000, \n            'reg_alpha': 0.3,\n            'n_jobs':1,\n            'max_depth':10, \n            'gamma':10\n           }\n\ntest_in_range(x_train, x_test, y_train, y_test, pars_xgb, param_test1)","4a5cb3ad":"# select_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n# order_ind = select_feature.scores_.argsort()\n# print('Score list:', select_feature.scores_[order_ind[::-1]])\n# print('Feature list:', x_train.columns[order_ind[::-1]])","9238d893":"resampling = SMOTEENN(random_state=7)\n# x_tmp, y_tmp = resampling.fit_resample(x_train, y_train)\n# x_train_tmp = pd.DataFrame(x_tmp,columns=df_cols)\n# y_train_tmp = pd.Series(y_tmp)\n\nclf_rf = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf, n_features_to_select=5, step=1)\npip_res = Pipeline([('rsp',resampling), \n                              ('rf',rfe)])\nrfe = rfe.fit(x_train, y_train)\nprint('Selected features:',x_train.columns[rfe.support_])","50ffdebe":"pars_xgb = {'objective':'binary:logistic',\n            'random_state':7,\n            'scale_pos_weight':1,\n            'use_label_encoder':False,\n            'eval_metric':'error',\n            'learning_rate':0.01,\n            'colsample_bytree': 0.05,\n            'subsample': 0.8,\n            'n_estimators':2000, \n            'reg_alpha': 0.3,\n            'n_jobs':1,\n            'max_depth':10, \n            'gamma':10\n           }\n\nrf_rfe_columns = list(x_train.columns[rfe.support_])\ntest_resampling(resampling,x_train[rf_rfe_columns], \n                x_test[rf_rfe_columns], y_train, y_test, rf_rfe_columns)","6006c1a1":"from sklearn.feature_selection import RFECV\n\nclf_rf = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf, step=1, cv=10,scoring='f1')\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","811d9080":"rf_rfecv_columns = list(x_train.columns[rfecv.support_])\ntest_resampling(resampling,x_train[rf_rfecv_columns], \n                x_test[rf_rfecv_columns], y_train, y_test, rf_rfecv_columns)","c6670d59":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n\ntmp = Pipeline([\n       ('outliers',RemoveOutliers()),\n        ('preprocess',MergeFeatures(num_pipeline,DataFrameSelector(categ_feat)))\n]).fit_transform(df.drop(['id'],axis=1))\n\ntmp_dic = {}\nfor c in categ_feat:\n    tmp_dic[c]='str'\nfor c in num_feat:\n    tmp_dic[c]=float\ntmp = tmp.astype(tmp_dic)\n\n\nfamd_stroke = prince.FAMD(\n     n_components=16,\n     n_iter=10,\n     copy=True,\n     check_input=True,\n     engine='auto',\n     random_state=42)\nprincipalComponents_stroke = famd_stroke.fit_transform(tmp)","7019c978":"sns.set()\nplt.figure(1, figsize=(10, 9))\nsns.lineplot(data=np.cumsum(famd_stroke.explained_inertia_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\nplt.bar(range(0,16), famd_stroke.explained_inertia_,\n        alpha=0.5,\n        align='center')\nplt.step(range(0,16), np.cumsum(famd_stroke.explained_inertia_),\n         where='mid',\n         color='red')","3f9e06ad":"n_pca = 5\npca_cols = []\nfor j in range(n_pca):\n    pca_cols.append('pca%d'%(j))\nprincipalComp_stroke_df = pd.DataFrame(data = np.array(principalComponents_stroke)[:,:n_pca],\n                                      columns=pca_cols)","6e724a14":"x_train, x_test, y_train, y_test = train_test_split(principalComp_stroke_df, df_clean['stroke'], \n                                                    test_size=0.25, random_state=42)","da1f4013":"test_resampling(resampling,x_train, \n                x_test, y_train, y_test, pca_cols)","9ce72a41":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom keras.regularizers import l2\nprint(tf.__version__)","3ee9bf9d":"df_clean.drop('stroke',axis=1).head()","ccd6d47f":"resampling = SMOTE(random_state=7)\n\nx_train, x_test, y_train, y_test = train_test_split(df_clean.drop('stroke',axis=1),df_clean['stroke'], \n                                                    test_size=0.25, random_state=42)\n\nx_tmp, y_tmp = resampling.fit_resample(x_train, y_train)\nx_train_tmp = pd.DataFrame(x_tmp,columns=df_cols)\ny_train_tmp = pd.Series(y_tmp)\n\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(len(df_clean.columns)-1,)),\n    tf.keras.layers.Dense(20, activation=\"relu\",kernel_regularizer=l2(0.01)),\n#     tf.keras.layers.Dense(50, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-3 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.Adam(lr=0.5*1e-3)\n\nMETRICS = [\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'), \n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=optimizer,\n              metrics=[METRICS])\n\nprint(len(df_clean.columns))\n\ncounts = np.bincount(y_train)\nweight_for_0 = (1.0 \/ counts[0])\nweight_for_1 = (1.0 \/ counts[1])\nclass_weight = {0: weight_for_0, 1: weight_for_1}\nclass_weight = {0: 1, 1: 1}\nhistory = model.fit(x_train_tmp,y_train_tmp,epochs=150,verbose=0,#, callbacks=[lr_schedule],\n                       validation_data=(x_test, y_test),\n                        class_weight=class_weight)","561b7c1c":"def plot_loss(history, label, n):\n    plt.semilogy(history.epoch, history.history['loss'], label='Train ' + label)\n    plt.semilogy(history.epoch, history.history['val_loss'], label='Val ' + label,\n               linestyle=\"--\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n","5e82b418":"plot_loss(history, \"set\", 0)","9bead048":"def plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')","63dfa0b9":"treshold = 0.5\nout = model.predict(x_test)\ntest_pred = np.array([0 if p < treshold else 1 for p in out])\nout = model.predict(x_train_tmp)\ntrain_pred = np.array([0 if p < treshold else 1 for p in out])\nplot_cm(y_test, test_pred)\ncompute_score(y_train_tmp,y_test,train_pred,test_pred)","38517e28":"## 3. Testing resampling techniques with XGBoost binary classifier\nThe dataset is strongly imbalanced, just 5% of the patients were subjected to a stroke. For this reason, it is necessary to employ a **resampling** technique. All the algorithms employed here to tackle the issue are present in the [imblearn](https:\/\/imbalanced-learn.org\/stable\/) library. \n* In particular, I will employ some oversampling techniques (i.e. *SMOTE, Random oversampling*), one undersampling technique (i.e. Random undersampling) and some mixed algorithms (i.e. *SMOTE + ENN* and *SMOTE + Tomek links*).\n* The resampling algorithms will be tested with the XGBoost binary classifier.\n* All the resampling techniques will be employed **on the training set only**. IMPORTANT: this is automatically accomplished by using an imblearn pipeline object.\n* To test the different model we will use the stratified k-fold cross-validation, that preserves the imbalanced class distribution in each fold. ","4f16c5f9":"#### SMOTE","230f4f87":"#### SMOTE + Tomek links","d984711b":"## 2. Data visualization\nVisit the [dashboard](https:\/\/stroke-risk-factors.herokuapp.com) allowing you to interactively explore the stroke risk factors.\n### 2.1 Distributions\nIn this section, I will compare the distribution of the variables in the two subsets 'Stroke' and \"No Stroke'. If the distribution of a given feature matches in the two sets within statistical uncertainties, then we can conclude that the selected feature is not correlated with the target. The results obtained in this section will be compared in Sec. 4 with the findings of the dimensionality reduction.","e7e95d06":"### 4.C RFE with Cross Validation (RFECV) with random forest\nBelow, I am going to the RFE algorithm with cross-validation, employing the random forest classifier. This will allow to crosscheck the best features selected in the previous subsection and to get the optimal number of features.","efeab751":"The score list above confirms the findings of the [data visualization section](#2.-Data-visualization). In particular, age is the most important feature to consider here, together with the average glucose level. Among the other important variables selected by the univariate algorithm it is interesting to mention the  'ever_married_Yes', that we know is strongly correlated with age, as well as 'work_type_children'. For this reason, before proceeding with the dimensionality reduction it is necessary to employ a multivariate algorithm.","75e54e3b":"Findings:\n1. From the plot below it is clear that age is one of the most important features in the dataset, being the distribution among patients who were subjected to stroke shifted towards higher values. In particular, the distribution of the subset 'Stroke' is strongly peaked around 80 years. \n2. **BMI** may have an impact on the probability to be subjected to stroke since the distribution of the subset 'Stroke' shows a smaller variance compared to the 'No Stroke' one.\n3. The distributions of the **average glucose level** show similar qualitative behavior in the two datasets. However, among the patients subjected to stroke is more common to have higher glucose levels (i.e. ~200).\n4. **Gender** seems not to be a predominant feature here, since no significant differences between the two distributions have been observed.\n5. **Hypertension** is a relevant feature, which is statistically more present among patients subjected to stroke.\n6. **Heart disease**, like hypertension, can be relevant.\n7. The feature **ever_married** shows a correlation with the target 'Stroke'. However, this can be explained from the correlation of this feature with the age, which we know is an important variable in this analysis. \n8. The **work type** shows a correlation with the target, which is however due to the presence of children in the sample. Thus, this feature is expected to be correlated with age.\n9. The distributions of the **residence type** show a mild asymmetry between the two datasets. However, this can be due to statistical fluctuations.\n10. Finally, the **smoking status** is correlated with the target. Indeed, there are fewer patients with 'unknown' smoking status in the dataset 'Stroke' in favor of the status 'formerly smoked'. \n","fbed3998":"## Neural network modeldef plot_cm(labels, predictions, p=0.5):\n  \n","a82b6bec":"#### SMOTE + ENN","e583ba3e":"The optimal number of features is 3, and they correspond with the ones selected in the previous subsection.","ef771523":"The $F_1$ score on the test set has slightly improved compared to the classification performed including all the available features in [Sec. 3](#3.-Testing-resampling-techniques-with-XGBoost-binary-classifier). However, the number of features has been drastically reduced to 5. In the next section, I am going to test another multivariate feature selection algorithm and compare the results with what obtained here.","3b2b2ece":"## Introduction\n\n***\n\nIn what follows, I will analyze the [Stroke Prediction Dataset](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset) containing relevant information about ~5k patients. The aim of this study is to check whether, given a set of features describing the health status of the patient, it is possible to predict a stroke event.   \n\nThe complete list of attributes includes 12 features which are listed below:\n\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n\n***\n\n## Index\n\nThe notebook has been structured as follows:\n\n1. [Read the dataset and get familiar with the data](#1.-Read-the-dataset-and-get-familiar-with-the-data)\n    1. [Feature engineering](#1.A-Feature-engineering)\n2. [Data visualization](#2.-Data-visualization)\n    1. [Distributions](#2.1-Distributions)\n    2. [Study of the correlation matrix](#2.2-Study-of-the-correlation-matrix)\n3. [Testing resampling techniques with XGBoost binary classifier](#3.-Testing-resampling-techniques-with-XGBoost-binary-classifier)\n    1. [Hyperparameters tuning with GridSearchCV](#3.1-Hyperparameters-tuning-with-GridSearchCV)\n4. [Dimensionality reduction](#4.-Dimensionality-reduction)\n    1. [Univariate analysis](#4.A-Univariate-analysis)\n    2. <a href=\"#4.B-Recursive-feature-elimination-(RFE)-with-random-forestt\">Recursive feature elimination (RFE) with random forest<\/a>\n    3. <a href=\"#4.C-RFE-with-Cross-Validation-(RFECV)-with-random-forest\">RFE with Cross Validation (RFECV) with xgboost<\/a>\n    4. <a href=\"#4.D-Factor-Analysis-of-Mixed-Data-(FAMD)-for-feature-extraction\">Factor Analysis of Mixed Data (FAMD) for feature extraction<\/a>","1324d742":"Random forest can be employed in combination with the [feature ranking with recursive feature elimination](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html) algorithm. Due to the strong correlation between features, the expected outcome significantly differs from the output of the univariate algorithm employed in the previous section.","88f2d85d":"## 4.D Factor Analysis of Mixed Data (FAMD) for feature extraction\n\nPrincipal component analysis (PCA) is by far the most common dimensionality reduction techniques, but can be employed with datasets containing continuous variable only. In this case, the dataset contains instead a combination of continuous and categorical variables. For this reason, [factor analysis of mixed data (FAMD)](https:\/\/en.wikipedia.org\/wiki\/Factor_analysis_of_mixed_data) is the technique most suitable to perform feature extraction on the stroke prediction dataset. It can be viewed as the combination of PCA for continuous variables and multiple correspondence analysis (MCA) for categorical variables.  \nFAMD is not available in scikit-learn, while in R it can be found in the [PCAmix](https:\/\/cran.r-project.org\/web\/packages\/PCAmixdata\/index.html) package. In Python, it was implemented in a privately maintained package, [Prince](https:\/\/github.com\/MaxHalford\/prince\/tree\/988f7fe01b6e4c9476517d1939f5fe0e13deb158).\n\n\n\n![r-blogger](https:\/\/i0.wp.com\/www.sthda.com\/sthda\/RDoc\/images\/multivariate-analysis-factoextra.png?w=578)  \n\nFigure taken from [r-blogger](https:\/\/www.r-bloggers.com\/2017\/02\/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization\/) website.","06a6243f":"Encoding the categorical variables with the [sklearn ordinal encoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) for computing the correlation between features. However, given that there is no intrinsic order among the categories in each feature, [one-hot encoding](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) is more suitable for this task.\n\nMany other algorithms can be tested. In particular, there is [a very interesting package](http:\/\/contrib.scikit-learn.org\/category_encoders\/index.html#) including many encoding algorithms.\n","6f7846b7":"#### Random undersampler","35d348f3":"### Build a pipeline performing the preprocessing steps","de5b21b5":"### 4.B Recursive feature elimination (RFE) with random forest","1dc2a53f":"Analogously, I will remove the rows corresponding to BMI>47, which are 117, with just 3 stroke occurencies.","f8e246a7":"In the plot below we see the distribution of the \"inertia\" (i.e. variance in the nomenclature of the package Prince), among the components selected by the FAMD algorithm. The first component carries 20% of the variance, the remaining ~80% of the variance is distributed among 13 components. For instance, 80% of the variance is contained in the first 8 components.","de0537ec":"#### 4.A Univariate analysis\nUnivariate tests measure the correlation between the selected feature and the target. Each feature is analyzed independently, not taking into account the correlation between different features.    ","075d1051":"### 3.1 Hyperparameters tuning with GridSearchCV","88c46af2":"The same apply to average glucose level < 57, with 81 rows and just 1 occurrence of stroke.","08c9b30c":"#### Random oversampler","925c200d":"From the results above it is clear that all the algorithms (except for the undersampling one) show qualitatively the same performances. \n* Indeed, in all cases, the $F_1$ score, which is the appropriate metric for imbalanced binary datasets, is greater than 0.9 on the training set and ~0.25 on the test set. By tuning the hyperparameters of these classifiers it is possible to further increase the $F_1$ score on the training set, even to 0.99, but this would result in overfitting since no gain would be obtained on the $F_1$ score on the test set.\n* In all cases, the number of false negatives is quite large compared to the number of true positives. This is bad, due to the relatively small number of patients subjected to stroke in the dataset.\n* The undersampling algorithm gives a smaller $F_1$ score on the training set. ","366ef919":"There are 201 NaNs values in the bmi field, for this reason I will employ the [nearest neighbors](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) imputation provided by the scikit-learn library.","1c57d233":"## 4. Dimensionality reduction","badfbcfd":"## Visit the [dashboard](https:\/\/stroke-risk-factors.herokuapp.com) allowing you to interactively explore the stroke risk factors ","3a3755e5":"#### Remove outliers\nThe gender section can take three different values: 'Male', 'Female' and 'Other'. \nHowever, in the next line I check that there is just one patient in the category 'Other'. For this reson, since there is not a sufficiently high number of representatives in this category to perform a statistical analysis, I will drop the corresponding line. ","6edfa168":"As expected, the best features selected by the algorithm do not include the varialbe 'ever_married_Yes', which is strongly correlated with age. Among the five best features the variables 'smoking_status_never smoked' and 'smoking_status_smokes' have been included, which however have a weak correlation with the target. These variables may not give significative additional information to the classifier. Let's test now the XGBoost classifier with SMOTEENN employing the selected features...","ec0343b1":"## 1. Read the dataset and get familiar with the data\n### 1.A Feature engineering\nThe dataset includes the data for 5110 patients, each one being associated with a unique ID which is not a relevant feature for the analysis presented here. There are 8 categorical features:   `gender`,`hypertension`,`heart_disease`,`ever_married`,`work_type`,`residence_type`,`smoking status`,`stroke`,  \nand 3 non-categorical (discrete or continuous) features  \n`age`,`avg_glucose_level` and `bmi`.\n\nThe `stroke` feature will be the target variable we aim to predict. ","3e696195":"The $F_1$ score on the test set remains constant compared to the results of [Sec. 4.C](#4.C-Recursive-feature-elimination-(RFE)-with-random-forest).","f5401a92":"### 2.2 Study of the correlation matrix\n\nThe dataset contains continuous and categorical variables. Moreover, the categorical variables can have from 2 (binary) possible values (Residence type) to 5 categories (Work type). Given these premises, we would need to define an inhomogeneous correlation matrix, by applying different correlation coefficients between different couples of variables. For instance, we could use the Pearson correlation coefficient with continuous-continuous variables, the point-biserial between continuos-binary variables and the [correlation ratio](https:\/\/en.wikipedia.org\/wiki\/Correlation_ratio) between categorical-categorical variables. However effective, this may have been misleading, since the comparison between the different correlation coefficients would be meaningless. Instead, I adopted a new method proposed in [this paper](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167947320301341) allowing to define a unique correlation coefficient for every couple of variables in the dataset. The method is implemented in the [Phi_K Correlation Analyzer Library](https:\/\/phik.readthedocs.io\/en\/latest\/) in Python. The library can be easily installed through pip.\n\nBelow I show the correlation matrix between the features. \n* In the first place, the 'stroke' feature is correlated (in reverse order of importance) with age, heart disease,hypertension, average glucose level, marital status, smoking status, work type, BMI.\n* However, as mentioned in [Sec. 2.1](#2.1-Distributions), the correlation with gender is negligible, as well as the one with the residence type. \n* Among the variables correlated with the target, many of them are strongly correlated with age. In particular the marital status, the work type, the bmi and smoking status have correlation coefficient greater than 0.3 with the age. For this reason it will be important to reduce the number of features."}}