{"cell_type":{"a776877a":"code","20a9b30c":"code","4efd7c45":"code","265368f5":"code","00ec5e7b":"code","7cd17bf3":"code","653b40f1":"code","38164f4e":"code","a6b3eebc":"code","49742d15":"code","296bbae3":"code","5c20b0af":"code","85fc5ac8":"code","dab4b263":"code","5691a4f0":"code","6cbc5da4":"code","3af59d36":"code","4e72cda1":"code","69c15d3c":"code","19a0c393":"code","385038dd":"code","beb79a6e":"code","a06f1ea2":"code","c0285e01":"code","3b31b70e":"code","8cca0aad":"code","86e62436":"code","5b0c71ec":"code","5de8386e":"code","ec8f310f":"code","ba55f63b":"code","03e5d64c":"code","da9d1d34":"code","7deb0eae":"code","aa6ce206":"code","681e2a76":"markdown","78a928c5":"markdown","4bef0f4d":"markdown","7620c227":"markdown","c20507b2":"markdown","988d1c53":"markdown","3b54ecdf":"markdown","82716d5a":"markdown","39b1b116":"markdown","c26b0bd1":"markdown","5b51e267":"markdown","432167e5":"markdown","cf2314cb":"markdown","369f42ad":"markdown","789aa7c7":"markdown","ef590bb1":"markdown","be55d481":"markdown"},"source":{"a776877a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm\n\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Modeling\nimport xgboost as xgb\n\nfrom colorama import Fore, Back, Style\ng_ = Fore.GREEN\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","20a9b30c":"df = pd.read_csv('..\/input\/pesquisa-nacional-por-amostra-de-domiclios-pnad\/pnad_2015_clean.csv')\ndf","4efd7c45":"class CFG:\n    debug = False\n#     Transform_target = True\n    Transform_target = False\n    scaler = MinMaxScaler()\n#     scaler = RobustScaler()\n    threshold = 15000\n    eval_split = 0.10\n    eval_split_seed = 42\n    n_folds = 5\n    seeds = [0, 1]","265368f5":"if CFG.debug:\n    df = df.head(10_000)","00ec5e7b":"df = df[df['Income'] <= CFG.threshold]\ndf = df.reset_index(drop=True)\ndf.shape","7cd17bf3":"df.dtypes","653b40f1":"numeric_feats = df.dtypes[df.dtypes != \"object\"].index\nprint(\"There are {} numeric columns.\".format(numeric_feats.shape[0]))\n\n# Check the skew of all numerical features\nskewed_features = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical columns: \\n\")\nskewness = pd.DataFrame({'Skew' : skewed_features})\nskewness","38164f4e":"high_skew = skewed_features[skewed_features > 0.75]\nskew_index = high_skew.index\n\nprint(\"{} numeric column(s) with Skew > 0.75.\".format(high_skew.shape[0]))\nhigh_skewness = pd.DataFrame({'High Skewed' : high_skew})\nhigh_skewness.head(10)","a6b3eebc":"f, ax = plt.subplots(figsize=(10, 6))\nsns.distplot(df['Income'], color=\"b\", fit=norm);\nax.xaxis.grid(True)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Income\")\nax.set(title=\"Target original distribution\");","49742d15":"print(\"Skewness: %f\" % df['Income'].skew()) # curve lopsidedness\nprint(\"Kurtosis: %f\" % df['Income'].kurt()) # curve tailedness\n\nprint(\"\\nIncome min: %.2f\" % df['Income'].min())\nprint(\"Income max: %.2f\" % df['Income'].max())","296bbae3":"# log(1+x) transform\ndf_original = df.copy() # save the dataframe before any transformation\n\ndf[\"Income\"] = np.log1p(df[\"Income\"])\n\ncomparison = pd.DataFrame({'Target (original)' : df_original['Income'], 'Target (transformed)' : df['Income']})\ncomparison","5c20b0af":"comparison.describe()","85fc5ac8":"comparison.hist(); # comparing distributions","dab4b263":"fig, ax = plt.subplots(1, 2, figsize = (10, 8))\n\nax1 = plt.subplot(2, 1, 1)\nstats.probplot(df_original['Income'], plot=plt);\nax1.set_title('Income (original)')\nplt.tight_layout();\n\nax2 = plt.subplot(2, 1, 2)\nstats.probplot(df['Income'], plot=plt);\nax2.set_title('Income (transformed)');\nplt.tight_layout();","5691a4f0":"# if CFG class is set to not do target transformation, just undo it\n\nif CFG.Transform_target == False:\n    df['Income'] = np.expm1(df['Income'])","6cbc5da4":"df","3af59d36":"# convert Sex feature into integers\n# because later I want the model to calculate the impact of Sex column, not 'Male' and 'Female' categories\nSex_dict = {\n    'Male' : 0,\n    'Female' : 1\n}\n\ndf[\"Sex\"] = df[\"Sex\"].map(Sex_dict)","4e72cda1":"# One-hot-encoding\ndf = pd.get_dummies(df, drop_first = False)\ndf.shape","69c15d3c":"# dataframe ready to feed the models\ndf","19a0c393":"# take out a sample to use to evalute predictions later\neval_set = df.sample(frac = CFG.eval_split, random_state = CFG.eval_split_seed)\neval_set.shape","385038dd":"# indexes that will be used to evaluate predictions\nlist_of_indexes_in_eval_set = list(df.sample(frac = CFG.eval_split, random_state = CFG.eval_split_seed).index)\n\n# train_set: remove indexes above\ndf = df.drop(list_of_indexes_in_eval_set, axis=0).reset_index(drop=True)\ndf.shape","beb79a6e":"features = df.drop('Income', axis = 1)\nfeatures.shape","a06f1ea2":"target = df['Income']\ntarget.shape","c0285e01":"eval_set_features = eval_set.drop('Income', axis = 1)\neval_set_features = eval_set_features.reset_index(drop=True)\neval_set_features.shape","3b31b70e":"eval_set_target = eval_set['Income']\neval_set_target = eval_set_target.reset_index(drop=True)\neval_set_target.shape","8cca0aad":"# scale only features\nscaler = CFG.scaler.fit(features)\nfeatures_scaled = scaler.transform(features)\n\nfeatures_scaled.shape","86e62436":"# scale only features (eval_set)\nscaler_eval = CFG.scaler.fit(eval_set_features)\neval_set_features_scaled = scaler_eval.transform(eval_set_features)\n\neval_set_features_scaled.shape","5b0c71ec":"def rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","5de8386e":"def get_preds_xgb(X = features_scaled, y = np.array(target),\n                  nfolds = CFG.n_folds,\n                  n_estimators = 100):\n    \n    seeds_avg = list() # to store seed runs and calculate average\n    xgb_models = [] # to store models\n\n    for seed in CFG.seeds:\n        scores = list() # to store scores\n        \n        kfold = KFold(n_splits = CFG.n_folds, shuffle=True, random_state = seed)\n        \n        for k, (train_idx,valid_idx) in enumerate(kfold.split(X, y)):\n            \n            # instantiate model\n            model = xgb.XGBRegressor(objective ='reg:squarederror',\n                                     n_estimators = n_estimators,\n                                     random_state = seed) # use tree_method='gpu_hist' to enable gpu\n            \n            # KFold split\n            X_train, y_train = X[train_idx], y[train_idx] # train set for this fold\n            X_valid, y_valid = X[valid_idx], y[valid_idx] # validation set for this fold\n            \n            model.fit(X_train, y_train) # train (fit) model\n            xgb_models.append(model) # store model\n            prediction = model.predict(X_valid) # make predictions for the current fold\n            \n            # Undo target transformation on the predictions made (if it was done)\n            if CFG.Transform_target == True:\n                prediction = np.expm1(prediction)\n            \n            score = round(rmse_score(prediction, y_valid), 4) # calculate score (RMSE)\n            scores.append(score) # store score\n            \n            print(f'Seed {seed} | Fold {k} | RMSE score: {score}') # print fold score (RMSE)\n            \n        print(f\"{m_}\\nMean RMSE for seed {seed} : {round(np.mean(scores), 4)}{sr_}\\n\") # print seed score (RMSE)\n        \n        seeds_avg.append(round(np.mean(scores), 4)) # calculate run score (RMSE)\n        \n    print(f\"{g_}Average score: {round(np.mean(seeds_avg), 4)}{sr_}\") # print run score (RMSE)\n    \n    return round(np.mean(seeds_avg), 4), xgb_models","ec8f310f":"%%time\nscore_xgb, xgb_models = get_preds_xgb()","ba55f63b":"# choose model (fold) to evaluate\nmodel_fold = 1 # first one\n\nfeatures_importances = xgb_models[model_fold].feature_importances_\nargsort = np.argsort(features_importances)\nfeatures_importances_sorted = features_importances[argsort]\n\nfeature_names = features.columns\nfeatures_sorted = feature_names[argsort]\n\n# plot feature importances\nplt.figure(figsize = (8, 10))\nplt.barh(features_sorted, features_importances_sorted)\nplt.title(\"Feature Importances\");","03e5d64c":"# predict on eval_set using all models trained\n# clip to avoid predicting negative incomes\ny_pred = np.mean([np.clip(xgb_models[i].predict(eval_set_features_scaled), a_min = 0, a_max=None)\\\n         for i in range(0, len(xgb_models))], axis=0)\n \n# Undo target transformation on the predictions made (if it was done)\nif CFG.Transform_target == True:\n    y_pred = np.expm1(y_pred)\n\ny_test = eval_set_target\n\n# Undo target transformation on the eval set (if it was done)\nif CFG.Transform_target == True:\n    y_test = np.expm1(eval_set_target)\n\n# compute error (RMSE for evaluation set)\nrmse = rmse_score(y_test, y_pred)\nprint(\"RMSE on test data: %.2f\" % rmse)","da9d1d34":"%%time\n\nprint_every = 50\n\nfig = plt.figure(figsize=(20,5))\n\nplt.bar(list(range(len(y_test[::print_every]))), y_test.values[::print_every],\n        alpha = 1, color = 'red', width = 1, label = 'true values')\n\nplt.bar(list(range(len(y_pred[::print_every]))), y_pred[::print_every],\n        alpha = 0.5, color = 'blue', width = 1, label = 'predicted values')\n\nplt.legend();","7deb0eae":"my_pred = np.array([[\n\n# Sex (0: Male; 1: Female)\n1,\n# Age\n45,\n# Years of study\n12,\n# Height\n1.60,\n# State_Acre\n0,\n# State_Alagoas\n0,\n# State_Amap\u00e1\n0,\n# State_Amazonas\n0,\n# State_Bahia\n0,\n# State_Cear\u00e1\n0,\n# State_Distrito Federal\n0,\n# State_Esp\u00edrito Santo\n0,\n# State_Goi\u00e1s\n0,\n# State_Maranh\u00e3o\n0,\n# State_Mato Grosso\n0,\n# State_Mato Grosso do Sul\n0,\n# State_Minas Gerais\n0,\n# State_Paran\u00e1\n0,\n# State_Para\u00edba\n0,\n# State_Par\u00e1\n0,\n# State_Pernambuco\n0,\n# State_Piau\u00ed\n0,\n# State_Rio Grande do Norte\n0,\n# State_Rio Grande do Sul\n0,\n# State_Rio de Janeiro\n1,\n# State_Rond\u00f4nia\n0,\n# State_Roraima\n0,\n# State_Santa Catarina\n0,\n# State_Sergipe\n0,\n# State_S\u00e3o Paulo\n0,\n# State_Tocantins\n0,\n# Color_White \n0,\n# Color_Indigenous\n0,\n# Color_Brown\n1,\n# Color_Black\n0,\n# Color_Yellow\n0\n]])","aa6ce206":"res = np.mean([np.clip(xgb_models[i].predict(my_pred), a_min = 0, a_max=None)\\\n               for i in range(0, len(xgb_models))], axis=0)\n\nprint(\"Income predicted for information in my_pred array:\", round(res[0], 2), \"reais.\")","681e2a76":"# One-Hot-Encoding","78a928c5":"# Evaluate\n\nPredict on eval set: Compute predicted values (y_pred) and RMSE between predictions and real values (y_test).\n\nThe models have not seen the data in the evaluation set (eval_set_features_scaled and eval_set_target).\n\nHere we use all xgb models to generate the predictions. The ensemble of xgb models has (CFG.n_folds * len(CFG.seeds)) models.","4bef0f4d":"# Load data","7620c227":"# Split\n\nNow we take out some fraction of the dataset that will later be used to evaluate the models and predictions. This avoids data leakage and overfitting, and ensures the models will be evaluated on data they have not seen during training.","c20507b2":"# Config\n\nClass to help simulating results.","988d1c53":"# Simulate\n\nMake any prediction you want!\n\nDefine your features array: Set the values below for each column","3b54ecdf":"# Outliers\n\nAs seen in the [data analysis notebook](https:\/\/www.kaggle.com\/hinepo\/pnad-data-analysis), this dataset has very few outliers, but the ones in it are very far from the normal cases (check boxplots).\n\nFor modeling, a good decision would be to remove them, as they will certainly degrade the performance of the models.\n\nI will just remove some based in an arbitrary threshold. But this decision could also be based on a more detailed statiscal analysis.","82716d5a":"# Target transformation\n\nIncome is the variable that we want to predict (target variable), and it is highly skewed. So I tried to correct its distribution before modeling, but I could not find a statiscal transformation that improved the results, probably because the distribution is too far from the normal distribution.\n\nIt is obviously unrealistic to expect the variable Income to have some distribution close to one normal\/Gaussian one.\n\nIn case we choose to perform the target transformation (CFG class settings), after calculating the predictions we will need to transform back the target to get some reasonable results (np.expm1() function).","39b1b116":"# XGB\n\n[XGB docs](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html)","c26b0bd1":"# Metric\n\nHere I choose Root Mean Squared Error (RMSE) as an evaluation metric because in this case I want some measure that is sensitive to outliers. So I will evaluate the models using RMSE since it calculates the square of the deviation before taking the average and the square root of the error.\n\nAlso, I want to be able to compare the predicted values against the real values (ground truth). Like MAE (Mean Absolute Error), RMSE also measures the error on the same dimension of the target, which is not the case if we use MSE (Mean Squared Error).","5b51e267":"# Scaling\n\nSince it is a regression problem and we might use algorithms that depend on distance (non tree-based models) we need to scale the features. We should not scale the target though.","432167e5":"# Feature importances","cf2314cb":"# Skewed features","369f42ad":"# Features and Target\n\nHere we separate the features (variables that we will use to predict the target) from the target (variable that we want to predict).","789aa7c7":"# Imports","ef590bb1":"# Predicted x Real\n\nPlot predictions x real values.","be55d481":"# About this notebook\n\nThis notebook uses the output of [this kernel](https:\/\/www.kaggle.com\/hinepo\/pnad-data-analysis) to create and train some AI models to predict the income for brazilian people.\n\nPart 1: [Data analysis](https:\/\/www.kaggle.com\/hinepo\/pnad-data-analysis)\n\nPart 2: Modeling (this notebook)\n\nPart 3: [Lazy Predict](https:\/\/www.kaggle.com\/hinepo\/pnad-lazy-predict?scriptVersionId=74288711)"}}