{"cell_type":{"43a1d379":"code","e02bcbe3":"code","1d3a310d":"code","c1d5d9e9":"code","a0721386":"code","64424685":"code","004d2208":"code","ced91342":"code","7242fdaa":"code","fecbc292":"code","7adcf2a0":"code","ef48c36f":"code","a7a9ed8d":"code","92270adf":"code","eb92a881":"code","628fa23d":"code","967cbd0f":"code","4cf56b9c":"code","7b0e45f5":"code","c01ceedb":"code","6e1aaffb":"code","063aac16":"code","68c1bddc":"code","b46e021a":"code","f5290e88":"code","951b6844":"code","1f43331e":"code","70cbffba":"code","08ce62ed":"code","98ea96c5":"code","d94965b4":"code","86af8bbc":"code","56397fbb":"code","a1647f41":"code","6615f20c":"code","b208e6ff":"code","0b835d73":"code","797ed3d6":"code","4cdb093c":"code","4b02abf2":"code","3dcf1f6d":"code","925d8dd4":"code","a1536f3b":"code","45f4292d":"code","ace38eb7":"code","9af293de":"code","4efed7b1":"code","1cfcb622":"code","5d8d1dd0":"code","bca2b9a7":"code","267e2921":"code","d58e762e":"code","29881004":"code","abbf899e":"code","79270cbd":"code","c50c49cf":"code","73c45da4":"code","5c66fb89":"code","26ec773b":"code","795aa145":"code","5fc53b9e":"code","6705ca3c":"code","5df88f23":"code","8a65baef":"code","3490ba33":"code","a9767224":"code","e921947e":"code","98d6cdf2":"code","597cdbcd":"code","af7a9beb":"code","71ad58db":"code","d1057445":"code","09b989f2":"code","8a81ca99":"code","b6e7518c":"code","e9759e4d":"code","fae27fc5":"code","4e43572d":"code","f285d585":"code","827108e4":"code","591af020":"code","67e08b45":"code","61069528":"code","631e2e2b":"code","abe098d1":"code","2ed06e85":"code","ef4c0106":"code","5c3aaf6e":"markdown","b224b7dd":"markdown","71ae6f82":"markdown","64a057e9":"markdown","0ef0a212":"markdown","2cfe7859":"markdown","c6bef0bf":"markdown","cbbf5138":"markdown","ee33482a":"markdown","b296287e":"markdown","3ca8772e":"markdown","1c2f44d9":"markdown","108a6fc8":"markdown","3f462139":"markdown","2bfe6402":"markdown","a9e2f8a3":"markdown","89a4a397":"markdown","03bdc135":"markdown","2bec4c5e":"markdown","30b16e3b":"markdown","7e1f5e3e":"markdown","cac73e56":"markdown","b1c41220":"markdown","f26a8a4a":"markdown","39cdd971":"markdown","f360fdae":"markdown","79d2c789":"markdown","5bf070c1":"markdown","fa5f9ea5":"markdown","ca300cf2":"markdown","9ae2ace2":"markdown","0eb40460":"markdown","d3359b3c":"markdown","12b9416e":"markdown","df1370f4":"markdown","8bfd92d1":"markdown","580b8f4c":"markdown","19ca893d":"markdown","9a67107a":"markdown","f4ceb6ac":"markdown","55d666d4":"markdown","956c5d5b":"markdown","d949458d":"markdown","d6fba7a9":"markdown","a2610b92":"markdown","804905e1":"markdown","6a7f9711":"markdown","4d2420a6":"markdown","af73fde3":"markdown","b5491c0e":"markdown","8ad7a708":"markdown","9a240e2f":"markdown","87323c56":"markdown","3454b368":"markdown","6d3f95fa":"markdown","b5fc7a6b":"markdown","77bc31df":"markdown","a639556e":"markdown","03145b64":"markdown"},"source":{"43a1d379":"import pandas as pd\n\n# Read csv file\ndf = pd.read_csv('..\/input\/liver-falilure-fibrosis-hepatitis-cirrhosis\/processed_unscaled_data.csv')","e02bcbe3":"fibrosis_and_alp = df[['Fibrosis','ALP']]\nfibrosis_and_alp.head(5)","1d3a310d":"#ALP levels of patients with Fibrosis.\nfibrosis = fibrosis_and_alp['ALP'] [fibrosis_and_alp['Fibrosis'] == True]\nfibrosis.tail(5)","c1d5d9e9":"#ALP levels of people without Fibrosis.\nnon_fibrosis = fibrosis_and_alp['ALP'] [fibrosis_and_alp['Fibrosis'] == False]\nnon_fibrosis.head()","a0721386":"import matplotlib.pyplot as plt\n\n# Distibution of ALP for people with and without Fibrosis.\nfibrosis_and_alp.hist(by ='Fibrosis')\nplt.ylabel('ALP Levels',fontsize=10)","64424685":"means_df = fibrosis_and_alp.groupby('Fibrosis').mean()\nmeans_df","004d2208":"observed_difference = means_df['ALP'][1] - means_df['ALP'][0]\nobserved_difference","ced91342":"fibrosis_and_alp = df[['Fibrosis', 'ALP']]\nshuffled = fibrosis_and_alp.sample(596, replace = False)\nshuffled_weights = shuffled['ALP']\noriginal_and_shuffled = fibrosis_and_alp.assign(shuffled_weights = shuffled_weights.values )\nall_group_means= original_and_shuffled.groupby('Fibrosis').mean()\ndifference = all_group_means['shuffled_weights'][0]- all_group_means['shuffled_weights'][1]\ndifference","7242fdaa":"import numpy as np\nimport array\n\ndifferences = np.zeros(5000)\n\nfor i in np.arange(5000):\n    fibrosis_and_alp = df[['Fibrosis', 'ALP']]\n    shuffled = fibrosis_and_alp.sample(596, replace = False)\n    shuffled_weights = shuffled['ALP']\n    original_and_shuffled = fibrosis_and_alp.assign(shuffled_weights = shuffled_weights.values )\n    all_group_means = original_and_shuffled.groupby('Fibrosis').mean()\n    difference = all_group_means['shuffled_weights'][0]- all_group_means['shuffled_weights'][1]\n    differences[i] = difference","fecbc292":"differences_df = pd.DataFrame(differences)\ndifferences_df.head(5)","7adcf2a0":"import matplotlib.pyplot as plt\n\ndifferences_df.hist(bins = np.arange(-5, 5, 0.5))\nplt.title('Prediction Under Null Hypotheses');\nplt.xlabel('Differences between Group Averages', fontsize = 15)\nplt.ylabel('Units', fontsize = 15);\nprint('Observed Difference:', observed_difference)","ef48c36f":"# Empirical P-Value\nnp.count_nonzero(differences <= observed_difference)\/differences.size","a7a9ed8d":"ages = df['Age']\nlen(ages)","92270adf":"fib_ages = df['Age'][df['Fibrosis'] == 1]\nlen(fib_ages)","eb92a881":"import numpy as np\n\nfib_mean = np.mean(fib_ages)\nage_mean = np.mean(ages)\n\nprint(\"Mean Age of (Fibrosis)Sample is :\", fib_mean)\nprint(\"Mean Age of Population is :\", age_mean)","628fa23d":"from scipy.stats import ttest_1samp\n\nttest, p_val = ttest_1samp(fib_ages, age_mean)\np_val","967cbd0f":"import pandas as pd\n\n# Read csv file\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category']\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']]\n\nX.head()","4cf56b9c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = Y, test_size=0.2, random_state=52)\n\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of Y_train:', Y_train.shape)\nprint('Shape of Y_test:', Y_test.shape)","7b0e45f5":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nmodelnb1 = GaussianNB()\nmodelnb1.fit(X_train, Y_train)\n\nY_pred = modelnb1.predict(X_test)\nY_pred[:10]","c01ceedb":"acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nacc","6e1aaffb":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_test,Y_pred))","063aac16":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(6,6), \n                                title='Confusion Matrix: Naive Bayes',\n                                normalize=True,\n                                cmap='Blues')","68c1bddc":"from sklearn.model_selection import KFold\nimport numpy as np\n\nscores = []\nbest_nb = GaussianNB()\n\nY = df['category'].to_numpy()\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']].to_numpy()\n\ncv = KFold(n_splits = 10,  shuffle = False)\n\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, Y_train, Y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n    \n    best_nb.fit(X_train, Y_train)\n    scores.append(best_nb.score(X_test, Y_test))","b46e021a":"best_nb.fit(X_train, Y_train)\nscores.append(best_nb.score(X_test, Y_test))\nscores","f5290e88":"from sklearn.model_selection import cross_val_score\ncross_val_score(best_nb, X_test, Y_test, cv=10)","951b6844":"import pandas as pd\nfrom imblearn.over_sampling import SMOTE\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category']\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']]\n\nX_smote,Y_smote = SMOTE().fit_sample(X, Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)","1f43331e":"print('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:', X_test.shape)\nprint('Shape of Y_train:', Y_train.shape)\nprint('Shape of Y_test:', Y_test.shape)","70cbffba":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nmodelnb2 = GaussianNB()\nmodelnb2.fit(X_train, Y_train)","08ce62ed":"Y_pred = modelnb2.predict(X_test)\nacc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nacc","98ea96c5":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_test,Y_pred))","d94965b4":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(6,6), \n                                title='Confusion Matrix: Naive Bayes',\n                                normalize=False,\n                                cmap='Blues')","86af8bbc":"from sklearn.tree import DecisionTreeClassifier\n\nmodeldt1 = DecisionTreeClassifier()\nmodeldt1.fit(X_train, Y_train)\n\nDecisionTreeClassifier()\n\nY_pred = modeldt1.predict(X_test)","56397fbb":"acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nacc","a1647f41":"from sklearn.metrics import classification_report \n\nprint(classification_report(Y_test, Y_pred))","6615f20c":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=False,\n                                    cmap='Blues')\n\nplt.show()","b208e6ff":"from sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\ncriterion = ['gini', 'entropy']\nmax_depth = [2,4,6,8,10,12]\n\nmin_samples_leaf = [0.10, 0.2, 0.3, 0.4, 0.5]\nmin_samples_split = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]\n\ndec_tree = tree.DecisionTreeClassifier()","0b835d73":"parameters = dict(dec_tree__criterion=criterion,\n                dec_tree__max_depth=max_depth,\n                dec_tree__min_samples_leaf = min_samples_leaf,\n                dec_tree__min_samples_split = min_samples_split)","797ed3d6":"pipe = Pipeline(steps=[('dec_tree', dec_tree)])","4cdb093c":"clf_GS = GridSearchCV(pipe, parameters)\nclf_GS.fit(X_smote, Y_smote)","4b02abf2":"print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\nprint('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n\n\nprint('min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\nprint('min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\nprint()\nprint(clf_GS.best_estimator_.get_params()['dec_tree'])","3dcf1f6d":"from sklearn.tree import DecisionTreeClassifier\n\nmodeldt2 = DecisionTreeClassifier()\nmodeldt2.fit(X_train, Y_train)\n\nDecisionTreeClassifier(criterion=\"gini\", \n                       max_depth=6, min_samples_leaf=0.1,\n                       min_samples_split=0.1)\n\nY_pred = modeldt2.predict(X_test)","925d8dd4":"acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nacc","a1536f3b":"from sklearn.metrics import classification_report \n\nprint(classification_report(Y_test, Y_pred))","45f4292d":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=False,\n                                    cmap='Blues')\n\nplt.show()","ace38eb7":"from sklearn import tree\n\ntext_rep = tree.export_text(modeldt2, feature_names=['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT'])\nprint(text_rep)","9af293de":"from sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, Y_train)\n\nY_pred = classifier.predict(X_test)\nY_pred[:10]","4efed7b1":"acc = round(accuracy_score(Y_test,Y_pred) * 100, 2)\nacc","1cfcb622":"print(classification_report(Y_test, Y_pred))","5d8d1dd0":"from sklearn.metrics import classification_report, confusion_matrix\nimport scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(8,8), \n                                    title='Confusion Matrix: Decision Tree',\n                                    normalize=False,\n                                    cmap='Blues')","bca2b9a7":"from sklearn.model_selection import KFold\nimport numpy as np","267e2921":"from sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category'].to_numpy()\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']].to_numpy()\n\nX_smote,Y_smote = SMOTE().fit_sample(X, Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)","d58e762e":"scores = []\nbest_knn = KNeighborsClassifier(n_neighbors=5)\n\ncv = KFold(n_splits=10,  shuffle=False)\nfor train_index, test_index in cv.split(X_smote):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, Y_train, Y_test = X_smote[train_index], X_smote[test_index], Y_smote[train_index], Y_smote[test_index]\n    best_knn.fit(X_train, Y_train)\n    scores.append(best_knn.score(X_test, Y_test))","29881004":"best_knn.fit(X_train, Y_train)\nscores.append(best_knn.score(X_test, Y_test))\nscores","abbf899e":"print(np.mean(scores))","79270cbd":"from sklearn.model_selection import cross_val_score\ncross_val_score(best_knn, X_smote, Y_smote, cv=10)","c50c49cf":"import pandas as pd\nfrom imblearn.over_sampling import SMOTE\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category']\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']]\n\nX_smote,Y_smote = SMOTE().fit_sample(X, Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)","73c45da4":"# from sklearn.svm import SVC\n# from sklearn.model_selection import GridSearchCV \n  \n# # defining parameter range \n# param_grid = {'C': [0.01, 0.1, 0.2, 0.5, 1, 10, 100],  \n#               'gamma': [1, 0.1, 0.01, 0.001], \n#               'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}  \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# # fitting the model for grid search \n# grid.fit(X_train, Y_train)","5c66fb89":"# # print best parameter after tuning \n# print(grid.best_params_) \n  \n# # print how our model looks after hyper-parameter tuning \n# print(grid.best_estimator_)","26ec773b":"from sklearn.svm import SVC\n \nsvm_model = SVC(C=10, gamma=0.001, kernel='poly')\nsvm_model.fit(X_train, Y_train)\n\nY_pred = svm_model.predict(X_test)","795aa145":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_test, Y_pred)\nacc","5fc53b9e":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_test, Y_pred))","6705ca3c":"skplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(6,6), \n                                    title='Confusion Matrix: SVM',\n                                    normalize=False,\n                                    cmap='Blues')\nplt.show()","5df88f23":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, Y_train)\nY_pred = log_reg.predict(X_test)","8a65baef":"from sklearn.metrics import accuracy_score\n\nacc = accuracy_score(Y_test,Y_pred)\nacc","3490ba33":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_test, Y_pred))","a9767224":"skplt.metrics.plot_confusion_matrix(Y_test, Y_pred, figsize=(6,6), \n                                    title='Confusion Matrix: SVM',\n                                    normalize=False,\n                                    cmap='Blues')\nplt.show()","e921947e":"from sklearn.linear_model import LinearRegression\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['CREA']\nX = df[['Age', 'Sex', 'category']]\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=52)\n\nlinear = LinearRegression()\nlinear.fit(X_train, Y_train)","98d6cdf2":"from sklearn.metrics import r2_score\n\nY_pred = linear.predict(X_test)\nscore = r2_score(Y_test, Y_pred)\nscore","597cdbcd":"print(\"The linear model is: Y = \",linear.intercept_, \"+\", linear.coef_[0], \"X1 + \", linear.coef_[1], \"X2 + \", linear.coef_[2], \"X3\")","af7a9beb":"# KFold cross validation\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 100)\nscores = cross_val_score(linear, X_train, Y_train, scoring='r2', cv=folds)\nscores ","71ad58db":"from sklearn.model_selection import train_test_split\n\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category']\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']]\n\nX_smote,Y_smote = SMOTE().fit_sample(X, Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)\n","d1057445":"from sklearn.svm import SVR\n\nsv_regressor = SVR(kernel='linear', degree=3)\n\nsv_regressor.fit(X_train,Y_train)\nY_pred = sv_regressor.predict(X_test)\nY_pred[:10]","09b989f2":"# class to convert regression values to Class\ndef reg_to_class(val):\n    cc = 0\n    if val<=0.5:\n        cc = 0\n    elif val<=1.5 and val>0.5:\n        cc = 1\n    elif val<=2.5 and val>1.5:\n        cc = 2\n    elif val<=3.5 and val>2.5:\n        cc = 3\n    else:\n        cc = 4\n    return cc","8a81ca99":"pred = []\n\nfor i in Y_pred:\n    pred.append(reg_to_class(i))\n\npred[:10]","b6e7518c":"print(sv_regressor.score(X_test, Y_test))","e9759e4d":"# create a KFold object with 10 splits\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 100)\nscores = cross_val_score(sv_regressor, X_train, Y_train, scoring='r2', cv=folds)\nscores ","fae27fc5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","4e43572d":"log_model = LogisticRegression()\nnb_model = GaussianNB()\nsvc_model = SVC()\ndes_model = DecisionTreeClassifier(class_weight=None, criterion=\"gini\", \n                       max_depth=6, min_samples_leaf=0.1,\n                       min_samples_split=0.1)","f285d585":"# Multi-class classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# Read csv file into dataframe\ndf = pd.read_csv('https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv')\n\nY = df['category'].to_numpy()\nX = df[['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']].to_numpy()\n\nX_smote,Y_smote = SMOTE().fit_sample(X, Y)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, stratify = Y_smote, test_size=0.2, random_state=52)\n\nlabel_dict = {0: 'Blood donor',1 :'Cirrhosis', 2: 'Fibrosis', 3 :'Hepatits', 4 : 'Suspect Blood Donor'}\n\ndef find_roc(classifier):\n    # fit model\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, Y_train)\n    pred = clf.predict(X_test)\n    pred_prob = clf.predict_proba(X_test)\n\n    fpr = {}\n    tpr = {}\n    thresh ={}\n\n    n_class = 5\n\n    for i in range(n_class):    \n        fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, pred_prob[:,i], pos_label=i)\n        roc_auc  = auc(fpr[i], tpr[i])\n        plt.plot(fpr[i], tpr[i], linestyle='--', label='%s ROC (area = %0.2f)' % (label_dict[i], roc_auc))\n\n    plt.title(str(classifier) + ' Multiclass ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.legend(loc='best')","827108e4":"# Logistic Regression\nlog_model.max_iter=1000\nfind_roc(log_model)","591af020":"# Naive Bayes Model\nfind_roc(nb_model)","67e08b45":"# SVC Model\nsvc_model = SVC()\nsvc_model.probability = True\nfind_roc(svc_model)","61069528":"# Decision Tree\nfind_roc(des_model)","631e2e2b":"# !pip install gradio","abe098d1":"def convert(opt):\n    if opt==0:\n        strr = \"Blood Donor\"\n    elif opt == 1:\n        strr= \"Cirrhosis\"\n    elif opt == 2:\n        strr = \"Fibrosis\"\n    elif opt == 3:\n        strr = \"Hepatitis\"\n    else:\n        strr = \"Suspected Blood Donor\"\n    return strr","2ed06e85":"import gradio as gr\n\nimport pandas as pd\n\ndef gpredict(Age, Sex, ALB, ALP, ALT, AST, BIL, CHE, CHOL, CREA, GGT, PROT):\n    \n    if Sex == \"Male\":\n        Sex = 1\n    else:\n        Sex = 0\n        \n    test = [[Age, Sex, ALB, ALP, ALT, AST, BIL, CHE, CHOL, CREA, GGT, PROT]]\n    test = pd.DataFrame(test, columns =['Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT'], dtype = float)\n\n    opt1 = int(svm_model.predict(test))\n    opt2 = int(classifier.predict(test))\n    opt3 = int(modelnb2.predict(test))\n    opt4 = int(modeldt2.predict(test))\n    opt5 = int(log_reg.predict(test))\n    \n    listt = [opt1, opt2, opt3, opt4, opt5]\n    \n    opt = max(set(listt), key=listt.count)\n \n    return convert(opt1), convert(opt2), convert(opt3), convert(opt4), convert(opt5), convert(opt)\n","ef4c0106":"iface = gr.Interface(\n  fn = gpredict, \n    \n  inputs=[gr.inputs.Slider(0, 80, default=42), gr.inputs.Radio(['female', 'male'], label=\"Sex\"), \n          gr.inputs.Slider(0, 70), gr.inputs.Slider(0, 210),\n          gr.inputs.Slider(0, 330), gr.inputs.Slider(0, 190),\n          gr.inputs.Slider(0, 100), gr.inputs.Slider(0, 18),\n          gr.inputs.Slider(0, 10), gr.inputs.Slider(0, 160),\n          gr.inputs.Slider(0, 300), gr.inputs.Slider(0, 90)],\n    \n  outputs=[gr.outputs.Textbox(label=\"SVM (98%)\"), \n           gr.outputs.Textbox(label=\"KNN (97%)\"), \n           gr.outputs.Textbox(label=\"Naive Bayes (87%)\"), \n           gr.outputs.Textbox(label=\"Decision Tree (96%)\"),\n           gr.outputs.Textbox(label=\"Logistic Regression (94%)\"),\n           gr.outputs.Textbox(label=\"Final Result\")]\n\n  )\n\niface.launch()","5c3aaf6e":"#### 5. Logistic Regression(Model1)","b224b7dd":"#### PERMUTATION TEST","71ae6f82":"X (Patient ID\/No., \nAge - age (in years), \nSex - male or female(f,m), \nALB - Albumin, \nALP - Alkaline phosphatase, \nALT - Alanine transaminase, \nAST - Aspartate aminotransferase, \nBIL - Bilirubin, \nCHE - cholinesterase, \nCHOL - Cholesterol, \nCREA - Creatine, \nGGT - Gamma-glutamyl transferase, \nPROT - Protien in urine, ","64a057e9":"### ROC Tests","0ef0a212":"The observed difference in the original sample is -17.23 units, which doesnt appear on the x - axis of the histogram.\nTherefore, the data supports alternate hypothesis more than the Null hypothesis.\n\nConclusion :\n    In population, Distribution of ALP levels for patients with fibrosis are different compared to people without                    fibrosis. Its not because of pure chance.","2cfe7859":"All True positive rates are aboce 90%. This is a good model with 98% Accuracy. This model is better than the Naive Bayes model.","c6bef0bf":"## Attribute Information:","cbbf5138":"#### 4. SVM (Model1-GridSearch)","ee33482a":"#### 2. Decision Tree Classifier (Model2 - GridSearchCV)","b296287e":"We can see that the distribution of ALP(Alkaline phosphatase) is different for people with fibrosis compared to rest of the population from the Tableau Dashboard. So lets inspect this behaviour.","3ca8772e":"#### 3. KNN Classifier (Model1)","1c2f44d9":"Power Bi : https:\/\/app.powerbi.com\/groups\/me\/reports\/a77111c3-b9cb-4d53-a7c4-72a64d21fbea\/ReportSectionb6f4bcc3ec64ad39500a?bookmarkGuid=567c6dd7-d4be-43b5-b458-4e4238535ad4","108a6fc8":"#### 1. Naive Bayes - Classification (Model1 - Manual)","3f462139":"#### SVR (Model1) - Purely Experimental","2bfe6402":"All attributes except Category and Sex are numerical. The laboratory data are the attributes 5-14.","a9e2f8a3":"## Part : 2","89a4a397":"Null Hypothesis : Age distribution is same in the dataset for Fibrosis and the whole population.\nAlternate Hypothesis : Age distribution in the dataset is different for fibrosis patients and wh0le population.","03bdc135":"Good cross validation score.","2bec4c5e":"Category (diagnosis) (values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis')","30b16e3b":"We have find some method to balance this class imbalance in the data set.","7e1f5e3e":"#### ONE SIMULATION","cac73e56":"14 attributes - Data set from UCI (UNIVERSITY OF CALIFORNIA)- Irwin Machine Learning Reository","b1c41220":"#### 1. Naive Bayes - Classification (Model2 - using SMOTE)","f26a8a4a":"ALP levels for people with fibrosis tend to be in between 20 and 60. The ALP levels of rest lies between 20 and 150.\nThis difference in ALP distribution could be due to chance. Lets find out. ","39cdd971":"Processed data set URL : https:\/\/raw.githubusercontent.com\/toshihiroryuu\/Machine_learning\/master\/ML_004_Hepatitis\/processed_unscaled_data.csv","f360fdae":"An infection caused by a virus that attacks the liver and leads to inflammation.","79d2c789":"#### Gradio UI (User Interface)","5bf070c1":"Data set url : http:\/\/archive.ics.uci.edu\/ml\/datasets\/HCV+data","fa5f9ea5":"The virus is spread by contact with contaminated blood; for example, from sharing needles or from unsterile tattoo equipment.\nMost people have no symptoms. Those who do develop symptoms may have fatigue, nausea, loss of appetite and yellowing of the eyes and skin","ca300cf2":"Hepatitis C","9ae2ace2":"### Hypothesis testing","0eb40460":"### Regression Models","d3359b3c":"#### 2. Decision Tree Classifier (Model1 - Manual)","12b9416e":"p value is greater than 0.05(alpha), therefore Accepting Null hypotheses.","df1370f4":"All True positive rates are aboce 75%.","8bfd92d1":"SMOTE - Synthetic Minority Oversampling Technique. \nThis is a statistical technique for increasing the number of cases in your dataset in a balanced way.","580b8f4c":"The data set has been preprocessed and saved as csv file for convinience. This processed data set can be found at below url.","19ca893d":"Tableau Online : https:\/\/public.tableau.com\/views\/Liver-Faliure\/Summary?:language=en&:display_count=y&:origin=viz_share_link","9a67107a":"Apply Grid search to find the best parameters for SVC","f4ceb6ac":"The observed value of the test statistic is -17.23 . Which favours Alternate Hypothesis.","55d666d4":"https:\/\/github.com\/toshihiroryuu\/Machine_learning\/blob\/master\/ML_004_Hepatitis\/19AI611_CaseStudyReview2_Athul_016.ipynb","956c5d5b":"### Visualisation using Power Bi and  Tableau","d949458d":"Test Statistic:\n\n    We calculate the difference between mean ALP levels between fibrosis patients and others.\n    Larger this difference, more it indicates that the alternate hypothesis to be true.","d6fba7a9":"#### Linear Regression (Model1)","a2610b92":"#### Accuracy","804905e1":"### Classification Models","6a7f9711":"SUMMARY:\n    ---------------------------------------------------------\n    |  Model             |    Accuracy    |    F-measure    | \n    ---------------------------------------------------------\n    |Naive Bayes         |    87.22       |       0.87      |\n    ---------------------------------------------------------\n    |Decision Tree       |    96.05       |       0.96      |\n    ---------------------------------------------------------\n    |KNN                 |    97.37       |       0.98      |\n    ---------------------------------------------------------\n    |SVM                 |    98.6        |       0.97      |           \n    ---------------------------------------------------------\n    |Logistic Regression |    93.7        |       0.94      |\n    ---------------------------------------------------------","4d2420a6":"#### 10 Fold Cross Validation - KNN","af73fde3":"#### 10 Fold cross validation","b5491c0e":"# 0 - Blood donor\n# 1 - Cirrhosis\n# 2 - Fibrosis\n# 3 - Hepatits\n# 4 - Suspect Blood Donor","8ad7a708":"Cross validation score is really less. The least populated class has less number of members than 10.","9a240e2f":"## Liver Faliure (Hepatitis C) - Prediction. - Review 2","87323c56":"### Student T-test","3454b368":"#### GitHub Repo for this Document (19AI611_CaseStudy_Athul_016_Review2.ipynb) : ","6d3f95fa":"#### Confusion Matrix","b5fc7a6b":"Best SVC model using Grdi Search.","77bc31df":"Hypothesis : \n\n    Null Hypothesis : In Population, Distribution of ALP levels for patients with fibrosis is same as people without fibrosis.\n                    \n    Alternate Hypothesis : In population, Distribution of ALP levels for patients with fibrosis are different as people without                            fibrosis.","a639556e":"The empirical P-value is approximately  0, \nmeaning that none of the 5,000 observed samples resulted in a difference of -17.23 units or lower. \nThat is, the exact chance of getting a difference in that range is not 0 but it is vanishingly small.","03145b64":"The target attribute for classification is Category (blood donors vs. Hepatitis C (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis)"}}