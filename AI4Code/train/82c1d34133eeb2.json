{"cell_type":{"b7b0ae3b":"code","4a6cc854":"code","a0c63b31":"code","7ce97c74":"code","17f83062":"code","ca3b9b30":"code","87190c22":"code","8dc15f47":"code","5e570ce5":"code","d14f40cf":"code","4f6743bd":"code","7ce83621":"code","6f57f0fb":"code","b0a1d88b":"code","ef582f08":"code","46033843":"code","c567e33f":"code","18ce529b":"code","432166c6":"code","ff0ee2cc":"code","9ed5fcf2":"code","5276f418":"code","f9436e19":"code","eeda890a":"code","1f977a6e":"code","10af6d51":"code","9389df96":"markdown","edfb4257":"markdown","5537cf41":"markdown","17e98d6b":"markdown","ddeb555f":"markdown","9f230513":"markdown","96bdc0d4":"markdown","673b15c7":"markdown","5da11adc":"markdown","aa558ac2":"markdown","f09deb05":"markdown","43d960fe":"markdown"},"source":{"b7b0ae3b":"!pip install pytorch-tabnet","4a6cc854":"#===========================================================\n# Library\n#===========================================================\nimport os\nimport gc\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom contextlib import contextmanager\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom functools import partial\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor ##Import Tabnet \n\n\n\nfrom pathlib import Path\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a0c63b31":"os.listdir('..\/input\/trends-assessment-prediction\/')","7ce97c74":"#===========================================================\n# Utils\n#===========================================================\ndef get_logger(filename='log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\n\ndef seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n    \ndef load_df(path, df_name, debug=False):\n    if path.split('.')[-1]=='csv':\n        df = pd.read_csv(path)\n        if debug:\n            df = pd.read_csv(path, nrows=1000)\n    elif path.split('.')[-1]=='pkl':\n        df = pd.read_pickle(path)\n    if logger==None:\n        print(f\"{df_name} shape \/ {df.shape} \")\n    else:\n        logger.info(f\"{df_name} shape \/ {df.shape} \")\n    return df","17f83062":"#===========================================================\n# Config\n#===========================================================\nOUTPUT_DICT = ''\n\nID = 'Id'\nTARGET_COLS = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nSEED = 42\nseed_everything(seed=SEED)\n\nN_FOLD = 5","ca3b9b30":"train = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtype={'Id':str})\\\n            .dropna().reset_index(drop=True) # to make things easy\nreveal_ID = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv', dtype={'Id':str})\nICN_numbers = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nloading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtype={'Id':str})\nfnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv', dtype={'Id':str})\nsample_submission = pd.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv', dtype={'Id':str})","87190c22":"train.head()","8dc15f47":"reveal_ID.head()","5e570ce5":"ICN_numbers.head()","d14f40cf":"loading.head()","4f6743bd":"fnc.head()","7ce83621":"sample_submission.head()","6f57f0fb":"sample_submission['ID_num'] = sample_submission[ID].apply(lambda x: int(x.split('_')[0]))\ntest = pd.DataFrame({ID: sample_submission['ID_num'].unique().astype(str)})\ndel sample_submission['ID_num']; gc.collect()\ntest.head()","b0a1d88b":"fnc_features =list(fnc.columns[1:])","ef582f08":"# merge\ntrain = train.merge(loading, on=ID, how='left')\ntrain = train.merge(fnc, on=ID, how='left')\ntrain.head()","46033843":"# merge\ntest = test.merge(loading, on=ID, how='left')\ntest = test.merge(fnc, on=ID, how='left')\ntest.head()","c567e33f":"FNC_SCALE = 1\/500\n\ntrain[fnc_features] *= FNC_SCALE\ntest[fnc_features] *= FNC_SCALE","18ce529b":"folds = train[[ID]+TARGET_COLS].copy()\nFold = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[TARGET_COLS])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nfolds.head()","432166c6":"#===========================================================\n# model\n#===========================================================\ndef run_single_tabnet(clf,train_df, test_df, folds, features, target, fold_num=0, categorical=[]):\n    \n    trn_idx = folds[folds.fold != fold_num].index\n    val_idx = folds[folds.fold == fold_num].index\n    logger.info(f'len(trn_idx) : {len(trn_idx)}')\n    logger.info(f'len(val_idx) : {len(val_idx)}')\n    X_train= train_df.iloc[trn_idx][features].values ###Converted this into Numpy array because TabNet will give error otherwise .\n    y_train=target.iloc[trn_idx].values\n    X_valid = train_df.iloc[val_idx][features].values\n    y_valid= target.iloc[val_idx].values\n\n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n    \n\n    clf.fit(\n                X_train=X_train, y_train=y_train, ##Train features and train targets\n                X_valid=X_valid, y_valid=y_valid, ##Valid features and valid targets\n                weights =0,#0 for no balancing,1 for automated balancing,dict for custom weights per class\n                max_epochs=1000,##Maximum number of epochs during training , Default 1000. I used 10\n                patience=70, ##Number of consecutive non improving epoch before early stopping , Default 50\n                batch_size=1024, ##Training batch size\n                virtual_batch_size=128 )##Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features].values)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importances_\n    fold_importance_df[\"fold\"] = fold_num\n\n    predictions += clf.predict(test_df[features].values)\n    \n    # RMSE\n    logger.info(\"fold{} RMSE score: {:<8.5f}\".format(fold_num, np.sqrt(mean_squared_error(target[val_idx], oof[val_idx]))))\n    \n    return oof, predictions, fold_importance_df\n\n\ndef run_kfold_tabnet(clf,train, test, folds, features, target, n_fold=5, categorical=[]):\n    \n    logger.info(f\"================================= {n_fold}fold TabNet =================================\")\n    \n    oof = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    feature_importance_df = pd.DataFrame()\n\n    for fold_ in range(n_fold):\n        print(\"Fold {}\".format(fold_))\n        _oof, _predictions, fold_importance_df = run_single_tabnet(clf,train,\n                                                                     test,\n                                                                     folds,\n                                                                     features,\n                                                                     target,\n                                                                     fold_num=fold_,\n                                                                     categorical=categorical)\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        oof += _oof\n        predictions += _predictions \/ n_fold\n\n    # RMSE\n    logger.info(\"CV RMSE score: {:<8.5f}\".format(np.sqrt(mean_squared_error(target, oof))))\n\n    logger.info(f\"=========================================================================================\")\n    \n    return feature_importance_df, predictions, oof\n\n    \ndef show_feature_importance(feature_importance_df, name):\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:50].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n    plt.figure(figsize=(8, 16))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('Features importance (averaged\/folds)')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DICT+f'feature_importance_{name}.png')","ff0ee2cc":"prediction_dict = {}\noof_dict = {}\n\nfor TARGET in TARGET_COLS: ## I think this model will work for multiple targets altogether , let me try that later\n    \n    logger.info(f'### TABNET for {TARGET} ###')\n\n    target = train[TARGET]\n    test[TARGET] = np.nan\n\n    # features\n    cat_features = []\n    num_features = [c for c in test.columns if (test.dtypes[c] != 'object') & (c not in cat_features)]\n    features = num_features + cat_features\n    drop_features = [ID] + TARGET_COLS\n    features = [c for c in features if c not in drop_features]\n\n    if cat_features:\n        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='impute')\n        ce_oe.fit(train)\n        train = ce_oe.transform(train)\n        test = ce_oe.transform(test)\n        \n    cat_idxs = [ i for i, f in enumerate(features) if f in cat_features]\n\n    cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in cat_features]    \n        \n    clf = TabNetRegressor(\n                        n_d = 16,##Width of the decision prediction layer. Bigger values gives more capacity to the model with the risk of overfitting. Values typically range from 8 to 64.\n                        n_a = 16,##Width of the attention embedding for each mask. According to the paper n_d=n_a is usually a good choice. (default=8)\n                        n_steps = 3,##Number of steps in the architecture (usually between 3 and 10)\n                        gamma =1.3,##This is the coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. Values range from 1.0 to 2.0.\n                        cat_idxs=cat_idxs, ##List of categorical features indices.\n                        cat_dims=cat_dims,\n                        cat_emb_dim =1, ##List of embeddings size for each categorical features. (default =1)\n                        n_independent =2,##Number of independent Gated Linear Units layers at each step. Usual values range from 1 to 5.\n                        n_shared =2,##Number of shared Gated Linear Units at each step Usual values range from 1 to 5\n                        epsilon  = 1e-15,##Should be left untouched.\n                        seed  =0,##Random seed for reproducibility\n                        momentum = 0.02, ##Momentum for batch normalization, typically ranges from 0.01 to 0.4 (default=0.02)\n                        lr = 0.01, ##Initial learning rate used for training. As mentionned in the original paper, a large initial learning of 0.02 with decay is a good option.\n                        clip_value =None,\n                        lambda_sparse =1e-3,##This is the extra sparsity loss coefficient as proposed in the original paper. The bigger this coefficient is, the sparser your model will be in terms of feature selection. Depending on the difficulty of your problem, reducing this value could help.\n                        optimizer_fn =torch.optim.Adam, ## Optimizer\n                        scheduler_fn = None, #torch.optim.lr_scheduler.ReduceLROnPlateau, ## LR scheduler \n                        scheduler_params = None,#{\"mode\":'min', \"factor\":0.1, \"patience\":10, \"verbose\":\"False\"}, ## LR scheduler parameters dictionary\n                        verbose =1,\n                        device_name = 'auto' ## Auto or 'gpu' ## I have no GPU\n                        \n                        )    \n\n    feature_importance_df, predictions, oof = run_kfold_tabnet(clf,train, test, folds, features, target, \n                                                                 n_fold=N_FOLD, categorical=cat_features)\n    \n    prediction_dict[TARGET] = predictions\n    oof_dict[TARGET] = oof\n    \n    show_feature_importance(feature_importance_df, TARGET)","9ed5fcf2":"# https:\/\/www.kaggle.com\/akurmukov\/trends-starter-rf-0-168-lb-metric\n\ndef lb_metric(y_true, y_pred):\n    '''Computes lb metric, both y_true and y_pred should be DataFrames of shape n x 5'''\n    y_true = y_true[['age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']]\n    y_pred = y_pred[['age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']]\n    weights = np.array([.3, .175, .175, .175, .175])\n    return np.sum(weights * np.abs(y_pred.values - y_true.values).sum(axis=0) \/ y_true.values.sum(axis=0))","5276f418":"oof_df = pd.DataFrame()\n\nfor TARGET in TARGET_COLS:\n    oof_df[TARGET] = oof_dict[TARGET]\n","f9436e19":"score = lb_metric(train, oof_df)\nlogger.info(f'Local Score: {score}')","eeda890a":"sample_submission.head()","1f977a6e":"pred_df = pd.DataFrame()\n\nfor TARGET in TARGET_COLS:\n    tmp = pd.DataFrame()\n    tmp[ID] = [f'{c}_{TARGET}' for c in test[ID].values]\n    tmp['Predicted'] = prediction_dict[TARGET]\n    pred_df = pd.concat([pred_df, tmp])\n\nprint(pred_df.shape)\nprint(sample_submission.shape)\n\npred_df.head()","10af6d51":"submission = sample_submission.drop(columns='Predicted').merge(pred_df, on=ID, how='left')\nprint(submission.shape)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","9389df96":"### As per TabNet Paper , TabNet has three specialities : \n    a) Unlike tree-based methods, TabNet inputs raw tabular data\n    b)TabNet uses sequential a\u0088ention \n    c) Trained using Gradient -Descent Based Optimizations\n![image.png](attachment:image.png)","edfb4257":"# Utils","5537cf41":"# Submission","17e98d6b":"# Prepare folds","ddeb555f":"### TabNet, a novel deep learning architecture for tabular learning. TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful features to process at each decision step.TabNet uses canonical DNNs to act as decision trees .\n![image.png](attachment:image.png)\n\n\n### Above represented encoder decoder architecture for TabNet. \n#### (a) TabNet encoder for classification or regression, composed of a feature transformer, an attentive transformer and feature masking at each decision step. A split block divides the processed representation into two, to be used by the attentive transformer of the subsequent step as well as for constructing the overall output. At each decision step, the feature selection mask can provide interpretable information about the model\u2019s functionality, and the masks can be aggregated to obtain global feature important attribution. (b) TabNet decoder, composed of a feature transformer block at each step. (c) A feature transformer block example \u2013 4-layer network is shown, where 2 of the blocks are shared across all decision steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, BN and GLU nonlinearity. (d) An attentive transformer block example \u2013 a single layer mapping is modulated with a prior scale information which aggregates how much each feature has been used before the current decision step. Normalization of the coefficients is done using sparsemax for sparse selection of the most salient features at each decision step. (Collected from Original Paper)\n","9f230513":"# Library","96bdc0d4":"## Installation","673b15c7":"# Config","5da11adc":"## MODEL","aa558ac2":"# FE","f09deb05":"##### Hello this complete code workflow is adopted from Y. Nakama's fantastic starter code . \n\n##### I have got to know about this Attention Based Interpretable Machine Learning -TabNet from https:\/\/www.kaggle.com\/abhishek  and  wanted to try this out in TReNDs problem . I have limited experience in Machine Learning , so  may not have understood many places or there might be mistakes.\n\nI have used the below github repo \nhttps:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/develop\/pytorch_tabnet\n\nContent is taken mainly from the original paper for TabNet .\n\nhttps:\/\/arxiv.org\/abs\/1908.07442\n\n\n##### Note : I have not really played around with parameters of TabNet . Planning to do that in subsequent versions. Anyone interested also can try.\n","43d960fe":"# Data Loading"}}