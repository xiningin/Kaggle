{"cell_type":{"b1dd8e25":"code","8dddf991":"code","37267d61":"code","2c8ebbc3":"code","516da92c":"code","6c688f36":"code","012a8b52":"code","bbc61774":"code","ec468eaa":"code","9e934c59":"code","d9cc0756":"code","a12f9561":"code","7c42d571":"code","9dcda505":"code","447daa6c":"code","a0c5903e":"code","ac6df860":"code","d8ca1a50":"markdown","ab8c1921":"markdown","6ac9c7ad":"markdown","da0a34e5":"markdown","b9cc21b7":"markdown","99beb417":"markdown","2d3f038c":"markdown","71494ee7":"markdown","a10bb598":"markdown","10e3fda3":"markdown","9acaa10a":"markdown","834686fe":"markdown","33fa815e":"markdown","04db0d52":"markdown","46a490ff":"markdown"},"source":{"b1dd8e25":"import pandas as pd  # data processing\nimport numpy as np   # linear algebra\nimport matplotlib.pyplot as plt  #Plotting\nfrom sklearn.preprocessing import StandardScaler\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","8dddf991":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","37267d61":"data.info()","2c8ebbc3":"data.describe()","516da92c":"data.isnull().sum()","6c688f36":"data[['Glucose','BloodPressure','SkinThickness','BMI','DiabetesPedigreeFunction','Age']] = data[['Glucose','BloodPressure','SkinThickness','BMI','DiabetesPedigreeFunction','Age']].replace(0,np.NaN)\ndata.head(10)","012a8b52":"data_nan = data.isna().sum()\ndata_nan = pd.DataFrame(data_nan, columns=['NaN count'])\ndata_nan","bbc61774":"data['Glucose'].fillna(data['Glucose'].median(), inplace=True)\ndata['BloodPressure'].fillna(data['BloodPressure'].median(), inplace=True)\ndata['SkinThickness'].fillna(data['SkinThickness'].median(), inplace=True)\ndata['BMI'].fillna(data['BMI'].median(), inplace=True)\ndata['DiabetesPedigreeFunction'].fillna(data['DiabetesPedigreeFunction'].median(), inplace=True)\ndata['Age'].fillna(data['Age'].median(), inplace=True)\ndata.describe()\n","ec468eaa":"sns.countplot(x = 'Outcome', data = data)","9e934c59":"data.corr()","d9cc0756":"plt.subplots(figsize = (12,8))\nsns.set(font_scale = 1.5)\nsns.heatmap(data.corr(), annot=True, fmt='.2f')\nplt.title('Correlation Plot', fontsize = 20)\nplt.show()","a12f9561":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.BMI, bins = 20, ax=ax[3,1]) ","7c42d571":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","9dcda505":"LR = LogisticRegression()\n\n#fiting the model\nLR.fit(X_train, y_train)\n\n#prediction\ny_pred = LR.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", LR.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","447daa6c":"DT = DecisionTreeClassifier()\n\n#fiting the model\nDT.fit(X_train, y_train)\n\n#prediction\ny_pred = DT.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", DT.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","a0c5903e":"NB = GaussianNB()\n\n#fiting the model\nNB.fit(X_train, y_train)\n\n#prediction\ny_pred = NB.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", NB.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","ac6df860":"KNN = KNeighborsClassifier(n_neighbors=3)\n\n#fiting the model\nKNN.fit(X_train, y_train)\n\n#prediction\ny_pred = KNN.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", KNN.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","d8ca1a50":"### Naive Bayes\nNaive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.","ab8c1921":"## EDA(exploratory data analysis)\n\nIn this section, we will create graphs to displays different distributions of the data and available relationships to allow us to understand it much better.\n\n### Checking the distribution of the target variable","6ac9c7ad":"### Logistic Regression\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary. Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","da0a34e5":"Since we have indenfied misleading values, we need to have an idea about how many misleading values present and how they are going to effect the final prediction.","b9cc21b7":"Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. A classification problem is when the output variable is a category, such as \u201cred\u201d or \u201cblue\u201d or \u201cdisease\u201d and \u201cno disease\u201d.\n\nPopular algorithms that can be used for binary classification include:\n\n* Logistic Regression\n* k-Nearest Neighbors\n* Decision Trees\n* Support Vector Machine\n* Naive Bayes\n\nLogistic Regression and Support Vector Machines algorithms are specifically designed for binary classification and do not natively support more than two classes.\n\nFirst we need to import libraries which need and import dataset to have a insight look.","99beb417":"## Initial look at Data","2d3f038c":"## Check Missing Values\nHere by looking about results we can see that there are some misleading data points. For example here in the SkinThickness varibale contains 0 as a value which is not correct. The vaibales 'pregnancies', 'Insulin' & 'Outcome' can have 0 as its value, but in other vaibles it can't. Therefore we have to identify those values as missing or misleading value.","71494ee7":"## Univariate Distribution using Displot","a10bb598":"### Replacing 0's with 'NaN'","10e3fda3":"### Decision Tree\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.","9acaa10a":"### Filling Missing Values\n\nThe missing values are filled by thier medians","834686fe":"# Appplying Classification Models","33fa815e":"# Classification EXP 8","04db0d52":"Next, we will proceed in checking the relationships by visualizing correlations as shown in the table below.\nAnd then we will plot the correlation using 'Heatmap'","46a490ff":"### KNN\nThe k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\nKNN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\nK-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data."}}