{"cell_type":{"b4869012":"code","37238ff6":"code","5a82e61d":"code","168129d7":"code","599feaa6":"code","22962e2e":"code","f063b7da":"code","b30a483b":"code","e0c97de6":"code","879c6751":"code","1905574a":"markdown","3bd1884f":"markdown","343937f1":"markdown","f2041db7":"markdown","60b7bd84":"markdown","0072bd18":"markdown","5ae23784":"markdown","b30cb39d":"markdown","fe8cd273":"markdown","b4b72b8e":"markdown","1e138639":"markdown"},"source":{"b4869012":"# Imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pymc3 as pm\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\n\n# Import dataset\ndata = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\n\n# Set constants and seed\nNUM_SAMPLES, NUM_VARS = data.shape\nSEED = 999\nnp.random.seed(SEED)\n\n# Preview\ndata.head()","37238ff6":"# Make sure there are no missing values\nassert not data.isnull().values.any()\n\n# Index binary variables \nis_binary = data.isin([0,1]).all().values\n\n# Split variable types and keep record of var names\nX_bin = data.iloc[:, is_binary]\nX_quant = data.iloc[:, ~is_binary]\n\n# Store var names as lists\nbin_names = data.columns[is_binary].tolist()\nquant_names = data.columns[~is_binary].tolist()\n\n# Box-Cox transform of quant variables, then standardize\nX_quant = X_quant.apply(lambda x: stats.boxcox(x)[0])\nX_quant = StandardScaler().fit_transform(X_quant)\n\n# Look into proportions in binary variables\nprint('Proportions in binary variables:\\n', X_bin.mean())","5a82e61d":"# NOTE: This function should only take binary variables\ndef cramers_v(x, y):\n    # Confusion matrix\n    conf_matrix = pd.crosstab(x, y)\n    # Compute chi-squared w\/o Yates' continuity correction\n    chi2 = stats.chi2_contingency(conf_matrix, correction=False)[0]\n    n = sum(conf_matrix.sum())\n    # With binary variables we have V = np.sqrt(chi2 \/ n)\n    return np.sqrt(chi2 \/ n)\n\n# Create and populate correlation matrix\ncorr_bin = np.eye(X_bin.shape[1])\nfor r in range(X_bin.shape[1]):\n    for c in range(X_bin.shape[1]):\n        corr_bin[r,c] = cramers_v(X_bin.iloc[:,r], X_bin.iloc[:,c])\n\n# Plot correlation matrix\nsns.set(rc={'figure.figsize':(9, 7)})\nsns.heatmap(corr_bin, vmin=0, vmax=1, annot=True,\n            cbar_kws={'label': \"Cramer's V\"},\n            xticklabels=bin_names, yticklabels=bin_names)","168129d7":"# Create dataframe to enable pairplot\ndata_pairplot = pd.DataFrame(X_quant, columns=quant_names)\ndata_pairplot['smoking'] = X_bin.smoking\n\n# Pairplot (quantitative variables)\nsns.pairplot(data=data_pairplot, hue='smoking')","599feaa6":"# Fit PCA\npca_model = PCA(n_components=2).fit(X_quant)\nexplained_var = pca_model.explained_variance_ratio_*100\nscores = pd.DataFrame(pca_model.transform(X_quant), columns=['PC1','PC2'])\nscores['DEATH_EVENT'] = X_bin.DEATH_EVENT\n\n# PC1-2 scatterplot\nsns.set(rc={'figure.figsize':(7, 7)})\nsns.scatterplot(x='PC1', y='PC2', data=scores,\n               hue='DEATH_EVENT')\n\nplt.xlim(-4, 4)\nplt.ylim(-4, 4)\nplt.axhline(0, linestyle='--', color='black', alpha=.25)\nplt.axvline(0, linestyle='--', color='black', alpha=.25)\nplt.xlabel('PC1 ({exp_var:.2f}%)'.format(exp_var=explained_var[0]))\nplt.ylabel('PC2 ({exp_var:.2f}%)'.format(exp_var=explained_var[1]))","22962e2e":"# Pop DEATH_EVENT from X_bin\nY = X_bin.pop('DEATH_EVENT')\n\n# Combine X and binary_vars along with vector of ones to accommodate intercept\nX = np.concatenate([np.ones((NUM_SAMPLES, 1)), X_bin.to_numpy(), X_quant], axis=1)\n\nwith pm.Model() as model:\n    # Intercept and coefficients\n    beta = pm.Normal('beta', mu=0, sigma=5, shape=X.shape[1])\n    logit = pm.math.dot(X, beta)\n    # Logistic link\n    p = 1 \/ (1 + np.exp(-logit))\n    # Return loglik of Y\n    obs = pm.Bernoulli('obs', p, observed=Y)","f063b7da":"# Sample\nwith model:\n    start = pm.find_MAP()\n    opt = pm.HamiltonianMC(beta)\n    trace = pm.sample(10000, opt, start=start,\n                      return_inferencedata=True, random_seed=SEED)\n    \n# Trace plot\npm.traceplot(trace)","b30a483b":"# Extract posterior samples of beta\ntrace_beta = trace.posterior['beta'].values.reshape(-1, NUM_VARS)\nbeta_names = ['intercept'] + bin_names[:-1] + quant_names + ['sex_x_smoking']\n\n# Violin plot with seaborn\nsns.set(rc={'figure.figsize':(6, 12)})\nax = sns.violinplot(data=trace_beta, orient='h')\nax.set(xlabel=r'$\\beta$')\nax.set_yticklabels(beta_names)\nplt.axvline(0., linestyle='--', color='black', alpha=.25)","e0c97de6":"# Compute logit and probabilities from the posterior samples\npost_logit = np.matmul(X, trace_beta.T)\npost_p = 1 \/ (1 + np.exp(-post_logit))\n\n# Plot\nsns.set(rc={'figure.figsize':(12, 4)})\nplt.hist(post_p[Y == 1,:].flatten(), alpha=.25, label='Died', bins=25)\nplt.hist(post_p[Y == 0,:].flatten(), alpha=.25, label='Survived', bins=25)\nplt.legend()\nplt.xlabel(r'$P(Death|X)$')\nplt.ylabel('Frequency')","879c6751":"# Flag observation w\/ posterior prob > .01 that P(Death) > 0.5\nacc, sens, spec = [], [], []\nfor p in (.01,.25,.5,.75,.99):\n    pred = np.mean(post_p > .5, axis=1) > p\n    tn, fp, fn, tp = confusion_matrix(Y, pred.astype(np.int16)).ravel()\n    acc.append((tp + tn) \/ (tn + fp + fn + tp))\n    sens.append(tp \/ (tp + fn))\n    spec.append(tn \/ (tn + fp))\n    \nresults = {'accuracy': acc, 'sensitivity': sens, 'specificity': spec}\nresults_df = pd.DataFrame(results, index=(.01,.25,.5,.75,.99))\nresults_df.index.name = 'cutoff'\nresults_df","1905574a":"What clearly jumps to sight from the correlation matrix above is the intriguing association between smoking and sex. On closer look, we have 96% of non-smokers among women (101\/105) but only 53% among men (102\/194). Here too, you can see this for yourself using a confusion matrix, *e.g.* `pd.crosstab(X_bin['smoking'], X_bin['sex'])`. This dependence between the two factors would require special handling upon modelling, by means of introducing interaction effects or other strategies. However, I will refrain from using interaction effects or effecting any further treatment.\n\n## Quantitative variables\n\nIn a similar vein we proceed to examine any existing interdepencencies among the normalized quantitative variables. Here I would argue we do not need to compute and examine similarity metrics, and can directly visualize the bivariate distributions over all such features. The following figure will additionally distinguish smokers from non-smokers using different colors.","3bd1884f":"As seen from above, the mean of all binary variables falls in the range $[0.3, 0.7]$ which given the sample size of $N = 299$ should present no problem.\n\n# Exploratory data analysis\n\nFollowing pre-processing we can set to investigate the relationships among the variables. I will do this separately for binary and quantitative features, but a joint exploratory analysis is definitely possible and recommended.\n\n## Binary variables\n\nAny interdependencies among binary variables can be assessed from a [Cram\u00e9r's V](https:\/\/en.wikipedia.org\/wiki\/Cram%C3%A9r%27s_V) correlation matrix. Note that I discard the $\\chi^2$ Yates' continuity correction below, by passing `correction=False` to more rigorously fulfil unit value with self-correlation. You can see for yourself how little difference it makes if enabled.","343937f1":"First, you will note that the posterior of the intercept $\\alpha$ (also within `beta`) is shifted towards negative values. This should not be surprising since the deceased proportion in the population is 32.1%, as shown at the pre-processing stage. In simple terms, in the lack of any evidence from $X$ we have $P(Death) = \\frac{1}{1+e^{-\\alpha}}$ and hence, with negative values of $\\alpha$, $P(Death) < 0.5$ close to the sample mean.\n\nThe strongest effect over heart failure risk is `time`, the patient follow-up period. The model is quite certain that the longer the follow-up period, the more likely a patient survives. Keeping this variable is kind of cheating, don't you agree?\n\nSome other strong effects are those from serum creatine levels and ejection fraction. Large levels of serum creatine and small ejection fractions seem to associate with heart failure. Regarding these two, the dataset publication title is self-evident: *Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone*.\n\nAlso noteworthy are the differences in spread over the different posterior distributions (ignore the intercept, as it tends to be wider and hence less certain). Recalling the strange correlation between `age` and `sex` aforementioned, we can make better sense of the large spread in the corresponding posteriors, *i.e.* a large posterior variance denotes greater uncertainty from the model. The negative effect of `sex` suggests men are on average more protected agaisnt heart failure compared to women - does this make sense? On the other hand, the moderately positive effect of `smoking` suggests a general increase in risk - I was frankly expecting a much larger effect.\n\n## Predictions\n\nWe shall conclude this analysis with model predictions, but before that look into the posterior heart failure probabilities over all observations.","f2041db7":"We can see there is a satisfying separation between deceased and survivors, albeit some cases are grossly misrepresented in either group. \n\nSo, how can we determine what level of risk or probability warrants medical assistance? Prediction accuracy alone can be misleading since the majority of the observations comprises survivors. Instead, sensitivity is more appropriate to cover more potential occurrences at the comparatively unexpensive misdiagnosis of healthy individuals that do not require assistance.\n\nAs such, I suggest predicting heart failure for all individuals that have a 1% chance of their risk being over 50%. This is a conservative choice that should boost sensitivity even if decreasing specificity or accuracy. Going one step further, we can compare that 1% threshold to various others.","60b7bd84":"Using the quantitative variables alone there is a noticeable separation between deceased and survivors following PC1. We can also observe that these two PCs capture approximately 40% of the total variation contained in this feature set. On the whole, the Box-Cox transformation seems to preserve predictive information.\n\n# Bayesian model\n\nTime for setting up the Bayesian model. This part covers *i)* model structure and priors, *ii)* the Markov chain Monte Carlo (MCMC) sampling and *iii)* inference using the samples from the posterior distribution.\n\n## Model structure and priors\n\nThere is a myriad ways we can model heart failure risk using this dataset, so how exactly does one know what model works the best? Unfortunately there is no clear answer, but Bayesian model comparison helps with identifying good models and hypotheses. Here, tools in the likes of the widely applicable information criterion (WAIC) and the deviance information criteria (DIC) compare goodness-of-fit across models. By looking into how much knowledge the different models capture and the number of parameters they use (*i.e.* degrees of freedom), these criteria highlight those that explain the most with the fewest parameters. Model selection is a topic in itself and would deserve a lot more discussion than I intend to cover.\n\nInstead, I will fit a generalized linear model using all features, quantitative and binary alike, to predict the probability of  the target variable $Y$, a.k.a. `DEATH_EVENT`. This being a binary variable, the most appropriate distribution to model it is a Bernoulli parameterized by $P(Death|X)$. Our job is to build a linear model of the logit expression,\n\n$$log(\\frac{P(Death|X)}{1-P(Death|X)}) = \\alpha + \\beta_1 . X_1 + \\beta_2 . X_2 + ... \\beta_p . X_p$$\n\nextract the probability using a logistic transformation,\n\n$$P(Death | X) = \\frac{1}{1+e^{-(\\alpha + \\beta_1 . X_1 + \\beta_2 . X_2 + ... \\beta_p . X_p)}}$$\n\nlink it to our observed $Y$,\n\n$$Y \\sim \\mathcal{B}(P(Death|X))$$\n\nand finally let the Bayesian backend work out, for all $p = 12$ coefficients plus intercept $\\beta$, the famous relationship\n\n$$P(\\beta|data) = \\frac{P(data|\\beta).P(\\beta)}{P(data)} \\Leftrightarrow Posterior = \\frac{Likelihood.Prior}{Constant}$$\n\nThe prior distributions of the coefficients $\\beta$ will each have the form $\\beta \\sim \\mathcal{N}(0, 5^2)$. Centering them at $\\mu = 0$ conveys our ignorance regarding their magnitude and sign, while $\\sigma = 5$ provides sufficient spread to capture regions of high likelihood.","0072bd18":"The normalized quantitative variables seem rather uncorrelated, which is good news for Bayesian optimization. As for differences associated with smoking status, there seem to be few - if any - differences overall.\n\n## Principal Component Analysis (PCA)\n\nTo conclude this exploratory phase I suggest performing a principal component analysis (PCA) of the normalized quantitative variables and visualize the scores over the first two principal components (PCs). This should both provide us an overview over the sample composition, *e.g.* presence of outliers and help assessing whether the projections carrying the most variance relate to heart failure incidence.","5ae23784":"# Pre-processing\n\nStarting off, each of the quantitative variables present in the dataset will be transformed and standardized to approximate a standard normal distribution, *i.e.* $X \\sim \\mathcal{N}(0,1)$. This procedure brings about many advantages in the scope of a Bayesian analysis, such as\n\n- Prior distributions of the model coefficients can be shared\n- Inference becomes straightforward, *e.g.* discarding input variables defaults to the respective sample averages\n\nTo this end I decided to apply a Box-Cox transformation followed by mean-centering and unit-variance scaling to the quantitative variables, which I will heretoforth designate as normalized.\n\nRegarding the binary variables that constitute the rest of the variable set, we will instead check for near-zero variance - skewed binary variables can seriously hurt model inference and performance.","b30cb39d":"<img src=\"https:\/\/www.vippng.com\/png\/full\/234-2341597_heart-anatomical-drawing-vintage-old-heart-drawing.png\" width=\"200\">\n\n# Introduction\n\nThe purpose of this notebook is to introduce a Bayesian generalized linear model approach aiming at *i)* predicting heart failure risk and *ii)* understanding which factors contribute the most, using the Heart Failure Prediction dataset. This dataset comprises a total of 299 observations and 13 clinical covariates, including well-known risk factors.\n\nMy choice for a backend framework in this analyis was PyMC, although I would like to try Tensorflow Probability. For a practical introduction to Bayesian modelling using PyMC I recommend the book [Bayesian Methods for Hackers](https:\/\/camdavidsonpilon.github.io\/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\/); for a theoretical introduction, I suggest instead the book [Statistical Rethinking](https:\/\/xcelab.net\/rm\/statistical-rethinking\/), originally written along R examples but recently accompanied by the PyMC3 equivalent. Finally, for a in-depth introduction based on R with a behavioural ecology use-case you can also refer to my [own blog post](https:\/\/poissonisfish.com\/2019\/05\/01\/bayesian-models-in-r\/) from two years ago.\n\nI hope this use-case will convince you to consider Bayesian inference in tackling small modelling problems.","fe8cd273":"Indeed, the 1% threshold yielded the highest sensitivity (87.5%) along with a moderate specificity (72.9%). As demonstrated above, larger thresholds revert the trade-off, thereby decreasing sensitivity and increasing specificity. Accuracy can also be shown to be optimal between the two most extreme choices for threshold.\n\n\n# Conclusion\n\nMuch more could be done thoroughout this analysis. To list a few recommended steps:\n\n- Use a hierarchical model introducing a prior for the $\\beta$ prior's $\\mu$ and a separate prior for $\\alpha$\n- Resolve the dependence between `sex` and `smoking`\n- Produce ROC curve to better understand the sensitivity-specificity trade-off\n- Experiment with Cox regression for survival analysis\n\nAll in all, being able to anticipate 87.5% of the occurrences should be deemed a success. I hope this Bayesian approach made sense and that the underlying strenghts compel you to try it out in different modelling problems.\n\nOf course, I look forward to your remarks, suggestions and questions. Greetings and stay healthy!","b4b72b8e":"[](http:\/\/)From the trace plot above we can see the chains converged really quick - I suspect much because of the MAP pre-estimation. In the next section we will take a deep-dive into these 20,000 posterior samples and make sense of them.\n\n## Inference\n\nIn my perspective the beauty of the Bayesian framework lies in how uncertainty propagates throughout the model. Such models survey a comprehensive range of parameter values and combinations that may, with a certain probability, lead to the observed response variable. This is in contrast to traditional machine and deep learning approaches that rely on single point estimates instead.\n\nIn what follows we will leverage this uncertainty over unknown parameters to assess the impact of each predictor on heart failure risk. To this end we can plot the posterior distribution of each coefficient plus the intercept stored under `beta`.","1e138639":"## Sampling\n\nNow that the model is defined, we can set up the optimization procedure using MCMC. In the present case I will choose the Hamiltonian method with maximum a posteriori (MAP) estimation for starting values. The number of both burn-in samples and chains can also be specified. With the setting below we will end up with 10,000 effective samples from each of two chains, totalling 20,000. At last, a look into the sampling trace will help diagnose convergence issues."}}