{"cell_type":{"1c5d83f2":"code","80ec2c21":"code","722f030b":"code","5c1e0c05":"code","1e301cf2":"code","09ff2a04":"code","16b1b139":"code","7cd8c91d":"code","530c0221":"code","c0e51523":"code","3fc339a4":"code","aef12015":"code","e8462bcc":"code","23a0eb43":"code","1ac1a2b7":"code","fb3b7d6d":"code","5ea6e5b2":"code","becde9fa":"code","ee31d190":"markdown","87f2d312":"markdown","d96f98c5":"markdown","5c16f6c5":"markdown","07008f68":"markdown","3c8314c9":"markdown","56bf1e35":"markdown","92e2f113":"markdown"},"source":{"1c5d83f2":"import tensorflow as tf","80ec2c21":"import numpy as np\n\nimport matplotlib as mpl\n\nimport IPython.display as display\nimport PIL.Image\n\nfrom tensorflow.keras.preprocessing import image","722f030b":"url = 'https:\/\/i.pinimg.com\/originals\/ef\/49\/72\/ef49721d1c5dc5bf86419ece1db7b547.jpg'","5c1e0c05":"# Download an image and read it into a NumPy array.\ndef download(url, max_dim=None):\n  name = url.split('\/')[-1]\n  image_path = tf.keras.utils.get_file(name, origin=url)\n  img = PIL.Image.open(image_path)\n  if max_dim:\n    img.thumbnail((max_dim, max_dim))\n  return np.array(img)\n\n# Normalize an image\ndef deprocess(img):\n  img = 255*(img + 1.0)\/2.0\n  return tf.cast(img, tf.uint8)\n\n# Display an image\ndef show(img):\n  display.display(PIL.Image.fromarray(np.array(img)))\n\n\n# Downsizing the image makes it easier to work with.\noriginal_img = download(url, max_dim=500)\nshow(original_img)","1e301cf2":"base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')","09ff2a04":"# Maximize the activations of these layers\nnames = ['mixed3', 'mixed5']\nlayers = [base_model.get_layer(name).output for name in names]\n\n# Create the feature extraction model\ndream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)","16b1b139":"def calc_loss(img, model):\n  # Pass forward the image through the model to retrieve the activations.\n  # Converts the image into a batch of size 1.\n  img_batch = tf.expand_dims(img, axis=0)\n  layer_activations = model(img_batch)\n  if len(layer_activations) == 1:\n    layer_activations = [layer_activations]\n\n  losses = []\n  for act in layer_activations:\n    loss = tf.math.reduce_mean(act)\n    losses.append(loss)\n\n  return  tf.reduce_sum(losses)","7cd8c91d":"class DeepDream(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),\n        tf.TensorSpec(shape=[], dtype=tf.float32),)\n  )\n  def __call__(self, img, steps, step_size):\n      print(\"Tracing\")\n      loss = tf.constant(0.0)\n      for n in tf.range(steps):\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img`\n          # `GradientTape` only watches `tf.Variable`s by default\n          tape.watch(img)\n          loss = calc_loss(img, self.model)\n\n        # Calculate the gradient of the loss with respect to the pixels of the input image.\n        gradients = tape.gradient(loss, img)\n\n        # Normalize the gradients.\n        gradients \/= tf.math.reduce_std(gradients) + 1e-8 \n        \n        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n        # You can update the image by directly adding the gradients (because they're the same shape!)\n        img = img + gradients*step_size\n        img = tf.clip_by_value(img, -1, 1)\n\n      return loss, img","530c0221":"deepdream = DeepDream(dream_model)","c0e51523":"def run_deep_dream_simple(img, steps=100, step_size=0.01):\n  # Convert from uint8 to the range expected by the model.\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n  img = tf.convert_to_tensor(img)\n  step_size = tf.convert_to_tensor(step_size)\n  steps_remaining = steps\n  step = 0\n  while steps_remaining:\n    if steps_remaining>100:\n      run_steps = tf.constant(100)\n    else:\n      run_steps = tf.constant(steps_remaining)\n    steps_remaining -= run_steps\n    step += run_steps\n\n    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n    \n    display.clear_output(wait=True)\n    show(deprocess(img))\n    print (\"Step {}, loss {}\".format(step, loss))\n\n\n  result = deprocess(img)\n  display.clear_output(wait=True)\n  show(result)\n\n  return result","3fc339a4":"dream_img = run_deep_dream_simple(img=original_img, \n                                  steps=100, step_size=0.01)","aef12015":"import time\nstart = time.time()\n\nOCTAVE_SCALE = 1.30\n\nimg = tf.constant(np.array(original_img))\nbase_shape = tf.shape(img)[:-1]\nfloat_base_shape = tf.cast(base_shape, tf.float32)\n\nfor n in range(-2, 3):\n  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n\n  img = tf.image.resize(img, new_shape).numpy()\n\n  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n\ndisplay.clear_output(wait=True)\nimg = tf.image.resize(img, base_shape)\nimg = tf.image.convert_image_dtype(img\/255.0, dtype=tf.uint8)\nshow(img)\n\nend = time.time()\nend-start","e8462bcc":"def random_roll(img, maxroll):\n  # Randomly shift the image to avoid tiled boundaries.\n  shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n  shift_down, shift_right = shift[0],shift[1] \n  img_rolled = tf.roll(tf.roll(img, shift_right, axis=1), shift_down, axis=0)\n  return shift_down, shift_right, img_rolled","23a0eb43":"shift_down, shift_right, img_rolled = random_roll(np.array(original_img), 512)\nshow(img_rolled)","1ac1a2b7":"class TiledGradients(tf.Module):\n  def __init__(self, model):\n    self.model = model\n\n  @tf.function(\n      input_signature=(\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n        tf.TensorSpec(shape=[], dtype=tf.int32),)\n  )\n  def __call__(self, img, tile_size=512):\n    shift_down, shift_right, img_rolled = random_roll(img, tile_size)\n\n    # Initialize the image gradients to zero.\n    gradients = tf.zeros_like(img_rolled)\n    \n    # Skip the last tile, unless there's only one tile.\n    xs = tf.range(0, img_rolled.shape[0], tile_size)[:-1]\n    if not tf.cast(len(xs), bool):\n      xs = tf.constant([0])\n    ys = tf.range(0, img_rolled.shape[1], tile_size)[:-1]\n    if not tf.cast(len(ys), bool):\n      ys = tf.constant([0])\n\n    for x in xs:\n      for y in ys:\n        # Calculate the gradients for this tile.\n        with tf.GradientTape() as tape:\n          # This needs gradients relative to `img_rolled`.\n          # `GradientTape` only watches `tf.Variable`s by default.\n          tape.watch(img_rolled)\n\n          # Extract a tile out of the image.\n          img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n          loss = calc_loss(img_tile, self.model)\n\n        # Update the image gradients for this tile.\n        gradients = gradients + tape.gradient(loss, img_rolled)\n\n    # Undo the random shift applied to the image and its gradients.\n    gradients = tf.roll(tf.roll(gradients, -shift_right, axis=1), -shift_down, axis=0)\n\n    # Normalize the gradients.\n    gradients \/= tf.math.reduce_std(gradients) + 1e-8 \n\n    return gradients ","fb3b7d6d":"get_tiled_gradients = TiledGradients(dream_model)","5ea6e5b2":"def run_deep_dream_with_octaves(img, steps_per_octave=100, step_size=0.01, \n                                octaves=range(-2,3), octave_scale=1.3):\n  base_shape = tf.shape(img)\n  img = tf.keras.preprocessing.image.img_to_array(img)\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\n\n  initial_shape = img.shape[:-1]\n  img = tf.image.resize(img, initial_shape)\n  for octave in octaves:\n    # Scale the image based on the octave\n    new_size = tf.cast(tf.convert_to_tensor(base_shape[:-1]), tf.float32)*(octave_scale**octave)\n    img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n\n    for step in range(steps_per_octave):\n      gradients = get_tiled_gradients(img)\n      img = img + gradients*step_size\n      img = tf.clip_by_value(img, -1, 1)\n\n      if step % 10 == 0:\n        display.clear_output(wait=True)\n        show(deprocess(img))\n        print (\"Octave {}, Step {}\".format(octave, step))\n    \n  result = deprocess(img)\n  return result","becde9fa":"img = run_deep_dream_with_octaves(img=original_img, step_size=0.01)\n\ndisplay.clear_output(wait=True)\nimg = tf.image.resize(img, base_shape)\nimg = tf.image.convert_image_dtype(img\/255.0, dtype=tf.uint8)\nshow(img)","ee31d190":"In this notebook we will demonstrate the implementation of DeepDream program to dreamify images. The pre-trained deep convolutional neural network used in the program will learn the patterns of an image that will be visualized in the processed image. First, the input image will be passed to the InceptionNet after that the gradient of the image will be calculated with respect to the activations of a particular layer of the network. Finally, the image is modified to increase the activations, and the patterns seen by the network are enhanced. In this way, a dream-like image is created.","87f2d312":"As the resulting image is not much good and having a low resolution and noise, it will be fine-tuned using the octave that means an approach to perform the previous gradient ascent approach, then increase the size of the image and repeat this process for multiple octaves.","d96f98c5":"In the next step, the InceptionV3 model will be loaded with pre-trained ImageNet weights.","5c16f6c5":"## Choose an image to dream-ify","07008f68":"# DeepDream in Keras using TensorFlow","3c8314c9":"For this tutorial, let's use an image of a [labrador](https:\/\/commons.wikimedia.org\/wiki\/File:YellowLabradorLooking_new.jpg).","56bf1e35":"In the DeepDream, the loss is maximized (generally, it is minimized) using the gradient descent. Once the loss is obtained for the chosen layers, the gradient is calculated with respect to the images and added to the original image. Adding the gradients to the image enhances the patterns seen by the network.\n\nIn the next step, the DeepDream is defined via a class that will be instantiated to create the model.","92e2f113":"To use the InceptionV3 as a feature extractor, its layers need to be added to our model that actually perform feature extraction. In InceptionV3, there are 11 such layers out of which two are used below."}}