{"cell_type":{"9895a81e":"code","30fc67a1":"code","2ac98341":"code","db7b7e6d":"code","61f8b5e7":"code","995752e4":"code","ece12226":"code","4da75dd6":"code","ccb92cd0":"code","458568a5":"code","f6d049a9":"code","dad14061":"code","6f873c09":"code","947d3a85":"code","f7b0e4d0":"code","70746bba":"code","bd2f45ac":"code","c6e05527":"code","eaff3b40":"code","9a3f0460":"code","3ecfc90f":"code","f521141d":"code","3d3a3d5c":"code","408577ef":"code","bd423042":"code","113c6884":"code","8152ca65":"code","3ef07d0f":"code","637522e0":"code","44b808b9":"code","16d1cadf":"code","25df0a12":"code","8d83c12b":"code","1952cd51":"code","9e1d6b9d":"code","34221422":"code","a2a099bb":"code","32b72449":"code","57247b2e":"code","0b315737":"code","e3a32f06":"code","6fcc4957":"code","0894be95":"code","73395d1f":"code","459af4ec":"code","9ba1b5b2":"code","1783025a":"code","def57083":"code","f38382c2":"code","663df92c":"code","966ac4bc":"code","9954a129":"code","871dac14":"code","2d8e76af":"code","8e69277b":"code","2f852285":"code","de5b8d3e":"code","0766885f":"code","09360191":"code","79011662":"code","0b36ca34":"code","acd6ead1":"code","fc7d157b":"code","9b414b2b":"code","b0b816db":"code","13c3823a":"code","15ff54ca":"code","d3045ddb":"code","308a7754":"code","a1c08001":"code","b5194ca6":"markdown","0a30f1c5":"markdown","b341efa4":"markdown","a0e71fd0":"markdown","90185727":"markdown","94474654":"markdown","b0fb23a3":"markdown","e4303857":"markdown","dd7a82b3":"markdown","edb39195":"markdown","384e0889":"markdown","3a611609":"markdown","e8057117":"markdown","8e7b4c63":"markdown","149aeb0e":"markdown","90b3b4e4":"markdown","a59f43a3":"markdown","12d3b5d3":"markdown"},"source":{"9895a81e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport random as rnd\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy import stats\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","30fc67a1":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',na_values='?')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',na_values='?')\ncombile=[train_df, test_df]\ntest_df_2=test_df","2ac98341":"train_df.columns","db7b7e6d":"train_df.info()\nprint('-'*40)\ntest_df.info()","61f8b5e7":"columns =train_df.columns","995752e4":"null_values=pd.DataFrame(columns=['Fetures','Persent_null'])","ece12226":"for i in columns:\n    value = (train_df[i].isnull().sum()\/len(train_df[i]))*100\n    null_values.loc[len(null_values.index)] =[i,value]","4da75dd6":"plt.scatter(x='Fetures', y='Persent_null' , data=null_values, alpha=0.5)","ccb92cd0":"null_values['Fetures'][null_values['Persent_null']>40]","458568a5":"train_df=train_df.drop(null_values['Fetures'][null_values['Persent_null']>40],axis=1)\ntest_df=test_df.drop(null_values['Fetures'][null_values['Persent_null']>40],axis=1)","f6d049a9":"train_df.info()\nprint('-'*40)\ntest_df.info()","dad14061":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);","6f873c09":"corr_columns=corrmat.columns","947d3a85":"for i in corr_columns:\n    for j in corr_columns:\n        if i == j:\n            continue \n        if i == 'SalePrice' or j == 'SalePrice':\n            continue\n        if (corrmat.loc[i,j] > .5) or (corrmat.loc[i,j] < -.4):\n            print(i,\" Vs \",j,\"=\",corrmat.loc[i,j])\n            print(i,\" Vs \",'SalePrice',\"=\",corrmat.loc[i,'SalePrice'])\n            print(j,\" Vs \",'SalePrice',\"=\",corrmat.loc[j,'SalePrice'])\n            print('-'*40)","f7b0e4d0":"#train_df=train_df.drop(['YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','GarageYrBlt','GarageCars','GarageArea','BsmtUnfSF','BsmtFinSF1','1stFlrSF','HalfBath','BedroomAbvGr','2ndFlrSF','FullBath'],axis=1)\n#test_df=test_df.drop(['YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','GarageYrBlt','GarageCars','GarageArea','BsmtUnfSF','BsmtFinSF1','1stFlrSF','HalfBath','BedroomAbvGr','2ndFlrSF','FullBath'],axis=1)","70746bba":"train_df.info()\nprint('-'*40)\ntest_df.info()","bd2f45ac":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);","c6e05527":"col=test_df.columns\nprint(\"number of nulls in each feature in Train\")\nfor dataset in combile:\n    for i in col:\n        if dataset[i].isnull().sum() > 0:\n            nulls=dataset[i].isnull().sum()\n            print(i,\"==>\",nulls)\n    print('-'*40)\n    print(\"number of nulls in each feature in Test\")","eaff3b40":"train_df['LotFrontage'].describe()","9a3f0460":"#box plot overallqual\/saleprice\nvar = train_df.describe().columns\nf, ax = plt.subplots(figsize=(9, 5))\nfig = sns.boxplot(data=train_df['LotFrontage'])","3ecfc90f":"train_des=train_df.describe()\ntrain_des","f521141d":"out_index=[]","3d3a3d5c":"for i in var:\n    IQR = train_des.loc['75%',i]-train_des.loc['25%',i]\n    condition_1=train_des.loc['75%',i]+(1.5*IQR)\n    condition_2=train_des.loc['25%',i]-(1.5*IQR)\n    for j in range(len(train_df)):\n        #if (train_df.loc[j,i]>train_des.loc['max',i]) or (train_df.loc[j,i]<train_des.loc['min',i]):\n        if (train_df.loc[j,i]>condition_1) or (train_df.loc[j,i]<condition_2):\n            out_index.append(j)\n            ","408577ef":"def unique(list1):\n    x = np.array(list1)\n    print(np.unique(x))\nunique(out_index)","bd423042":"len(out_index)","113c6884":"#error=0\n#for i in out_index:\n#    try:\n#        train_df=train_df.drop(i,axis=0)\n#    except :\n#        error=error+1\n#print(error)","8152ca65":"var = train_df.describe().columns\nf, ax = plt.subplots(figsize=(20, 20))\nfig = sns.boxplot(data=train_df)\nfig.axis(ymin=0, ymax=800000);","3ef07d0f":"train_df.info()\nprint('-'*40)\ntest_df.info()","637522e0":"train_des_num=train_df.describe()\ntrain_des_cat=train_df.describe(include=['O'])","44b808b9":"train_des_num","16d1cadf":"num=train_des_num.columns\ncat=train_des_cat.columns","25df0a12":"for i in num:\n        if i=='Id':\n            continue\n        mean = train_des_num.loc['mean',i]\n        std = train_des_num.loc['std',i]\n        guess = rnd.uniform(mean - std, mean + std)\n        train_df[i]=train_df[i].fillna(guess)\nfor j in cat:\n    frq = train_des_cat.loc['top',j]\n    train_df[j]=train_df[j].fillna(frq)\n\n#---------------------------------------------------------------------------------\n\nfor i in num:\n        if (i=='Id')or (i=='SalePrice'):\n            continue\n        mean = train_des_num.loc['mean',i]\n        std = train_des_num.loc['std',i]\n        guess = rnd.uniform(mean - std, mean + std)\n        test_df[i]=test_df[i].fillna(guess)\nfor j in cat:\n    frq = train_des_cat.loc['top',j]\n    test_df[j]=test_df[j].fillna(frq)","8d83c12b":"train_df.info()\nprint('-'*40)\ntest_df.info()","1952cd51":"#train_df=pd.get_dummies(train_df,drop_first=True)\n#test_df=pd.get_dummies(test_df,drop_first=True)","9e1d6b9d":"train_df.info()\nprint('-'*40)\ntest_df.info()","34221422":"all_data = pd.concat([train_df,test_df],axis=0)","a2a099bb":"all_data","32b72449":"all_data=pd.get_dummies(all_data,drop_first=True)","57247b2e":"new_train=pd.DataFrame()\nnew_test=pd.DataFrame()","0b315737":"train_id=train_df['Id']\ntest_id=test_df['Id']\nfor i in train_id:\n    new_train = new_train.append(all_data[all_data['Id']==i], ignore_index=True)\nfor j in test_id:\n    new_test = new_test.append(all_data[all_data['Id']==j], ignore_index=True)","e3a32f06":"#train_col=train_df.columns\n#test_col=test_df.columns\n#for i in test_col:\n#    if i in train_col :\n#        continue\n#    else:\n#        test_df[i]=0","6fcc4957":"#train_col=train_df.columns\n#test_col=test_df.columns\n#for i in train_col:\n#    if i in test_col or i == 'SalePrice':\n#        continue\n#    else:\n#        train_df[i]=0","0894be95":"new_test=new_test.drop('SalePrice',axis=1)","73395d1f":"train_df=new_train\ntest_df=new_test","459af4ec":"train_df.info()\nprint('-'*40)\ntest_df.info()","9ba1b5b2":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);","1783025a":"train_df.info()","def57083":"corr_columns=corrmat.columns\nfor i in corr_columns:\n    for j in corr_columns:\n        if i == j:\n            continue \n        if i == 'SalePrice' or j == 'SalePrice':\n            continue\n        if (corrmat.loc[i,j] > .6) or (corrmat.loc[i,j] < -.6)or (corrmat.loc[i,j]==None):\n            corr_1=corrmat.loc[i,'SalePrice']\n            corr_2=corrmat.loc[j,'SalePrice']\n            if corr_1>corr_2:\n                try:\n                    train_df=train_df.drop(j,axis=1)\n                    test_df=test_df.drop(j,axis=1)\n                except:\n                    continue\n            elif corrmat.loc[i,j]==None:\n                try:\n                    train_df=train_df.drop(j,axis=1)\n                    train_df=train_df.drop(i,axis=1)\n                    test_df=test_df.drop(j,axis=1)\n                    test_df=test_df.drop(i,axis=1)\n                except:\n                    continue\n            else:\n                try:\n                    train_df=train_df.drop(i,axis=1)\n                    test_df=test_df.drop(i,axis=1)\n                except:\n                    continue","f38382c2":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);","663df92c":"train_df=train_df.drop(['BsmtHalfBath','PoolArea','3SsnPorch','LowQualFinSF','ScreenPorch','EnclosedPorch','MiscVal'],axis=1)\ntest_df=test_df.drop(['BsmtHalfBath','PoolArea','3SsnPorch','LowQualFinSF','ScreenPorch','EnclosedPorch','MiscVal'],axis=1)","966ac4bc":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);","9954a129":"train_df.info()","871dac14":"#norm_data = (train_df-train_df.min())\/(train_df.max()-train_df.min())\n#test_df=(test_df-test_df.min())\/(test_df.max()-test_df.min())","2d8e76af":"from sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(train_df.values)\nnorm_data = pd.DataFrame(scaled_features, index=train_df.index, columns=train_df.columns)\nscaled_features_t = StandardScaler().fit_transform(test_df.values)\ntest_df = pd.DataFrame(scaled_features_t, index=test_df.index, columns=test_df.columns)","8e69277b":"norm_data","2f852285":"norm_data['SalePrice']=train_df['SalePrice']","de5b8d3e":"#histogram and normal probability plot\nsns.distplot(norm_data['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(norm_data['SalePrice'], plot=plt)","0766885f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","09360191":"train, test = train_test_split(norm_data, test_size=0.3)\ntrain_x = train.drop(['Id','SalePrice'], axis=1)\ntrain_y = train['SalePrice']\n\ntest_x = test.drop(['Id','SalePrice'], axis=1)\ntest_y = test['SalePrice']\nlr = LinearRegression()\nlr.fit(train_x, train_y)\n\ny_x= lr.predict(test_x)\n\nmean_squared_error(test_y, y_x)","79011662":"y_x","0b36ca34":"R_train_x = norm_data.drop(['SalePrice','Id'], axis=1)\nR_train_y = norm_data['SalePrice']\n\nR_test_x = test_df.drop('Id', axis=1)\n","acd6ead1":"#from sklearn import decomposition\n#from sklearn import datasets\n\n## PCA decomposition\n#pca = decomposition.PCA(n_components=230) #Finds first 170 PCs\n#pca.fit(train_x)\n#plt.plot(pca.explained_variance_ratio_)\n#plt.ylabel('% of variance explained')\n#plot reaches asymptote at around 50, which is optimal number of PCs to use. \n#pca = decomposition.PCA(n_components=200) #use first 3 PCs (update to 100 later)\n#pca.fit(train_x)\n#PCtrain = pd.DataFrame(pca.transform(train_x))\n#PCtrain['SalePrice'] = train_y\n\n#decompose test data\n#pca.fit(test)\n#PCtest = pd.DataFrame(pca.transform(test))","fc7d157b":"R_test_x.info()","9b414b2b":"R_train_x.info()","b0b816db":"lr = LinearRegression()\nlr.fit(R_train_x, R_train_y)","13c3823a":"y= lr.predict(R_test_x)","15ff54ca":"submission = pd.DataFrame(columns=['Id','SalePrice'])","d3045ddb":"submission['Id']=test_df_2['Id']","308a7754":"submission['SalePrice']=y","a1c08001":"submission.to_csv('sample_submission.csv', index=False)","b5194ca6":"**{ Alley , FireplaceQu , PoolQC , Fence , MiscFeature }**\n\n**These are the most features have nulls**","0a30f1c5":"# Outliars!\n**Outliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.**\n\n**Outliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.**","b341efa4":"## This is number of nulls in each feature","a0e71fd0":"**I commented on the previous cell because I got the small error after it**\n\n**but if you need to try just remove the #**","90185727":"**After i see the correlations i found these features have strong correlations and we need to drop them**\n\n{'YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','GarageYrBlt','GarageCars','GarageArea','BsmtUnfSF','BsmtFinSF1','1stFlrSF','HalfBath','BedroomAbvGr','2ndFlrSF','FullBath'}","94474654":"# Conclusion\nThat's it! We reached the end of our exercise.\n\nThroughout this kernel we put in practice many of the strategies . We philosophied about the variables, we analysed 'SalePrice' alone and with the most correlated variables, we dealt with missing data and outliers, we tested some of the fundamental statistical assumptions and we even transformed categorial variables into dummy variables. That's a lot of work that Python helped us make easier.\n","b0fb23a3":"## we have 4 data frames \n1. train_df contains the train data\n\n2. test_df contains the test data\n\n3. combile contains train_df and test_df \n\n**in case we have to deal with these two datasets together**\n\n4. test_df_ it a copy of test_df and we will need it in the last","e4303857":"Completing a numerical continuous feature\nNow we should start estimating and completing features with missing or null values. \n\nWe can consider three methods to complete a numerical continuous feature.\n\nA simple way is to generate random numbers between mean and standard deviation.\n\nMore accurate way of guessing missing values is to use other correlated features. In our case we note correlation . \n\nCombine methods 1 and 2. So instead of guessing null values based on median, use random numbers between mean and standard deviation.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","dd7a82b3":"## we need to delete these features because they have many nulls and we can't deal with ","edb39195":"**But the figure is so large so if I see each feature I will be blinded**\n\n**so I print the correlation between each feature and correlation between those and SalePrice**","384e0889":"#### We read it simply","3a611609":"# Nulls\n## Now we need to find which features have nulls and the number of nulls in each feature","e8057117":"# House Price Problem\n## in this notebook, we discuss how to deal with data in many steps with simple code implementation\n**How to deal with these problems:**\n1. Nulls\n2. Outliers\n3. Correlations\n#### If the feature has normal distribution or not and if not how to deal with it","8e7b4c63":"**This is so many nulls so note there are 5 features that have many nulls**\n\n**so what are these features?**","149aeb0e":"# Linear regression can't deal with categorical data \n## so now I convert it to numerical data","90b3b4e4":"# Correlations:\n### we need to see the correlations ","a59f43a3":"# First We need to Read data and save it in dataframe","12d3b5d3":"# This cell below dorps the outliers but I commented on it because when I run submission with this I got bad accuracy \n\n# so if you want to try, remove # and go ahead"}}