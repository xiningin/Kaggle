{"cell_type":{"94b7eea2":"code","ccd1de5b":"code","bfeeb4a5":"code","02463a72":"code","879b8f6e":"code","7923740f":"code","93f81384":"code","77539218":"code","d652dc17":"code","ac0a84cf":"code","d66bec35":"code","895fce9b":"code","5f643ec6":"markdown","427bd5a9":"markdown"},"source":{"94b7eea2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccd1de5b":"import PIL\nimport PIL.Image\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, optimizers, metrics, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bfeeb4a5":"IMSIZE = 28\nNUM_CLASSES = 10\nNFILTERS = 16","02463a72":"train_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nX, y = train_data.drop('label', axis=1).values, train_data['label'].values\nX = (X - 127.5) \/ 127.5\nX = X.reshape(-1, IMSIZE, IMSIZE, 1)\n\n#y = np.zeros((y_sparse.shape[0], NUM_CLASSES))\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=0)\n\ntest_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nX_test = (test_data.values - 127.5) \/ 127.5\nX_test = X_test.reshape(-1, IMSIZE, IMSIZE, 1)\n\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape,  X_test.shape)","879b8f6e":"def to_dataset(X, y):\n    ds = tf.data.Dataset.from_tensor_slices((X,y))\n    ds = ds.cache().shuffle(X.shape[0] + 1).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ntrain_ds = to_dataset(X_train, y_train)\nval_ds = to_dataset(X_val, y_val)","7923740f":"NBLOCKS = 3\nNLAYERS = 3\ndef create_model():\n    inputs = layers.Input(shape=(IMSIZE,IMSIZE,1))\n    x = inputs\n    \n    for block in range(3):\n        for layer in range(3):\n            x = layers.Conv2D(NFILTERS,(3,3),padding=\"same\",activation=\"relu\")(x)\n    if(block < NBLOCKS - 1):\n        x = layers.MaxPooling2D((2,2))(x)\n        \n    x = layers.Flatten()(x)\n    x = layers.Dense(32, activation='relu')(x)\n    x = layers.Dense(16, activation='relu')(x)\n    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n    model = models.Model(inputs=inputs, outputs=outputs)\n    loss = losses.SparseCategoricalCrossentropy(from_logits=False)\n    metric = [metrics.SparseCategoricalAccuracy()]\n    optimizer = optimizers.Adam()\n    model.compile(loss=loss, optimizer=optimizer, metrics=metric)\n    return model\nmodel = create_model()\nmodel.summary()","93f81384":"reducer = callbacks.ReduceLROnPlateau(monior='val_loss', factor=0.5, patience=3, mode='min', cooldown=1)\nstopper = callbacks.EarlyStopping(monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\nhist = model.fit(train_ds,\n                 validation_data=val_ds,\n                 callbacks=[stopper, reducer],\n                 epochs=100)","77539218":"model.evaluate(val_ds)","d652dc17":"plt.figure(figsize=(12,12), tight_layout=True)\n\nplt.subplot(311)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper right')\n\nplt.subplot(312)\nplt.plot(hist.history['sparse_categorical_accuracy'])\nplt.plot(hist.history['val_sparse_categorical_accuracy'])\nplt.title('Model Categorical Accuracy')\nplt.ylabel('Categorical Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\n\nplt.subplot(313)\nplt.plot(hist.history['lr'])\nplt.title('Learining Rate')\nplt.ylabel('Learning Rate')\nplt.xlabel('Epoch')\n\nplt.show()","ac0a84cf":"predictions = model.predict(X_val)\ncf = confusion_matrix(y_val, np.argmax(predictions, axis=-1))\ncf","d66bec35":"class_names = [x for x in range(1, 10, 1)]\nplt.figure(figsize=(12,12))\nsns.heatmap(cf, annot=True, xticklabels=class_names, yticklabels=class_names, cmap='Blues', robust=True)\nplt.show()","895fce9b":"test_predictions = np.argmax(model.predict(X_test), axis=-1)\noutput = pd.DataFrame(data={\"ImageId\":[x + 1 for x in range(test_predictions.shape[0])], \"Label\":test_predictions})\noutput.to_csv(\"mnist_data_classifications.csv\", index=False, quoting=3)","5f643ec6":"# My imports","427bd5a9":"# Load in the data"}}