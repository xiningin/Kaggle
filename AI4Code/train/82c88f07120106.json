{"cell_type":{"7afc4d6a":"code","c532354b":"code","cb844bc9":"code","db2fad27":"code","c63c8022":"code","14d73755":"code","d27ba54e":"code","dd1078f7":"code","5d505662":"code","ce5da39a":"code","db1c030e":"code","999b21b3":"code","be09d538":"code","b98262f7":"code","0952d018":"code","89021d7b":"code","b1fbbc66":"code","367cc728":"code","55f4a63b":"code","276e215a":"code","52cc37f1":"code","c2b2a9de":"code","61f50fd8":"code","3a22f5df":"code","9dd45d55":"code","53e2ee89":"code","66d3fec6":"code","2d64cf0e":"code","cde41693":"code","3a172595":"code","aac2a128":"code","985d5178":"code","2da64cc4":"code","1e6f1227":"code","b0955e62":"markdown","84e84d04":"markdown","05d56af2":"markdown"},"source":{"7afc4d6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c532354b":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 400\nimport seaborn as sns #plotting package\nimport graphviz #to visualize decision trees\nfrom sklearn import datasets\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","cb844bc9":"df.shape","db2fad27":"df.columns = ['age','sex', 'chest_pain', 'resting_blood_pressure', 'cholestoral', 'fasting_blood_sugar', 'resting_ECG', 'max_heart_rate', 'angina_from_exercise',\n          'st_depression', 'st_slope','major_vessels_with_flourosopy','thalassemia','target']\n","c63c8022":"df.head()","14d73755":"df.describe()","d27ba54e":"for i, col in enumerate(df.columns):\n    plt.figure(i)\n    sns.distplot(df[col])","dd1078f7":"features_response = df.columns.tolist()\nfeatures_response","5d505662":"corr = df[features_response].corr()","ce5da39a":"sns.heatmap(corr,xticklabels = corr.columns.values,\n           yticklabels = corr.columns.values, center = 0)","db1c030e":"X = df[features_response].iloc[:,:-1].values\ny = df[features_response].iloc[:,-1].values\nprint(X.shape, y.shape)","999b21b3":"from sklearn.feature_selection import f_classif\n[f_stat, f_p_value] = f_classif(X,y)\nf_test_df = pd.DataFrame({'Feature': features_response[:-1],\n                         'F statistic': f_stat,\n                         'p value': f_p_value})\nf_test_df.sort_values('p value')","be09d538":"#Determining the best features from statistics using 80th percentile and above\nfrom sklearn.feature_selection import SelectPercentile\nselector = SelectPercentile(f_classif, percentile = 20)\nselector.fit(X,y)\nbest_feature_ix = selector.get_support()\nbest_feature_ix\nfeatures = features_response[:-1]\nbest_features = [features[counter] for counter in range(len(features))\n                if best_feature_ix[counter]]\nbest_features","b98262f7":"labels = np.array(df['target'])\nfeatures = df.drop('target', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)","0952d018":"#Using logisitc regression with 'angina' to determine a heart disease patient or not\nfrom sklearn.linear_model import LogisticRegression\nmy_lr = LogisticRegression() #Binary classifier\nmy_lr.C = 0.1\nmy_lr.solver = 'liblinear'\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\ndf['angina_from_exercise'].values.reshape(-1,1), df['target'].values, test_size =0.2, random_state=24)\nprint('The mean values of the train predictions are {}, \\nThe mean values of the testing predictions are {}.'.format(np.mean(y_train), np.mean(y_test)))","89021d7b":"my_lr.fit(X_train, y_train)\ny_pred = my_lr.predict(X_test)\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)","b1fbbc66":"metrics.confusion_matrix(y_test, y_pred)","367cc728":"y_pred_proba = my_lr.predict_proba(X_test) #obtaining predicted probabilites\npos_proba = y_pred_proba[:,1] #putting second column of predicted probabilites into an array\n#plot an roc auc curve\nfpr,tpr, thresholds = metrics.roc_curve(y_test, pos_proba)\nplt.plot(fpr,tpr,'*-')\nplt.plot([0,1],[0,1],'r--')\nplt.legend(['Logistic regression','Random chance'])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')","55f4a63b":"metrics.roc_auc_score(y_test, pos_proba)","276e215a":"#Using a decision tree classifier to decide variable importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nXA_train, XA_test, ya_train, ya_test = \\\ntrain_test_split(df[features_response[:-1]].values, df['target'].values, \n                test_size=0.2, random_state = 24)","52cc37f1":"dt = tree.DecisionTreeClassifier(max_depth = 4)\ndt.fit(XA_train, ya_train)\ndot_data = tree.export_graphviz(dt, out_file = None, filled = True, \n                               rounded= True, feature_names = features_response[:-1],\n                                proportion=True, class_names = ['Not Heart Disease', 'Heart Disease'])\ngraph = graphviz.Source(dot_data)\ngraph","c2b2a9de":"ya_pred = dt.predict(XA_test)\nconfusionmatrix = metrics.confusion_matrix(ya_test, ya_pred)\nconfusionmatrix","61f50fd8":"#plot an roc auc curve\nfpr,tpr, thresholds = metrics.roc_curve(ya_test, ya_pred)\nplt.plot(fpr,tpr,'*-')\nplt.plot([0,1],[0,1],'r--')\nplt.legend(['Logistic regression','Random chance'])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nacc = metrics.accuracy_score(ya_test,ya_pred)\nprint('The Decision Trees accuracy is {}'.format(acc))","3a22f5df":"P = sum(ya_test)\nTP = sum((ya_test==1) & ya_pred==1)\nFN = sum((ya_test==1) & (ya_pred==0))\nN = sum(ya_test ==0)\nTN = sum((ya_test==0)&(ya_pred==0))\nFP = sum((ya_test==0) & (ya_pred==1))\nSE = TP\/(TP+FN)\nSP= TN\/(TN+FP)\nprint('The sensitivity is {} and the the specificity is {}'.format(SE,SP))","9dd45d55":"from sklearn.tree import export_graphviz\n\nvar_importance = list(dt.feature_importances_)\nfeature_importances1 = [(features, round(importance,2)) for features, importance in zip(feature_list, var_importance)]\n\n\nfeature_importances1 = sorted(feature_importances1, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances1];","53e2ee89":"#Plotting variable importance\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(var_importance)))\n# Make a bar chart\nplt.bar(x_values, var_importance, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n","66d3fec6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","2d64cf0e":"# Create a random forest Classifier. By convention, clf means 'Classifier'\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier to take the training features and learn how they relate\n# to the training y (the species)\nclf.fit(XA_train, ya_train)","cde41693":"rf_pred = clf.predict(XA_test)\n\nacc2 = metrics.accuracy_score(ya_test,rf_pred)\nprint('The Decision Trees accuracy is {}'.format(acc2))","3a172595":"n_nodes = []\nmax_depths = []\n\n# Stats about the trees in random forest\nfor ind_tree in clf.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint(f'Average number of nodes {int(np.mean(n_nodes))}')\nprint(f'Average maximum depth {int(np.mean(max_depths))}')","aac2a128":"from sklearn.tree import export_graphviz\n\nvar_importance1 = list(clf.feature_importances_)\nfeature_importances2 = [(features, round(importance,2)) for features, importance in zip(feature_list, var_importance1)]\n\n\nfeature_importances2 = sorted(feature_importances2, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances2];","985d5178":"#Plotting variable importance\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(var_importance1)))\n# Make a bar chart\nplt.bar(x_values, var_importance1, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n","2da64cc4":"features_response = df.columns.tolist()\n\nitems_to_remove = ['age', 'cholestoral', 'st_slope','max_heart_rate','sex', 'resting_blood_pressure', 'fasting_blood_sugar', 'resting_ECG','angina_from_exercise','target']\n\nfeatures_response = [item for item in features_response if item not in items_to_remove]\nfeatures_response\n","1e6f1227":"newXA_train, newXA_test, newya_train, newya_test = \\\ntrain_test_split(df[features_response[:-1]].values, df['target'].values, \n                test_size=0.2, random_state = 24)","b0955e62":"Using a random forest classifier for the decision tree","84e84d04":"**Creating the decision tree**","05d56af2":"age: age in years \n\nsex: (1 = male; 0 = female) \n\ncp: chest pain type \n\ntrestbps: resting blood pressure (in mm Hg on admission to the hospital) \n\nchol: serum cholestoral in mg\/dl \n\nfbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n\nrestecg: resting electrocardiographic results \n\nthalach: maximum heart rate achieved \n\nexang: exercise induced angina (1 = yes; 0 = no) \n\noldpeak: ST depression induced by exercise relative to rest \n\nslope: the slope of the peak exercise ST segment \n\nca: number of major vessels (0-3) colored by flourosopy \n\nthal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n\ntarget: 1 or 0"}}