{"cell_type":{"c5840991":"code","db765ee1":"code","e8d2c08a":"code","832d0f4b":"code","41846661":"code","1b80048c":"code","7625a599":"code","aba6c802":"code","aa8de052":"code","9b817d0c":"code","ee01aea0":"code","be0e114a":"code","232a4826":"code","cd0249e7":"code","ea427cc2":"code","2c624c90":"markdown","44adc5c0":"markdown","d284ab0b":"markdown","feea220f":"markdown","3d812bda":"markdown","b2ff3008":"markdown","b43a67ec":"markdown","87c34025":"markdown","260381b3":"markdown"},"source":{"c5840991":"!git clone https:\/\/github.com\/tensorflow\/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout ac8d06519","db765ee1":"%cd models\/research\n\n# Compile protos.\n!protoc object_detection\/protos\/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow and keras to 2.4.1 to be compatible with the current Kaggle TPU coordinator version\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection\/packages\/tf2\/setup.py .\n!wget -O setup.py https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup_tf27.py\n!pip install -q --user .\n\n# Test if the Object Dectection API is working correctly\n!python object_detection\/builders\/model_builder_tf2_test.py\n\n# Upgrade tensorflow_gcs_config to the same version as Tensorflow.\n!pip install tensorflow_gcs_config==2.7.0\n\n%cd ..\/..","e8d2c08a":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","832d0f4b":"# The notebook is supposed to run with TF 2.4.1\n!pip install -q --user cloud_tpu_client\nfrom cloud_tpu_client import Client\n\nc = Client()\nc.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n\nprint(tf.__version__)\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","41846661":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)\nuser_secrets.set_gcloud_credentials()","1b80048c":"# # Load the credentials from Kaggle Secrets.\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n\n# # Activate the service account stored in the Kaggle Secrets to access Cloud Storage\n# with open('\/tmp\/service_account.json', 'w') as f:\n#     f.write(user_secrets.get_secret('service_account_json'))\n# !gcloud auth activate-service-account --key-file=\/tmp\/service_account.json --no-user-output-enabled","7625a599":"# Create a folder on Google Cloud Storage to store the checkpoints.\nimport pytz\nfrom datetime import datetime\nJST = pytz.timezone('Asia\/Tokyo')\nutc_dt = datetime.now()\nFOLDER_NAME = utc_dt.astimezone(JST).strftime(\"%Y%m%d-%H%M\")\nBUCKET_NAME = user_secrets.get_secret('gcs_bucket_name')\nGCS_OUTPUT_PATH = f\"gs:\/\/{BUCKET_NAME}\/kaggle\/{FOLDER_NAME}\"\nprint(GCS_OUTPUT_PATH)","aba6c802":"# Load the path to the preprocessed dataset.\nfrom kaggle_datasets import KaggleDatasets\nGCS_TFRECORD_BUCKET_PATH = KaggleDatasets().get_gcs_path('crown-of-thorns-starfish-dataset-in-tfrecord')\n# GCS_TFRECORD_BUCKET_PATH='gs:\/\/cots_data_public'","aa8de052":"from string import Template\n\nconfig_file_template = \"\"\"\nmodel {\n  ssd {\n    num_classes: 1\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n      }\n    }\n    feature_extractor {\n      type: \"ssd_efficientnet-b4_bifpn_keras\"\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 3.9999998989515007e-05\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.029999999329447746\n          }\n        }\n        activation: SWISH\n        batch_norm {\n          decay: 0.9900000095367432\n          scale: true\n          epsilon: 0.0010000000474974513\n        }\n        force_use_bias: true\n      }\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 7\n        num_filters: 224\n      }\n    }\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 1.0\n        x_scale: 1.0\n        height_scale: 1.0\n        width_scale: 1.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 3.9999998989515007e-05\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.009999999776482582\n            }\n          }\n          activation: SWISH\n          batch_norm {\n            decay: 0.9900000095367432\n            scale: true\n            epsilon: 0.0010000000474974513\n          }\n          force_use_bias: true\n        }\n        depth: 224\n        num_layers_before_predictor: 4\n        kernel_size: 3\n        class_prediction_bias_init: -4.599999904632568\n        use_depthwise: true\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 3\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 9.99999993922529e-09\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          gamma: 1.5\n          alpha: 0.25\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    add_background_class: false\n  }\n}\ntrain_config {\n  batch_size: $batch_size\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.007999999821186066\n          total_steps: $training_steps\n          warmup_learning_rate: 0.00050000000474974513\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.8999999761581421\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"$fine_tune_checkpoint\"\n  num_steps: $training_steps\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\"\n  use_bfloat16: true\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader: {\n  label_map_path: \"$label_map_path\"\n  tf_record_input_reader {\n    input_path: \"$train_input_path\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1;\n}\n\neval_input_reader: {\n  label_map_path: \"$label_map_path\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"$val_input_path\"\n  }\n}\n\"\"\"","9b817d0c":"# Define the training pipeline\n\nBATCH_SIZE=16\nTRAINING_STEPS = 5000\nWARMUP_STEPS = 500\nPIPELINE_CONFIG_PATH='pipeline.config'\n\nGCS_TFRECORD_TRAIN_PATH = GCS_TFRECORD_BUCKET_PATH + '\/tfrecord\/cots_train-?????-of-00008'\nGCS_TFRECORD_VAL_PATH = GCS_TFRECORD_BUCKET_PATH + '\/tfrecord\/cots_val-?????-of-00008'\nGCS_PRETRAINED_CHECKPOINT_PATH = GCS_TFRECORD_BUCKET_PATH + '\/efficientdet_d4_coco17_tpu-32\/checkpoint\/ckpt-0'\nGCS_LABEL_MAP_PATH = GCS_TFRECORD_BUCKET_PATH + '\/label_map.pbtxt'\n\npipeline = Template(config_file_template).substitute(\n    batch_size=BATCH_SIZE,\n    training_steps=TRAINING_STEPS, \n    warmup_steps=WARMUP_STEPS,\n    label_map_path=GCS_LABEL_MAP_PATH,\n    train_input_path=GCS_TFRECORD_TRAIN_PATH,\n    val_input_path=GCS_TFRECORD_VAL_PATH,\n    fine_tune_checkpoint=GCS_PRETRAINED_CHECKPOINT_PATH\n)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)\n\nMODEL_DIR=GCS_OUTPUT_PATH + '\/cots_efficientdet_d4'","ee01aea0":"!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --use_tpu=true \\\n    --alsologtostderr","be0e114a":"!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","232a4826":"!python models\/research\/object_detection\/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_dir={MODEL_DIR} \\\n    --output_directory=output","cd0249e7":"!ls output","ea427cc2":"# Remove data downloaded during training.\n!rm -rf models","2c624c90":"# Evaluate the object detection model","44adc5c0":"# Prepare the training dataset","d284ab0b":"# Install TensorFlow Object Detection API\n\nPip may report some dependency errors. You can safely ignore these errors and proceed if all tests in `model_builder_tf2_test.py` passed. ","feea220f":"# Clean up","3d812bda":"# Import dependencies","b2ff3008":"See this [notebook](https:\/\/www.kaggle.com\/khanhlvg\/tensorflow-prepare-cots-dataset-for-tpu\/) to learn how to convert the training images to the TFRecord format required by TensorFlow Object Detection API. We'll use the output of the conversion notebook here and start training a model.","b43a67ec":"# This notebook contains code to train a crown-of-thorns starfish (COTS) detection model to serve as a baseline model for [this competition](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/overview). We use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) to apply transfer learning on an [EfficientDet-D4](https:\/\/arxiv.org\/abs\/1911.09070) pretrained model. ","87c34025":"# Train an object detection model\n\nWe'll use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) and an EfficientDet-D0 base model and apply transfer learning to train a COTS detection model. EfficientDet-D0 is the smallest model in the EfficientDet model family and we pick it to reduce training time for demonstration purpose. You can probably increase accuracy by switch to using a larger EfficientDet model.","260381b3":"# Export as SavedModel for inference"}}