{"cell_type":{"c4528732":"code","23bb35d4":"code","c30f91b4":"code","7fa3c14f":"code","c4a349d0":"markdown","2db5c50d":"markdown","f0ddac61":"markdown"},"source":{"c4528732":"import pandas as pd","23bb35d4":"data0 = pd.read_csv('..\/input\/30-days-xgboost-with-5-folds-eda-gpu\/submission.csv')\ndata1 = pd.read_csv('..\/input\/30dml-catboost\/submission.csv')\ndata2 = pd.read_csv('..\/input\/30daysmlxgbrandautoeda\/xgb_submission_k.csv')\ndata3 = pd.read_csv('..\/input\/30dml-lightgbm\/submission_lgb_5.csv')\ndata4 = pd.read_csv('..\/input\/30-days-kfold-xgboost\/submission_3.csv')\ndata5 = pd.read_csv('..\/input\/30-days-of-ml-v1\/submission.csv')\ndata6 = pd.read_csv('..\/input\/lightgbm-with-feature-engineering\/submission.csv')\ndata7 = pd.read_csv('..\/input\/30daysmlxgbrandautoeda\/xgb_submission_k.csv')\ndata8 = pd.read_csv('..\/input\/voting-regression\/submission.csv')\ndata9 = pd.read_csv('..\/input\/30-days-ensemble\/1.csv')\ndata10 = pd.read_csv('..\/input\/30-days-ensemble\/2.csv')\ndata11 = pd.read_csv('..\/input\/30-days-ensemble\/3.csv')\n\ndata = [data0, data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11]\ndata11.head()","c30f91b4":"# Assigning higher weights to the better scores and trying several values, tone of the best seems to be:\n#weights = [0, 0, 0, 0.05, 0.25, 0, 0, 0.7] \nweights = [0, 0, 0, 0, 0, 0.2, 0, 0.4, 0, 0.4, 0, 0]\nlen(weights)","7fa3c14f":"data_final = pd.concat([w*x.target for x, w in zip(data, weights)], axis=1).sum(axis=1)\ndata_final.name='target'\ndata_final = pd.concat([data[0]['id'], data_final], axis=1)\ndata_final.to_csv(\"submission.csv\", index=False)\ndata_final.head()","c4a349d0":"### Taking the result from open source code:\n- https:\/\/www.kaggle.com\/rishirajacharya\/30-days-xgboost-with-5-folds-eda-gpu\n- https:\/\/www.kaggle.com\/maximkazantsev\/30dml-eda-simple-catboost\n- https:\/\/www.kaggle.com\/mohammadghanaym\/eda-xgboost-hyperparameters-tuning\n- https:\/\/www.kaggle.com\/ivezga\/lightgbm\n- https:\/\/www.kaggle.com\/miguelquiceno\/30-days-kfold-xgboost\n- https:\/\/www.kaggle.com\/seraphwedd18\/lightgbm-with-feature-engineering\n- https:\/\/www.kaggle.com\/boneacrabonjac\/30-days-ml-xgbr-and-auto-eda\n- https:\/\/www.kaggle.com\/kennethquisado\/voting-regression-xgb-lightgbm-catboost\/output?select=submission.csv\n- https:\/\/www.kaggle.com\/rishirajacharya\/30-days-ensemble","2db5c50d":"#### The weight values assigned are hyperparameters that should be tuned manually\n#### For this, each model's scores can be compared. Note, that it is better to use validation score as this avoids overfitting in the public test dataset\n\n* data0 public score -> 0.71946\n* data1 public score -> 0.71952\n* data2 public score -> 0.71907\n* data3 public score -> 0.71865\n* data4 public score -> 0.71789\n* data5 public score -> 0.71952\n* data6 public score -> 0.71890\n* data7 public score -> 0.71769\n* data8 public score -> 0.71867\n* data9 oublic score -> 0.71759\n* data10 oublic score -> 0.71765\n* data11 oublic score -> 0.71845","f0ddac61":"## Please, vote if it's useful\n## In the ensemble:\n* train several models on the same dataset \n* assign weight value to each according to its importance \n* use weighted average of all predictions to make the final prediction"}}