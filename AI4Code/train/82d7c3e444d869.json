{"cell_type":{"352e21a9":"code","239d75f6":"code","73ba54f2":"code","754e3e01":"code","41bc1444":"code","a72deb08":"code","9d755a90":"code","83df6856":"code","0e26846c":"code","a07410d4":"code","2721b8b8":"code","7b8ba77a":"code","b7648af3":"code","facd6832":"code","2f02d37f":"code","7687f4a0":"code","541e7590":"code","f7c24ff3":"code","726aa820":"code","fc90f068":"code","eb513dff":"code","38165def":"code","e2a87853":"code","edd478b8":"code","775e88fb":"code","40f97941":"code","e9cd5720":"code","405325bd":"code","5773cc88":"code","e544f6a4":"code","35213d3b":"code","9f919686":"code","73288e93":"code","84187da5":"code","7e39cd82":"code","a37553ca":"code","fce94e55":"code","8dae342a":"code","683ef992":"code","c039defe":"code","ebd2cc53":"code","51815280":"code","4af0c2dd":"code","741004a1":"markdown","de46438b":"markdown","30ca1130":"markdown","95ae7e25":"markdown","3e290917":"markdown","42423704":"markdown","d1760720":"markdown","88a907b3":"markdown","dbc68ff1":"markdown","2b784b15":"markdown","43f0d61c":"markdown","3330fb1b":"markdown","d69fda53":"markdown","bbc90462":"markdown","04f02574":"markdown","97f3b288":"markdown","bd533f04":"markdown","db742c81":"markdown","dd802ef0":"markdown","dbda1aec":"markdown","cdf5c494":"markdown","91f681b0":"markdown","54d64157":"markdown","00691b14":"markdown","6419dc10":"markdown","aec5516b":"markdown","ed6ff242":"markdown","0393d8c3":"markdown","2089828b":"markdown","92f7df1d":"markdown","48389ffb":"markdown","36ecb6aa":"markdown","a09a25ac":"markdown","ca495e54":"markdown","88f1d097":"markdown","32b8f2f7":"markdown","be8b0612":"markdown","ca62a201":"markdown"},"source":{"352e21a9":"import numpy as np              # for working with numerical data in Python\nimport pandas as pd             # for data manipulation and analysis\nimport matplotlib.pyplot as plt # loading allies as plt for plotting \nfrom sklearn import preprocessing  # importing preprossing to clean and process our data\nfrom sklearn.model_selection import train_test_split # loading to split our data into training and testing\nfrom lightgbm import LGBMRegressor # loading gradient boosting regression for implementation\nfrom xgboost import XGBRegressor  # loading XG boosting algorithm for accuracy comparision with lgbm\nimport sklearn.metrics as metrics # loading algebric matrics processor\nimport math # to do some descriptive statistics","239d75f6":"test = pd.read_csv(\"..\/input\/datafiles\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/datafiles\/train.csv\")\n#Creating a copy of the train and test datasets\nCopy_test  = test.copy()\nCopy_train  = train.copy()","73ba54f2":"Copy_train.head()","754e3e01":"Copy_test.head()","41bc1444":"Copy_train['train']  = 1\nCopy_test['train']  = 0\ndf = pd.concat([Copy_train, Copy_test], axis=0,sort=False)","a72deb08":"NAN = [(c, df[c].isna().mean()*100) for c in df]                    # In this code we are extracting out NAN values by using .isna() function and calculating mean \nNAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])      # In this code, we are preparing a datasframe with the NAN values.","9d755a90":"NAN = NAN[NAN.percentage > 50] # extracting columns in which null values are more than 50% of whole dataset.\nNAN.sort_values(\"percentage\", ascending=False) # as we have used pd.DataFrame...we can use sort_values module in that...hence extracting out sorted NAN values","83df6856":"#Drop PoolQC, MiscFeature, Alley and Fence features\ndf = df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)","0e26846c":"object_columns_df = df.select_dtypes(include=['object'])\nnumerical_columns_df =df.select_dtypes(exclude=['object'])","a07410d4":"object_columns_df.dtypes","2721b8b8":"numerical_columns_df.dtypes","7b8ba77a":"null_counts = object_columns_df.isnull().sum()  # Number of null values in each feature\nprint(\"Number of null values in each column:\\n{}\".format(null_counts)) # using print formatting feature ... printing those values","b7648af3":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\nobject_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')","facd6832":"columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n\n#fill missing values for each column (using its own most frequent value)\nobject_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0])","2f02d37f":"# Calculating Number of null values in each feature\n\nnull_counts = numerical_columns_df.isnull().sum()\n\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","7687f4a0":"print((numerical_columns_df['YrSold']- numerical_columns_df['YearBuilt']).median())\nprint(numerical_columns_df[\"LotFrontage\"].median())","541e7590":"numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)\nnumerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)","f7c24ff3":"numerical_columns_df= numerical_columns_df.fillna(0)","726aa820":"object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Utilities'].value_counts() ","fc90f068":"object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Street'].value_counts() ","eb513dff":"object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Condition2'].value_counts() ","38165def":"object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['RoofMatl'].value_counts() ","e2a87853":"object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Heating'].value_counts() #======> Drop feature one Type","edd478b8":"object_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)","775e88fb":"numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","40f97941":"Negatif = numerical_columns_df[numerical_columns_df['Age_House'] < 0]\nNegatif","e9cd5720":"numerical_columns_df.loc[numerical_columns_df['YrSold'] < numerical_columns_df['YearBuilt'],'YrSold' ] = 2009\nnumerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()","405325bd":"numerical_columns_df['TotalBsmtBath'] = numerical_columns_df['BsmtFullBath'] + numerical_columns_df['BsmtFullBath']*0.5\nnumerical_columns_df['TotalBath'] = numerical_columns_df['FullBath'] + numerical_columns_df['HalfBath']*0.5 \nnumerical_columns_df['TotalSA']=numerical_columns_df['TotalBsmtSF'] + numerical_columns_df['1stFlrSF'] + numerical_columns_df['2ndFlrSF']","5773cc88":"numerical_columns_df.head()","e544f6a4":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\"None\" : 0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6\n            }\nobject_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)\nobject_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)\nobject_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)\nobject_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)\nobject_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)\nobject_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)\nobject_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)\nobject_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)\nobject_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)\nobject_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)\nobject_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)\nobject_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)\nobject_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)\nobject_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\nobject_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)\n\n","35213d3b":"#Select categorical features\nrest_object_columns = object_columns_df.select_dtypes(include=['object'])\n#Using One hot encoder\nobject_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) \n","9f919686":"object_columns_df.head()","73288e93":"df_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)\ndf_final.head()","84187da5":"df_final = df_final.drop(['Id',],axis=1)\n\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train',],axis=1)\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train',],axis=1)\n","7e39cd82":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","a37553ca":"x_train,x_test,y_train,y_test = train_test_split(df_train,target,test_size=0.33,random_state=0)","fce94e55":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\nlgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=12000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       )","8dae342a":"#Fitting\nxgb.fit(x_train, y_train)\nlgbm.fit(x_train, y_train,eval_metric='rmse')","683ef992":"predict1 = xgb.predict(x_test)\npredict = lgbm.predict(x_test)","c039defe":"print('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict))))","ebd2cc53":"xgb.fit(df_train, target)\nlgbm.fit(df_train, target,eval_metric='rmse')","51815280":"predict4 = lgbm.predict(df_test)\npredict3 = xgb.predict(df_test)\npredict_y = ( predict3*0.45 + predict4 * 0.55)\n\nimport matplotlib\nprint(predict4,predict_y)\nplt.plot(predict4,predict3)","4af0c2dd":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predict_y\n    })\nsubmission.to_csv('submission.csv', index=False)","741004a1":" <font color='black'> \n* TotalBsmtBath : Sum of :\nBsmtFullBath and  1\/2 BsmtHalfBath\n\n* TotalBath : Sum of :\nFullBath and 1\/2 HalfBath\n\n* TotalSA : Sum of : \n1stFlrSF and 2ndFlrSF and basement area\n<\/font>\n\n\n\n","de46438b":"Full Dataset contains 2919 Rows and 81 Columns. Excluding ID AND SalePrice we have 79 FEATURES in the dataset.\n\nData is divided into 2 main parts:\nTraining data and Testing Data\n\nTraining data contains 1460 rows and 81 columns: excluding one row of names assigned for every column. \nTest data contains 1459 rows and 80 columns: excluding one row for column names and one column data as SalePrice.\n\nIn train dataset: First Column name is ID and Last Column name is SalePrice\nIn test dataset : First Column name is ID and SalePrice data is not give.\nSo we have to predict SalePrice of every test row by using training datasest.\n\n","30ca1130":"<font color='black'>\n            Checking Percentage of NAN values as it will tell us data quality. <\/font>","95ae7e25":"\n* <font color='black'>  Will we use One hot encoder to encode the rest of categorical features  <font>","3e290917":"# <font color='Green'> 1. Understanding the Problem <\/font>","42423704":"#  <font color='Brown'> Step guide to be followed in this kernal\n<\/font>\n\n    1. Understanding the Problem\n    2. Understanding the different types of Variable and data cleaning requirements.\n    3. Relationships among data variables and figuring out   dependent and independant data variables properties.\n    4. Cleaning of training and testing data.\n    5. Performing assumptions criteria test and finding.\n    \n    ","d1760720":"<font color='blue'>  **Numerical Features** :  <font>","88a907b3":"### <font color='black'>Now as we have clean categorical features so In the next step we will deal with the numericaL features. <\/font> ","dbc68ff1":"# <font color='black'>  Getting information about test dataset <\/font>\n","2b784b15":"\n# <font color='black'>Concat Train and Test datasets <\/font>\n","43f0d61c":"# <font color='black'>Now we will select numerical and categorical features  <\/font>","3330fb1b":"\n* <font color='black'> Fitting With all the dataset <font>","d69fda53":"<font color='black'> Making **categorical** features worthy  <\/font>","bbc90462":"\n* <font color='black'>  **Ordinal categories features** - Mapping from 0 to N  <font>","04f02574":"# <font color='black'>  Getting information about train dataset <\/font>","97f3b288":"\n* <font color='black'> After making some plots we found that we have some colums with low variance so we decide to delete them  <font>\n","bd533f04":"<font color='black'>  Importing **train** and **test** datasets <\/font>","db742c81":"# <font color='Blue' >Loading the required Packages <\/font>","dd802ef0":"\n* <font color='black'>  Concat Categorical (after encoding) and numerical features  <font>\n","dbda1aec":"\n* <font color='black'>  Separate Train and Targets  <font>","cdf5c494":"* <font color='black'> **Now we will create some new features**  <font>","91f681b0":"#   <font color='Green'>2. Understanding the different types of Variable and data cleaning requirements. <\/font>","54d64157":"* <font color='black'>  We finally end up with a clean dataset  <font>","00691b14":"Steps to be followed:\n\n1. First we will download the python packages for doing some numerical calculation, plotting some data columns, and changing some data scales so that our data should be looking nice and allow us to do prediction. \n\n2. Then we will import our dataset into this kaggle kernal and save our train.csv file which you can download from Data section of this problem. \n\n3. You can also load data using right hand side option as +Add Data, just click on in and load the dataset but as dataset is already a part of kaggle platform you don't need to use this step. \n\n4. After loading dataset, first you should analyse our target variabel because in the end you will be getting this type of output only for the prediction. \n\n5. Check various descriptive statistics for the SalePrice in the training dataset.\n\n6. You can also plot the SalePrice variable to understand more about the data.\n\n7. After that check relationship of other variables in the dataset. \n","6419dc10":"<font color='blue'>  \n        Now We can drop PoolQC, MiscFeature, Alley and Fence features because they have more than 80% of missing values. <font>","aec5516b":"<font color='blue'>  **Categorical Features** :  <\/**font>","ed6ff242":"\n* <font color='black'> Like we see here tha the minimun is -1 ??? <font>\n* <font color='black'>It is strange to find that the house was sold in 2007 before the YearRemodAdd 2009.\n\n    So we decide to change the year of sold to 2009 <font>","0393d8c3":" <font color='black'> Fill the rest of columns with 0  <font>\n","2089828b":" <font color='Blue'>Following are the findings.<\/font>\n        1. We have 81 columns.\n        2. Our target variable is SalePrice.\n        3. Id is just an index that we can drop but we will need it in the final submission.\n        4. We have many missing values \n <font color='Blue'>We have 79 features in our dataset AS discribed above in the Problem.<\/font>","92f7df1d":"#  <font color='black'> Data preprocessing <\/font>","48389ffb":"<font color='black'>  \n        Features with more than 50% of missing values. <\/font>","36ecb6aa":"#  <font color='red'> Modeling  <\/font>","a09a25ac":"1. <font color='black'>  Fill GarageYrBlt and LotFrontage <\/font>\n1. <font color='black'>  Fill the rest of columns with 0 <\/font>","ca495e54":"## <font color='black'>We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Take a look in the data description). <\/font>\n## <font color='black'>We will fill the rest of features with the most frequent value (using its own most frequent value: MODE (Statistics)). <\/font>","88f1d097":"#  <font color='Blue'>Predicting House Prices using Linear Regression and some advancement on it <\/font>","32b8f2f7":"# <font color='Blue'>  Calculating the percentage of missing values of each feature <\/font>","be8b0612":" <font color='black'>  So we will fill the year with 1979 and the Lot frontage with 68 <\/font>\n","ca62a201":"\n* <font color='black'>  Now the next step is to encode categorical features  <font>\n"}}