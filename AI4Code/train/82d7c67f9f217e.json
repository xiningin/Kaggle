{"cell_type":{"ccb7c4d4":"code","5dbff430":"code","df11c945":"code","6d1c249a":"code","35f609ac":"code","5f0aeda8":"code","3f817dd7":"code","3f9ba8b5":"code","aa8f8829":"code","916c8267":"code","07bfb89e":"code","3351125d":"code","eb3749c7":"code","cb09f0ba":"code","45b0d0ee":"code","c8135cd4":"markdown","3f464b1d":"markdown","5b663584":"markdown","24254bd6":"markdown","efb631c2":"markdown","e44d899e":"markdown","971271ad":"markdown","08fca013":"markdown","ec7a4884":"markdown","bcfd9807":"markdown","f1c17c2b":"markdown","502dd0a7":"markdown"},"source":{"ccb7c4d4":"import pandas as pd, numpy as np, os, matplotlib.pyplot as plt, seaborn as sns\nimport datatable as dt\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns',None)\nimport gc\n#import cudf #only works when gpu on\nfrom sklearn.metrics import roc_auc_score,auc, roc_curve\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","5dbff430":"PLOT = False\n\n#notebook setup\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    \nTARGET = 'target'\nFOLD = 5\nSEED = 69\nN_ESTIMATORS=10000\nDEVICE = 'CPU'\nEVAL_METRIC = \"AUC\"\n\nSTUDY_TIME = 60*60*8\nseed_everything(SEED)","df11c945":"%time\n\n#import data\ntrain = dt.fread(r\"..\/input\/d\/ankitkalauni\/tps-october-2021-dataset\/train.csv\").to_pandas()\ntest = dt.fread(r\"..\/input\/d\/ankitkalauni\/tps-october-2021-dataset\/test.csv\").to_pandas()\n\n# train.columns = ['f1', 'f2', 'f3', 'f4', 'f5', 'target', 'f6',\n#        'f7', 'f8', 'f9', 'f10', 'f11',\n#        'f12', 'f13', 'f14', 'f15', 'f16',\n#        'f17', 'f18', 'f19', 'f20','f21']\n\n# test.columns = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6',\n#        'f7', 'f8', 'f9', 'f10', 'f11',\n#        'f12', 'f13', 'f14', 'f15', 'f16',\n#        'f17', 'f18', 'f19', 'f20','f21']\n\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\n\ntrain[TARGET] = train[TARGET].astype('int64') ","6d1c249a":"print('Train Shape: ',train.shape)\ntrain.tail(10).reset_index(drop=True)","35f609ac":"print('Test Shape: ',test.shape)\ntest.head(10)","5f0aeda8":"#setup for preprocessing\nX = train.drop(TARGET, axis=1)\ny = train[TARGET]\nX_test = test\n\n#delete the old datframes from the memory\ndel train, test\ngc.collect()","3f817dd7":"# helper functions\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score","3f9ba8b5":"#best parameters searched using optuna \nhist_params = {'l2_regularization': 1.3244040135051264e-10,\n               'early_stopping': 'True',\n               'learning_rate': 0.0366777965884429, \n               'max_iter': 10000, \n               'max_depth': 3, \n               'max_bins': 129, \n               'min_samples_leaf': 13449, \n               'max_leaf_nodes': 68}","aa8f8829":"if PLOT == True:\n    X_data = [X.f1,  X.f2,  X.f3,  X.f4,  X.f5,  X.f6,  X.f7,  X.f8,  X.f9,  X.f10,  X.f11,  X.f12,  X.f13,  X.f14,  X.f15,  X.f16,  X.f17,  X.f18,  X.f19,  X.f20, X.f21]\n    group_labels = X.columns.to_list()\n    fig = ff.create_distplot(X_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\n    fig.show()","916c8267":"if PLOT == True:\n    _data = [X_test.f1,  X_test.f2,  X_test.f3,  X_test.f4,  X_test.f5,  X_test.f6,  X_test.f7,  X_test.f8,  X_test.f9,  X_test.f10,  X_test.f11,  X_test.f12,  X_test.f13,  X_test.f14,  X_test.f15,  X_test.f16,  X_test.f17,  X_test.f18,  X_test.f19,  X_test.f20, X_test.f21]\n    fig = ff.create_distplot(_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\n    fig.show()","07bfb89e":"if PLOT == True:\n    #correlation between all models pred\n    data = np.corrcoef(X_data)\n    fig=px.imshow(data,x=group_labels, y=group_labels)\n\n    fig.show()","3351125d":"if PLOT == True:\n    #correlation between all models pred\n    data = np.corrcoef(_data)\n    fig=px.imshow(data,x=group_labels, y=group_labels)\n\n    fig.show()","eb3749c7":"#check of all the columns in train is in test set\nassert X.columns.to_list() == X_test.columns.to_list()\n\nmeta_pred_tmp = []\nscores_tmp = []\n\n# create cv\nkf = StratifiedKFold(n_splits=50, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    # create train, validation sets\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = HistGradientBoostingClassifier(**hist_params)\n    model.fit(X_train, y_train)\n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    \n    score = get_auc(y_valid, pred_valid)\n    scores_tmp.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*20)\n    \n    # test prediction based on oof_set\n    y_hat = model.predict_proba(X_test)[:,1]\n    meta_pred_tmp.append(y_hat)\n# print overall validation scores\nprint(f\"Overall Validation Score | Meta: {np.mean(scores_tmp)}\")\nprint('::'*20)","cb09f0ba":"#average meta predictions over each fold\nmeta_predictions = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n\n# create submission file\nstacked_submission = sample_submission.copy()\nstacked_submission[TARGET] = meta_predictions\nstacked_submission.to_csv('.\/HistGBM.csv', index=False)","45b0d0ee":"if PLOT == True:\n    plot = pd.concat([X_test,stacked_submission[TARGET]],axis=1)\n\n    pred = [plot.f1,  plot.f2,  plot.f3,  plot.f4,  plot.f5,  plot.f6,  plot.f7,  plot.f8,  plot.f9,  plot.f10,  plot.f11,  plot.f12,  plot.f13,  plot.f14,  plot.f15,  plot.f16,  plot.f17,  plot.f18,  plot.f19,  plot.f20, plot.f21, plot.target]\n    group_labels = plot.columns.to_list()\n    fig = ff.create_distplot(pred, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\n    fig.show()","c8135cd4":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nPreprocessing\n<\/div>\n\n___","3f464b1d":"[Tutorial Gradient Boosting - StatQuest](https:\/\/www.youtube.com\/embed\/3CC4N4z3GJc)","5b663584":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nHistGBM\n<\/div>","24254bd6":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTest-set Heatmap Plot\n<\/div>","efb631c2":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nHistGBM Prediction KDE Plot\n<\/div>","e44d899e":"# Histogram-based Gradient Boosting Classification Tree.\n\nThis estimator is much faster than\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nfor big datasets (n_samples >= 10 000). The input data ``X`` is pre-binned\ninto integer-valued bins, which considerably reduces the number of\nsplitting points to consider, and allows the algorithm to leverage\ninteger-based data structures. For small sample sizes,\n:class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\nmight be preferred since binning may lead to split points that are too\napproximate in this setting.\n\nThis implementation is inspired by\n`LightGBM <https:\/\/github.com\/Microsoft\/LightGBM>`_.\n\n## note:\n\n  This estimator is still **experimental** for now: the predictions\n  and the API might change without any deprecation cycle. To use it,\n  you need to explicitly import ``enable_hist_gradient_boosting``::\n\n    >>> # explicitly require this experimental feature\n    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n    >>> # now you can import normally from ensemble\n    >>> from sklearn.ensemble import HistGradientBoostingClassifier\n","971271ad":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTabular Playground Series - Oct 2021\n<\/div>","08fca013":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTest-set KDE Plot\n<\/div>","ec7a4884":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nNote: Upvote is Free!!!\n<\/div>","bcfd9807":"<a><img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=400><\/a>","f1c17c2b":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTrain-set Heatmap Plot\n<\/div>","502dd0a7":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#336b87;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTrain-set KDE Plot\n<\/div>"}}