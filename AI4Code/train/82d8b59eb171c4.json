{"cell_type":{"8eb5801d":"code","5c0fa76c":"code","4b4273c9":"code","5074c92c":"code","f70bb5ac":"code","da8a7692":"code","200b19f4":"code","25941392":"code","d3abc5f8":"code","afb5ece5":"code","37dcd77e":"code","b4b1e9c9":"code","9989f3f4":"code","ee29fc89":"code","268a039e":"code","2daf1370":"code","f35467a8":"code","9020659d":"code","a9839ebb":"code","d5c4a3d5":"code","c0af3a44":"code","262311c7":"code","91d43909":"code","f98f4b7d":"code","345ba3dd":"code","01cec16e":"code","6566fb08":"code","06c39548":"code","d69d5a28":"code","3f15a332":"code","6e0dff44":"code","8b391d07":"code","098b9eb5":"code","36910de1":"code","dd697814":"code","92ff8e7d":"code","b73bd90f":"code","593b52db":"code","296a0214":"code","12379d4e":"code","64af1d37":"code","54f8cf89":"code","ac02c097":"code","1780937f":"code","57e21280":"code","8bd258b2":"code","3bfd4b63":"code","ac12406b":"code","79c4bdb0":"code","fe417c59":"markdown","1f058e78":"markdown","04de1a6e":"markdown","73e7c39a":"markdown","0a3a20e6":"markdown","b1392991":"markdown","aca0af2c":"markdown","a850c1a9":"markdown","718e570a":"markdown","b0a60051":"markdown","734c96a7":"markdown","a9eb6c98":"markdown","a6d7a3b9":"markdown","fb3fa90e":"markdown","09349c8e":"markdown","21becbe1":"markdown","d68aa4df":"markdown","77295814":"markdown","b1c8a30d":"markdown","962e8940":"markdown","c1276e97":"markdown","59ea1d06":"markdown","fae6313e":"markdown","f338ca6a":"markdown","66e16478":"markdown","6a8595f8":"markdown","c0e390cb":"markdown","127c0e99":"markdown","bcad86b3":"markdown","5e0fe51f":"markdown","74212383":"markdown","0caa3de5":"markdown","6c2d8f50":"markdown","2d28697d":"markdown","8fdf496c":"markdown","09ca63fb":"markdown"},"source":{"8eb5801d":"import itertools\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris, load_wine, load_breast_cancer","5c0fa76c":"def plot_dataset(X, y):\n    combinations = list(itertools.combinations([i for i in range(X.shape[1])], 2))\n    n_comb = len(combinations)\n    \n    if n_comb == 0:\n        fig = make_subplots(rows=1, cols=1)\n        fig.add_trace(\n            go.Scatter(x=list(X.flatten()), y=[0 for i in range(len(X))], mode='markers', marker=dict(color=y)),\n            row=1, col=1\n        )\n        fig.update_layout(height=1000, width=1000, title_text=\"Dataset Columns Pairs\")\n        fig.show()\n        return\n    \n    elif n_comb == 1:\n        rows = 1\n        cols = 1\n    else:\n        rows = int(n_comb \/ 2)\n        cols = 2\n\n    fig = make_subplots(rows=rows, cols=cols)\n    flag = True\n    \n    col = 1\n    i = 1\n\n    for combination in combinations:\n        if i > rows:\n            i = 1\n            col = 2\n            \n        fig.add_trace(\n            go.Scatter(x=list(X[:,combination[0]]), y=list(X[:, combination[1]]), mode='markers', marker=dict(color=y)),\n            row=i, col=col\n        )\n        fig.add_vline(x=0, row=i, col=col)\n        fig.add_hline(y=0, row=i, col=col)\n        i += 1\n\n    fig.update_layout(height=1000, width=1000, title_text=\"Dataset Columns Pairs\")\n    fig.show()","4b4273c9":"X, y = load_iris(return_X_y = True)\nplot_dataset(X, y)","5074c92c":"# Normalization\nX = StandardScaler().fit_transform(X)\nplot_dataset(X, y)","f70bb5ac":"# Covariance Matrix\n# Numpy works with transposed datasets on this function, so we need to use th rowvar=False\ncov_matrix = np.cov(X, rowvar=False)\ncov_matrix","da8a7692":"# Eigenvectors and eigenvalues\neig_val, eig_vec = np.linalg.eig(cov_matrix)","200b19f4":"# Sorting the eigenvalues and eigenvectors\nidx = eig_val.argsort()[::-1]\neig_val = eig_val[idx]\neig_vec = eig_vec[:, idx]","25941392":"eig_val","d3abc5f8":"# Obtaining the explained variance by each\nvariance = eig_val \/ sum(eig_val)\nvariance","afb5ece5":"# Selecting the first two components\nn_components = 2\nprint(f'Explained Variance by the first {n_components} components: {sum(variance[0:n_components])}')\nfeature_vector = eig_vec[:, 0:n_components]","37dcd77e":"PCA_X = np.matmul(feature_vector.T, X.T).T","b4b1e9c9":"plot_dataset(PCA_X, y)","9989f3f4":"feature_vector = eig_vec[:, 0:1]\nPCA_X = np.matmul(feature_vector.T, X.T).T\nplot_dataset(PCA_X, y)","ee29fc89":"def generalized_variance(X, normalize=True):\n    cov = np.cov(X, rowvar=False)\n    if normalize:\n        return np.linalg.det(cov) \/ np.trace(cov)\n    else:\n        return np.linalg.det(cov)","268a039e":"def scatter_coefficient(X, normalize=True):\n    corr = np.corrcoef(X, rowvar=False)\n    if normalize:\n        return np.linalg.det(corr) \/ np.trace(corr)\n    else:\n        return np.linalg.det(corr)","2daf1370":"def psi_index(X, normalize=False):\n    corr = np.corrcoef(X, rowvar=False)\n    \n    # Eigenvalues and eigenvectors from the correlation matrix\n    eig_val, eig_vec = np.linalg.eig(corr)\n    idx = eig_val.argsort()[::-1]\n    eig_val = eig_val[idx]\n\n    if normalize:\n        p = X.shape[0]\n        return np.sum((eig_val - 1)**2) \/ (p*(p-1))\n    else:\n        return np.sum((eig_val - 1)**2)","f35467a8":"def index_of_matrix(X):\n    corr = np.corrcoef(X, rowvar=False)\n    \n    # Eigenvalues and eigenvectors from the correlation matrix\n    eig_val, eig_vec = np.linalg.eig(corr)\n    idx = eig_val.argsort()[::-1]\n    eig_val = eig_val[idx]\n    \n    return np.sqrt(eig_val[0]\/eig_val[-1])","9020659d":"def information_statistic(X):\n    corr = np.corrcoef(X, rowvar=False)\n    \n    # Eigenvalues and eigenvectors from the correlation matrix\n    eig_val, eig_vec = np.linalg.eig(corr)\n    idx = eig_val.argsort()[::-1]\n    eig_val = eig_val[idx]\n    \n    return -(1\/2) * np.sum(np.log(eig_val))","a9839ebb":"def divergence_statistic(X):\n    corr = np.corrcoef(X, rowvar=False)\n    \n    # Eigenvalues and eigenvectors from the correlation matrix\n    eig_val, eig_vec = np.linalg.eig(corr)\n    idx = eig_val.argsort()[::-1]\n    eig_val = eig_val[idx]\n    \n    return np.sum((1 - eig_val) \/ (2*eig_val))","d5c4a3d5":"generalized_variance(X)","c0af3a44":"scatter_coefficient(X)","262311c7":"psi_index(X)","91d43909":"index_of_matrix(X)","f98f4b7d":"information_statistic(X)","345ba3dd":"divergence_statistic(X)","01cec16e":"# This function changes the order of the columns independently to remove correlations\ndef de_correlate_df(df):\n    X_aux = df.copy()\n    for col in df.columns:\n        X_aux[col] = df[col].sample(len(df)).values\n        \n    return X_aux","6566fb08":"df = pd.DataFrame(X)","06c39548":"pca = PCA()\npca.fit(df)\n\noriginal_variance = pca.explained_variance_ratio_\n\nN_permutations = 1000\n\nvariance = np.zeros((N_permutations, len(df.columns)))\n\nfor i in range(N_permutations):\n    X_aux = de_correlate_df(df)\n    \n    pca.fit(X_aux)\n    variance[i, :] = pca.explained_variance_ratio_","d69d5a28":"average_permuted_variance = np.mean(variance, axis=0)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=original_variance, name='Explained Variance'))\nfig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=average_permuted_variance, name='Explained by Chance'))\n\nfig.update_layout(title=\"PCA Permutation Test Explained Variance\")\n\nfig.show()","3f15a332":"p_val = np.sum(variance > original_variance, axis=0) \/ N_permutations\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=p_val, name='p-value on significance'))\n\nfig.update_layout(title=\"PCA Permutation Test p-values\")","6e0dff44":"p_val = np.sum(np.abs(np.diff(variance, axis=1, prepend=0)) > np.abs(np.diff(original_variance, prepend=0)), axis=0) \/ N_permutations\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=p_val, name='p-value on significance'))\n\nfig.update_layout(title=\"PCA Permutation Test p-values\")","8b391d07":"def correlation_matrix(cov_matrix):\n    eig_val, eig_vec = np.linalg.eig(cov_matrix)\n    r = np.zeros((len(eig_vec), cov_matrix.shape[0]))\n    for i in range(len(eig_vec)):\n        for j in range(cov_matrix.shape[0]):\n            r[i][j] = (eig_vec[i][j] * np.sqrt(eig_val[i])) \/ np.sqrt(cov_matrix[j][j])\n            \n    return r","098b9eb5":"def index_of_loadings_matrix(cov_matrix):\n    eig_val, eig_vec = np.linalg.eig(cov_matrix)\n    r = np.zeros((len(eig_vec), cov_matrix.shape[0]))\n    for i in range(len(eig_vec)):\n        for j in range(cov_matrix.shape[0]):\n            r[i][j] = (eig_vec[i][j]**2 * eig_val[i])**2 \/ np.sqrt(cov_matrix[j][j])\n\n    return r","36910de1":"correlation_matrix(cov_matrix)","dd697814":"index_of_loadings_matrix(cov_matrix)","92ff8e7d":"iris_X, iris_y = load_iris(return_X_y=True)\nwine_X, wine_y = load_wine(return_X_y=True)\ncancer_X, cancer_y = load_breast_cancer(return_X_y=True)","b73bd90f":"uncorr_dataset = pd.DataFrame()\nfor i in range(15):\n    uncorr_dataset[f'f{i}'] = np.random.normal(loc=i, scale=(i + 1)\/2, size=500)\n\nuncorr_dataset_y = pd.DataFrame()\nfor i in range(5):\n    df = pd.DataFrame()\n    df['target'] =  [i for j in range(100)]\n    uncorr_dataset_y = uncorr_dataset_y.append(df)","593b52db":"semi_corr_dataset = pd.DataFrame(iris_X)\nfor i in range(10):\n    semi_corr_dataset[f'f{i}'] = np.random.normal(loc=i, scale=(i + 1)\/2, size=150)","296a0214":"# Normalizing the datasets\niris_X = StandardScaler().fit_transform(iris_X)\nwine_X = StandardScaler().fit_transform(wine_X)\ncancer_X = StandardScaler().fit_transform(cancer_X)\nuncorr_dataset = StandardScaler().fit_transform(uncorr_dataset)\nsemi_corr_dataset = StandardScaler().fit_transform(semi_corr_dataset)","12379d4e":"datasets_X = [uncorr_dataset, semi_corr_dataset, iris_X, wine_X, cancer_X]\ndatasets_y = [uncorr_dataset_y, iris_y, iris_y, wine_y, cancer_y]","64af1d37":"names = ['Uncorrelated', 'Semi-Correlated', 'Iris', 'Wine', 'Breast Cancer']\n\nuse_metrics = pd.DataFrame()\nfor i, X in enumerate(datasets_X):\n    df = pd.DataFrame()\n    \n    df = df.append({'metric': 'generalized_variance', 'value': generalized_variance(X)}, ignore_index=True)\n    df = df.append({'metric': 'scatter_coefficient', 'value': scatter_coefficient(X)}, ignore_index=True)\n    df = df.append({'metric': 'index_of_matrix', 'value': index_of_matrix(X)}, ignore_index=True)\n    df = df.append({'metric': 'psi_index', 'value': psi_index(X)}, ignore_index=True)\n    df = df.append({'metric': 'information_statistic', 'value': information_statistic(X)}, ignore_index=True)\n    df = df.append({'metric': 'divergence_statistic', 'value': divergence_statistic(X)}, ignore_index=True)\n    \n    df['dataset'] = names[i]\n    \n    use_metrics = use_metrics.append(df, ignore_index=False)","54f8cf89":"use_metrics.pivot_table(index='dataset', columns='metric', values='value').reset_index()","ac02c097":"N_permutations = 1000\np_vals_components = []\n\nfor j, X in enumerate(datasets_X):\n    df = pd.DataFrame(X)\n    pca = PCA()\n    pca.fit(df)\n\n    original_variance = pca.explained_variance_ratio_\n    variance = np.zeros((N_permutations, len(df.columns)))\n    \n    for i in range(N_permutations):\n        X_aux = de_correlate_df(df)\n\n        pca.fit(X_aux)\n        variance[i, :] = pca.explained_variance_ratio_\n        \n    average_permuted_variance = np.mean(variance, axis=0)\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=[f'PC{k}' for k in range(len(df.columns))], y=original_variance, name='Explained Variance'))\n    fig.add_trace(go.Scatter(x=[f'PC{k}' for k in range(len(df.columns))], y=average_permuted_variance, name='Explained by Chance'))\n    fig.update_layout(title=f\"PCA Permutation Test Explained Variance for {names[j]}\")\n    fig.show()\n    \n    p_val = np.sum(variance > original_variance, axis=0) \/ N_permutations\n\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=[f'PC{k}' for k in range(len(df.columns))], y=p_val, name='p-value on significance'))\n\n    fig.update_layout(title=f\"PCA Permutation Test p-values for {names[j]}\")\n    fig.show()\n    \n    p_vals_components.append(np.sum(p_val < 0.05))","1780937f":"def fit_rf(X, y, i):\n    rf = RandomForestClassifier(random_state=i, n_jobs=-1)\n\n    parameters = {'n_estimators': [10, 50, 100, 300], 'max_depth': [None, 3, 10]}\n\n    clf = GridSearchCV(rf, parameters)\n    clf.fit(X, y)\n    \n    return clf.best_estimator_","57e21280":"def validate_model(clf, X, y):\n    df = pd.DataFrame()\n    names_ = [f'cross_{i}' for i in range(5)]\n\n    values = cross_val_score(clf, X, y, cv=5)\n\n    df['values'] = values\n    df = df.T\n    df.columns = names_\n    df['average'] = df[names_].mean(axis=1).values[0]\n    df['std'] = df[names_].std(axis=1).values[0]\n    return df","8bd258b2":"results = pd.DataFrame()\n\nfor i in tqdm(range(len(datasets_X))):\n    for w in range(15):\n        # Fit without PCA\n        clf = fit_rf(datasets_X[i], np.array(datasets_y[i]).ravel(), w)\n\n        df_val = validate_model(clf, datasets_X[i], np.array(datasets_y[i]).ravel())\n        df_val['type'] = 'Without PCA'\n        df_val['Dataset'] = names[i]\n        df_val['n_components'] = 0\n        df_val['round'] = w\n        results = results.append(df_val, ignore_index=True)\n\n        # Fit with PCA with all components\n        pca = PCA()\n        X = pca.fit_transform(datasets_X[i])\n        clf = fit_rf(X, np.array(datasets_y[i]).ravel(), w)\n\n        df_val = validate_model(clf, datasets_X[i], np.array(datasets_y[i]).ravel())\n        df_val['type'] = 'PCA All Components'\n        df_val['Dataset'] = names[i]\n        df_val['n_components'] = X.shape[1]\n        df_val['round'] = w\n        results = results.append(df_val, ignore_index=True)\n\n\n        # Fit PCA with 80%\n        pca = PCA()\n        pca.fit(datasets_X[i])\n\n        n_components = np.sum((np.cumsum(pca.explained_variance_ratio_) <= 0.8))\n        if n_components == 0:\n            n_components = 1\n        pca = PCA(n_components=n_components)\n\n        X = pca.fit_transform(datasets_X[i])\n        clf = fit_rf(X, np.array(datasets_y[i]).ravel(), w)\n\n        df_val = validate_model(clf, datasets_X[i], np.array(datasets_y[i]).ravel())\n        df_val['type'] = 'PCA 80%'\n        df_val['Dataset'] = names[i]\n        df_val['n_components'] = n_components\n        df_val['round'] = w\n        results = results.append(df_val, ignore_index=True)\n\n        # Fit with PCA with permutation test components\n        n_components = p_vals_components[i]\n        if n_components == 0:\n            n_components = 1\n        pca = PCA(n_components=n_components)\n        X = pca.fit_transform(datasets_X[i])\n        clf = fit_rf(X, np.array(datasets_y[i]).ravel(), w)\n\n        df_val = validate_model(clf, datasets_X[i], np.array(datasets_y[i]).ravel())\n        df_val['type'] = 'PCA Permutation Test Components'\n        df_val['Dataset'] = names[i]\n        df_val['n_components'] = n_components\n        df_val['round'] = w\n        results = results.append(df_val, ignore_index=True)","3bfd4b63":"use_metrics.pivot(index='dataset', columns='metric', values='value').reset_index().sort_values('generalized_variance')","ac12406b":"results.to_parquet('results.parquet')","79c4bdb0":"for dataset in results['Dataset'].unique():\n    df = results[(results['Dataset'] == dataset)]\n    fig = px.box(df, x='type', y='average', points='all')\n    fig.update_layout(title_text=f'Experiment for {dataset}')\n    fig.show()","fe417c59":"### Index of the Loadings (IL)\n\nWith this metric, we square the values on the numerator to avoid the permutation of sign and also to allow only high loadings and high eigenvalues to be considered in a significant manner. The equation is:\n\n$$IL_{ij} = \\frac{u_{ij}^2 \\lambda_i^2}{s_j}$$","1f058e78":"### Creating the Synthetic Datasets","04de1a6e":"### Divergence Statistic\n\nSimilar to the one before. However, tries to normalize the values using a fraction and not a log. Given by the following formula:\n\n$$\\sum \\frac{1 - \\lambda_i}{2 \\lambda_i}$$","73e7c39a":"Now we calculate the eigenvectors and eigenvalues from this matrix. The eigenvectors will be the axis of the new space and will point towards the 'line' of greater variance on our original space.\n\nWe will then sort the eigenvectors by the value of their eigenvalue. The bigger the eigenvalue, the greater the explained variance by that eigenvector.","0a3a20e6":"### Generalized Variance\n\nThe generalized variance is the determinant of the covariance matrix. Its geometrical interpretation is the hyper-volume generated by the dataset on the space. It decreases with a greater amount of covariance, so smaller values are better.\n\nThe idea is that if the variables are highly correlated, then this hyper-volume will tend to shrink to a lower dimension. The minimum value for this metric is zero and the maximum value (no correlation at all) is the trace of the matrix.","b1392991":"### $\\psi$ Index\n\nThis metric looks for the magnitude of the eigenvalues taken from the correlation matrix. If the variables are uncorrelated, each PC tends to explain as much variance as a single variable and their eigenvalues tend to 1. Therefore, the closer to the y = 1 row, the smaller the area and the more uncorrelated the dataset.\n\nFor this metric, bigger values are better. Its maximum value is p(p-1) and its minimum value is zero. It is given by the equation:\n\n$$\\psi = \\sum (\\lambda_i - 1)^2$$","aca0af2c":"As we can see, this new dataset has only two dimensions and we can see a very good separation between the classes with only a little overlap with the classes 1 and 2.\n\nIf we would go even far and use only 1 principal component, we would have the following:","a850c1a9":"## Permutation Test for Components Selection\n\nSo, how do we select the actual number of Principal Components we should use? Usually one would define a threshold of explained variance (such as 80% or 90%) and cut-off the components once this value is achieved.\n\nHowever, some of that variance may as well be noise on our dataset and adding them would hurt the performance. To solve that, one can generate permutation tests on the PCA to find out how many Principal Components should be used. The method implemented here is based on the one found on a R snippet from [this post](https:\/\/towardsdatascience.com\/how-to-tune-hyperparameters-of-tsne-7c0596a18868?source=collection_tagged---------21-------------------------------) on Medium.\n\nThe basic idea here is to remove the correlation between the features of the dataset by doing permutations of the columns of the dataset independently from each other and then verify the amount of variance explained by each Principal Component. It is expected that the variance explained by the component, if it should be used, will be greater than te variance generated by chance on an uncorrelated dataset. This is what we are going to verify.","718e570a":"### Information Statistic\n\nTries to take into account all the eigenvalues, however, this makes this metric has no upper bound. The formula is given by:\n\n$$-\\frac{1}{2} \\sum \\ln{\\lambda_i}$$","b0a60051":"As we can see, on some dimensions the division between classes are clearer than on others. You can see that the data is not normalized given the range of the variables that go up to 8 on some cases.\n\nBecause the PCA measures variance, it is useful to normalize the data before applying it so we can be sure that one variable will not dominate over the other because it has a bigger scale.","734c96a7":"### Getting the Real Datasets","a9eb6c98":"## PCA Implementation\n\nLet's implement the PCA from the ground-zero. The following steps are required to build the PCA and they will be explained in more detail later on the notebook:\n- Normalize and centralize the data\n- Calculate the covariance matrix\n- Calculate the eigenvalues and eigenvectors from the covariance matrix\n- Select the number of dimensions\n- Apply the space transformation to the original data\n\nFor this implementation we will be looking in greater detail to the Iris dataset because it is a small dataset with few dimensions, so we can have a better understanding of the inner workings of the PCA.","a6d7a3b9":"As expected we will have more overlap because less variance is explained, however, we still see a good division between the classes.","fb3fa90e":"Now we will calculate the covariance matrix of that normalized dataset. The covariance measures if variables are correlated with other, this is: if they grow together or if they decrease together.\n\nOne can think about the covariance as the 'not normalized' correlation matrix because it only tell us the direction of the relation but not the strength of this relation.","09349c8e":"Here we can see that the first eigenvector alone is responsible for 72% of the variance of the dataset.","21becbe1":"## Statistics for usefulness of PCA\n\nFirst of all, we should be able to answer the question: it is worth it to apply PCA on our data? The PCA does not work well when the variables are uncorrelated with each other, because on this case, the explained variance by each Principal Component will be close to the variance of the variables itself, so we will not have a dimensionality reduction as we expect.\n\nThere are several methods to assess if PCA is a good option for a dataset. On this section we will talk about them and see how we can apply them.","d68aa4df":"## Datasets Experiments\n\nOn this section we will try to use the metrics defined above to see the usefulness of them. The step-by-step procedure is stated as follows:\n\n- Grab 3 real datasets (Iris, Wine and Breast Cancer)\n- Create 2 synthetic datasets, one iris half-correlated and one uncorrelated\n- Run the usefullness metrics for each one\n- Run the permutation test for each one\n- Apply a model with and without PCA for every dataset\n    - Apply the PCA with all components\n    - Apply the PCA with a selected number of components to 80% of variance\n    - Apply the PCA with the permutation test","77295814":"If we divide the value of each eigenvalue by the sum of the eigenvalues we will get the the explained variance by each of the eigenvectors.","b1c8a30d":"### Difference of Roots\n\nThe method above is called the equality of roots since it measures how each PC changes from the one of the same rank on the random dataset. There is, however, another way of doing that: we can make the diff of the variances for each pair of PC and then compare it to the result from the random experiment and define our p-value as the times it was greater than the original.","962e8940":"Let's look now at the average explained variance ratio of the permuted datasets in comparison with the non-permuted dataset.","c1276e97":"So, according to this test, only the first PC should be used. However, as with any statistical test, this test is not perfect. We saw by the scatter plots that two PCs would yield a better space separation of the classes. This is probably due to the low dimension and low number of instances on the Iris dataset.","59ea1d06":"### Correlation Between Features and PCs\n\nWe can calculate how likely was each variable to be associated with each PC by calculating the correlation of the PC with the variables. To do so, we will use:\n\n$$r_{ij} = \\frac{u_{ij} \\sqrt{\\lambda_i}}{s_j}$$\n\nWhere $r_{ij}$ is the correlation between the $i^{th}$ PC and the $j^{th}$ variable, the $u_{ij}$ is the loading of the $j^{th}$ on the $i^{th}$ PC and $s_j$ is the standard deviation of the $j^{th}$ variable.\n\nThis metric, however, presents some problems because the sign of the loadings can change.","fae6313e":"### Index of a Matrix\n\nIt compares the biggest and smallest eigenvalues and its applied to the correlation matrix. With increasing correlation, we have increased metric. This, however, neglects the distribution of variance allocated to all other PCs. It is given by the formula:\n\n$$\\sqrt{\\frac{\\lambda_1}{\\lambda_p}}$$","f338ca6a":"First, let's define a function to plot the pairs of columns from a given dataset.","66e16478":"### Applying the model","6a8595f8":"### Running the Usefullness Metrics","c0e390cb":"### Verifying Results","127c0e99":"Now, to apply the PCA to the dataset we will multiply the eigenvectors transposed by the dataset transposed. This will generate the transformed dataset on the new space of the principal components.","bcad86b3":"### Running the Permutation Test","5e0fe51f":"As we can see the metrics were able of finding that the uncorrelated dataset is not suitable for PCA. By looking at that one can predict that the Breast Cancer dataset will be the one with greater impact from the PCA since its metrics have a huge difference. ","74212383":"### Scatter Coefficient\n\nIt is basically the same of the generalized variance, however, it uses the correlation matrix instead of the covariance matrix. In this case, the maximum value for the metric will be the number of dimensions.","0caa3de5":"# PCA\n\nThe PCA is one of the most common dimensionality reduction techniques on the Data Science realm. On this notebook a deep study of the method will be done and some advanced aspects gathered from this [paper](https:\/\/www.researchgate.net\/publication\/255728363_Permutation_tests_to_estimate_significances_on_Principal_Components_Analysis) will be analyzed. ","6c2d8f50":"Now, let's grab the Iris dataset and see how it behave.","2d28697d":"We will set a number of permutations, for each permutation we will calculate the PCA and save the explained variance ratio for each PC.","8fdf496c":"As we can see, it seems that only the first PC presents a variance greater than one we would expect to find on a uncorrelated dataframe. Let's calculate the p-value for this analysis.\n\nTo do so, we will count the number of times each PC explained variance ratio is bigger than the original case and normalize it by the number of permutations.","09ca63fb":"## Interpretability of PCA\n\nOne of the biggest issues with using PCA on some domains is that it makes interpretability of the model a little bit more difficult. Because each PC is composed of several variables, applying a SHAP on the model, for example, will not tell us much about the impact of each feature on the model.\n\nHowever, we can start to sutudy some ways of overcoming this. First of all, let's understandthe concept of loading. For that, let's remember that each PC is a linear combination of the features of our dataset, so we can write it as follows:\n\n$$ PC_1 = w_{1,1}X_1 + w_{1,2}X_2 + \\dots + w_{1,n}X_n$$\n\nThe weights of this linear combination (the $w_{x,y}$) are the loadings of the Principal Component. You can see right away that this can give us an idea of how much each variable impacts each component.\n\nYou can also see that these values are the eigenvector values on each of the dimensions of the original dataset. It is useful, however, to multiply those loadings by the square-root of the eigenvalues as a form of normalization.\n\nWith this in hands, we can start creating measures to relate each feature to each Principal Component."}}