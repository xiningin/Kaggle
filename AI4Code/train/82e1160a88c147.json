{"cell_type":{"cb17503d":"code","2261d92a":"code","63ea3e74":"code","afa55c8d":"code","f5e9125c":"code","58fa25b6":"code","445e13df":"markdown","4f9619bf":"markdown","3dd72f29":"markdown","dc4bd1c5":"markdown","984e486e":"markdown","46a6231c":"markdown","875f2821":"markdown","3c792c8d":"markdown","86e73d7b":"markdown","58012a63":"markdown"},"source":{"cb17503d":"import tensorflow as tf","2261d92a":"# Custom Loss\n\nclass Loss(tf.keras.losses.Loss):\n    \n    def __init__(self, threshold = 1):\n        super().__init__()\n        self.threshold = threshold\n    \n    def call(self, y_true, y_pred):\n        error = y_true - y_pred\n        error_squared = tf.square(error)\n        return tf.where(\n            error > self.threshold,    # Condition\n            error_squared,              # If True\n            error                       # If False\n        )\n    ","63ea3e74":"# Lambda layers\n\ndef my_func(x):\n    return x*x\n\na = tf.keras.Sequential([tf.keras.layers.Lambda(lambda x: my_func(x))])","afa55c8d":"class Layer(tf.keras.layers.Layer):\n    \n    def __init__(self, units = 32, activation = None):\n        super(Layer, self).__init__()\n        self.units = units\n        self.activation = tf.keras.activations.get(activation)\n        \n    \n    def build(self, input_shape):\n        w_init = tf.random_normal_initializer()    # Initialize weight values\n        self.w = tf.Variable(\n            initial_value = w_init(\n                shape = (input_shape[-1], self.units),\n                dtype = \"float32\"\n            ), \n            trainable = True\n        )\n        \n        b_init = tf.zeros_initializer()    # Initialize bias values\n        self.b = tf.Variable(\n            initial_value = b_init(\n                shape=(self.units,),\n                dtype=\"float32\"\n            ),\n            trainable = True\n        )\n        \n        super().build(input_shape)\n    \n    \n    def call(self, inputs):\n        # Computes outputs passing\n        # through activation function\n        return self.activation(tf.matmul(self.w, inputs) + self.b)\n    ","f5e9125c":"class IdentityBlock(tf.keras.Model):\n    \n    def __init__(self, filters, kernal_size):\n        super(IdentityBlock, self).__init__()\n        \n        self.conv1 = tf.keras.layers.Conv2D(filters, kernal_size, padding=\"same\")\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        \n        self.conv2 = tf.keras.layers.Conv2D(filters, kernal_size, padding=\"same\")\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        \n        self.add = tf.keras.layers.Add()\n        self.act = tf.keras.layers.Activation(\"relu\")\n        \n        \n    def call(self, input_tensor):\n        x = self.conv1(input_tensor)\n        x = self.bn1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        x = self.add([x, input_tensor])\n        return self.act(x)\n    \n\nclass ResNet(tf.keras.Model):\n    \n    def __init__(self, num_classes):\n        super(ResNet, self).__init__()\n        \n        self.conv = tf.keras.layer.Conv2D(64, 7, padding=\"same\")\n        self.bn = tf.keras.layers.BatchNormalization()\n        \n        self.act = tf.keras.layer.Activation(\"relu\")\n        self.max_pool = tf.keras.layers.MaxPool2D((3, 3))\n        \n        self.idt1 = IdentityBlock(64, 3)\n        self.idt2 = IdentityBloc(64, 3)\n        \n        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n        self.classifier = tf.keras.layers.Dense(num_labels, activation=\"softmax\")\n        \n        \n    def call(self, input_tensor):\n        x = self.conv(input_tensor)\n        x = self.bn(x)\n        \n        x = self.act(x)\n        x = self.max_pool(x)\n        \n        x = self.idt1(x)\n        x = self.idt2(x)\n        \n        x = self.global_pool(x)\n        return self.classifier(x)\n        ","58fa25b6":"class DetectOverfittingCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, threshold = 1.3):\n        self.thresshold = threshold\n        \n    def on_epoch_end(self, epoch, logs = None):\n        ratio = logs[\"val_loss\"] \/ logs[\"loss\"]\n        print(\"Epoch: {}, val-train loss ratio: {:.2f}\".format(epoch, ratio))\n        \n        if ratio > threshold:\n            self.model.stop_training = True\n            print(\"Training stopped !\")\n            \n    # There are other methods available such as on_epoch_start,\n    # on_train_start, on_train_end etc... which can be used in appropriate situations.\n        ","445e13df":"# **Custom Callbacks :**","4f9619bf":"# **Custom Loss Function :**","3dd72f29":"#### Callbacks are designed to be able to monitor the model performance in metrics at certain points in the training run and perform some actions that might depend on those performances in metric values. \n\n#### We can create a custom callback by inheriting 'tf.keras.callbacks.Callback' class. In the following example, we are creating a callback which stops model training when overfitting occurs.\n\n#### Constructor : Initialize any properties of the class.\n#### on_epoch_end method : executed at the end of each epoch.\n<br>\n\n### Other Methods available :\n#### Global methods :\n**on_(train|test|predict)\\_begin(self, logs=None)** : called at the beginning of fit(), evaluate(), and predict().\n**on_(train|test|predict)\\_end(self, logs=None)** : called at the end of fit(), evaluate(), and predict().<br><br>\n\n#### Batch-level method (training, testing, and predicting) :\n**on_(train|test|predict)\\_batch_begin(self, batch, logs=None)** : Called right before processing a batch during training\/testing\/predicting.\n**on_(train|test|predict)\\_batch_end(self, batch, logs=None)** : Called at the end of training\/testing\/predicting a batch.<br><br>\n\n#### Epoch-level method (training only) :\nWithin this method, logs is a dict containing the metrics results.<br>\n**on_epoch_begin(self, epoch, logs=None)** : Called at the beginning of an epoch during training.<br>\n**on_epoch_end(self, epoch, logs=None)** : Called at the end of an epoch during training.\n<br>\n\n**Refer : https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback**","dc4bd1c5":"# **Custom Layers :**","984e486e":"# **Custom Losses | Layers | Models | Callbacks in TensorFlow**\n\n![image.png](attachment:77c9eb09-6946-4f70-9886-6ddf7628d0cf.png)\n\nBy : [Balamurugan P](https:\/\/www.linkedin.com\/in\/bala-murugan-62073b212\/)","46a6231c":"#### We can create a custom loss by inheriting 'tf.keras.losses.Loss' class like in the following example. We must define the error calculations inside 'call' function. A custom loss function requires two arguments. The first one is the actual value ( y_true ) and the second one is the predicted value via the model ( y_pred ). Both these are TF Tensors and not Numpy arrays. Inside the function any calculations can be done with the exception that the return value needs to be a TF scalar. \n<br>\n\n#### In the example below, We are defining a loss such that It will square the error whenever the error is larger than the threshold value which can be supplied while instantiating the loss class.\n<br>\n\n**Refer : https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/Loss**","875f2821":"# **Custom Models :**","3c792c8d":"#### The Model class has the same API as Layer, with the following differences:\n* It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict()).\n* It exposes the list of its inner layers, via the model.layers property.\n* It exposes saving and serialization APIs (save(), save_weights()...)\n\n#### We can create a custom model by inheriting 'tf.keras.Model' class like in the following example.\n<br>\n\n#### Constructor : Initialize all layers of the model. Parent class constructor must be called.\n#### call method : Defines the forward pass. Passes the input tensor through all layers and returns output.\n<br>\n\n**Refer : https:\/\/www.tensorflow.org\/guide\/keras\/custom_layers_and_models#the_model_class**","86e73d7b":"#### Now, Let's walk through creating custom layers using Layer subclassing. We can create a custom layer by inheriting 'tf.keras.layers.Layer' class like in the following example. In the constructor, we must call constructor of parent class and can define any properties of the class ( number of units and activation function ). \n<br>\n\n#### Constructor : Initialize any properties of the layer like number of units, acivation function etc...Parent class constructor must be called.\n#### build method : Initiaizes weights and biases tensors. Parent class' build method must be called.\n#### call method : Computes outputs using weights, biases and activation function.\n<br>\n\n**Refer : https:\/\/www.tensorflow.org\/guide\/keras\/custom_layers_and_models#the_layer_class_the_combination_of_state_weights_and_some_computation**","58012a63":"#### First, Let's walk through lambda layers. Lambda layers can be convenient for simple stateless computation, but anything more complex should use a subclass Layer instead. In the following example, we are creating a function which the squares the input. Then, we are creating a Sequential network with a single lambda layer. Here, The lambda layer maps the input 'x' to function 'my_func' using a lambda function and will return 'x\\*\\*2'. We can also pass my_func directly to the lambda layer without using lambda function which works the same.\n<br>\n\n**Refer : https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Lambda**"}}