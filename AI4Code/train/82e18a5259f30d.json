{"cell_type":{"5f384ede":"code","a7f45247":"code","513fe9d4":"code","754c2b94":"code","ee197806":"code","2b5359ac":"code","3ad32052":"code","b6e48aca":"code","f4820b57":"code","d322a8e5":"code","a61d5d59":"code","6c096a91":"code","1041613f":"code","03e2c437":"code","fe2713ab":"code","94225096":"code","1076d0e2":"code","b41c76e4":"code","dcf277ed":"code","237ba198":"code","a7ddb42b":"code","5ff0a2ad":"code","37f6e9b1":"code","9fae5127":"code","cb498e79":"code","2cd6e566":"code","0079b18d":"code","605fa0a3":"code","c3267411":"code","7c44f89e":"code","f395c5b2":"code","d677f783":"code","f4e8c09b":"code","d7269d4c":"code","7a1b29c3":"code","8c81c3d9":"code","e15964eb":"code","5490270b":"markdown","6a50800d":"markdown","0595c297":"markdown","e091e9d4":"markdown","951d6300":"markdown","7fdd01ed":"markdown","05fddca7":"markdown","36ae0a0f":"markdown","f449daa4":"markdown","634d90d1":"markdown","8f8f9d80":"markdown","3ae14f8a":"markdown","a804b733":"markdown"},"source":{"5f384ede":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","a7f45247":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","513fe9d4":"df.head()","754c2b94":"df.columns","ee197806":"df.describe()","2b5359ac":"df.isnull().sum()\n#There is no missing values","3ad32052":"# As we see there is much inbalace in our data  \ndf['Class'].value_counts()","b6e48aca":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","f4820b57":"sns.countplot(x='Class', data=df);","d322a8e5":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=False)\nplt.suptitle(\"Average Amount of Each class\")\nplt.show();","a61d5d59":"tmp = df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']","6c096a91":"class_0.describe()","1041613f":"class_1.describe()","03e2c437":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)]);","fe2713ab":"plt.figure(figsize = (14,14))\nplt.title('Transactions features correlation')\ncorr = df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","94225096":"from sklearn.preprocessing import StandardScaler, RobustScaler\n# Will choose Robust as it's less prone to outliers\n \nrob_scaler = RobustScaler()\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","1076d0e2":"df.head()","b41c76e4":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount','scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\ndf.head()","dcf277ed":"#shuffling the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","237ba198":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\n\n\nsns.countplot('Class', data=new_df)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","a7ddb42b":"normal = new_df[new_df['Class'] == 0]\nfraud = new_df[new_df['Class'] == 1]","5ff0a2ad":"print (normal.shape, fraud.shape)","37f6e9b1":"#Cheking the heatmap again with the sampled data\nplt.figure(figsize = (14,14))\nplt.title('Transactions features correlation')\ncorr = new_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","9fae5127":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD","cb498e79":"X = new_df.drop('Class', axis =1)\ny = new_df['Class']\n\n#t-SNE Implmentation\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n\n#PCA Implmentation\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n\n#TruncatedSVD\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)","2cd6e566":"import matplotlib.patches as mpatches","0079b18d":"#Dimensionality Reduction algrithms vis\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,6))\nf.suptitle('Dimensionality Reduction Clusters')\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Not Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label = 'Fraud')\n\n#t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)\nax1.legend(handles=[blue_patch, red_patch])\n \n#PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\nax2.grid(True)\nax2.legend(handles=[blue_patch, red_patch])\n\n#TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\nax3.grid(True)\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","605fa0a3":"X = new_df.drop('Class', axis = 1)\ny = new_df['Class']","c3267411":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_curve, roc_auc_score","7c44f89e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)","f395c5b2":"#turning the into arrays for the algorithm\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","d677f783":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\nscore = cross_val_score(clf, X_train, y_train, cv = 6)\nprint ('train score of LogisticRegression is', score.mean(),'%')","f4e8c09b":"from sklearn.model_selection import GridSearchCV\n# We will use GridSearchCV to find the best parameters for this model\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_","d7269d4c":"train_score = cross_val_score(log_reg, X_train, y_train, cv = 6)","7a1b29c3":"y_pred = log_reg.predict(X_test)\ntest_score = accuracy_score(y_test, y_pred)","8c81c3d9":"print ('Score for training model is',train_score.mean(), '%')\nprint ('Score for test model is    ',test_score, '%')","e15964eb":"# Lastly will plot a ROC Curve\n# calculating the probabilities\ny_pred_prob = log_reg.predict_proba(X_test)[:,1]\n\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,label='Logistic Regression')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Logistric Regression ROC Curve\")\nplt.show()","5490270b":"there is no notable correlation between features V1-V28\nThere are certain correlations between some of these features and time(inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V1 and V5).","6a50800d":"### Refrences\n- Under and Over Sampling: https:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/\n- t-SNE StatQuest explain: https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM&ab_channel=StatQuestwithJoshStarmer\n- GridSearchCV: https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html","0595c297":"The datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\nThe dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation.\nDue to confidentiality issues, there are not provided the original features and more background information about the data.\n\n- Features V1, V2, ... V28 are the principal components obtained with PCA;\n- The only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n- Feature Class is the response variable and it takes value 1 in case of fraud and 0 if it was normal transaction","e091e9d4":"# Dimensionality Reduction and Clustering","951d6300":"#### Logistic Regression\nLogistic Regression will be good here as wee detect if fraud or not (0 or 1)","7fdd01ed":"Plz Consider Upvoting if learned something new","05fddca7":"# Introduction","36ae0a0f":"We need to fix this imbalace problem as if trained our model with the current issue in Class the model will predict that all will be not fraud","f449daa4":"As we can see there is no much overfitting in our model","634d90d1":"- What does t-SNE does is to find a way to project data into low dimentional space so that clustering in high dimnsional space is is preserved, it turns high dimnsional data into less dimensions withouth losing much data and it's very similar to PCA\n\nThis gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases","8f8f9d80":"- Why we use sampling ?\n\nif we train model with current imbalance it won't give us good accuracy as it very baised to non fraud transactions\n- Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n- Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.\n\nour subsample will be a dataframe with a 50\/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\nwe will scale Amount and Time ","3ae14f8a":"# Scaling and Sampling","a804b733":"We can see that the real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outlier"}}