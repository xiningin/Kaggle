{"cell_type":{"5e8866d8":"code","c70bfd50":"code","ea565738":"code","5faec029":"code","1a497345":"code","f60a40fd":"code","e896678f":"code","6c245a2e":"code","25c0dff7":"code","94d87c71":"code","adbf530f":"code","2401e634":"code","b8351a71":"code","cb0b7b93":"code","0cfa306f":"code","3fedc994":"code","478a0e39":"code","40558f40":"code","543b34ed":"code","a1107f08":"code","4509580f":"code","43cdd8f3":"code","b6dece53":"code","b8683428":"code","687e08cb":"code","55cce0ae":"code","e73c952e":"code","5346227c":"code","8f690c83":"code","5410e319":"code","42bb5883":"code","d2c86c96":"code","e1964154":"markdown","6f5db665":"markdown","5c2aca5a":"markdown","d8c109ee":"markdown","ca31c3e6":"markdown","4d3b0d67":"markdown","3e346cbe":"markdown","f4b0b32f":"markdown","52cf3cc4":"markdown","8aef6b70":"markdown","b4c6b1b3":"markdown","3ab43eb0":"markdown","958c2625":"markdown","5627dcd9":"markdown","4ecb325e":"markdown","96c0214b":"markdown","9cb4d394":"markdown","7fa15117":"markdown","083bc14f":"markdown","dd3dccb0":"markdown","476cfdc1":"markdown","71a9f505":"markdown","7987ebd4":"markdown","c03fa3c3":"markdown","b936ff57":"markdown","de6caf1c":"markdown","4c04b247":"markdown","bb2e5832":"markdown","2bc2f41b":"markdown","a07cdc20":"markdown","30ed10f9":"markdown","3cc9d853":"markdown","79549dba":"markdown","c22303c4":"markdown","26a91ef4":"markdown","19ac6a91":"markdown","532c98af":"markdown","915009e6":"markdown","9dbc324f":"markdown","4dfa4ac5":"markdown","d0a9f494":"markdown","ba00bed1":"markdown","5823056b":"markdown","a96d1c92":"markdown","997c4321":"markdown","ed853052":"markdown","cd26471a":"markdown","ed140bfa":"markdown","528a08bd":"markdown","6ecd533a":"markdown","835cd0d0":"markdown","cf1c56c8":"markdown","4c1e4a4d":"markdown"},"source":{"5e8866d8":"!pip install textstat\n!pip install chart_studio","c70bfd50":"import os\nimport json\nimport string\nimport math\n\nfrom statistics import *\nfrom tqdm import tqdm\n\n# Factory function to supply missing values\nfrom collections import defaultdict\n\n# Warnings control\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n# Textstat\nimport textstat\n\n# Imports for visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\ncolor = sns.color_palette()\n\n# plotly based imports\nfrom plotly import tools\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# Wordcloud library\nfrom wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# spaCy Parser for questions\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nparser = English()","ea565738":"!ls ..\/input\/quora-insincere-questions-classification","5faec029":"train_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nprint(\"Train shape: \", train_df.shape)\nprint(\"Test shape: \", test_df.shape)","1a497345":"train_df.head()","f60a40fd":"train_df.dtypes","e896678f":"!ls ..\/input\/quora-insincere-questions-classification\/embeddings.zip","6c245a2e":"# Target count\ncnt_srs = train_df['target'].value_counts()\ntrace = go.Bar(\nx=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n# Target distribution\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","25c0dff7":"# Split sentences into a dictionary of uniquely occuring words and their frequencies\ndef word_freq_dict(text):\n    # Convert text into word list\n    wordList = text.split()\n    # Generate word freq dictionary\n    wordFreqDict = {word: wordList.count(word) for word in wordList}\n    return wordFreqDict\n\n# Plot a wordcloud from a word frequency dictionary\ndef word_cloud_from_frequency(word_freq_dict, title, figure_size=(10,6)):\n    wordcloud.generate_from_frequencies(word_freq_dict)\n    plt.figure(figsize=figure_size)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()","94d87c71":"# Wordcloud of a random sample of 1000 insincere questions\ninsincere_questions = train_df.question_text[train_df['target'] == 1]\ninsincere_sample = \" \".join(insincere_questions.sample(1000, random_state=1).values)\ninsincere_word_freq = word_freq_dict(insincere_sample)\nwordcloud = WordCloud(width= 5000,\n    height=3000,\n    max_words=200,\n    colormap='Reds',\n    background_color='white')\n\nword_cloud_from_frequency(insincere_word_freq, \"Most Frequent Words in a sample of 1000 raw questions flagged insincere\")","adbf530f":"# Wordcloud of a random sample of 1000 sincere questions\nsincere_questions = train_df.question_text[train_df['target'] == 0]\nsincere_sample = \" \".join(sincere_questions.sample(1000, random_state=1).values)\nsincere_word_freq = word_freq_dict(sincere_sample)\nwordcloud = WordCloud(width= 5000,\n    height=3000,\n    max_words=200,\n    colormap='Blues',\n    background_color='white')\n\nword_cloud_from_frequency(sincere_word_freq, \"Most Frequent Words in a sample of 1000 raw questions flagged sincere\")","2401e634":"train_insincere_df = train_df[train_df[\"target\"]==1]\ntrain_sincere_df = train_df[train_df[\"target\"]==0]\n\nstopwords = set(STOPWORDS)\nmore_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\nstopwords = stopwords.union(more_stopwords)\n    \n# N-gram generation\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Horizontal bar chart\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace","b8351a71":"# Get the bar chart from sincere questions #\nfreq_dict = defaultdict(int)\nfor sent in train_sincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Get the bar chart from insincere questions #\nfreq_dict = defaultdict(int)\nfor sent in train_insincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","cb0b7b93":"freq_dict = defaultdict(int)\nfor sent in train_sincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train_insincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere questions\", \n                                          \"Frequent bigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","0cfa306f":"freq_dict = defaultdict(int)\nfor sent in train_sincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train_insincere_df[\"question_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of sincere questions\", \n                                          \"Frequent trigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","3fedc994":"# Number of words in the text\ntrain_df[\"num_words\"] = train_df[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_df[\"num_unique_words\"] = train_df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n# Number of characters in the text\ntrain_df[\"num_chars\"] = train_df[\"question_text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"question_text\"].apply(lambda x: len(str(x)))\n\n# Number of stopwords in the text\ntrain_df[\"num_stopwords\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# Number of punctuations in the text\ntrain_df[\"num_punctuations\"] =train_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n# Number of title case words in the text\ntrain_df[\"num_words_upper\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# Number of title case words in the text\ntrain_df[\"num_words_title\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n# Average length of the words in the text\ntrain_df[\"mean_word_len\"] = train_df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","478a0e39":"# Truncate some extreme values for better visuals\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 \ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 \ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350\n\n# Box plot making\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","40558f40":"def spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\ntqdm.pandas()\nsincere_questions = train_sincere_df[\"question_text\"].progress_apply(spacy_tokenizer)\ninsincere_questions = train_insincere_df[\"question_text\"].progress_apply(spacy_tokenizer)","543b34ed":"# One function for all plots\ndef plot_readability(a,b,title,bins=0.1,colors=['#0000FF', '#FF0000']):\n    trace1 = ff.create_distplot([a,b], [\"Sincere questions\",\"Insincere questions\"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\"Sincere questions\",\"Insincere questions\"],\n                [\"Mean\",mean(a),mean(b)],\n                [\"Standard Deviation\",pstdev(a),pstdev(b)],\n                [\"Variance\",pvariance(a),pvariance(b)],\n                [\"Median\",median(a),median(b)],\n                [\"Maximum value\",max(a),max(b)],\n                [\"Minimum value\",min(a),min(b)]]\n    trace2 = ff.create_table(table_data)\n    iplot(trace2, filename='Table')","a1107f08":"syllable_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.syllable_count))\nsyllable_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.syllable_count))\nplot_readability(syllable_sincere,syllable_insincere,\"Syllable Analysis\",5)","4509580f":"lexicon_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.lexicon_count))\nlexicon_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.lexicon_count))\nplot_readability(lexicon_sincere,lexicon_insincere,\"Lexicon Analysis\",4)","43cdd8f3":"length_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(len))\nlength_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(len))\nplot_readability(length_sincere,length_insincere,\"Question Length\",40)","b6dece53":"spw_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.avg_syllables_per_word))\nspw_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.avg_syllables_per_word))\nplot_readability(spw_sincere,spw_insincere,\"Average syllables per word\",0.2)","b8683428":"lpw_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.avg_letter_per_word))\nlpw_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.avg_letter_per_word))\nplot_readability(lpw_sincere,lpw_insincere,\"Average letters per word\",2)","687e08cb":"fre_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.flesch_reading_ease))\nfre_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.flesch_reading_ease))\nplot_readability(fre_sincere,fre_insincere,\"Flesch Reading Ease\",20)","55cce0ae":"fkg_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.flesch_kincaid_grade))\nfkg_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.flesch_kincaid_grade))\nplot_readability(fkg_sincere,fkg_insincere,\"Flesch Kincaid Grade\",4)","e73c952e":"fog_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.gunning_fog))\nfog_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.gunning_fog))\nplot_readability(fog_sincere,fog_insincere,\"The Fog Scale (Gunning FOG Formula)\",4)","5346227c":"ari_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.automated_readability_index))\nari_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.automated_readability_index))\nplot_readability(ari_sincere,ari_insincere,\"Automated Readability Index\",10)","8f690c83":"cli_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.coleman_liau_index))\ncli_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.coleman_liau_index))\nplot_readability(cli_sincere,cli_insincere,\"The Coleman-Liau Index\",10)","5410e319":"lwf_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.linsear_write_formula))\nlwf_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.linsear_write_formula))\nplot_readability(lwf_sincere,lwf_insincere,\"Linsear Write Formula\",2)","42bb5883":"dcr_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(textstat.dale_chall_readability_score))\ndcr_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_sincere,dcr_insincere,\"Dale-Chall Readability Score\",1)","d2c86c96":"def consensus_all(text):\n    return textstat.text_standard(text,float_output=True)\n\ncon_sincere = np.array(train_sincere_df[\"question_text\"].progress_apply(consensus_all))\ncon_insincere = np.array(train_insincere_df[\"question_text\"].progress_apply(consensus_all))\nplot_readability(con_sincere,con_insincere,\"Readability Consensus based upon all the above tests\",2)","e1964154":"***Observations from unigram count plot***\n\n* Some of the top words are common across both the classes like 'people', 'will', 'think' etc.\n* Many of top words in sincere questions (after excluding the both common ones) used for describe and comparison purpose: 'best', 'good', 'much', etc.\n* The other top words in insincere questions (after excluding the both common ones) often involve matters that may be sensitive or controversial: 'trump', 'women', 'white', etc.","6f5db665":"Code cell below are modules and libraries used in this notebook:\n\n* os: provides functions for interacting with the operating system. \n* json: built-in package which can be used to work with JSON data.\n* string: provides additional tools to manipulate strings.\n* math: built-in module for mathematical tasks.\n* collections: implements specialized container datatypes providing alternatives to Python\u2019s general purpose built-in containers, dict, list, set, and tuple.\n* warnings: filterwarnings(action) with action as \"ignore\" to suppress all warnings.\n* statistics: provides functions for calculating mathematical statistics of numeric (Real-valued) data.\n* tqdm: output a progress bar by wrapping around any iterable.\n\n\n* NumPy: NumPy is the fundamental package for array computing with Python.\n* pandas: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n* Matplotlib: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n* Seaborn: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n* WordCloud: Word cloud is a technique for visualising frequent words in a text where the size of the words represents their frequency.\n* plotly: An open-source, interactive data visualization library for Python.\n* spaCy: spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.\n","5c2aca5a":"There are various of word presents in both types of questions (obviously).\nBut these two wordcloud seems to be not useful at all, which can be explained by two main reason: (i) wordcloud is made by gathering only 2000 of over 1000000 questions, so total figure can be way more different, and (ii) too many noise and uninformative words(such as what, when, etc., which obviously appears in a question no matter it is toxic or not).\n\nMaybe it is a good idea to look at the most frequent words in each of the classes separately. ","d8c109ee":"## Input","ca31c3e6":"Generally speaking, questions given in the dataset are fairly easy to read (mean value is ~70-75).","4d3b0d67":"## Problem Description\n        \nAn existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n\nQuora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions - those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n\nIn this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. More scalable methods could be developed to detect toxic and misleading content.","3e346cbe":"### Bigram","f4b0b32f":"### Unigram","52cf3cc4":"**Flesch reading ease**\n\nIn the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?206.835&space;-&space;1.015\\left(\\frac{total\\;words}{total\\;sentences}\\right)&space;-&space;84.6\\left(\\frac{total\\;syllables}{total\\;words}\\right)\" title=\"206.835 - 1.015\\left(\\frac{total\\;words}{total\\;sentences}\\right) - 84.6\\left(\\frac{total\\;syllables}{total\\;words}\\right)\" \/>\n\nScores can be interpreted as shown in the table below.\n\n|     Score    |  School level (US) |                                  Notes                                  |\n|:------------:|:------------------:|:-----------------------------------------------------------------------:|\n| 100.00\u201390.00 | 5th grade          | Very easy to read. Easily understood by an average 11-year-old student. |\n| 90.0\u201380.0    | 6th grade          | Easy to read. Conversational English for consumers.                     |\n| 80.0\u201370.0    | 7th grade          | Fairly easy to read.                                                    |\n| 70.0\u201360.0    | 8th & 9th grade    | Plain English. Easily understood by 13- to 15-year-old students.        |\n| 60.0\u201350.0    | 10th to 12th grade | Fairly difficult to read.                                               |\n| 50.0\u201330.0    | College            | Difficult to read.                                                      |\n| 30.0\u201310.0    | College graduate   | Very difficult to read. Best understood by university graduates.        |\n| 10.0\u20130.0     | Professional       | Extremely difficult to read. Best understood by university graduates.   |","8aef6b70":"* GoogleNews-vectors-negative300 - https:\/\/code.google.com\/archive\/p\/word2vec\/\n* glove.840B.300d - https:\/\/nlp.stanford.edu\/projects\/glove\/\n* paragram_300_sl999 - https:\/\/cogcomp.org\/page\/resource_view\/106\n* wiki-news-300d-1M - https:\/\/fasttext.cc\/docs\/en\/english-vectors.html","b4c6b1b3":"***Observations from trigram count plot***\n\n* We can now easily inspect many trigrams are combinations of popular uni- and bigrams. So further plot count (```n>3```) is now unnecessary.\n* More detailed and informational phrases appear: 'black lives matter', 'gun control advocates', etc. \n* Unexpectedly, people's ages were mentioned frequently in the questions, in both categories. Perhaps people really care a lot about characteristics such as physical, psychological, cognitive level of others - traits that are influenced by certain age.","3ab43eb0":"## Detailed Statistics for given data","958c2625":"# Abstract\nAfter having built [the baseline model (version 0.0)](https:\/\/www.kaggle.com\/lukeshrek\/classify-toxic-question) with Logistics Regression and TF-IDF Bi-grams Vectorizer, I personally found that model is still very simple and can be further improved to give better results.\n\nIn this version 1.0, I will proceed to go deeper into data analysis and visualization, improve the preprocessing steps and build the model based on the Bidirectional GRU network.\n\nThis notebook represent Data Exploration and Analysis works. ","5627dcd9":"### Question Length","4ecb325e":"In this notebook I will use various package to support data visualize and analyze process.\n\n* Textstat: Textstat is an easy to use library to calculate statistics from text. It helps determine readability, complexity, and grade level.\n* Chart Studio: Chart Studio provides a web-service for hosting graphs. Graphs are saved inside online Chart Studio account.","96c0214b":"## Data Description\nIn this competition the model should be able to detect whether a question asked on Quora is sincere or not. An insincere question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is insincere:\n\n* Has a non-neutral tone\n    * Has an exaggerated tone to underscore a point about a group of people\n    * Is rhetorical and meant to imply a statement about a group of people\n* Is disparaging or inflammatory\n    * Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n    * Makes disparaging attacks\/insults against a specific person or group of people\n    * Based on an outlandish premise about a group of people\n    * Disparages against a characteristic that is not fixable and not measurable\n* Isn't grounded in reality\n    * Based on false information, or contains absurd assumptions\n* Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n\nThe training data includes the question that was asked, and whether it was identified as insincere (target = 1) or not (target = 0).","9cb4d394":"Let's see how train data and test data are distributed.","7fa15117":"## Target Count and Target Distribution","083bc14f":"Basically embeddings folder are zipped so we cannot show its details by using ```!ls```, but information about the allowed embeddings are available in the competition's data description.","dd3dccb0":"### Plotting meta features","476cfdc1":"### Readability Features","71a9f505":"As mentioned above, bigram and trigram count plot are made by the same process as unigram: Lowercase non-stopword words, zip into grams and count. ","7987ebd4":"### Average Letter per Word","c03fa3c3":"## Meta Features\n\nFor further exploration, let's create some meta features and then look at how they are distributed between the classes. The ones that we will create are\n\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words","b936ff57":"So about 6% of the training data are insincere questions (target=1) and rest of them are sincere.","de6caf1c":"It can be easily detected that the dataset is imbalance, in which the vast majority of questions are sincere, and only a small number are insincere. Let us look at the distribution of the target variable by plotting a bar chart and a pie chart to understand more.","4c04b247":"## Embeddings ","bb2e5832":"**Linsear Write Formula**\n\nThe standard Linsear Write metric Lw runs on a 100-word sample:\n1. For each \"easy word\", defined as words with 2 syllables or less, add 1 point.\n2. For each \"hard word\", defined as words with 3 syllables or more, add 3 points.\n3. Divide the points by the number of sentences in the 100-word sample.\n4. Adjust the provisional result r:\n* If r > 20, Lw = r \/ 2.\n* If r \u2264 20, Lw = r \/ 2 - 1.\nThe result is a \"grade level\" measure, reflecting the estimated years of education needed to read the text fluently","2bc2f41b":"***Observations from bigram count plot***\n\n* Some of the top bigrams are common across both the classes are the combinations of a top word among both classes and a top word among one class, especially among sincere questions, for example: 'best way'. People tend to find useful answer for the best method to do something, therefore, they may need to provide informative question.\n* It can be inspected that several topics appear: 'computer science', 'machine learning', 'tv shows', etc. \n* The top bigrams in insincere questions still involve matters that may be sensitive or controversial (and significantly related to people, religion, politics) such as 'donald trump', 'white people', 'black people', etc. This observation is quite reasonable and similar to characteristics (provided in the competition description) that can signify that a question is insincere.","a07cdc20":"To illustrate some popular summarized figures, I created a table data for all plots (which will be described below). \n\nThe figures used are provided by ```math``` module: Mean, Standard Deviation, Variance, Median, Max, Min.","30ed10f9":"**Readability Consensus based upon all the above tests**\n\nThe estimated school grade level required to understand the text based on all above tests.","3cc9d853":"***Observations from Meta Features*** \n\nWe can see that the insincere questions have more number of words as well as characters compared to sincere questions. So this might be a useful feature in our model.","79549dba":"### Trigram","c22303c4":"```textstat``` package is used for this purpose. \n\nBefore any further analyze, all questions must be tokenized. I used spaCy tokenizer to perform the task, following with ```tqdm``` to visualize progress bars.","26a91ef4":"To have a better view of how these meta features are distributed I use some box plots.\n\nIn descriptive statistics, a box plot or boxplot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis. Box plots visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages.\n\nBox plots show the five-number summary of a set of data: including the minimum score, first (lower) quartile, median, third (upper) quartile, and maximum score.\n\n* Minimum Score: The lowest score, excluding outliers (shown at the end of the left whisker).\n* Lower Quartile: Twenty-five percent of scores fall below the lower quartile value (also known as the first quartile).\n* Median: The median marks the mid-point of the data and is shown by the line that divides the box into two parts (sometimes known as the second quartile). Half the scores are greater than or equal to this value and half are less.\n* Upper Quartile: Seventy-five percent of the scores fall below the upper quartile value (also known as the third quartile). Thus, 25% of data are above this value.\n* Maximum Score: The highest score, excluding outliers (shown at the end of the right whisker).\n\nSome other summary are:\n* Whiskers: The upper and lower whiskers represent scores outside the middle 50% (i.e. the lower 25% of scores and the upper 25% of scores).\n* The Interquartile Range (or IQR): This is the box plot showing the middle 50% of scores (i.e., the range between the 25th and 75th percentile).","19ac6a91":"### Lexicon Analysis","532c98af":"We will take a look at the input data directory","915009e6":"## Word n-grams Count Plot\n\nIn the following cell, I have created 1, 2 and 3-gram count plot with similar process.\nBefore looking up for most useful frequent grams, it has to be ensure that all of the stopwords need to be eliminated from the counter. Words are all lowercased before zipping a number of words (based on parameter ```n_gram``` value). Finally, a frequency dictionary is created and count all words appear.","9dbc324f":"### Syllable Analysis","4dfa4ac5":"**Automated Readability Index**\n\nThe automated readability index (ARI) is a readability test for English texts, designed to gauge the understandability of a text. Like the Flesch\u2013Kincaid grade level, Gunning fog index, SMOG index, Fry readability formula, and Coleman\u2013Liau index, it produces an approximate representation of the US grade level needed to comprehend the text.\n\nThe formula for calculating the automated readability index is given below:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?4.71\\left(\\frac{character}{words}\\right)&space;+&space;0.5\\left(\\frac{words}{sentences}\\right)&space;-&space;21.43\" title=\"4.71\\left(\\frac{character}{words}\\right) + 0.5\\left(\\frac{words}{sentences}\\right) - 21.43\" \/>\n\n| Score | Age   | Grade Level        |\n|-------|-------|--------------------|\n| 1     | 5-6   | Kindergarten       |\n| 2     | 6-7   | First\/Second Grade |\n| 3     | 7-9   | Third Grade        |\n| 4     | 9-10  | Fourth Grade       |\n| 5     | 10-11 | Fifth Grade        |\n| 6     | 11-12 | Sixth Grade        |\n| 7     | 12-13 | Seventh Grade      |\n| 8     | 13-14 | Eighth Grade       |\n| 9     | 14-15 | Ninth Grade        |\n| 10    | 15-16 | Tenth Grade        |\n| 11    | 16-17 | Eleventh Grade     |\n| 12    | 17-18 | Twelfth grade      |\n| 13    | 18-24 | College student    |\n| 14    | 24+   | Professor          |","d0a9f494":"### Average Syllables per Word","ba00bed1":"**The Coleman-Liau Index**\n\nThe Coleman\u2013Liau index is a readability test designed by Meri Coleman and T. L. Liau to gauge the understandability of a text. Its output approximates the U.S. grade level thought necessary to comprehend the text.\n\nLike the ARI but unlike most of the other indices, Coleman\u2013Liau relies on characters instead of syllables per word.\n\nThe Coleman\u2013Liau index is calculated with the following formula:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?CLI&space;=&space;0.0588L&space;-&space;0.296S&space;-&space;15.8\" title=\"CLI = 0.0588L - 0.296S - 15.8\" \/>","5823056b":"# 1. Introduction","a96d1c92":"**The Flesch-Kincaid Grade Level**\n\nThe \"Flesch\u2013Kincaid Grade Level Formula\" presents a score as a U.S. grade level, making it easier to judge the readability level of texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?0.39&space;-&space;11.8\\left(\\frac{total\\;words}{total\\;sentences}\\right)&space;-&space;15.59\\left(\\frac{total\\;syllables}{total\\;words}\\right)\" title=\"0.39 - 11.8\\left(\\frac{total\\;words}{total\\;sentences}\\right) - 15.59\\left(\\frac{total\\;syllables}{total\\;words}\\right)\" \/>\n\nThe result is a number that corresponds with a U.S. grade level. ","997c4321":"## WordCloud of questions","ed853052":"**Dale\u2013Chall Readability Formula**\n\nThe Dale\u2013Chall readability formula is a readability test that provides a numeric gauge of the comprehension difficulty that readers come upon when reading a text. It uses a list of 3000 words that groups of fourth-grade American students could reliably understand, considering any word not on that list to be difficult.\n\nThe formula for calculating the raw score of the Dale\u2013Chall readability score (1948) is given below:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?0.1579\\left&space;(&space;&space;\\frac{difficult\\;words}{words}\\ast&space;&space;100\\right&space;)&space;&plus;&space;0.0496\\left&space;(&space;\\frac{words}{sentences}&space;\\right&space;)\" title=\"0.1579\\left ( \\frac{difficult\\;words}{words}\\ast 100\\right ) + 0.0496\\left ( \\frac{words}{sentences} \\right )\" \/>\n\nIf the percentage of difficult words is above 5%, then add 3.6365 to the raw score to get the adjusted score, otherwise the adjusted score is equal to the raw score.\n\n|     Score    |                                 Notes                                |\n|:------------:|:--------------------------------------------------------------------:|\n| 4.9 or lower | easily understood by an average 4th-grade student or lower           |\n| 5.0\u20135.9      | easily understood by an average 5th or 6th-grade student             |\n| 6.0\u20136.9      | easily understood by an average 7th or 8th-grade student             |\n| 7.0\u20137.9      | easily understood by an average 9th or 10th-grade student            |\n| 8.0\u20138.9      | easily understood by an average 11th or 12th-grade student           |\n| 9.0\u20139.9      | easily understood by an average 13th to 15th-grade (college) student |","cd26471a":"**The Fog Scale (Gunning FOG Formula)**\n\nThe Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. \n\nThe Gunning fog index is calculated with the following formula:\n\n<img src=\"https:\/\/latex.codecogs.com\/svg.image?0.4\\left&space;[&space;\\left&space;(&space;\\frac{words}{sentences}&space;\\right&space;)&plus;&space;100&space;&space;\\left&space;(&space;\\frac{complex\\;words}{words}&space;\\right)&space;\\right]\" title=\"0.4\\left [ \\left ( \\frac{words}{sentences} \\right )+ 100 \\left ( \\frac{complex\\;words}{words} \\right) \\right]\" \/>\n\nThe fog index is commonly used to confirm that text can be read easily by the intended audience. Texts for a wide audience generally need a fog index less than 12. Texts requiring near-universal understanding generally need an index less than 8.","ed140bfa":"* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - A sample submission in the correct format\n* enbeddings\/ - Folder containing word embeddings.\n\nExternal data sources are not allowed to use. The following embeddings are given to us which can be used for building our models.","528a08bd":"To find the most frequently occuring words in questions, I built word clouds of a random sample of 1000 insincere and 1000 sincere questions on the ```question_text``` column.\n\nFirst of all, questions are concatenated into a single string. Next, the question string is splitted into a dictionary and each words are counted uniquely to find the time that word appears. Finally, ```generate_from_frequencies``` from WordCloud libraries is used to plot the word cloud from frequency dictionary.","6ecd533a":"These packages are available for installing using pip.","835cd0d0":"* qid - unique question identifier\n* question_text - Quora question text\n* target - a question labeled \"insincere\" has a value of 1, otherwise 0","cf1c56c8":"# 2. Data Exploration and Visualization","4c1e4a4d":"## Initial Configurations"}}