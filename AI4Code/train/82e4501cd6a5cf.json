{"cell_type":{"5e7e1257":"code","28741906":"code","8bd68a6b":"code","b16c732b":"code","504270b5":"code","976d1501":"code","2f92f216":"code","d82787b7":"code","1a0bb13a":"code","768d19b2":"code","8147d71d":"code","396514c7":"code","074809c1":"code","7603c44a":"code","d2b584c2":"code","18045a45":"code","37c21f54":"code","2a638476":"code","f1ae09a8":"code","d14f8477":"markdown","bc76076f":"markdown","9f8d9528":"markdown","155a1ae7":"markdown","5fb30533":"markdown","b13227a9":"markdown","7cc8a749":"markdown","e6c6eaeb":"markdown","609a41d4":"markdown"},"source":{"5e7e1257":"import torch\nimport numpy as np","28741906":"file_object = open(r'..\/input\/kanye_verses.txt',mode='r',encoding='utf8')\nraps = file_object.read().replace('\\n',\" \\\\n \")\nraps = raps.replace(',',' ,')\nraps = \" \".join(raps.split())","8bd68a6b":"import unicodedata\nimport string","b16c732b":"characters = string.ascii_letters + \" .,;'\\\\\"\nn_characters = len(characters)\n\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in characters+'\\n'\n    )\nraps = unicodeToAscii(raps)","504270b5":"def letter_encode(letter):\n    x = np.zeros((1,n_characters))\n    x[0][characters.index(letter)] = 1\n    return x\n\ndef word_encode(word):\n    return (words_arr == word).astype(np.float).reshape(1,-1)","976d1501":"words_arr = np.array(list(set(raps.split())))\nn_words = len(words_arr)","2f92f216":"# input is (batch,seq_len,input_size)\nX_train = torch.FloatTensor([word_encode(x) for x in raps.split()])\ny_train = X_train[1:].argmax(dim=2).long()\nX_train = X_train[:-1]","d82787b7":"input_size = n_words\nhidden_size = 256 \nsequence_length = 1 # character by character\nnum_layers = 1 # one-layer rnn\noutput_size = n_words","1a0bb13a":"import torch.nn as nn","768d19b2":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        \n        self.cell = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          num_layers=num_layers,\n                          batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size,output_size)\n        \n#         self.encoder = nn.Embedding(input_size, hidden_size)\n        \n    def forward(self,input):\n        batch_size = input.size()[0]\n        hidden = self._init_hidden(batch_size)\n        output,hidden = self.cell(input,hidden)\n        fc_output = self.fc(output.view(-1,hidden_size))\n        return fc_output\n    \n    def forward2(self,input,hidden):\n        batch_size = input.size()[0]\n        output,hidden = self.cell(input,hidden)\n        fc_output = self.fc(output.view(-1,hidden_size))\n        return fc_output,hidden\n    \n    def _init_hidden(self,batch_size):\n        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n        return hidden","8147d71d":"rnn = Model()\nindexer = rnn.forward((X_train[0:64])).argmax(dim=1)","396514c7":"import torch.optim as optim","074809c1":"optimizer = optim.Adam(rnn.parameters(),lr=.005)\nloss_fn = nn.CrossEntropyLoss()","7603c44a":"chunk_len = 200\ndef random_chunk():\n    start_index = torch.randint(0,X_train.size()[0]-chunk_len,(1,1))\n    end_index = start_index + chunk_len + 1\n    char_chunk = X_train[start_index:end_index]\n    return char_chunk[:-1].permute(1,0,2),char_chunk[1:].argmax(dim=2).squeeze()","d2b584c2":"import torch.nn.functional as F","18045a45":"def evaluate():\n    string = \"\"\n    \n    start_letter = words_arr[torch.randint(0,n_words,(1,1))]\n    hidden = rnn._init_hidden(1)\n    with torch.no_grad():\n        for i in range(chunk_len-1):\n            letter,hidden = rnn.forward2(torch.FloatTensor(word_encode(start_letter)).unsqueeze(0),hidden)\n            letter = words_arr[torch.multinomial(F.softmax(letter.view(1,-1)),1)]\n            start_letter = str(letter)\n            string += (\" \" + letter)\n    \n    print(string.replace('\\\\n','\\n'))","37c21f54":"rap_length = 50\ndef rap_lyrics():\n    string = \"\"\n    \n    start_letter = words_arr[torch.randint(0,n_characters,(1,1))]\n    hidden = rnn._init_hidden(1)\n    with torch.no_grad():\n        for i in range(rap_length -1):\n            \n            letter,hidden = rnn.forward2(torch.FloatTensor(word_encode(start_letter)).unsqueeze(0),hidden)\n            if (i+1)%2 == 0:\n                letter = words_arr[torch.multinomial(F.softmax(letter.view(1,-1)),1)]\n            else:\n                letter = words_arr[letter.view(1,-1).argmax(1)]\n            start_letter = str(letter)\n            string += (\" \" + letter)\n    \n    print(string.replace('\\\\n','\\n'))","2a638476":"epochs = 4000\nfor epoch in range(epochs):\n    epoch_loss = 0\n    xb,yb = random_chunk()\n    output = rnn(xb)\n    loss = loss_fn(output,yb)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if (epoch+1) % 50 == 0:\n        print()\n        print(epoch+1,'\/',epochs,\" Loss: \",loss.item(),sep='')\n        print()        \n#         evaluate()\n        rap_lyrics()","f1ae09a8":"# write a wrap dog\nrap_length = 1000\n\nrap_lyrics()","d14f8477":"## Write A Rap","bc76076f":"## Preprocess Text","9f8d9528":"## Define GRU RNN Model","155a1ae7":"## Model Parameters","5fb30533":"## Train!","b13227a9":"# Read in text","7cc8a749":"## Create letter and word encoder (one-hot encodings)","e6c6eaeb":"## Optimizer and Loss Criterion","609a41d4":"## Create training test set as one-hot encoded words"}}