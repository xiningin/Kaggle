{"cell_type":{"3f1ba0a2":"code","89ce36e0":"code","9af7da89":"code","bfbb97f5":"code","8513d56b":"code","96c20156":"code","300c41bb":"code","dff88697":"code","5d5dd4c3":"code","ba8ab89d":"code","cf371b7f":"code","1813f031":"code","12cb39c2":"code","3ed7ae7e":"code","e511b361":"code","1e660715":"code","a42ce06f":"code","244521fb":"code","9408db5d":"code","d0cc169c":"code","dc892b49":"code","d522eddb":"code","da57cb6c":"code","d09e6de8":"code","d4800c51":"code","6dd85800":"code","2322ba15":"code","4a8c7865":"code","332c5ea5":"code","dfdd90cb":"code","a614fc5a":"code","acef4443":"code","3ac39dcc":"code","b1f57a59":"code","49537b20":"code","c9db56c2":"code","dbd18b84":"code","80a881dd":"code","c93c0751":"code","95dbde8f":"code","840ca47e":"code","12f2f173":"markdown","d3b11b88":"markdown","3a743720":"markdown","bfc9922c":"markdown","72a01ed6":"markdown","60996771":"markdown","9ecdb1b1":"markdown","235b8671":"markdown","18f3c212":"markdown","fb5d8626":"markdown","56f2390a":"markdown","07d9861c":"markdown","aab1a48e":"markdown","5f35af9f":"markdown","185bd695":"markdown","5db51fd9":"markdown","c5c140ae":"markdown","e2fbf4b9":"markdown","efa22851":"markdown","875fc8c7":"markdown","9f6a2e79":"markdown","22918d7d":"markdown","404c22a1":"markdown","8d02858b":"markdown","762687f7":"markdown","e077d64c":"markdown","d86d37b8":"markdown","e724e21b":"markdown","657a0dc0":"markdown","624330ac":"markdown","d71cef2c":"markdown","ca0fcbe8":"markdown","1ba4d372":"markdown","56bed5af":"markdown","cbaee507":"markdown","301abd58":"markdown"},"source":{"3f1ba0a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting libraries\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Statistical libraries\nfrom scipy import stats\n\n# Model libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89ce36e0":"train = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/train.csv')\ntest = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/test.csv')","9af7da89":"train.info()","bfbb97f5":"test.info()","8513d56b":"# I want to see general distribution of the continuous data\npd.set_option(\"max_columns\", 25)\ntrain.drop(['id', 'one if net income was negative for the last two year zero otherwise'], axis=1).describe()","96c20156":"train.loc[:, 'Bankrupt'].describe()","300c41bb":"train.loc[:, 'Bankrupt'].value_counts().sort_values(ascending=False) \/ train.loc[:, 'Bankrupt'].count()","dff88697":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(data=train, x=\"Bankrupt\", hue=\"Bankrupt\")","5d5dd4c3":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(data=train, x='one if net income was negative for the last two year zero otherwise')","ba8ab89d":"corr = train.drop('id', axis=1).corr()\ncorr","cf371b7f":"plt.figure(figsize=(30,20))\nhm = sns.heatmap(corr.iloc[:32,:32], cmap=\"viridis\", square=True,\n           linewidth=0.5, annot=True)\nhm.set_xticklabels(hm.get_xticklabels(), rotation=60, rotation_mode='anchor', position=(0,1.11)) ","1813f031":"plt.figure(figsize=(25,20))\nhm = sns.heatmap(corr.iloc[32:64,32:64], cmap=\"viridis\", square=True,\n           linewidth=0.5, annot=True)\nhm.set_xticklabels(hm.get_xticklabels(), rotation_mode='anchor', rotation=60, position=(0, 1.11)) ","12cb39c2":"plt.figure(figsize=(25,20))\nhm = sns.heatmap(corr.iloc[64:,64:], cmap=\"viridis\", square=True,\n           linewidth=0.5, annot=True)\nhm.set_xticklabels(hm.get_xticklabels(), rotation_mode='anchor', rotation=60, position=(0, 1.11))","3ed7ae7e":"train.isnull().sum()","e511b361":"test.isnull().sum()","1e660715":"# Lets create a boxplot to check the outliers of just one feature to start with\nsns.boxplot(x=train[' ROA(C) before interest and depreciation before interest'])","a42ce06f":"# This gets the z scores of the train dataset features\nz = np.abs(stats.zscore(train.drop('one if net income was negative for the last two year zero otherwise', axis=1)))\nz_df = pd.DataFrame(z)\nz_df","244521fb":"z_df.describe()","9408db5d":"# Filters out the samples of the dataset with outliers\ntrain_filtered = train[(z_df < 3).all(axis=1)]\ntrain_filtered.info()","d0cc169c":"sns.countplot(data=train_filtered, x='Bankrupt')","dc892b49":"y_train = train['Bankrupt']\ny_train","d522eddb":"X_train = train.iloc[:, 2:]\nX_train= pd.get_dummies(X_train)\ny_train = train[\"Bankrupt\"]\nX_test = pd.get_dummies(test.iloc[:,1:])","da57cb6c":"# Create train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.2, random_state=12)\n\n# Setup the pipeline steps: steps\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=12)\n\n# Create the pipeline: pipeline\n# pipeline = Pipeline(steps)\n\n# Fit the pipeline to the training set: tree_scaled\ntree.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred_prob_tree= tree.predict_proba(X_test)[:,1]\ny_pred_prob_tree.shape","d09e6de8":"# Drop last row\ny_train = y_train.iloc[:-1]","d4800c51":"# Generate forest  ROC curve values: fpr, tpr, thresholds\nfpr_tree,tpr_tree, thresholds_tree = roc_curve(y_train, y_pred_prob_tree)\n\n# Plot forest ROC curve\nfig, ax1 = plt.subplots(1,1, figsize=(8,5))\nax1.plot([0, 1], [0, 1], 'k--')\nax1.plot(fpr_forest, tpr_forest)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('Tree ROC Curve')","6dd85800":"# Area under ROC curve for scaled dataset\nroc_auc = roc_auc_score(y_train, y_pred_prob_tree)\nroc_auc\n","2322ba15":"print('Classification Report: \\n{}'.format(classification_report(y_train, y_pred_prob_tree.astype(int))))","4a8c7865":"X_train = train.iloc[:, 2:]\nX_train= pd.get_dummies(X_train)\ny_train = train[\"Bankrupt\"]\nX_test = pd.get_dummies(test.iloc[:,1:])","332c5ea5":"#  Create Random forest\nforest = RandomForestClassifier(n_estimators=2000, max_depth=500, criterion='entropy')\n\n# Fit random forest\nforest.fit(X_train,y_train)\n\n# Create predictions\ny_pred_prob_forest = forest.predict_proba(X_test)[:,1]","dfdd90cb":"# Drop last row\ny_train = y_train.iloc[:-1]","a614fc5a":"# Generate forest  ROC curve values: fpr, tpr, thresholds\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_pred_prob_forest)\n\n# Plot forest ROC curve\nfig, ax1 = plt.subplots(1,1, figsize=(8,5))\nax1.plot([0, 1], [0, 1], 'k--')\nax1.plot(fpr_forest, tpr_forest)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('Forest ROC Curve')\n","acef4443":"roc_auc_forest = roc_auc_score(y_train, y_pred_prob_forest)\nroc_auc_forest","3ac39dcc":"y_pred_prob_forest","b1f57a59":"print('Classification Report: \\n{}'.format(classification_report(y_train, y_pred_prob_forest.astype(int))))","49537b20":"X_train = train.iloc[:, 2:]\nX_train= pd.get_dummies(X_train)\ny_train = train[\"Bankrupt\"]\nX_test = pd.get_dummies(test.iloc[:,1:])","c9db56c2":"clf = GradientBoostingClassifier(n_estimators=30, learning_rate=1.0, max_depth=8, random_state=22).fit(X_train, y_train)\nclf.fit(X_train, y_train)\ny_pred_prob_clf=forest.predict_proba(X_test)[:,1]","dbd18b84":"# Drop last row\ny_train = y_train.iloc[:-1]","80a881dd":"# Generate forest  ROC curve values: fpr, tpr, thresholds\nfpr_clf,tpr_clf, thresholds_clf = roc_curve(y_train, y_pred_prob_clf)\n\n# Plot forest ROC curve\nfig, ax1 = plt.subplots(1,1, figsize=(8,5))\nax1.plot([0, 1], [0, 1], 'k--')\nax1.plot(fpr_clf, tpr_clf)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('Gradient Boosted ROC Curve')","c93c0751":"roc_auc_clf = roc_auc_score(y_train, y_pred_prob_clf)\nroc_auc_clf","95dbde8f":"print('Classification Report: \\n{}'.format(classification_report(y_train, y_pred_prob_clf.astype(int))))","840ca47e":"# format predictions into a dataframe\npredictions = pd.DataFrame(y_pred_prob_forest, columns = ['Bankrupt'])\nids = pd.DataFrame(test.iloc[:, :1])\npredictions = pd.concat([ids, predictions], axis=1 )\n# output.insert(0,'id', range(0, len(output)))\npredictions\n\n# Store predictions to csv file\npredictions.to_csv('submission.csv', index=False)","12f2f173":"Below I create an array of the zscores of the features of every sample in the dataset, and convert it to a dataframe.","d3b11b88":"This would better understood visually if I use a heatmap of the correlation between column features. In order to make the heatmap digestable, I split the 97x97 correlation heatmap into thirds.","3a743720":"## Using Z scores to identify dataset outliers","bfc9922c":"I first want to exclude the categorical data and the feature indeces from the train dataset. Then I can get a statistical report of only the **continuous features** in the dataframe.","72a01ed6":"The second heatmap next one looks at the correlation between the next 32 features starting from feature 32.","60996771":"### Bankruptcy","9ecdb1b1":"## General information about the dataset","235b8671":"## Description of the categorical features in the dataset","18f3c212":"While I have a rough understanding of the dataset as a whole. I still dont understand which distinct features are more likely to have an impact on the financial health of a company in the dataset. I can create a pairwise correlation of the column features below to help me.","fb5d8626":"## Check for missing data","56f2390a":"## Check the data for any outliers","07d9861c":"Looking at the 'train' dataframe above, the dataframe has 97 columns (features) and 3,410 rows (samples).  \n\nAdditionally we can see that:\n* Every feature in the dataframe is either a Dtype of **float64** or **int64**.  \n* There are **no missing values** in any of the columns features.  \n* There are two categorical features **'Bankrupt'** and **'one if net income was negative for the last two year zero otherwise'**","aab1a48e":"The heatmap above looks at the correlation between the first 32 features.  \n  \nExamining the heatmap I can see which features correlate to each other and by how much. I can ignore features that are directly correlated with themselves because this is redundent.  \n\nThe feature 'operating profit rate' is interesting. It has a 0.88 positive correlation with the feature 'after-tax net interest rate', but a -0.66 negative correlation with the feature 'non-industry income and expenditure\/revenue'. \n","5f35af9f":"## Random Forest Classifier","185bd695":"Above I use **'.isnull().sum()'** methods to sum up all the **NaN** values that might exist in each column feature. Luckily, are no missing values in this dataset.","5db51fd9":"In the above box plot you can see that there are many outliers. This makes me curious to see how many other outliers we can find in the other features of the dataset.","c5c140ae":"Wow, well this is even worse... If I filter out the outliers in the train dataframe, it removes the remaining samples classified as bankrupt. This is bad instead of deleting I will just leave them alone.","e2fbf4b9":"Above the minimum value is 0 and the maximum value 1. The 1st, 2nd, and 3rd quantiles are empty and this makes sense because the 'Bankrupt' feature is categorical.","efa22851":"## Predictions on test set","875fc8c7":"## Correlation Of Features ","9f6a2e79":"precision (positive predicted values) = The number of correctly labeled 'Bankrupt' businesses  \nrecall = true positive rate of 'Bankrupt' businesses\nF1 score  \nHigh precision = Not many non bankrupt businesses are classified as 'Bankrupt'\nHigh recall = Most 'Bankrupt' businesses were classified correctly","22918d7d":"## Tree Classifier","404c22a1":"This is much much better. :)  \nRandom forest performs much better on the train dataset, so I will use this model instead of the decision tree.","8d02858b":"# Exploratory data analysis","762687f7":"No standardization or normalization used, it turns our that decision trees dont benefit much from either of the scaling methods.","e077d64c":"Looking at the description of the z score dataframe above, there exists some samples of features with z scores greater than 3, these are outliers.","d86d37b8":"Wow, the distribution of this feature is completely imbalanced. It seems a bit odd to me that while almost 97% of the companies in the dataset are not bankrupt, all of them reported a **negative net income** for the previous 2 years.  \n  \nThis makes no sense as a company with a negative net income is most likely to go bankrupt.\n1. The features **'Bankrupt'** and **'one if net income was negative for the last two year zero otherwise'** just have little to no correlation to each other.\n2. There is a data quality issue pertaining to the cardinality of the **'one if net income was negative for the last two year zero otherwise'** feature, and should be removed.  \n  \nHowever, the 1st possibility can be ruled out because a low correlation between the two features would the existence of samples where 'one if net income was negative for the last two year zero otherwise' is 1.  \n  \nThis means that it is a cardinality issue, and this feature should be removed from the dataset entirely. Lets do that now.","e724e21b":"It seems that the distribution of bankrupt companies is **imbalanced**. Close to **97% of the businesses** listed in the dataset **classified as not bankrupt**.","657a0dc0":"## Description of the continuous features in the dataset","624330ac":"Before I create a classifier, I want to know more about my data. For instance, it would help to know what features the 'train' dataframe has, the type of values that correspond to those features, and the number of rows\/columns in the dataframe.","d71cef2c":"First I want to load the dataset for this assignment into different pandas dataframes for later use. I do this below using the '.read_csv' method below:","ca0fcbe8":"## Decision Tree Metrics","1ba4d372":"The third and final heatmap looks at the correlation between the last 32 features starting from feature 64.","56bed5af":"I can get general information about the contents of 'train' dataframe using the **'.info()'** method below.","cbaee507":"The dataset is loaded into train and test sets. Now it is time for some exploratory data analysis of the dataset.","301abd58":"# Load dataset into notebook"}}