{"cell_type":{"85c62c19":"code","db77d96e":"code","9b50f79b":"code","6ecb57d8":"code","2b845dc3":"code","579767aa":"code","e2672d32":"code","b57c25fd":"code","d9b87f7e":"code","64b83465":"code","acb1b584":"code","a706cd7e":"code","bba9e2b3":"code","ae1e9e5a":"markdown","fbe12aae":"markdown","db07574a":"markdown","412d1220":"markdown","95601e74":"markdown","eabd90be":"markdown"},"source":{"85c62c19":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","db77d96e":"# solve_missing {'drop' , 'mean', 'median'}\ndef clean_data(dataframe, \n               solve_missing : str ='drop', \n               solve_missing_cat : str = 'drop',\n               drop_list : list = [], \n               category_list : list = []): \n    df = dataframe.drop_duplicates()\n    df = df.drop(drop_list, axis=1)\n    if solve_missing == 'drop' or solve_missing_cat == 'drop':\n        df = df.dropna()\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n    else:\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n                if solve_missing_cat == 'mode':\n                    df[col] = df[col].fillna(df[col].mode()[0], inplace=False)\n            else:\n                if solve_missing == 'mean':\n                    df[col] = df[col].fillna(df[col].mean(), inplace=False)\n                else:\n                    df[col] = df[col].fillna(df[col].median(), inplace=False)\n    return df\n\ndef split_xy(dataframe, label : str):\n    return dataframe.drop(labels=label, axis=1), dataframe[label]\n\ndef encode(df):\n    category_dtype = list(df.select_dtypes(include='category').columns)\n    cat = pd.get_dummies(df, columns = category_dtype, drop_first = True)\n    for col in cat.columns:\n        if cat[col].dtype == np.uint8:\n            cat[col] = cat[col].astype('category')\n    return cat\n\ndef scale(features : tuple):\n    trainFeatures, valFeatures, testFeatures = features # type pandas DataFrame\n    scaler = StandardScaler()\n    category_dtype = list(trainFeatures.select_dtypes(include='category').columns)\n    continuous_dtype = list(filter(lambda c: c not in category_dtype, trainFeatures.columns))\n    # SCALING\n    scaler.fit(trainFeatures[continuous_dtype])\n    cont_xtrain = scaler.transform(trainFeatures[continuous_dtype])\n    cont_xval = scaler.transform(valFeatures[continuous_dtype])\n    cont_xtest = scaler.transform(testFeatures[continuous_dtype])\n    # ENCODING\n    cat_xtrain = trainFeatures[category_dtype]\n    cat_xval = valFeatures[category_dtype]\n    cat_xtest = testFeatures[category_dtype]\n    print(cat_xtrain.shape, cat_xval.shape, cat_xtrain.shape)\n    xtrain = np.concatenate((cont_xtrain, cat_xtrain), axis=1)\n    xval = np.concatenate((cont_xval, cat_xval), axis=1)\n    xtest = np.concatenate((cont_xtest, cat_xtest), axis=1)\n    return scaler, xtrain, xval, xtest","9b50f79b":"df = pd.read_csv('\/kaggle\/input\/patient\/dataset.csv')\nTARGET = 'hospital_death'\n#df.info()\nunique_labels = np.unique(df[TARGET])\nplt.pie(np.array([len(df[df[TARGET]==label]) for label in unique_labels ]), labels = list(unique_labels), autopct='%1.1f%%')","6ecb57d8":"category_list = ['elective_surgery', 'ethnicity', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem', \n                 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'hospital_death']\ndrop_list = ['encounter_id', 'patient_id', 'hospital_id', 'Unnamed: 83']\n\nclean_df = clean_data(df, 'mean', 'mode', drop_list, category_list)\nplt.pie(np.array([len(clean_df[clean_df[TARGET]==label]) for label in unique_labels ]), labels = list(unique_labels), autopct='%1.1f%%')\nplt.show()\nplt.bar(list(unique_labels), [len(clean_df[clean_df[TARGET]==label]) for label in unique_labels ], color = ['cyan', 'green'])\nplt.show()","2b845dc3":"features, labels = split_xy(clean_df, TARGET)\nVAL_SIZE = 0.2\nTEST_SIZE = 0.3\nencoded_features = encode(features)\nprint('Encoded Features Shape : {}'.format(encoded_features.shape))\n# split all samples to train and test with respect to the ratio of each class\nX_train, X_test, ytrain, ytest = train_test_split(encoded_features, labels, test_size = (VAL_SIZE+TEST_SIZE), stratify = labels)\n# split test sample to test and validation with respect to the ratio of each class\nX_test, X_val, ytest, yval = train_test_split(X_test, ytest, test_size = VAL_SIZE\/(VAL_SIZE+TEST_SIZE), stratify = ytest)\n# scale continous variables with standard scaler\nscaler, xtrain, xval, xtest = scale((X_train, X_val, X_test))\n#print(scaler.mean_)\n#print(scaler.var_)\n#print(xtrain.shape, ytrain.shape)\n#print(xval.shape, yval.shape)\n#print(xtest.shape, ytest.shape)\n# convert features to numpy array format\nxtrain = np.array(xtrain, dtype='float32')\nxval = np.array(xval, dtype='float32')\nxtest = np.array(xtest, dtype='float32')\nytrain = np.array(ytrain, dtype='float32')\nyval = np.array(yval, dtype='float32')\nytest = np.array(ytest, dtype='float32')","579767aa":"import tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","e2672d32":"from tensorflow.keras import layers, activations, regularizers\nfrom tensorflow_addons import layers as tfa_layers\nimport tensorflow as tf\n\nclass MyModel(tf.keras.Model):\n\n  def __init__(self, outdim):\n    super().__init__()\n    self.dense1 = layers.Dense(1024, kernel_regularizer=regularizers.l2(l2=0.0001))\n    self.gnorm1 = tfa_layers.GroupNormalization(32)\n    self.dense2 = layers.Dense(512, kernel_regularizer=regularizers.l2(l2=0.0001))\n    self.gnorm2 = tfa_layers.GroupNormalization(16)\n    self.dense3 = layers.Dense(128, kernel_regularizer=regularizers.l2(l2=0.0001))\n    self.gnorm3 = tfa_layers.GroupNormalization(4)\n    self.dense4 = layers.Dense(outdim, kernel_regularizer=regularizers.l2(l2=0.0001))\n    \n  def call(self, inputs):\n    x = activations.relu(self.gnorm1(self.dense1(inputs)))\n    x = activations.relu(self.gnorm2(self.dense2(x)))\n    x = activations.relu(self.gnorm3(self.dense3(x)))\n    x = activations.sigmoid(self.dense4(x))\n    return x","b57c25fd":"from tensorflow.keras.utils import Sequence\nfrom sklearn.utils import shuffle\nimport math\n\nclass CustomDataLoader(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self._x, self._y = x_set, y_set\n        self._batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self._y) \/ self._batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self._x[idx * self._batch_size:(idx + 1) *\n        self._batch_size]\n        batch_y = self._y[idx * self._batch_size:(idx + 1) *\n        self._batch_size]\n        return np.array(batch_x, dtype='float32'), np.array(batch_y, dtype='float32')\n    \n    def on_epoch_end(self):\n        seed = np.random.randint(low=0, high=100)\n        self._x = shuffle(self._x, random_state=seed)\n        self._y = shuffle(self._y, random_state=seed)\n","d9b87f7e":"TRAIN_BATCH_SIZE, VAL_BATCH_SIZE, TEST_BATCH_SIZE = 256, 256, 1\ntrain_loader = CustomDataLoader(xtrain, ytrain, TRAIN_BATCH_SIZE)\nval_loader = CustomDataLoader(xval, yval, VAL_BATCH_SIZE)\ntest_loader = CustomDataLoader(xtest, ytest, TEST_BATCH_SIZE)","64b83465":"from tensorflow.keras.metrics import Accuracy, Precision, Recall, MeanSquaredError\nfrom tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\nloss_fn = BinaryCrossentropy()\nmetrics_list = [Accuracy(), Precision(), Recall(), MeanSquaredError()]","acb1b584":"from tensorflow.keras.optimizers import Adam\nmodel = MyModel(1)\noptim = Adam(learning_rate=0.000001)\nmodel.compile(loss=loss_fn, optimizer=optim, metrics=metrics_list)","a706cd7e":"EPOCHS = 50\ntrain_history = model.fit(train_loader, epochs=EPOCHS, validation_data=val_loader, \\\n                          steps_per_epoch=len(train_loader), validation_steps=len(val_loader))","bba9e2b3":"score = model.evaluate(test_loader, steps=len(test_loader))","ae1e9e5a":"# **Code Block - Convert Dataset to PyTorch Compatible Dataset**","fbe12aae":"# **Data Loading**","db07574a":"# **Code Block - Preprocessing \u524d\u8655\u7406**","412d1220":"# **Model**","95601e74":"# **Evaluate**","eabd90be":"# **Declare Metrics to Monitor and Load to Device**"}}