{"cell_type":{"3f7444cb":"code","edde5408":"code","f60df368":"code","c9f412f5":"code","07c6384d":"code","6998894f":"code","02e122f0":"code","19110c44":"code","56f8b77a":"code","96b6efd2":"code","457b6bc2":"code","2fb3aa07":"code","050702d0":"code","c02c2bd2":"code","6727648b":"code","53c8149c":"code","e0523cef":"code","43dded07":"code","4afa9d5f":"code","ac207b70":"code","cfe6b4ad":"code","bd070f80":"code","fb760772":"code","cf56abeb":"code","21831fef":"code","31eaaa05":"code","0953b181":"code","a81501e2":"code","3f7b2ec5":"code","57313319":"code","82a6c550":"code","5d2ae64b":"code","9bc3ab66":"code","d1628018":"code","5b6a60fd":"code","29637626":"code","9b9964ac":"code","eae0443d":"code","c7279ff2":"code","70134b5a":"code","66bcc1c7":"markdown","0ad3b513":"markdown","4b9d2d8d":"markdown","6488d6ae":"markdown","c70cff1e":"markdown","c96077be":"markdown","25d74c63":"markdown","497defd7":"markdown","c5fc1aec":"markdown","bcce2753":"markdown","f82acd5d":"markdown","596d1db9":"markdown","0e4fc7f7":"markdown","9702d755":"markdown","62bfbda7":"markdown","d0960027":"markdown","376517ca":"markdown","e482e937":"markdown","7b4fb3f3":"markdown","64462bec":"markdown","bcdf2d35":"markdown","ee7e7eae":"markdown"},"source":{"3f7444cb":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","edde5408":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","f60df368":"df.describe(include='all')","c9f412f5":"df.isnull().sum()","07c6384d":"df.duplicated().sum()","6998894f":"from sklearn.model_selection import StratifiedKFold\nx = df.drop(columns=[\"HeartDisease\"])\ny = df[[\"HeartDisease\"]]\nstf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\nfor train_index,test_index in stf.split(x,y):\n    x_train,y_train = x.iloc[train_index],y.iloc[train_index]\n    x_test,y_test = x.iloc[test_index],y.iloc[test_index]\ndf_train = pd.concat([x_train,y_train],axis=1)\ndf_test = pd.concat([x_test,y_test],axis=1)","02e122f0":"def feature_plot(data_train,data_test,feature,target,cat=True,text=True):\n    \n    df = data_train.copy()\n    df_t = data_test.copy()\n    backcolor='#FFFFFF'\n    if cat == True:\n        #train-------------------------------------------\n        palette_1 = [\"#DD4124\",\"#2A4BFF\",\"#E5C1AE\",\"#98C484\"]\n        sns.set_palette(palette_1)\n        sm = df.shape[0]\n        fig,ax = plt.subplots(1,4,figsize=(24,6))\n        g = sns.countplot(data=df,x=feature,hue=target,edgecolor=\"black\",linewidth=2,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.spines['left'].set_linewidth(1.5)\n        g.set_title(feature+\"_counts and percent\", size=15, weight='bold',alpha=0.65)\n        if  text == True:\n            for patch in g.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                g.text(x + width \/ 2, height + 9, f'{height} \/ {height \/ sm * 100:2.2f}%', va='center', ha='center', size=8, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n            \n        dt = df[feature].value_counts(normalize=True).reset_index()\n        p = sns.barplot(data=dt,x=\"index\",y=feature,edgecolor=\"black\",linewidth=2,ax=ax[1])\n        sns.set_palette(palette_1)\n        p.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        p.set_facecolor(backcolor)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.spines['left'].set_linewidth(1.5)\n        p.set_title(feature+\"_percent\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        if  text == True:\n            for patch in p.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                p.text(x + width \/ 2, height, f'{round(height,2)}', va='center', ha='center', size=10, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n                \n        #test---------------------------------------------------\n        smt = df_t.shape[0]\n        g = sns.countplot(data=df_t,x=feature,hue=target,edgecolor=\"black\",linewidth=2,ax=ax[2])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.spines['left'].set_linewidth(1.5)\n        g.set_title(feature+\"_counts and percent\", size=15, weight='bold',alpha=0.65)\n        if  text == True:\n            for patch in g.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                g.text(x + width \/ 2, height, f'{height} \/ {height \/ smt * 100:2.2f}%', va='center', ha='center', size=8, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n            \n        dt = df_t[feature].value_counts(normalize=True).reset_index()\n        p = sns.barplot(data=dt,x=\"index\",y=feature,edgecolor=\"black\",linewidth=2,ax=ax[3])\n        sns.set_palette(palette_1)\n        p.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        p.set_facecolor(backcolor)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.spines['left'].set_linewidth(1.5)\n        p.set_title(feature+\"_percent\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        if  text == True:\n            for patch in p.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                p.text(x + width \/ 2, height, f'{round(height,2)}', va='center', ha='center',size=10, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n        \n    \n    else:\n        #train-------------------------------------------------------\n        palette_1 = [\"#DD4124\",\"#2A4BFF\"]\n        sns.set_palette(palette_1)\n        fig,ax = plt.subplots(1,4,figsize=(24,6))\n        g = sns.kdeplot(data=df,x=feature,hue=target,shade=True,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_KDE\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.spines['left'].set_linewidth(1.5)\n        g.set_title(feature+\"_KDE_FIGURE--train\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        p=sns.boxplot(y=df[feature],ax=ax[1])\n        p.set_facecolor(backcolor)\n        p.set_xlabel(feature,weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_VALUES\",weight='bold', size=13,alpha=0.65)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.spines['left'].set_linewidth(1.5)\n        p.set_title(feature+\"_BOX--train\",size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        #test-------------------------------------------------\n        g = sns.kdeplot(data=df_t,x=feature,hue=target,shade=True,ax=ax[2])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_KDE\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.spines['left'].set_linewidth(1.5)\n        g.set_title(feature+\"_KDE_FIGURE--test\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        p=sns.boxplot(y=df_t[feature],ax=ax[3])\n        p.set_facecolor(backcolor)\n        p.set_xlabel(feature,weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_VALUES\",weight='bold', size=13,alpha=0.65)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.spines['left'].set_linewidth(1.5)\n        p.set_title(feature+\"_BOX--test\",size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n","19110c44":"feature_plot(df_train,df_test,\"Age\",\"HeartDisease\",cat=False,text=False)","56f8b77a":"feature_plot(df_train,df_test,\"Sex\",\"HeartDisease\")","96b6efd2":"feature_plot(df_train,df_test,\"ChestPainType\",\"HeartDisease\")","457b6bc2":"feature_plot(df_train,df_test,\"RestingBP\",\"HeartDisease\",cat=False,text=False)","2fb3aa07":"feature_plot(df_train,df_test,\"Cholesterol\",\"HeartDisease\",cat=False,text=False)","050702d0":"feature_plot(df_train,df_test,\"FastingBS\",\"HeartDisease\")","c02c2bd2":"feature_plot(df_train,df_test,\"RestingECG\",\"HeartDisease\")","6727648b":"feature_plot(df_train,df_test,\"MaxHR\",\"HeartDisease\",cat=False,text=False)","53c8149c":"feature_plot(df_train,df_test,\"ExerciseAngina\",\"HeartDisease\")","e0523cef":"feature_plot(df_train,df_test,\"Oldpeak\",\"HeartDisease\",cat=False,text=False)","43dded07":"feature_plot(df_train,df_test,\"ST_Slope\",\"HeartDisease\")","4afa9d5f":"def plot_target(data_train,data_test):\n    df_train = data_train.copy()\n    df_test = data_test.copy()\n    palette_1 = [\"#DD4124\",\"#2A4BFF\"]\n    sns.set_palette(palette_1)\n    fig,ax=plt.subplots(1,2,figsize=(14,6))\n    sm = df_train.shape[0]\n    g = sns.countplot(data=df,x=\"HeartDisease\",edgecolor=\"black\",linewidth=2,ax=ax[0])\n    g.set_xlabel(\"HeartDisease\", weight='bold', size=13,alpha=0.65)\n    g.set_ylabel(\"HeartDisease\"+\"_counts\", weight='bold', size=13,alpha=0.65)\n    g.spines[['top', 'right','bottom']].set_visible(False)\n    g.spines['left'].set_linewidth(1.5)\n    g.set_title(\"HeartDisease_counts and percent--train\", size=15, weight='bold',alpha=0.65)\n    for patch in g.patches:\n        x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n        g.text(x + width \/ 2, height + 9, f'{height} \/ {height \/ sm * 100:2.2f}%', \n                va='center', ha='center', size=8, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'})\n        \n    #test------------------------------\n    smt = df_test.shape[0]\n    g = sns.countplot(data=df_test,x=\"HeartDisease\",edgecolor=\"black\",linewidth=2,ax=ax[1])\n    g.set_xlabel(\"HeartDisease\", weight='bold', size=13,alpha=0.65)\n    g.set_ylabel(\"HeartDisease\"+\"_counts\", weight='bold', size=13,alpha=0.65)\n    g.spines[['top', 'right','bottom']].set_visible(False)\n    g.spines['left'].set_linewidth(1.5)\n    g.set_title(\"HeartDisease_counts and percent--test\", size=15, weight='bold',alpha=0.65)\n    for patch in g.patches:\n        x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n        g.text(x + width \/ 2, height, f'{height} \/ {height \/ smt * 100:2.2f}%', \n                va='center', ha='center', size=8, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'})","ac207b70":"plot_target(df_train,df_test)","cfe6b4ad":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score,f1_score,precision_score,classification_report\nfrom sklearn.model_selection import cross_val_score,RandomizedSearchCV\nimport category_encoders as ce\ndf_train[\"FastingBS\"] = df_train[\"FastingBS\"].astype(str)\ndf_test[\"FastingBS\"] = df_test[\"FastingBS\"].astype(str)\ntarget = df_train[[\"HeartDisease\"]]\nfeature = df_train.drop(columns=[\"HeartDisease\"])\ncat_list = [\"Sex\",\"ChestPainType\",\"RestingECG\",\"ExerciseAngina\",\"ST_Slope\",\"FastingBS\"]\nnum_list = [\"Age\",\"RestingBP\",\"Cholesterol\",\"MaxHR\",'Oldpeak']","bd070f80":"for col in cat_list:\n    catboost_ = ce.CatBoostEncoder()\n    catboost_.fit(feature[col],target)\n    feature[col] = catboost_.transform(feature[col],target)\n    df_test[col] = catboost_.transform(df_test[col])\n\nfor col in num_list:\n    minscal = MinMaxScaler()\n    minscal.fit(feature[col].values.reshape(-1,1))\n    feature[col] = minscal.transform(feature[col].values.reshape(-1,1))\n    df_test[col] = minscal.transform(df_test[col].values.reshape(-1,1))","fb760772":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.25,random_state=123)","cf56abeb":"def model_metrics(model,y_true,y_pre):\n    print(\"---------------------The current model is:{}.---------------------------\".format(model))\n    a = round(recall_score(y_true,y_pre),2)\n    b = round(precision_score(y_true,y_pre),2)\n    c = round(f1_score(y_true,y_pre),2)\n    print(\"recall_score:{}.\".format(a))\n    print(\"precision_score:{}.\".format(b))\n    print(\"f1_score:{}.\".format(c))\n    data = pd.DataFrame({\"recall_score\":[a],\"precision_score\":[b],\"f1_score\":[c]},index=[model])\n    return data","21831fef":"gbdt = GradientBoostingClassifier(random_state=123) \ngbdt_score =cross_val_score(estimator=gbdt,X=x_train,y=y_train.values.ravel(),cv=5,scoring=\"recall\")\nprint(\"gbdt_score_mean for cv=5:{}.\".format(round(np.mean(gbdt_score),2)))","31eaaa05":"from sklearn.model_selection import GridSearchCV\nparams = {\n    \"learning_rate\":[0.1,0.2,0.3,0.4,0.5],\n    \"n_estimators\":[11,12,13,14],\n    \"subsample\":[0.7,0.8,0.9,1]\n}\n\nrc_gbdt = GridSearchCV(gbdt,param_grid=params,cv=5,scoring=\"recall\")\nrc_gbdt.fit(np.array(x_train),np.array(y_train).ravel())\nprint(\"The best combination of parameters GridSearch is:\")\nprint(rc_gbdt.best_params_)\nprint(\"------------------------------------------------\")\nprint(\"GridSearch to get the best score:\")\nprint(rc_gbdt.best_score_)\n\n\"\"\"Since GBDT trees will be used later, these trees are reduced to the same number of features. \nThe following is a score using more than 100 trees, the best score is 0.93.\n\nThe best combination of parameters randomly searched is:\n{'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.8}\n------------------------------------------------\nRandom search to get the best score:\n0.9365079365079364\"\"\"","0953b181":"gbdt_gs = GradientBoostingClassifier(learning_rate=0.1,n_estimators=12,subsample=0.7,random_state=123)\ngbdt_gs.fit(np.array(x_train),np.array(y_train).ravel())\ngbdt_pre = gbdt_gs.predict(x_test)\ngbdt_data = model_metrics(\"gbdt\",np.array(y_test).ravel(),gbdt_pre)","a81501e2":"gbdt_data","3f7b2ec5":"from sklearn.linear_model import LogisticRegression\nlogis = LogisticRegression(random_state=123) \nlogis_score =cross_val_score(estimator=logis,X=x_train,y=y_train.values.ravel(),cv=5,scoring=\"recall\")\nprint(\"logis_score_mean for cv=5:{}.\".format(round(np.mean(logis_score),2)))","57313319":"params = {\n    \"C\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n}\nrc_logis = GridSearchCV(logis,param_grid=params,cv=5,scoring=\"recall\")\nrc_logis.fit(np.array(x_train),np.array(y_train).ravel())\nprint(\"The best combination of parameters GridSearch is:\")\nprint(rc_logis.best_params_)\nprint(\"------------------------------------------------\")\nprint(\"GridSearch to get the best score:\")\nprint(rc_logis.best_score_)","82a6c550":"logis_gs = LogisticRegression(C=0.3,random_state=123)\nlogis_gs.fit(np.array(x_train),np.array(y_train).ravel())\nlogis_pre = logis_gs.predict(x_test)\nlogis_data = model_metrics(\"logistic\",np.array(y_test).ravel(),logis_pre)","5d2ae64b":"metric_frame = pd.concat([gbdt_data,logis_data],axis=0)\nmetric_frame","9bc3ab66":"x_train","d1628018":"gbdt_gs.apply(x_train)[:,:,0]","5b6a60fd":"from sklearn.preprocessing import OneHotEncoder\none_hot = OneHotEncoder()\none_hot.fit(gbdt_gs.apply(x_train)[:,:,0])\nlogis_gs.fit(one_hot.transform(gbdt_gs.apply(x_train)[:,:,0]),y_train.values.ravel())\nlogs_pre = logis_gs.predict(one_hot.transform(gbdt_gs.apply(x_test)[:,:,0]))\nlogis_data = model_metrics(\"logistic_gbdt_first\",np.array(y_test).ravel(),logs_pre)","29637626":"data = pd.concat([metric_frame,logis_data],axis=0)\ndata","9b9964ac":"train_new = gbdt_gs.apply(x_train)[:,:,0]\ntest_new = gbdt_gs.apply(x_test)[:,:,0]\no_hot = OneHotEncoder(dtype='int')\no_hot.fit(train_new)\ntrain_ = pd.DataFrame(o_hot.transform(train_new).toarray())\ntest_ = pd.DataFrame(o_hot.transform(test_new).toarray())\nx_train_new = np.hstack([x_train,train_])\nx_test_new = np.hstack([x_test,test_])\nlogis_gs.fit(x_train_new,y_train.values.ravel())\npre_ = logis_gs.predict(x_test_new)\nlo_data = model_metrics(\"logistic_gbdt_second\",np.array(y_test).ravel(),pre_)","eae0443d":"end_data = pd.concat([data,lo_data],axis=0)\nend_data","c7279ff2":"end_data = end_data.reset_index().rename({\"index\":\"model\"},axis=1)","70134b5a":"import plotly.express as px\nfig = px.bar(end_data,x=\"model\",y=\"f1_score\",text=\"f1_score\")\nfig.show()","66bcc1c7":"- Obviously, when the type of chest pain is ASY, the rate of heart disease is the highest.","0ad3b513":"We need to consider the following points in the feature engineering:\n1. If the algorithm we use is a tree model, such as XGboost\/LightGBM\/random forest, we should try to minimize the use of one-hot encoding for features, because this will cause some features to split and no benefit.\n2. If we use KNN\/logistic regression and other algorithms, we can use one-hot encoding. Compared with the first point, although its effect is not obvious.\n3. Considering that I want to use GBDT and logistic regression, I will partially use one-hot coding and other coding methods.\n4. The divided data has inconsistent data distribution. We should process this part of the data to make it consistent in the training set and test set.\n5. I use the sample nodes predicted by GBDT, merge them with the original data to form a new data set, and then make predictions through logistic regression. It is important here that the use of repeated data may cause overfitting.\n6. I will compare the effects of using GBDT\/logistic regression alone and combining the three models of GBDT and logistic regression. In the two model fusion stages, I use the sample nodes predicted by GBDT, merge them with the original data to form a new data set, and then make predictions through logistic regression. It is important here that the use of repeated data may cause overfitting.\n7. At the same time, in order to control the influencing factors as much as possible, all categorical variables should use catboost coding as much as possible, and the numerical variables are uniformly scaled by MIN-MAX. Although the use of catboost encoding will affect the effect of logistic regression, but the effect is not large, we can ignore this point for the time being.","4b9d2d8d":"- The age with heart disease is mainly around 55 years old.","6488d6ae":"- In the second way, all samples are mapped to tree nodes and merged with the original data. Note: This method will be too optimistic about the model, that is, it will actually cause overfitting.","c70cff1e":"- In the first way, the nodes of the tree are used as training data for training.","c96077be":"- The full name of gbdt is gradient descent tree. Among traditional machine learning algorithms, it is one of the best algorithms for fitting the real distribution. Before deep learning became popular in the past few years, gbdt was brilliant in various competitions. There are probably several reasons, one is that the effect is really good. The second is that it can be used for classification and regression. The third is to filter features.\n![image.png](attachment:2f470ea7-544f-4c06-ad5c-cdd6d8ff97b3.png) ","25d74c63":"# GBDT","497defd7":"- The graph of Cholesterol has double peaks, which are 0 and 230 for cholesterol respectively.","c5fc1aec":"# Logistic","bcce2753":"-Let's compare f1_score","f82acd5d":"- Very good, we use the parameters obtained by the grid search..","596d1db9":"- The proportion of men suffering from heart disease exceeds that of women.","0e4fc7f7":"- The distribution of the training set and the test set are inconsistent.","9702d755":"# Feature engineering","62bfbda7":"- The maximum heart rate of people with heart disease is concentrated at 125","d0960027":"- Logistic regression and linear regression are both a generalized linear model. Logistic regression assumes that the dependent variable y follows a Bernoulli distribution, while linear regression assumes that the dependent variable y follows a Gaussian distribution. Therefore, there are many similarities with linear regression. If the Sigmoid mapping function is removed, the logistic regression algorithm is a linear regression. It can be said that logistic regression is theoretically supported by linear regression, but logistic regression introduces nonlinear factors through the Sigmoid function, so it can easily handle the 0\/1 classification problem.\n![image.png](attachment:b66cc808-7898-42bc-9689-be362f4136ec.png)","376517ca":"- 1. When we use the fusion model, f1_score is 1 point higher than GBDT and 2 points higher than logistic regression. Pay special attention to the second method. You will mistake it for it to be effective, but it will cause over-fitting. The solution to this method is to prepare two training sets and hand one of them to the GBDT division node. Then map the second training set and test set, and merge the two training sets.\n- 2. I don't know if you have found a point, when we use Catboost encoding, the effect of GBDT is better than that of logistic regression. Perhaps, when we use the tree model, on the one hand, we don\u2019t need to consider feature scaling. On the other hand, we should also reduce one-hot encoding and use other encoding methods, such as target encoding and frequency encoding.","e482e937":"- The data is basically balanced, and there is no jealousy imbalance.","7b4fb3f3":"# Fusion of GBDT and logistic regression\n<br> 1. Using the nature of the GBDT split feature, the sample is mapped to each tree node to represent the classification feature of each sample feature, the mapped feature node is one-hot coded, and the logistic regression model is trained together with the original training set.\nNote: The best situation should be that GBDT uses a training set alone instead of the original training set, which will cause overfitting. Since our current data volume is relatively small, I did not separate it separately. If you use this method, the best way is to take out a separate piece of data to GBDT, and then GBDT obtains the mapping rules and applies the mapping rules to the new training data and test set.\n<br> 2. The two models are fused, and we use the same parameters as the previous model to test whether the score has improved.","64462bec":"- In the second way, the node of the tree and the original training data set are combined into a new data set.\n- Comparing the results, we found that although the recall rate did not improve, our f1_score score increased by one point.\n- I wonder if you found a problem? We use catboost coding, the effect of using GBDT is better than logistic regression, but if you use one-hot coding, I believe the effect of logistic regression will be better.","bcdf2d35":"- The fine blood pressure of patients with heart disease is mainly concentrated at 130. It can be seen that there are abnormal values of fine blood pressure.\n- We can see that the distribution of the training set and the test set are inconsistent.","ee7e7eae":"- When ExerciseAngina is present, the rate of heart disease is high."}}