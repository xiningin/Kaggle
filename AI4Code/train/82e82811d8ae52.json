{"cell_type":{"a1378a8f":"code","be0096d9":"code","51a58278":"code","c84abbca":"code","ac601424":"code","ff72156c":"code","2cbc4932":"code","b77ac8df":"code","afcdbfc1":"code","50c89f68":"code","a0539a9f":"code","1e0b42ec":"code","9eca6c0f":"markdown","33d823ba":"markdown","f72d6b93":"markdown","db7c4dff":"markdown","8e378b5b":"markdown","2862a26c":"markdown","e6ba6440":"markdown","e39c1b7d":"markdown"},"source":{"a1378a8f":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Reading Data \ndf = pd.read_csv('..\/input\/Iris.csv')\n#df.describe()\ndf.info()\ndf['Class']=df['Species']\ndf['Class'] = df['Class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\ndf[\"Class\"].unique()\ndf.head()","be0096d9":"# Define a function for Euclidien distance\ndef eucdist(p1, p2): \n    dist = 0\n    for i in range(len(p1)):\n        dist = dist + np.square(p1[i]-p2[i])\n    dist = np.sqrt(dist)\n    return dist;","51a58278":"# Define kNN function\ndef kNN(X,y,k,test):\n    # Calculating distances for our test point\n    newdist = np.zeros(len(y))\n\n    for j in range(len(y)):\n        newdist[j] = eucdist(test, X[j,:])\n\n    # Merging actual labels with calculated distances\n    newdist = np.array([newdist, y])\n\n    ## Finding the closest k neighbors\n    # Sorting index\n    idx = np.argsort(newdist[0,:])\n\n    # Sorting the all newdist\n    newdist = newdist[:,idx]\n    #print(newdist)\n\n    # We should count neighbor labels and take the label which has max count\n    # Define a dictionary for the counts\n    c = {'0':0,'1':0,'2':0 }\n\n    for i in range(k):\n        c[str(int(newdist[1,i]))] = c[str(int(newdist[1,i]))] + 1\n\n    key_max = max(c.keys(), key=(lambda k: c[k]))\n    return int(key_max)","c84abbca":"# Define sigmoid function \ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","ac601424":"# Define cost function\n# I do not use cost function in the code but \n# use it to check theta and choice of alpha\n# in gradientdescent function\ndef cost(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","ff72156c":"# To minimize our cost function we use Gradient Descent briefly:\n# Repeat {\n#  1. Calculate gradient average\n#  2. Multiply by learning rate alpha\n#  3. Subtract from weights }\ndef gradientdescent(X, y, alpha, num_iter):\n\n    # select initial values zero\n    theta = np.zeros(X.shape[1])\n\n    for i in range(num_iter):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        gradient = np.dot(X.T, (h - y)) \/ y.size\n        theta = theta - alpha * gradient\n\n        # This part is used for the check  \n        #calcloss = cost(h, y)\n        #if( i % 1000 == 0): \n        #    print('Number of Iterations: ', i, 'Cost : ', calcloss, 'Theta: ', theta)\n    return theta","2cbc4932":"# Define predict function to calculate probabilities\n# for each class\ndef predict(test, theta):\n    z = np.dot(np.transpose(test), theta)\n    return sigmoid(z)","b77ac8df":"def logistic(X, y,  alpha, num_iter, test):\n    # We have 3 classifiers because of this, I need to select a method\n    # and I decided to apply one vs all (OvA)\n\n    # Model for class 0\n    y0 = np.copy(y)\n    y0[y==2] = 1\n    y0 = y0 - 1\n    y0 = y0 * -1 \n    theta0 = gradientdescent(X, y0, alpha, num_iter)\n\n    # Model for class 1 \n    y1 = np.copy(y)\n    y1[y==2] = 0\n    theta1 = gradientdescent(X, y1, alpha, num_iter)\n\n    # Model for class 2\n    y2 = np.copy(y)\n    y2[y==1] = 0\n    y2[y==2] = 1\n    theta2 = gradientdescent(X, y2, alpha, num_iter)\n\n    # Calculate probabilties\n    preds = []\n    preds.append(predict(test,theta0))\n    preds.append(predict(test,theta1))\n    preds.append(predict(test,theta2))\n    \n    # Select max probability\n    m=max(preds)\n    c=0\n    if(m==preds[1]):\n        c=1\n    if(m==preds[2]):\n        c=2\n        \n    return c","afcdbfc1":"# Define X (features) and Y (target)\nadf = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Class']].values\nX = adf[:,:4]\ny = adf[:,4]\n\n# I chose data points close to the real data points\ntest0 = [5.77,4.44,1.55,0.44] # close to X[15]\ntest1 = [5.66,3.01,4.55,1.55] # close to X[66]\ntest2 = [7.44, 2.88, 6.11, 1.99] # close to X[130]","50c89f68":"plt.figure(figsize=(10,10))\nt=np.unique(y)\n\nax1=plt.subplot(2, 2, 1)\nax1.set(xlabel='Sepal Length (cm)', ylabel='Sepal Width (cm)')\nplt.plot(X[y==t[0],0], X[y==t[0],1], 'o', color='y')\nplt.plot(X[y==t[1],0], X[y==t[1],1], 'o', color='r')\nplt.plot(X[y==t[2],0], X[y==t[2],1], 'o', color='b')\n# test datapoints\nplt.plot(test0[0],test0[1],'*',color=\"k\")\nplt.plot(test1[0],test1[1],'*',color=\"k\")\nplt.plot(test2[0],test2[1],'*',color=\"k\")\n\nax2=plt.subplot(2, 2, 2)\nax2.set(xlabel='Petal Length (cm)', ylabel='Petal Width (cm)')\nax2.yaxis.set_label_position(\"right\")\nax2.yaxis.tick_right()\nplt.plot(X[y==t[0],2], X[y==t[0],3], 'o', color='y')\nplt.plot(X[y==t[1],2], X[y==t[1],3], 'o', color='r')\nplt.plot(X[y==t[2],2], X[y==t[2],3], 'o', color='b')\n# test datapoints\nplt.plot(test0[2],test0[3],'*',color=\"k\")\nplt.plot(test1[2],test1[3],'*',color=\"k\")\nplt.plot(test2[2],test2[3],'*',color=\"k\")\n\nax3=plt.subplot(2, 2, 3)\nax3.set(xlabel='Sepal Length (cm)', ylabel='Petal Length (cm)')\nplt.plot(X[y==t[0],0], X[y==t[0],2], 'o', color='y')\nplt.plot(X[y==t[1],0], X[y==t[1],2], 'o', color='r')\nplt.plot(X[y==t[2],0], X[y==t[2],2], 'o', color='b')\n# test datapoints\nplt.plot(test0[0],test0[2],'*',color=\"k\")\nplt.plot(test1[0],test1[2],'*',color=\"k\")\nplt.plot(test2[0],test2[2],'*',color=\"k\")\n\nax4=plt.subplot(2, 2, 4)\nax4.set(xlabel='Sepal Width (cm)', ylabel='Petal Width (cm)')\nax4.yaxis.set_label_position(\"right\")\nax4.yaxis.tick_right()\nplt.plot(X[y==t[0],1], X[y==t[0],3], 'o', color='y')\nplt.plot(X[y==t[1],1], X[y==t[1],3], 'o', color='r')\nplt.plot(X[y==t[2],1], X[y==t[2],3], 'o', color='b')\n# test datapoints\nplt.plot(test0[1],test0[3],'*',color=\"k\")\nplt.plot(test1[1],test1[3],'*',color=\"k\")\nplt.plot(test2[1],test2[3],'*',color=\"k\")","a0539a9f":"# Predicting the classes of the test data by kNN \n\n# Decide k value\nk = 5\n\nprint(\"kNN\\n\")\nc = kNN(X,y,k,test0)\nprint(\"Test point \"+str(test0)+\" has label \"+str(c)+\" according to \"\n      +str(k)+\"-nearest neighbors.\\n\")\n\nc=kNN(X,y,k,test1)\nprint(\"Test point \"+str(test1)+\" has label \"+str(c)+\" according to \"\n      +str(k)+\"-nearest neighbors.\\n\")\n\nc=kNN(X,y,k,test2)\nprint(\"Test point \"+str(test2)+\" has label \"+str(c)+\" according to \"\n      +str(k)+\"-nearest neighbors.\\n\")","1e0b42ec":"# Predicting the classes of the test data by Logistic Regression\n\n# Adjustments for the logistic regression\nalpha = 0.1\nnum_iter = 30000\n\n# Add intercept\nintercept = np.ones((X.shape[0], 1))\nX = np.concatenate((intercept, X), axis=1)\ntest0 = np.concatenate(([1],test0))\ntest1 = np.concatenate(([1],test1))\ntest2 = np.concatenate(([1],test2))\n\n# Run logistic function and determine results\nprint(\"Logistic Regression\\n\")\nc=logistic(X,y, alpha, num_iter, test0)\nprint(\"Test point \"+str(test0)+\" has label \"+str(c)+\" according to logistic regression.\\n\")\n\nc=logistic(X,y, alpha, num_iter, test1)\nprint(\"Test point \"+str(test1)+\" has label \"+str(c)+\" according to logistic regression.\\n\")\n\nc=logistic(X,y, alpha, num_iter, test2)\nprint(\"Test point \"+str(test2)+\" has label \"+str(c)+\" according to logistic regression.\\n\")","9eca6c0f":"# <a id=\"2\"><\/a> Importing Modules, Reading the Dataset\n#### [Return Contents](#0)\n<hr\/>\nIn order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***. ","33d823ba":"# <a id=\"3\"><\/a> k-Nearest Neighbors (k-NN)\n#### [Return Contents](#0)\n<hr\/>\nAt k-NN, we find the k nearest neighbors of a point and then count these neighbors' labels. Afterwards, this point gets the label that has the highest count. In order to calculate distance among points, we use Euclidien distance and I defined the below function to write my main function easier.    ","f72d6b93":"# <a id=\"4\"><\/a> Logistic Regression\n#### [Return Contents](#0)\n<hr\/>\nIn order to get results between 0 and 1, a function which is called **sigmoid** is used to transform our hypothesis function. It is defined as\n$$h_{\\theta}(X) = g(\\theta X)$$ where $h_{\\theta}(X)$ is the hypothesis function, $X$ is a matrix and $$g(z)=\\dfrac{1}{1+e^{-z}}$$\nBy the definition, I defined the below ***sigmoid*** function.","db7c4dff":"# <a id=\"0\"><\/a> Contents\n1. [Overview](#1)\n1. [Importing Modules, Reading the Dataset](#2)\n1. [k-Nearest Neighbors (k-NN)](#3)\n1. [Logistic Regression](#4)\n1. [Testing the Functions](#5)","8e378b5b":"# <a id=\"5\"><\/a> Testing the Functions\n#### [Return Contents](#0)\n<hr\/>\nTo test my functions, I defined 3 different points which are very close to the existing 3 points. I expect that the predicted labels will be the same as the real points. ","2862a26c":"The below code is my main function. It calculates the distance between a point and all points in the dataset. Then, It takes the k nearest points and count the labels. Finally, it returns the label that has the maximum count.","e6ba6440":"Before starting to predict test points' labels, I wanted to see the places of these points according to some features. Thus, I draw the below charts which will help us to guess new points' labels.","e39c1b7d":"# <a id=\"1\"><\/a> Overview\n<hr\/>\nWelcome to my Kernel! In this kernel I aim to apply k-NN and logistic regression by my own functions. By doing this, I belive that we will undestand the mechanism and theory behind the scence better.\n\nFor the logistic regression and k-NN, I use the gradient descent and Euclidien distance. The below functions work but they need more improvement and by time I will improve them and add more explanation.\n\nIf you have a question or feedback, feel free to write and if you like this kernel, please <b><font color=\"red\">do not forget to <\/font><font color=\"green\">UPVOTE <\/font><\/b> \ud83d\ude42 \n<br\/>\n<img src=\"https:\/\/i.imgur.com\/QPWu3Rd.png\" title=\"source: Gradient Descent\" height=\"400\" width=\"800\" \/>"}}