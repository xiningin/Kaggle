{"cell_type":{"4da8dbfd":"code","12a53eee":"code","2f9d7bf9":"code","20c775b3":"code","a479c822":"code","1a09ee02":"code","6a7b813f":"code","ebcaf23d":"code","227cce68":"code","2496b550":"code","2f4e09cf":"code","5fb9402d":"code","2586b02e":"code","fb119622":"code","8d595b29":"code","24bef26d":"code","38049ede":"code","e07daa21":"code","d878bdf9":"code","b8cf80bc":"code","aa62b017":"code","b756c637":"code","4db17b6f":"code","eb962595":"code","e0932932":"code","851d7c51":"code","2b5af17a":"code","d322135b":"code","57296155":"code","3f57673c":"code","2b69a584":"markdown","6e9ab2e7":"markdown","615d2c04":"markdown","6178bea9":"markdown"},"source":{"4da8dbfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","12a53eee":"#Importing libraries.................\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier ,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve ,KFold\nfrom sklearn.metrics import accuracy_score,f1_score,auc,confusion_matrix\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n","2f9d7bf9":"#Checking the dataset.....................\ndf=pd.read_csv('..\/input\/Churn_Modelling.csv')\n","20c775b3":"df.head()","a479c822":"\n\n#Checking missing values..............\ndf.isnull().sum()\n\n","1a09ee02":"#Checking dataset shape\ndf.shape","6a7b813f":"#Separating the dtypes based on objects and integer or float.......\ng = df.columns.to_series().groupby(df.dtypes).groups\nprint(g)\n\n","ebcaf23d":"\ndf.info()","227cce68":"df.describe()","2496b550":"#Checking the correlation...........................\nplt.figure(figsize=(11,7)) #7 is the size of the width and 4 is parts.... \nsns.heatmap(df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","2f4e09cf":"#While analysing its look like \"RowNumber\",\"CustomerId\",\"Surname\" are not needing for this prediction...\ndf.drop(columns= [\"RowNumber\",\"CustomerId\",\"Surname\"],inplace=True)","5fb9402d":"df.head()\n","2586b02e":"\n#Visualizing the Geography\ndf['Geography'].value_counts().plot(kind='bar')\n#As per data i can see that france have high customer_rate","fb119622":"#Visualizing the gender\ndf['Gender'].value_counts().plot(kind='bar')\n#As per data most of the customers are male","8d595b29":"\n#Visualizing the Exited\ndf['Exited'].value_counts().plot(kind='bar')\n#As per analysis most of the customer having less chances of leaving the bank. ","24bef26d":"\n#Encoding Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in df.columns:\n    df[col] = labelencoder.fit_transform(df[col])\n","38049ede":"#Gender-female(0),male(1)\n#Geography-France(0),Spain(2),Germany(1)    \n    \n#Now one hot encoding\ndf=pd.get_dummies(df, columns=[\"Gender\",\"Geography\"],drop_first=False)","e07daa21":"\ndf.rename(columns={'Gender_0':'Female','Gender_1':'Male','Geography_0':'France','Geography_2':'Spain','Geography_1':'Germany'}, inplace=True)\n","d878bdf9":"#Rearranged the order of the dataframe....\ndf = df[['CreditScore','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Female','Male','France','Germany','Spain','Exited']]\ndf.head()\n   ","b8cf80bc":"#Separating features and label\nX = df.iloc[:,0:13].values\ny = df.iloc[:,-1].values","aa62b017":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n\n","b756c637":"# =============================================================================\n# Cross validation on differnet set of algorithm!!!\n# =============================================================================\n################################################################\nkfold = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n\n\nrs = 15\nclrs = []\n\nclrs.append(AdaBoostClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(LogisticRegression(random_state = rs))\nclrs.append(ExtraTreesClassifier(random_state = rs))\n#clrs.append(BaggingClassifier(random_state = rs))\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X_train, y_train , scoring = 'accuracy', cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_df = pd.DataFrame({\"CrossVal_Score_Means\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"RandomForestClassifier\",\"Logistic Regression\",\"AdaBoostClassifier\",\"Gradient Boosting\",'ExtraTreesClassifier']})\n\n\n","4db17b6f":"import seaborn as sns\ng = sns.barplot(\"CrossVal_Score_Means\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)   ","eb962595":"\n# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n    \nprint(explained_variance)\n\n    \nlen(explained_variance)\n\n\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(16, 8))\n    \n    plt.bar(range(13), explained_variance, alpha=0.5, align='center',label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n\n","e0932932":"# =============================================================================\n# Selecting top 5 components\n# =============================================================================\n# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 7 )\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n    \nprint(explained_variance)\n\n","851d7c51":"\nrs = 15\nclrs = []\n\nclrs.append(AdaBoostClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(LogisticRegression(random_state = rs,))\nclrs.append(ExtraTreesClassifier(random_state = rs))\n\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X, y, scoring = 'accuracy', cv = kfold, n_jobs=-1))\n\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\n\n\ncv_df = pd.DataFrame({\"CrossVal_Score_Means\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"AdaBoostClassifier\",\"RandomForestClassifier\",\"Gradient Boosting\",\"Logistic Regression\",'ExtraTreesClassifier']})\n\ng = sns.barplot(\"CrossVal_Score_Means\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)","2b5af17a":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 0)\n\ngsGBC.fit(X_train,y_train)\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_,gsGBC.best_params_\n","d322135b":"# =============================================================================\n# Building a model...\n# =============================================================================\n\n\nGBC = GradientBoostingClassifier(learning_rate= 0.05,loss='deviance',max_depth=8,max_features=0.3,min_samples_leaf=150,n_estimators=300)\nGBC.fit(X_train, y_train)\n\n\n#predicting the test set\ny_pred = GBC.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n\nprint(accuracy_score(y_test,y_pred))\n  \n\nfrom sklearn.metrics import classification_report \nprint(classification_report(y_test,y_pred))\n\n\nprint(f1_score(y_test,y_pred))","57296155":"#Adaboosting hypertunning\nfrom sklearn.ensemble import AdaBoostClassifier\nABC = AdaBoostClassifier()\nABC_parameter = {'n_estimators' :[100,200,300],'random_state' : [10,20,30,40,50]}\nABC = GridSearchCV(ABC,param_grid = ABC_parameter, cv=kfold, scoring=\"accuracy\")\n\nABC.fit(X_train,y_train)\nABC_best = ABC.best_estimator_\n\n# Best score\nABC.best_score_,ABC.best_params_\n","3f57673c":"#Adaboosting algorithm\nfrom sklearn.ensemble import AdaBoostClassifier\n\nABC = AdaBoostClassifier(n_estimators = 200,random_state = 10)\nABC.fit(X_train, y_train)\n\n#predicting the test set\ny_pred = ABC.predict(X_test)\n\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n\nprint(accuracy_score(y_test,y_pred))\n  \n\nfrom sklearn.metrics import classification_report \nprint(classification_report(y_test,y_pred))\n\n\nprint(f1_score(y_test,y_pred))\n\n","2b69a584":"# Hyper parameter tunning","6e9ab2e7":"\n#After using pca Gradient boosting  is give accuracy of 86%\n#And AdaBoostClaasifier is giving accuracy of 85%\n#Random forest is giving accuracy of 84% \n","615d2c04":"\n# Checking the important variables using PCA....\n'''Hypertunning doesn't seem to be helping much for above case so will \nperform PCA and see if accuracy improves or not'''\n","6178bea9":"#||Real time model||"}}