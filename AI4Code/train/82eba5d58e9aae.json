{"cell_type":{"9f31f1dd":"code","29455b6e":"code","e64a7e5f":"code","b4ad9c33":"code","49dd3386":"code","56d499d3":"code","6a5e7491":"code","329315fc":"code","cc6fa0e9":"code","1379c31c":"code","9aa1dd75":"code","010fbd67":"code","51efbe85":"code","34102bf8":"code","59cf99a9":"code","922c6ded":"code","9da79758":"markdown","64a0f93d":"markdown","aa079ec2":"markdown","4591816a":"markdown","e7eee2ef":"markdown"},"source":{"9f31f1dd":"from transformers import AutoTokenizer, AutoModel\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport random","29455b6e":"tokenizer = AutoTokenizer.from_pretrained('..\/input\/murilbertcased')","e64a7e5f":"df = pd.read_csv('..\/input\/iiitd-abuse-detection-challenge\/eam2021-train-set\/bq-results-20210825-203004-swh711l21gv2.csv')\ndf = df.reset_index()","b4ad9c33":"df.head()","49dd3386":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n    \n    input_ids = []\n    tt_ids = []\n    at_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer(\n                    text_chunk,\n                    max_length = 128,\n                    padding='max_length',\n                    truncation=True\n                    )\n        \n        input_ids.extend(encs['input_ids'])\n        tt_ids.extend(encs['token_type_ids'])\n        at_ids.extend(encs['attention_mask'])\n    \n    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}","56d499d3":"token_data = fast_encode(list(df['commentText'].values), tokenizer)\ntoken_data['index'] = list(df['index'].values)\ntoken_data['label'] = list(df['label'].values)","6a5e7491":"token_data.keys()","329315fc":"unk_id = tokenizer.unk_token_id\nunk_id\n","cc6fa0e9":"unknown_items = [item for item in tqdm(token_data['input_ids']) if unk_id in item]        ","1379c31c":"print(\"Number of rows with Unknown elements: \", len(unknown_items))","9aa1dd75":"unknown_count = [item.count(unk_id) for item in unknown_items]","010fbd67":"print(\"Number of unknown tokens:\", sum(unknown_count))","51efbe85":"## Print some samples\n\ntokenizer.decode(unknown_items[100])","34102bf8":"df[df['commentText'].str.contains('\u0915\u094d\u092f\u093e \u092c\u093e\u0924 \u0939\u0948')] ","59cf99a9":"import numpy as np\nnp.save('train.npy', token_data )","922c6ded":"# To load\n# test = np.load('train.npy', allow_pickle=True)\n# test.tolist()","9da79758":"## Mostly emoji's are not known to tokenizer ","64a0f93d":"## Saving to disk using numpy","aa079ec2":"## Checking for unknown tokens in MURIL","4591816a":"**So unknown has a index of 100, lets check if unknown is there in input ids**","e7eee2ef":"### Since Training data in this project is large, so if you don't have on the fly tokenization then your kernel can give out of memory error. So its better to save tokenized data to disk and load at later stage"}}