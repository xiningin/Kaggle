{"cell_type":{"d5db66fe":"code","ec2f13a2":"code","f8632eff":"code","e04194af":"code","56862b89":"code","03088979":"code","f66bfb17":"code","8307a904":"code","170a9e41":"code","c91e567e":"code","7b82b627":"code","d29668c7":"code","26c1bab3":"code","5c35bc1d":"code","8623263d":"code","8909cc9d":"code","7cbbcefd":"code","0338c263":"markdown","a8a6be40":"markdown","9c26ab9d":"markdown","9373a69c":"markdown","14b132fd":"markdown","9b7c7a1c":"markdown","1c266ad4":"markdown","0bbe3376":"markdown"},"source":{"d5db66fe":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom numpy import mean\nplt.style.use('ggplot')","ec2f13a2":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","f8632eff":"print(data)","e04194af":"data.isnull().sum()","56862b89":"sb.countplot(x='Class', data=data)","03088979":"data.Class[data.Class == 1].count()","f66bfb17":"data.Class[data.Class == 0].count()","8307a904":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndata['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['Time'] = scaler.fit_transform(data['Time'].values.reshape(-1,1))","170a9e41":"data.Time.describe()","c91e567e":"train=data.sample(frac=0.8,random_state=200)\ntest=data.drop(train.index)","7b82b627":"positives = train[train.Class == 1]\nnegatives = train[train.Class == 0]","d29668c7":"negativeSample = negatives.sample(positives.Class.count())","26c1bab3":"df = pd.concat([negativeSample, positives], axis=0)","5c35bc1d":"sb.countplot(x='Class', data=df)","8623263d":"\nX = df.drop('Class', axis=1)\ny = df['Class']","8909cc9d":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X, y)","7cbbcefd":"X = test.drop('Class', axis=1)\ny = test['Class']\nscore = clf.score(X, y)\nprint(score)","0338c263":"### Now we undersample our negatives so we have the same amount of positive and negative fradulent transactions ","a8a6be40":"### Now lets split our data. We will use 80% of our data to train on. We do this before undersampling so we don't train on data we will be testing on","9c26ab9d":"### Now that our data is no longer inbalanced, we can start training","9373a69c":"### Hope I did everything correct, leave a comment with advice or if I messed something up. Toodles","14b132fd":"### In this notebook I will train a model on an imbalanced dataset by undersampling the data","9b7c7a1c":"### Note: it is important to train on the original dataset, not the undersampled dataset. Also, since we split our data before undersampling it, the data we trained on is not a subset of the data we test on","1c266ad4":"### Here you can see our data is very imbalanced\nTraining models on inbalanced data can result in overfitting. If our model just guessed that every transaction was a legitiment transaction, it would have a high accuracy. To combat this, we will undersample our data.","0bbe3376":"First lets scale our time and amount features"}}