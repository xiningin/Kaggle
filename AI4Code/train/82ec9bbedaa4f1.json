{"cell_type":{"c6c606c7":"code","460ecb80":"code","2e9edf4a":"code","1df12e9e":"code","771fd674":"code","2b8c46ec":"code","d15b114b":"code","4513db29":"code","fa053446":"code","49a6e9e2":"code","d90378cb":"code","e9391423":"code","7d186eec":"code","4caa2083":"code","518521ec":"code","ac080227":"code","770f5ba3":"code","39e1c4ab":"code","6a3d3e78":"code","ef79360c":"code","f5681172":"code","9cdb888d":"code","373e16ba":"code","0da93328":"code","e2246f0e":"code","7c9a86fa":"code","7af3779e":"code","2eb7b169":"code","f8de5ee1":"code","64fdbe3e":"code","fcfd2857":"code","7cce39fa":"code","291b73d6":"code","7fad75f9":"code","efabc1fa":"code","99ad805a":"markdown","677b03c3":"markdown","4133dd3a":"markdown","55cd8030":"markdown","d4198fa5":"markdown","c3dff5e4":"markdown"},"source":{"c6c606c7":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.stem.porter import PorterStemmer \nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import ngrams, FreqDist\nimport re\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n","460ecb80":"train_df  = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","2e9edf4a":"train_df.head()","1df12e9e":"test_df.head()","771fd674":"train_df.info()","2b8c46ec":"train_df.target.value_counts()","d15b114b":"train_df.isnull().sum()","4513db29":"test_df.isnull().sum()","fa053446":"first_sent_example = train_df.text[0].lower()\nfirst_sent_example","49a6e9e2":"stop_words=set(stopwords.words('english'))","d90378cb":"tokens_first= word_tokenize(first_sent_example)\nprint(tokens_first)","e9391423":"filtered_words = [w for w in tokens_first if w not in stop_words]\nfiltered_words","7d186eec":"wordnet_lemmatizer = WordNetLemmatizer()\nlemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\nlemmatized_word","4caa2083":"print(nltk.pos_tag(lemmatized_word))","518521ec":"only_words = [word for word in lemmatized_word if word.isalpha()]\nonly_words","ac080227":"anthoer_sent_example = train_df.text[15].lower()\nanthoer_sent_example","770f5ba3":"tokens_not_disaster= word_tokenize(anthoer_sent_example)\nprint(tokens_not_disaster)","39e1c4ab":"filtered_words_not_disaster = [w for w in tokens_not_disaster if w not in stop_words]\nfiltered_words_not_disaster","6a3d3e78":"lemmatized_word_not_disaster = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words_not_disaster]\nlemmatized_word_not_disaster","ef79360c":"only_words_not_disaster = [word for word in lemmatized_word_not_disaster if word.isalpha()]\nonly_words_not_disaster","f5681172":"anthoer_sent_example_2 = train_df.text[32].lower()\nanthoer_sent_example_2","9cdb888d":"tokens_not_disaster_2= word_tokenize(anthoer_sent_example_2)\nprint(tokens_not_disaster_2)","373e16ba":"filtered_words_not_disaster_2 = [w for w in tokens_not_disaster_2 if w not in stop_words]\nprint(filtered_words_not_disaster_2)","0da93328":"lemmatized_word_not_disaster_2 = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words_not_disaster_2]\nprint(lemmatized_word_not_disaster_2)","e2246f0e":"only_words_not_disaster_2 = [word for word in lemmatized_word_not_disaster_2 if word.isalpha()]\nonly_words_not_disaster_2","7c9a86fa":"print(nltk.pos_tag(only_words_not_disaster_2))","7af3779e":"vectorizer = CountVectorizer()\ntrain_vectors = vectorizer.fit_transform(train_df.text)\ntest_vectors = vectorizer.transform(test_df.text)","2eb7b169":"print(train_vectors)","f8de5ee1":"vectorizer_tfid = TfidfVectorizer()\nvectorizer_tfid.fit(train_df.text)","64fdbe3e":"print(vectorizer_tfid.vocabulary_)\nprint(vectorizer_tfid.idf_)\n# encode document\nvector = vectorizer_tfid.transform([train_df.text[1]])\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","fcfd2857":"clean_tex = []\nPorterS = PorterStemmer()\nfor i in range(train_df.text.shape[0]):\n    without_stop_words = [PorterS.stem(word.lower()) for word in train_df.text.str.split()[i] if not word  in stop_words  if word.isalpha()]\n    ff = ' '.join(without_stop_words)\n    clean_tex.append(ff)","7cce39fa":"print(pd.DataFrame(clean_tex)[0].head(5))","291b73d6":"vectorizer_tfid_after_cleaning = TfidfVectorizer()\nvectorizer_tfid_after_cleaning.fit(clean_tex)\nprint(vectorizer_tfid_after_cleaning)","7fad75f9":"print(vectorizer_tfid_after_cleaning.vocabulary_)","efabc1fa":"dict_count = vectorizer_tfid_after_cleaning.vocabulary_\n\nd_xtrain = dict(list(dict_count.items())[len(dict_count)\/\/2:])\nd_ytrain = dict(list(dict_count.items())[:len(dict_count)\/\/2])\n","99ad805a":"this is not a disaster tweets","677b03c3":"so i found out those 3 sentence and need to get better disintc this is after cleaning:\n\n1) ['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']\n\n2) ['man']\n\n3) ['always', 'try', 'bring', 'heavy', 'metal', 'rt', 'http']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","4133dd3a":"ok nice , so now we have all the words counting inside a dict from the text","55cd8030":"so how can i get the one words like earthquake and not \"man\" to get the context:\nfirst step is to suggest if the sentence is without \"NN\" it's not a disaster tweets in some probality.","d4198fa5":"need to take care to all the null but not to forget the amount of the nulls in the test_df","c3dff5e4":"clearly it's not a disaster"}}