{"cell_type":{"ca87dfa8":"code","2f34a5f6":"code","ea757d4c":"code","17482bca":"code","4382548a":"code","1e0b3aef":"code","a9a04b0c":"code","6c71fb16":"code","1eb9d4a6":"code","52232e80":"code","b7198aa4":"code","9b8769b8":"code","6e284b95":"code","dc86f3ae":"code","bc7c3460":"code","9d94867f":"code","50a58ba8":"code","e618bd6b":"code","13ccec5a":"code","28172bcf":"code","1ee69f59":"code","04bc0d5f":"code","56fb7d7c":"code","3ae5ba80":"code","b10fdca9":"code","3fa11c6a":"code","a802296c":"code","4cb6db66":"code","a94f5bf2":"code","ab271d8b":"code","cccfd5bc":"code","f3045ef7":"code","69e5bf6e":"code","85dc7abd":"code","ac744f3b":"code","48a32c48":"code","68771046":"code","3a9e994e":"code","e31ffa0e":"code","eb8a4a4e":"code","f9f626e1":"code","19365009":"code","8d88ea21":"code","86802552":"code","ed31c4d6":"code","d490d8d9":"code","0297be63":"code","c29d49cd":"code","1610937e":"code","c6557225":"code","f0789351":"code","90271fb3":"code","376dc8b3":"code","f11b7d1b":"code","c0900b38":"code","4f25434d":"code","f06876ee":"code","2587fc0a":"code","e9585b1c":"code","865fa623":"code","7bc9346a":"code","758abf91":"code","4587c206":"code","12d2bad5":"code","af639ba9":"code","fc3e6ae1":"code","bf078635":"code","c521791e":"code","b2ad2b65":"code","d9161378":"code","f9dd17a4":"code","26e1be16":"code","5a932af9":"code","81b48d31":"code","5397c1b7":"code","691b33e8":"code","41981b86":"code","e529cdda":"code","a4b44ab7":"code","fb2ccc0f":"code","77418c61":"markdown","3f015a3b":"markdown","84c0ae12":"markdown","0f7b673c":"markdown","5ac14d0b":"markdown","b21824c4":"markdown","a02569ea":"markdown","509ecdb0":"markdown","2ab135c5":"markdown","9cf185ed":"markdown","37a4eec9":"markdown","b02cc65a":"markdown","a886136a":"markdown","02f9f93b":"markdown","1aec67e9":"markdown","9a3f273a":"markdown","e3996dfa":"markdown","e91784f5":"markdown","6802a13b":"markdown","583f83d6":"markdown","1c044f5d":"markdown","8b617603":"markdown","2be3fa49":"markdown","34860345":"markdown","a9dc658b":"markdown","83c89eb1":"markdown","e4c007d5":"markdown","f67b3110":"markdown","fb0f4b4b":"markdown","4fd100df":"markdown","743d0d5f":"markdown","86d75927":"markdown","d3679ea1":"markdown","edb0458f":"markdown","41a2c7aa":"markdown","f228f56d":"markdown","d92022cb":"markdown","9645764a":"markdown","ab9a2a86":"markdown","1a59257f":"markdown","e924ea0a":"markdown","7fd41197":"markdown","6f619b69":"markdown","7df269c4":"markdown","e074a990":"markdown","ad16186d":"markdown","6158c809":"markdown","77431eb7":"markdown","06fa9911":"markdown"},"source":{"ca87dfa8":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')","2f34a5f6":"# Read in bureau\nbureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\nbureau.head()","ea757d4c":"# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","17482bca":"# Join to the training dataframe\ntrain = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Fill the missing values with 0 \ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","4382548a":"# kde \uadf8\ub798\ud504\ub97c \uadf8\ub9ac\uae30 \uc704\ud55c \ud568\uc218\n\n# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","1e0b3aef":"kde_target('EXT_SOURCE_3', train)","a9a04b0c":"kde_target('previous_loan_counts', train)","6c71fb16":"# Group by the client id, calculate aggregation statistics\nbureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\nbureau_agg.head()","1eb9d4a6":"# List of column names\ncolumns = ['SK_ID_CURR']\n\n# Iterate through the variables names\nfor var in bureau_agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'SK_ID_CURR':\n        \n        # Iterate through the stat names\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('bureau_%s_%s' % (var, stat))","52232e80":"# Assign the list of columns names as the dataframe column names\nbureau_agg.columns = columns\nbureau_agg.head()","b7198aa4":"# Train \ub370\uc774\ud130\uc5d0 merge \ud558\uc790\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","9b8769b8":"# List of new correlations\nnew_corrs = []\n\n# Iterate through the columns \nfor col in columns:\n    # Calculate correlation with the target\n    corr = train['TARGET'].corr(train[col])\n    \n    # Append the list as a tuple\n\n    new_corrs.append((col, corr))","6e284b95":"# Sort the correlations by the absolute value\n# Make sure to reverse to put the largest values at the front of list\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","dc86f3ae":"kde_target('bureau_DAYS_CREDIT_mean', train)","bc7c3460":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","9d94867f":"bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","50a58ba8":"bureau_agg.head()","e618bd6b":"# Function to calculate correlations with the target for a dataframe\ndef target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","13ccec5a":"# one-hot-encoding\ncategorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","28172bcf":"categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","1ee69f59":"categorical_grouped.columns.levels[0][:10]","04bc0d5f":"categorical_grouped.columns.levels[1]","56fb7d7c":"group_var = 'SK_ID_CURR'\n\n# Need to create new column names\ncolumns = []\n\n# Iterate through the variables names\nfor var in categorical_grouped.columns.levels[0]:\n    # Skip the grouping variable\n    if var != group_var:\n        # Iterate through the stat names\n        for stat in ['count', 'count_norm']:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (var, stat))\n\n#  Rename the columns\ncategorical_grouped.columns = columns\n\ncategorical_grouped.head()","3ae5ba80":"train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()\n","b10fdca9":"train.shape","3fa11c6a":"train.iloc[:10, 123:]","a802296c":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","4cb6db66":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","a94f5bf2":"# Read in bureau balance\nbureau_balance = pd.read_csv('..\/input\/home-credit-default-risk\/bureau_balance.csv')\nbureau_balance.head()","ab271d8b":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","cccfd5bc":"# Calculate value count statistics for each `SK_ID_CURR` \nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","f3045ef7":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","69e5bf6e":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","85dc7abd":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","ac744f3b":"# Read in new copies of all the dataframes\ntrain = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nbureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\nbureau_balance = pd.read_csv('..\/input\/home-credit-default-risk\/bureau_balance.csv')","48a32c48":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","68771046":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","3a9e994e":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","e31ffa0e":"bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","eb8a4a4e":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","f9f626e1":"original_features = list(train.columns)\nprint('Original Number of Features: ', len(original_features))","19365009":"# Merge with the value counts of bureau\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","8d88ea21":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))\n","86802552":"\n# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","ed31c4d6":"missing_train = missing_values_table(train)\nmissing_train.head(10)","d490d8d9":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","0297be63":"\n# Read in the test dataframe\ntest = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\n\n# Merge with the value counts of bureau\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","c29d49cd":"print('Shape of Testing Data: ', test.shape)","1610937e":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels","c6557225":"print('Training Data Shape: ', train.shape)\nprint('Testing Data Shape: ', test.shape)","f0789351":"missing_test = missing_values_table(test)\nmissing_test.head(10)","90271fb3":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\nlen(missing_test_vars)","376dc8b3":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))","f11b7d1b":"# Drop the missing columns\ntrain = train.drop(columns = missing_columns)\ntest = test.drop(columns = missing_columns)","c0900b38":"# # \uc800\uc7a5\ud55c\ubc88 \ud558\uae30\n# train.to_csv('train_bureau_raw.csv', index = False)\n# test.to_csv('test_bureau_raw.csv', index = False)","4f25434d":"corrs = train.corr()\n\ncorrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","f06876ee":"# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","2587fc0a":"kde_target(var_name='client_bureau_balance_counts_mean', df=train)","e9585b1c":"kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)","865fa623":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","7bc9346a":"\n# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","758abf91":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","4587c206":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","12d2bad5":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","af639ba9":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    # feature\ub791 test_features \ub294 dataframe \uc790\uccb4\uac00 \ub4e4\uc5b4\uc628\ub2e4.\n    # \uc704\uc5d0\uc11c \uc9c4\ud589\ud55c train_corrs_removed \ub4f1\uc774 \ub4e4\uc5b4\uc628\ub2e4.\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    # pk \uac12\ub4e4\uc744 \ube80\ub2e4\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    # TARGET \uc744 \ube80\ub2e4\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    # \uceec\ub7fc \uc81c\uac70\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    # \uc778\ucf54\ub529\uc774 ohe \uc77c \ub54c\ub294 \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc9c4\ud589\ud55c\ub2e4.\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        # \ud2b9\uc131\ub9cc\ud07c \ub3cc\ub9ac\uba74\uc11c\n        for i, col in enumerate(features):\n            # \ubc94\uc8fc\ud615\uc774\uba74\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                # label encode \ub97c \uc9c4\ud589\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    # KFold \ub9cc\ub4e0\ub2e4\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    # \ud2b9\uc131 \uc911\uc694\ub3c4 \ubc30\uc5f4\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    # Test \uc608\uce21\ud588\uc744 \ub54c \ub123\uc744 \ubc30\uc5f4\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    # validation \ud560 \ub54c \ub098\uc624\ub294 \uc608\uce21 \uac12\ub4e4 \ub123\ub294 \ubc30\uc5f4\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        # \ubaa8\ub378\uc744 \ub9cc\ub4e0\ub2e4\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        # \ud6c8\ub828\uc744 \uc2dc\ud0a8\ub2e4\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        # \ud55c\ubc88 kfold \ub3cc\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \ub4a4 \ucd5c\uace0 \uc131\uc801\uc744 \uac00\uc838\uc628\ub2e4\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        # \ud55c\ubc88 kfold \ub3cc\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \ub4a4 \uadf8 \ud6c8\ub828\ud560 \ub54c \uc911\uc694\ud558\uac8c\n        # \ubcf8 \ud2b9\uc131\uc744 \uac00\uc838\uc640\uc11c feature_importance_values\uc5d0 \ub123\uc5b4\uc900\ub2e4\n        # \uadfc\ub370 kfold \ub9cc\ud07c \ub3c4\ub2c8\uae4c \ub098\ub220\uc900\ub2e4\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        # \uc608\uce21\uc744 \ub123\uc5b4\uc900\ub2e4\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        # validation \uc5d0 \ub300\ud55c \uac12\uc744 \ub123\uc5b4\uc900\ub2e4\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        # \ucd5c\uace0 \uc88b\uc740 \uc131\uc801\uc744 \ubf51\uc544\ub0b8\ub2e4\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    # \uc5ec\uae30\uc5d0\uc11c test \uc5d0 \ub300\ud55c \uc608\uce21\uac12\uc744 \ub123\uc5b4\uc900\ub2e4\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    # \uc5ec\uae30\uc5d0\uc11c \ud2b9\uc131 \uc911\uc694\ub3c4\ub97c \ub123\uc5b4\uc900\ub2e4\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    # valid \uc5d0 \ub300\ud55c roc_auc_socre \ubf51\uc544\ub0b8\ub2e4\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    # valid \uc640 train\uc758 score\ub97c \ub123\uc5b4\uc8fc\uace0\n    # valid_scores \ub294 \uc704\uc5d0\uc11c for \ubb38\uc5d0\uc11c \ub123\uc5b4\uc92c\ub358 \ubc30\uc5f4\uc774\ub2e4\n    # train scores \ub3c4 \ub9c8\ucc2c\uac00\uc9c0.\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","fc3e6ae1":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","bf078635":"train_control = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntest_control = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')","c521791e":"submission, fi, metrics = model(train_control, test_control)","b2ad2b65":"metrics","d9161378":"fi_sorted = plot_feature_importances(fi)","f9dd17a4":"submission.to_csv('control.csv', index = False)","26e1be16":"submission_raw, fi_raw, metrics_raw = model(train, test)","5a932af9":"metrics_raw","81b48d31":"fi_raw_sorted = plot_feature_importances(fi_raw)","5397c1b7":"top_100 = list(fi_raw_sorted['feature'])[:100]\nnew_features = [x for x in top_100 if x not in list(fi['feature'])]\n\nprint('%% of Top 100 Features created from the bureau data = %d.00' % len(new_features))","691b33e8":"submission_raw.to_csv('test_one.csv', index = False)","41981b86":"submission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)","e529cdda":"metrics_corr","a4b44ab7":"fi_corrs_sorted = plot_feature_importances(fi_corrs)","fb2ccc0f":"submission_corrs.to_csv('test_two.csv', index = False)","77418c61":"---\n\n - \ucee4\ub9ac\ud058\ub7fc : https:\/\/kaggle-kr.tistory.com\/32?category=868318\n\n - \uc6d0\ubb38 : https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction\n---","3f015a3b":"### Value counts of Bureau Balance dataframe by loan","84c0ae12":"## Aggregated Stats of Bureau Balance by Client","0f7b673c":"bureau \uc5d0\uc11c \uac00\uc9c0\uace0 \uc628 previous_loan\uc774 \ubcc4 \ud2b9\ubcc4\ud55c \uac83\uc774 \uc5c6\uc5c8\ub2e4.\n\n\uc774\uc81c bureau\uc5d0\uc11c \ub2e8\uc21c\ud55c count \uac12\uc774 \uc544\ub2cc mean, max \ub4f1 \uceec\ub7fc\uc744 \uc0c8\ub85c \ucd94\uac00\ud574\ubcf4\uc790","5ac14d0b":"\uc790 \uc9c0\uae08\uae4c\uc9c0\uac00 bureau.csv \uc758 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \ub2e4\ub8e8\uc5c8\uc73c\uba70,\n\uc774 \uacfc\uc815\ub4e4\uc744 \ud568\uc218\ub85c \ubb36\uc5c8\ub2e4.\n\n\uc774\uc81c bureau_balance.csv \ub370\uc774\ud130\ub97c \ub2e4\ub8e8\uc5b4\ubcf4\uc790.\n\n - bureau_balance : bareau \uc6d4\ubcc4 \ub370\uc774\ud130","b21824c4":"\uba3c\uc800 dict \uc758 \ud615\ud0dc\ub85c key, value \uac12\uc73c\ub85c corr\uc774 \ub192\uc740 \uc598\ub4e4\uc744 \ub123\uc5b4\uc8fc\uc790.\nkey\ub294 \uceec\ub7fc\uba85\uc774\uace0 value\ub294 \ud574\ub2f9 \uceec\ub7fc\uacfc corr \uac12\uc774 0.8 \uc774\uc0c1\uc778 \uc598\ub4e4\uc758 \uac12\uc774\ub2e4.\n\n\uc5ec\uae30\uc11c \uac19\uc740 \uceec\ub7fc\ub07c\ub9ac\ub294 \uc0c1\uad00\uacc4\uc218\uac00 1\uc774\ubbc0\ub85c \uac19\uc740 \uceec\ub7fc \uac12\uc774 \ub4e4\uc5b4\uac04\ub2e4.\n\uc774 \uac12\ub4e4\uc740 \ube7c\uc8fc\uc790","a02569ea":"Train, Test \ub370\uc774\ud130\ub97c \uc815\ub82c\ud558\uc790.","509ecdb0":"\uc704\uc758 \ud568\uc218\uac00 \uc81c\ub300\ub85c \uc791\uc131\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 <br>\n\uc9c1\uc811 \ud574\ubd24\ub358 \ub370\uc774\ud130 \ud504\ub808\uc784\uacfc \ube44\uad50\ud574\ubcf4\uc790","2ab135c5":"90% \uc774\uc0c1\uc758 \uacb0\uce21\uac12\uc744 \uac00\uc9c4 feature\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0, <br>\n\uc704\uc758 \uacfc\uc815\uc5d0\uc11c feature\uc758 \uc218\ub97c \uc904\uc774\uc9c0 \ubabb\ud588\ub2e4.\n\n\uc774\uc5d0 \ub530\ub77c \ub2e4\ub978 feature selection \ubc29\ubc95\uc744 \ud65c\uc6a9\ud558\uc790.","9cf185ed":"## Function for Numeric Aggregations","37a4eec9":"### Aggregated stats of Bureau Balance dataframe by loan","b02cc65a":"# <center>Level 3. Home Credit Default Risk<\/center>\n### <center>- Manual Feature Engineering -<\/center>\n# <center>**\uc8fc\ud0dd \ub300\ucd9c \ubbf8\uc0c1\ud658 \uace0\uac1d \uc608\uce21**<\/center>\n## <center>\uc81c\uc791\uc790 : \uc11c\uc0c1\ud6c8<\/center>","a886136a":"\uc5ec\uae30\uc11c \uc0c1\uad00\uacc4\uc218\uac00 \ub108\ubb34 \ub192\uc740 \ubd80\ubd84\uc744 \uc81c\uac70\ud558\uc790.\nthreshold \ub97c 0.8\ub85c \uc7a1\uace0 \uadf8 \uc774\uc0c1\uc774 \ub418\ub294 \uac12\ub4e4\uc740 \uc81c\uac70\ud558\uc790.","02f9f93b":"## Correlation","1aec67e9":"\uba3c\uc800 \ud14c\uc2a4\ud2b8\ub85c \uc624\ub9ac\uc9c0\ub110 \ub370\uc774\ud130\ub85c \ub2e8\uc21c\ud788 \ud6c8\ub828\uc744 \ud558\uace0 \uc608\uce21\uac12\uc744 \ubf51\uc544\ub0b4\uc790","9a3f273a":"## Modeling","e3996dfa":"\uc81c\ucd9c\uc744 \ud574\ubcf4\ub2c8 0.745\uc758 \uc815\ud655\ub3c4\uac00 \ub098\uc654\ub2e4.","e91784f5":"## Control","6802a13b":"\uc704\uc758 \uacfc\uc815\ub4e4\uc744 \uc124\uba85\ud558\uc790\uba74,\nbureau_balance \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0\n1. categorical \ub370\uc774\ud130\ub97c count, mean \ub9cc\ub4e0\ub2e4\n2. numeric \ub370\uc774\ud130\ub97c count, mean, median, max, mun \ub9cc\ub4e0\ub2e4.\n3. 2\uc758 \ub370\uc774\ud130\ub97c outer \uc870\uc778\uc73c\ub85c \ud569\uccd0\uc900\ub2e4.\n4. \uc774\ud6c4 bureau[['SK_ID_BUREAU', 'SK_ID_CURR']] \uc744 \uae30\uc900\uc73c\ub85c<br>\n   left \uc870\uc778\uc744 \ud574\uc918\uc11c SK_ID_CURR\uc744 \ucd94\uac00\ud574\uc900\ub2e4.\n   \n5. \uadf8 \ub2e4\uc74c \uc774\ub807\uac8c \ub098\uc628 \uceec\ub7fc \uac12\uc744 \uae30\uc900\uc73c\ub85c numeric \ub370\uc774\ud130\ub97c<br>\n   agg_numeric\uc5d0\ub123\uc5b4\uc11c count, mean, min \ub4f1\uc758 \uceec\ub7fc\uc744 \ubf51\uc544\ub0b4\uc900\ub2e4.","583f83d6":"## Correlations\n\n\uba3c\uc800 \ubcc0\uc218\uc640 TARGET \uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc0b4\ud3b4\ubcf4\uc790.","1c044f5d":"\uacb0\uce21\uac12\uc774 \ub192\uc740 feature\uac00 \ub9ce\ub2e4.<br>\n\uc5ec\uae30\uc11c\ub294 90% \uc774\uc0c1\uc758 \uacb0\uce21\uac12\uc744 \uac16\ub294 Test \ub370\uc774\ud130\uc758 feature\ub97c \uc81c\uac70\ud558\uc790.","8b617603":"## Putting the Functions Together","2be3fa49":"\uc544\uae4c \uc704\uc5d0\uc11c \ub9cc\ub4e4\uc5c8\ub358 \ubc94\uc8fc\ud615 \ubcc0\uc218 \ud568\uc218\ub97c \ud65c\uc6a9\ud574\uc11c <br>\nbureau_balance_count \ub3c4 return \ubc1b\uc790.","34860345":"\uc774 \uacfc\uc815\ub4e4\uc774 \ubcf5\uc7a1\ud558\uace0 \ub09c\ud574\ud574 \ubcf4\uc77c \uc218 \uc788\ub2e4.\n\uc0ac\uc2e4 \ub098\ub3c4 \ubb34\uc2a8 \ub9d0\uc778\uc9c0 \uc774\ud574\uac00 \ub418\uc9c0 \uc54a\ub294\ub2e4.\n\uadf8\ub798\uc11c \uc6d0 \ub178\ud2b8 \uc81c\uc791\uc790\ub3c4 \ub2e4\uc2dc \uc815\ub9ac\ud560 \uacb8 \uc9c4\ud589\ud55c\ub2e4.","a9dc658b":"## Insert Computed Features into Training Data\n\n\uc704\uc758 \ub370\uc774\ud130\ub4e4\uc744 \ud558\ub098\uc529 \ucc28\ub840\ub300\ub85c train\uc5d0 merge \ud558\uc790.\nSK_ID_CURR\uc744 \uae30\uc900\uc73c\ub85c \ud569\uccd0\uc8fc\uace0 how='left'\ub85c \ud569\uccd0\uc8fc\uc790.","83c89eb1":"## Categorical Variables\nnumeric \ub370\uc774\ud130\ub97c \ub2e4\ub8e8\uc5c8\uc73c\ub2c8 \uc774\uc81c categorical (\ubc94\uc8fc\ud615) \ubcc0\uc218\ub97c \ub2e4\ub904\ubcf4\uc790\n\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \ub2e4\ub8f0 \ub54c \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294 one-hot-encoding\uc744 \ud574\ubcf4\uc790","e4c007d5":"## Correlation Function\nbureau \ub370\uc774\ud130\uc5d0 \uc5ec\ub7ec \uceec\ub7fc\uc744 \ucd94\uac00\ud55c \uacfc\uc815\uc744 \ud568\uc218\ub85c \ud45c\ud604\ud55c \uac83\ucc98\ub7fc\n\uc0c1\uad00\uad00\uacc4\ub97c \uad6c\ud588\ub358 \uacfc\uc815\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ud568\uc218\ub85c \ub9cc\ub4e4\uc5b4\ubcf4\uc790.","f67b3110":"### Aggregating Numeric Columns","fb0f4b4b":"\uc9c0\ub09c \ub178\ud2b8\uc5d0\uc11c\ub294 application_train \/ application_test \ub9cc \uc0ac\uc6a9\ud588\ub294\ub370\n\uc774\ubc88 \ub178\ud2b8\uc5d0\uc11c\ub294 bureau \ub77c\ub294 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud574\ubcf4\uc790\n\n - bureau : \ub2e4\ub978 \uae08\uc735 \uae30\uad00\uc73c\ub85c\ubd80\ud130\uc758 \ub300\ucd9c \ub0b4\uc5ed","4fd100df":"\uc81c\ucd9c\uc744 \ud574\ubcf4\ub2c8 0.759\uc758 \uc815\ud655\ub3c4\uac00 \ub098\uc654\ub2e4.","743d0d5f":"\ubaa9\ud45c\uc640 \uac00\uc7a5 \ub192\uc740 \uc0c1\uad00 \uacc4\uc218\ub97c \uac16\ub294 \ubcc0\uc218\ub294 \uc6b0\ub9ac\uac00 \ub9cc\ub4e4\uc5c8\ub358 \ubcc0\uc218\ub2e4.\n\uc880 \ub354 \uc790\uc138\ud788 \ud30c\uc545\ud558\uae30 \uc704\ud574 \uc544\uae4c \ub9cc\ub4e4\uc5c8\ub358 kde plot \ud568\uc218\ub97c \ud65c\uc6a9\ud574\ubcf4\uc790","86d75927":"\uac01 \uceec\ub7fc \ubcc4\ub85c count, mean, max, min, sum \uc774 \ucd94\uac00\ub418\uc11c \uadf8\ub824\uc9c4\ub2e4.","d3679ea1":"\uc0c1\uad00\uacc4\uc218\uac00 \ub9e4\uc6b0 \uc57d\ud558\uace0 \ub531\ud788 \uc774 \ubcc0\uc218\uac00 \uc911\uc694\ud55c\uc9c0 \ud655\uc778\ud558\uae30 \uc5b4\ub835\ub2e4.\nbureau \ub370\uc774\ud130\uc5d0\uc11c \uba87 \uac00\uc9c0 \ubcc0\uc218\ub97c \ub354 \ub9cc\ub4e4\uc5b4\ubcf4\uc790.","edb0458f":"bureau \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0, agg \uc5f0\uc0b0\uc790\ub97c \ud1b5\ud574 <br>\nmea, max, min, sum, count \uac12\uc744 \ud65c\uc6a9\ud558\uc790\n\n\uc774\ud6c4 \ucc98\ub9ac\ub41c \ub370\uc774\ud130\ub97c train\uc5d0 merge \ud558\uc790.","41a2c7aa":"\uc774\uc81c train \ub370\uc774\ud130\uc5d0 merge \ud558\uc790.","f228f56d":"\uc774\uc81c \ub450 \ub370\uc774\ud130 \ud504\ub808\uc784\uc740 TARGET \uc744 \uc81c\uc678\ud558\uace0 \ub3d9\uc77c\ud55c \uc5f4\uc744 \uac16\ub294\ub2e4.<br>\n\uc774\ub294 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4.\n\n\uc774\uc81c Test \ub370\uc774\ud130\uc5d0\uc11c \uacb0\uce21\uac12\uc744 \uc0b4\ud3b4\ubcf4\uace0<br>\n\uc0ad\uc81c\ud560 feature\uac00 \uc788\ub294\uc9c0 \ud655\uc778\ud574\ubcf4\uc790","d92022cb":"\uc774\uc81c \uc6b0\ub9ac\uac00 \uc0c8\ub85c \ub9cc\ub4e4\uc5c8\ub358 \ubcc0\uc218\ub97c kde plot \uc744 \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\uc790","9645764a":"\uce74\ud14c\uace0\ub9ac \ub370\uc774\ud130\uc758 level\uc774 2\ub2e8\uacc4\ub85c \ub098\ub204\uc5b4\uc838 \uc788\uc73c\ubbc0\ub85c\n\ub2e4\ub8e8\uae30 \uc704\ud574\uc11c for\ubb38\uc744 \ud65c\uc6a9\ud558\uc5ec 1\ub2e8\uacc4\ub85c \ud1b5\ud569\ud558\uc790","ab9a2a86":"\uc81c\ucd9c\uc744 \ud574\ubcf4\ub2c8 0.753\uc758 \uc815\ud655\ub3c4\uac00 \ub098\uc654\ub2e4.","1a59257f":"## Calculate Information for Testing Data\n","e924ea0a":"## Test One\n\n\uc774\uc81c \uc6b0\ub9ac\uac00 \uc5ec\ud0dc\uaecf \ub9cc\ub4e4\uc5c8\ub358 \ub370\uc774\ud130\uc14b\uc744 \uc774\uc6a9\ud574\uc11c \ud6c8\ub828\uc744 \ud574\ubcf4\uc790\n\uba3c\uc800 \uc544\uae4c corr \uad00\ub828\ud574\uc11c drop\uc744 \uc2dc\ud0a4\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\ub85c \ud574\ubcf4\uc790","7fd41197":"## Feature Engineering Outcomes\n\n### Missing Values","6f619b69":"\uc790 ! \uc774\uc81c \uc704\uc758 \uacfc\uc815\uc744 \ud1b5\ud574\uc11c numeric \ub370\uc774\ud130\ub97c\ncount, mean, max, mun, sum\uc744 \ucd94\uac00\ud558\ub294 \uacfc\uc815\uc744 \uc0b4\ud3b4\ubcf4\uc558\ub2e4.\n\uc774\uc81c \uc774 \uacfc\uc815\uc744 \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4e4\uc5b4\ubcf4\uc790","7df269c4":"\uc55e\uc11c \uac01 \ud0c0\uc785\uc758 \ubcc0\uc218\ub4e4\uc744 \ud568\uc218\ub85c \ub9cc\ub4e4\uc5c8\ub4ef\uc774,\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub3c4 \ud568\uc218\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\uc790.","e074a990":"one-hot-encoding \uc744 \uc2dc\ud589\ud55c \ub370\uc774\ud130\uc5d0\uc11c sum\uacfc mean \ub370\uc774\ud130\ub9cc \uac00\uc838\uc624\uc790","ad16186d":"\uc0c8\ub85c\uc6b4 \ubcc0\uc218\uac00 \uc720\uc6a9\ud55c\uc9c0 \uc0c1\uad00\uacc4\uc218\ub85c \ud655\uc778\ud574\ubcf4\uc790","6158c809":"## Test Two\ncorr \uad00\ub828\ud574\uc11c drop \uc2dc\ud0a8 \uac83\uc73c\ub85c \uc9c4\ud589\ud574\ubcf4\uc790","77431eb7":"\uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\ub2c8 \uc0c8\ub85c\uc6b4 \ubcc0\uc218\ub4e4 \uc911 \uc5b4\ub290\uac83\ub3c4 TARGET\uacfc \uc720\uc758\ud55c\n\uc0c1\uad00\uad00\uacc4\ub97c \uac16\uc9c0 \uc54a\ub294\ub2e4. \uc808\ub300\uc801\uc778 \uce21\uba74\uc5d0\uc11c bureau_DAYS_CREDIT_mean\uc774\n\uc81c\uc77c \ub192\uc740 \uc0c1\uad00 \ubcc0\uc218\uc784\uc73c\ub85c kde plot \uc744 \ud655\uc778\ud574\ubcf4\uc790","06fa9911":"### Aggregated Stats of Bureau Dataframe"}}