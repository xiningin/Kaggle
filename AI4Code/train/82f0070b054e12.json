{"cell_type":{"404d7c0b":"code","09872fe8":"code","e267d0c7":"code","b013f424":"code","9a687853":"code","d99a4c8d":"code","5e143fe4":"code","e97442d5":"markdown"},"source":{"404d7c0b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport transformers\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nimport torch\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\ngc.enable()","09872fe8":"TRAINED_ROBERTA_FOLDER=\".\/robertabase_clrp_model\"\nTRAIN_FILE_IN=\"..\/input\/commonlitreadabilityprize\/test.csv\"\nVAL_FILE_IN=\"..\/input\/commonlitreadabilityprize\/test.csv\"\nBOOK_DATA=\"..\/input\/additional-clrp-input\/books.csv\" #from gutenberg project\nADDL_CLRP_DATA = \"..\/input\/additional-clrp-input\/CRLP_Input.csv\" #additional passages from\n                                                                #common.lit.org\nTRAIN_FILE_OUT= \".\/clrp_corpus.csv\"\nMODEL_PATH  = '..\/input\/roberta-base'\nEPOCHS=5","e267d0c7":"#set up gpu\nscaler = torch.cuda.amp.GradScaler() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","b013f424":"train = pd.read_csv(TRAIN_FILE_IN)\ntest = pd.read_csv(VAL_FILE_IN)\nlit = pd.read_csv(ADDL_CLRP_DATA)\nbooks=pd.read_csv(BOOK_DATA)\ntrain2=train[[\"excerpt\"]]\ntest2=test[[\"excerpt\"]]\nlit2=lit[[\"excerpt\"]]\nbooks2=books[[\"excerpt\"]]\n\n#use everything for training\ntrain=pd.concat([train2,test2, lit2, books2])\n\ntrain['excerpt']=train['excerpt'].apply(lambda x: x if len(x)<= 512 else x[:512])\ntrain['excerpt'] = train['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n\ntrain.to_csv(TRAIN_FILE_OUT, index=False)","9a687853":"model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","d99a4c8d":"#To train model using all data for training and evaluation\n# due to limited data size\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT,\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT, \n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/checkpoints\", \n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=500,\n    save_steps=1000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","5e143fe4":"#~45 minutes\ntrainer.train()\ntrainer.save_model(TRAINED_ROBERTA_FOLDER)","e97442d5":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model -- **This Notebook**](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertalarge-masked-lm-model\/)\n* [Fine Tune Trained Roberta-Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\nhttps:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-pretrain\n\nand by examples\/documentation at https:\/\/huggingface.co\/\n\nNote that due to copyright concerns I am not making the data in the additional-clrp-input folder public. CRLP_Input.csv contains excerpts I manually downloaded from various places, including the site of the contest sponsor [CommonLit](https:\/\/www.commonlit.org\/)  and [Simple English Wikipedia](https:\/\/simple.wikipedia.org\/wiki\/Main_Page). Books.csv was auto-generated using the code at https:\/\/www.kaggle.com\/charliezimmerman\/fetch-clrp-data-from-web\/ \n\n"}}