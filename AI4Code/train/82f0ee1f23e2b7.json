{"cell_type":{"dad11258":"code","6f4fff9d":"code","e35fdc82":"code","aff49cee":"code","ec241456":"code","8a125882":"code","d075976f":"code","802bf89f":"code","e91cde40":"code","a8be8880":"code","cb99a795":"code","a68c0ab5":"markdown","58d2660a":"markdown","7403cd99":"markdown","0c232bff":"markdown"},"source":{"dad11258":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f4fff9d":"df=pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf.head()","e35fdc82":"df.isnull().sum()# NO missing value","aff49cee":"df.Species.value_counts()","ec241456":"# Converting labels into numerical values\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf.Species=le.fit_transform(df.Species)","8a125882":"#Dividing into features and target\nX=df.drop(columns=['Species','Id'])\nY=df.Species","d075976f":"#Train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=.2,random_state=1)","802bf89f":"X.head()","e91cde40":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nknn=KNeighborsClassifier(n_neighbors=5)\n\n#Fitting the model on train data\nknn.fit(x_train,y_train)\n\n#Calculating accuracy score\nacc_train=accuracy_score(y_train,knn.predict(x_train))\nacc_test=accuracy_score(y_test,knn.predict(x_test))\nprint(f'accuracy score on train is {acc_train}')\nprint(f'accuracy score on test is {acc_test}')","a8be8880":"#Lets see results when n_neighbours is different\nacc_train=[]\nacc_test=[]\nfor i in range(2,10):\n    print(f'For n_neighbours = {i}')\n    knn1=KNeighborsClassifier(n_neighbors=i)\n    \n    #Fitting the model on train data\n    knn1.fit(x_train,y_train)\n    \n   #Calculating accuracy score\n    acc_train.append(accuracy_score(y_train,knn1.predict(x_train)))\n    acc_test.append(accuracy_score(y_test,knn1.predict(x_test)))\n    print(f'accuracy score on train is {accuracy_score(y_train,knn1.predict(x_train))}')\n    print(f'accuracy score on test is {accuracy_score(y_test,knn1.predict(x_test))}')\n    print()","cb99a795":"import matplotlib.pyplot as plt\n#plotting test and train score\nplt.plot(range(2,10),acc_train,color='g')\nplt.plot(range(2,10),acc_test,color='orange')","a68c0ab5":"# **KNN Algorithm**\nKNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.","58d2660a":"## Cons of KNN\nThe testing phase of K-nearest neighbor classification is slower and costlier in terms of time and memory. It requires large memory for storing the entire training dataset for prediction. KNN requires scaling of data because KNN uses the Euclidean distance between two data points to find nearest neighbors. Euclidean distance is sensitive to magnitudes. The features with high magnitudes will weight more than features with low magnitudes. KNN also not suitable for large dimensional data.","7403cd99":"## Steps for KNN\n1. Calculate distance\n2. Find closest neighbors\n3. Vote for labels","0c232bff":"## Pros of KNN\nThe training phase of K-nearest neighbor classification is much faster compared to other classification algorithms. There is no need to train a model for generalization, That is why KNN is known as the simple and instance-based learning algorithm. KNN can be useful in case of nonlinear data. It can be used with the regression problem. Output value for the object is computed by the average of k closest neighbors value."}}