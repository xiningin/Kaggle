{"cell_type":{"f7962675":"code","89608a5c":"code","28b0df80":"code","b18937cc":"code","e38c3180":"code","b40b02df":"code","cf97f5b5":"code","af6648f6":"code","0d0d460a":"code","01015e76":"code","105fcc4f":"code","b3d42a6c":"code","0ec1d0f6":"code","cfd1712d":"code","eefd3b1d":"code","9ff6d5ee":"code","e3e1bba2":"code","ecb4beab":"code","ba080445":"code","25027d3d":"code","b86ccc2e":"code","b078ea2f":"code","c3eb9636":"markdown","4d6271fc":"markdown","4c46fa04":"markdown","75a84add":"markdown","1ea9698d":"markdown","5f5273ca":"markdown","6fc90529":"markdown","38a959bb":"markdown"},"source":{"f7962675":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\nplt.style.use(\"ggplot\")\n\nimport tensorflow as tf\nprint('Tensorflow version:', tf.__version__)\nprint('GPU detected:', tf.config.list_physical_devices('GPU'))","89608a5c":"data = pd.read_csv(r\"..\/input\/clientdat\/UTPBatchModified_final.csv\",encoding  ='ISO-8859-1')\nfor i in range(1,18):\n    data=data.drop(['ATTRIBUTE_NAME_'+str(i)], axis = 1)\ndata=data.drop(['NOUN'], axis = 1)\ndata=data.drop(['MFR_NAME_1'], axis = 1)\ndata=data.drop(['COMMENTS'], axis = 1)\ndata=data.drop(['STATUS (CLEANSED\/HOLD\/ENRICHED)'], axis = 1)\ndata=data.drop(['Mfr\/Vendor_Remarks'], axis = 1)","28b0df80":"for i in range(0,(len(data))):\n    if (data['STANDARDIZED_VALUE_2'].loc[i][-2:]!='IN' and data['STANDARDIZED_VALUE_2'].loc[i]!='-'):\n        data['STANDARDIZED_VALUE_2'].loc[i]=data['STANDARDIZED_VALUE_2'].loc[i]+' IN'\n    elif(data['STANDARDIZED_VALUE_2'].loc[i]== '-'):\n        data['STANDARDIZED_VALUE_2'].loc[i]=data['STANDARDIZED_VALUE_2'].loc[i]+' IN'\n    elif (data['STANDARDIZED_VALUE_2'].loc[i][-2:]=='IN'):\n        data['STANDARDIZED_VALUE_2'].loc[i]=data['STANDARDIZED_VALUE_2'].loc[i][:-2]+'-IN'\n    elif (data['STANDARDIZED_VALUE_2'].loc[i][-2:]=='MM'):\n        data['STANDARDIZED_VALUE_2'].loc[i]=data['STANDARDIZED_VALUE_2'].loc[i][:-2]+'-MM'\n        \nfor i in range(0,(len(data))):\n    if (data['STANDARDIZED_VALUE_10'].loc[i][-2:]!='IN' and data['STANDARDIZED_VALUE_10'].loc[i]!='-'):\n        data['STANDARDIZED_VALUE_10'].loc[i]=data['STANDARDIZED_VALUE_10'].loc[i]+'_IN'\n    elif(data['STANDARDIZED_VALUE_10'].loc[i]== '-'):\n        data['STANDARDIZED_VALUE_10'].loc[i]='_IN'\n    elif (data['STANDARDIZED_VALUE_10'].loc[i][-2:]=='IN'):\n        data['STANDARDIZED_VALUE_10'].loc[i]=data['STANDARDIZED_VALUE_10'].loc[i][:-2]+'_IN'\n    elif (data['STANDARDIZED_VALUE_10'].loc[i][-2:]=='MM'):\n        data['STANDARDIZED_VALUE_10'].loc[i]=data['STANDARDIZED_VALUE_2'].loc[i][:-2]+'_MM'","b18937cc":" data['STANDARDIZED_VALUE_10']","e38c3180":"test_data=pd.DataFrame()\nfor i in range(0,len(data)):\n    data1 = data.iloc[i].reset_index()\n    for j in range(len(data.iloc[i])):\n        if(data1[i][j]=='-'):\n            data1=data1.drop(j)\n    t_data=pd.DataFrame([{'Words' : 'wrd', 'Tags' : 'tg'}])\n    t_data=t_data['Words'].append((data1))\n    t_data=t_data.drop(0)\n    t_data=t_data.drop(1)\n    t_data=t_data.rename(columns={'index': 'Tags', i :'Words'})\n    t_data=t_data.reset_index()[['Tags','Words']]\n    l = [(i+1) for s in range(len(t_data))]\n    Sentence=pd.Series(l)\n    t_data['Sentence #']=Sentence\n    test_data=test_data.append(t_data)\ntest_data=test_data.reset_index()[['Tags','Words','Sentence #']]\ntest_data['Tags']=test_data['Tags'].replace({'MFR_NAME_1':'MFR_NAME'})\ntest_data.shape    ","b40b02df":"test_data.drop_duplicates(subset =\"Words\",keep = 'first', inplace = True)\ndf2 = pd.DataFrame({\"Tags\":['O'], \n                    \"Words\":['None'], 'Sentence #':[1007]})\ntest_data=test_data.append(df2)\ntest_data=test_data.reset_index()[['Tags','Words','Sentence #']]","cf97f5b5":"words = list(set(test_data[\"Words\"].values))\nwords.append(\"ENDPAD\")\nnum_words = len(words)\ntags = list(set(test_data[\"Tags\"].values))\nnum_tags = len(tags)\nprint(\"Unique words in corpus:\", test_data['Words'].nunique())\nprint(\"Unique tags in corpus:\", test_data['Tags'].nunique())","af6648f6":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Words\"].values.tolist(), s[\"Tags\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","0d0d460a":"getter = SentenceGetter(test_data)\nsentences = getter.sentences","01015e76":"word2idx = {w: i + 1 for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}","105fcc4f":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sentence length:', maxlen)","b3d42a6c":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","0ec1d0f6":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_len = 20\n\nX = [[word2idx[w[0]] for w in s] for s in sentences]\n\n\ny = [[tag2idx[w[1]] for w in s] for s in sentences]\n","cfd1712d":"X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=num_words-1)\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])","eefd3b1d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","9ff6d5ee":"from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense\nfrom tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional","e3e1bba2":"input_word = Input(shape=(max_len,))\nmodel = Embedding(input_dim=num_words, output_dim=20, input_length=max_len)(input_word)\nmodel = SpatialDropout1D(0.1)(model)\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\nout = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(model)\nmodel = Model(input_word, out)\nmodel.summary()","ecb4beab":"model.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])","ba080445":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","25027d3d":"%%time\n\nchkpt = ModelCheckpoint(\"model_weights.h5\", monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=1, verbose=0, mode='max', baseline=None, restore_best_weights=False)\nhistory = model.fit(x=x_train,y=y_train,validation_data=(x_test,y_test),batch_size=32,epochs=20,verbose=1)","b86ccc2e":"model.evaluate(x_test, y_test)","b078ea2f":"i = np.random.randint(0, x_test.shape[0]) #659\np = model.predict(np.array([x_test[i]]))\np = np.argmax(p, axis=-1)\ny_true = y_test[i]\nprint(\"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\"))\nprint(\"-\" *30)\nfor w, true, pred in zip(x_test[i], y_true, p[0]):\n    print(\"{:15}{}\\t{}\".format(words[w-1], tags[true], tags[pred]))","c3eb9636":"### Task 3: Retrieve Sentences and Corresponsing Tags","4d6271fc":"### Task 8: Evaluate Named Entity Recognition Model","4c46fa04":"### Task 6: Build and Compile a Bidirectional LSTM Model","75a84add":"### Task 2: Load and Explore the NER Dataset","1ea9698d":"### Task 7: Train the Model","5f5273ca":"### Task 4: Define Mappings between Sentences and Tags","6fc90529":"### Task 1: Project Overview and Import Modules","38a959bb":"### Task 5: Padding Input Sentences and Creating Train\/Test Splits"}}