{"cell_type":{"5e04f7a5":"code","d9607e7f":"code","898ef45c":"code","97d884f8":"code","a9eeeb23":"code","b7a7acb4":"code","3677dd47":"code","3cac010f":"code","6420c803":"code","fa49c8e1":"code","b0af3da1":"code","ba1b9161":"code","93016159":"code","d4186319":"code","a2d034b3":"code","0784899e":"code","541a59f9":"code","d432f3f9":"code","52a6d4f5":"code","1215bd3a":"code","a96ff291":"code","cf5b3d4c":"markdown","b842a59d":"markdown","e2ccd355":"markdown","f1a89ed9":"markdown","d8097f85":"markdown","5dc33f28":"markdown","2044be46":"markdown"},"source":{"5e04f7a5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport imageio\nimport time\nfrom IPython.display import HTML\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs","d9607e7f":"my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",[\"red\",\"yellow\",\"green\"])","898ef45c":"np.random.seed(0)","97d884f8":"N = 100\nM = 200\na = np.random.randn(N, M)\nb = np.random.randn(N, M)\nc = np.zeros((N, M))","a9eeeb23":"%%time # magic commands\nfor i in range(N):\n    for j in range(M):\n        c[i, j] = a[i, j] + b[i, j]","b7a7acb4":"%%time \nc = a + b","3677dd47":"%%time\nfor i in range(N):\n    for j in range(M):\n        c[i, j] = np.sin(a[i, j] + 1)","3cac010f":"%%time \nc = np.sin(a +1)","6420c803":"data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\nprint(data.shape, labels.shape)","fa49c8e1":"plt.scatter(data[:,0], data[:,1], c = labels, cmap=my_cmap)\nplt.show()","b0af3da1":"labels_orig = labels\nlabels = np.mod(labels_orig, 2)","ba1b9161":"plt.scatter(data[:,0], data[:,1], c=labels, cmap=my_cmap)\nplt.show()","93016159":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels_orig, stratify=labels_orig,random_state=0)\nprint(X_train.shape, X_val.shape, labels_orig.shape)","d4186319":"enc = OneHotEncoder()\n# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\ny_OH_train = enc.fit_transform(np.expand_dims(Y_train, 1)).toarray()\ny_OH_val = enc.fit_transform(np.expand_dims(Y_val,1)).toarray()\nprint(y_OH_train.shape, y_OH_val.shape)","a2d034b3":"W1 = np.random.randn(2,2)\nW2 = np.random.randn(2,4)\nprint(W1)\nprint(W2)","0784899e":"class FF_MultiClass_Scalar:\n  \n  def __init__(self, W1, W2):\n    self.w1 = W1[0][0].copy()\n    self.w2 = W1[1][0].copy()\n    self.w3 = W1[0][1].copy()\n    self.w4 = W1[1][1].copy()\n    self.w5 = W2[0][0].copy()\n    self.w6 = W2[1][0].copy()\n    self.w7 = W2[0][1].copy()\n    self.w8 = W2[1][1].copy()\n    self.w9 = W2[0][2].copy()\n    self.w10 = W2[1][2].copy()\n    self.w11 = W2[0][3].copy()\n    self.w12 = W2[1][3].copy()\n    self.b1 = 0\n    self.b2 = 0\n    self.b3 = 0\n    self.b4 = 0\n    self.b5 = 0\n    self.b6 = 0\n  \n  def sigmoid(self, x):\n    return 1.0\/(1.0 + np.exp(-x))\n  \n  def forward_pass(self, x):\n    # input layer\n    self.x1, self.x2 = x\n    \n    # hidden layer\n    self.a1 = self.w1*self.x1 + self.w2*self.x2 + self.b1\n    self.h1 = self.sigmoid(self.a1)\n    self.a2 = self.w3*self.x1 + self.w4*self.x2 + self.b2\n    self.h2 = self.sigmoid(self.a2)\n    \n    # output layer\n    self.a3 = self.w5*self.h1 + self.w6*self.h2 + self.b3\n    self.a4 = self.w7*self.h1 + self.w8*self.h2 + self.b4\n    self.a5 = self.w9*self.h1 + self.w10*self.h2 + self.b5\n    self.a6 = self.w11*self.h1 + self.w12*self.h2 + self.b5\n    sum_exps = np.sum([np.exp(self.a3), np.exp(self.a4), np.exp(self.a5), np.exp(self.a6)])\n    self.h3 = np.exp(self.a3)\/sum_exps\n    self.h4 = np.exp(self.a4)\/sum_exps\n    self.h5 = np.exp(self.a5)\/sum_exps\n    self.h6 = np.exp(self.a6)\/sum_exps\n    \n    return np.array([self.h3, self.h4, self.h5, self.h6])\n  \n  def grad(self, x, y):\n    self.forward_pass(x)\n    self.y1, self.y2, self.y3, self.y4 = y\n    \n    self.da3 = (self.h3-self.y1)\n    self.da4 = (self.h4-self.y2)\n    self.da5 = (self.h5-self.y3)\n    self.da6 = (self.h6-self.y4)\n    \n    self.dw5 = self.da3*self.h1\n    self.dw6 = self.da3*self.h2\n    self.db3 = self.da3\n    \n    self.dw7 = self.da4*self.h1\n    self.dw8 = self.da4*self.h2\n    self.db4 = self.da4\n    \n    self.dw9 = self.da5*self.h1\n    self.dw10 = self.da5*self.h2\n    self.db5 = self.da5\n    \n    self.dw11 = self.da6*self.h1\n    self.dw12 = self.da6*self.h2\n    self.db6 = self.da6\n    \n    self.dh1 = self.da3*self.w5 + self.da4*self.w7 + self.da5*self.w9 + self.da6*self.w11\n    self.dh2 = self.da3*self.w6 + self.da4*self.w8 + self.da5*self.w10 + self.da6*self.w12\n    \n    self.da1 = self.dh1 * self.h1*(1-self.h1)\n    self.da2 = self.dh2 * self.h2*(1-self.h2)\n    \n    self.dw1 = self.da1*self.x1\n    self.dw2 = self.da1*self.x2\n    self.db1 = self.da1\n    \n    self.dw3 = self.da2*self.x1\n    self.dw4 = self.da2*self.x2\n    self.db2 = self.da2\n    \n  \n  def fit(self, X, Y, epochs=1, learning_rate=1, display_loss=False, display_weight=False):\n      \n    if display_loss:\n      loss = {}\n    \n    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dw1, dw2, dw3, dw4, dw5, dw6, dw7, dw8, dw9, dw10, dw11, dw12, db1, db2, db3, db4, db5, db6  = [0]*18\n      for x, y in zip(X, Y):\n        self.grad(x, y)\n        dw1 += self.dw1\n        dw2 += self.dw2\n        dw3 += self.dw3\n        dw4 += self.dw4\n        dw5 += self.dw5\n        dw6 += self.dw6\n        dw7 += self.dw7\n        dw8 += self.dw8\n        dw9 += self.dw9\n        dw10 += self.dw10\n        dw11 += self.dw11\n        dw12 += self.dw12\n        db1 += self.db1\n        db2 += self.db2\n        db3 += self.db3\n        db4 += self.db4\n        db2 += self.db5\n        db3 += self.db6\n        \n      m = X.shape[0]\n      self.w1 -= (learning_rate * (dw1 \/ m))\n      self.w2 -= (learning_rate * (dw2 \/ m))\n      self.w3 -= (learning_rate * (dw3 \/ m))\n      self.w4 -= (learning_rate * (dw4 \/ m))\n      self.w5 -= (learning_rate * (dw5 \/ m))\n      self.w6 -= (learning_rate * (dw6 \/ m))\n      self.w7 -= (learning_rate * (dw7 \/ m))\n      self.w8 -= (learning_rate * (dw8 \/ m))\n      self.w9 -= (learning_rate * (dw9 \/ m))\n      self.w10 -= (learning_rate * (dw10 \/ m))\n      self.w11 -= (learning_rate * (dw11 \/ m))\n      self.w12 -= (learning_rate * (dw12 \/ m))\n      self.b1 -= (learning_rate * (db1 \/ m))\n      self.b2 -= (learning_rate * (db2 \/ m))\n      self.b3 -= (learning_rate * (db3 \/ m))\n      self.b4 -= (learning_rate * (db4 \/ m))\n      self.b5 -= (learning_rate * (db5 \/ m))\n      self.b6 -= (learning_rate * (db6 \/ m))\n      \n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[i] = log_loss(np.argmax(Y, axis=1), Y_pred)\n    \n    if display_loss:\n      Wt1 = [[self.w1, self.w3], [self.w2, self.w4]]\n      Wt2 = [[self.w5, self.w6, self.w7, self.w8], [self.w9, self.w10, self.w11, self.w12]]\n      plt.plot(np.fromiter(loss.values(), dtype = float))\n      plt.xlabel('Epochs')\n      plt.ylabel('Log Loss')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.forward_pass(x)\n      Y_pred.append(y_pred)\n    return np.array(Y_pred)","541a59f9":"class FF_MultiClass_WeightVectorised:\n  \n  def __init__(self, W1, W2):\n    self.W1 = W1.copy()\n    self.W2 = W2.copy()\n    self.B1 = np.zeros((1,2))\n    self.B2 = np.zeros((1,4))\n  \n  def sigmoid(self, x):\n    return 1.0\/(1.0 + np.exp(-x))\n  \n  def softmax(self, x):\n    exps = np.exp(x)\n    return exps \/ np.sum(exps)\n  \n  def forward_pass(self, x):\n    x = x.reshape(1, -1) # (1, 2)\n    self.A1 = np.matmul(x,self.W1) + self.B1  # (1, 2) * (2, 2) -> (1, 2)\n    self.H1 = self.sigmoid(self.A1) # (1, 2)\n    self.A2 = np.matmul(self.H1, self.W2) + self.B2 # (1, 2) * (2, 4) -> (1, 4) \n    self.H2 = self.softmax(self.A2) # (1, 4)\n    return self.H2\n    \n  def grad_sigmoid(self, x):\n    return x*(1-x) \n  \n  def grad(self, x, y):\n    self.forward_pass(x)\n    x = x.reshape(1, -1) # (1, 2)\n    y = y.reshape(1, -1) # (1, 4)\n    \n    self.dA2 = self.H2 - y # (1, 4) \n    \n    self.dW2 = np.matmul(self.H1.T, self.dA2) # (2, 1) * (1, 4) -> (2, 4)\n    self.dB2 = self.dA2 # (1, 4)\n    self.dH1 = np.matmul(self.dA2, self.W2.T) # (1, 4) * (4, 2) -> (1, 2)\n    self.dA1 = np.multiply(self.dH1, self.grad_sigmoid(self.H1)) # -> (1, 2)\n    \n    self.dW1 = np.matmul(x.T, self.dA1) # (2, 1) * (1, 2) -> (2, 2)\n    self.dB1 = self.dA1 # (1, 2)\n\n  \n  def fit(self, X, Y, epochs=1, learning_rate=1, display_loss=False):\n      \n    if display_loss:\n      loss = {}\n    \n    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dW1 = np.zeros((2,2))\n      dW2 = np.zeros((2,4))\n      dB1 = np.zeros((1,2))\n      dB2 = np.zeros((1,4))\n      for x, y in zip(X, Y):\n        self.grad(x, y)\n        dW1 += self.dW1\n        dW2 += self.dW2\n        dB1 += self.dB1\n        dB2 += self.dB2  \n        \n      m = X.shape[0]\n      self.W2 -= learning_rate * (dW2\/m)\n      self.B2 -= learning_rate * (dB2\/m)\n      self.W1 -= learning_rate * (dW1\/m)\n      self.B1 -= learning_rate * (dB1\/m)\n\n      if display_loss:\n        Y_pred = self.predict(X)\n        loss[i] = log_loss(np.argmax(Y, axis=1), Y_pred)\n        \n    \n    if display_loss:\n      plt.plot(np.fromiter(loss.values(), dtype = float))\n      plt.xlabel('Epochs')\n      plt.ylabel('Log Loss')\n      plt.show()\n      \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.forward_pass(x)\n      Y_pred.append(y_pred)\n    return np.array(Y_pred).squeeze()","d432f3f9":"class FF_MultiClass_InputWeightVectorised:\n\n    def __init__(self, W1, W2):\n        self.W1 = W1.copy()\n        self.W2 = W2.copy()\n        self.B1 = np.zeros((1,2))\n        self.B2 = np.zeros((1,4))\n\n    def sigmoid(self, X):\n        return 1.0\/(1.0 + np.exp(-X))\n    \n    def softmax(self, X):\n        exps = np.exp(X)\n        return exps \/ np.sum(exps, axis=1).reshape(-1,1)\n    \n    def forward_pass(self, X):\n        self.A1 = np.matmul(X, self.W1) + self.B1 # (N, 2) * (2, 2) -> (N, 2)\n        self.H1 = self.sigmoid(self.A1) # (N, 2)\n        self.A2 = np.matmul(self.H1, self.W2) + self.B2 # (N, 2) * (2, 4) -> (N, 4)\n        self.H2 = self.softmax(self.A2) # (N, 4)\n        return self.H2\n\n    def grad_sigmoid(self, X):\n        return X*(1-X)\n\n    def grad(self, X, Y):\n        self.forward_pass(X)\n        m = X.shape[0]\n\n        self.dA2 = self.H2 -Y #(N, 4) - (N, 4) -> (N, 4)\n\n        self.dW2 = np.matmul(self.H1.T, self.dA2) # (2, N) * (N, 4) -> (2, 4)\n        self.dB2 = np.sum(self.dA2, axis=0).reshape(1,-1) # (N, 4) -> (1, 4)\n        self.dH1 = np.matmul(self.dA2, self.W2.T) # (N, 4) * (4, 2) -> (N, 2)\n        self.dA1 = np.multiply(self.dH1, self.grad_sigmoid(self.H1)) # (N, 2) .* (N, 2) -> (N, 2)\n\n        self.dW1 = np.matmul(X.T, self.dA1) # (2, N) * (N, 2) -> (2, 2) \n        self.dB1 = np.sum(self.dA1, axis=0).reshape(1, -1) # (N, 2) -> (1, 2)\n\n    def fit(self, X, Y, epochs=1, learning_rate=1, display_loss=False):\n\n        if display_loss:\n            loss={}\n\n        for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n\n            self.grad(X, Y) # X -> (N, 2), Y -> (N, 4)\n\n            m = X.shape[0]\n            self.W2 -= learning_rate * (self.dW2\/m)\n            self.B2 -= learning_rate * (self.dB2\/m)\n            self.W1 -= learning_rate * (self.dW1\/m) \n            self.B1 -= learning_rate * (self.dB1\/m) \n\n            if display_loss:\n                Y_pred = self.predict(X)\n                loss[i] = log_loss(np.argmax(Y, axis=1), Y_pred)\n\n        if display_loss:\n            plt.plot(np.fromiter(loss.values(), dtype = float))\n            plt.xlabel('Epochs')\n            plt.ylabel('Log Loss')\n            plt.show()\n\n    def predict(self, X):\n        Y_pred = self.forward_pass(X)\n        return np.array(Y_pred).squeeze()","52a6d4f5":"models_init = [FF_MultiClass_Scalar(W1, W2), FF_MultiClass_WeightVectorised(W1, W2), FF_MultiClass_InputWeightVectorised(W1, W2)]\nmodels = []\nfor idx, model in enumerate(models_init, start=1):\n    tic = time.time()\n    ffsn_multi_specific = model\n    ffsn_multi_specific.fit(X_train, y_OH_train, epochs=2000,learning_rate=.5,display_loss=True)\n    models.append(ffsn_multi_specific)\n    toc = time.time()\n    print(\"Time taken by model {}: {}\".format(idx, toc-tic))","1215bd3a":"for idx, model in enumerate(models, start=1):\n    Y_pred_train = model.predict(X_train)\n    Y_pred_train = np.argmax(Y_pred_train, 1)\n    \n    Y_pred_val = model.predict(X_val)\n    Y_pred_val = np.argmax(Y_pred_val, 1)\n    \n    accuracy_train = accuracy_score(Y_pred_train, Y_train)\n    accuracy_val = accuracy_score(Y_pred_val, Y_val)\n    \n    print(\"Model {}\".format(idx))\n    print(\"Training Accuracy\",round(accuracy_train,2))\n    print(\"Validation Accuracy\",round(accuracy_val,2))","a96ff291":"plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_train, cmap=my_cmap, s=15*(np.abs(np.sign(Y_pred_train-Y_train))+.1))\nplt.show()","cf5b3d4c":"Input + Weight Vectorised Version","b842a59d":"### Multi class Classification","e2ccd355":"### Weight Vectorised Version","f1a89ed9":"### Testing vectorisation","d8097f85":"### Generate Data","5dc33f28":"### Scalar Version","2044be46":"### Outline \n* Why vectorisation\n* Vectorisation examples\n* Scalar class - recap\n* Class with vectorised weights\n* Class with vectorised weights and inputs"}}