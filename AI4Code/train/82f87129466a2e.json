{"cell_type":{"cae9a6a9":"code","7fd8874f":"code","cd1be0ce":"code","4fdce772":"code","f96641f2":"code","45945109":"code","8094f743":"code","6ce53fe1":"code","7ca19692":"code","a3a8cab4":"code","69240644":"code","d1d5b834":"code","81f2c48a":"code","973ddf91":"code","973fe7c9":"code","59f6548b":"code","0997530a":"code","d1d715aa":"code","ebe329da":"code","e26d858d":"code","fbedfdc9":"code","e716afb5":"markdown","2e80eb92":"markdown","f439811b":"markdown"},"source":{"cae9a6a9":"# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# This is so we can check the files we have in the current system. Most Kaggle Notebook files are found under kaggle\/input\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# Load csv file that we found\n# Take note that this data set is just 15kb, so we are confident in putting everything in memory. \n# Fortunately Kaggle gives a provision of 16gb of running memory for Free! \n# So if your data set gets to more than that, I suggest you either breakdown the data or you use a database instead\n# as it will be more manageable for larger sets of data\ndf = pd.read_csv('\/kaggle\/input\/TankStatRank.csv') # df usually is used to abbreviate \"Data Frame\" from pandas library\n\n\n# Remember we are using Python 3 in this Notebook, so the last statement will be evaluated as string for output if any.\n# If you want to explicitly print, you can use the \"print()\" function\n\n# Shape shows you the # of samples and the number of features \/ cols you have in your data set.\nprint(f'Data Frame Shape (rows, columns): {df.shape}') \n\n# This method is one of your basic tool to quickly view data. It shows first 5 rows of a frame\ndf.head() ","7fd8874f":"# I really like this method. Quick way to visualize everything.\n# Take note that this is just a quick visualizations of what we have so far\n# And from this we can explore further. If you want to know more about the different visualization\n# functions; search for Seaborn. There are various libraries but this is my personal liking.\n# https:\/\/seaborn.pydata.org\/examples\/index.html\n\n# The \"hue\" refers to how you would want to differentiate the data in a visualization\n# In this case I selected the species as these are the classes we have in our data set\nsns.pairplot(df, hue=\"Hero\") ","cd1be0ce":"sns.countplot(data=df, x=\"Hero\").set_title(\"Hero Spread\")","4fdce772":"sns.relplot(data=df, x=\"Win_rate\", y=\"Pick_rate\", hue=\"Hero\", palette=\"bright\", height=6)","f96641f2":"sns.relplot(data=df, x=\"Pick_rate\", y=\"Rank\", hue=\"Hero\", palette=\"bright\", height=6)","45945109":"sns.relplot(data=df, x=\"Win_rate\", y=\"Rank\", hue=\"Hero\", palette=\"bright\", height=6)","8094f743":"# Check if there are any null values\ndf.isnull().values.any()","6ce53fe1":"# Remove null values\ndf = df.dropna()","7ca19692":"# Check if there are any null values\ndf.isnull().values.any()","a3a8cab4":"#Drop not needed columns \/ features\ndf.drop('ID', axis=1, inplace=True)\ndf.drop('Platform', axis=1, inplace=True)\ndf.drop('Role', axis=1, inplace=True)\ndf.drop('Rank', axis=1, inplace=True)\n\n\n\n\ndf.head()","69240644":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split","d1d5b834":"def get_performance_measures(actual, prediction):\n    matrix = confusion_matrix(actual, prediction)\n    FP = matrix.sum(axis=0) - np.diag(matrix)  \n    FN = matrix.sum(axis=1) - np.diag(matrix)\n    TP = np.diag(matrix)\n    TN = matrix.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","81f2c48a":"def sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP\/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP\/(TP+FN))[1] # Since the [0] part is the index\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN\/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN\/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","973ddf91":"# Define dictionary with performance metrics\n# To know what everaging to use: https:\/\/stats.stackexchange.com\/questions\/156923\/should-i-make-decisions-based-on-micro-averaged-or-macro-averaged-evaluation-mea#:~:text=So%2C%20micro%2Daveraged%20measures%20add,is%20more%20like%20an%20average.\n\n\nscoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted', zero_division=1),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitvity':make_scorer(sensitivity_score, mode=\"multiclass\"), \n            'specificity':make_scorer(specificity_score, mode=\"multiclass\"), \n           }","973fe7c9":"# Import required libraries for machine learning classifiers\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\nfrom sklearn.neighbors import KNeighborsClassifier #K-nearest Neighbors\nfrom sklearn.cluster import KMeans #K-means\n\n# Instantiate the machine learning classifiers\ndecisionTreeClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nlogisticRegression_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\nkNeighbors_model = KNeighborsClassifier()","59f6548b":"# features = data frame set that contain your features that will be used as input to see if prediction is equal to actual result\n# target = data frame set (1 column usually) that will contain your target or actual results.\n# folds = this is added so we can easily change the number of folds we want to do with our data set.\n# folding is a technique to minimise overfitting and therefore make our model more accurate.\ndef models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\n    decisionTreeClassifier_result = cross_validate(decisionTreeClassifier_model, features, target, cv=folds, scoring=scoring)\n    gaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\n    logisticRegression_result = cross_validate(logisticRegression_model, features, target, cv=folds, scoring=scoring)\n    linearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\n    kNeighbors_result = cross_validate(kNeighbors_model, features, target, cv=folds, scoring=scoring)\n    # kMeans_result = cross_validate(kMeans_model, features, target, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        decisionTreeClassifier_result['test_accuracy'].mean(),\n                        decisionTreeClassifier_result['test_precision'].mean(),\n                        decisionTreeClassifier_result['test_recall'].mean(),\n                        decisionTreeClassifier_result['test_sensitvity'].mean(),\n                        decisionTreeClassifier_result['test_specificity'].mean(),\n                        decisionTreeClassifier_result['test_f1_score'].mean()\n                       ],\n\n      'Gaussian Naive Bayes':[\n                                gaussianNB_result['test_accuracy'].mean(),\n                                gaussianNB_result['test_precision'].mean(),\n                                gaussianNB_result['test_recall'].mean(),\n                                gaussianNB_result['test_sensitvity'].mean(),\n                                gaussianNB_result['test_specificity'].mean(),\n                                gaussianNB_result['test_f1_score'].mean()\n                              ],\n\n      'Logistic Regression':[\n                                logisticRegression_result['test_accuracy'].mean(),\n                                logisticRegression_result['test_precision'].mean(),\n                                logisticRegression_result['test_recall'].mean(),\n                                logisticRegression_result['test_sensitvity'].mean(),\n                                logisticRegression_result['test_specificity'].mean(),\n                                logisticRegression_result['test_f1_score'].mean()\n                            ],\n\n      'Support Vector Classifier':[\n                                    linearSVC_result['test_accuracy'].mean(),\n                                    linearSVC_result['test_precision'].mean(),\n                                    linearSVC_result['test_recall'].mean(),\n                                    linearSVC_result['test_sensitvity'].mean(),\n                                    linearSVC_result['test_specificity'].mean(),\n                                    linearSVC_result['test_f1_score'].mean()\n                                   ],\n\n       'K-nearest Neighbors':[\n                        kNeighbors_result['test_accuracy'].mean(),\n                        kNeighbors_result['test_precision'].mean(),\n                        kNeighbors_result['test_recall'].mean(),\n                        kNeighbors_result['test_sensitvity'].mean(),\n                        kNeighbors_result['test_specificity'].mean(),\n                        kNeighbors_result['test_f1_score'].mean()\n                       ],\n\n      },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)","0997530a":"# Let's try to look at our data frame again one last time\ndf.head()","d1d715aa":"# Specify features columns\n# Actually what we are doing here is that we are just dropping the Species column since that is our class\n# and the remaining columns will then be our features (eg. inputs to come up to a class)\n# axis 0 basically means to drop all of that column\nfeatures = df.drop(columns=\"Hero\", axis=0)\n\n\n# Now let's see what features looks like\nfeatures\n\n# Don't mind the left hand side, those are just index mainly used for viewing","ebe329da":"# Specify target column\n# Now we try to get the frame of only our target. Which is the \"Species\" column\ntarget = df[\"Hero\"]\n\n# Do note that csv files are also zero-index, that means a row starts from zero.\ntarget","e26d858d":"evaluationResult = models_evaluation(features, target, 5)\nview = evaluationResult\nview = view.rename_axis('Test Type').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Test Type')\n# result\nsns.catplot(data=view, x=\"Test Type\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","fbedfdc9":"# In here we just add a new column to our raw data frame, that gets the result for the highest\n# scoring classifier in every score test.\nevaluationResult['Best Score'] = evaluationResult.idxmax(axis=1)\nevaluationResult","e716afb5":"**Data Preparation, Balancing and Cleanup**","2e80eb92":"# Conclusion\n\nThe final table clearly indicates that the best classifier which best suits the given data set would be Support Vector Classifier","f439811b":"**Correlation of between a hero's Win Rate and Pick Rate**\n\nThis visualization depicts the relationship between a heroes Win Rate and Pick Rate in the current rank season of the game Overwatch. The graph is somewhat scattered as there is a lot of external factors that owes to the statistics gained in the course of a game but generally, we can say that the hero \"Zarya\" has an average pick rate and win rate."}}