{"cell_type":{"733b316b":"code","c7419aba":"code","1069ea99":"code","f648b937":"code","ea653cc9":"code","1973d0c0":"code","e2af265f":"code","8090e747":"code","0d5d4ed4":"code","5efd473f":"code","73c136bf":"code","499e6c69":"code","0966065f":"code","655cfa86":"code","bc2c041b":"code","5ea56369":"code","57d94983":"code","ed9e3202":"markdown","ee1a0a4b":"markdown","6332d62b":"markdown","76d8d20e":"markdown","f538d915":"markdown","3b61e724":"markdown","d05fb755":"markdown","cf5b7f6d":"markdown","0c4d0450":"markdown","cb171c36":"markdown","ebe4008f":"markdown"},"source":{"733b316b":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom transformers import TFAutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport math, re, os\nimport string\n\n# Set seed for experiment reproducibility\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)","c7419aba":"train_loc = '..\/input\/nlp-getting-started\/train.csv'\ntest_loc = '..\/input\/nlp-getting-started\/test.csv'\nfile_path = '\/kaggle\/input\/huggingface-bert\/'\nMODEL_NAME = \"bert-large-uncased\"\nbatch_size = 16\nepochs = 6","1069ea99":"train_df = pd.read_csv(train_loc)\ntest_df = pd.read_csv(test_loc)\nprint('training items ' + str(train_df.shape[0]) )\nprint('test items ' + str(test_df.shape[0]) )","f648b937":"train_df.head(10)","ea653cc9":"maxlenght = train_df.text.map(len).max()\nmaxlenght_test = test_df.text.map(len).max()\n\nprint(f'maxmum str lenght in training  is : {maxlenght}\\n   max str lunght in test is : {maxlenght_test}\\n')","1973d0c0":"def clean(title):\n\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    title = title.lower()\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","e2af265f":"train_df[\"text\"] = train_df[\"text\"].map(clean)\ntest_df[\"text\"] = test_df[\"text\"].map(clean)\n","8090e747":"def tokeniz_dataset(tokenizer,max_len):\n\n    return {\n        \"train\": {\n            \"data\": tokenizer(list(train_df[\"text\"].values), padding = \"max_length\", max_length = max_len, truncation = True, return_tensors = \"tf\").data,\n            \"labels\": train_df[\"target\"].values,\n        },\n        \"test\": {\n            \"data\": tokenizer(list(test_df[\"text\"].values), padding = \"max_length\", max_length = max_len, truncation = True, return_tensors = \"tf\").data\n        }\n    }","0d5d4ed4":"class ClassifModel(tf.keras.Model):\n\n    def __init__(self, checkpoint):\n        super(ClassifModel, self).__init__()\n        \n        self.base_model = TFAutoModel.from_pretrained(checkpoint)\n        self.flatten = layers.Flatten()\n        \n        self.dropout1 = layers.Dropout(rate = 0.2)\n        self.linear1 = layers.Dense(units = 1024, kernel_regularizer = \"l1_l2\")\n        self.batchNorm1 = layers.BatchNormalization()\n        self.activation1 = layers.Activation(\"relu\")\n        \n        self.out = layers.Dense(units = 1, activation = \"sigmoid\")\n\n    def call(self, inputs, training = False):\n        x = self.base_model(inputs).last_hidden_state\n        x = self.flatten(x)\n        \n        x = self.dropout1(x) if training else x\n        x = self.linear1(x)\n        x = self.batchNorm1(x)\n        x = self.activation1(x)\n\n        x = self.out(x)\n        return x","5efd473f":"class F1_score(tf.keras.metrics.Metric):\n\n    def __init__(self, name = \"f1_score\", **kwargs):\n        super(F1_score, self).__init__(name = name, **kwargs)\n        \n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n        \n    def update_state(self, y_true, y_pred, sample_weight = None):\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n        \n    def reset_states(self):\n        self.precision.reset_states()\n        self.recall.reset_states()\n        \n    def result(self):\n        return 2 \/ ((1 \/ self.precision.result()) + (1 \/ self.recall.result()))","73c136bf":"def lrfn(epoch, bs=batch_size, epochs=epochs):\n    # Config\n    LR_START = 1e-5\n    LR_MAX = 2e-3\n    LR_FINAL = 1e-5\n    LR_RAMPUP_EPOCHS = 4\n    LR_SUSTAIN_EPOCHS = 0\n    DECAY_EPOCHS = epochs  - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n    LR_EXP_DECAY = (LR_FINAL \/ LR_MAX) ** (1 \/ (epochs - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1))\n\n    if epoch < LR_RAMPUP_EPOCHS: # exponential warmup\n        lr = LR_START + (LR_MAX + LR_START) * (epoch \/ LR_RAMPUP_EPOCHS) ** 2.5\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS: # sustain lr\n        lr = LR_MAX\n    else: # cosine decay\n        epoch_diff = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        decay_factor = (epoch_diff \/ DECAY_EPOCHS) * math.pi\n        decay_factor= (tf.math.cos(decay_factor).numpy() + 1) \/ 2        \n        lr = LR_FINAL + (LR_MAX - LR_FINAL) * decay_factor\n\n    return lr\n","499e6c69":"# plots the learning rate schedule\ndef show_lr_schedule(bs=batch_size, epochs=epochs):\n    rng = [i for i in range(epochs)]\n    y = [lrfn(x, bs=bs, epochs=epochs) for x in rng]\n    x = np.arange(epochs)\n    x_axis_labels = list(map(str, np.arange(1, epochs+1)))\n    print('init lr {:.1e} to {:.1e} final {:.1e}'.format(y[0], max(y), y[-1]))\n    \n    plt.figure(figsize=(30, 10))\n    plt.xticks(x, x_axis_labels, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.plot(rng, y)\n    plt.grid()\n    plt.show()\n    \nshow_lr_schedule()","0966065f":"\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-large-uncased' )\nmodel = ClassifModel('..\/input\/huggingface-bert\/bert-large-uncased')\nloss = BinaryCrossentropy()\nmodel.compile(loss = loss, optimizer = tf.keras.optimizers.Adam(), metrics = [\"accuracy\", F1_score()])\ntokenized_dataset = tokeniz_dataset(tokenizer,65)","655cfa86":"checkpoint_filepath = '.\/modebest-stlr.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_f1_score',\n    mode='max',\n    save_best_only=True)\nearlystop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_f1_score', patience=3,  \n    mode='max' \n)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch, epochs=epochs), verbose=1)\nhistory = model.fit(\n    x = tokenized_dataset[\"train\"][\"data\"],\n    y = tokenized_dataset[\"train\"][\"labels\"],\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split = 0.2, callbacks=[model_checkpoint_callback , earlystop , lr_callback]\n)","bc2c041b":"\nplt.figure(figsize = (14, 14))\n\n\nplt.plot(history.history[\"f1_score\"], label = \"f1_score\")\nplt.plot(history.history[\"val_f1_score\"], label = \"val_f1_score\")\nplt.title(\"F1 Score\")\nplt.ylabel(\"F1 Score\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")","5ea56369":"model.load_weights(checkpoint_filepath)\n\npredictions = model.predict(tokenized_dataset[\"test\"][\"data\"], verbose = True)\npredictions = np.where(predictions >= 0.5, 1, 0)\npredictions","57d94983":"submissions = test_df.drop(labels = [\"keyword\", \"location\", \"text\"], axis = 1)\nsubmissions[\"target\"] = predictions\nsubmissions.to_csv(\"submissions.csv\", index = False)","ed9e3202":"**f1 score for the compution porpuse **","ee1a0a4b":"# Define the model \n\n**bert + our own linear layers**","6332d62b":"# define paths for data and  the offline load of transformers lib","76d8d20e":"# very simple cleaning  function","f538d915":"# learning rate schudler ","3b61e724":"# Data Read","d05fb755":"**function to do tokenize and padd or truncation on the data **","cf5b7f6d":"# **import lib** # \n\nthe main part for bert is in transformers\n","0c4d0450":"# Start training ","cb171c36":"\n# load best model and make prediactions","ebe4008f":"# init tokenizer and model "}}