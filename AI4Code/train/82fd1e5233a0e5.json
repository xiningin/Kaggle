{"cell_type":{"e7a9a21c":"code","75dff13b":"code","7286ff9b":"code","04dc5ff5":"code","3d880368":"code","c1386554":"code","a5bd2781":"code","8dd430eb":"code","e3db48d9":"code","203036a8":"code","b293a5a9":"code","0d7648ae":"code","e9e41a33":"code","72b2f52c":"markdown","fd73c0f7":"markdown","131ab7c7":"markdown","34d1f2d8":"markdown","263ac44f":"markdown"},"source":{"e7a9a21c":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","75dff13b":"# Importing the data in to a dataframe\ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","7286ff9b":"print(train_data.isnull().any())","04dc5ff5":"#Exploring the data for missing values\n\nnull_data = train_data.isnull().any(axis=0)\npartial_null_data_list = []\nfull_null_data_list = []\ncounter_dict = {}\nthreshold_for_removal = 85\n\n# The following will display the number of NaN values in each column of the training data\nfor i in range(0,81):\n    counter_i = 0\n    if null_data[i] == True:\n        for j in range(len(train_data)):\n            if train_data[null_data.index[i]][j] != train_data[null_data.index[i]][j]:\n                counter_i += 1\n        full_null_data_list.append(null_data.index[i])\n        if counter_i >= threshold_for_removal:\n            partial_null_data_list.append(null_data.index[i])\n        print(null_data.index[i], 'column has', counter_i, 'null values')\n        counter_dict[null_data.index[i]] = counter_i","3d880368":"# Choosing the data to keep\nother_data_to_remove = ['SalePrice', 'Electrical']\nfeatures = []\nadditional_features = []\n\nfor column in train_data.columns:\n    if not ((column in full_null_data_list) or (column in other_data_to_remove)):\n        features.append(column)\n    elif not column == 'SalePrice':\n        #Exploring the data that we have decided to remove:\n        try:\n            plt.plot(train_data[column], train_data['SalePrice'], 'o')\n            plt.title(str(str(column) + '   NaN: ' + str(100*counter_dict[column]\/len(train_data)) + '%'))\n            plt.xlabel(column)\n            plt.ylabel('Sale price')\n            plt.show()\n            additional_features.append(column)\n        except ValueError:\n            pass\n        except TypeError:\n            pass","c1386554":"# Defining functions to check for and replace any NaN values in the data\n\ndef remove_nan(input_list):\n    output = []\n    for i in input_list:\n        if i == i:\n            output.append(i)\n    return output\n\ndef replace_nan(input_list):\n    list_avg =  np.mean(remove_nan(input_list))\n    for i in range(len(input_list)):\n        if input_list[i] != input_list[i]:\n            input_list[i] = list_avg\n    return input_list\n\ndef replace_nan_array(array):\n    new_array = []\n    assert len(array.shape) == 2\n    for i in range(array.shape[0]):\n        new_array.append(replace_nan(array[i]))\n    return np.array(new_array)","a5bd2781":"#Formatting the data in to a numpy array\n\ntrain_test_data = pd.merge(train_data, test_data, how='outer')\n\ndef toarray(data, features):\n    data = pd.get_dummies(data[features])\n    data = data.values\n    data = np.array(data, dtype=np.float64)\n    return data\n\ntrain_test_data = toarray(train_test_data, features)\nnum_features = train_test_data.shape[1]\n\ntrain_X = train_test_data[:1460, :num_features]\ntrain_y = toarray(train_data, ['SalePrice']).reshape(1460)\ntest_X = train_test_data[1460:, :num_features]\n\ntrain_X = replace_nan_array(train_X)\ntest_X = replace_nan_array(test_X)\n\n#Normalising the data\nmean = train_X.mean(axis=0)\nstd = train_X.std(axis=0)\ntrain_X -= mean\ntrain_X \/= std\ntest_X -= mean\ntest_X \/= std\n\nprint(train_X.shape)\nprint(test_X.shape)","8dd430eb":"#Setting up a function to build the model\n\nfrom keras import models, layers, regularizers                          \n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(128, activation='relu', input_shape=(num_features,), kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","e3db48d9":"# 4-fold validation of the data to help fine-tune the model, particularly deciding how many epochs to use in order\n# to not overfit or underfit\n\nk=4\nnum_val_samples = len(train_X) \/\/ k\nnum_epochs = 300\nbatch_size = 32\nall_mae_histories = []\n\nfor i in range(k):\n    print('Processing fold ', i)\n    val_X = train_X[i*num_val_samples: (i+1)*num_val_samples]\n    val_y = train_y[i*num_val_samples: (i+1)*num_val_samples]\n    \n    partial_train_X = np.concatenate([train_X[:i*num_val_samples], train_X[(i+1)*num_val_samples:]], axis=0)\n    partial_train_y = np.concatenate([train_y[:i*num_val_samples], train_y[(i+1)*num_val_samples:]], axis=0)\n    \n    model = build_model()\n    history = model.fit(partial_train_X, partial_train_y, validation_data=(val_X, val_y),\n                        epochs=num_epochs, batch_size=batch_size, verbose=0)\n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)\n\naverage_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","203036a8":"# # Plotting the graph of mae to check for overfitting\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","b293a5a9":"# Deciding exactly how many epochs to train the model with\noutput = 'Epochs:  '\nmin_positions = []\nfor i in range(len(average_mae_history)):\n    if average_mae_history[i] <= min(average_mae_history)*1.03:\n        output += str(i) + ': ' + str(average_mae_history[i]) + '   '\n        min_positions.append(i)\nprint(output)","0d7648ae":"# Building a fresh model, using the 4-fold validation results and the above to decide how many epochs to use\n\nmodel = build_model()\nmodel.fit(train_X, train_y, epochs=min_positions[len(min_positions)\/\/2], batch_size=batch_size, verbose=0)","e9e41a33":"# Making predictions for the test data\npredictions = model.predict(test_X)\npredictions = predictions.reshape((len(test_X)))\npredictions = replace_nan(predictions)\n\n# Outputting the data\noutput = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint('Complete')","72b2f52c":"# Validating the model","fd73c0f7":"# Final build and predictions","131ab7c7":"# Importing and exploring the data","34d1f2d8":"# Formatting the data ","263ac44f":"# Defining the model"}}