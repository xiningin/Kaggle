{"cell_type":{"83d22256":"code","1cfd261f":"code","d01b51f0":"markdown"},"source":{"83d22256":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# keep only docsuments with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint (df.shape)\n#drop duplicates\n#df=df.drop_duplicates()\n#drop NANs \ndf=df.fillna('no data provided')\ndf = df.drop_duplicates(subset='title', keep=\"first\")\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (df.shape)\ndf.head()","1cfd261f":"import functools\nfrom IPython.core.display import display, HTML\nfrom nltk import PorterStemmer\n\n# function to stem keyword list into a common base word\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\ndef search_dataframe(df,search_words):\n    search_words=stem_words(search_words)\n    df1=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search_words))]\n    return df1\n\n# function analyze search results for relevance with word count \/ abstract length\ndef search_relevance(rel_df,search_words):\n    rel_df['score']=\"\"\n    search_words=stem_words(search_words)\n    for index, row in rel_df.iterrows():\n        abstract = row['abstract']\n        result = abstract.split()\n        len_abstract=len(result)\n        score=0\n        for word in search_words:\n            score=score+result.count(word)\n        final_score=(score\/len_abstract)\n        rel_score=score*final_score\n        rel_df.loc[index, 'score'] = rel_score\n    rel_df=rel_df.sort_values(by=['score'], ascending=False)\n    #rel_df= rel_df[rel_df['score'] > .01]\n    return rel_df\n\n# function to get best sentences from the search results\ndef get_sentences(df1,search_words):\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"authors\",\"title\",\"excerpt\",\"rel_score\"])\n    search_words=stem_words(search_words)\n    for index, row in df1.iterrows():\n        pub_sentence=''\n        sentences_used=0\n        #break apart the absracrt to sentence level\n        sentences = row['abstract'].split('. ')\n        #loop through the sentences of the abstract\n        highligts=[]\n        for sentence in sentences:\n            # missing lets the system know if all the words are in the sentence\n            missing=0\n            #loop through the words of sentence\n            for word in search_words:\n                #if keyword missing change missing variable\n                if word not in sentence:\n                    missing=1\n                #if '%' in sentence:\n                    #missing=missing-1\n            # after all sentences processed show the sentences not missing keywords\n            if missing==0 and len(sentence)<1000 and sentence!='':\n                sentence=sentence.capitalize()\n                if sentence[len(sentence)-1]!='.':\n                    sentence=sentence+'.'\n                pub_sentence=pub_sentence+'<br><br>'+sentence\n        if pub_sentence!='':\n            sentence=pub_sentence\n            sentences_used=sentences_used+1\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            score=row[\"score\"]\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            sentence='<p fontsize=tiny\" align=\"left\">'+sentence+'<\/p>'\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            to_append = [row['publish_time'],authors[0]+' et al.',final_link,sentence,score]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    return df_table\n    \n################ main program ########################\n\ndisplay(HTML('<h1>Task 1: What is known about transmission, incubation, and environmental stability?<\/h1>'))\n\ndisplay(HTML('<h3>Results currently limited to two (10) for ease of scanning<\/h3>'))\n\n# list of lists of search terms\nquestions=[\n['Q: What is the range of incubation periods for the disease in humans?'],\n['Q: How long are individuals are contagious?'],\n['Q: How long are individuals are contagious, even after recovery.'],\n['Q: Does the range of incubation period vary across age groups?'],\n['Q: Does the range of incubation period vary with children?'],\n['Q: Does the range of incubation period vary based on underlying health?'],\n['Q: What do we know about the basic reproduction number?'],\n['Q: What is the prevalance of asymptomatic transmission?'],\n['Q: What do we know about asymptomatic transmission in children?'],\n['Q: What do we know about seasonality of transmission?'],\n['Q: Informing decontamination based on physical science of the coronavirus?'],\n['Q: What do we know about stability of the virus in environmental conditions?'],\n['Q: What do we know about persistence of the virus on various substrates? (e.g., nasal discharge, sputum, urine, fecal matter, blood)'],\n['Q: What do we know about persistence of the virus on various surfaces? (e,g., copper, stainless steel, plastic) '],\n['Q: What do we know about viral shedding duration?'],\n['Q: What do we know about viral shedding in fecal\/stool?'],\n['Q: What do we know about viral shedding from nasopharynx?'],\n['Q: What do we know about viral shedding in blood?'],\n['Q: What do we know about viral shedding in urine?'],\n['Q: What do we know about implemtation of diagnostics?'],\n['Q: What do we know about disease models?'],\n['Q: What do we know about models for disease infection?'],\n['Q: What do we know about models for disease transmission?'],\n['Q: Are there studies about phenotypic change?'],\n['Q: What is know about adaptations (mutations) of the virus?'],\n['Q: What do we know about the human immune response and immunity?'],\n['Q: Is population movement control effective in stopping transmission (spread)?'],\n['Q: Effectiveness of personal protective equipment (PPE)?'],\n['Q: What is the role of environment in transmission?']\n]\n#search=[['incubation','period','range','mean','%']]\nsearch=[['incubation','period','range'],\n['viral','shedding','duration'],\n['asymptomatic','shedding'],\n['incubation','period','age','statistically','significant'],\n['incubation','period','child'],\n['incubation','groups','risk'],\n['basic','reproduction', 'number','%'],\n['asymptomatic','infection','%'],\n['asymptomatic','children'],\n['seasonal','transmission'],\n['contaminat','object'],\n['environmental', 'conditions'],\n['sputum','stool','blood','urine'],\n['persistence','surfaces'],\n['duration','viral','shedding'],\n['shedding','stool'],\n['shedding','nasopharynx'],\n['shedding','blood'],\n['shedding','urine'],\n['diagnostics','point'],\n['model', 'disease'],        \n['model', 'infection'],\n['model', 'transmission'],\n['phenotypic'],\n['mutation'],\n['immune','response'],\n['restriction', 'movement'],\n['protective','clothing'],\n['transmission','routes']\n]\nq=0\nfor search_words in search:\n    str1=''\n    # a make a string of the search words to print readable version above table\n    str1=' '.join(questions[q])\n    \n    #search the dataframe for all words\n    df1=search_dataframe(df,search_words)\n\n    # analyze search results for relevance \n    df1=search_relevance(df1,search_words)\n\n    # get best sentences\n    df_table=get_sentences(df1,search_words)\n    \n    length=df_table.shape[0]\n    #limit 3 results\n    df_table=df_table.head(15)\n    df_table=df_table.drop(['rel_score'], axis=1)\n    #convert df to html\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    \n    # display search topic\n    display(HTML('<h3>'+str1+'<\/h3>'))\n    \n    #display table\n    if length<1:\n        print (\"No reliable answer could be located in the literature\")\n    else:\n        display(df_table)\n    q=q+1\nprint ('done')","d01b51f0":"# **COVID-19 TASK - 1 transmission incubation environment 2.0**\n\n**REVISION - 2020-03-30 code cleaned, modularized and commented**\n\n![](https:\/\/sportslogohistory.com\/wp-content\/uploads\/2018\/09\/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**PROBLEM:** When a new virus is discovered and causes a pandemic, it is important for scientists to get information coming from all scientific sources that may help them combat the pandemic.  The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this mass of data.\n\n**SOLUTION:** Create an unsupervised scientific literature understanding system that can take in common terms and analyze a very large corpus of scientific papers and return highly relevant text excerpts from papers containing topical data relating to the common text inputed, allowing a single researcher or small team to gather targeted information and quickly and easily locate relevant text in the scientific papers to answer important questions about the new virus from a large corpus of documents.\n\n**APPROACH:** The newest implementation outlined:\n- open the meta CSV file in a dataframe - 44K entries\n- drop the duplicates by abstract\n- loop (comprehension) a list of lists for the keyword for searches. eg. search=[['incubation','period','range']]\n- stem the keywords so incubation becomes incubat etc.\n- append indpendently - different variations of corona references - (covid, cov, -cov, hcov) to the keywords avoids pulling older research\n- query function in pandas to query the above terms with the keywords - the abstract has to have all the keywords and at least one of (covid, cov, -cov, hcov) \n- drop the duplicates in the df again after the queries\n- caclulate a relevance score for all the abstracts in the query result df\n- raw_score = total count of the keywords in the abstract\n- final_score = (raw_score\/len(abstract))*raw_score\n- sort the df on the final_score ascending=False (best on top)\n- drop rows with a relevance score < .02\n- parse the relevant abstarcts into sentences split('. ') works well.\n- test if keywords are in the sentences\n- if sentence has all keywords, it is added to the df for display.\n- if you are seeking statistics in the data, adding % to the search terms works well\n- df with search results displayed in HTML - currently limiting results to 3 for ease of scanning\n\n**Pros:** Currently the system is a very simple **(as Einstein said \"make it as simple as possible, but no simpler\")**, but quite effective solutiuon to providing insight on topical queries.  The code is easy to read and simple to understand.\n\n**Cons:** Currently the system requires some human understanding in crafting keyword combinations, however, the recent relevance measure changes and stemming of keywords, have made it so the keywords can be very close to the NL questions and return very good results.\n"}}