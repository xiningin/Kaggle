{"cell_type":{"8a24d8e4":"code","3cbb8962":"code","72b5f894":"code","29272048":"code","9f700427":"code","880b1e95":"code","e746b4ed":"code","2f14321a":"code","853ea249":"code","f6a37bef":"code","20d42eaf":"code","e16dcc86":"code","e56dc23e":"code","287ceaad":"code","75ceccb5":"code","44f0a3dc":"code","47103e48":"code","3e04794c":"code","a2290c3b":"code","a8df2818":"code","2beda564":"code","6dc61951":"code","8f44e0bd":"code","d33de49b":"code","24d5cc74":"code","cea8edf4":"code","d702a05b":"code","f352b856":"code","ee7b471c":"code","be14e250":"code","ecaa3c0f":"code","4f771bc4":"code","3d23f65f":"code","7b590d58":"code","742f3c4b":"code","667c3679":"markdown","a896e3b7":"markdown","1527ee38":"markdown","7f212f24":"markdown","bca5baec":"markdown","e79f154f":"markdown","760a1ba8":"markdown","25f92ebc":"markdown","1b73b2be":"markdown","7c1979fc":"markdown","d16bdf5f":"markdown","a8ee7532":"markdown","8f13e993":"markdown","b92ddfba":"markdown","ea90bf12":"markdown","e6e42533":"markdown","060ad101":"markdown","e00de216":"markdown","b9f8d4c3":"markdown","80a826d7":"markdown","8ecfbc38":"markdown","dbdc29a3":"markdown","e6009dde":"markdown","c6e8b4af":"markdown","06da5810":"markdown","6f816dc6":"markdown","07f7fec6":"markdown","1d353d94":"markdown","9e545667":"markdown","eee8bc8a":"markdown","a897dcb4":"markdown","c7ae4e32":"markdown","5543452b":"markdown","7b099039":"markdown","5765ef92":"markdown","928e9acc":"markdown","9c71dd45":"markdown","c3889198":"markdown","d1eb4684":"markdown","baaf879c":"markdown","5d90fffe":"markdown","fbc77174":"markdown","4cf5b9ee":"markdown"},"source":{"8a24d8e4":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline","3cbb8962":"data = pd.read_csv('..\/input\/heart-disease-dataset\/datasets_heart.csv') ","72b5f894":"data.head()","29272048":"print('Number of datapoints : {}'.format(data.shape[0]))\nprint('Number of Columns : {}'.format(data.shape[1])) \n\nprint(data.dtypes) \n\nprint( ' Distribution of class label points ' )\ndata['target'].value_counts()","9f700427":"data.describe().T","880b1e95":"print( data.isnull().sum() )\nprint( 'skewness table : \\n' , data.skew() )","e746b4ed":"data.columns = [ 'age', 'sex', 'chest pain type', 'resting blood pressure', 'serum cholestoral', 'fasting blood sugar > 120 mg\/dl', 'resting EC results', 'maximum heart rate',\n       'induced angina', 'ST depression', 'slope', 'num_vessels', 'thalassemia', 'target'  ] ","2f14321a":"# look the data now \n\ndata.head() ","853ea249":"data['induced angina'].value_counts() ","f6a37bef":"# age , resting blood pressure , serum cholestoral , maximum heart rate , ST depression \nplt.figure( figsize= (25,25) )\ndata[ ['age' , 'resting blood pressure' , 'serum cholestoral' , 'maximum heart rate' , 'ST depression'] ].hist(bins=10)\nplt.show()\n\n","20d42eaf":"# copying the existing data for future use and trying to map some categorical values to strings for better interpretation\n\nworkout_data = data.copy()\ndata['sex'] = data['sex'].map( { 1 : 'MALE' , 0 : 'FEMALE'} )\ndata['fasting blood sugar > 120 mg\/dl'] = data['fasting blood sugar > 120 mg\/dl'].map( { 1 : 'HIGH SUGAR' , 0 : 'LOW SUGAR' } ) ","e16dcc86":"data['target'] = data['target'].map({ 1 : 'HEART DISEASE' , 0 : 'NO HEART DISEASE' })","e56dc23e":"plt.figure(figsize=(10,10))\npx.box( data_frame=data , x='sex' , y= 'age' , color='target'  ) ","287ceaad":"sns.countplot( data = data , x = 'sex' , hue = 'target' )","75ceccb5":"plt.figure( figsize = (30,10) )\nsns.countplot( data = data , x = 'age' , hue = 'target' )","44f0a3dc":"sns.countplot( data = data , x = 'fasting blood sugar > 120 mg\/dl' , hue = 'target' )","47103e48":"px.box( data_frame= data , x= 'sex' , y= 'serum cholestoral' , color= 'target' )","3e04794c":"# ST depression \n\npx.box( data_frame= data , x= 'sex' , y= 'ST depression' , color= 'target' )","a2290c3b":"plt.figure( figsize = (30,10) )\nsns.countplot( data= data , x= 'ST depression' , hue = 'target' )","a8df2818":"# 'resting blood pressure' , 'maximum heart rate' \n\n\npx.box( data_frame= data , x = 'sex' , y= 'maximum heart rate' , color= 'target' )\n","2beda564":"plt.figure( figsize= (30,10) )\nsns.countplot(data = data , x = 'resting blood pressure' , hue = 'target')","6dc61951":"sns.heatmap( workout_data.corr() , vmin = 0 , vmax= 0.5 )\n\ncorr_matrix = workout_data.corr()\ncorr_matrix['target'].sort_values( ascending = False )","8f44e0bd":"# checking missing values \n\nworkout_data.isnull().sum()","d33de49b":"dummy_data = pd.get_dummies( data=workout_data , columns=['chest pain type' , 'slope' , 'thalassemia'] , \n               prefix= [ 'cp' , 'slope' , 'thal' ] ) \nprint('Dimension of our dummy data is : \\n')\ndummy_data.shape","24d5cc74":"x = dummy_data.drop( labels= ['target'] , axis = 1)\ny = dummy_data['target']\n\nseed = 20 \ntest_size = 0.2 \n\nx_train , x_test , y_train , y_test = train_test_split( x.values , y.values , test_size = test_size , random_state = seed ,\n                                                       stratify = y.values ) \n\n","cea8edf4":"models = [] \nmodels.append( ( 'Logistic' , LogisticRegression( n_jobs = -1 )   ) )\nmodels.append( ( 'KNN' , KNeighborsClassifier()   ) )\nmodels.append( ( 'NB' , GaussianNB()  ) )\nmodels.append( ( 'tree' , DecisionTreeClassifier()  ) )\nmodels.append( ( 'SVM' , SVC()   ) )\nmodels.append( ( 'RandForest' , RandomForestClassifier() ) )\n\nRESULTS = []\nNAME = [] \n\nscale = MinMaxScaler() \nscale.fit(x_train) # scaling the x_train for spot check validation stage \n\nx_train_scaled = scale.transform( x_train )\n\nfor name , model in models:\n    \n    kfold = KFold( n_splits= 10 , random_state= seed , shuffle= True )\n    cv_score = cross_val_score( model , x_train_scaled , y_train , cv = kfold , scoring='accuracy' )\n    RESULTS.append(cv_score)\n    NAME.append(name)\n    \n    result = 'name of the model : {} , score returned : {} % '.format( name , (cv_score.mean()*100).round(3) )\n    print(result) ","d702a05b":"sns.boxplot( x= NAME , y= RESULTS )","f352b856":"x_test_scaled = scale.transform( x_test ) \n\ntestmodels = [] \n\ntestmodels.append( ( 'SVM' , SVC()   ) )\ntestmodels.append( ( 'Logistic' , LogisticRegression( n_jobs = -1 )   ) )\ntestmodels.append( ( 'KNN' , KNeighborsClassifier()   ) )\ntestmodels.append( ( 'RandForest' , RandomForestClassifier(n_jobs= -1) ) )\n\n\nparams = []\n\nparams.append( ( 'SVM' ,     { 'C' : [ 0.1,1,3,5,7,10,50,100,1000 ] , 'gamma' : [ 1,0.1,0.01,0.001 ] , 'kernel': ['rbf', 'poly', 'sigmoid']} ) )\nparams.append( ( 'Logistic', { 'C' : [ 0.1,1,3,5,7 ,10, 100 , 1000 ] }  ) )\nparams.append( ( 'KNN' ,     { 'n_neighbors' : [ 3,5,7,11,19] , 'weights' : [ 'uniform' , 'distance' ] , 'metric' : ['euclidean' , 'manhattan' ] } ) )\nparams.append( ( 'RandForest'  , {'n_estimators': [100, 200, 300, 500]  } ) )\n\n\n\ndef tune( models_collection , param_grid , x_train , y_train  ) :\n\n  for name_ , model in models_collection :  \n    for name , paramdict in params:\n      if name_ == name :\n        \n        kfold = KFold( n_splits= 10 , random_state= seed , shuffle= True )\n        grid = GridSearchCV( model , param_grid= paramdict , n_jobs= -1 , cv=kfold )\n        grid.fit( x_train , y_train )  \n   \n        print('name of the model : ',name) \n        print( grid.best_estimator_  )  \n\n\n      \ntune( models_collection= testmodels , param_grid= params , x_train= x_train_scaled , y_train= y_train )\n     ","ee7b471c":"estimators = []\n\nestimators.append( ('scaler' , MinMaxScaler() ) )\nestimators.append( ( 'SVM' , SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='sigmoid',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)  ) )\n\nfinal_model = Pipeline(estimators)\n\nfinal_model.fit(x_train , y_train) \n\nprint('train score : {}'.format( (final_model.score(x_train , y_train)*100).round(3)) )\n\nprint('test score : {}'.format( (final_model.score(x_test , y_test)*100).round(3))) \n\nprediction_svm = final_model.predict( x_test )","be14e250":"estimators = []\n\nestimators.append( ('scaler' , MinMaxScaler() ) )\nestimators.append( ( 'Logistic' , LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=-1, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)  ) )\n\nfinal_model = Pipeline(estimators)\n\nfinal_model.fit(x_train , y_train) \n\nprint('train score : {}'.format( (final_model.score(x_train , y_train)*100).round(3)) )\n\nprint('test score : {}'.format( (final_model.score(x_test , y_test)*100).round(3)))","ecaa3c0f":"estimators = []\n\nestimators.append( ('scaler' , MinMaxScaler() ) )\nestimators.append( ( 'KNN' , KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n                     weights='uniform')  ) )\n\nfinal_model = Pipeline(estimators)\n\nfinal_model.fit(x_train , y_train) \n\nprint('train score : {}'.format( (final_model.score(x_train , y_train)*100).round(3)) )\n\nprint('test score : {}'.format( (final_model.score(x_test , y_test)*100).round(3)))","4f771bc4":"estimators = []\n\nestimators.append( ('scaler' , MinMaxScaler() ) )\nestimators.append( ( 'Random Forest' , RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n                       warm_start=False)  ) )\n\nfinal_model = Pipeline(estimators)\n\nfinal_model.fit(x_train , y_train) \n\nprint('train score : {}'.format( (final_model.score(x_train , y_train)*100).round(3)) )\n\nprint('test score : {}'.format( (final_model.score(x_test , y_test)*100).round(3)))","3d23f65f":"# added more hyperparameters to be optimized . \n\nkfold = KFold( n_splits= 10 , random_state= seed , shuffle= True )\ngrid = GridSearchCV( RandomForestClassifier() , param_grid= { 'max_depth': [80, 90, 100, 110], 'min_samples_leaf': [2, 3, 4, 5], 'min_samples_split': [8, 10, 12], 'n_estimators': [100, 200, 300, 1000] } , \n                    n_jobs= -1 , cv=kfold )\n\n\ngrid.fit( x_train , y_train )  \n\ngrid.best_score_ , grid.best_estimator_","7b590d58":"estimators = []\n\nestimators.append( ('scaler' , MinMaxScaler() ) )\nestimators.append( ( 'Random Forest' , RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                        criterion='gini', max_depth=100, max_features='auto',\n                        max_leaf_nodes=None, max_samples=None,\n                        min_impurity_decrease=0.0, min_impurity_split=None,\n                        min_samples_leaf=5, min_samples_split=10,\n                        min_weight_fraction_leaf=0.0, n_estimators=200,\n                        n_jobs=None, oob_score=False, random_state=None,\n                        verbose=0, warm_start=False)  ) ) #you could get all these values from colab\n\nfinal_model = Pipeline(estimators)\n\nfinal_model.fit(x_train , y_train) \n\nprint('train score : {}'.format( (final_model.score(x_train , y_train)*100).round(3)) )\n\nprint('test score : {}'.format( (final_model.score(x_test , y_test)*100).round(3)))","742f3c4b":"print( classification_report( y_test , prediction_svm ) )","667c3679":"renaming our columns for easier interpretation ","a896e3b7":"#### Preparing the data","1527ee38":"MODEL CHOSEN : SVM ","7f212f24":"There is no missing value . Only the scale of certain attributes need to be normalised , which could be done by a template . We have handled the categorical attributes by dummy technique .","bca5baec":"3. KNN","e79f154f":"Using Hyperparameter tuning","760a1ba8":"Females were already a leading contender in heart disease based on this dataset , but females who had higher serum cholestoral had equal say in getting and not getting heart complaint , almost a similar say in case of male . seems like serum cholestoral is not a big indicator of our target.","25f92ebc":"SUMMARY : Seems like Random Forest has returned some favourable score , but since it looks like a bit overfitting due to the complexity our model offers . So either try to tune more hyper parameters in RF or Go with Support vector machines since its a clean binary classification problem .\n\n ","1b73b2be":"Looks ok to me :-) \n\ni'm a beginner in datascience , so feel free to correct me .","7c1979fc":"when the ST segement depression is increasing , chance of not getting heart disease is dominating . ","d16bdf5f":"#### Creating our validation set","a8ee7532":"Pickking the continuous numerical attributes for univariate plots .","8f13e993":"#### Checking the dimension , dtypes , descriptive info and the balance of the dataset ","b92ddfba":"1. SVM","ea90bf12":"#### Finalising Model and presenting Results ","e6e42533":"Seems like , there is good correlation contributed by chest pain , resting EC results , max heart rate , slope  .","060ad101":"ST depression looks like a log distribution , age and heart rate has negative skew . where as BP and cholestrol apparently looks gaussian ","e00de216":"#### Peak at the data ","b9f8d4c3":"SVM is promising ","80a826d7":"from the plot , we can analyse that heart disease  chance is really favourable to happen for ages lesser than 54 , where as lesser occurance post 54 in a general sense . \n","8ecfbc38":"Following dataset has Only numeric attributes and of different scales ","dbdc29a3":"Relatively low scores from KNN","e6009dde":"Seems like Random Forest is badly overfittting , lets solve that independently","c6e8b4af":"Observing the spot check stage , we could pick - SVM , KNN , Logistic Regression , and finally Random Forest , because of its robust performance upon tuning . This spot check was performed on training set to filter out best techniques .","06da5810":"1. When the resting blood pressure is lower , occurence of heart disease is quite leading and goes dominating even though resting bp increases .\n\n2. People having higher maximum heart rate got to have high chance of getting heart disease irrespective male or female .","6f816dc6":"2.  lOGISTIC REGRESSION","07f7fec6":"Ratio of females catching heart complaint to that of males is pretty much high . ","1d353d94":"#### Improving our results ","9e545667":"Our dataset has few datapoints . Most of the attributes are Numeric and its a slightly imbalanced dataset .","eee8bc8a":"After waiting over 20 minutes of Grid Search , we gonna use this estimations for our randomforest  ","a897dcb4":"Starting with the descriptive statistics approach ","c7ae4e32":"Much better :-)","5543452b":"Observing the results from grid search , we could say our algorithms could use these best estimated parameters for better results","7b099039":"#### Template for spot check algorithm ","5765ef92":"#####checking the nullity and skewness with these attributes ","928e9acc":"4. Random Forest","9c71dd45":"Looks like our Logistic regression is overfitting . Try to avoid such algorithm","c3889198":"age range of females who catch heart complaint is pretty much wide compared to males , which is not so good for female category ","d1eb4684":"Accessing the dataset ","baaf879c":"People having high sugar level has equal chance of getting and not getting a heart disease , where as for those who have low sugar , chance of getting heart disease is a bit high . so keep your sugar level above a threshold .","5d90fffe":"#### Lets pick age and analyse the data","fbc77174":"There is no missing values . Most of the skewness are nearer to zero except few .","4cf5b9ee":"#### Visualization aprroach"}}