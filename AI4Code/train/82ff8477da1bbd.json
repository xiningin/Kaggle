{"cell_type":{"4102655e":"code","9455c3ef":"code","41a04189":"code","5eb344e9":"code","b070ebff":"code","2f501155":"code","952fdfec":"code","ab014343":"code","b556762c":"code","e0de6893":"code","f75b4091":"code","a3b99b8f":"code","70952234":"code","06cf0544":"code","05d0d5b7":"code","368201c0":"code","e70ef2c3":"code","93ac77c9":"code","58ec11a2":"markdown","3b6bf419":"markdown","b73a57fe":"markdown","9aa4761a":"markdown","1c67ecc3":"markdown","1261c163":"markdown","d5aba84f":"markdown","22d870f4":"markdown","b598e56d":"markdown","d6bed8e0":"markdown","b2c7a7a6":"markdown","2e0b98c2":"markdown","f0864ab9":"markdown","6e44c8cf":"markdown","4f62e298":"markdown","8b21de79":"markdown","103f86fe":"markdown","16da22bc":"markdown","00e2a151":"markdown","86302b53":"markdown","39bc250a":"markdown","0b4155bc":"markdown","7a240cd9":"markdown","c7dd6150":"markdown","7322cce8":"markdown","54368145":"markdown","90ea942a":"markdown","4080a29b":"markdown","7f93f539":"markdown","450d68cf":"markdown"},"source":{"4102655e":"from cycler import cycler\n\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\n\nraw_dark_palette = [\n    (10, 132, 255), # Blue\n    (255, 159, 10), # Orange\n    (48, 209, 88),  # Green\n    (255, 69, 58),  # Red\n    (191, 90, 242), # Purple\n    (94, 92, 230),  # Indigo\n    (255, 55, 95),  # Pink\n    (100, 210, 255),# Teal\n    (255, 214, 10)  # Yellow\n]\n\nraw_gray_light_palette = [\n    (142, 142, 147),# Gray\n    (174, 174, 178),# Gray (2)\n    (199, 199, 204),# Gray (3)\n    (209, 209, 214),# Gray (4)\n    (229, 229, 234),# Gray (5)\n    (242, 242, 247),# Gray (6)\n]\n\nraw_gray_dark_palette = [\n    (142, 142, 147),# Gray\n    (99, 99, 102),  # Gray (2)\n    (72, 72, 74),   # Gray (3)\n    (58, 58, 60),   # Gray (4)\n    (44, 44, 46),   # Gray (5)\n    (28, 28, 39),   # Gray (6)\n]\n\n\nlight_palette = np.array(raw_light_palette)\/255\ndark_palette = np.array(raw_dark_palette)\/255\ngray_light_palette = np.array(raw_gray_light_palette)\/255\ngray_dark_palette = np.array(raw_gray_dark_palette)\/255\n\nmpl.rcParams['axes.prop_cycle'] = cycler('color',dark_palette)\nmpl.rcParams['figure.facecolor']  = gray_dark_palette[-2]\nmpl.rcParams['figure.edgecolor']  = gray_dark_palette[-2]\nmpl.rcParams['axes.facecolor'] =  gray_dark_palette[-2]\n\nwhite_color = gray_light_palette[-2]\nmpl.rcParams['text.color'] = white_color\nmpl.rcParams['axes.labelcolor'] = white_color\nmpl.rcParams['axes.edgecolor'] = white_color\nmpl.rcParams['xtick.color'] = white_color\nmpl.rcParams['ytick.color'] = white_color\n\nmpl.rcParams['figure.dpi'] = 200\n\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","9455c3ef":"sns.palplot(dark_palette)","41a04189":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as pltreducer  \nimport seaborn as sns\nfrom umap ","5eb344e9":"train= pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntrain = train.drop('id', axis=1)\ntrain[:2]","b070ebff":"train_sub = train.sample(10000, random_state=0)\n","2f501155":"# Import label encoder \nfrom sklearn import preprocessing \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n# Encode labels in column 'Species'. \ntrain_sub['target']= label_encoder.fit_transform(train_sub['target']) \ntrain_sub['target'].unique()\ntarget = train_sub['target']\n","952fdfec":"dr = umap.fit_transform(train_sub.iloc[:,:-1], target)","ab014343":"fig = plt.figure(figsize=(12, 12))\ngs = fig.add_gridspec(5, 4)\nax = fig.add_subplot(gs[:-1,:])\n\nsub_axes = [None] * 4\nfor idx in range(4): \n    sub_axes[idx] = fig.add_subplot(gs[-1,idx])\n\nfor idx in range(4):\n    ax.scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],\n              s=10, alpha=0.2\n              )\n\n    for j in range(4):\n        sub_axes[j].scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],\n                              s=10, alpha = 0.4 if idx==j else 0.008, color = (dark_palette[j%9]) if idx==j else white_color,\n                            zorder=(idx==j)\n                           )\n        \n    \n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    sub_axes[idx].set_xlabel('')\n    sub_axes[idx].set_ylabel('')\n    sub_axes[idx].set_title(f'Class_{idx+1}')\n    sub_axes[idx].spines['right'].set_visible(True)\n    sub_axes[idx].spines['top'].set_visible(True)\n\nax.set_title('Dimenstion Reduction (UMAP) of Original Test Set', fontweight='bold', fontfamily='serif', fontsize=20, loc='left')   \n    \nax.set_xticks([])\nax.set_yticks([])\nax.set_xlabel('')\nax.set_ylabel('')\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\nfig.tight_layout()\nplt.show()","b556762c":"train_res= pd.read_csv('..\/input\/resampled-traincsv\/resampled_train.csv')\ntrain_res.shape\n","e0de6893":" train_res_sub = train_res.sample(22998, random_state=0)\n","f75b4091":"# Import label encoder \nfrom sklearn import preprocessing \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n# Encode labels in column 'target'. \ntrain_res_sub['target']= label_encoder.fit_transform(train_res_sub['target']) \ntrain_res_sub['target'].unique()\ntarget = train_res_sub['target']\n","a3b99b8f":"dr_r = umap.fit_transform(train_res_sub.iloc[:,:-1], target)","70952234":"fig = plt.figure(figsize=(12, 12))\ngs = fig.add_gridspec(5, 4)\nax = fig.add_subplot(gs[:-1,:])\n\nsub_axes = [None] * 4\nfor idx in range(4): \n    sub_axes[idx] = fig.add_subplot(gs[-1,idx])\n\nfor idx in range(4):\n    ax.scatter(x=dr_r[:,0][target==idx], y=dr_r[:,1][target==idx],\n              s=10, alpha=0.2\n              )\n\n    for j in range(4):\n        sub_axes[j].scatter(x=dr_r[:,0][target==idx], y=dr_r[:,1][target==idx],\n                              s=10, alpha = 0.4 if idx==j else 0.008, color = (dark_palette[j%9]) if idx==j else white_color,\n                            zorder=(idx==j)\n                           )\n        \n    \n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    sub_axes[idx].set_xlabel('')\n    sub_axes[idx].set_ylabel('')\n    sub_axes[idx].set_title(f'Class_{idx+1}')\n    sub_axes[idx].spines['right'].set_visible(True)\n    sub_axes[idx].spines['top'].set_visible(True)\n\nax.set_title('Dimenstion Reduction (UMAP) of SMOTE Set', fontweight='bold', fontfamily='serif', fontsize=20, loc='left')   \n    \nax.set_xticks([])\nax.set_yticks([])\nax.set_xlabel('')\nax.set_ylabel('')\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\nfig.tight_layout()\nplt.show()","06cf0544":"train_smotek= pd.read_csv('..\/input\/smotetomek\/smotek_resampled_train(1).csv')\ntrain_res.shape","05d0d5b7":" train_smotek_sub = train_smotek.sample(22998, random_state=34)","368201c0":"# Import label encoder \nfrom sklearn import preprocessing \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n# Encode labels in column 'target'. \ntrain_smotek_sub['target']= label_encoder.fit_transform(train_smotek_sub['target']) \ntrain_smotek_sub['target'].unique()\ntarget = train_smotek_sub['target']\n","e70ef2c3":"dr_rs = umap.fit_transform(train_smotek_sub.iloc[:,:-1], target)","93ac77c9":"fig = plt.figure(figsize=(12, 12))\ngs = fig.add_gridspec(5, 4)\nax = fig.add_subplot(gs[:-1,:])\n\nsub_axes = [None] * 4\nfor idx in range(4): \n    sub_axes[idx] = fig.add_subplot(gs[-1,idx])\n\nfor idx in range(4):\n    ax.scatter(x=dr_rs[:,0][target==idx], y=dr_rs[:,1][target==idx],\n              s=10, alpha=0.2\n              )\n\n    for j in range(4):\n        sub_axes[j].scatter(x=dr_rs[:,0][target==idx], y=dr_rs[:,1][target==idx],\n                              s=10, alpha = 0.4 if idx==j else 0.008, color = (dark_palette[j%9]) if idx==j else white_color,\n                            zorder=(idx==j)\n                           )\n        \n    \n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    sub_axes[idx].set_xlabel('')\n    sub_axes[idx].set_ylabel('')\n    sub_axes[idx].set_title(f'Class_{idx+1}')\n    sub_axes[idx].spines['right'].set_visible(True)\n    sub_axes[idx].spines['top'].set_visible(True)\n\nax.set_title('Dimenstion Reduction (UMAP) of SMOTE-TOMEK resampled MAY TPS', fontweight='bold', fontfamily='serif', fontsize=20, loc='left')   \n    \nax.set_xticks([])\nax.set_yticks([])\nax.set_xlabel('')\nax.set_ylabel('')\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\nfig.tight_layout()\nplt.show()","58ec11a2":"FastAI Tabular NN after 100 epochs of Fit_one_cycle ","3b6bf419":"## Comparison of the UMAPS of the different train and test sets","b73a57fe":"This notebook has the results of my May Tabular Playground Series entries, the models, their details, and a summary of the findings. . \n\nI trained around 20-25 models, with slight variations and differences in some parameters and hyperparameters. \n\nThe core models I trained are\n\n1. FastAI Tabular Methods Random Forest (Good baseline)\n2. FastAI Tabular Neural Network (Pretty good results, surprisingly)\n3. LightAutoML (This gave me the best results, and was simple to use)\n4. Catboost\n5. Scikit-learn Voting Ensemble of Random Forest and Extra Trees (Pretty good)\n6. Scikit-learn Random Forest and Extra Trees single classifier ensemble\n\nThree feature engineering \/ Data preperation approaches\n\n1. Use the data as is -- this is what gave the best results, despite the class imbalance\n2. Use SMOTE alone (Overall worst results) and SMOTE-Tomek resampling (comparable to unchanged).\n3. Zero-removal by adding 1 to the whole dataset (like a constant) and then resampling (SMOTOMEK). I did this because one of the EDA notebooks pointed out how sparse the data was and that got me wondering if it mattered. Overall, I dont think it really did. This was computationally much heavier but didnt really give better results as far as I can tell.\n\nSo tried out each of those 6 models with these datasets but am posting only the most interesting findings. I dont really know how to plot things properly on this notebook, so the images are screenshots of the various experiments I ran and then ensembled in GIMP :). ","9aa4761a":"## Comparison of Public \/Partial Scores","1c67ecc3":"This set looks pretty weird.like a brain","1261c163":"\n![image.png](attachment:24199bf1-ed9a-4a9c-8ba2-3040126b0c11.png)","d5aba84f":"![image.png](attachment:2463e6cf-2a66-47f0-a271-a0e9f1774e12.png) ","22d870f4":"![image.png](attachment:62eb4bd9-4f28-48f6-92fd-eea877101805.png)\n","b598e56d":"![image.png](attachment:f5896c48-cdd8-4384-bba1-acbc65a60471.png)\n\nI tried the No-zeros and various permutions of smotek and smote with other models, but they all did much worse and so I am not including them in the comparison. These are at present the best models\n","d6bed8e0":"This looks a lot more like the original dataset","b2c7a7a6":"This, on the otherhand looks VERY different, but still continues to vote largely for class_2","2e0b98c2":"Feature importance of the Random Forest ensemble trained on resampled data\nSurprisingly like the LAMA results\n","f0864ab9":"## Performance","6e44c8cf":"Comparison of the predictions -- All the modes are largely just predicting category two.\n\nmodel code   \n`pred_lama_1.describe().style.background_gradient(axis=1,\n                 cmap=sns.light_palette('green', as_cmap=True))`","4f62e298":"I Loved this mapping which I found in ","8b21de79":" ![image.png](attachment:895ce17c-6959-498a-a13a-e3f616bab527.png)\n\n\n The confidence with which it is voting for class_2 has come down.\n","103f86fe":"`\n\n![image.png](attachment:7c845fa1-f3b6-4b85-a57e-0282bab8f622.png)","16da22bc":"Scikit-Learn Random forest n_estimators=500 on SMOTOMEK resampled data. \nThis looks like it's over fitting.","00e2a151":"Scikit-learn Voting Ensemble of Random Forest and Extra Trees on un-resampled data","86302b53":"![image.png](attachment:966d03a7-3f48-4483-a0ad-abb4e608bf7b.png)!\n![image.png](attachment:259d8b1f-2121-4f90-84dc-1a8fdc083a0a.png)\nDespite the fact that error, score, accuracy etc. just slightly over 50%, on submission this gave a pretty good score. I will wait to see if this stays so after the competition ends.This indicatest that this model generalizes better than others.","39bc250a":"Feature importance of the LightAutoML model unresampled data","0b4155bc":"Users and Notebooks that Helped me:\n\n1. @alexryzhkov LightAutoML Baseline TPS https:\/\/www.kaggle.com\/alexryzhkov\/lightautoml-baseline-tps-may-2021 plus i asked a bunch of questions on the notebook and he answered all of them. I loved using this library\n2. @remekkinas Sparsity-Shap EDA https:\/\/www.kaggle.com\/remekkinas\/tps5-is-about-sparsity-shap-extensive\n3. @subinium Categoriacal EDA https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda Disovering the UMAP plots on this notebook was eye opening! This gives me a new tool for my EDA\/visualization toolkit. \n","7a240cd9":"## Comparisong of Test Predictions of the various models","c7dd6150":"### SMOTE TOMEK","7322cce8":"## UMAP of the SMOTE resized training set","54368145":"![image.png](attachment:8c549eb5-415a-430c-9340-143e90e94a9a.png) ","90ea942a":"### UMAP of Original Train Set","4080a29b":"## Feature Importance Comparisons","7f93f539":"Here are some random row values from the predictions, there's a good amount of similarity between all the models. \n\n![image.png](attachment:457e7b07-8570-495d-9185-0f45c1409f39.png)","450d68cf":"![image.png](attachment:2516301e-c1b3-46be-a977-e6e56e27102a.png)"}}