{"cell_type":{"26637c5f":"code","818b569c":"code","c05e999d":"code","ad22610c":"code","a0525b8b":"code","321be070":"code","362a0169":"code","213ac495":"code","b0ff97be":"code","f2c6f2d6":"code","e59ebd21":"code","265b48bd":"code","7f23a47e":"code","ebf9ed4d":"code","ffcea7e4":"code","e7980b07":"code","99c97c45":"code","e44cde46":"code","f9dd2971":"code","ea4369c0":"markdown","8d315274":"markdown","037978e9":"markdown","5002a0bb":"markdown"},"source":{"26637c5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","818b569c":"from sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold , GridSearchCV\nfrom sklearn.metrics import roc_auc_score \nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c05e999d":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID' ,nrows = 200000)\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID' )\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n","ad22610c":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","a0525b8b":"%%time\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\n","321be070":"%%time\nY_train = train['isFraud'].copy()\nX_train = train.drop('isFraud' , axis = 1)\nX_test = test.copy()\ndel train_transaction, train_identity, test_transaction, test_identity\ndel train , test\ngc.collect()","362a0169":"print(X_train.shape)\nprint(X_test.shape)","213ac495":"for i in X_train.columns:\n    if X_train[i].dtype == 'object' or X_test[i].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[i].values) + list(X_test[i].values))\n        X_train[i] = lbl.transform(list(X_train[i].values))\n        X_test[i] = lbl.transform(list(X_test[i].values))","b0ff97be":"for i in X_train.columns:\n    if X_train[i].dtype == 'object' or X_test[i].dtype == 'object':\n        print(i)","f2c6f2d6":"%%time \nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","e59ebd21":"# null_data = pd.DataFrame(X_train.isnull().sum()\/X_train.shape[0]*100)\n# null_data = pd.DataFrame()\n# null_data = pd.concat([pd.DataFrame(X_train.isnull().sum()\/X_train.shape[0]*100 ,columns=['train']) ,pd.DataFrame(X_test.isnull().sum()\/X_test.shape[0]*100,columns=['test']) ] ,axis = 1).reset_index()\n# null_data.head()","265b48bd":"# columns_drop = null_data.sort_values(by = 'train'  , ascending = 0).head(100)['index'].values","7f23a47e":"train_corr = X_train.corr().abs()*100\ntrain_corr = train_corr.where(np.triu(np.ones(train_corr.shape)).astype(np.bool))\ntrain_corr.values[[np.arange(train_corr.shape[0])]*2] = np.nan\nprint(train_corr.shape)\n# display(train_corr.tail(20))\ncounter =0\ncolumns_drop =[]\ntrain_corr_matrix = train_corr.values\nfor i in range(1 , train_corr.shape[0] ,1 ):\n    for j in range(i , train_corr.shape[0] , 1):\n        if train_corr_matrix[i][j] >= 98:\n            counter+=1\n            columns_drop.append(train_corr.columns[j])\n            if counter%20 ==0:\n                print('Comman Columns pair reached ... ' , counter)\nprint(' Total Common Pair Found .... ',counter)","ebf9ed4d":"# columns_drop = list(set(columns_drop))\n# X_train.drop(columns = columns_drop , inplace = True)\n# X_test.drop(columns = columns_drop , inplace = True)","ffcea7e4":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","e7980b07":"Y_train.value_counts()","99c97c45":"EPOCHS = 5\ny_pred = np.zeros(sample_submission.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nkf = KFold(n_splits = EPOCHS , shuffle = True)\nfor x_train_index , x_val_index in kf.split(X_train , Y_train):\n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=4,\n        learning_rate=0.005,\n        subsample=0.2,\n        colsample_bytree = 0.2\n    )\n    x_tr , x_val = X_train.iloc[x_train_index , :] , X_train.iloc[x_val_index,:]\n    y_tr , y_val = Y_train.iloc[x_train_index] , Y_train.iloc[x_val_index]\n    clf.fit(x_tr,y_tr)\n    y_pred_train = clf.predict_proba(x_val)[:,1]\n    y_oof[x_val_index] = y_pred_train\n    print('ROC AUC {}'.format(roc_auc_score(y_val, y_pred_train)))\n    y_pred+= clf.predict_proba(X_test)[:,1] \/ EPOCHS\n\n ","e44cde46":"col = X_train.columns\nimportances = clf.feature_importances_\ndataframe = pd.DataFrame({'col':col , 'importance':importances})\ndataframe = dataframe.sort_values(by=['importance'] ,ascending = False)\ndataframe['importance_ratio'] = dataframe['importance']\/dataframe['importance'].max()*100\ndataframe = dataframe.head(35)\ndataframe['col'] = dataframe['col'].apply(lambda x:  str(x))\nplt.figure(figsize=(18,12))\nplt.barh(dataframe['col'], dataframe['importance_ratio'], color='orange' , align='center' ,linewidth =30 )\nplt.yticks(rotation=30)\nplt.show()","f9dd2971":"sample_submission['isFraud'] = y_pred\nsample_submission.to_csv('second_simple_xgboost_Fraud_Baseline.csv')","ea4369c0":"* So Bascially 93.9 % of data final test set is not fraud ","8d315274":"Best score: 0.97750<br>\nBest parameters set:\n* \tlearning_rate: 0.005<br>\n* \tmax_depth: 4<br>\n* \tn_estimators: 500<br>\n* \tsubsample: 0.2<br>","037978e9":"%%time\nparameters = {'n_estimators': [500], \n              'max_depth':[4] ,\n              'learning_rate' :[ 0.005 , 0.01] , \n              'subsample' : [0.2 , 0.3],\n              'colsample_bytree' : [0.2 , 0.5 , 0.9]\n             }\nclf = xgb.XGBClassifier()\ngrid_search = GridSearchCV(estimator=clf, param_grid=parameters, cv=3, n_jobs=-1 , verbose = 6)\ngrid_search.fit(X_train , Y_train)\nprint(\"Best score: %0.5f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters=grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","5002a0bb":"* ROC AUC 0.7886114967821933\n* ROC AUC 0.8359518198481278\n* ROC AUC 0.8312436224489796\n* ROC AUC 0.8269948717948717\n* ROC AUC 0.8615600904653375\n* Result for each new initialisation \n* ROC AUC 0.7696746809314766\n* ROC AUC 0.8463722611468055\n* ROC AUC 0.8169901274903227\n* ROC AUC 0.871218112244898\n* ROC AUC 0.8578023148656989\n* for one model and training it on new data on earlier data\n"}}