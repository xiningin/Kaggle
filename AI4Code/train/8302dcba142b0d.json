{"cell_type":{"5d59f319":"code","5ab3013b":"code","008d49eb":"code","58eb6abe":"code","dd155822":"code","cf9ebe80":"code","a5d2488c":"code","4d975cf4":"code","deffbede":"code","7ece91ac":"code","44c00eb6":"code","25c83935":"code","131e2d76":"code","4581f7a9":"code","db6864cf":"code","671e011d":"code","e954da5f":"code","f60adae6":"code","2913a395":"code","025f73e3":"code","0633733c":"code","45c8c493":"code","c973ca81":"code","e7a10225":"code","f5467ac7":"code","7d74b890":"code","18bd7283":"code","8d5ed8c4":"code","4afa6d86":"code","ff946d15":"code","26bfee2c":"code","4d37a14a":"code","7b20d886":"code","d4d850a7":"code","5aa0a523":"code","13ce6fac":"code","3dd9d3c4":"code","8f9e3c71":"code","328a735b":"code","0f045c27":"code","1a47883c":"code","6e0ff1f0":"code","6c340cac":"code","fabfdab8":"code","07c59a36":"code","9957ff9a":"code","ac40eee1":"code","02eea38e":"code","14ae6b81":"code","8530d5ef":"code","4085a614":"code","f129e129":"code","848eebf7":"code","908155d4":"code","590ecfdf":"code","e153cdc6":"code","0ca2c9fb":"code","fca9fe31":"code","5be01554":"markdown","45e50fee":"markdown","e56f38eb":"markdown","672b36d0":"markdown","2ffc86dc":"markdown","a28b81a6":"markdown","83506cda":"markdown","ee506567":"markdown","d2277ca6":"markdown","f66128e8":"markdown","2f7b4326":"markdown","9d31c153":"markdown","7fd44e72":"markdown","5585e1c6":"markdown","4d40e987":"markdown","647227b2":"markdown","0d57649f":"markdown","006abb9e":"markdown","c4539bd9":"markdown","087ce6f8":"markdown","9c7195e4":"markdown","0ccdbe86":"markdown","3f1687de":"markdown","c1b9049c":"markdown","0565c8e4":"markdown","af0408ed":"markdown","dda3aabd":"markdown","b548f7cd":"markdown","34a16f02":"markdown","d7271501":"markdown"},"source":{"5d59f319":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import rcParams\nfrom IPython.display import display\n\nrcParams['figure.figsize'] = 15, 7\n\ntrain = pd.read_csv(r'..\/input\/train.csv', parse_dates=['date'])\ntest = pd.read_csv(r'..\/input\/test.csv', parse_dates=['date'])\n","5ab3013b":"print('Training set head:')\ndisplay(train.head())\nprint('Test set head:')\ndisplay(test.head())\nn_stores = train['store'].nunique()\nprint('# of stores: {:,}'.format(n_stores))\nn_items = train['item'].nunique()\nprint('# of items: {:,}'.format(n_items))\nn_samples = len(train)\/(n_stores*n_items)\nprint('# of training samples per item and store:')\ndisplay(train.groupby(['store','item'])[['date']].nunique().describe().style.format('{:,.0f}'))\nprint('# of test samples per item and store:')\ndisplay(test.groupby(['store','item'])[['date']].nunique().describe().style.format('{:,.0f}'))\nprint('First and last date of training set:')\ndisplay(train.drop_duplicates(subset=['date'])[['date']].describe().loc[['first','last']].applymap(lambda x: x.date()))\nprint('First and last date of test set:')\ndisplay(test.drop_duplicates(subset=['date'])[['date']].describe().loc[['first','last']].applymap(lambda x: x.date()))","008d49eb":"def expand(df,date_col):\n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    df['day'] = df[date_col].dt.day\n    df['dayofweek'] = df[date_col].dt.dayofweek","58eb6abe":"expand(train,'date')\nexpand(test,'date')","dd155822":"print('Training set details by year and month:')\ndisplay(train.drop_duplicates(subset=['date']).groupby(['year','month'])[['day']].count().unstack(1))","cf9ebe80":"print('Test set details by year and month:')\ndisplay(test.drop_duplicates(subset=['date']).groupby(['year','month'])[['day']].count().unstack(1))","a5d2488c":"item_number = 1\nstore_number = 1\nfig = plt.figure(figsize=(15,7))\nax = fig.gca()\ntrain.loc[(train['item'] == item_number) & (train['store'] == store_number)].plot(x='date',y='sales',ax=ax)\nax.legend().remove()\nax.set_xlabel('')\nax.set_title('Sales for item #{} and store #{}'.format(item_number,store_number))\nax.grid(axis='y',ls='--')","4d975cf4":"n_items = 4\nn_stores = 4\nmin_df = train.loc[(train['item'] < n_items) & (train['store'] < n_stores)].groupby(['item','store','year'])[['sales']].min().rename(columns = {'sales':'min sales'}).unstack(1)\nmax_df = train.loc[(train['item'] < n_items) & (train['store'] < n_stores)].groupby(['item','store','year'])[['sales']].max().rename(columns = {'sales':'max sales'}).unstack(1)\nmean_df = train.loc[(train['item'] < n_items) & (train['store'] < n_stores)].groupby(['item','store','year'])[['sales']].mean().rename(columns = {'sales':'mean sales'}).unstack(1)\nstd_df = train.loc[(train['item'] < n_items) & (train['store'] < n_stores)].groupby(['item','store','year'])[['sales']].std().rename(columns = {'sales':'stddev sales'}).unstack(1)\nprint('A few descriptive statistics by item and store:')\ndisplay(pd.concat([min_df,max_df,mean_df,std_df],axis=1).style.format('{:,.0f}'))","deffbede":"def smape(y_true, y_pred):\n    return 200.0*np.mean((np.abs(y_pred - y_true) \/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))","7ece91ac":"model_train_smape_dict = {}\nmodel_test_smape_dict = {}","44c00eb6":"most_basic_model = train.groupby(['store','item'])[['sales']].mean().reset_index()\nmost_basic_model['sales'] = np.round(most_basic_model['sales']).astype(int)\ndisplay(most_basic_model.head())\ncross_val = pd.merge(train,most_basic_model.rename(columns={'sales':'salespred'}),on=['store','item'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Most Basic Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))\n\nsub = pd.merge(test,most_basic_model,on=['store','item'],how='left')\nsub = sub[['id','sales']].copy()\nsub.to_csv('submission-most_basic_model.csv',index=False)\nmodel_test_smape_dict['Most Basic Model'] = 28.34071 #given by kaggle","25c83935":"item_number = 1\nstore_number = 1\nfig = plt.figure(figsize=(15,7))\nax = fig.gca()\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='sales',ax=ax)\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='salespred',ax=ax,c='r',ls='--')\nax.legend().remove()\nax.set_xlabel('')\nax.set_title('Sales and Predicted Sales for item #{} and store #{}'.format(item_number,store_number))\nax.grid(axis='y',ls='--')","131e2d76":"cross_val['error'] = cross_val['sales'] - cross_val['salespred']\ncross_val['abserror'] = np.abs(cross_val['error'])\n\nsma_window = 90\ncross_val['error_sma'] = cross_val['error'].rolling(sma_window).mean()\ncross_val['abserror_sma'] = cross_val['abserror'].rolling(sma_window).mean()\n\nfig = plt.figure(figsize=(15,7))\nax = fig.gca()\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='error',ax=ax)\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='error_sma',ax=ax,c='r',ls='--')\nax.legend().remove()\nax.set_xlabel('')\nax.set_title('Sales prediction error for item #{} and store #{}'.format(item_number,store_number))\nax.grid(axis='y',ls='--')","4581f7a9":"fig = plt.figure(figsize=(15,7))\nax = fig.gca()\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='abserror',ax=ax)\ncross_val.loc[(cross_val['item'] == item_number) & (cross_val['store'] == store_number)].plot(x='date',y='abserror_sma',ax=ax,c='r',ls='--')\nax.legend().remove()\nax.set_xlabel('')\nax.set_title('Sales prediction abs error for item #{} and store #{}'.format(item_number,store_number))\nax.grid(axis='y',ls='--')","db6864cf":"year_basic_model = train.groupby(['store','item','year'])[['sales']].mean().reset_index()\nyear_basic_model['sales'] = np.round(year_basic_model['sales']).astype(int)\ndisplay(year_basic_model.head())","671e011d":"cross_val = pd.merge(train,year_basic_model.rename(columns={'sales':'salespred'}),on=['store','item','year'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Year Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","e954da5f":"year_month_model = train.groupby(['store','item','year','month'])[['sales']].mean().reset_index()\nyear_month_model['sales'] = np.round(year_month_model['sales']).astype(int)\ndisplay(year_month_model.head())","f60adae6":"cross_val = pd.merge(train,year_month_model.rename(columns={'sales':'salespred'}),on=['store','item','year','month'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Year Month Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","2913a395":"year_month_dow_model = train.groupby(['store','item','year','month','dayofweek'])[['sales']].mean().reset_index()\nyear_month_dow_model['sales'] = np.round(year_month_dow_model['sales']).astype(int)\ndisplay(year_month_dow_model.head())\n\ncross_val = pd.merge(train,year_month_dow_model.rename(columns={'sales':'salespred'}),on=['store','item','year','month','dayofweek'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Year Month Dow Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","025f73e3":"month_model = train.groupby(['store','item','month'])[['sales']].mean().reset_index()\nmonth_model['sales'] = np.round(month_model['sales']).astype(int)\ndisplay(month_model.head())","0633733c":"cross_val = pd.merge(train,month_model.rename(columns={'sales':'salespred'}),on=['store','item','month'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Month Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","45c8c493":"sub = pd.merge(test,month_model,on=['store','item','month'],how='left')\nsub = sub[['id','sales']].copy()\nsub.to_csv('submission-month_model.csv',index=False)\nmodel_test_smape_dict['Month Model'] = 20.25930 #given by kaggle ","c973ca81":"month_dow_model = train.groupby(['store','item','month','dayofweek'])[['sales']].mean().reset_index()\nmonth_dow_model['sales'] = np.round(month_dow_model['sales']).astype(int)\ndisplay(month_dow_model.head())","e7a10225":"cross_val = pd.merge(train,month_dow_model.rename(columns={'sales':'salespred'}),on=['store','item','month','dayofweek'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Month Dow Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","f5467ac7":"sub = pd.merge(test,month_dow_model,on=['store','item','month','dayofweek'],how='left')\nsub = sub[['id','sales']].copy()\nsub.to_csv('submission-month_dow_model.csv',index=False)\nmodel_test_smape_dict['Month Dow Model'] = 18.95844 #given by kaggle","7d74b890":"month_day_model = train.groupby(['store','item','month','day'])[['sales']].mean().reset_index()\nmonth_day_model['sales'] = np.round(month_day_model['sales']).astype(int)\ndisplay(month_day_model.head())\n\ncross_val = pd.merge(train,month_day_model.rename(columns={'sales':'salespred'}),on=['store','item','month','day'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Month Day Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))\n\nsub = pd.merge(test,month_day_model,on=['store','item','month','day'],how='left')\nsub = sub[['id','sales']].copy()\n\nsub.to_csv('submission-month_day_model.csv',index=False)\nmodel_test_smape_dict['Month Day Model'] = 22.13108 #given by kaggle","18bd7283":"models_list = ['Most Basic Model',\n               'Year Model','Year Month Model','Year Month Dow Model',\n               'Month Model','Month Dow Model','Month Day Model']\n\ndisplay(pd.merge(\n    pd.DataFrame(index=model_train_smape_dict.keys(),data=list(model_train_smape_dict.values()),columns=['SMAPE train']),\n    pd.DataFrame(index=model_test_smape_dict.keys(),data=list(model_test_smape_dict.values()),columns=['SMAPE test']),\n    right_index = True,left_index=True,how='outer').loc[models_list].style.format('{:,.1f}'))\n    ","8d5ed8c4":"sales_year = train.groupby(['store','year'])[['sales']].mean()\nsales_year = sales_year.unstack(0).pct_change().dropna()\ndisplay(sales_year.head())","4afa6d86":"sales_year.plot(kind='bar')\nplt.legend(loc='upper left',bbox_to_anchor=[1.0,0.5]);\nplt.grid(axis='y',ls='--')\nplt.title('Sales YoY growth per store');","ff946d15":"sales_growth = sales_year.mean(1)\n\np1 = np.poly1d(np.polyfit(sales_growth.index, sales_growth.values, 1))\np2 = np.poly1d(np.polyfit(sales_growth.index, sales_growth.values, 2))\np3 = np.poly1d(np.polyfit(sales_growth.index, sales_growth.values, 3))\n\nsales_growth = pd.DataFrame(np.c_[sales_growth.values,p1(sales_growth.index),p2(sales_growth.index),p3(sales_growth.index)],index=sales_growth.index,columns=['Actual','Linear','Quadratic','Cubic'])\ndisplay(sales_growth.head())\n\ndisplay(pd.DataFrame(np.abs((sales_growth.subtract(sales_growth['Actual'],axis='index')).drop('Actual',axis=1)).mean()).rename(columns={0:'MAE'}))","26bfee2c":"#The cubic fit is of course accurate (4 points to fit) but we can see the overfitting:\nyear_to_forecast = 2018\nfor i,p in enumerate([p1,p2,p3]):\n    print('D\u00b0 {} prediction of the YoY growth %: {:,.3f}'.format(i+1,p(2018)))","4d37a14a":"sales_year = train.groupby(['store','year'])[['sales']].sum()\nsales_year = sales_year.groupby(['store'])[['sales']].transform(lambda x: x\/x.sum())\ndisplay(sales_year.head())\n\nsales_year.unstack(0).plot(kind='bar')\nplt.legend(loc='right',bbox_to_anchor=[1.4,0.5])\nplt.title('Sales per year per store as a % of total store sales over the period');","7b20d886":"sales_year = train.groupby('year')[['sales']].mean() \/ train.groupby('year')[['sales']].mean().mean()\ndisplay(sales_year)","d4d850a7":"x = sales_year.index\ny = sales_year.values.ravel()\n\np1 = np.poly1d(np.polyfit(x, y, 1))\np2 = np.poly1d(np.polyfit(x, y, 2))\n\nplt.scatter(x,y,c='k')\nplt.plot(x,p1(x),c='b')\nplt.plot(x,p2(x),c='orange')\nplt.grid(axis='y',ls='--')\nplt.title('Linear and Quadratic Fit of Sales per Year');","5aa0a523":"# If we choose the quadratic fit:\nadj_factor = p2(2018)\nprint('Adjustment factor: {:,.4f}'.format(p2(2018)))","13ce6fac":"sales_year_alt = train['year'].nunique() * train.groupby('year')[['sales']].sum() \/ train['sales'].sum()\ndisplay(pd.concat([sales_year.rename(columns = {'sales' : 'sales per yer per store averaged'}),\n                   sales_year_alt.rename(columns = {'sales' : 'sales per year averaged'})],\n                  axis = 1))","3dd9d3c4":"x = sales_year_alt.index\ny = sales_year_alt.values.ravel()\n\np1_alt = np.poly1d(np.polyfit(x, y, 1))\np2_alt = np.poly1d(np.polyfit(x, y, 2))\n\nplt.scatter(x,y,c='k')\nplt.plot(x,p1_alt(x),c='b')\nplt.plot(x,p2_alt(x),c='orange')\nplt.grid(axis='y',ls='--')\nplt.title('Linear and Quadratic Fit of Sales per Year');","8f9e3c71":"# If we choose the quadratic fit:\nadj_factor_alt = p2_alt(2018)\nprint('Adjustment factor alt: {:,.4f}'.format(p2_alt(2018)))","328a735b":"year_basic_model = train.groupby(['store','item','year'])[['sales']].mean().reset_index()\nyear_basic_model['sales'] = np.round(year_basic_model['sales']).astype(int)\n\nyear_basic_model = pd.merge(year_basic_model,\n                            train.groupby(['store','item'])[['sales']].mean().reset_index().rename(columns={'sales' : 'mean_sales'}),\n                            on = ['store','item'])\n\nadj_factor_model = pd.DataFrame(index=np.arange(2013,2018),data=[p2(x) for x in np.arange(2013,2018)],columns=['sales_adj_factor'])\nadj_factor_model = adj_factor_model.reset_index().rename(columns={'index' : 'year'})\n\nyear_basic_model = pd.merge(year_basic_model,adj_factor_model,on='year',how='left')\nyear_basic_model['salespred'] = np.round(year_basic_model['mean_sales'] * year_basic_model['sales_adj_factor']).astype(int)\n\ndisplay(year_basic_model.head())","0f045c27":"model_smape = smape(year_basic_model['sales'],year_basic_model['salespred'])\nprint('SMAPE between Year Model & Year Model Completed set is {:,.4f}'.format(model_smape))","1a47883c":"cross_val = pd.merge(train,year_basic_model[['store','item','year','salespred']],on=['store','item','year'],how='left')\nmodel_smape = smape(cross_val['sales'],cross_val['salespred'])\nmodel_train_smape_dict['Year Model Completed'] = model_smape\nprint('SMAPE on train set of the Year Model Completed is {:,.4f}'.format(model_smape))\nprint('SMAPE on train set of the Year Model was {:,.4f}'.format(model_train_smape_dict['Year Model']))","6e0ff1f0":"year_basic_model_aux = train.groupby(['store','item'])[['sales']].mean().reset_index()\nyear_basic_model_aux['year'] = 2018\nyear_basic_model_aux.rename(columns={'sales' : 'mean_sales'},inplace=True)\nyear_basic_model_aux['sales'] = np.nan \nyear_basic_model_aux['sales_adj_factor'] = adj_factor\nyear_basic_model_aux['salespred'] = np.round(year_basic_model_aux['mean_sales'] * year_basic_model_aux['sales_adj_factor']).astype(int)\nyear_basic_model_aux = year_basic_model_aux[year_basic_model.columns]\ndisplay(year_basic_model_aux.head())","6c340cac":"year_basic_model = pd.concat([year_basic_model, year_basic_model_aux],axis=0)\n\nsub = pd.merge(test,year_basic_model.loc[year_basic_model['year'] == 2018],on=['store','item'],how='left')\nsub = sub[['id','salespred']].copy()\nsub.rename(columns = {'salespred' : 'sales'}, inplace=True)\nsub.to_csv('submission-year_basic_model.csv',index=False)\nmodel_test_smape_dict['Year Model'] = 40.30474 #given by kaggle","fabfdab8":"year_month_model = pd.merge(year_month_model,\n                            train.groupby(['store','item','month'])[['sales']].mean().reset_index().rename(columns={'sales' : 'mean_sales'}),\n                            on = ['store','item','month'])\n\nyear_month_model = pd.merge(year_month_model,adj_factor_model,on='year',how='left')\nyear_month_model['salespred'] = np.round(year_month_model['mean_sales'] * year_month_model['sales_adj_factor']).astype(int)\n\ndisplay(year_month_model.head())","07c59a36":"year_month_model_aux = train.groupby(['store','item','month'])[['sales']].mean().reset_index()\nyear_month_model_aux['year'] = 2018\nyear_month_model_aux.rename(columns={'sales' : 'mean_sales'},inplace=True)\nyear_month_model_aux['sales'] = np.nan \nyear_month_model_aux['sales_adj_factor'] = adj_factor\nyear_month_model_aux['salespred'] = np.round(year_month_model_aux['mean_sales'] * year_month_model_aux['sales_adj_factor']).astype(int)\nyear_month_model_aux = year_month_model_aux[year_month_model.columns]\ndisplay(year_month_model_aux.head())","9957ff9a":"year_month_model = pd.concat([year_month_model, year_month_model_aux],axis=0)\n\nsub = pd.merge(test,year_month_model.loc[year_month_model['year'] == 2018],on=['store','item','month'],how='left')\nsub = sub[['id','salespred']].copy()\nsub.rename(columns = {'salespred' : 'sales'}, inplace=True)\nsub.to_csv('submission-year_month_model.csv',index=False)\nmodel_test_smape_dict['Year Month Model'] = 17.43060 #given by kaggle","ac40eee1":"year_month_dow_model = pd.merge(year_month_dow_model,\n                            train.groupby(['store','item','month','dayofweek'])[['sales']].mean().reset_index().rename(columns={'sales' : 'mean_sales'}),\n                            on = ['store','item','month','dayofweek'])","02eea38e":"year_month_dow_model = pd.merge(year_month_dow_model,adj_factor_model,on='year',how='left')\nyear_month_dow_model['salespred'] = np.round(year_month_dow_model['mean_sales'] * year_month_dow_model['sales_adj_factor']).astype(int)\n\ndisplay(year_month_dow_model.head())","14ae6b81":"year_month_dow_model_aux = train.groupby(['store','item','month','dayofweek'])[['sales']].mean().reset_index()\nyear_month_dow_model_aux['year'] = 2018\nyear_month_dow_model_aux.rename(columns={'sales' : 'mean_sales'},inplace=True)\nyear_month_dow_model_aux['sales'] = np.nan \nyear_month_dow_model_aux['sales_adj_factor'] = adj_factor\nyear_month_dow_model_aux['salespred'] = np.round(year_month_dow_model_aux['mean_sales'] * year_month_dow_model_aux['sales_adj_factor']).astype(int)\nyear_month_dow_model_aux = year_month_dow_model_aux[year_month_dow_model.columns]\ndisplay(year_month_dow_model_aux.head())","8530d5ef":"year_month_dow_model = pd.concat([year_month_dow_model, year_month_dow_model_aux],axis=0)","4085a614":"sub = pd.merge(test,year_month_dow_model.loc[year_month_dow_model['year'] == 2018],on=['store','item','month','dayofweek'],how='left')\nsub = sub[['id','salespred']].copy()\nsub.rename(columns = {'salespred' : 'sales'}, inplace=True)\nsub.to_csv('submission-year_month_dow_model.csv',index=False)\nmodel_test_smape_dict['Year Month Dow Model'] = 14.25263 #given by kaggle","f129e129":"models_list = ['Most Basic Model',\n               'Year Model','Year Month Model','Year Month Dow Model',\n               'Month Model','Month Dow Model','Month Day Model']\n\ndisplay(pd.merge(\n    pd.DataFrame(index=model_train_smape_dict.keys(),data=list(model_train_smape_dict.values()),columns=['SMAPE train']),\n    pd.DataFrame(index=model_test_smape_dict.keys(),data=list(model_test_smape_dict.values()),columns=['SMAPE test']),\n    right_index = True,left_index=True,how='outer').loc[models_list].style.format('{:,.1f}'))","848eebf7":"store_item_model = train.groupby(['store','item'])[['sales']].mean().reset_index()\n\nmonth_model = train.groupby(['month'])[['sales']].mean().reset_index() \nmonth_model['sales'] = month_model['sales'] \/ train['sales'].mean()\n\ndayofweek_model = train.groupby(['dayofweek'])[['sales']].mean().reset_index() \ndayofweek_model['sales'] = dayofweek_model['sales'] \/ train['sales'].mean()","908155d4":"kiss_model = pd.merge(train,store_item_model.rename(columns={'sales':'storeitem_sales'}),on=['store','item'],how='left')\nkiss_model = pd.merge(kiss_model,month_model.rename(columns={'sales':'monthfactor'}),on=['month'],how='left')\nkiss_model = pd.merge(kiss_model,dayofweek_model.rename(columns={'sales':'dayofweekfactor'}),on=['dayofweek'],how='left')\nkiss_model = pd.merge(kiss_model,adj_factor_model,on='year',how='left')\n\nkiss_model['salespred'] = kiss_model['storeitem_sales']*kiss_model['sales_adj_factor']*kiss_model['monthfactor']*kiss_model['dayofweekfactor']\nkiss_model['salespred'] = np.round(kiss_model['salespred']).astype(int)\ndisplay(kiss_model.head())","590ecfdf":"model_smape = smape(kiss_model['sales'],kiss_model['salespred'])\nmodel_train_smape_dict['Kiss Model'] = model_smape\nprint('SMAPE on train set is {:,.4f}'.format(model_smape))","e153cdc6":"kiss_model_aux = pd.merge(test,store_item_model,on=['store','item'],how='left')\nkiss_model_aux = pd.merge(kiss_model_aux,month_model.rename(columns={'sales':'monthfactor'}),on=['month'],how='left')\nkiss_model_aux = pd.merge(kiss_model_aux,dayofweek_model.rename(columns={'sales':'dayofweekfactor'}),on=['dayofweek'],how='left')\nkiss_model_aux['sales'] = kiss_model_aux['sales']*adj_factor*kiss_model_aux['monthfactor']*kiss_model_aux['dayofweekfactor']\nkiss_model_aux['sales'] = np.round(kiss_model_aux['sales']).astype(int)","0ca2c9fb":"sub = kiss_model_aux[['id','sales']].copy()\nsub.to_csv('submission-kiss.csv',index=False)\nmodel_test_smape_dict['Kiss Model'] = 13.87596 #given by kaggle ","fca9fe31":"models_list = ['Most Basic Model',\n               'Year Model','Year Month Model','Year Month Dow Model',\n               'Month Model','Month Dow Model','Month Day Model',\n              'Kiss Model']\n\ndisplay(pd.merge(\n    pd.DataFrame(index=model_train_smape_dict.keys(),data=list(model_train_smape_dict.values()),columns=['SMAPE train']),\n    pd.DataFrame(index=model_test_smape_dict.keys(),data=list(model_test_smape_dict.values()),columns=['SMAPE test']),\n    right_index = True,left_index=True,how='outer').loc[models_list].style.format('{:,.1f}'))","5be01554":"## Summary of the evaluated models so far","45e50fee":"### Month and Day of Week","e56f38eb":"## Summary of the evaluated models so far","672b36d0":"The dispersion of the above forecast is unfortunately not surprising given the very small # of data points (namely 4). In particular, this shows the danger of overfitting a degree three polynomial. Since forecasting the YoY growth, which is a derivative of the yearly sales, does not work - small sample and maybe too much noise, we can attempt to forecast the yearly sales themselves.\n\n> ## Forecasting the sales","2ffc86dc":"While the year-on-year (YoY) sales % growth seem to be approximately the same for all stores, there is no obvious trend to this growth unfortunately. We can attempt to fit a growth factor but there wouldn't be any intuition behind the 2018 resulting growth.","a28b81a6":"### Year, Month & Day of Week (DOW) Completed","83506cda":"The difference in sales forecast is not huge between the two forecasting methods but it does exist nonetheless.\n# Using this sales prediction to complete the year models","ee506567":"# Keeeping it simple but no simpler\n\nLet's start by revisiting the great kernel byt XYZT which shows that EDA and some thinking can go a long way without black-box algorithms:\nhttps:\/\/www.kaggle.com\/thexyzt\/keeping-it-simple-by-xyzt\n\nA few points to highlight in the below tour:\n1. We implement a simple SMAPE function to evaluate the quality of our predictions on the training set. No need to rely blindly on the upcoming submission score to get an idea whether our predictor has a decent chance to make the cut or not\n2. We start by an even more basic model than XYZT, predicting constant sales equal to the average of the item sales per store to get an idea of the variability of the data\n3. We show a few mistakes or glitches that we found easy to make when attempting to follow the spirit and not the letter of XYZT's kernel. Indeed for this competition's dataset, larger averages tend to perform better - for instance the \"month factor\" model is more precise if computed across all stores and items rather than for very store and item pair.","d2277ca6":"We now add the usual suspects fields when working with time series (year, month, day, dayofweek) and keep getting acquainted with the dataset","f66128e8":"We  need to check whether the item mix stays constant every year per store but if we dive directly into completing our year models:\n## Year Model Completed","2f7b4326":"## The Most Basic Model\nThe most basic model is to predict the average sales per item and per store","9d31c153":"If we illustrate graphically what this most basic model does:","7fd44e72":"So our adjustement to forecast sales growth makes the year model much worse than the most basic model. What is going on here?\n\n### Year and Month Completed","5585e1c6":"Coming up next, more efficient models to break the 13.9 barrier.","4d40e987":"We can look at the prediction error and absolute error:","647227b2":"## Summary of the evaluated models so far","0d57649f":"If we average across all stores:","006abb9e":"\n### Year and Month\n\nWe keep trying to refine our still basic model and check whether adding the month improves our results","c4539bd9":"> ## Forecasting the sales growth","087ce6f8":"We notice that the Year Month Dow Model SMAP test score is however 14.3 (14.25263 to be precise) and not the 13.87573 mentioned by Keeping it simple in his kernel.\n\n## Keeping it Simple (Kiss) Model\n\nTo get to the 13.87573 score, the devil is actually in the details: instead of computing the average for a store & item on a particular month and dayofweek and adjusting for the yearly sales growth, the monthly and dayofweek factors are computed by averaging over all stores and items. The broader averaging seems to be keeping the signal while filtering more noise.","9c7195e4":"We start by looking at the sales growth per store for each year before zooming in on the the sales per store and per item for each year.","0ccdbe86":"Let us note that it is meaningless to try the Year, Month & Day model since there would be no datapoint in the averaging window as the triplet Year, Month & Day always uniquely identify a single day\n\n## Month Models","3f1687de":"## Using the Day instead of the Day of the Week\nNow we come to XYZT's first model:\n\"Find the average of sales of an item at a store on the day and month of sales and use that as the prediction. This effectively gives us a sample size of 5 (since the training set is five years long) to find the mean. This is clearly a sub-optimal solution because almost no thought goes into it.\"\n\nPlease note that our implementation takes only a few seconds to run.","c1b9049c":"With hardly any technical machinery we were able to compute a predictor of SMAPE 19.0 \nLet's take this model (Month Dow Model)  and value (18.95844 to be precise) as our benchmark\n\nLet's also notice that the Month Day model has worse predictive power than the Month Dow or even the Month model. So the apparent precision (using the exact same days in the past) does not compensate in our dataset for the loss of averaging - only 5 days are used to predict each test sample vs. approx 25 (5 dows x 5 years)  or 130 (30 days in a month x 5 years) for the Month Dow and the Month models respectively.\n# Predicting the sales per year\nWe now focus on trying to predict the sales per year so as to be able to submit something for our Year Model family, of which the Year Month Dow model looks promising based on the SMAPE train of 11.0","0565c8e4":"# First models of the dataset\n\nWe now turn to implementing a few simple models of the dataset to better understand where the sales variability come from","af0408ed":"# Still basic models\nWe can refine the previous most basic model into in still basic model which attempt to predict a more accurate and relevant average than the store & item sales average over the training set.\n\n\n## Year models\n\nWe could start by trying to add the year information to the window used to compute the prediction average. However, here comes the first glitch: such a model will involve predicting the average for the test set year which will require some more modelling which we shall tackler later on.\n\nFor now, let's pretend that we have an oracle that for each past year of the training set allowed us to predict with 100% accuracy the average sales for the coming year.\n\n### Year only","dda3aabd":"Since for now we cannot submit the previous models for lack of sales prediction for the test year 2018, we can look into removing the year from the previous models and look at some month models\n\n### Month Only","b548f7cd":"### Year, Month & Day of Week (DOW)","34a16f02":"# Getting a feel of the dataset","d7271501":"To cross-check our results, we look into an analogous computation, in which however we avoid the intermediate step of computing the sales per store as a % of total sales for this store and then average. We directly compute the yearly sales for all stores divided by the total sales for all stores over the period.:"}}