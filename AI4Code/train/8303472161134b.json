{"cell_type":{"098d3fef":"code","a12140bd":"code","06da737d":"code","42f37f3e":"code","9d0e9d7b":"code","89214524":"code","49a6b456":"code","a172243d":"code","206ece89":"code","4443c324":"code","d5aef434":"code","dbf3720a":"code","5ebd273d":"code","320af62d":"code","5224578a":"code","3d7a8476":"code","7f569cc6":"code","4cb4171e":"code","249e7e4d":"code","da64c744":"code","9426f7bd":"code","d64cb0c1":"code","39db3c7f":"code","1e52cadd":"code","ef9973e9":"code","0d87e5c5":"code","617814f1":"code","9c6b8b10":"code","ebb762ac":"code","e08ee830":"code","5a1c77ae":"code","0f1f6946":"code","9337617f":"code","d0ff1f51":"code","e2bea9ff":"code","4633f3fc":"code","a42cd32b":"code","175a2224":"code","4a32ea0e":"code","3268b016":"code","064974b8":"code","381af135":"code","8ff6d6e2":"code","00f8a79f":"code","265808b2":"code","49e7f528":"code","3caf2c14":"code","79919682":"code","b75708ad":"code","e1381f9d":"code","52a0dc9e":"code","3639a653":"code","ce807506":"code","2684b568":"code","38b777db":"code","a286c5d6":"code","117652b3":"code","24df9177":"code","b6be3add":"code","eeda0efe":"code","c230d705":"code","c0f6261a":"code","eef17ae5":"code","fa3a4c9f":"code","668f4440":"code","be7721d1":"code","8c079bb7":"code","b3c0f6a5":"code","46b636eb":"code","d75bb302":"code","b7b00931":"code","16577bc1":"code","25234d89":"code","ad7b1e1c":"code","f716ee7e":"markdown","1b706a1a":"markdown","df52a1b3":"markdown","ed1d123a":"markdown","1ee735e1":"markdown","dc11e8ba":"markdown","3834fbcd":"markdown","00635270":"markdown","1d60deaa":"markdown","66884acf":"markdown","74018278":"markdown","b1cc7dbd":"markdown","840604b3":"markdown","9051ca6c":"markdown","2cc356f5":"markdown","bc6a49b6":"markdown","816047a8":"markdown","0cb5b80c":"markdown","5a14b1f3":"markdown","367b871f":"markdown","f48b5783":"markdown","19718d09":"markdown","afc90b6a":"markdown","ebd43143":"markdown","e5e822bb":"markdown","03bb518c":"markdown","ff80b16b":"markdown","2dd80e75":"markdown","03717b77":"markdown","ca5bf034":"markdown","9c345db7":"markdown","38b639a2":"markdown","b568af75":"markdown","30e363bc":"markdown","45e6a738":"markdown","8aa86e9c":"markdown","e234b346":"markdown","2bb22028":"markdown","91b8c253":"markdown","75778707":"markdown","e6bc3d0f":"markdown","231585ce":"markdown","55d76648":"markdown","28cc68b2":"markdown","2aaa1d85":"markdown","064c51c3":"markdown","fcfe6d61":"markdown","441e3ca3":"markdown","255fb1e2":"markdown","6d600265":"markdown","be381b94":"markdown","96798a01":"markdown","e0ba599e":"markdown","841dabf7":"markdown"},"source":{"098d3fef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nsns.set(style='white')","a12140bd":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_passenger_id = test_data['PassengerId'] ## For the Submission purpose","06da737d":"train_len = len(train_data)\ndataset = pd.concat([train_data, test_data] , axis=0)","42f37f3e":"dataset.head()","9d0e9d7b":"dataset.describe()","89214524":"train_data = train_data.drop('PassengerId',axis=1)","49a6b456":"train_data.isnull().sum()","a172243d":"plt.figure(figsize = (8,5))\nsns.countplot(x=train_data['Survived'])","206ece89":"plt.figure(figsize = (10,6))\nsns.countplot(x=train_data['Survived'],hue='Pclass' , data=train_data, hue_order=[1,2,3])","4443c324":"plt.figure(figsize = (10,6))\nsns.barplot(x  = 'Pclass' , y='Survived', data=train_data)","d5aef434":"plt.figure(figsize = (10,6))\nsns.countplot(x=train_data['Survived'],hue='Sex' , data=train_data)","dbf3720a":"plt.figure(figsize = (12,6))\nsns.kdeplot(train_data[train_data['Survived'] == 1]['Age'], color = 'Blue' , label = 'Survived')\nsns.kdeplot(train_data[train_data['Survived'] == 0]['Age'], color = 'Red', label = 'Not Survived' ) \nplt.xlabel('Age')\nplt.ylabel('Survival Probability')\nplt.grid()","5ebd273d":"plt.figure(figsize=(10,6))\nsns.countplot(x='Survived', hue = 'SibSp', data=train_data)\nplt.legend(bbox_to_anchor=[0.20,0,1,1])","320af62d":"plt.figure(figsize=(10,6))\nsns.barplot(x='SibSp', y = 'Survived', data=train_data)","5224578a":"plt.figure(figsize=(10,6))\nsns.countplot(x='Survived', hue = 'Parch', data=train_data)\nplt.legend(bbox_to_anchor=[0.20,0,1,1])","3d7a8476":"plt.figure(figsize=(10,6))\nsns.barplot(x = 'Parch', y='Survived', data=train_data)","7f569cc6":"plt.figure(figsize = (10,6))\nsns.kdeplot(train_data['Fare'], label = \"Skewness {:.2f}\".format(train_data['Fare'].skew()))","4cb4171e":"train_data['Fare'] = train_data['Fare'].apply(lambda x: 0 if x == 0 else np.log(x))","249e7e4d":"plt.figure(figsize=(10,6))\nsns.distplot(train_data['Fare'])","da64c744":"plt.figure(figsize=(10,6))\nsns.countplot(x='Survived', hue = 'Embarked', data=train_data)\nplt.legend(bbox_to_anchor=[0.20,0,1,1])","9426f7bd":"plt.figure(figsize=(8,6))\ng = sns.barplot(x = 'Embarked', y='Survived', data=train_data)","d64cb0c1":"train_data['Age'].isnull().sum()","39db3c7f":"train_data.corrwith(train_data['Age'])","1e52cadd":"plt.figure(figsize = (8,6))\nsns.boxplot(x='Pclass', y=\"Age\", data=train_data)","ef9973e9":"print(math.ceil(train_data[train_data['Pclass'] == 1]['Age'].mean()))\nprint(math.ceil(train_data[train_data['Pclass'] == 2]['Age'].mean()))\nprint(math.ceil(train_data[train_data['Pclass'] == 3]['Age'].mean()))","0d87e5c5":"def age_impute(cols):\n    age = cols[0]\n    pclass = cols[1]\n    if np.isnan(age) :\n        if pclass == 1:\n            return 39\n        elif pclass == 2:\n            return 30\n        else :\n            return 26\n    else :\n        return age\n    \ntrain_data['Age'] = train_data[['Age', 'Pclass']].apply(age_impute,axis=1)\n\n# Converting all ages to their Upper bound\ntrain_data['Age'] = train_data['Age'].apply(math.ceil)","617814f1":"train_data['Cabin'].isnull().sum()","9c6b8b10":"train_data['Cabin'] = train_data['Cabin'].fillna('Other')","ebb762ac":"train_data['Ticket'] = train_data['Ticket'].apply(lambda x : x.split()[0])","e08ee830":"train_data['Ticket'].value_counts()[:10]","5a1c77ae":"def Ticket_categorize(tick):\n    if tick == 'PC':\n        return 'PC'\n    elif tick == 'C.A.':\n        return 'CA'\n    elif tick == 'STON\/O':\n        return 'STON'\n    else:\n        return 'Other'","0f1f6946":"train_data['Ticket'] = train_data['Ticket'].apply(Ticket_categorize)","9337617f":"plt.figure(figsize=(10,6))\nsns.barplot(x = 'Ticket', y= 'Survived', data=train_data)","d0ff1f51":"train_data.corrwith(train_data['Ticket'].map({'PC' : 4, 'CA': 3, 'STON' : 2, 'Other': 1}))","e2bea9ff":"train_data['Cabin'] = train_data['Cabin'].dropna().apply(lambda x: x[0])\ntrain_data['Cabin'].value_counts()","4633f3fc":"plt.figure(figsize=(10,6))\nsns.barplot(x ='Cabin', y='Survived', data=train_data.dropna(),order='A B C D E F G T O'.split(' '))","a42cd32b":"train_data.corrwith(train_data['Cabin'].map({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'T':7,'O':8}))","175a2224":"train_data['Name'].head()","4a32ea0e":" train_data['Name'].apply(lambda x : x.split(',')[1].split('.')[0]).value_counts()","3268b016":"train_data['Name'] =  train_data['Name'].apply(lambda x : x.split(',')[1].split('.')[0].strip())\n\n## Keeping the Mr, Mrs , Miss, Master as it is, and replacing the others with Others\n\ntrain_data['Name'] = train_data['Name'].replace(\n    to_replace = ['Dr' , 'Rev', 'Mlle', 'Col', 'Major', 'Jonkheer', 'the Countess', 'Ms', 'Lady', 'Capt', 'Don', 'Mme' ], \n    value=  'Others')","064974b8":"plt.figure(figsize=(10,6))\nsns.barplot(x = train_data['Name'], y= train_data['Survived'])","381af135":"train_data['FamilySize'] = train_data['Parch'] + train_data['SibSp'] + 1","8ff6d6e2":"def cal_family(col):\n    if col == 1:\n        return 'Single'\n    elif col > 1 and col <= 4 :\n        return 'Small'\n    else:\n        return 'Large'","00f8a79f":"train_data['FamilySize']  = train_data['FamilySize'].apply(cal_family)","265808b2":"plt.figure(figsize = (8,6))\nsns.barplot(x = 'FamilySize' , y = 'Survived', data = train_data)","49e7f528":"train_data['Sex'] = train_data['Sex'].map({'male' :1, 'female':0})","3caf2c14":"train_data.head()","79919682":"columns = [ 'Name', 'Ticket','Cabin', 'Embarked', 'FamilySize']","b75708ad":"train_data = pd.concat([train_data, pd.get_dummies(train_data[columns], drop_first=True)],axis=1)","e1381f9d":"train_data = train_data.drop(columns,axis=1)","52a0dc9e":"train_data","3639a653":"train_data.isnull().sum()","ce807506":"X = train_data.drop('Survived',axis=1).astype(int)\ny = train_data['Survived'].astype(int)","2684b568":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\npipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVC', SVC())])))\npipelines.append(('ScaledDT', Pipeline([('Scaler', StandardScaler()),('DT', DecisionTreeClassifier())])))\npipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))\npipelines.append(('ScaledGB', Pipeline([('Scaler', StandardScaler()),('GB', GradientBoostingClassifier())])))\npipelines.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostClassifier())])))\npipelines.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier())])))\npipelines.append(('ScaledKN', Pipeline([('Scaler', StandardScaler()),('KN', KNeighborsClassifier())])))","38b777db":"column_values = ['Name', 'Score', 'Std Dev']\nmodel_comp = pd.DataFrame(columns= column_values)","a286c5d6":"row = 0\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10)\n    cv_score = cross_val_score(model,X, y, cv = kfold, scoring = 'accuracy', verbose = 2, n_jobs= -1)\n    print('{} model -  accuracy mean {}  std {}'.format(name, cv_score.mean(),cv_score.std()))\n    model_comp.loc[row, 'Name'] = name\n    model_comp.loc[row, 'Score'] = cv_score.mean()\n    model_comp.loc[row, 'Std Dev'] = cv_score.std()\n    row += 1 ","117652b3":"model_comp.sort_values(by='Score', ascending = False)","24df9177":"X","b6be3add":"columns_to_scale = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nscaler = StandardScaler().fit(X[columns_to_scale])\nX_scaled = scaler.transform(X[columns_to_scale])","eeda0efe":"columns_left = list(set(X.columns) - set(columns_to_scale)) ## Left out columns","c230d705":"X_scaled = np.column_stack((X_scaled, X[columns_left].values)) ## combining the standardized columns and categorical columns","c0f6261a":"X_scaled.shape","eef17ae5":"model_comp_after_gridsearch = pd.DataFrame(columns = ['Name', 'Best Score'])","fa3a4c9f":"models_param_grid  = [\n                { 'C' : [0.5,0.8,1,1.5,2], 'max_iter' : [100,120,150,200] },\n    \n                { 'C' : [0.1,1,5,10,15],\n              'gamma' : [0.1,0.03,0.05,0.08,0.5,0.8],\n              'degree' : [2,3,5,8]},\n                \n                { 'learning_rate' : [0.01, 0.05, 0.1, 0.5, 0.8, 1],\n                   'max_depth' : [2,3,4,5],\n                    'n_estimators': [ 70, 90, 100,105, 110,115, 120 ]},\n                \n                {'learning_rate' : [0.01, 0.05, 0.1, 0.5, 0.8, 1],\n                    'n_estimators': [ 70, 90, 100,105, 110,115, 120 ] }  ,    \n                      \n                {  'leaf_size' : [20,25,30,35,40 ],\n                    'n_neighbors' : [3,5,7,9,11]},      \n                      \n                {'n_estimators' : [75,95,100,105,110,120],\n              'max_features' :  [5,8,10,12,14],\n              'min_samples_split' : [1,3,5,7],\n              'min_samples_leaf' : [1,3,5,7]}\n                      \n                      \n               ]\nmodels = [ ('LogisticRegression', LogisticRegression()),('SVM',SVC()) , ('GradientBoosting',GradientBoostingClassifier()), ('AdaBoost', AdaBoostClassifier()), \n          ('KNeighbors' , KNeighborsClassifier()), ('RandomForest',RandomForestClassifier())]","668f4440":"for i in range(0,len(models)) :\n    \n    if i == 0:\n        logistic_regression_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i] , \n                                            cv = kfold, verbose = 2, scoring = 'accuracy', n_jobs= -1)\n        logistic_regression_gridsearch_model.fit(X_scaled,y)\n        model_comp_after_gridsearch.loc[i, 'Best Score'] = logistic_regression_gridsearch_model.best_score_\n        \n    elif i == 1:\n        svm_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i] , \n                                            cv = kfold, verbose = 2, scoring = 'accuracy', n_jobs= -1)\n        svm_gridsearch_model.fit(X_scaled,y)\n        model_comp_after_gridsearch.loc[i, 'Best Score'] = svm_gridsearch_model.best_score_\n        \n    elif i == 2 :\n        gradientboosting_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i] , \n                                            cv = kfold, verbose = 2, scoring = 'accuracy', n_jobs= -1)\n        gradientboosting_gridsearch_model.fit(X_scaled , y)\n        model_comp_after_gridsearch.loc[i, 'Best Score'] = gradientboosting_gridsearch_model.best_score_\n    \n    elif i ==3 :\n        adaboost_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i],\n                                                cv = kfold, verbose = 2, scoring = 'accuracy', n_jobs = -1)\n        adaboost_gridsearch_model.fit(X_scaled, y)\n        model_comp_after_gridsearch.loc[i , 'Best Score'] = adaboost_gridsearch_model.best_score_\n        \n    elif i == 4:\n        knn_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i],\n                                           cv = kfold, verbose = 2, scoring = 'accuracy' , n_jobs = -1)\n        knn_gridsearch_model.fit(X_scaled,y)\n        model_comp_after_gridsearch.loc[i , 'Best Score'] = knn_gridsearch_model.best_score_\n    \n    else:\n        randomforest_gridsearch_model = GridSearchCV(models[i][1], param_grid = models_param_grid[i] , \n                                            cv = kfold, verbose = 2, scoring = 'accuracy', n_jobs= -1)\n        randomforest_gridsearch_model.fit(X_scaled , y)\n        model_comp_after_gridsearch.loc[i, 'Best Score'] = randomforest_gridsearch_model.best_score_\n        \n    model_comp_after_gridsearch.loc[i, 'Name'] = models[i][0]\n    \n\n","be7721d1":"model_comp_after_gridsearch.sort_values('Best Score', ascending = False)","8c079bb7":"def learning_curve_plot(model, title, X, y, cv, train_sizes = np.linspace(0.1,1,10)):\n    \n    train_sizes, test_scores, train_scores = learning_curve(model, X, y, cv = cv, train_sizes= train_sizes)\n    \n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores , axis =1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.figure(figsize = (8,6))\n    plt.title(title)\n    plt.xlabel('Training sizes')\n    plt.ylabel('Scores')\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean + train_scores_std, train_scores_mean - train_scores_std, alpha = 0.2)\n    plt.fill_between(train_sizes, test_scores_mean + test_scores_std, test_scores_mean - test_scores_std, alpha = 0.2)\n\n    plt.plot(train_sizes, train_scores_mean, 'o-', label = 'training score' )\n    plt.plot(train_sizes, test_scores_mean, 'o-', label = 'test score' )\n    \n    return plt\n\ng = g = learning_curve_plot(logistic_regression_gridsearch_model.best_estimator_,\"Logistic_regression learning curves\",X_scaled,y,cv=kfold)\ng = learning_curve_plot(svm_gridsearch_model.best_estimator_,\"SVM learning curves\",X_scaled,y,cv=kfold)\ng = learning_curve_plot(gradientboosting_gridsearch_model.best_estimator_,\"GradientBoosting learning curves\",X_scaled,y,cv=kfold)\ng = learning_curve_plot(adaboost_gridsearch_model.best_estimator_, \"AdaBoost Learning Curve\", X_scaled, y, cv = kfold)\ng = learning_curve_plot(knn_gridsearch_model.best_estimator_ , \"KNN Learning Curve\", X_scaled, y, cv = kfold)\ng = learning_curve_plot(randomforest_gridsearch_model.best_estimator_,\"RandomForest learning curves\",X_scaled,y,cv=kfold)\n","b3c0f6a5":"logistic_regression_model = logistic_regression_gridsearch_model.best_estimator_\nsvm_model = SVC(C = 1, degree = 2, gamma = 0.03, probability = True)\nrandomforest_model = randomforest_gridsearch_model.best_estimator_\ngradientboosting_model = gradientboosting_gridsearch_model.best_estimator_\nadaboost_model = adaboost_gridsearch_model.best_estimator_\nknn_model = knn_gridsearch_model.best_estimator_","46b636eb":"voting_model = VotingClassifier( [ \n    ('Logistic_Regression', logistic_regression_model), \n    (\"Random Forest\", randomforest_model) ,\n    (\"SVM\",svm_model), \n    ('GradientBoosting', gradientboosting_model),\n    (\"AdaBoost\", adaboost_model), \n    (\"knn_model\", knn_model)\n                                ] ,\n                                voting='soft', n_jobs = -1)\nvoting_model.fit(X_scaled, y)","d75bb302":"def preprocess_data(test_data) :\n    ## age imputation\n    test_data = test_data.reset_index(drop=True)\n    test_data['Age'] =  test_data[['Age','Pclass']].apply(age_impute,axis=1)\n    \n    ## Cabin\n    test_data['Cabin'] = test_data['Cabin'].fillna('Others')\n    test_data['Cabin'] = test_data['Cabin'].apply(lambda x: x[0])\n    \n    ## Name\n    test_data['Name'] = test_data['Name'].apply(lambda x : x.split(',')[1].split('.')[0].strip())\n    test_data['Name'] = test_data['Name'].replace(\n        to_replace = ['Dr' , 'Rev', 'Mlle', 'Col', 'Major', 'Jonkheer', 'the Countess', 'Ms', 'Lady', 'Capt', 'Don', 'Mme' ],\n        value=  'Others')\n    \n    \n    ## Ticket\n    test_data['Ticket'] = test_data['Ticket'].apply(lambda x:x.split()[0])\n    test_data['Ticket'] = test_data['Ticket'].apply(Ticket_categorize)\n    \n    ## Fare\n    test_data['Fare'] = test_data['Fare'].apply(lambda x: 0 if x == 0 else np.log(x))\n    test_data['Fare'] = test_data['Fare'].fillna(np.mean(test_data['Fare']))\n    \n    ## Embarked\n    test_data['Embarked'] = test_data['Embarked'].fillna('S')\n    \n    ## sex\n    test_data['Sex'] = test_data['Sex'].map({'male' :1, 'female':0})\n    \n    # familysize\n    test_data['FamilySize'] = test_data['Parch'] + test_data['SibSp'] + 1\n    test_data['FamilySize']  = test_data['FamilySize'].apply(cal_family)\n\n    \n    ## categorical \n    test_cols = list(test_data.columns)\n    train_categorical_cols = [ 'Cabin_B', 'Cabin_C','Cabin_D','Cabin_E','Cabin_F','Cabin_G',\n                              'Cabin_O' ,'Cabin_T','Embarked_Q', 'Embarked_S' ,'Name_Miss','Name_Mr',\n                              'Name_Mrs','Name_Others', 'Name_Sir','Ticket_Other', 'Ticket_PC' ,'Ticket_STON','FamilySize_Single',\n                                'FamilySize_Small']\n    \n    columns = ['Cabin', 'Embarked', 'Name', 'Ticket', 'FamilySize' ]\n    test_categorical_cols = pd.get_dummies(test_data[columns])\n    \n    \n    left_categorical_cols = list(set(train_categorical_cols) - set(list(test_categorical_cols.columns)))\n    \n    left_df = pd.DataFrame(columns = left_categorical_cols, data = np.zeros((test_data.shape[0],len(left_categorical_cols))))\n    test_data = pd.concat([test_data, test_categorical_cols, left_df], axis=1) ## combining the left out columns and test categorical cols\n    test_data = test_data[test_cols + train_categorical_cols]  ## Rearranging the columns\n    \n    #dropping\n    test_data = test_data.drop(columns,axis=1)\n    \n    test_data = test_data.dropna()\n    test_scaled_data = standardize_data(test_data)\n    \n    predictions = voting_model.predict(test_scaled_data)\n    \n    return predictions\n\n\ndef standardize_data(test_data):\n    columns_to_scale = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n    scaler = StandardScaler()\n    scaler.fit(test_data[columns_to_scale])\n    X_scaled = scaler.transform(test_data[columns_to_scale])\n    columns_left = list(set(test_data.columns) - set(columns_to_scale)) ## Left out columns\n    X_scaled = np.column_stack((X_scaled, test_data[columns_left].values)) ## combining the standardized columns and categorical columns\n    return X_scaled","b7b00931":"predictions = preprocess_data(test_data.drop(['PassengerId'], axis=1))","16577bc1":"submission = pd.DataFrame(data = { 'PassengerId' : test_data['PassengerId'], 'Prediction' : predictions})","25234d89":"submission","ad7b1e1c":"submission.to_csv('Submission.csv', index=False)","f716ee7e":"## Building a Voting Classifier","1b706a1a":"### Sibsp","df52a1b3":"This shows that the people from Cherbourg(C) had higher probability to survive than Queenstown (Q), Southampton (S).","ed1d123a":"## Learning Curve","1ee735e1":"Very few people had Cabin. 687 people didn't have cabin so Let's fill the Null values of Cabin column with others","dc11e8ba":"## Feature Engineering","3834fbcd":"### Fare","00635270":"### Adding the Family size column ","1d60deaa":"## GridSearchCV  - For the tuning our models","66884acf":"### Pclass","74018278":"### Importing the Libraries","b1cc7dbd":"### Cabin","840604b3":"The data shows that the people with more siblings have lower survival rate. This can be because they would have got stuck searching for their siblings.","9051ca6c":"The Age and Cabin columns have many null values and needs to be imputed.","2cc356f5":"The categorical columns needn't be standarized because they are only in ones and zeros. Therefore we are standardizing all the columns but the categorical\/dummies","bc6a49b6":"### Splitting the Features and Targets","816047a8":"### Prediction","0cb5b80c":"### Taking Pclass as the reference to impute the Missing Ages","5a14b1f3":"The dataset is not balanced but fine for training on it","367b871f":"The Fare graph is very skewed and needs to be transformed. I'll use the log transform for that","f48b5783":"## Feature Analysis","19718d09":"## Imputation - Filling the Missing values","afc90b6a":"### Ticket","ebd43143":"### Embarked","e5e822bb":"### Categorical Columns for Categorical Features","03bb518c":"I'm taking the first few words of the ticket as they might give me the clusters of people who sat together or somewhere near to each other. ","ff80b16b":"### Loading the Data and basic check","2dd80e75":"### Dropping the PassengerId as it is not a feature","03717b77":"Adding a main_cabin column which is just the first alphabet of the Cabin","ca5bf034":"### Age","9c345db7":"### Parch","38b639a2":"### Sex into Male 1 and Female 0","b568af75":"### Pipelines\n","30e363bc":"The data has 177 Missing values. There are various methods of imputation but I'm imputing based on the highly correlated. ","45e6a738":"### Dropped the Cabin, Ticket, Name, Embarked, title,  columns","8aa86e9c":"So we'll take the first 6 models further for tuning them.","e234b346":"### Title","2bb22028":"It's clear that the Passengers in the 1st class the first priority ones during Evacuation, then the 2nd and at last the 3rd class.","91b8c253":"From the above graph we can interpret that the people with Mr. Title had very less survival rate whereas the Sir title had a lot more than other. Females i.e Mrs and Miss had great probability to survive than others and Master","75778707":"The models performs good with the training dataset Not much of overfitting can be observed","e6bc3d0f":"## Standardization","231585ce":"## Modeling","55d76648":"### Cabin","28cc68b2":"Fare has very less correlation with the Age. But Pclass, Sibsp and Parch has good -ve correlation. I'll take the Pclass for imputing the Age. As the age increases the people tend spend money on comfort and hence older the peole better the class","2aaa1d85":"I'm taking the Mean of the Age of particular Class for imputation","064c51c3":"As the fare is positively correlated to the tickets in above manner, we see that PC is the most expensive and people with that had more probablity for survival as compared to other. Then comes tickets with CA and then STON and least for Others ","fcfe6d61":"The data shows that female had higher probability for survival ","441e3ca3":"Fetching the Titlte of each person ","255fb1e2":"### Sex ","6d600265":"Since the all three models performs well, I am combining all and using VotingClassifier of sklearn","be381b94":"Graph shows that the young children(between 20 to 40) and infants(0-5) had a probabilty of surviving whereas the very old people had very less chances of surviving ","96798a01":"### Check whether the dataset is balanced or not","e0ba599e":"Now the fare column is log transformed ","841dabf7":"### Age"}}