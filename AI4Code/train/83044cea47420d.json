{"cell_type":{"eb5baf37":"code","1f411597":"code","ed05a235":"code","902e9be8":"code","db3b33bb":"code","4bf3f7a8":"code","4e0a50d5":"code","85a60178":"code","332b3520":"code","ee55c5b6":"code","62555e1e":"code","cda4eab1":"code","f65b856a":"code","d3b7841c":"code","7b5a060b":"code","f9f0731f":"code","0bc5036a":"code","af27d76f":"code","786021c1":"code","e2f89adb":"code","99b5eb24":"code","a158835a":"code","d4a5dd37":"code","9778092d":"code","5041d698":"code","54fb29c2":"code","65b461b6":"code","0c99e664":"code","35326c98":"code","b62c51b1":"code","5c560401":"code","4e41ff2c":"markdown","c850867a":"markdown","8b6e6644":"markdown","ec26b1e2":"markdown","97294a52":"markdown","ce53e748":"markdown","6195b743":"markdown","7a3c653b":"markdown","786aea63":"markdown","ea10dc07":"markdown","230353a2":"markdown","310a9c6a":"markdown","5cebd244":"markdown","cf05ffa9":"markdown","22cc4d3c":"markdown","ea76020a":"markdown"},"source":{"eb5baf37":"import gc\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\n\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","1f411597":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","ed05a235":"# Dimension of Train and Test Data\ntrain_df.shape, test_df.shape","902e9be8":"train_df.head()","db3b33bb":"test_df.head()","4bf3f7a8":"print(train_df.info())\nprint('\\n')\nprint(test_df.info())","4e0a50d5":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n#     if np.tranpose(tt).loc['Total'].sum()==0:\n#         print(\"No missing value in the entire data\")\n    return(np.transpose(tt))","85a60178":"# Checking for missing data for train data\nmissing_data(train_df)\nprint(missing_data(train_df).loc['Total'].sum())","332b3520":"# Checking for missing data for test data\nmissing_data(test_df)\nprint(missing_data(test_df).loc['Total'].sum())","ee55c5b6":"train_df.describe()","62555e1e":"test_df.describe()","cda4eab1":"sns.countplot(train_df['target'], palette='Set3')","f65b856a":"print(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]\/train_df.shape[0]))","d3b7841c":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(8,8,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show()","7b5a060b":"# The first 64 features\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[1:65]\nplot_feature_distribution(t0, t1, '0', '1', features)\nplt.tight_layout()","f9f0731f":"# Another 64 features\n%time\nfeatures = train_df.columns.values[65:129]\nplot_feature_distribution(t0, t1, '0', '1', features)","0bc5036a":"# Another 64 features\n%time\nfeatures = train_df.columns.values[129:193]\nplot_feature_distribution(t0, t1, '0', '1', features)","af27d76f":"# Last 64 features\n%time\nfeatures = train_df.columns.values[193:257]\nplot_feature_distribution(t0, t1, '0', '1', features)","786021c1":"train_df['wheezy-copper-turtle-magic'].describe()","e2f89adb":"test_df['wheezy-copper-turtle-magic'].describe()","99b5eb24":"plt.figure(figsize=[20,5])\n\nplt.subplot(1,2,1)\nsns.distplot(train_df[\"wheezy-copper-turtle-magic\"])\nplt.title(\"train\")\n\nplt.subplot(1,2,2)\nsns.distplot(test_df[\"wheezy-copper-turtle-magic\"])\nplt.title(\"test\")","a158835a":"plt.figure(figsize=[9,5])\nplt.subplot(1,2,1)\ntrain_df.groupby(\"wheezy-copper-turtle-magic\").size().sort_values()[::-1].hist(bins=50)\nplt.title(\"train\")\n\nplt.subplot(1,2,2)\ntest_df.groupby(\"wheezy-copper-turtle-magic\").size().sort_values()[::-1].hist(bins=50)\nplt.title(\"test\")","d4a5dd37":"# The first 64 features\nfeatures = train_df.columns.values[1:65]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)\nplt.tight_layout()","9778092d":"# Another 64 features\nfeatures = train_df.columns.values[65:129]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)\nplt.tight_layout()","5041d698":"# Another 64 features\nfeatures = train_df.columns.values[129:193]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)\nplt.tight_layout()","54fb29c2":"# Last 64 features\nfeatures = train_df.columns.values[193:257]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)\nplt.tight_layout()","65b461b6":"# Correlations between features in training data\n# Top10 lowest correlation pairs\n\ncorrelations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","0c99e664":"# Correlations between features in training data\n# Top10 highest correlation pairs\ncorrelations.tail(10)","35326c98":"cols = [c for c in train_df.columns if c not in ['id']]\ncorr = train_df[cols].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorr = corr[corr['level_0'] != corr['level_1']]\ncorr = corr[corr['level_0'] == 'target']","b62c51b1":"# features with lowest correlation with target\ncorr.head(10)","5c560401":"# features with highest correlation with target\ncorr.tail(10)","4e41ff2c":"No missing values in both train and test data!","c850867a":"We see that distributions of 'wheezy-copper-turtle-magic' in both train and test data here look different from others","8b6e6644":"# Summary Statistics of data","ec26b1e2":"# Correlation between features","97294a52":"# Missing Data?","ce53e748":"# Distribution of Target","6195b743":"# Load Library and Data","7a3c653b":"Target is very well balanced (surprisingly)!!!","786aea63":"Correlation amongst features seems very low! probably because it's an artifical data designed to have less correlation amongst features?!","ea10dc07":"# Density Plots of Features","230353a2":"Train Data\n- id\n- target (0 or 1: binary)\n- 256 features\n\nTest Data\n- id\n- 256 features","310a9c6a":"# Correlation between features and target","5cebd244":"For all features, distribution of target==0 and target==1 seems almost identical (completely overlaid) :oooo","cf05ffa9":"# How is the distribution of each feature different between train and test?","22cc4d3c":"But there is one feature that looks very different from all other features! That is the \"wheezy-copper-turtle-magic\"! Let's look at that feature a little bit more!","ea76020a":"While other features are usually within the range of -2 to 2 and are floats, this \"wheezy-copper-turtle-magic\" feature seems to be comprised of discrete integers and has very large values (e.g. max value is 511)"}}