{"cell_type":{"eac59740":"code","9e7d055c":"code","8f6e7b73":"code","448c3904":"code","ca1dc2aa":"code","bdf67c0b":"code","b8a120ce":"code","f8e9239b":"code","33ca2242":"code","4eb46306":"code","9f3b327c":"code","85c21d92":"code","b32d01e7":"code","1f703e90":"code","cbba34bb":"code","dc08b67e":"code","8216bf0a":"code","2d99a810":"code","f2237dd3":"code","635745bd":"code","6aaf5c1b":"code","c7c7a495":"code","b60b1702":"code","756af640":"code","5ffbe3d5":"code","4b57cb71":"code","e3e67460":"code","b7e17fae":"code","c61c4eaf":"code","17b53986":"code","c1c8d14f":"code","aa25c6f0":"code","15253ac9":"markdown"},"source":{"eac59740":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e7d055c":"train_labels = pd.read_csv('..\/input\/data-science-london-scikit-learn\/trainLabels.csv', header=None) # (999, 1)\ntrain = pd.read_csv('..\/input\/data-science-london-scikit-learn\/train.csv', header=None) # (999, 40)\ntest = pd.read_csv('..\/input\/data-science-london-scikit-learn\/test.csv', header=None) # (8999, 40)","8f6e7b73":"from sklearn.model_selection import train_test_split, cross_val_score\n\ntrain_labels = np.ravel(train_labels)\n\nX_train, X_test, y_train, y_test = train_test_split(train, train_labels)","448c3904":"type(y_train)","ca1dc2aa":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\nclf.predict(X_test)\nclf.score(X_test, y_test)","bdf67c0b":"neighbors = np.arange(1, 20)\nkfold = 10\ntrain_acc = []\nval_acc = []\nbestKnn = None\nbestAcc = 0.0\n\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    train_acc.append(knn.score(X_train, y_train))\n    nominee = np.mean(cross_val_score(knn, train, train_labels, cv=kfold))\n    val_acc.append(nominee)\n    if nominee > bestAcc:\n        bestAcc = nominee\n        bestKnn = knn\n","b8a120ce":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=[13, 8])\nplt.plot(neighbors, val_acc, label='Validation Accuracy')\nplt.plot(neighbors, train_acc, label='Training Accuracy')\nplt.legend()\nplt.title('K values VS Accuarcy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neighbors)\nplt.show()\n\nprint(\"Best Accuracy without feature scaling: \", bestAcc)\nprint(bestKnn)","f8e9239b":"plt.style.use('ggplot')","33ca2242":"test_fill = np.nan_to_num(test)\nsubmission = pd.DataFrame(bestKnn.predict(test_fill))\nprint(submission.shape)","4eb46306":"submission.columns = ['Solution']\nsubmission['Id'] = np.arange(1, submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission.head()","9f3b327c":"submission.to_csv('submission_with_copy.csv', index=False)\nfrom subprocess import check_output\nprint(check_output(['ls', '..\/working']).decode('utf8'))","85c21d92":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n\nstd = StandardScaler()\nX_std = std.fit_transform(train)\n\nmms = MinMaxScaler()\nX_mms = mms.fit_transform(train)\n\nnorm = Normalizer()\nX_norm = norm.fit_transform(train)","b32d01e7":"val_acc = {'std': [], 'mms': [], 'norm': []}\nbestScaling = None\nbestKnn = None\nbestAcc = 0.0\n\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    s1 = np.mean(cross_val_score(knn, X_std, train_labels, cv=kfold))\n    val_acc['std'].append(s1)\n    \n    s2 = np.mean(cross_val_score(knn, X_mms, train_labels, cv=kfold))\n    val_acc['mms'].append(s2)\n    \n    s3 = np.mean(cross_val_score(knn, X_norm, train_labels, cv=kfold))\n    val_acc['norm'].append(s3)\n    \n    if s1 > bestAcc:\n        bestAcc = s1\n        bestKnn = knn\n        bestScaling = 'std'\n        \n    if s2 > bestAcc:\n        bestAcc = s2\n        bestKnn = knn\n        bestScaling = 'mms'\n        \n    if s3 > bestAcc:\n        bestAcc = s3\n        bestKnn = knn\n        bestScaling = 'norm'","1f703e90":"plt.figure(figsize=(13, 8))\nplt.plot(neighbors, val_acc['std'], label='Cross Validation Accuracy with Standard Scaler')\nplt.plot(neighbors, val_acc['mms'], label='Cross Validation Accuracy with Min Max Scalser')\nplt.plot(neighbors, val_acc['norm'], label='Cross Validation Accuracy with Normalizer')\nplt.legend()\nplt.title('Find best K')\nplt.xlabel('# of neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neighbors)\nplt.show()\n\nprint('Best Accuracy with feature scaling: ', bestAcc)\nprint('Best KNN Classifier: ', bestKnn)\nprint('Best Scaler: ', bestScaling)","cbba34bb":"bestKnn.fit(X_norm, train_labels)\nsubmission = pd.DataFrame(bestKnn.predict(norm.transform(test_fill)))\nprint(submission.shape)","dc08b67e":"submission.columns = ['Solution']\nsubmission['Id'] = np.arange(1, submission.shape[0] + 1)\nsubmission = submission[['Id', 'Solution']]\nsubmission.head()","8216bf0a":"submission.to_csv('submission_with_normalize.csv', index=False)\nprint(check_output(['ls', '..\/working']).decode('utf8'))","2d99a810":"import seaborn as sns","f2237dd3":"f, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(pd.DataFrame(X_std).corr(), annot=True, linewidths=.5, fmt='.1f', ax=ax)","635745bd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X_std, train_labels, test_size=.3)","6aaf5c1b":"clf_rf = RandomForestClassifier()\nclf_rf = clf_rf.fit(X_train, y_train)","c7c7a495":"ac = accuracy_score(y_test, clf_rf.predict(X_test))\nprint(\"Accuracy: \", ac)","b60b1702":"cm = confusion_matrix(y_test, clf_rf.predict(X_test))\nsns.heatmap(cm, annot=True, fmt='d')","756af640":"from sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV","5ffbe3d5":"kfold = 10\nbestSVC = None\nbestAcc = 0.0\nval_acc = []\ncv_range = np.arange(5, 11)\nn_features = []\n\nfor cv in cv_range:\n    svc = SVC(kernel='linear')\n    rfecv = RFECV(estimator=svc, step=1, cv=cv, scoring='accuracy')\n    rfecv.fit(X_std, train_labels)\n    \n    val_acc.append(np.mean(cross_val_score(svc, X_std[:, rfecv.support_],\n                                           train_labels, cv=kfold)))\n    \n    n_features.append(rfecv.n_features_)\n    if val_acc[-1] > bestAcc:\n        bestAcc = val_acc[-1]","4b57cb71":"plt.figure(figsize=(13, 8))\nplt.plot(cv_range, val_acc, label=\"CV Accuracy\")\n\nfor i in range(len(cv_range)):\n    plt.annotate(str(n_features[i]), xy=(cv_range[i], val_acc[i]))\n    \nplt.legend()\nplt.title('Cross Validation Accuracy')\nplt.xlabel('K Fold')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint(\"Best Accuracy with feature scaling and RFECV: \", bestAcc)","e3e67460":"import numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\n\nX_train = train\ny_train = train_labels\nX_test = test\n\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\nX_test = np.asarray(X_test)\ny_train = y_train.ravel()\n\nprint(\"Training Data Shape: \", X_train.shape)\nprint(\"Training Target Shape: \", y_train.shape)\nprint(\"Testing Data Shape: \", X_test.shape)","b7e17fae":"X_all = np.r_[X_train, X_test]\nprint(\"Whole Data Shape: \", X_all.shape)","c61c4eaf":"from sklearn.mixture import GaussianMixture\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\n\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(X_all)\n        bic.append(gmm.aic(X_all))\n        \n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X_all)\nX_train = best_gmm.predict_proba(X_train)\nX_test = best_gmm.predict_proba(X_test)\n\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier()\n\nparam_grid = dict()\n\ngrid_search_knn = GridSearchCV(knn, param_grid=param_grid, cv=10, scoring='accuracy').fit(X_train, y_train)\nprint(\"Best estimator KNN: \", grid_search_knn.best_estimator_,\n     \"Best score: \", grid_search_knn.best_estimator_.score(X_train, y_train))\nknn_best = grid_search_knn.best_estimator_\n\ngrid_search_rf = GridSearchCV(rf, param_grid=dict(), verbose=3, cv=10, scoring='accuracy').fit(X_train, y_train)\nprint(\"Best estimator RandomForest: \", grid_search_rf.best_estimator_,\n     \"Best score: \", grid_search_rf.best_estimator_.score(X_train, y_train))\nrf_best = grid_search_rf.best_estimator_","17b53986":"knn_best.fit(X_train, y_train)\nrf_best.fit(X_train, y_train)\n\nprint(\"Score for KNN: \", cross_val_score(knn_best, X_train, y_train, cv=10, scoring='accuracy').mean())\nprint(\"Score for Random Forest: \", cross_val_score(rf_best, X_train, y_train, cv=10, scoring='accuracy').max())","c1c8d14f":"knn_best_pred = pd.DataFrame(knn_best.predict(X_test))\nknn_best_pred.index += 1\nknn_best_pred.columns = ['Solution']\nknn_best_pred['Id'] = np.arange(1, knn_best_pred.shape[0] + 1)\nknn_best_pred = knn_best_pred[['Id', 'Solution']]\nknn_best_pred.to_csv('submission.csv', index=False)","aa25c6f0":"print(check_output(['ls', '..\/working']).decode('utf8'))","15253ac9":"Summary"}}