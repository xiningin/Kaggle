{"cell_type":{"5cea5b91":"code","02146a5b":"code","b4f6a91a":"code","858240d3":"code","0eb1fc53":"code","496bad56":"code","e0304f0e":"code","8836ac93":"code","aaf01939":"code","b4f8ad33":"code","b259f0be":"code","2dac0220":"code","d5a901fa":"code","4cb48041":"markdown","2a8273a0":"markdown","f89ea295":"markdown","c58d2cde":"markdown","7f1ac4d7":"markdown"},"source":{"5cea5b91":"import json\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","02146a5b":"file_path1 = '\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json'\ndf = pd.read_json(file_path1,lines=True)\ndf = df[['headline','is_sarcastic']]\ndf.head()\n","b4f6a91a":"headlines = df['headline'].values.tolist()\nsarcastic = df['is_sarcastic'].values.tolist()\n\nprint('Length of data {}'.format(len(headlines)))","858240d3":"training_size = 20000\ntest_size = 6709\n\ntrain_x = headlines[:training_size]\ntest_x = headlines[training_size:]\ntrain_y = np.array(sarcastic[:training_size])\ntest_y = np.array(sarcastic[training_size:])","0eb1fc53":"print(train_x[0])\nprint(train_y[0])","496bad56":"# vocab_size = 2000   #number of words in tokenizer\nembedding_dim = 100\nmax_len = 16\n\ntokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_x)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\nsequence_train = tokenizer.texts_to_sequences(train_x)\nseq_padd_train = pad_sequences(sequence_train,padding='post',truncating='post',maxlen=max_len)\n\n#test\nsequence_test = tokenizer.texts_to_sequences(test_x)\nseq_padd_test = pad_sequences(sequence_test,padding='post',truncating='post',maxlen=max_len)","e0304f0e":"print(sequence_train[0])\nprint(seq_padd_train[0])\nprint(seq_padd_train.shape)","8836ac93":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\n    -O \/tmp\/glove.6B.100d.txt","aaf01939":"embeddings_index = {};\nwith open('\/tmp\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:],dtype='float32')\n        embeddings_index[word] = coefs\n    ","b4f8ad33":"# creating embedding matrix\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","b259f0be":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_len,weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n    #return_sequences: will ensure output of first LSTM layer matches next\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","2dac0220":"num_epochs = 20\nhistory = model.fit(seq_padd_train, train_y , epochs=num_epochs, validation_data=(seq_padd_test,test_y))","d5a901fa":"train_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(num_epochs)\n\nplt.subplot(2,1,1)\nplt.plot(epochs,train_accuracy)\nplt.plot(epochs,val_accuracy)\nplt.legend(['train_acc','val_acc'])\nplt.title('Accuracy')\nplt.show()\n\nplt.subplot(2,1,2)\nplt.plot(epochs,train_loss)\nplt.plot(epochs,val_loss)\nplt.legend(['train_loss','val_loss'])\nplt.title('Loss')\nplt.show()\n","4cb48041":"As next step we will load the file using pandas asnd filter necessery columns","2a8273a0":"Now we will split the data to train and test","f89ea295":"The model is overfitting.You can try out various methods to overcome it.\n","c58d2cde":"Now we will follow the following steps: <br>\n1. Tokenize the sentances.\n2. Convert it to sequence.\n3. Padding the sequences.","7f1ac4d7":"Now we will use transfer learning for creating an embedding matrix.<br>\n1. Download pretrained glove file\n2. Create embedding dict\n3. Create embedding matrix"}}