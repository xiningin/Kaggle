{"cell_type":{"bcfe8639":"code","0b3d4c3c":"code","f5791cba":"code","b7b0f39f":"code","ef410af7":"code","3ff09075":"code","2d67bede":"code","aafef3c5":"code","cd631279":"code","0b03a1c9":"code","0b5d4273":"code","f1677692":"code","61d2aaaa":"code","d659055c":"code","8b3a4ffe":"code","72d47c99":"code","081cee3d":"code","3e0ffbe3":"code","ca2df97a":"code","ade95734":"code","e7d1471e":"code","319c2b25":"code","0f561c78":"code","9971d51c":"markdown","c5d88837":"markdown","d226631b":"markdown","a2dd8bcb":"markdown","5773548e":"markdown","4fe2bfb7":"markdown","80ee3508":"markdown","a72ada50":"markdown"},"source":{"bcfe8639":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb \n\nfrom sklearn.metrics import accuracy_score\n\nimport joblib   # save and load ML models\nimport gc       # garbage collection\n\n","0b3d4c3c":"train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntrain.set_index(['SK_ID_CURR'], inplace = True)\ntrain.shape","f5791cba":"#convert catergorical festures to cat\ncat_cols = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', \n            'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', \n            'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n            'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', \n            'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', \n            'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']\n\ntrain[cat_cols] = train[cat_cols].astype('category')","b7b0f39f":"train.info(max_cols = 125)","ef410af7":"#load bureau_balance and bureau into memory\nbureau_bal = pd.read_csv('..\/input\/home-credit-default-risk\/bureau_balance.csv')\nbureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\n\nbb = pd.merge(bureau, bureau_bal, on = 'SK_ID_BUREAU', how = 'left')","3ff09075":"#feature engineering\nbb['REMAIN_CRED'] = bb['AMT_CREDIT_SUM'] - bb['AMT_CREDIT_SUM_DEBT'] - bb['AMT_CREDIT_SUM_LIMIT']\nbb['AC_RATIO'] = bb['AMT_ANNUITY'] \/ bb['AMT_CREDIT_SUM'] \n\n#add prefix to bureau columns\nbb.columns = ['BU_'+column if column != ('SK_ID_CURR') \n                       else column for column in bb.columns]\n\n#group categorical features in bureau\nbur_cat = pd.get_dummies(bb.select_dtypes('object'))\nbur_cat['SK_ID_CURR'] = bb['SK_ID_CURR']\nbur_cat = bur_cat.groupby(by = ['SK_ID_CURR']).agg(['mean'])\n  \n#group numerical features    \nbur_num = bb.groupby(by = ['SK_ID_CURR']).agg(['max', 'mean', 'sum']).astype('float32')\n\n# merge cat and num columns\nbureau_rev = bur_cat.merge(bur_num, on = ['SK_ID_CURR'], how = 'left')\n\n#merge bureau_rev and train\ntrain = train.merge(bureau_rev, on = ['SK_ID_CURR'], how = 'left')\n\n#remove unneeded datasets from memory\ndel bur_cat\ndel bur_num\ndel bureau\ndel bureau_bal\n\ngc.collect()","2d67bede":"#load data into memory\ncc_bal = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv')\n\n#feature engineering\ncc_bal['DRAW_RATIO'] = cc_bal['AMT_DRAWINGS_CURRENT'] \/ cc_bal['CNT_DRAWINGS_CURRENT']\ncc_bal['RECEIVE_RATIO'] = cc_bal['AMT_RECIVABLE'] \/ cc_bal['AMT_RECEIVABLE_PRINCIPAL']\ncc_bal['RECEIVE_PER'] = cc_bal['AMT_RECIVABLE'] \/ cc_bal['AMT_TOTAL_RECEIVABLE']\n\n\n#create prefix for columns\ncc_bal.columns = ['CC_'+ column if column !='SK_ID_CURR' \n                  else column for column in cc_bal.columns]\n\n#group categorical features by SK_ID_CURR\ncc_cat = pd.get_dummies(cc_bal.select_dtypes('object'))\ncc_cat['SK_ID_CURR'] = cc_bal['SK_ID_CURR']\ncc_cat = cc_cat.groupby(by = ['SK_ID_CURR']).mean()\n\n#group numerical features in credit card balance by SK_ID_CURR\ncc_num = cc_bal.groupby(by = ['SK_ID_CURR']).agg(['max', 'mean', 'sum']).astype('float32')\n\ntrain = train.merge(cc_cat, on = ['SK_ID_CURR'], how = 'left')\ntrain = train.merge(cc_num, on = ['SK_ID_CURR'], how = 'left')\n\ndel cc_bal\ndel cc_cat\ndel cc_num\ngc.collect()\n","aafef3c5":"#load installments_payments into memory\ninstall = pd.read_csv('..\/input\/home-credit-default-risk\/installments_payments.csv')\n\n#feature engineering\ninstall['PAY_PERCENT'] = install['AMT_INSTALMENT'] \/ install['AMT_PAYMENT']\ninstall['PAY_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n\ninstall['DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\ninstall['DPD'] = install['DPD'].apply(lambda x: x if x>0 else 0)\n\ninstall['DBD'] = install['DAYS_INSTALMENT'] - install['DAYS_ENTRY_PAYMENT']\ninstall['DBD'] = install['DBD'].apply(lambda x: x if x>0 else 0)\n\n#create prefix\ninstall.columns = ['IP_'+ column if column !='SK_ID_CURR' \n                   else column for column in install.columns]  \n\n\n#group numeric features (no cat features in install)\ninst_num = install.groupby(by = ['SK_ID_CURR']).agg(['max', 'mean']).astype('float32')\n\n#merge install with prev\ntrain = train.merge(inst_num, on = 'SK_ID_CURR', how='left')\n\ndel install\ndel inst_num\ngc.collect()","cd631279":"#load POS_CASH into memory\npos = pd.read_csv('..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\n\n#create prefix\npos.columns = ['PC_'+ column if column !='SK_ID_CURR' \n                   else column for column in pos.columns]\n\n#group numeric features (no cat features in install)\npos_num = pos.groupby(by = ['SK_ID_CURR']).agg(['max', 'mean', 'sum']).astype('float32')\n\ntrain = train.merge(pos_num, on = ['SK_ID_CURR'], how = 'left')\n\ndel pos\ndel pos_num\ngc.collect()\n","0b03a1c9":"#load data\nprev = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')\n\n#feature engineering\nprev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace = True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace = True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace = True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace = True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace = True)\n\nprev['AppCred_RATIO'] = prev['AMT_APPLICATION'] \/ (prev['AMT_CREDIT'] + 1)\nprev['AppGoods_RATIO'] = prev['AMT_APPLICATION'] \/ (prev['AMT_GOODS_PRICE'] + 1)\nprev['AnnCred_RATIO'] = prev['AMT_ANNUITY'] \/ (prev['AMT_CREDIT'] + 1)\nprev['CredGoods_RATIO'] = prev['AMT_CREDIT'] \/ (prev['AMT_GOODS_PRICE'] + 1)\n\n\n#calculate APR and add it as a feature\ndef calc_rate(row):\n    return np.rate(row['CNT_PAYMENT'], -row['AMT_ANNUITY'], row['AMT_CREDIT'], 0, guess = 0.05, maxiter = 10)\n\nprev['CALC_RATE'] = prev.apply(calc_rate, axis=1)\n\n\n#Remove unnecessary features\np_dels = ['RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED']\nprev = prev.drop(prev[p_dels], axis = 1)\n\n#create prefix\nprev.columns = ['PR_'+ column if column != 'SK_ID_CURR' \n                else column for column in prev.columns]\n\n#group categorical features in previous_application\nprev_cat = pd.get_dummies(prev.select_dtypes('object'))\nprev_cat['SK_ID_CURR'] = prev['SK_ID_CURR']\nprev_cat = prev_cat.groupby(by = ['SK_ID_CURR']).agg(['mean'])\n\n#group numeric features\nprev_num = prev.groupby(by = ['SK_ID_CURR']).agg(['max', 'mean', 'sum']).astype('float32')\n\n#combine previous_application categorical and numeric features\nprev_rev = prev_num.merge(prev_cat, on = ['SK_ID_CURR'], how = 'left')\n\n#merge revised previous_application features into training dataset\ntrain = train.merge(prev_rev, on = ['SK_ID_CURR'], how = 'left')\n\ndel prev_rev\ndel prev_cat\ndel prev_num\ngc.collect()","0b5d4273":"#replace 365243 in days employed with nan\ntrain['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n\n#set max income to 2.5 million\ntrain = train[train['AMT_INCOME_TOTAL'] < 2500000]\n\n#convert age to years\ntrain['AGE'] = train['DAYS_BIRTH'] \/ - 365\n\n#create avg of each row of EXIT_SOURCE values\ntrain['AVG_EXT'] = train.iloc[:, 41:44].sum(axis=1)\/(3- train.iloc[:,41:44].isnull().sum(axis=1))\ntrain.EXT_SOURCE_1.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_2.fillna(train.AVG_EXT, inplace=True)\ntrain.EXT_SOURCE_3.fillna(train.AVG_EXT, inplace=True)\n\n","f1677692":"#percentage of days employed \ntrain['EmpAge_RATIO'] = train['DAYS_EMPLOYED'] \/ train['AGE']\n\n#create credit\/income ratio \ntrain['CredInc_RATIO'] = train['AMT_CREDIT'] \/ train['AMT_INCOME_TOTAL']\n\n#create annuity to income ration\ntrain['AnnInc_RATIO'] = train['AMT_ANNUITY'] \/ train['AMT_INCOME_TOTAL']\n\n#create credit\/annuity ratio \ntrain['AnnCred_RATIO'] = train['AMT_ANNUITY'] \/ (train['AMT_CREDIT'] + 1)\n\n#create credit\/cost of goods ratio feature\ntrain['CredGoods_RATIO'] = train['AMT_CREDIT'] \/ (train['AMT_GOODS_PRICE'] + 1)\n\n\ntrain['AVG_EXT_INCOME'] = train['AMT_INCOME_TOTAL'] * train['AVG_EXT']\ntrain['AVG_EXT_GOODS'] = train['AMT_GOODS_PRICE'] * train['AVG_EXT']\n","61d2aaaa":"dels = ['APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', \n        'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', \n        'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', \n        'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', \n        'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', \n        'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI',\n        'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', \n        'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', \n        'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE', \n        'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'FLAG_DOCUMENT_2', \n        'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n        'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', \n        'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n        'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n        'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', \n        'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n        'FLAG_DOCUMENT_21', 'DAYS_BIRTH', 'LIVINGAPARTMENTS_AVG', \n        'LIVINGAREA_AVG', 'CNT_FAM_MEMBERS',  'OBS_30_CNT_SOCIAL_CIRCLE',\n        'OBS_60_CNT_SOCIAL_CIRCLE', 'ELEVATORS_AVG', 'AVG_EXT']\n\n\ntrain = train.drop(train[dels], axis =1)\ngc.collect()\n","d659055c":"train_noTARGET= train.loc[:, train.columns != 'TARGET']\n\nnum_feat = train_noTARGET.select_dtypes(include=np.number).columns.tolist()\ncat_feat = train.select_dtypes(('object', 'category')).columns.tolist()\n\nfeatures = num_feat + cat_feat\n\nprint(features)","8b3a4ffe":"train = train.replace([np.inf, -np.inf], np.nan)","72d47c99":"#create a Pipeline for processing the num_feat\nnum_pipe = Pipeline(\n    steps = [('imputer', SimpleImputer(strategy = 'median')),\n           ('scaler', StandardScaler())\n    ])\n\n\n#create a Pipeline for processing the cat_feat\ncat_pipe = Pipeline(\n    steps = [('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n           ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])","081cee3d":"#create a ColumnTransformer that combines the two pipelines\npreprocessor = ColumnTransformer(\n    transformers = [('num', num_pipe, num_feat),\n                    ('cat', cat_pipe, cat_feat)\n    ])","3e0ffbe3":"preprocessor.fit(train[features])","ca2df97a":"train30k = train.sample(frac=0.10, replace=False, random_state=1)","ade95734":"#define y_train, apply the fitted preprocessor to the training data\ny_train = train30k['TARGET'].values\nX_train = preprocessor.transform(train30k[features])","e7d1471e":"print('Shape of features: ', X_train.shape)\nprint('Shape of target: ', y_train.shape)","319c2b25":"%%time\n\nLGBM_clf = lgb.LGBMClassifier(boosting_type = 'gbdt',objective = 'binary',\n                              n_estimators = 5000, num_leaves = 35, \n                              subsample = 0.87, colsample_bytree = 0.94, \n                              silent = -1, verbose = -1)\n\nLGBM_parameters = {\n    'max_depth': range (1, 3, 8),\n    'learning_rate': [0.01, 0.02, 0.03], \n    'metric' : ['auc']\n}\n\n\nLGBM_grid = GridSearchCV(LGBM_clf, LGBM_parameters, cv=10, n_jobs=10, \n                         verbose=True, scoring= 'roc_auc')\nLGBM_grid.fit(X_train, y_train)\n\n\nLGBM_model = LGBM_grid.best_estimator_\n\nprint('Best Parameters:', LGBM_grid.best_params_)\nprint('Best CV Score:  ', LGBM_grid.best_score_)\nprint('Training Acc:   ', LGBM_model.score(X_train, y_train))","0f561c78":"#save the pipeline and best model to a file \njoblib.dump(preprocessor, 'wk6default_preprocessor.joblib')\n\nLGBM_model = lgb.LGBMClassifier(boosting_type = 'gbdt',objective = 'binary',\n                                n_estimators = 5000, num_leaves = 35, \n                                subsample = 0.87, colsample_bytree = 0.94, \n                                silent = -1, verbose = -1,\n                                learning_rate = 0.02, max_depth = 1, metric = 'auc')\n\nLGBM_model.fit(X_train, y_train)\njoblib.dump(LGBM_model, 'wk6_LGBM_default_model.joblib')","9971d51c":"#### Merge Credit_Card_Balance with Training Dataset","c5d88837":"#### Merge POS_CASH into Training data","d226631b":"#### Merge Previous_Application with Training Dataset","a2dd8bcb":"#### Merge Bureau and Bureau_Balance with Training Data","5773548e":"## Build pipelines","4fe2bfb7":"#### Merge Installments with Training Dataset","80ee3508":"#### Added features to improve predictive power\n\n* The percentage of days employed - Does length of employment predict ability to keep paying off a loan?\n* Available credit to income ratio - Does the amount of credit available as a percentage of income predict ability to pay off a loan?\n* Annuity to income ratio - Does receiving annuity predict ability to pay off a loan?\n* Annuity as a percentage of available credit -  Does annuity compared to credit availability predict ability to pay off a loan?\n* Cost of goods to credit ratio - Does how much was financed vs how much was paid for goods predict ability to pay off the loan?","a72ada50":"## Combining additional tables and performing feature engineering to improve model performance"}}