{"cell_type":{"0d67eac6":"code","b15b777d":"code","56149cf1":"code","a2bae428":"code","28b95db0":"code","13bc8d79":"code","cf825af0":"code","4e8cae98":"code","4814b146":"code","32b852cf":"code","dc6ca369":"code","1c1898a7":"code","caa13fee":"code","f086bf24":"code","2fd4be90":"code","b9370ce7":"code","74bd58c1":"code","3157573d":"code","ae2461e0":"code","f61d5307":"code","158d295b":"code","591bfbde":"code","aa48d571":"code","500e92ca":"code","6a7831a2":"code","0c1b9a14":"code","6a97b55c":"code","538414bd":"code","b8f2b2ae":"code","480cf143":"code","80c41dbc":"code","a53429b8":"code","b6b05d3b":"code","b847a10b":"code","dc63cdd2":"code","425d4c2f":"code","b660ffd7":"code","6f54e808":"code","4f6eadea":"code","8e430e9a":"code","b742e15b":"code","bd76e6fe":"code","30f2f35d":"code","d279b871":"code","d9538d1d":"code","b96e6103":"code","82f877d5":"code","3d146946":"code","732fd725":"code","29f47dca":"code","d247ddea":"code","e48e9036":"code","c288c038":"code","e17775e6":"code","5387f35e":"code","de15227d":"code","cfa2a00a":"code","2abff6c8":"code","d7179e4f":"code","10a45626":"code","e08cfd7e":"code","1c4e8f73":"code","8e8ae48a":"code","20ebcf26":"code","77a162b4":"code","948535e7":"code","fb7d2bdd":"code","bf346fd5":"code","97559cd0":"code","aceca161":"code","51e24531":"code","582edfa2":"code","6af5995e":"code","319ec30d":"code","6f8182c7":"code","e4ed3e23":"code","61201041":"code","5eb8def6":"code","4eaae24d":"code","9167e04c":"code","b38b55da":"code","731e9ab0":"code","a2d2a85a":"markdown","aba61bce":"markdown","e440617c":"markdown","ac0999d9":"markdown","68c9fb9c":"markdown","8e2c210a":"markdown","3fec5dd7":"markdown","615f0d9d":"markdown","1c308b1b":"markdown","d119da2e":"markdown","9f7a2e86":"markdown","82b83c35":"markdown","86e4f475":"markdown","7fb89067":"markdown","84be8c24":"markdown","95783379":"markdown","961460fa":"markdown","537e2142":"markdown","d4b09497":"markdown","2ff243f7":"markdown","fd659f46":"markdown","33b83552":"markdown","92bc3839":"markdown","63f06d4a":"markdown","273165a6":"markdown","2d423d79":"markdown"},"source":{"0d67eac6":"%load_ext autoreload\n%autoreload 2","b15b777d":"%matplotlib inline\n\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics","56149cf1":"set_plot_sizes(12,14,16)","a2bae428":"PATH = \"..\/input\/\"\n\ndf_raw = pd.read_feather('..\/input\/fast-ai-machine-learning-lesson-1\/tmp\/bulldozers-raw')\ndf_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')","28b95db0":"def split_vals(a,n): return a[:n], a[n:]\nn_valid = 12000\nn_trn = len(df_trn)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)","13bc8d79":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","cf825af0":"df_raw","4e8cae98":"set_rf_samples(50000)","4814b146":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","32b852cf":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\nnp.mean(preds[:,0]), np.std(preds[:,0])","dc6ca369":"def get_preds(t): return t.predict(X_valid)\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:,0]), np.std(preds[:,0])","1c1898a7":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh();","caa13fee":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\nenc_summ","f086bf24":"enc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\nenc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0,11));","2fd4be90":"enc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0,11));","b9370ce7":"raw_valid.ProductSize.value_counts().plot.barh();","74bd58c1":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby(flds[0]).mean()\nsumm","3157573d":"(summ.pred_std\/summ.pred).sort_values(ascending=False)","ae2461e0":"fi = rf_feat_importance(m, df_trn); fi[:10]","f61d5307":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","158d295b":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","591bfbde":"plot_fi(fi[:30]);","aa48d571":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)","500e92ca":"df_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","6a7831a2":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","0c1b9a14":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi);","6a97b55c":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","538414bd":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25]);","b8f2b2ae":"from scipy.cluster import hierarchy as hc","480cf143":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","80c41dbc":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","a53429b8":"get_oob(df_keep)","b6b05d3b":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","b847a10b":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","dc63cdd2":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","425d4c2f":"os.makedirs('tmp', exist_ok=True)\nnp.save('tmp\/keep_cols.npy', np.array(df_keep.columns))","b660ffd7":"keep_cols = np.load('tmp\/keep_cols.npy')\ndf_keep = df_trn[keep_cols]","6f54e808":"reset_rf_samples()","4f6eadea":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","8e430e9a":"from pdpbox import pdp\nfrom plotnine import *","b742e15b":"set_rf_samples(50000)","bd76e6fe":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train);","30f2f35d":"plot_fi(rf_feat_importance(m, df_trn2)[:10]);","d279b871":"df_raw.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01, figsize=(10,8));","d9538d1d":"x_all = get_sample(df_raw[df_raw.YearMade>1930], 500)","b96e6103":"!pip install scikit-misc","82f877d5":"ggplot(x_all, aes('YearMade', 'SalePrice'))+stat_smooth(se=True, method='loess')","3d146946":"x = get_sample(X_train[X_train.YearMade>1930], 500)","732fd725":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, x, x.columns, feat)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n                        cluster=clusters is not None,\n                        n_cluster_centers=clusters)","29f47dca":"plot_pdp('YearMade')","d247ddea":"plot_pdp('YearMade', clusters=5)","e48e9036":"feats = ['saleElapsed', 'YearMade']\np = pdp.pdp_interact(m, x, x.columns, feats)\npdp.pdp_interact_plot(p, feats)","c288c038":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')","e17775e6":"df_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear-df_raw.YearMade","5387f35e":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep));","de15227d":"!pip install treeinterpreter","cfa2a00a":"from treeinterpreter import treeinterpreter as ti","2abff6c8":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","d7179e4f":"row = X_valid.values[None,0]; row","10a45626":"prediction, bias, contributions = ti.predict(m, row)","e08cfd7e":"prediction[0], bias[0]","1c4e8f73":"idxs = np.argsort(contributions[0])","8e8ae48a":"[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]","20ebcf26":"contributions[0].sum()","77a162b4":"df_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')","948535e7":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","fb7d2bdd":"fi = rf_feat_importance(m, x); fi[:10]","bf346fd5":"feats=['SalesID', 'saleElapsed', 'MachineID']","97559cd0":"(X_train[feats]\/1000).describe()","aceca161":"(X_valid[feats]\/1000).describe()","51e24531":"x.drop(feats, axis=1, inplace=True)","582edfa2":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","6af5995e":"fi = rf_feat_importance(m, x); fi[:10]","319ec30d":"set_rf_samples(50000)","6f8182c7":"feats=['SalesID', 'saleElapsed', 'MachineID', 'age', 'YearMade', 'saleDayofyear']","e4ed3e23":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","61201041":"for f in feats:\n    df_subs = df_keep.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)","5eb8def6":"reset_rf_samples()","4eaae24d":"df_subs = df_keep.drop(['SalesID', 'MachineID', 'saleDayofyear'], axis=1)\nX_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","9167e04c":"plot_fi(rf_feat_importance(m, X_train));","b38b55da":"np.save('tmp\/subs_cols.npy', np.array(df_subs.columns))","731e9ab0":"m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","a2d2a85a":"One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features.","aba61bce":"# Extrapolation","e440617c":"We saw how the model averages predictions across the trees to get an estimate - but how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions, instead of just the mean. This tells us the *relative* confidence of predictions - that is, for rows where the trees give very different results, you would want to be more cautious of using those results, compared to cases where they are more consistent. Using the same example as in the last lesson when we looked at bagging:","ac0999d9":"When we use python to loop through trees like this, we're calculating each in series, which is slow! We can use parallel processing to speed things up:","68c9fb9c":"# Removing redundant features","8e2c210a":"Now we try removing each variable one at a time.","3fec5dd7":"## One-hot encoding","615f0d9d":"# Confidence based on tree variance","1c308b1b":"For model interpretation, there's no need to use the full dataset on each tree - using a subset will be both faster, and also provide better interpretability (since an overfit model will not provide much variance across trees).","d119da2e":"# Our final model!","9f7a2e86":"Looking good! Let's use this dataframe from here. We'll save the list of columns so we can reuse it later.","82b83c35":"We can see that different trees are giving different estimates this this auction. In order to see how prediction confidence varies, we can add this into our dataset.","86e4f475":"# Random Forest Model interpretation","7fb89067":"## Load in our data from last lesson","84be8c24":"# Feature importance","95783379":"It's not normally enough to just to know that a model can make accurate predictions - we also want to know *how* it's making predictions. The most important way to see this is with *feature importance*.","961460fa":"This is a copy of lesson 2 notebook from fast.ai course Introduction to Machine Learning for Coders. It was modified in the data input only, so that i can run on Kaggle kernels.\n\nSource: https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/ml1\/lesson2-rf_interpretation.ipynb\n\nBased on notebook version: 186739f\n\nCourse page: http:\/\/course.fast.ai\/ml.html","537e2142":"Here's our baseline.","d4b09497":"Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.","2ff243f7":"And let's see how this model looks on the full dataset.","fd659f46":"# Partial dependence","33b83552":"This next analysis will be a little easier if we use the 1-hot encoded categorical variables, so let's load them up again.","92bc3839":"*Question*: Why are the predictions nearly exactly right, but the error bars are quite wide?","63f06d4a":"proc_df's optional *max_n_cat* argument will turn some categorical variables into new columns.\n\nFor example, the column **ProductSize** which has 6 categories:\n\n* Large\n* Large \/ Medium\n* Medium\n* Compact\n* Small\n* Mini\n\ngets turned into 6 new columns:\n\n* ProductSize_Large\n* ProductSize_Large \/ Medium\n* ProductSize_Medium\n* ProductSize_Compact\n* ProductSize_Small\n* ProductSize_Mini\n\nand the column **ProductSize** gets removed.\n\nIt will only happen to columns whose number of categories is no bigger than the value of the *max_n_cat* argument.\n\nNow some of these new columns may prove to have more important features than in the earlier situation, where all categories were in one column.","273165a6":"# Tree interpreter","2d423d79":"It looks like we can try one from each group for removal. Let's see what that does."}}