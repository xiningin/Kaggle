{"cell_type":{"e9ded481":"code","8fade1c2":"code","50a99cd0":"code","719d83d2":"code","9fa2afe2":"code","bef20bb2":"code","0082e399":"code","efc476e1":"code","8dc7beca":"code","d8f2e4ef":"code","491e0f98":"code","dd20b5f8":"code","8b4b78e7":"code","fa78faf0":"code","9abc814e":"code","ef6650b5":"code","5212bd18":"code","5c8cced1":"code","6b3f72f1":"code","81b7995a":"code","465da091":"code","2168a55d":"code","b32904cd":"code","91b4b963":"code","51df376d":"code","445ff57a":"code","cc1dd86a":"code","8fb078b5":"markdown","7b839c03":"markdown","49c43ee0":"markdown","54952331":"markdown","9a1c6247":"markdown","61b5ec2b":"markdown","77e70c4d":"markdown","ef7ba802":"markdown","d830d840":"markdown","117de5c8":"markdown","50d4c31e":"markdown","617870f4":"markdown"},"source":{"e9ded481":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# Model libraries\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import r2_score\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\n#visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#model import XGB\nfrom xgboost import XGBRegressor\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fade1c2":"data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata.head()","50a99cd0":"#vemos el tipo de dato de las columnas\ndata.info()","719d83d2":"\nf, ax = plt.subplots(figsize=(23, 23))\nsns.heatmap(data.corr(),annot=True, fmt=\".2f\", linewidths=.5, ax=ax)\nplt.show()","9fa2afe2":"null_columns_count = data.isnull().sum()[data.isnull().sum() > 0]\ntotal_rows_of_null_columns = data.isnull().count()[data.isnull().sum() > 0]\nnull_percentage = (null_columns_count\/total_rows_of_null_columns)*float(100)\nmissing_data = pd.concat([null_columns_count, null_percentage], axis=1, keys=['Total', 'Percentage'])\nnull_percentage.sort_values(ascending = False)\n","bef20bb2":"#Quitamos las columnas que tengan mas del 50% dde valores nulos\ndata_clean = data.drop((missing_data[missing_data['Percentage'] >50]).index,1)\ndata_clean1 = data_clean\n","0082e399":"null_columns_count = data_clean.isnull().sum()[data_clean.isnull().sum() > 0]\ntotal_rows_of_null_columns = data_clean.isnull().count()[data_clean.isnull().sum() > 0]\nnull_percentage = (null_columns_count\/total_rows_of_null_columns)*float(100)\n\nnull_percentage.sort_values(ascending = False)","efc476e1":"#Cambiamos los valores nulos por 0\ndata_clean.fillna(0, inplace=True)","8dc7beca":"#Remplazamos los valores de las columnas objetos por enteros para que entren al modelo\nfor col in data_clean.columns:\n    if data_clean[col].dtypes == object:     \n        for num, v in enumerate(data_clean[col].unique()):\n            data_clean = data_clean.replace({col: {v : num}})\n            \n            \n#Desp\u00faes de asignale un numero entero podemos quitar las columnas que no son muy significativas con respecto a (SalePrice)\n#Utilizamos la correlaci\u00f3n como medida.\nfor col in data_clean.columns:\n    correlacion = data_clean['SalePrice'].corr(data_clean[col])\n    #Si la correlaci\u00f3n entre la columna y \"SalePrice\" es menor a 0.05, eliminamos la columna. \n    if correlacion < 0.05:\n        del data_clean[col]\ndata_clean.head()","d8f2e4ef":"for col in data_clean:\n    data_clean[col].plot.hist()\n    plt.suptitle(col)\n    plt.show()","491e0f98":"#Primero definiremos nuestros datos de entrenamiento y de prueba.\nmodel = []\nscore = []\n\nx_train, x_test, y_train, y_test = train_test_split(data_clean.drop([\"SalePrice\"],axis=1),data[\"SalePrice\"],test_size=0.2,random_state=0)\n\nprint(\"X Train Shape\", x_train.shape)\nprint(\"Y Train Shape\", y_train.shape)\nprint(\"X Test Shape\", x_test.shape)\nprint(\"Y Test Shape\", y_test.shape)","dd20b5f8":"#Regresi\u00f3n lineal\nlinear_model = LinearRegression()\nlinear_model.fit(x_train,y_train)\nlinear_model_predict = linear_model.predict(x_test)\nprint(\"Score: \",r2_score(linear_model_predict,y_test))\nmodel.append(\"Multi Linear Regression\")\nscore.append(r2_score(linear_model_predict,y_test))","8b4b78e7":"#Arbol de desiciones\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x_train,y_train)\ntree_reg_predict = tree_reg.predict(x_test)\nprint(\"Score: \",r2_score(tree_reg_predict,y_test))\nmodel.append(\"Decision Tree Regression\")\nscore.append(r2_score(tree_reg_predict,y_test))","fa78faf0":"#\nlasso_model = Lasso()\nlasso_model.fit(x_train,y_train)\nlasso_model_predict = lasso_model.predict(x_test)\nprint(\"Score: \",r2_score(lasso_model_predict,y_test))\nmodel.append(\"Lasso Regression\")\nscore.append(r2_score(lasso_model_predict,y_test))","9abc814e":"#Red elastica.\nelasticnet_model = ElasticNet()\nelasticnet_model.fit(x_train,y_train)\nelasticnet_model_predict = elasticnet_model.predict(x_test)\nprint(\"Score: \",r2_score(elasticnet_model_predict,y_test))\nmodel.append(\"Elastic Net Regression\")\nscore.append(r2_score(elasticnet_model_predict,y_test))","ef6650b5":"#Random Forest\nreg = RandomForestRegressor(n_estimators=100, random_state = 42)\nreg.fit(x_train,y_train)\nreg_predict = reg.predict(x_test)\nprint(\"Score: \",r2_score(reg_predict,y_test))\nmodel.append(\"Random Forest Regression\")\nscore.append(r2_score(reg_predict,y_test))","5212bd18":"#Ada boost\nreg_ada = AdaBoostRegressor(random_state=0, n_estimators=5)\nreg_ada.fit(x_train,y_train)\nreg_ada_predict = reg_ada.predict(x_test)\nprint(\"Score: \",r2_score(reg_ada_predict,y_test))\nmodel.append(\"Ada Boost Regression\")\nscore.append(r2_score(reg_ada_predict,y_test))","5c8cced1":"#Gradient boost\nreg_gb = GradientBoostingRegressor(\n    n_estimators = 51\n)\nreg_gb.fit(x_train,y_train)\nreg_gb_predict = reg_gb.predict(x_test)\nprint(\"Score: \",r2_score(reg_gb_predict,y_test))\nmodel.append(\"Gradient Boosting Regression\")\nscore.append(r2_score(reg_gb_predict,y_test))\n","6b3f72f1":"#XGB\nmodel_params = {}\nreg_xgb = XGBRegressor(**model_params)\nreg_xgb.fit(x_train,y_train)\nreg_xgb_predict = reg_xgb.predict(x_test)\nprint(\"Score: \",r2_score(reg_xgb_predict,y_test))\nmodel.append(\"XGBoost Regression\")\nscore.append(r2_score(reg_xgb_predict,y_test))","81b7995a":"plt.subplots(figsize=(15, 5))\nsns.barplot(x=score,y=model,palette = sns.cubehelix_palette(len(score)))\nplt.xlabel(\"Score\")\nplt.ylabel(\"Regression\")\nplt.title('Regression Score')\nplt.show()","465da091":"datatest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nids = datatest['Id']","2168a55d":"# Tratamos los valores nulos\nnull_columns_count = datatest.isnull().sum()[data.isnull().sum() > 0]\ntotal_rows_of_null_columns = data.isnull().count()[data.isnull().sum() > 0]\nnull_percentage = (null_columns_count\/total_rows_of_null_columns)*float(100)\nmissing_data = pd.concat([null_columns_count, null_percentage], axis=1, keys=['Total', 'Percentage'])\nnull_percentage.sort_values(ascending = False)","b32904cd":"#Quitamos las columnas que tengan mas del 50% dde valores nulos\ndatatest = datatest.drop((missing_data[missing_data['Percentage'] >50]).index,1)\n","91b4b963":"#Cambiamos los valores nulos por 0\ndatatest.fillna(0, inplace=True)","51df376d":"\n#Remplazamos los valores de las columnas objetos por enteros para que entren al modelo\nfor col in datatest.columns:\n    if datatest[col].dtypes == object:\n        num = 0\n        for v in datatest[col].unique():\n            datatest = datatest.replace({col: {v : num}})\n            num += 1\n           \nfor col in data_clean1.columns:\n    if data_clean1[col].dtypes == object:\n        num = 0\n        for v in data_clean1[col].unique():\n            data_clean1 = data_clean1.replace({col: {v : num}})\n            num += 1\n            \n#Desp\u00faes de asignale un numero entero podemos quitar las columnas que no son muy significativas con respecto a (SalePrice)\n#Utilizamos la correlaci\u00f3n como medida.\nfor col in data_clean1.columns:\n        correlacion = data_clean1['SalePrice'].corr(data_clean1[col])\n        if correlacion < 0.05: \n             del datatest[col]\n\n","445ff57a":"datatest.head()","cc1dd86a":"#Hacemos la prediccion con el modelo elegido (Gradient Boosting)\npredict = reg_gb.predict(datatest)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'Id' : ids, 'SalePrice': predict})\n\noutput.to_csv('submission.csv', index=False)\n","8fb078b5":"# Leemos los datos para la competici\u00f3n (test.csv)","7b839c03":"## Empezamos a tratar los datos","49c43ee0":"# Calculamos el porcentaje de nulos de las columnas","54952331":" # Hacemos una grafica de correlaci\u00f3n para darnos una idea de la relaci\u00f3n de la columa \"SalePrice\" con las demas columnas.","9a1c6247":"# Creamos nuestro modelo.","61b5ec2b":"# Probraremos disntintos tipos de regresi\u00f3n para hacer las predicciones:\n","77e70c4d":"Podemos observar que para nuestro modelo, obtenemos mejores resultados con el algoritmo de Gradient Bossting, por lo que es el que usaremos para predecir nuestros resultados.","ef7ba802":"> # Graficaremos los puntajes obtenido (r2_score) para comparar los algoritmos.","d830d840":"# Aplicaremos el mismo tratamiento a los datos ","117de5c8":"# Realizamos un histograma para cada columna para darnos una idea de la distribuci\u00f3n de las variables.","50d4c31e":"# Para medir nuestro aprendizajes usamos coeficiente de determinaci\u00f3n (r2 score) con cada algoritmo de regresi\u00f3n.\nEl coeficiente de determinaci\u00f3n (r2_score), determina la capacidad de un modelo para predecir futuros resultados. El mejor resultado posible es 1.0, y ocurre cuando la predicci\u00f3n coincide con los valores de la variable objetivo. R2 puede tomar valores negativos pues la predicci\u00f3n puede ser arbitrariamente mala. Cuando la predicci\u00f3n coincide con la esperanza de los valores de la variable objetivo, el resultado de R2 es 0.","617870f4":"# Leemos los datos :)"}}