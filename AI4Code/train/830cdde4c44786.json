{"cell_type":{"7e38df07":"code","10b93fa3":"code","4d216e24":"code","b7f7820d":"code","f1a91c6e":"code","76494605":"code","09ba0487":"code","873c8a4f":"code","06b22d0f":"code","9ca767da":"code","cd93788d":"code","4d14e911":"code","197bad71":"code","4aa6aa35":"code","a18e20dd":"code","083c5bdd":"code","0915a8b6":"markdown","0a40ff37":"markdown","ce6846f0":"markdown","5fe0d177":"markdown","7e62c489":"markdown","ade1e1ca":"markdown","8d13b235":"markdown"},"source":{"7e38df07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import eigh as sc_eigh\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')","10b93fa3":"# loading dataset\ntrain = pd.read_csv('..\/input\/train.csv')\nprint(train.head())\n\n#features\ntrain_x = train.iloc[:,1:]\nprint('Training features:\\n', train_x.head())\n\n#label\ntrain_y = train['label']\nprint('Labels:\\n', train_y.head())\n","4d216e24":"# shape of our data:\nprint('Feature\\'s shape:', train_x.shape)\nprint('Label\\'s shape:', train_y.shape)","b7f7820d":"# plotting some number images:\nprint('Some images are : ')\nfig, axes = plt.subplots(nrows = 2, ncols = 2)\n\nnum_img = train_x.iloc[7].values.reshape(28, 28)\naxes[0,0].imshow(num_img, interpolation = 'none')\n\nnum_img2 = train_x.iloc[43].values.reshape(28,28)\naxes[0,1].imshow(num_img2, interpolation = 'none')\n\nnum_img3 = train_x.iloc[34].values.reshape(28,28)\naxes[1,0].imshow(num_img3, interpolation = 'none')\n\nnum_img4 = train_x.iloc[718].values.reshape(28,28)\naxes[1,1].imshow(num_img4, interpolation = 'none')\n\nplt.show()\n","f1a91c6e":"# STEP 1: Data Standardization: mean = 0 , standard deviation = 1\nsc = StandardScaler()\ntrain_x_scaled = sc.fit_transform(train_x)\nprint('Shape of scaled data: ',train_x_scaled.shape)\n","76494605":"# STEP 2: Compute Covariance:\ncovar_train_x = np.cov(train_x_scaled.T) # S = X.T*X\nprint('Shape of our covariance matrix:' ,covar_train_x.shape) # square symmetric matrix","09ba0487":"# calculating eigen values and corresponding eigrn vectors using scipy eigh:\neig_val, eig_vec = sc_eigh(covar_train_x, eigvals = (782,783)) # Only getting top 2 eigen values and corresponding eigen vectors.\nprint('eigen values:\\n',eig_val)\nprint('eigen vectors:\\n',eig_vec.T)\nprint('eigen vectors shape:\\n',eig_vec.T.shape)\nprint('train_x_scaled transposed shape:\\n', train_x_scaled.T.shape)\n","873c8a4f":"# Getting new coordinates:\nnew_values = np.matmul(eig_vec.T, train_x_scaled.T) # matrix multiplication\nprint('New values:\\n', new_values)\nprint('New values shape:\\n', new_values.shape)","06b22d0f":"# Adding labels so we can perfrom visualization based on labels:\nnew_values = np.vstack((new_values, train_y)).T\nprint(new_values)","9ca767da":"# Converting to dataframe:\npca_df = pd.DataFrame(new_values, columns = ['1st principal component', '2nd principal component', 'label'])\nprint(pca_df.head())","cd93788d":"# visualization after dimensionality reduction (PCA):\nsns.FacetGrid(pca_df, hue = 'label', height = 6).map(plt.scatter, '2nd principal component', '1st principal component')\\\n.add_legend()\nplt.show()","4d14e911":"pca = PCA()\npca.n_components = 2 # top 2\nsci_pca = pca.fit_transform(train_x_scaled)\nprint('Shape after reducing dimensionality:', sci_pca.shape)\nprint('pca data:\\n', sci_pca.T)","197bad71":"sci_new_values = np.vstack((sci_pca.T, train_y)).T\nprint(sci_new_values)","4aa6aa35":"# creating the dataframe of our new coordinates:\nsci_pca_df = pd.DataFrame(sci_new_values, columns = ['1st principal component', '2nd principal component', 'label'])\nprint(sci_pca_df.head()) # first 5 rows","a18e20dd":"# Visualization in 2D:\nsns.FacetGrid(sci_pca_df, hue = 'label', height = 6).map(plt.scatter, '1st principal component', '2nd principal component')\\\n.add_legend()\nplt.show()","083c5bdd":"pca.n_components = 784 # considering all the features\npca_dat = pca.fit_transform(train_x_scaled)\n\n# pca.explained_variance_ -  The amount of variance explained by each of the selected components\nvar_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_) \nvar_explained_cummulative = np.cumsum(var_explained)\n\nplt.plot(var_explained_cummulative, linewidth = 3)\nplt.grid()\nplt.ylabel('Cummulative Variance')\nplt.xlabel('number of components')\nplt.show()","0915a8b6":"## IMPORTING DATASET","0a40ff37":"## IMPORTING REQUIRED LIBRARIES","ce6846f0":"## REFERENCES:\n\n1. Applied AI Course : [www.appliedaicourse.com]\n2. Dataset : [https:\/\/www.kaggle.com\/c\/digit-recognizer]\n","5fe0d177":"## APPLYING PCA STEP BY STEP FOR VISUALIZATION","7e62c489":"## VARIANCE EXPLAINATION WITH NO. OF FEATURES","ade1e1ca":"## PCA using scikit learn","8d13b235":"* * In above plot, we can see that more than 90% variance is explained by 350 components which is less than half of the total number of components."}}