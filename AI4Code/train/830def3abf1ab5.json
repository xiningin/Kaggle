{"cell_type":{"87eb33d6":"code","0c800b7e":"code","fe4015f6":"code","d09b0e2d":"code","6d6cc425":"code","544d9edb":"code","13116258":"code","b513d43f":"code","0dbc69b3":"code","c13fb3ba":"code","77630de0":"code","ea571951":"code","1b38e91f":"code","0e46b4ac":"code","c47cec0f":"code","3c1a2dfd":"code","8b3c5b63":"code","16caa7eb":"code","ba5eac44":"code","92e36ed1":"code","a4a82a7d":"code","a567ef16":"code","22f0e819":"code","77c31cf4":"code","412383c0":"code","deeb2536":"code","efeda7f0":"code","edf37f4b":"code","72499f37":"code","80fed106":"code","900a8962":"code","f2cb2d03":"code","3967dc78":"code","3bb53a21":"code","132f6f92":"code","8c207fb8":"code","80d9a008":"code","f45935a6":"code","d186f1b0":"code","f1a60571":"code","a26113e4":"code","d957bb5c":"code","a749cec9":"code","35ac99ea":"code","4ffd3b82":"code","43a0e115":"code","cef1d51e":"code","6f312560":"code","5d210181":"code","cd7f810d":"code","d1de0bde":"code","0ee38943":"code","ae21a760":"code","2bffa2e1":"code","85089adf":"code","f7215cc6":"code","34d07151":"code","da90717d":"markdown","f623de6c":"markdown","577a8db9":"markdown","9907f2ef":"markdown","d9dd5434":"markdown","617f2523":"markdown","e0ede6ff":"markdown","691b275c":"markdown","103a6341":"markdown","bc7f0a67":"markdown","6a9b0236":"markdown","37bfaebc":"markdown","ab6d51c8":"markdown","55c9d4f4":"markdown","e0ef6870":"markdown","3a043a16":"markdown","b5fcb56b":"markdown","57072c62":"markdown","1695adbc":"markdown","5e2d6d4c":"markdown"},"source":{"87eb33d6":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","0c800b7e":"!pip install python-box # \u8f9e\u66f8\u578b\u3092\u3064\u306a\u3044\u3067\u304f\u308c\u308b\u3000ex. model[\"A\"][\"B\"][\"C\"] \u3068\u304b\u3092 model.A.B.C \u3068\u8868\u73fe\u53ef\u80fd\u306b\u3002","fe4015f6":"from box import Box\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule\n\n#from glob import glob\nimport glob\nimport time","d09b0e2d":"config = {'seed': 2021,\n          \n          'n_splits': 5,\n          'epoch': 500,\n          'trainer': {\n              'gpus': 0, # gpu\u3092\u4f7f\u3046\u3068\u304d\u306f1\u306b\n              'accumulate_grad_batches': 1,\n              'progress_bar_refresh_rate': 1,\n              'fast_dev_run': False,\n              'num_sanity_val_steps': 0,\n              'resume_from_checkpoint': None,\n          },\n         \n          'train_loader':{\n              'batch_size': 256,\n              'shuffle': True,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': True,\n          },\n          'val_loader': {\n              'batch_size': 512,\n              'shuffle': False,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': False\n         },\n          \n          'optimizer':{\n              'name': 'optim.Adam',\n              'params':{\n                  'lr': 1e-3\n              },\n          },\n          'scheduler':{\n              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n              'params':{\n                  'T_0': 20,\n                  'eta_min': 7.5e-5,\n              }\n          },\n          'loss': 'nn.CrossEntropyLoss',\n          \n          \"wandbprojectname\":\"Titanicmodel\", # wandb save folder name & project name\n          \n                   \n          \"wandbversion\": \"v1\", # save subfolder name & project version\n          \n          \"comment\":\"PytorchLightning test from kaggle\", # write comment what you want to keep a record on wandb\n          \n          \"mode\":\"train\",\n          \n          \n}\n\nconfig = Box(config)","6d6cc425":"from pytorch_lightning.loggers.wandb import WandbLogger\n\ntry:\n    \n    wlogger = WandbLogger(project=config.wandbprojectname,version=config.wandbversion)\n\n    # \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u4fdd\u5b58\n    wlogger.log_hyperparams(config)\n    \nexcept:\n    \n    wlogger = TensorBoardLogger(config.wandbprojectname)","544d9edb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset,Dataset\n\nimport gc\n\nimport random\n\nimport transformers\nimport warnings\nwarnings.simplefilter('ignore')\n\n#scaler = torch.cuda.amp.GradScaler() # GPU\u3067\u306e\u9ad8\u901f\u5316\u3002\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cpu\u304cgpu\u304b\u3092\u81ea\u52d5\u5224\u65ad\ndevice","13116258":"from sklearn.metrics import roc_curve,roc_auc_score\nimport matplotlib.pyplot as plt","b513d43f":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf","0dbc69b3":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest","c13fb3ba":"df.info()","77630de0":"df.columns","ea571951":"for col in df.columns:\n    print(str(col) + \":\" + str(len(df[col].unique())))","1b38e91f":"df[\"Age\"]","0e46b4ac":"# In order to get the average value, dropna is used to remove all but Nan data.\n# \u5e73\u5747\u5024\u3092\u51fa\u3059\u305f\u3081\u306b\u3001dropna\u3067Nan\u30c7\u30fc\u30bf\u4ee5\u5916\u3092\u629c\u304d\u307e\u3059\u3002\ndf[\"Age\"].dropna()","c47cec0f":"dmean = df[\"Age\"].dropna().mean()\ndmean","3c1a2dfd":"# filling an mean value. : \u5e73\u5747\u5024\u3067\u57cb\u3081\u308b\n\ndf[\"Age\"] = df[\"Age\"].fillna(dmean)","8b3c5b63":"df","16caa7eb":"test[\"Age\"] = test[\"Age\"].fillna(dmean)","ba5eac44":"from sklearn.preprocessing import LabelEncoder","92e36ed1":"le=LabelEncoder()\nle.fit(df[\"Sex\"])\ndf[\"Sex\"] = le.transform(df[\"Sex\"])\ntest[\"Sex\"] = le.transform(test[\"Sex\"])","a4a82a7d":"df","a567ef16":"# firstly, combining df and test. Combine vertically with axis = 0.\n# \u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u7d50\u5408\u3057\u307e\u3059\u3002axis = 0 \u3067\u7e26\u306b\u7d50\u5408\u3002\n\ndfall = pd.concat([df,test],axis=0)\ndfall","22f0e819":"# one-hot encoding using pd.get_dummies. NaN can also be separated by dummy_na = True.\n# get_dummies\u3092\u4f7f\u3063\u3066\u3001one-hot encoding\u3057\u307e\u3059\u3002dummy_na=True\u3067NaN\u3082\u5206\u3051\u308b\u3053\u3068\u304c\u53ef\u80fd\u3002\n\ndfall2 = pd.get_dummies(dfall[\"Embarked\"],dummy_na=True)\ndfall2","77c31cf4":"# Combine horizontally with axis = 1.\n# axis = 1 \u3067\u6a2a\u306b\u7d50\u5408\u3002\n\ndfall = pd.concat([dfall,dfall2],axis=1)\ndfall","412383c0":"# Separate the train data and test data and restore them.\n# \u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u96e2\u3057\u3066\u5143\u306b\u623b\u3057\u307e\u3059\u3002\n\ntrain = dfall.iloc[:len(df),:]\ntest = dfall.iloc[len(df):,:]","deeb2536":"train","efeda7f0":"test","edf37f4b":"from sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","72499f37":"folds = train.copy()\nFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[\"Survived\"])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.groupby(['fold', \"Survived\"]).size())","80fed106":"folds","900a8962":"# defining the feature columns and the target\n\nFEATURES = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"C\",\"Q\",\"S\",np.nan]\nTARGET = \"Survived\"","f2cb2d03":"from sklearn.preprocessing import StandardScaler\nNormarizescaler = StandardScaler()\nNormarizescaler.fit(np.array(train[FEATURES]))","3967dc78":"class PytorchDataset(Dataset):\n    \n    def __init__(self,df):\n        \n        # for test data, In test data, it's easier to fill it with something on purpose.\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3082\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u308f\u3056\u30689999\u3067\u57cb\u3081\u3066\u3044\u307e\u3059\u3002\n        \n        if \"Survived\" not in df.columns:\n            df[\"Survived\"] = 9999\n        \n        self.df = df\n        \n        self.train_X = np.array(self.df[FEATURES])\n        self.train_Y = np.array(self.df[TARGET])\n        \n        self.train_X = Normarizescaler.transform(self.train_X)\n        \n        self.train_X = torch.from_numpy(self.train_X).float()\n        self.train_Y = torch.from_numpy(self.train_Y).long() # long : int64\n\n    def __len__(self):\n        \n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        \n        return self.train_X[idx],self.train_Y[idx]","3bb53a21":"class PytorchDataModule(LightningDataModule):\n    def __init__(\n    self,\n        train_df,\n        val_df,\n        cfg,    \n    ):\n        \n        \n        super().__init__()\n        self._train_df = train_df\n        self._val_df = val_df\n        self._cfg = cfg\n        \n    def __create_dataset(self,train=True):\n        \n        \n        return (\n        PytorchDataset(self._train_df)\n            if train\n            else PytorchDataset(self._val_df)\n        \n        )\n    \n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset,**self._cfg.train_loader)\n    \n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset,**self._cfg.val_loader)\n    ","132f6f92":"torch.autograd.set_detect_anomaly(True)\nseed_everything(config.seed) # \u3053\u308c\u3060\u3051\u3067\u521d\u671f\u5316\u5b8c\u4e86","8c207fb8":"from sklearn.metrics import accuracy_score","80d9a008":"class Net(pl.LightningModule):\n    def __init__(self,cfg,val_losses=None):\n        super().__init__() \n        \n        self.cfg = cfg\n        self.__build_model()\n        self._criterion = eval(self.cfg.loss)()\n        self.save_hyperparameters(cfg)\n        self.val_losses = val_losses\n        \n        self.t=time.time()\n        self.steps_per_epoch = len(datamodule.train_dataloader())\n        self.step = 0\n        self.epoch = 0\n        \n    def __build_model(self):\n               \n        \n        self.fc1 = nn.Linear(len(FEATURES),512) #input number and middle layer fc1\n        self.fc2 = nn.Linear(512,256) # middle layer fc2\n        self.fc3 = nn.Linear(256,2) # output\n        \n    \n    def forward(self,x): \n        x= F.relu(self.fc1(x)) # Put the relu function after fc1\n        x= F.relu(self.fc2(x)) # Put the relu function after fc2\n        x = self.fc3(x) # fc3\n        return x \n    \n    def training_step(self,batch,batch_idx): # for a in train_dataloader\u306e1\u500b\u5206\n        \n        loss,pred,labels = self.__share_step(batch,\"train\")\n        #self.log(\"train_loss\",loss)\n        \n        self.step +=1\n        \n        print (\"Step [{}\/{}] Loss: {:.3f} Time: {:.1f} Epoch:[{}\/{}]\"\n                           .format(self.step, self.steps_per_epoch, loss, time.time()-self.t,self.epoch,self.cfg.epoch),end='\\r',flush=True)\n        \n        return {\"loss\":loss,\"pred\":pred,\"labels\":labels}\n    \n    def validation_step(self,batch,batch_idx):\n        \n        loss,pred,labels = self.__share_step(batch,\"val\")\n        self.log(\"valid_loss\",loss) # wanbd\u306b\u98db\u3070\u3057\u3066\u304f\u308c\u308b\n        \n        return {\"pred\":pred,\"labels\":labels}\n    \n    \n    def __share_step(self,batch,mode):\n        \n        data,labels = batch\n        \n        logits = self.forward(data).squeeze(1)\n        loss = self._criterion(logits,labels)\n            \n        #pred = logits.sigmoid().detach().cpu() \n        labels = labels.detach().cpu() \n        \n        return loss,logits,labels\n    \n    \n    def training_epoch_end(self,outputs):\n        self.__share_epoch_end(outputs,\"train\")\n        \n    def validation_epoch_end(self,outputs):\n        self.__share_epoch_end(outputs,\"val\")\n        \n    def __share_epoch_end(self,outputs,mode):\n        \n        preds = []\n        labels = []\n        \n        for out in outputs:\n            pred,label = out[\"pred\"],out[\"labels\"]\n            preds.append(pred)\n            labels.append(label)\n          \n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        \n        \n        preds = preds.detach().cpu().numpy()\n        labels = labels.detach().cpu().numpy()\n        \n        preds = [s.argmax() for s in preds]\n        \n        metrics = accuracy_score(labels,preds)\n        \n        self.log(f\"{mode}_score\",metrics)\n        \n        if mode==\"val\":\n            self.val_losses.append(metrics)\n            \n            self.epoch +=1\n            self.step = 0\n        \n                \n                   \n    def configure_optimizers(self):\n                 \n        optimizer = eval(self.cfg.optimizer.name)(self.parameters(),**self.cfg.optimizer.params)\n\n        scheduler = eval(self.cfg.scheduler.name)(optimizer,**self.cfg.scheduler.params)\n        \n        return [optimizer],[scheduler]\n       ","f45935a6":"def inference(dataloader):\n    \n    preds = []\n    with torch.no_grad():\n        \n        for a in dataloader:\n            \n            a = a[0].to(device)\n            logits = model(a).squeeze(1)\n            #pred = logits.sigmoid().detach().cpu() \n            preds.append(logits)\n            \n        preds = np.concatenate(preds)\n        \n        return preds","d186f1b0":"config","f1a60571":"config.trainer.progress_bar_refresh_rate = 0 # progress bar\u3092\u8868\u793a\u3057\u306a\u3044","a26113e4":"if config.mode == \"train\":\n    \n    allscores = []\n    allvaliddf = pd.DataFrame()\n    \n\n    for fold in range(config.n_splits):\n        \n        print(f\"fold{fold}-----------start\")\n        \n        seed_everything(config.seed)\n        \n        scores = []       \n        \n\n        train_df = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n        val_df = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n\n        datamodule = PytorchDataModule(train_df,val_df,config)\n\n        model = Net(config,scores)\n\n        earystopping = EarlyStopping(monitor=\"val_score\",mode=\"max\",patience = 100)\n\n        lr_monitor = callbacks.LearningRateMonitor()\n\n        loss_checkpoint = callbacks.ModelCheckpoint(\n\n        filename = f\"best_score{fold}\",\n            monitor = \"val_score\",\n            save_top_k=1,\n            mode=\"max\",\n            save_last=False,\n\n        )\n\n        #logger = TensorBoardLogger(config.model.name)\n\n        trainer = pl.Trainer(\n\n            logger = wlogger, # \u3053\u3053\u306bwandb\u306e\u30ed\u30b0\n            max_epochs = config.epoch,\n            callbacks = [lr_monitor,loss_checkpoint,earystopping],\n            **config.trainer,    \n\n        )\n\n        trainer.fit(model,datamodule=datamodule)\n        \n        \n        ## valscore ##\n        \n        print(f\"fold{fold} : valscore is {np.max(scores)}\")\n        \n        \n        allscores.append(np.max(scores))\n        \n        \n        modelpath = f\".\/{config.wandbprojectname}\/{config.wandbversion}\/checkpoints\/best_score{fold}.ckpt\"\n        \n            \n        tmp = torch.load(modelpath)\n        model.load_state_dict(tmp[\"state_dict\"])\n        model = model.to(device).eval()\n        \n        oofpreds = inference(datamodule.val_dataloader())\n        \n        oofpreds = [s.argmax() for s in oofpreds]\n        \n        val_df[\"preds\"] = oofpreds\n        \n        allvaliddf = pd.concat([allvaliddf,val_df])\n        \n        \n","d957bb5c":"try:\n    wandb.finish()\nexcept:\n    pass","a749cec9":"allscores","35ac99ea":"np.mean(allscores)","4ffd3b82":"allvaliddf","43a0e115":"allvaliddf = allvaliddf.sort_values(\"PassengerId\").reset_index(drop=True)\nallvaliddf.to_pickle(\"allvaliddf.pkl\")","cef1d51e":"accuracy_score(allvaliddf[\"Survived\"],allvaliddf[\"preds\"])","6f312560":"test","5d210181":"datamodule2 = PytorchDataModule(train,test,config)","cd7f810d":"pthes = glob.glob(f\".\/{config.wandbprojectname}\/{config.wandbversion}\/checkpoints\/*.ckpt\")","d1de0bde":"model = Net(config) \n\nallpreds = []\n\nfor pth in tqdm(pthes):\n    \n    preds = []\n\n    \n    tmp = torch.load(pth)\n    model.load_state_dict(tmp[\"state_dict\"])\n    model = model.to(device).eval()\n    \n    with torch.no_grad():\n        \n        for a,b in datamodule2.val_dataloader():\n            \n            a = a.to(device)\n            logits = model(a).squeeze(1)\n            pred = logits.sigmoid().detach().cpu() \n            preds.append(pred)\n            \n        preds = np.concatenate(preds)\n        allpreds.append(preds)\n","0ee38943":"allpreds = np.mean(allpreds,axis=0)","ae21a760":"allpreds = [s.argmax() for s in allpreds]","2bffa2e1":"submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nsubmission","85089adf":"submission[\"Survived\"] = allpreds","f7215cc6":"submission","34d07151":"submission.to_csv(\"submission.csv\",index=False)","da90717d":"# Add 1 Setting wandb\n\nWith wandb, you can see logs such as validation loss at a glance and compare with the previous experiment while turning machine learning. very convenient.\n\n1) Register with wandb, sign in, and get api\n\n\n2) Save the api as wandb_api in the tab Add-ons Secret at the top of this edit screen and check it.\n\n\n(This feature is a mechanism that prevents others from seeing the contents of the variable on kaggle)\n\n\n-------------\u65e5\u672c\u8a9e--------------\nwandb\u3092\u4f7f\u3046\u3068\u6a5f\u68b0\u5b66\u7fd2\u3092\u56de\u3057\u3066\u3044\u308b\u9593\u306b\u3001validation loss\u306a\u3069\u306e\u30ed\u30b0\u304c\u4e00\u76ee\u3067\u898b\u308c\u305f\u308a\u3001\u524d\u306e\u5b9f\u9a13\u3068\u6bd4\u8f03\u3067\u304d\u305f\u308a\u3057\u307e\u3059\u3002\u3068\u3066\u3082\u4fbf\u5229\u3067\u3059\u3002\n\n1) wandb\u306b\u767b\u9332\u3057\u3066\u3001signin\u5f8c\u3001api\u3092\u53d6\u5f97\n\n\n2) \u3053\u306eedit\u753b\u9762\u306e\u4e0a\u306e\u30bf\u30d6Add-ons Secret \u306b wandb_api\u3068\u3044\u3046\u540d\u524d\u3067api\u3092\u4fdd\u5b58\u3057\u3066\u30c1\u30a7\u30c3\u30af\u3092\u5165\u308c\u308b \n\n\n(\u3053\u306e\u6a5f\u80fd\u306f\u3001kaggle\u4e0a\u3067\u4ed6\u306e\u4eba\u306b\u5909\u6570\u306e\u4e2d\u8eab\u304c\u898b\u308c\u306a\u3044\u3088\u3046\u306a\u4ed5\u7d44\u307f\u3067\u3059)\n\n","f623de6c":"# 5.3 **Pytorch Lightning**  DataModule ","577a8db9":"# 5.1 Defining features and target\n##     \u7279\u5fb4\u91cf\u3068\u30e9\u30d9\u30eb(\u30bf\u30fc\u30b2\u30c3\u30c8)\u3092\u5b9a\u7fa9\u3057\u307e\u3059","9907f2ef":"# Pytorch_Lightning + Wandb Starter\n## About this notebook\n*  This notebook is based on https:\/\/www.kaggle.com\/chumajin\/pytorch-neural-network-starter-detail 1)\n* It incorporates pytorch lightning and also includes a very useful logger called wandb.\n\n* pytorch lightning was made with reference to the following notebook. Thank you very much. Please also upvote this.\nhttps:\/\/www.kaggle.com\/phalanx\/train-swin-t-pytorch-lightning 2) \n\n* If you are new to pytorch, I think it is better to use the notebook1).\n\n\n---------------------------------------------------------------------------------------------------------\n\n*  \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001https:\/\/www.kaggle.com\/chumajin\/pytorch-neural-network-starter-detail 1) \u3092base\u306b\n* pytorch lightning\u3092\u53d6\u308a\u5165\u308c\u3001\u3055\u3089\u306b\u3068\u3066\u3082\u4fbf\u5229\u306awandb\u3068\u3044\u3046\u30ed\u30ac\u30fc\u3092\u5165\u308c\u305f\u3082\u306e\u3067\u3059\u3002\n\n* pytorch lightning\u306f\u4ee5\u4e0b\u306enotebook\u3092\u53c2\u8003\u306b\u4f5c\u308a\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\u3053\u3061\u3089\u3082upvote\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\nhttps:\/\/www.kaggle.com\/phalanx\/train-swin-t-pytorch-lightning 2)\n\n* pytorch\u521d\u3081\u3066\u306e\u65b9\u306f\u2191\u306e1)\u306enotebook\u304b\u3089\u306e\u65b9\u304c\u3044\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u3002\n\n","d9dd5434":"# 7. inference","617f2523":"# 2 label encoding\n#### Automatically convert strings to numbers. Since there is a significant difference such as 0 and 1, Sex is divided in this way.\n#### \u6587\u5b57\u5217\u3092\u6570\u5b57\u306b\u81ea\u52d5\u5909\u63db. 0\u30681\u306a\u3069\u6709\u610f\u5dee\u304c\u3042\u308b\u305f\u3081\u3001\u3053\u306e\u3084\u308a\u65b9\u3067\u6027\u5225\u3092\u5206\u3051\u3066\u3044\u307e\u3059\u3002","e0ede6ff":"# 3. One-hot encoding\n#### Automatically convert strings to numbers line by line. \n#### There are four Embarked places including NaN, but since no significant difference can be considered, divide them in parallel.\n#### \u6587\u5b57\u5217\u3092\u884c\u3054\u3068\u306b\u6570\u5b57\u306b\u81ea\u52d5\u5909\u63db\u3002Embarked\u306fNaN\u3092\u5165\u308c\u30664\u3064\u3042\u308b\u304c\u3001\u6709\u610f\u5dee\u304c\u8003\u3048\u3089\u308c\u306a\u3044\u306e\u3067\u3001\u4e26\u5217\u306b\u5206\u3051\u307e\u3059\u3002","691b275c":"# 4. Kfold\n#### Prepare training data and verification data in 5 combinations.\n#### \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u30925\u3064\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u6e96\u5099\u3059\u308b\u3002","103a6341":"# 5.5 Inference function to make out of fold (my original)","bc7f0a67":"# 5.4 **Pytorch Lightning** Modeling","6a9b0236":"![image.png](attachment:85f45690-03bb-4e3d-b618-d9105a2f13ed.png)","37bfaebc":"# 6. Main","ab6d51c8":"#### There are 891 rows. There are NaN data in Age, Cabin, Embarked.\n#### \u5168\u90e8\u3067891\u884c\u3042\u3063\u3066\u3001Age,Cabin,Embarked\u306b\u306fnull\u30c7\u30fc\u30bf\u306f\u306a\u3044\u304c\u3001NaN\u30c7\u30fc\u30bf\u304c\u3042\u308a\u305d\u3046\u3002","55c9d4f4":"# 0. Confirming the train\/test data : \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\n","e0ef6870":"# 5.2 Dataset","3a043a16":"## About data\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\nparch\t# of parents \/ children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n#########\u65e5\u672c\u8a9e#################\n\nsurvival\t\u751f\u6b7b\t0 = \u6b7b\u4ea1, 1 = \u751f\u5b58\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\t\u6027\u5225\t\nAge\t\u5e74\u9f62\t\nsibsp\t# of siblings \/ \u89aa\u65cf\u306e\u6570\t\nparch\t# of parents \/ \u5b50\u4f9b\u306e\u6570\t\nticket\tTicket number\u3000\u30c1\u30b1\u30c3\u30c8\u30ca\u30f3\u30d0\u30fc\t\nfare\tPassenger fare\t\u904b\u8cc3\ncabin\tCabin number\t\u90e8\u5c4b\u306e\u756a\u53f7\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\u3000\u4e57\u8239\u3057\u305f\u5834\u6240\n","b5fcb56b":"# Below, it is the same as the last time until the Dataset is created\n\n\u4ee5\u4e0b\u3001Dataset\u4f5c\u6210\u307e\u3067\u306f\u524d\u56de\u3068\u540c\u3058","57072c62":"# 1. Handling of NaN values : \u6b20\u640d\u5024\u306e\u51e6\u7406\n### 1.1 filling an mean value. : \u5e73\u5747\u5024\u3067\u57cb\u3081\u308b","1695adbc":"# Addition 2 config: It will read the dictionary type and save it as it is on wandb\n\u8ffd\u52a02 config : wandb\u306b\u98db\u3070\u3059\u3068\u8f9e\u66f8\u578b\u3092\u8aad\u3093\u3067\u305d\u306e\u307e\u307e\u4fdd\u5b58\u3057\u3066\u304f\u308c\u308b","5e2d6d4c":"# 5.Neural Network using **pytorch-lightning**"}}