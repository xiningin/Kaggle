{"cell_type":{"d1734290":"code","5f33cf2a":"code","49a0b58f":"code","56b30989":"code","d14517c3":"code","67d4771f":"code","584d3968":"code","84f522de":"code","e178a053":"code","0d16cc3a":"code","3aa4d23f":"code","3df41849":"code","3a5c878d":"code","239781bf":"code","366c8ae6":"code","3afecbee":"code","54f8412c":"code","2303951d":"code","507309f7":"code","ebfba472":"code","a889674a":"code","d0bfbc09":"code","b6e39853":"code","888f2a0f":"code","62584887":"code","056e1cbb":"code","1b3edd5d":"code","ba15b4b2":"code","1d352145":"code","51cd6650":"code","65190f8d":"code","159e6c85":"code","b00fe562":"code","687f67c3":"code","3e97ce75":"code","8908717d":"code","87aafd12":"code","bbff2ac0":"code","0fadda8b":"markdown","e2a5cd3a":"markdown","4bbe325d":"markdown","edc6d72b":"markdown","5c950663":"markdown","b336a624":"markdown","326363ca":"markdown","f250c278":"markdown","51604224":"markdown","3b74cec3":"markdown","4dcb496f":"markdown","a640503d":"markdown","e50bb7cd":"markdown","2e0bb62a":"markdown","13b70de3":"markdown","f206cdb8":"markdown"},"source":{"d1734290":"!pip install keract\nimport keract","5f33cf2a":"import pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport albumentations as A\nfrom IPython.display import SVG\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os, re, sys, random, shutil, cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom tensorflow.keras import applications, optimizers\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.utils import model_to_dot, plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout","49a0b58f":"def augment(width, height):\n    transform = A.Compose([\n        A.RandomCrop(width=width, height=height, p=1.0),\n        A.HorizontalFlip(p=1.0),\n        A.VerticalFlip(p=1.0),\n        A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n        A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n        A.OneOf([\n            A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),\n            A.GridDistortion(p=0.5),\n            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n        ], p=1.0),\n    ], p=1.0)\n    \n    return transform","56b30989":"def visualize(image, mask, original_image=None, original_mask=None):\n    fontsize = 16\n\n    if original_image is None and original_mask is None:\n        f, ax = plt.subplots(2, 1, figsize=(10, 10), squeeze=True)\n        f.set_tight_layout(h_pad=5, w_pad=5)\n\n        ax[0].imshow(image)\n        ax[1].imshow(mask)\n    else:\n        f, ax = plt.subplots(2, 2, figsize=(16, 12), squeeze=True)\n        plt.tight_layout(pad=0.2, w_pad=1.0, h_pad=0.01)\n\n        ax[0, 0].imshow(original_image)\n        ax[0, 0].set_title('Original Image', fontsize=fontsize)\n\n        ax[1, 0].imshow(original_mask)\n        ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n\n        ax[0, 1].imshow(image)\n        ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n\n        ax[1, 1].imshow(mask)\n        ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n        \n    plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)","d14517c3":"image = cv2.imread(\"..\/input\/dubai-aerial-imagery-dataset\/train_images\/train\/image_t8_007.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nmask = cv2.imread(\"..\/input\/dubai-aerial-imagery-dataset\/train_masks\/train\/image_t8_007.png\")\nmask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n\ntransform = augment(1920, 1280)\ntransformed = transform(image=image, mask=mask)\ntransformed_image = transformed['image']\ntransformed_mask = transformed['mask']\n\ncv2.imwrite('.\/image.png',cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\ncv2.imwrite('.\/mask.png',cv2.cvtColor(transformed_mask, cv2.COLOR_BGR2RGB))\n\nvisualize(transformed_image, transformed_mask, image, mask)","67d4771f":"!mkdir aug_images\n!mkdir aug_masks","584d3968":"images_dir = '..\/input\/dubai-aerial-imagery-dataset\/train_images\/train\/'\nmasks_dir = '..\/input\/dubai-aerial-imagery-dataset\/train_masks\/train\/'","84f522de":"file_names = np.sort(os.listdir(images_dir)) \nfile_names = np.char.split(file_names, '.')\nfilenames = np.array([])\nfor i in range(len(file_names)):\n    filenames = np.append(filenames, file_names[i][0])","e178a053":"def augment_dataset(count):\n    '''Function for data augmentation\n        Input:\n            count - total no. of images after augmentation = initial no. of images * count\n        Output:\n            writes augmented images (input images & segmentation masks) to the working directory\n    '''\n    transform_1 = augment(512, 512)\n    transform_2 = augment(480, 480)\n    transform_3 = augment(512, 512)\n    transform_4 = augment(800, 800)\n    transform_5 = augment(1024, 1024)\n    transform_6 = augment(800, 800)\n    transform_7 = augment(1600, 1600)\n    transform_8 = augment(1920, 1280)\n    \n    i = 0\n    for i in range(count):\n        for file in filenames:\n            tile = file.split('_')[1]\n            img = cv2.imread(images_dir+file+'.jpg')\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            mask = cv2.imread(masks_dir+file+'.png')\n            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n            \n            if tile == 't1':\n                transformed = transform_1(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t2':\n                transformed = transform_2(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t3':\n                transformed = transform_3(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t4':\n                transformed = transform_4(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t5':\n                transformed = transform_5(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t6':\n                transformed = transform_6(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t7':\n                transformed = transform_7(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n            elif tile =='t8':\n                transformed = transform_8(image=img, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n                \n            cv2.imwrite('.\/aug_images\/aug_{}_'.format(str(i+1))+file+'.jpg',cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n            cv2.imwrite('.\/aug_masks\/aug_{}_'.format(str(i+1))+file+'.png',cv2.cvtColor(transformed_mask, cv2.COLOR_BGR2RGB))","0d16cc3a":"augment_dataset(8)","3aa4d23f":"!zip -r aug_images.zip '.\/aug_images\/'\n!zip -r aug_masks.zip '.\/aug_masks\/'","3df41849":"!rm -rf '.\/aug_images\/'\n!rm -rf '.\/aug_masks\/'","3a5c878d":"train_images = \"..\/input\/augmented-dubai-aerial-imagery-dataset\/train_images\/\"\ntrain_masks = \"..\/input\/augmented-dubai-aerial-imagery-dataset\/train_masks\/\"\nval_images = \"..\/input\/augmented-dubai-aerial-imagery-dataset\/val_images\/\"\nval_masks = \"..\/input\/augmented-dubai-aerial-imagery-dataset\/val_masks\/\"","239781bf":"file_names = np.sort(os.listdir(train_images + 'train\/')) \nfile_names = np.char.split(file_names, '.')\nfilenames = np.array([])\nfor i in range(len(file_names)):\n    filenames = np.append(filenames, file_names[i][0])","366c8ae6":"def show_data(files, original_images_dir, label_images_dir):\n\n    for file in files:\n        fig, axs = plt.subplots(1, 2, figsize=(15, 6), constrained_layout=True)\n        \n        axs[0].imshow(cv2.resize(cv2.imread(original_images_dir+'train\/'+str(file)+'.jpg'), (2000,1400)))\n        axs[0].set_title('Original Image', fontdict = {'fontsize':14, 'fontweight': 'medium'})\n        axs[0].set_xticks(np.arange(0, 2001, 200))\n        axs[0].set_yticks(np.arange(0, 1401, 200))\n        axs[0].grid(False)\n        axs[0].axis(True)\n\n        semantic_label_image = cv2.imread(label_images_dir+ 'train\/'+str(file)+'.png')\n        semantic_label_image = cv2.cvtColor(semantic_label_image, cv2.COLOR_BGR2RGB)\n        semantic_label_image = cv2.resize(semantic_label_image, (2000,1400))\n        axs[1].imshow(semantic_label_image)\n        axs[1].set_title('Semantic Segmentation Mask', fontdict = {'fontsize':14, 'fontweight': 'medium'})\n        axs[1].set_xticks(np.arange(0, 2001, 200))\n        axs[1].set_yticks(np.arange(0, 1401, 200))\n        axs[1].grid(False)\n        axs[1].axis(True)\n\n        plt.savefig('.\/sample_'+file, facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)\n        plt.show()\n\n        \nfiles = ['image_t4_001', 'image_t4_005', 'image_t6_002', 'image_t7_002', 'image_t8_003', 'image_t8_004', 'image_t8_006']    \nshow_data(files, train_images, train_masks)    ","3afecbee":"augmented_files = ['image_t1_002', 'image_t3_002', 'image_t4_006', 'image_t5_004', 'image_t6_007']\ndims = np.array([(512, 512), (512, 512), (800, 800), (800, 800), (800, 800)])\n\ndef show_augmented_images(files, original_images_dir, label_images_dir):\n    dim_index = 0\n    for file in files:\n        count_img = 1\n        count_msk = 1\n        fig, axs = plt.subplots(3, 6, figsize=(25, 12), constrained_layout=True)\n        for i in range(3):\n            for j in range(6):\n                if i == 0 and j == 0:\n                    axs[i][j].imshow(cv2.resize(cv2.cvtColor(cv2.imread(original_images_dir+'train\/'+str(file)+'.jpg'), cv2.COLOR_BGR2RGB), tuple(dims[dim_index])))\n                    axs[i][j].set_title('Original Image: {}.jpg'.format(file), fontdict = {'fontsize':12, 'fontweight': 'medium'})\n                    axs[i][j].set_xticks(np.arange(0, dims[dim_index][0]+1, 100))\n                    axs[i][j].set_yticks(np.arange(0, dims[dim_index][1]+1, 100))\n                    axs[i][j].grid(False)\n                    axs[i][j].axis(True)\n                elif i == 0 and j == 1:\n                    axs[i][j].imshow(cv2.resize(cv2.cvtColor(cv2.imread(label_images_dir+'train\/'+str(file)+'.png'), cv2.COLOR_BGR2RGB), tuple(dims[dim_index])))\n                    axs[i][j].set_title('Original Mask: {}.png'.format(file), fontdict = {'fontsize':12, 'fontweight': 'medium'})\n                    axs[i][j].set_xticks(np.arange(0, dims[dim_index][0]+1, 100))\n                    axs[i][j].set_yticks(np.arange(0, dims[dim_index][1]+1, 100))\n                    axs[i][j].grid(False)\n                    axs[i][j].axis(True)\n                else:\n                    if j%2 == 0:\n                        axs[i][j].imshow(cv2.resize(cv2.cvtColor(cv2.imread(original_images_dir+'train\/aug_'+str(count_img)+'_'+str(file)+'.jpg'), cv2.COLOR_BGR2RGB), tuple(dims[dim_index])))\n                        axs[i][j].set_title('Augmented Image: aug_{}.jpg'.format(str(count_img)+'_'+str(file)), fontdict = {'fontsize':12, 'fontweight': 'medium'})\n                        axs[i][j].set_xticks(np.arange(0, dims[dim_index][0]+1, 100))\n                        axs[i][j].set_yticks(np.arange(0, dims[dim_index][1]+1, 100))\n                        axs[i][j].grid(False)\n                        axs[i][j].axis(True)\n                        count_img += 1\n                    elif j%2 != 0:\n                        axs[i][j].imshow(cv2.resize(cv2.cvtColor(cv2.imread(label_images_dir+'train\/aug_'+str(count_msk)+'_'+str(file)+'.png'), cv2.COLOR_BGR2RGB), tuple(dims[dim_index])))\n                        axs[i][j].set_title('Augmented Mask: aug_{}.png'.format(str(count_msk)+'_'+str(file)), fontdict = {'fontsize':12, 'fontweight': 'medium'})\n                        axs[i][j].set_xticks(np.arange(0, dims[dim_index][0]+1, 100))\n                        axs[i][j].set_yticks(np.arange(0, dims[dim_index][1]+1, 100))\n                        axs[i][j].grid(False)\n                        axs[i][j].axis(True)\n                        count_msk += 1\n\n        plt.savefig('aug_image_'+file, facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)\n        dim_index += 1\n        plt.show()\n\nshow_augmented_images(augmented_files, train_images, train_masks)","54f8412c":"class_dict_df = pd.read_csv('..\/input\/dubai-aerial-imagery-dataset\/class_dict.csv', index_col=False, skipinitialspace=True)\nclass_dict_df","2303951d":"label_names= list(class_dict_df.name)\nlabel_codes = []\nr= np.asarray(class_dict_df.r)\ng= np.asarray(class_dict_df.g)\nb= np.asarray(class_dict_df.b)\n\nfor i in range(len(class_dict_df)):\n    label_codes.append(tuple([r[i], g[i], b[i]]))\n    \nlabel_codes, label_names","507309f7":"code2id = {v:k for k,v in enumerate(label_codes)}\nid2code = {k:v for k,v in enumerate(label_codes)}\n\nname2id = {v:k for k,v in enumerate(label_names)}\nid2name = {k:v for k,v in enumerate(label_names)}","ebfba472":"id2code","a889674a":"id2name","d0bfbc09":"def rgb_to_onehot(rgb_image, colormap = id2code):\n    '''Function to one hot encode RGB mask labels\n        Inputs: \n            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n            colormap - dictionary of color to label id\n        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n    '''\n    num_classes = len(colormap)\n    shape = rgb_image.shape[:2]+(num_classes,)\n    encoded_image = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(colormap):\n        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n    return encoded_image\n\n\ndef onehot_to_rgb(onehot, colormap = id2code):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    return np.uint8(output)","b6e39853":"# Normalizing only frame images, since masks contain label info\ndata_gen_args = dict(rescale=1.\/255)\nmask_gen_args = dict()\n\ntrain_frames_datagen = ImageDataGenerator(**data_gen_args)\ntrain_masks_datagen = ImageDataGenerator(**mask_gen_args)\nval_frames_datagen = ImageDataGenerator(**data_gen_args)\nval_masks_datagen = ImageDataGenerator(**mask_gen_args)\n\n# Seed defined for aligning images and their masks\nseed = 1","888f2a0f":"def TrainAugmentGenerator(train_images_dir, train_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n    '''Train Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n            train_images_dir - train images directory\n            train_masks_dir - train masks directory\n            target_size - tuple of integers (height, width)\n            \n        Output: Decoded RGB image (height x width x 3) \n    '''\n    train_image_generator = train_frames_datagen.flow_from_directory(\n    train_images_dir,\n    batch_size = batch_size, \n    seed = seed, \n    target_size = target_size)\n\n    train_mask_generator = train_masks_datagen.flow_from_directory(\n    train_masks_dir,\n    batch_size = batch_size, \n    seed = seed, \n    target_size = target_size)\n\n    while True:\n        X1i = train_image_generator.next()\n        X2i = train_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n\ndef ValAugmentGenerator(val_images_dir, val_masks_dir, seed = 1, batch_size = 8, target_size = (512, 512)):\n    '''Validation Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n            val_images_dir - validation images directory\n            val_masks_dir - validation masks directory\n            target_size - tuple of integers (height, width)\n            \n        Output: Decoded RGB image (height x width x 3) \n    '''\n    val_image_generator = val_frames_datagen.flow_from_directory(\n    val_images_dir,\n    batch_size = batch_size, \n    seed = seed, \n    target_size = target_size)\n\n\n    val_mask_generator = val_masks_datagen.flow_from_directory(\n    val_masks_dir,\n    batch_size = batch_size, \n    seed = seed, \n    target_size = target_size)\n\n\n    while True:\n        X1i = val_image_generator.next()\n        X2i = val_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)","62584887":"batch_size = 16\nnum_train_samples = len(np.sort(os.listdir(train_images+'train')))\nnum_val_samples = len(np.sort(os.listdir(val_images+'val')))\nsteps_per_epoch = np.ceil(float(num_train_samples) \/ float(batch_size))\nprint('steps_per_epoch: ', steps_per_epoch)\nvalidation_steps = np.ceil(float(4 * num_val_samples) \/ float(batch_size))\nprint('validation_steps: ', validation_steps)","056e1cbb":"def conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_inception_resnetv2_unet(input_shape):\n    \"\"\" Input \"\"\"\n    inputs = Input(input_shape)\n\n    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n\n    \"\"\" Encoder \"\"\"\n    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n\n    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n\n    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n\n    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n\n    \"\"\" Bridge \"\"\"\n    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n    \n    \"\"\" Output \"\"\"\n    dropout = Dropout(0.3)(d4)\n    outputs = Conv2D(6, 1, padding=\"same\", activation=\"softmax\")(dropout)\n\n    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n    return model","1b3edd5d":"K.clear_session()\n\ndef dice_coef(y_true, y_pred):\n    return (2. * K.sum(y_true * y_pred) + 1.) \/ (K.sum(y_true) + K.sum(y_pred) + 1.)\n\nmodel = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\nmodel.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])\nmodel.summary()","ba15b4b2":"SVG(model_to_dot(model).create(prog='dot', format='svg'))\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, expand_nested=True)","1d352145":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.0001, 60)\n\nlr_scheduler = LearningRateScheduler(\n    exponential_decay_fn,\n    verbose=1\n)\n\ncheckpoint = ModelCheckpoint(\n    filepath = 'InceptionResNetV2-UNet.h5',\n    save_best_only = True, \n#     save_weights_only = False,\n    monitor = 'val_loss', \n    mode = 'auto', \n    verbose = 1\n)\n\nearlystop = EarlyStopping(\n    monitor = 'val_loss', \n    min_delta = 0.001, \n    patience = 12, \n    mode = 'auto', \n    verbose = 1,\n    restore_best_weights = True\n)\n\ncsvlogger = CSVLogger(\n    filename= \"model_training.csv\",\n    separator = \",\",\n    append = False\n)\n\ncallbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]","51cd6650":"history = model.fit(\n    TrainAugmentGenerator(train_images_dir = train_images, train_masks_dir = train_masks, target_size = (512, 512)), \n    steps_per_epoch=steps_per_epoch,\n    validation_data = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, target_size = (512, 512)), \n    validation_steps = validation_steps, \n    epochs = 50,\n    callbacks=callbacks,\n    use_multiprocessing=False,\n    verbose=1\n)","65190f8d":"df_result = pd.DataFrame(history.history)\ndf_result","159e6c85":"fig, ax = plt.subplots(1, 4, figsize=(40, 5))\nax = ax.ravel()\nmetrics = ['Dice Coefficient', 'Accuracy', 'Loss', 'Learning Rate']\n\nfor i, met in enumerate(['dice_coef', 'accuracy', 'loss', 'lr']): \n    if met != 'lr':\n        ax[i].plot(history.history[met])\n        ax[i].plot(history.history['val_' + met])\n        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n        ax[i].set_xlabel('Epochs')\n        ax[i].set_ylabel(metrics[i])\n        ax[i].set_xticks(np.arange(0,46,4))\n        ax[i].legend(['Train', 'Validation'])\n        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n    else:\n        ax[i].plot(history.history[met])\n        ax[i].set_title('{} vs Epochs'.format(metrics[i]), fontsize=16)\n        ax[i].set_xlabel('Epochs')\n        ax[i].set_ylabel(metrics[i])\n        ax[i].set_xticks(np.arange(0,46,4))\n        ax[i].xaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n        ax[i].yaxis.grid(True, color = \"lightgray\", linewidth = \"0.8\", linestyle = \"-\")\n        \nplt.savefig('model_metrics_plot.png', facecolor= 'w',transparent= False, bbox_inches= 'tight', dpi= 150)","b00fe562":"model.load_weights(\".\/InceptionResNetV2-UNet.h5\")","687f67c3":"testing_gen = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, target_size = (512, 512))","3e97ce75":"count = 0\nfor i in range(2):\n    batch_img,batch_mask = next(testing_gen)\n    pred_all= model.predict(batch_img)\n    np.shape(pred_all)\n    \n    for j in range(0,np.shape(pred_all)[0]):\n        count += 1\n        fig = plt.figure(figsize=(20,8))\n\n        ax1 = fig.add_subplot(1,3,1)\n        ax1.imshow(batch_img[j])\n        ax1.set_title('Input Image', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n        ax1.grid(False)\n\n        ax2 = fig.add_subplot(1,3,2)\n        ax2.set_title('Ground Truth Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n        ax2.imshow(onehot_to_rgb(batch_mask[j],id2code))\n        ax2.grid(False)\n\n        ax3 = fig.add_subplot(1,3,3)\n        ax3.set_title('Predicted Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n        ax3.imshow(onehot_to_rgb(pred_all[j],id2code))\n        ax3.grid(False)\n\n        plt.savefig('.\/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n        plt.show()","8908717d":"image = load_img('..\/input\/augmented-dubai-aerial-imagery-dataset\/val_images\/val\/image_t4_008.jpg', target_size= (512, 512))\nimage = img_to_array(image)\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\nimage = preprocess_input(image)\ny_hat = model.predict(image)","87aafd12":"layers=['conv2d', 'conv2d_4', 'conv2d_8', 'conv2d_10', 'conv2d_22', 'conv2d_28', 'conv2d_34', 'conv2d_40', 'conv2d_52', 'conv2d_61', 'conv2d_67', 'conv2d_70']","bbff2ac0":"activations= keract.get_activations(model, image, layer_names= layers, nodes_to_evaluate= None, output_format= 'simple', auto_compile= True)\nkeract.display_activations(activations, cmap='viridis', save= False, directory= '.\/activations')","0fadda8b":"# Working with Augmented Dataset","e2a5cd3a":"# Custom Image Data Generators for Creating Batches of Frames and Masks","4bbe325d":"## InceptionResNetV2-UNet","edc6d72b":"# Model","5c950663":"## Predictions","b336a624":"# Dubai Satellite Imagery Semantic Segmentation\nHumans in the Loop has published an open access dataset annotated for a joint project with the Mohammed Bin Rashid Space Center in Dubai, the UAE. \n\nThe dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The images were segmented by the trainees of the Roia Foundation in Syria.\n\nOriginal Dataset Link: https:\/\/humansintheloop.org\/resources\/datasets\/semantic-segmentation-dataset\/","326363ca":"## Model's Activations (Outputs) Visualization","f250c278":"# Model Training","51604224":"# Creating Custom Image Data Generators\n## Defining Data Generators","3b74cec3":"# Define Functions for One Hot Encoding RGB Labels & Decoding Encoded Predictions","4dcb496f":"# Data Augmentation using Albumentations Library\n[Albumentations](https:\/\/albumentations.ai\/) is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.\n\nData augmentation is done by the following techniques:\n\n1. Random Cropping\n2. Horizontal Flipping\n3. Vertical Flipping\n4. Rotation\n5. Random Brightness & Contrast\n6. Contrast Limited Adaptive Histogram Equalization (CLAHE)\n7. Grid Distortion\n8. Optical Distortion","a640503d":"## Training Results","e50bb7cd":"### Model Layout","2e0bb62a":"# Saving Augmented Images to Disk\nI have already performed data augmentation and saved the images. I am not running it's code in this notebook. It is a very time consuming process, so be patient while the code cell runs!","13b70de3":"# Create Useful Label & Code Conversion Dictionaries\n\nThese will be used for:\n\n* One hot encoding the mask labels for model training\n* Decoding the predicted labels for interpretation and visualization","f206cdb8":"# Installing & Importing Libraries"}}