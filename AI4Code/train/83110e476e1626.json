{"cell_type":{"7667032c":"code","295491c5":"code","b0c5c960":"code","83fede68":"code","321197f2":"code","cde883da":"code","fca3020e":"code","b5469b4d":"code","1d00e807":"code","6063edc2":"markdown","a189c704":"markdown","28c7e580":"markdown","5f6d94c6":"markdown","179cb480":"markdown","1cbc0423":"markdown","6471a70c":"markdown","e581df61":"markdown","fe38b894":"markdown","213ecc17":"markdown"},"source":{"7667032c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","295491c5":"test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')","b0c5c960":"submission.shape","83fede68":"n = 0.72 ** 2\nprint(n)","321197f2":"import numpy.random as rd","cde883da":"np.sqrt(sum(rd.chisquare(n, 5000))\/5000)","fca3020e":"samples = 10000\n\nres = np.zeros(samples)\n\nfor i in range(samples):\n    res[i] = np.sqrt(sum(rd.chisquare(n, 5000))\/5000)","b5469b4d":"plt.hist(res, bins = 100)","1d00e807":"np.mean(res), np.std(res)","6063edc2":"Let's say I got 0.72 for the public score. ","a189c704":"Let us estimate the underlying error distribution is $\\chi^2$ distribution with $n = 0.5184$.","28c7e580":"It seems like a good shot!","5f6d94c6":"Now let's do this repeatedly for 10000 times.","179cb480":"In this notebook, I tried to measure the sampling vias to quantify how much we should stick to the LB","1cbc0423":"## Conclusion\n\u30fb If your improvement or difference between you and the target kaggler was up to 0.06, it might be just a samling vias.\n\n\u30fbGiven how competitive this competition is, this is actually a $\\bf{huge}$ difference.\n\n\u30fb Don't be too crazy about the LB.","6471a70c":"Checking I'm not wrong with random sampling of 5000 data points and calculating RMSE.","e581df61":"There are 20000 data points in the test set. Namely, LB is calculated upon 5000 data points, the final results is upon 1500 data points.","fe38b894":"NOTE: this is just a guess, as [@paddykb](http:\/\/https:\/\/www.kaggle.com\/paddykb) pointed out, If the split is stratified, the difference between public and private could be much smaller. (Say 0.001ish)","213ecc17":"### Acknowledgement:\nMy work is heavily influenced by https:\/\/www.kaggle.com\/paddykb\/tps-08-shake-it-up\/comments#1462560.\n\nIf you found this notebook is useful, I'd appreciated if you could also upvote his notebook as well."}}