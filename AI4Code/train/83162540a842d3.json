{"cell_type":{"37a7fc1e":"code","4a039ada":"code","8d2d40bd":"code","a1ad3003":"code","57f696eb":"code","11e0ce45":"code","8a10d193":"code","0ec6f480":"code","4e51ede5":"code","7a63a020":"code","d98dd102":"code","bcf0bc7b":"code","fcb6e22f":"code","5f553dd8":"code","f90c2776":"code","c2ebae29":"code","0e11e598":"code","ff4dbace":"code","b1ce591d":"code","252a7c47":"code","3ac6551d":"code","d5378f17":"code","9530bd01":"code","2673236c":"code","bb5981f5":"code","74a48e48":"code","84b83e0e":"code","7acab498":"code","a99dc7c2":"code","0b0f7624":"code","d60a23e3":"code","7682e95e":"code","0a9b4dd2":"code","872a72e1":"code","ba8032c4":"code","4e262e79":"code","8621bd0f":"code","b2affcf5":"code","a2c98b2b":"code","4f8dd902":"code","4ed22f81":"code","1ed5d4d6":"code","7c265e72":"code","4be48e2b":"code","22ca03a5":"code","c9a2ef58":"code","3f6e50fb":"code","9ee90c07":"code","174b3995":"code","1e38cdde":"code","ed6fef01":"code","0ba6c454":"code","40b7066e":"code","15f71bb0":"code","781199d7":"code","80c2fa61":"code","bd4450be":"code","945a8994":"code","7cad407c":"code","68883216":"code","a0894f91":"code","0ebb3720":"code","4add62eb":"code","fe456423":"code","80b2b69b":"code","0c782aea":"code","2abbb807":"code","0db49440":"code","93839df7":"code","8b44bcf3":"code","342a1c87":"code","fa18fff0":"code","6becd4af":"code","015b456f":"code","6f3076a4":"code","9b103742":"code","52fec6c0":"code","9d9fa8bf":"code","04c50586":"code","93823491":"code","040f0f66":"code","3f2aa975":"code","8f675789":"code","91a9b315":"code","37305b81":"code","6b6ec47c":"code","ed9fe157":"code","a082ea47":"code","efa3ab26":"code","633c40a7":"code","9a43f3e1":"code","aaa63e1d":"code","247c9b7b":"code","85e0d131":"code","12bafd15":"code","45264b73":"code","2854b99c":"code","7d2ed044":"code","d57d24b5":"code","dc0c5a30":"markdown","a4205973":"markdown","ddb09c51":"markdown","ef54d6a5":"markdown","ccb08c1c":"markdown","5cce67f6":"markdown","3f0ec1f0":"markdown","9421b4d8":"markdown","6a53b99f":"markdown","e2abd8e8":"markdown","66b61702":"markdown","d6442fea":"markdown","0297af71":"markdown","018c41fd":"markdown","19b19f76":"markdown","1307aa5f":"markdown","8550f80c":"markdown","5cc991f3":"markdown","95a67414":"markdown","e5a9124c":"markdown","407f06aa":"markdown","89d9957a":"markdown","80171581":"markdown","1ef6c2de":"markdown","5fe86349":"markdown","34645aea":"markdown","c2bca0fb":"markdown","3582c444":"markdown","d5900c7f":"markdown","7d5a1fa0":"markdown"},"source":{"37a7fc1e":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\n\nimport random\nimport math\n\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\n\n%matplotlib inline","4a039ada":"np.random.seed(43)","8d2d40bd":"PROJECT_NAME='final-course-project'","a1ad3003":"!pip install jovian --upgrade -q","57f696eb":"import jovian","11e0ce45":"dir(jovian)","8a10d193":"jovian.commit(project=PROJECT_NAME, environment=None)","0ec6f480":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nbad_file = False\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if not filename.endswith('.jpg') and not filename.endswith('.jpeg') and not filename.endswith('.png'):  # If it does not end with .jpg or .jpeg or .png extension\n            bad_file = True\n            print(os.path.join(dirname, filename))  # Show the file that does not have a .jpg or .jpeg extension\n            print('-'*80)  # Print a line just under the file that does not end with .jpg or .jpeg or .png extension\n\nif bad_file == False:\n    print(\"All files in the dataset are of .jpg or .jpeg or .png format only.\")\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e51ede5":"DATA_DIR = os.path.join('..', 'input', 'newFlowers')\nprint(DATA_DIR)\nprint(type(DATA_DIR))","7a63a020":"os.listdir(DATA_DIR)","d98dd102":"os.getcwd()","bcf0bc7b":"class DataLoadingPipeline:\n    def __init__(self, data_dir: str, validation_fraction:float=0.15,\n                 test_fraction:float=0.2, train_transforms:list=[T.ToTensor(),],\n                 valid_and_test_transforms:list=[T.ToTensor(),], batch_size:int=64):\n        \"\"\"\n            This function sets up the basic data loader for the dataset. The data loaded is not moved to GPU by this method.\n            data_dir: {str}  ==> This is the absolute path to the dataset folder\n            validation_fraction: {float} ==> This is the fraction of train set that will be used for validation\n            test_fraction: {float} ==> This is the fraction of dataset that will be used for testing, rest will be used for training (and validation of course)\n            train_transforms: {list} ==> This is the list of transforms which will be applied to the train set\n            valid_and_test_transforms: {list} ==> This is the list of transforms which will be applied to the validation and test set\n        \"\"\"\n        # Creating the transforms pipeline for each of the fractions of the dataset\n        self.train_transforms = T.Compose(train_transforms)\n        self.valid_transforms = T.Compose(valid_and_test_transforms)\n        self.test_transforms  = T.Compose(valid_and_test_transforms)\n        \n        # Reading the folder containing the images for creating the initial train, validation and test datasets\n        self.train_data = ImageFolder(data_dir, transform=self.train_transforms)\n        self.validation_data = ImageFolder(data_dir, transform=self.valid_transforms)\n        self.test_data = ImageFolder(data_dir, transform=self.test_transforms)\n        self.train_validation_data = ImageFolder(data_dir, transform=self.train_transforms)\n        \n        # Creating a dictionary for storing the conversion from the lable number to the flower name\n        self.classes = {}\n        # Print out the classes in the dataset along with their corresponding index\n        print(\"The classes in this dataset are:\")\n        for ctr, i in enumerate(self.train_data.classes):\n            print(f\"{ctr}: {i.capitalize()}\")\n            self.classes[ctr] = i\n        print()  # Print a newline for better output formatting\n\n        self.count = num_train = len(self.train_data)     # Get the total number of images in the dataset\n        print(f\"The dataset has {num_train} images\")\n        indices = list(range(num_train))  # Create a list of indices for the all images in the dataset\n\n        test_split = int(np.floor(test_fraction * num_train))  # Getting the number of images in the test set\n        train_validation_split = num_train - test_split        # Getting the number of images in the train and validation set\n        validation_split = int(np.floor(validation_fraction * train_validation_split))  # Getting the number of images in the validation set\n        train_split = train_validation_split - validation_split  # Getting the number of images in the train set\n\n        # Construct a new Generator with the default BitGenerator (PCG64), this will hellp us shuffle the indices randomly\n        rng = np.random.default_rng()\n        rng.shuffle(indices)  # Shuffling the indices so that every set gets approximately equal number of images for each class\n\n        # Splitting the indices list into train_validation and test indices lists\n        train_validation_idx, test_idx = indices[test_split:], indices[:test_split]\n        \n        # Reshuffling the train_validation indices list; preparing it for another split\n        rng = np.random.default_rng()\n        rng.shuffle(train_validation_idx)\n        \n        # Further split the train_validation indices list into train indices list and validation indices list\n        train_idx, validation_idx = train_validation_idx[validation_split:], train_validation_idx[:validation_split]\n\n        # Just for a sanity check, lets check if the train, validation and test sets have all got unique indices and none have overlapped\n        # as that would mean that the corresponding image is in both the sets its indice occurs in\n        if not set(train_idx).intersection(set(validation_idx)) and not set(validation_idx).intersection(set(test_idx)) and \\\n           not set(train_idx).intersection(set(test_idx)):\n            print(\"[PASS] The splits are mutually exculsive of each other!\")\n        else:\n            print(\"[FAIL] The splits are not mutually exculsive of each other!\")\n            \n        \n        \n        # We now create random samplers to take random samples of indices from the indices lists we created.\n        # This will make sure that the train set only accesses the images referred to by the train images indices list\n        # and the same applies to the validation and test sets and their validation and test images indices lists.\n        train_sampler = SubsetRandomSampler(train_idx)\n        validation_sampler = SubsetRandomSampler(validation_idx)\n        test_sampler = SubsetRandomSampler(test_idx)\n        \n        # This will be used to make sure that the model gets an opportunity to train on more data.\n        # As we cannot use the test set for this (for obvious reasons), we instead use the validation set, so hence the below dataloader\n        # for using the train and validation set together for training, this will be used towards the end of the training process\n        train_validation_sampler = SubsetRandomSampler(train_idx + validation_idx)  # This is a combination of the train and validation sets\n        \n        # We now create the dataloaders for the train, validation and test sets\n        self.train_loader = DataLoader(self.train_data, sampler=train_sampler, batch_size=batch_size, num_workers=3, pin_memory=True)\n        self.validation_loader = DataLoader(self.validation_data, sampler=validation_sampler, batch_size=batch_size, num_workers=3, pin_memory=True)\n        self.test_loader = DataLoader(self.test_data, sampler=test_sampler, batch_size=batch_size, num_workers=3, pin_memory=True)\n        self.train_validation_loader = DataLoader(self.train_validation_data, sampler=train_validation_sampler, batch_size=batch_size, num_workers=3, pin_memory=True)\n       ","fcb6e22f":"RESIZE_DIM = 300\nIMG_DIM = 256     # We want all images to be of dimension 128x128\nBATCH_SIZE = 128  # 64\nimagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\nTRAIN_TRANSFORMS = [\n                    T.Resize(RESIZE_DIM, interpolation=Image.BICUBIC),\n                    T.CenterCrop(IMG_DIM),\n                    T.ColorJitter(brightness=0.1, contrast=0.2, saturation=0.1),\n                    T.RandomHorizontalFlip(), \n                    # T.RandomCrop(IMG_DIM, padding=8, padding_mode='reflect'),\n                    # T.RandomRotation(10),  #  Did not give any improvements, some images lost important details as they go cut off\n                    T.ToTensor(), \n                    # T.Normalize(*imagenet_stats,inplace=True),  #  Did not give good results, converted some images into just white squares\n                    T.RandomErasing(inplace=True, scale=(0.01, 0.23)),\n                   ]\n\nVALIDATION_and_TEST_TRANSFORMS = [\n                                  T.Resize(RESIZE_DIM, interpolation=Image.BICUBIC),\n                                  T.CenterCrop(IMG_DIM),\n                                  T.ToTensor(), \n                                  # T.Normalize(*imagenet_stats)  #  Did not give good results, converted some images into just white squares\n                                 ]","5f553dd8":"VALIDATION_FRACTION = 0.15\nTEST_FRACTION = 0.2\n\n\nflowers_data_loader = DataLoadingPipeline(\n                                          data_dir=DATA_DIR,\n                                          validation_fraction=VALIDATION_FRACTION,\n                                          test_fraction=TEST_FRACTION,\n                                          train_transforms=TRAIN_TRANSFORMS,\n                                          valid_and_test_transforms=VALIDATION_and_TEST_TRANSFORMS,\n                                          batch_size=BATCH_SIZE\n                                         )","f90c2776":"help(jovian.log_dataset)","c2ebae29":"train_loader_batches_count, test_loader_batches_count, validation_loader_batches_count = len(flowers_data_loader.train_loader),\\\n                                                                                         len(flowers_data_loader.test_loader),\\\n                                                                                         len(flowers_data_loader.validation_loader)\n\n# Let's see the batch sizes for the different sets of data\nprint(f\"{'The number of training batches are': <40} {train_loader_batches_count:^4}, each of size of {BATCH_SIZE: ^4}\")\nprint(f\"{'The number of testing batches are': <40} {test_loader_batches_count: ^4}, each of size of {BATCH_SIZE: ^4}\")\nprint(f\"{'The number of validation batches are': <40} {validation_loader_batches_count: ^4}, each of size of {BATCH_SIZE: ^4}\")\n\n# Also lets show the data in the form of a simple tuple representation\nprint(f\"({train_loader_batches_count}, {test_loader_batches_count}, {validation_loader_batches_count})\")","0e11e598":"jovian.log_dataset(dataset_url='https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition',\n                   val_fraction=VALIDATION_FRACTION,\n                   test_fraction=TEST_FRACTION,\n                   train_batches=train_loader_batches_count,\n                   test_batches=test_loader_batches_count,\n                   validation_batches=validation_loader_batches_count)","ff4dbace":"flowers_data_loader.train_loader.dataset.classes == flowers_data_loader.test_loader.dataset.classes\nflowers_data_loader.test_loader.dataset.classes == flowers_data_loader.validation_loader.dataset.classes","b1ce591d":"for images, labels in flowers_data_loader.train_loader:\n    print(images)\n    print(labels)\n    break","252a7c47":"for images, labels in flowers_data_loader.train_validation_loader:\n    print(images)\n    print(labels)\n    break","3ac6551d":"for images, labels in flowers_data_loader.test_loader:\n    print(images)\n    print(labels)\n    break","d5378f17":"def show_sample(data_item_obj, classes:dict, invert:bool=False):\n    print(\"The tensor representing the image and the target image\", data_item_obj)\n    img, target = data_item_obj  # This is a particular data item from the data set having its own image and label\n    if invert:\n        plt.imshow(1 - img.permute((1, 2, 0)))\n    else:\n        plt.imshow(img.permute(1, 2, 0))\n    plt.title(classes[target])\n    print('Labels:', classes[target])","9530bd01":"def random_no_gen(no_of_elements:int, lower_limit:int=0) -> int:\n    return lower_limit + math.floor(random.random() * no_of_elements)","2673236c":"show_sample(flowers_data_loader.train_data[random_no_gen(flowers_data_loader.count)], flowers_data_loader.classes)","bb5981f5":"show_sample(flowers_data_loader.train_data[random_no_gen(flowers_data_loader.count)], flowers_data_loader.classes)","74a48e48":"show_sample(flowers_data_loader.train_data[random_no_gen(flowers_data_loader.count)], flowers_data_loader.classes)","84b83e0e":"show_sample(flowers_data_loader.train_data[random_no_gen(flowers_data_loader.count)], flowers_data_loader.classes)","7acab498":"show_sample(flowers_data_loader.train_data[random_no_gen(flowers_data_loader.count)], flowers_data_loader.classes)","a99dc7c2":"def show_batch(dl, invert:bool=False):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(8, 16))\n        ax.set_xticks([]); ax.set_yticks([])\n        data = 1-images if invert else images\n        ax.imshow(make_grid(data, nrow=8).permute(1, 2, 0))\n        break","0b0f7624":"show_batch(flowers_data_loader.train_loader, invert=True)","d60a23e3":"show_batch(flowers_data_loader.train_loader)","7682e95e":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","0a9b4dd2":"device = get_default_device()\ndevice","872a72e1":"flowers_data_loader.train_loader = DeviceDataLoader(flowers_data_loader.train_loader, device)\nflowers_data_loader.validation_loader = DeviceDataLoader(flowers_data_loader.validation_loader, device)\nflowers_data_loader.test_loader = DeviceDataLoader(flowers_data_loader.test_loader, device)\nflowers_data_loader.train_validation_loader = DeviceDataLoader(flowers_data_loader.train_validation_loader, device)","ba8032c4":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    @torch.no_grad()\n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n\n    @torch.no_grad()\n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    @torch.no_grad()\n    def evaluate_test_set(self, test_dataset):\n        result = evaluate(self, test_dataset)\n        print(\"The results are: test_loss: {:.4f}, test_acc: {:.4f}\".format(result['val_loss'], result['val_acc']))\n        return {'test_loss': result['val_loss'], 'test_acc': result['val_acc']}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","4e262e79":"COMMON_IN = 2048  # The input to the classifier layer is 2048 from the rest of ResNet50, so this value is fixed for ResNet50\nNUM_CLASSES = 5   # There are 5 classes of flowers and for each we need to return a prediction probability, in the end\n\nCLASSIFIER_ARCHITECTURES = {\n                            \"Simple With Dropout\": nn.Sequential(nn.Linear(2048, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128, NUM_CLASSES)),\n                            \"Simple Without Dropout\": nn.Sequential(nn.Linear(2048, 128),nn.ReLU(), nn.Linear(128, NUM_CLASSES)),\n                            \"Medium With Dropout\": nn.Sequential(nn.Linear(2048, 256), nn.Dropout(0.1), nn.ReLU(), nn.Linear(256, 64), nn.Dropout(0.01), nn.ReLU(), nn.Linear(64, NUM_CLASSES)),\n                            \"Medium Without Dropout\": nn.Sequential(nn.Linear(2048, 256), nn.ReLU(), nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, NUM_CLASSES))\n                            }\n\nclass ResNet50(ImageClassificationBase):\n    def __init__(self, num_classes:int):\n        super().__init__()\n        self.model = models.resnet50(pretrained=True)\n        # Freeze parameters so we don't backprop through them\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        self.model.fc = CLASSIFIER_ARCHITECTURES[\"Medium With Dropout\"]\n        \n        for param in self.model.fc.parameters():\n            param.require_grad = True\n        \n    def forward(self, xb):\n        return self.model(xb)\n\n    def switch_on_gradients(self):\n        for param in self.model.parameters():\n            param.requires_grad = True\n\n    def switch_off_gradients_except_classifier(self):\n        # We first switch off the requires_grad parameter for all the layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        # We then only switch on the requires_grad parameter for the layers of the (fc) classifer layer\n        for param in self.model.fc.parameters():\n            param.require_grad = True","8621bd0f":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader),\n                                                cycle_momentum=True)\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train() # Switches on training mode\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad() # reset the gradients\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","b2affcf5":"model = to_device(ResNet50(num_classes=5), device)\nmodel","a2c98b2b":"jovian.commit(project=PROJECT_NAME, environment=None)","4f8dd902":"torch.cuda.empty_cache()","4ed22f81":"lrs = []\nepochs_list = []\ntrain_times = []","1ed5d4d6":"history = [evaluate(model, flowers_data_loader.validation_loader)]\nhistory","7c265e72":"epochs = 10\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-5\nopt_func = torch.optim.Adam\n\n# Logging the hyper-parameters\nlrs.append(max_lr)\nepochs_list.append(epochs)","4be48e2b":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, flowers_data_loader.train_loader, flowers_data_loader.validation_loader, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","22ca03a5":"train_times.append('5min 36s')","c9a2ef58":"model.switch_on_gradients()\nmodel","3f6e50fb":"epochs = 20\nmax_lr = 0.001\ngrad_clip = 0.05\nweight_decay = 1e-4\nopt_func = torch.optim.Adam\n\n# Logging the hyper-parameters\nlrs.append(max_lr)\nepochs_list.append(epochs)","9ee90c07":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, flowers_data_loader.train_loader, flowers_data_loader.validation_loader, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","174b3995":"train_times.append('4min 4s')","1e38cdde":"def save_clear_reload_model(model):\n    torch.save(model.state_dict(), 'flowers_cnn.pth')\n    torch.cuda.empty_cache()\n    model = to_device(ResNet50(num_classes=5), device)\n    model.load_state_dict(torch.load('flowers_cnn.pth'))","ed6fef01":"save_clear_reload_model(model)\nmodel.switch_on_gradients()\nmodel","0ba6c454":"epochs = 10\nmax_lr = 0.001\ngrad_clip = 0.015\nweight_decay = 1e-4\nopt_func = torch.optim.Adam\n\n# Logging the hyper-parameters\nlrs.append(max_lr)\nepochs_list.append(epochs)","40b7066e":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, flowers_data_loader.train_validation_loader,flowers_data_loader.test_loader, \n                                           grad_clip=grad_clip, \n                                           weight_decay=weight_decay, \n                                           opt_func=opt_func)","15f71bb0":"train_times.append('2min 19s')","781199d7":"save_clear_reload_model(model)","80c2fa61":"epochs = 10\nmax_lr = 0.0001\ngrad_clip = 0.005\nweight_decay = 1e-5\nopt_func = torch.optim.Adam\n\n# Logging the hyper-parameters\nlrs.append(max_lr)\nepochs_list.append(epochs)","bd4450be":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, flowers_data_loader.train_validation_loader, flowers_data_loader.test_loader,\n                                           grad_clip=grad_clip, \n                                           weight_decay=weight_decay, \n                                           opt_func=opt_func)","945a8994":"train_times.append('2min 20s')","7cad407c":"T_MODEL = \"RESNET-50-Pretrained\"\nCLASSIFER_LAYER_1 = 2048\nCLASSIFER_LAYER_2 = 256\nCLASSIFER_LAYER_3 = 64\nCLASSIFER_LAYER_4 = 5","68883216":"jovian.log_hyperparams(arch=f\"{T_MODEL} --> Classifer layers: ({CLASSIFER_LAYER_1}, {CLASSIFER_LAYER_2}, {CLASSIFER_LAYER_3}, {CLASSIFER_LAYER_4})\", \n                       lrs=lrs, \n                       epochs=epochs_list,\n                       times=train_times,\n                       img_dimensions=IMG_DIM,\n                       batch_size=BATCH_SIZE,\n                       validation_fraction=VALIDATION_FRACTION,\n                       test_fraction=TEST_FRACTION\n                       )","a0894f91":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","0ebb3720":"plot_accuracies(history)","4add62eb":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","fe456423":"plot_losses(history)","80b2b69b":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","0c782aea":"plot_lrs(history)","2abbb807":"history.append(model.evaluate_test_set(flowers_data_loader.test_loader))","0db49440":"history[-1]","93839df7":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    print(\"The prediction tensor is:\")\n    for i in range(len(flowers_data_loader.classes)):\n        print(f\"{flowers_data_loader.classes[i]:^10}\",\" : \",F.softmax(yb, dim=1)[0][i].item())\n    # Retrieve the class label\n    return flowers_data_loader.classes[preds[0].item()]","8b44bcf3":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","342a1c87":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","fa18fff0":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","6becd4af":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","015b456f":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","6f3076a4":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","9b103742":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","52fec6c0":"for imgs, labels in flowers_data_loader.test_loader:\n    for img, label in zip(imgs[:1], labels[:1]):\n        plt.imshow(img.cpu().permute(1, 2, 0))\n        print('Label:', flowers_data_loader.classes[label.item()], ', Predicted:', predict_image(img.cpu(), model))\n    break","9d9fa8bf":"other_imgs_path = os.path.join('..', 'input', 'other_imgs')\nother_imgs_path","04c50586":"other_images = ImageFolder(other_imgs_path, transform=T.Compose(VALIDATION_and_TEST_TRANSFORMS))","93823491":"LEN_OF_OTHER_IMAGES = len(other_images)\nLEN_OF_OTHER_IMAGES","040f0f66":"other_images.classes","3f2aa975":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    print(\"The prediction tensor is:\")\n    for i in range(len(flowers_data_loader.classes)):\n        print(f\"{flowers_data_loader.classes[i]:^10}\",\" : \",F.softmax(yb, dim=1)[0][i].item())\n    # Retrieve the class label\n    return flowers_data_loader.classes[preds[0].item()]","8f675789":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', other_images.classes[label], ', Predicted:', predict_image(img, model))","91a9b315":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","37305b81":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","6b6ec47c":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","ed9fe157":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","a082ea47":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","efa3ab26":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","633c40a7":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","9a43f3e1":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","aaa63e1d":"img, label = other_images[random_no_gen(LEN_OF_OTHER_IMAGES)]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', flowers_data_loader.classes[label], ', Predicted:', predict_image(img, model))","247c9b7b":"jovian.log_metrics(test_loss=history[-1]['test_loss'],\n                   test_acc=history[-1]['test_acc'])","85e0d131":"jovian.commit(project=PROJECT_NAME, environment=None)","12bafd15":"torch.save(model.state_dict(), 'flowers_cnn.pth')","45264b73":"sanity_check_model = to_device(ResNet50(num_classes=5), device)","2854b99c":"sanity_check_model.load_state_dict(torch.load('flowers_cnn.pth'))","7d2ed044":"sanity_check_model.evaluate_test_set(flowers_data_loader.test_loader)","d57d24b5":"jovian.commit(project=PROJECT_NAME, outputs=['flowers_cnn.pth'], environment=None)","dc0c5a30":"## Preparing the data for training\nWe'll use a validation set with about 510 images (12% of the dataset). To ensure we get the same train, validation and test set each time, we'll set the random number generator to a seed value of 43. This is done so that the results of running this notebook can be reproduced by others as well.","a4205973":"## Let's try evaluating the model with the validation set without any prior training\n","ddb09c51":"---","ef54d6a5":"## Now lets finally run the model on the test set and see the results","ccb08c1c":"### Now lets take a look at the structure of the dataset folder","5cce67f6":"## We now train the model on the train and validation sets together so that we can give our model one final boost, to get the best results during the final run on the test set","3f0ec1f0":"Namely its quite interesting that eventhough the classification layer is randomly initiated, it is still able to get a **accuracy of 15%**. This may be because the *rest of the model* is already `pretrained`.","9421b4d8":"# Flower Recognition\n## Recognizing the type of flower from its image\n\n### About the dataset:\n- This dataset has been derived from the [Kaggle Flowers Recognition Dataset](https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition). I have further cleaned the dataset by removing irrevelant files and images contained in the different folders corresponding to each class of flower (More on this later)\n- This dataset contains approx. 4138 images of flowers, the distribution of images for each type of flower is not equal\n- The data collection is based on the data from **flicr**, **google images**, **yandex images**\n\n### Dataset Contents:\n- The images are divided into five classes: \n    1. *daisy*\n    2. *dandelion*\n    3. *rose*\n    4. *sunflower*\n    5. *tulip*\n- For each class of flowers there are about 800 photos each\n- The images are not high resolution, each of them are about `320x240` pixels\n- The images are not reduced to a single size, they are of different proportions\n\n### Acknowledgements\n- The data collection is based on scraped data from **flicr**, **google images**, **yandex images**","6a53b99f":"Just as a sanity check, let's verify that this model has the same loss and accuracy on the test set as before.","e2abd8e8":"### Also before starting work on the project, lets also set the name of out project","66b61702":"## The function below generates random integers in the range of: *0 to (number_of_elements_in_the_data_set - 1)*\n### The reason for this upper and lower limit is that we need to generate random integers representing the indices of data points in the dataset","d6442fea":"Let's make one final commit using jovian, just to be sure everything is committed.","0297af71":"## Now we are checking the train, validation and test data loaders to see if they all got images from the different classes in the dataset","018c41fd":"So at the end of this notebook, we can see that we have quite good results on a dataset that is not very clean and whose images are not regular in size, as was visible by the large black borders around many of the images. This shows the power of CNNs and transfer learning as well as picking a good classifier layer for the task.\n\nIn terms of future work, I would love to scrape and collect more images of the flowers in this dataset as well as images of flowers not it this dataset from various websites and other online sources, in an attempt to make the model more generalizable.\n\nThe uses of a such a model are quite a lot, it can be used by people visiting botanical garderns and flower shows alike, as many at times people may not know what flower they are looking at. Rather than doing pointless Google searches, they can instead use their camera which can send the video output to the model which can then figure out which flower is being presented to it and show the result text on top of the image itself as an overlay on the user's screen.","19b19f76":"### Also let's get the absolute path to the current directory","1307aa5f":"### In the original [Kaggle Flowers Recognition Dataset](https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition), a few of the image folders had `.py` and `.pyc` files in them, due to which the model while loading the data from the folders would load these files as well thinking that they are images. To correct this issue, I downloaded the dataset and removed these files from the various image folders and re-uploaded the corrected dataset onto my Kaggle account.\n### The below cell of code just checks all the image folders to make sure that there are no such files left in the folders, which are not in `.jpg` or `.jpeg` or `.png` format. Just in case there are such files which are not of `.jpg` or `.jpeg` or `.png` format then the program prints their names out, so that I can remove those files.\n#### On Kaggle the dataset folder is read-only so if any corrections are required then I need to first download the dataset and then make the needed corrections and finally re-upload it onto Kaggle.","8550f80c":"## Taking a Glance at the Data\n### Now lets take a look at some of the images in the dataset, along with their respective labels","5cc991f3":"Check out the **Files tab** on the project page to view or download the trained model weights. You can also download all the files together using the *Download Zip* option in the *Clone* dropdown.","95a67414":"## Now let's set the basic parameters for the dataset before we create a `DataLoaderPipeline object` for it","e5a9124c":"## Creating a DataLoadingPipeline class to accumulate all the dataloading operations in one place.\n### In order to bring all the dataloading operations to one place, I have created a class that will load the data and apply all the required transforms to the it as well.\n\n\n- The object of this class contains the train, validation and test data loaders as well\n- The other perculiar feature of the dataset is that there is no separate train, validation and test set, there is just one dataset of images that needs to be manually split into train, validation and test sets. \n    * Rather than restructuring the whole dataset, which can be time consuming and unnecessary, we will use a SubsetRandomSampler which will only pick those images whose respective indices we provide. \n    * This way eventhough the train_data, validation_data and test_data attributes of the object, all have the same images; we can create one `SubsetRandomSampler` object for each set and provide each *one different and mutually exclusive indices*. This way the images in the train_loader, validation_loader and test_loader attributes of the object will all be different.","407f06aa":"Now let's install jovain in our environment and perform an inital commit","89d9957a":"Looks like all the dataloaders got at least some images from each class in the dataset","80171581":"---","1ef6c2de":"## Before starting the various training steps, let's clear the GPU cache first\n### This is done so that the GPU does not run out of memory during the training steps","5fe86349":"# Summary & Future Work\n","34645aea":"## Training the model\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\n* **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the **\"One Cycle Learning Rate Policy\"**, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n* **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab\n\n* **Gradient clipping**: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48\n\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","c2bca0fb":"In case you forget, the `.state_dict` method returns an **OrderedDict** containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method.","3582c444":"## Now lets try the model on some images not in this dataset","d5900c7f":"## Saving and Loading the model for later:\n\nSince we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch.","7d5a1fa0":"## Now lets see the number of batches in each of the dataloaders"}}