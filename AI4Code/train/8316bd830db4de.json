{"cell_type":{"19d460c1":"code","5027f203":"code","db2a5bdb":"code","8bb8edb9":"code","e53bb671":"code","5619e7ac":"code","e91eaedc":"code","efbf016c":"code","92903d1b":"code","44333768":"code","df06de03":"code","1b058e6f":"code","a3ca469e":"code","797f2e40":"code","b64f3615":"code","7e0d649d":"code","46fd9968":"code","79b09ec0":"code","5f003e4e":"code","5e4a0881":"code","e0ac6bd4":"code","8e9c02db":"code","76bb5532":"code","caff9dd4":"code","4ae9b676":"code","c829a26b":"code","e0c34c9e":"code","4ded74bd":"code","a110c981":"code","7294f111":"code","91dcb37d":"code","5e94b0dc":"code","71a32518":"code","c09ba5dd":"code","b2f865eb":"code","c754b886":"code","8c220173":"code","11e9888e":"code","efaa395f":"code","c6adaadb":"code","4b2a67a1":"code","6594a385":"code","460732a5":"code","01da14b8":"markdown","1d8eea1d":"markdown","e4c7be46":"markdown","a7f691ac":"markdown","f2ec8777":"markdown","5626376d":"markdown","25c60783":"markdown","d89d9eab":"markdown","846bc48c":"markdown","74a68f73":"markdown","ca313495":"markdown","462dc461":"markdown","69c4a630":"markdown","9d906d1a":"markdown","33ef7b91":"markdown","e80b8555":"markdown","1993b424":"markdown","ff40a559":"markdown","b98a3735":"markdown","3fb0d38e":"markdown","f989ca95":"markdown","f065064b":"markdown","7ea9f81f":"markdown","2e42a1c2":"markdown","13593786":"markdown","9648bdeb":"markdown","b3674028":"markdown","1280c5e1":"markdown"},"source":{"19d460c1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, GRU\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.callbacks import History \n\nfrom wordcloud import WordCloud, STOPWORDS","5027f203":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nreal.head()","db2a5bdb":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake.head()","8bb8edb9":"real['Category'] = 1\nfake['Category'] = 0","e53bb671":"print(real.shape)\nreal.head()","5619e7ac":"print(fake.shape)\nfake.head()","e91eaedc":"dataset = pd.concat([real, fake]).reset_index(drop=True)","efbf016c":"print(dataset.shape)\ndataset.head()","92903d1b":"import gc\ndel [[real,fake]]\ngc.collect()","44333768":"dataset.isnull().sum()","df06de03":"dataset['final_text'] = dataset['title'] + dataset['text']\ndataset['final_text'].head()","1b058e6f":"dataset['Category'].value_counts()","a3ca469e":"sns.countplot(dataset[\"Category\"])","797f2e40":"dataset[['Category','subject','final_text']].groupby(['Category','subject']).count()","b64f3615":"plt.figure(figsize=(10,5))\nsns.countplot(x= \"subject\", hue = \"Category\", data=dataset)","7e0d649d":"final_text_result = []\nfor text in dataset['final_text']:\n    result = re.sub('[^a-zA-Z]', ' ', text)\n    result = result.lower()\n    result = result.split()\n    result = [r for r in result if r not in set(stopwords.words('english'))]\n    final_text_result.append(\" \".join(result))","46fd9968":"print(len(final_text_result))","79b09ec0":"X_final = np.array(final_text_result)\ny_final = dataset['Category']","5f003e4e":"X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.3, random_state = 0)","5e4a0881":"from tensorflow.keras.preprocessing import text\nmax_words = 10000\ntokenizer = text.Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)","e0ac6bd4":"# Generate sequence of Tokens\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)","8e9c02db":"# Pad the sequences\nsent_length = 400\nX_train_pad = sequence.pad_sequences(X_train_sequence, maxlen=sent_length)\nX_test_pad = sequence.pad_sequences(X_test_sequence, maxlen=sent_length)","76bb5532":"word_index = tokenizer.word_index","caff9dd4":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","4ae9b676":"dataset = reduce_mem_usage(dataset)\ndel final_text_result\ndel X_final\ndel y_final\ndel X_train\ndel X_test\ndel X_train_sequence\ndel X_test_sequence\ngc.collect()","c829a26b":"GLOVE_EMBEDDINGS_FILE = \"..\/input\/glovetwitter27b100d\/glove.twitter.27B.100d.model\"","e0c34c9e":"%%time\nembedding_vectors = {}\nwith open(GLOVE_EMBEDDINGS_FILE,'r',encoding='utf-8') as file:\n    for row in file:\n        values = row.split(' ')\n        word = values[0]\n        weights = np.asarray([float(val) for val in values[1:]])\n        embedding_vectors[word] = weights\nprint(\"Size of vocabulary in GloVe: \", len(embedding_vectors))  ","4ded74bd":"emb_dim = 100\nif max_words is not None: \n    vocab_len = max_words \nelse:\n    vocab_len = len(word_index)+1\nembedding_matrix = np.zeros((vocab_len, emb_dim))\noov_count = 0\noov_words = []\nfor word, idx in word_index.items():\n    if idx < vocab_len:\n        embedding_vector = embedding_vectors.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[idx] = embedding_vector","a110c981":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='auto')","7294f111":"lstm_model = Sequential()\nlstm_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix]))\nlstm_model.add(LSTM(128, return_sequences=False))\n# lstm_model.add(Dropout(0.25))\nlstm_model.add(Dense(1, activation = 'sigmoid'))\nlstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(lstm_model.summary())","91dcb37d":"%%time\nseq_model1 = lstm_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))","5e94b0dc":"epochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = seq_model1.history['accuracy']\ntrain_loss = seq_model1.history['loss']\nval_acc = seq_model1.history['val_accuracy']\nval_loss = seq_model1.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","71a32518":"train_lstm_results = lstm_model.evaluate(X_train_pad, y_train, verbose=0, batch_size=256)\ntest_lstm_results = lstm_model.evaluate(X_test_pad, y_test, verbose=0, batch_size=256)\nprint(\"Train accuracy: {}\".format(train_lstm_results[1]*100))\nprint(\"Test accuracy: {}\".format(test_lstm_results[1]*100))","c09ba5dd":"y_pred = lstm_model.predict_classes(X_test_pad)\nprint(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))","b2f865eb":"cm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)","c754b886":"print(\"Classification Report: \\n\", classification_report(y_test, y_pred))","8c220173":"emb_dim = embedding_matrix.shape[1]\ngru_model = Sequential()\ngru_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix]))\ngru_model.add(GRU(128, return_sequences=False))\ngru_model.add(Dropout(0.5))\ngru_model.add(Dense(1, activation = 'sigmoid'))\ngru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(gru_model.summary())","11e9888e":"%%time\nseq_model2 = gru_model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=20, batch_size = 256, callbacks=([reduce_lr, early_stop]))","efaa395f":"epochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = seq_model2.history['accuracy']\ntrain_loss = seq_model2.history['loss']\nval_acc = seq_model2.history['val_accuracy']\nval_loss = seq_model2.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","c6adaadb":"train_gru_results = gru_model.evaluate(X_train_pad, y_train, verbose=0, batch_size=256)\ntest_gru_results = gru_model.evaluate(X_test_pad, y_test, verbose=0, batch_size=256)\nprint(\"Train accuracy: {}\".format(train_gru_results[1]*100))\nprint(\"Test accuracy: {}\".format(test_gru_results[1]*100))","4b2a67a1":"y_pred = gru_model.predict_classes(X_test_pad)\nprint(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))","6594a385":"cm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix: \\n\", cm)","460732a5":"print(\"Classification Report: \\n\", classification_report(y_test, y_pred))","01da14b8":"# Read Data\n<a id = \"read-data\"><\/a>\n","1d8eea1d":"# Table of Contents\n1. [Objective](#objective)\n2. [Import Packages](#import-packages)\n3. [Read Data](#read-data)\n    * [Create Target based on Real and Fake data](#create-target)\n    * [Concat both real and fake data](#concat-data)\n4. [Data Analysis](#data-analysis)\n    * [Missing value Treatment](#treat-missing-value)\n    * [Merge Title and Text data](#merge-title-text)\n5. [Data Visualization](#data-visualization)\n6. [Data Cleaning](#data-cleaning)\n    * [Create text sequences and padding](#create-sequence-padding)\n7. [Reduce Memory Usage (Free unused dataframe and objects)](#reduce_memory)\n8. [Word Embeddings using GloVe vectors](#word-embeddings)\n    * [Create embedding matrix](#embedding-matrix)\n9. [Train with LSTM](#lstm_train)\n    * [Evaluate LSTM Model](#evaluate-lstm)\n10. [Train with GRU](#train_gru)\n    * [Evaluate GRU Model](#evaluate-gru)","e4c7be46":"## So here we see Glove Vectors with GRU are giving the best results with an accuracy of 99.8%.","a7f691ac":"**Concat both real and fake data**\n<a id = \"concat-data\"><\/a>\n","f2ec8777":"# Reduce Memory Usage (Free unused dataframe and objects)\n<a id=\"reduce_memory\"><\/a>\n\nTill this point we have consumed lots of memory and as there is limited memory for kernel, some unused dataframes and objects need to be free in order to train the dataset.<br>\nBelow is the Session Metrics of this kernel at this point and almost 92% of the total memory is used.<br>\n\n![Pic%201.PNG](attachment:Pic%201.PNG)","5626376d":"# Data Cleaning\n<a id = \"data-cleaning\"><\/a>","25c60783":"# Data Analysis\n<a id = \"data-analysis\"><\/a>","d89d9eab":"**Evaluate LSTM Model**\n<a id = \"evaluate-lstm\"><\/a>","846bc48c":"This is the Session Metrics after memory optimization.<br>\n\n![Pic%202.PNG](attachment:Pic%202.PNG)","74a68f73":"# Word Embeddings using GloVe vectors\n<a id  = \"word-embeddings\"><\/a>\n\nGloVe, stands for \"Global Vectors\", is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. <br><br>\nGloVe is developed as an open-source project at Stanford. \n<br><br>As log-bilinear regression model for unsupervised learning of word representations, it combines the features of two model families, namely the global matrix factorization and local context window methods.<br>\nGloVe captures both global statistics and local statistics of a corpus, in order to come up with word vectors. Turns out, it each type of statistic has their own advantage. For example, Word2vec which captures local statistics do very well in analogy tasks. However a method like LSA which uses only global statistics does not do that well in analogy tasks.\n<br><br>\nTraining is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n","ca313495":"**Missing value Treatment**\n<a id = \"treat-missing-value\"><\/a>","462dc461":"# Data Visualization\n<a id =\"data-visualization\"><\/a>","69c4a630":"# Train with LSTM\n<a id = \"lstm_train\"><\/a>","9d906d1a":"# Train with GRU\n<a id = \"train_gru\"><\/a>","33ef7b91":"In this step I am creating and initializing the embedding_matrix with zeros and use this in the LSTM model weight.","e80b8555":"<div align='center'><font size=\"4\" color=\"grey\">Fake news is a real problem<\/font><br><br>\n\n![fake%20news1.jpg](attachment:fake%20news1.jpg)","1993b424":"# Problem Statement\n<p> We have 2 datasets with Real News data and Fake News data. Each dataset has around 22000 articles. As we merge both, we have around 45000 articles of real and fake. The aim is to build a model to correctly predict if a news is real or fake. ","ff40a559":"# Objective\n<a id = \"objective\"><\/a>\n\nThis kernel is the continuation of my previous work with the same dataset, where I have used CountVectorizer and TFIDF Vectorizer with Classification models and OneHotVector with LSTM.<br>\nIn this kernel I will predict real and fake news using Glove Vector with LSTM and GRU models.","b98a3735":"**Merge Title and Text data**\n<a id = \"merge-title-text\"><\/a>","3fb0d38e":"**Evaluate GRU Model**\n<a id = \"evaluate-gru\"><\/a>","f989ca95":"**Create embedding matrix**\n<a id = \"embedding-matrix\"><\/a>","f065064b":"In this step we will clean the data that will be used for training. The cleaning will involve these steps-\n1. Removing all the extra information like brackets, any kind of puctuations - commas, apostrophes, quotes, question marks, and more.\n2. Remove all the numeric text, urls\n3. Remove all the stop words like - just, and, or, but, if\n4. Convert all the remaining text in the lower case, separated by space","7ea9f81f":"<p> There is no missing value in the dataset, so we we can work directly on title and text columns","2e42a1c2":"**Create text sequences and padding**\n<a id=\"create-sequence-padding\"><\/a>","13593786":"**Create Target based on Real and Fake data**\n<a id=\"create-target\"><\/a>\n","9648bdeb":"**And Memory consumption is reduced from 14.7 GB to 1.6 GB, i.e. almost 90% of the memory is free.**","b3674028":"**If you like the kernel please upvote.\nIf you have any questions\/feedback feel free to comment and I will be happy to answer.\n**","1280c5e1":"# Import Packages\n<a id = \"import-packages\"><\/a>\n"}}