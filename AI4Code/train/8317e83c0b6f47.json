{"cell_type":{"586a19ce":"code","f2b1e9f1":"code","52dded10":"code","598aa6c8":"code","10d3f5c3":"code","9a422040":"code","8e536fbc":"code","8dfb35a5":"markdown"},"source":{"586a19ce":"import numpy as np \nimport pandas as pd \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectPercentile, f_classif","f2b1e9f1":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","52dded10":"# all rows, all cols but the first \"ids\" and last one \"dependent variable\" \nX_train = df_train.iloc[:,1:-1].copy()\n\n# all rows, the column of the dependent variable only\ny_train = df_train.iloc[:, -1].copy()\n\n\nX_test = df_test.copy()\nX_test.drop('id', axis = 1, inplace = True)\n\n#del df_train,df_test","598aa6c8":"# plt.figure()\n# fig, ax = plt.subplots(5, 2,figsize=(20, 22))\n# for i in range(1, 11):\n#     plt.subplot(5, 2,i)\n#     sns.histplot(data=X_train.iloc[:,random.randint(0,100)])\n# plt.show()","10d3f5c3":"percentile = [i for i in range(10,101,10)]\nfor p in percentile:\n    pipe = Pipeline(\n        steps =[('scaler', QuantileTransformer()),\n                ('feature_selection', SelectPercentile(score_func = f_classif, percentile=p)),\n                ('nb', GaussianNB())])\n    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n    pipe.fit(X_train_split, y_train_split)\n    print(f\"roc_auc_score with {p}% of the features: \",roc_auc_score(y_test_split,pipe.predict_proba(X_test_split)[:,1]))\n","9a422040":"pipe = Pipeline(\n        steps =[('scaler', QuantileTransformer()),\n                ('feature_selection', SelectPercentile(score_func = f_classif, percentile=90)),\n                ('nb', GaussianNB())])\npipe.fit(X_train,y_train)\ny_pred = pipe.predict_proba(X_test)[:,1]","8e536fbc":"output = pd.DataFrame({'id': df_test.id, 'target' : y_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"submission saved\")","8dfb35a5":"### Checking the data distribution\nI took a random sample (10 features) to look at how the data is distributed. \nWhat is the point?\nWell... since we are trying out Naive Bayes in this problem. Distribution matters. \nNaive Bayes gives best results when data is normaly distributed. Since the plots show that \nthe data is not normaly distributed, we will need to do some normalization.\n\nA better approach can be seen in this notebook: https:\/\/www.kaggle.com\/raahulsaxena\/tps-nov-21-extracting-the-power-of-naive-bayes\nby trying all the possible normalizations {none, MinMax, Standard, Robust, Quantile} with pipelines and comparing them. Simply because randomly plotting 10 features might not be a good representation of the data in every iteration."}}