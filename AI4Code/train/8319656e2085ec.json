{"cell_type":{"ee94f362":"code","fd4925d9":"code","a2a63d91":"code","c9428ec2":"code","158d5722":"code","b4be2051":"code","153546b1":"code","f9ff0740":"code","289578c8":"code","45082fb7":"code","96f38b31":"code","bd57b631":"code","6aae73e2":"code","2d4f68be":"code","ebe6e429":"code","6d4fa03e":"code","f2206b5a":"code","07d3a872":"code","a967e74b":"code","45d3cabd":"code","07bfa54b":"code","1fb4da8e":"code","7555c208":"code","cb60830c":"code","7f72ab2f":"code","b8712220":"code","e0c6e139":"code","09b3d4f0":"code","19a080f9":"code","9db37312":"code","3fd3b45a":"code","2c0b2fc4":"code","156f8222":"code","f3b8b4e9":"code","4c7626b0":"code","75fe5f35":"code","71d367d1":"code","94a3ca2f":"code","6d53e759":"code","dbf7d80c":"code","87febe9f":"code","1a7d00de":"code","76ce4656":"code","6e53b0d2":"markdown","9b353d45":"markdown","c6c74898":"markdown","5f6deedd":"markdown","213768f5":"markdown","5917e5e6":"markdown","96b1617c":"markdown","85ebbd60":"markdown","f434b51c":"markdown","0b951b8e":"markdown","17297e40":"markdown","3d3f1766":"markdown","d2ac18c6":"markdown","87f9e0c4":"markdown","13509045":"markdown","3777cd0d":"markdown","f0a9f564":"markdown","abbf1c0e":"markdown","e781742f":"markdown","423a64e9":"markdown","4da6beac":"markdown","923387b5":"markdown","579be29b":"markdown","37323e57":"markdown","95ff895a":"markdown","b8531b13":"markdown","8c0d9c78":"markdown","c0193258":"markdown","7626abd4":"markdown","db52be49":"markdown","56faf486":"markdown","8a8a2b36":"markdown","ee26991b":"markdown"},"source":{"ee94f362":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, runninga this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","fd4925d9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a2a63d91":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","c9428ec2":"train.describe()","158d5722":"train.head(3)","b4be2051":"test.head(3)","153546b1":"train.shape,test.shape","f9ff0740":"#check for dupes for Id\nidsUnique = len(set(train.Id))\nidsTotal = train.shape[0]\nidsdupe = idsTotal - idsUnique\nprint(\"No. of duplicate IDs present: \" + str(idsdupe))\n#drop id col\ntrain.drop(['Id'],axis =1,inplace=True)","289578c8":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);","45082fb7":"corrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","96f38b31":"sns.barplot(train.OverallQual,train.SalePrice)","bd57b631":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","6aae73e2":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","2d4f68be":"train.SalePrice = np.log1p(train.SalePrice )\ny = train.SalePrice","ebe6e429":"plt.scatter(y =train.SalePrice,x = train.GrLivArea,c = 'black')\nplt.show()\n#we can see the outlier in the below image","6d4fa03e":"train_nas = train.isnull().sum()\ntrain_nas = train_nas[train_nas>0]\ntrain_nas.sort_values(ascending=False)","f2206b5a":"test_nas = test.isnull().sum()\ntest_nas = test_nas[test_nas>0]\ntest_nas.sort_values(ascending = False)","07d3a872":"print(\"Find most important features relative to target i.e SalePrice\")\ncorr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)\n#this you can see at the time of heatmap also.","a967e74b":"categorical_features = train.select_dtypes(include=['object']).columns\ncategorical_features","45d3cabd":"numerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features","07bfa54b":"categorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","1fb4da8e":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))\ntrain_num = train_num.fillna(train_num.median())\nprint(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))","7555c208":"from scipy.stats import skew \nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","cb60830c":"skewness = skewness[abs(skewness)>0.5]\nskew_features = train[skewness.index]\nskew_features.columns","7f72ab2f":"skew_features = np.log1p(skew_features)","b8712220":"print(\"Original train dataset shape: \" + str(train_cat.shape))","e0c6e139":"train_cat = pd.get_dummies(train_cat)\nprint(\"New train dataset shape after one-hot encoding: \" + str(train_cat.shape))","09b3d4f0":"train_cat.head()","19a080f9":"str(train_cat.isnull().values.sum())","9db37312":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3fd3b45a":"train = pd.concat([train_cat,train_num],axis=1)\ntrain.shape","2c0b2fc4":"X_train,X_test,y_train,y_test = train_test_split(train,y,test_size = 0.3,random_state= 0)","156f8222":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","f3b8b4e9":"X_train.head(3)","4c7626b0":"n_folds = 5\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","75fe5f35":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)\nprint('rmse on train',rmse_CV_train(lr).mean())\nprint('rmse on test',rmse_CV_test(lr).mean())\n","71d367d1":"# Plot predictions - Real values\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","94a3ca2f":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train,y_train)\nalpha = ridge.alpha_\nprint('best alpha',alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\nprint(\"Ridge RMSE on Training set :\", rmse_CV_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_CV_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)","6d53e759":"coef = pd.Series(ridge.coef_, index = X_train.columns)\n\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n","dbf7d80c":"print('rmse on train',rmse_CV_train(ridge).mean())\nprint('rmse on test',rmse_CV_test(ridge).mean())","87febe9f":"# Plot predictions - Real values\nplt.scatter(y_train_rdg, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","1a7d00de":"import pickle\npickle.dump(ridge, open('house_price_model', 'wb'))","76ce4656":"model = pickle.load(open('house_price_model', 'rb'))","6e53b0d2":"**Modeling**","9b353d45":"### Loading the model","c6c74898":"**Here, we will be using Ridge Regularization to improve upon the existing Linear regression model**\n\n**We will pass different values of alpha to the RigeCV() and it will try to find the most optimum value of alpha depending on the model performance.**","5f6deedd":"### Save the finalized model","213768f5":"**We will use median() to fill NaN (i.e missing) values**","5917e5e6":"# Conclusion","96b1617c":"Since, the SalePrice distribution is skewed, we can use the log function to get rid of its skewness","85ebbd60":"## **Visualization of features coorelation with each other**","f434b51c":"### Regularization is a very useful method to filter out noise from data, and eventually prevent overfitting. \n### The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights. ","0b951b8e":"Check for duplicate ids","17297e40":"Reading the training and test data","3d3f1766":"### Defining  cross_val_score function for both train and test sets separately\n\n**We will use Root Mean Square Error to get an estimate of the model's performance.**","d2ac18c6":"**Checking how many values are missing for each feature in the test data**","87f9e0c4":"It might sound confusing  what is validation data? \n\nWe are still working on train data only, which split was into x_train,x_test.  Here x_test is the validation set we call because \n\nWe are checking how model is performing on our own data(x_test).","13509045":"**We will consider those features skewed which will be having a positive or negative skewness greater than 0.5**","3777cd0d":"**Check for skewness in the dataset features**","f0a9f564":"**Differentiate numerical features (excluding the SalePrice) and categorical features**","abbf1c0e":"### Most correlated features","e781742f":"Earlier we split the train set into categorical and numerical features.  \n\nNow after transformation(preprocessing) we'll join them to get the whole train set back.","423a64e9":"**Most of the features are correlated with each other like Garage Cars and Garage Area, isnt it?**\n- OverallQual is highly correlated with target feature SalePrice 0.79 can you see. we'll see how it effected the saleprice in below graph.","4da6beac":"## Importing the required libraries for data analysis and visualization","923387b5":"**Checking how many values are missing for each feature in the training data**","579be29b":"### Pre processing","37323e57":"###  Linear Regression model without Regularization","95ff895a":"### Create dummy features for categorical values via one-hot encoding","b8531b13":"**The final Linear Regression model trained with Ridge Regularization gave an average RMSE score of about 0.17 on the test set, showing that the real and the predicted values were actually pretty close!**","8c0d9c78":"We can see there are so many NAN  values in both train and test sets which needs to treated well.\n","c0193258":"**Checking for missing values in categorical features**","7626abd4":"**Her, we can have a look at how many features our model seleced and how many it eliminated**","db52be49":"### Split the data to train the model ","56faf486":"**We can treat skewness of a feature with the help of log transformation, so we'll apply the same here.**","8a8a2b36":"Here we can see how each feature is correlated  with SalePrice.\n","ee26991b":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.\n\nWe will use a distribution plot for the SalePrice along with its mean (mu) and standard deviation (sigma)"}}