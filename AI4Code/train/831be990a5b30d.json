{"cell_type":{"cfec5372":"code","8219df02":"code","668690e1":"code","25bff4f7":"code","8a17abf9":"code","f8f82c07":"code","7038ea6d":"code","3ade443b":"code","86b1e80f":"code","4c82ccd8":"code","9c18c821":"code","678cb1ac":"code","dc6125a2":"code","1421c40c":"code","f3b8d565":"code","bc8e2b92":"code","6fe481b5":"code","0c3596d5":"code","f1cce8e4":"markdown","76e50081":"markdown","f4fcf1e9":"markdown","c10477e2":"markdown","739cd9f6":"markdown"},"source":{"cfec5372":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm_notebook\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.simplefilter('ignore')","8219df02":"import tensorflow as tf\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Activation, Dropout\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAvgPool2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as vgg_preprocess_input\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as resnet_preprocess_input\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess_input","668690e1":"def read_image(location, size=(224, 224)):\n    img = image.load_img(location, target_size=size)\n    img = image.img_to_array(img)\n    \n    return img\n\nclass DataLoader(object):\n    \n    def __init__(self, image_size=(224, 224)):\n        \n        self.set_seed()\n        self.image_size=image_size\n        \n        self.CATEGORIES = os.listdir(\"..\/input\/plant-seedlings-classification\/train\/\")\n        self.N_CLASSES = len(self.CATEGORIES)\n        print(f\"Length of Categories : {self.N_CLASSES}\")\n        self.data_dir = \"..\/input\/plant-seedlings-classification\/\"\n        self.train_dir = os.path.join(self.data_dir, \"train\")\n        self.test_dir = os.path.join(self.data_dir, \"test\")\n        self.sub = pd.read_csv(\"..\/input\/plant-seedlings-classification\/sample_submission.csv\")\n\n        print(f\"Sub Shape : {self.sub.shape}\")\n    \n    def set_seed(self, seed=13):\n        \n        self.seed = seed\n        np.random.seed(self.seed)\n    \n    def plot_distribution(self, print_cat_wise=False):\n        \n        distribution = {}\n\n        for category in self.CATEGORIES:\n            num_samples = len(os.listdir(os.path.join(self.train_dir, category)))\n            distribution[category] = num_samples\n            if print_cat_wise:\n                print(f\"{category} has {num_samples} samples.\")\n\n        plt.figure(figsize=(24, 12))\n        plt.xlabel(\"Category\")\n        plt.ylabel(\"Count\")\n        plt.title(\"Target Distribution\")\n        sns.barplot(list(distribution.keys()), list(distribution.values()))\n        plt.show()\n    \n    def retrieve_data(self):\n        \n        # Making the datasets\n        # Schema  : file_location | category | category_id\n        \n        self.train = []\n        self.test = []\n        self.class_names = {}\n        \n        for category_id, category in tqdm_notebook(enumerate(self.CATEGORIES)):\n            category_path = os.path.join(self.train_dir, category)\n            cur_cat_files = os.listdir(category_path)\n            cur_cat_files = [[os.path.join(category_path, i), category, category_id] for i in cur_cat_files]\n            \n            if not self.class_names.get(category):\n                self.class_names[category] = category_id\n            \n            self.train.extend(cur_cat_files)\n\n        print(f\"Total Train Samples : {len(self.train)}\")\n        self.train = pd.DataFrame(self.train, columns=['location', 'target', 'target_id'])\n        \n        for file in tqdm_notebook(os.listdir(self.test_dir)):\n            cur_item = os.path.join(self.test_dir, file)\n            self.test.append(cur_item)\n\n        print(f\"Total Test Samples : {len(self.test)}\")\n        self.test = pd.DataFrame(self.test, columns=['location'])\n        \n        self.split_data()\n#         return self.train, self.test\n    \n    def split_data(self):\n\n        self.train, self.valid, self.y_train, self.y_valid = train_test_split(self.train, self.train['target_id'], test_size=0.2, random_state=self.seed)\n        print(f\"Train Shape : {self.train.shape}\\nValid Shape : {self.valid.shape}\\n\")\n        \n#         return self.get_data()\n    \n    def make_data_gens(self, batch_size=32):\n\n        self.batch_size = batch_size\n        \n        self.train_gen = self.data_gen.flow_from_dataframe(\n            dataframe=self.train, \n            x_col='location',\n            y_col='target',\n            batch_size=self.batch_size,\n            seed=dataloader.seed,\n            shuffle=False, # True\n            class_mode='categorical',\n            target_size=self.image_size,\n        )\n\n        self.valid_gen = self.data_gen.flow_from_dataframe(\n            dataframe=self.valid, \n            x_col='location',\n            y_col='target',\n            batch_size=self.batch_size,\n            seed=dataloader.seed,\n            shuffle=False, # True\n            class_mode='categorical',\n            target_size=self.image_size,\n        )\n\n        self.test_gen = self.data_gen.flow_from_dataframe(\n            dataframe=self.test, \n            x_col='location',\n            y_col=None,\n            batch_size=1, # 397 \n            seed=dataloader.seed,\n            shuffle=False,\n            class_mode=None,\n            target_size=self.image_size,\n        )\n\n        self.train_stepsize = self.train_gen.n \/\/ self.train_gen.batch_size\n        self.valid_stepsize = self.valid_gen.n \/\/ self.valid_gen.batch_size\n        \n        return (self.train_gen, self.train_stepsize), (self.valid_gen, self.valid_stepsize), self.test_gen\n    \n    def get_data_gens(self):\n        \n        return self.train_gen, self.valid_gen, self.test_gen\n    \n    def get_stepsizes(self):\n        \n        return self.train_stepsize, self.valid_stepsize\n    \n    def get_data(self):\n        \n        return self.train, self.valid, self.test\n    \n    def set_datagen(self, data_gen=ImageDataGenerator(rescale=1.\/255)):\n        self.data_gen = data_gen\n        \n    \n    def set_image_size(self, image_size=(224, 224)):\n        \n        self.image_size = image_size\n        \n    def get_labels(self):\n        \n        return self.y_train, self.y_valid","25bff4f7":"dataloader = DataLoader(image_size=(224, 224))\ndataloader.retrieve_data()\ndataloader.plot_distribution()","8a17abf9":"class PreTrainedModels(object):\n    \n    def __init__(self):\n        \n        self.model_utils = {\n            'resnet_50': {\n                'model': ResNet50,\n                'preprocessor': resnet_preprocess_input,\n            },\n            'vgg_16': {\n                'model': VGG16,\n                'preprocessor':vgg_preprocess_input,\n            },\n            'inception_v3': {\n                'model': InceptionV3,\n                'preprocessor': inception_preprocess_input,\n            },\n        }\n        \n        self.loss_history = {\n            'resnet_50': {},\n            'vgg_16': {},\n            'inception_v3': {},\n        }\n    \n    \n    def tune_model(self, dataloader, choice, params):\n        \n        self.params = params\n        \n        self.pre_trained_model = self.model_utils[choice]['model']\n        self.pre_trained_model_preprocessor = self.model_utils[choice]['preprocessor']\n        \n        self.datagen = ImageDataGenerator(preprocessing_function=self.pre_trained_model_preprocessor)\n        dataloader.set_datagen(self.datagen)\n        \n        (self.train_gen, self.train_stepsize), (self.valid_gen, self.valid_stepsize), self.test_gen = dataloader.make_data_gens(self.params['batch_size'])\n        \n        # 1. Check if higher\/lower input sizes help?\n        \n        early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', patience=3)\n        reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.00001)\n        \n        print(\"\\nDownloading & Compiling the model ... \")\n        self.model_name = choice\n        input_shape = (dataloader.image_size[0], dataloader.image_size[1], 3)\n        if choice == 'vgg_16':\n\n            file_path = \"best_vgg16.hdf5\"\n            checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=1)\n            \n            # VGG16\n            model_vgg16 = self.pre_trained_model(include_top=False, input_shape=input_shape)\n\n            # Training only the newly connected Dense Layer\n            for layer in model_vgg16.layers:\n                layer.trainable = False\n                \n            x = Flatten()(model_vgg16.output)\n            x = Dense(512)(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n            x = Dropout(0.8)(x)\n            x = BatchNormalization()(x)\n            x = Dense(dataloader.N_CLASSES, activation='softmax')(x)\n\n            self.model = Model(inputs=model_vgg16.input, outputs=x)\n            \n        elif choice == 'resnet_50':\n\n            file_path = \"best_resnet50.hdf5\"\n            checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=1)\n\n            # ResNet50\n            model_resnet50 = self.pre_trained_model(include_top=False, input_shape=input_shape)\n\n            for layer in model_resnet50.layers:\n                layer.trainable = False\n\n            x = Flatten()(model_resnet50.output)\n            x = Dense(512)(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n            x = Dropout(0.8)(x)\n            x = BatchNormalization()(x)\n            x = Dense(dataloader.N_CLASSES, activation='softmax')(x)\n\n            self.model = Model(inputs=model_resnet50.input, outputs=x)\n            \n        elif choice == 'inception_v3':\n            \n            file_path = \"best_inceptionv3.hdf5\"  # {epoch:02d}\n            checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=1)\n            \n            # Inception_v3\n            model_inception = self.pre_trained_model(include_top=False, input_shape=input_shape)\n\n            for layer in model_inception.layers:\n                layer.trainable = False\n\n            x = Flatten()(model_inception.output)\n            x = Dense(512)(x)\n            x = BatchNormalization()(x)\n            x = Activation('relu')(x)\n            x = Dropout(0.8)(x)\n            x = BatchNormalization()(x)\n            x = Dense(dataloader.N_CLASSES, activation='softmax')(x)\n\n            self.model = Model(inputs=model_inception.input, outputs=x)\n            \n        else:\n            return \"Choose correct model.\"\n        \n        # lr = 1e-4\n        optimizer = Adam() # learning_rate=lr\n        self.model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n\n        print(\"Compiled.\")\n        print(f\"Fitting Model : {self.model_name} .. \", end='\\n\\n')\n        \n        hist = self.model.fit_generator(\n            generator = self.train_gen,\n            validation_data = self.valid_gen,\n            steps_per_epoch = self.train_stepsize,\n            validation_steps = self.valid_stepsize,\n            epochs = self.params['epochs'],\n            verbose = 1,\n            callbacks=[checkpoint, early_stopping, reduce_lr],\n        )\n        \n        self.loss_history[choice] = hist.history\n        \n        return self.model\n    \n    def plot_single_metric(self, choice):\n        \n        history = self.loss_history[choice]\n\n        epoch_range = [i+1 for i, loss in enumerate(history['loss'])]\n        xticks = range(0, max(epoch_range), 2)\n\n        max_loss = max([max(history['loss']), max(history['val_loss'])])\n        min_loss = min([min(history['loss']), min(history['val_loss'])])\n        max_acc = max([max(history['accuracy']), max(history['val_accuracy'])])\n        min_acc = min([min(history['accuracy']), min(history['val_accuracy'])])\n\n        plt.figure(figsize=(18, 7))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(epoch_range, history['loss'], color='red', label='Train')\n        plt.plot(epoch_range, history['val_loss'], color='green', label='Valid')\n        plt.xticks(xticks)\n        plt.yticks(np.linspace(min_loss, max_loss, 10))\n        plt.grid(False)\n        plt.legend(loc='best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.title(f\"{self.model_name} | Loss Curve\")\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epoch_range, history['accuracy'], color='red', label='Train')\n        plt.plot(epoch_range, history['val_accuracy'], color='green', label='Valid')\n        plt.xticks(xticks)\n        plt.yticks(np.linspace(min_acc, max_acc, 10))\n        plt.grid(False)\n        plt.legend(loc='best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.title(f\"{self.model_name} | Accuracy Curve\")\n\n        plt.tight_layout()\n        plt.show()\n        \n    def plot_multiple_metric(self):\n        \n        history_1 = self.loss_history['vgg_16']\n        history_2 = self.loss_history['resnet_50']\n        history_3 = self.loss_history['inception_v3']\n\n        epoch_range = [i+1 for i, loss in enumerate(history_1['loss'])]\n        xticks = range(0, max(epoch_range), 2)\n\n        plt.figure(figsize=(18, 18))\n\n        # Valid Loss\n        plt.subplot(2, 1, 1)\n        plt.plot(epoch_range, history_1['val_loss'], color='red', label='VGG-16')\n        plt.plot(epoch_range, history_2['val_loss'], color='green', label='Resnet-50')\n        plt.plot(epoch_range, history_3['val_loss'], color='purple', label='Inception-v3')\n        plt.xticks(xticks)\n        plt.grid(False)\n        plt.legend(loc='best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.title(f\"Validation Loss Comparision\")\n\n        # Valid Accuracy\n        plt.subplot(2, 1, 2)\n        plt.plot(epoch_range, history_1['val_accuracy'], color='red', label='VGG-16')\n        plt.plot(epoch_range, history_2['val_accuracy'], color='green', label='Resnet-50')\n        plt.plot(epoch_range, history_3['val_accuracy'], color='purple', label='Inception-v3')\n        plt.xticks(xticks)\n        plt.grid(False)\n        plt.legend(loc='best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.title(f\"Validation Accuracy Comparision\")\n\n        plt.tight_layout()\n        plt.show()\n        \n    def plot_confusion_matrix(self, dataloader, model, model_name):\n        \n        y_pred = model.predict_generator(dataloader.valid_gen)\n        y_pred = np.argmax(y_pred, axis=1)\n\n        con_mat = tf.math.confusion_matrix(labels=dataloader.valid_gen.classes, predictions=y_pred).numpy()\n        con_mat_norm = np.around(con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n        con_mat_df = pd.DataFrame(con_mat_norm,\n                             index = list(dataloader.class_names.keys()), \n                             columns = list(dataloader.class_names.keys()))\n\n        fig = plt.figure(figsize=(16, 16))\n        sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues)\n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        plt.title(f'{model_name} Confusion Matrix')\n\n        plt.show()","f8f82c07":"params = {\n    'batch_size': 64,\n    'epochs': 15,\n}","7038ea6d":"trans_models = PreTrainedModels()","3ade443b":"model_inception = trans_models.tune_model(dataloader=dataloader, choice='inception_v3', params=params)\ntrans_models.plot_single_metric(choice='inception_v3')\ntrans_models.plot_confusion_matrix(dataloader=dataloader, model=model_inception, model_name='inception_v3')","86b1e80f":"model_inception.save(\"inception_v3.h5\")","4c82ccd8":"model_vgg = trans_models.tune_model(dataloader=dataloader, choice='vgg_16', params=params)\ntrans_models.plot_single_metric(choice='vgg_16')\ntrans_models.plot_confusion_matrix(dataloader=dataloader, model=model_vgg, model_name='vgg_16')","9c18c821":"model_vgg.save(\"vgg_16.h5\")","678cb1ac":"model_resnet = trans_models.tune_model(dataloader=dataloader, choice='resnet_50', params=params)\ntrans_models.plot_single_metric(choice='resnet_50')\ntrans_models.plot_confusion_matrix(dataloader=dataloader, model=model_resnet, model_name='resnet_50')","dc6125a2":"model_resnet.save(\"resnet_50.h5\")","1421c40c":"train = dataloader.train.copy()","f3b8d565":"# First Misclassification \n\nprint(\"51% of Loose Silky-bent are mis-classified as Common wheat.\")\n\nplt.figure(figsize=(18, 18))\nprint(\"Loose Silky-bent\")\n\nfor i in range(5):\n    \n    plt.subplot(1, 5, i+1)\n    \n    img = read_image(train['location'][train['target'] == 'Loose Silky-bent'].values[i], size=(400, 400))\n    img = img\/255.\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n\nplt.show()\n\nplt.figure(figsize=(18, 18))\nprint(\"Common wheat\")\n\nfor i in range(5):\n    \n    plt.subplot(1, 5, i+1)\n    \n    img = read_image(train['location'][train['target'] == 'Common wheat'].values[i], size=(400, 400))\n    img = img\/255.\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n\n\nplt.show()","bc8e2b92":"# Second Misclassification \n\nprint(\"16%% of Small-flowered Cranesbill are mis-classified as Black-grass.\")\nprint(\"5%% of Small-flowered Cranesbill are mis-classified as Charlock.\", end='\\n\\n')\n\n\nplt.figure(figsize=(18, 18))\nprint(\"Small-flowered Cranesbill\")\n\nfor i in range(5):\n    \n    plt.subplot(1, 5, i+1)\n    \n    img = read_image(train['location'][train['target'] == 'Small-flowered Cranesbill'].values[i], size=(400, 400))\n    img = img\/255.\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n\nplt.show()\n\nplt.figure(figsize=(18, 18))\nprint(\"Black-grass\")\n\nfor i in range(5):\n    \n    plt.subplot(1, 5, i+1)\n    \n    img = read_image(train['location'][train['target'] == 'Black-grass'].values[i], size=(400, 400))\n    img = img\/255.\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n\n\nplt.show()\n\nplt.figure(figsize=(18, 18))\nprint(\"Charlock\")\n\nfor i in range(5):\n    \n    plt.subplot(1, 5, i+1)\n    \n    img = read_image(train['location'][train['target'] == 'Charlock'].values[i], size=(400, 400))\n    img = img\/255.\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n\n\nplt.show()","6fe481b5":"trans_models.plot_multiple_metric()","0c3596d5":"# y_pred = model_resnet.predict_generator(dataloader.test_gen, verbose=0)\n# y_pred = np.argmax(y_pred, axis=1)\n# print(y_pred.shape)\n\n# id_to_target = {}\n# for key, value in dataloader.class_names.items():\n#     id_to_target[value] = key\n\n# sub = dataloader.sub.copy()\n# test = dataloader.test.copy()\n\n# test['location'] = test['location'].apply(lambda x: x.split(\"\/\")[-1])\n# test['species'] = np.nan\n# test['species'] = y_pred\n# test['species'] = test['species'].apply(lambda x: id_to_target[x])\n# test.rename({\n#     \"location\": \"file\",\n# }, axis=1, inplace=True)\n\n# # test.head()\n\n# sub.drop(['species'], axis=1, inplace=True) \n# sub = pd.merge(sub, test, on=['file'], how='outer')\n# # sub.shape\n# # sub.head()\n# sub.to_csv(\"sub.csv\", index=False)","f1cce8e4":"### Modelling","76e50081":"# Assignment 6\n\n### Use the Plant Seedlings Dataset to attempt the following questions. Find the dataset here\n\n1) Write your own functions to load the data, normalize the data, OHE for categories.\n\n2) If you find unbalanced data, augment the classes accordingly.\n\n3) Use pre-trained VGG16, ResNet50 and InceptionV3 networks to extract bottleneck features and build a model on top of each of them to evaluate and compare the model performances. \n\n(Model performances include classification report, confusion matrices, plots of Loss Vs Epochs)\n\n4) Write 2 separate scripts: \n\n    a. To train the network and save the model with highest validation accuracy. Save model - model.save(\"model.h5\")\n    b. Load the model and predict on the test data. Load Model - model = load_model(\"model.h5\")","f4fcf1e9":"### Multiple Plots","c10477e2":"## Mis-Classification Analysis","739cd9f6":"How to use preprocess_input with datagenerator: https:\/\/stackoverflow.com\/questions\/53961017\/keras-proper-use-of-preprocess-input-within-imagedatagenerator\n\npreprocess_input vs rescale : https:\/\/stackoverflow.com\/a\/58291480\/9230565"}}