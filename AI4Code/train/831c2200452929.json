{"cell_type":{"f7bff726":"code","343a0c63":"code","fe692ea6":"code","ac0a555f":"code","79d657f7":"code","ed39db90":"code","c75e20d4":"code","b1b51fe2":"code","bc49629a":"code","0237bee0":"code","0204f231":"code","0127c4f1":"code","17ad5867":"code","fe6e2ccf":"code","81accccd":"code","f0502288":"code","da70272f":"code","a5f037bc":"code","4f38eab3":"code","9885c12f":"markdown","013cdd97":"markdown","36ad4522":"markdown","b8bf3676":"markdown","91cf5565":"markdown","96b51d4c":"markdown","78564feb":"markdown","d4cd5cdb":"markdown","14ab68cc":"markdown","9c28e605":"markdown","a0ec4666":"markdown"},"source":{"f7bff726":"from keras.datasets import imdb","343a0c63":"(train_data, train_labels) , (test_data, test_lebels) = imdb.load_data(num_words=10000)","fe692ea6":"train_data[0]\ntrain_labels[0]","ac0a555f":"max([max(seq) for seq in train_data])","79d657f7":" #decode one of these reviews\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key)for (key, value) in word_index.items()])\ndecoded_review = ' '.join(\n[reverse_word_index.get(i - 3, '?') for i in train_data[0]])","ed39db90":"import numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results","c75e20d4":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","b1b51fe2":"#Here\u2019s what the samples look like now:\nx_train[0]","bc49629a":"#You should also vectorize your labels, which is straightforward:","0237bee0":"y_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(train_labels).astype('float32')","0204f231":"#Now the data is ready to be fed into a neural network","0127c4f1":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16,activation ='relu',input_shape = (10000,)))\nmodel.add(layers.Dense(16, activation = 'relu'))\nmodel.add(layers.Dense(1, activation ='sigmoid'))","17ad5867":"from keras import losses\nfrom keras import metrics\nfrom keras import optimizers\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","fe6e2ccf":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n","81accccd":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","f0502288":"history_dict = history.history\nhistory_dict.keys()","da70272f":"#Plotting the training and validation loss","a5f037bc":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","4f38eab3":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","9885c12f":"# Training your model","013cdd97":"#  Building your network","36ad4522":"#Validating your approach\n\nIn order to monitor during training the accuracy of the model on data it has never\nseen before, you\u2019ll create a validation set by setting apart 10,000 samples from the\noriginal training data.","b8bf3676":"* Having 16 hidden units means the weight matrix W will have shape (input_dimension,\n* 16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you\u2019ll add the bias vector b and apply the relu operation). You\n* can intuitively understand the dimensionality of your representation space as \u201chow\n* much freedom you\u2019re allowing the network to have when learning internal representations.\u201d Having more hidden units (a higher-dimensional representation space)\n* allows your network to learn more-complex representations, but it makes the network\n* more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).\n*  There are two key architecture decisions to be made about such a stack of Dense layers: How many layers to use\n* How many hidden units to choose for each layer\n* ","91cf5565":"* Finally, you need to choose a loss function and an optimizer. Because you\u2019re facing a\n* binary classification problem and the output of your network is a probability (you end\n* your network with a single-unit layer with a sigmoid activation), it\u2019s best to use the\n* binary_crossentropy loss. It isn\u2019t the only viable choice: you could use, for instance,\n* mean_squared_error. But crossentropy is usually the best choice when you\u2019re dealing\n* with models that output probabilities. Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this\n* case, between the ground-truth distribution and your predictions.\n*  Here\u2019s the step where you configure the model with the rmsprop optimizer and\n* the binary_crossentropy loss function. Note that you\u2019ll also monitor accuracy\n* during training.","96b51d4c":"* Two intermediate layers with 16 hidden units each. \n* A third layer that will output the scalar prediction regarding the sentiment of\n* the current review.\n \n* The intermediate layers will use relu as their activation function, and the final layer\n* will use a sigmoid activation so as to output a probability (a score between 0 and 1 indicating how likely the sample is to have the target \u201c1\u201d: how likely the review is to be positive). A relu (rectified linear unit) is a function meant to zero out negative values.\n\n* whereas a sigmoid \u201csquashes\u201d arbitrary values into the [0, 1] interval","78564feb":"* You\u2019ll now train the model for 20 epochs (20 iterations over all samples in the\n* x_train and y_train tensors), in mini-batches of 512 samples. At the same time,\n* you\u2019ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by\n* passing the validation data as the validation_data argument.","d4cd5cdb":"Dense layer with a relu activation\nimplements the following chain of tensor operations:\n\n* output = relu(dot(W, input) + b)","14ab68cc":"We can not feed lists of integers into a neural network. you have to turn your list into tensors. \nThere are two ways to do that:\n\n1) Pad your list so that they have the same length, turn them into integers tensors of shape (samples, word_indices), and then use as the first layer in your a layer capable of handling such integers (Embedding layers)\n\n2) One-hot encode your lists to turn them into vectors of 0s and 1s. This would\nmean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you\ncould use as the first layer in your network a Dense layer, capable of handling\nfloating-point vector data.\n\n","9c28e605":"* The input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup\n* you\u2019ll ever encounter. A type of network that performs well on such a problem is\n* a simple stack of fully connected (Dense) layers with relu activations: Dense(16,\n* activation='relu').\n*  The argument being passed to each Dense layer (16) is the number of hidden\n* units of the layer. A hidden unit is a dimension in the representation space of the layer.\n* ","a0ec4666":"# Preparing the data"}}