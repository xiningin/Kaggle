{"cell_type":{"af22ac4c":"code","d84cf8ea":"code","7386b272":"code","37a0ad73":"code","7d53327b":"code","cb116e12":"code","00ae8e63":"code","ac75f3f6":"code","56b5dcf4":"code","d7608f5f":"code","7c801bf0":"code","3b70212d":"code","ff835ec9":"code","bf816f49":"code","ac7f32a4":"code","60620b8e":"code","da83be58":"code","d98121c2":"code","913b89e5":"code","20ba2c96":"code","2ffba605":"code","955d856f":"code","ee2ea986":"code","67a928c4":"code","02e6f0c2":"code","60e9db22":"code","fe623d29":"code","d8e68839":"code","747ddac7":"code","d07025ad":"code","f777a7de":"code","6c1b53bb":"code","b5c74ffe":"code","f5bd30ca":"code","8c5ab404":"code","0790bb83":"code","c0f4fd1a":"code","27c10277":"code","aac4db78":"code","17874002":"code","0a676665":"code","3c2b19f7":"code","ff067145":"code","a81d19a8":"code","0f23dbaa":"code","fe2eff52":"code","d7351645":"code","657f790a":"code","a77c1752":"code","7f9fef88":"code","c5e0ad2e":"code","b86825f1":"code","a95e4710":"code","1815822a":"markdown","dc60e4e5":"markdown","f92859c5":"markdown","42c809a2":"markdown","510fa60f":"markdown","aaa6a5a7":"markdown","345919ff":"markdown","7d18c59d":"markdown","2b2864b5":"markdown","6c647b87":"markdown","a18fa3ad":"markdown","b57b484d":"markdown","9acb575a":"markdown","85cb72e4":"markdown","a60a894f":"markdown","724041a9":"markdown","e314bf94":"markdown","6979b796":"markdown","fc376205":"markdown","d967c234":"markdown","d2ea8f9e":"markdown"},"source":{"af22ac4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d84cf8ea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plot\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"ticks\")\n\nfrom scipy.stats import zscore\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection","7386b272":"data = pd.read_excel('..\/input\/Bank_Personal_Loan_Modelling.xlsx','Data')\ndata.columns = [\"ID\",\"Age\",\"Experience\",\"Income\",\"ZIPCode\",\"Family\",\"CCAvg\",\"Education\",\"Mortgage\",\"PersonalLoan\",\"SecuritiesAccount\",\"CDAccount\",\"Online\",\"CreditCard\"]","37a0ad73":"data.head()","7d53327b":"data.columns","cb116e12":"data.shape","00ae8e63":"data.info()","ac75f3f6":"# No columns have null data in the file\ndata.apply(lambda x : sum(x.isnull()))","56b5dcf4":"# Eye balling the data\ndata.describe().transpose()","d7608f5f":"#finding unique data\ndata.apply(lambda x: len(x.unique()))","7c801bf0":"sns.pairplot(data.iloc[:,1:])","3b70212d":"# there are 52 records with negative experience. Before proceeding any further we need to clean the same\ndata[data['Experience'] < 0]['Experience'].count()","ff835ec9":"#clean the negative variable\ndfExp = data.loc[data['Experience'] >0]\nnegExp = data.Experience < 0\ncolumn_name = 'Experience'\nmylist = data.loc[negExp]['ID'].tolist() # getting the customer ID who has negative experience","bf816f49":"# there are 52 records with negative experience\nnegExp.value_counts()","ac7f32a4":"for id in mylist:\n    age = data.loc[np.where(data['ID']==id)][\"Age\"].tolist()[0]\n    education = data.loc[np.where(data['ID']==id)][\"Education\"].tolist()[0]\n    df_filtered = dfExp[(dfExp.Age == age) & (dfExp.Education == education)]\n    exp = df_filtered['Experience'].median()\n    data.loc[data.loc[np.where(data['ID']==id)].index, 'Experience'] = exp","60620b8e":"# checking if there are records with negative experience\ndata[data['Experience'] < 0]['Experience'].count()","da83be58":"data.describe().transpose()","d98121c2":"sns.boxplot(x='Education',y='Income',hue='PersonalLoan',data=data)","913b89e5":"sns.boxplot(x=\"Education\", y='Mortgage', hue=\"PersonalLoan\", data=data,color='yellow')","20ba2c96":"sns.countplot(x=\"SecuritiesAccount\", data=data,hue=\"PersonalLoan\")","2ffba605":"sns.countplot(x='Family',data=data,hue='PersonalLoan',palette='Set1')","955d856f":"sns.countplot(x='CDAccount',data=data,hue='PersonalLoan')","ee2ea986":"sns.distplot( data[data.PersonalLoan == 0]['CCAvg'], color = 'r')\nsns.distplot( data[data.PersonalLoan == 1]['CCAvg'], color = 'g')","67a928c4":"print('Credit card spending of Non-Loan customers: ',data[data.PersonalLoan == 0]['CCAvg'].median()*1000)\nprint('Credit card spending of Loan customers    : ', data[data.PersonalLoan == 1]['CCAvg'].median()*1000)","02e6f0c2":"fig, ax = plot.subplots()\ncolors = {1:'red',2:'yellow',3:'green'}\nax.scatter(data['Experience'],data['Age'],c=data['Education'].apply(lambda x:colors[x]))\nplot.xlabel('Experience')\nplot.ylabel('Age')","60e9db22":"# Correlation with heat map\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = data.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","fe623d29":"sns.boxplot(x=data.Family,y=data.Income,hue=data.PersonalLoan)\n# Looking at the below plot, families with income less than 100K are less likely to take loan,than families with \n# high income","d8e68839":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(data.drop(['ID','Experience'], axis=1), test_size=0.3 , random_state=100)","747ddac7":"train_labels = train_set.pop('PersonalLoan')\ntest_labels = test_set.pop('PersonalLoan')","d07025ad":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\ndt_model=DecisionTreeClassifier(criterion = 'entropy',max_depth=3)\ndt_model.fit(train_set, train_labels)","f777a7de":"dt_model.score(test_set , test_labels)","6c1b53bb":"y_predict = dt_model.predict(test_set)\ny_predict[:5]","b5c74ffe":"test_set.head(5)","f5bd30ca":"naive_model = GaussianNB()\nnaive_model.fit(train_set, train_labels)\n\nprediction = naive_model.predict(test_set)\nnaive_model.score(test_set,test_labels)","8c5ab404":"randomforest_model = RandomForestClassifier(max_depth=2, random_state=0)\nrandomforest_model.fit(train_set, train_labels)","0790bb83":"Importance = pd.DataFrame({'Importance':randomforest_model.feature_importances_*100}, index=train_set.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )","c0f4fd1a":"predicted_random=randomforest_model.predict(test_set)\nrandomforest_model.score(test_set,test_labels)","27c10277":"train_set_indep = data.drop(['Experience' ,'ID'] , axis = 1).drop(labels= \"PersonalLoan\" , axis = 1)\ntrain_set_dep = data[\"PersonalLoan\"]\nX = np.array(train_set_indep)\nY = np.array(train_set_dep)\nX_Train = X[ :3500, :]\nX_Test = X[3501: , :]\nY_Train = Y[:3500, ]\nY_Test = Y[3501:, ]","aac4db78":"knn = KNeighborsClassifier(n_neighbors= 21 , weights = 'uniform', metric='euclidean')\nknn.fit(X_Train, Y_Train)    \npredicted = knn.predict(X_Test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_Test, predicted)\nprint(acc)","17874002":"X=data.drop(['PersonalLoan','Experience','ID'],axis=1)\ny=data.pop('PersonalLoan')","0a676665":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=12345)\n\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","3c2b19f7":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier","ff067145":"knn = KNeighborsClassifier(n_neighbors=15)\nclf = knn.fit(X_Train, Y_Train)\ny_pred = clf.predict(X_Test)\nacc_knb_model=roc_auc_score(Y_Test, y_pred)*100\nacc_knb_model","a81d19a8":"lr = LogisticRegression(C = 0.2)\nclf1 = lr.fit(X_Train, Y_Train)\ny_pred1 = clf1.predict(X_Test)\nacc_log_reg=roc_auc_score(Y_Test, y_pred1)*100\nacc_log_reg","0f23dbaa":"clf2 = GaussianNB().fit(X_Train, Y_Train)\ny_pred2 = clf2.predict(X_Test)\nacc_nb=roc_auc_score(Y_Test, y_pred2)*100\nacc_nb","fe2eff52":"clf3 = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\ny_pred3 = clf3.predict(X_Test)\nacc_dt=roc_auc_score(Y_Test, y_pred3)*100\nacc_dt","d7351645":"clf4 = RandomForestClassifier(max_depth=5, random_state=0).fit(X_Train, Y_Train)\ny_pred4 = clf4.predict(X_Test)\nacc_rmf_model=roc_auc_score(Y_Test, y_pred4)*100\nacc_rmf_model","657f790a":"clf5 = SVC(gamma='auto').fit(X_Train, Y_Train)\ny_pred5 = clf5.predict(X_Test)\nacc_svm_model=roc_auc_score(Y_Test, y_pred5)*100\nacc_svm_model","a77c1752":"sgd_model=SGDClassifier()\nsgd_model.fit(X_Train,Y_Train)\nsgd_pred=sgd_model.predict(X_Test)\nacc_sgd=round(sgd_model.score(X_Train,Y_Train)*100,10)\nacc_sgd","7f9fef88":"xgb_model=XGBClassifier()\nxgb_model.fit(X_Train,Y_Train)\nxgb_pred=xgb_model.predict(X_Test)\nacc_xgb=round(xgb_model.score(X_Train,Y_Train)*100,10)\nacc_xgb","c5e0ad2e":"lgbm = LGBMClassifier()\nlgbm.fit(X_Train,Y_Train)\nlgbm_pred=lgbm.predict(X_Test)\nacc_lgbm=round(lgbm.score(X_Train,Y_Train)*100,10)\nacc_lgbm","b86825f1":"regr = linear_model.LinearRegression()\nregr.fit(X_Train,Y_Train)\nregr_pred=regr.predict(X_Test)\nacc_regr=round(regr.score(X_Train,Y_Train)*100,10)\nacc_regr","a95e4710":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest','Stochastic Gradient Decent','Linear Regression','Naive Bayes','XGBoost','LightGBM','Decision Tree'],\n    'Score': [acc_svm_model, acc_knb_model, acc_log_reg, \n              acc_rmf_model,acc_sgd,acc_regr,acc_nb,acc_xgb,acc_lgbm,acc_dt]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","1815822a":"### Decision tree classifier","dc60e4e5":"### Naive Bayes","f92859c5":"**Observation** : Majority of customers who does not have loan have securities account","42c809a2":"**Observation**\n* Income and CCAvg is moderately correlated. \n* Age and Experience is highly correlated","510fa60f":"### Model comparison","aaa6a5a7":"**Observation**: Customers who does not have CD account , does not have loan as well. This seems to be majority. But almost all customers who has CD account has loan as well","345919ff":"#### Information on the features or attributes\n\nThe attributes can be divided accordingly :\n* The variable **ID** does not add any interesting information. There is no association between a person's customer ID  and loan, also it does not provide any general conclusion for future potential loan customers. We can neglect this information for our model prediction.\n\nThe binary category have five variables as below:\n\n* Personal Loan - Did this customer accept the personal loan offered in the last campaign? ** This is our target variable**\n* Securities Account - Does the customer have a securities account with the bank?\n* CD Account - Does the customer have a certificate of deposit (CD) account with the bank?\n* Online - Does the customer use internet banking facilities?\n* Credit Card - Does the customer use a credit card issued by UniversalBank?\n\nInterval variables are as below:\n\n* Age - Age of the customer\n* Experience - Years of experience\n* Income - Annual income in dollars\n* CCAvg - Average credit card spending\n* Mortage - Value of House Mortgage\n\nOrdinal Categorical Variables are:\n* Family - Family size of the customer\n* Education - education level of the customer\n\nThe nominal variable is :\n\n* ID\n* Zip Code","7d18c59d":"**Observation**: Family size does not have any impact in personal loan. But it seems families with size of 3 are more likely to take loan. When considering future campaign this might be good association.","2b2864b5":"The following code does the below steps:\n* For the record with the ID, get the value of `Age` column\n* For the record with the ID, get the value of `Education` column\n* Filter the records matching the above criteria from the data frame which has records with positive experience and take the median\n* Apply the median back to the location which had negative experience","6c647b87":"**Observation**: The graph show persons who have personal loan have a higher credit card average. Average credit card spending with a median of 3800 dollar indicates a higher probability of personal loan.  Lower credit card spending with a median of 1400 dollars is less likely to take a loan. This could be useful information.","a18fa3ad":"### Random Forest classifier","b57b484d":"### KNN ( K - Nearest Neighbour )","9acb575a":"**Inference** : From the above chart it seems that customer who do not have personal loan and customer who has personal loan have high mortgage","85cb72e4":"### Influence of income and education on personal loan ","a60a894f":"* **Age** feature is normally distributed with majority of customers falling between 30 years and 60 years of age. We can confirm this by looking at the `describe` statement above, which shows **mean** is almost equal to **median**\n* **Experience** is normally distributed with more customer having experience starting from 8 years. Here the **mean** is equal to **median**. There are negative values in the **Experience**. This could be a data input error as in general it is not possible to measure negative years of experience. We can delete these values, because we have 3 or 4 records from the sample.\n* **Income** is positively skewed. Majority of the customers have income between 45K and 55K. We can confirm this by saying the **mean** is greater than the **median**\n* **CCAvg** is also a positively skewed variable and average spending is between 0K to 10K and majority spends less than 2.5K\n* **Mortgage**  70% of the individuals have a mortgage of less than 40K. However the max value is 635K\n* The variables family and education are ordinal variables. The distribution of families is evenly distributes","724041a9":"**Observation** : It seems the customers whose education level is 1 is having more income. However customers who has taken the personal loan have the same income levels","e314bf94":"### Conclusion\nThe aim of the universal bank is to convert there liability customers into loan customers. They want to set up a new marketing campaign; hence, they need information about the connection between the variables given in the data. Four classification algorithms were used in this study. From the above graph , it seems like **Decision Tree** algorithm have the highest accuracy and we can choose that as our final model","6979b796":"Bank is has a growing customer base. The bank wants to increase borrowers (asset customers) base to bring in more loan business and earn more through the interest on loans. So , bank wants to convert the liability based customers to personal loan customers. (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success.\nThe department wants to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.","fc376205":"### Applying models\nSplit data into train and test","d967c234":"#### On the dataset \nThe file given below contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign","d2ea8f9e":"**Observation** The above plot show with experience and age have a positive correlation. As experience increase age also increases. Also the colors show the education level. There is gap in the mid forties of age and also more people in the under graduate level"}}