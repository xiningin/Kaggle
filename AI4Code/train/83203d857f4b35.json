{"cell_type":{"ab08d7cc":"code","5c31dfc2":"code","fab7198b":"code","071e41ee":"code","8d7c5e1e":"code","25114ff0":"code","307d3e14":"code","8c6044cd":"code","40d191f6":"code","d5c9c719":"code","8cc0b7a3":"code","27547858":"code","d217328e":"code","5ef9500d":"code","b50535e9":"code","6f8e5138":"code","173d0c26":"code","65c31355":"code","3acd306e":"code","1a89ef0e":"code","aeca3d47":"code","7a562eed":"code","ccdf6505":"code","b67f5235":"code","28e4f7ab":"code","5c6faf13":"code","518763ac":"code","a38ef633":"code","87498681":"code","49604171":"code","d4fd38ba":"code","8ac320ab":"code","f2cdc7e2":"code","4b372dff":"code","0524b227":"code","ead7fa02":"code","2cdb9479":"code","0401910e":"code","f7d329df":"code","e5e30cb9":"code","e16f252f":"code","13640e68":"code","8fbfb58b":"code","b83a36eb":"code","f5d6eaa9":"code","1d1a2474":"code","6fa8a582":"code","82191b3d":"code","6a8532d8":"code","82a34777":"code","6cac7920":"code","83f919e1":"code","b23d295c":"code","cc2b2802":"code","43dcd594":"code","8347f397":"code","1bec7026":"code","dea751dc":"code","d4b5c82a":"code","5ca7d4b6":"code","6c406e0b":"code","1f6a8c9c":"code","9f0eca4c":"code","02263a6d":"code","db9ba9f7":"code","c5e37abe":"code","20394e7e":"code","b7023d0b":"code","3115be82":"code","fd215b93":"code","fb877208":"code","a76893b5":"code","1a469204":"code","9651993c":"code","600692cf":"code","7da70ef5":"code","7769c8bb":"code","9ab4bd57":"code","cc246c83":"code","2954637e":"code","bda60629":"code","3c551f58":"code","59989e48":"code","9f532147":"code","12f74fa3":"code","23432be5":"code","8566c2ca":"code","73ffddf0":"code","d356f5b2":"code","b6f7615b":"code","a076b853":"code","bc881082":"code","81a4db43":"code","2d2ae977":"code","3cbe2cf9":"code","33ddf534":"code","031f5a3b":"code","c321c2a5":"code","62712255":"code","f31bf5c0":"code","9ee8ebb6":"markdown","0ba76f17":"markdown","ba09d12f":"markdown","840e9d1d":"markdown","5dd2b0c7":"markdown","305e22a1":"markdown","1f02c88a":"markdown","0a938f5e":"markdown","29619227":"markdown","65a7964a":"markdown","9d0474a1":"markdown","0dca4bde":"markdown","d607fd80":"markdown","f52ca978":"markdown","253ccd14":"markdown","8255f0a3":"markdown","7b3c4b06":"markdown","30b0d38b":"markdown","cf898e39":"markdown","6510c765":"markdown","86a4ca9b":"markdown","8d664f40":"markdown","c4991b37":"markdown","951f3e17":"markdown","929a42a9":"markdown","772dc04f":"markdown","60e162fb":"markdown","277da5d9":"markdown","680b7d1d":"markdown","f1a0966e":"markdown","389c221c":"markdown","64fcb159":"markdown","c21119c8":"markdown","7fc9f96f":"markdown","bcaf37f4":"markdown","25294d64":"markdown","cfa41ce0":"markdown","3937908c":"markdown","7fcf2791":"markdown","b0eeceba":"markdown","be9e7d54":"markdown","a6f33c7c":"markdown","2e446466":"markdown","fa0a9451":"markdown","fc5fa37f":"markdown","760182f1":"markdown","9703c4d0":"markdown","fbc85d7e":"markdown","c87cd8dd":"markdown","52407ab7":"markdown","25e2c1bd":"markdown","9dc38300":"markdown","c18f9716":"markdown","44b9bb6d":"markdown","6a715056":"markdown","80f34054":"markdown","b892bbbc":"markdown","4ead9cf5":"markdown","53b9e2bf":"markdown","a6049458":"markdown","c9a54d91":"markdown","85a8df39":"markdown","8d1530b8":"markdown","514be236":"markdown","a3d5fa98":"markdown","e5e0c539":"markdown","87b9b354":"markdown","6a005a90":"markdown","c4c408e0":"markdown","f29432a3":"markdown","56a0f972":"markdown","52a54a4e":"markdown","21a84d21":"markdown"},"source":{"ab08d7cc":"\n\nimport pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n%matplotlib inline","5c31dfc2":"housing = pd.read_csv('..\/input\/housingcsv\/housing.csv')\nhousing.head()","fab7198b":"housing.describe()","071e41ee":"housing.info()","8d7c5e1e":"housing[\"ocean_proximity\"].value_counts()","25114ff0":"\nsns.set_theme(style=\"whitegrid\")\nsns.countplot(x=housing.ocean_proximity,palette=\"rocket\")","307d3e14":"sns.set(style='whitegrid',  font_scale=1.1, rc={\"figure.figsize\": [20, 15]})\nhousing.hist(bins=50,color='#b53158')\nplt.show()","8c6044cd":"\nhousing.corr()","40d191f6":"sns.set(style='whitegrid', font_scale=1.1, rc={\"figure.figsize\": [15, 10]})\nsns.heatmap(housing.corr(), annot=True, cmap='rocket')\nplt.show()","d5c9c719":"\nhousing.corr().sort_values(ascending=False,by='median_house_value').median_house_value","8cc0b7a3":"sns.set(style='whitegrid', font_scale=1.1, rc={\"figure.figsize\": [10, 8]})\nsns.scatterplot(data=housing,x=housing.median_income,y=housing.median_house_value,color='#b53158',alpha=0.2)\nplt.show()","27547858":"\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","d217328e":"housing.corr().sort_values(ascending=False,by='median_house_value').median_house_value","5ef9500d":"sns.set(style='whitegrid', font_scale=1.1, rc={\"figure.figsize\": [10,8]})\nsns.scatterplot(data=housing,x=housing.latitude,y=housing.longitude,color='#b53158')\nplt.show()","b50535e9":"\nsns.set(style='whitegrid', font_scale=1.1, rc={\"figure.figsize\": [10, 8]})\nsns.scatterplot(data=housing,x=housing.latitude,y=housing.longitude,color='#b53158',alpha=0.1)\nplt.show()","6f8e5138":"sns.set(style='whitegrid', font_scale=1.1, rc={\"figure.figsize\": [10,8]})\nsns.scatterplot(\n    data=housing,\n    x=housing.latitude,\n    y=housing.longitude,\n    alpha=0.4,\n    palette='rocket_r',\n    hue=housing.median_house_value,\n    size=housing.population\/100)\nplt.show()","173d0c26":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","65c31355":"test_set.head()","3acd306e":"train_set.head()","1a89ef0e":"housing[\"median_income\"].hist()","aeca3d47":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","7a562eed":"housing[\"income_cat\"].value_counts()","ccdf6505":"housing[\"income_cat\"].hist()","b67f5235":"housing","28e4f7ab":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):      #we get the indexes split into test and train using stratification wrt income_cat attribute\n    strat_train_set = housing.loc[train_index]                                   #Using the indexes generated from the above generator we select all the reading using the respective indices split\n    strat_test_set = housing.loc[test_index]","5c6faf13":" strat_train_set","518763ac":"strat_test_set","a38ef633":"strat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)","87498681":"housing[\"income_cat\"].value_counts() \/ len(housing)","49604171":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","d4fd38ba":"compare_props","8ac320ab":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","f2cdc7e2":"strat_train_set","4b372dff":"strat_train_labels=strat_train_set.iloc[:,8]","0524b227":"strat_train_labels","ead7fa02":"strat_test_set","2cdb9479":"strat_test_labels=strat_test_set.iloc[:,8]","0401910e":"strat_test_labels","f7d329df":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndataset = strat_train_set\n\n# Define which columns should be encoded vs scaled\ncolumns_to_encode = ['ocean_proximity']\ncolumns_to_scale  = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','rooms_per_household','bedrooms_per_room','population_per_household']\n\n# Instantiate encoder\/scaler\n\nscaler = StandardScaler()\nohe    = OneHotEncoder(sparse=False)\n\n# Scale and Encode Separate Columns\n\nscaled_columns  = scaler.fit_transform(dataset[columns_to_scale]) \nencoded_columns =    ohe.fit_transform(dataset[columns_to_encode])\n\n# Concatenate (Column-Bind) Processed Columns Back Together\nprocessed_data1 = np.concatenate([scaled_columns, encoded_columns], axis=1)","e5e30cb9":"processed_data1","e16f252f":"housing_train= pd.DataFrame(processed_data1,columns =['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','rooms_per_household','bedrooms_per_room','population_per_household','1H OCEAN','INLAND','ISLAND','NEAR BAY','NEAR OCEAN'])","13640e68":"housing_train","8fbfb58b":"housing_train=housing_train.drop(['NEAR OCEAN'], axis = 1)","b83a36eb":"housing_train","f5d6eaa9":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndataset = strat_test_set\n\n# Define which columns should be encoded vs scaled\ncolumns_to_encode = ['ocean_proximity']\ncolumns_to_scale  = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','rooms_per_household','bedrooms_per_room','population_per_household']\n\n# Instantiate encoder\/scaler\n\nscaler = StandardScaler()\nohe    = OneHotEncoder(sparse=False)\n\n# Scale and Encode Separate Columns\n\nscaled_columns  = scaler.fit_transform(dataset[columns_to_scale]) \nencoded_columns =    ohe.fit_transform(dataset[columns_to_encode])\n\n# Concatenate (Column-Bind) Processed Columns Back Together\nprocessed_data2 = np.concatenate([scaled_columns, encoded_columns], axis=1)","1d1a2474":"processed_data2","6fa8a582":"housing_test= pd.DataFrame(processed_data2, columns =['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','rooms_per_household','bedrooms_per_room','population_per_household','1H OCEAN','INLAND','ISLAND','NEAR BAY','NEAR OCEAN'])","82191b3d":"housing_test","6a8532d8":"housing_test=housing_test.drop(['NEAR OCEAN'], axis = 1)","82a34777":"housing_test","6cac7920":"sample_incomplete_rows = housing_train[housing_train.isnull().any(axis=1)].head()   \nsample_incomplete_rows","83f919e1":"sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1(remove all the rows having Nan Values)","b23d295c":"sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2  #removing the column having Nan Values","cc2b2802":"median = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3 #find median of the column with Nan values and replace all Nan values with median value","43dcd594":"sample_incomplete_rows","8347f397":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","1bec7026":"imputer.fit(housing_train)","dea751dc":"imputer.statistics_","d4b5c82a":"housing.median().values","5ca7d4b6":"X = imputer.transform(housing_train)","6c406e0b":"X","1f6a8c9c":"housing_train = pd.DataFrame(X, columns=housing_train.columns,\n                          index=housing_train.index)","9f0eca4c":"housing_train.loc[sample_incomplete_rows.index.values]            # All Nan values filled with median value in total_bedrooms column","02263a6d":"housing_train","db9ba9f7":"strat_train_labels","c5e37abe":"Y = imputer.transform(housing_test)","20394e7e":"Y","b7023d0b":"housing_test = pd.DataFrame(Y, columns=housing_test.columns,\n                          index=housing_test.index)","3115be82":"housing_test","fd215b93":"strat_test_labels","fb877208":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_train, strat_train_labels)","a76893b5":"lin_reg.coef_","1a469204":"lin_reg.intercept_","9651993c":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_train)\n","600692cf":"housing_predictions","7da70ef5":"lin_mse = mean_squared_error(strat_train_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","7769c8bb":"from sklearn.metrics import  mean_squared_error\n\nlin_mse =  mean_squared_error(strat_train_labels, housing_predictions)\nlin_mse","9ab4bd57":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_train, strat_train_labels)","cc246c83":"housing_predictions = tree_reg.predict(housing_train)\ntree_mse = mean_squared_error(strat_train_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","2954637e":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_train, strat_train_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","bda60629":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","3c551f58":"lin_scores = cross_val_score(lin_reg, housing_train, strat_train_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","59989e48":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_train,strat_train_labels)","9f532147":"housing_predictions = forest_reg.predict(housing_train)\nforest_mse = mean_squared_error(strat_train_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","12f74fa3":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_train,strat_train_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","23432be5":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_train, strat_train_labels)\nhousing_predictions = svm_reg.predict(housing_train)\nsvm_mse = mean_squared_error(strat_train_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","8566c2ca":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_train, strat_train_labels)","73ffddf0":"grid_search.best_params_","d356f5b2":"grid_search.best_estimator_","b6f7615b":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","a076b853":"pd.DataFrame(grid_search.cv_results_)","bc881082":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","81a4db43":"feat_importance_series = pd.Series(feature_importances)","2d2ae977":"feat_importance_series","3cbe2cf9":"feat_importance_series.index = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','rooms_per_household','bedrooms_per_room','population_per_household','<1H OCEAN','INLAND','ISLAND','NEAR BA']\n","33ddf534":"feat_importance_series","031f5a3b":"final_model = grid_search.best_estimator_\nfinal_model","c321c2a5":"final_model = grid_search.best_estimator_\n\nX_test = housing_test\ny_test = strat_test_labels\n\n\nfinal_predictions = final_model.predict(X_test)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","62712255":"final_rmse","f31bf5c0":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","9ee8ebb6":"### Visualizing Geographical Data","0ba76f17":"                                              \n\u2666 One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\n\n\n\u2666 Instead you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values,using cross-validation.\n\n\u2666 For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:","ba09d12f":"It works, although the predictions are not exactly accurate (e.g., the first prediction is\noff by close to 40%!).","840e9d1d":"### Transform the training set:","5dd2b0c7":"Now that the model is trained, let\u2019s evaluate it on the training set:","305e22a1":"\u2666 Let\u2019s measure this regression model\u2019s RMSE on the whole training set using Scikit-Learn\u2019s mean_squared_error function","1f02c88a":"                                           \n\u2666 Let\u2019s assume that you now have a shortlist of promising models. \n\n\u2666 You now need to fine-tune them. Let\u2019s look at a few ways you can do that.\n","0a938f5e":"                            \n\u2666 To avoid creating too many strats, each stratum should be large enough. We will use the pd.cut() function to create an income category attribute with five categories.","29619227":"\u2666 This plot tells us that the housing prices are very much related to the location (e.g. close to the ocean) and to the population density.\n","65a7964a":"### Importing the required Libraries","9d0474a1":"### \u2666  Support vector regressor model","0dca4bde":"### Test-Set Generation","d607fd80":"\u2666 Looking at the correlation matrix again, we see that the new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms.","f52ca978":"\u2666 Of course, it is likely that the model has badly overfit the data. How can you be sure?\n\n\u2666As we saw earlier, you don\u2019t want to touch the test set until you are ready to launch a\nmodel you are confident about, so you need to use part of the training set for training, and part for model validation.","253ccd14":"\u2666 This is much better \n\n\u2666 Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set.\n\n\u2666 Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\n\n\u2666 However, before you dive much deeper in Random Forests, you should try out many other models from various categories of Machine Learning algorithms (several Support Vector Machines with different kernels, possibly a neural network, etc.), without\nspending too much time tweaking the hyperparameters. \n\n\u2666 The goal is to shortlist a few (two to five) promising models","8255f0a3":"### Prepare the data for Machine Learning algorithms","7b3c4b06":"###  Correlation visualisation using heatmaps","30b0d38b":"\u2666 One can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high density in the Central Valley, in particular around Sacramento and Fresno.","cf898e39":"###                            \u2666   Better Evaluation Using Cross-Validation","6510c765":"\u2666 The performance will usually be slightly worse than what you measured using crossvalidation if you did a lot of hyperparameter tuning (because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets). \n\n\u2666 It is not the case in this example, but when this happens you\nmust resist the temptation to tweak the hyperparameters to make the numbers look\ngood on the test set; \n\n\u2666 The improvements would be unlikely to generalize to new data.\n","86a4ca9b":"### One Hot Encoding and Feature Scaling","8d664f40":"### Sorting the correlations in descending order to get the most correlated attribute with the target attribute.\n","c4991b37":"\n\n\u2666 Many histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n\n\u2666 The median income attribute does not look like it is expressed in US dollars (USD).\n\n\u2666 Given in Problem Statement: \"The data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 5 actually means about $50,000)\"\n\n\u2666 The housing median age and the median house value were also capped.\n\n\u2666 The latter may be a serious problem since it is the target attribute.\n\n\u2666 Since the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using the corr() method.","951f3e17":"\u2666 Comparing how the data is split in proportions wrt income category in stratified V.S Random split","929a42a9":"\u2666 The correlation coefficient ranges from \u20131 to 1.\n\n\u2666 When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up.\n\n\u2666 When the coefficient is close to \u20131, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north).\n\n\u2666 Finally, coefficients close to 0 mean that there is no linear correlation.","772dc04f":"\u2666 Doing the median operation using sklearn library","60e162fb":"###                                     Let\u2019s look at the results","277da5d9":"### Checking for rows with null values","680b7d1d":"### Select and train a model ","f1a0966e":"### Plotting the above stats using Seaborn library","389c221c":"\u2666 Okay, this is better than nothing but clearly not a great score: most districts\u2019\n  median_housing_values range between $120,000 and $265,000, so a typical prediction error of $68,628 is not very satisfying. \n\n\u2666This is an example of a model underfitting the training data. \n\n\u2666When this happens it can mean that the features do not provide enough information to make good predictions, or that the model  is not powerful enough\n\n\u2666The main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or\nto reduce the constraints on the model. \n\n\u2666This model is not regularized, so this rules out the last option. You could try to add more features (e.g., the log of the population).\n\n\u2666But first let\u2019s try a more complex model to see how it does.","64fcb159":"\u2666 This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points.","c21119c8":"Check that this is the same as manually computing the median of each attribute:","7fc9f96f":"### Opening and reading the CSV file in pandas.","bcaf37f4":"###  Fine-Tune Your Model\n                                             ","25294d64":"\u2666 Let\u2019s train a DecisionTreeRegressor. \n\n\u2666This is a powerful model, capable of finding complex nonlinear relationships in the data","cfa41ce0":" \u2666 The RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions","3937908c":"### Grid Search","7fcf2791":"### \u2666  Decision Tree-Regressor","b0eeceba":" \u2666 Let\u2019s display these importance scores next to their corresponding attribute names:","be9e7d54":"\n                                              \n\u2666After taking a quick glance at the data, we should learn a whole lot more about it before you decide what algorithms to use, to avoid snooping bias.\n\n\u2666Considering purely random sampling methods is generally fine if we our dataset is large enough (especially relative to the number of attributes), but if it is not, we run the risk of introducing a significant sampling bias.\n\n\u2666Stratified Sampling: This is a sampling technique that is best used when a statistical population can easily be broken down into distinctive sub-groups. Then samples are taken from each sub-groups based on the ratio of the sub groups size to the total population.\n\n\u2666Looking at the correlation heatmap generated above, we can observe that median_income is highly correlated to the target, having \u20180.69\u2019, and therfore it is a very important attribute to predict median housing prices.\n\n\u2666Since the median income is a continuous numerical attribute, we first need to create an income category attribute. Looking at the median income histogram more closely we see that most median income values are clustered around 1.5 to 6 (i.e., 15,000\u201360,000), but some median incomes go far beyond 6.","a6f33c7c":"### Experimenting Attribute Combinations","2e446466":"\u2666  With this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try\ndropping the others).","fa0a9451":"\u2666 The most promising attribute to predict the median house value is the median income, so let\u2019s zoom in on their correlation scatterplot.","fc5fa37f":"### Splitting data into training and test sets using Stratified Sampling(using the new income-category atribute we created)","760182f1":"### Computing Pair Wise Correlation of Columns ","9703c4d0":"\u2666 In this example, we obtain the best solution by setting the max_features hyperparameter to 8, and the n_estimators hyperparameter to 30.\n\n\u2666 The RMSE score for this combination is 49,682, which is slightly better than the score you got earlier using the\ndefault hyperparameter values (which was 50,182). \n\n\u2666 Therfore We have successfully fine-tuned your best model!","fbc85d7e":"### Evaluating our Model on the Test Set","c87cd8dd":"                                      \n\u2666 Since there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data","52407ab7":"**Note**:We can get the RMSE directly by calling the `mean_squared_error()` function with `squared=False`.","25e2c1bd":"### \u2666  RandomForest Regressor","9dc38300":"\u2666 We will use Scikit-Learn\u2019s K-fold cross-validation feature. \n\n\u2666 The following code randomly splits the training set into 10   distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for\nevaluation every time and training on the other 9 folds. \n\n\u2666 The result is an array con taining the 10 evaluation scores:","c18f9716":"\u2666 In some cases, such a point estimate of the generalization error will not be quite\nenough to convince you to launch: what if it is just 0.1% better than the model currently in production? \n    \n\u2666 You might want to have an idea of how precise this estimate is.\n\n\u2666 For this, you can compute a 95% confidence interval for the generalization error using\nscipy.stats.t.interval():\n","44b9bb6d":"\u2666 That\u2019s right: the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model","6a715056":"\u2666 Let's look at the score of each hyperparameter combination tested during the grid search","80f34054":"                             \n\n\u2666 One last thing we want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. \n\n\u2666 For example, the total number of rooms in a district is not very useful if we don\u2019t know how many households there are. What we really want is the number of rooms per household.","b892bbbc":"\u2666 Checking if we have stratified properly that is if same proportions of income category is there in test and our whole housing data","4ead9cf5":"\u2666 Now the Decision Tree doesn\u2019t look as good as it did earlier. In fact, it seems to perform worse than the Linear Regression model!\n\n\u2666 Notice that cross-validation allowsyou to get not only an estimate of the performance of your model, but also a measure\nof how precise this estimate is (i.e., its standard deviation). \n\n\u2666 The Decision Tree has a score of approximately 70,789, generally \u00b12,439. \n\n\u2666 You would not have this information if you just used one validation set. \n\n\u2666 But cross-validation comes at the cost of training the model several times, so it is not always possible.\n","53b9e2bf":"### Train-Set Generation","a6049458":"\u2666 Let\u2019s compute the same scores for the Linear Regression model just to be sure:","c9a54d91":"\u2666 The best hyperparameter combination is found using the below command","85a8df39":"\u2666 Based on Median Income(as its highly correlated with our target,so based on this we create bins and give labels to it that is we are categorising in terms of median_income vale.","8d1530b8":"### Data Cleaning\/Transformation","514be236":"\u2666 The above plot reveals a few things,First, the correlation is indeed very strong; We can clearly see the upward trend, and the points are not too dispersed. \n\n\u2666 Second, the price cap that we noticed earlier is clearly visible as a horizontal line at  500,000.\n","a3d5fa98":"                                 \n\u2666  After tweaking your models for a while, you eventually have a system that performs sufficiently well. \n\n\u2666  Now is the time to evaluate the final model on the test set. \n\n\u2666 There is nothing special about this process; just get the predictors and the labels from your\ntest set, run your full_pipeline to transform the data (call transform(), not\nfit_transform(), you do not want to fit the test set) and evaluate the final model\non the test set:\n","e5e0c539":"\u2666  We can clearly notice that the total_bedrooms attribute has only 20,433 nonnull values, meaning that 207 districts are   missing this feature.\n\n\u2666 We will need to take care of this later.\n\n\u2666 Next, We will find out what categories exist and how many districts belong to each category by using the value_counts() method.","87b9b354":"\u2666 Another quick way to get a feel of the type of data we are dealing with is to plot a histogram for each numerical attribute.\n\n\u2666 A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis).\n\n\u2666 We can either plot this one attribute at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute.","6a005a90":"We can compute a 95% confidence interval for the test RMSE:","c4c408e0":"\u2666 Scikit-Learn\u2019s cross-validation features expect a utility function(greater is better) rather than a cost function (lower is better).\n\n\u2666 So the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding codecomputes -scores before calculating the square root.","f29432a3":"###  \u2666  Linear regression model","56a0f972":"### Splitting our data into training and testing set","52a54a4e":"### Insights from the above histograms:","21a84d21":"\u2666 We can see above, that the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset.\n\n\u2666 Now we should remove the income_cat attribute so the data is back to its original state."}}