{"cell_type":{"89c1d9bd":"code","425cfda4":"code","97c8e3b5":"code","5d5c7c33":"code","a2973dc6":"code","ae12f54c":"code","8869a2ff":"markdown","5eb32c18":"markdown","b5498669":"markdown","ecdbbbbd":"markdown","9f7f8491":"markdown","b528cc62":"markdown"},"source":{"89c1d9bd":"import pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\nlatent_vectors = 30\nnum_epochs = 1000","425cfda4":"%%time\ndfr = pd.read_csv('..\/input\/movielens-1m\/ml-1m\/ratings.dat', delimiter='::', header=None)\ndfr.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']\ndfr = dfr.drop(columns=['Timestamp'])\ndfr.head()","97c8e3b5":"%%time\nrating_matrix = dfr.pivot(index='UserID', columns='MovieID', values='Rating')\nn_users, n_movies = rating_matrix.shape\n# Scaling ratings to between 0 and 1, this helps our model by constraining predictions\nmin_rating, max_rating = dfr['Rating'].min(), dfr['Rating'].max()\nrating_matrix = (rating_matrix - min_rating) \/ (max_rating - min_rating)\n\nsparcity = rating_matrix.notna().sum().sum() \/ (n_users * n_movies)\nprint(f'Sparcity: {sparcity:0.2%}')","5d5c7c33":"class PMFLoss(torch.nn.Module):\n    def __init__(self, lam_u=0.3, lam_v=0.3):\n        super().__init__()\n        self.lam_u = lam_u\n        self.lam_v = lam_v\n    \n    def forward(self, matrix, u_features, v_features, non_zero_mask):\n        predicted = torch.sigmoid(torch.matmul(u_features, v_features.t()))\n        \n        diff = (matrix - predicted)**2\n        prediction_error = torch.sum(diff*non_zero_mask)\n\n        u_regularization = self.lam_u * torch.sum(u_features.norm(dim=1))\n        v_regularization = self.lam_v * torch.sum(v_features.norm(dim=1))\n        \n        return prediction_error + u_regularization + v_regularization","a2973dc6":"# Actual training loop now\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Replacing missing ratings with -1 so we can filter them out later\nrating_matrix[rating_matrix.isna()] = -1\nrating_matrix = torch.from_numpy(rating_matrix.values).to(device)\nnon_zero_mask = (rating_matrix != -1)\n\nuser_features = torch.randn(n_users, latent_vectors, requires_grad=True, device=device)\nuser_features.data.mul_(0.01)\nmovie_features = torch.randn(n_movies, latent_vectors, requires_grad=True, device=device)\nmovie_features.data.mul_(0.01)\n\npmferror = PMFLoss(lam_u=0.05, lam_v=0.05)\noptimizer = torch.optim.Adam([user_features, movie_features], lr=0.01)\n\nbar = trange(num_epochs)\nfor epoch in bar:\n    optimizer.zero_grad()\n    loss = pmferror(rating_matrix, user_features, movie_features, non_zero_mask)\n    loss.backward()\n    optimizer.step()\n    bar.set_postfix(loss=f'{loss:,.3f}')\n","ae12f54c":"# Checking if our model can reproduce the true user ratings\nuser_idx = 7\nuser_ratings = rating_matrix[user_idx, :]\ntrue_ratings = user_ratings != -1\nwith torch.no_grad():\n    predictions = torch.sigmoid(torch.mm(user_features[user_idx, :].view(1, -1), movie_features.t()))\npredicted_ratings = (predictions.squeeze()[true_ratings]*(max_rating - min_rating) + min_rating).round()\nactual_ratings = (user_ratings[true_ratings]*(max_rating - min_rating) + min_rating).round()\n\nprint(\"Predictions: \\n\", predicted_ratings)\nprint(\"Truth: \\n\", actual_ratings)","8869a2ff":"# 4. Loss","5eb32c18":"# 2. Imports and global variables","b5498669":"# 6. Inference","ecdbbbbd":"# 1. Simple Matrix Factorization PyTorch\nSeminal paper: [Probabilistic Matrix Factorization](https:\/\/papers.nips.cc\/paper\/3208-probabilistic-matrix-factorization.pdf).\nThis code is not probabilistic yet.","9f7f8491":"# 3. Data","b528cc62":"# 5. Training"}}