{"cell_type":{"c2bf1457":"code","ff5cadf5":"code","783c6e03":"code","4d1e2fcf":"code","2e7cc17d":"code","794e1f1c":"code","b6610daf":"code","0b0b98c6":"code","3e73e788":"code","7d2ab080":"code","e8657bb3":"code","40b645db":"code","cf27e031":"code","1b430f4b":"code","ec5e44e4":"code","3b5f460e":"code","e3b58af5":"code","d49dc501":"code","4da1896c":"code","71d18562":"code","edceb427":"code","039eea18":"code","aa8fdd06":"markdown","9c522ada":"markdown","e583bebe":"markdown","405b896d":"markdown","6c6ea218":"markdown","23f7dc4a":"markdown","243da33b":"markdown","7b3d0182":"markdown","09fd392f":"markdown"},"source":{"c2bf1457":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ff5cadf5":"train=pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv' )\nsub=pd.read_csv('\/kaggle\/input\/learn-together\/sample_submission.csv')\n\n","783c6e03":"train.head()","4d1e2fcf":"test.head()","2e7cc17d":"sub.head()","794e1f1c":"train.columns","b6610daf":"train.isna().sum().sum()","0b0b98c6":"train=train.drop([\"Id\"], axis=1)\nId=test.Id\ntest=test.drop(['Id'],axis=1)\nfrom sklearn.model_selection import train_test_split","3e73e788":"X_train,X_test,Y_train,Y_test=train_test_split(train.drop(['Cover_Type'],axis=1),train['Cover_Type'],test_size=0.2,random_state=111)","7d2ab080":"X_train.shape,X_test.shape,Y_train.shape,Y_test.shape","e8657bb3":"    from sklearn.ensemble import RandomForestClassifier","40b645db":"model=RandomForestClassifier(n_estimators=150)","cf27e031":"model.fit(X_train,Y_train)","1b430f4b":"from sklearn.metrics import accuracy_score,classification_report","ec5e44e4":"model.score(X_train,Y_train)","3b5f460e":"pred=model.predict(X_test)\naccuracy_score(pred,Y_test)","e3b58af5":"model.score(X_test,Y_test)","d49dc501":"test_pred=model.predict(test)","4da1896c":"sub.head()","71d18562":"\noutput=pd.DataFrame({'ID':Id,'Cover_Type':test_pred})","edceb427":"output.head()","039eea18":"output.to_csv('submission.csv',index=False )","aa8fdd06":"Model","9c522ada":"What we have as features take a look below","e583bebe":"Let's see if we have a NaN in our data","405b896d":"also you can calculate the accuracy by this methode","6c6ea218":"The next step is to  split the data(we will give to the train data 80 percent from our data)","23f7dc4a":"****Loading the Data","243da33b":"But before we do let's drop the column of the  Id because we don't need to fit it with the another features and don't forget to store it in a variable ","7b3d0182":"Let's see what we have as Data","09fd392f":"150 trees"}}