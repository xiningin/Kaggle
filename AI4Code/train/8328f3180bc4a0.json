{"cell_type":{"a80877b7":"code","a002409f":"code","fec35cae":"code","ed81aa50":"code","7b477129":"code","d34a515e":"code","d32ba88c":"code","e0271e79":"code","b876c564":"code","61cb4fc6":"code","f39dde77":"code","465134ee":"code","fd4e046e":"code","c740f991":"code","e06edc8d":"code","97ee2106":"code","31f1a18f":"markdown","9756a306":"markdown","6c21e9f2":"markdown","15eb05a5":"markdown","05525772":"markdown","d4a0d72c":"markdown","830a72c3":"markdown","4e55fcdb":"markdown","994577c0":"markdown","77ba351b":"markdown","7fdee83a":"markdown","86d6bd8a":"markdown","eb910f38":"markdown","d281156d":"markdown","fad9e880":"markdown","18cd64a1":"markdown","a8c61bb8":"markdown","fd575942":"markdown","8c6334e1":"markdown"},"source":{"a80877b7":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\n#import tensorflow as tf \n#from tensorflow.keras import layers\n#from tensorflow.keras.callbacks import EarlyStopping\n#from tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\n#from catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n#from xgboost import XGBClassifier\nfrom sklearn import set_config\nfrom itertools import combinations\n# Cluster :\nfrom sklearn.cluster import MiniBatchKMeans\n#from yellowbrick.cluster import KElbowVisualizer\n#import smong \nimport category_encoders as ce\nimport warnings\n#import optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\nfrom typing import List, Optional, Union\nset_config(display='diagram')\nwarnings.filterwarnings('ignore')","a002409f":"%%time\n# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/frauddetection\/transactions_train.csv\")\n# Preview the data\ntrain.head(3)","fec35cae":"# Convert Dtypes :\ntrain[train.select_dtypes(['int64','int16','float32','float64','int8']).columns] = train[train.select_dtypes(['int64','int16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','category']).columns] = train.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))","ed81aa50":"# Pour le train test\ntarget= \"isFraud\"\nX = train.drop(target, axis='columns')# axis=1\ny = train[target].to_numpy()","7b477129":"train.isFraud.value_counts()","d34a515e":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95,random_state=0,stratify=y )\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","d32ba88c":"# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','int16','float32','float64','int8']).columns","e0271e79":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','int16','float32','float64','int8']).columns","b876c564":"all_columns = (num_columns.append(cat_columns))\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","61cb4fc6":"if set(all_columns) == set(X.columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('in all_columns but not in  train  :', set(all_columns) - set(X.columns))\n    print('in X.columns   but not all_columns :', set(X.columns) - set(all_columns))","f39dde77":"del train \ndel X\ndel y \ndel X_test\ndel y_test","465134ee":"class ColumnsSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, positions):\n        self.positions = positions\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        #return np.array(X)[:, self.positions]\n        return X.loc[:, self.positions] \n########################################################################\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n    # https:\/\/towardsdatascience.com\/how-to-write-powerful-code-others-admire-with-custom-sklearn-transformers-34bc9087fdd\n    def __init__(self):\n        self._estimator = PowerTransformer()\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1\n        self._estimator.fit(X_copy)\n\n        return self\n\n    def transform(self, X):\n        X_copy = np.copy(X) + 1\n\n        return self._estimator.transform(X_copy)\n\n    def inverse_transform(self, X):\n        X_reversed = self._estimator.inverse_transform(np.copy(X))\n\n        return X_reversed - 1  \n\nclass TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n    # Temporal elapsed time transformer\n\n    def __init__(self, variables, reference_variable):\n        \n        if not isinstance(variables, list):\n            raise ValueError('variables should be a list')\n        \n        self.variables = variables\n        self.reference_variable = reference_variable\n\n    def fit(self, X, y=None):\n        # we need this step to fit the sklearn pipeline\n        return self\n\n    def transform(self, X):\n\n       # so that we do not over-write the original dataframe\n        X = X.copy()\n        \n        for feature in self.variables:\n            X[feature] = X[self.reference_variable] - X[feature]\n\n        return X\n    \nclass CustomImputer(BaseEstimator, TransformerMixin) : \n     def __init__(self, variable, by) : \n            #self.something enables you to include the passed parameters\n            #as object attributes and use it in other methods of the class\n            self.variable = variable\n            self.by = by\n\n     def fit(self, X, y=None) : \n          self.map = X.groupby(self.by)[variable].mean()\n          #self.map become an attribute that is, the map of values to\n          #impute in function of index (corresponding table, like a dict)\n          return self\n\n     def transform(self, X, y=None) : \n          X[variable] = X[variable].fillna(value = X[by].map(self.map))\n          #Change the variable column. If the value is missing, value should \n          #be replaced by the mapping of column \"by\" according to the map you\n          #created in fit method (self.map)\n          return X\n    \n# categorical missing value imputer\nclass Mapper(BaseEstimator, TransformerMixin):\n\n    def __init__(self, variables, mappings):\n\n        if not isinstance(variables, list):\n            raise ValueError('variables should be a list')\n\n        self.variables = variables\n        self.mappings = mappings\n\n    def fit(self, X, y=None):\n        # we need the fit statement to accomodate the sklearn pipeline\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        for feature in self.variables:\n            X[feature] = X[feature].map(self.mappings)\n\n        return X  \n    \n##########################################################################\nclass CountFrequencyEncoder(BaseEstimator, TransformerMixin):\n    #temp = df['card1'].value_counts().to_dict()\n    #df['card1_counts'] = df['card1'].map(temp)\n    def __init__(\n        self,\n        encoding_method: str = \"count\",\n        variables: Union[None, int, str, List[Union[str, int]]] = None,\n        keep_variable=True,\n                  ) -> None:\n\n        self.encoding_method = encoding_method\n        self.variables = variables\n        self.keep_variable=keep_variable\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        \"\"\"\n        Learn the counts or frequencies which will be used to replace the categories.\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n            The training dataset. Can be the entire dataframe, not just the\n            variables to be transformed.\n        y: pandas Series, default = None\n            y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        self.encoder_dict_ = {}\n\n        # learn encoding maps\n        for var in self.variables:\n            if self.encoding_method == \"count\":\n                self.encoder_dict_[var] = X[var].value_counts().to_dict()\n\n            elif self.encoding_method == \"frequency\":\n                n_obs = float(len(X))\n                self.encoder_dict_[var] = (X[var].value_counts() \/ n_obs).to_dict()\n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        # replace categories by the learned parameters\n        X = X.copy()\n        for feature in self.encoder_dict_.keys():\n            if self.keep_variable:\n                X[feature+'_fq_enc'] = X[feature].map(self.encoder_dict_[feature])\n            else:\n                X[feature] = X[feature].map(self.encoder_dict_[feature])\n        return X[self.variables].to_numpy()\n#################################################   \nclass FeaturesEngineerGroup(BaseEstimator, TransformerMixin):\n    def __init__(self,groupping_method =\"mean\",\n                   variables=  \"amount\",\n                   groupby_variables = \"nameOrig\"                         \n                 ) :\n        self.groupping_method = groupping_method\n        self.variables=variables\n        self.groupby_variables=groupby_variables\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n        The training dataset. Can be the entire dataframe, not just the\n        variables to be transformed.\n        y: pandas Series, default = None\n        y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        self.group_amount_dict_ = {}\n        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n        #df = pd.merge(df,temp,on='card1',how='left')\n        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n        # learn mean\/medain \n        #for groupby in self.groupby_variables:\n         #   for var in self.variables:\n        if self.groupping_method == \"mean\":\n            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['mean']).to_dict()\n        elif self.groupping_method == \"median\":\n            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['median']).to_dict()\n        else:\n            print('error , chose mean or median')\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        #for col in self.variables:\n         #   for agg_type in self.groupping_method:\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_[ self.variables][self.groupping_method])\n        return X[new_col_name].to_numpy().reshape(-1,1)    \n    \n################################################   \nclass FeaturesEngineerGroup2(BaseEstimator, TransformerMixin):\n    def __init__(self,groupping_method =\"mean\",\n                   variables=  \"amount\",\n                   groupby_variables = \"nameOrig\"                         \n                 ) :\n        self.groupping_method = groupping_method\n        self.variables=variables\n        self.groupby_variables=groupby_variables\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n        The training dataset. Can be the entire dataframe, not just the\n        variables to be transformed.\n        y: pandas Series, default = None\n        y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        X = X.copy()\n        self.group_amount_dict_ = {}\n        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n        #df = pd.merge(df,temp,on='card1',how='left')\n        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n        # learn mean\/medain \n        #for groupby in self.groupby_variables:\n         #   for var in self.variables:\n\n        print('we have {} unique clients'.format(X[self.groupby_variables].nunique()))\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method    \n        X[new_col_name] = X.groupby([self.groupby_variables])[[self.variables]].transform(self.groupping_method)\n        X = X.drop_duplicates(['nameOrig'])\n    \n        self.group_amount_dict_ = dict(zip(X[self.groupby_variables], X[new_col_name]))\n        del X\n        print('we have {} unique mean amount : one for each client'.format(len(self.group_amount_dict_)))\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        #for col in self.variables:\n         #   for agg_type in self.groupping_method:\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_)\n        return X[new_col_name].to_numpy().reshape(-1,1)   \n    \n############################################  \nclass FeaturesEngineerCumCount(BaseEstimator, TransformerMixin):\n    def __init__(self,group_one =\"step\",\n                   group_two=  \"nameOrig\"                       \n                 ) :\n        self.group_one =group_one\n        self.group_two=group_two\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        \"\"\"\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        new_col_name =  self.group_two+'_Transaction_count'\n        X[new_col_name] = X.groupby([self.group_one, self.group_two])[[self.group_two]].transform('count')\n        return X[new_col_name].to_numpy().reshape(-1,1)","fd4e046e":"# Cat columns: \ncat_pipe = Pipeline([\n                     ('Encoder',ce.target_encoder.TargetEncoder())\n                     \n                    ])\n#Num_columns:\nnum_pipe = Pipeline([('imputer', SimpleImputer(strategy='median',add_indicator=False)),\n                     ('scaler', QuantileTransformer())\n                    ])\n#Feature Union fitting training data :\npreprocessor = FeatureUnion(transformer_list=[('cat', cat_pipe),\n                                              ('num', num_pipe)])\n# Using ColumnTransformer:\ndata_cleaning = ColumnTransformer([\n    ('cat_columns',  cat_pipe, cat_columns ),\n    ('num_columns', num_pipe , num_columns)\n])\n# preprocessor.fit(X_train)\n#############################\n# Complete Pipe \ndef create_pipeline(model,preprocessor,FeaturesEngineer=None):\n    pipeline = Pipeline([ \n        ('pre', preprocessor),\n        ('lgbm', model)\n    ])\n    return pipeline\n\npreprocessor ","c740f991":"train0=data_cleaning.fit_transform(X_train,y_train)\nprint('Every thing is OK:')\nprint('train have size of {}'.format(train0.shape))\ntrain0[0]","e06edc8d":"# complete pipe :\n# select the float\/cat columns\n#cat_feautres = X.select_dtypes(include=['object','category']).columns\n#num_features = X.select_dtypes(exclude=['object','category']).columns\n#Define vcat pipeline\nfeatures_cum_count=['step','nameOrig']\nfeatures_groupby_amount=['amount','nameOrig']\nfeatures_frequency_orig_dest=['nameOrig','nameDest']\nfeatures_cum_count_pipe = Pipeline([\n                     ('transformer_Encoder', FeaturesEngineerCumCount())\n                    ])\n\nfeatures_groupby_pipe = Pipeline([\n                     ('transformer_group_amount_mean', FeaturesEngineerGroup2()),\n                     ('transformer_group_scaler', PowerTransformer())\n                    ])\nfeatures_frequency_pipe = Pipeline([\n                     ('Encoder', CountFrequencyEncoder(variables=['nameOrig','nameDest'],encoding_method =\"frequency\", keep_variable=False))\n                    ])\ntype_pipe= Pipeline([\n                     ('transformer_Encoder', ce.cat_boost.CatBoostEncoder())\n                    ])\nnum_features0=[  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']\n#Define vnum pipeline\nnum_pipe = Pipeline([\n                     ('scaler', PowerTransformer()),\n                    ])\n#Featureunion fitting training data\npreprocessor = FeatureUnion(transformer_list=[('cum_count', features_cum_count_pipe),\n                                              ('mean_amount', features_groupby_pipe),\n                                              ('frequency_dest_orig', features_frequency_pipe),\n                                              ('trans_type', type_pipe),\n                                              ('num', num_pipe)])\ndata_preparing= ColumnTransformer([\n    ('cum_count', features_cum_count_pipe, features_cum_count ),\n    ('mean_amount', features_groupby_pipe, features_groupby_amount ),\n    ('frequency_dest_orig', features_frequency_pipe, features_frequency_orig_dest ),\n    ('trans_type', type_pipe, ['type'] ),\n    ('num', num_pipe, num_features0)\n], remainder='drop')\ndata_preparing","97ee2106":"train1=data_preparing.fit_transform(X_train,y_train)\nprint('Every thing is OK:')\nprint('train have size of {}'.format(train1.shape))\ntrain1[0]","31f1a18f":"<a id=0><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n\n<center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n    \n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    \n    Tasks:\n    \n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n   Tasks:\n    \n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    \n    Tasks:\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=1><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n\nThere may be two types of questions:\n\n**A.Technical Questions:**\n  \nCan ML be a solution to the problem?\n\n    \n                Do we have THE data?\n                Do we have all necessary related data?\n                Is there enough amount of data to develop algorithm?\n                Is data collected in the right way?\n                Is data saved in the right format?\n                Is the access to information guaranteed?\n\nCan we satisfy all the Business Questions by means of ML?\n\n**B.Business Questions:**\n    \nWhat are the organization's business goals?\n    \n                To reduce cost and increase revenue? \n                To increase efficiencies?\n                To avoid risks? To improve quality?\n    \nIs it worth to develop ML?\n    \n                In short term? In long term?\n                What are the success metrics?\n                Can we handle the risk if the project is unsuccessful?\n    \nDo we have the resources?\n    \n                Do we have enough time to develop ML?\n                Do we have a right talented team?\n\n\n    \nWE are provided a synthetic dataset for a mobile payments application. In this dataset, you are provided the sender and recipient of a transaction as well as whether transactions are tagged as fraud or not fraud. Your task is to build a fraud detection API that can be called to predict whether or not a transaction is fraudulent.\nYou can download the dataset here:https:\/\/www.kaggle.com\/bannourchaker\/frauddetection\n    \nYou are expected to build a REST API that predicts whether a given transaction is fraudulent or not. You are also to assume that the previous API calls are to be stored in order to engineer\nfeatures relevant to finding fraud. The API calls will include the time step of the transaction, so you can assume that a transaction happens sequentially within the same time step.\nFor example, if I make the following transactions in the same time step:  \n    \n\n    \nThe first transaction is unlikely to be fraudulent, since anon is initiating a normal transfer.\nHowever, multiple successive transfers of the same amount in the same hour is potentially fraudulent, since anon\u2019s account might have been taken over by a fraudster. On the first API call,your model is unlikely to classify the transaction as fraudulent. However, on the fifth call, it\u2019s likely that it will be tagged as fraudulent.\nThe REST API only has 1 endpoint \/is-fraud that takes in a POST request:\n    \nThe body is expected to receive the following fields(which are also the fields that can be found in your dataset:\nThe following is a sample body when making a POST request to your\n    \n    \n            {\n        \"step\":1,\n        \"type\":\"PAYMENT\",\n        \"amount\":9839.64,\n        \"nameOrig\":\"C1231006815\",\n        \"oldbalanceOrig\":170136.0,\n        \"newbalanceOrig\":160296.36,\n        \"nameDest\":\"M1979787155\",\n        \"oldbalanceDest\":0.0,\n        \"newbalanceDest\":0.0\n        }\n    \n    \nYour API is expected to return a JSON object with a boolean field isFraud. You may find a\nsample response below:\n    \n    {\"isFraud\": true}\n    \n**summary:**\nwe are expecting the following:\n    \n- 1. Deployed REST API:\n    \n    a. As mentioned above, we would need an API that takes in a POST request for the\n    \/is-fraud url and returns a prediction on whether or not a transaction is\n    fraudulent.\n    \n    b. Your REST API should be public for us to call the API and evaluate the accuracy\n    of your model\n    \n    c. Given the nature of the data, your REST API will likely need to take into account\n    previous transactions, so make sure it is able to take note of transactions from\n    your training dataset as well as previous API calls.\n\n- 2. Model\n    \n    a. We are expecting a machine learning model that can correctly classify whether or\n    not a transaction is fraudulent.\n\n**What is the objective of the machine learning model?**\n\nWe aim to predict  the real transactions fraud  and the fraud estimated by our model. We will evaluate model performance with the:\n\n   - F beta score\n    \n   - ROC AUC score\n    \n   - PR AUC score | Average precision\n    \n    \n## Step 1: Import helpful libraries","9756a306":"# Build OOP Features Engineer Classes \nFeature generation creates new features using knowledge about the problem and data. In the real-world example, you saw that fraud features could be derived from the transaction history. In this dataset, some features have already been derived: \n\n    Step: Timedelta. \n \n\nHere are some ways that feature columns can be analyzed, combined, and transformed to create new features: \n\n    Splitting or combining columns\n    Encoding target, frequency, and aggregation\n\n**Splitting or combining column:s**\n\nIf this correlates better with the target, combine columns or split one column into two.\n\n\nEncoding target, frequency, and aggregation\n\nTo detect credit card fraud, you are looking for unusual credit card behavior. Target, frequency, and aggregation encodings add features that measure the rarity of features or combinations of features. \n\nWith target encoding, features are replaced or added with the probability of the categorical value corresponding to the target. For example, if 3% of fraudulent transactions are of card type \u201cdebit,\u201d then the value \u201cdebit\u201d is replaced with .03.  \n\nWith frequency encoding, features are replaced or added with the count of the categorical value. For example, to add a column for how frequently NameDis or NameOrig  occurs in transactions, you can add a value count column for that feature, as in the following code example: \n\n        temp = df['NameOrig'].value_counts().to_dict()\n        df['NameOrig_counts'] = df['NameOrig'].map(temp)\n        \n\nI used frequency encoding to create the following new features: \n\n    NameDis\n    NameOrig\n    \n\nAggregation encoding adds features based on feature aggregation statistics, such as mean or standard deviation for groups of features. This allows machine learning algorithms such as decision trees to determine if a value is common or rare for a particular group. \n\nYou can calculate group statistics by providing pandas with three variables: group, variable of interest, and type of statistic. For example, the following code example adds to each row the average TransactionAmt value for that row\u2019s NameOrig group, allowing the ML algorithm to determine if a row has an abnormal TransactionAmt value for the NameOrig value (the Name holder\u2019s account).\n\n        temp = df.groupby('card1')['TransactionAmt'].agg(['mean'])   \n            .rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n        df = pd.merge(df,temp,on='card1',how='left')\n        \n        \n\nClassification identifies the class or category to which an item belongs, based on the training data of labels and features. Features are the interesting properties in the data that you can use to make predictions. To build a classifier model, you extract and test to find the features of interest that most contribute to the classification. For feature engineering for credit card fraud, the goal is to distinguish normal card usage from fraudulent unusual card usage, for example, features that measure the differences between recent and historical activities.\n\nFor online credit card transactions, there are features associated with the transaction or credit card holder and features that can be engineered from transaction histories. \n\nFeatures associated with the transaction: \n\n    Date and time\n    Transaction amount\n    Merchant\n    Product\n\nFeatures associated with the credit card holder:\n\n    Card type\n    Credit card number \n    Expiration date\n    Billing address (street address, state, zip code), \n    Phone number \n    Email address\n\n\nPossible features generated from transaction history:\n\nNumber of transactions a credit card holder has made in the last 24 hours (holder features plus device ID and IP address) its not availble in our data \ni have only step so i add new feature cum count for each client in  each step.\nif we can get more data we can  exctract other useful features : \n\n    Same or different credit card numbers?\n    Same or different shipping addresses? \n    The total amount a credit cardholder has spent in the last 24 hours\n    Average amount last 24 hours compared to the historical daily average amount\n    Number of transactions made from the same device or IP address in the last 24 hours\n    Multiple online charges in close temporal proximity?\n    Frequent fraud at a specific merchant?\n    Group of fraud charges in certain areas?\n    Multiple charges from a merchant within a short time span?\n    \n  \nClassification identifies the class or category to which an item belongs, based on the training data of labels and features. Features are the interesting properties in the data that you can use to make predictions. To build a classifier model, you extract and test to find the features of interest that most contribute to the classification. For feature engineering for credit card fraud, the goal is to distinguish normal card usage from fraudulent unusual card usage, for example, features that measure the differences between recent and historical activities. \nFinally for this step we get the most import features by coding some transformer .  \n","6c21e9f2":"\n## Step 2: Load the data\nComplete guid to read data : \nNext, we'll load the training and test data.","15eb05a5":"# Advanced Pipe :\nThis pipe include features engineer+ some advanced preprocessing steps for each columns.","05525772":"**Num Features**\n\n","d4a0d72c":"<a id=3><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Preparation<\/center><\/h3>\n\n\n## Data preprocessing\n\n\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling.\n\n\n### Missing Values  :\n\n- A Simple Option: Drop Columns with Missing Values\n\n-  Replacing missing values with constants        \n    \n-  A Better Option: Imputation\n\nImputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n\n- An Extension To Imputation\n\nImputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. \n                    \nA popular approach to missing data imputation is to use a model to predict the missing values. This requires a model to be created for each input variable that has missing values. Although any one among a range of different models can be used to predict the missing values, the k-nearest neighbor (KNN) algorithm has proven to be generally effective, often referred to as **\u201cnearest neighbor imputation.\u201d**\n\n- Iterative imputation\n\nOne approach to imputing missing values is to use an iterative imputation model.\nIterative imputation refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted. Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n\nIt is iterative because this process is repeated multiple times, allowing ever improved estimates of missing values to be calculated as missing values across all features are estimated.\n\n### Scaling \n\nWhile this assumption of similar scales is necessary, it is rarely true in real world data. For this reason you need to rescale your data to ensure that it is on the same scale. There are many different approaches to doing this but we will discuss the two most commonly used approaches here, Min-Max scaling (sometimes referred to as **normalization**), and **standardization**.\nyou need to rescale your data to ensure that it is on the same scale. There are many different approaches to doing this:\n\n**Normalization:**\nIn normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest. When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.)\nNormalization scales all points linearly between the upper and lower bound.\n\n**Standardization:**\nThe other commonly used scaler is called standardization. As opposed to finding an outer boundary and squeezing everything within it, standardization instead finds the mean of your data and centers your distribution around it, calculating the number of standard deviations away from the mean each point is. These values (the number of standard deviations) are then used as your new values. This centers the data around 0 but technically has no limit to the maximum and minimum values as you can see here.\n\n**Log Transformer:**\nBoth normalization and min-max scaling are types of scalers, in other words the data remained in the same shape but was squashed or scaled. A log transformation on the other hand can be used to make highly skewed distributions less skewed. Take for example one of the salary columns from the stack overflow dataset shown here where there is a very long right tail.\n\nHelps with skewness No predetermined range for scaled data Useful only on non-zero, non-negative data.\n\nThe Log Transform is one of the most popular Transformation techniques out there. It is primarily used to convert a skewed distribution to a normal distribution\/less-skewed distribution. In this transform, we take the log of the values in a column and use these values as the column instead.\n\nWhy does it work? It is because the log function is equipped to deal with large numbers. Here is an example-\n\nlog(10) = 1\n\nlog(100) = 2, and\n\nlog(10000) =4\n\nThus, the log operation had a dual role:\n\n    Reducing the impact of too-low values\n    Reducing the impact of too-high values.\n\nA small caveat though \u2013 if our data has negative values or values ranging from 0 to 1, we cannot apply log transform directly \u2013 since the log of negative numbers and numbers between 0 and 1 is undefined, we would get error or NaN values in our data. In such cases, we can add a number to these values to make them all greater than 1. Then, we can apply the log transform.\n\n**Min-Max Scaler:**\nRescales to predetermined range [0\u20131] Doesn\u2019t change distribution\u2019s center (doesn\u2019t correct skewness) Sensitive to outliers\n\n**Max Abs Scaler:**\nRescales to predetermined range [-1\u20131] Doesn\u2019t change distribution\u2019s center Sensitive to outliers\n\nIn simplest terms, the MaxAbs scaler takes the absolute maximum value of each column and divides each value in the column by the maximum value.\n\nThus, it first takes the absolute value of each value in the column and then takes the maximum value out of those. This operation scales the data between the range [-1, 1]\n\n**Standard Scaler:**\nShifts distribution\u2019s mean to 0 & unit variance No predetermined range Best to use on data that is approximately normally distributed\nFor each feature, the Standard Scaler scales the values such that the mean is 0 and the standard deviation is 1(or the variance).\nx_scaled = x \u2013 mean\/std_dev\n\nHowever, Standard Scaler assumes that the distribution of the variable is normal. Thus, in case, the variables are not normally distributed, we\n\n    either choose a different scaler\n    or first, convert the variables to a normal distribution and then apply this scaler\n\n\n**Robust Scaler:**\n0 mean & unit variance Use of quartile ranges makes this less sensitive to (a few) outliers No predetermined range\nIf you have noticed in the scalers we used so far, each of them was using values like the mean, maximum and minimum values of the columns. All these values are sensitive to outliers. If there are too many outliers in the data, they will influence the mean and the max value or the min value. Thus, even if we scale this data using the above methods, we cannot guarantee a balanced data with a normal distribution.\n\nThe Robust Scaler, as the name suggests is not sensitive to outliers. This scaler-\n\n    removes the median from the data\n    scales the data by the InterQuartile Range(IQR)\n\nAre you familiar with the Inter-Quartile Range? It is nothing but the difference between the first and third quartile of the variable. The interquartile range can be defined as-\n\n    IQR = Q3 \u2013 Q1\n\nThus, the formula would be:\n\nx_scaled = (x \u2013 Q1)\/(Q3 \u2013 Q1)\n\n\n**Power Transformer:**\n\nHelps correct skewness 0 mean & unit variance No predetermined range Yeo-Johnson or Box-Cox Box-Cox can only be used on non-negative data\n\nI often use this feature transformation technique when I am building a linear model. To be more specific, I use it when I am dealing with heteroskedasticity. Like some other scalers we studied above, the Power Transformer also changes the distribution of the variable, as in, it makes it more Gaussian(normal). We are familiar with similar power transforms such as square root, and cube root transforms, and log transforms.\n\nHowever, to use them, we need to first study the original distribution, and then make a choice. The Power Transformer actually automates this decision making by introducing a parameter called lambda. It decides on a generalized power transform by finding the best value of lambda using either the:\n\n1. Box-Cox transform\n\n2. The Yeo-Johnson transform\n\nWhile I will not get into too much detail of how each of the above transforms works, it is helpful to know that Box-Cox works with only positive values, while Yeo-Johnson works with both positive and negative values\n\n**Quantile Transformer Scaler:**\nOne of the most interesting feature transformation techniques that I have used, the Quantile Transformer Scaler converts the variable distribution to a normal distribution. and scales it accordingly. Since it makes the variable normally distributed, it also deals with the outliers. Here are a few important points regarding the Quantile Transformer Scaler:\n\n1. It computes the cumulative distribution function of the variable\n\n2. It uses this cdf to map the values to a normal distribution\n\n3. Maps the obtained values to the desired output distribution using the associated quantile function\n\nA caveat to keep in mind though: Since this scaler changes the very distribution of the variables, linear relationships among variables may be destroyed by using this scaler. Thus, it is best to use this for non-linear data.\n\n**Unit Vector Scaler\/Normalizer:**\nNormalization is the process of scaling individual samples to have unit norm. The most interesting part is that unlike the other scalers which work on the individual column values, the Normalizer works on the rows! Each row of the dataframe with at least one non-zero component is rescaled independently of other samples so that its norm (l1, l2, or inf) equals one.\n\nJust like MinMax Scaler, the Normalizer also converts the values between 0 and 1, and between -1 to 1 when there are negative values in our data.\n\nHowever, there is a difference in the way it does so.\n\n    If we are using L1 norm, the values in each column are converted so that the sum of their absolute values along the row = 1\n    If we are using L2 norm, the values in each column are first squared and added so that the sum of their absolute values along the row = 1\n    \n    \n**Custom Transformer:**\nConsider this situation \u2013 Suppose you have your own Python function to transform the data. Sklearn also provides the ability to apply this transform to our dataset using what is called a FunctionTransformer.\n\nLet us take a simple example. I have a feature transformation technique that involves taking (log to the base 2) of the values. In NumPy, there is a function called log2 which does that for us.\n\nThus, we can now apply the FunctionTransformer:\n\n**Binning:**\nWhile numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. \n\nFor many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n\nBins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n\n\n**Cat Features:** \n\n    Label Encoding or Ordinal Encoding\n    One hot Encoding\n    Dummy Encoding\n    Effect Encoding\n    Binary Encoding\n    BaseN Encoding\n    Hash Encoding\n    Target Encoding\n\n## Data Umbalanced \n\n\n**Resampling**\n\n- Random Undersampling\n\n'not minority' = resample all classes but not the minority class\n\n- Random Oversampling\n\n\"not majority\" = resample all classes but not the majority class\n\n- Stratified Sampling\n\n**Applying SMOTE**\n\n**Adjusting your algorithm weights**\n\n- Model building with Class weight balancing\n\n        \n**Ensemble methods**\n\n- Are Robust \n\n- Avoid Overfitting \n\n- Improve Predictions Performance  \n         \n**Clustering methods to detect Minority** \n        \n**Other clustering fraud detection methods**\nExplore using a density based clustering method (DBSCAN) to detect fraud. The advantage of DBSCAN is that you do not need to define the number of clusters beforehand. Also, DBSCAN can handle weirdly shaped data (i.e. non-convex) much better than K-means can. This time, you are not going to take the outliers of the clusters and use that for fraud, but take the smallest clusters in the data and label those as Minority .\n\n   \n- you first need to figure out how big the clusters are, and filter out the smallest\n\n- then, you're going to take the smallest ones and flag those as fraud\n\n- last, you'll check with the original labels whether this does actually do a good job in detecting fraud.\n\n        \n            \n**Using text data**   \n\n          \n**Using list of terms**  \n\n**Creating a flag**\n\n  \n\n**Topic modeling on Minority example fraud**  \n\n    \n**Flagging fraud based on topics**\n\n   \n    \n**Threshold**\n\nwe can try using the original model (trained on the original \u201cimbalanced\u201d data set) and simply plot the trade-off between false positives and false negatives to choose a threshold that may produce a desirable business result.    \n    \n\n## Outlier Handling\n\n**Statistical outlier removal**\n\nWhile removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together. we can trim data like this :\n\n        #train_std = train['cont1'].mean()\n        #train_mean = train['cont1'].std()\n\n        #cut_off = train_std * 3\n        #train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n\n        # Trim the test DataFrame\n        #trimmed_df = so_test_numeric[(train['cont1'] < train_upper) \\\n                                    # & (train['cont1'] > train_lower)]\n\n    \n","830a72c3":"# Features selection : \n**Feature Selection**\nFeature selection is a method of selecting features from your feature set to be used for modeling. It draws from a set of existing features, so it's different than feature engineering because it doesn't create new features. The overarching goal of feature selection is to improve your model's performance. Perhaps your existing feature set is much too large, or some of the features you're working with are unnecessary. There are different ways you can perform feature selection. It's possible to do it in an automated way. Scikit-learn has several methods for automated feature selection, such as choosing a variance threshold and using univariate statistical tests\n\n**Why reduce dimensionality?**\n\nYour dataset will become simpler and thus easier to work with, require less disk space to store and computations will run faster. In addition, models are less likely to overfit on a dataset with fewer dimensions.\n\n**Selection vs extraction**\n\nWhen we apply feature selection, we completely remove a feature and the information it holds from the dataset. We try to minimize the information loss by only removing features that are irrelevant or hold little unique information, but this is not always possible.\n\nCompared to feature selection, feature extraction is a completely different approach but with the same goal of reducing dimensionality. Instead of selecting a subset of features from our initial dataset, we'll be calculating, or extracting, new features from the original ones. These new features have as little redundant information in them as possible and are therefore fewer in number. One downside is that the newly created features are often less intuitive to understand than the original ones. \nPCA Calculating.\n\n\n![image.png](attachment:da2cbaec-dca7-4184-af88-830c5de5ed27.png)\n\nI will not do this step in this project , but you can try those methods in order to get better results .\n ","4e55fcdb":"<a id=5><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Evaluation<\/center><\/h3>\n\n# Model accuracy scoring\n\nThe easiest way to analyze performance is with accuracy. \nIt measures how many observations, both positive and negative, were correctly classified.\n\n\nYou shouldn\u2019t use accuracy on imbalanced problems. Then, it is easy to get a high accuracy score by simply classifying all observations as the majority class. For example in our case, by classifying all transactions as non-fraudulent we can get an accuracy of over 0.9.\n\n**When to use it:**\n\n    When your problem is balanced using accuracy is usually a good start. An additional benefit is that it is really easy to explain it to non-technical stakeholders in your project,\n    When every class is equally important to you.\n\n# Confusion Matrix\n\n**How to compute:**\n\nIt is a common way of presenting true positive (tp), true negative (tn), false positive (fp) and false negative (fn) predictions. Those values are presented in the form of a matrix where the Y-axis shows the true classes while the X-axis shows the predicted classes.\n\nIt is calculated on class predictions, which means the outputs from your model need to be thresholded first.\n\n**When to use it:**\n\n    Pretty much always. I like to see the nominal values rather than normalized to get a feeling on how the model is doing on different, often imbalanced, classes.\n\n\n\n# ROC Curve\n\n\nIt is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n\nOf course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left side are better.\n\nSince we have an imbalanced data set, Receiver Operating Characteristic Curves are not that useful although it's an expected output of most binary classifiers.\nBecause you can generate a pretty good-looking curve by just simply guessing each one is the non-fraud case.\n\n**When to use it:**\n\n    You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities (read this article by Jason Brownlee if you want to learn about probability calibration).\n    You should not use it when your data is heavily imbalanced. It was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n    You should use it when you care equally about positive and negative classes.. If we care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.\n    \n# ROC AUC score   \nAUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.\n\n**When to use it:**\n\n    You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities (read this article by Jason Brownlee if you want to learn about probability calibration).\n    You should not use it when your data is heavily imbalanced. It was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n    You should use it when you care equally about positive and negative classes. It naturally extends the imbalanced data discussion from the last section. If we care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.\n\n# Recall    \nIt measures how many observations out of all positive observations have we classified as positive. It tells us how many fraudulent transactions we recalled from all fraudulent transactions.\ntrue positive rate\n\nWhen you are optimizing recall you want to put all guilty in prison.\n**When to use it:**\n\n    Usually, you will not use it alone but rather coupled with other metrics like precision.\n    That being said, recall is a go-to metric, when you really care about catching all fraudulent transactions even at a cost of false alerts. Potentially it is cheap for you to process those alerts and very expensive when the transaction goes unseen.\n    \n# Precision\n\nIt measures how many observations predicted as positive are in fact positive. Taking our fraud detection example, it tells us what is the ratio of transactions correctly classified as fraudulent.\npositive predictive value\n\nWhen you are optimizing precision you want to make sure that people that you put in prison are guilty. \n\n**When to use it:**\n\n    Again, it usually doesn\u2019t make sense to use it alone but rather coupled with other metrics like recall.\n    When raising false alerts is costly, when you want all the positive predictions to be worth looking at you should optimize for precision.\n    \n\n\n**Precision vs. Recall for Imbalanced Classification:**\n\nYou may decide to use precision or recall on your imbalanced classification problem.\n\nMaximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives.\n\n    Precision: Appropriate when minimizing false positives is the focus.\n    Recall: Appropriate when minimizing false negatives is the focus.\n\nSometimes, we want excellent predictions of the positive class. We want high precision and high recall.\n\nThis can be challenging, as often increases in recall often come at the expense of decreases in precision.\n\n    In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since in order to increase the TP for the minority class, the number of FP is also often increased, resulting in reduced precision.\n    \n    \n# PR AUC score | Average precision\n\nSimilarly to ROC AUC score you can calculate the Area Under the Precision-Recall Curve to get one number that describes model performance.\n\nYou can also think about PR AUC as the average of precision scores calculated for each recall threshold [0.0, 1.0]. You can also adjust this definition to suit your business needs by choosing\/clipping recall thresholds if needed.\n\n**When to use it:**\n\n    when you want to communicate precision\/recall decision to other stakeholders\n    when you want to choose the threshold that fits the business problem.\n    when your data is heavily imbalanced. As mentioned before, it was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: since PR AUC focuses mainly on the positive class (PPV and TPR) it cares less about the frequent negative class.\n    when you care more about positive than negative class. If you care more about the positive class and hence PPV and TPR you should go with Precision-Recall curve and PR AUC (average precision).\n    \n# F beta score\n\nSimply put, it combines precision and recall into one metric. The higher the score the better our model is. You can calculate it in the following way:\n\n\n\n\n\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nF beta by beta\n\nWith 0<beta<1 we care more about precision and so the higher the threshold the higher the F beta score. When beta>1 our optimal threshold moves toward lower thresholds and with beta=1 it is somewhere in the middle.  \n\n**When to use it:**\n\n    Pretty much in every binary classification problem. It is my go-to metric when working on those problems. It can be easily explained to business stakeholders.\n    \n for more details see this article:[https:\/\/neptune.ai\/blog\/evaluation-metrics-binary-classification](http:\/\/)    \n \n==>Complete evaluation will be done when we train the model on all data that we have and with the best tuned model.","994577c0":"\n# Feature Engineering\n\nFeature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables \n\n**Binning**\n\nWhile working with numeric data we come across some features where distributions of variables are skewed in the sense that some sets of values will occur a lot and some will be very rare. Directly using this type of feature may cause issues or can give inaccurate results.\n\nBinning is a way to convert numerical continuous variables into discrete variables by categorizing them on the basis of the range of values of the column in which they fall. In this type of transformation, we create bins. Each bin allows a specific range of continuous numerical values. It prevents overfitting and increases the robustness of the model.\n\nLet\u2019s understand this using an example. We have scores of 10 students as 35, 46, 89, 20, 58, 99, 74, 60, 18, 81. Our task is to make 3 teams. Team 1 will have students with scores between 1-40, Team 2 will have students with scores between 41-80, and Team 3 will have students with scores between 81-100.\n\n\n\nBinning can be done in different ways listed below.\n\n      Fixed \u2013 Width Binning\n      Quantile Binning\n      Binning by Instinct\n      \nthe formula is:\nK = 1 + 3. 322*logN\n\nwhere:\nK = number of class intervals (bins).\nN = number of observations in the set.\nlog = logarithm of the number.\n\nAlso we have others techniques : \n\n\n **Sparse Interactions\/Kmeans Features\/Polynominal Features.....ex**\n \n\n\n![image.png](attachment:0ea1b671-18e5-48e8-8125-5eda709415aa.png)\n\n","77ba351b":"\n<a id=2><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n\n### Explore the data\/Analysis \n\nWe will analyse the following:\n\n    The target variable\n    \n    Variable types (categorical and numerical)\n    \n    Numerical variables\n        Discrete\n        Continuous\n        Distributions\n        Transformations\n\n    Categorical variables\n        Cardinality\n        Rare Labels\n        Special mappings\n\n    Null Data\n\n    Text data \n    \n    wich columns will we use\n    \n    IS there outliers that can destory our algo\n    \n    IS there diffrent range of data\n    \n    Curse of dimm...\n    \nThis Step is done here : [https:\/\/www.kaggle.com\/bannourchaker\/frauddetection-part1-eda\/edit](http:\/\/)","7fdee83a":"## Define the model features and target\n\n### Extract X and y ","86d6bd8a":"# Convert Dtypes :","eb910f38":"\n# Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the ************size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.\n","d281156d":"# Introduction\n    \nDue to rapid growth in field of cashless or digital \ntransactions, credit cards are widely used in all \naround the world. Credit cards providers are \nissuing thousands of cards to their customers.\n Providers have to ensure all the credit card users \nshould be genuine and real. Any mistake in issuing \na card can be reason of financial crises. \nDue to rapid growth in cashless transaction,\n the chances of number of fraudulent transactions can also increasing.\n A Fraud transaction can be identified by analyzing various\n behaviors of credit card customers from previous \ntransaction history datasets. If any deviation\n is noticed in spending behavior from available patterns, \nit is possibly of fraudulent transaction. \nData mining and machine learning techniques are widely used in credit card \nfraud detection. In this article we are presenting review \nof various data mining and machine learning methods\n which are widely used for credit card fraud detections and  complete this project end to end from Data Understanding to deploy Model via API .  \n    \n    \n ","fad9e880":"# What should we do for each colmun\n\n**Separate features by dtype**\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n\n**Cat Features**\n\n\n\n","18cd64a1":"<a id=6><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Deploy<\/center><\/h3>\n\nThe deployment of machine learning models is the process for making models available in production environments, where they can provide predictions to other software systems.\n\n\u25cfOne of the last stages in the Machine Learning Lifecycle.\n\n\u25cfPotentially the most challenging stage.\n\n\u25cfChallenges of traditional software\n\noReliability\noReusability\noMaintainability\noFlexibility\n\n\u25cfAdditional challenges specific to Machine Learning\n\noReproducibility\n\nNeeds coordination of data scientists, IT teams, software developers and business professionals:\n\noEnsure model works reliably\noEnsure model delivers the intended result.\n\n\u25cfPotential discrepancy between programming language in which the model is developed and the production system language.\n\noRe-coding the model extends the project timeline and risks lack of reproducibility\n\nWhy is Model Deployment important?\n\n\u25cfTo start using a Machine Learning Model, it needs to be effectively deployed into production, so that they can provide predictions to other software systems.\n\n\u25cfTo maximize the value of the Machine Learning Model, we need to be able to reliably extract the predictions and share them with other systems.\n\n\n**Research Environment**\n\n\u25cfThe Research Environment is a setting with tools, programs and software suitable for data analysis and the development of machine learning models.\n\n\u25cfHere, we develop the Machine Learning Models and identify their value.\nIts done by a data scientist : i prefer work on jupyter for this phase .\n\n**Production Environment**\n\n\u25cfThe Production Environment is a real-time setting with running programs and hardware setups that allow the organization\u2019s daily operations.\n\n\u25cfIt\u2019s the place where the machine learning models is actually available for business use.\n\n\u25cfIt allows organisations to show clients a \u201clive\u201d service.\nThis job is done by solid sofware+ml engineer+ devops team\n\n\n\nwe have 4 ways to deploy models .\nML System Architectures:\n1. Model embedded in application\n\n2. Served via a dedicated service\n\n3. Model published as data(streaming)\n\n4. Batch prediction (offline process)\n\n\nI developed  a baseline how to deploy model using Fastapi+docker on herokou :\n\nhttps:\/\/github.com\/DeepSparkChaker\/FraudDetection_Fastapi\n\n\nComplete deployment of our model is done here : \n<a id=7><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Summary<\/center><\/h3> \n\nWe had developed end-to-end machine learning using the CRISP_DM methodology. Work still in progress. Always keep in mind that the data science \/ ML project must be done as a team and iteratively in order to properly exploit our data and add value to our business. Also keep in mind that AI helps you make the decision by using the added value extracted from the data but not the accountability. So we have to keep in mind to always use a composite AI in order to make the final decision.\nDon't forgot to upvote if you find it useful .\n\nhttps:\/\/www.kaggle.com\/bannourchaker\/frauddetection-part3-modeling2-selectbestmodel?scriptVersionId=81276811\n\nfor complete deployement baseline see : \n\nhttps:\/\/github.com\/DeepSparkChaker\/FraudDetection_Fastapi\n\nReferences :\n\nhttps:\/\/developer.nvidia.com\/blog\/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution\/\n\npython guidline : \n\nhttps:\/\/gist.github.com\/sloria\/7001839\n\nfeatures  selections :\n\nhttps:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n\nhttps:\/\/pub.towardsai.net\/feature-selection-and-removing-in-machine-learning-dd3726f5865c\n\nhttps:\/\/www.kaggle.com\/bannourchaker\/1-featuresengineer-selectionpart1?scriptVersionId=72906910\n\nCripspdm :\nhttps:\/\/www.kaggle.com\/bannourchaker\/4-featureengineer-featuresselectionpart4?scriptVersionId=73374083\n\nQuanrile transformer : \n\nhttps:\/\/machinelearningmastery.com\/quantile-transforms-for-machine-learning\/\n\nBest link for all : \n\nhttps:\/\/neptune.ai\/blog\/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\n\ncomplete guide Stacking :\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/08\/ensemble-stacking-for-machine-learning-and-deep-learning\/\n\nhttps:\/\/neptune.ai\/blog\/ensemble-learning-guide\n\nhttps:\/\/www.kaggle.com\/prashant111\/adaboost-classifier-tutorial\n\n\nMissing : \n\nhttps:\/\/www.kaggle.com\/dansbecker\/handling-missing-values\n\nBinning : \n\nhttps:\/\/heartbeat.fritz.ai\/hands-on-with-feature-engineering-techniques-variable-discretization-7deb6a5c6e27\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/getting-started-with-feature-engineering\/\n\nCat :\n\nhttps:\/\/innovation.alteryx.com\/encode-smarter\/\n\nhttps:\/\/github.com\/alteryx\/categorical_encoding\/blob\/main\/guides\/notebooks\/categorical-encoding-guide.ipynb\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/\n\nhttps:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\n\nChoice of kmeans : \n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/k-mean-getting-the-optimal-number-of-clusters\/\n\nImputation : \n\nhttps:\/\/machinelearningmastery.com\/knn-imputation-for-missing-values-in-machine-learning\/\n\nhttps:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/\n\nChoice of  roc vs precssion_recall : \n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nHow to tune for he futur work : \n\nhttps:\/\/www.kaggle.com\/hamidrezabakhtaki\/xgboost-catboost-lighgbm-optuna-final-submission\n\nhttps:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding\n\n\n\nDeploy:\n\nhttps:\/\/towardsdatascience.com\/from-jupyter-notebook-to-deployment-a-straightforward-example-1838c203a437\n\n https:\/\/github.com\/DeepSparkChaker\/Titanic_Deep_Spark\/blob\/main\/app.py\nhttps:\/\/github.com\/Kunal-Varma\/Deployment-of-ML-model-using-FASTAPI\/tree\/2cc0319abbec469010a5139f460004f2a75a7482\nhttps:\/\/realpython.com\/fastapi-python-web-apis\/\n https:\/\/github.com\/tiangolo\/fastapi\/issues\/3373\n https:\/\/www.freecodecamp.org\/news\/data-science-and-machine-learning-project-house-prices\/\nhttps:\/\/github.com\/tiangolo\/fastapi\/issues\/1616\nhttps:\/\/stackoverflow.com\/questions\/68244582\/display-dataframe-as-fastapi-output\nhttps:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers\nhttps:\/\/github.com\/renanmouraf\/data-science-house-prices    \nhttps:\/\/towardsdatascience.com\/data-science-quick-tips-012-creating-a-machine-learning-inference-api-with-fastapi-bb6bcd0e6b01\nhttps:\/\/towardsdatascience.com\/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857\nhttps:\/\/analyticsindiamag.com\/complete-hands-on-guide-to-fastapi-with-machine-learning-deployment\/\n\nhttps:\/\/github.com\/shaz13\/katana\/blob\/develop\/Dockerfile\n\n\nhttps:\/\/github.com\/TripathiAshutosh\/FastAPI\/blob\/main\/main.py\n\nBest practices : \n    \nhttps:\/\/theaisummer.com\/best-practices-deep-learning-code\/    \nhttps:\/\/github.com\/The-AI-Summer\/Deep-Learning-In-Production\/tree\/master\/2.%20Writing%20Deep%20Learning%20code:%20Best%20Practises\n\n Docker :\n \n https:\/\/towardsdatascience.com\/docker-in-pieces-353525ec39b0?fbclid=IwAR102sks2L0vRTde2qz1g4I4NhqXxnoqfV4IFzmZke4DvGcuiuYhj25eVSY\n \nhttps:\/\/github.com\/dkhundley\/ds-quick-tips\/blob\/master\/012_dockerizing_fastapi\/Dockerfile\n\n\n Deploy + scaling :\nhttps:\/\/towardsdatascience.com\/deploying-ml-models-in-production-with-fastapi-and-celery-7063e539a5db\nhttps:\/\/github.com\/jonathanreadshaw\/ServingMLFastCelery\n\nhttps:\/\/github.com\/trainindata\/deploying-machine-learning-models\/blob\/aaeb3e65d0a58ad583289aaa39b089f11d06a4eb\/section-04-research-and-development\/07-feature-engineering-pipeline.ipynb\n\nMl OPS : \nhttps:\/\/www.linkedin.com\/posts\/vipulppatel_getting-started-with-mlops-21-page-tutorial-activity-6863895411837415424-dWMh\/?fbclid=IwAR3Y4clbzujS_s2FFWg3tTYMKaGhh3vo25NUyoVdKHAJ7zynmCTNtzlHQ4M\n\nhttps:\/\/towardsai.net\/p\/machine-learning\/mlops-demystified?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR3MimsSXCFq3GqiLKoaQqXbeb3bkSwKhSkfQSKT_c1gsHDMGSBAv63s7Po\nhttps:\/\/www.youtube.com\/watch?v=9I8X-3HIErc\n\nhttps:\/\/pub.towardsai.net\/deployment-ml-ops-guide-series-2-69d4a13b0dcf\n\nPublish to medium : \n\nhttps:\/\/towardsai.net\/p\/data-science\/how-to-publish-a-jupyter-notebook-as-a-medium-blogpost?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR2-an7kknO3bsI5xjRdjL3jiwuPy7MBN5lVBc6fzx15mGY2iLS5KndCYWc\n\n\n","a8c61bb8":" # Baseline Pipe :\n This the first round to get the best preprocess steps ","fd575942":"<a id=4><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Modeling<\/center><\/h3>\n\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n\nTasks\n\n1. Select modeling technique Select technique\n\n2. Generate test design\n\n3. Build model\n\n4. Assess model\n\nThis Step is done here : \n\n\n","8c6334e1":"# check that we have all column"}}