{"cell_type":{"814c8918":"code","dc4e5a95":"code","00d36b15":"code","41bc40ff":"code","694c0699":"code","7901d4c0":"code","d6187ce8":"code","3e7f5726":"code","3563c5d3":"code","b55b793f":"code","072ac1ec":"code","4fe7a3e1":"code","40412307":"code","f9a36621":"code","efafdb35":"code","d5734b11":"code","9740b6d3":"code","4fca9a29":"code","a3d42cf3":"code","1004fa0b":"code","75708d2f":"markdown","c7e142cd":"markdown"},"source":{"814c8918":"# Set environment variables\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nVERSION = 1\nINPUT_PATH = f\"\/kaggle\/input\/m5-forecasting-accuracy\"\nBASE_PATH = f\"\/kaggle\/working\/m5-forecasting-accuracy-ver{VERSION}\"","dc4e5a95":"# Turn off warnings\n\nwarnings.filterwarnings(\"ignore\")","00d36b15":"# Change directory\n\nos.chdir(INPUT_PATH)\nprint(f\"Change to directory: {os.getcwd()}\")","41bc40ff":"# Memory usage function and merge by concat function (not to lose data type)\n\ndef format_memory_usage(total_bytes):\n    unit_list = [\"\", \"Ki\", \"Mi\", \"Gi\"]\n    for unit in unit_list:\n        if total_bytes < 1024:\n            return f\"{total_bytes:.2f}{unit}B\"\n        total_bytes \/= 1024\n    return f\"{total_bytes:.2f}{unit}B\"\n\ndef merge_by_concat(df1, df2, columns):\n    df_temp = df1[columns]\n    df_temp = df_temp.merge(df2, on = columns, how = \"left\")\n    new_columns = [column for column in list(df_temp) if column not in columns]\n    df1 = pd.concat([df1, df_temp[new_columns]], axis = 1)\n    return df1","694c0699":"# Load and check dataset\n\ndf_sales_train_validation = pd.read_csv(\"sales_train_validation.csv\")\ndf_sales_train_validation.head(10)","7901d4c0":"# Add another 28 days with null values to make predictions successfully\n\nnumber_of_train = 1913\ndays_to_predict = 28\n\nfor i in range(days_to_predict):\n    prediction_d = number_of_train + (i + 1)\n    df_sales_train_validation[f\"d_{prediction_d}\"] = np.nan\ndf_sales_train_validation.head(10)","d6187ce8":"# Create features\n# Melt the dataframe to have \"sales everyday\" as a feature\n\nindex_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\ndf_sales_features = df_sales_train_validation.melt(\n    id_vars = index_columns\n    , var_name = \"d\"\n    , value_name = \"sales\"\n)\ndf_sales_features.head(10)","3e7f5726":"# Memory usage control\n\nmemory_usage_string = format_memory_usage(df_sales_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\n# Technics: converting strings to categorical variables\nfor column in index_columns:\n    df_sales_features[column] = df_sales_features[column].astype(\"category\")\n\nmemory_usage_string = format_memory_usage(df_sales_features.memory_usage().sum())\nprint(f\"Reduced memory usage: {memory_usage_string}\")","3563c5d3":"# Load price dataset\n\ndf_sell_prices = pd.read_csv(\"sell_prices.csv\")\ndf_sell_prices.head(10)","b55b793f":"# Create features\n# Items are available after that a certain time\n\ndf_available_after = df_sell_prices.groupby([\"store_id\",\"item_id\"])[\"wm_yr_wk\"].agg([\"min\"]).reset_index()\ndf_available_after.columns = [\"store_id\", \"item_id\", \"available_after\"]\ndf_available_after.head(10)","072ac1ec":"# Join df_sales_features and df_available_after\n\ndf_sales_features = merge_by_concat(df_sales_features, df_available_after, [\"store_id\", \"item_id\"])\ndf_sales_features.head(10)","4fe7a3e1":"# We can drop those rows before available date\n# To achieve this, we need df_calendar's help\n\ndf_calendar = pd.read_csv(\"calendar.csv\")\ndf_calendar.head(10)","40412307":"# Join df_sales_features and df_calendar\n\ndf_sales_features = merge_by_concat(df_sales_features, df_calendar[[\"d\", \"wm_yr_wk\"]], [\"d\"])\ndf_sales_features.head(10)","f9a36621":"# We only need those entries after \"available_after\"\n\ndf_sales_features = df_sales_features[df_sales_features[\"wm_yr_wk\"] >= df_sales_features[\"available_after\"]]\ndf_sales_features = df_sales_features.reset_index(drop = True)\ndf_sales_features.head(10)","efafdb35":"# Memory usage control\n\nmemory_usage_string = format_memory_usage(df_sales_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\n# Technics: we know the minimum of a certain column, so we find the difference between each row and its minimum\n# and store those differences in int16\ndf_sales_features.drop([\"wm_yr_wk\"], axis = 1, inplace = True)\ndf_sales_features[\"available_after\"] = (df_sales_features[\"available_after\"] - df_sales_features[\"available_after\"].min()).astype(np.int16)\n\n# Technics: for column \"d\", we would like to store it with int16 format\ndf_sales_features[\"d\"] = df_sales_features[\"d\"].apply(lambda x: int(x[2:])).astype(np.int16)\n\nmemory_usage_string = format_memory_usage(df_sales_features.memory_usage().sum())\nprint(f\"Reduced memory usage: {memory_usage_string}\")","d5734b11":"# Sort values to easily join features later\n\ndf_sales_features.sort_values(by = [\"id\", \"d\"], inplace = True)\ndf_sales_features.reset_index(drop = True, inplace = True)","9740b6d3":"# Check dataset\n\ndf_sales_features.head(10)","4fca9a29":"# Check data type\n\ndf_sales_features.info()","a3d42cf3":"# Change to output path\n\ntry:\n    os.chdir(BASE_PATH)\n    print(f\"Change to directory: {os.getcwd()}\")\nexcept:\n    os.mkdir(BASE_PATH)\n    os.chdir(BASE_PATH)\n    print(f\"Create and change to directory: {os.getcwd()}\")","1004fa0b":"# Save pickle file\n\ndf_sales_features.to_pickle(\"sales_basic_features.pkl\")","75708d2f":"# Feature Engineering - Sales with Price and Calendar\n- Joining DataFrames in Pandas is memory-consuming, so we do the join work after creating basic features.\n- A lot of 0s before the first positive number in sales may mean that the item is only available after a certain time.\n- So, \"when the product has price in a certain store\", this means that product is available after that day.","c7e142cd":"# Feature Engineering - Sales - Basic Features\n- Our final goal is to predict sales for 28 days after day 1913,\n- so the time series of sales should be one of the most essential features in our model."}}