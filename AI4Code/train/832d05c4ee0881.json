{"cell_type":{"fa8efbb1":"code","3a802715":"code","6aae1606":"code","0f7de776":"code","34ced59f":"code","0fbe419a":"code","29aa4b8e":"code","1da16153":"code","3dc3d400":"code","e7603647":"code","5f95e93a":"code","7de40d87":"code","fad91ea4":"code","20806ee3":"code","c2c848e7":"code","0cd3e557":"code","e02cb2cf":"code","cb2682ba":"code","b8f08144":"code","429eaa10":"code","fc7fc150":"code","a19bf816":"code","8b6c6029":"code","6fa500a0":"code","2448deab":"code","dad22df0":"code","74b5277f":"code","dea273d5":"code","a0c1f74c":"code","9def3f1f":"code","9055c032":"code","2d22f38d":"code","32adcc36":"code","4caec554":"code","37563bd7":"code","f01a54c8":"code","a30e6b82":"code","adb13e83":"code","974afe25":"code","9be5c2fe":"code","b6b5900f":"code","808a10bf":"code","14b1d57e":"code","c4af4b32":"code","e2704f30":"code","9c50ccec":"code","3462b129":"code","c70e2047":"code","8777f68a":"code","fa925e66":"code","000f8ff1":"code","01b824f7":"code","66d66380":"code","67ffdf2a":"code","99c0aaa9":"code","21a3435e":"code","408aaaa3":"code","938351f7":"code","33b8c1ba":"code","cb879b34":"code","576f3a53":"code","6f687d48":"code","4981adf8":"code","f9e7579d":"code","8c13492b":"code","1331cc96":"code","0bbeb516":"code","6ffd0aa8":"code","837279eb":"code","57684502":"code","268543d9":"code","c05fc830":"code","4eebd4fa":"code","7cf610f5":"code","ab71cd3a":"code","ede44e72":"code","dab9c6c7":"code","8660eef4":"code","f0be7754":"code","68f1113d":"code","cade5593":"code","488a0d2d":"code","4b53f8d3":"code","17539808":"code","d47454c8":"code","c72b779a":"code","6e2e57ae":"code","f0e6e637":"code","6ed42596":"code","9ded3998":"code","640a9caf":"code","1878ff19":"code","f317044c":"code","8544bec6":"markdown","b2241e93":"markdown","b1174b69":"markdown","5291a692":"markdown","057c84b0":"markdown","dca5248c":"markdown","2ba43130":"markdown","79d396eb":"markdown","19d75c3c":"markdown","723a019b":"markdown","7601ef9f":"markdown","db6a3712":"markdown","13165c2b":"markdown","8a207916":"markdown","a6bad21e":"markdown","f13b789e":"markdown","edca5320":"markdown","464c5d80":"markdown","a968cdc4":"markdown","ee3e43ad":"markdown","9b7d6e4f":"markdown","bd434a9a":"markdown","0340e3f3":"markdown","356442a7":"markdown","26e63c1f":"markdown","243664d7":"markdown","9083ca46":"markdown","de550d64":"markdown","ced15fbb":"markdown","39140130":"markdown","4ad6f6ce":"markdown","1dcd246c":"markdown","bcb7ec73":"markdown","bba6b89a":"markdown","334869f1":"markdown","b0947d4c":"markdown","94489e99":"markdown","2ea1d6d4":"markdown","7b755af9":"markdown","635f5cc1":"markdown","21927aff":"markdown","2b637070":"markdown","4decbec0":"markdown","2870336b":"markdown","10771002":"markdown","eaa018b8":"markdown","66118314":"markdown","cbe9d10d":"markdown","79c2c372":"markdown","0cd0834d":"markdown","7f29c828":"markdown","47fcc616":"markdown","1f5fcb0b":"markdown","98639f29":"markdown","bb25624e":"markdown","dc520548":"markdown","457b45a1":"markdown","85b109ae":"markdown","574d0e6d":"markdown","4ea8ffa5":"markdown","e8311feb":"markdown","9e53b831":"markdown","b6ca6f7f":"markdown"},"source":{"fa8efbb1":"#Exploratory Data Analysis and Wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd","3a802715":"#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","6aae1606":"#Machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score","0f7de776":"#Read the csv file\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")","34ced59f":"#Combine train and test data\ncombine = [df_train, df_test]","0fbe419a":"#Dimensions of train\ndf_train.shape","29aa4b8e":"#Dimensions of test\ndf_test.shape","1da16153":"#Obtain Feature Names\nprint(df_train.columns.values)","3dc3d400":"#Overview of train\ndf_train.head()","e7603647":"#Overview of test\ndf_test.head()","5f95e93a":"#Description of train\ndf_train.describe()","7de40d87":"#Description of test\ndf_test.describe()","fad91ea4":"#Check for missing values in train\ndf_train.isnull().sum()","20806ee3":"#Check for missing values in test\ndf_test.isnull().sum()","c2c848e7":"#To check the datatypes of the features in train\ndf_train.info()","0cd3e557":"#To check the datatypes of the features in test\ndf_test.info()","e02cb2cf":"#Frequency table for Sex\ndf_train['Sex'].value_counts()","cb2682ba":"#Frequency table for Embarked\ndf_train['Embarked'].value_counts()","b8f08144":"#Frequency table for Ticket\ndf_train['Ticket'].value_counts()","429eaa10":"#Frequency table for Name\ndf_train['Name'].value_counts()","fc7fc150":"#Frequency table for Cabin\ndf_train['Cabin'].value_counts()","a19bf816":"df_train.describe(include=['O'])","8b6c6029":"#To determine how representative is the training dataset of the actual problem domain\ndf_train.Survived.value_counts(normalize=True)","6fa500a0":"df_train.Parch.value_counts(normalize=True)","2448deab":"df_train.SibSp.value_counts(normalize=True)","dad22df0":"#Boxplot - Age\nplt.figure(figsize = (8,3))\nsns.boxplot(x = 'Age',data = df_train,color = \"pink\")\nplt.title(\"Age of the passenger\")","74b5277f":"#Histogram (distribution analysis) - Age of the passenger\ndf_train['Age'].hist(bins = 30)\nplt.title(\"Age of the passenger\")","dea273d5":"#Boxplot - Fare of the ticket\nplt.figure(figsize = (10,5))\nsns.distplot(df_train['Age'].dropna(),kde=True,color = 'green')\nplt.title(\"Fare of the ticket\")","a0c1f74c":"#As subplots\nfig, axes = plt.subplots(2,4, figsize=(16, 10))\nsns.countplot('Survived',data=df_train,ax=axes[0,0])\nsns.countplot('Pclass',data=df_train,ax=axes[0,1])\nsns.countplot('Sex',data=df_train,ax=axes[0,2])\nsns.countplot('SibSp',data=df_train,ax=axes[0,3])\nsns.countplot('Parch',data=df_train,ax=axes[1,0])\nsns.countplot('Embarked',data=df_train,ax=axes[1,1])\nsns.distplot(df_train['Fare'], kde=True,ax=axes[1,2])\nsns.distplot(df_train['Age'].dropna(),kde=True,ax=axes[1,3])","9def3f1f":"g = sns.FacetGrid(df_train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","9055c032":"grid = sns.FacetGrid(df_train, col='Survived', row='Pclass', size=2.9, aspect=1.6) \ngrid.map(plt.hist, 'Age', alpha=.5, bins=20) \ngrid.add_legend();","2d22f38d":"grid = sns.FacetGrid(df_train, col='Pclass', hue='Survived')\n#grid = sns.FacetGrid(df_train, col='Survived', row='Pclass', size=2.9, aspect=1.6) \ngrid.map(plt.hist, 'Age', alpha=.5, bins=20) \ngrid.add_legend();","32adcc36":"grid = sns.FacetGrid(df_train, row='Embarked', size=2.2, aspect=1.6) \ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep') \ngrid.add_legend()","4caec554":"grid = sns.FacetGrid(df_train, col='Embarked') \ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep') \ngrid.add_legend()","37563bd7":"grid = sns.FacetGrid(df_train, row='Embarked', col='Survived', size=2.2, aspect=1.6) \ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None,color = 'purple') \ngrid.add_legend()","f01a54c8":"grid = sns.FacetGrid(df_train, col='Embarked', hue='Survived', palette={0: 'k', 1: 'g'})  \ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None) \ngrid.add_legend()","a30e6b82":"print(\"Before\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape)","adb13e83":"#Correcting by dropping features\ndf_train = df_train.drop(['Ticket'], axis=1)\ndf_test = df_test.drop(['Ticket'], axis=1)","974afe25":"#Combine test and train\ncombine = [df_train, df_test]","9be5c2fe":"print(\"After\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape)","b6b5900f":"#Extract new feature using regular expressions\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(df_train['Title'], df_train['Sex'])","808a10bf":"#Replace less frequent titles as 'Others'\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')\n#Replace typos or less common title with more frequently used titles\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","14b1d57e":"#To combine Parch and SibSp into a single variable \ncombine = [df_train, df_test]\nfor dataset in combine:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    #Creating a new variable called travelled_alone\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'","c4af4b32":"#Drop Parch, SibSp\ndf_train = df_train.drop(['Parch', 'SibSp'], axis=1)\ndf_test = df_test.drop(['Parch', 'SibSp'], axis=1)\ncombine = [df_train, df_test]","e2704f30":"#Fill the missing values with random numbers computed based on mean and the standard deviation of the column.\n#for dataset in combine:\n #   mean = df_train[\"Age\"].mean()\n #  std = df_train[\"Age\"].std()\n # is_null = dataset[\"Age\"].isnull().sum()\n # compute random numbers between the mean, std and is_null\n # rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n   # age_slice = dataset[\"Age\"].copy()\n   # age_slice[np.isnan(age_slice)] = rand_age\n   # dataset[\"Age\"] = age_slice\n   # dataset[\"Age\"] = df_train[\"Age\"].astype(int)","9c50ccec":"df_train['Embarked'].value_counts()","3462b129":"for dataset in combine:\n    dataset['Embarked'].fillna('S',inplace = True)","c70e2047":"#Check for missing values\ndf_train.isnull().sum()","8777f68a":"df_test.isnull().sum()","fa925e66":"df_test['Fare'].fillna(df_test['Fare'].dropna().median(), inplace=True)\ndf_test.head()","000f8ff1":"for dataset in combine:\n    dataset['Cabin_ID'] = dataset['Cabin'].str[0]","01b824f7":"for dataset in combine:\n    dataset['Cabin_ID'].fillna('O',inplace = True)","66d66380":"df_train['Cabin_ID'].value_counts()","67ffdf2a":"#Correcting by dropping features\ndf_train = df_train.drop(['Cabin'], axis=1)\ndf_test = df_test.drop(['Cabin'], axis=1)","99c0aaa9":"df_train.dtypes","21a3435e":"#Convert the categorical titles to ordinal\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Others\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","408aaaa3":"df_train.head()","938351f7":"#Drop the Name feature from train and test\n#Drop PassengerId feature from train\ndf_train = df_train.drop(['Name', 'PassengerId'], axis=1)\ndf_test = df_test.drop(['Name'], axis=1)\ncombine = [df_train, df_test]\ndf_train.shape, df_test.shape","33b8c1ba":"#Import Library\nfrom sklearn.preprocessing import LabelEncoder","cb879b34":"#LABEL ENCODING\nle = LabelEncoder()","576f3a53":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map({'female': 1, 'male': 0}).astype(int)","6f687d48":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ) ","4981adf8":"#Convert to numericals\nfor dataset in combine:\n    dataset['travelled_alone'] = le.fit_transform(dataset['travelled_alone'].astype(str))\n\n### No - 0, Yes - 1 ","f9e7579d":"df_train.head()","8c13492b":"#Convert fare from float to numerical\n#df_train['Fare'] = df_train['Fare'].astype(int)\n#df_test['Fare'] = df_test['Fare'].astype(int)","1331cc96":"for dataset in combine:\n    dataset['Cabin_ID'] = le.fit_transform(dataset['Cabin_ID'].astype(str))","0bbeb516":"df_train.dtypes","6ffd0aa8":"for dataset in combine:\n    dataset =pd.get_dummies(dataset,columns=['Cabin_ID'])","837279eb":"guess_ages = np.zeros((2,3)) \nguess_ages ","57684502":"#CLEANING\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            age_mean = guess_df.mean()\n            #age_std = guess_df.std()\n            age_median = guess_df.median()\n            age_guess = rnd.uniform(age_median - age_mean, age_median + age_mean)\n\n            \n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","268543d9":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16.136, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16.136) & (dataset['Age'] <= 32.102), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32.102) & (dataset['Age'] <= 48.068), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48.068) & (dataset['Age'] <= 64.034), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64.034, 'Age']","c05fc830":"df_train.head()","4eebd4fa":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ndf_train.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","7cf610f5":"df_train.isnull().sum()","ab71cd3a":"df_test.isnull().sum()","ede44e72":"X_train = df_train.drop(\"Survived\", axis=1)\nY_train = df_train[\"Survived\"]\nX_test  = df_test.drop(\"PassengerId\", axis=1).copy()","dab9c6c7":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_log)","8660eef4":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_knn)","f0be7754":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test)  \n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_knn)","68f1113d":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_svc)","cade5593":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_decision_tree)","488a0d2d":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_random_forest)","4b53f8d3":"results = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN','Naive Bayes','Support Vector Machines','Decision Tree','Random Forest'],\n    'Score': [acc_log, acc_knn, acc_gaussian, acc_svc, acc_decision_tree, acc_random_forest]})\ndf_result = results.sort_values(by='Score', ascending=False)\n#Display\ndf_result","17539808":"rf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","d47454c8":"importances = pd.DataFrame({'Feature':X_train.columns,'Importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('Importance',ascending=False)\nimportances","c72b779a":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(\"Score = \",acc_random_forest)","6e2e57ae":"#Create param grid object \nforest_params = dict(     \n    max_depth = [n for n in range(9, 14)],     \n    min_samples_split = [n for n in range(4, 11)], \n    min_samples_leaf = [n for n in range(2, 5)],     \n    n_estimators = [n for n in range(10, 60, 10)],\n)","f0e6e637":"#Instantiate Random Forest model\nforest = RandomForestClassifier()","6ed42596":"#Build and fit model \nforest_cv = GridSearchCV(estimator=forest, param_grid=forest_params, cv=5) \nforest_cv.fit(X_train, Y_train)","9ded3998":"print(\"Best score: {}\".format(forest_cv.best_score_))\nprint(\"Optimal params: {}\".format(forest_cv.best_estimator_))","640a9caf":"Y_pred = forest_cv.predict(X_test)","1878ff19":"predictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","f317044c":"submission = pd.DataFrame({\"PassengerId\": df_test[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('..\/output\/submission.csv', index=False)","8544bec6":"## 3.3 Cleaning the data - Age, Embarked and Fare","b2241e93":"#### (b) EMBARKED","b1174b69":"Infants had a high survival rate.\n \nOldest passengers (Age = 80) survived.\n \nLarge number of 15-25 year olds did not survive.\n\nMost passengers are in 15-35 age range.","5291a692":"## 3.2 Creating new features","057c84b0":"## 4.2 K Nearest Neighbour","dca5248c":"#### (d) FARE","2ba43130":"One missing value in the Fare feature in the test dataset","79d396eb":"Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n\nCabin > Age are incomplete in case of test dataset.","19d75c3c":"# 2.2 Bivariate Analysis","723a019b":"#### (c) EMBARKED","7601ef9f":"Seven features are integer or \ufb02oats. Six in case of test dataset. \n\nFive features are strings (object).","db6a3712":"## 5.2 K-Fold Cross Validation","13165c2b":"Higher fare paying passengers had better survival.  \n\nPort of embarkation correlates with survival rates.  ","8a207916":"# 3. DATA WRANGLING","a6bad21e":"#### (d) CABIN_ID","f13b789e":"More number of passengers travel in the 3rd class.\n\nMost passengers are travelling without parents or children.\n\nMost passengers are travelling without siblings or spouse.\n\nMajority of the passengers with the port of embarkation as Southampton.","edca5320":"## 5.1 Best Model\n","464c5d80":"# 2.1 Univariate Analysis","a968cdc4":"Ticket is a mix of numeric and alphanumeric data types. \n\nCabin is alphanumeric.\n\nName feature may contain typos.","ee3e43ad":"No more missing values in train dataset","9b7d6e4f":"## 3.6 Creating a Variable with Age and Pclass","bd434a9a":"## 4.1 Logistic Regression","0340e3f3":"Pclass=3 had most passengers, however most did not survive. \n\nInfant passengers in Pclass=2 and Pclass=3 mostly survived.  \n\nMost passengers in Pclass=1 survived. \n\nPclass varies in terms of Age distribution of passengers.","356442a7":"## 5.5 Precision and Recall","26e63c1f":"## 3.1 Correcting - by dropping unnecessary features","243664d7":"# INITIAL STEP - Import Libraries and Acquire Data","9083ca46":"# 5. MODEL ACCURACY AND SCORES","de550d64":"We will perform K-fold cross validation with 10 folds to output an array of 10 results.","ced15fbb":"#### (e) CABIN_ID","39140130":"#### (a) AGE - done later (below)","4ad6f6ce":"Now, we replace Age feature with ordinals using bands","1dcd246c":"## 5.3 Feature Importance","bcb7ec73":"## 4.6 Random Forest","bba6b89a":"# 4. PREDICTIVE MODELLING","334869f1":"Female passengers had much better survival rate than males. Exception in Embarked=C where males had higher survival rate. ","b0947d4c":"## 4.5 Decision Tree","94489e99":"Most common level is S - Southampton","2ea1d6d4":"#### (c) TRAVELLED_ALONE:","7b755af9":"#### (a) TITLE:","635f5cc1":"# 6. To create a CSV file to Upload","21927aff":"#### TRAIN THE MODEL AGAIN:","2b637070":"Travelled_alone isn't of much importance, so we can drop that feature and train the model again.","4decbec0":"## 4.3 Gaussian Naive Bayes","2870336b":"Categorical: Survived, Sex, and Embarked. \n\nOrdinal: Pclass\n\nContinous: Age, Fare. \n\nDiscrete: SibSp, Parch","10771002":"## 5.4 Hyperparameter Tuning","eaa018b8":"Around 38% samples survived representative of the actual survival rate at 32%.\n\nMost passengers (> 75%) did not travel with parents or children.\n\nNearly 30% of the passengers had siblings and\/or spouse aboard.\n\nThe fare varied from 0 to as high as 512 (from df_train.describe())","66118314":"# 1. DATA EXPLORATION","cbe9d10d":"## 3.5 Cleaning and Creating bands for age","79c2c372":"Decision Tree and Random Forest have the same score. But, we choose Random Forest because it corrects Decision Tree's \nhabit of overfitting to its training set.","0cd0834d":"This says that our model has an accuracy of around 80% and can differ +\/- 4%.\n\nLet us now try to increase our model's accuracy even more.","7f29c828":"#### (c) FARE","47fcc616":"Reference:- https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8","1f5fcb0b":"Most of the people in titanic areaged between 20 to 40.\n\nFare of ship lying below 100 but there are outliers that go as far as 500","98639f29":"# 2. DATA VISUALIZATION","bb25624e":"Note:- Return and run Title mapping.","dc520548":"## 3.4 Converting datatypes to numericals","457b45a1":"Reference:- https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9","85b109ae":"#### (b) SEX:","574d0e6d":"#### INFERENCE:","4ea8ffa5":"## 4.4 Support Vector Machines","e8311feb":"Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n\nSurvival among Title Age bands varies slightly.\n\nCertain titles mostly survived (Mme, Lady, Sir) or did not survive (Don, Rev, Jonkheer).\n\nHence, we will retain the new Title feature for model training.","9e53b831":"Same score, so our model predicts as good as before.","b6ca6f7f":"Sex variable has two possible values - male and female. (65% male since freq=577\/count=891 and 35%)\n\nEmbarked takes three possible values. S port is used by most passengers.\n\nTicket feature has high ratio (22%) of duplicate values (unique=681).\n\nNames are unique across the dataset (count=unique=891)  \n\nCabin values have several dupicates across samples. Alternatively several passengers shared a cabin.    "}}