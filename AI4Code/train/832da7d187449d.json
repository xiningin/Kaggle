{"cell_type":{"7e1102aa":"code","6235b74d":"code","46b3564f":"code","6574a793":"code","e36f4e8f":"code","031ec516":"code","d31c49ce":"code","2335d4ee":"code","d1b89d05":"code","8d445efa":"code","cdbc9aa0":"code","aba5dc0d":"code","c6066aba":"code","03f3f370":"code","0ffe6c47":"code","18f02e45":"code","b242d8e7":"code","bc79eb3f":"code","3a094547":"code","53b3eef7":"code","8657f3fa":"code","6e926786":"code","45832a16":"code","6fb9579b":"code","0220b87c":"code","e98c0b68":"code","efb04b7e":"code","9d123584":"code","2ae8c219":"code","6ccc19bc":"code","b7380850":"code","dbc8c5e0":"code","ffb32f26":"code","836a8133":"code","1017e649":"code","11d83f27":"code","be0b9455":"code","67255b40":"code","05a0eaf5":"code","1f825d2b":"code","1e08ac01":"code","c8608957":"code","d080e1a4":"code","1bf77a14":"code","d5247dc8":"code","5222f17e":"code","1dc9761f":"code","9f494998":"code","ee560e93":"code","2b822301":"code","2972506b":"code","816c8d31":"code","7da33a02":"code","0e1ec59c":"code","aea3ff95":"code","4595ab64":"code","dcb8aa5d":"code","e766156e":"code","a459d778":"code","fd1fecdb":"code","0b470cc3":"code","1ad7d08d":"code","e9ee6243":"code","28d86c6c":"code","d4ed4821":"code","87a994e4":"code","16c1395c":"code","4c84015f":"code","2b7e7211":"code","a2c96f97":"code","ce1d46c0":"code","90bfa584":"code","b1be5cfe":"code","eab82fb4":"code","7f0c64a5":"code","5d47a5f5":"code","8204feaa":"code","d556286b":"code","d35c77b9":"code","a0ee73e3":"code","0c486474":"code","576a619a":"code","744d85f9":"code","9a1d631a":"code","3e77c7fa":"code","565dd503":"code","4f5a5995":"code","67171ecb":"code","69e73e7b":"code","9b9e0c25":"code","50b353ab":"code","8eaf2d45":"code","249c0612":"code","78c65fdd":"code","5a4059ae":"code","0dda6da2":"code","1d745e9e":"code","aad97333":"code","defd6f38":"code","dad1b3f6":"code","139ae9cf":"code","5776fd8c":"code","5cd70767":"code","94e0791e":"code","b7b1c912":"code","202b86a6":"code","4f686cae":"code","b2b29558":"code","949fcd86":"code","e998e11c":"code","069cb53e":"code","5b1475d3":"code","018670a5":"code","d9b693e0":"code","e40a8bb4":"code","5c9d76b1":"code","33027b23":"code","4cf40706":"code","a1342dd4":"code","0f9980a8":"code","2d3d0a66":"code","392e468a":"code","3e75d50b":"code","482699a0":"code","b94fc3f0":"code","a80e7398":"code","33030636":"code","2b5aa25b":"code","7ee4e5c7":"code","2b607ee7":"code","8edcc83e":"code","74496760":"code","95a178df":"code","ce8ed63a":"code","7c396e9f":"markdown","b5e357f9":"markdown","2193b581":"markdown","c867e152":"markdown","e61e1116":"markdown","fd439dd6":"markdown","c8aa7184":"markdown","6908c4d7":"markdown","4be4a6e4":"markdown","034a6789":"markdown","ccac8b31":"markdown","2fffb940":"markdown","98f16b15":"markdown","7d5e6428":"markdown","71af496b":"markdown","a3119c80":"markdown","e6a2e475":"markdown","ac2604fd":"markdown","fd4f282e":"markdown","9fe46b69":"markdown","ea17bdb3":"markdown","01073473":"markdown","cb0dd064":"markdown","7f61c3fc":"markdown","199b85a5":"markdown","2f9c9dad":"markdown","50731f70":"markdown","a495d19c":"markdown","b8b7515e":"markdown","4bc9a07c":"markdown","cc9ff602":"markdown","efe52b0c":"markdown","86cd98ce":"markdown","bc964de0":"markdown","df8e2e3b":"markdown","e74a1023":"markdown","1504eeeb":"markdown","80f61e57":"markdown","29bd4ff7":"markdown","117013f5":"markdown","7f3ac345":"markdown","d913d3e5":"markdown","c0e38d6f":"markdown","50dd4cce":"markdown","8c335197":"markdown","80cfcc62":"markdown","e9b5fd71":"markdown","2a120275":"markdown","b48a3124":"markdown","0732ebbe":"markdown","95422f5a":"markdown","80019905":"markdown","a76d1bb0":"markdown","16155df6":"markdown","e1450921":"markdown","67c52060":"markdown","76b76338":"markdown","549faae5":"markdown","a611aa79":"markdown","7a96af3b":"markdown","10750606":"markdown","93ca30c2":"markdown","8bf76dca":"markdown","c74fffbe":"markdown","bc5787ad":"markdown","82cb84c9":"markdown","e20b19ef":"markdown","d9bfe584":"markdown","8ca00a68":"markdown","6d822764":"markdown","5a72245b":"markdown","b85658d1":"markdown","1ff744a0":"markdown","cb485d91":"markdown","eda77f73":"markdown","ef077492":"markdown","c1f25d93":"markdown","3bdad864":"markdown","4317c0d2":"markdown","53d73bea":"markdown","b0eaeb27":"markdown","f834c059":"markdown","78288b45":"markdown","eda9d369":"markdown","ad08e03a":"markdown","8712f5cd":"markdown","d20c0e91":"markdown","8e7e1ad5":"markdown","fb2b9cbb":"markdown","f39776d8":"markdown","00baa727":"markdown","2fa62e33":"markdown","f4fed250":"markdown","2c7314f3":"markdown","f41da1dd":"markdown","33a25eca":"markdown","5bc5fc59":"markdown","8377613c":"markdown","7ca4a665":"markdown","b0a95259":"markdown","e8722302":"markdown","92d885f4":"markdown","4ef5c43c":"markdown","071f7bcd":"markdown","2e8fd9a6":"markdown","81b3e0de":"markdown","7eb15d79":"markdown","d9a2041a":"markdown","2b1b9ded":"markdown","4757b763":"markdown","a59a1d7b":"markdown","fe62721e":"markdown","01b57c7d":"markdown","6fcd0d44":"markdown","b2f9bfa8":"markdown","0d7d8eb1":"markdown","cd30f143":"markdown","4de3becd":"markdown","966d44bc":"markdown","c5c5ef10":"markdown","90406a8a":"markdown","25ec893a":"markdown","a61f38b0":"markdown","417e4341":"markdown","2fd6e6e8":"markdown","1957a69f":"markdown","6d2fc43f":"markdown","096c91e6":"markdown","eac1e989":"markdown","70dbddab":"markdown","751e0c34":"markdown","4f4e6439":"markdown","e6bd0ade":"markdown","05dd5fe1":"markdown","3c4e1671":"markdown","7a4e16ce":"markdown","48cc5618":"markdown","2bcf371f":"markdown","59cdb492":"markdown","4c8e58e7":"markdown","9a46ff01":"markdown","4850f14b":"markdown","54177a52":"markdown","0f536595":"markdown","78ab906d":"markdown","d2830cd5":"markdown","9ac07ca2":"markdown","205cdb4a":"markdown","b351e86b":"markdown","8923568c":"markdown","35805000":"markdown","1c39a682":"markdown","612c1528":"markdown","2ae481f1":"markdown","43bf25ec":"markdown","ff3af83e":"markdown","2291d13c":"markdown","dc583868":"markdown","60d5cf37":"markdown","47240276":"markdown","bb25805e":"markdown","7eae3804":"markdown","c4b07071":"markdown","fc1dd650":"markdown","9125016e":"markdown","02feb01c":"markdown","bc47db5c":"markdown","205d9941":"markdown","9bd2328b":"markdown","46936f54":"markdown","c767764b":"markdown","f1680189":"markdown","7fa588e4":"markdown","43b8c3f5":"markdown","946c47cd":"markdown","64269da1":"markdown","f53e9a2d":"markdown","920c0b7e":"markdown","3099e36c":"markdown","80200b15":"markdown","7a933852":"markdown","c0298a2b":"markdown","e4fe734d":"markdown","f96c8cb2":"markdown","0ed7c1e6":"markdown","8c785a4b":"markdown","012d70a7":"markdown","bfa3621c":"markdown","ff8e40bd":"markdown","5962063e":"markdown","c9b85c22":"markdown","a8b47a8b":"markdown","31773dcb":"markdown","ca6111d4":"markdown","d4d2a99a":"markdown","0e969956":"markdown","4353524b":"markdown","4397acc8":"markdown","4be42736":"markdown","2fa10591":"markdown"},"source":{"7e1102aa":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns  \n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Third party\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import KNNImputer\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn import FunctionSampler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\n\n# Local application\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","6235b74d":"random_state = 27912","46b3564f":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\nindex_col = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index_col, target)","6574a793":"data.sample(5,random_state=random_state)","e36f4e8f":"(X, y) = utils.divide_dataset(data, target)","031ec516":"X.sample(5,random_state=random_state)","d31c49ce":"y.sample(5,random_state=random_state)","2335d4ee":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","d1b89d05":"X_train.sample(5, random_state=random_state)","8d445efa":"X_test.sample(5,random_state=random_state)","cdbc9aa0":"delete_colum = ['SkinThickness','Insulin']","aba5dc0d":"imputer_col = ['Glucose','BloodPressure','BMI','DiabetesPedigreeFunction','Age']","c6066aba":"knnimputer = KNNImputer(n_neighbors=5, weights=\"uniform\",missing_values=0)","03f3f370":"column_transformer = make_column_transformer((\"drop\", delete_colum),(knnimputer, imputer_col), remainder=\"passthrough\")","0ffe6c47":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\", encode=\"onehot-dense\")","18f02e45":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","b242d8e7":"X = X_train\ny = y_train","bc79eb3f":"# ****Los comentarios en codigo no tendran tildes o caracteres especiales para ahorrarnos errores. *****\n\nn_bins_discretizer = [1, 2, 3]           # Numero de particiones\nn_imputer = [5, 10]                      # Numero de vecinos\nknn_weights = [\"uniform\", \"distance\"]    # Peso de los vecinos","3a094547":"n_neighbors = 10 # Por defecto, el optimizador encontrara el numero de vecinos que de m\u00e1s score.\n\nk_neighbors_model = KNeighborsClassifier(n_neighbors)","53b3eef7":"estimator = k_neighbors_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","8657f3fa":"weights = [\"uniform\", \"distance\"]\nn_neighbors = [5, 6, 7, 8, 9]\n\nk_neighbors_clf = utils.optimize_params(pip,\n                                        X, y, cv,\n                                        estimator__weights=weights,\n                                        estimator__n_neighbors=n_neighbors,\n                                        ct__knnimputer__n_neighbors = n_imputer,\n                                        ct__knnimputer__weights = knn_weights,\n                                        discretizer__n_bins = n_bins_discretizer\n                                        )","6e926786":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)","45832a16":"estimator = clone(decision_tree_model)\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","6fb9579b":"criterion = [\"gini\", \"entropy\"]\nmax_depth = [0, 1, 2, 3, 4]\nccp_alpha = [0.0, 0.1, 0.2]\n\ndecision_tree_clf = utils.optimize_params(pip,\n                                          X, y, cv,\n                                          estimator__criterion=criterion,\n                                          estimator__max_depth=max_depth,\n                                          estimator__ccp_alpha=ccp_alpha,\n                                          ct__knnimputer__n_neighbors = n_imputer,\n                                          ct__knnimputer__weights = knn_weights,\n                                          discretizer__n_bins = n_bins_discretizer\n                                         )","0220b87c":"adaboost_model = AdaBoostClassifier(random_state=random_state)","e98c0b68":"estimator = adaboost_model\n\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","efb04b7e":"# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pip,\n                                     X, y, cv,\n                                     estimator__base_estimator=base_estimator,\n                                     estimator__learning_rate=learning_rate,\n                                     estimator__base_estimator__criterion=criterion,\n                                     estimator__base_estimator__max_depth=max_depth,\n                                     estimator__base_estimator__ccp_alpha=ccp_alpha,\n                                     ct__knnimputer__n_neighbors = n_imputer,\n                                     ct__knnimputer__weights = knn_weights,\n                                     discretizer__n_bins = n_bins_discretizer\n                                     )","9d123584":"bagging_model = BaggingClassifier(random_state=random_state)","2ae8c219":"estimator = bagging_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","6ccc19bc":"# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1,4,10]\n\nbagging_clf = utils.optimize_params(pip,\n                                    X, y, cv,\n                                    estimator__base_estimator=base_estimator,\n                                    estimator__base_estimator__criterion=criterion,\n                                    estimator__base_estimator__max_depth=max_depth,\n                                    ct__knnimputer__n_neighbors = n_imputer,\n                                    ct__knnimputer__weights = knn_weights,\n                                    discretizer__n_bins = n_bins_discretizer\n                                    )","b7380850":"random_forest_model = RandomForestClassifier(random_state=random_state)","dbc8c5e0":"estimator = random_forest_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","ffb32f26":"criterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(pip,\n                                          X, y, cv,\n                                          estimator__criterion=criterion,\n                                          estimator__max_features=max_features,\n                                          ct__knnimputer__n_neighbors = n_imputer,\n                                          ct__knnimputer__weights = knn_weights,\n                                          discretizer__n_bins = n_bins_discretizer\n                                          )","836a8133":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)","1017e649":"estimator = gradient_boosting_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","11d83f27":"learning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(pip,\n                                              X, y, cv,\n                                              estimator__learning_rate=learning_rate,\n                                              estimator__criterion=criterion,\n                                              estimator__max_depth=max_depth,\n                                              estimator__ccp_alpha=ccp_alpha,\n                                              ct__knnimputer__n_neighbors = n_imputer,\n                                              ct__knnimputer__weights = knn_weights,\n                                              discretizer__n_bins = n_bins_discretizer\n                                              )","be0b9455":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)","67255b40":"estimator = hist_gradient_boosting_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","05a0eaf5":"learning_rate = [0.01, 0.05, 0.1]\nmax_leaf_nodes = [5,10,15, 31]\n\nhist_gradient_boosting_clf = utils.optimize_params(pip,\n                                                   X, y, cv,\n                                                   estimator__learning_rate=learning_rate,\n                                                   estimator__max_leaf_nodes=max_leaf_nodes,\n                                                   ct__knnimputer__n_neighbors = n_imputer,\n                                                   ct__knnimputer__weights = knn_weights,\n                                                   discretizer__n_bins = n_bins_discretizer\n                                                   )","1f825d2b":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","1e08ac01":"X = X_test\ny = y_test\n\nutils.evaluate_estimators(estimators, X, y)","c8608957":"filepath = \"..\/input\/wisconsin\/wisconsin.csv\"\n\nindex_col = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index_col, target)","d080e1a4":"data.sample(5, random_state=random_state)","1bf77a14":"(X, y) = utils.divide_dataset(data, target)","d5247dc8":"X.sample(5, random_state=random_state)","5222f17e":"y.sample(5, random_state=random_state)","1dc9761f":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","9f494998":"X_train.sample(5, random_state=random_state)","ee560e93":"y_train.sample(5, random_state=random_state)","2b822301":"X_test.sample(5, random_state=random_state)","2972506b":"y_test.sample(5, random_state=random_state)","816c8d31":"column_rm_mean = [\"smoothness_mean\", \"symmetry_mean\", \"fractal_dimension_mean\"]\ncolumn_rm_se = [\"texture_se\", \"smoothness_se\", \"symmetry_se\", \"fractal_dimension_se\"]\ncolumn_rm_worst = [\"symmetry_worst\", \"fractal_dimension_worst\"]\ncolumn_error = [\"Unnamed: 32\"]\ncolumn_rm = column_rm_mean + column_rm_se + column_rm_worst + column_error\n\ndrop_col = make_column_transformer((\"drop\", column_rm), remainder=\"passthrough\")","7da33a02":"outlier_rm = FunctionSampler(func=utils.outlier_rejection, kw_args={'seed':random_state})","0e1ec59c":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\", encode=\"onehot-dense\")","aea3ff95":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","4595ab64":"X = X_train\ny = y_train","dcb8aa5d":"n_bins = [2, 3]","e766156e":"k_neighbors_model = KNeighborsClassifier()\n\npipe_k_neighbors = make_pipeline(drop_col, outlier_rm, discretizer, k_neighbors_model)\npipe_k_neighbors.get_params().keys()","a459d778":"weights = [\"uniform\", \"distance\"]\nn_neighbors = [1, 2, 3, 4, 5, 6]\n\nk_neighbors_clf = utils.optimize_params(pipe_k_neighbors,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kbinsdiscretizer__n_bins=n_bins)","fd1fecdb":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)\n\npipe_tree = make_pipeline(drop_col, outlier_rm, discretizer, decision_tree_model)\npipe_tree.get_params().keys()","0b470cc3":"# Should not modify the original model\nestimator = clone(decision_tree_model)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [3, 4, 5]\nccp_alpha = [0.0, 0.1]\n\ndecision_tree_clf = utils.optimize_params(pipe_tree,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          kbinsdiscretizer__n_bins=n_bins)","1ad7d08d":"adaboost_model = AdaBoostClassifier(random_state=random_state)\n\npipe_adaboost = make_pipeline(drop_col, outlier_rm, discretizer, adaboost_model)\npipe_adaboost.get_params().keys()","e9ee6243":"estimator = adaboost_model\n\n# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pipe_adaboost,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha,\n                                     kbinsdiscretizer__n_bins=n_bins)","28d86c6c":"bagging_model = BaggingClassifier(random_state=random_state)\n\npipe_bagging = make_pipeline(drop_col, outlier_rm, discretizer, bagging_model)\npipe_bagging.get_params().keys()","d4ed4821":"estimator = bagging_model\n\n# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nn_estimators = [10, 20, 30]\n\nbagging_clf = utils.optimize_params(pipe_bagging,\n                                    X, y, cv,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    kbinsdiscretizer__n_bins=n_bins)","87a994e4":"random_forest_model = RandomForestClassifier(random_state=random_state)\n\npipe_rd_forest = make_pipeline(drop_col, outlier_rm, discretizer, random_forest_model)\npipe_rd_forest.get_params().keys()","16c1395c":"estimator = random_forest_model\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nn_estimators = [50, 100]\n\nrandom_forest_clf = utils.optimize_params(pipe_rd_forest,\n                                          X, y, cv,\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          kbinsdiscretizer__n_bins=n_bins)","4c84015f":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)\n\npipe_grad_boosting = make_pipeline(drop_col, outlier_rm, discretizer, gradient_boosting_model)\npipe_grad_boosting.get_params().keys()","2b7e7211":"estimator = gradient_boosting_model\n\nlearning_rate = [0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2]\n\ngradient_boosting_clf = utils.optimize_params(pipe_grad_boosting,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              kbinsdiscretizer__n_bins=n_bins)","a2c96f97":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)\n\npipe_hist_grad_boosting = make_pipeline(drop_col, outlier_rm, discretizer, hist_gradient_boosting_model)\npipe_hist_grad_boosting.get_params().keys()","ce1d46c0":"estimator = hist_gradient_boosting_model\n\nlearning_rate = [0.03, 0.1]\nmax_leaf_nodes = [15, 31, 65]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipe_hist_grad_boosting,\n                                                   X, y, cv,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes,\n                                                   kbinsdiscretizer__n_bins=n_bins)","90bfa584":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","b1be5cfe":"X = X_test\ny = y_test\n\nutils.evaluate_estimators(estimators, X, y)","eab82fb4":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_data.columns","7f0c64a5":"train_data.head()","5d47a5f5":"train_data.info()","8204feaa":"train_data.describe()","d556286b":"def outlier_detect(feature, data):\n    outlier_index = []\n\n    for each in feature:\n        Q1 = np.percentile(data[each], 25)\n        Q3 = np.percentile(data[each], 75)\n        IQR = Q3 - Q1\n        min_quartile = Q1 - 1.5*IQR\n        max_quartile = Q3 + 1.5*IQR\n        outlier_list = data[(data[each] < min_quartile) | (data[each] > max_quartile)].index\n        outlier_index.extend(outlier_list)\n        \n    outlier_index = Counter(outlier_index)\n    #If there are three or more outlier data features we must delete them. (n)\n    outlier_data = list(i for i, n in outlier_index.items() if n > 3)\n    return outlier_data","d35c77b9":"outlier_data = outlier_detect([\"Age\",\"SibSp\",\"Parch\",\"Fare\"], train_data)\ntrain_data.loc[outlier_data]","a0ee73e3":"train_data = train_data.drop(outlier_data, axis=0).reset_index(drop=True)","0c486474":"data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)","576a619a":"sns.countplot('Survived',data=train_data)","744d85f9":"#data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean()\ntrain_data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean()","9a1d631a":"sns.factorplot(x=\"Sex\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","3e77c7fa":"sns.factorplot(x=\"Pclass\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","565dd503":"sns.factorplot(x=\"Embarked\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","4f5a5995":"sns.factorplot(x=\"SibSp\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","67171ecb":"sns.factorplot(x=\"Parch\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","69e73e7b":"g = sns.FacetGrid(train_data, row=\"Survived\")\ng.map(sns.distplot, \"Age\", bins=25)\nplt.show()","9b9e0c25":"g = sns.FacetGrid(train_data, row=\"Survived\")\ng.map(sns.distplot, \"Fare\", bins=25)\nplt.show()","50b353ab":"train_data['Sex'].replace(['male','female'],[0,1],inplace=True) # Se sustituyen las categoias de la variable por 0 y 1\ntrain_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n# male: 0, famela: 1\nsns.heatmap(train_data[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",\"Fare\",\"Embarked\", \"Survived\"]].corr(), annot = True)\nplt.show()","8eaf2d45":"train_data.columns[train_data.isnull().any()]","249c0612":"train_data.isnull().sum()","78c65fdd":"train_data[train_data[\"Age\"].isnull()]","5a4059ae":"data_age_nan_index = train_data[train_data[\"Age\"].isnull()].index\nfor i in data_age_nan_index:\n    mean_age = train_data[\"Age\"][(train_data[\"Pclass\"]==train_data.iloc[i][\"Pclass\"])].median()\n    train_data[\"Age\"].iloc[i] = mean_age","0dda6da2":"train_data[train_data[\"Age\"].isnull()]","1d745e9e":"train_data[train_data[\"Embarked\"].isnull()]","aad97333":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(1)","defd6f38":"train_data[train_data[\"Embarked\"].isnull()]","dad1b3f6":"train_clone = train_data.copy()","139ae9cf":"train_data","5776fd8c":"def suma_columnas(data, column1, column2, result):\n    data[result] = data[column1] + data[column2]\n    return data","5cd70767":"train_clone[\"Family\"] = train_clone[\"SibSp\"] + train_clone[\"Parch\"]\n#test_data[\"Family\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\nsns.factorplot(x=\"Family\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\nplt.show()","94e0791e":"def alone_column(data, nombre, family):\n    data[nombre] = [1 if i == 0 else 0 for i in data[family]]\n    return data","b7b1c912":"train_clone[\"Alone\"] = [1 if i == 0 else 0 for i in train_clone[\"Family\"]]\n#test_data[\"Alone\"] = [1 if i == 0 else 0 for i in test_data[\"Family\"]]\n#train_data[\"Family\"].replace([0,1,2,3,4,5,6,7,10], [0,1,1,1,0,2,0,2,2], inplace=True)\ntrain_clone.head()","202b86a6":"def name_title(data, nombre):\n    data[nombre] = data.Name.str.extract('([A-Za-z]+)\\.')\n    data[nombre].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)\n    # Se convierten los titulos en identificadores numericos\n    data[nombre].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)\n    return data","4f686cae":"train_clone['Title']=train_clone.Name.str.extract('([A-Za-z]+)\\.')\n#test_data['Title']=test_data.Name.str.extract('([A-Za-z]+)\\.')","b2b29558":"#sns.countplot(train_data[\"Title\"])\n#plt.xticks(rotation = 90)\n#plt.show()","949fcd86":"train_clone['Title'].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)\n#test_data['Title'].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)","e998e11c":"sns.countplot(train_clone[\"Title\"])\nplt.show()","069cb53e":"sns.factorplot(x=\"Title\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\nplt.show()","5b1475d3":"train_clone[\"Title\"].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)\n#test_data[\"Title\"].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)","018670a5":"\"\"\"\ndef age_limit(data):\n    data['Age_Limit'] = pd.cut(data['Age'], 5)\n    data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')\n    return data\n\"\"\"","d9b693e0":"#train_data['Age_Limit']=pd.cut(train_data['Age'], 5)\n#test_data['Age_Limit']=pd.cut(test_data['Age'], 5)\n\n#train_data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')\n#test_data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","e40a8bb4":"#train_data['Age_Limit'] = LabelEncoder().fit_transform(train_data['Age_Limit'])\n#test_data['Age_Limit'] = LabelEncoder().fit_transform(test_data['Age_Limit'])","5c9d76b1":"#train_clone['Fare_Limit']=pd.qcut(train_clone['Fare'],4)\n#train_clone.groupby(['Fare_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","33027b23":"#train_clone['Fare_Limit'] = LabelEncoder().fit_transform(train_clone['Fare_Limit'])\n#test_data['Age_Limit'] = LabelEncoder().fit_transform(test_data['Age_Limit'])","4cf40706":"#sns.factorplot(x=\"Fare_Limit\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\n#plt.show()","a1342dd4":"sns.heatmap(train_clone[[\"Ticket\",\"Cabin\",\"Pclass\",\"Embarked\",\"Sex\",\"Age\", \"Title\",\"Family\", \"Survived\"]].corr(), annot = True)\nplt.show()","0f9980a8":"train_data['Age']=train_data['Age'].astype(int)\n\ncolumns_drop = [\"SibSp\",\"Parch\",\"Cabin\",\"Fare\", \"Ticket\", \"Name\", \"PassengerId\"]\n\n#train_data.drop(labels=columns_drop, axis=1, inplace = True)\n#test_data.drop(labels=columns_drop, axis=1, inplace = True)\n\n#train_data.head()","2d3d0a66":"#train_data = pd.get_dummies(data,columns=[\"Pclass\"])\n#train_data = pd.get_dummies(data,columns=[\"Embarked\"])\n#train_data = pd.get_dummies(data,columns=[\"Family\"])\n#train_data = pd.get_dummies(data,columns=[\"Title\"])\n\n#train_data.head()","392e468a":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","3e75d50b":"#if len(data) == (len(train_data) + len(test_data)):\n#    print(\"success\")","482699a0":"#test = data[len(train_data):]\n#test.drop(labels=\"Survived\", axis=1, inplace=True)","b94fc3f0":"#train = data[:len(train_data)]\n#X_train = train.drop(labels = \"Survived\", axis=1)\n#y_train = train[\"Survived\"]\n#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n","a80e7398":"target = \"Survived\"\n(X_train,y_train) = (train_data.drop(target, axis=1), train_data[target])\n\n#(X_test,y_test) = (test_data.drop(target, axis=1), test_data[target])\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)","33030636":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\n\nsum_col = FunctionTransformer(suma_columnas, kw_args={'column1':\"SibSp\", 'column2':'Parch', 'result':'Family'})\nalone_col = FunctionTransformer(alone_column, kw_args={'nombre':\"Alone\", 'family':'Family'})\nname = FunctionTransformer(name_title, kw_args={'nombre':\"Title\"})\ndrop_col = make_column_transformer((\"drop\", columns_drop), remainder=\"passthrough\")","2b5aa25b":"log_reg = LogisticRegression(random_state=42)\n\np = make_pipeline(sum_col, alone_col, name, drop_col, log_reg)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","7ee4e5c7":"rf_reg = RandomForestClassifier(random_state=42)\n#rf_reg.fit(X_train, y_train)\n#print(\"Accuracy: \", rf_reg.score(X_test,y_test))\n\np = make_pipeline(sum_col, alone_col, name, drop_col, rf_reg)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","2b607ee7":"svm_clsf = SVC()\n#svm_clsf.fit(X_train, y_train)\n#print(\"Accuracy: \", svm_clsf.score(X_test,y_test))\n\np = make_pipeline(sum_col, alone_col, name, drop_col, svm_clsf)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","8edcc83e":"best_knn = []\nfor n in range(1,12):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    #knn.fit(X_train, y_train)\n    #best_knn.insert(n, knn.score(X_test,y_test))\n    p = make_pipeline(sum_col, alone_col, name, drop_col, knn)\n    p.fit(X_train, y_train)\n    best_knn.insert(n, p.score(X_test,y_test))\nbest_knn\n","74496760":"knn_clsf = KNeighborsClassifier(n_neighbors=5)\np = make_pipeline(sum_col, alone_col, name, drop_col, knn_clsf)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","95a178df":"voting_classfication = VotingClassifier(estimators = [('knn', knn_clsf),('lg', log_reg), ('rfg', rf_reg), ('svc', svm_clsf)], voting=\"hard\", n_jobs=-1)\np = make_pipeline(sum_col, alone_col, name, drop_col, voting_classfication)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","ce8ed63a":"#test_result = pd.Series(voting_classfication.predict(test_data), name = \"Survived\").astype(int)\n#results = pd.concat([test_data[\"PassengerId\"], test_result],axis = 1)\n#results.to_csv(\"titanic_submission.csv\", index = False)","7c396e9f":"## Selecci\u00f3n de modelos de clasificaci\u00f3n supervisada","b5e357f9":"<a id='16'><\/a><br>\n# 5. Modelos","2193b581":"Tambi\u00e9n lo haremos sobre los datos copiados para seguir la estructura, aunque no lo haremos sobre los datos de test para no comenter la fuga de datos.","c867e152":"### Correlation Matrix","e61e1116":"### Histogram-Based Gradient Boosting (Histogram Gradient Boosting","fd439dd6":"Hemos obtenido como configuraci\u00f3n \u00f3ptima una profundidad m\u00e1xima de 3, con un criterio gini, un 0.0 en el par\u00e1metro de post poda y ha discretizado en 2 intervalos.\n\nPrimero podemos ver que en este caso ha funcionado mucho mejor el discretizado en 2 intervalos como vimos en la pr\u00e1ctica anterior, obteniendo mejores resultados en los que se ha usado 2 intervalos en lugar de 3, al contrario que en el k vecinos, en el que las 5 configuraciones \u00f3ptimas utilizaban 3 intervalos para discretizar. Seguiremos observando en el resto de modelos para ver si obtienen 2 intervalos como en este caso.\n\nPodemos ver tambi\u00e9n que la configuraci\u00f3n \u00f3ptima ha utilizado gini aunque si continuamos mirando en la tabla podemos ver que tambi\u00e9n se han obtenido buenos resultados con el criterio entrop\u00eda. As\u00ed podemos concluir que el valor \u00f3ptimo de este hiperpar\u00e1metro var\u00eda en funci\u00f3n de otros factores, como otros hiperpar\u00e1metros o los datos que tengamos.\n\nComo profundidad m\u00e1xima hemos obtenido una profundidad de 3, generandose as\u00ed un \u00e1rbol que no es muy complejo. Esto es importante, ya que un \u00e1rbol muy complejo implica que est\u00e1 sobreajustado a los datos de entrenamiento, adem\u00e1s de generar reglas m\u00e1s complejas. Por eso podemos ver en la tabla que se han obtenido mejores resultados con \u00e1rboles de profundidad 3 y 4 que los \u00e1rboles de profundidad 5.\n\nTambi\u00e9n vemos que se han obtenido mejores resultados con un valor de ccp_alpha de 0. Esto implica que no se poda en los \u00e1rboles obtenidos. Este par\u00e1metro nos permite reducir la varianza en los \u00e1rboles complejos, pero en este caso como no hemos generado \u00e1rboles demasiado complejos pues parece que no ha obtenido buenos resultados pod\u00e1ndolos. Ya que podemos ver que los mejores resultados se han obtenido con un valor de 0 para este hiperpar\u00e1metro, mejor que con un valor de 0.1. A\u00fan as\u00ed es un hiperpar\u00e1metro a tener en cuenta si se van a obtener \u00e1rboles complejos en los que ser\u00e1 necesario aplicar una postpoda.","c8aa7184":"###\u00a05.2.1. Regresi\u00f3n log\u00edstica","6908c4d7":"###\u00a05.2.2. Random Forest","4be4a6e4":"Aqu\u00ed et\u00e1 m\u00e1s repartido ya que los valores de 1, 2 y 3 para `Parch` tienen la misma frecuencia para la clase.","034a6789":"En este apartado se van a detectar los outlier. Para ello, primero se va a obtener informaci\u00f3n de las variables num\u00e9ricas.","ccac8b31":"El siguiente modelo a analizar es el de `random forest`, mediante la clase `RandomForestClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `criterion` (criterio de partici\u00f3n)\n* `max_features` (n\u00famero de atributos para buscar la mejor partici\u00f3n)","2fffb940":"Por ejemplo el hiperpar\u00e1metro n_neighbors ahora se llama kneighborsclassifier__n_neighbors. Y el hiperpar\u00e1metro n_bins del discretizador se llama kbinsdiscretizer__n_bins.\n\nPodemos llegar a la conclusi\u00f3n que la forma de llamar a los hiperpar\u00e1metros del pipeline es el nombre de la clase en min\u00fasculas (kneighborsclassifier, kbinsdiscretizer), seguido de dos barras bajas (__) y seguido del nombre del hiperpar\u00e1metro original.","98f16b15":"# Dataset Diabetes","7d5e6428":" ### Age - Survived","71af496b":"Hemos obtenido como hiperpar\u00e1metros \u00f3ptimos un learning rate de 1, utilizando 2 intervalos para discretizar en el preprocesamiento, y se ha utilizado como estimador un \u00e1rbol de profundidad 1, con un criterio gini y un ccp_alpha de 0.\n\nPodemos ver que ha funcionado mucho mejor el discretizado de dos intervalos en lugar de tres. Con lo que seguimos confirmando lo que vimos en el an\u00e1lisis exploratorio de la pr\u00e1ctica anterior, que es que funciona mejor un discretizado en dos intervalos para este dataset.\n\nTambi\u00e9n hemos obtenido un learning rate de 1, aunque tambi\u00e9n funciona bien el learning rate de 0.95, ya que en las 5 primeras combinaciones de hiperpar\u00e1metros \u00f3ptimos obtenidos podemos ver que en las dos primeras se ha obtenido un learning rate de 1, pero en las tres siguientes de 0.95.\n\nComo \u00e1rbol podemos ver que cuanto m\u00e1s simple era el \u00e1rbol mejores resultados hemos obtenido, ya que los cuatro primeros \u00e1rboles son de profundidad 1. Esto sigue lo que se indic\u00f3 anteriormente y es que el algoritmo AdaBoost funciona mejor con estimadores muy simples (en este caso \u00e1rboles de clasificaci\u00f3n de 1 nivel) y al combinarlos permite reducir el error debido al sesgo. Tambi\u00e9n vemos que se ha utilizado un ccp_alpha de 0, ya que teniendo \u00e1rboles de un solo nivel no los puedes podar. Y como criterio ha dado mejores resultados gini, aunque al igual que vimos en los \u00e1rboles de decisi\u00f3n, entrop\u00eda tambi\u00e9n da buenos resultados, dependiendo m\u00e1s de su combinaci\u00f3n con el valor de otros hiperpar\u00e1metros.","a3119c80":"<a id='19'><\/a><br>\n##\u00a05.3.\u00a0Modelos \"Ensemble\"","e6a2e475":"Variables tipo Object:\n1. Name        : nombre de los pasajeros\n1. Sex         : hombre y mujer (male and female)\n1. Ticket      : n\u00famero del ticket\n1. Cabin       : categor\u00eda de la cabina\n1. Embarked    : puerto (puede ser C, Q, S)\n\nVariables tipo Int64\n1. PassengerId : id \u00fanico de los pasajeros\n1. Survived    : 0 -> muerto ,1-> sobrevivi\u00f3. Esta es la variable clase.\n1. Pclasss     : Clase de ticket: 1, 2 y 3 \n1. SibSp       : Cantidad de hermanos \/ esposo\/a a bordo\n1. Parch       : Cantidad de padres \/ hijos a bordo\n\nVariables tipo Float64:\n1. Age         : edad del pasajero\n1. Fare        : precio del ticket","ac2604fd":"<a id='12'><\/a><br>\n##\u00a04.1. Nuevos atributos.","fd4f282e":"Por \u00faltimo, vamos a ver de todos los modelos que hemos visto y creado cual es el que obtiene una tasa de acierto mayor para el conjunto de entrenamiento que hemos dividido con el *houldout*.","9fe46b69":"La tasa de acierto en este caso es muy baja, respecto a los anteriores. Este modelo no servir\u00e1.","ea17bdb3":"Para estos dos nuevos atributos se utiliza la variable clase para agrupar los de un mismo rango de edad. Como la variable clase no la obtenemos en el dataset del test, todos los pasos siguientes se comentar\u00e1n. Esta variable adem\u00e1s es redundante con el atributo edad. As\u00ed que no lo pasaremos al pipeline.","01073473":"Para finalizar con la selecci\u00f3n de modelos, vamos a estudiar el algoritmo Histogram-Based Gradient Boosting. Este algoritmo es una optimizaci\u00f3n del anterior que discretiza los datos de entrada para reducir el n\u00famero de puntos de corte a probar, reduciendo el tiempo que tarda en el entrenamiento y en la inferencia.\n\nUtilizaremos la clase HistGradientBoostingClassifier y lo a\u00f1adiremos al pipeline.","cb0dd064":"# Libreta reproducida sobre Titanic","7f61c3fc":"Como es habitual, cargamos los datos del dataset de `Diabetes`.","199b85a5":"## Creaci\u00f3n del pipeline.","2f9c9dad":"<a id='8'><\/a><br>\n# 4. Ingenier\u00eda de datos.\n","50731f70":"###\u00a0Name - Title","a495d19c":"El primer modelo a estudiar es de los `K-Vecinos`, mediante la clase `KNeighborsClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes de este modelo y los que analizaremos son:\n* `n_neighbors` (N\u00famero de vecinos)\n* `weights` (Pesos de los vecinos)","b8b7515e":"Comprobamos las particiones.","4bc9a07c":"Para finalizar la pr\u00e1ctica vamos a replicar una libreta que realiza un an\u00e1lisis exploratorio, preprocesamiento y al final entrena varios modelos del dataset `Titanic` e iremos comentando lo que se hace.\n\nEl enlace de la libreta original: https:\/\/www.kaggle.com\/hasanburakavci\/titanic-eda-and-classification-top-5","cc9ff602":"La eliminaci\u00f3n de las columnas y el imputador estar\u00e1n dentro de un `Column Transformer`, ya que solo queremos que estas acciones las hagan las columnas que hemos visto anteriormente.","efe52b0c":"### Gradient Boosting.","86cd98ce":"Se utilizar\u00e1 la mediana de los valores, que tengan el mismo valor en la variable `Pclass`. Es decir, para cada registro con `Age` on valor nulo, se escoger\u00e1n todos los casos en los que la variable `Pclass` valga lo mismo que el de ese registro, y se calcular\u00e1 la mediana del valor de `Age` en todos esos casos y se le asignar\u00e1 ese valor al registro con `Age` nulo.","bc964de0":"Con el estudio de los estad\u00edsticos vemos que las variables `Age`, `SibSp`, `Parch` y `Fare` presentan outlier:\n* Para `Age` vemos que la media de sus valores es 29, y el tercer cuartil es 38, sin embargo el m\u00e1ximo valor es 80. Por lo que hemos detectado que esta variable tiene al menos un outlier con valor 80.\n* Para `SibSp`vemos la media con un valor de 0.52, el tercer cuartil con 1, y sin embargo el m\u00e1ximo valor es 8, por lo que hemos detectado un outlier.\n* Para `Parch` vemos su madia de 0.38, su tercer cuartil vale 0, y el m\u00e1ximo vale 6.\n* Para `Fare` su media vale 32 y su tercer cuartil 31, sin embargo tiene un valor m\u00e1ximo de 512. Este outlier es el m\u00e1s alejado de todos los que hemos detectado.\n\nAhora se define una funci\u00f3n para eliminar estos outlier.","df8e2e3b":"### Bagging.","e74a1023":"Dividimos la base de datos.","1504eeeb":"Vemos que los pasajeros que ten\u00edan un ticket de 1 sobrevivieron m\u00e1s que los que ten\u00edan un ticket de 2, que a su vez, sobrevivieron m\u00e1s que los que ten\u00edan un ticket de 3.","80f61e57":"Vemos que las dos gr\u00e1ficas siguen una distribuci\u00f3n normal, por lo que no podemos intuir a priori si esta variable es in\u00fatil para la clasificaci\u00f3n. Aunque si nos fijamos en las dos gr\u00e1ficas vemos que hay la misma cantidad de valores para los mismos datos, es decir, que independientemente de si `Survived `vale 1 o 0, los dos gr\u00e1ficos son muy similares, por lo que parece que esta variable no aporta mucha informaci\u00f3n.","29bd4ff7":"### 3.1. Encontrar valores perdidos","117013f5":"Como es habitual tenemos empates en el primer puesto. Diremos que valores de loshiperpar\u00e1metros son los mejores y cuales dan igual entre unos y otros.\n\nPara el imputador KNN el n\u00famero de vecinos es de `5` y un peso `distance`\n\nPara el discretizador el n\u00famero de particiones es de `3 particiones`\n\nPara el estimador el alpha elegido es `0.0` (para el \u00e1rbol de decisi\u00f3n hemos visto que es el mejor alpha en el problema), la tasa de aprendizaje es la intermedia entre los diferentes valores elegidos `0.05`, la m\u00e1xima profundidad es de `1`. Por \u00faltimo, para el criterio da igual entre el ECM y el ECM de friedman, el empate ocurre en esta dos combinaciones de los valores ya vistos y los posibles del criterio.\n\nTasa de acierto resultante: $0.752 \\pm 0.046$\t","7f3ac345":"Para los hiperpar\u00e1metros de gradient boosting:\n\n* `learning rate`: Por defecto la tasa de aprendizaje es de 0.01. Entonces, utilizaremos valores cercanos a este.\n* `criterion`: Para este modelo el clasificador es diferente a los \u00e1rboles de decisi\u00f3n ya que se usan los residuos de las modelos aprendidos. Tenemos entonces mse o error cuadr\u00e1tico medio (ECM) y Friedman mse o el ECM de Friedman.\n* `max_depth`: M\u00e1xima profundidad del \u00e1rbol que aprende, cogeremos valores peque\u00f1os que hemos visto que funcionan bien.\n* `cpp_alpha`: Y el coste de complejidad como siempre.","d913d3e5":"#### Sex - Survived\n\nAqu\u00ed se ha realizado una fuga de datos (aqu\u00ed y en todo el an\u00e1lisis exploratorio), pues se ha realizado el an\u00e1lisis exploratorio con la variable `data`, que incluye a los datos de entrenamiento y a los de test. Se ha dejado comentada la l\u00ednea de c\u00f3digo original que utilizaba todos los datos para que se vea el error en este caso, pero nosotros vamos a utilizar solo los datos de entrenamiento, y para el resto de casos corregiremos esta l\u00ednea.","c0e38d6f":"Por \u00faltimo, dividimos el conjunto de datos en entrenamiento y prueba mediante un holdout estratificado:","50dd4cce":"El ensemble de los modelos anteriores por un metodo de voto da peores resultados qque un modelo m\u00e1s simple. Por tanto, para evaluar el modelo vamos a utilizar modelos simples.","8c335197":"Vemos que los que pertenec\u00edan al puerto de embarcaci\u00f3n de C, sobrevivieron m\u00e1s que los del puerto S y Q. Mientras que la cantidad de pasajeros que sobrevivieron de los puertos S y Q es muy similar.","80cfcc62":"Y continuamos con la variable clase:","e9b5fd71":"Como hiperpar\u00e1metros de este algoritmo tenemos:\n\n* learning_rate. Ratio de aprendizaje igual que en el anterior. Probaremos con 0.03 y 0.1.\n* max_iter. Es el n\u00famero m\u00e1ximo de iteraciones del algoritmo. Lo dejaremos por defecto que vale 100 para no aumentar m\u00e1s el coste.\n* Tiene algunos hiperpar\u00e1metros propios de los \u00e1rboles como max_depth o max_leaf_nodes. Estos hiperpar\u00e1metros sirven para ajustar la complejidad de los \u00e1rboles. Como en los modelos anteriores hemos optimizado max_depth, en este caso vamos a optimizar max_leaf_nodes, que imaginamos que funcionar\u00e1 mejor cuanto menor valor tenga pues generar\u00e1 \u00e1rboles m\u00e1s simples.","2a120275":"Para el imputador KNN el n\u00famero de vecinos es de `10` y un peso `uniform`.\n\nPara el discretizador el n\u00famero de particiones es de `3 particiones`.\n\nPara el estimador la tasa de aprendizaje es `0.1` la mayor del conjunto y por defecto y el m\u00e1ximo n\u00famero de nodos hoja es 15 o 31, hay un empate.\n\nTasa de acierto resultante: $0.751 \\pm 0.046$","b48a3124":"Vamos a empezar cargando las datos al igual que hicimos en la pr\u00e1ctica anterior.","0732ebbe":"El siguiente modelo a analizar es el de `\u00e1rboles de decisi\u00f3n`, mediante la clase `DecisionTreeClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `criterion` (criterio de corte)\n* `max_depth` (profundidad m\u00e1xima del \u00e1rbol)\n* `cpp_alpha` (coste de complejidad para encontrar el sub\u00e1rbol \u00f3ptimo, este par\u00e1metro decidir\u00e1 si hacer la poda)","95422f5a":"Ahora se van a mostrar algunas instancias para comporbar que se han cargado los datos correctamente. Vamos a utilizar la funci\u00f3n `head()` que nos muestra las primeras instancias. Nosotros prefeririamos usar la funci\u00f3n `sample()` que mostrar\u00eda instancias al azar para as\u00ed evitar obtener una muestra sesgada que no represente al conjunto de datos. Pero hemos usado `head()` porque es la que se ha usado en la libreta que estamos reproduciendo.","80019905":"## Pipeline","a76d1bb0":"## Construcci\u00f3n y validaci\u00f3n del modelo final","16155df6":"Para regresi\u00f3n log\u00edstica obtenemos la mayor tasa de acierto, al ser un modelo simple lo consideraremos por encima del resto.","e1450921":"Como ocurre en el resto, hay muchos empates.Entonces, para el imputador KNN el n\u00famero de vecinos y el peso produce un mismo resultado final.\n\nPara el discretizador el n\u00famero de particiones es de `2 particiones`. Para este clasificador, vemos en los resultados que este valor de partici\u00f3n es el m\u00e1s idoneo.\n\nPor los empates que encontramos, para el estimador el criterio del \u00e1rbol de decision da igual si es por entrop\u00eda o gini. Mientra que la profundidad m\u00e1xima del \u00e1rbol encontrada es de `1 de profundidad`. Con el valor por defecto encuentra el mejor *score*\n\nTasa de acierto resultante: $0.747 \\pm 0.062$","67c52060":"Siguiendo el orden que hemos establecido:\n\nPara el imputador KNN el n\u00famero de vecinos es de `5` y un peso `uniform`. Los valores por defecto que hemos elegido y hemos visto que funcionan bien.\n\nPara el discretizador el n\u00famero de particiones es de `3 particiones`. Como hemos comentado en k-vecinos, este resultado no lo esperabamos pero funciona mejor que discretizar en 2 particiones.\n\nPara el estimador el n\u00famero de profundidad m\u00e1xima es de `1`. Vemos que con la m\u00ednima profunidad que hemos elegido obtiene el mejor resultado. El alpha de `0.0`, como ocurr\u00eda en \u00e1rboles de decisi\u00f3n. Para los dos \u00faltimos hiperpar\u00e1metros, alpha y learning rate, vemos que cualquier combinaci\u00f3n con el resto de hiperpar\u00e1metros anteriores se obtiene el m\u00e1ximo *score*. Hay 4 empates en la primera posici\u00f3n, pero como ocurr\u00eda anteriormente, se elegir\u00e1 el que devuelve el algortimo que es el primero en crear.\n\nTasa de acierto resultante: $0.753 \\pm 0.045$\t","76b76338":"Indicar que todav\u00eda no podemos crear el pipeline, pues hay que crear un pipeline para cada modelo, ya que hay que incluir dicho modelo en el pipeline.","549faae5":"En este apartado se va a realizar el an\u00e1lisis exploratorio. Cuando se realizo el `data_train.info()` obtuvimos el tipo de cada variable:","a611aa79":"Creamos el pipeline con lo visto anteriormente y el estimador. Lo creamos de una forma diferente al primer estudio, con `Pipeline` en lugar de `make_pipeline` para poder ponerle los nombres en lugar de que te los genere automaticamente. Obtenemos el nombre de los par\u00e1metros para saber cuales son y como escribirlos a la hora de optimizarlos.","7a96af3b":"### Vecinos m\u00e1s cercanos","10750606":"El siguiente modelo a analizar es el de `gradient boosting`, mediante la clase `GradientBoostingClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `learning rate` (tasa de aprendizaje)\n* `criterion` (criterio de partici\u00f3n)\n* `max_depth` (m\u00e1xima profundidad)\n* `cpp_alpha` (coste de complejidad)","93ca30c2":"Y comprobando que se han cargado correctamente:","8bf76dca":"# Dataset Wisconsin","c74fffbe":"Vimos al principio con el `data_train.info()` que hab\u00eda valores perdidos en los datos. En este apartado se van a buscar y a tratar estos valores. En la libreta original encuentra muchos m\u00e1s valores perdidos que nosotros ya que utiliza el dataset completo (train y test) para el an\u00e1lisis exploratorio. Nosotros corregiremos esto.","bc5787ad":"Obtenemos que no se ha detectado ning\u00fan outlier porque hay menos de tres outlier en cada variable por lo que no merece la pena eliminar ninuna instancia.","82cb84c9":"Y nos aseguramos que se ha realizado adecuadamente. Comenzamos con el conjunto de datos de entrenamiento:","e20b19ef":"Para empezar vamos a definir la validaci\u00f3n cruzada que vamos a utilizar para entrenar los modelos. Utilizamos una validaci\u00f3n cruzada estratificada de $5x10-cv$, que como vimos en clase de pr\u00e1cticas es la m\u00e1s idonea para este tipo de validaci\u00f3n cruzada.","d9bfe584":"<a id='13'><\/a><br>\n##\u00a04.2. Eliminaci\u00f3n de variables\n* Ticket, Cabin, Name y PassengerId son eliminadas por la poca correlaci\u00f3n que hemos visto con otras variables.\n* Adem\u00e1s eliminaremos Cabin porque casi todos sus valores son desconocidos (valen null).\n* El resto de variables que hemos utilizado para crear nuevas variables tambien las eliminaremos.\n\nLas eliminaremos en el pipeline.","8ca00a68":"La configuraci\u00f3n \u00f3ptima de hiperpar\u00e1metros obtenida es un learning rate de 0.03, con \u00e1rboles con 15 nodos hoja como m\u00e1ximo y se ha aplicado un discretizado con dos intervalos en el preprocesamiento.\n\nAhora si que podemos concluir que como vimos en el an\u00e1lisis exploratorio de la pr\u00e1ctica anterior el discretizado en el preprocesamiento de dos intervalos a funcionado mejor que el de tres, ya que en todos los modelos (excepto en el k vecinos) han funcionado mejor los modelos con un discretizado de dos intervalos. El cambio de tendencia en el k vecinos se puede deber a que como para clasificar se fija en los vecinos m\u00e1s cercanos, si se discretiza en m\u00e1s intervalos tiene m\u00e1s posibles valores para generar los vecinos.\n\nEl learning rate ha funcionado mejor con 0.03 que con 0.1. Aunque si nos fijamos en los modelos en los que se ha discretizado en tres intervalos ha funcionado al contrario, mejor con 0.1 que con 0.03.\n\nPara el n\u00famero m\u00e1ximo de hojas (max_leaf_nodes) ha funcionado mejor con menos hojas. Podemos ver que los modelos con un valor de 15 han funcionado mejor que los de 31, y estos a su vez han funcionado mejor de los de 65. Esto puede ser porque si recordamos que este algoritmo es un tipo de Boosting, y Boosting funciona mejor con estimadores simples, que tiene error debido al sesgo y lo que hace es agregarlos para reducir este error. Pues cuantas menos hojas tengan los \u00e1rboles significa que son \u00e1rboles m\u00e1s peque\u00f1os y m\u00e1s simples.","6d822764":"Vemos que un 74% de las mujeres sobreviven, mientras que de los hombres solo sobreviven un 18%. Por lo que las mujeres sobreviven m\u00e1s que los hombres.","5a72245b":"### Random Forest.","b85658d1":"Ahora vamos a ver el algoritmo Gradient Tree Boosting, que es otro ensembler, que es un tipo de algoritmo de Boosting que es capaz de optimizar cualquier tipo de funci\u00f3n de p\u00e9rdida.\n\nUtilizaremos la clase GradientBoostingClassifier y lo agregaremos al pipeline.","1ff744a0":"Para simplificar vamos a renombrar a las variables de entrenamiento.","cb485d91":"## Modelos de clasificaci\u00f3n supervisada","eda77f73":"## Carga de datos","ef077492":"Dado que en la pr\u00e1ctica anterior se realiz\u00f3 un an\u00e1lisis exploratorio de los datos para sacar unas conclusiones que nos ayudaran a hacer un buen preprocesamiento de datos, vamos a aprovechar que ya disponemos de esta informaci\u00f3n y a aplicar el pipeline que calculamos en la pr\u00e1ctica anterior.\n\nEn la pr\u00e1ctica anterior hab\u00edamos definido una funci\u00f3n para eliminar estas columnas y las elimin\u00e1bamos mediante un FuctionTransformer, pero ahora las vamos a eliminar con la funci\u00f3n make_column_transformer ya que as\u00ed no necesitamos crear nosotros nuestra propia funci\u00f3n.\n\nAhora vamos a crear un column transformer que nos permita eliminar las variables que hab\u00edamos detectado que eran in\u00fatiles y la variable corrupta `Unnamed: 32`.","c1f25d93":"### 5.2.3. Support Vector Machine (SVM)","3bdad864":"## Validaci\u00f3n cruzada.","4317c0d2":"Vemos que la muestra est\u00e1 desbalanceada, ya que hay casi el doble de casos en los que la variable clase vale 0, que los que vale 1. Por lo que tenemos m\u00e1s casos en los que no se sobrevivi\u00f3, que los que si se sobrevivi\u00f3.","53d73bea":"Como sabemos de la matriz de correlaci\u00f3n `Age` tiene relaci\u00f3n con las variables `Pclass`(0.37) y `SibSp` (0.31), por lo que para asignarle un valor a los casos perdidos de `Age`, se usar\u00e1 la informaci\u00f3n de la variable que m\u00e1s este relacionada con ella, que en este caso es `Pclass`.\n\nVisualizamos las instancias en las que `Age` est\u00e1 a null.","b0eaeb27":"### 3.2. Rellenar los valores perdidos\n\n*\u00a0Age tiene 177 valores perdidos\n* Cabin tiene 687 valores perdidos\n* Embarked tiene 2 valores perdidos","f834c059":"### Histogram Gradient Boosting.","78288b45":"### Pclass - Survived","eda9d369":"El siguiente modelo a analizar es el de `adaboost` o `boosting`, mediante la clase `AdaBoostClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `base_estimator` (el estimador base del modelo que ser\u00e1 un \u00e1rbol de decision)\n* `learning_rate` (tasa de aprendizaje, reduce la importancia de cada clasificador)\n\nHemos visto que como hiperpar\u00e1metro, adaboost tiene un \u00e1rbol de decisi\u00f3n por lo que los hiperpar\u00e1metros de este modelo tambi\u00e9n estar\u00e1n incluidos. Muchos de los *ensembles* que estabmos viendo funcionaran con \u00e1rboles de decisi\u00f3n y por lo tanto con sus hiperpar\u00e1metros.\n* `criterion` (criterio)\n* `max_depth` (profundidad m\u00e1xima)\n* `cpp_alpha` (coste de complejidad)","ad08e03a":"<a id='17'><\/a><br>\n##\u00a05.1 Train Test Split","8712f5cd":"Gradient Boosting es el modelo que obtiene la mejor tasa de acierto con un `0.757576` de todos los estudiados. La mayor\u00eda de ensembles nos dan una tasa de acierto mayor que modelos simples como \u00e1rboles de decisi\u00f3n. Cabe remarcar que KNN da una tasa de acierto considerable para el tipo de modelo que es. Por ello, vamos a hacer una evaluaci\u00f3n en terminos de coste para elegir el mejor clasificador para este dataset.\n\nLos modelos *ensemble* son modelos computacionalmente m\u00e1s costos, por tanto el tiempo de inferencia es mucho mayor que modelos como \u00c1rboles de Decisi\u00f3n o KNN. A la hora de elegir que modelo es mejor no solo nos podemos fijar en terminos de tasa de aciertos (*score*), si un modelo *ensemble* y un modelo sencillo consiguen resultados muy parecidos o iguales es mejor elegir el modelo sencillo.\n\nEn los resultados observanos que KNN obtiene la misma tasa de acierto que Random Forest (`0.727273`). En este caso eligiriamos hacer nuestro clasificador con KNN por la sencillez de este. Tambi\u00e9n, Decision Tree y Bagging obtienen un mismo *score* (`0.718615`), ocurrir\u00eda lo mismo que anteriormente.\n\nEntonces, vemos que modelos como KNN consiguen m\u00e1s puntuaci\u00f3n que muchos de los *ensembles* e incluso estos modelos que quedan por encima de KNN tienen una diferencia poco significativa que har\u00eda que nos decatasemos por el modelo de los vecinos cercanos.","d20c0e91":"### Parch - Survived","8e7e1ad5":"A su vez, lo dividimos en variables predictoras (X) y variable clase (y):","fb2b9cbb":"Y por \u00faltimo definimos el discretizador, donde el hiperpar\u00e1metro de `n_bins` tambien sera optimizado.","f39776d8":"Hemos obtenido que la configuraci\u00f3n de hiperpar\u00e1metros \u00f3ptima para este caso en concreto es utlizar 4 vecinos, la funci\u00f3n de peso distancia y discretizar las variables predictoras en tres intervalos.\n\nComo nos pod\u00edamos imaginar hemos obtenido un n\u00famero de vecinos intermedio, en este caso 4. Esto es debido a que si generamos un modelo utilizando solo al vecino m\u00e1s cercano, estar\u00edamos creando un modelo muy sobreajustado a los datos de entrenamiento que no ser\u00eda capaz de generalizar, ya que solo se fija en el dato m\u00e1s cercano. Si por el contrario generamos un modelo que utiliza una gran cantidad de vecinos para clasificar, se generar\u00eda un modelo similar al 0R ya que clasificar\u00eda como la clase mayoritar\u00eda.\n\nPodemos ver que los valores elegidos para n_neighbors en las mejores configuraciones de par\u00e1metros tienen un valor de entre 3 y 6. Pudiendo ver que utilizar solo 1 o 2 vecinos han obtenido peores resultados porque como hemos indicado se ajustan a los datos y no permiten que el modelo sea capaz de generalizar.\n\nComo vemos hemos obtenido como funci\u00f3n de peso distance en la configuraci\u00f3n \u00f3ptima, incluso parece que funciona bien con este dataset pues si miramos las cinco mejores configuraciones obtenidas se ha elegido distance en cuatro de ellas. Esto es porque esta funci\u00f3n le asigna un peso en funci\u00f3n de la distancia que hay entre el punto a clasificar y los puntos m\u00e1s cercanos, mientras que uniform le aigna el mismo peso, independientemente de la distancia que haya. A\u00fan as\u00ed, dependiendo del dataset que estemos tratando y de otros factores (como el preprocesamiento que se haya realizado y como afecten el resto de hiperpar\u00e1metros) puede ser que funcione mejor otra funci\u00f3n de pesos.\n\nPor \u00faltimo, tenemos que indicar que por el contrario a lo que nos esper\u00e1bamos, en este caso ha funcionado mejor un discretizado con tres intervalos que con dos, tal y como vimos que deber\u00eda de funcionar mejor en el an\u00e1lisis exploratorio de la pr\u00e1ctica anterior.\n\nComprobaremos con el resto de modelos si se trata de un caso aislado y es que para este modelo en concreto funciona mejor con 3 intervalos, o es que nuestra conclusi\u00f3n del an\u00e1lisis exploratorio estaba equivocada.","00baa727":"<a id='18'><\/a><br>\n##\u00a05.2. Metodos de clasificaci\u00f3n.\n\n\n* Logistic Regression\n* Random Forest Regression\n* Support Vector Machine (SVM)\n* K-Nearest Neighbors (KNN)","2fa62e33":"En este apartado el autor realiza cambios en los datos del dataset, como la creaci\u00f3n de nuevos atributos con la combinaci\u00f3n de otros ya existentes para incrementar la informaci\u00f3n de clasificaci\u00f3n. La eliminaci\u00f3n de valores dentro de un atributo para generalizar. Y la eliminaci\u00f3n de atributos que no aportan m\u00e1s informaci\u00f3n al clasificador (por ejemplo, `Cabin` que casi todos sus valores valen null).","f4fed250":"Como ocurr\u00eda con la pr\u00e1ctica 1 de an\u00e1lisis exploratorio, vamos a crear un *notebook* replicando la libreta y utilizando el script proporcionado por `J\u00faan Carlos Alfaro Jim\u00e9nez`, al que al igual que en la pr\u00e1ctica anterior le hemos agregado la funci\u00f3n que nos permite eliminar outlier (`outlier_rejection`).\n\nEn este caso los dataset de `Diabetes` y `Winconsin` crearemos modelos simples y *ensembles* de estos modelos, conociendo y estudiando sus hiperpar\u00e1metros para luego optimizarlos y sacar el mejor resultado de `tasa de acierto` posible para cada modelo.\n\nComo tercer punto de esta pr\u00e1ctica hemos replicado una libreta de kaggle, corrigiendo algunos de sus errores, que para adelantar podemos decir que comete fuga de datos.","2c7314f3":"Para knn comprobamos del 1 al 12 en n\u00famero de vecinos.","f41da1dd":"Se van a eliminar los outlier de las variables que hemos detectado que tienen alg\u00fan outlier, que eran: `Age`, `SibSp`, `Parch` y `Fare`. Se van a eliminar los outlier de las variables solo si hay m\u00e1s de tres outlier en esa variable.\n\nNosotros habr\u00edamos eliminado los outlier en el preprocesamiento despu\u00e9s de haber terminado el an\u00e1lisis exploratorio, y lo habr\u00edamos incluido en el pipeline como hicimos en la pr\u00e1ctica 1. Adem\u00e1s, aqu\u00ed se define una funci\u00f3n para eliminar los outlier, y este es un trabajo innecesario, ya que `Scikit` tiene funciones implementadas que realizan esta funci\u00f3n, y no ser\u00eda necesario escribir nuestra propio funci\u00f3n.","33a25eca":"Y tamb\u00eden, antes de empezar vamos a establecer el conjunto de los valores de los hiperpar\u00e1metros de nuestro pipeline que utilizaremos para encontrar el m\u00e1s \u00f3ptimo en todos los diferentes modelos (excepto los hiperpar\u00e1metros de los estimadores).","5bc5fc59":"Utilizaremos para los hiperpar\u00e1metros:\n* `weights`: Todos los posibles valores de weights (uniform, distance)\n* `n_neighbors`: Los valores peque\u00f1os los eliminaremos ya que el modelo se sobreajusta, valores altos llevar\u00e1n al modelo a parecerse a un 0-R y los valores pares de menor valor los eliminaremos porque producen empate. ","8377613c":"En el proceso de optimizaci\u00f3n con validaci\u00f3n cruzada el mejor resultado de tasa de acierto lo obteniamos con K-Vecinos con `0.755` seguido muy cerca de AdaBoost (`0.753`) y Grandient Boosting (`0.752`). Todo esto dentro del conjunto de entrenamiento.","7ca4a665":"Comprobamos que en los datos de entrenamiento ya no hay instancias con `Embarked` a null.","b0a95259":"### SibSp - Survived","e8722302":"Esta parte la vamos a comentar entera. Como el test y el train est\u00e1n en diferentes dataset cada uno cargados en una odentificador independiente no es necesario realizar un *houldout*. Al estar a divididos no tiene sentido unirlos y luego separarlos.","92d885f4":"####\u00a0Embarked\n\nDe la matriz de correlaci\u00f3n anterior vemos que `Embarked` no est\u00e1 relacionado con ninguna variable (excepto la variable clase). Y tambi\u00e9n tiene un pocoo de relaci\u00f3n con `Sex`, aunque tiene poca (0.12), y ya que solo hay dos valores perdidos, para rellenarlos se les asignar\u00e1 el valor mayoritario, que como en el gr\u00e1fico del an\u00e1lisis univariado era 1 (C).\n\nVisualizamos las instancias en las que `Embarked` est\u00e1 a null.","4ef5c43c":"Podemos ver que los modelos que han obtenido mejores resultados son los ensemblers, que han obtenido todos mejores resultados que los k vecinos y los \u00e1rboles de decisi\u00f3n. Esto era de esperar ya que como se nos dijo en clase o podemos ver en las competiciones (donde se tiende a utilizar ensemblers en vez de otros algoritmos) suelen funcionar mejor los ensembler.\n\nVemos que el algoritmo que peores resultados ha obtenido es el k vecinos, aunque para ser un modelo que no aprende ning\u00fan clasificador, sino que 'memoriza' todos los datos de entrenamiento y luego clasifica seg\u00fan los casos que m\u00e1s se le parezcan, ha obtenido muy buenos resultados (un score de 0.918).\n\nDentro de los ensembler vemos que el Gradient Boosting y el Histogram Gradient Boosting son los que mejores resultados han obtenido.\n\nA\u00fan as\u00ed hay que tener en cuenta que los ensembler han obtenido mejores resultados si tenemos en cuenta solo la tasa de aciertos. Si tenemos otros factores como el tiempo de aprendizaje y el tiempo de inferencia, tenemos que modelos m\u00e1s sencillos como el \u00e1rbol de decisi\u00f3n tardan mucho menos tiempo y han obtenido unos resultados (en tasa de aciertos) muy similares a los ensembler.","071f7bcd":"Para los hiperpar\u00e1metros de random forest:\n* `criterion`: Los dos diferentes valores que hemos visto en el resto de ejemplos.\n* `max_features`: Para este hiperpar\u00e1metro utilizaremos sqrt y log2 aunque tamb\u00eden existe la posibilidad de auto (visto en la documentaci\u00f3n de sklearn), funciona de la misma forma que sqrt. Como su nombre indica, sqrt hace la raiz cuadrada del n\u00famero de atributos y log2 hace el logaritmo en base 2 del n\u00famero de atributos.","2e8fd9a6":"Hemos obtenido como hiperpar\u00e1metros \u00f3ptimos 20 n\u00famero de estimadores, hacer el discretizado en dos intervalos y como criter\u00edo de los \u00e1rboles de decisi\u00f3n que us\u00e1bamos como estimadores hemos obtenido entrop\u00eda.\n\nPodemos ver que seguimos obteniendo que funcionan mejor nuestros modelos para este dataset discretizando en dos intervalos en el preprocesamiento.\n\nTambi\u00e9n podemos observar que tienden a funcionar mejor los casos en los que hemos aprendido m\u00e1s estimadores. Se ha obtenido que para la mejor configuraci\u00f3n de hiperpar\u00e1metros se han aprendido 20 estimadores, pero si miramos la tabla vemos una tendencia en la que se han obtenido mejores resultados utilizando 20 o 30 estimadores que con 10.","81b3e0de":"Para realizar un an\u00e1lisis multivariado, en el que vemos como se relacionan las variables, se va utilizar una matriz de correlaci\u00f3n. Se ha vuelto a realizar una fuga de datos, ya que ha utilizado todos los datos para calcular la matriz de correlaci\u00f3n. Nosotros lo corregiremos y utilizaremos solo los datos de entrenamiento.","7eb15d79":"Hemos obtenido como configuraci\u00f3n de hiperpar\u00e1metros \u00f3ptima un 0.1 para el learning rate, friedman_mse como criterio, como estimadores se utilizan \u00e1rboles de profundidad 1.\n\nSeguimos obteniendo mejores resultados al discretizar en el preprocesamiento las variables en dos intervalos que con tres, cumpliendo con las conclusiones que obtuvimos en el an\u00e1lisis exploratorio de la pr\u00e1ctica anterior.\n\nHemos obtenido que funciona mejor el learning rate de 0.1 que de 0.05, esto puede ser debido a que 0.05 es un valor demasiado peque\u00f1o y as\u00ed cada \u00e1rbol aprendido aportar\u00eda muy poco al modelo final formado por la agregaci\u00f3n de todos los \u00e1rboles. Como sabemos de teor\u00eda tampoco es bueno tener un learning rate muy alto ya que los modelos no podr\u00edan converger bien. Pero en este caso 0.1 es un buen valor ya que no es alto, siendo un valor peque\u00f1o, pero no demasiado peque\u00f1o como 0.05.\n\nPara los \u00e1rboles entrenados tenemos que ha utilizado como criterio friedman_mse, aunque tambi\u00e9n ha obtenido buenos resultados con mse. Lo importante de los \u00e1rboles es que hay una tendencia por la que ha funcionado mejor con \u00e1rboles con una profundidad m\u00e1xima de 1 que de 2. Esto era de esperar, ya que este algoritmo funciona mejor con estimadores muy simples con mucho error debido al sesgo, que se encargar\u00e1 de solucionar mediante la agregaci\u00f3n de todos los estimadores aprendidos al modelo final, por eso ha funcionado mejor con \u00e1rboles de profundidad 1, que son m\u00e1s simples.","d9a2041a":"Vemos que las variables `Age`, `Cabin` y `Embarked` presentan alg\u00fan valor perdido. Vamos a comprobar cuantos valores perdidos tienen cada variable.\n\nCorrelation Matrix\n* Pclass is associated with Fare.\n* Embarked is not associated with any feature.\n* Pclass and SibSp are associated with Age.","2b1b9ded":"Primero como es habitual, cargaremos las librerias que hemos utilizado.","4757b763":"A partir del n\u00famero de hermanos y n\u00famero de padres, se puede crear una variable del n\u00famero de miembros de la familia. ","a59a1d7b":"Hemos obtenido como configuraci\u00f3n \u00f3ptima de hiperpar\u00e1metros gini como criterio para seleccionar las particiones de los \u00e1rboles, un discretizado de dos intervalos a aplicar en el preprocesamiento, utilizar 100 estimadores, y como t\u00e9cnica para conseguir el n\u00famero de caracter\u00edsticas a aplicar en cada nodo sqrt.\n\nHemos vuelto a obtener que es mejor discretizar en dos intervalos que en tres, ya que ha obtenido mejores resultados con dos intervalos que con tres. En las dos mejores configuraciones ha obtenido gini aunque tambi\u00e9n ha conseguido buenos resultados con entrop\u00eca.\n\nVemos una tendencia en la que ha obtenido mejores resultados con 100 estimadores que con 50, con lo que puede que obtenga mejores resultados al utilizar m\u00e1s estimadores. Esto se cumplir\u00e1 aunque llegar\u00e1 un momento que no resulte \u00f3ptimo utilizar m\u00e1s estimadores pues se obtendr\u00eda una ventaja demasiado peque\u00f1a con referente al coste de utilizar m\u00e1s estimadores.\n\nTambi\u00e9n vemos que ha obtenido como hiperpar\u00e1metro \u00f3ptimo sqrt, que significa que ha aplicado una raiz cuadrada para obtener el n\u00famero de caracter\u00edsticas a utilizar en cada nodo, aunque si miramos la tabla vemos que tambi\u00e9n se han obtenido buenos resultados si utiliza el logaritmo (log2), viendo que en este caso este hiperpar\u00e1metro no ha jugado un papel diferenciador.","fe62721e":"Y finalizamos con el conjunto de datos de prueba:","01b57c7d":"### 1.2. Combina los datos","6fcd0d44":"<a id='20'><\/a><br>\n## 5.4. Resultados\n","b2f9bfa8":"Vamos a definir que valores para el discretizador vamos a optimizar mediante la validaci\u00f3n cruzada. En este caso como se dijo anteriormente vamos a probar si efectivamente es mejor el discretizador con dos intervalos que con tres (se validar\u00e1 para todos los modelos).","0d7d8eb1":"#\u00a0Pr\u00e1ctica 2: Aprendizaje y selecci\u00f3n de modelos de clasificaci\u00f3n\n\n###\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n* \u00c1ngel Torres del \u00c1lamo\n* Jos\u00e9 \u00c1ngel Royo L\u00f3pez","cd30f143":"Estas son las columnas que utilizar\u00e1 el imputador para establecer un valor v\u00e1lido a los valores perdidos.","4de3becd":"El siguiente modelo a analizar es el de `bagging`, mediante la clase `BaggingClssifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `base_estimator` (el estimador base del modelo que ser\u00e1 un \u00e1rbol de decision)\n* `criterion` (criterio de corte del \u00e1rbol)","966d44bc":"Establecemos una semilla para que los resultados sean reproducibles.","c5c5ef10":"Por \u00faltimo, los hiperpar\u00e1metros para historical gradient boosting son:\n* `learning rate`: Igual que con gradient boosting, el valor por defecto es 0.1. Tendremos valores cercanos a este.\n* `max_leaf_nodes`: Y por \u00faltimo, el n\u00famero m\u00e1ximo nodos hoja. Tendremos valores diferentes entre ellos para ver cual mejora el modelo, pondremos valores bajos porque hemos visto que nuestro \u00e1rbol no crea mucha profundidad y no especifica tanto.","90406a8a":"El siguiente modelo a analizar es el de `histogram gradient boosting`, mediante la clase `HistgradientBoostingClassifier` de sklearn. Los hiperpar\u00e1metros m\u00e1s importantes son:\n* `learning rate` (tasa de aprendizaje)\n* `max_leaf_nodes` (m\u00e1ximo n\u00famero de nodos hoja)","25ec893a":"### Embarked - Survived","a61f38b0":"Los \u00e1rboles de decisi\u00f3n tienen m\u00e1s hiperpar\u00e1metros que el modelo de los k vecinos, aunque no podemos optimizarlos todos ya que implicar\u00eda un gran coste computacional, necesitando mucho tiempo.\n\nPor eso seleccionaremos los que creamos m\u00e1s importantes. Optimizaremos:\n\n* criterion. El criterio utilizado para seleccionar la mejor partici\u00f3n, que puede ser gini o entrop\u00eda.\n* max_depth. La m\u00e1xima profundidad del \u00e1rbol, que probaremos con 3, 4 y 5. No probando con 1 y 2 porque el \u00e1rbol saldr\u00eda muy peque\u00f1o y obtendr\u00edamos un modelo con underfitting. Por el contrario si obtenemos un \u00e1rbol muy grande (con profundidad mayor que 6) tendr\u00edamos un modelo con overfitting, es decir ser\u00eda demasiado complejo y se ajustar\u00eda demasiado a los datos de entrenamiento.\n* ccp_alpha. Par\u00e1metro de complejidad que usa el algoritmo de post-poda, es decir, este hiperpar\u00e1metro nos permitir\u00e1 decidir si podar alguna rama del \u00e1rbol. \n\nOtro hiperpar\u00e1metro que podr\u00eda resultar interesante ser\u00eda el min_samples_split o el min_samples_leaf que representan al m\u00ednimo n\u00famero de registros que tiene que haber en un nodo para poder ramificarlo, y el m\u00ednimo n\u00famero de ejemplos que tiene que haber en un nodo para que pueda ser hoja. Esto nos ayudar\u00eda a reducir el sobreajuste, aunque como esto ya lo hacemos con el max_depth no vamos a optimizar estos hiperpar\u00e1metros pues ser\u00eda muy costoso.","417e4341":"Le asignamos un 1 a las instancias correspondientes.","2fd6e6e8":"Vemos que las dos gr\u00e1ficas siguen una distrbuci\u00f3na asim\u00e9trica positiva, por lo que al no seguir una distribuci\u00f3n uniforme pueden aportar algo de informaci\u00f3n. Aunque si nos fijamos en que las dos gr\u00e1ficas son muy similares, concluimos que por si sola esta variable no aporta mucha informaci\u00f3n.\n\nVeremos como se relaciona con el resto en el an\u00e1lisis multivariado, al igual que `Age`, que por si sola tampoco parece que aporte informaci\u00f3n.","1957a69f":"Crearemos nuevas columnas para la base de datos con la informacion de otras. Aqu\u00ed se ha realizado fuga de datos, nosotros en su lugar utilizaremos un pipeline.","6d2fc43f":"### \u00c1rboles de decisi\u00f3n.","096c91e6":"### Bootstrap Aggregating (Bagging)","eac1e989":"#### Age","70dbddab":"<a id='14'><\/a><br>\n##\u00a04.3. One Hot Encoding","751e0c34":"Con las nuevas variables, la matriz de correlaciones quedan de esta manera:","4f4e6439":"Aqu\u00ed ha utilizado la funci\u00f3n `info()` que nos proporciona informaci\u00f3n de las variables. Por ejemplo, podemos ver que hay variables como `Cabin`, que tienen valores nulos (debido a que en total nos dicen que hay 891 casos, de las cuales `Cabin` solo tiene 204 con valores no nulos. Tambi\u00e9n nos indica informaci\u00f3n del tipo de cada variable.\n\nAunque no se han sacado conclusiones en la libreta de esta parte ya que todav\u00eda no ha iniciado el an\u00e1lisis exploratorio, y vamos a intentar seguir la misma estructura que en la libreta origial. Se analizar\u00e1 cada variable m\u00e1s adelante, y la detecci\u00f3n de variables con valores nulos se realizar\u00e1 m\u00e1s adelante en el an\u00e1lisis exploratorio.","e6bd0ade":"Hay diferentes categor\u00edas de los titulos con los que se llama a una persona. Podemos reducir los titulos que tenga menos instancias en la base de datos los podemos denominar como \"Other\".","05dd5fe1":"Una vez que hemos seleccionado para cada modelo su mejor configuraci\u00f3n de hiperpar\u00e1metros mediante la validaci\u00f3n cruzada, vamos a evaluarlos con el conjunto de test y veremos que resultados han obtenido.","3c4e1671":"### Gradient Tree Boosting (Gradient Boosting)","7a4e16ce":"###\u00a0Alone and Family \n* SibSp + Parch = family","48cc5618":"### AdaBoost.","2bcf371f":"Vemos que los pasajeros con 1 `SibSp` tienen un mayor porcentaje de haber sobrevivido que el resto.","59cdb492":"El imputador funcionar\u00e1 mediante los k-vecinos como ya vimos en el primer estudio. Con la diferencia de que en este estudio se b\u00fascara el n\u00famero de vecinos y el peso de los vecinos \u00f3ptimo para el modelo que estamos estudiando.","4c8e58e7":"Para estudiar el resultado que nos ha dado el algortimo de optimizaci\u00f3n vamos a ir por partes. Primero analizaremos los hiperpar\u00e1metros del imputador, luego los del discretizador y por \u00faltimo los del estimador.\n\nPara KNN el imputador ha sacado un resultado de `10 vecinos` y un peso `uniforme`. Estos resultados son los mejores que el algortimo al imputar los valores perdidos dan m\u00e1s informaci\u00f3n al clasificador para mejorar el score.\n\nPara el discretizador ha resultado en `3 particiones`. Lo cual es diferente a como lo vimos en el an\u00e1lisis exploratorio, seg\u00fan las gr\u00e1ficas el resultado m\u00e1s claro que teniamos es de 2 particiones. Pero, al ojo humano hay cosas que se escapan y m\u00e1s con una base de datos grande, que una m\u00e1quina \"ve\" de mejor forma.\n\nPara el estimador KNN ha sacado un resultado de `5 vecinos` y un peso `uniforme`. Es un n\u00famero de vecinos intermedio que permite que las fronteras de decisi\u00f3n queden m\u00e1s definidas. Al tener todos los vecinos el mismo peso, la informaci\u00f3n que debe guardar es mejor. Son unos buenos hiperpar\u00e1metros que sin este algortimo de decisi\u00f3n podiamos haber utilizado por criterio.\n\nTasa de acierto resultante: $0.755 \\pm 0.05$","9a46ff01":"### Fare Limit","4850f14b":"Para los hiperpar\u00e1metros de bagging escogeremos:\n* `base_estimator`: El \u00e1rbol de decisi\u00f3n creado anteriormente.\n* `criterion`: Todas las combinaciones de criterio para el \u00e1rbol (gini, entropy)\n\nTambi\u00e9n vamos a incluir la profundidad m\u00e1xima del \u00e1rbol, ya que bagging funciona mejor con \u00e1rboles de decisi\u00f3n profundos.\n* `max_depth`: Vamos a probar con la profundidad por defecto que es 1, 4 y 10 (muy profundo).","54177a52":"## 1. Carga de datos","0f536595":"Despu\u00e9s de validar AdaBoost, vamos a validar Bagging.","78ab906d":"Utilizaremos para los hiperpar\u00e1metros:\n* `criterion`: Todos los posibles valores de criterion (gini, entropy)\n* `max_depth`: Hasta un valor de profundidad 4, como hemos dicho en el primer estudio no hemos encontrado muchos cortes para separar la variable clase, por tanto, el \u00e1rbol no se ramificar\u00e1 tanto y ramificarse tanto supone que el modelo se sobreajuste.\n* `ccp_alpha`: Los valores de alpha son muy peque\u00f1os y un buen \u00e1rbol intentar\u00e1 tener el m\u00e1s bajo.","d2830cd5":"Viendo los resultados que nos ofrecen los modelos al entrenarlo, no necesitamos m\u00e1s c\u00f3digo. Ya hemos visto que el mejor es regresi\u00f3n log\u00edstica y por debajo queda KNN que tambi\u00e9n nos aporta buenos resultados.\n\nAdem\u00e1s destacar que regresi\u00f3n log\u00edstica ha obtenido muy buenos resultados para ser un modelo tan simple.","9ac07ca2":"### 1.1. Detecci\u00f3n de outlier\n\n\n![oie_384549KoGQkTap.png](attachment:oie_384549KoGQkTap.png)\n\n* Q1 = 1.Quartile 25%\n* Q2 = 2.Quartile 50% (median)\n* Q3 = 3.Quartile 75%\n* IQR = Q3 - Q1\n* Outlier data = (Q1 - 1.5 IQR ) U (Q3 + 1.5 IQR)\n\n","205cdb4a":"Nuestra base de datos por tanto queda muy reducida y con gran valor informativo para la clasificaci\u00f3n.","b351e86b":"Los hiperpar\u00e1metros que vamos a optimizar en este caso son:\n\n* n_neighbors. El n\u00famero de vecinos. Vamos a probar entre 1 y 6 vecinos.\n* weights. La funci\u00f3n de peso que utilizar\u00e1 el algoritmo de los vecinos m\u00e1s cercanos. Probaremos con las funciones de peso uniform y distance.\n* Adem\u00e1s, como se indic\u00f3 anteriormente que siempre vamos a optimizar el n\u00famero de intervalos del discretizador (2 o 3).","8923568c":"Convertimos los titulos en identificadores num\u00e9ricos.","35805000":"Para los \u00e1rboles de decisi\u00f3n vamos a usar la clase DecisionTreeClassifier y la vamos a agregar al pipeline de la misma forma hicimos con los k vecinos. Tambi\u00e9n observaremos el nombre de los hiperpar\u00e1metros, aunque por las conclusiones que sacamos en el k vecinos nos podemos imaginar como ser\u00e1n.","1c39a682":"Vamos a empezar la selecci\u00f3n de modelos con el algoritmo de los vecinos m\u00e1s cercanos.\n\nPara ello igual que con el dataset Diabetes vamos a utilizar la clase KNeighborsClassifier. As\u00ed que vamos a crearlo y lo vamos a a\u00f1adir en el pipeline junto con los otros elementos que definimos en el apartado del pipeline.\n\nDespu\u00e9s vamos a usar el m\u00e9todo get_params().key() que nos devuelve una lista con el nombre de todos los hiperpar\u00e1metros que hay en el pipeline. Esto es importante porque para indicar los hiperpar\u00e1metros a optimizar necesitamos conocer su nombre, y al incluir el modelo k vecinos en el pipeline los nombres de sus hiperpar\u00e1metros cambian.","612c1528":"Dividimos el conjunto de entrenamiento y de test con un *houdout* estratificado, de igual manera que lo hicimos en la pr\u00e1ctica 1.","2ae481f1":"### K-Vecinos","43bf25ec":"# Introducci\u00f3n","ff3af83e":"Ahora vamos a ver la relaci\u00f3n que tiene cada variable predictora con la variable clase de forma individual.\n* Sex - Survived\n* Pclass - Survived\n* Embarked - Survived\n* SibSp - Survived\n* Parch - Survived\n* Age - Survived\n* Fare - Survived","2291d13c":"## 3. Valores perdidos\n","dc583868":"Ahora se van a combinar los datos de test y de entrenamiento para obtener todos los datos juntos. Esta variable no la usaremos para nada, ya que en la libreta ha utilizado todos los datos para hacer el an\u00e1lisis exploratorio, pero esto es incorrecto ya que no podemos utilizar la informaci\u00f3n de los datos de test en el an\u00e1lisis exploratorio.","60d5cf37":"Para la simplificaci\u00f3n a la hora de escribir y como el dataset completo lo tenemos ya guardado. Vamos a cambiar el nombre de `X_train` e `y_train` a `X` e `Y` respectivamente.","47240276":"### Random Forests","bb25805e":"Vamos a empezar a validar los ensembles, para ello vamos a empezar con Boosting, en concreto con el algoritmo AdaBoost, utilizando la clase AdaBoostClassifier. Al igual que con los k vecinos y los \u00e1rboles de decisi\u00f3n, lo vamos a incluir en el pipeline y ver el nombre de sus hiperpar\u00e1metros.","7eae3804":"Lo primero que se va a hacer en la libreta es cargar los datos. En este caso el dataset `titanic` viene dividido en dos conjuntos, uno de entrenamiento y otro de test, as\u00ed que se van a cargar estos dos conjuntos en las variables `train_data` y `test_data`.","c4b07071":"* `n-bins`: Hemos escogido este conjunto para el discretizador porque en el primer estudio, de forma gr\u00e1fica, vimos que el n\u00famero de particiones no iba a ser muy elevado, como mucho veiamos 2 particiones claras.\n* `n_neighbors`: En el conjunto para los vecinos del imputador hemos cogido n\u00famero impares porque reduce los casos de empate y no hemos puesto un n\u00famero elevado de opciones para que el algortimo no tarde mucho en ejecutarse, ya que tiene un alto coste computacional.\n* `weights`: Por \u00faltimo, en el peso de los vecinos, son todas las opciones que existen para ese hiperpar\u00e1metro.","fc1dd650":"Vamos a crear a su vez una dataframe por separado para hacer las mismas transformaciones fuera del pipeline, para ver las gr\u00e1ficas de como quedaria nuestra base de datos y al final sacar una matriz de correlaciones, para ver si se puede eliminar columnas que aportan la misma informaci\u00f3n que otras al haber creado nuevas columnas. Y as\u00ed seguimos la misma estructura que la libreta original.","9125016e":"Primero se realiza un an\u00e1lisis univariado de la variable clase, que es la variable `Survived`, y es una variable categ\u00f3rica, que est\u00e1 formada por dos estados distintos, 0 y 1.","02feb01c":"Tenemos dos empates en la primera posici\u00f3n, vamos a indicar que valores son los mejores y cuales dan igual.\n\nPara el imputador KNN el n\u00famero de vecinos es de `10` y un peso `uniform`. Al igual que ocurria con el modelo de clasificaci\u00f3n de K-Vecinos.\n\nPara el discretizador el n\u00famero de particiones es de `3 particiones`.\n\nPara el estimador el criterio elegido es por `entrop\u00eda` y con el m\u00e1ximo de caracter\u00edsticas da igual si son por raiz cuadrada o logartimo en base 2.\n\nTasa de acierto resultante: $0.746 \\pm 0.052$","bc47db5c":"Comprobamos que en los datos de entrenamiento ya no hay instancias con `Age` a null.","205d9941":"Comprobamos que se ha dividido correctamente.","9bd2328b":"Con el n\u00famero de vecinos de 5 obtenemos la mayor tasa de acierto. Esta no es nada despreciable y queda muy cerca de regresi\u00f3n log\u00edstica.","46936f54":"## Construcci\u00f3n y validaci\u00f3n del modelo final","c767764b":"En este caso los hiperpar\u00e1meetros del \u00e1rbol de decisi\u00f3n ser\u00e1n menores que anteriormente para que el coste computacional baje y viendo los resultados de la optimizaci\u00f3n anterior nos parece lo correcto.\n* `ccp_alpha`: Solo dos conjuntos de alpha, los que hemos visto anteriormente que eran mejor para el \u00e1rbol de decisi\u00f3n.\n* `criterion`: Todos los criterios posibles ya que no son muchos (gini y entropy).\n* `max_depth`: Como nos indica en la libreta de \"Aprendizaje de Modelos\", el algortimo de adaboost funciona mejor con \u00e1rboles de decisi\u00f3n poco profundos (*shallow decision trees*), por su poder de generalizaci\u00f3n. Por esto, el conjunto de profundidad va a ser de n\u00fameros bajos (1 y 2).\n\nPara el resto de hiperpar\u00e1metros:\n* `base_estimator`: Con los hiperpar\u00e1metro anteriores se encontrar\u00e1 el mejor estimador base, siendo este un \u00e1rbol de decisi\u00f3n.\n* `learning_rate`: Por defecto este hiperpar\u00e1metro es de 1. Entonces necesitaremos n\u00fameros cercanos a este. Probaremos con 1 y 0.95.","f1680189":"Mediante la lista de estimadores que hemos creado y una funci\u00f3n en el script vamos a evaular todos los modelos.","7fa588e4":"Tambi\u00e9n podemos crear una variable de si esa persona viajaba sola. Si el valor de familia es 0 viajaba solo y se le pondr\u00e1 un valor de 1. Si estaba acompa\u00f1ado un valor de 0.","43b8c3f5":"Bagging tiene m\u00e1s hiperpar\u00e1metros que AdaBoost. No vamos a probarlos todos ya que tardar\u00eda mucho tiempo y hay que tener en cuenta que si sumamos el tiempo de cada modelo que estamos optimizando, tardar\u00eda demasiado, as\u00ed que para reducir este tiempo solo vamos a optimizar:\n\n* base_estimator. Este ensembler a diferencia de AdaBoost se beneficia de utilizar estimadores complejos, por lo que en lugar de optimizar el hiperpar\u00e1metro base_estimator como hicimos en AdaBoost le vamos a pasar como valor para este hiperpar\u00e1metro un \u00e1rbol de decisi\u00f3n (de la clase DecissionTreeClassifier) con todos los hiperpar\u00e1metros por defecto, para que as\u00ed entrene los \u00e1rboles m\u00e1s complejos posibles, dej\u00e1ndolos ramificar hasta el final. El \u00fanico hiperpar\u00e1metro que vamos a validar de los \u00e1rboles es si utiliza como criterio gini o entrop\u00eca.\n* n_estimators. N\u00famero de estimadores a aprender, que en AdaBoost no optimizamos ya que ten\u00edamos que optimizar algunas hiperpar\u00e1metros del estimador base, pero en este caso como Bagging se aprovecha de estimadores complejos para reducir el error provocado por su varianza, en este caso vamos a probar con 10, 20 y 30 estimadores.","946c47cd":"### Age Limit","64269da1":"Los hiperpar\u00e1metros de este modelo son:\n\n* Learning rate. Mide la aportaci\u00f3n de cada \u00e1rbol, probaremos con 0.05 y 0.1.\n* loss. Es la funci\u00f3n de perdida a optimizar. La dejaremos por defecto, que vale deviance.\n* n_estimators. Dejaremos el valor por defecto que es 100.\n* Los hiperpar\u00e1metros propios de los \u00e1rboles. La diferencia con los anteriores es que el criterio que utiliza para medir la calidad de una partici\u00f3n no es gini y entrop\u00eda, sino que es el mse, friedman_mse y mae. Nosotros probaremos con mse y friedman_mse. Tambi\u00e9n probaremos con \u00e1rboles de profundidad m\u00e1xima de 1 y de 2, ya que al ser un algoritmo de Boosting funciona mejor con \u00e1rboles simples para reducir su error debido al sesgo al agregarlos.","f53e9a2d":"Creamos una funci\u00f3n que realizar\u00e1 esto para incorporarla en el pipeline.","920c0b7e":"Los hiperpar\u00e1metros de RandomForest son los que mismos que los de los \u00e1rboles, adem\u00e1s del n\u00famero de estimadores. Como nos interesa que entrene \u00e1rboles complejos, vamos a dejar sus hiperpar\u00e1metros por defecto.\n\nSolo vamos a utilizar el criterio utilizado, como viene siendo habitual. El n\u00famero de estimadores (50 o 100), y el n\u00famero de caracter\u00edsticas a considerar en cada nodo, que es la t\u00e9cnica que se utilizar\u00e1 para reducir la varianza.","3099e36c":"Estos ser\u00e1n los modelos que se estudian en el notebook original. Primero dividimos los datos test y train en sus diferentes variables. Para tener la variable clase dividida del resto. ","80200b15":"Al igual que on `Age limit` para generar esta variable utiliza los datos de test as\u00ed que comentaremos estos pasos.","7a933852":"#### Cabin\n\nLa mayor\u00eda de los valores de esta variable son nulos, por lo que se eliminar\u00e1 esta variable por completo, en lugar de tratar sus valores.","c0298a2b":"Como \u00faltimo elemento del Pipeline creamos el discretizador que le vamos a pasar. En la pr\u00e1ctica anterior mediante el an\u00e1lisis multivariado observamos que lo mejor era discretizar las variables en dos intervalos, pero como ahora vamos a realizar una validaci\u00f3n cruzada para elegir los mejores hiperpar\u00e1metros de cada algoritmo para este dataset, tambi\u00e9n vamos a incluir que compruebe a discretizar en 3 intervalos, para comprobar que si las conclusiones del an\u00e1lisis multivariado eran correctas.","e4fe734d":"Ahora definimos un FuctionSampler que nos permite eliminar los outlier.","f96c8cb2":"Antes de empezar con los modelos tenemos que definir la validaci\u00f3n cruzada que les vamos a aplicar. En concreto vamos a definir una 5x10 validaci\u00f3n cruzada estratificada al igual que para `Diabetes`.","0ed7c1e6":"Estas son las columnas que eliminaremos.","8c785a4b":"### Adaptative Boosting (AdaBoost)","012d70a7":"Utilizaremos el mismo orden de evaluaci\u00f3n que con el modelo anterior.\n\nEn la primera posici\u00f3n hay muchos empates, el resultado de la tabla nos muestra las 5 primeras filas y todas consiguen la misma puntaci\u00f3n. El algortimo nos ha imprimido por pantalla los mejores hiperpar\u00e1metros resultados, pero esta no es la \u00fanica combinaci\u00f3n posible, si no que es la primera que el algortimo ha entrenado.\n\nComo vemos en el resultado y en las tablas hay muchas combinaciones de mejores resultados: \n\n* Teniendo para el imputador todo el conjunto de hiperpar\u00e1metros. \n* Para el discretizador por 2 y 3 particiones.\n* Para el estimador, todas las profundidades de \u00e1rboles del conjunto que hemos indicado, los diferentes criterios de corte y un alpha de 0.0 y 0.1. Un resultado de alpha peque\u00f1o permite tener resultados de sub\u00e1rboles \u00f3ptimos.\n\nEstos resultados son los mejores que podemos utilizar para este modelo. Aunque los que ha devuelto el algoritmo son los que se utilizar\u00e1n en la evaluaci\u00f3n de todos los modelos.\n\nTasa de acierto resultante: $0.747 \\pm 0.062$\t","bfa3621c":"Random forest, consigue menor *score* que el anterior a\u00fan siendo un modelo m\u00e1s complejo. Por tanto, lo descartaremos por modelos simples.","ff8e40bd":"Como haremos en todos los diferentes modelos, creamos el pipeline y generamos el diccionario para conocer los diferentes hiperpar\u00e1metros de el pipeline creado. Por tanto, no volveremos a comentar este paso.","5962063e":"Los hiperpar\u00e1metros m\u00e1s importantes de AdaBoost son:\n\n* n_estimator. Es el n\u00famero de estimadores, que por defecto es 50. En este caso no lo vamos a validar ya que aumentar\u00eda bastante la complejidad computacional y tardar\u00eda mucho. Lo vamos a dejar en su valor por defecto. Como se nos indic\u00f3 en clase, sabemos que en la teor\u00eda los ensembles funcionan mejor cuantos m\u00e1s estimadores utilicen, aunque en la pr\u00e1ctica sabemos que esto no es siempre cierto, ya que depende de cada problema. As\u00ed que en este caso nos vamos a quedar con 50.\n* base_estimator. Representa al estimador a utilizar por el AdaBoost. Utilizaremos un \u00e1rbol de decisi\u00f3n y validaremos varios de sus hiperpar\u00e1metros. Probaremos con una profundidad m\u00e1xima de 1 y 2, con el criterio gini y entrop\u00eda, y con un ccp_alpha de 0 y 0.1. Como se puede ver todos los \u00e1rboles que vamos a probar son muy simples debido a que el AdaBoost funciona muy bien con modelos muy simples, reduciendo as\u00ed su error debido al sesgo combinando estos modelos simples.\n* learning_rate. Par\u00e1metro del ratio de aprendizaje que nos permite controlar cuanto contribuye cada estimador. Probaremos con 0.95 y 1.0.","c9b85c22":"Vamos a seguir con RandomForests, que al igual que Bagging busca generar \u00e1rboles complejos para reducir su varianza al agregarlos. A diferencia de Bagging aplica otra t\u00e9cnica para reducir a\u00fan m\u00e1s la varianza, y es que aplica aleatorizaci\u00f3n al aprender los \u00e1rboles.\n\nVamos a utilizar la clase RandomForestClassifier y a incluirla en el pipeline.","a8b47a8b":"Podemos ver que las variables `Sex`, `Pclass`, `Fare`y `Embarked` est\u00e1n relacionadas con la variable clase `Survived`.\n\nPara saber si est\u00e1n relacionandas nos fijamos en que el valor que pueden tomar est\u00e1 en el intervalo [-1, +1]. Y sabemos que cuanto m\u00e1s cerca del 0 est\u00e9, significa que no hay relaci\u00f3n entre las dos variables, mientras que cuanto m\u00e1s cerca est\u00e9 del -1 o +1 significa que hay relaci\u00f3n entre esas dos variables.\n* Vemos que la relaci\u00f3n entre `Sex` y `Survived` es de 0.54, que est\u00e1 m\u00e1s pr\u00f3ximo de 1 que de 0.\n* La vairable `Pclass` tiene una relaci\u00f3n de -0.34 con `Survived`, que aunque est\u00e9 m\u00e1s cerca del 0 que del -1, vemos que tiene algo de relaci\u00f3n.\n* Con `Fare` y `Embarked` pasa lo mismo, tienen una relaci\u00f3n con `Survived`del 0.26 y 0.11, y aunque no haya tanta relacion como con `Sex`, si quetienen relaci\u00f3n con la variable clase.\n* Por el contrario las variables`Age`, `SibSp` y `Parch` no tienen casi relaci\u00f3n con la variable clase pues su valor de la matriz de correlaci\u00f3n es menor de 0.1.","31773dcb":"### 5.2.4.\u00a0KNN\n","ca6111d4":"<a id='7'><\/a><br>\n### 2.2. Matriz de correlaci\u00f3n","d4d2a99a":"Vamos a comprobar que se ha separado correctamente. Comenzamos con las variables predictoras:","0e969956":" ### Fare - Survived","4353524b":"### \u00c1rboles de decisi\u00f3n","4397acc8":"Utilizaremos el pipeline discretizado que creamos y analizamos en el estudio de la pr\u00e1ctica anterior. Con las funciones y decisiones ya vistas en la anterior pr\u00e1ctica.","4be42736":"## Carga de datos.","2fa10591":"## 2. An\u00e1lisis de datos"}}