{"cell_type":{"b636cd85":"code","f6c385be":"code","ef9d284a":"code","fd7a139b":"code","23eec770":"code","1b69b546":"code","4baea951":"code","8f07b65f":"code","27303270":"code","a661142f":"code","bb21746f":"code","e70e6f16":"code","0333c8f5":"code","46b5e9f1":"code","e144fb2f":"code","6b58f0ef":"code","b2054eff":"code","641d9fb4":"code","965d13f9":"code","16c03232":"code","6199ab5c":"code","6881d301":"code","238a1d9f":"code","62834efb":"code","88241a99":"code","ab4b5c89":"code","dc777d7d":"code","9cb9dfb3":"code","fe2dc113":"code","c047ea27":"code","2c358947":"code","21e39098":"code","41f8c3b5":"code","454e0c6e":"code","ed9d6537":"code","0bd025ad":"markdown","2ccbaf66":"markdown","7b82cfc0":"markdown","5dd7d92a":"markdown","80c9fbb2":"markdown","62b44587":"markdown","d03193af":"markdown"},"source":{"b636cd85":"import csv\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","f6c385be":"URL_Train ='https:\/\/raw.githubusercontent.com\/cacoderquan\/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset\/master\/train.tsv'\nURL_Test ='https:\/\/raw.githubusercontent.com\/cacoderquan\/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset\/master\/test.tsv'","ef9d284a":"train = pd.read_csv(URL_Train,sep='\\t')\ntest = pd.read_csv(URL_Test,sep='\\t')\n","fd7a139b":"train.head()","23eec770":"train['Sentiment'].value_counts()","1b69b546":"# Put all the fraud class in a separate dataset.\nfraud_df1 = train.loc[train['Sentiment'] == 1].sample(n=7072,random_state=42)\nfraud_df2 = train.loc[train['Sentiment'] == 2].sample(n=7072,random_state=42)\nfraud_df3 = train.loc[train['Sentiment'] == 3].sample(n=7072,random_state=42)\nfraud_df4 = train.loc[train['Sentiment'] == 4].sample(n=7072,random_state=42)\n\n#Randomly select 492 observations from the non-fraud (majority class)\nnon_fraud_df = train[train['Sentiment'] == 0]\n\n# Concatenate both dataframes again\nnormalized_df = pd.concat([non_fraud_df, fraud_df1,fraud_df2,fraud_df3,fraud_df4])\n\n#plot the dataset after the undersampling\n#plt.figure(figsize=(8, 8))\n#sns.countplot('Sentiment', data=normalized_df)\n#plt.title('Balanced Classes')\n#plt.show()\nnormalized_df.head()\nnormalized_df.shape","4baea951":"#from imblearn.over_sampling import SMOTE\n#sm = SMOTE(ratio='minority', random_state=7)\n\n#oversampled_trainX, oversampled_trainY = sm.fit_sample(train.drop('Sentiment', axis=1), train['Sentiment'])\n#oversampled_train = pd.concat([pd.DataFrame(oversampled_trainY), pd.DataFrame(oversampled_trainX)], axis=1)\n#oversampled_train.columns = train.columns\n\n\n#from imblearn.over_sampling import SMOTENC\n#smote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\n#X_resampled, y_resampled = smote_nc.fit_resample(train.drop('Sentiment', axis=1), train['Sentiment'])","8f07b65f":"test.head()\n#test['Phrase'][4]","27303270":"print(train.shape,\"\\n\",test.shape)","a661142f":"print(\"\\t\",train.isnull().values.any(), \"\\n\\t\",\n      test.isnull().values.any()\n     )","bb21746f":"#sanitization\nfullSent_train = normalized_df\n\nfullSent_train.head()\n#fullSent['Phrase'][156]","e70e6f16":"#sanitization\nfullSent_test = test\n\nfullSent_test.head()\n#fullSent['Phrase'][156]\n\nfullSent_test.shape","0333c8f5":"print (len(train.groupby('SentenceId').nunique()),\n      len(test.groupby('SentenceId').nunique())\n      )","46b5e9f1":"senti = train.groupby([\"Sentiment\"]).size()\nsenti = senti \/ senti.sum()\nfig, ax = plt.subplots(figsize=(8,8))\nsns.barplot(senti.keys(), senti.values);","e144fb2f":"StopWords = ENGLISH_STOP_WORDS\nprint(StopWords)","6b58f0ef":"text = \" \".join(review for review in train.Phrase)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","b2054eff":"# Create stopword list:\nstopwords = set(StopWords)\nstopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","641d9fb4":"BOW_Vectorizer = CountVectorizer(strip_accents='unicode',\n                                 stop_words=StopWords,\n                                 ngram_range=(1,3),\n                                 analyzer='word',\n                                 min_df=5,\n                                 max_df=0.5)\n\n#BOW_Vectorizer.fit(list(fullSent['Phrase']))\n\n#create tfidf vectorizer \ntfidf_vectorizer = TfidfVectorizer(min_df=5,\n                                 max_df=5,\n                                  analyzer='word',\n                                  strip_accents='unicode',\n                                  ngram_range=(1,3),\n                                  sublinear_tf=True,\n                                  smooth_idf=True,\n                                  use_idf=True,\n                                  stop_words=StopWords)\n\ntfidf_vectorizer.fit(list(fullSent_train['Phrase']))","965d13f9":"tfidf_vectorizer.fit(list(fullSent_test['Phrase']))","16c03232":"X_train = fullSent_train['Phrase']\nY_train = fullSent_train['Sentiment']\nX_test = fullSent_test['Phrase']\nX_train.shape, Y_train.shape, X_test.shape","6199ab5c":"#build train and test datasets\n#phrase = fullSent['Phrase']\n#sentiment = fullSent['Sentiment']\n#phrase[0], sentiment[0]","6881d301":"#X_train,X_test,Y_train,Y_test = train_test_split(phrase,sentiment,test_size=0.3,random_state=4)\n\n#X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n","238a1d9f":"#calling BoW Model and transforming to spase matrix\ntrain_bow=tfidf_vectorizer.transform(X_train)\ntest_bow=tfidf_vectorizer.transform(X_test)\ntrain_bow.shape[1]\n","62834efb":"bow_fea_vec_train = pd.DataFrame(train_bow.toarray(), columns = tfidf_vectorizer.get_feature_names())\nbow_fea_vec_train.head(5)\n\nbow_fea_vec_test = pd.DataFrame(test_bow.toarray(), columns = tfidf_vectorizer.get_feature_names())\nbow_fea_vec_test.head(5)","88241a99":"from keras import backend as K\ndef recall_m(y_true, y_pred):\n  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n  recall = true_positives \/ (possible_positives + K.epsilon())\n  return recall\n\ndef precision_m(y_true, y_pred):\n  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n  predicted_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n  precision = true_positives \/ (predicted_positives + K.epsilon())\n  return precision\n\ndef f1_m(y_true, y_pred):\n  precision = precision_m(y_true, y_pred)\n  recall = recall_m(y_true, y_pred)\n  return 2*((precision*recall)\/(precision+recall+K.epsilon()))","ab4b5c89":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten\nfrom keras.layers import Activation, Conv1D, GlobalMaxPooling1D\nfrom keras import optimizers\nimport keras","dc777d7d":"lr = 1e-3\nbatch_size = 128\nnum_epochs = 10\ndecay = 1e-4\nmode = \"reg\"\nn_class = 5 #5","9cb9dfb3":"fea_train_dim = bow_fea_vec_train.shape[1]\nprint(fea_train_dim, n_class)\n\nX_train = bow_fea_vec_train.values.reshape((bow_fea_vec_train.shape[0], bow_fea_vec_train.shape[1], 1))\nX_train.shape\n\n\nfea_test_dim = bow_fea_vec_test.shape[1]\nprint(fea_test_dim, n_class)\n\nX_test = bow_fea_vec_test.values.reshape((bow_fea_vec_test.shape[0], bow_fea_vec_test.shape[1], 1))\nX_test.shape\n#Y_train\n\nY_train = keras.utils.to_categorical(Y_train, num_classes=None, dtype='float32')\n#Y_test = keras.utils.to_categorical(X_test)\nX_test.shape","fe2dc113":"def baseline_cnn_model(fea_matrix, n_class, mode, compiler):\n  #create model\n  model = Sequential()\n  model.add(Conv1D(filters=64, kernel_size = 3, activation = 'relu',\n                  input_shape=(fea_matrix.shape[1], fea_matrix.shape[2])))\n  model.add(MaxPooling1D(pool_size = 2))\n  model.add(Conv1D(filters=128, kernel_size = 3, activation = 'relu'))\n  model.add(MaxPooling1D(pool_size=2))\n  model.add(Dropout(0.25))\n  model.add(Flatten())\n  model.add(Dense(128, activation='relu'))\n  model.add(Dropout(0.5))\n  model.add(Activation('relu'))\n  model.add(Dense(n_class))\n  if n_class==1 and mode == \"cla\":\n    model.add(Activation('softmax'))  \n    #comoile the model\n    model.compile(optimizer=compiler, loss = 'sparse_categorical_crossentropy',\n                 metrics=['acc', f1_m, precision_m, recall_m])\n  else:\n    model.add(Activation('sigmoid'))\n    # compile the model\n    model.compile(optimizer=compiler, loss = 'binary_crossentropy',\n            metrics=['acc', f1_m, precision_m, recall_m])\n  return model\n  ","c047ea27":"adm = optimizers.Adam(lr = lr, decay = decay)\nsgd = optimizers.SGD(lr = lr, nesterov = True, momentum = 0.7, decay = decay)\nNadam = optimizers.Nadam(lr = lr, beta_1=0.9, beta_2=0.999, epsilon = 1e-08)\nmodel = baseline_cnn_model(X_train, n_class, mode, Nadam)\nmodel.summary()","2c358947":"\nacc_loss = model.fit(X_train, Y_train, batch_size = batch_size, \n          epochs = num_epochs, verbose=1, validation_split = 0.2)","21e39098":"Xnew=model.predict_classes(X_test)\nXnew","41f8c3b5":"submission = pd.DataFrame({'PhraseId':fullSent_test['PhraseId'],'Sentiment':Xnew})\n#fullSent_test['Phrase']\nsubmission.head()","454e0c6e":"fullSent_test.shape","ed9d6537":"submission.to_csv('submission.csv', index = False)","0bd025ad":"**Generating Word Cloud**","2ccbaf66":"**Stop Words**","7b82cfc0":"**Convolutional Neural Network**\nClassification techniques are widely used to classify data\namong various classes. There are many algorithms used for\nSentiment classification. There are mainly two types of Sentiment\nclassification algorithms Machine Learning Approach\nand Lexicon-based Approach. Fig 2 Clearly explains the Sentiment\nclassification techniques based on Machine Learning\nApproach.\nAlthough the text documents can not directly be processed\ninto their original form using the classification techniques and\nlearning algorithms, it is because of those techniques expects\nnumerical vectors rather text documents with a variable size.\nSo, while doing the preprocessing, the text data are to be\ntransformed into numeric data.","5dd7d92a":"**Bag of Words Vectorizer**","80c9fbb2":"**Importing Packages**","62b44587":"**Dataset Loading**","d03193af":"**Sparse Matrix**"}}