{"cell_type":{"0e5c89b1":"code","73c956c6":"code","c545071b":"code","4798c11c":"code","bff09238":"code","22be53f5":"code","dbc1cfce":"code","8d7c956f":"code","f83771d0":"code","a08a8a75":"code","263bbc47":"code","608486e6":"code","48f9db18":"code","5a9ef05e":"code","92cd5e1b":"code","d9527d21":"markdown","73953c68":"markdown","f7038e22":"markdown","ff8a3e9a":"markdown","6a3949fe":"markdown","e96ca6ff":"markdown","f56234b6":"markdown"},"source":{"0e5c89b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73c956c6":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom sklearn.cluster import KMeans","c545071b":"customer_data = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","4798c11c":"# first 5 rows in the dataframe\ncustomer_data.head()","bff09238":"# finding the number of rows and columns\ncustomer_data.shape\n\n# data available for 200 customers","22be53f5":"# getting some informations about the dataset\ncustomer_data.info()","dbc1cfce":"# checking for missing values\ncustomer_data.isnull().sum()","8d7c956f":"X = customer_data.iloc[:,[3,4]].values\n# x-axis : annual income \n# y-axis : spending score","f83771d0":"# finding wcss value for different number of clusters\n# cost function --> within cluster the sum of square distances of each data point from the centroid of that cluster\n\nwcss = []\n\nfor i in range(1,11):\n  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n  # \u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence\n\n  kmeans.fit(X)\n\n  wcss.append(kmeans.inertia_)\n  # inertia_float : Sum of squared distances of samples to their closest cluster center.","a08a8a75":"# plot an elbow graph\n\nsns.set()\nplt.plot(range(1,11), wcss)\nplt.title('The Elbow Point Graph')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","263bbc47":"kmeans = KMeans(n_clusters=5, init='k-means++', random_state=0) #best-fit model\n\n# return a label for each data point based on their cluster\nY = kmeans.fit_predict(X)\n\nprint(Y)","608486e6":"# Cluster Centers\nkmeans.cluster_centers_","48f9db18":"# plotting all the clusters and their Centroids\n# x-axis : annual income \n# y-axis : spending score\n\nplt.figure(figsize=(8,8))\nplt.scatter(X[Y==0,0], X[Y==0,1], s=50, c='green', label='Cluster 1')\nplt.scatter(X[Y==1,0], X[Y==1,1], s=50, c='red', label='Cluster 2')\nplt.scatter(X[Y==2,0], X[Y==2,1], s=50, c='yellow', label='Cluster 3')\nplt.scatter(X[Y==3,0], X[Y==3,1], s=50, c='violet', label='Cluster 4')\nplt.scatter(X[Y==4,0], X[Y==4,1], s=50, c='blue', label='Cluster 5')\n\n# plot the centroids\n  # cluster_centers_ndarray of shape (n_clusters, n_features) : Coordinates of cluster centers. \nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, c='cyan', label='Centroids')\n\nplt.title('Customer Groups')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.show()","5a9ef05e":"# Metrics for clustering algorithms\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n# Based on the elbow plot we choose various candidates for number of clusters\nclusters = [4,5,6,7,8,9]\n\n\n\n\nfor cluster in clusters:\n  # creating a sub-plot with 2 columns\n  fig, (ax1, ax2) = plt.subplots(1,2) # 1 row, 2 columns\n  fig.set_size_inches(18, 7)\n\n  # silhouette coefficient can range from -1 to 1\n  # -1 being worst, 1 being the best\n  ax1.set_xlim([-0.2,1])\n\n  # we need to insert blank space between silhouette plots, \n  ax1.set_ylim([0, len(X) + (cluster + 1) * 10])\n\n  km_cluster = KMeans(n_clusters=cluster, random_state= 1)\n  cluster_labels = km_cluster.fit_predict(X)\n\n\n  # \"silhoutte_score\" gives the average value for all the samples, a perspective into density and separation of clusters formed\n\n  silhoutte_avg = silhouette_score(X, cluster_labels)\n  print(\"For n_clusters = \", cluster, \" The average silhouette_score is :\", silhoutte_avg)\n\n\n  # Compute the silhouette scores for each sample\n  sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n\n\n  # plotting silhoutte graph\n  y_lower = 10\n  for i in range(cluster):\n      # Aggregate the silhouette scores for samples belonging to\n      # cluster i, and sort them\n      ith_cluster_silhouette_values = \\\n          sample_silhouette_values[cluster_labels == i]\n\n      ith_cluster_silhouette_values.sort()\n\n      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n      y_upper = y_lower + size_cluster_i\n\n      color = cm.nipy_spectral(float(i) \/ cluster)\n      ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n      # Label the silhouette plots with their cluster numbers at the middle\n      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n      # Compute the new y_lower for next plot\n      y_lower = y_upper + 10  # 10 for the 0 samples\n\n  ax1.set_title(\"The silhouette plot for the various clusters.\")\n  ax1.set_xlabel(\"The silhouette coefficient values\")\n  ax1.set_ylabel(\"Cluster label\")\n\n  # The vertical line for average silhouette score of all the values\n  ax1.axvline(x=silhoutte_avg, color=\"red\", linestyle=\"--\")\n\n  ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n  ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n  # 2nd Plot showing the actual clusters formed\n  colors = cm.nipy_spectral(cluster_labels.astype(float) \/ cluster)\n  ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n              c=colors, edgecolor='k')\n\n  # Labeling the clusters\n  centers = km_cluster.cluster_centers_\n\n\n  # Draw white circles at cluster centers\n  ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n              c=\"white\", alpha=1, s=200, edgecolor='k')\n\n  for i, c in enumerate(centers):\n      ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                  s=50, edgecolor='k')\n\n  ax2.set_title(\"The visualization of the clustered data.\")\n  ax2.set_xlabel(\"Feature space for the 1st feature\")\n  ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n  plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                \"with n_clusters = %d\" % cluster),\n                fontsize=14, fontweight='bold')\n\nplt.show()","92cd5e1b":"# Note: a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n# Observations: clusters = 5 has got the highest silhoutte score","d9527d21":"## Training the k-Means Clustering Model using k=5","73953c68":"## Choosing the number of clusters\n### WCSS  ->  Within Clusters Sum of Squares","f7038e22":"## Data Collection & Analysis","ff8a3e9a":"## Missing Values","6a3949fe":"## Visualizing all the Clusters","e96ca6ff":"### Measuring performance of clusteres - silhouette_score","f56234b6":"### Feature Selection\n#### Choosing the Annual Income Column & Spending Score column"}}