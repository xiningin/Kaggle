{"cell_type":{"018f7d42":"code","62b62b26":"code","9adb536e":"code","47cfd60d":"code","5ce282e3":"code","deca5785":"code","5da1f15b":"code","e567b886":"code","99589f47":"code","e231834b":"code","4b8c4a1e":"code","d4e4e2c2":"code","14ab21f4":"code","904ec81f":"code","3e90d27a":"code","923aaad5":"code","17bc6dcf":"code","b6c965a8":"code","39b0e557":"code","0953dffb":"code","5ef3cf13":"code","c2de3d94":"code","d649e6ad":"code","6ae9aa5a":"code","f8f52455":"code","7785e57d":"code","ec0fcb06":"code","328e16ac":"code","c3e5e8da":"code","b2dcfb04":"code","e3490123":"code","a86acd52":"code","e6099c5f":"code","dc03d61a":"code","ba276694":"code","09373970":"code","d2fff33e":"code","0c4b9a37":"code","e9d0cab1":"code","736ca978":"code","1064f639":"code","92c9812d":"markdown","8534ed8f":"markdown","1b06a6e2":"markdown","7358e224":"markdown","f37a117d":"markdown","14dc3f09":"markdown","3856a7d3":"markdown","e66ed682":"markdown","0b297427":"markdown","06d1cd83":"markdown","d6ea163e":"markdown","e32274bc":"markdown","6d17dce3":"markdown","6d9c2987":"markdown","a9a28c9a":"markdown","944eaeb3":"markdown","9824ec96":"markdown","666978c1":"markdown","89d2d2e1":"markdown","746b7c62":"markdown","fc2e0c78":"markdown","ae74d7ea":"markdown","3c1694c5":"markdown","a3cb30dd":"markdown","63afb1ee":"markdown","93a6df2d":"markdown","9cf296ca":"markdown","e8ac3c53":"markdown","bb220863":"markdown","11ab78da":"markdown","7cace008":"markdown","92a78849":"markdown","ac6d5ee1":"markdown","8bbc17a5":"markdown","f4abd974":"markdown","4a2ca733":"markdown","40a30688":"markdown","6594c0a6":"markdown","7c3e3282":"markdown","cd103a25":"markdown","edae6d48":"markdown","4e75b167":"markdown","362ac1e7":"markdown","0af5b368":"markdown"},"source":{"018f7d42":"# library imports\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.style.use('ggplot')\nimport matplotlib.pyplot as plt\nwidth = 0.75\nplt.rcParams['xtick.labelsize'] = 15\nplt.rcParams['ytick.labelsize'] = 15\nplt.rcParams[\"font.weight\"] = \"bold\"\nplt.rcParams[\"axes.labelweight\"] = \"bold\"\nplt.axis('off')\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport scattertext as st\nimport spacy\nimport spacy_cld\n\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\nfrom collections import Counter\nfrom tqdm import tqdm_notebook as tqdm  # cool progress bars\ntqdm().pandas()  # Enable tracking of progress in dataframe `apply` calls","62b62b26":"tweets = pd.read_csv('..\/input\/twcs\/twcs.csv',encoding='utf-8')\nprint(tweets.shape)\ntweets.head()","9adb536e":"first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n\nQnR = pd.merge(first_inbound, tweets, left_on='tweet_id', \n                                  right_on='in_response_to_tweet_id')\n\n# Filter to only outbound replies (from companies)\nQnR = QnR[QnR.inbound_y ^ True]\nprint(f'Data shape: {QnR.shape}')\nQnR.head()","47cfd60d":"# removing anonymized screen names \ndef sn_replace(match):\n    _sn = match.group(2).lower()\n    if not _sn.isnumeric():\n        # This is a company screen name\n        return match.group(1) + match.group(2)\n    return ''\n\nsn_re = re.compile('(\\W@|^@)([a-zA-Z0-9_]+)')\nprint(\"Removing anonymized screen names in X...\")\nQnR[\"text_x\"] = QnR.text_x.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))\nprint(\"Removing anonymized screen names in Y...\")\nQnR[\"text_y\"] = QnR.text_y.progress_apply(lambda txt: sn_re.sub(sn_replace, txt))","5ce282e3":"#making sure the dataframe contains only the needed columns\nQnR = QnR[[\"author_id_x\",\"created_at_x\",\"text_x\",\"author_id_y\",\"created_at_y\",\"text_y\"]]\nQnR.head(5)","deca5785":"count = QnR.groupby(\"author_id_y\")[\"text_x\"].count()\nc = count[count>15000].plot(kind='barh',figsize=(10, 8), color='#619CFF', zorder=2, width=width,)\nc.set_ylabel('')\nplt.show()","5da1f15b":"amazonQnR = QnR[QnR[\"author_id_y\"]==\"AmazonHelp\"]","e567b886":"amazonQnR.tail(10)[\"text_x\"]","99589f47":"# amazonQnR[\"text_x\"] = amazonQnR[\"text_x\"].str.encode(\"utf-8\")\n# amazonQnR[\"text_x\"] = amazonQnR[\"text_x\"].apply(str)","e231834b":"nlp_cld = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\nlanguage_detector = spacy_cld.LanguageDetector()\nnlp_cld.add_pipe(language_detector)","4b8c4a1e":"doc = nlp_cld(amazonQnR.iloc[4][\"text_x\"])\nprint(doc)\nprint(doc._.languages)  \nprint(doc._.language_scores)","d4e4e2c2":"mask = []\ntry:\n    for i,doc in tqdm(enumerate(nlp_cld.pipe(amazonQnR[\"text_x\"], batch_size=512))):\n            if 'en' not in doc._.languages or len(doc._.languages) != 1:\n                mask.append(False)\n            else:\n                mask.append(True)\nexcept Exception:\n    print(\"excepted \")","14ab21f4":"amazonQnR = amazonQnR[mask]\n# sample a random fraction to visually ensure that we have only English tweets\namazonQnR.sample(frac=0.0002)    ","904ec81f":"amazonQnR.tail(10)[\"text_x\"]","3e90d27a":"nlp = spacy.load(\"en_core_web_lg\",disable_pipes=[\"tagger\"])\n\nfrom spacymoji import Emoji\nemoji = Emoji(nlp)\nnlp.add_pipe(emoji, first=True)","923aaad5":"print(nlp.pipe_names)","17bc6dcf":"emojis = []\nfor doc in tqdm(nlp.pipe(amazonQnR[\"text_x\"], batch_size=512)):\n    if doc._.has_emoji:\n        for e in doc._.emoji:\n            emojis.extend(e[0])","b6c965a8":"eCount = Counter(emojis)\neCount.most_common(10)","39b0e557":"response_emojis = []\nfor doc in tqdm(nlp.pipe(amazonQnR[\"text_y\"], batch_size=512)):\n    elist = []\n    if doc._.has_emoji:\n        for e in doc._.emoji:\n            elist.append(e[0])\n    response_emojis.append(elist)","0953dffb":"Counter([item for sublist in response_emojis for item in sublist]).most_common(10)","5ef3cf13":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsent_analyser = SentimentIntensityAnalyzer()\npositive_text = \"love this phone! its the best one I've owned over the years\"\nnegative_text = \"what sort of company makes such products? this phone hangs up all the time and is totally useless\"\nprint(\"positive_text sentiment : \",sent_analyser.polarity_scores(positive_text)[\"compound\"])\nprint(\"negative_text sentiment : \",sent_analyser.polarity_scores(negative_text)[\"compound\"])","c2de3d94":"def sentiment(text):\n    return (sent_analyser.polarity_scores(text)[\"compound\"] + TextBlob(text).sentiment.polarity)\/2\namazonQnR[\"text_x_sentiment\"] = amazonQnR[\"text_x\"].apply(sentiment)","d649e6ad":"response_emojis_for_positive_queries = []\nresponse_emojis_for_negative_queries = []\nfor i,sentiment in enumerate(amazonQnR[\"text_x_sentiment\"]):\n    if sentiment > 0.0:\n        response_emojis_for_positive_queries.extend(response_emojis[i])\n    elif sentiment < 0.0:\n        response_emojis_for_negative_queries.extend(response_emojis[i])","6ae9aa5a":"amazonQnR[amazonQnR[\"text_x_sentiment\"]>0].head()","f8f52455":"Counter(response_emojis_for_positive_queries).most_common(10)","7785e57d":"Counter(response_emojis_for_negative_queries).most_common(10)","ec0fcb06":"count = QnR.groupby(\"author_id_y\")[\"text_x\"].count()\nc = count[count>15000].plot(kind='barh',figsize=(10, 8), color='#619CFF', zorder=2, width=width,)\nc.set_ylabel('')\nplt.show()","328e16ac":"airlinesQnR = QnR[(QnR[\"author_id_y\"]==\"AmericanAir\")|(QnR[\"author_id_y\"]==\"British_Airways\")]\nairlinesQnR.head(4)","c3e5e8da":"airlinesQnR[\"text_y\"] = airlinesQnR[\"text_y\"].str.lower()  \nstop = stopwords.words('english')\nbig_regex = re.compile(' | '.join(stop))\nairlinesQnR[\"text_y\"].progress_apply(lambda x: big_regex.sub(\" \",x))","b2dcfb04":"import scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\nairlinesQnR['parsed'] = airlinesQnR.text_y.progress_apply(nlp)","e3490123":"corpus = st.CorpusFromParsedDocuments(airlinesQnR,\n                             category_col='author_id_y',\n                             parsed_col='parsed').build()","a86acd52":"html = st.produce_scattertext_explorer(corpus,\n          category='British_Airways',\n          category_name='British Airways',\n          not_category_name='American Airlines',\n          width_in_pixels=600,\n          minimum_term_frequency=10,\n          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n          )","e6099c5f":"# uncomment this cell to load the interactive scattertext visualisation\n# filename = \"americanAir-vs-britishAirways.html\"\n# open(filename, 'wb').write(html.encode('utf-8'))\n# IFrame(src=filename, width = 800, height=700)","dc03d61a":"feat_builder = st.FeatsFromOnlyEmpath()\nempath_corpus = st.CorpusFromParsedDocuments(airlinesQnR,\n                                              category_col='author_id_y',\n                                              feats_from_spacy_doc=feat_builder,\n                                              parsed_col='parsed').build()","ba276694":"html = st.produce_scattertext_explorer(empath_corpus,\n                                        category='British_Airways',\n                                        category_name='British Airways',\n                                        not_category_name='American Airlines',\n                                        width_in_pixels=700,\n                                        metadata=airlinesQnR['author_id_y'],\n                                        use_non_text_features=True,\n                                        use_full_doc=True,\n                                        topic_model_term_lists=feat_builder.get_top_model_term_lists())","09373970":"# uncomment this cell to load the interactive scattertext visualisation\n# filename = \"empath-BA-vs-AA.html\"\n# open(filename, 'wb').write(html.encode('utf-8'))\n# IFrame(src=filename, width = 900, height=700)","d2fff33e":"corpus = (st.CorpusFromParsedDocuments(airlinesQnR,\n                             category_col='author_id_y',\n                             parsed_col='parsed').build().get_stoplisted_unigram_corpus())","0c4b9a37":"target_term = 'delay'\nhtml = st.word_similarity_explorer(corpus,\n                                   category='British_Airways',\n                                   category_name='British Airways',\n                                   not_category_name='American Airlines',\n                                   target_term=target_term,\n                                   minimum_term_frequency=5,\n                                   width_in_pixels=800)","e9d0cab1":"# file_name = 'similarity.html'\n# open(file_name, 'wb').write(html.encode('utf-8'))\n# IFrame(src=file_name, width = 1000, height=700)","736ca978":"html = st.produce_projection_explorer(corpus,\n                                   category='British_Airways',\n                                   category_name='British Airways',\n                                   not_category_name='American Airlines',\n                                   width_in_pixels=800)","1064f639":"# file_name = 'projection.html'\n# open(file_name, 'wb').write(html.encode('utf-8'))\n# IFrame(src=file_name, width = 1200, height=700)","92c9812d":"<a id=\"embedding\"><\/a>","8534ed8f":"## Word Embedding projection plots using Scattertext\n\nScatter text can also be used to visualize word embedding projections. We'll explore two such plots \n\n** 1. Word Association with a specific term**\n\nThe first shows word associations to a specific term using Spacy's pretrained embedding vectors. Let's use this to see the terms most associated with the term `delay` since a lot of customer tweets to flight providers will be due to flight delays.","1b06a6e2":"# Customer Support meets Spacy Universe\n\nThis analysis delves into a large dataset of about 3 million tweets focusing around Customer Support queries and responses. Let's see what we can find and explore what the wonderful Spacy library and its ecosystem has to offer, documenting the trial and errors, ideas and possible further analysis along the way.\n\n**Contents : **\n\n*  [Data Wrangling](#data-wrangling)\n*  [Visualisation](#visual)\n*  [Houston! We have a problem!](#problem)\n*  [Emojis with spacymoji](#emojis)\n*  [Sentimental Emojis!](#senti)\n*  [Scattertext - a hidden gem in the Spacy Universe](#scatter)\n*  [Word Embedding projection plots using Scattertext](#embedding)","7358e224":"**One more look at the number of tweets per company**","f37a117d":"The above code loads up a spacy pipeline disabling the tagger and ner, since we don't require it right now. Then the `spacy_cld` `LanguageDetector` pipe is added for detecting languages in the text data we have.\n\n\nThe following code is an example of how `spacy_cld` adds `languages` and `language_scores`","14dc3f09":"![Imgur](https:\/\/i.imgur.com\/Gxa086C.png)","3856a7d3":"Let's make a mask which will allow us to select the values","e66ed682":"My hypothesis for this result is that responses from customer support teams often do not put emojis when responding to Angry queries. They tend to respond to positive customer tweets with the smiling \ud83d\ude0aand cheerful \ud83d\ude01emojis. Need to validate this hypothesis using sentiment analysis and checking for common emojis in response to positve and negative queries\n\nLet's give it a shot!","0b297427":"While we're at it, Spacy has pretrained models for entity extraction, let's use that as well and see which product gets most queries.\nThe pipes in spacy can be seen using the following code :","06d1cd83":"Problem solved, we have removed all the text which is in a language other than English\n\nNow back to the analysis, where were we? \nPeeking into queries to @AmazonHelp","d6ea163e":"## Scattertext - a hidden gem in the Spacy Universe\n\n[Scattertext](https:\/\/spacy.io\/universe\/?id=scattertext) is an excellent exploratory text analysis tool, which I would never have stumbled on if not for spacy!\nIt allows cool visualisations differentiating between the terms used by different documents using an interactive scatter plot.\n\nLet's build one to compare tweet responses by American Airlines vs British Airways :","e32274bc":"## Houston, we have a problem!\n\nToo many different languages! It was assumed that the dataset contained only English tweets, but as seen above, that is definitely not the case.\n\nFortunately, the solution exists within the [spacy universe](https:\/\/spacy.io\/universe\/) itself! (PUN totally intended)\n\nSince version 2, spacy has support for custom \"extensions\" and the **[spacy-cld](https:\/\/github.com\/nickdavidhaynes\/spacy-cld) ** extension which wraps the Python wrapper of Google's `Compact Language Detector 2` is just what we require.\n\nPluggable interfaces are so awesome!","6d17dce3":"As expected, the red angry face \ud83d\ude21, the confused \ud83e\udd14and the annoyed \ud83d\ude44emojis are two of the most commonly used one by customers who are clearly venting their anger, confusion and annoyance at something that went wrong and requires support.\n\nIts a little surprising to find the laughter \ud83d\ude02emoji being this common, but that probably indicates that twitter users are quite sarcastic?","6d9c2987":"Looks like our hypothesis is not quite true, emoji's like \ud83d\ude0aand \ud83d\ude01are used by support teams for both positive and negative customer queries. However, there is a skew in the number of tweets for each, so we need a more representative dataset and a better sentiment model to study this in depth. We'll defer that for a different session.","a9a28c9a":"**Emojis in responses for positive queries**","944eaeb3":"The last 10 queries to @AmazonHelp","9824ec96":"A lot more can be done using this dataset and I hope you've got some good ideas to extend this work. This kernel depends on a lot of custom packages, so if you fork and build on top of this kernel, make sure to install the following custom pip packages from the Kaggle Kernel settings interface : vaderSentiment, spacymoji, spacy-cld, scattertext and empath.\n\nHope this inspired you to try out the different Spacy Extensions available to your own text data.\n\nThank you :)","666978c1":"## Visualising Empath topics and categories\n\n[Empath](http:\/\/empath.stanford.edu\/) is a lexical analysis research project at Stanford for Understanding Topic Signals in Large-Scale Text. Read more about it from this [research paper.](https:\/\/hci.stanford.edu\/publications\/2016\/ethan\/empath-chi-2016.pdf) \n\nScattertext has an option for visualising Empath topics and categories, lets use that for our American Airlines vs British Airways analysis","89d2d2e1":"<a id='data-wrangling'><\/a>","746b7c62":"![Imgur](https:\/\/i.imgur.com\/x4AM7mg.png)","fc2e0c78":"<a id='senti'><\/a>","ae74d7ea":"At the top right corner we see the most commonly associated words with the term `delay` are `late` , `team`, `info` and `patience`. If you click on the interactive version, the list of tweets with the terms can be explored.","3c1694c5":"**2.  T-SNE style word embedding projection**","a3cb30dd":"## Sentimental Emojis\n\nThis is where we hit a wall in the Spacy Universe, spacy does not have a pretrained model for sentiment analysis, the reason being non availability of a good public dataset as mentioned by spacy creator Honnibal [ here](https:\/\/github.com\/explosion\/spaCy\/issues\/765#issuecomment-372058333)\n\nHere's another idea for a spacy extension - a quick, rudimentary sentiment analysis extension. If you feel inspired, go ahead and make one, while I put it on my list as well.\n\nSince we just want a quick analysis just to see whether the hypothesis is valid and are not very concerned with accuracy, lets use [vaderSentiment](https:\/\/textblob.readthedocs.io\/en\/dev\/)","63afb1ee":"There's a good number of tweets for AmericanAir, British_Airways, Delta and SouthwestAir. Since all these companies provide the same service i.e. flights, it would be interesting to do a comparison\n\nLet's first take a look specifically at American Air and British Airways","93a6df2d":"* Very interesting to see all the different names of people clustered on the top right corner. \n* At the bottom, slightly towards right side, we see a cluster for places - heathrow, glassgow\n* Now, bottom, slightly towards left, we can see terms related to time clustered together - later, close, min, current, hours, prior, daily\n* In the top slightly towards left , we see a cluster that is related to passenger information - address, surname etc\n\nBe sure to try the interactive version by running the kernel and mouse over the points to find other clusters, that's where all the fun of Scattertext is!\n\nScattertext has many more interesting visualisations like this, do check out its [documentation](https:\/\/github.com\/JasonKessler\/scattertext).","9cf296ca":"<a id='visual'><\/a>","e8ac3c53":"Since the D3 visualisation done by scattertext is real awesome, it takes up quite some time to load while viewing as a kaggle kernel and probably take more than 5 minutes to load properly. Check out version 3 or 4 of this kernel if you want to experiment ;) \nSharing the screenshot image of the plot below.\n\nMake sure you try this kernel out, uncomment the following cell and run it to interact with the plot. It is really worth it!","bb220863":"<a id='emojis'><\/a>","11ab78da":"Looks like the dataset has a lot of tweets for AmazonHelp. Lets take a closer look at queries to @AmazonHelp and their responses","7cace008":"On the top right corner, clearly `air_travel` is the most frequent topic for both British Airways and American Airlines since of-course they are flight providers. Topics `urban`, `technology` and `fun` are frequent for British Airways and topics `magic`, `cheerfulness` and `affection` are frequent for American Airlines.\n\nTry clicking on different topics to see which tweets contribute towards the different topics. (Try it out by forking the kernel and running the commented out cell above)","92a78849":"![Imgur](https:\/\/i.imgur.com\/MX7y2Dy.png)","ac6d5ee1":" **Emoji Count**","8bbc17a5":"## Emojis with spacymoji\n\nInterestingly, during the process of cleaning our data, I saw that some queries and replies had emoji's in them. Wouldn't it be nice to know which smiley is most used by Customers in their queries and the Companies in their responses?\n\nShout out to Spacy Universe again, with its cool extension named [spacymoji](https:\/\/github.com\/ines\/spacymoji\/)","f4abd974":"Now let's get the number of tweets in the dataset for each company and plot the data where number of tweets > 15000","4a2ca733":"**Top Emojis in query tweets by Customers**","40a30688":"This visualisation can be overwhelming in the beginning, but on a closer look, one can find typical American Airlines references like `aateam`, `aadventures`, `faabulous` and `aadvantage` on the bottom right corner. American Airlines love to use the `double 'a'` reference quite a lot!\nBritish Airways responses,  on the opposite corner in blue color, seem to be very concerned about the `booking reference`\n\nLooks like American Airlines are fond of `kudos`! Try clicking on `kudos` in the chart to see some sample tweets which you can scroll through in the interactive plot. (Try it out by forking the kernel and running the commented out cell above)","6594c0a6":"**Top Emojis in response tweets by Customer Support teams**","7c3e3282":"First lets get our data and process it to our needs","cd103a25":"<a id='problem'><\/a>","edae6d48":"<a id='scatter'><\/a>","4e75b167":"## Data Wrangling\n\nThis is a very interesting tweet data set, about 3 million tweets, and we have information on the author of the tweets and whether the tweet was a query or a response (the \"inbound\" column). If the tweet was a query, the response_tweet_id gives the response made by the support team.\n\nIt would be interesting to modify this dataframe to get query - response pairs in every row.\nThe following code, to do just what we want, was pulled from [this kernel](https:\/\/www.kaggle.com\/soaxelbrooke\/first-inbound-and-response-tweets)","362ac1e7":"![Imgur](https:\/\/i.imgur.com\/uBmPKwE.png)","0af5b368":" **Emojis in responses for negative queries**"}}