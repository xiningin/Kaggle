{"cell_type":{"a8cdcc79":"code","34cc84b1":"code","4610e21e":"code","d2be9f23":"code","14681399":"code","7a46d9ff":"code","4a14f105":"code","215487db":"code","c0d57853":"code","f4b18fe5":"code","7b51ed24":"code","8cad3f44":"code","6ef00f13":"code","40112285":"code","9773d0c5":"code","bc2c58d1":"code","241d4395":"code","b59a70cf":"code","72c05ebe":"code","c9e94bf4":"code","a959d983":"code","beadfd52":"code","ecc5e22b":"markdown","09386200":"markdown","64f82981":"markdown","ae64d91b":"markdown","8a31307a":"markdown","c826494b":"markdown","6fb6f539":"markdown","ed5959f5":"markdown","8e1fbec3":"markdown","d140b466":"markdown","15b05e05":"markdown","c7720525":"markdown","e1eab1d6":"markdown","a4531915":"markdown","0a5dfc13":"markdown","dcf21a9d":"markdown","99d79ed2":"markdown","f8024a7d":"markdown","8544b0e1":"markdown"},"source":{"a8cdcc79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34cc84b1":"data = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')","4610e21e":"data.head()","d2be9f23":"list_to_drop = ['textID','selected_text','sentiment']\ndata.drop(list_to_drop,axis=1,inplace=True)","14681399":"data.head()","7a46d9ff":"print(f\"Total number of examples to be used is : {len(data)}\")","4a14f105":"from nltk.corpus import wordnet\n\ndef get_synonyms(word):\n    \n    synonyms = set()\n    \n    for syn in wordnet.synsets(word):\n        for l in syn.lemmas():\n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    if word in synonyms:\n        synonyms.remove(word)\n    \n    return list(synonyms)","215487db":"from nltk.corpus import stopwords\nstop_words = []\nfor w in stopwords.words('english'):\n    stop_words.append(w)\nprint(stop_words)","c0d57853":"import random\n","f4b18fe5":"\ndef synonym_replacement(words, n):\n    \n    words = words.split()\n    \n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    \n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        \n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            num_replaced += 1\n        \n        if num_replaced >= n: #only replace up to n words\n            break\n\n    sentence = ' '.join(new_words)\n\n    return sentence\n","7b51ed24":"print(f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',3)}\")","8cad3f44":"trial_sent = data['text'][6]\nprint(trial_sent)\n","6ef00f13":"# Create 3 Augmented Sentences per data \n\nfor n in range(3):\n    print(f\" Example of Synonym Replacement: {synonym_replacement(trial_sent,n)}\")","40112285":"def random_deletion(words, p):\n\n    words = words.split()\n    \n    #obviously, if there's only one word, don't delete it\n    if len(words) == 1:\n        return words\n\n    #randomly delete words with probability p\n    new_words = []\n    for word in words:\n        r = random.uniform(0, 1)\n        if r > p:\n            new_words.append(word)\n\n    #if you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n\n    sentence = ' '.join(new_words)\n    \n    return sentence","9773d0c5":"print(random_deletion(trial_sent,0.2))\nprint(random_deletion(trial_sent,0.3))\nprint(random_deletion(trial_sent,0.4))","bc2c58d1":"def swap_word(new_words):\n    \n    random_idx_1 = random.randint(0, len(new_words)-1)\n    random_idx_2 = random_idx_1\n    counter = 0\n    \n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words)-1)\n        counter += 1\n        \n        if counter > 3:\n            return new_words\n    \n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n    return new_words\n\n# This will Swap the words\n\n","241d4395":"def random_swap(words, n):\n    \n    words = words.split()\n    new_words = words.copy()\n    # n is the number of words to be swapped\n    for _ in range(n):\n        new_words = swap_word(new_words)\n        \n    sentence = ' '.join(new_words)\n    \n    return sentence","b59a70cf":"print(random_swap(trial_sent,1))\nprint(random_swap(trial_sent,2))\nprint(random_swap(trial_sent,3))","72c05ebe":"def random_insertion(words, n):\n    \n    words = words.split()\n    new_words = words.copy()\n    \n    for _ in range(n):\n        add_word(new_words)\n        \n    sentence = ' '.join(new_words)\n    return sentence\n\ndef add_word(new_words):\n    \n    synonyms = []\n    counter = 0\n    \n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return\n        \n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)\n","c9e94bf4":"print(random_insertion(trial_sent,1))\nprint(random_insertion(trial_sent,2))\nprint(random_insertion(trial_sent,3))","a959d983":"def aug(sent,n,p):\n    print(f\" Original Sentence : {sent}\")\n    print(f\" SR Augmented Sentence : {synonym_replacement(sent,n)}\")\n    print(f\" RD Augmented Sentence : {random_deletion(sent,p)}\")\n    print(f\" RS Augmented Sentence : {random_swap(sent,n)}\")\n    print(f\" RI Augmented Sentence : {random_insertion(sent,n)}\")\n    \n    ","beadfd52":"aug(trial_sent,4,0.3)","ecc5e22b":"#  ***Simple Data Augmentatons Techniques* are:**\n1. SR : Synonym Replacement \n2. RD : Random Deletion\n3. RS : Random Swap\n4. RI : Random Insertion\n\n","09386200":"# 1. Synonym Replacement :\n\nSynonym replacement is a technique in which we replace a word by one of its synonyms\n\nFor identifying relevent Synonyms we use WordNet","64f82981":"# 4. Random Insertion (RI)\nFinally, in Random Insertion, we randomly insert synonyms of a word at a random position.\n\nData augmentation\noperations should not change the true label of\na sentence, as that would introduce unnecessary\nnoise into the data. Inserting a synonym of a word\nin a sentence, opposed to a random word, is more\nlikely to be relevant to the context and retain the\noriginal label of the sentence.","ae64d91b":"![EDA_nlp.PNG](attachment:EDA_nlp.PNG)","8a31307a":"# 2.Random Deletion (RD)\n\nIn Random Deletion, we randomly delete a word if a uniformly generated number between 0 and 1 is smaller than a pre-defined threshold. This allows for a random deletion of some words of the sentence.\n\n","c826494b":"\n# 3. Random Swap (RS)\n\nIn Random Swap, we randomly swap the order of two words in a sentence.\n","6fb6f539":"To Get Larger Diversity of Sentences we could try replacing 1,2 3, .. Words in the given sentence.\n\nNow lets get an example from out dataset and try augmenting it so that we could create 3 additional sentences per tweet ","ed5959f5":"The above Line Shows the Augmentations possible ","8e1fbec3":"Now we are able to augment this Data :)\n\nYou can create New colums for the Same text-id  in our tweet - sentiment Dataset","d140b466":" # Augmentations in NLP\n\nData Augmentation techniques in NLP show substantial improvements on datasets with less than 500 observations, as illustrated by the original paper.\n\nhttps:\/\/arxiv.org\/abs\/1901.11196\n\nThe Paper Considered here is EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n\n\n","15b05e05":"We covered the main Data Augmentation techniques in NLP. This is an active field of research, and the papers about this topic are quite recent.","c7720525":"Lets test out this Augmentation with our test_sample","e1eab1d6":"This Random Swapping will help to make our models robust and may inturn help in text classification. \n\nHigh order of swapping may downgrade the model\n\nThere is a high chance to loose semantics of language so be careful while using this augmentaion.\n\n","a4531915":"The get_synonyms funtion will return pre-processed list of synonyms of given word\n\nNow we will replace the words with synonyms","0a5dfc13":"This Could help us in reducing Overfitting and may help to imporve our Model Accuracy ","dcf21a9d":"Link to Paper : https:\/\/arxiv.org\/abs\/1901.11196\n\nRepo Link : https:\/\/github.com\/jasonwei20\/eda_nlp\n","99d79ed2":"Open to new ideas and Suggestions and hope you were able to learn something Good. \n\n*Thank you so much for Reading the Kernal*   !!\n\n","f8024a7d":"# Final Thoughts Regarding EDA:\n\n","8544b0e1":"\n* EDA might not help much if you\u2019re using a large enough dataset.\n\n* Models that have been pre-trained on massive datasets probably don\u2019t need EDA. So in most of the Cases Transformer Based Models wont require EDA\n\n* generating augmented data similar to original data introduces some degree of noise that helps prevent overfitting. This may give a clearn understanding , why augmenation helps in making robust models.\n\n* EDA can introduce new vocabulary through the synonym replacement and random insertion operations, allowing models to generalize to words in the test set that were not in the training set. \n\n* Finally this effects are more pronounced for smaller datasets.  \n"}}