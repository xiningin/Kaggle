{"cell_type":{"7ff1902e":"code","9ed6cd38":"code","294fdd4a":"code","776851e3":"code","45245360":"code","30ebfabb":"code","13497b0e":"code","b068d0e1":"code","00edcc26":"code","9cb9116b":"code","84a25cc6":"code","cb3c40c1":"code","f47dd692":"code","8d38c923":"code","bbd92c14":"code","e092cb2f":"code","e3db6977":"code","f3935bd9":"code","5230a9e6":"code","4cca4915":"code","731f6cef":"code","0337cd93":"code","ca4615c6":"code","e56450aa":"code","13dbc5e6":"code","419755d9":"code","39193e03":"code","6ecb1620":"code","b4a63279":"code","a26f4cab":"code","a5894e50":"code","8d0ed7a5":"code","34e7ca9b":"code","27cff743":"markdown","b9e4ec60":"markdown","07b3d74e":"markdown","e71eb120":"markdown","fea5fad7":"markdown","c1715a32":"markdown","4dd66c57":"markdown","41ea5d68":"markdown","d93a51bf":"markdown","fb312e4d":"markdown","8e84dbc0":"markdown","8212a385":"markdown","56efac32":"markdown","969694a9":"markdown","f5ad977b":"markdown","6a4a6755":"markdown","f840aabe":"markdown","22b9d3d4":"markdown","65ad5570":"markdown","8615adb6":"markdown","f1c2e007":"markdown","3e62e9b0":"markdown","b1328ec9":"markdown","9892c34d":"markdown","ac46a090":"markdown","af4d37a2":"markdown","6db2fbcd":"markdown"},"source":{"7ff1902e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ed6cd38":"# Data analytics libraries\nimport pandas as pd\nimport numpy as np\n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","294fdd4a":"# Load the training data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Check training data\ntrain.head()","776851e3":"# Load the testing data\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Check the testing data\ntest.head()","45245360":"train.dtypes","30ebfabb":"train.head(2)","13497b0e":"# Describe the training data\ntrain.describe(include='all')","b068d0e1":"# Describe the testing data\ntest.describe(include='all')","00edcc26":"# Explore the 'Survived' category\npd.DataFrame({'Volumne' : train['Survived'].value_counts(),\n              'Percentage': train['Survived'].value_counts(normalize=True)})","9cb9116b":"# Explore the 'Sex' category\npd.DataFrame({'Volume': train['Sex'].value_counts(),\n              'Percentage': train['Sex'].value_counts(normalize=True)})","84a25cc6":"# Explore the 'Pclass' category\npd.DataFrame({'Volume': train['Pclass'].value_counts(),\n              'Percentage': train['Pclass'].value_counts(normalize=True)})","cb3c40c1":"# Explore the 'Embarked' category\npd.DataFrame({'Volume': train['Embarked'].value_counts(),\n              'Percentage': train['Embarked'].value_counts(normalize=True)})","f47dd692":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))","8d38c923":"# Create a combined group of both datasets\ncombine = [train, test]\n\n# Extract a title for each name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract( '([A-Za-z]+)\\.', expand = False)\n    \n#pd.crosstab(train['Title'], train['Sex'])","bbd92c14":"# Replace titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace(['Mlle'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace(['Mme'], 'Mrs')\n    dataset['Title'] = dataset['Title'].replace(['Ms'], 'Miss')\n    \n# Check that it's worked\ntrain['Title'].value_counts()","e092cb2f":"# Map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6, np.NaN: 0}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","e3db6977":"# Identify columns with null or missing values\n\nprint('Train columns with null values:\\n', train.isnull().sum())\nprint('-'*10)\n\nprint('Test columns with null values:\\n', test.isnull().sum())","f3935bd9":"# Value counts\npd.DataFrame({'Volume': train['Embarked'].value_counts(),\n              'Percentage': train['Embarked'].value_counts(normalize=True)})","5230a9e6":"# Replace missing values\ntrain.fillna({'Embarked': 'S'}, inplace=True)\n\n# Check that it's worked\ntrain['Embarked'].isnull().sum()","4cca4915":"# Note that, if I wanted to cheat, I could simply uncomment the code below, find the passengers who are 'null', and google to seach where they boarded! But I'm not going to do that because it would be (a) cheating and (b) boring\n# train.loc[train['Embarked'].isnull()]","731f6cef":"# Step 1: Categorise passengers into age groups and create 'AgeGroup' columns\nage_groups_train = []\n\nfor i in train.index:\n    if (train['Age'].loc[i] < 5):\n        age_groups_train.append('Baby')\n    elif (train['Age'].loc[i] < 16):\n        age_groups_train.append('Child')\n    elif (train['Age'].loc[i] < 50):\n        age_groups_train.append('Adult')\n    elif (train['Age'].loc[i] >= 50):\n        age_groups_train.append('Senior')\n    else:\n        age_groups_train.append('Unknown')\n        \ntrain['AgeGroup'] = age_groups_train\n\nage_groups_test = []\n\nfor i in test.index:\n    if (test['Age'].loc[i] < 5):\n        age_groups_test.append('Baby')\n    elif (test['Age'].loc[i] < 16):\n        age_groups_test.append('Child')\n    elif (test['Age'].loc[i] < 50):\n        age_groups_test.append('Adult')\n    elif (test['Age'].loc[i] >= 50):\n        age_groups_test.append('Senior')\n    else:\n        age_groups_test.append('Unknown')\n        \ntest['AgeGroup'] = age_groups_test","0337cd93":"mr_age = train[train['Title'] == 1]['Age'].mean()\nmiss_age = train[train['Title'] == 2]['Age'].mean()\nmrs_age = train[train['Title'] == 3]['Age'].mean()\nmaster_age = train[train['Title'] == 4]['Age'].mean()\nroyal_age = train[train['Title'] == 5]['Age'].mean()\nrare_age = train[train['Title'] == 6]['Age'].mean()\n\nfor i in train.index:\n    if train['AgeGroup'].loc[i] == 'Unknown':\n        if train['Title'].loc[i] == 1:\n            train['Age'].loc[i] = mr_age\n        elif train['Title'].loc[i] == 2:\n            train['Age'].loc[i] = miss_age\n        elif train['Title'].loc[i] == 3:\n            train['Age'].loc[i] = mrs_age\n        elif train['Title'].loc[i] == 4:\n            train['Age'].loc[i] = master_age\n        elif train['Title'].loc[i] == 5:\n            train['Age'].loc[i] = royal_age\n        elif train['Title'].loc[i] == 6:\n            train['Age'].loc[i] == rare_age\n        else:\n            train['Age'].loc[i] = train['Age'].mean()","ca4615c6":"for i in test.index:\n    if test['AgeGroup'].loc[i] == 'Unknown':\n        if test['Title'].loc[i] == 1:\n            test['Age'].loc[i] = mr_age\n        elif test['Title'].loc[i] == 2:\n            test['Age'].loc[i] = miss_age\n        elif test['Title'].loc[i] == 3:\n            test['Age'].loc[i] = mrs_age\n        elif test['Title'].loc[i] == 4:\n            test['Age'].loc[i] = master_age\n        elif test['Title'].loc[i] == 5:\n            test['Age'].loc[i] = royal_age\n        elif test['Title'].loc[i] == 6:\n            test['Age'].loc[i] == rare_age\n        else:\n            test['Age'].loc[i] = test['Age'].mean()","e56450aa":"age_groups_train = []\n\nfor i in train.index:\n    if (train['Age'].loc[i] < 5):\n        age_groups_train.append('Baby')\n    elif (train['Age'].loc[i] < 16):\n        age_groups_train.append('Child')\n    elif (train['Age'].loc[i] < 50):\n        age_groups_train.append('Adult')\n    elif (train['Age'].loc[i] >= 50):\n        age_groups_train.append('Senior')\n    else:\n        age_groups_train.append('Unknown')\n        \ntrain['AgeGroup'] = age_groups_train\n\nage_groups_test = []\n\nfor i in test.index:\n    if (test['Age'].loc[i] < 5):\n        age_groups_test.append('Baby')\n    elif (test['Age'].loc[i] < 16):\n        age_groups_test.append('Child')\n    elif (test['Age'].loc[i] < 50):\n        age_groups_test.append('Adult')\n    elif (test['Age'].loc[i] >= 50):\n        age_groups_test.append('Senior')\n    else:\n        age_groups_test.append('Unknown')\n        \ntest['AgeGroup'] = age_groups_test","13dbc5e6":"age_mapping = {'Baby':1, 'Child': 2, 'Adult': 3, 'Senior': 4, 'Unknown': 0}\n\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","419755d9":"for x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","39193e03":"# Sex feature\n\nsex_mapping = {'male': 0, 'female': 1}\n\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)","6ecb1620":"# Embarked feature\n\nembarked_mapping = {'S': 1, 'Q': 2, 'C': 3}\n\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","b4a63279":"train.drop(['Name', 'Age', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntrain.head()","a26f4cab":"test.drop(['Name', 'Age', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.head()","a5894e50":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.3, random_state = 0)","8d0ed7a5":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","34e7ca9b":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","27cff743":"<a id=\"section-three-b-2\"><\/a>\n##### 3.2.2 Title feature\nThe basic idea is that we can extract a title from the 'Name' feature","b9e4ec60":"<a id=\"section-two-a\"><\/a>\n#### 2.1 Understand the dtypes","07b3d74e":"<a id=\"section-two-b\"><\/a>\n#### 2.2 Quick and dirty\nWe can use the `describe()` function for a quick and dirty description of the data, however it's not that good for describing the categorical variables (e.g. 'Sex', 'Pclass', 'Embarked', etc.), so we'll need to explore these in a bit more depth separately. Still, this is a good place to start.","e71eb120":"## Steps\n\n* Before we begin: [Import libraries](#section-zero)\n* Step 1: [Gather the data](#section-one)\n* Step 2: [Exploratory data analysis](#section-two)\n    - [Explore data types](#section-two-a)\n    - [Quick and dirty data exploration](#section-two-b)\n    - [Explore categorical variables](#section-two-c)\n* Step 3: [Data cleaning](#section-three)\n    - [Correcting](#section-three-a)\n    - [Creating (i.e. feature engineering)](#section-three-b)\n    - [Completing](#section-three-c)\n    - [Converting](#section-three-d)\n    - [Dropping](#section-three-e)\n* Step 4: [Modeling](#section-four)\n    - [Split data into train and test sets](#section-four-a)\n    - [Implement Gradient Boosting Classifier model](#section-four-b)\n* Step 5: [Implementation](#section-five)","fea5fad7":"<a id=\"section-three-c-2\"><\/a>\n##### 3.3.2 Age feature\nNext we'll fill in the missing values in the 'Age' feature. Since there are a lot of values missing, we'll want to do something a bit more clever...","c1715a32":"# Titanic GO HERE ONCE YOU'VE DONE THE INTRODUCTORY TUTORIAL\n\nI wanted to create an easy-to-follow notebook that's suitable for a beginner once they've completed the tutorial and are looking to level-up and take the next step.\n\nThe methodology is based on the approach of [Nadin Tamer](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner) and [this guy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#Table-of-Contents). ","4dd66c57":"<a id=\"section-two\"><\/a>\n## 2. Exploratory data analysis\n\nNow that we've loaded the data, we need to conduct some quick exploratory analysis in order to understand the structure and variables. \nWe will rely on the `describe()` `dtypes` and `value_counts()` functions, and summarise our findings in a markdown cell at the end of this section.","41ea5d68":"<a id=\"section-three-b-1\"><\/a>\n##### 3.2.1 Cabin feature\nThe cabin number is not much use to us, but knowing whether or not there IS a cabin number... now that just might be!","d93a51bf":"<a id=\"section-two-c\"><\/a>\n#### 2.3 Explore categorical data\nWe'll use `value_counts()` to do this.","fb312e4d":"<a id=\"section-three-c-1\"><\/a>\n##### 3.3.1 Embarked feature\nSince there are only 2 null values for the 'Embarked' feature, I'm not going to do anything fancy. My logic here is: replace any missing 'Embarked' values with the modal embarked value.","8e84dbc0":"<a id=\"section-four\"><\/a>\n## 4. Modelling","8212a385":"<a id=\"section-three-b\"><\/a>\n#### 3.2 Creating","56efac32":"<a id=\"section-three-d\"><\/a>\n#### 3.4 Converting\nWe want to convert categorical data into dummy variables \/ numerical values, as this will ensure the format is suitable for modelling. ","969694a9":"<a id=\"section-three\"><\/a>\n## 3. Cleaning data","f5ad977b":"<a id=\"section-three-e\"><\/a>\n#### 3.5 Dropping\nDrop unnecessary columns","6a4a6755":"Findings: \n* 891 records in our training data; 418 records in our test data\n* In the training data, passengers are marked as either 0 (Died) or 1 (Survived). Roughly 38% survived.\n* In the training data, 65% of passengers were male and 35% were female\n* In the training data, there were three classes: 55% of passengers were in 3rd class, 21% were in 2nd class, and 24% were in 1st class\n* In the training data, 72% embarked at 'S', 19% embarked at 'C', and 9% embarked at 'Q'","f840aabe":"<a id=\"section-zero\"><\/a>\n## 0. Import libraries","22b9d3d4":"<a id=\"section-four-b\"><\/a>\n#### 4.2 Implement GradientBoostingClassifier model","65ad5570":"<a id=\"section-four-a\"><\/a>\n#### 4.1 Split train dataset into training and test sets","8615adb6":"<a id=\"section-one\"><\/a>\n## 1. Gather the data\n\nBecause we need some data to work with! `pd.read_csv()` is our best friend here.","f1c2e007":"<a id=\"section-five\"><\/a>\n## 5. Create submission file","3e62e9b0":"<a id=\"section-three-c-3\"><\/a>\n##### 3.3.3 Fare field\nCategorise","b1328ec9":"* **Numerical features**: 'PassengerId', 'Age' (continuous), 'SibSp' (discrete), 'Parch' (discrete), 'Fare' (continuous)\n* **Categorical data**: 'Sex', 'Embarked', 'Survived', 'Pclass'\n* **Alphanumeric features**: 'Ticket', 'Cabin'","9892c34d":"<a id=\"section-three-a\"><\/a>\n#### 3.1 Correcting\nFrom reviewing the results of our exploratory data analysis, there does not appear to be aberrant or non-acceptable data inputs. There are outliers in age and fare, however they are still plausible values. We will proceed without correcting any values.","ac46a090":"We can see that there are some null\/missing values in the 'Age', 'Cabin' and 'Embarked' fields (in both the train and test datasets) and the 'Fare' field (in the test set only).","af4d37a2":"<a id=\"section-three-c\"><\/a>\n#### 3.3 Completing\nFirst, we will check the datasets for null\/missing values. Then, there are essentially two options for dealing with null\/missing values: either delete the records, or populate the missing values with a reasonable input. It is not advisable to delete records, so we will impute the values. ","6db2fbcd":"72% of passengers embarked in S, so I will replace the missing 'Embarked' values with 'S'. "}}