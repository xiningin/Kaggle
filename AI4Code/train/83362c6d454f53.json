{"cell_type":{"29ed66b5":"code","e9919353":"code","4cb3e737":"code","f9030b6d":"code","8ba4e887":"code","4716c551":"code","e47c2677":"code","6a19193a":"code","567fedca":"code","dbfdd8a0":"code","2ac18395":"code","fdf0b342":"code","ef34ce23":"code","a59cb002":"code","f3c86083":"code","8cd78579":"code","dbb96a66":"code","b9ded01f":"code","298751ee":"code","90bfbe6c":"code","1e2ddaff":"code","ff1aefc6":"code","4ba8d8fe":"code","9c41e867":"code","ed24edee":"code","5d230674":"code","a16a0283":"code","f97e9255":"code","69228f31":"code","3c55d1f9":"code","a6bc9591":"code","aa7e42a8":"code","a12d0e08":"code","cef02140":"code","040843fc":"code","883d6740":"code","467565e3":"code","df194ea6":"code","36c8ad06":"code","1f0cf6ad":"code","ee7cac19":"code","8865b06f":"code","93b9a20b":"code","3aa7eb92":"code","541755de":"code","e38d6286":"code","82a12769":"markdown","2477cf1a":"markdown","b6e5c514":"markdown","02dd127e":"markdown","2c1ccc52":"markdown","d3c613b6":"markdown","f2997bc8":"markdown","53444f58":"markdown","26fa9577":"markdown","d4a9d787":"markdown","1b8a2795":"markdown","c9c3e38d":"markdown","8921fbc3":"markdown","1e4158b5":"markdown","146d6178":"markdown","e612f37a":"markdown","227891a6":"markdown","eed12cf6":"markdown","cf5ca550":"markdown","78c74970":"markdown","2c875cbe":"markdown","1b319aa9":"markdown","cc6c2868":"markdown","2b4a830e":"markdown","303638ad":"markdown","d8ddb847":"markdown","c1168274":"markdown"},"source":{"29ed66b5":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom torch.utils.data import TensorDataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport catboost\nfrom sklearn.metrics import mean_squared_error\nimport scipy\nfrom scipy import stats\nfrom scipy.stats import norm","e9919353":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', sep=',')","4cb3e737":"train","f9030b6d":"test_ID = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","8ba4e887":"target = 'SalePrice'\nprint(train.loc[:, target].isnull().any()) # all target values is filled\n(mu, sigma) = norm.fit(train['SalePrice']) # get the fitted parameters used by the function\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')","4716c551":"sns.set_style('whitegrid')","e47c2677":"y = train[target]\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21, 4))\nfig.suptitle(f'Original $\\mu=$ {mu:.2f}, $\\sigma=$ {sigma:.2f}', fontsize=16)\nsns.distplot(y, fit=norm, ax=ax[0], label = 'asdasdasdasd')\nax[0].set_title('SalePrice distribution')\nax[0].set_ylabel('Frequency')\nax[0].legend(labels=['Normal dist', 'Our dist.'])\n\nres = stats.probplot(y, plot=ax[1])\nplt.show()","6a19193a":"y = train[target]\nlog_y = np.log1p(y)\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21, 4))\nfig.suptitle(f'Log-transformation $\\mu=$ {mu:.2f}, $\\sigma=$ {sigma:.2f}', fontsize=16)\nsns.distplot(log_y, fit=norm, ax=ax[0])\nax[0].set_title('SalePrice distribution')\nax[0].set_ylabel('Frequency')\nax[0].legend(labels=['Normal dist.', 'Our log transformed dist.'])\n\nres = stats.probplot(log_y, plot=ax[1])\nplt.show()","567fedca":"y.corr(np.expm1(log_y))","dbfdd8a0":"train['Alley'] = train['Alley'].fillna('No alley access')\ntrain['BsmtQual'] = train['BsmtQual'].fillna('No Basement')\ntrain['BsmtCond'] = train['BsmtCond'].fillna('No Basement')\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna('No Basement')\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna('No Basement')\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna('No Basement')\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('No Fireplace')\ntrain['GarageType'] = train['GarageType'].fillna('No Garage')\ntrain['GarageFinish'] = train['GarageFinish'].fillna('No Garage')\ntrain['GarageQual'] = train['GarageQual'].fillna('No Garage')\ntrain['GarageCond'] = train['GarageCond'].fillna('No Garage')\ntrain['PoolQC'] = train['PoolQC'].fillna('No Pool')\ntrain['Fence'] = train['Fence'].fillna('No Fence')\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None')\n\ntest['Alley'] = test['Alley'].fillna('No alley access')\ntest['BsmtQual'] = test['BsmtQual'].fillna('No Basement')\ntest['BsmtCond'] = test['BsmtCond'].fillna('No Basement')\ntest['BsmtExposure'] = test['BsmtExposure'].fillna('No Basement')\ntest['BsmtFinType1'] = test['BsmtFinType1'].fillna('No Basement')\ntest['BsmtFinType2'] = test['BsmtFinType2'].fillna('No Basement')\ntest['FireplaceQu'] = test['FireplaceQu'].fillna('No Fireplace')\ntest['GarageType'] = test['GarageType'].fillna('No Garage')\ntest['GarageFinish'] = test['GarageFinish'].fillna('No Garage')\ntest['GarageQual'] = test['GarageQual'].fillna('No Garage')\ntest['GarageCond'] = test['GarageCond'].fillna('No Garage')\ntest['PoolQC'] = test['PoolQC'].fillna('No Pool')\ntest['Fence'] = test['Fence'].fillna('No Fence')\ntest['MiscFeature'] = test['MiscFeature'].fillna('None')","2ac18395":"train['Pool_presence'] = [0 if val=='No Pool' else 1 for val in train['PoolQC']]\ntest['Pool_presence'] = [0 if val=='No Pool' else 1 for val in test['PoolQC']]\ntrain['CentralAir'] = [1 if x == train['CentralAir'].unique()[0] else 0 for x in train.loc[:,'CentralAir'].values]\ntest['CentralAir'] = [1 if x == test['CentralAir'].unique()[0] else 0 for x in test.loc[:,'CentralAir'].values]","fdf0b342":"from catboost.eval.catboost_evaluation import *\nfrom catboost.utils import create_cd","ef34ce23":"cat_features = list(train.select_dtypes(include='object').columns)\ntrain[cat_features].describe().T.sort_values('unique', ascending=False)","a59cb002":"cols = list(train.columns)\na, b = cols.index('SalePrice'), cols.index('Pool_presence')\ncols[b], cols[a] = cols[a], cols[b]\ntrain = train[cols]\ntrain.head()","f3c86083":"train.to_csv('train2.csv', index=False)","8cd78579":"feature_names = dict(list(enumerate(train.keys())))\ncat_dict  = {i:label for i,label in enumerate(train.columns) if label in cat_features+['Fence', 'MiscFeature', 'Alley', 'PoolQC']}\ndel feature_names[80]\n# train[cat_dict.values()].dtypes, \\\n# cat_dict","dbb96a66":"create_cd(\n    label=80,\n    cat_features=cat_dict.keys(),\n    feature_names=feature_names,\n    output_path='train.cd'\n)\n!cat .\/train.cd","b9ded01f":"fold_size = int(round(len(train)\/2, -2))\nfolds_count=10\ndescription_file='.\/train.cd'\ntrain2_file='.\/train2.csv'\n\n# We can chose best params with grid_search function\nlearn_params={'iterations':505,\n              'task_type' : 'GPU',\n              'random_seed': 2, \n              'learning_rate' : 0.1, \n              'max_depth': 10, \n              'l2_leaf_reg': 9.8, \n              'loss_function': 'RMSE', \n              'max_ctr_complexity' : 2, \n              'logging_level': 'Silent', \n              'boosting_type': 'Plain',}\n\nevaluator = CatboostEvaluation(train2_file,\n                               fold_size,\n                               folds_count,\n                               delimiter=',',\n                               column_description=description_file,\n                               partition_random_seed=2,\n                               has_header=True,\n)\n\n# print(evaluator.get_working_dir())\n\nresult = evaluator.eval_features(learn_config=learn_params,\n                                 eval_metrics=[\"RMSE\"],\n                                 features_to_eval=range(1,80),\n                                )","298751ee":"logloss_result = result.get_metric_results(\"RMSE\")\nlogloss_result.get_baseline_comparison()","90bfbe6c":"good_features = logloss_result.get_baseline_comparison()[logloss_result.get_baseline_comparison()['Decision']=='GOOD'].index\ngood_features = [pd.read_csv(train2_file).columns[int(str(feature)[-2:])] for i,feature in enumerate(good_features)]\nlen(good_features), good_features","1e2ddaff":"train = train[good_features+[target]]\ntest = test[good_features]","ff1aefc6":"def empty_features(test, train, threshhold=0, verbose=False):\n    if verbose:\n        print(' column \\t test \\t\\t train\\n', '*'*40)\n    i=1\n    empty_list=[]\n    for col in test.columns:\n        percentage_train = (train[col].isnull().sum()\/len(train))*100\n        percentage_test = (test[col].isnull().sum()\/len(test))*100\n        if percentage_train and percentage_test:\n            if ((percentage_test>=threshhold)|(percentage_train>=threshhold)):\n                empty_list.append(col)\n                if verbose:\n                    print(i,'{}{}% \\t{}%'.format(col.ljust(15,' '), round(percentage_test, 3), round(percentage_train,3 )))\n                    i+=1\n    return empty_list","4ba8d8fe":"empty_features_list=empty_features(test=test, train=train, verbose=True, threshhold=80) # features with more then 80% (threshold) empty values\nempty_features_list","9c41e867":"only_1_value = [feature for feature in train.nunique().index if train.nunique()[feature]==1]\nonly_1_value","ed24edee":"corr_matrix = train.corr()\n\nsns.set(rc={'figure.figsize':(20,15)})\nax = sns.heatmap(corr_matrix,\n                 annot = True, \n                 annot_kws = {'size': 8}, \n                 fmt = '.1f', \n                 cmap = 'PiYG', \n                 linewidths = 1, \n                )","5d230674":" # Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.85\ndrop_by_corr = [column for column in upper.columns if any(upper[column] > 0.875)]\nprint(drop_by_corr)","a16a0283":"def quasi_constant(df, threshold=0, verbose=True):\n    features_list = []\n    for feature in df:\n        table = df[feature].value_counts() \/ np.float(len(df))\n        first=1\n        for name in table.index:\n            if table[name]>=threshold:\n                if first:\n                    if verbose:\n                        print(f'for feature \"{feature}\":', 'value\\t\\tscore', sep='\\n')\n                    first=0\n                if verbose:\n                    print(name, table[name], sep='\\t', end='\\n'+'*'*50+'\\n')\n        if not first:\n            features_list.append(feature)\n    return features_list","f97e9255":"quasi_constant_features = quasi_constant(train, 0.95)\nquasi_constant_features","69228f31":"bad_features = drop_by_corr+only_1_value+quasi_constant_features+empty_features_list\nlen(bad_features), bad_features","3c55d1f9":"len(train.columns), train.columns","a6bc9591":"good_features = [feature for feature in good_features if feature not in bad_features]\nlen(good_features), good_features","aa7e42a8":"train = train[good_features+[target]]\ntest = test[good_features]","a12d0e08":"len(train.columns), len(pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', sep=',').columns)","cef02140":"def pipelining_preprocessor(df1, dropcolls=None, target=None, filler=None, type='cat'): # return processed  dataFrame\n    def fill_empty_by(df, filler):\n        df_nums = df.select_dtypes(exclude='object')\n        df_cats = df.select_dtypes(include='object')\n        if filler == 'pop':\n            obj = df_cats.describe().loc['top',:]\n            digits = df_nums.median()\n        elif filler == 'zero':\n            obj = 'No info'\n            digits = 0\n        elif filler == 'out_of_range':\n            obj = 'No info'\n            digits = -9999\n        elif filler == None:\n            return df\n        else:\n            raise ValueError('filler vallues is not allowed [\"zero\", \"pop\", \"out_of_range\", None]')\n        return df_cats.fillna(obj).join(df_nums.fillna(digits))[df.columns]\n    \n    df = df1.copy()\n    if target:\n        y = df[target]\n        df.drop(target, axis=1, inplace=True)\n    if dropcolls:\n        df.drop(dropcolls, axis=1, inplace=True)\n    if type=='cat':\n        output = fill_empty_by(df, filler=filler)\n    elif type=='ohe':\n        output = pd.get_dummies(fill_empty_by(df, filler=filler), drop_first=True, prefix_sep=': ',)\n    else:\n        raise ValueError('type vallues is not allowed [\"cat\", \"ohe\"]')\n    if target:\n        return output,y\n    else:\n        return output","040843fc":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)","883d6740":"X,y = pipelining_preprocessor(train, filler = 'out_of_range', target='SalePrice', type='cat')\ny = np.log1p(y)","467565e3":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=2)","df194ea6":"cats = list(train.select_dtypes(include='object').columns)","36c8ad06":"full_pool = catboost.Pool(X, label = y, cat_features = cats)\ntrain_pool = catboost.Pool(X_train, label = y_train, cat_features = cats)\nval_pool = catboost.Pool(X_val, label = y_val, cat_features = cats)\ntest_pool = catboost.Pool(pipelining_preprocessor(test[X.columns], filler = 'out_of_range', type='cat'), cat_features = cats)","1f0cf6ad":"cat_params = {\n#     'l2_leaf_reg' : [7, 8, 9],\n#     'random_strength' : [1,2,3,4],\n#     'learning_rate' : [0.01, 0.05, 0.005],\n#     'max_ctr_complexity': [1, 2, 3],\n#     'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n}\n\ncat_model = catboost.CatBoostRegressor(loss_function='RMSE', \n                                       random_seed=2, \n                                       max_depth = 10,\n                                       learning_rate = 0.05,\n                                       random_strength=1,\n                                       max_ctr_complexity=1,\n                                       l2_leaf_reg=8,\n                                       grow_policy = 'Lossguide',\n                                       task_type='GPU',\n                                      )\ngrid_search_results = cat_model.grid_search(cat_params, full_pool, \n                                            partition_random_seed=2, cv = skf, \n                                            search_by_train_test_split=True, \n                                            plot=True)\n\ngrid_search_results['params']","ee7cac19":"cat_model = catboost.CatBoostRegressor(loss_function='RMSE', \n                                       random_seed=2, \n                                       max_depth = 10,\n                                       learning_rate = 0.05,\n                                       random_strength=1,\n                                       max_ctr_complexity=1,\n                                       l2_leaf_reg=8,\n                                       grow_policy = 'Lossguide',\n                                       task_type='GPU',\n                                      )","8865b06f":"cat_model.fit(train_pool, eval_set=val_pool)","93b9a20b":"import shap","3aa7eb92":"shap_values = cat_model.get_feature_importance(train_pool, type='ShapValues')\nshap.summary_plot(shap_values[:,:-1], X_train, plot_type='bar', max_display=30)","541755de":"vals= np.abs(shap_values).mean(0)\nfeature_importance_shap = pd.DataFrame(list(zip(X_train.columns, vals)), columns=['feature','feature_importance_shap'])\nfeature_importance_shap.sort_values(by=['feature_importance_shap'], ascending=False, inplace=True)\nfeature_importance_shap.reset_index(inplace=True, drop=True)\nfeature_importance_shap","e38d6286":"gb_pred = list(map(int, map(np.expm1, cat_model.predict(test_pool))))\ngb_output = pd.DataFrame({'Id': pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')['Id'], \n                          'SalePrice': gb_pred})\n\ngb_output.to_csv('submission_catboost.csv', index=False)\ngb_output","82a12769":"### Feature meaning evaluation by Catboost","2477cf1a":"### Almost empty features","b6e5c514":"### Little feature engeneering","02dd127e":"That is better! :)","2c1ccc52":"Save changed train dataset","d3c613b6":"# Analysis of target","f2997bc8":"# Analysis of featutes","53444f58":"First of all in this section we should fill empty values in features by data description","26fa9577":"This features are almost constant. We should drop it too","d4a9d787":"> Target have massive right tail. It's not seems like normal destribution. We need to change it!","1b8a2795":"# Final step","c9c3e38d":"Trying to use embeddings for encoding categorical features","8921fbc3":"### Correlation-Matrix with Heatmap","1e4158b5":"We should drop this features","146d6178":"We also should drop this useless feature","e612f37a":"> We will use only 'good' features","227891a6":"First of all in this section we should fill empty values in features by data description","eed12cf6":"### Almost constant features","cf5ca550":"### Filling empties from data description","78c74970":"---\n# Pipeline function making with embedding categorical features, fillna and normalization","2c875cbe":"So \"CentralAir\" and \"Street\" is a binary features, so we can encoding ones with 0 and 1. Also we can create a new feature that mean pool presence","1b319aa9":"## SHAP","cc6c2868":"We droped almost 40 useless features","2b4a830e":"first of all we should drop the most useless feature. It is a feature \"*Id*\"","303638ad":"## Gradient boosting with CatBoost","d8ddb847":"### Fetures with only 1 value","c1168274":"# Modeling"}}