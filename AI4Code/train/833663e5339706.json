{"cell_type":{"f3c86e32":"code","2a05dda4":"code","8f442ddd":"code","6442de44":"code","a9fdf99d":"code","82266081":"code","c483c237":"code","ef719e0e":"code","e3f63c87":"code","a4b6a784":"code","828088a4":"code","6e923870":"code","762ebbcd":"code","31ef5e25":"code","ffb0c91f":"code","fd5d877c":"code","662875b9":"code","d180f0d3":"code","9303dc7e":"code","a5db3795":"code","220ea36f":"code","19555a14":"markdown","f703e0e6":"markdown","9c6f4b04":"markdown","c316e302":"markdown","9854afcb":"markdown","e567e07c":"markdown","e946866b":"markdown","63d34cdc":"markdown","eca574e9":"markdown","6b94c748":"markdown","4e9d203d":"markdown","ca81aa97":"markdown","9535d2a7":"markdown","4b6af3b9":"markdown","ae1a8db2":"markdown","55201121":"markdown","dc1ecffc":"markdown"},"source":{"f3c86e32":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","2a05dda4":"# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","8f442ddd":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","6442de44":"print(f'Train Shape :  {train.shape}')\nprint(f'Test Shape :  {test.shape}')","a9fdf99d":"target = train['loss']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","82266081":"train.head()","c483c237":"train.info()","ef719e0e":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_cnt = train['loss'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt, color=['#d4dddd' if i%2==0 else '#fafafa' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(20):\n    ax.annotate(f'{target_cnt[i]\/len(train)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',\n               )\n\nax.set_title('Target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","e3f63c87":"target_cnt_df = pd.DataFrame(target_cnt)\ntarget_cnt_df['ratio(%)'] = target_cnt_df\/target_cnt.sum()*100\ntarget_cnt_df.sort_values('ratio(%)', ascending=False, inplace=True)\ntarget_cnt_df['cummulated_sum(%)'] = target_cnt_df['ratio(%)'].cumsum()\ntarget_cnt_df.style.bar(subset=['cummulated_sum(%)'], color='#205ff2')","a4b6a784":"train.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","828088a4":"discrete_features = []\n\nfor col in train.columns:\n    if np.array_equal(train[col].values, train[col].values.astype(int)):\n        discrete_features.append(col)\n\nprint(f'Total {len(discrete_features)} : ')\nprint(discrete_features)","6e923870":"for dcol in discrete_features:\n    print(f'{dcol} unique value : {train[dcol].nunique()}')","762ebbcd":"f1_loss = train.groupby(['f1'])['loss'].mean().sort_values()\nprint((f1_loss==0).sum())","31ef5e25":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f1_loss)), f1_loss, alpha=0.7, color='lightgray', label='Test Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f1', loc='left', fontweight='bold')\nax.legend()\nplt.show()","ffb0c91f":"f86_loss = train.groupby(['f86'])['loss'].mean().sort_values()\nprint((f86_loss==0).sum())","fd5d877c":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n\nax.bar(range(len(f86_loss)), f86_loss, alpha=0.7, color='lightgray', label='Test Dataset')\nax.set_yticks(range(0, 20, 3))\nax.margins(0.01)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('Average of loss grouped by f86', loc='left', fontweight='bold')\nax.legend()\nplt.show()","662875b9":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","d180f0d3":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","9303dc7e":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\nsns.heatmap(train.groupby('loss').mean().sort_index(),\n            square=True, vmin=-0.5, vmax=0.5, center=0, linewidth=1,\n            cmap=sns.diverging_palette(240, 10, as_cmap=True),\n            cbar=False, \n           )\n\nax.set_title('Mean : Group by Target(Loss)',loc='left')\nplt.show()","a5db3795":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()\n    \n    ","220ea36f":"fig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","19555a14":"## Correlation\n\nIt can be seen that most of the correlations are close to zero.","f703e0e6":"## Statistics Check\n\nThe scale of this data is really diverse.\n\nIt doesn't matter if you use a tree-based model, but scaling is essential by default.","9c6f4b04":"## [TPS-AUG] Simple EDA\n\nThis time, it's a competition with so many features.\n\nFor a fun contest, I lightly conducted EDA.\n\n- No missing value.\n- There are 100 numerical continuous features.\n- The target variable loss ranges from 0 to 42 for a total of 43 discrete values. However, this is a regression problem and it is OK to submit as decimal values.","c316e302":"- There is data without decimal point.\n- The range of data is diverse.","9854afcb":"## Target & Feature Relation\n\nAs the value of targets increases, the mean moves away from zero.","e567e07c":"- There are a total of 43 discrete losses.\n- The top 12 distributions account for 80% of the total.\n- All except the order of 2 and 1 are in increasing order.","e946866b":"A total of 6 features have no decimal point.\n\n- `f1`\n- `f16`\n- `f27`\n- `f55`\n- `f60`\n- `f86`\n","63d34cdc":"## Discrete Features\n\nSome data are found to have no decimal point.","eca574e9":"## Feature Distribution","6b94c748":"- Depending on the value of f1, we can check the imbalance of loss.\n- In 5 cases, we confirmed that the loss is all 0.","4e9d203d":"- I noticed that there is an imbalance, though not as much as f1.","ca81aa97":"It's scaled up, but it's a pretty interesting aspect of the data.\n\nIt is safe to assume that the distributions of train and test are almost the same.","9535d2a7":"While the total number of data is 250000, most of the data in `f16` and `f60` are confirmed as continuous with different values, but the remaining `f1`, `f27`, `f55`, and `f86` look relatively categorical.\n\nLooking at f1 and f86 with a small number of unique values:\nFor the relationship with the loss, we averaged after groupby.","4b6af3b9":"Let's take a look by adjusting the range of expression.","ae1a8db2":"## Scaling\n\nExcept for tree-based models, you need to scale the data.\n\nBefore visualization, we will adjust the line and proceed with the visualization.","55201121":"There seems to be an increasing trend and a decreasing trend.","dc1ecffc":"## Load Data & Library\n\nLet's load a library for basic data."}}