{"cell_type":{"ab9549f9":"code","27cf79bc":"code","ddeb93ea":"code","d1b9ded5":"code","2463c726":"code","f1016297":"code","dedda8f0":"code","2a0c9975":"code","d1ef57ac":"code","577c99fb":"code","adaa3600":"code","3eb8f127":"code","dbb35598":"code","53327543":"code","e8543b9f":"code","cf55f0b4":"code","a31aca10":"code","f50b470f":"code","a04ecd9c":"code","f92cd01a":"code","9edf62a4":"code","ba4c76a7":"code","de01bd66":"code","e4e74c3d":"code","3603dde9":"code","ba031b7b":"code","bca92e2a":"code","6542dbcd":"code","a6347ca4":"code","4c96f418":"code","fcf9efe2":"code","491f5a80":"code","99ac92aa":"code","a80ceeb8":"code","35e20ca3":"code","c562d28e":"code","3966f81d":"code","2364441c":"code","65f99567":"code","959132ec":"code","2c7d0cce":"code","f77274cc":"code","c81ad216":"code","90fa718b":"code","061ad4d0":"code","19c78875":"code","c2507dd9":"code","822bb779":"code","e0947a82":"code","1499a2fb":"code","b98345a6":"markdown","c994e346":"markdown","f6cbd9a5":"markdown","9896500a":"markdown","0370ab4d":"markdown","575be9b5":"markdown","6e20f79a":"markdown","fefafd97":"markdown","c5943184":"markdown","364b55a7":"markdown","91827164":"markdown","1c5480a4":"markdown","8995ce98":"markdown","ca48718c":"markdown","eb01119c":"markdown","69868c64":"markdown","a82ab39d":"markdown","3371c64a":"markdown","67e9380d":"markdown"},"source":{"ab9549f9":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","27cf79bc":"df_train=pd.read_csv('\/kaggle\/input\/av-jantahack-machine-learning-in-agriculture\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/av-jantahack-machine-learning-in-agriculture\/test.csv')\ndf_sample=pd.read_csv('\/kaggle\/input\/av-jantahack-machine-learning-in-agriculture\/sample_submission.csv')","ddeb93ea":"df_train.shape","d1b9ded5":"df_train.head()","2463c726":"df_train['Crop_Type'].unique()","f1016297":"# Target Variable \ndf_train['Crop_Damage'].unique()","dedda8f0":"\ndf_train['Soil_Type'].unique()","2a0c9975":"df_train['Pesticide_Use_Category'].unique()","d1ef57ac":"df_train['Number_Doses_Week'].unique()","577c99fb":"df_train['Season'].unique()","adaa3600":"color=sns.color_palette()[1]\nsns.countplot(data=df_train,x='Crop_Damage',color=color)\n","3eb8f127":"sns.distplot(df_train['Estimated_Insects_Count'],bins=10,kde=False)","dbb35598":"sns.boxplot(x=df_train['Crop_Damage'],y=df_train['Estimated_Insects_Count'])","53327543":"df_train['Estimated_Insects_Count'].describe()","e8543b9f":"df_train.Number_Weeks_Used.unique()","cf55f0b4":"df_train['Number_Weeks_Quit'].unique()","a31aca10":"plt.figure(figsize=(12,12));\nsns.heatmap(df_train.corr(),annot=True)","f50b470f":"df_test['is_test']=1\ndf_train['is_test']=0\n\ndata=pd.concat([df_train,df_test]).reset_index(drop=True)\ndata.shape\n\n","a04ecd9c":"data.isnull().sum()","f92cd01a":"# Lets one hot encode the categorical variable \n\ndata=pd.get_dummies(data,columns=['Crop_Type','Soil_Type','Season','Pesticide_Use_Category'])\ndata.shape","9edf62a4":"# Let build a Simple Lasso Liner regression model to predict the missing values \n\n# Split Data into Train and Test sets \nnull_train=data[data['Number_Weeks_Used'].notnull()]\nnull_test=data[data['Number_Weeks_Used'].isnull()]\n\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(null_train.drop(columns=['ID','is_test','Crop_Damage','Number_Weeks_Used'],axis=1),\n                                                                             null_train['Number_Weeks_Used'].values,random_state=7)\n\n#Normalize the features \n\nfor col in ['Estimated_Insects_Count','Number_Weeks_Quit', 'Number_Doses_Week']:\n    scaler=preprocessing.StandardScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    null_test.loc[:,col]=scaler.transform(null_test[col].values.reshape(-1,1))\n    \n# Normalize Y variable \n\nscaler=preprocessing.StandardScaler()\nscaler.fit(y_train.reshape(-1,1))\ny_train=scaler.transform(y_train.reshape(-1,1))\ny_val=scaler.transform(y_val.reshape(-1,1))\n\n#Define model \n\nlr=linear_model.LassoCV()\nlr.fit(X_train,y_train)\n\nprint('The R2 score for Lasso model is {}'.format(lr.score(X_val,y_val)))\n\nnull_predict=lr.predict(null_test.drop(columns=['ID','is_test','Crop_Damage','Number_Weeks_Used'],axis=1))\n\nnull_test.loc[:,'Number_Weeks_Used']=scaler.inverse_transform(null_predict.reshape(-1,1))","ba4c76a7":"null_train=null_train[['ID','Number_Weeks_Used']]\nnull_test=null_test[['ID','Number_Weeks_Used']]\n\ndata_lasso=pd.concat([null_train,null_test]).reset_index(drop=True)\ndata_lasso.shape","de01bd66":"data=pd.merge(data,data_lasso,how='left',on='ID')\ndata.drop(axis=1,columns='Number_Weeks_Used_x',inplace=True)\n\ndata.loc[data['Number_Weeks_Used_y']<0,'Number_Weeks_Used_y']=0","e4e74c3d":"# Now lets enchance the power of numerical columns using Skleanr polynomial \n\n#polynomial=preprocessing.PolynomialFeatures(degree=2,include_bias=False)\n\n#polynomial.fit(data[['Estimated_Insects_Count','Number_Weeks_Quit','Number_Doses_Week','Number_Weeks_Used_y']])\n\n","3603dde9":"#poly=polynomial.transform(data[['Estimated_Insects_Count','Number_Weeks_Quit','Number_Doses_Week','Number_Weeks_Used_y']])\n#data=pd.concat([data,pd.DataFrame(poly)],axis=1)\n\n#data.head()","ba031b7b":"b","bca92e2a":"# Creating some additional Features \n\n#data['crop_soil_pest']=data['Crop_Type']+data['Pesticide_Use_Category']+data['Soil_Type']\n#data['crop_soil_pest_season']=data['Crop_Type']+data['Pesticide_Use_Category']+data['Soil_Type']+data['Season']\n\n#data['crop_soil']=data['Crop_Type']+data['Soil_Type']\n#data['soil_pest']=data['Soil_Type']+data['Pesticide_Use_Category']\n#data['crop_pest']=data['Crop_Type']+data['Pesticide_Use_Category']\n#data['Pest_season']=data['Pesticide_Use_Category']+data['Season']\n\n\n#data['Total_pest_used']=data['Number_Doses_Week']*data['Number_Weeks_Used']\n#data['Total_pest_quit']=data['Number_Doses_Week']*data['Number_Weeks_Quit']\n\n#data['Estimated_Insects_weeks_Used']=data['Estimated_Insects_Count']*data['Number_Weeks_Used']\n#data['Estimated_Insects_Used_1']=data['Estimated_Insects_Count']*data['Total_pest_used']\n#data['Estimated_Insects_Used_2']=data['Estimated_Insects_Count']*data['Total_pest_quit']\n\n\n#data['mean1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].mean(axis=1)\n#data['sum1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].sum(axis=1)\n#data['std1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].std(axis=1)\n#data['kurt1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].kurtosis(axis=1)\n#data['median1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].median(axis=1)\n\n#data['mean2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].mean(axis=1)\n#data['sum2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].sum(axis=1)\n#data['std2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].std(axis=1)\n#data['kurt2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].kurtosis(axis=1)\n#data['median2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].median(axis=1)\n\n#data['Estimated_Insects_cut']=pd.cut(data['Estimated_Insects_Count'],bins=4,labels=[0,1,2,3])\n#data['Estimated_Insects_cut']=data['Estimated_Insects_cut'].astype(int)\n\n\n\n\ndata['Estimated_Insects_Count_square']=data['Estimated_Insects_Count']*data['Estimated_Insects_Count']\ndata['Number_Weeks_Used_y_square']=data['Number_Weeks_Used_y']*data['Number_Weeks_Used_y']\n#data['Number_Doses_Week_square']=data['Number_Doses_Week']*data['Number_Doses_Week']\ndata['Number_Weeks_Quit_square']=data['Number_Weeks_Quit']*data['Number_Weeks_Quit']\n\ndata['Estimated_Insects_doses']=data['Estimated_Insects_Count']*data['Number_Doses_Week']\ndata['Estimated_Insects_used']=data['Estimated_Insects_Count']*data['Number_Weeks_Used_y']\ndata['Estimated_Insects_quit']=data['Estimated_Insects_Count']*data['Number_Weeks_Quit']\n\ndata['Number_Weeks_Quit_Used']=data['Number_Weeks_Used_y']*data['Number_Weeks_Quit']","6542dbcd":"#Sepeate the data \n\ntrain=data[data['is_test']!=1]\ntrain.drop('is_test',axis=1,inplace=True)\n\ntest=data[data['is_test']==1]\ntest.drop(columns=['Crop_Damage','is_test'],axis=1,inplace=True)\n\ntest.shape,train.shape","a6347ca4":"from sklearn import model_selection\n\nX=train.drop(columns=['Crop_Damage','ID'],axis=1)\ny=train['Crop_Damage']\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,stratify=y,random_state=101,test_size=0.1)","4c96f418":"b","fcf9efe2":"from sklearn.dummy import DummyClassifier\n\nclf = DummyClassifier(strategy='stratified',random_state=101)\n\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_val)\nprint('Accuracy of a random classifier is: %.2f%%'%(metrics.accuracy_score(y_val,y_pred)*100))","491f5a80":"from xgboost import XGBClassifier\n\nclf = XGBClassifier(objective='multi:softmax',n_jobs=-1, max_depth=6,n_estimators=300,num_class=3)\n\nXGB_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf.predict(X.loc[val_idx,:])\n    XGB_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of XGBoost Baseline is {}'.format(np.mean(XGB_prediction)*100))\n\npredict=clf.predict_proba(test.drop(columns='ID',axis=1))\n\nXGB_baseline=pd.DataFrame(predict,columns=['XBG_baseline_0','XGB_baseline_1','XGB_baseline_2'])","99ac92aa":"\nrf_baseline=ensemble.RandomForestClassifier(n_estimators=300,random_state=101,class_weight='balanced')\n\nrf_prediction_baseline=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    rf_baseline.fit(X.loc[train_idx,:],y[train_idx])\n    predict=rf_baseline.predict(X.loc[val_idx,:])\n    rf_prediction_baseline.append(metrics.accuracy_score(y[val_idx],predict))\n\nprint('Accuracy of Random Forest Baseline is {}'.format(np.mean(rf_prediction_baseline)))\n\n\npredict=rf_baseline.predict_proba(test.drop(columns='ID',axis=1))\n\nRF_baseline=pd.DataFrame(predict,columns=['RF_baseline_0','RF_baseline_1','RF_baseline_2'])","a80ceeb8":"#classifier= ensemble.RandomForestClassifier(class_weight='balanced',random_state=101)\n\n#param_grid={\n#    'n_estimators':[50,100,150,200,250],\n#    'criterion' : ['gini','entropy'],\n#    'max_depth':[5,10,15,20,25],\n#    'min_samples_split':[2,3,5],\n    #'max_features':['auto', 'sqrt', 'log2']\n     \n#}\n\n#model=model_selection.GridSearchCV(\n#                        estimator=classifier,\n#                        param_grid=param_grid,\n#                        scoring='accuracy',\n#                        cv=5,\n#                        refit=True,\n #                       verbose=5,\n #                       n_jobs=-1\n #                       )\n\n#model.fit(X,y)\n\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('\/n')\n\n#print('Best Parameters{}'.format(model.best_params_))\n\n#predict=model.predict_proba(test.drop(columns='ID',axis=1))\n\n#RF_Tuned=pd.DataFrame(predict,columns=['RF_Tuned_0','RF_Tuned_1','RF_Tuned_2'])","35e20ca3":"# Implementing the Grid Search Version \n\nrf_tuned=ensemble.RandomForestClassifier(n_estimators=250,random_state=101,criterion='entropy',max_depth=25,min_samples_split=2,\n                                         class_weight='balanced')\n\nrf_prediction_tuned=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    rf_tuned.fit(X.loc[train_idx,:],y[train_idx])\n    predict=rf_tuned.predict(X.loc[val_idx,:])\n    rf_prediction_tuned.append(metrics.accuracy_score(y[val_idx],predict))\n\nprint('Accuracy of Random Forest Tuned by Grid Search is {}'.format(np.mean(rf_prediction_tuned)))\n\n\npredict=rf_tuned.predict_proba(test.drop(columns='ID',axis=1))\n\nRF_tuned=pd.DataFrame(predict,columns=['RF_tuned_0','RF_tuned_1','RF_tuned_2'])","c562d28e":"#import xgboost as xgb\n#from sklearn.model_selection import RandomizedSearchCV\n\n#params = {\n#        'learning_rate': np.arange(0,1,0.1),\n#       'n_estimators':np.arange(50,250,50),\n#       'colsample_bytree': np.arange(0.4,1,0.2),\n#        'max_depth': np.arange(5,15,5),\n#        'reg_lambda':np.arange(0.5,1,0.5)\n#        }\n\n#xgb = xgb.XGBClassifier(objective='multi:softmax',\\\n #                    nthread=1,num_class=3)\n\n#folds = 5\n#param_comb = 100\n\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=kfold.split(X,y), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X, y)\n\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('\/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_params_))\n\n\n#predict=random_search.predict_proba(test.drop(columns='ID',axis=1))\n\n#XGB_Tuned=pd.DataFrame(predict,columns=['XGB_Tuned_0','XGB_Tuned_1','XGB_Tuned_2'])","3966f81d":"from xgboost import XGBClassifier\n\nclf_tuned = XGBClassifier(objective='multi:softmax',n_jobs=-1,n_estimators=100,num_class=3,colsample_bytree= 0.6\n                         ,gamma=2,max_depth=10,min_child_weight=11,reg_lambda=0.5,subsample=0.8)\n\nXGB_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf_tuned.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf_tuned.predict(X.loc[val_idx,:])\n    XGB_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of XGBoost Tuned by Random Search is {}'.format(np.mean(XGB_prediction)))\n\npredict=clf_tuned.predict_proba(test.drop(columns='ID',axis=1))\n\nXGB_Tuned=pd.DataFrame(predict,columns=['XGB_Tuned_0','XGB_Tuned_1','XGB_Tuned_2'])","2364441c":"\ntree_classifier=ensemble.ExtraTreesClassifier(n_estimators=250,max_depth=5)\n\ntree_classifier_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    tree_classifier.fit(X.loc[train_idx,:],y[train_idx])\n    predict=tree_classifier.predict(X.loc[val_idx,:])\n    tree_classifier_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of Extra Tree CLassifier is {}'.format(np.mean(tree_classifier_prediction)))\n\npredict=tree_classifier.predict_proba(test.drop(columns='ID',axis=1))\n\nET_baseline=pd.DataFrame(predict,columns=['ET_baseline_0','ET_baseline_1','ET_baseline_2'])\n","65f99567":"# Data Preparation for NN \n\n\n\n# Making catergorical variables as one-hot encoding \n\n#data=pd.get_dummies(data,columns=['Pesticide_Use_Category','Season','crop_soil_pest','crop_soil_pest_season','crop_soil'\\\n                                  # ,'soil_pest','crop_pest','Pest_season'],drop_first=True)\n\nTrain=data[data['is_test']!=1]\nTrain.drop(columns=['is_test','ID'],axis=1,inplace=True)\n\nX=Train.drop('Crop_Damage',axis=1)\ny=Train['Crop_Damage']\n\nTest=data[data['is_test']==1]\nTest.drop(columns=['is_test','Crop_Damage'],axis=1,inplace=True)\n\n","959132ec":"# Using Oversampling using SMOTE \n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,random_state=101,test_size=0.20)\n\nnormalize_col=['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used_y','Number_Weeks_Quit',\n              'Estimated_Insects_Count_square','Number_Weeks_Used_y_square','Number_Weeks_Quit_square','Estimated_Insects_doses',\n       'Estimated_Insects_used', 'Estimated_Insects_quit','Number_Weeks_Quit_Used']\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    Test.loc[:,col]=scaler.transform(Test[col].values.reshape(-1,1))\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote=SMOTE('minority')\nX_sm,y_sm=smote.fit_sample(X_train,y_train)   \n\n\ny_train=pd.get_dummies(y_train)\ny_val=pd.get_dummies(y_val)","2c7d0cce":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom keras.regularizers import l2,l1\nfrom keras import layers\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(units = 200, kernel_initializer = 'he_normal', activation = 'relu', input_dim = X_train.shape[1]))\n\nmodel.add(layers.Dropout(0.05))\n\nmodel.add(BatchNormalization())\n\n# Adding second hidden layer\nmodel.add(Dense(units = 200,kernel_initializer = 'he_normal', activation = 'relu'))\n\nmodel.add(layers.Dropout(0.05))\n\nmodel.add(BatchNormalization())\n\n#Adding third hidden layer \nmodel.add(Dense(units = 100,kernel_initializer = 'he_normal', activation = 'relu'))\n\nmodel.add(layers.Dropout(0.1))\n\nmodel.add(BatchNormalization())\n\n\n# Adding the output layer\nmodel.add(Dense(units = 3, kernel_initializer = 'he_normal', activation = 'softmax'))\n\n# Compiling the ANN\n\nadam=optimizers.Adam(lr=0.0001)\n\nmodel.compile(optimizer =adam, loss = 'categorical_crossentropy',metrics=['accuracy'])\n\nmodel.fit(X_train,y_train,batch_size=128,epochs=110,validation_data=(X_val,y_val),verbose=2)","f77274cc":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()","c81ad216":"predict=model.predict_proba(Test.drop('ID',axis=1))\n\nNN=pd.DataFrame(predict,columns=['NN_0','NN_1','NN_2'])","90fa718b":"#Test.drop(columns=['Crop_Damage_0','Crop_Damage_1','Crop_Damage_2'],axis=1,inplace=True)","061ad4d0":"# Ensemble of Various Model Public Leaderboard : 0.844\n\n#Test['Crop_Damage_0']=(0.5*NN['NN_0']+0.0*RF_tuned2['RF_tuned2_0']+0.0*XGB_baseline['XBG_baseline_0']+0.0*RF_baseline['RF_baseline_0']+0*RF_tuned['RF_tuned_0']+0.45*XGB_Tuned['XGB_Tuned_0']+0.25*ET_baseline['ET_baseline_0'])\n#Test['Crop_Damage_1']=(0.5*NN['NN_1']+0.0*RF_tuned2['RF_tuned2_1']+0.0*XGB_baseline['XGB_baseline_1']+0.0*RF_baseline['RF_baseline_1']+0*RF_tuned['RF_tuned_1']+0.45*XGB_Tuned['XGB_Tuned_1']+0.25*ET_baseline['ET_baseline_1'])\n#Test['Crop_Damage_2']=(0.5*NN['NN_2']+0.0*RF_tuned2['RF_tuned2_2']+0.0*XGB_baseline['XGB_baseline_2']+0.0*RF_baseline['RF_baseline_2']+0*RF_tuned['RF_tuned_2']+0.45*XGB_Tuned['XGB_Tuned_2']+0.25*ET_baseline['ET_baseline_2'])\n\n","19c78875":"Test['Crop_Damage_0']=(0.4*NN['NN_0']+0.4*XGB_Tuned['XGB_Tuned_0']+0.2*ET_baseline['ET_baseline_0']).values.reshape(-1,1)\nTest['Crop_Damage_1']=(0.4*NN['NN_1']+0.4*XGB_Tuned['XGB_Tuned_1']+0.2*ET_baseline['ET_baseline_1']).values.reshape(-1,1)\nTest['Crop_Damage_2']=(0.4*NN['NN_2']+0.4*XGB_Tuned['XGB_Tuned_2']+0.2*ET_baseline['ET_baseline_2']).values.reshape(-1,1)","c2507dd9":"Test.sample(10)","822bb779":"Test['Crop_Damage']=Test[['Crop_Damage_0','Crop_Damage_1','Crop_Damage_2']].idxmax(axis=1)","e0947a82":"Test['Crop_Damage'].replace({'Crop_Damage_0':'0','Crop_Damage_1':'1','Crop_Damage_2':'2'},inplace=True)","1499a2fb":"\nTest[['ID','Crop_Damage']].to_csv('\/kaggle\/working\/Ensemble_weighted_NN+XGB+ET.csv',index=False)","b98345a6":"# XGB Classifier Baseline ","c994e346":"Wow!!! Highly baised set ","f6cbd9a5":"# Extra Tree Classifier Baseline ","9896500a":"From the above chart we see that crop damage has a decent corelation with the numerical variables \n\nEstimated_Insects_Count , Number_Weeks_quit and Number weeks used ,Pesticide_Use_Catergory \n\nSo lets magnify this effects by using SKleanr polynomal feature extractor ","0370ab4d":"\nimport lightgbm as lgb\n\nclf = lgb.LGBMClassifier(boosting_type='gbdt', class_weight='balanced', colsample_bytree=0.5,\n                importance_type='split', learning_rate=0.1, max_depth=4,\n                min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=5,\n                min_split_gain=0.0, n_estimators=650, n_jobs=-1, num_class=3,\n                num_leaves=63, objective='multiclass', random_state=42,\n                reg_alpha=2, reg_lambda=0, silent=True, subsample=0.7,\n                subsample_for_bin=200000, subsample_freq=0, verbose=1)\n\nlgbm_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf.predict(X.loc[val_idx,:])\n    lgbm_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of LGBM Baseline is {}'.format(np.mean(lgbm_prediction)*100))\n\n#predict=clf.predict_proba(test.drop(columns='ID',axis=1))\n\n#XGB_baseline=pd.DataFrame(predict,columns=['XBG_baseline_0','XGB_baseline_1','XGB_baseline_2'])","575be9b5":"# The best paramerts from RandomSearch CV\n\n> {'subsample': 0.8000000000000002, 'reg_lambda': 0.5, 'n_estimators': 100, 'min_child_weight': 11, 'max_depth': 10, 'learning_rate': 1300, 'gamma': 2.0, 'colsample_bytree': 0.6000000000000001}","6e20f79a":"https:\/\/towardsdatascience.com\/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a\n\nhttps:\/\/medium.com\/@gabrielziegler3\/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n\nhttps:\/\/towardsdatascience.com\/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n\nhttps:\/\/towardsdatascience.com\/dealing-with-multiclass-data-78a1a27c5dcc\n\nhttps:\/\/machinelearningmastery.com\/voting-ensembles-with-python\/\n\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n\nhttps:\/\/towardsdatascience.com\/random-forest-in-python-24d0893d51c0\n\nhttps:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d\n\nhttps:\/\/towardsdatascience.com\/predictive-modeling-and-multiclass-classification-a4d2c428a2eb\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nhttps:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\n\nhttps:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63","fefafd97":"# XGB Boost Tuned after Random Grid CV ","c5943184":"## Random Forest Baseline ","364b55a7":"# Explore the Values of various independent columns ","91827164":"# Neural Network ","1c5480a4":"# Neural Network ","8995ce98":"## Result of Grid Search CV \n\n> Best Parameters{'criterion': 'entropy', 'max_depth': 25, 'min_samples_split': 2, 'n_estimators': 250}","ca48718c":"# XGB Boost optimized with RandomSearch CV ","eb01119c":"# Creating Various Models for Ensembling ","69868c64":"## Dummy Classifier \n\nTo see how much we can improve over a baseline ","a82ab39d":"## Random Forest with Optimized with GridSearchCV \n\n#### uncomment the below cell to run Grid Search CV ","3371c64a":"# Let do some Feature Engineering \n\n","67e9380d":"# Building a Linear Regression model to Predict Null Values in Number_Weeks_Used column"}}