{"cell_type":{"9adbfd97":"code","d1b34912":"code","2a08c1b9":"code","66df2b77":"code","7725dd1b":"markdown","4a41e75e":"markdown","6735732e":"markdown","96692046":"markdown","ac83e967":"markdown","c244d57a":"markdown","69a96d74":"markdown","771960b3":"markdown","d65fcea7":"markdown","b5738701":"markdown","3b7eddf1":"markdown","d0d28847":"markdown","195327cd":"markdown","cc4c337c":"markdown","e3d20395":"markdown","e557b338":"markdown","47195f82":"markdown","308db0a2":"markdown","5735990e":"markdown","a4c86abf":"markdown","9d26907e":"markdown","275c574c":"markdown","02e15864":"markdown","3251e79d":"markdown","72f8a2ed":"markdown"},"source":{"9adbfd97":"from torch.utils.data import Dataset\nimport torch\n\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","d1b34912":"from torch import nn\n\n\n# TODO: Adapt this to Pytorch Lightning.\n\nclass CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output\n","2a08c1b9":"# TODO: Integrate with the TPU model. ","66df2b77":"#\u00a0TODO: What about JAX? Maybe not. ","7725dd1b":"As always, before leaving, some resources to dig deeper into the subject.","4a41e75e":"Recently, I was lucky to get accepted into the [TRC](https:\/\/sites.research.google\/trc\/) program (short for TPU Research Cloud). I have also participated a bit in the HuggingFace TPU program (even though I wasn't much active due to personal things).\n\nIn a nutshell, this is a free (more details later) TPU program. To apply, here is the link.\n\nThe only requirement is that you follow \"ethical\" AI practices (as defined here) and share as much as possible with the team, write a blog post, a research paper, i.e. contribute something back. \n\nThe requriements don't seem that hard to achieve. \n\nTo contribute my own share, I am writing this notebook detailing my journey. I hope it is good enough. :D\n\n\nAlright, once you have access to the program, you cant start using TPUs.\n\nYou might ask: but how, given I have 0 experience with TPUs?\n\nDon't panic, I will try to make the process as smooth as possible. The documentation is good enough but there are some rough edges and I have tried to take notes of the harder bits. \n\nLet's got!","6735732e":"# TPU VMs","96692046":"[JAX](https:\/\/github.com\/google\/jax) is another interesting library that can be used with TPUs. Some even say that it is the way to go.\n\nIndeed, that makes sense since it was designed with XLA in mind.","ac83e967":"Still a **WIP**. ","c244d57a":"Alright, enough with exposition and theory. Time for some application.\n\nFor that, we will use the Kaggle TPU and PyTorch thanks to XLA.\n\nWe will also use the CommonLit readability [dataset](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize).","69a96d74":"In order to use something different than Tensorflow which is equipped to understand low level TPU things, \nPyTorch needs some understanding of these things. \n\nFor that, there is [XLA](https:\/\/www.tensorflow.org\/xla?hl=en).","771960b3":"<img src=\"https:\/\/sites.research.google\/trc\/static\/img\/research_cloud_hero.png\" width=320>","d65fcea7":"Before we start, notice that there are many other methods but I will only focus on the \nmost convenient and scalable one I have found. Also, this method \n\nIndeed, TPUs are available using at least these methods:\n\n# TODO: Finish adapting\nInstead of setting your own TPU pod\/instance, you can also take advantage of the Colab offered TPU and the Kaggle one.\n\nHere is how to do it in Colab: https:\/\/colab.research.google.com\/notebooks\/tpu.ipynb\n\nAnd here is how to do it in Kaggle: https:\/\/www.kaggle.com\/docs\/tpu\n\n- Kaggle of course\n- TPU pods\n- Colab\n- TPU VMs\n\n\nThe last method is the one we will focus on. This is probably the easist why to get started and have your env setup with everything needed. \n\nIn fact, since you have access to a VM, what you do will be persisted.\n\nNotice that this is still an early feature (as of July 2021) so be careful (don't use it in \nproduction) and help the dev by reporting bugs. ","b5738701":"## Bonus: JAX","3b7eddf1":"Before that, we will make a short detour to explain what **TPUs** are and why they might be useful to \nsome of your use cases.","d0d28847":"# What are TPUs?","195327cd":"## Model","cc4c337c":"- Original notebook for the model: https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3 and \nhttps:\/\/www.kaggle.com\/tensorchoko\/commonlit-readability-roberta\n- https:\/\/cloud.google.com\/tpu\n- https:\/\/cloud.google.com\/tpu\/docs\/beginners-guide\n\n- https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/159723\n\n- https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/159723\n\n- https:\/\/blog.goodaudience.com\/how-to-use-google-cloud-tpus-177c3a025067\n\n- Very useful notebook on using TPU with XLA for NLP: https:\/\/www.kaggle.com\/philippsinger\/xlm-roberta-large-pytorch-pytorch-tpu?scriptVersionId=3846258\n\n- Pytorch's XLA guide: https:\/\/pytorch.org\/xla\/release\/1.7\/index.html\n\n- Introductory post to TPU VMs: https:\/\/cloud.google.com\/blog\/products\/compute\/introducing-cloud-tpu-vms\n- Cloud TPU pricing: https:\/\/cloud.google.com\/tpu\/pricing","e3d20395":"<img src=\"data:image\/jpeg;base64,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\">","e557b338":"We start as usual with any PyTorch dataset.\n\nTODO: Finish adapting this dataset.","47195f82":"That's it for today, I hope you found something useful here (or in the different links).\n\nStay tuned for the upcoming notebook: JAX meets TPUs. ","308db0a2":"**[TPU](https:\/\/en.wikipedia.org\/wiki\/Tensor_Processing_Unit)s**, short for **Tensor Processing Units**, are custom hardware developed by Google around 2015 (probably even before), annonced in the 2016's Google I\/O event and made available to the public around 2018.\n\nSimilar to GPU which are custom hardaware adapted to graphical processing and later on to deep learning and \nother data processing tasks, TPUs have been designed from the get go to work and scale with machine learning workloads. They are, what is called **ASIC**: application-specific integrated circuit.\n\nSo far, there are mainly two versions (check the graph below) that you can use: \n\n\n- v2.8\n- v3.8\n\nThere are slight variations of each major version, check it in this table: \n\n![tpu versions](https:\/\/drive.google.com\/uc?id=15JXDTc2NAA7pafArHgdRX-wonXfs9BeN)\n\n\nNext question is: how to access these TPUs?","5735990e":"## Resources","a4c86abf":"# Application","9d26907e":"<img src=\"https:\/\/cloud.google.com\/images\/products\/tpu\/google-cloud-ai_2x.png\" width=480>","275c574c":"Here is an extract from the [XLA Github](https:\/\/github.com\/pytorch\/xla) repo:\n    \n> PyTorch\/XLA is a Python package that uses the XLA deep learning compiler to connect the PyTorch deep learning framework and Cloud TPUs. You can try it right now, for free, on a single Cloud TPU with Google Colab, and use it in production and on Cloud TPU Pods with Google Cloud.","02e15864":"# What is XLA?","3251e79d":"Under the hood, it uses the TF XLA [compiler](https:\/\/www.tensorflow.org\/xla).","72f8a2ed":"## Dataset"}}