{"cell_type":{"32ac1858":"code","11a3bb11":"code","7800fe9a":"code","470cd99d":"code","ffe43b15":"code","b77bb08c":"code","de6c47bb":"code","b26228cf":"code","31d8653c":"code","fdc4e6bd":"code","22608481":"code","9c9ef33c":"code","cb0515ad":"code","cfb73979":"code","7fdcc2a9":"code","8065f37b":"code","b1a9a737":"code","e03502f4":"code","1b7ea614":"code","cfe974ad":"code","c5a9402e":"code","3be2ad68":"code","50345221":"code","4518a4d1":"markdown","26d8a364":"markdown","78ec7f6b":"markdown","1b6799f7":"markdown","10a99490":"markdown","b405089a":"markdown","e10f5fb0":"markdown","a39c0620":"markdown","642dff72":"markdown","f19c6cff":"markdown","1ec688b2":"markdown","452bf07c":"markdown","d92049e0":"markdown","34c4cb77":"markdown","52f45cda":"markdown","c27306db":"markdown"},"source":{"32ac1858":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.models import Model\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","11a3bb11":"PATH = '..\/input\/breast-cancer-wisconsin-data\/data.csv'\ndf = pd.read_csv(PATH)\ndf.head()","7800fe9a":"df.describe()","470cd99d":"df.info()","ffe43b15":"drop_cols = ['id', 'Unnamed: 32']\ndf.drop(drop_cols, axis=1, inplace=True)","b77bb08c":"rep_dict = {'B': 0.0, 'M': 1.0}\ndf['diagnosis'].replace(rep_dict, inplace=True);","de6c47bb":"print(f'Data size is {df.shape}.')","b26228cf":"df['diagnosis'].value_counts().plot(kind='bar', figsize=(8, 4));\nplt.title('Diagnosis Value Counts');\nplt.xlabel('Diagnosis');\nplt.ylabel('Frequency');\nplt.xticks([0.0, 1.0], ['Benign', 'Malignant'], rotation=45);","31d8653c":"df['anomaly'] = df['diagnosis'] == 1.0\nanomaly = df[df['anomaly'] == True]\nnormal = df[df['anomaly'] == False]","fdc4e6bd":"sns.distplot(normal);\nsns.distplot(anomaly);\n\nplt.title('normal vs  anomaly Dist.');\nplt.ylabel('Dist.');","22608481":"# The last element contains the labels\nlabels = df.iloc[:, 0]\n\n# The other data points are the electrocadriogram data\ndata = df.iloc[:, 1:31]\n\nlabels = labels.astype(bool)\n\ndata = normalize(data)","9c9ef33c":"train_data, test_data, train_labels, test_labels = train_test_split(\n    data, labels, test_size=0.2, random_state=42\n)","cb0515ad":"normal_train_data = train_data[train_labels]\nnormal_test_data = test_data[test_labels]\n\nanomalous_train_data = train_data[~train_labels]\nanomalous_test_data = test_data[~test_labels]","cfb73979":"class AnomalyDetector(Model):\n    \n    def __init__(self):\n        super(AnomalyDetector, self).__init__()\n        self.encoder = tf.keras.Sequential([\n          layers.Dense(64, activation=\"relu\"),\n          layers.Dense(32, activation=\"relu\"),\n          layers.Dense(16, activation=\"relu\")]) # bottleneck layer\n\n        self.decoder = tf.keras.Sequential([\n          layers.Dense(32, activation=\"relu\"),\n          layers.Dense(64, activation=\"relu\"),\n          layers.Dense(data.shape[1], activation=\"sigmoid\")])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nautoencoder = AnomalyDetector()\nautoencoder.compile(optimizer='adam', loss='mae')","7fdcc2a9":"history = autoencoder.fit(normal_train_data, normal_train_data, \n          epochs=20, \n          batch_size=8,\n          validation_data=(test_data, test_data),\n          shuffle=True)","8065f37b":"plt.figure(figsize=(12, 8))\nplt.plot(history.history[\"loss\"], label=\"Training Loss\");\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\");\nplt.legend();","b1a9a737":"i = 0 # you can chooce any sample from here\nencoded_data = autoencoder.encoder(normal_test_data).numpy()\ndecoded_data = autoencoder.decoder(encoded_data).numpy()\n\nplt.figure(figsize=(12, 8))\nplt.plot(normal_test_data[i], 'b')\nplt.plot(decoded_data[i], 'r')\nplt.fill_between(np.arange(30), decoded_data[i], normal_test_data[i], color='lightcoral')\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nplt.show()","e03502f4":"encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\ndecoded_data = autoencoder.decoder(encoded_data).numpy()\n\nplt.figure(figsize=(12, 8))\nplt.plot(anomalous_test_data[0], 'b')\nplt.plot(decoded_data[0], 'r')\nplt.fill_between(np.arange(30), decoded_data[0], anomalous_test_data[0], color='lightcoral')\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nplt.show()","1b7ea614":"reconstructions = autoencoder.predict(normal_train_data)\ntrain_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n\nplt.figure(figsize=(10, 8))\nplt.hist(train_loss[None,:], bins=50)\nplt.xlabel(\"Train loss\")\nplt.ylabel(\"No of examples\")\nplt.show()","cfe974ad":"threshold = np.mean(train_loss) + np.std(train_loss)\nprint(\"Threshold: \", threshold)","c5a9402e":"reconstructions = autoencoder.predict(anomalous_test_data)\ntest_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n\nplt.figure(figsize=(10, 8))\nplt.hist(test_loss[None, :], bins=50)\nplt.xlabel(\"Test loss\")\nplt.ylabel(\"No of examples\")\nplt.show()","3be2ad68":"def predict(model, data, threshold):\n    reconstructions = model(data)\n    loss = tf.keras.losses.mae(reconstructions, data)\n    return tf.math.less(loss, threshold)\n\ndef print_stats(predictions, labels):\n    print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n    print(\"Precision = {}\".format(precision_score(labels, predictions)))\n    print(\"Recall = {}\".format(recall_score(labels, predictions)))","50345221":"preds = predict(autoencoder, test_data, threshold)\nprint_stats(preds, test_labels)","4518a4d1":"Drop unnecessary columns","26d8a364":"# Detect anomalies\nDetect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set","78ec7f6b":"# Split the data","1b6799f7":"# Data Cleaning","10a99490":"<center><h3 style='color:red'>Getting Starts with Anomaly Detection and Keras<\/h3><br>KASSEM@ELCAISERI<\/center><hr>\n\n# Introduction\nThis notebook introduces Anomaly Detection with Keras applying autoencoders.\n\nAn autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error.","b405089a":"# Build the model","e10f5fb0":"# Normalize The Data","a39c0620":"# Predict and Print Stats","642dff72":"# Data Wrapping ","f19c6cff":"# Reference:\n* https:\/\/www.tensorflow.org\/tutorials\/generative\/autoencoder\n* https:\/\/www.youtube.com\/watch?v=2K3ScZp1dXQ&ab_channel=TensorFlow\n* https:\/\/www.anodot.com\/blog\/what-is-anomaly-detection\/ (theory)","1ec688b2":"# EDA and Visualization","452bf07c":"# What to do to get better results:\n* Use a deeper network \n* Use LSTM in the network\n* Feature importance and selection\n* Try different batchsize\n* Try to train it for longer time","d92049e0":"Choose a threshold value that is one standard deviations above the mean.","34c4cb77":"Convert str into float","52f45cda":"<h4>If find it interesting am wating for your feedback in the comments section, <span style='color:red'>UPVOTE<\/span> If you Like it<\/h4> <br>\n\nNotebook still under modifications","c27306db":"# Import libraries"}}