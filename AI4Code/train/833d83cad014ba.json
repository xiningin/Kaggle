{"cell_type":{"5387e002":"code","b7b4157e":"code","09a2320f":"code","473c751c":"code","c386e4b0":"code","8201c91d":"code","ea1e3d3e":"code","2dda5ee5":"code","5876e001":"code","bd095dc0":"code","bdc896f1":"code","be77f5f4":"code","35c746b5":"code","2900578f":"code","4615568f":"code","af8dc4df":"code","20cb26a7":"code","0fb8ea61":"code","cadf3343":"code","fe922fea":"code","2180b851":"code","856437ab":"code","7b962bf7":"code","9658eb43":"code","e40a03b2":"code","64703944":"code","6ba8de9e":"code","34cdca0d":"code","a7c5c240":"code","3ea0cce1":"code","44243390":"code","b66423a6":"code","fc81a887":"code","468e8220":"markdown","06345727":"markdown","2d5b4d3a":"markdown","eb64106a":"markdown","8961f091":"markdown","f6130f96":"markdown","3ec93afd":"markdown","72910e24":"markdown","c6aa88a6":"markdown","445b5295":"markdown","4c61b9f4":"markdown","f56beab8":"markdown","add74842":"markdown","39b92194":"markdown","473e9419":"markdown","714fe742":"markdown","76640d3f":"markdown","436f79ee":"markdown","7f37107b":"markdown","acb0dc81":"markdown"},"source":{"5387e002":"# Importing required libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","b7b4157e":"# Reading the dataset \n\ndf = pd.DataFrame(pd.read_csv('..\/input\/titanic-dataset-with-logistic-regression\/train (1).csv'))\ntest_data = pd.DataFrame(pd.read_csv('..\/input\/titanic-dataset-with-logistic-regression\/test.csv'))\ngender_df =  pd.DataFrame(pd.read_csv('..\/input\/titanic-dataset-with-logistic-regression\/gender_submission.csv'))","09a2320f":"df.head()","473c751c":"# Checking for the NaN values\n\nfor i in df.columns:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","c386e4b0":"df = df.drop([\"Cabin\"], axis=1)","8201c91d":"#Filling the Nan values for Age\ndf['Age'].fillna(df['Age'].median(), inplace=True)\ndf['Embarked'].fillna(df['Embarked'].mode(), inplace=True)","ea1e3d3e":"df.info()","2dda5ee5":"#We can drop PassengerId, Fare, Name, Ticket values because they are not affecting our prediction.\n\ndf = df.drop([\"PassengerId\", \"Fare\", \"Ticket\", \"Name\"], axis = 1)   ","5876e001":"from sklearn.preprocessing import LabelEncoder\n\ncat_col= df.drop(df.select_dtypes(exclude=['object']), axis=1).columns\nprint(cat_col)\n\nenc1 = LabelEncoder()\ndf[cat_col[0]] = enc1.fit_transform(df[cat_col[0]].astype('str'))\n\nenc2 = LabelEncoder()\ndf[cat_col[1]] = enc2.fit_transform(df[cat_col[1]].astype('str'))","bd095dc0":"df.head()","bdc896f1":"df.info()","be77f5f4":"# Pclass\n\nsns.FacetGrid(df, col= 'Survived').map(plt.hist,'Pclass')","35c746b5":"# Sex\n\nsns.FacetGrid(df, col='Survived').map(plt.hist, 'Sex')","2900578f":"# Age\n\nsns.FacetGrid(df, col='Survived').map(plt.hist, 'Age')","4615568f":"# SibSp\n\nsns.FacetGrid(df, col='Survived').map(plt.hist, 'SibSp')","af8dc4df":"# Parch\n\nsns.FacetGrid(df, col='Survived').map(plt.hist, 'Parch')","20cb26a7":"# Embarked\n\nsns.FacetGrid(df, col='Survived').map(plt.hist, 'Embarked')","0fb8ea61":"X = df.drop(['Survived'], axis=1)\ny = df['Survived']","cadf3343":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","fe922fea":"# model training \n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()   \nmodel.fit(X_train, y_train)","2180b851":"y_pred = model.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()","856437ab":"#Data visualisation\n\nplt.scatter([i for i in range(len(X_test[\"Age\"]))], y_test, color='red')\nplt.plot([i for i in range(len(X_test[\"Age\"]))], y_pred, color='green')\n\nplt.ylabel('Survived')\nplt.xlabel('Passenger')\n\nplt.show()","7b962bf7":"# Accuracy check\n\nfrom sklearn import metrics\n\n# Generating the roc curve using scikit-learn.\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\nplt.plot(fpr, tpr)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.show()\n\n# Measuring the area under the curve  \nprint(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n\n# Measuring the Accuracy Score\nprint(\"Accuracy score of the predictions: {0}\".format(metrics.accuracy_score(y_pred, y_test)))","9658eb43":"test_data.head()","e40a03b2":"# Displaying the column wise %ge of NaN values \n\nfor i in df.columns:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","64703944":"test_data = test_data.drop([\"Cabin\"], axis=1)\n\ntest_data['Age'].fillna(test_data['Age'].median(), inplace=True) #filling Nan values of Age\ndf['Embarked'].fillna(df['Embarked'].mode(), inplace=True)","6ba8de9e":"test_data.info()","34cdca0d":"PassengerId = test_data[\"PassengerId\"]\n\ntest_data = test_data.drop([\"PassengerId\", \"Fare\", \"Ticket\", \"Name\"], axis = 1)   #Since PassengerId, Fare, Name, Ticket does not has any role in price prediction","a7c5c240":"test_data[cat_col[0]] = enc1.transform(test_data[cat_col[0]].astype('str'))\n\ntest_data[cat_col[1]] = enc2.transform(test_data[cat_col[1]].astype('str'))","3ea0cce1":"test_data.head()","44243390":"y_test_pred = model.predict(test_data)","b66423a6":"#Data Visualization\n\nplt.scatter([i for i in range(len(test_data[\"Age\"]))], y_test_pred, color='red')\n\nplt.ylabel('Survived')\nplt.xlabel('Passenger')\n\nplt.show()","fc81a887":"submission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": y_test_pred\n    })\n\nsubmission.to_csv('.\/submission.csv', index=False)","468e8220":"> We can say tha Embarked has a fine co-realtion with our target class as its histogram against no of passager as it is shows an nice dirtibution of Target class in 3 classes.","06345727":"> Now lets visualise the relation of columns to our target in order to decide weather to add during classification or not.","2d5b4d3a":"> From above graph we can also infer that :\n   + Most of Males were not able to survive\n   + No of Males travelling on ship was greater than that of females\n   + Most of Females were survived as compared to males","eb64106a":"> From above graph we can also infer that :\n   + Most of passengers travelling from Southampton were not able to survive.","8961f091":"> Since Cabin  has very significant no of Nan values , so we can drop it for better results","f6130f96":"> Since we got an AUC score of 0.79 we can say that our classifier is not too good but acceptable. Since accuracy score is 0.81, we can say aour model is 81% accurate.","3ec93afd":"> From above graph we can also infer that :\n   + Most of Pclass 3 were not able to survive.","72910e24":"> Now we can move towards prediction","c6aa88a6":"# LOGISTIC REGRESSION \n\n#### Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.\n#### Mathematically, a logistic regression model predicts P(Y=1) as a function of X. It is one of the simplest ML algorithms that can be used for various classification problems.\n\n#### We will perform logistic regression on the famous Titanic Dataset\n\n","445b5295":"> Let's first encode the categorical data into numerical for futher analysis","4c61b9f4":"> We can say tha age has a fine co-realtion with our target class as its histogram against no of passager as it is not skewd in both the classes. ","f56beab8":"> We can say tha Pclass has a fine co-realtion with our target class as its histogram against no of passager as it is shows an nice dirtibution of Target class in 3 classes of Pclass.","add74842":"> From above graph we can also infer that :\n   + Most of passengers travelling alone were survived.","39b92194":"> We can say tha Sex has a fine co-realtion with our target class.","473e9419":"> Now let's make predictions for test dataset ","714fe742":"> From above graph we can also infer that :\n   + Most of passengers travelling alone were survived.","76640d3f":"> From above graph we can also infer that :\n   + Most of infants were not survived.\n   + Most of passengers who were not able to survive was of age grp 20-40.\n   + Oldest passenger to survive was in 80's (or of 90) .","436f79ee":"***","7f37107b":"> From above gapph we can infer that SibSp is  well co-related with our target. Most of passengers are  with either 0 or 1 sibling\/spouse. ","acb0dc81":"> From above gapph we can infer that Parch is  well co-related with our target.Most of passengers are  with either 0, 1 or 2 parents. "}}