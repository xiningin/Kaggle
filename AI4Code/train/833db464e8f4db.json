{"cell_type":{"8774aa6b":"code","0ffb55ee":"code","db37a45c":"code","5ecf2bb9":"code","985e70b7":"code","6b92de27":"code","237fc15c":"code","155f9eec":"code","f62b3897":"code","2ad02b5b":"code","48a33a4d":"code","881e8d12":"code","7fb1215f":"code","3588785a":"code","89c181e5":"code","adda2dd7":"code","4ef5ea6d":"code","dad738b5":"code","ef990ef1":"code","fb10dddc":"code","51693e38":"code","84da1c66":"code","e5297062":"code","c3f8b357":"code","5d99c59e":"code","635d4c85":"code","f7ba4357":"code","24258d57":"code","60136a61":"markdown","2edad53b":"markdown","df801643":"markdown","dbcfca3a":"markdown","6af722ec":"markdown","7afb9fce":"markdown","02b53ac5":"markdown","06a4bbfb":"markdown","9353be3b":"markdown","8630dbca":"markdown","e12024e2":"markdown","affb9080":"markdown","8ac5111f":"markdown","010c81c7":"markdown","f9dbd743":"markdown","01d321f6":"markdown","8a24421f":"markdown","d01a534e":"markdown","205f8b4f":"markdown","e95b1967":"markdown","61c504c8":"markdown","843d97ab":"markdown","33f87176":"markdown","628ff607":"markdown","3a50071f":"markdown","fb623e35":"markdown","719a5d82":"markdown"},"source":{"8774aa6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ffb55ee":"# load data set\nx_l = np.load('\/kaggle\/input\/signlanguagedigitdatauploaded-pc\/X.npy')\nY_l = np.load('\/kaggle\/input\/signlanguagedigitdatauploaded-pc\/Y.npy')\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')","db37a45c":"# Join a sequence of arrays along an row axis.\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","5ecf2bb9":"# Then lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","985e70b7":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","6b92de27":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","237fc15c":"# short description and example of definition (def)\ndef dummy(parameter):\n    dummy_parameter = parameter + 5\n    return dummy_parameter\nresult = dummy(3)     # result = 8\n\n# lets initialize parameters\n# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","155f9eec":"#w,b = initialize_weights_and_bias(4096)","f62b3897":"def initialize_weight_and_bias(dimension):\n    \n    w=np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","2ad02b5b":"# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","48a33a4d":"y_head = sigmoid(0)\ny_head","881e8d12":"# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost ","7fb1215f":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","3588785a":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","89c181e5":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","adda2dd7":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","4ef5ea6d":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension = x_train.shape[0]\n    w,b = initialize_weight_and_bias(dimension)\n    parameters,gradients,cost_list=update=(w,b,x_train,y_train,learning)","dad738b5":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","ef990ef1":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter=100)\nprint(\"Test Accuracy is : {}\".format(logreg.fit(x_train.T,y_train.T).score(x_test.T,y_test.T)))","fb10dddc":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","51693e38":"\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache\n","84da1c66":"# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost\n","e5297062":"# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","c3f8b357":"# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","5d99c59e":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","635d4c85":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","f7ba4357":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","24258d57":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","60136a61":"<a id=\"5\"><\/a> <br>\n## Initializing parameters\n* As you know input is our images that has 4096 pixels(each image in x_train).\n* Each pixels have own weights.\n* The first step is multiplying each pixels with their own weights.\n* The question is that what is the initial value of weights?\n    * There are some techniques that I will explain at artificial neural network but for this time initial weights are 0.01.\n    * Okey, weights are 0.01 but what is the weight array shape? As you understand from computation graph of logistic regression, it is (4096,1)\n    * Also initial bias is 0.\n* Lets write some code. In order to use at coming topics like artificial neural network (ANN), I make definition(method).","2edad53b":"<font color='purple'>\nUp to this point we create 2 layer neural network and learn how to implement\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model\n\n<br> Now lets learn how to implement L layer neural network with keras.","df801643":"<a id=\"14\"><\/a> <br>\n## Loss function and Cost function\n* Loss and cost functions are same with logistic regression\n* Cross entropy function\n","dbcfca3a":"* Woow, I get tired :) Up to this point we learn our parameters. It means we fit the data. \n* In order to predict we have parameters. Therefore, lets predict.\n* In prediction step we have x_test as a input and while using it, we make forward prediction.","6af722ec":"<a id=\"15\"><\/a> <br>\n## Backward propagation\n* As you know backward propagation means derivative.\n* If you want to learn (as I said I cannot explain without talking bc it is little confusing), please watch video in youtube.\n* However the logic is same, lets write code.","7afb9fce":"<a id=\"11\"><\/a> <br>\n## 2-Layer Neural Network\n* Size of layers and initializing parameters weights and bias\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learnt parameters weight and bias\n* Create Model","02b53ac5":"<a id=\"6\"><\/a> <br>\n## Forward Propagation\n* The all steps from pixels to cost is called forward propagation\n","06a4bbfb":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    *  410 means that we have 410 labels (0 and 1) \n* Lets split X and Y into train and test sets.\n    * test_size = percentage of test size. test = 15% and train = 75%\n    * random_state = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always creates same train and test distribution because we have same random_state.","9353be3b":"<a id=\"16\"><\/a> <br>\n## Update Parameters \n* Updating parameters also same with logistic regression.\n* We actually do alot of work with logistic regression","8630dbca":"<a id=\"13\"><\/a> <br>\n## Forward propagation\n* Forward propagation is almost same with logistic regression.\n* The only difference is we use tanh function and we make all process twice.\n* Also numpy has tanh function. So we do not need to implement it.","e12024e2":"<a id=\"Overview the Data Set\"><\/a> <br>\n# Overview the Data Set\n","affb9080":"* As you can see, we have 348 images and each image has 4096 pixels in image train array.\n* Also, we have 62 images and each image has 4096 pixels in image test array.\n* Then lets take transpose. You can say that WHYY, actually there is no technical answer. I just write the code(code that you will see oncoming parts) according to it :)","8ac5111f":"<a id=\"10\"><\/a> <br>\n# Artificial Neural Network (ANN)\n* It is also called deep neural network or deep learning.\n* **What is neural network:** It is basically taking logistic regression and repeating it at least 2 times.\n* In logistic regression, there are input and output layers. However, in neural network, there is at least one hidden layer between input and output layer.\n* **What is deep, in order to say \"deep\" how many layer do I need to have:** When I ask this question to my teacher, he said that \"\"Deep\" is a relative term; it of course refers to the \"depth\" of a network, meaning how many hidden layers it has. \"How deep is your swimming pool?\" could be 12 feet or it might be two feet; nevertheless, it still has a depth--it has the quality of \"deepness\". 32 years ago, I used two or three hidden layers. That was the limit for the specialized hardware of the day. Just a few years ago, 20 layers was considered pretty deep. In October, Andrew Ng mentioned 152 layers was (one of?) the biggest commercial networks he knew of. Last week, I talked to someone at a big, famous company who said he was using \"thousands\". So I prefer to just stick with \"How deep?\"\"\n* **Why it is called hidden:** Because hidden layer does not see inputs(training set)\n* For example you have input, one hidden and output layers. When someone ask you \"hey my friend how many layers do your neural network have?\" The answer is \"I have 2 layer neural network\". Because while computing layer number input layer is ignored. \n    ","010c81c7":"* We learn logic behind simple neural network(logistic regression) and how to implement it.\n* Now that we have learned logic, we can use sklearn library which is easier than implementing all steps with hand for logistic regression.\n\n\n","f9dbd743":"* Up to this point we learn \n    * Initializing parameters (implemented)\n    * Finding cost with forward propagation and cost function (implemented)\n    * Updating(learning) parameters (weight and bias). Now lets implement it.","01d321f6":"<a id=\"8\"><\/a> <br>\n## Logistic Regression with Sklearn\n* In sklearn library, there is a logistic regression method that ease implementing logistic regression.\n","8a24421f":"<a id=\"18\"><\/a> <br>\n## Create Model\n* Lets put them all together","d01a534e":"<a id=\"3\"><\/a> <br>\n# Logistic Regression\n* When we talk about binary classification( 0 and 1 outputs) what comes to mind first is logistic regression.\n* However, in deep learning tutorial what to do with logistic regression there??\n* The answer is that  logistic regression is actually a very simple neural network. \n* By the way neural network and deep learning are same thing. When we will come artificial neural network, I will explain detailed the terms like \"deep\".\n* In order to understand logistic regression (simple deep learning) lets first learn computation graph.","205f8b4f":"* We make prediction.\n* Now lets put them all together.","e95b1967":"<a id=\"22\"><\/a> <br>\n## Implementing with keras library\n","61c504c8":"<a id=\"19\"><\/a> <br>\n# L Layer Neural Network\n\n    \n    ","843d97ab":"<a id=\"12\"><\/a> <br>\n## Size of layers and initializing parameters weights and bias\n* For x_train that has 348 sample $x^{(348)}$:\n$$z^{[1] (348)} =  W^{[1]} x^{(348)} + b^{[1] (348)}$$ \n$$a^{[1] (348)} = \\tanh(z^{[1] (348)})$$\n$$z^{[2] (348)} = W^{[2]} a^{[1] (348)} + b^{[2] (348)}$$\n$$\\hat{y}^{(348)} = a^{[2] (348)} = \\sigma(z^{ [2] (348)})$$\n\n* At logistic regression, we initialize weights 0.01 and bias 0. At this time, we initialize weights randomly. Because if we initialize parameters zero each neuron in the first hidden layer will perform the same comptation. Therefore, even after multiple iterartion of gradiet descent each neuron in the layer will be computing same things as other neurons. Therefore we initialize randomly. Also initial weights will be small. If they are very large initially, this will cause the inputs of the tanh to be very large, thus causing gradients to be close to zero. The optimization algorithm will be slow.\n* Bias can be zero initially.","33f87176":"<a id=\"7\"><\/a> <br>\n##  Optimization Algorithm with Gradient Descent\n\n\n","628ff607":"<font color='orange'>\n<br>Content:\n* [Introduction](#1)\n* [Overview the Data Set](#2)\n* [Logistic Regression](#3)\n    * [Computation Graph](#4)\n    * [Initializing parameters](#5)\n    * [Forward Propagation](#6)\n        * Sigmoid Function\n        * Loss(error) Function\n        * Cost Function\n    * [Optimization Algorithm with Gradient Descent](#7)\n        * Backward Propagation\n        * Updating parameters\n    * [Logistic Regression with Sklearn](#8)\n    * [Summary and Questions in Minds](#9)\n    \n* [Artificial Neural Network](#10)\n    * [2-Layer Neural Network](#11)\n        * [Size of layers and initializing parameters weights and bias](#12)\n        * [Forward propagation](#13)\n        * [Loss function and Cost function](#14)\n        * [Backward propagation](#15)\n        * [Update Parameters](#16)\n        * [Prediction with learnt parameters weight and bias](#17)\n        * [Create Model](#18)\n    * [L-Layer Neural Network](#19)\n        * [Implementing with keras library](#22)\n\n\n","3a50071f":"* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images.","fb623e35":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).\n","719a5d82":"<a id=\"17\"><\/a> <br>\n## Prediction with learnt parameters weight and bias\n* Lets write predict method that is like logistic regression."}}