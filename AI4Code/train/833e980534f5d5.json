{"cell_type":{"74a4fcd6":"code","581fab7f":"code","f1f4ad90":"code","93c7faa4":"code","68014dfd":"code","57697c2b":"code","fab723ff":"code","af72b003":"code","9a46fb38":"code","07aded04":"code","c335cd18":"code","90e2b878":"code","011291a2":"code","59cbe859":"code","a8526b82":"code","d1576b9f":"markdown","1e006fa9":"markdown","db0b5119":"markdown","7b1fd334":"markdown","dff4cc93":"markdown","f2bb6c31":"markdown","9247c14f":"markdown","bc15f0da":"markdown","33dd3a5c":"markdown","6d4e5095":"markdown","2765b1e9":"markdown","3c3373c1":"markdown","da1d245a":"markdown"},"source":{"74a4fcd6":"import re\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom matplotlib import pyplot as plt\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\nfrom kaggle_datasets import KaggleDatasets","581fab7f":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU\/GPU\/multi-GPU\/cluster-GPU detection code\n\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","f1f4ad90":"GCS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_PATH\"","93c7faa4":"EPOCHS = 12\nIMAGE_SIZE = [331, 331]\n\nFLOWERS_DATASETS = { # available image sizes\n    192: GCS_PATH + '\/tfrecords-jpeg-192x192\/*.tfrec',\n    224: GCS_PATH + '\/tfrecords-jpeg-224x224\/*.tfrec',\n    331: GCS_PATH + '\/tfrecords-jpeg-331x331\/*.tfrec',\n    512: GCS_PATH + '\/tfrecords-jpeg-512x512\/*.tfrec'\n}\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\nassert IMAGE_SIZE[0] == IMAGE_SIZE[1], \"only square images are supported\"\nassert IMAGE_SIZE[0] in FLOWERS_DATASETS, \"this image size is not supported\"\n\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync # this is 8 on TPU v3-8, it is 1 on CPU and GPU\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","68014dfd":"def dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    label = np.argmax(label, axis=-1)  # one-hot to class number\n    correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=16, color='red' if red else 'black')\n    return subplot+1\n  \ndef display_9_images_from_dataset(dataset):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    images, labels = dataset_to_numpy_util(dataset, 9)\n    for i, image in enumerate(images):\n        title = CLASSES[np.argmax(labels[i], axis=-1)]\n        subplot = display_one_flower(image, title, subplot)\n        if i >= 8:\n            break;\n              \n    #plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()  \n\ndef display_9_images_with_predictions(images, predictions, labels):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    for i, image in enumerate(images):\n        title, correct = title_from_label_and_target(predictions[i], labels[i])\n        subplot = display_one_flower(image, title, subplot, not correct)\n        if i >= 8:\n            break;\n              \n    #plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        #plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","57697c2b":"def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ngcs_pattern = FLOWERS_DATASETS[IMAGE_SIZE[0]]\nvalidation_split = 0.19\nfilenames = tf.io.gfile.glob(gcs_pattern)\nsplit = len(filenames) - int(len(filenames) * validation_split)\nTRAINING_FILENAMES = filenames[:split]\nVALIDATION_FILENAMES = filenames[split:]\nTRAIN_STEPS = count_data_items(TRAINING_FILENAMES) \/\/ BATCH_SIZE\nVALIDATION_STEPS = -(-count_data_items(VALIDATION_FILENAMES) \/\/ BATCH_SIZE) # The \"-(-\/\/)\" trick rounds up instead of down :-)\nprint(\"TRAINING IMAGES: \", count_data_items(TRAINING_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\nprint(\"VALIDATION IMAGES: \", count_data_items(VALIDATION_FILENAMES))\n        \ndef read_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3) # pixel format uint8 [0,255] range\n    class_label = tf.cast(example['class'], tf.int32) # not used\n    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n    one_hot_class = tf.reshape(one_hot_class, [5])\n    return image, one_hot_class\n    \ndef force_image_sizes(dataset, image_size):\n    # explicit size needed for TPU\n    reshape_images = lambda image, label: (tf.reshape(image, [*image_size, 3]), label)\n    dataset = dataset.map(reshape_images, num_parallel_calls=AUTO)\n    return dataset\n\ndef load_dataset(filenames):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    dataset = force_image_sizes(dataset, IMAGE_SIZE)\n    return dataset\n\ndef data_augment(image, one_hot_class):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0, 2)\n    return image, one_hot_class   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset():\n    dataset = load_dataset(VALIDATION_FILENAMES)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","fab723ff":"validation_dataset = get_validation_dataset()","af72b003":"display_9_images_from_dataset(validation_dataset)","9a46fb38":"with strategy.scope():\n    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.xception.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n    pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False)\n\n    # alternative: EfficientNetB0\n\n    #img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.efficientnet.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n    #pretrained_model = tf.keras.applications.EfficientNetB0(include_top=False)\n\n    # alternative: load a model from Tensorflow Hub.\n    # On TPU, the load_options '\/job:localhost' is required to load models directly from TF Hub\n    # The expected image format for all TFHub image models is float32 in [0,1) range.\n\n    #img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.image.convert_image_dtype(data, tf.float32), input_shape=[*IMAGE_SIZE, 3])\n    #load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n    #pretrained_model = tfhub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\", load_options=load_locally)\n\n    # Please remove GlobalAveragePooling2D fro the model below if using EfficientNetB0 from TF Hub as it is already included.\n\n    pretrained_model.trainable = True\n\n    model = tf.keras.Sequential([\n        img_adjust_layer,\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        #tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(5, activation='softmax')\n    ])\n\nmodel.compile(\n    optimizer='adam',\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy'],\n    # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n    # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n    steps_per_execution=8\n)\n\nmodel.summary()","07aded04":"history = model.fit(get_training_dataset(), steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS,\n                    validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS,\n                    callbacks=[lr_callback])\n\nfinal_accuracy = history.history[\"val_accuracy\"][-5:]\nprint(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))","c335cd18":"display_training_curves(history.history['accuracy'][1:], history.history['val_accuracy'][1:], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'][1:], history.history['val_loss'][1:], 'loss', 212)","90e2b878":"# a couple of images to test predictions too\nsome_flowers, some_labels = dataset_to_numpy_util(get_validation_dataset(), 160)","011291a2":"# randomize the input so that you can execute multiple times to change results\npermutation = np.random.permutation(8*20)\nsome_flowers, some_labels = (some_flowers[permutation], some_labels[permutation])\n\npredictions = model.predict(some_flowers, batch_size=16)\nevaluations = model.evaluate(some_flowers, some_labels, batch_size=16)\n  \nprint(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\nprint('[val_loss, val_acc]', evaluations)\n\ndisplay_9_images_with_predictions(some_flowers, predictions, some_labels)","59cbe859":"# New in Tensorflow 2.4: models can be save locally from TPUs in Tensorflow's SavedModel format\n\n# TPUs need this extra setting to save to local disk, otherwise, they can only save models to GCS (Google Cloud Storage).\n# The setting instructs Tensorflow to retrieve all parameters from the TPU then do the saving from the local VM, not the TPU.\n# This setting does nothing on GPUs.\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nmodel.save('.\/model', options=save_locally) # saving in Tensorflow's \"saved model\" format","a8526b82":"# New in Tensorflow 2.4: models can be reloaded locally to TPUs in Tensorflow's SavedModel format\n\nwith strategy.scope():\n    # TPUs need this extra setting to load from local disk, otherwise, they can only load models from GCS (Google Cloud Storage).\n    # The setting instructs Tensorflow do the model loading on the local VM, not the TPU. Tensorflow can then still\n    # instantiate the model on the TPU if the loading call is placed in a TPUStrategy scope. This setting does nothing on GPUs.\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n    model = tf.keras.models.load_model('.\/model', options=load_locally) # loading in Tensorflow's \"saved model\" format\n\npredictions = model.predict(tf.cast(some_flowers, tf.float32), batch_size=16)\nevaluations = model.evaluate(tf.cast(some_flowers, tf.float32), some_labels, batch_size=16)\nprint(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\nprint('[val_loss, val_acc]', evaluations)\ndisplay_9_images_with_predictions(some_flowers, predictions, some_labels)","d1576b9f":"# Reload the model","1e006fa9":"\n\n---\n\n\nauthor: Martin Gorner<br>\ntwitter: @martin_gorner\n\n\n---\n\n\nCopyright 2021 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n---\n\n\nThis is not an official Google product but sample code provided for an educational purpose\n","db0b5119":"# Save the model","7b1fd334":"# Read images and labels from TFRecords","dff4cc93":"# Training","f2bb6c31":"## Visualization utilities\n\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section.","9247c14f":"# Imports","bc15f0da":"# Model","33dd3a5c":"# Configuration\nThe Flowers dataset is availabe in multiple image sizes. 331x331px is the default. 512x512px will OOM on GPU but works on TPU.","6d4e5095":"# Predictions","2765b1e9":"# TPU or GPU detection","3c3373c1":"# Kaggle dataset access\nTPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use `!ls \/kaggle\/input\/` to list attached datasets.","da1d245a":"# Training and validation datasets"}}