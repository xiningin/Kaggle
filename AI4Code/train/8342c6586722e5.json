{"cell_type":{"d1f77a38":"code","d859cfaa":"code","d5cfd205":"code","37ab8c60":"code","fd7ee0fb":"code","c11dc8b9":"code","53784824":"code","ad74e703":"code","05ea5180":"code","eff363e0":"code","57311a7b":"code","79d5f486":"code","508e4f80":"code","5a1bad29":"code","36157d63":"code","31c6e7cb":"code","ae10660e":"markdown","85749ad7":"markdown","7cde50af":"markdown","b67edf01":"markdown","96156eca":"markdown","694e3148":"markdown","f44d18ad":"markdown","9e475cb6":"markdown","1e504d81":"markdown","4a91f3ba":"markdown","798c400b":"markdown","415749e4":"markdown","94c490e4":"markdown","545e0f51":"markdown"},"source":{"d1f77a38":"import re\nimport numpy as np \nimport pandas as pd \nimport nltk\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\n\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n","d859cfaa":"data = pd.read_csv('..\/input\/Sentiment.csv')\ndata = data[['text','sentiment']]\ndata.head()","d5cfd205":"data = data[data.sentiment != \"Neutral\"]\ndata.head()","37ab8c60":"data['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata['text'] = data['text'].apply((lambda x: x.replace('rt',' ') ))\ndata.head()","fd7ee0fb":"print(data[ data['sentiment'] == 'Positive'].size)\nprint(data[ data['sentiment'] == 'Negative'].size)","c11dc8b9":"Maxlen = 0\nunique_words = set()\ncount = 0\n\nfor i in range(len(data['text'].values)):\n    txt = data['text'].values[i]\n    words = nltk.word_tokenize(txt)\n    unique_words = unique_words.union(set(words))\n    lenth = len(words)\n    count += 1\n    if lenth > Maxlen:\n        Maxlen = lenth\nprint('all_sent:',count)\nprint('max length:',Maxlen)\nprint('total unique words:',len(unique_words))","53784824":"max_fatures = 10000\nMaxlen = 30\n\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X,maxlen = Maxlen)       #\u5e8f\u5217\u8f6c\u5316\u4e3a\u7ecf\u8fc7\u586b\u5145\u4ee5\u540e\u7684\u4e00\u4e2a\u957f\u5ea6\u76f8\u540c\u7684\u65b0\u5e8f\u5217\uff0c\u75280\u586b\u5145\u3002\nX.shape","ad74e703":"Y = np.zeros(len(data['sentiment'].values))\n\nfor i in range(len(data['sentiment'].values)):\n    if data['sentiment'].values[i] == 'Positive':\n        Y[i]=1","05ea5180":"print(Y)\nprint(Y.shape)","eff363e0":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 1)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","57311a7b":"embed_dim = 128\nlstm_out = 64\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = Maxlen))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\nmodel.summary()","79d5f486":"# \u5f53\u76d1\u6d4b\u503c\u4e0d\u518d\u6539\u5584\u65f6\uff0c\u8be5\u56de\u8c03\u51fd\u6570\u5c06\u4e2d\u6b62\u8bad\u7ec3\nearlyStopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3,verbose=1)\n\n# \u5728\u6bcf\u4e2aepoch\u540e\u4fdd\u5b58\u6700\u4f18\u6a21\u578b\nmodelCheckpoint = ModelCheckpoint('..\/my_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# \u5f53\u8bc4\u4ef7\u6307\u6807\u4e0d\u518d\u63d0\u5347\u65f6\uff0c\u51cf\u5c11\u5b66\u4e60\u7387\nreduceLROnPlateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='min', min_delta=0.0001)\n\n# callbacks\u662f\u4e2a\u5217\u8868\u5f62\u5f0f\ncallbacks_list = [earlyStopping,modelCheckpoint,reduceLROnPlateau]","508e4f80":"batch_size = 32\nhistory = model.fit(X_train, Y_train, epochs = 20, batch_size=batch_size,validation_split=1\/4,verbose = 1,callbacks=callbacks_list)","5a1bad29":"score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\nprint(\"loss: %.2f\" % (score))\nprint(\"accuracy: %.2f\" % (acc))","36157d63":"test1 = ['this is a great kernel']\nprint('the sentence is\uff1a',test1[0])\n\ntest1 = tokenizer.texts_to_sequences(test1)\ntest1 = pad_sequences(test1, maxlen=Maxlen, dtype='int32', value=0)\n\nsentiment = model.predict(test1)[0][0]\n\nif sentiment > 0.5:\n    print(\"I am {:.2%} sure it's Positive\".format(sentiment))\nelse:\n    print(\"I am {:.2%}sure it's Negative\".format(1- sentiment))","31c6e7cb":"test2 = ['this is a bad kernel']\nprint('the sentence is\uff1a',test2[0])\n\ntest2 = tokenizer.texts_to_sequences(test2)\ntest2 = pad_sequences(test2, maxlen=Maxlen, dtype='int32', value=0)\n\nsentiment = model.predict(test2)[0][0]\n\nif sentiment > 0.5:\n    print(\"I am {:.2%} sure it's Positive\".format(sentiment))\nelse:\n    print(\"I am {:.2%} sure it's Negative\".format(1- sentiment))","ae10660e":"\u8bbe\u7f6e\u6a21\u578b\u7684callbacks","85749ad7":"\u4e3e\u4e24\u4e2a\u4f8b\u5b50\u6d4b\u8bd5\u4e0b\u6548\u679c\u3002","7cde50af":"LSTM\u6a21\u578b","b67edf01":"\u53ea\u505a\u6b63\u8d1f\u9762\u7684\uff0c\u5220\u9664\u4e2d\u6027\u7684\u8bc4\u8bba\u3002","96156eca":"\u52a0\u8f7d\u6570\u636e\uff0c\u53ea\u9700\u8981'text'\u548c'sentiment'\u8fd9\u4e24\u5217\u3002","694e3148":"\u4e0b\u9762\u4ee3\u7801\u662f\u627e\u51fa\u6240\u6709\u53e5\u5b50\u4e2d\u7684\u552f\u4e00\u5355\u8bcd\uff0c\u4ee5\u53ca\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\u3002\u5728\u6240\u6709\u7684\u8bed\u6599\u4e2d\uff0c\u4e00\u5171\u670915693\u4e2a\u5355\u8bcd\uff0c\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\u662f30.","f44d18ad":"\u53e5\u5b50\u5e8f\u5217\u5316\uff0c\u53d8\u6210\u53ef\u653e\u5165\u6a21\u578b\u7684\u5f62\u5f0f\u3002\u6240\u6709\u5e8f\u5217\u957f\u5ea6\u90fd\u53d8\u621030\uff0c\u5982\u679c\u4e0d\u591f\u75280\u586b\u5145\u3002","9e475cb6":"\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u3002","1e504d81":"\u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6","4a91f3ba":"\u6240\u6709\u53e5\u5b50\u91cc\u7684\u5355\u8bcd\u90fd\u53d8\u6210\u5c0f\u5199\uff0c\u53bb\u9664\u7279\u6b8a\u5b57\u7b26\uff0c\u5220\u6389rt\u3002","798c400b":"\u8bad\u7ec3\u6a21\u578b","415749e4":"\u6b63\u8d1f\u6837\u672c\u5f88\u4e0d\u5747\u8861\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u62df\u5408\u73b0\u8c61\u3002","94c490e4":"\u6b63\u8d1f\u6837\u672c\u6253\u6807\u7b7e\uff0cPositive -> 1,Negative -> 0","545e0f51":"\u9a8c\u8bc1\u6a21\u578b"}}