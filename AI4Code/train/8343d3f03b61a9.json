{"cell_type":{"07cbab47":"code","835a5a45":"code","e137271c":"code","c5d946dd":"code","fc6c86e2":"code","0f88c882":"code","79244185":"code","a7ad78fa":"code","fe4c1b9f":"code","da64b71f":"code","5eec7d48":"code","0ae7818f":"code","aaaa82b6":"code","2f7752c9":"code","b4887871":"code","25869085":"code","00eaa5b7":"code","f0e15833":"code","adac6bb5":"code","ba4f8297":"code","a0e54e46":"code","9014c87d":"code","ba0b0d0a":"code","5a9be3f9":"code","5361b3cc":"code","a2fff2dd":"code","d8ebe4e6":"code","0f38b62e":"code","af409689":"code","585094a3":"code","3f38dcb2":"code","2dae0d79":"code","a1722b5a":"code","fead6060":"code","2007b22a":"code","af936767":"code","2839c0ed":"code","3b4e520e":"code","07535f06":"code","bcc33e9a":"code","345cb73f":"code","9d1ba2e8":"code","e6a55772":"code","b280176c":"code","8008c84e":"code","7b940a4f":"code","42f2b8ed":"code","ce2a2109":"code","51fd01fa":"code","c9269d1e":"code","353c94e8":"code","b954dd4c":"code","6d1be970":"markdown","ea31fda6":"markdown","3800c6d6":"markdown","c2a57e45":"markdown","d6c393ec":"markdown","11d9b0fd":"markdown","a86729ab":"markdown","a74b1e97":"markdown","5c28105e":"markdown","f4e9eb74":"markdown","40dd3ac5":"markdown","8c851c6c":"markdown","6adf7a6f":"markdown","443df3a6":"markdown","82be401f":"markdown"},"source":{"07cbab47":"import pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport codecs\nimport re\nfrom nltk import FreqDist, word_tokenize\nfrom wordcloud import STOPWORDS\n#import tensorflow as tf\n#import tensorflow_hub as hub\n#import tensorflow_text","835a5a45":"train = pd.read_csv(r\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(r\"..\/input\/nlp-getting-started\/test.csv\")\nsample_sub = pd.read_csv(r\"..\/input\/nlp-getting-started\/sample_submission.csv\")","e137271c":"train.head(50)","c5d946dd":"train.shape","fc6c86e2":"test.head(50)","0f88c882":"test.shape","79244185":"train.info()","a7ad78fa":"train['keyword'].value_counts()","fe4c1b9f":"test['keyword'].value_counts()","da64b71f":"train.isnull().sum()","5eec7d48":"test.isnull().sum()","0ae7818f":"mising_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols = 2, figsize=(17,4), dpi = 100)\nsns.barplot(x=train[mising_cols].isnull().sum().index, y=train[mising_cols].isnull().sum().values, ax = axes[0])\nsns.barplot(x=test[mising_cols].isnull().sum().index, y = train[mising_cols].isnull().sum().values, ax = axes[1])\n\naxes[0].set_ylabel('Missing value Count', size = 15,labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training set', fontsize=13)\naxes[1].set_title('Test set', fontsize=13)\nplt.show()\n\nfor df in [train, test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","aaaa82b6":"train.head(20)","2f7752c9":"test.head()","b4887871":"def clean(text):\n    text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n    text = re.sub(r\"RT \", \" \", text) # remove rt\n    text = re.sub(r\"[^a-zA-Z\\'\\.\\,\\d\\s]\", \" \", text) # remove special character except # @ . ,\n    text = re.sub(r\"[0-9]\", \" \", text) # remove number\n    text = re.sub(r'\\t', ' ', text) # remove tabs\n    text = re.sub(r'\\n', ' ', text) # remove line jump\n    text = re.sub(r\"\\s+\", \" \", text) # remove extra white space\n    text = text.strip()\n    return text","25869085":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    # df['hashtags'].fillna(value='no', inplace=True)\n    # df['mentions'].fillna(value='no', inplace=True)\n    \n    return df\n    \ntrain = process_text(train)\ntest = process_text(test)","00eaa5b7":"train.head(50)","f0e15833":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()","adac6bb5":"kw_d = train[train.target==1].keyword.value_counts().head(10)\nkw_nd = train[train.target==0].keyword.value_counts().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","ba4f8297":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","a0e54e46":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","9014c87d":"raw_loc = train.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train[train.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","ba0b0d0a":"def clean_loc(x):\n    if x == 'None' :\n        return 'None'\n    elif x == 'Earth' or x == 'worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n    \ntrain['clean_location'] = train['location'].apply(lambda x: clean_loc(str(x)))\ntest['clean_location'] = test['location'].apply(lambda x: clean_loc(str(x)))","5a9be3f9":"top_l2 = train.groupby('clean_location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","5361b3cc":"# Make a set of stop words\nstopwords = set(STOPWORDS)","a2fff2dd":"pip install wordcloud","d8ebe4e6":"pip install \"wordcloud==1.6.0\"","0f38b62e":"# Unigrams\nword_freq = FreqDist(w for w in word_tokenize(' '.join(train['text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\ntop20w = df_word_freq.sort_values('count',ascending=False).head(20)\n\nplt.figure(figsize=(8,6))\nsns.barplot(top20w['count'], top20w.index)\nplt.title('Top 20 words')\nplt.show()","af409689":"plt.figure(figsize=(16,7))\nplt.subplot(121)\nfreq_d = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\ntop20_d = df_d.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_d['count'], top20_d.index, color='c')\nplt.title('Top words in disaster tweets')\nplt.subplot(122)\nfreq_nd = FreqDist(w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n                     (w not in stopwords) & (w.isalpha()))\ndf_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\ntop20_nd = df_nd.sort_values('count',ascending=False).head(20)\nsns.barplot(top20_nd['count'], top20_nd.index, color='y')\nplt.title('Top words in non-disaster tweets')\nplt.show()","585094a3":"# Bigrams\n\nfrom nltk import bigrams\n\nplt.figure(figsize=(16,7))\nplt.subplot(121)\nbigram_d = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==1, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nd_fq = FreqDist(bg for bg in bigram_d)\nbgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\nbgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\nbgdf_d = bgdf_d.sort_values('count',ascending=False)\nsns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\nplt.title('Top bigrams in disaster tweets')\nplt.subplot(122)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train.loc[train.target==0, 'text_clean']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(20)['count'], bgdf_nd.index[:20], color='yellow')\nplt.title('Top bigrams in non-disaster tweets')\nplt.show()","3f38dcb2":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'clean_location']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'),lsuffix='_left', rsuffix='_right')\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'),lsuffix='_left', rsuffix='_right')","2dae0d79":"from sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?:\/\/\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train['links'])\nlink_vec_test = vec_links.transform(test['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Mentions\nvec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train['mentions'])\nmen_vec_test = vec_men.transform(test['mentions'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hashtags'])\nhash_vec_test = vec_hash.transform(test['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\nprint (X_train_link.shape, X_train_men.shape, X_train_hash.shape)","a1722b5a":"_ = (X_train_link.transpose().dot(train['target']) \/ X_train_link.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(10,6))\nsns.barplot(x=_, y=_.index)\nplt.axvline(np.mean(train.target))\nplt.title('% of disaster tweet given links')\nplt.show()","fead6060":"_ = (X_train_men.transpose().dot(train['target']) \/ X_train_men.sum(axis=0)).sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=_.index, y=_)\nplt.axhline(np.mean(train.target))\nplt.title('% of disaster tweet given mentions')\nplt.xticks(rotation = 50)\nplt.show()","2007b22a":"hash_rank = (X_train_hash.transpose().dot(train['target']) \/ X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))","af936767":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n# Only include >=10 occurrences\n# Have unigrams and bigrams\ntext_vec = vec_text.fit_transform(train['text_clean'])\ntext_vec_test = vec_text.transform(test['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)","2839c0ed":"# Joining the dataframes together\n\ntrain = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_men, rsuffix='_mention')\ntrain = train.join(X_train_hash, rsuffix='_hashtag')\ntrain = train.join(X_train_text, rsuffix='_text')\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_men, rsuffix='_mention')\ntest = test.join(X_test_hash, rsuffix='_hashtag')\ntest = test.join(X_test_text, rsuffix='_text')\nprint (train.shape, test.shape)","3b4e520e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures_to_drop = ['id', 'keyword','location','text','clean_location','text_clean', 'hashtags', 'mentions','links']\nscaler = MinMaxScaler()\n\nX_train = train.drop(columns = features_to_drop + ['target'])\nX_test = test.drop(columns = features_to_drop)\ny_train = train.target\n\nlr = LogisticRegression(solver='liblinear', random_state=777) # Other solvers have failure to converge problem\n\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(X_train, y_train)\ny_test = pipeline.predict(X_test)\n\nsubmit = sample_sub.copy()\nsubmit.target = y_test\nsubmit.to_csv('submisson1.csv',index=False)","07535f06":"print ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))","bcc33e9a":"# F-1 score\nfrom sklearn.metrics import f1_score\n\nprint ('Training f-1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))","345cb73f":"from sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))","9d1ba2e8":"# Cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\ncv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score))","e6a55772":"# Top features\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()","b280176c":"# Feature selection\nfrom sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features\/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n\npipeline2 = Pipeline([('scale',scaler), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()","8008c84e":"print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])","7b940a4f":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]","42f2b8ed":"# lr2 = LogisticRegression(solver='liblinear', random_state=37)\npipeline.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\ncv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score2))","ce2a2109":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nlr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n\npipeline_grid = Pipeline([('scale',scaler), ('gridsearch', lr_cv),])\n\npipeline_grid.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", lr_cv.best_params_)\nprint(\"F-1 score: %.3f\" %lr_cv.best_score_)\n","51fd01fa":"y_test2 = pipeline_grid.predict(X_test2)\nsubmit2 = sample_sub.copy()\nsubmit2.target = y_test2\nsubmit2.to_csv('.\/submission2.csv',index=False)","c9269d1e":"# Top features with fine-tuned model\nplt.figure(figsize=(16,7))\ns1 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values(ascending=False)[:20]\ns2 = pd.Series(np.transpose(lr.coef_[0]), index=X_train2.columns).sort_values()[:20]\nplt.subplot(121)\nsns.barplot(y=s1.index, x=s1)\nplt.title('Top positive coefficients')\nplt.subplot(122)\nsns.barplot(y=s2.index, x=s2)\nplt.title('Top negative coefficients')\nplt.show()","353c94e8":"# Error analysis\ny_hat = pipeline_grid.predict_proba(X_train2)[:,1]\nchecker = train.loc[:,['text','keyword','location','target']]\nchecker['pred_prob'] = y_hat\nchecker['error'] = np.abs(checker['target'] - checker['pred_prob'])\n\n# Top 50 mispredicted tweets\nerror50 = checker.sort_values('error', ascending=False).head(50)\nerror50 = error50.rename_axis('id').reset_index()\nerror50.target.value_counts()","b954dd4c":"pd.options.display.max_colwidth = 200\n\nerror50.loc[0:10,['text','target','pred_prob']]","6d1be970":"## Hypertuning","ea31fda6":"## TD IDF Count","3800c6d6":"## Task 3. Data Cleaning and Pre-processing","c2a57e45":"## Task 4. Feature Engineering\n\n### 1. Encoding and Vectorizers\nAs part of feature generation, we will:\n\n1. Apply target encoding to keyword and location (cleaned)\n2. Count Vectorize cleaned text, links, hashtags and mentions columns","d6c393ec":"Among the top 50 mispredicted tweets, only 4 are false positive","11d9b0fd":"Reference by - Saurabh Prakash Giri","a86729ab":"## Logistic Regression","a74b1e97":"### Task 1. Understanding the problem statement and Importing Data set\n1.  build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. ","5c28105e":"Findings:\n\n- 'keyword_target' is the top positive coefficient, meaning the keyword column made a good feature\n- hiroshima both as text and hashtag made the top 20 positive coefficients\n- Punctuation count and stop word count are among top 20 negative coefficients\n- None of the bigrams made the top features","f4e9eb74":"### F1- Score","40dd3ac5":"## Output data for submission","8c851c6c":"### Confusion Matrix","6adf7a6f":"We then pick up the 1133 selected features to do Grid Search CV to find optimal hyperparameters","443df3a6":"Next, we inspect the tweets that predicted probability differs the most from target outcome","82be401f":"# Task  2. Exploratory data analysis"}}