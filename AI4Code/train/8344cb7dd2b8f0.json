{"cell_type":{"4e31af92":"code","ab708974":"code","9a911092":"code","d11ecc62":"code","57c8e0d2":"code","8f735e14":"code","3954abd5":"code","fafc5df6":"code","fe1aa36f":"code","3a088933":"code","ad311ba1":"code","5de9613a":"code","cacd14d5":"code","9aa77ed0":"code","28310332":"code","7c182f3a":"code","18072fa0":"code","2063fce8":"code","d8bcc194":"code","446a6a91":"code","445efd94":"code","5920a0df":"code","8f6c49ae":"code","952e3cf4":"code","c7165d47":"code","fa0ffac4":"markdown","b1e34175":"markdown","1bc47ef5":"markdown","daad61bd":"markdown","ff9c9d68":"markdown","93c7ce52":"markdown"},"source":{"4e31af92":"# let's start by importing the files \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# As we can see the input in is zip files ","ab708974":"# let's extract these files first\nimport zipfile\n\nzip_files=['\/kaggle\/input\/spooky-author-identification\/train.zip','\/kaggle\/input\/spooky-author-identification\/test.zip']\nfor zip_file in zip_files:\n    with zipfile.ZipFile(zip_file,'r') as z:\n        z.extractall()\n","9a911092":"os.listdir()\n# As we can see now the files have been extracted and now we have our train and text csv files","d11ecc62":"# Since we will we dealing with text in English Language let's import the stop words for english.\n\n\nfrom nltk.corpus import stopwords\n\n# these are the general words i.e ( a , the , is , an , i me , my) which are very general and does not \n#make much sense so we will fiter out these words and will consider the more robust words in our text\n# example ( he is a dangerous man , here is ,a can be skipped and we get the essence of the sentence from the \n# word dangerous )","57c8e0d2":"# let's look at some of the stop words\nstop=stopwords.words('english')\nstop","8f735e14":"# let's read our data into our dataframes\ntrain=pd.read_csv('train.csv',index_col=False)\ntest=pd.read_csv('test.csv',index_col=False)\n","3954abd5":"# let's take a peek at our data\ntrain.sample(10)","fafc5df6":"#let's see how our target variable is distributed in our overall  data\nimport seaborn as sns\nsns.countplot(train['author'])","fe1aa36f":"# since our target varibale has 3 different values we will now convert these into 0,1,2 using the label encoder\n\nfrom sklearn.preprocessing import LabelEncoder\ny=train.loc[:,['author']]\nen=LabelEncoder()\ny=en.fit_transform(y.author.values)\n","3a088933":"# now let's split our data into train and test datasets\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(train.text,y,stratify=y,test_size=.25,shuffle=True)\nsns.countplot(y_train)","ad311ba1":"sns.countplot(y_test)","5de9613a":"# defining the loss function ( log_loss)\nfrom sklearn.metrics import log_loss","cacd14d5":"# let's import the TfidfVectorizer and see how it works\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","9aa77ed0":"# let's create and instance of the vectorizer \n\ntfid=TfidfVectorizer(stop_words='english')\n\nxx=['hello today we will try' ,\n   'to understand how  tf-idf works',\n   'this is sample text for this nlp program']\n\ncc=tfid.fit_transform(xx)\n\ndd=pd.DataFrame(cc.toarray(),columns=tfid.get_feature_names())\ndd\n\n# I have created a dataframe to demonstrate what happens under the hood\n\n# In the text we have 4 lines, \n\n# let's start with first line which has 4 distinct words but notice that (will and we got dropped because those\n# are stop words and the rest of the words are assigned a numeric value which depends on how many times the word\n# appeared in the line and how many times the words appears in all the text)\n\n# there is math involved in this process but that is beyond the scope of this notebook, so let's try to understand\n# the working and leave the math for another notebook.","28310332":"# we pass all the text into our vectorizer and fit it\ntfid.fit(list(x_train)+list(x_test))","7c182f3a":"# we then transform our x_train and x_test using this vetcorizer \nx_train=tfid.transform(x_train)\n\n","18072fa0":"# if we want to understand this process we can think of it as we tell our model these are the total words we have in\n# our text ,  \n#1.please remove the stop words,\n#2. Please assign a numeric value to each word so that we can feed it to our machine learning model\nx_test=tfid.transform(x_test)","2063fce8":"# Now that we have our data in numeric Values we can implement any of our Machine learning models\n\n# let's try a simple linear model , Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlgr=LogisticRegression(C=1.0)\n","d8bcc194":"# let's fit our model\nlgr.fit(x_train,y_train)","446a6a91":"# now our target variable is in shape (4895,) but we want it to be in shape (4895,3) because our data has there\n# different classes so let's do binarization of our target variable \nx_test\na=lgr.predict_proba(x_test)\ndef conver(actual,a):\n    if len(actual.shape)==1:\n        temp=np.zeros((actual.shape[0],a.shape[1]))\n        for i,j in enumerate(actual):\n            temp[i,j]=1\n        actual=temp    \n    return actual\naa=conver(y_test,a)\n\na.shape\n\n\n# To put it in simple words , intially our target ( y_test ) was in form 2,1,0,0,0,1,2,1 but we want it to \n#be in th form \n[[1,0,0],\n [0,1,0],\n [0,1,0]]\n","445efd94":"# now that our predictions and target are in the same shape we can calculate the log loss\naa.shape","5920a0df":"\nlog_loss(aa,a)","8f6c49ae":"# let's try to use a support vetcor classifier for the same data\nfrom sklearn.svm import SVC\nsv_model=SVC(probability=True)\nsv_model.fit(x_train,y_train)\n\n\n","952e3cf4":"predicted=sv_model.predict_proba(x_test)\npredicted.shape","c7165d47":"# As we can see our SVC model performs better than the Logistic Regression model\nlog_loss(y_test,predicted)","fa0ffac4":"# If you like the Notebook kindly upvote and if you are interested please check my other notebooks, if you any questions or suggestions you can write in the comments.\n","b1e34175":"In this notebook we will cover the following\n* How to extract data out of the zip files provided by Kaggle.\n* How to use Tf-idf and convectors \n* How to make a simple classification model using text input\n","1bc47ef5":"In this notebook we will try to understand how to make sense of our textual data and methods to make a classification model for the textual data, So without wasting any time let's jump into it","daad61bd":"# **Many times while creating machine learning models we come across textual data so this notebook will help you get familiar with it and how to build a model with text input** ","ff9c9d68":"# please pay attention as this is the most import part of this notebook, We will try to understand how the Tfidf(term frequency - inverse document frequency) works","93c7ce52":"Here we have some text in XX varibale and we will try to fit the Tfidf to this text"}}