{"cell_type":{"016698a6":"code","411c66b9":"code","5c83d711":"code","b0c521bc":"code","7d587bb3":"code","067b3cf0":"code","b2af669a":"code","19df9c99":"code","3fa1feea":"code","21ca11aa":"code","e0b3081d":"code","9a01619e":"code","54b5ef1b":"code","6c65d4d6":"code","2e5fc60d":"code","648fc3dd":"code","48198a46":"code","fcf7d6f6":"code","2538a73e":"code","7f853a21":"code","a98865af":"code","f31765d8":"code","d34645ad":"code","9705f28d":"code","08aa6631":"code","e6782ec9":"code","59c7bb06":"code","827ed7cb":"code","ee7417a8":"code","51c5c0e0":"code","9deb5902":"code","aed2ea7b":"code","4a90fb7b":"code","edebe0d5":"code","69a317b7":"code","4b063e75":"code","c3adaa7e":"code","5dc7d890":"code","128dc9b2":"code","5a10e538":"code","e214b916":"code","1a297e0e":"code","e832027c":"code","4ef2d6d3":"code","60465b12":"code","3124fdac":"code","4083f6f5":"code","8edf21a7":"code","50f8ad20":"code","725ff43f":"code","454c87b0":"code","0fb2b759":"code","6f48f27c":"code","956a26f8":"code","0670abb8":"code","97b4fab5":"code","8c5dec8a":"code","37c47054":"code","52757c48":"code","1cd6fec4":"code","c00a0bc2":"code","dec538c4":"code","f298da34":"code","c02756aa":"code","3989d2c7":"markdown","2a9e6d74":"markdown","f65e98cd":"markdown","9c8806b0":"markdown","0e0dd69a":"markdown","94152672":"markdown","bcee9d55":"markdown","cdfce311":"markdown","775dbe9d":"markdown"},"source":{"016698a6":"#import libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport warnings\n\n#suppress warnings\nwarnings.filterwarnings(\"ignore\")","411c66b9":"#import data\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')","5c83d711":"train_data.head()","b0c521bc":"train_data.info()","7d587bb3":"train_data.describe()","067b3cf0":"train_data.describe().columns","b2af669a":"train_data.select_dtypes(include='object').columns","19df9c99":"#numerical variables\nnumerical = [\n    'PassengerId',\n    'Age',\n    'SibSp',\n    'Parch',\n    'Fare'\n]\n\n#categorical variables\ncategorical = [\n    'Name',\n    'Sex',\n    'Pclass',\n    'Ticket',\n    'Cabin',\n    'Embarked',\n    'Survived'\n]","3fa1feea":"#numerical data distribution\nfor i in train_data[numerical].columns:\n    plt.hist(train_data[numerical][i])\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","21ca11aa":"#categorical data distribution\nfor i in train_data[categorical].columns:\n    sns.barplot(x=train_data[categorical][i].value_counts().index,y=train_data[categorical][i].value_counts())\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","e0b3081d":"#splits the ticket column into three parts and puts \"empty\" if ticket part does not exist\n\ndef split_ticket(df):\n    \n    ticket_part_1 = []\n    ticket_part_2 = []\n    ticket_part_3 = []\n    \n    for ticket in df.Ticket:\n\n        if len(ticket.split(' ')) == 2:\n            part_2, part_1 = ticket.split(' ')\n            ticket_part_1.append(part_1)\n            ticket_part_2.append(part_2)\n            ticket_part_3.append('empty')\n\n        elif len(ticket.split(' ')) == 3:\n            part_3, part_2, part_1 = ticket.split(' ')\n            ticket_part_1.append(part_1)\n            ticket_part_2.append(part_2)\n            ticket_part_3.append(part_3)\n\n        else:\n            ticket_part_1.append(ticket)\n            ticket_part_2.append('empty')\n            ticket_part_3.append('empty')\n            \n    split_ticket = pd.DataFrame([np.array(ticket_part_3), np.array(ticket_part_2), np.array(ticket_part_1)]).T\n    \n    split_ticket.columns = ['Ticket_3', 'Ticket_2', 'Ticket_1']\n    \n    return split_ticket","9a01619e":"#fixing what looks like misprints in ticket column\n\ndef fix_ticket(df):\n    \n    for i in df.Ticket:\n        if 'STON\/O2.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'STON\/O 2. {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'SOTON\/O2 ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'STON\/O 2. {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'A\/5.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/5 {}'.format(num)\n\n    for i in df.Ticket:\n        if 'A.\/5.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/5 {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'A.5.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/5 {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'A\/S' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/5 {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'A\/4.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/4 {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'A4.' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'A\/4 {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'SOTON\/OQ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'SOTON\/O.Q. {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'C.A.\/SOTON' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'SOTON\/C.A. {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'CA ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'C.A. {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'CA. ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'C.A. {}'.format(num)\n    \n    for i in df.Ticket:\n        if 'SC\/Paris ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'PC\/PARIS {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'S.C.\/PARIS ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'PC\/PARIS {}'.format(num)\n            \n    for i in df.Ticket:\n        if 'W\/C ' in i:\n            throw_out, num = i.split(' ')\n            df.Ticket[df.Ticket == i] = 'W.\/C. {}'.format(num)\n    \n            \n    return df","54b5ef1b":"#fix then split training data\ntrain_data = fix_ticket(train_data)\nsplit_train = split_ticket(train_data)","6c65d4d6":"#fix then split test data\ntest_data = fix_ticket(test_data)\nsplit_test = split_ticket(test_data)","2e5fc60d":"#make a two column ticket dataframe where column 1 is the number part and column two is the string part\n\ndef two_column_ticket(df):\n    \n    ticket_column_1 = []\n    ticket_column_2 = []\n    \n    for ticket in df.Ticket:\n\n        if len(ticket.split(' ')) == 2:\n            part_2, part_1 = ticket.split(' ')\n            ticket_column_1.append(part_1)\n            ticket_column_2.append(part_2)\n\n        elif len(ticket.split(' ')) == 3:\n            part_3, part_2, part_1 = ticket.split(' ')\n            ticket_column_1.append(part_1)\n            ticket_column_2.append('{} {}'.format(part_3,part_2))\n            \n        elif 'LINE' in ticket:\n            ticket_column_1.append(np.nan)\n            ticket_column_2.append(ticket)\n\n        else:\n            ticket_column_1.append(ticket)\n            ticket_column_2.append('empty')\n            \n    two_column_ticket = pd.DataFrame([np.array(ticket_column_2), np.array(ticket_column_1)]).T\n    \n    two_column_ticket.columns = ['Ticket_2', 'Ticket_1']\n    \n    return two_column_ticket\n    ","648fc3dd":"#make two two-column dataframes (one for training one for test)\ntwo_column_train = two_column_ticket(train_data)\ntwo_column_test = two_column_ticket(test_data)","48198a46":"#add two column ticket to each dataframe\ntrain_data['Ticket_Num'] = two_column_train.Ticket_1\ntest_data['Ticket_Num'] = two_column_test.Ticket_1\n\ntrain_data['Ticket_Prefix'] = two_column_train.Ticket_2\ntest_data['Ticket_Prefix'] = two_column_test.Ticket_2\n\n#add new columns to numerical and categorical variable arrays\nnumerical.append('Ticket_Num')\ncategorical.append('Ticket_Prefix')\n\n#remove ticket columns and remove ticket variable from categorical\ntrain_data = train_data.drop('Ticket',axis=1)\ntest_data = test_data.drop('Ticket',axis=1)\ncategorical.remove('Ticket')","fcf7d6f6":"#get titles from name column (e.g., Mr., Ms., Dr.)\n\nimport re\n\ndef get_titles(df):\n    \n    titles=[]\n\n    for i in df.Name:\n        \n        titles.append(re.findall(r'(?<=,\\s)[a-z]+.',i,re.I))\n        \n    return titles","2538a73e":"titles_train = get_titles(train_data)\ntitles_test = get_titles(test_data)\n\ntrain_data['Title'] = titles_train\ntest_data['Title'] = titles_test\n\ncategorical.append('Title')","7f853a21":"for i in range(len(train_data.Title)):\n    train_data.Title[i] = train_data.Title[i][0]","a98865af":"for i in range(len(test_data.Title)):\n    test_data.Title[i] = test_data.Title[i][0]","f31765d8":"def get_last_names(df):\n    \n    last_names=[]\n\n    for i in df.Name:\n        \n        last_names.append(i.split(',')[0])\n        \n    return last_names","d34645ad":"last_names_train = get_last_names(train_data)\nlast_names_test = get_last_names(test_data)\n\ntrain_data['Last_Name'] = last_names_train\ntest_data['Last_Name'] = last_names_test\n\ncategorical.append('Last_Name')","9705f28d":"train_data = train_data.drop('Cabin', axis=1)\ntest_data = test_data.drop('Cabin', axis=1)\ncategorical.remove('Cabin')","08aa6631":"train_data = train_data.drop('Name', axis=1)\ntest_data = test_data.drop('Name', axis=1)\ncategorical.remove('Name')","e6782ec9":"train_data = train_data.drop('PassengerId',axis=1)\npassenger_id = test_data.pop('PassengerId')\nnumerical.remove('PassengerId')","59c7bb06":"train_data.head()","827ed7cb":"train_data.describe()","ee7417a8":"#numerical data distribution\nfor i in train_data[numerical].columns:\n    plt.hist(train_data[numerical][i])\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","51c5c0e0":"#categorical data distribution\nfor i in train_data[categorical].columns:\n    sns.barplot(x=train_data[categorical][i].value_counts().index,y=train_data[categorical][i].value_counts())\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","9deb5902":"pd.pivot_table(train_data, values='Survived', index=['Embarked'], columns=['Pclass'], aggfunc=np.mean)","aed2ea7b":"pd.pivot_table(train_data, values='Survived', index=['Sex'], columns=['Pclass'], aggfunc=np.mean)","4a90fb7b":"#heat map to see numerical correlations, pearson measures monotonic relationship (numerical or ordinal categorical)\nplt.figure(figsize=(16, 6))\nsns.heatmap(train_data[numerical].corr(method='pearson'), vmin=-1, vmax=1, annot=True)\nplt.title('Pearson Correlation Heatmap for Numerical Variables', fontdict={'fontsize':12}, pad=12);","edebe0d5":"#look at how target is distributed among variables\nsns.pairplot(train_data,hue='Survived')\nplt.legend()\nplt.show()","69a317b7":"train_data.Title.unique()","4b063e75":"categorical.remove('Survived')\n\nX_train = train_data.copy()\ny_train = X_train.pop('Survived')\n\nX_test = test_data.copy()","c3adaa7e":"from sklearn.impute import SimpleImputer\n\nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')\nimp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nfor i in categorical:\n    X_train[i] = imp_mode.fit_transform(X_train[i].values.reshape(-1,1))\n    X_test[i] = imp_mode.fit_transform(X_test[i].values.reshape(-1,1))\nfor i in numerical:\n    X_train[i] = imp_median.fit_transform(X_train[i].values.reshape(-1,1))\n    X_test[i] = imp_median.fit_transform(X_test[i].values.reshape(-1,1))","5dc7d890":"#import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n#get feature names\nX_train = pd.concat([X_train[numerical],pd.get_dummies(X_train[categorical])],axis=1)\n\n#define scaler\nscaler=MinMaxScaler()\n\n#apply preprocessing to split data with scaler\nX_train[numerical] = scaler.fit_transform(X_train[numerical])","128dc9b2":"#combine numerical and categorical\/dummy variables into one dataframe\nX_test = pd.concat([X_test[numerical],pd.get_dummies(X_test[categorical])],axis=1)\n\n#scale test data\nX_test[numerical] = scaler.transform(X_test[numerical])","5a10e538":"#any dummy variables not in both train and test\ndifference = list(set(X_train.columns)-set(X_test.columns)) + list(set(X_test.columns)-set(X_train.columns))\n\n#add columns to make train and test dataframes uniform\nfor i in difference:\n    if i not in X_train.columns:\n        X_train[i] = pd.DataFrame(index=range(len(X_train)),columns=[i])\n        X_train.loc[:,i] = 0\n    elif i not in X_test.columns:\n        X_test[i] = pd.DataFrame(index=range(len(X_test)),columns=[i])\n        X_test.loc[:,i] = 0    ","e214b916":"#import ml algorithms\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom numpy import mean, std\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import VotingClassifier","1a297e0e":"#naive Bayes with five-fold cross validation\ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","e832027c":"#logistic regression with five-fold cross validation\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","4ef2d6d3":"#decession tree with five-fold cross validation\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","60465b12":"#random forest classifier with five-fold cross validation\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","3124fdac":"#support vector classifier with five-fold cross validation\nsvc = SVC(probability = True)\ncv = cross_val_score(svc,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","4083f6f5":"#k-nearest neighbors classifier with five-fold cross validation\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","8edf21a7":"#xgboost classifier with five-fold cross validation\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\ncv = cross_val_score(xgb,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","50f8ad20":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV \n\n#performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(classifier.best_score_),str(classifier.cv_results_['std_test_score'][classifier.best_index_])))\n    print('Best Parameters: ' + str(classifier.best_params_))","725ff43f":"#naive Bayes performance tuner\ngnb = GaussianNB()\nparam_grid = {\n              'var_smoothing': np.logspace(0,-10, num=100)\n             }\nclf_lr = GridSearchCV(gnb, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_gnb = clf_lr.fit(X_train,y_train)\nclf_performance(best_clf_gnb,'Naive Bayes')","454c87b0":"#logistic regression performance tuner\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [15000],\n              'C' : np.arange(.5,1.5,.1)\n             }\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","0fb2b759":"#decision tree performance tuner\ndt = tree.DecisionTreeClassifier(random_state = 1)\nparam_grid = {\n             'criterion':['gini','entropy'],\n             'max_depth': np.arange(1, 15)\n             }\nclf_dt = GridSearchCV(dt, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_dt = clf_dt.fit(X_train,y_train)\nclf_performance(best_clf_dt,'Decision Tree')","6f48f27c":"#k-nearest neighbors classifier performance tuner\nknn = KNeighborsClassifier()\nparam_grid = {\n              'n_neighbors' : np.arange(5,50,5),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree','brute'],\n              'p' : [2,3,4,5]\n             }\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train,y_train)\nclf_performance(best_clf_knn,'K-Nearest Neighbors Classifier')","956a26f8":"#random forest performance tuner\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {\n                'n_estimators': np.arange(100,300,10), \n                'bootstrap': [False], #bagging (T) vs. pasting (F)\n                'max_depth': np.arange(4,16,2),\n                'max_features': ['auto'],\n                'min_samples_leaf': [2],\n                'min_samples_split': [2]\n              }\nclf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')","0670abb8":"#support vector classifier performance tuner\nsvc = SVC(probability = True, random_state = 1)\nparam_grid = {\n              'kernel': ['linear','poly','sigmoid','rbf'],\n              'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4],\n#                'kernel': ['rbf'],\n#                'gamma': [1],\n               'C': np.arange(1,2,.1)\n             }\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train,y_train)\nclf_performance(best_clf_svc,'Support Vector Classifier')","97b4fab5":"#xgboost classifier performance tuner\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\nparam_grid = {\n#               'max_depth': np.arange(2,12,2),\n              'n_estimators': np.arange(50,200,50),\n#               'learning_rate': [1.2]\n             }\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,y_train)\nclf_performance(best_clf_xgb,'XGBoost Classifier')","8c5dec8a":"#stacking def\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.04862601580065353)))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)))\n    level0.append(('lr', LogisticRegression(C= 1.3, max_iter= 15000)))\n    level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')))\n    level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)))\n    level0.append(('svc', SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return stacking_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.04862601580065353)\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)\n    models['lr'] = LogisticRegression(C= 1.3, max_iter= 15000)\n    models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')\n    models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)\n    models['svc'] = SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)\n    models['stacking'] = get_stacking()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","37c47054":"#stacking def\ndef get_hard_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.04862601580065353)))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)))\n    level0.append(('lr', LogisticRegression(C= 1.3, max_iter= 15000)))\n    level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')))\n    level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)))\n    level0.append(('svc', SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)))\n    hard_voting_model = VotingClassifier(estimators=level0, voting='hard')\n    return hard_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.04862601580065353)\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)\n    models['lr'] = LogisticRegression(C= 1.3, max_iter= 15000)\n    models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')\n    models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)\n    models['svc'] = SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)\n    models['hv'] = get_hard_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","52757c48":"#stacking def\ndef get_soft_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.04862601580065353)))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)))\n    level0.append(('lr', LogisticRegression(C= 1.3, max_iter= 15000)))\n    level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')))\n    level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)))\n    level0.append(('svc', SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)))\n    # define meta learner model\n    soft_voting_model = VotingClassifier(estimators=level0, voting='soft')\n    return soft_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.04862601580065353)\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)\n    models['lr'] = LogisticRegression(C= 1.3, max_iter= 15000)\n    models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 20, p= 2, weights= 'distance')\n    models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False,max_depth= 12, max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 140)\n    models['svc'] = SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, n_estimators=100)\n    models['sv'] = get_soft_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","1cd6fec4":"dt_model = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 13)\ndt_model.fit(X_train,y_train)\npredictions1 = dt_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions1})\noutput.to_csv('submission1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c00a0bc2":"svc_model = SVC(probability = True, random_state = 1, gamma = .1, kernel= 'poly',C=1.6)\nsvc_model.fit(X_train,y_train)\npredictions2 = svc_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions2})\noutput.to_csv('submission2.csv', index=False)\nprint(\"Your submission was successfully saved!\")","dec538c4":"stacking_model = get_stacking()\nstacking_model.fit(X_train,y_train)\npredictions3 = stacking_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions3})\noutput.to_csv('submission3.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f298da34":"hv_model = get_hard_voting()\nhv_model.fit(X_train,y_train)\npredictions4 = hv_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions4})\noutput.to_csv('submission4.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c02756aa":"sv_model = get_soft_voting()\nsv_model.fit(X_train,y_train)\npredictions5 = sv_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': passenger_id, 'Survived': predictions5})\noutput.to_csv('submission5.csv', index=False)\nprint(\"Your submission was successfully saved!\")","3989d2c7":"# Prediction and Submission","2a9e6d74":"# Cleaning\n\n1. Seperate ticket into two parts: prefix and number\n2. Seperate name into two parts: title and last name\n3. Remove ticket, name, cabin, and id columns","f65e98cd":"# Ensemble Classifiers","9c8806b0":"# Soft Voting Classifier Test Accuracy: 0.78468","0e0dd69a":"# Hyperparameter Tuning","94152672":"# ML Model Baselines","bcee9d55":"# EDA after Cleaning","cdfce311":"# Initial EDA","775dbe9d":"# Imputation, Create Dummies, and Scale"}}