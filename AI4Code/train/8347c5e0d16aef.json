{"cell_type":{"936b927b":"code","2472b3df":"code","c5c42a0b":"code","249301c7":"code","f59c0945":"code","3d084ae5":"code","44b7ec8c":"code","dc090924":"code","3d59ddcb":"code","1351b97b":"code","92e84b8c":"code","d7d727d4":"code","5281d572":"code","61ed3dd1":"code","6433ce35":"code","2233ec68":"code","091e8244":"code","14c2435c":"code","c7d1e911":"code","7d4c7540":"code","5c50e329":"code","7502e78c":"code","6588d8d4":"code","b3c47095":"code","17f0121e":"code","f2815c1c":"code","f22c8fb2":"code","9e08363c":"code","dadaee8c":"code","da995ec4":"code","457329b5":"code","af1dd1d5":"code","ccae39ad":"code","6b1e9fff":"code","f4e6ac04":"code","32a97187":"code","26135c29":"code","b3247a4a":"code","75450e33":"code","757f4a91":"code","7edb6319":"code","e01f5f54":"code","5895ecaa":"code","7866a7bd":"code","3086996b":"code","49dbd8a7":"code","1a36fa2b":"code","da1defa9":"code","3fcd04b9":"code","b4539143":"code","6fd8497f":"code","d06e7796":"code","7746be98":"code","ce9290ae":"markdown","e99aaaeb":"markdown","822648f9":"markdown","7a6dd973":"markdown","e32558ac":"markdown","57c250f1":"markdown","fb911906":"markdown","aedf5fc6":"markdown","6babf20d":"markdown","d9d6cd7f":"markdown","c080c259":"markdown","649876af":"markdown","63a3bd58":"markdown","1c30a7c0":"markdown","c2b6ca6e":"markdown","c82229a9":"markdown","e3cf3a01":"markdown","0779393e":"markdown","9b9fe4aa":"markdown","467bcc63":"markdown","8120611a":"markdown","6d1925c1":"markdown","5ead2b9a":"markdown","f7c64e8c":"markdown","13690da3":"markdown","fb209163":"markdown","3bb4fa39":"markdown","52f17f34":"markdown","1fc8dc90":"markdown","ab7cb789":"markdown","8333953b":"markdown","db5c8d56":"markdown","9142d62b":"markdown","7bf0ba9e":"markdown","64ddf0dd":"markdown","4cc54d50":"markdown","225562a3":"markdown","c408828b":"markdown","73456fb2":"markdown","1f3a8596":"markdown","cfcabff2":"markdown","f82c9fd8":"markdown","df1ecfdd":"markdown","81c6b465":"markdown","9d7a6d80":"markdown","becd0bc4":"markdown","1326bebd":"markdown","be86c611":"markdown","43cd4045":"markdown","cee8db06":"markdown","66bca784":"markdown","5b257df3":"markdown","88f4f0dc":"markdown","3768beaf":"markdown","ef6f1c77":"markdown","6464b8c1":"markdown","ae5bd1e7":"markdown","4e7ef2a1":"markdown","ba3f32aa":"markdown","010dbf4a":"markdown","c30a50d4":"markdown","9c28c5ea":"markdown","9b791ad9":"markdown","27cf77d9":"markdown","ed443bf3":"markdown","56a1ba92":"markdown","ca7a2d2d":"markdown","34e44f92":"markdown","9ed6beb3":"markdown","78a5fa77":"markdown","2ab0afa4":"markdown","21c9cea5":"markdown","c19eba55":"markdown","83cd147c":"markdown","f0aa6613":"markdown"},"source":{"936b927b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import tokenize,stem\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nimport lightgbm as lgb\nimport nltk\nfrom nltk.util import ngrams\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud, STOPWORDS\nimport shap\nshap.initjs()","2472b3df":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5c42a0b":"df = pd.read_csv(\"\/kaggle\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv\")\ndf.head(3)","249301c7":"df.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndf.rename(columns={'Data':'Date', 'Countries':'Country', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)\ndf.head(3)","f59c0945":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].apply(lambda x : x.year)\ndf['Month'] = df['Date'].apply(lambda x : x.month)\ndf['Day'] = df['Date'].apply(lambda x : x.day)\ndf['Weekday'] = df['Date'].apply(lambda x : x.day_name())\ndf['WeekofYear'] = df['Date'].apply(lambda x : x.weekofyear)\ndf.head(3)","3d084ae5":"def month2seasons(x):\n    if x in [9, 10, 11]:\n        season = 'Spring'\n    elif x in [12, 1, 2]:\n        season = 'Summer'\n    elif x in [3, 4, 5]:\n        season = 'Autumn'\n    elif x in [6, 7, 8]:\n        season = 'Winter'\n    return season","44b7ec8c":"df['Season'] = df['Month'].apply(month2seasons)\ndf.head(3)","dc090924":"STOPWORDS.update([\"cm\", \"kg\", \"mr\", \"wa\" ,\"nv\", \"ore\", \"da\", \"pm\", \"am\", \"cx\"])\nprint(STOPWORDS)","3d59ddcb":"def nlp_preprocesser(row):\n    sentence = row.Description\n    #convert all characters to lowercase\n    lowered = sentence.lower()\n    tok = tokenize.word_tokenize(lowered)\n\n    #lemmatizing & stemming\n    lemmatizer = stem.WordNetLemmatizer()\n    lem = [lemmatizer.lemmatize(i) for i in tok if i not in STOPWORDS]\n    stemmer = stem.PorterStemmer()\n    stems = [stemmer.stem(i) for i in lem if i not in STOPWORDS]\n\n    #remove non-alphabetical characters like '(', '.' or '!'\n    alphas = [i for i in stems if i.isalpha() and (i not in STOPWORDS)]\n    return \" \".join(alphas)","1351b97b":"df['Description_processed'] = df.apply(nlp_preprocesser, axis=1)\ndf.head(3)","92e84b8c":"def sentiment2score(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sent_score = analyzer.polarity_scores(text)[\"compound\"]\n    return float(sent_score)","d7d727d4":"df['Description_sentiment_score'] = df['Description'].apply(lambda x: sentiment2score(x))\ndf.head(3)","5281d572":"country_cnt = np.round(df['Country'].value_counts(normalize=True) * 100)\nhv.Bars(country_cnt).opts(title=\"Country Count\", color=\"green\", xlabel=\"Countries\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n            * hv.Text('Country_01', 15, f\"{int(country_cnt.loc['Country_01'])}%\")\\\n            * hv.Text('Country_02', 15, f\"{int(country_cnt.loc['Country_02'])}%\")\\\n            * hv.Text('Country_03', 15, f\"{int(country_cnt.loc['Country_03'])}%\")","61ed3dd1":"local_cnt = np.round(df['Local'].value_counts(normalize=True) * 100)\nhv.Bars(local_cnt).opts(title=\"Local Count\", color=\"green\", xlabel=\"Locals\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True))","6433ce35":"sector_cnt = np.round(df['Industry Sector'].value_counts(normalize=True) * 100)\nhv.Bars(sector_cnt).opts(title=\"Industry Sector Count\", color=\"green\", xlabel=\"Sectors\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n                * hv.Text('Mining', 15, f\"{int(sector_cnt.loc['Mining'])}%\")\\\n                * hv.Text('Metals', 15, f\"{int(sector_cnt.loc['Metals'])}%\")\\\n                * hv.Text('Others', 15, f\"{int(sector_cnt.loc['Others'])}%\")","2233ec68":"ac_level_cnt = np.round(df['Accident Level'].value_counts(normalize=True) * 100)\npot_ac_level_cnt = np.round(df['Potential Accident Level'].value_counts(normalize=True) * 100, decimals=1)\nac_pot = pd.concat([ac_level_cnt, pot_ac_level_cnt], axis=1,sort=False).fillna(0).rename(columns={'Accident Level':'Accident', 'Potential Accident Level':'Potential'})\nac_pot = pd.melt(ac_pot.reset_index(), ['index']).rename(columns={'index':'Severity', 'variable':'Levels'})\nhv.Bars(ac_pot, ['Severity', 'Levels'], 'value').opts(opts.Bars(title=\"Accident Levels Count\", width=700, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=45, ylabel=\"Percentage\", yformatter='%d%%'))","091e8244":"gender_cnt = np.round(df['Gender'].value_counts(normalize=True) * 100)\nhv.Bars(gender_cnt).opts(title=\"Gender Count\", color=\"green\", xlabel=\"Gender\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","14c2435c":"emp_type_cnt = np.round(df['Employee type'].value_counts(normalize=True) * 100)\nhv.Bars(emp_type_cnt).opts(title=\"Employee type Count\", color=\"green\", xlabel=\"Employee Type\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","c7d1e911":"cr_risk_cnt = np.round(df['Critical Risk'].value_counts(normalize=True) * 100)\nhv.Bars(cr_risk_cnt[::-1]).opts(title=\"Critical Risk Count\", color=\"green\", xlabel=\"Critical Risks\", ylabel=\"Percentage\", xformatter='%d%%')\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","7d4c7540":"year_cnt = np.round(df['Year'].value_counts(normalize=True,sort=False) * 100)\ny = hv.Bars(year_cnt).opts(title=\"Year Count\", color=\"green\", xlabel=\"Years\")\nmonth_cnt = np.round(df['Month'].value_counts(normalize=True,sort=False) * 100)\nm = hv.Bars(month_cnt).opts(title=\"Month Count\", color=\"skyblue\", xlabel=\"Months\") * hv.Curve(month_cnt).opts(color='red', line_width=3)\nday_cnt = np.round(df['Day'].value_counts(normalize=True,sort=False) * 100)\nd = hv.Bars(day_cnt).opts(title=\"Day Count\", color=\"skyblue\", xlabel=\"Days\") * hv.Curve(day_cnt).opts(color='red', line_width=3)\nweekday_cnt = pd.DataFrame(np.round(df['Weekday'].value_counts(normalize=True,sort=False) * 100))\nweekday_cnt['week_num'] = [['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'].index(i) for i in weekday_cnt.index]\nweekday_cnt.sort_values('week_num', inplace=True)\nw = hv.Bars((weekday_cnt.index, weekday_cnt.Weekday)).opts(title=\"Weekday Count\", color=\"green\", xlabel=\"Weekdays\") * hv.Curve(weekday_cnt['Weekday']).opts(color='red', line_width=3)\n(y + m + d + w).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(2)","5c50e329":"season_cnt = pd.DataFrame(np.round(df['Season'].value_counts(normalize=True,sort=False) * 100).reset_index())\nseason_cnt['season_order'] = season_cnt['index'].apply(lambda x: ['Spring','Summer','Autumn','Winter'].index(x))\nseason_cnt.sort_values('season_order', inplace=True)\nseason_cnt.index = season_cnt['index']\nseason_cnt.drop(['index','season_order'], axis=1, inplace=True)\nhv.Bars(season_cnt).opts(title=\"Season Count\", color=\"green\", xlabel=\"Season\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","7502e78c":"f = lambda x : np.round(x\/x.sum() * 100)\ncon_sector = df.groupby(['Country','Industry Sector'])['Industry Sector'].count().unstack().apply(f, axis=1)\nhv.Bars(pd.melt(con_sector.reset_index(), ['Country']), ['Country', 'Industry Sector'], 'value').opts(opts.Bars(title=\"Industry Sector by Countries Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","6588d8d4":"f = lambda x : np.round(x\/x.sum() * 100)\nem_gen = df.groupby(['Gender','Employee type'])['Employee type'].count().unstack().apply(f, axis=1)\nhv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Employee type'], 'value').opts(opts.Bars(title=\"Employee type by Gender Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","b3c47095":"f = lambda x : np.round(x\/x.sum() * 100)\nem_gen = df.groupby(['Gender','Industry Sector'])['Industry Sector'].count().unstack().apply(f, axis=1)\nhv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Industry Sector'], 'value').opts(opts.Bars(title=\"Industry Sector by Gender Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","17f0121e":"f = lambda x : np.round(x\/x.sum() * 100)\nac_gen = df.groupby(['Gender','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_gen.reset_index(), ['Gender']), ['Gender','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Gender Count\"))\npot_ac_gen = df.groupby(['Gender','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_gen.reset_index(), ['Gender']), ['Gender','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Gender Count\"))\n(ac + pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","f2815c1c":"f = lambda x : np.round(x\/x.sum() * 100)\nac_em = df.groupby(['Employee type','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_em.reset_index(), ['Employee type']), ['Employee type','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Employee type Count\"))\npot_ac_em = df.groupby(['Employee type','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_em.reset_index(), ['Employee type']), ['Employee type','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Employee type Count\"))\n(ac + pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%',fontsize={'title':9}))","f22c8fb2":"f = lambda x : np.round(x\/x.sum() * 100)\nac_mo = df.groupby(['Month','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac = hv.Curve(ac_mo['I'], label='I') * hv.Curve(ac_mo['II'], label='II') * hv.Curve(ac_mo['III'], label='III') * hv.Curve(ac_mo['IV'], label='IV') * hv.Curve(ac_mo['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Month Count\"))\npot_ac_mo = df.groupby(['Month','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\npot_ac = hv.Curve(pot_ac_mo['I'], label='I') * hv.Curve(pot_ac_mo['II'], label='II') * hv.Curve(pot_ac_mo['III'], label='III') * hv.Curve(pot_ac_mo['IV'], label='IV')\\\n        * hv.Curve(pot_ac_mo['V'], label='V') * hv.Curve(pot_ac_mo['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Month Count\"))\n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","9e08363c":"f = lambda x : np.round(x\/x.sum() * 100)\nac_weekday = df.groupby(['Weekday','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac_weekday['week_num'] = [['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'].index(i) for i in ac_weekday.index]\nac_weekday.sort_values('week_num', inplace=True)\nac_weekday.drop('week_num', axis=1, inplace=True)\nac = hv.Curve(ac_weekday['I'], label='I') * hv.Curve(ac_weekday['II'], label='II') * hv.Curve(ac_weekday['III'], label='III') * hv.Curve(ac_weekday['IV'], label='IV') * hv.Curve(ac_weekday['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Weekday Count\"))\npot_ac_weekday = df.groupby(['Weekday','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=0).fillna(0)\npot_ac_weekday['week_num'] = [['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'].index(i) for i in pot_ac_weekday.index]\npot_ac_weekday.sort_values('week_num', inplace=True)\npot_ac_weekday.drop('week_num', axis=1, inplace=True)\npot_ac = hv.Curve(pot_ac_weekday['I'], label='I') * hv.Curve(pot_ac_weekday['II'], label='II') * hv.Curve(pot_ac_weekday['III'], label='III') * hv.Curve(pot_ac_weekday['IV'], label='IV')\\\n        * hv.Curve(pot_ac_weekday['V'], label='V') * hv.Curve(pot_ac_weekday['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Weekday Count\"))\n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","dadaee8c":"f = lambda x : np.round(x\/x.sum() * 100)\nac_season = df.groupby(['Season','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac_season['season_num'] = [['Spring', 'Summer', 'Autumn', 'Winter'].index(i) for i in ac_season.index]\nac_season.sort_values('season_num', inplace=True)\nac_season.drop('season_num', axis=1, inplace=True)\nac = hv.Curve(ac_season['I'], label='I') * hv.Curve(ac_season['II'], label='II') * hv.Curve(ac_season['III'], label='III') * hv.Curve(ac_season['IV'], label='IV') * hv.Curve(ac_season['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Season Count\"))\npot_ac_season = df.groupby(['Season','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=0).fillna(0)\npot_ac_season['season_num'] = [['Spring', 'Summer', 'Autumn', 'Winter'].index(i) for i in pot_ac_season.index]\npot_ac_season.sort_values('season_num', inplace=True)\npot_ac_season.drop('season_num', axis=1, inplace=True)\npot_ac = hv.Curve(pot_ac_season['I'], label='I') * hv.Curve(pot_ac_season['II'], label='II') * hv.Curve(pot_ac_season['III'], label='III') * hv.Curve(pot_ac_season['IV'], label='IV')\\\n        * hv.Curve(pot_ac_season['V'], label='V') * hv.Curve(pot_ac_season['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Season Count\"))\n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","da995ec4":"def ngram_func(ngram, trg='', trg_value=''):\n    #trg_value is list-object\n    if (trg == '') or (trg_value == ''):\n        string_filterd =  df['Description_processed'].sum().split()\n    else:\n        string_filterd =  df[df[trg].isin(trg_value)]['Description_processed'].sum().split()\n    dic = nltk.FreqDist(nltk.ngrams(string_filterd, ngram)).most_common(30)\n    ngram_df = pd.DataFrame(dic, columns=['ngram','count'])\n    ngram_df.index = [' '.join(i) for i in ngram_df.ngram]\n    ngram_df.drop('ngram',axis=1, inplace=True)\n    return ngram_df","457329b5":"hv.Bars(ngram_func(1)[::-1]).opts(title=\"Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","af1dd1d5":"hv.Bars(ngram_func(2)[::-1]).opts(title=\"Bigram Count top-30\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","ccae39ad":"hv.Bars(ngram_func(3)[::-1]).opts(title=\"Trigram Count top-30\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","6b1e9fff":"uni_ma=hv.Bars(ngram_func(1, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Unigram with Male\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_fe=hv.Bars(ngram_func(1, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Unigram with Female\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\n\nbi_ma=hv.Bars(ngram_func(2, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Bigram with Male\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_fe=hv.Bars(ngram_func(2, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Bigram with Female\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\n\ntri_ma=hv.Bars(ngram_func(3, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Trigram with Male\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_fe=hv.Bars(ngram_func(3, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Trigram with Female\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n                \n\n(uni_ma + uni_fe + bi_ma + bi_fe + tri_ma + tri_fe).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False).cols(2)","f4e6ac04":"uni_ac_lo=hv.Bars(ngram_func(1, 'Accident Level', ['I','II'])[0:15][::-1]).opts(title=\"Unigram with High Accident Level\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_ac_hi=hv.Bars(ngram_func(1, 'Accident Level', ['III','IV','V'])[0:15][::-1]).opts(title=\"Unigram with Low Accident Level\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\n\nbi_ac_lo=hv.Bars(ngram_func(2, 'Accident Level', ['I','II'])[0:15][::-1]).opts(title=\"Bigram with High Accident Level\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_ac_hi=hv.Bars(ngram_func(2, 'Accident Level', ['III','IV','V'])[0:15][::-1]).opts(title=\"Bigram with Low Accident Level\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\n\ntri_ac_lo=hv.Bars(ngram_func(3, 'Accident Level', ['I','II'])[0:15][::-1]).opts(title=\"Trigram with High Accident Level\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_ac_hi=hv.Bars(ngram_func(3, 'Accident Level', ['III','IV','V'])[0:15][::-1]).opts(title=\"Trigram with Low Accident Level\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n                \n(uni_ac_lo + uni_ac_hi + bi_ac_lo + bi_ac_hi + tri_ac_lo + tri_ac_hi).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False).cols(2)","32a97187":"uni_mine=hv.Bars(ngram_func(1, 'Industry Sector', ['Mining'])[0:15][::-1]).opts(title=\"Unigram with Mining Sector\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_metal=hv.Bars(ngram_func(1, 'Industry Sector', ['Metals'])[0:15][::-1]).opts(title=\"Unigram with Metal Sector\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_others=hv.Bars(ngram_func(1, 'Industry Sector', ['Others'])[0:15][::-1]).opts(title=\"Unigram with Other Sector\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\n\nbi_mine=hv.Bars(ngram_func(2, 'Industry Sector', ['Mining'])[0:15][::-1]).opts(title=\"Bigram with Mining Sector\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_metal=hv.Bars(ngram_func(2, 'Industry Sector', ['Metals'])[0:15][::-1]).opts(title=\"Bigram with Metal Sector\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_others=hv.Bars(ngram_func(2, 'Industry Sector', ['Others'])[0:15][::-1]).opts(title=\"Bigram with Other Sector\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\n\ntri_mine=hv.Bars(ngram_func(3, 'Industry Sector', ['Mining'])[0:15][::-1]).opts(title=\"Trigram with Mining Sector\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_metal=hv.Bars(ngram_func(3, 'Industry Sector', ['Metals'])[0:15][::-1]).opts(title=\"Trigram with Metal Sector\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_others=hv.Bars(ngram_func(3, 'Industry Sector', ['Others'])[0:15][::-1]).opts(title=\"Trigram with Other Sector\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n\n(uni_mine + uni_metal + uni_others + bi_mine + bi_metal + bi_others + tri_mine + tri_metal + tri_others)\\\n            .opts(opts.Bars(width=265, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False,fontsize={'title':6.5,'labels':7,'yticks':8.5})).opts(shared_axes=False).cols(3)","26135c29":"uni_emp=hv.Bars(ngram_func(1, 'Employee type', ['Employee'])[0:15][::-1]).opts(title=\"Unigram with Employee\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_third=hv.Bars(ngram_func(1, 'Employee type', ['Third Party','Third Party (Remote)'])[0:15][::-1]).opts(title=\"Unigram with Third Party\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\n\nbi_emp=hv.Bars(ngram_func(2, 'Employee type', ['Employee'])[0:15][::-1]).opts(title=\"Bigram with Employee\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_third=hv.Bars(ngram_func(2, 'Employee type', ['Third Party','Third Party (Remote)'])[0:15][::-1]).opts(title=\"Bigram with Third Party\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\n\ntri_emp=hv.Bars(ngram_func(3, 'Employee type', ['Employee'])[0:15][::-1]).opts(title=\"Trigram with Employee\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_third=hv.Bars(ngram_func(3, 'Employee type', ['Third Party','Third Party (Remote)'])[0:15][::-1]).opts(title=\"Trigram with Third Party\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n\n(uni_emp + uni_third+ bi_emp + bi_third + tri_emp + tri_third).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False).cols(2)","b3247a4a":"wordcloud = WordCloud(width = 1500, height = 800, random_state=0, background_color='black', colormap='rainbow', \\\n                      min_font_size=5, max_words=300, collocations=False, min_word_length=3, stopwords = STOPWORDS).generate(\" \".join(df['Description_processed'].values))\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","75450e33":"v1 = hv.Curve(df.groupby('Month')[\"Description_sentiment_score\"].mean())\\\n    .opts(opts.Curve(xlabel=\"Month\", ylabel=\"Sentiment Score\", width=800, height=300,tools=['hover'],show_grid=True,title='Month Average Sentiment Score'))\nv2 = hv.Curve(df.groupby('Weekday')[\"Description_sentiment_score\"].mean().reindex(index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\\\n    .opts(opts.Curve(xlabel=\"Weekday\", ylabel=\"Sentiment Score\", width=800, height=300,tools=['hover'],show_grid=True,title='Weekday Average Sentiment Score'))\n(v1 + v2).cols(1)","757f4a91":"feature_df = pd.DataFrame()\nfor i in [1,2,3]:\n    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))\n    X = vec_tfidf.fit_transform(df['Description_processed']).toarray()\n    tfs = pd.DataFrame(X, columns=[\"TFIDF_\" + n for n in vec_tfidf.get_feature_names()])\n    feature_df = pd.concat([feature_df, tfs], axis=1)\nfeature_df = pd.concat([df, feature_df], axis=1)\nfeature_df.head(3)","7edb6319":"feature_df['Country'] = LabelEncoder().fit_transform(feature_df['Country']).astype(np.int8)\nfeature_df['Local'] = LabelEncoder().fit_transform(feature_df['Local']).astype(np.int8)\nfeature_df['Industry Sector'] = LabelEncoder().fit_transform(feature_df['Industry Sector']).astype(np.int8)\nfeature_df['Accident Level'] = LabelEncoder().fit_transform(feature_df['Accident Level']).astype(np.int8)\nfeature_df['Potential Accident Level'] = LabelEncoder().fit_transform(feature_df['Potential Accident Level']).astype(np.int8)\nfeature_df['Gender'] = LabelEncoder().fit_transform(feature_df['Gender']).astype(np.int8)\nfeature_df['Employee type'] = LabelEncoder().fit_transform(feature_df['Employee type']).astype(np.int8)\nfeature_df['Critical Risk'] = LabelEncoder().fit_transform(feature_df['Critical Risk']).astype(np.int8)\nfeature_df['Weekday'] = LabelEncoder().fit_transform(feature_df['Weekday']).astype(np.int8)\nfeature_df['Season'] = LabelEncoder().fit_transform(feature_df['Season']).astype(np.int8)\nfeature_df.drop(['Date','Description', 'Description_processed'],axis=1,inplace=True)\nfeature_df.head(3)","e01f5f54":"y_series = feature_df['Accident Level']\nx_df = feature_df.drop(['Accident Level','Potential Accident Level'], axis=1) \nX_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y_series, test_size=0.2, random_state=0, stratify=y_series)\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_valid = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)","5895ecaa":"params = {\n    'task' : 'train',\n    'boosting' : 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 5,\n    'metric': 'multi_logloss',\n    'num_leaves': 200,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 5\n}\ngbm_ac = lgb.train(params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=lgb_valid,\n            early_stopping_rounds=100)","7866a7bd":"ac_label = ['Accident Level : I','Accident Level : II','Accident Level : III','Accident Level : IV','Accident Level : V']\nexplainer = shap.TreeExplainer(model=gbm_ac)\nshap_values_ac = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_ac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=30, class_names=ac_label)","3086996b":"t = lgb.plot_tree(gbm_ac, figsize=(20, 20), precision=1, tree_index=0, show_info=['split_gain'])\nplt.title('Visulalization of Tree in Accident Level')\nplt.show()","49dbd8a7":"_feature_df = feature_df[~feature_df['Potential Accident Level'].isin([5])]\ny_series = _feature_df['Potential Accident Level']\nx_df = _feature_df.drop(['Accident Level','Potential Accident Level'], axis=1) \nX_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y_series, test_size=0.2, random_state=0, stratify=y_series)\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_valid = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)","1a36fa2b":"params = {\n    'task' : 'train',\n    'boosting' : 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 5,\n    'metric': 'multi_logloss',\n    'num_leaves': 200,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 5\n}\ngbm_pac = lgb.train(params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=lgb_valid,\n            early_stopping_rounds=100)","da1defa9":"pac_label = ['Potential Accident Level : I','Potential Accident Level : II','Potential Accident Level : III','Potential Accident Level : IV','Potential Accident Level : V']\nexplainer = shap.TreeExplainer(model=gbm_pac)\nshap_values_pac = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_pac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=30, class_names=pac_label)","3fcd04b9":"t = lgb.plot_tree(gbm_pac, figsize=(20, 20), precision=1, tree_index=0, show_info=['split_gain'])\nplt.title('Visulalization of Tree in Potential Accident Level')\nplt.show()","b4539143":"f1 = lambda x : np.round(x\/len(df) * 100)\ngender_cnt = df.groupby(['Gender'])['Accident Level'].count().apply(f1)\ng = hv.Bars(pd.melt(gender_cnt.reset_index(), ['Gender']), ['Gender'], 'value').opts(opts.Bars(title=\"Gender Count\", color='green'))\n\nf2 = lambda x : np.round(x\/x.sum() * 100)\nac_gen = df.groupby(['Gender','Accident Level'])['Accident Level'].count().unstack().apply(f2, axis=1)\nac = hv.Bars(pd.melt(ac_gen.reset_index(), ['Gender']), ['Gender','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Gender Count\"))\n\n(g + ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%', shared_axes=True))","6fd8497f":"df_em_tmp = df.copy()\ndf_em_tmp.loc[df_em_tmp['Employee type'].isin(['Third Party','Third Party (Remote)']), 'Employee type'] = 'Third Party(+Remote)'\nf1 = lambda x : np.round(x\/len(df_em_tmp) * 100)\nemp_type_cnt = df_em_tmp.groupby(['Employee type'])['Accident Level'].count().apply(f1)\ng = hv.Bars(pd.melt(emp_type_cnt.reset_index(), ['Employee type']), ['Employee type'], 'value').opts(opts.Bars(title=\"Employee type Count\", color='green'))\n\n\nf2 = lambda x : np.round(x\/x.sum() * 100)\nac_em = df_em_tmp.groupby(['Employee type','Accident Level'])['Accident Level'].count().unstack().apply(f2, axis=1)\nac = hv.Bars(pd.melt(ac_em.reset_index(), ['Employee type']), ['Employee type','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Employee type Count\"))\n(g + ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%',fontsize={'title':9}))","d06e7796":"uni=hv.Bars(ngram_func(1)[0:15][::-1]).opts(title=\"Unigram Count\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi=hv.Bars(ngram_func(2)[0:15][::-1]).opts(title=\"Bigram Count\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\ntri=hv.Bars(ngram_func(3)[0:15][::-1]).opts(title=\"Trigram Count\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n(uni + bi + tri).opts(opts.Bars(width=265, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False)","7746be98":"shap.summary_plot(shap_values=shap_values_ac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=15, class_names=ac_label)\nshap.summary_plot(shap_values=shap_values_pac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=15, class_names=pac_label)","ce9290ae":"### Gender mostly involved in accidents\n><div class=\"alert alert-info\" role=\"alert\">\n>In <a href='https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\/tasks?taskId=240'>this task<\/a>, the question is : <b>Which gender is mostly involved in accidents at these plants?<\/b><br\/>\n>Answer : <b>Male<\/b><br\/><br\/>\n><ul>\n><li>Though the staffs of the manufacturing plants are mostly males, <u>EDA shows that <b>males<\/b> are likely involved in accidents(95%)<\/u>.<\/li>\n><li>And males are tend to get involved in accidents with higher risk levels than females.<\/li>\n><\/ul>\n><\/div>","e99aaaeb":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","822648f9":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","7a6dd973":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","e32558ac":"### Unigram\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>There are several words which is related to <b>hands<\/b>. For example <u>left, hand, right and finger<\/u>.<\/li>\n><li>Moreover there are several words which is related to <b>movement of something<\/b>. For example <u>hit, remov, fall and move<\/u>.<\/li>\n><\/ul>\n><\/div>","57c250f1":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","fb911906":"### Calendar\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>It seems that the number of accidents decreased in latter of the year \/ month.<\/li>\n><li>The number of accidents increased during the middle of the week and declined since the middle of the week.<\/li>\n><\/ul>\n><\/div>","aedf5fc6":"### Industry Sector by Countries\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>We can see that there are major industries by countries.<\/li><br\/>\n><li><b>Country_01<\/b> : Mining<\/li>\n><li><b>Country_02<\/b> : Metals<\/li>\n><li><b>Country_03<\/b> : Others<\/li>\n><\/ul>\n><\/div>","6babf20d":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","d9d6cd7f":"### Label Encoding","c080c259":"### Ngram with Gender","649876af":"## Feature Engineering","63a3bd58":"### Trigram\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Like Unigram and Bigram, there are also many phrases which is related to <b>hands or other body parts<\/b>, but concreteness seems to increase.<\/li>\n><li>For example <u>one hand glove, left arm uniform and wear safeti uniform<\/u>.<\/li>\n><\/ul>\n><\/div>","1c30a7c0":"### Industry Sector by Gender\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>There are differences mainly in metals and mining between males and females.<\/li>\n><li>Same as employee type above, it is thought that <u>this is due to different safety level by industry sector<\/u>.<\/li>\n><\/ul>\n><\/div>","c2b6ca6e":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","c82229a9":"### Seasonal variable\n><div class=\"alert alert-success\" role=\"alert\">\n>Accordin to <a href='https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\/discussion\/54113'>this discussion<\/a>, countries where the dataset was collected is anonymized but they are all located in South America. So in this analysis, let's assume the dataset was collected in Brazil.<br\/>\n>It is said in <a href='https:\/\/seasonsyear.com\/Brazil'>this web page<\/a> that Brazil has four climatological seasons as below.\n><ul>\n>    <li><b>Spring<\/b> : September to November<\/li>\n>    <li><b>Summer<\/b> : December to February<\/li>\n>    <li><b>Autumn<\/b> : March to May<\/li>\n>    <li><b>Winter<\/b> : June to August<\/li>\n><\/ul>\n>We can create seasonal variable based on month variable.\n><\/div>","e3cf3a01":">convert text into applicable format by lower-casing, tokenizing, lemmatizing and stemming.","0779393e":"# 5. EDA","9b9fe4aa":"### Accident Levels by Employee type\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>For both accident levels, the incidence of Employee is higher at low accident levels, but <u>the incidence of Third parties seems to be slightly higher at <b>high accident levels<\/b><\/u>.<\/li>\n><\/ul>\n><\/div>","467bcc63":"### WordCloud\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>As same as Ngram analysis above, there are many hand-related and movement-related words.<\/li>\n><li><b>Hand-related<\/b> : left, right, hand, finger and glove<\/li>\n><li><b>Movement-related<\/b> : fall, hit, carri, lift and slip<\/li>\n><\/ul>\n><\/div>","8120611a":"### Accident Levels\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n>    <li>The number of accidents decreases as the Accident Level increases.<\/li>\n>    <li>The number of accidents increases as the Potential Accident Level decreases.<\/li>\n><\/ul>\n><\/div>","6d1925c1":"### Ngram with Employee type","5ead2b9a":"### Season\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>The number of accidents increased in Summer and Autumn.<\/li>\n><li>It is thought that the occurrence of accidents is related to the climate(especially tempeature).<\/li>\n><\/ul>\n><\/div>","f7c64e8c":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","13690da3":"### Ngram with Industry Sector","fb209163":">Accidents may increase or decrease throughout the year or month, so I added datetime features such as year,month and day.","3bb4fa39":"# 2. Import libraries","52f17f34":"### Local","1fc8dc90":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","ab7cb789":"### Main cause of accidents\n><div class=\"alert alert-info\" role=\"alert\">\n>In <a href='https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\/tasks?taskId=242'>this task<\/a>, the question is : <b>What usually causes these accidents?<\/b><br\/>\n>Answer : <b>Mistakes in hands operations.<\/b><br\/><br\/>\n><ul>\n><li>According to Ngram analysis, we can say that <u>operations related to hands are mainly the causes of accidents<\/u>.<\/li>\n><li>According to the modeling to classify accident levels shows that in addition to hands-operation, time-related features also affect to the occurrence of accidents.<\/li>\n><\/ul>\n><\/div>","8333953b":"### Ngram with Accident Level\n>Classifing accident levels into two part, Low Accident Level(I,II) and High Accident Level(III,IV,V).","db5c8d56":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","9142d62b":"# 6. Modeling\n><div class=\"alert alert-success\" role=\"alert\">\n>Objectives:\n><ul>\n><li>Presumption of cause of accidents<\/li>\n><li>Surveying a factor that increases severity of accidents<\/li>\n><\/ul>\n>Building the model which classify the severity of accidents, we can understand the factor related to the causality of accidents.<br\/>\n>So, two models were built based on those cases below.\n><ul>\n><li><a href='#ac'>Case1 : Accident Level<\/a><\/li>\n><li><a href='#pac'>Case2 : Potential Accident Level<\/a><\/li>\n><\/ul>\n><\/div>","7bf0ba9e":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","64ddf0dd":"## Sentiment Analysis\n>Positiveness or negativeness of the description of accidents may be related with their severeness.","4cc54d50":"# 3. Load the dataset","225562a3":"###  Employee type by Gender\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Ratio of employee types by gender is not different in each gender.<\/li>\n><li>The proportion of female with Third Party(Remote) is slightly higher than that of males.<\/li>\n><li>It is thought that this is because <u>males have more on-site work and females often do work far away from relatively safe sites<\/u>.<\/li>\n><\/ul>\n><\/div>","c408828b":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","73456fb2":">NLP preprocessing pipeline is a little complicated, so I made preprocessing function.","1f3a8596":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","cfcabff2":"### Critical Risks\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Because most part of the Critical Risks are classified as 'Others', it is thought that there are too many risks to classify precisely<\/li>\n><li>And it is also thought that it takes so many time to analyze risks and reasons why the accidents occur.<\/li>\n><\/ul>\n><\/div>","f82c9fd8":"# 8. References\n>* **Good EDA Notebook**  \n>https:\/\/www.kaggle.com\/schorsi\/industrial-safety-totw  \n>https:\/\/www.kaggle.com\/schorsi\/industrial-safety-totw-part-2\n>* **Pandas value_counts() tips**  \n>https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.value_counts.html\n>* **Holoviews plot tips**  \n>http:\/\/holoviews.org\/user_guide\/Customizing_Plots.html\n>* **NLP Pre-processing tutorial**  \n>http:\/\/haya14busa.com\/python-nltk-natural-language-processing\/\n>* **WORDCLOUD example**  \n>https:\/\/towardsdatascience.com\/simple-wordcloud-in-python-2ae54a9f58e5  \n>* **LightGBM Parameters**  \n>https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n>* **LightGBM Tree Visulizing**  \n>https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.plot_tree.html","df1ecfdd":"# 4. Pre-processing","81c6b465":"## Table of Contents<a id='top'><\/a>\n>1. [Overview](#1.-Overview)  \n>    * [Project Detail](#Project-Detail)\n>    * [Goal of this notebook](#Goal-of-this-notebook)\n>1. [Import libraries](#2.-Import-libraries)\n>1. [Load the dataset](#3.-Load-the-dataset)\n>1. [Pre-processing](#4.-Pre-processing)\n>    * [NLP Pre-processing](#NLP-Pre-processing)\n>    * [Sentiment Analysis](#Sentiment-Analysis)\n>1. [EDA](#5.-EDA)  \n>    * [Univariate Analysis](#Univariate-Analysis)\n>    * [Multivariate Analysis](#Multivariate-Analysis)\n>    * [NLP Analysis](#NLP-Analysis)\n>1. [Modeling](#6.-Modeling)\n>    * [Feature Engineering](#Feature-Engineering)\n>    * [Case1 : Accident Level](#Case1-:-Accident-Level)\n>    * [Case2 : Potential Accident Level](#Case2-:-Potential-Accident-Level)\n>1. [Conclusion](#7.-Conclusion)\n>    * [Task Submission](#Task-Submission)\n>1. [References](#8.-References)","9d7a6d80":"### TFIDF Feature","becd0bc4":"### Accident Levels by Weekday\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Both of the two accident level is thought that non-severe levels decreased in the first and the last of the week, but severe levels did not changed much.<\/li>\n><li>It can be said that <u>employees' experiences against work can reduce minor mistakes<\/u>.<\/li>\n><\/ul>\n><\/div>","1326bebd":"### Sentiment Trend","be86c611":">function to calculate ngram under several conditions","43cd4045":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","cee8db06":"### Country","66bca784":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","5b257df3":"## Univariate Analysis","88f4f0dc":"### Industry Sector","3768beaf":"### Gender\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>As a characteristic of the industry, the proportion of men is overwhelmingly.<\/li>\n><\/ul>\n><\/div>","ef6f1c77":"### Bigram\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>There are so many phrases which is related to <b>hands<\/b>. For example <u>left hand, right hand, finger left, finger right, middl finger and ring finger<\/u>.<\/li>\n><li>There are also some phrases which is related to other body parts. For example <u>left foot and right reg<\/u>.<\/li>\n><\/ul>\n><\/div>","6464b8c1":"## Task Submission\n>Through the EDA & Modeling above, we can answer [several tasks](https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\/tasks).","ae5bd1e7":">In addition to the predifined stopwords in WORDCLOUD, I defined handmade-stopwords list by inspecting the documents in 'Description' column.","4e7ef2a1":"## Case1 : Accident Level<a id='ac'><\/a>\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>There are many time-series features with high importance such as <b>Day, Weekday and Month<\/b>, and it is thought that <u>the occurrence of accidents and the accident level will change  easily depending on the time<\/u>.<\/li>\n><li><u>Sentiment score is much related with accident level.<\/u> Sentiment score is calculated based on specific words in text, so it is thought that specific words may increase severness of accident level.<\/li>\n><li>Since there are many TFIDF features with high importance related to a part of the body, and in particular many features are related to the hands such as <b>hand, left and right<\/b>, so it is considered that <u>mistakes in manual work are related to the occurrence and severity of accidents<\/u>.<\/li>\n><\/ul>\n><\/div>","ba3f32aa":"# 1. Overview\n## Project Detail\n>In [this dataset](https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database), the information about accidents in 12 manufacturing plants in 3 countries are given by a brazilian company, [IHM Stefanini](https:\/\/en.ihm.com.br\/). We need to use this dataset to understand why accidents occur, and discover clues to reduce tragedic accidents.<br\/>\n><p>Dataset columns are below:<\/p>\n><ul>\n><li><b>Date<\/b> : timestamp or time\/date information<\/li>\n><li><b>Countries<\/b> : which country the accident occurred (<b>anonymized<\/b>)<\/li>\n><li><b>Local<\/b> : the city where the manufacturing plant is located (<b>anonymized<\/b>)<\/li>\n><li><b>Industry sector<\/b> : which sector the plant belongs to<\/li>\n><li><b>Accident level<\/b> : from I to VI, it registers how severe was the accident (I means not severe but VI means very severe)<\/li>\n><li><b>Potential Accident Level<\/b> : Depending on the Accident Level, the database also registers how severe the accident could have been (due to other factors involved in the accident)<\/li>\n><li><b>Genre<\/b> : if the person is male of female<\/li>\n><li><b>Employee or Third Party<\/b> : if the injured person is an employee or a third party<\/li>\n><li><b>Critical Risk<\/b> : some description of the risk involved in the accident<\/li>\n><li><b>Description<\/b> : Detailed description of how the accident happened<\/li>\n><\/ul>\n\n## Goal of this notebook\n>* Practice Pre-processing technique\n    * Time-related feature extraction\n    * NLP pre-precessing(lower-casing, lemmatizing, stemming and removing stopwords)\n>* Practice EDA technique\n>* Practice visualising technique(especially using bokeh via holoviews)\n>* Practice feature enginieering technique\n>    * Time-related features\n>    * NLP features(TF-IDF)\n>* Practice modeling technique\n>    * LightGBM(+ plotting the tree)\n>* Causal analysis skill","010dbf4a":"### Accident Levels by Month\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Both of the two accident level have the tendency that non-severe levels decreased throughout the year, <u>but severe levels did not changed much, and some of these levels increased slightly in the second half of the year<\/u>.<\/li>\n><li>The fact above seems to be related to <b>the skill level of the employees<\/b>, and <u>while their experiences can reduce minor mistakes, sometimes they can make serious mistakes accidentally<\/u>.<\/li>\n><\/ul>\n><\/div>","c30a50d4":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","9c28c5ea":">function to convert month variable into seasons","9b791ad9":"## NLP Pre-processing\n>Description column contains the details of why accidents happend. So I tried to add new features by using this important information with NLP technique.","27cf77d9":"### Employee type\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>The large number of Third Party employee type indicates the difference of employement system in gender or industry sector.<\/li>\n><\/ul>\n><\/div>","ed443bf3":"### Accident Levels by Season\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>As same as accident levels by month, both of the two accident level have the tendency that non-severe levels decreased throughout the year, <u>but severe levels did not changed much, and some of these levels increased slightly in the second half of the year<\/u>.<\/li>\n><\/ul>\n><\/div>","56a1ba92":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","ca7a2d2d":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","34e44f92":"## Multivariate Analysis","9ed6beb3":"## Case2 : Potential Accident Level<a id='pac'><\/a>\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Similar to the model of Accident Level above, the features of time series and TFIDF features of hands are highly important, but the TFIDF features seem to be slightly more important.<\/li>\n><\/ul>\n><\/div>","78a5fa77":"# 7. Conclusion\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>In this project, we discovered that the main causes of accidents are <b>mistakes in hand-operation and time-related factor<\/b>.<\/li>\n><li>To reduce the occurrences of accidents, <u>more stringent safety standards in hand-operation will be needed in period when many accidents occur<\/u>.<\/li>\n><\/ul>\n><ul>\n><li>I realized that the detail information of accidents like 'Description' are so useful to analyze the cause.<\/li>\n><li>With more detailed information such as <b>machining data(ex. CNC, Current, Voltage) in plants, weather information, employee's personal data(ex. age, experience in the industry sector, work performance\n)<\/b>, we can clarify the cause of accidents more correctly.<\/li>\n><\/ul>\n><\/div>","2ab0afa4":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","21c9cea5":"### Third Parties Or Employees?\n><div class=\"alert alert-info\" role=\"alert\">\n>In <a href='https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\/tasks?taskId=241'>this task<\/a>, the question is : <b>Are third parties usually involved in these accidents or it is mainly the employees?<\/b><br\/>\n>Answer : <b>Yes, they are. Third parties more likely get involved in accidents.<\/b><br\/><br\/>\n><ul>\n><li>Comparing employee's accidents count with third parties' accidents count, <u>EDA shows that <b>third parties<\/b> are likely involved in accidents(58%)<\/u>.<\/li>\n><li>And third parties are slightly tend to get involved in accidents with higer risk levels than employee.<\/li>\n><\/ul>\n><\/div>","c19eba55":"<h2 style=\"text-align:center;font-size:200%;;\">Industrial Accident Causal Analysis <\/h2>\n<h3  style=\"text-align:center;\">Keywords : <span class=\"label label-success\">Manufacturing<\/span> <span class=\"label label-success\">EDA<\/span> <span class=\"label label-success\">Visualization<\/span> <span class=\"label label-success\">Feature Engineering<\/span> <span class=\"label label-success\">NLP<\/span> <span class=\"label label-success\">Causal Analysis<\/span> <span class=\"label label-success\">Sentiment Analysis<\/span><\/h3>","83cd147c":"## NLP Analysis\n>Description about accidents is important to understand the cause of accidents, so we need to discover characteristical words or phrases indicating situation when accidents occured.","f0aa6613":"### Accident Levels by Gender\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>In terms of <b>accident levels<\/b>, there are many mild risks at general accident level, but many serious risks at potential accident level.\n    <p>It can be said that <u>many potential accidents are overlooked and potentially high-risk accidents are possible<\/u>.<\/p><\/li>\n><li>In terms of <b>gender<\/b>, the general trend is the same, but males have a higher accident levels than females.\n    <p>Same as discussion above, it is thought that <u>this is due to different safety level by industry sector<\/u>.<\/p><\/li>\n><\/ul>\n><\/div>"}}