{"cell_type":{"c2f9ac3f":"code","73531e5e":"code","32369663":"code","770591b2":"code","482ad70b":"code","d6a3d4b6":"code","ff7e2f6b":"code","f6937950":"code","76baf0b2":"code","9f4b1232":"code","27dba05c":"code","12b324db":"code","b2c5dd53":"code","4a4e612b":"code","a4b9e0a2":"code","c2c4e579":"code","bc680240":"code","1231290e":"code","909ad15d":"code","bddaf8de":"code","134c0541":"code","95dabf6c":"code","f48f5c65":"code","8d3d6381":"code","ada88954":"markdown","af779edb":"markdown","46d70597":"markdown","56185c79":"markdown","75942c69":"markdown","7fdc3dc3":"markdown","61e17525":"markdown","d2b1ad5b":"markdown","71fc4334":"markdown","d10debef":"markdown","dfe480d6":"markdown","e6ce486d":"markdown","3fb1c940":"markdown","30c03093":"markdown","90a2dcea":"markdown","9f51f431":"markdown","706c6152":"markdown","401e1c58":"markdown","eb026e46":"markdown","03aa621f":"markdown","e5454f00":"markdown","e144db56":"markdown","2c020a9d":"markdown","bc22f0f0":"markdown","de4cbfc1":"markdown","b2c9f4bf":"markdown"},"source":{"c2f9ac3f":"from IPython.display import Image\nImage('..\/input\/predicting-wins-in-lol\/1200px-League_of_Legends_2019_vector.svg.png')","73531e5e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix\n\nfrom xgboost import XGBClassifier\n\nfrom scipy.stats import skew\nfrom scipy.stats import randint, truncnorm, uniform\n\nimport warnings\nwarnings.filterwarnings('ignore')","32369663":"data = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndata.head()","770591b2":"data.info()","482ad70b":"data.isna().values.any()","d6a3d4b6":"data.hist(bins = 50, figsize = (20,40))\nplt.show()","ff7e2f6b":"train_set, test_set = train_test_split(data, test_size = 0.2, random_state = 2)\n\n# I will create a small function that separates the target variable\n\ndef get_target(data_set):\n    return data_set.drop('blueWins', axis = 1), data_set['blueWins']\n\nX_train, y_train = get_target(train_set)\nX_test, y_test = get_target(test_set)","f6937950":"L_reg = LogisticRegression()\nX_train_copy = X_train.copy() # I want to avoid changing the original\nscores = cross_val_score(L_reg, X_train_copy, y_train, scoring = 'accuracy', cv = 10)\n\nprint(scores.mean())","76baf0b2":"def add_attributes(X):\n    X['blueVisionScore'] = X['blueWardsPlaced'] + X['redWardsDestroyed']\n    X['redVisionScore'] = X['redWardsPlaced'] + X['blueWardsDestroyed']\n    return X\n\nX_train_copy = X_train.copy()\nX_train_tr = add_attributes(X_train_copy)\nprint(X_train_tr.columns)","9f4b1232":"class attributes_adder(BaseEstimator, TransformerMixin):\n    def fit(self, X):\n        return self\n    def transform(self, X):\n        return add_attributes(X)","27dba05c":"def intersection(x,y):\n    return [val for val in x if val in y]\n\ndef find_high_skew(X):\n    non_norm = ['gameId','blueFirstBlood','blueEliteMonsters','blueHeralds','blueDragons','blueTowersDestroyed',\n              'redFirstBlood','redEliteMonsters','redHeralds','redDragons','redTowersDestroyed']\n    non_norm = intersection(list(X.columns), non_norm)\n    temp = X.drop(non_norm, axis = 1)\n    cols = []\n    skews = []\n    for col in temp.columns:\n        if abs(skew(temp[col])) > 0.5: # we will transform the attributes with high skew\n            cols.append(col)\n            skews.append(skew(temp[col]))\n    return cols, skews","12b324db":"X_train_copy = X_train.copy()\n\ncols, skews = find_high_skew(X_train_copy)\n\nprint(\"Columns with skew > 0.5\")\n    \nX_train_copy[cols].hist(bins = 50, figsize = (20,40))\nplt.show()","b2c5dd53":"class fix_skew(BaseEstimator, TransformerMixin):\n    def __init__(self, try_sqrt = True):\n        self.try_sqrt = try_sqrt\n    def fit(self,X):\n        return self\n    def transform(self,X):\n        skewed_cols, _ = find_high_skew(X)\n        if self.try_sqrt:\n            for col in skewed_cols:\n                X[col] = np.sqrt(X[col]) # try the square root tranformation\n        else:\n            for col in skewed_cols:\n                X[col] = np.log(1 + X[col]) # otherwise, try a log tranformation\n        return X\n\n# now we test\ntestClass = fix_skew() # log transformation\nX_train_copy = X_train.copy()\nX_train_log = testClass.transform(X_train_copy)\n\nprint(\"Histograms after fixing the skew using a Log transformation.\")\nX_train_log[cols].hist(bins = 50, figsize = (20,40))\nplt.show()","4a4e612b":"testClass = fix_skew(try_sqrt = True) #square-root\nX_train_copy = X_train.copy()\nX_train_sqrt = testClass.transform(X_train_copy)\n\nprint(\"Histograms after fixing the skew using a Square Root transformation.\")\nX_train_sqrt[cols].hist(bins = 50, figsize = (20,40))\nplt.show()","a4b9e0a2":"cols1, skews1 = find_high_skew(X_train_log)\ncols2, skews2 = find_high_skew(X_train_sqrt)\n\nprint(\"Leftover high skew after log transformation\")\nprint(cols1, skews1)\n\nprint(\"==========================\\n\")\nprint(\"Leftover high skew after square root transformation\")\nprint(cols2, skews2)","c2c4e579":"def remove_corr(X, corrTol = 0.75):\n    colToDrop = set()\n    for col in X.columns:\n        corr = X.corr()\n        corrDict = corr[col].to_dict()\n        del(corrDict[col])\n    for key in corrDict:\n        if abs(corrDict[key]) > corrTol:\n            colToDrop.add(key)\n    colToDrop = list(colToDrop)\n    X.drop(colToDrop, axis = 1, inplace = True)\n    X.drop('gameId', axis = 1, inplace = True) # gameId does not contribute, so we remove it here.\n    return X\n\nX_train_copy = X_train.copy()\n\nX_train_tr = remove_corr(X_train_copy)\nprint(X_train_tr.columns)","bc680240":"class removeCorr(BaseEstimator, TransformerMixin):\n    def __init__(self,corrTol = 0.75):\n        self.corrTol = corrTol\n    def fit(self, X):\n        return self\n    def transform(self, X):\n        return remove_corr(X, self.corrTol)","1231290e":"class scaleData(BaseEstimator, TransformerMixin):\n    def fit(self, X):\n        return self\n    def transform(self,X):\n        tmp = X.copy()\n        X = StandardScaler().fit_transform(X)\n        return pd.DataFrame(X, index = tmp.index, columns = tmp.columns)\n\nmyPipe = Pipeline([\n    ('attributes_adder', attributes_adder()),\n    ('fix_skew', fix_skew()),\n    ('removeCorr',removeCorr(corrTol = 0.95)),\n    ('scaler', scaleData())\n])\n\nX_train_copy = X_train.copy()\nX_train_tr = myPipe.fit_transform(X_train_copy)\n\nX_train_tr.hist(bins = 50, figsize = (20,40))\nplt.show()","909ad15d":"def trainModel(models, X, y):\n    modelScores = {}\n    for model in models:\n        scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = 10)\n        avgScore = scores.mean()\n        modelScores[model] = avgScore\n    return modelScores\n\nmodels = [LogisticRegression(random_state = 1), \n          DecisionTreeClassifier(random_state = 2),\n          RandomForestClassifier(random_state = 3),\n          GradientBoostingClassifier(random_state = 4)\n         ]\n\nmodelScores = trainModel(models, X_train_tr, y_train)\n\nfor model in modelScores:\n    print(model, modelScores[model])","bddaf8de":"param_distributions = [{\n    'C': uniform(0.01,2),\n    'max_iter': [val*10 for val in range(5,16)],\n    'n_jobs' : [val*10 for val in range(9,20)],\n    'l1_ratio': truncnorm(a = 0, b = 1, loc = 0.5, scale = 0.1),\n    'intercept_scaling': truncnorm(a = 0, b = 2, loc = 1, scale = 0.5)\n}]\n\nLR = LogisticRegression(random_state = 1)\ngrid_LR = RandomizedSearchCV(LR, param_distributions, cv = 10, scoring = 'accuracy', n_iter = 20)\nmodel = grid_LR.fit(X_train_tr, y_train)\nprint(model.best_score_)\nprint(model.best_params_)\npreds = model.predict(X_train_tr)\n","134c0541":"X_test_copy = X_test.copy()\nX = myPipe.fit_transform(X_test_copy)\nplot_confusion_matrix(model, X, y_test)\nplt.show()","95dabf6c":"X_test_copy = X_test.copy()\nX = myPipe.fit_transform(X_test_copy)\npreds = model.predict(X)\ntn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = preds).ravel()\n\nacc = (tn + tp)\/(tn + fp + fn + tp)\n\nprint('Accuracy: {:.2f}%'.format(acc*100))","f48f5c65":"X = X_test.copy()\nindex = X.index\ny = pd.Series(data = preds, index = index, name = 'blueWins')\ndata = pd.concat([X,y], axis = 1)\nblueWins = data[data['blueWins'] == 1]\nblueLoses = data[data['blueWins'] == 0]\n\n# I only want to keep the blue attributes, so I'll drop red columns, gameId, and the 'blueWins' column\ndelCols = []\nfor col in blueWins.columns:\n    if 'red' in col:\n        delCols.append(col)\ndelCols += ['blueWins','gameId']\n\nblueWins.drop(delCols, axis = 1, inplace = True)\nblueLoses.drop(delCols, axis = 1, inplace = True)","8d3d6381":"for col in blueWins.columns:\n    fig, ax = plt.subplots(figsize = (10,10))\n    plt.hist(blueWins[col], bins = 50, label = 'Wins', alpha = 0.5)\n    plt.hist(blueLoses[col], bins = 50, label = 'Loses', alpha = 0.5)\n    plt.title(col)\n    plt.legend(loc = 'upper right')\n    plt.show()","ada88954":"# Data\n\nThis project uses 10-minutes of data collected from over nine-thousand unique games. The games are all ranked games at high elo (Diamond 1 to Masters).\n\nThis dataset contains 40 columns including the \"gameId\" column which is a unique identifier. The target column is called \"blueWins\". The blue team wins the gain if \"blueWins\" is 1, otherwise the red team wins.\n\nThe remaining 38 columns are split into 19 columns describing the characteristics of the game for each team. Among these 38 columns are: \"blueWardsDestroyed\", \"redWardsPlaced\", \"blueAvgLevel\", and \"redTowersDestroyed\".","af779edb":"## Baseline Model\n\nNow I want to create a baseline model and the goal will be to pass the baseline's accuracy. The baseline model will use LogisticRegression.\n\nFirst, we need to split the data into test\/train sets and remove the target columns. We will train 80% of the data.","46d70597":"## Create a Pipeline\n\nNow, we will add all the classes into a pipeline. The final step in the pipeline will be to scale the data using sklearn's StandardScaler. First, we want the StandardScaler to return a dataframe. So I will create a class that will do that.","56185c79":"It appears that both tranformations worked equally well!","75942c69":"Looks like the baseline model has an accuracy of 50%. This is not the best model, but that is okay. Now the goal is to pass 50%.","7fdc3dc3":"# Analyzing Predictions\n\nNext, I would like to see the characteristics of the predicted wins.","61e17525":"Using RandomizedSearchCV, we increased our accuracy by a small amount.\nNext, we predict using our fitted model and assess the accuracy using a confusion matrix.","d2b1ad5b":"# Model Building\n\nNow it is time to train models! I will first use LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, and GradientBoostingClassifier with default parameters. Then, I will chose the most accurate model and use RandomizedSearchCV to try to improve the accuracy further.","71fc4334":"### Fixing High Skew\n\nNow we will create a function that finds the skew. By looking at the histograms above, we can see that some of the data is not normal. So I will exclude that when calculating the skew. We will attempt to fix the skew if the absolute value of the skew is greater than 0.5.","d10debef":"### Optimizing Best Model\n\nLogisticRegression is the best model without any hyperparameter tuning. Now we will use RandomizedGridSearch to find the optimal parameters.","dfe480d6":"Now we create a custom class.","e6ce486d":"From the histograms above, we see a few key characteristics between wins and loses:\n\n* First blood leads to a win half of the time.\n* The team with the most kills is more likely to win.\n* The team with the lease deaths is more likely to win.\n* The team with the most assists is more likely to win. (League of Legends is a team sport!)\n* The team the most elite monster kills is more likely to win.\n* The team with the most dragon kills is more likely to win.\n* The team that gets the rift herald is more likely to win.\n* The team with more gold is most likely to win.\n* The team with a higher average level is more likely to win.\n* The team with the higher experience is more likely to win.\n* The team with the most minions kills is more likely to win.\n\nMost of these characteristics go hand in hand. For example, if your team has more minion kills, your team probably has higher experience level because minion kills give experience. ","3fb1c940":"We can also see a summary of the data as follows.","30c03093":"# Purpose\n\nThe purpose of this project is to build an accurate machine learning model that can determine whether a team will win a game based on the first 10 minutes of the game. From this insight, we can then look at the characteristics of the winning team.\n\nIt is important to note that this model cannot be 100% accurate because there is a lot that happens after the first 10 minutes of the game that can quickly change the tides of the game.","90a2dcea":"# Introduction\n\nLeague of Legends (LoL) is a competitive, online-multiplayer video game created by Riot Games in 2009. Two teams composed of five champions (these are the avatars of the game) each are placed on Summoner's Rift and are tasked with destroying the enemy team's nexus. Along the way, each summoner (player in control of the champion) can collect gold and gain experience by destroying towers or killing enemy champions or minions. The first team to destroy a nexus wins.","9f51f431":"# Feature Engineering\n\nNow I would like to try some feature engineering. This will include adding new features, fixing skew, scaling the data and removing highly-correlated features.\n\n### Creating new Features\n\nWe begin by adding new features. In the dataset, there are four attributes that relate to vision \"(blue\/red)WardsPlaced\" and \"(blue\/red)WardsDestroyed\". I will create a new feature \"(blue\/red)VisionScore\", which demonstrates vision dominance. A team has better vision when they place their own wards and destroy enemy wards.","706c6152":"The goal is to place this function into a pipeline, so I will create a class that has fit and transform methods.","401e1c58":"It seems like there are no missing values, but we run the following line to be sure.","eb026e46":"Below, we see the accuracy of the model.","03aa621f":"Next we import the data and have a quick view at the first 5 elements.","e5454f00":"It appears that most of the data is normally distributed!","e144db56":"# Conclusion\n\nThis data reinforces what most League of Legend players believe: \"Get more kills, try not to die, make sure you farm minions, and help your teammates!\"\n\nIt is worth noting that players \"throwing\" the game is always a possibility. This means that a team has a massive lead (either many more kills or many more objectives) could \"throw\" the game by misplaying and giving the opposite team an advantage. During this advantage, the opposite team could end the game. With this possibility, the losing team could have more kills, more objectives, and more experience but still lose. This could explain why the best performing model only has 73% since it cannot predict a team that \"throws\" the game.\n\nThank you for reading. This was my first Kaggle notebook and any feedback is appreciated!","2c020a9d":"Just as before, I will create a custom class for this skew function that will later be put into a pipeline.\n\nTo fix the skew, we will attempt a square-root or log transformation.","bc22f0f0":"# Analysis\n\nNow we take a look at our dataset, we begin by importing all the neccesary modules.","de4cbfc1":"### Removing Highly Correlated Features\n\nNext, I would like to remove columns that are highly correlated. ","b2c9f4bf":"We have no missing values! Next we get a quick visualization of the data using histograms."}}