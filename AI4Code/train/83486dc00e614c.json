{"cell_type":{"9991e152":"code","3491840a":"code","e83e07b3":"code","7a86e561":"code","0314a721":"code","336dd40e":"code","40fdb807":"code","6fa43951":"code","02d766da":"code","84249d1a":"code","caac0d26":"code","a6c3bffb":"code","e0bf1117":"code","c2d87502":"code","16de2875":"code","184f0dca":"code","46804a0a":"code","6b80ebe5":"code","9a129660":"code","92cfb0a7":"code","8f2fc943":"code","e7ab9445":"code","de8f9b29":"code","17aa1f71":"code","485519c7":"code","ec211fe8":"code","b6354e8c":"code","bfb68be3":"code","b9125d97":"code","a772f483":"code","9a635282":"code","5cda8d93":"code","57a6aa24":"code","cc26614d":"markdown","10d13fb4":"markdown","96c646c8":"markdown","f9a8a3d0":"markdown","05b97aed":"markdown","ab14d4b7":"markdown","9f8f0b76":"markdown","8042ce5e":"markdown","e551696a":"markdown","68480538":"markdown","6ac6adf6":"markdown","8bada532":"markdown","8e8d9021":"markdown","766f0378":"markdown","c64929e5":"markdown","2567755c":"markdown","877af5c0":"markdown","35105d07":"markdown","6f07b9a3":"markdown","b0f7e08a":"markdown","a2978bad":"markdown","872b1cd4":"markdown","22c3d18f":"markdown","bb4db11c":"markdown","f04c5bb0":"markdown","2cfea2f0":"markdown","b7172f58":"markdown","f9088c6e":"markdown"},"source":{"9991e152":"from keras.datasets import fashion_mnist\n(train_X,train_Y), (test_X,test_Y) = fashion_mnist.load_data()","3491840a":"import numpy as np\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint('Training data shape : ', train_X.shape, train_Y.shape)\n\nprint('Testing data shape : ', test_X.shape, test_Y.shape)","e83e07b3":"classes = np.unique(train_Y)\nnClasses = len(classes)\nprint('Total number of outputs : ', nClasses)\nprint('Output classes : ', classes)","7a86e561":"plt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\nplt.imshow(train_X[0,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(train_Y[0]))\n\n# Display the first image in testing data\nplt.subplot(122)\nplt.imshow(test_X[0,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(test_Y[0]))","0314a721":"train_X = train_X.reshape(-1, 28,28, 1)\ntest_X = test_X.reshape(-1, 28,28, 1)\ntrain_X.shape, test_X.shape","336dd40e":"train_X = train_X.astype('float32')\ntest_X = test_X.astype('float32')\ntrain_X = train_X \/ 255.\ntest_X = test_X \/ 255.","40fdb807":"# Change the labels from categorical to one-hot encoding\ntrain_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)\n\n# Display the change for category label using one-hot encoding\nprint('Original label:', train_Y[0])\nprint('After conversion to one-hot:', train_Y_one_hot[0])","6fa43951":"from sklearn.model_selection import train_test_split\ntrain_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)","02d766da":"train_X.shape,valid_X.shape,train_label.shape,valid_label.shape","84249d1a":"import keras\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU","caac0d26":"batch_size = 64\nepochs = 20\nnum_classes = 10","a6c3bffb":"fashion_model = Sequential()\nfashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D((2, 2),padding='same'))\nfashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))                  \nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Flatten())\nfashion_model.add(Dense(128, activation='linear'))\nfashion_model.add(LeakyReLU(alpha=0.1))                  \nfashion_model.add(Dense(num_classes, activation='softmax'))","e0bf1117":"fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","c2d87502":"fashion_model.summary()","16de2875":"fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))","184f0dca":"test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=0)","46804a0a":"print('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1])","6b80ebe5":"accuracy = fashion_train.history['accuracy']\nval_accuracy = fashion_train.history['val_accuracy']\nloss = fashion_train.history['loss']\nval_loss = fashion_train.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","9a129660":"batch_size = 64\nepochs = 20\nnum_classes = 10","92cfb0a7":"fashion_model = Sequential()\nfashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,1)))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D((2, 2),padding='same'))\nfashion_model.add(Dropout(0.25))\nfashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Dropout(0.25))\nfashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))                  \nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Dropout(0.4))\nfashion_model.add(Flatten())\nfashion_model.add(Dense(128, activation='linear'))\nfashion_model.add(LeakyReLU(alpha=0.1))           \nfashion_model.add(Dropout(0.3))\nfashion_model.add(Dense(num_classes, activation='softmax'))","8f2fc943":"fashion_model.summary()","e7ab9445":"fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","de8f9b29":"fashion_train_dropout = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))","17aa1f71":"fashion_model.save(\"fashion_model_dropout.h5py\")","485519c7":"test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=1)","ec211fe8":"print('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1])","b6354e8c":"accuracy = fashion_train_dropout.history['accuracy']\nval_accuracy = fashion_train_dropout.history['val_accuracy']\nloss = fashion_train_dropout.history['loss']\nval_loss = fashion_train_dropout.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","bfb68be3":"predicted_classes = fashion_model.predict(test_X)","b9125d97":"predicted_classes = np.argmax(np.round(predicted_classes),axis=1)","a772f483":"predicted_classes.shape, test_Y.shape","9a635282":"correct = np.where(predicted_classes==test_Y)[0]\nprint(\"Found %d correct labels\" % len(correct))\nfor i, correct in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_X[correct].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_Y[correct]))\n    plt.tight_layout()","5cda8d93":"incorrect = np.where(predicted_classes!=test_Y)[0]\nprint(\"Found %d incorrect labels\" % len(incorrect))\nfor i, incorrect in enumerate(incorrect[:9]):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_X[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_Y[incorrect]))\n    plt.tight_layout()","57a6aa24":"from sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(test_Y, predicted_classes, target_names=target_names))","cc26614d":"It's finally time to train the model with Keras' fit() function! The model trains for 20 epochs. The fit() function will return a history object; By storying the result of this function in fashion_train, you can use it later to plot the accuracy and loss function plots between training and validation which will help you to analyze your model's performance visually.","10d13fb4":"# Predict Labels","96c646c8":"# The Network","f9a8a3d0":"You'll use three convolutional layers:\n\nThe first layer will have 32-3 x 3 filters,\nThe second layer will have 64-3 x 3 filters and\nThe third layer will have 128-3 x 3 filters.\nIn addition, there are three max-pooling layers each of size 2 x 2.\n\n![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1512486717\/fashion-mnist-architecture_htbpsz.png)","05b97aed":"Since the predictions you get are floating point values, it will not be feasible to compare the predicted labels with true test labels. So, you will round off the output which will convert the float values into an integer. Further, you will use np.argmax() to select the index number which has a higher value in a row.\n\nFor example, let's assume a prediction for one test image to be 0 1 0 0 0 0 0 0 0 0, the output for this should be a class label 1.","ab14d4b7":"# Classification Report","9f8f0b76":"# Model Evaluation on the Test Set","8042ce5e":"By looking at a few images, you cannot be sure as to why your model is not able to classify the above images correctly, but it seems like a variety of the similar patterns present on multiple classes affect the performance of the classifier although CNN is a robust architecture. For example, images 5 and 6 both belong to different classes but look kind of similar maybe a jacket or perhaps a long sleeve shirt.","e551696a":"# Neural Network Architecture","68480538":"The output of above two plots looks like an ankle boot, and this class is assigned a class label of 9. Similarly, other fashion products will have different labels, but similar products will have same labels. This means that all the 7,000 ankle boot images will have a class label of 9.","6ac6adf6":"# Adding Dropout into the Network","8bada532":"Classification report will help us in identifying the misclassified classes in more detail. You will be able to observe for which class the model performed bad out of the given ten classes.","8e8d9021":"Let's save the model so that you can directly load it and not have to train it again for 20 epochs. This way, you can load the model later on if you need it and modify the architecture; Alternatively, you can start the training process on this saved model. It is always a good idea to save the model -and even the model's weights!- because it saves you time. Note that you can also save the model after every epoch so that, if some issue occurs that stops the training at an epoch, you will not have to start the training from the beginning.","766f0378":"# Model Evaluation on the Test Set","c64929e5":"Wow! Looks like adding Dropout in our model worked, even though the test accuracy did not improve significantly but the test loss decreased compared to the previous results.\n\nNow, let's plot the accuracy and loss plots between training and validation data for the one last time.","2567755c":"The test accuracy looks impressive. It turns out that your classifier does better than the benchmark that was reported [here](http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/), which is an SVM classifier with mean accuracy of 0.897. Also, the model does well compared to some of the deep learning models mentioned on the [GitHub](https:\/\/github.com\/zalandoresearch\/fashion-mnist) profile of the creators of fashion-MNIST dataset.\n\nHowever, you saw that the model looked like it was overfitting. Are these results really all that good?\n\nLet's put your model evaluation into perspective and plot the accuracy and loss plots between training and validation data:","877af5c0":"# Analyze the Data","35105d07":"Finally! You trained the model on fashion-MNIST for 20 epochs, and by observing the training accuracy and loss, you can say that the model did a good job since after 20 epochs the training accuracy is 99% and the training loss is quite low.\n\nHowever, it looks like the model is overfitting, as the validation loss is 0.4396 and the validation accuracy is 92%. Overfitting gives an intuition that the network has memorized the training data very well but is not guaranteed to work on unseen data, and that is why there is a difference in the training and validation accuracy.","6f07b9a3":"You will use a batch size of 64 using a higher batch size of 128 or 256 is also preferable it all depends on the memory. It contributes massively to determining the learning parameters and affects the prediction accuracy. You will train the network for 20 epochs.","b0f7e08a":"From the above two plots, you can see that the validation accuracy almost became stagnant after 4-5 epochs and rarely increased at certain epochs. In the beginning, the validation accuracy was linearly increasing with loss, but then it did not increase much.\n\nThe validation loss shows that this is the sign of overfitting, similar to validation accuracy it linearly decreased but after 4-5 epochs, it started to increase. This means that the model tried to memorize the data and succeeded.\n\nWith this in mind, it's time to introduce some dropout into our model and see if it helps in reducing overfitting.","a2978bad":"Finally, you can see that the validation loss and validation accuracy both are in sync with the training loss and training accuracy. Even though the validation loss and accuracy line are not linear, but it shows that your model is not overfitting: the validation loss is decreasing and not increasing, and there is not much gap between training and validation accuracy.\n\nTherefore, you can say that your model's generalization capability became much better since the loss on both test set and validation set was only slightly more compared to the training loss.","872b1cd4":"# Compile the Model","22c3d18f":"# Train the Model","bb4db11c":"# Model Building Using Keras\n","f04c5bb0":"# Data Preprocessing","2cfea2f0":"You can add a dropout layer to overcome the problem of overfitting to some extent. Dropout randomly turns off a fraction of neurons during the training process, reducing the dependency on the training set by some amount. How many fractions of neurons you want to turn off is decided by a hyperparameter, which can be tuned accordingly. This way, turning off some neurons will not allow the network to memorize the training data since not all the neurons will be active at the same time and the inactive neurons will not be able to learn anything.\n\nSo let's create, compile and train the network again but this time with dropout. And run it for 20 epochs with a batch size of 64.","b7172f58":"In Keras, you can just stack up layers by adding the desired layer one by one. That's exactly what you'll do here: you'll first add a first convolutional layer with Conv2D(). Note that you use this function because you're working with images! Next, you add the Leaky ReLU activation function which helps the network learn non-linear decision boundaries. Since you have ten different classes, you'll need a non-linear decision boundary that could separate these ten classes which are not linearly separable.\n\nMore specifically, you add Leaky ReLUs because they attempt to fix the problem of dying Rectified Linear Units (ReLUs). The ReLU activation function is used a lot in neural network architectures and more specifically in convolutional networks, where it has proven to be more effective than the widely used logistic sigmoid function. As of 2017, this activation function is the most popular one for deep neural networks. The ReLU function allows the activation to be thresholded at zero. However, during the training, ReLU units can \"die\". This can happen when a large gradient flows through a ReLU neuron: it can cause the weights to update in such a way that the neuron will never activate on any data point again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. Leaky ReLUs attempt to solve this: the function will not be zero but will instead have a small negative slope.\n\nNext, you'll add the max-pooling layer with MaxPooling2D() and so on. The last layer is a Dense layer that has a softmax activation function with 10 units, which is needed for this multi-class classification problem.","f9088c6e":"# Model the Data"}}