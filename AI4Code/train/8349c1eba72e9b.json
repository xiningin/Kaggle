{"cell_type":{"6cd36df1":"code","96caac67":"code","ad7234c7":"code","429ef43a":"code","177520e9":"code","5782f79b":"code","d2a67ca0":"code","efbbf8aa":"code","8bd18893":"code","56f3e98f":"code","8235fc53":"code","4f58638b":"code","102b7bb0":"code","f532b3c5":"code","8e8a5d5c":"code","deb40a42":"code","a94ff5fa":"code","8733ef31":"code","c88d0a1e":"code","97bddf43":"code","2ce95198":"code","5116747d":"code","daf96e4c":"code","fb49dc67":"code","6563172a":"code","64256586":"code","3ade616f":"code","34501f65":"markdown","38bfdd60":"markdown","06a4e639":"markdown","efa983f8":"markdown","be47a761":"markdown","a012585e":"markdown","4643889b":"markdown","67e0cf2c":"markdown","4af68bb4":"markdown","32e118fc":"markdown"},"source":{"6cd36df1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96caac67":"## i like declaring imports when needed at the place \nimport pandas as pd\nimport numpy as np\n","ad7234c7":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train.head()","429ef43a":"df_test.head()","177520e9":"## first step we remove the target variable from train data , and being both train and test df n line\n\ndf_y = df_train.SalePrice ## this willbecome the y training var later\n\ndf_train.drop('SalePrice',axis=1,inplace=True)\n\n## we drop the id columns and MoSold and  mo year columns , reason they are sold dates not buy dates \n## they mght have an impact but our train does not know when the house will be sold \n\ndf_train.drop(['Id','MoSold','YrSold'],axis=1,inplace=True)\ndf_test.drop(['Id','MoSold','YrSold'],axis=1,inplace=True)\n\nprint(df_train.shape,df_test.shape,df_y.shape)\n","5782f79b":"## start with aalysisng and cleaning training andtest data for normal irregularities\n## get a list of numerical var and categorical var will come in handy \nc_var = df_train.columns[df_train.dtypes=='object']  ## max data rows = 1460\nn_var = df_train.columns[df_train.dtypes != 'object']  ## max data rows = 1460\nprint(c_var.values)\nprint(n_var.values) \n\n","d2a67ca0":"## lots of na values in data \nprint(df_train.isna().sum().sum(),  df_test.isna().sum().sum())\n","efbbf8aa":"## we now prepare to impute numerical values using code to locate numerica data with na values\n## we can use the simpleimputer class from sklearn however here we will do it manually in pandas for \n## simplicity purpose. for numerical valiables first we will calculate the mean values \n## and replace them for mssing na or null values \n\n\nprint(\"before train process :\" , df_train[n_var].isna().sum().sum() )\n\n## for train data \nfor idx,val in enumerate(n_var):\n  if df_train[val].isna().sum() > 0 :\n    ser = df_train[val].fillna(df_train[val].mean()) ## create series for na mean values\n    df_train.drop(val,axis=1,inplace=True)\n    df_train[val] = ser \n    \nprint(\"after train process :\" , df_train[n_var].isna().sum().sum() )\n\n\n## for test data \n\nprint(\"before test process :\" , df_test[val].isna().sum().sum() )\n\nfor idx,val in enumerate(n_var):\n  if df_test[val].isna().sum() > 0 :\n    ser = df_test[val].fillna(df_test[val].mean()) ## create series for na mean values\n    df_test.drop(val,axis=1,inplace=True)\n    df_test[val] = ser \n    \nprint(\"after test process :\" , df_test[n_var].isna().sum().sum() )\n","8bd18893":"## now we know there are no null or NA values for numerical factors 1460 not null\nprint(df_train[n_var].isna().sum().sum(),df_test[n_var].isna().sum().sum() )\nprint(df_train.shape,df_test.shape)","56f3e98f":"## check for na values in categorical var\nprint(df_train[c_var].isna().sum().sum())  ## gives us the null values for categoricall data , we will impute these\nprint(df_test[c_var].isna().sum().sum())  ## gives us the null values for categoricall data , we will impute these\n","8235fc53":"## there are some columns with large NaN data more than 30-40% this is unusual and while we can impute it, \n## a better and IRL approcah is to remove this kind of data to lower model descrepency  or \n## fill the data with max co-occurance features  . here we will do the same and \n## replace categorical data Na values with max occurance for the feature\n\nprint(\"----------------train data ---------------------\")\n\nprint(\"before train process\", df_train[c_var].isna().sum().sum())\n\n## lets find out na values in data \nfor idx,val in enumerate(c_var):\n  if df_train[val].isna().sum()>0:\n    ## there are null values in columns\n    most_val = df_train[val].value_counts(dropna=True).index[0] ## take the most occuring non na element from columns \n    ser = df_train[val].fillna(most_val) ## create series for na mean values\n    df_train.drop(val,axis=1,inplace=True)\n    df_train[val] = ser \n\nprint(\"after train process :\", df_train[c_var].isna().sum().sum())\n\n\n## now for test data same processing \nprint(\"----------------test data ---------------------\")\n\nprint(\"before train process\",df_test[c_var].isna().sum().sum())\n\n## lets find out na values in data \nfor idx,val in enumerate(c_var):\n  if df_test[val].isna().sum()>0:\n    ## there are null values in columns\n    most_val = df_test[val].value_counts(dropna=True).index[0] ## take the most occuring non na element from columns \n    ser = df_test[val].fillna(most_val) ## create series for na mean values\n    df_test.drop(val,axis=1,inplace=True)\n    df_test[val] = ser \n\nprint(\"after test process :\", df_test[c_var].isna().sum().sum())\n\nprint(df_train.shape,df_test.shape)","4f58638b":"## finally we can see our datset in numerical and working is all cleaned and free of na values\n## as a final check we will see unique values in category copy to check if there is any NA values missed out\n## we can do it simply by comparing uniques\n\nfor idx,val in enumerate(df_train.columns):\n  if df_train[val].nunique() != df_train[val].nunique(dropna=False):\n    print(\"val:\",val,\"|\",df_train[val].nunique(),\"|\",df_train[val].nunique(dropna=False))\n\nfor idx,val in enumerate(df_test.columns):\n  if df_test[val].nunique() != df_test[val].nunique(dropna=False):\n    print(\"val:\",val,\"|\",df_test[val].nunique(),\"|\",df_test[val].nunique(dropna=False))\n\n## no records we have clean data","102b7bb0":"## as category convertion before we convert all data to same case UPPER here to remove any typo errors\n## this is not needed, but is a good practice if dealing with words , cause people type wrong \n\n## for train data \nfor idx,val in enumerate(c_var):  \n    ser = df_train[val].astype(str).str.upper()  ## create series for na mean values\n    df_train.drop(val,axis=1,inplace=True)\n    df_train[val] = ser \n\nprint(\"after process :\" , df_train[val].isna().sum().sum() )\n\n## for test data \nfor idx,val in enumerate(c_var):\n    ser = df_test[val].astype(str).str.upper()  ## create series for na mean values\n    df_test.drop(val,axis=1,inplace=True)\n    df_test[val] = ser \n\nprint(\"after process :\" , df_test[val].isna().sum().sum() )\n\n  \n    ","f532b3c5":"## There are various ways to encode data , converting to 1-hot encoding or Lael, or Ordinal\n## we will convert categorical features  with sklearn ordinal encoding to simply because it keeps our feature count fied to variable\n## and saves lot of time and space in processing\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n##\ndf_train_cat = df_train[c_var]\ndf_test_cat = df_test[c_var]\n\nord = OrdinalEncoder()\n\ntrain_ord = ord.fit_transform(df_train_cat) ## remember fit-transform on train, and only transform on test, most ppl make mistake\ntest_ord = ord.transform(df_test_cat)\n\nprint(train_ord.shape,test_ord.shape)\n\n## the shapes are correct and in sync\n\n\n","8e8a5d5c":"## now that the category features are encoded we will transform a second array with numerical features and scalng them\nfrom sklearn.preprocessing import StandardScaler\n\ndf_train_num = df_train[n_var]\ndf_test_num = df_test[n_var]\n\nprint(df_train_num.shape,df_test_num.shape)\nscalar = StandardScaler()\n\ntrain_num = scalar.fit_transform(df_train_num)\ntest_num = scalar.transform(df_test_num)\n\nprint(train_num.shape,test_num.shape)\n\n\n","deb40a42":"## now we have our data encoded into 2 nice seperate numpy arrays, we wil now combine them for processing and the final dataset\n\nx_train = np.concatenate((train_num,train_ord),axis=1)\nx_test = np.concatenate((test_num,test_ord),axis=1)\n\nprint(x_train.shape,x_test.shape)\n## data looks good to go at this point \n\n","a94ff5fa":"## train test split and we are good to go \nfrom sklearn.model_selection import train_test_split\n## remember the y col that we split in beginnng of sheet y-trin is there\ny_train = df_y.to_numpy()\n\nx_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.2,train_size=0.8)\nprint(x_train.shape,x_val.shape,y_train.shape,y_val.shape)\n","8733ef31":"## now for model we will use XGBoost , cause its the best there is apart from NN models\n## for XGB we still need some processing one, data and parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV \nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score\n","c88d0a1e":"## we will first use the grid serach CV for determining optimum hyperparameters using 2 params to optimize \n## can use more, just deends on the time taken \nparams = {\n    'learning_rate': np.arange(0.01, 0.10, 0.01)\n    #,'n_estimators' : [500,1000]\n    #, 'max_depth' : [5,10]    \n    }\n\nparams","97bddf43":"## define the parametr optimizer\noptimizer = GridSearchCV(\n    XGBRegressor(\n      n_estimators=500, max_depth=4, \n      min_child_weight=3, gamma=0, colsample_bytree=0.4, \n      subsample=0.7, learning_rate=0.05, verbosity=0, objective ='reg:squarederror'\n      ), \n    params,cv=5\n    )\n","2ce95198":"\noptimizer.fit(x_train, y_train)","5116747d":"## get best params from grid serach\n\nprint(optimizer.best_params_)\n## save the param run sin a DF\nopt_params = pd.DataFrame(optimizer.cv_results_)","daf96e4c":"## lets get the validation scores on the data , we an also use the inbuilt eval_metric in XGB boost for this\n\ny_pred = optimizer.predict(x_val) \nmsescore = mean_squared_error(y_val, y_pred)\nr2score = r2_score(y_val,y_pred)\nprint(\"mse=\",msescore,\"--r2 = \",r2score)\n## looks good ","fb49dc67":"## define the XGB regressor model parameters\n\n## now we train model\n\nmodel_params = {\n    'colsample_bytree': 0.4,\n    'n_estimators': 500 ,\n    'min_child_weight': 3,\n    'max_depth': 5 ,\n    'subsample': 0.4,\n    'learning_rate': optimizer.best_params_['learning_rate'],\n    'gamma': 0,\n    'reg_lambda': 0.02,\n    'verbosity' : 2,\n    'objective' : 'reg:squarederror'\n}\nmodel = XGBRegressor(tree_method='hist', **model_params)\n","6563172a":"## fit the model ## remember ghere x_train is a subset of train data , with val data taken out\n## we can rejoin it again but well too uch back work \nmodel.fit(x_train, y_train)\n","64256586":"y_pred = model.predict(x_test)\ndf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv') \noutput = pd.DataFrame({'Id': df.Id,\n                       'SalePrice': y_pred})\noutput.to_csv('submission.csv', index=False)","3ade616f":"output.head()","34501f65":"Data Cleaning and Corrections ","38bfdd60":"### MOdel Parameter Tuning\nGridSerachCV is he preferred method for tuning model parameters we wil use same ","06a4e639":"NOw we start with Categorical data for cleanup which is more time consuming","efa983f8":"Here we will do a  simple prediction of housing regression data run using pandas , sklearn and XGBoost .\nI wil present the code implementation and not a detailed charting analysis as others have done lot of data EDA for this dataset and its ot a very complex dataset to begin with and very small also ","be47a761":"## Submit Final Predictions ","a012585e":"## Data Transformation\n\nNowwe apply data transformationa and encoding before final feeding to network ","4643889b":"## Model Prediction Submission\n\nNow last part use the best params and submit the model  ","67e0cf2c":"## Model Building","4af68bb4":"Our Numerical data is cleaned up and imputed now","32e118fc":"Extract the Target variable from Train set ,and remove irrelevant columns\n"}}