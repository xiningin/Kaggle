{"cell_type":{"9b29a7e5":"code","9b62c27a":"code","3f75fa6c":"code","75c0853c":"code","07106793":"code","5395f446":"code","d30ed133":"code","b02653fb":"code","50841474":"code","462ac015":"code","137108b8":"code","94a250dd":"code","4dbc2f30":"code","77651467":"markdown","16433cdc":"markdown","0ca41f7c":"markdown","c30cee41":"markdown","3c65c791":"markdown","10ba237a":"markdown","61656420":"markdown","b88cf180":"markdown","9b261a63":"markdown","abb96429":"markdown","f72b4911":"markdown"},"source":{"9b29a7e5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n!pip install skompiler\nfrom skompiler import skompile\n!pip install astor","9b62c27a":"X,y=make_regression( n_samples=100,\n    n_features=10,\n    n_informative=5,\n    n_targets=1,\n    bias=0.3,\n    tail_strength=0.5,\n    noise=0.4,\n    shuffle=True,\n    random_state=42)","3f75fa6c":"X=pd.DataFrame(X)","75c0853c":"y=pd.DataFrame(y)","07106793":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","5395f446":"def error_calculator_N_visualizer(model,random_state=42, max_depth=3, option=0, n_estimators=5):\n    if option==0:\n        md=model(random_state=random_state,max_depth=max_depth)\n        md.fit(X_train,y_train[0])\n        pred=md.predict(X_test)\n        return (f'Error of the {(str(md))[:12]}: {np.sqrt(mean_squared_error(y_test[0],pred))}')\n    elif option==1:\n        md=model(random_state=random_state,max_depth=max_depth)\n        md.fit(X_train,y_train[0])\n        plt.figure(figsize=(25,20))\n        plot_tree(md,filled=True,rounded =True);\n        plt.show()\n        return\n    elif option==2:\n        md=model(random_state=random_state,max_depth=max_depth,n_estimators=n_estimators)\n        md.fit(X_train,y_train[0])\n        for i in range (n_estimators):\n            plt.figure(figsize=(16,9))\n            plot_tree(md.estimators_[i])\n            plt.title(f'Random Forest Estimator {i+1}')\n            plt.show()\n        return    ","d30ed133":"error_calculator_N_visualizer(model=DecisionTreeRegressor, option=1)","b02653fb":"error_calculator_N_visualizer(model=RandomForestRegressor, option=2)","50841474":"model=DecisionTreeRegressor (max_depth=None, \n                            random_state=42)\nmodel.fit(X_train,y_train[0])\nprint(skompile(model.predict).to('python\/code'))","462ac015":"x=[47,61,31,47,68,47,82,15,65,47]","137108b8":"(((((-234.99987763976304 if x[2] <= -0.3397384285926819 else \n    -194.42367049000353) if x[3] <= -1.413666546344757 else \n    -161.97405925305466 if x[6] <= 0.2716367542743683 else \n    -150.35894010436274 if x[7] <= 0.12535598874092102 else \n    -147.06851848391244) if x[3] <= -0.5235290974378586 else ((((((\n    -75.4333216363932 if x[6] <= 0.22984668612480164 else \n    -76.41703223445938) if x[8] <= -0.1856009140610695 else \n    -72.0976576324538) if x[3] <= 0.24283629655838013 else \n    -66.5107372582468) if x[5] <= -0.7947739660739899 else \n    -84.37760373038758) if x[9] <= 0.1784413456916809 else \n    -106.64282409123987 if x[0] <= -0.6097627207636833 else \n    -98.10453732222359 if x[6] <= 0.2187390923500061 else \n    -97.21181821943615) if x[5] <= -0.6795729696750641 else \n    -48.422184966727364) if x[8] <= 1.112101137638092 else \n    -29.730571200212328 if x[7] <= -0.5410499572753906 else \n    -45.804957569760184 if x[2] <= -1.0056066890247166 else \n    -44.009296728004095 if x[1] <= 0.3508207555860281 else \n    -43.57481479840333) if x[5] <= -0.5163491070270538 else ((((((\n    -61.92631640077975 if x[3] <= -0.6101557835936546 else \n    -55.571675257272396) if x[8] <= -0.20088261365890503 else \n    -79.7128829568596) if x[6] <= -1.2143710851669312 else ((\n    -27.142412083822467 if x[4] <= -1.2988774478435516 else \n    -19.836096786864758) if x[8] <= -0.5006479769945145 else (\n    -32.676451358147496 if x[5] <= -0.2299812864512205 else \n    -30.22707085818243) if x[2] <= -0.38922587037086487 else \n    -37.43570263451787) if x[2] <= 0.2926444709300995 else \n    -56.48613374394321) if x[0] <= 0.15404751151800156 else \n    -47.389042692860556 if x[4] <= -0.939104437828064 else (\n    -31.773434766207963 if x[7] <= -0.6866011321544647 else \n    -3.5700422455439798 if x[0] <= 0.23075515776872635 else \n    -15.576263985600797 if x[1] <= -0.6713697016239166 else (\n    -18.038633671493507 if x[4] <= -0.5117617696523666 else \n    -17.42591722834646) if x[1] <= 0.7654775828123093 else \n    -19.074327932538804) if x[0] <= 1.4515044689178467 else \n    3.9336995853034353) if x[5] <= 0.6398012042045593 else (\n    -44.84198276614646 if x[7] <= -1.4174052476882935 else \n    15.64941814871393 if x[2] <= 1.225375920534134 else -6.364601530502048) if\n    x[7] <= 0.418075829744339 else 86.92784991563094) if x[8] <= \n    0.5387247204780579 else (-17.593898287650077 if x[3] <= \n    -0.8789926469326019 else 5.525335076605818 if x[3] <= \n    -0.4731644243001938 else -2.238403656552951) if x[1] <= \n    -0.14830999309197068 else (48.01052044574529 if x[3] <= \n    0.13571885228157043 else 58.29295562995143) if x[1] <= \n    0.4652978181838989 else 31.563707539421507 if x[6] <= \n    0.8883429169654846 else 26.051379740972763) if x[5] <= \n    1.2780643701553345 else ((122.79915533816764 if x[6] <= \n    -0.8967394307255745 else 124.63164758755943) if x[8] <= \n    0.04790176451206207 else 95.16985041157827) if x[4] <= \n    0.969599187374115 else 23.203410644566624) if x[3] <= \n    0.4436162859201431 else ((-52.65965843727761 if x[8] <= \n    -0.45905765891075134 else ((-4.014315699060744 if x[5] <= \n    -1.0272375047206879 else 1.4707486542539414) if x[8] <= \n    0.7751165926456451 else 11.253875469017732) if x[8] <= \n    1.5482046902179718 else 29.72580444841806) if x[5] <= \n    -0.5854470729827881 else (((64.87169222844017 if x[3] <= \n    0.8643921613693237 else 63.207365843232104) if x[6] <= \n    0.08087699115276337 else 55.16534135191311) if x[5] <= \n    -0.12955719605088234 else 27.346911751516277) if x[4] <= \n    0.522654578089714 else 125.6609116923737) if x[5] <= \n    -0.09761787950992584 else (((53.33887415494062 if x[1] <= \n    -0.8160464763641357 else 75.13712890688797 if x[3] <= 0.834075540304184\n     else 81.0300166173223) if x[1] <= -0.4552418142557144 else ((\n    117.71252457987599 if x[8] <= 0.9636443853378296 else \n    121.91122961429109) if x[3] <= 0.8897090554237366 else \n    127.77607143894782) if x[3] <= 1.2004898190498352 else \n    101.5870400550364) if x[5] <= 1.0131782293319702 else ((\n    156.20120222961282 if x[7] <= -0.5302148163318634 else \n    155.9777089910325) if x[7] <= 0.2112601101398468 else \n    150.80872032646695) if x[0] <= 0.7963403761386871 else \n    129.77575172886404) if x[3] <= 1.4490607380867004 else (\n    151.85519835490615 if x[9] <= 0.7814179062843323 else \n    172.61525345119514) if x[4] <= -0.6071960628032684 else \n    211.22926782814753 if x[4] <= -0.44945862889289856 else 211.19954782875956)\n","94a250dd":"model.predict([[47,61,31,47,68,47,82,15,65,47]])","4dbc2f30":"print(error_calculator_N_visualizer(model=XGBRegressor))\nprint(error_calculator_N_visualizer(model=DecisionTreeRegressor))\nprint(error_calculator_N_visualizer(model=RandomForestRegressor))","77651467":"In this notebook we will,\n1. Create a new dataset for a regression problem.\n2. Explain Decision Tree Algorithm\n3. Prove that its performance is slightly worse than the other algorithms.","16433cdc":"# Decision Tree Algorithm\nDecision Tree is one of the supervised learning algorithms. Although, it is not as popular as the time it was released, learning how this algorithm works is still important. Because by doing that, we can learn other more complex and successful algorithms better.\nA couple of advantages of a decision tree are the following,\n1. It is a nonlinear algorithm.\n2. The model still works fine for the models that are specific to the given task.\n3. It can be visualized so learning the algorithm is easier compared to the others.\n4. It requires almost no data preprocessing.\n\nAnd a couple of disadvantages of a decision tree are,\n\n1. Overfitting tends to occur in this algorithm if the hyperparameters are not set well.\n2. It can be unstable. Meaning that because of the small changes it can possibly give an entirely different output or outputs. \n3. Performance of the decision tree is generally lower compared to the newer machine learning algorithms. \n\nTo read further about advantages and disadvantages you can visit  [[1]](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree)","0ca41f7c":"Thanks...","c30cee41":"Both are the same. So we successfully computed the decision tree regression model algorithm manually.","3c65c791":"# Creating Dataset\nTo create a dataset, we will use \"make_regression()\" from sklearn.datasets. We also set hyperparameters to make things a little harder for the machine learning algorithms.","10ba237a":"# Performance Comparison of the Algorithms","61656420":"As expected, the performance of DecisionTreeRegressor is lower compared to the newer machine learning algorithms.","b88cf180":"## Decision Tree and Random Forest Visualization\nTo visualize how the algorithm works, we will use \"plot_tree()\" from sklearn.tree.\n","9b261a63":"# Numerical Representation of These Trees\nUp to now, we have learned how decision trees look like. We also probably know how to run a machine learning algorithm. \n\nIn this section, we will learn how to compute these results manually. First, we will conduct a classical prediction using a Decision Tree then we will compute the result manually. Finally, we will compare the results.\n\nTo compute manual calculation, we will use \"skompile()\" from skompiler module. We will print the decision algorithm in the form of python code.","abb96429":"If we run this code directly, it will cause an error because there is an undefined unknown x in the code.\n\nSo before running the code, we will assign a list to unknown x say [47,61,31,47,68,47,82,15,65,47]","f72b4911":"Then let's see what model predicts for unknown x."}}