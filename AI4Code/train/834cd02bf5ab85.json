{"cell_type":{"99da38ee":"code","7fdcaeb4":"code","71a60d2d":"code","34d66d57":"code","82a7e1ef":"code","220c02f8":"code","a5ff4d04":"code","e08a3f46":"code","8d7ccc9e":"code","142664fd":"code","e257fb86":"code","7a67fc2a":"code","f8af7097":"code","4edeb7f3":"code","43162f1e":"code","0812aef2":"code","99c84008":"code","ed3dcb0e":"code","c6870a07":"code","0762925f":"code","1be72fe2":"code","0810634b":"code","23e905d7":"code","d077f828":"code","614c5daf":"code","416a7001":"code","6cc3496f":"code","7143a4e8":"code","e34a553e":"code","5ce6b65b":"code","9661ff04":"code","ed8bf6c4":"code","c7252a73":"code","f38cac56":"code","db35d20c":"code","10e10f53":"code","95e51f37":"code","402b7c6c":"code","8dc1a9fc":"code","4c901eb0":"code","c8cf3e3d":"code","bf7d12a1":"code","16e85946":"code","c219057f":"code","4d9c19dc":"code","c58b7df2":"code","ab43f3ee":"code","7bbbb879":"code","097e0b23":"code","95024e1a":"code","40138114":"code","d1b0d069":"code","af2fa301":"code","918b3937":"code","6b88bfa9":"code","8c2a6176":"code","dfdc20cc":"code","d703a123":"markdown","2a06a9b9":"markdown","cd5308ec":"markdown","580c8bfd":"markdown","1221b14f":"markdown","26af965b":"markdown","56b7b799":"markdown","0072dbff":"markdown","0adb2416":"markdown","e9dd22a4":"markdown","c8b665a8":"markdown","3c4dee71":"markdown","23053043":"markdown","49c3f701":"markdown","299dc12a":"markdown","e7959db6":"markdown","4887440c":"markdown"},"source":{"99da38ee":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom tqdm.notebook import tqdm\n\npd.set_option(\"display.max_columns\", 181)\npd.set_option(\"display.min_rows\", 200)","7fdcaeb4":"data_dictionary = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest_id = test.encounter_id.values\n","71a60d2d":"column_datatype_mapping = dict(zip(data_dictionary['Variable Name'], data_dictionary['Data Type']))","34d66d57":"#column_datatype_mapping","82a7e1ef":"del train['Unnamed: 0']\ndel test['Unnamed: 0']","220c02f8":"train.diabetes_mellitus.value_counts()","a5ff4d04":"data_dictionary['Data Type'].value_counts()","e08a3f46":"len(train), len(test)","8d7ccc9e":"test_id = test.encounter_id.values\ny = train['diabetes_mellitus']\ndel train['diabetes_mellitus']","142664fd":"train = train.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\ntest = test.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\ntrain.loc[train.age == 0, 'age'] = np.nan\ntrain = train.drop(['readmission_status','encounter_id','hospital_id'], axis=1)\ntest = test.drop(['readmission_status','encounter_id','hospital_id'], axis=1)\ntrain = train.replace([np.inf, -np.inf], np.nan)\ntest = test.replace([np.inf, -np.inf], np.nan)","e257fb86":"min_max_feats=[f[:-4] for f in train.columns if f[-4:]=='_min']\n#if value in min column greater than that in max column, swap the values\nfor col in min_max_feats:\n    #print(train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_min', f'{col}_max']])\n    #print(train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_max', f'{col}_min']])\n    train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_min', f'{col}_max']] = train.loc[train[f'{col}_min'] > train[f'{col}_max'], [f'{col}_max', f'{col}_min']].values\n    test.loc[test[f'{col}_min'] > test[f'{col}_max'], [f'{col}_min', f'{col}_max']] = test.loc[test[f'{col}_min'] > test[f'{col}_max'], [f'{col}_max', f'{col}_min']].values\n","7a67fc2a":"train['comorbidity_score'] = train['aids'].values * 23 + train['cirrhosis'] * 4  + train['hepatic_failure'] * 16 + train['immunosuppression'] * 10 + train['leukemia'] * 10 + train['lymphoma'] * 13 + train['solid_tumor_with_metastasis'] * 11\ntest['comorbidity_score'] = test['aids'].values * 23 + test['cirrhosis'] * 4  + test['hepatic_failure'] * 16 + test['immunosuppression'] * 10 + test['leukemia'] * 10 + test['lymphoma'] * 13 + test['solid_tumor_with_metastasis'] * 11\ntrain['comorbidity_score'] = train['comorbidity_score'].fillna(0)\ntest['comorbidity_score'] = test['comorbidity_score'].fillna(0)\ntrain['gcs_sum'] = train['gcs_eyes_apache']+train['gcs_motor_apache']+train['gcs_verbal_apache']\ntest['gcs_sum'] = test['gcs_eyes_apache']+test['gcs_motor_apache']+test['gcs_verbal_apache']\ntrain['gcs_sum'] = train['gcs_sum'].fillna(0)\ntest['gcs_sum'] = test['gcs_sum'].fillna(0)\ntrain['apache_2_diagnosis_type'] = train.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\ntest['apache_2_diagnosis_type'] = test.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\ntrain['apache_3j_diagnosis_type'] = train.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\ntest['apache_3j_diagnosis_type'] = test.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\ntrain['bmi_type'] = train.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['bmi_type'] = test.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['height_type'] = train.height.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['height_type'] = test.height.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['weight_type'] = train.weight.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['weight_type'] = test.weight.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['age_type'] = train.age.fillna(0).apply(lambda x: 10 * (round(int(x)\/10)))\ntest['age_type'] = test.age.fillna(0).apply(lambda x: 10 * (round(int(x)\/10)))\ntrain['gcs_sum_type'] = train.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\ntest['gcs_sum_type'] = test.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\ntrain['apache_3j_diagnosis_x'] = train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntrain['apache_2_diagnosis_x'] = train['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntest['apache_3j_diagnosis_x'] = test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntest['apache_2_diagnosis_x'] = test['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntrain['apache_3j_diagnosis_split1'] = np.where(train['apache_3j_diagnosis'].isna() , np.nan , train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\ntest['apache_3j_diagnosis_split1']  = np.where(test['apache_3j_diagnosis'].isna() , np.nan , test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\ntrain['apache_2_diagnosis_split1'] = np.where(train['apache_2_diagnosis'].isna() , np.nan , train['apache_2_diagnosis'].apply(lambda x : x % 10)  )\ntest['apache_2_diagnosis_split1']  = np.where(test['apache_2_diagnosis'].isna() , np.nan , test['apache_2_diagnosis'].apply(lambda x : x % 10) )\n\n","f8af7097":"df = pd.concat([train['icu_id'], test['icu_id']])\nagg = df.value_counts().to_dict()\ntrain['icu_id_counts'] = np.log1p(train['icu_id'].map(agg))\n# create new feature count of icu id\ntest['icu_id_counts'] = np.log1p(test['icu_id'].map(agg))\ndf = pd.concat([train['age'], test['age']])\nagg = df.value_counts().to_dict()\ntrain['age_counts'] = np.log1p(train['age'].map(agg))\ntest['age_counts'] = np.log1p(test['age'].map(agg))\ntrain[\"diff_bmi\"] = train['bmi'].copy() \ntrain['bmi'] = train['weight']\/((train['height']\/100)**2)\ntrain[\"diff_bmi\"] = train[\"diff_bmi\"]-train['bmi']\ntest[\"diff_bmi\"] = test['bmi'].copy()\ntest['bmi'] = test['weight']\/((test['height']\/100)**2)\ntest[\"diff_bmi\"] = test[\"diff_bmi\"]-test['bmi']\ntrain['abmi'] = train['age']\/train['bmi']\ntrain['agi'] = train['weight']\/train['age']\ntest['abmi'] = test['age']\/train['bmi']\ntest['agi'] = test['weight']\/train['age']","4edeb7f3":"train['diasbp_indicator'] = (\n(train['d1_diasbp_invasive_max'] == train['d1_diasbp_max']) & (train['d1_diasbp_noninvasive_max']==train['d1_diasbp_invasive_max'])|\n(train['d1_diasbp_invasive_min'] == train['d1_diasbp_min']) & (train['d1_diasbp_noninvasive_min']==train['d1_diasbp_invasive_min'])|\n(train['h1_diasbp_invasive_max'] == train['h1_diasbp_max']) & (train['h1_diasbp_noninvasive_max']==train['h1_diasbp_invasive_max'])|\n(train['h1_diasbp_invasive_min'] == train['h1_diasbp_min']) & (train['h1_diasbp_noninvasive_min']==train['h1_diasbp_invasive_min'])\n).astype(np.int8)\n\n\ntrain['mbp_indicator'] = (\n(train['d1_mbp_invasive_max'] == train['d1_mbp_max']) & (train['d1_mbp_noninvasive_max']==train['d1_mbp_invasive_max'])|\n(train['d1_mbp_invasive_min'] == train['d1_mbp_min']) & (train['d1_mbp_noninvasive_min']==train['d1_mbp_invasive_min'])|\n(train['h1_mbp_invasive_max'] == train['h1_mbp_max']) & (train['h1_mbp_noninvasive_max']==train['h1_mbp_invasive_max'])|\n(train['h1_mbp_invasive_min'] == train['h1_mbp_min']) & (train['h1_mbp_noninvasive_min']==train['h1_mbp_invasive_min'])\n).astype(np.int8)\n\ntrain['sysbp_indicator'] = (\n(train['d1_sysbp_invasive_max'] == train['d1_sysbp_max']) & (train['d1_sysbp_noninvasive_max']==train['d1_sysbp_invasive_max'])|\n(train['d1_sysbp_invasive_min'] == train['d1_sysbp_min']) & (train['d1_sysbp_noninvasive_min']==train['d1_sysbp_invasive_min'])|\n (train['h1_sysbp_invasive_max'] == train['h1_sysbp_max']) & (train['h1_sysbp_noninvasive_max']==train['h1_sysbp_invasive_max'])|\n(train['h1_sysbp_invasive_min'] == train['h1_sysbp_min']) & (train['h1_sysbp_noninvasive_min']==train['h1_sysbp_invasive_min'])   \n).astype(np.int8)\n\ntrain['d1_mbp_invnoninv_max_diff'] = train['d1_mbp_invasive_max'] - train['d1_mbp_noninvasive_max']\ntrain['h1_mbp_invnoninv_max_diff'] = train['h1_mbp_invasive_max'] - train['h1_mbp_noninvasive_max']\ntrain['d1_mbp_invnoninv_min_diff'] = train['d1_mbp_invasive_min'] - train['d1_mbp_noninvasive_min']\ntrain['h1_mbp_invnoninv_min_diff'] = train['h1_mbp_invasive_min'] - train['h1_mbp_noninvasive_min']\ntrain['d1_diasbp_invnoninv_max_diff'] = train['d1_diasbp_invasive_max'] - train['d1_diasbp_noninvasive_max']\ntrain['h1_diasbp_invnoninv_max_diff'] = train['h1_diasbp_invasive_max'] - train['h1_diasbp_noninvasive_max']\ntrain['d1_diasbp_invnoninv_min_diff'] = train['d1_diasbp_invasive_min'] - train['d1_diasbp_noninvasive_min']\ntrain['h1_diasbp_invnoninv_min_diff'] = train['h1_diasbp_invasive_min'] - train['h1_diasbp_noninvasive_min']\ntrain['d1_sysbp_invnoninv_max_diff'] = train['d1_sysbp_invasive_max'] - train['d1_sysbp_noninvasive_max']\ntrain['h1_sysbp_invnoninv_max_diff'] = train['h1_sysbp_invasive_max'] - train['h1_sysbp_noninvasive_max']\ntrain['d1_sysbp_invnoninv_min_diff'] = train['d1_sysbp_invasive_min'] - train['d1_sysbp_noninvasive_min']\ntrain['h1_sysbp_invnoninv_min_diff'] = train['h1_sysbp_invasive_min'] - train['h1_sysbp_noninvasive_min']\n\ntest['diasbp_indicator'] = (\n(test['d1_diasbp_invasive_max'] == test['d1_diasbp_max']) & (test['d1_diasbp_noninvasive_max']==test['d1_diasbp_invasive_max'])|\n(test['d1_diasbp_invasive_min'] == test['d1_diasbp_min']) & (test['d1_diasbp_noninvasive_min']==test['d1_diasbp_invasive_min'])|\n(test['h1_diasbp_invasive_max'] == test['h1_diasbp_max']) & (test['h1_diasbp_noninvasive_max']==test['h1_diasbp_invasive_max'])|\n(test['h1_diasbp_invasive_min'] == test['h1_diasbp_min']) & (test['h1_diasbp_noninvasive_min']==test['h1_diasbp_invasive_min'])\n).astype(np.int8)\n\n\ntest['mbp_indicator'] = (\n(test['d1_mbp_invasive_max'] == test['d1_mbp_max']) & (test['d1_mbp_noninvasive_max']==test['d1_mbp_invasive_max'])|\n(test['d1_mbp_invasive_min'] == test['d1_mbp_min']) & (test['d1_mbp_noninvasive_min']==test['d1_mbp_invasive_min'])|\n(test['h1_mbp_invasive_max'] == test['h1_mbp_max']) & (test['h1_mbp_noninvasive_max']==test['h1_mbp_invasive_max'])|\n(test['h1_mbp_invasive_min'] == test['h1_mbp_min']) & (test['h1_mbp_noninvasive_min']==test['h1_mbp_invasive_min'])\n).astype(np.int8)\n\ntest['sysbp_indicator'] = (\n(test['d1_sysbp_invasive_max'] == test['d1_sysbp_max']) & (test['d1_sysbp_noninvasive_max']==test['d1_sysbp_invasive_max'])|\n(test['d1_sysbp_invasive_min'] == test['d1_sysbp_min']) & (test['d1_sysbp_noninvasive_min']==test['d1_sysbp_invasive_min'])|\n (test['h1_sysbp_invasive_max'] == test['h1_sysbp_max']) & (test['h1_sysbp_noninvasive_max']==test['h1_sysbp_invasive_max'])|\n(test['h1_sysbp_invasive_min'] == test['h1_sysbp_min']) & (test['h1_sysbp_noninvasive_min']==test['h1_sysbp_invasive_min'])   \n).astype(np.int8)\n\ntest['d1_mbp_invnoninv_max_diff'] = test['d1_mbp_invasive_max'] - test['d1_mbp_noninvasive_max']\ntest['h1_mbp_invnoninv_max_diff'] = test['h1_mbp_invasive_max'] - test['h1_mbp_noninvasive_max']\ntest['d1_mbp_invnoninv_min_diff'] = test['d1_mbp_invasive_min'] - test['d1_mbp_noninvasive_min']\ntest['h1_mbp_invnoninv_min_diff'] = test['h1_mbp_invasive_min'] - test['h1_mbp_noninvasive_min']\ntest['d1_diasbp_invnoninv_max_diff'] = test['d1_diasbp_invasive_max'] - test['d1_diasbp_noninvasive_max']\ntest['h1_diasbp_invnoninv_max_diff'] = test['h1_diasbp_invasive_max'] - test['h1_diasbp_noninvasive_max']\ntest['d1_diasbp_invnoninv_min_diff'] = test['d1_diasbp_invasive_min'] - test['d1_diasbp_noninvasive_min']\ntest['h1_diasbp_invnoninv_min_diff'] = test['h1_diasbp_invasive_min'] - test['h1_diasbp_noninvasive_min']\n\ntest['d1_sysbp_invnoninv_max_diff'] = test['d1_sysbp_invasive_max'] - test['d1_sysbp_noninvasive_max']\ntest['h1_sysbp_invnoninv_max_diff'] = test['h1_sysbp_invasive_max'] - test['h1_sysbp_noninvasive_max']\ntest['d1_sysbp_invnoninv_min_diff'] = test['d1_sysbp_invasive_min'] - test['d1_sysbp_noninvasive_min']\ntest['h1_sysbp_invnoninv_min_diff'] = test['h1_sysbp_invasive_min'] - test['h1_sysbp_noninvasive_min']\n\n\nfor v in ['albumin','bilirubin','bun','glucose','hematocrit','pao2fio2ratio','arterial_ph','resprate','sodium','temp','wbc','creatinine']:\n    train[f'{v}_indicator'] = (((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'd1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_max'])) \n                ).astype(np.int8)\n    test[f'{v}_indicator'] = (((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'd1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_max'])) \n                ).astype(np.int8)","43162f1e":"train['apache_3j'] = np.where(train['apache_3j_diagnosis_type']<0 , np.nan ,\n                            np.where(train['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n                            np.where(train['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n                            np.where(train['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n                            np.where(train['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n                            np.where(train['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n                            np.where(train['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n                            np.where(train['apache_3j_diagnosis_type'] < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(train['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )\ntest['apache_3j'] = np.where(test['apache_3j_diagnosis_type']<0 , np.nan ,\n                            np.where(test['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n                            np.where(test['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n                            np.where(test['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n                            np.where(test['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n                            np.where(test['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n                            np.where(test['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n                            np.where(test['apache_3j_diagnosis_type'] < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(test['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )","0812aef2":"train.shape, test.shape","99c84008":"percent_missing = train.isnull().sum() * 100 \/ len(train)\nmissing_value_df = pd.DataFrame({'column_name': train.columns,\n                                 'percent_missing': percent_missing})\n#pd.set_option('display.max_rows', None)\nmissing_value_df.sort_values('percent_missing', ascending=False).head(5)\nsparse_cols = missing_value_df[missing_value_df['percent_missing'] > 80]['column_name'].values.tolist()","ed3dcb0e":"train = train.drop(sparse_cols, axis=1)\ntest = test.drop(sparse_cols, axis=1)","c6870a07":"def impute_gender(cols):\n    height = cols[1]\n    gender = cols[0]\n    if pd.isnull(gender):\n        if (height<161.834):\n            return 'F'\n        else:\n            return 'M'\n    else:\n        return gender\n    \ndef impute_ethnicity(cols):\n    height = cols[1]\n    ethnicity = cols[0]\n    if pd.isnull(ethnicity):\n        if (height>170.361):\n            return 'African American'\n        elif(height<162.740):\n            return 'Asian'\n        elif (height>167 and height<170):\n            return 'Caucasian'\n        elif (height>162.740 and height<166):\n            return 'Hispanic'\n        elif (height>166. and height<168):\n                return 'Other\/Unknown'\n        else:\n            return 'Native American'\n                \n    else:\n        return ethnicity\n    \n","0762925f":"train['gender'] = train[['gender','height']].apply(impute_gender,axis = 1)\ntest['gender'] = test[['gender','height']].apply(impute_gender,axis = 1)\n\ntrain['ethnicity'] = train[['ethnicity','height']].apply(impute_ethnicity,axis = 1)\ntest['ethnicity'] = test[['ethnicity','height']].apply(impute_ethnicity,axis = 1)","1be72fe2":"categorical_cols = ['gender','ethnicity','elective_surgery','icu_id',\n  'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type','aids','cirrhosis','hepatic_failure','immunosuppression',\n 'leukemia','lymphoma','solid_tumor_with_metastasis','elective_surgery','apache_post_operative','arf_apache','fio2_apache','gcs_unable_apache','gcs_eyes_apache',\n 'gcs_motor_apache','gcs_verbal_apache','intubated_apache','ventilated_apache','solid_tumor_with_metastasis']","0810634b":"cat_cols = train.select_dtypes(exclude = np.number).columns.tolist()\ncategorical_cols_1 = list(set(categorical_cols).union(set(cat_cols)))","23e905d7":"numeric_cols = []\nfor col in train.columns:\n    if col in categorical_cols or col in cat_cols:\n        temp = train[col].value_counts().index[0]\n        train[col] = train[col].fillna(temp)\n        test[col] = test[col].fillna(temp)\n    else:\n        temp = train[col].median()\n        train[col] = train[col].fillna(temp)\n        test[col] = test[col].fillna(temp)\n        numeric_cols.append(col)      \n\n        ","d077f828":"train.shape, test.shape","614c5daf":"len(numeric_cols), len(categorical_cols_1)","416a7001":"le = LabelEncoder()\n#all_data['gender'] = le.fit_transform(all_data['gender'])\n#all_data['ethnicity'] = le.fit_transform(all_data['ethnicity'])\nfor i, col in tqdm(enumerate(categorical_cols_1)):\n    le = LabelEncoder().fit(pd.concat([train[col].astype(str),test[col].astype(str)]))   \n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))","6cc3496f":"features = train.columns\nnum_feature = [col for col in features if col not in categorical_cols_1 and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.999 :\n            if columns[j]:\n                columns[j] = False\n                #print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False","7143a4e8":"highly_corr_cols = train[num_feature].columns[columns == False].values.tolist()","e34a553e":"highly_corr_cols","5ce6b65b":"train = train.drop(highly_corr_cols, axis=1)\ntest = test.drop(highly_corr_cols, axis=1)","9661ff04":"training_df = pd.concat([train, y], axis=1)\ntraining_df_minority_class = training_df[training_df['diabetes_mellitus'] == 1]\ntraining_df_majority_class = training_df[training_df['diabetes_mellitus'] == 0 ]","ed8bf6c4":"training_df_minority_upsampled = resample(training_df_minority_class, \n                                 replace=True,     # sample with replacement\n                                 n_samples=100000,    # to match majority class\n                                 random_state= 303)","c7252a73":"training_df_upsampled = pd.concat([training_df_majority_class, training_df_minority_upsampled])","f38cac56":"y = training_df_upsampled['diabetes_mellitus']\ntrain = training_df_upsampled.drop(['diabetes_mellitus'], axis=1)","db35d20c":"y.value_counts()","10e10f53":"#https:\/\/stats.stackexchange.com\/questions\/495252\/can-oversampling-be-moved-outside-stratified-k-fold-cv","95e51f37":"kf = StratifiedKFold(n_splits=3, shuffle=True)\n","402b7c6c":"lgbmc = LGBMClassifier()","8dc1a9fc":"# lgbmc = LGBMClassifier( random_state=33,\n                              \n#                               n_estimators=1000,\n#                               boosting_type='gbdt', num_leaves=151, max_depth=- 1, learning_rate=0.02, subsample_for_bin=200, \n#                               min_split_gain=0.5, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n#                               colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,\n#                               silent=True, importance_type='split')","4c901eb0":"scores = []\npredicts = []\nfor train_index, test_index in kf.split(train, y):\n    print(\"###\")\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    lgbmc.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n            eval_metric=['logloss','auc'], early_stopping_rounds=20)\n    prval = lgbmc.predict_proba(X_val)[:,1]\n    roc = metrics.roc_auc_score(y_val, prval)\n    \n    scores.append(roc)\n    #predicts.append(lgbmc.predict(test))","c8cf3e3d":"print('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))\n","bf7d12a1":"#lgbmc.fit(train, y)\n#AUC_FINAL=metrics.roc_auc_score(y.values, lgbmc.predict(train))\n","16e85946":"feature_imp = pd.DataFrame(sorted(zip(lgbmc.feature_importances_, train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(10))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","c219057f":"train['adv_target'] = 0\ntest['adv_target'] = 1\ntarget = 'adv_target'","4d9c19dc":"def create_adversarial_data(df_train, df_test, cols, N_val=20000):\n    df_master = pd.concat([df_train[cols], df_test[cols]], axis=0)\n    adversarial_val = df_master.sample(N_val, replace=False)\n    adversarial_train = df_master[~df_master.index.isin(adversarial_val.index)]\n    return adversarial_train, adversarial_val\n\n#features = cat_cols + numeric_cols + ['TransactionDT']\n#all_cols = features + [target]\nadversarial_train, adversarial_test = create_adversarial_data(train, test, train.columns)","c58b7df2":"lgbm_adv = LGBMClassifier()","ab43f3ee":"drop_columns = ['adv_target'] \nlgbm_adv.fit(adversarial_train.drop(drop_columns, axis=1), adversarial_train['adv_target'])\nadv_pred = lgbm_adv.predict(adversarial_test.drop(drop_columns, axis=1))\nmetrics.roc_auc_score(adversarial_test['adv_target'], adv_pred)","7bbbb879":"feature_imp = pd.DataFrame(sorted(zip(lgbm_adv.feature_importances_, adversarial_train.drop(drop_columns, axis=1))), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(10))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","097e0b23":"drop_columns.extend(['diff_bmi', 'abmi'])","95024e1a":"lgbm_adv.fit(adversarial_train.drop(drop_columns, axis=1), adversarial_train['adv_target'])\nadv_pred = lgbm_adv.predict(adversarial_test.drop(drop_columns, axis=1))\nmetrics.roc_auc_score(adversarial_test['adv_target'], adv_pred)","40138114":"feature_imp = pd.DataFrame(sorted(zip(lgbm_adv.feature_importances_, adversarial_train.drop(drop_columns, axis=1))), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(10))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","d1b0d069":"#sorted(zip(lgbm_adv.feature_importances_, adversarial_train.drop('adv_target', axis=1)))","af2fa301":"train.drop(drop_columns, axis=1, inplace=True)\ntest.drop(drop_columns, axis=1, inplace=True)","918b3937":"# lgbmc = LGBMClassifier(\n#                               random_state=33,\n#                               n_estimators=10000,min_data_per_group=5, # reduce overfitting when using categorical_features\n#                               boosting_type='gbdt', num_leaves=151, max_depth=- 1, learning_rate=0.02, subsample_for_bin=200000, \n#                               min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n#                               colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,cat_smooth=1.0, \n#                               silent=True, importance_type='split')\nlgbmc = LGBMClassifier()","6b88bfa9":"# scores = []\n\nfor train_index, test_index in kf.split(train, y):\n    print(\"###\")\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    lgbmc.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n            eval_metric=['logloss','auc'], early_stopping_rounds=20)\n    prval = lgbmc.predict_proba(X_val)[:,1]\n    roc = metrics.roc_auc_score(y_val, prval)\n    \n    scores.append(roc)\nprint('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))\n","8c2a6176":"lgbmc.fit(train, y)","dfdc20cc":"pd.DataFrame({'encounter_id':test_id,\n    'diabetes_mellitus':lgbmc.predict_proba(test)[:,1]}).to_csv('\/kaggle\/working\/Predictions1.csv',\n                                                                   index=False)","d703a123":"## Building Model","2a06a9b9":"To convert boolean to int we use astype(np.int8)","cd5308ec":"#### Stratified K Fold CV","580c8bfd":"**Terminology**\n\n**APACHE Score** \u2014 Acute Physiology, Age, and Chronic Health Evaluation. A severity score and mortality estimation tool developed in the United States. It is designed to measure the severity of disease for adult patients admitted to intensive care units\n\n**Comorbodity Score** - Measure that consists of the sum of all known comorbid conditions and all associated medications. To predict one-year mortality for patients who may have a range of comorbid conditions. (https:\/\/cran.r-project.org\/web\/packages\/comorbidity\/vignettes\/comorbidityscores.html)\n\nBlood pressure readings are given in two numbers.\n\n**Systolic pressure** - Maximum pressure your heart exerts while beating \n\n**Diastolic pressure** - Amount of pressure in your arteries between beats ","1221b14f":"We generally assume that train and test data come from the same distribution. But that might not be the case. ","26af965b":"#### Missing Value Analysis","56b7b799":"We build a classifier which tries to predict if data row belongs to train data and test data. If classifier performs very well, it implies that the distributions may not be similar. So better the classifier bigger the problem. So we analyse the most important features of the classifier to get an idea of features which may be causing overfitting","0072dbff":"Drop these 2 columns which are causing overfitting","0adb2416":"## Feature Engineering","e9dd22a4":"**Problem Statement**: Determine whether a patient admitted to an ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus, using data from the first 24 hours of intensive care","c8b665a8":"## Model Building (post Adversarial Validation)","3c4dee71":"Here we used a simple upsampling technique to upsample minority class","23053043":"**What We Learnt from Datathon?**\n\n- Feature Engineering (creating features based on domain knowledge, missing value imputation, removing correlated columns etc) \n- Dealing With Imbalanced Data\n- Curse of Overfitting and Adversarial Validation","49c3f701":"## Importing Libraries","299dc12a":"#### Removing Highly Correlated Columns","e7959db6":"## Dealing with Imbalanced Dataset","4887440c":"## Adversarial Validation"}}