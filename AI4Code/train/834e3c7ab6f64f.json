{"cell_type":{"b39b3b03":"code","f9678fe7":"code","5c6ed233":"code","592e63a2":"code","e67f75e5":"code","8ab5a3dd":"code","e7a9f8e3":"code","c43257d3":"code","08062144":"code","232ad423":"code","05c556f1":"code","744188ea":"code","a77dc506":"code","57f6df24":"code","8a8c2902":"code","3726f0d5":"code","6ca795ec":"code","0ef2f598":"code","b3b2f3df":"code","e74d8082":"code","dee285a1":"code","9e0aa0cf":"code","660779a7":"code","90155585":"code","a203fdbe":"code","687958da":"code","872d8966":"code","23c17a48":"code","289ccf95":"code","9d189988":"code","cc9be66d":"code","cbaad201":"code","c8893cd7":"code","7814d3a7":"code","e65f0c5d":"code","691864d8":"code","3be34170":"code","e3be4ce0":"code","f90ff599":"code","3b870d77":"code","0f2e7a0a":"code","6b35488e":"code","d13ec9d7":"code","dd3c2756":"code","bedf71b0":"code","1285bef8":"markdown","0235b083":"markdown","e5eb1cf0":"markdown","bc71293a":"markdown","cc946973":"markdown"},"source":{"b39b3b03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport statsmodels.api as sm\nimport pylab\nfrom scipy import stats\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,StandardScaler\nfrom sklearn.decomposition import PCA\nimport tensorflow_addons as tfa\n# from sklearn.components import PCA\n# from keras.utils import to_categorical\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9678fe7":"train_features=pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntrain_targets=pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\ntest_features=pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nsubmission=pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")","5c6ed233":"train_features.head(10)","592e63a2":"test_features.head(10)","e67f75e5":"# Checking the Shape of Data\ntrain_features.shape","8ab5a3dd":"test_features.shape","e7a9f8e3":"train_targets.head(10)","c43257d3":"submission.head(10)","08062144":"# Checking the Missing Values\ntrain_features.isnull().sum()\/len(train_features)","232ad423":"# Missing HeatMap\nplt.figure(figsize=(12,8))\nsns.heatmap(train_features.isnull(),cbar=False).set_title(\"Missing Values\")","05c556f1":"# Checking Feature Correlation\nplt.figure(figsize=(12,7))\nsns.heatmap(train_features[:10].corr())","744188ea":"# Checking CP Type Distribution\ny=Counter(train_features.cp_type).most_common(train_features.cp_type.nunique())\ncp_type=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_type).set_title(\"CP Type Distribution\")\nplt.xlabel(\"CP Count\")\nplt.ylabel(\"CP Type\")","a77dc506":"# Checking CP Dose Distribution\ny=Counter(train_features.cp_dose).most_common(train_features.cp_dose.nunique())\ncp_dose=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_dose).set_title(\"CP Dose Distribution\")\nplt.xlabel(\"Dose Count\")\nplt.ylabel(\"Dose Type\")","57f6df24":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.distplot(train_features['g-0'])\nplt.subplot(2,2,2)\nsns.distplot(train_features['g-7'])\n\nplt.subplot(2,2,3)\nsns.distplot(train_features['c-0'])\nplt.subplot(2,2,4)\nsns.distplot(train_features['c-7'])","8a8c2902":"# Q-Q Plot\nplt.figure(figsize=(12,10))\nsm.qqplot(train_features['g-0'], line='45')","3726f0d5":"y=Counter(train_features.cp_time).most_common(train_features.cp_time.nunique())\ncp_time=[i[0] for i in y]\ncp_count=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(cp_count,cp_time).set_title(\"CP Time Distribution\")","6ca795ec":"train_targets","0ef2f598":"y=Counter(train_targets[\"5-alpha_reductase_inhibitor\"]).most_common(train_targets[\"5-alpha_reductase_inhibitor\"].nunique())\nname=[i[0] for i in y]\ncount=[i[1] for i in y]\nplt.figure(figsize=(12,7))\nsns.barplot(name,count).set_title(\"5-alpha_reductase_inhibitor Distribution\")","b3b2f3df":"train_features=pd.get_dummies(train_features,columns=['cp_type'])\ntest_features=pd.get_dummies(test_features,columns=['cp_type'])\ncp_dose_enc={'D1':0,'D2':1}\ntrain_features['cp_dose']=train_features['cp_dose'].replace(cp_dose_enc)\n\ntest_features['cp_dose']=test_features['cp_dose'].replace(cp_dose_enc)","e74d8082":"# Dropping Columns\ntrain_features.drop(columns=['sig_id','cp_time','cp_type_ctl_vehicle'],inplace=True)\ntrain_targets.drop(columns=['sig_id'],inplace=True)\n\ntest_features.drop(columns=['sig_id','cp_type_ctl_vehicle','cp_time'],inplace=True)","dee285a1":"train_features.head(10)","9e0aa0cf":"trt_cp=train_features[\"cp_type_trt_cp\"]\ntrain_features.drop(labels=['cp_type_trt_cp'], axis=1,inplace = True)\ntrain_features.insert(0, 'cp_type_trt_cp', trt_cp)","660779a7":"trt_cp=test_features[\"cp_type_trt_cp\"]\ntest_features.drop(labels=['cp_type_trt_cp'], axis=1,inplace = True)\ntest_features.insert(0, 'cp_type_trt_cp', trt_cp)","90155585":"train_features.head(10)","a203fdbe":"test_features","687958da":"# train_features.iloc[:,0:2]=train_features.iloc[:,0:2].astype('category')\n# test_features.iloc[:,0:2]=test_features.iloc[:,0:2].astype('category')","872d8966":"# train_features.dtypes","23c17a48":"# # # Scaling the Features\n# scaler=MinMaxScaler()\n# num_cols = list(train_features.select_dtypes(include=['float64']).columns)\n# train_features[num_cols] = scaler.fit_transform(train_features[num_cols])\n\n# test_features[num_cols]=scaler.transform(test_features[num_cols])","289ccf95":"# # Variance VS Components\n# pca = PCA().fit(train_features)\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('number of components')\n# plt.ylabel('cumulative explained variance')","9d189988":"# # PCA\n# pca=PCA(n_components=400)\n# train_components=pca.fit_transform(train_features)\n# test_components=pca.transform(test_features)","cc9be66d":"# Train Test Split\n# x_train,x_val,y_train,y_val=train_test_split(train_components,train_targets,test_size=0.20)","cbaad201":"# # Transforming Skewed Data\n# pt=PowerTransformer()\n# pt.fit(x_train.iloc[:,2:])\n# x_train_pt=pd.DataFrame(pt.transform(x_train.iloc[:,2:]),columns=x_train.iloc[:,2:].columns).set_index(x_train.index)\n\n# x_val_pt=pd.DataFrame(pt.transform(x_val.iloc[:,2:]),columns=x_val.iloc[:,2:].columns).set_index(x_val.index)\n\n# test_features_pt=pd.DataFrame(pt.transform(test_features.iloc[:,2:]),columns=test_features.iloc[:,2:].columns).set_index(test_features.index)","c8893cd7":"# x_train.drop(columns=x_train.iloc[:,2:],inplace=True)\n# x_val.drop(columns=x_val.iloc[:,2:],inplace=True)\n\n# test_features.drop(columns=test_features.iloc[:,2:],inplace=True)\n\n# x_train=pd.concat([x_train,x_train_pt],axis=1)\n# x_val=pd.concat([x_val,x_val_pt],axis=1)\n\n# test_features=pd.concat([test_features,test_features_pt],axis=1)","7814d3a7":"# x_train.skew(axis=0)","e65f0c5d":"LR=0.001\nBATCH_SIZE=16\nEPOCHS=30","691864d8":"\ndef build_model():\n    inp=tf.keras.layers.Input(shape=(train_features.shape[1],))\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(128,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.5)(x)\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(64,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.25)(x)\n\n    x=tfa.layers.WeightNormalization(tf.keras.layers.Dense(32,activation='relu'))(inp)\n    x=tf.keras.layers.BatchNormalization()(x)\n    x=tf.keras.layers.Dropout(0.25)(x)\n\n    out=tf.keras.layers.Dense(train_targets.shape[1],activation=\"sigmoid\")(x)\n\n    model=tf.keras.models.Model(inputs=inp,outputs=out)\n    \n    return model","3be34170":"save_best=tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\",monitor='val_loss',save_best_only=True)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.6,min_lr_rate=0.000000001)\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)","e3be4ce0":"# # Custom Metric\n# def log_loss(y_true,y_pred):\n# #     y_true = tf.cast(y_true,tf.float32)\n# #     y_pred = tf.cast(y_pred,tf.float32)\n#     loss = ((y_true*tf.math.log(y_pred))+((1.0-y_true)*tf.math.log(1.0-y_pred)))\n    \n#     return loss\n","f90ff599":"# Compiling the Model\nmodel=build_model()\nopt=tf.keras.optimizers.Adam(learning_rate=LR)\nmodel.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[tf.metrics.AUC()])","3b870d77":"# Model Summary\nmodel.summary()","0f2e7a0a":"train_features=train_features.values","6b35488e":"train_targets=train_targets.values","d13ec9d7":"# K-fold Cross Validation model Training and Evaludation\n# Define per-fold score containers <-- these are new\nacc_per_fold = []\nloss_per_fold = []\nkfold=KFold(n_splits=10)\nfold_no = 1\n\nfor train_index, val_index in kfold.split(train_features, train_targets):\n\n    # Define the model architecture\n    model=build_model()\n    # Compiling the Model\n    \n    opt=tf.keras.optimizers.Adam(learning_rate=LR)\n    \n    model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[tf.keras.metrics.AUC()])\n\n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n  # Fit data to model\n    history = model.fit(train_features[train_index], train_targets[train_index],\n              batch_size=BATCH_SIZE,\n              epochs=EPOCHS,callbacks=[reduce_lr])\n\n\n    # Generate generalization metrics\n    scores = model.evaluate(train_features[val_index], train_targets[val_index], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    model.save(f'model_{fold_no}.h5')\n    \n    # Increase fold number\n    fold_no = fold_no + 1\n    \n    # == Provide average scores ==\n    print('------------------------------------------------------------------------')\n    print('Score per fold')\n    for i in range(0, len(acc_per_fold)):\n      print('------------------------------------------------------------------------')\n      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)}')\n    print('------------------------------------------------------------------------')\n\n\n# model.fit(x_train,y_train,batch_size=BATCH_SIZE,validation_data=(x_val,y_val),callbacks=[save_best,reduce_lr,early_stopping],epochs=EPOCHS)","dd3c2756":"# Loading the Model\nmodel=tf.keras.models.load_model(\".\/model_10.h5\")","bedf71b0":"# Submitting the Predictions\nsig_id=submission['sig_id']\nsubmission.drop(columns=['sig_id'],inplace=True)\npredictions=pd.DataFrame(model.predict(test_features),columns=submission.columns)\npredictions.insert(0, 'sig_id', sig_id)\npredictions.to_csv(\"submission.csv\",index=False)","1285bef8":"Exploring the Data","0235b083":"Model Constants","e5eb1cf0":"Importing the Dataset","bc71293a":"Building Model","cc946973":"Featuring Engineering"}}