{"cell_type":{"fdc50c51":"code","9c740023":"code","ddc521e7":"code","6d603448":"code","4596ccf5":"code","0135cc5e":"code","d8d30783":"code","da0658f9":"code","8517f4cc":"code","7e13a000":"code","e117997f":"code","c4612f5d":"code","2b29799a":"code","a6441afb":"code","fde4ec3a":"code","bc1e5ca4":"code","2f0c1401":"code","10fec16f":"code","544b87c7":"code","fa85b9fb":"code","e542119e":"code","557ca788":"code","a1893f2a":"code","2ddeff04":"code","47dbd509":"code","6d49431b":"code","3cf75f07":"code","4ed86a82":"code","0e1f176e":"code","5a78b842":"code","8418c5e0":"code","e0f03353":"code","9d5a6dda":"code","4fc4ccf2":"code","5884f2fc":"code","c6507984":"code","3d6a1e56":"code","6c37e13e":"code","5d4a6e04":"code","248386c9":"code","9271d0ce":"code","78b3724d":"code","01f72e7c":"code","2bdcc72e":"code","9dc4d765":"code","fe0ea30a":"code","a2d2e40a":"code","04e0fd13":"code","f3204432":"code","6d291704":"code","96277370":"code","88ccd2dd":"code","281d7ef2":"code","da8fd834":"code","aac2fb3b":"code","891bfd91":"code","37e15f9c":"code","94a685f6":"code","933d60a3":"code","9b6c1f32":"code","a4f76b48":"code","5fa1275a":"code","644e06ca":"code","a788bead":"code","394fcb58":"code","ee6337c9":"code","ee52d811":"markdown","4220ef74":"markdown","34fcd5c2":"markdown","1490b658":"markdown","7f5b093c":"markdown","a5812ae7":"markdown","b63d4f54":"markdown","41928295":"markdown","e8bb50a3":"markdown","651907d2":"markdown","2b783d98":"markdown","64c8750d":"markdown","7571bc22":"markdown","d3f8181b":"markdown","6e4fae5b":"markdown","6b76f822":"markdown","e37ebdd6":"markdown","947c089f":"markdown"},"source":{"fdc50c51":"# Importing libraries \nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Importing spacy iabrary\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Importing nltk library\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n# Importing Classical Machine Learning libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n \n# Sklearn libraries\nimport sklearn \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Deep learning libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Bidirectional, Dense, Dropout, Embedding, Input\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\n# Importing Dependencies BERT, transformers\nimport os\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer\n\n# Ignoring warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9c740023":"# Reading the training dataset\ntrain = pd.read_csv(r'..\/input\/zs-challenge-data\/train.csv')\ntrain","ddc521e7":"# Reading the test dataset\ntest = pd.read_csv(r'..\/input\/zs-challenge-data\/test.csv')\ntest","6d603448":"train.info()","4596ccf5":"train.nunique()","0135cc5e":"# Checking the imbalance in the target variable\ntrain['CLASS'].value_counts()","d8d30783":"# Basic Feature Engineering\n# Label Encoding\nl = LabelEncoder()\n\n#Combining the dataset\ncombine = [train, test]\n\nfor dataset in combine:\n    \n    # Splitting Date column to make various features\n    dataset['DATE_1'] = dataset['DATE'].str.split(\"T\", expand=True)[0]\n    dataset['Year'] = dataset['DATE'].str.split(\"T\", expand=True)[0].str.split(\"-\", expand=True)[0].astype(\"float\")\n    dataset['Month'] = dataset['DATE'].str.split(\"T\", expand=True)[0].str.split(\"-\", expand=True)[1].astype(\"float\")\n    dataset['Day'] = dataset['DATE'].str.split(\"T\", expand=True)[0].str.split(\"-\", expand=True)[2].astype(\"float\")\n    dataset['HOUR'] = dataset['DATE'].str.split(\"T\", expand= True)[1].str.split(\":\", expand=True)[0].astype(\"float\")\n    dataset['MINUTE'] = dataset['DATE'].str.split(\"T\", expand= True)[1].str.split(\":\", expand=True)[1].astype(\"float\")\n    dataset['SECOND'] = dataset['DATE'].str.split(\"T\", expand= True)[1].str.split(\":\", expand=True)[2].astype(\"float\")\n    \n    # label encoding the author names ude to some repeated author\n    dataset['AUTHOR_code'] = l.fit_transform(dataset[\"AUTHOR\"])\n    \n    # Filling the missing values\n    dataset['Year'] = dataset['Year'].fillna(dataset['Year'].median())\n    dataset['Month'] = dataset['Month'].fillna(dataset['Month'].median())\n    dataset['Day'] = dataset['Day'].fillna(dataset['Day'].median())\n    dataset['HOUR'] = dataset['HOUR'].fillna(dataset['HOUR'].mean())\n    dataset['MINUTE'] = dataset['MINUTE'].fillna(dataset['MINUTE'].mean())\n    dataset['SECOND'] = dataset['SECOND'].fillna(dataset['SECOND'].mean())\n    \n    # Counting the numbers of words in comment\n    dataset['num_words_comment'] = dataset['CONTENT'].apply(lambda x:len(str(x).split())) \n\nprint(\"real_comment_words:\", train[train.CLASS ==0].num_words_comment.mean(), \", ad_comment_words:\", train[train.CLASS ==1].num_words_comment.mean())","da0658f9":"# wordclouds\nad_text = train[train.CLASS==1].CONTENT\nreal_text = train[train.CLASS ==0].CONTENT\n\n# ad text\nSTOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(ad_text)))\nplt.imshow(wc,interpolation = 'bilinear')","8517f4cc":"# real text\nSTOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(real_text)))\nplt.imshow(wc,interpolation = 'bilinear')","7e13a000":"hist_data = [train[train.CLASS ==1].num_words_comment,train[train.CLASS ==0].num_words_comment]\ngroup_labels = ['ad_comment', 'real_comment']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","e117997f":"# kde plot\nplt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train.CLASS ==0].num_words_comment, shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train[train.CLASS ==1].num_words_comment, shade=True, color=\"b\")","c4612f5d":"# Top 20 frequent words in whole comment corpus\ntrain['temp_list'] = train['CONTENT'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","2b29799a":"# Important defined functions\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\n\n\ndef comment_cleaning(text):\n    '''Make text lowercase, remove punctuation, cleaning and preprocessing the data\n    '''\n    text = str(text).lower()\n#     text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\ufeff', '', text)\n    return text\n\n# Removing the emoji\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i in range (cm.shape[0]):\n        for j in range (cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a6441afb":"#MosT common ad_comment words\ntop = Counter([item for sublist in train[train[\"CLASS\"]==1].temp_list for item in sublist])\ntemp_ad = pd.DataFrame(top.most_common(20))\ntemp_ad.columns = ['Common_words','count']\ntemp_ad.style.background_gradient(cmap='Greens')","fde4ec3a":"#MosT common real comment words\ntop = Counter([item for sublist in train[train[\"CLASS\"]==0].temp_list for item in sublist])\ntemp_real = pd.DataFrame(top.most_common(20))\ntemp_real.columns = ['Common_words','count']\ntemp_real.style.background_gradient(cmap='Greens')","bc1e5ca4":"train[\"clean_content\"] = train[\"CONTENT\"].apply(lambda x: comment_cleaning(x))\ntrain[\"clean_content\"] = train[\"clean_content\"].apply(lambda s: deEmojify(s))\nstop_words = set(stopwords.words(\"english\"))\n\n#Performing lemmetization\nlemmatizer = WordNetLemmatizer()\n#Performing stemming on the review dataframe\nps = PorterStemmer()\n\n\n#splitting and adding the stemmed words except stopwords\ncorpus = []\nfor i in range(0, len(train[\"clean_content\"])):\n    comment = re.sub('[^a-zA-Z0-9]', ' ', train[\"clean_content\"][i])\n#     comment = train[\"clean_content\"][i]\n    comment = comment.lower()\n    comment = comment.split()\n#     comment = [lemmatizer.lemmatize(word) for word in comment if not word in stop_words]\n    comment = [ps.stem(word) for word in comment if not word in stop_words]\n    comment = ' '.join(comment)\n    corpus.append(comment)  \ncorpus = pd.Series(corpus)\ntrain[\"corpus\"]= corpus\ntrain['corpus1']=train['corpus'].apply(lambda x:str(x).split())","2f0c1401":"#MosT common ad_comment words\ntop = Counter([item for sublist in train[train[\"CLASS\"]==1].corpus1 for item in sublist])\ntemp_ad = pd.DataFrame(top.most_common(20))\ntemp_ad.columns = ['Common_words','count']\ntemp_ad.style.background_gradient(cmap='Greens')","10fec16f":"#MosT common real_comment words\ntop = Counter([item for sublist in train[train[\"CLASS\"]==0].corpus1 for item in sublist])\ntemp_ad = pd.DataFrame(top.most_common(20))\ntemp_ad.columns = ['Common_words','count']\ntemp_ad.style.background_gradient(cmap='Greens')","544b87c7":"#MosT common real_comment words\ntop = Counter([item for sublist in train.corpus1 for item in sublist])\ntemp_ad = pd.DataFrame(top.most_common(20))\ntemp_ad.columns = ['Common_words','count']\ntemp_ad.style.background_gradient(cmap='Greens')","fa85b9fb":"# Analysing bigrams\ncommon_words = get_top_n_bigram(train['corpus'], 20)\ndf2 = pd.DataFrame(common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 unigrams used in articles',color='blue')","e542119e":"# Analysing trigrams\ncommon_words = get_top_n_trigram(train['corpus'], 20)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams used in articles', color='blue')","557ca788":"# Tried to remove duplicates but not giving good results\n# train.drop_duplicates(subset =\"corpus\", keep = False, inplace = True)\n# train.shape","a1893f2a":"#Creating the count of output based on date\nad=train[train[\"CLASS\"]==1].groupby(['DATE_1'])['CLASS'].count()\nad=pd.DataFrame(ad)\n\nreal=train[train[\"CLASS\"]==0].groupby(['DATE_1'])['CLASS'].count()\nreal=pd.DataFrame(real)\n\n#Plotting the time series graph\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=real.index,\n         y=real['CLASS'],\n         name='real',\n    line=dict(color='blue'),\n    opacity=0.8))\n\nfig.add_trace(go.Scatter(\n         x=ad.index,\n         y=ad['CLASS'],\n         name='ad',\n    line=dict(color='red'),\n    opacity=0.8))\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n    \nfig.update_layout(title_text='Real and Ad Comment',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()","2ddeff04":"a = train.groupby([\"AUTHOR\", \"CLASS\", \"corpus\"])[\"AUTHOR\"].count()\na.sort_values(ascending=False).head(10)","47dbd509":"# Vectorising the words present in comment corpus using tf-idf\n# Making features using unigrams, bigrams, trigrams\ntfidf_vectorizer = TfidfVectorizer(max_features=50, ngram_range=(1,3))\n# TF-IDF feature matrix\nX= tfidf_vectorizer.fit_transform(train['corpus']).toarray()\ny = train['CLASS']","6d49431b":"# joining the other features to embedded column\ncount_df = pd.DataFrame(X, columns=tfidf_vectorizer.get_feature_names())\ncount_df['Year'] = train['Year']\ncount_df['Month'] = train['Month']\ncount_df['Day'] = train['Day']\n# count_df['HOUR'] = train['HOUR']\n# count_df['MINUTE'] = train['MINUTE']\n# count_df['SECOND'] = train['SECOND']\ncount_df['num_words_comment'] = train['num_words_comment']\ncount_df['AUTHOR_code'] = train['AUTHOR_code']\n","3cf75f07":"## Divide the dataset into Train and Test\nX_train, X_valid, y_train, y_valid = train_test_split(count_df, y, test_size=0.25, random_state=0)\nX_train.shape","4ed86a82":"# Creating the pipeline to test important machine learning models\nlogreg_cv = LogisticRegression(solver='liblinear',random_state=0)\ndt_cv=DecisionTreeClassifier()\nknn_cv=KNeighborsClassifier()\nnb_cv=MultinomialNB(alpha=0.1) \nlgb_cv=LGBMClassifier()\nrf_cv = RandomForestClassifier()\nxgb_cv = XGBClassifier()\nsvm_cv = SVC()\ncb_cv = CatBoostClassifier(silent=True)\ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'Naive Bayes', 4:'LGBM', 5:'randomforest', 6:'XGBoost', 7:'SVM'\n          , 8:'catboost' \n          }\ncv_models=[logreg_cv,dt_cv,knn_cv,nb_cv, lgb_cv, rf_cv, xgb_cv, svm_cv\n          , cb_cv\n          ]\n\n#Printing the accuracy\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, count_df, y, cv=10, scoring ='accuracy').mean()))","0e1f176e":"# Hypertuning the randomforest model\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nclf = GridSearchCV(RandomForestClassifier(random_state=0), param_grid,cv=5, verbose=0)\nbest_model = clf.fit(X_train,y_train)\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_valid,y_valid))","5a78b842":"# Model Training with RandomForest\nrf = RandomForestClassifier(max_depth=8, max_features='log2', n_estimators=200,\n                       random_state=0)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_valid)\nprint('Accuracy of random forestclassifier on test set: {:.6f}'.format(rf.score(X_valid, y_valid)))\n\ncm = metrics.confusion_matrix(y_valid, y_pred)\nplot_confusion_matrix(cm, classes=['Ad','Real'])\n\nrf_roc_auc = roc_auc_score(y_valid, rf.predict(X_valid))\nprint(\"roc_auc_score:\", rf_roc_auc)\nfpr, tpr, thresholds = roc_curve(y_valid, rf.predict_proba(X_valid)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='RandomForest Classifier (area = %0.4f)' % rf_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","8418c5e0":"importance = pd.DataFrame()\nimportance[\"feature\"] = count_df.columns\nimportance[\"importance\"] = rf.feature_importances_\nimportance.sort_values(by=\"importance\", ascending = False)\nfeature_importance = importance\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('RandomForest (avg over folds)');","e0f03353":"param_grid = {'C': np.logspace(-4, 4, 50),\n             'penalty':['l1', 'l2']}\nclf = GridSearchCV(LogisticRegression( solver = 'liblinear', random_state=0), param_grid,cv=5, verbose=0)\nbest_model = clf.fit(X_train,y_train)\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_valid,y_valid))","9d5a6dda":"logreg = LogisticRegression(C=1.2067926406393288, penalty='l1', solver='liblinear', random_state=0)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_valid)\nprint('Accuracy of logistic regression classifier on test set: {:.6f}'.format(logreg.score(X_valid, y_valid)))\n\ncm = metrics.confusion_matrix(y_valid, y_pred)\nplot_confusion_matrix(cm, classes=['Ad','Real'])\n\nlogit_roc_auc = roc_auc_score(y_valid, logreg.predict(X_valid))\nprint(\"roc_auc_score:\", logit_roc_auc)\nfpr, tpr, thresholds = roc_curve(y_valid, logreg.predict_proba(X_valid)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n","4fc4ccf2":"test[\"clean_content\"] = test[\"CONTENT\"].apply(lambda x: comment_cleaning(x))\ntest[\"clean_content\"] = test[\"clean_content\"].apply(lambda s: deEmojify(s))\nstop_words = set(stopwords.words(\"english\"))\n#Performing stemming on the review dataframe\nps = PorterStemmer()\n\nlemmatizer = WordNetLemmatizer()\n\n#splitting and adding the stemmed words except stopwords\ncorpus = []\nfor i in range(0, len(test[\"clean_content\"])):\n    comment = re.sub('[^a-zA-Z0-9]', ' ', test[\"clean_content\"][i])\n    comment = comment.lower()\n    comment = comment.split()\n    comment = [ps.stem(word) for word in comment if not word in stop_words]\n    comment = ' '.join(comment)\n    corpus.append(comment)  \ncorpus = pd.Series(corpus)\ntest[\"corpus\"]= corpus\ntest['corpus1']=test['corpus'].apply(lambda x:str(x).split())\n","5884f2fc":"# test datset vectorisation same as train vectorisation\ntfidf_vectorizer = TfidfVectorizer(max_features=50, ngram_range = (1,3))\n# TF-IDF feature matrix\nt= tfidf_vectorizer.fit_transform(test['corpus']).toarray()\nt.shape","c6507984":"test_df = pd.DataFrame(t, columns=tfidf_vectorizer.get_feature_names())\ntest_df['Year'] = test['Year']\ntest_df['Month'] = test['Month']\ntest_df['Day'] = test['Day']\n# test_df['HOUR'] = test['HOUR']\n# test_df['MINUTE'] = test['MINUTE']\n# test_df['SECOND'] = test['SECOND']\ntest_df['num_words_comment'] = test['num_words_comment']\ntest_df['AUTHOR_code'] = test['AUTHOR_code']","3d6a1e56":"# RandomForest Model has been chosen as it is giving best results after hypertuning\nrf = RandomForestClassifier(max_depth=8, max_features='log2', n_estimators=200,\n                       random_state=0)\nrf.fit(count_df, y)\npred = rf.predict(test_df)\n","6c37e13e":"output=pd.DataFrame({'ID':test['ID'], 'CLASS':pred})\noutput[\"CLASS\"].value_counts()","5d4a6e04":"# Saving the output file\noutput.to_csv('output.csv', index=False)\nprint(\"Your submission was successfully saved!\")","248386c9":"# Splitting the datasets into training and validation in ratio of 80:20\nX_train, X_valid, y_train, y_valid = train_test_split(train.corpus.values, train.CLASS.values, test_size=0.2, random_state=0)\n\n# VOcabulary size from which words will be one hot coded\nvoc_size=5000\n# Sent length has been chosen keeping in mind the max length of sentence\nsent_length = 150\n\n# Using One hot encoding\nX = [one_hot(words,voc_size) for words in train.corpus.values] \ny = train.CLASS.values\nX_train_seq = [one_hot(words,voc_size) for words in X_train] \nX_valid_seq = [one_hot(words,voc_size) for words in X_valid] \n\n#zero pad the sequences\nX_pad = pad_sequences(X, maxlen=sent_length)\nX_train_pad = pad_sequences(X_train_seq, maxlen=sent_length)\nX_valid_pad = pad_sequences(X_valid_seq, maxlen=sent_length)","9271d0ce":"# # Creating the Simple RNN model\n# embedding_vector_features=50\n# model = Sequential()\n# model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n# model.add(SimpleRNN(100))\n# model.add(Dropout(0.3))\n# model.add(Dense(1, activation='sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n# model.summary()","78b3724d":"#Creating the lstm model\n# embedding_vector_features=50\n# model=Sequential()\n# model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n# model.add(Dropout(0.3))\n# model.add(LSTM(100)) #Adding 100 lstm neurons in the layer\n# model.add(Dropout(0.2))\n# model.add(Dense(1,activation='sigmoid'))\n\n# #Compiling the model\n# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# print(model.summary())","01f72e7c":"# Creating bidirectional lstm model\nembedding_vector_features=50\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Bidirectional(LSTM(100))) # Bidirectional LSTM layer\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\n# model.add(Dropout(0.1))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","2bdcc72e":"# #Creating the GRU model\n# embedding_vector_features=50\n# model=Sequential()\n# model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n# model.add(Dropout(0.3))\n# model.add(GRU(300)) #Adding 100 lstm neurons in the layer\n# model.add(Dropout(0.3))\n# model.add(Dense(1,activation='sigmoid'))\n\n# #Compiling the model\n# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# print(model.summary())","9dc4d765":"# Fitting with 20 epochs and 64 batch size\n# applying early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=3)\nmodel.fit(X_train_pad,y_train,validation_data=(X_valid_pad,y_valid),epochs=20,batch_size=64, callbacks=[es])","fe0ea30a":"# Predicting from valid data\ny_pred=model.predict_classes(X_valid_pad)\n\n#Creating confusion matrix\ncm = metrics.confusion_matrix(y_valid, y_pred)\nplot_confusion_matrix(cm,classes=['Ad','Real'])\n\n#Checking for roc score\nprint(\"accuracy score\", accuracy_score(y_valid, y_pred))\nprint(\"roc_score\", roc_auc_score(y_valid, y_pred))","a2d2e40a":"# One hot coding for testing datset similar to traing datsets\ntest_tok = [one_hot(words,voc_size) for words in test.corpus] \n\n#zero pad the sequences\ntest_pad = pad_sequences(test_tok, maxlen=sent_length)","04e0fd13":"model.fit(X_pad,y)\npred = model.predict_classes(test_pad)","f3204432":"pred= pred.flatten()\noutput=pd.DataFrame({'ID':test['ID'], 'CLASS':pred})\noutput[\"CLASS\"].value_counts()","6d291704":"# Saving output\noutput.to_csv('output.csv', index=False)\nprint(\"Your submission was successfully saved!\")","96277370":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","88ccd2dd":"# LOADING THE DATA\ntrain = pd.read_csv(r'..\/input\/zs-challenge-data\/train.csv')\ntest = pd.read_csv(r'..\/input\/zs-challenge-data\/test.csv')\n","281d7ef2":"# Dividing the training the datset into train and valid data\ntrain1 = train.iloc[:850,:]\nvalid = train.iloc[850:,:].reset_index()","da8fd834":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts))):\n        text_chunk = texts[i]\n        encs = tokenizer.encode(text_chunk, padding='max_length', truncation=True, max_length = maxlen)\n        all_ids.extend([encs])   \n    return np.array(all_ids)","aac2fb3b":"#IMP DATA FOR CONFIG\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 400\nBATCH_SIZE = 16* strategy.num_replicas_in_sync\nMAX_LEN = 200","891bfd91":"# loading the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","37e15f9c":"# Tokenising the datasets\nx_train = fast_encode(train1.CONTENT.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.CONTENT.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(test.CONTENT.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx = fast_encode(train.CONTENT.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train1.CLASS.values\ny_valid = valid.CLASS.values\ny = train.CLASS.values","94a685f6":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)\n\ndataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x, y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n","933d60a3":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","9b6c1f32":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","a4f76b48":"# Traing the data \nn_steps = x_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","5fa1275a":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","644e06ca":"# Predicting the valid data\ny_pred=model.predict(x_valid, verbose =1)\ny_pred = y_pred.round().astype(int)\n\n#Creating confusion matrix\ncm = metrics.confusion_matrix(y_valid, y_pred)\nplot_confusion_matrix(cm,classes=['Ad','Real'])\n\n#Checking for roc score\nprint(\"accuracy score\", accuracy_score(y_valid, y_pred))\nprint(\"roc_score\", roc_auc_score(y_valid, y_pred))","a788bead":"n_steps = x.shape[0] \/\/ BATCH_SIZE\nmodel.fit(dataset,steps_per_epoch=n_steps,epochs=EPOCHS)\npred=model.predict(x_test)\npred = pred.round().astype(int)\n","394fcb58":"pred= pred.flatten()\noutput=pd.DataFrame({'ID':test['ID'], 'CLASS':pred})\noutput[\"CLASS\"].value_counts()","ee6337c9":"# Saving the output file\noutput.to_csv('output.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ee52d811":"# BERT Model","4220ef74":"# Modelling","34fcd5c2":"## Distribution of number of words","1490b658":"# Deep Learning Model","7f5b093c":"## Visualising the effect of TIME on comments (To check if Time Series)","a5812ae7":"## Top frequency words in comment corpus","b63d4f54":"# Classical ML Prediction","41928295":"# EDA","e8bb50a3":"# ADS Classification","651907d2":"## Visualisation: Wordclouds","2b783d98":"### Analysing Feature Importance","64c8750d":"## Top frequency words after pre-process the comments text","7571bc22":"### Analysing authors corelation with target classes","d3f8181b":"### Wordcloud for Advertisement Comments","6e4fae5b":"### Basic Feature Engineering","6b76f822":"### Wordcloud for real comments","e37ebdd6":"### LSTM Bi-directional model perform best among Simple RNN, LSTM, GRU and bidirectional LSTM","947c089f":"### Analysing bigrams and trigrams"}}