{"cell_type":{"21c8d344":"code","741b153b":"code","a80be33b":"code","cbe79669":"code","a29d11d5":"code","bbe8b723":"code","4e9a08bd":"code","71bcfb94":"code","e02eefc4":"code","0f22512c":"code","a4e5c184":"code","6e8ecc1f":"code","e2dec94c":"code","8f115414":"code","7b006ad5":"code","903dd156":"code","8053e2c4":"code","c1a2d7fd":"code","3be66fc1":"code","a6766a0e":"code","870e3677":"code","a6357c35":"code","9e232994":"code","7b795bc2":"code","8882f6cc":"code","4dd5ba71":"code","57888461":"code","0b2c3e22":"code","a7b2a561":"code","16caccb0":"code","e1a57f33":"code","11499267":"markdown","3bc4f109":"markdown","77b00140":"markdown","3ae9d07c":"markdown","e2b41950":"markdown","fbbf1fd6":"markdown","4d22414f":"markdown","0a2ebaf7":"markdown","2b1eaeeb":"markdown","5b622693":"markdown","f409888a":"markdown","0104f8d5":"markdown","16a4feda":"markdown","6652d822":"markdown","8fcb19ee":"markdown","5b0b3343":"markdown","20ce1cbe":"markdown","f62fc45b":"markdown","db514607":"markdown","5cd09f49":"markdown","be62f5da":"markdown"},"source":{"21c8d344":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","741b153b":"df=pd.read_csv('..\/input\/confused-eeg\/EEG_data.csv')\ndata = pd.read_csv('..\/input\/confused-eeg\/demographic_info.csv')","a80be33b":"data = data.rename(columns = {'subject ID': 'SubjectID',' gender':'gender',' age':'age',' ethnicity':'ethnicity'})\ndf = df.merge(data,how = 'inner',on = 'SubjectID')\ndf.head()","cbe79669":"df.shape","a29d11d5":"df.info()","bbe8b723":"df.columns","4e9a08bd":"df['gender']=df['gender'].replace({'M':1,'F':0})\ndf['ethnicity']=df['ethnicity'].replace({'Han Chinese':0,'Bengali':1,'English':2})","71bcfb94":"df.head()","e02eefc4":"df['VideoID'].value_counts()","0f22512c":"df['predefinedlabel'].value_counts()","a4e5c184":"for col in df.columns:\n    if(df[col].isnull().sum()>0):\n        print(col)","6e8ecc1f":"df.describe()","e2dec94c":"sns.set_style('darkgrid')\nsns.displot(data=df,x='Mediation',kde=True,aspect=16\/7)","8f115414":"fig,ax=plt.subplots(figsize=(7,7))\nsns.scatterplot(data=df,x='Mediation',y='Attention',hue='user-definedlabeln')","7b006ad5":"fig,ax=plt.subplots(figsize=(7,7))\nsns.scatterplot(data=df,x='Mediation',y='Raw',hue='user-definedlabeln')","903dd156":"fig,ax=plt.subplots(figsize=(7,7))\nsns.scatterplot(data=df,x='Mediation',y='Theta',hue='user-definedlabeln')","8053e2c4":"fig,ax=plt.subplots(figsize=(7,7))\nsns.scatterplot(data=df,x='Mediation',y='Alpha1',hue='user-definedlabeln')","c1a2d7fd":"fig,ax=plt.subplots(figsize=(7,7))\nsns.scatterplot(data=df,x='Mediation',y='Gamma1',hue='user-definedlabeln')","3be66fc1":"from sklearn.feature_selection import mutual_info_classif","a6766a0e":"y=pd.get_dummies(df['user-definedlabeln'])\nmi_score=mutual_info_classif(df.drop('user-definedlabeln',axis=1),df['user-definedlabeln'])\nmi_score=pd.Series(mi_score,index=df.drop('user-definedlabeln',axis=1).columns)\nmi_score=(mi_score*100).sort_values(ascending=False)\nmi_score","870e3677":"mi_score.head(14).index","a6357c35":"top_fea=['VideoID', 'Attention', 'Alpha2', 'Delta', 'Gamma1', 'Theta', 'Beta1',\n       'Alpha1', 'Mediation', 'Gamma2', 'SubjectID', 'Beta2', 'Raw', 'age']","9e232994":"from sklearn.preprocessing import StandardScaler\ndf_sc=StandardScaler().fit_transform(df[top_fea])","7b795bc2":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import callbacks,layers","8882f6cc":"from sklearn.model_selection import train_test_split\nXtr,xte,Ytr,yte=train_test_split(df_sc,y,random_state=108,test_size=0.27)\nxtr,xval,ytr,yval=train_test_split(Xtr,Ytr,random_state=108,test_size=0.27)","4dd5ba71":"# Model-Building step, stacking the hidden layers\nmodel=keras.Sequential([\n    layers.Dense(64,input_shape=(14,),activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.27),\n    layers.Dense(124,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(248,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.32),   \n    layers.Dense(512,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.27),   \n    layers.Dense(664,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.32),\n    layers.Dense(264,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.27),\n    layers.Dense(124,activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(2,activation='sigmoid')\n])\n#Compiling the model with Adamax Optimizer\nmodel.compile(optimizer='adamax',loss='binary_crossentropy',metrics='accuracy')","57888461":"#Creating the callback feature to stop the training in-Between, in case of no improvement\ncall=callbacks.EarlyStopping(patience=20,min_delta=0.0001,restore_best_weights=True)\n#Fitting the model\nhistory=model.fit(xtr,ytr,validation_data=(xval,yval),batch_size=28,epochs=150,callbacks=[call])","0b2c3e22":"model.evaluate(xte,yte)","a7b2a561":"training=pd.DataFrame(history.history)","16caccb0":"training.loc[:,['loss','val_loss']].plot()","e1a57f33":"training.loc[:,['accuracy','val_accuracy']].plot()","11499267":"#### Checking for *Imbalanced-dataset*","3bc4f109":"#### Most of the subjects were doing meditation before experiment!","77b00140":"# Importing libraries to build **Neural-Network**","3ae9d07c":"# Feel free to *comment* or **Praise** on the method to reach the *Solution* of this problem.\n# As it is defined before every step!","e2b41950":"### Separating-out feature-set and `Target-column` ","fbbf1fd6":"# Scaling our *Feature*-set","4d22414f":"# More than 99% Accuracy achieved!!!","0a2ebaf7":"### Plotting the history of Neural-Network","2b1eaeeb":"### Mutual-info gives the score to each **Feature** which describes its *Relationship* with `Target` variable","5b622693":"# Time for some *EDA* to **Identify** the Feature-columns that are aligned with our *Target*-class","f409888a":"## 1. Building our model\n## 2. Compiling the model\n## 3. Fitting the model","0104f8d5":"## Time for some **Evaluations**","16a4feda":"# Feel-Free to give some advice or shower-support!","6652d822":"### *Reading* the datasets","8fcb19ee":"#### *Converting* the **Categorical** columns to numerical ones","5b0b3343":"# Importing library to perform\n#  *Feature*-**Selection** ","20ce1cbe":"### *Splitting* the dataset into:\n1. Training_Features, Training_Target\n2. Testing_Features, Testing_Target\n3. Validation_Features, Validation_Target","f62fc45b":"### Selecting top-14 features","db514607":"### *Exploring* the dataset is most crucial part for reaching till the solution.\n1. See the shape\n2. Find the missing columns\n3. Convert the **String** data-type columns to *Numerical* ones\n4. Check for imbalanced data-sets","5cd09f49":"## No *direct* relationship is identified between Features and Target-variable","be62f5da":"### Merging the datasets on **Subject-ID**\n#### *Few* of the columns-names had some extra space before them"}}