{"cell_type":{"303036ad":"code","5d77fe64":"code","e558cb49":"code","47c94b41":"code","78c2f69e":"code","e91e0040":"code","ad1b853d":"code","1e2919bc":"code","e0bf2f96":"code","e5020c92":"code","0ad109a3":"code","0ba36b22":"code","97698f06":"code","a15a534f":"code","aafdc98d":"code","3a8685e3":"code","1a72f987":"code","36783369":"code","30a6d1d2":"code","043dc916":"code","a92e9488":"code","31b45719":"code","ad42c46f":"code","f8c96269":"code","ac50d6bc":"code","cb7956e5":"code","5a814b20":"code","2e75fb00":"code","8733a5d0":"code","ef0265a5":"code","ec2c3a50":"code","7856ac5a":"code","979d20cb":"code","d3022efa":"code","f5fed476":"code","72a33f8f":"code","e6a69ee9":"code","48647d79":"code","da8c6223":"code","fe36fde1":"code","3d08ca14":"code","ed6c2ddb":"code","a9c49ff2":"code","e4eeb992":"code","3f33350e":"code","6f7d3ca5":"code","366324b9":"code","00a2cf87":"code","931811a0":"code","0d16c4ec":"code","178b57d6":"code","d76cbacf":"markdown","639d4890":"markdown","3bad47e3":"markdown","b34b170f":"markdown","d8a7dcaf":"markdown","5f6b49ec":"markdown","0fff979e":"markdown","d92c8cf4":"markdown","8c9159cc":"markdown","b617efdf":"markdown","c8302977":"markdown","ae32f658":"markdown","66ff066a":"markdown","bdd08ace":"markdown","5cd0bbf7":"markdown","0dda3f48":"markdown","56a67903":"markdown","7145b1d8":"markdown","ec8f1035":"markdown","ff38a4dc":"markdown","e62d469d":"markdown","d047b56b":"markdown","002e9c33":"markdown","117b556e":"markdown","628e9826":"markdown","a2fe7ed6":"markdown","4a6af9d5":"markdown","ee0ddfc0":"markdown","a3c0703d":"markdown","905bf3ff":"markdown","55ea4ad7":"markdown","7cc51d9b":"markdown","b3bb0054":"markdown","089fbe4d":"markdown","8be1bd33":"markdown","0990feb5":"markdown"},"source":{"303036ad":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport sklearn.datasets\nimport sklearn.feature_selection\nimport sklearn.metrics\nimport sklearn.linear_model\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nplt.rcParams.update({'font.size': 14})","5d77fe64":"# loading our dataset\ndata = sklearn.datasets.load_boston()\nprint(type(data))\nprint(data.keys()) # check attributes inside bunch","e558cb49":"# checking feature names\nprint(data['feature_names'])\n# saving the number of features as nf for future reference\nnf = len(data['feature_names'])","47c94b41":"# converting sklearn bunch to dataframe\ndf = pd.DataFrame(data=data['data'], columns=data['feature_names'])\ndf['target'] = data['target']\ndf.head()","78c2f69e":"# first, we check for null values (and we don't have any!)\ndf.isna().sum()","e91e0040":"# second, let look the descriptive statistics for each column\ndesc = df.describe()\ndesc","ad1b853d":"df.mode()","1e2919bc":"df.target.value_counts()","e0bf2f96":"df = df[df.target < 50]\nprint('No. of samples left:', len(df))","e5020c92":"hist = df.hist(figsize=(20,20))","0ad109a3":"corr = stats.spearmanr(df)","0ba36b22":"p_sig = []\nfor i in range(nf+1):\n    for j in range(nf+1):\n        p = corr.pvalue[i,j]\n        if p < .0001:\n            sig = '****'\n        elif p < .001:\n            sig = '***'\n        elif p < .01:\n            sig = '**'\n        elif p < .05:\n            sig = '*'\n        else:\n            sig = 'ns'\n        p_sig.append(sig)","97698f06":"# plot heatmap of correlation coefficients\nfig, ax = plt.subplots(figsize=(20,20))\nim = ax.imshow(corr.correlation, cmap='twilight_shifted')\n\nax.set_xticks(np.arange(nf+1))\nax.set_yticks(np.arange(nf+1))\n# ... and label them with the respective list entries\nax.set_xticklabels(df.columns)\nax.set_yticklabels(df.columns)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha='right',rotation_mode='anchor')\n\n# annotate correlations on top of figure\nidx = 0\nfor i in range(nf+1):\n    for j in range(nf+1):\n        if i!=j: # omit diagonal\n            text = ax.text(j, i, str(corr.correlation[i, j].round(3))+''+str(p_sig[idx]), \n                           ha='center', va='center', color='w') #rounding to 3 decimals\n        idx += 1\nax.set_title('Spearman\\' correlation')\nfig.tight_layout()\nfig.colorbar(im)\nplt.show()","a15a534f":"# casting the data and target into arrays\nX = df.values[:,:13]\ny = df.target.values\nprint(X.shape, y.shape)","aafdc98d":"f_regr = sklearn.feature_selection.f_regression(X, y, center=True)\nf_regr_df = pd.DataFrame()\nf_regr_df['Feature'] = data['feature_names']\nf_regr_df['F score'] = f_regr[0]\nf_regr_df['p value'] = f_regr[1]\nf_regr_df.sort_values(by='p value')","3a8685e3":"df = df.drop('CHAS', axis=1)","1a72f987":"# casting the data and target into arrays\nX = df.values[:,:12]\ny = df.target.values\nprint(X.shape, y.shape)","36783369":"lr = sklearn.linear_model.LinearRegression(normalize=True)\nlm_all = lr.fit(X, y)\nprint('Percentage of variance explained by all variables except CHAS:', (100.*lm_all.score(X,y)).round(1), '%')\nidx = np.where(df.columns=='LSTAT')[0][0]\nlm_LSTAT = lr.fit(X[:,idx].reshape(-1,1), y)\nprint('Percentage of variance explained by LSTAT alone:', (100.*lm_LSTAT.score(X[:,idx].reshape(-1,1), y)).round(1), '%')","30a6d1d2":"pt = PowerTransformer()\npt_target = PowerTransformer()\npt.fit(X)\npt_X = pt.transform(X)\npt_target.fit(y.reshape(-1, 1))\npt_y = pt_target.transform(y.reshape(-1, 1))","043dc916":"# to view the results of our normalization\ndf_pt = pd.DataFrame(data=pt_X, columns=df.columns.values[:-1])\ndf_pt['target'] = pt_y\nhist = df_pt.hist(figsize=(20,20))","a92e9488":"lr = sklearn.linear_model.LinearRegression(normalize=False)\nlm_all = lr.fit(pt_X, pt_y)\nprint('Percentage of variance explained by all normalized variables except CHAS:', (100.*lm_all.score(pt_X,pt_y)).round(1), '%')\nidx = np.where(df.columns=='LSTAT')[0][0]\nlm_LSTAT = lr.fit(pt_X[:,idx].reshape(-1,1), pt_y)\nprint('Percentage of variance explained by LSTAT alone:', (100.*lm_LSTAT.score(pt_X[:,idx].reshape(-1,1), pt_y).round(1)), '%')","31b45719":"coll = np.where(np.logical_and(df_pt.corr().abs() > 0.8, df_pt.corr().abs() < 1.0))","ad42c46f":"# print all collinear pairs \nfor i in range(len(coll[0])):\n    if coll[0][i] < 12 and coll[1][i] < 12:\n        print(df.columns[coll[0][i]], df.columns[coll[1][i]])","f8c96269":"df_pt.corr()['target']","ac50d6bc":"df_coll = df_pt.drop(['NOX', 'RAD', 'TAX', 'target'], axis=1)","cb7956e5":"pt_X = df_coll.values","5a814b20":"lr = sklearn.linear_model.LinearRegression(normalize=False)\nlm = lr.fit(pt_X, pt_y)\nprint('Percentage of variance explained by 9 variables:', (100.*lm.score(pt_X,pt_y)).round(1), '%')","2e75fb00":"seq = np.arange(490)\nnp.random.seed(1)\nnp.random.shuffle(seq)","8733a5d0":"train_n = int(490*.8)\ntrain_idx = seq[:train_n]\ntest_idx = seq[train_n:]\nprint('No. of training samples:' , len(train_idx))\nprint('No. of testing samples:' , len(test_idx))","ef0265a5":"train_X = pt_X[train_idx]\ntrain_y = pt_y[train_idx]\n\ntest_X = pt_X[test_idx]\ntest_y = pt_y[test_idx]\n\ny_true = y[test_idx]\n\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape, y_true.shape)","ec2c3a50":"lr = sklearn.linear_model.LinearRegression(normalize=False)\nlm = lr.fit(train_X, train_y)\ny_pred = lm.predict(test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of baseline model:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","7856ac5a":"# LSTAT only\nlm = lr.fit(train_X[:,-1].reshape(-1, 1), train_y)\ny_pred = lm.predict(test_X[:,-1].reshape(-1, 1))\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of LSTAT only model:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","979d20cb":"ridge = sklearn.linear_model.RidgeCV()\nlm = ridge.fit(train_X, train_y)\ny_pred = lm.predict(test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of RidgeCV model:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","d3022efa":"lasso = sklearn.linear_model.LassoCV()\nlm = lasso.fit(train_X, train_y.ravel())\ny_pred = lm.predict(test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of LassoCV model:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","f5fed476":"df_lasso = pd.DataFrame()\ndf_lasso['Features'] = df_coll.columns\ndf_lasso['Coefficient'] = lm.coef_\ndf_lasso","72a33f8f":"model = sklearn.feature_selection.SelectFromModel(lm, prefit=True, threshold=0.1)\ntrain_X_new = model.transform(train_X)\ntest_X_new = model.transform(test_X)\nprint('Features selected:', df_coll.columns[model.get_support()].values)","e6a69ee9":"lm = lasso.fit(train_X_new, train_y.ravel())\ny_pred = lm.predict(test_X_new)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of LassoCV model w\/ L1-based feature selection:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","48647d79":"pt_X = df_pt.values[:,:-1]","da8c6223":"train_X_full = pt_X[train_idx]\n\ntest_X_full = pt_X[test_idx]\n\nprint(train_X_full.shape, train_y.shape, test_X_full.shape, test_y.shape)","fe36fde1":"estimator = sklearn.linear_model.LinearRegression(normalize=False)\nselector = sklearn.feature_selection.RFE(estimator, n_features_to_select=4, step=1)\nselector = selector.fit(train_X_full, train_y.ravel())\nprint('Features selected:', df_pt.columns[:-1][selector.support_].values)\n\nrfe_X = selector.transform(train_X_full)\nrfe_test_X = selector.transform(test_X_full)","3d08ca14":"lr = sklearn.linear_model.LinearRegression(normalize=False)\nlm = lr.fit(rfe_X, train_y)\ny_pred = lm.predict(rfe_test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of RFE model:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","ed6c2ddb":"pca = PCA()\nX_pca = pca.fit_transform(pt_X)","a9c49ff2":"total_ratio = 0\nfor i in range(12):\n    ratio = pca.explained_variance_ratio_[i]\n    total_ratio += ratio\n    if total_ratio > 0.8:\n        break\nprint('The first', i+1, 'components explain', (100*total_ratio).round(1), '% of variance')","e4eeb992":"fig, axs = plt.subplots(2, 2, figsize=(15,15))\n\naxs[0, 0].bar(np.arange(12), pca.components_[0])\naxs[0, 0].set_title('PC1')\naxs[0, 0].set_xticks(np.arange(12))\naxs[0, 0].set_xticklabels(df_pt.columns[:-1].values, rotation=45, ha='right', rotation_mode='anchor')\naxs[0, 1].bar(np.arange(12), pca.components_[1])\naxs[0, 1].set_title('PC2')\naxs[0, 1].set_xticks(np.arange(12))\naxs[0, 1].set_xticklabels(df_pt.columns[:-1].values, rotation=45, ha='right', rotation_mode='anchor')\naxs[1, 0].bar(np.arange(12), pca.components_[2])\naxs[1, 0].set_title('PC3')\naxs[1, 0].set_xticks(np.arange(12))\naxs[1, 0].set_xticklabels(df_pt.columns[:-1].values, rotation=45, ha='right', rotation_mode='anchor')\naxs[1, 1].bar(np.arange(12), pca.components_[3])\naxs[1, 1].set_title('PC4')\naxs[1, 1].set_xticks(np.arange(12))\naxs[1, 1].set_xticklabels(df_pt.columns[:-1].values, rotation=45, ha='right', rotation_mode='anchor')\n\nplt.show()","3f33350e":"for i in range(4):\n    print(df_pt.columns[:-1].values[np.where(np.abs(pca.components_[i])>0.5)])","6f7d3ca5":"pca = PCA(4)\nX_pca = pca.fit_transform(train_X_full)\ntest_X_pca = pca.transform(test_X_full)","366324b9":"lr = sklearn.linear_model.LinearRegression(normalize=False)\nlm = lr.fit(X_pca, train_y)\ny_pred = lm.predict(test_X_pca)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of Linear Regression with 4PCs:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","00a2cf87":"rf = RandomForestRegressor(random_state=1)\n\nrandom_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n}\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_X, train_y.ravel())\nrf_random.best_params_","931811a0":"y_pred = rf_random.predict(test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native spacespace\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of Random Forest with 9 variables:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","0d16c4ec":"svr = SVR()\n\nrandom_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto']\n}\n\nsvr_random = RandomizedSearchCV(estimator = svr, param_distributions = random_grid, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n# Fit the random search model\nsvr_random.fit(train_X, train_y.ravel())\nsvr_random.best_params_","178b57d6":"y_pred = rf_random.predict(test_X)\ny_pred = pt_target.inverse_transform(y_pred.reshape(-1, 1)) # converting back to native space\nrmse = sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\nprint('RMSE of SVR (RBF kernel) with 9 variables:', rmse)\nprint('RMSE as a % of mean house price:', (100.*rmse\/np.mean(y_true)).round(1))","d76cbacf":"## 3.5 Nonlinear method (2): Support Vector Regression w\/ RBF kernel","639d4890":"For more clarity, we are going to use conventional notation to highlight statistical significance, i.e.\n\np value | symbol\n--- | ---\n\\>.05 | ns\n<.05 | *\n<.01 | **\n<.001 | ***\n<.0001 | ****","3bad47e3":"For easy reference, here is the list of 13 variables and the target with descriptions from the author's website:\n\n**Variable** | **Description**\n--- | ---\n**CRIM** | per capita crime rate by town,\n**ZN** | proportion of residential land zoned for lots over 25,000 sq.ft.\n**INDUS** | proportion of non-retail business acres per town\n**CHAS** | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n**NOX** | nitric oxides concentration (parts per 10 million)\n**RM** | average number of rooms per dwelling\n**AGE** | proportion of owner-occupied units built prior to 1940\n**DIS** | weighted distances to five Boston employment centres\n**RAD** | index of accessibility to radial highways\n**TAX** | full-value property-tax rate per \\$10,000\n**PTRATIO** | pupil-teacher ratio by town\n**B** | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n**LSTAT** | \\% lower status of the population\n**MEDV** | (Our target, house price) Median value of owner-occupied homes in $1000's","b34b170f":"After power transformation, variance explained by both models increases, but the model of LSTAT alone increases drastically from 57.7% to a whooping **70.0%**!","d8a7dcaf":"# Background\n\nDuring the recruiment process of a Data Scientist position, I was asked to use hypothesis testing to give some insights into the Boston house-prices dataset: http:\/\/lib.stat.cmu.edu\/datasets\/boston","5f6b49ec":"Using Random Forest gives us a big improvement in RMSE, dropping it below 3.5!","0fff979e":"From the descriptive statistics, we can see that ZN and INDUS are both predominantly zero. CRIM also has an average value equals to its upper quartile. So we can already expect that they have a positively skewed distributions, as evident in the histograms plotted below.","d92c8cf4":"The RMSE has dropped below 4. However, it is worth noting that using PCA for feature extraction has the downside of losing the interpretability of features.","8c9159cc":"Unfortunately, the RMSE has increased.","b617efdf":"Observations:\n\n* PC1: coefficients quite evenly distributed\n* PC2: coefficients with absolute value > 0.5 in RM\n* PC3: no coefficients with absolute value > 0.5\n* PC4: coefficients with absolute value > 0.5 in PTRATIO and B\n\nThe meanings of the PCs are not very obvious from the coefficient distributions. But PC2 seems to be related to the number of rooms, while PC4 seems to be related to education quality.","c8302977":"A linear model with LSTAT as the only variable can predict up to 57.7% variance in house prices!","ae32f658":"The color map was chosen such that darker colors indicate higher (both positive and negative) correlations. We focus on the last row (or column) for the correlation between our target and each variable. Almost every variable has a very significant correlation with our target except for CHAS.\n\nRegarding the five speculations from above, they were all correct except for that regarding the proportion of blacks.","66ff066a":"This is the best RMSE so far. The 4 features chosen by RFE is slightly different from L1-based selection, with 'TAX' replacing 'CRIM'. In our collinearity analysis, we knew that these two are collinear, and their correlation magnitudes are quite similar. It's not surprising that RFE chose 'TAX' instead of 'CRIM'.","bdd08ace":"### Performance metric\nThe performance metric we have chosen is RMSE (root mean squared error), which is easy to interpret because it has the same unit as the output.\n\nWe then check again for the baseline performance from our linear model:","5cd0bbf7":"* CRIM is collinear with NOX, RAD and TAX. Since CRIM has the highest correlation with house price, we keep CRIM and discard the rest.\n* Now that NOX is discarded, we should keep DIS as a variable even though it is collinear with NOX.","0dda3f48":"### 3.1.2 Lasso regularization\nTo test the hypothesis that we actually need less variables than 9, we also tried to apply Lasso regularization to get sparse coefficients. We will use the default 5-fold cross-validation.","56a67903":"## 3.1 Regularization\n### 3.1.1 Ridge regularization\nTo test the effect of regularization on the model, we use a simple Ridge regression model with built-in Leave-One-Out Cross-Validation.","7145b1d8":"## 2.4 Removing collinear variables\nNow that our variables are power transformed, we can try to apply Pearson's correlation analysis. To remove collinear variables, we set a threshold such that variables are identified as 'collinear' if Pearson's r (absolute value) is higher than 0.8.","ec8f1035":"The histograms show that not all transformations were successful. But we can see that INDUS, NOX, DIS, LSTAT, and our target have become much more normalized. We then re-apply our models to the normalized variables.","ff38a4dc":"### 3.3.2 Using these features in current models","e62d469d":"## 3.4 Nonlinear method (1): Random Forest (with CV)\nPerform random forest on the original features (top 9).","d047b56b":"## 3.3 Feature extraction: PCA\n### 3.3.1 Examining the components","002e9c33":"All features except CHAS have a significant effect in the linear model, with LSTAT having the highest effect. We can first discard CHAS, then compare the variance explained by LSTAT alone vs by all variables.\n\nNote that the above table is equivalent to performing an ANOVA feature selection process.","117b556e":"The performance of SVR is comparable to Random Forest.","628e9826":"### 3.2.2 Recursive feature elimination\nWith RFE, we are going to go back to the original dataset with 12 features (everything except CHAS) to start over. Since, it's only fair for this method to access (almost) the full set of features.","a2fe7ed6":"Both RidgeCV and LassoCV slightly improves performance from the baseline model! Results from Lasso regularization seems to suggest that ZN, AGE, and B are variables that can be possibly eliminated.","4a6af9d5":"## 2.2 Testing the individual effect of each variable\nTo test the individual effect of each variable, we use a linear model with each variable as a regressor.","ee0ddfc0":"Here we can see that the DIS, LSTAT, and target distrbution is positively skewed. AGE and B are also evidently negatively skewed.\n\nAlso, since CHAS is a dummy variable, the only values allowed were 0 and 1.","a3c0703d":"## 2.3 Normalizing the variables\nTo make our variables follow a normal distribution (since this is an assumption for a lot of models), we can use Yeo-Johnson transformation to force the data to become more 'Gaussian-like'.","905bf3ff":"# Part 1: Data Exploration and Visualization\nBefore any hypothesis testing or model building, we should first take a look at our data types, and plot some figures to observe any obvious patterns.","55ea4ad7":"## 3.2 (More) Feature selection\nPreviously we have already done a univariate ANOVA feature selection. Here we explore two more feature selection methods.\n\n### 3.2.1 L1-based feature selection\nFrom the Lasso model above, we can set a threshold such that any coefficients below an absolute value of 0.1 will be filtered.","7cc51d9b":"# Closing Remarks: some insights into Boston house prices\nHere is the summary table of RMSE values from different models:\n\nModel | # Features | RMSE | % of mean price\n--- | --- | --- | ---\nLinear Regression | top 9 | 4.164 | 20.0\nLinear Regression | LSTAT only | 4.821 | 23.2\nRidge Regression | top 9 | 4.160 | 20.0\nLasso Regression | top 9 | 4.150 | 19.9\nLasso Regression | top 4 w\/ L1 | 4.203 | 20.2\nLinear Regression | top 4 w\/ RFE | 4.064 | 19.5\nLinear Regression | top 4 PCs | 3.961 | 19.0\nRandom Forest | top 9 | 3.380 | 16.2\nSVR w\/ RBF kernel | top 9 | 3.380 | 16.2\n\nFrom the analysis done above, we can conclude the following:\n\n## Univariate insights:\n* Whether the Charles River flow through a district or not has a very low impact on house prices.\n* The higher the crime rate, the lower the house price.\n* The more industrialized, the lower the house price.\n* The higher the number of rooms, the higher the house price.\n* The older the house, the lower the house price.\n* The lower the pupil-teacher ratio, the higher the house price.\n* The higher the % lower status, the lower the house price. And this has the largest impace on house prices! In fact, this could explain over 70% of variance in house price under some transformation!\n\n## Multivariate insights:\n* Crime rate is collinear with nitro oxides concentration, accessibility to highways, and property-tax rate.\n* A baseline linear model using 9 out of the 13 variables can explain up to 76.3% of the variance in house price.\n* With L1 regularization, only 4 features, CRIM, RM, PTRATIO, and LSTAT are needed to achieve a good RMSE. RFE gave a slightly different set of 4 features, RM, TAX, PTRATIO, and LSTAT.\n\n## Predictive models:\n* Linear regression with PCA as feature extractio method gives the best performance among linear methods so far.\n* Random Forest and SVR w\/ RBF kernal have comparable performances, giving an RMSE as low as 3.4. This is equivalent to an error as small as 16.2% of the mean house price!\n\n## Things to note:\n* This dataset has a small sample size. Hence, a lot of deep learning methods might not be applicable or we would risk overfitting our data.\n* Ideally, we would want to iterate over the different feature selection methods for each model. This is not done here due to time constraint, so a lot of the models just used one feature selection method based on intuition.","b3bb0054":"It seems like we don't have much data cleaning to do, none of the values is null and none of the features really show abnormal minimum or maximum values.\n\nHowever, we see that 16 out of 506 samples have the same house price of 50.0 (in \\$1000's), which seems to be a dummy value for whenever data is not available. Hence we are going to screen those samples out.","089fbe4d":"# Part 2: Hypotheses Testing + Initial Model\n## 2.1 Significance of correlation coefficients\nWe first use some common sense to predict the correlation among variables and target:\n\n1. Crime rate (CRIM) should be negatively correlated with house prices.\n2. Non-retail business (INDUS) and nitric oxides concentration (NOX) both suggest an industrial-heavy district. Hence, they should be positively correlated to each other but both negatively correlated to house prices.\n3. Both number of rooms (RM) and the age of the house (AGE) concern the living quality. We can guess that RM should be positively correlated with house prices while AGE would be negatively correlated with house prices.\n4. A lower pupil-teacher ratio (PTRATIO) suggests that it is a better school district, so it should negatively correlate with house prices.\n5. A higher proportion of blacks (B) and and a higher % lower status (LSTAT) suggests that the community is relatively underprivileged. These two variables should correlate positively with each other but have a negative correlation with house prices.\n\nSince the distributions of variables were not normal, I chose to do the correlation analysis using Spearman's rank correlation coefficient instead.\n\n#### Spearman's correlation","8be1bd33":"Indeed, dropping 3 out of 13 variables only slightly decrease the % variance explained from 77.9 to 76.3.\n\nHence our baseline linear model is\n\nhouse price ~ CRIM + ZN + INDUS + RM + AGE + DIS + PTRATIO + B + LSTAT","0990feb5":"# Part 3: More Modeling\n## 3.0 Preliminary: dividing into training, validation, and testing sets\nTo fairly assess the performances of different models, we are going to split the 490 samples into 80% training and 20% testing. We will not have an independent validation set but will use cross-validation whenever applicable. Cross-validation is chosen over independent validation because of the small sample size."}}