{"cell_type":{"fae1ef21":"code","5e67a985":"code","d2e13df0":"code","505a415e":"code","ae833ae5":"code","de3dc57a":"code","00469393":"code","bb52a6dc":"code","d3b9f42a":"code","c94b2bd7":"code","12ee07e9":"code","59de9158":"code","e42a815b":"code","a63a6e34":"code","69ccd5b6":"code","23f72c27":"code","48359846":"code","ebf54823":"code","2b8090d8":"code","4c9d4ea7":"code","d8c538ac":"code","d3f6f60f":"code","be03769b":"code","f9299f36":"code","c88817b0":"code","b0bf92a1":"code","8748309a":"code","9c21f359":"code","5d777288":"code","1516b43e":"code","bba34f3e":"code","e3752672":"code","4fc4ceaf":"code","fa29c84b":"code","a1e4b9f3":"markdown","b4e7ee37":"markdown","41c8dbdc":"markdown","3ecf77a2":"markdown","a58fc357":"markdown","94109235":"markdown","3fda1dce":"markdown","c5827098":"markdown","936515cb":"markdown","095ce581":"markdown","e680b0de":"markdown","12936040":"markdown","0236eee4":"markdown","aeca47b1":"markdown","03ed822f":"markdown","bd74cd81":"markdown","fd65c37d":"markdown","ce2024ae":"markdown","e5695686":"markdown","0331b343":"markdown","41068fe6":"markdown","e1162f9e":"markdown","c6e1f017":"markdown","a3f50fce":"markdown","f06b13d0":"markdown","7938335a":"markdown"},"source":{"fae1ef21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e67a985":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection  import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns ## I like using the Styles and background provided by Seaborn.\nsns.set()\nsns.set_style(\"darkgrid\")","d2e13df0":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","505a415e":"train.head()","ae833ae5":"train.isnull().sum().sum()","de3dc57a":"train.describe()","00469393":"train.info()\nprint('\\n')\ntest.info()","bb52a6dc":"train = train.astype(float)\ntest = test.astype(float)","d3b9f42a":"train.info()\nprint('\\n')\ntest.info()","c94b2bd7":"x= train.drop(columns = ['label'])","12ee07e9":"y= train.iloc[:,0]","59de9158":"y.head()","e42a815b":"x_train,x_test , y_train ,  y_test = train_test_split(x,y,random_state =14, test_size = 0.2) ","a63a6e34":"x_train = x_train\/255.0\nx_test = x_test\/255.0\ntest = test\/255.0","69ccd5b6":"print(x_train.values.shape)\nprint(x_test.values.shape)\nprint(test.values.shape)","23f72c27":"x_train = x_train.values.reshape(33600,28,28,1)\nx_test = x_test.values.reshape(-1,28,28,1) ## -1 means include all the samples. Jz to show that this can also be used ^_^\ntest = test.values.reshape(-1,28,28,1) ","48359846":"y_train.shape","ebf54823":"y_train.head(2)","2b8090d8":"y_train = np.array(y_train)\ny_test = np.array(y_test)","4c9d4ea7":"print(y_train)\nprint(y_test)","d8c538ac":"class mycallback(tf.keras.callbacks.Callback):\n        def on_epoch_end(self,epoch,logs={}):\n            if(logs.get('accuracy')>0.99):\n                print(\"\\nReached 99% accuracy so cancelling training!\")\n                self.model.stop_training = True\ncall = mycallback()","d3f6f60f":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(32, (3,3), activation='relu',input_shape=(28,28,1)),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n  tf.keras.layers.MaxPooling2D(2,2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation='relu'),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n","be03769b":"model.summary()","f9299f36":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","c88817b0":"history = model.fit(x_train,y_train ,epochs = 20,validation_data=(x_test,y_test), callbacks = [call])","b0bf92a1":"pred = model.predict(test)","8748309a":"pred","9c21f359":"pred[0]","5d777288":"pred=[np.argmax(i) for i in pred]","1516b43e":"pred[0] ## Boom this is what we were looking for ","bba34f3e":"ImageID = []\n\nfor i in range(len(pred)):\n    ImageID.append(i+1)","e3752672":"Predictions = pd.DataFrame({'ImageId' : ImageID, 'Label' : pred})","4fc4ceaf":"Predictions.head()","fa29c84b":"Submission_1 = Predictions.to_csv('Sub_1.csv',index=False)","a1e4b9f3":"Since Label is the Target variable we gotta store it in the varibale Y as per general conventions\nand the rest of the data in variable X.\n <br> LOL again Safe Practices ","b4e7ee37":"As I mentioned earlier softmax will return the probabilities of the labels, as we can see the highest probability is 1 for the third element i.e label 2. Let's see the same output but in a bit understandble format using argmax.\n<br>Since I need to submit the predictions as the label and not the probabilities argmax is the best way to do it.","41c8dbdc":"### Defining the callback","3ecf77a2":"## Digit Recognizer \n\nThis is gonna be my \"Hello World\" submission to Kaggle Competitions, i.e My First ever Submission \n\nProblem Statement & Data background :- The Bacis Digit Recognization using the pouplar Dataset MNIST the AKA Hello world of Computer vision.\n\nThe Data have Handwritten Digits from 0-9.\n\nMy approach :- I'll be using Keras basics CNN model using 2 layers of Convolutions.","a58fc357":"Checking for Null values, though I highly doubt I would find any, but HEY! Safe Practice lol ^_^","94109235":"Scaling is very important, since we need to bring all the pixel values in the same range so that the pixels with a higer values won't affect our model's predictions & we gotta do it for both tarining and testing data.","3fda1dce":"### Importing Important Libraries ","c5827098":"I'm Leaving the batch size as defalut which is 32. There's no hardcore rule to set batch size it's purely arbitrary and usually depends on the data ","936515cb":"we had to reshape the data because the Convulation layer expects the data in the format of (dm * dm * 1 or 3) the 1 or 3 as the last argument means the extra bit for color or channel, since the data set is in grayscale(b\/w) we use 1 if our data would have been color images we should have put it to 3 for RGB colors.","095ce581":"Relu which is an acronym for Rectified linear units will eliminate any -ve numbers i.e return 0 if x<0 else if x> 0 return x \n<br>Softmax return the probablity of possibe outputs.","e680b0de":"### Segregating Target & Features","12936040":"## Building Model ","0236eee4":"### Reshaping our Data ","aeca47b1":"Well this was my First ever Submission on Kaggle. \n<br>To improve the Accuracy we can tweak a few parameters or so called Hyperparameter tuning in cool terms.\nWe can try adding one more layer of Convulation, or we can decrease the Learning rate for Adam optimizer, which would make the Gradient Descent process a bit slow and might increase the Accuracy but will make the model fitting a bit time consuming.\nWe can Try using the RMSprop Optimizer which is another good Optimizer.","03ed822f":"Basic Data Exploration, Though all the Data pre-processing steps are already done on Kaggle But <br>HEY! it's always good to still explore the data as Safe Practice. ^_^\n","bd74cd81":"This callback function will stop the training if the epoch ends with the accuracy of 0.99 or 99% ","fd65c37d":"Since the Y_train is a series of labels, I'm converting it into a ndarray so that computing the cost would be easy and since I'm gonna use softmax as my cost function in the output and it returns the Probablity of the o\/p values, using an array would make the job easier. Another way to deal with this is to hot-encode the label.","ce2024ae":"As we can see the model is performing pretty good on both the Training & Testing Data, we can now use it to make the Final predictions on the data provided by Kaggle","e5695686":"so as we can see that the Data type is Int, it's always a best practice to covert the data to Float i.e real numbers rather than Integers. Since I'm gonna use CNN which uses Backpropogation, and the weights are always in float so I'm converting them to float for my own Convenience. \n\nThough it's not mandatory, Just a Safe Practice.","0331b343":"## Making Predictions on Test Data","41068fe6":"### Scaling Data ","e1162f9e":"Now why exactly do we need a train and test split when we already have test data provided by kaggle.\n<br>well It's only for our model evaluation since the test dataset provied by the kaggle is not LABELLED we won't know how exactly is our model performing on the unseen data. \n<br>It got nothing to do with the test dataset provided by Kaggle but I'm doing it for my own convinence, again not mandotry\n<br> Jz a Safe Practice LOL !","c6e1f017":"Since my Callback function is set to end the training as soon as any epoch reaches the accuaracy of 99%, this is to save the computational power, Defining a callback function can somtimes even makes the training faster and we don't have to go through all the epochs. Sometimes callback can even helps to save a model from overfitting.","a3f50fce":"### Importing Data ","f06b13d0":"### Converting the output CSV file","7938335a":"### Splitting Data in Train & Test Sets "}}