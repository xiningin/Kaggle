{"cell_type":{"c0e71ec3":"code","000a6bba":"code","12c6fee7":"code","ba891840":"code","2dafd7ba":"code","428faadc":"code","9726b05f":"code","87ac82ab":"code","44ccc436":"code","d13c1c34":"code","d997274a":"code","a5abb673":"code","3f2e8405":"code","9ade0a33":"code","570f016a":"code","28506669":"code","3061e080":"code","b806e430":"code","f277bc4a":"code","afdf9f59":"code","c7572440":"code","2c325f2f":"code","8b74491c":"code","0bdd4986":"code","d00cd438":"code","e376c890":"code","27a3d51f":"code","1eeb39d4":"code","4a49ebe3":"code","719d55fc":"code","623f2305":"code","b21a3508":"code","bf869da2":"markdown","13b1b443":"markdown","6d0ebbf8":"markdown","4c96707f":"markdown","366c911c":"markdown","4fd44a61":"markdown","7d28bd41":"markdown","a3eb6a90":"markdown","c864b7e8":"markdown","a91d5552":"markdown","6691b70c":"markdown","1ce29a24":"markdown","eeb6413a":"markdown","d9a13098":"markdown","77f30aea":"markdown","a30c047a":"markdown","0a5c869a":"markdown","238adf04":"markdown","ddd0e2ee":"markdown","7fc32845":"markdown","3f2a3d60":"markdown","84b9d34c":"markdown","ce82b47f":"markdown","a9e6f445":"markdown","edc45d7d":"markdown","4f3854f2":"markdown","ee941976":"markdown","5f4fb8d5":"markdown","2cdbf484":"markdown","c86a128e":"markdown","67b92128":"markdown","a2c71545":"markdown","a726eb58":"markdown","9bbfccec":"markdown","816146fd":"markdown","d25241e9":"markdown","4e8e10be":"markdown","39b57a7a":"markdown"},"source":{"c0e71ec3":"# Import the dependencies\nimport numpy as np\nimport pandas as pd\nimport sys \nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed, CuDNNLSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\n","000a6bba":"#Load the dataset\ndataset = pd.read_csv('..\/input\/taylor_swift_lyrics.csv', encoding = \"latin1\")\n","12c6fee7":"dataset.head()","ba891840":"dataset.describe()","2dafd7ba":"def processFirstLine(lyrics, songID, songName, row):\n    lyrics.append(row['lyric'] + '\\n')\n    songID.append( row['year']*100+ row['track_n'])\n    songName.append(row['track_title'])\n    return lyrics,songID,songName","428faadc":"# define empty lists for the lyrics , songID , songName \nlyrics = []\nsongID = []\nsongName = []\n\n# songNumber indicates the song number in the dataset\nsongNumber = 1\n\n# i indicates the song number\ni = 0\nisFirstLine = True\n\n# Iterate through every lyrics line and join them together for each song independently \nfor index,row in dataset.iterrows():\n    if(songNumber == row['track_n']):\n        if (isFirstLine):\n            lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n            isFirstLine = False\n        else :\n            #if we still in the same song , keep joining the lyrics lines    \n            lyrics[i] +=  row['lyric'] + '\\n'\n    #When it's done joining a song's lyrics lines , go to the next song :    \n    else :\n        lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n        songNumber = row['track_n']\n        i+=1\n","9726b05f":"\n# Define a new pandas DataFrame to save songID , songName , Lyrics in it to use them later\nlyrics_data = pd.DataFrame({'songID':songID, 'songName':songName, 'lyrics':lyrics })\n","87ac82ab":"lyricsText =''\nfor listitem in lyrics:\n    lyricsText += listitem\n","44ccc436":"# Load the dataset and convert it to lowercase :\nraw_text = lyricsText\nraw_text = raw_text.lower()\n","d13c1c34":"# Mapping chars to ints :\nchars = sorted(list(set(raw_text)))\nint_chars = dict((i, c) for i, c in enumerate(chars))\nchars_int = dict((i, c) for c, i in enumerate(chars))\n","d997274a":"# Get number of chars and vocab in our text :\nn_chars = len(raw_text)\nn_vocab = len(chars)\n","a5abb673":"print('Total Characters : ' , n_chars) # number of all the characters in lyricsText.txt\nprint('Total Vocab : ', n_vocab) # number of unique characters\n","3f2e8405":"# process the dataset:\nseq_len = 100\ndata_X = []\ndata_y = []\n\nfor i in range(0, n_chars - seq_len, 1):\n    # Input Sequeance(will be used as samples)\n    seq_in  = raw_text[i:i+seq_len]\n    # Output sequence (will be used as target)\n    seq_out = raw_text[i + seq_len]\n    # Store samples in data_X\n    data_X.append([chars_int[char] for char in seq_in])\n    # Store targets in data_y\n    data_y.append(chars_int[seq_out])\nn_patterns = len(data_X)\nprint( 'Total Patterns : ', n_patterns)\n","9ade0a33":"# Reshape X to be suitable to go into LSTM RNN :\nX = np.reshape(data_X , (n_patterns, seq_len, 1))\n# Normalizing input data :\nX = X\/ float(n_vocab)\n# One hot encode the output targets :\ny = np_utils.to_categorical(data_y)","570f016a":"LSTM_layer_num = 4 # number of LSTM layers\nlayer_size = [256,256,256,256] # number of nodes in each layer","28506669":"model = Sequential()","3061e080":"model.add(CuDNNLSTM(layer_size[0], input_shape =(X.shape[1], X.shape[2]), return_sequences = True))","b806e430":"for i in range(1,LSTM_layer_num) :\n    model.add(CuDNNLSTM(layer_size[i], return_sequences=True))","f277bc4a":"model.add(Flatten())","afdf9f59":"model.add(Dense(y.shape[1]))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')","c7572440":"model.summary()","2c325f2f":"# Configure the checkpoint :\ncheckpoint_name = 'Weights-LSTM-improvement-{epoch:03d}-{loss:.5f}-bigger.hdf5'\ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose = 1, save_best_only = True, mode ='min')\ncallbacks_list = [checkpoint]","8b74491c":"# Fit the model :\nmodel_params = {'epochs':30,\n                'batch_size':128,\n                'callbacks':callbacks_list,\n                'verbose':1,\n                'validation_split':0.2,\n                'validation_data':None,\n                'shuffle': True,\n                'initial_epoch':0,\n                'steps_per_epoch':None,\n                'validation_steps':None}\n\nmodel.fit(X,\n          y,\n          epochs = model_params['epochs'],\n           batch_size = model_params['batch_size'],\n           callbacks= model_params['callbacks'],\n           verbose = model_params['verbose'],\n           validation_split = model_params['validation_split'],\n           validation_data = model_params['validation_data'],\n           shuffle = model_params['shuffle'],\n           initial_epoch = model_params['initial_epoch'],\n           steps_per_epoch = model_params['steps_per_epoch'],\n           validation_steps = model_params['validation_steps'])\n","0bdd4986":"# Load wights file :\nwights_file = '.\/models\/Weights-LSTM-improvement-004-2.49538-bigger.hdf5' # weights file path\nmodel.load_weights(wights_file)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')","d00cd438":"# set a random seed :\nstart = np.random.randint(0, len(data_X)-1)\npattern = data_X[start]\nprint('Seed : ')\nprint(\"\\\"\",''.join([int_chars[value] for value in pattern]), \"\\\"\\n\")\n\n# How many characters you want to generate\ngenerated_characters = 300\n\n# Generate Charachters :\nfor i in range(generated_characters):\n    x = np.reshape(pattern, ( 1, len(pattern), 1))\n    x = x \/ float(n_vocab)\n    prediction = model.predict(x,verbose = 0)\n    index = np.argmax(prediction)\n    result = int_chars[index]\n    #seq_in = [int_chars[value] for value in pattern]\n    sys.stdout.write(result)\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\nprint('\\nDone')\n","e376c890":"!pip install -q textgenrnn\nfrom google.colab import files\nfrom textgenrnn import textgenrnn\nimport os","27a3d51f":"model_cfg = {\n    'rnn_size': 500,\n    'rnn_layers': 12,\n    'rnn_bidirectional': True,\n    'max_length': 15,\n    'max_words': 10000,\n    'dim_embeddings': 100,\n    'word_level': False,\n}\n\ntrain_cfg = {\n    'line_delimited': True,\n    'num_epochs': 100,\n    'gen_epochs': 25,\n    'batch_size': 750,\n    'train_size': 0.8,\n    'dropout': 0.0,\n    'max_gen_length': 300,\n    'validation': True,\n    'is_csv': False\n}","1eeb39d4":"uploaded = files.upload()\nall_files = [(name, os.path.getmtime(name)) for name in os.listdir()]\nlatest_file = sorted(all_files, key=lambda x: -x[1])[0][0]","4a49ebe3":"model_name = '500nds_12Lrs_100epchs_Model'\ntextgen = textgenrnn(name=model_name)\n\ntrain_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n\ntrain_function(\n    file_path=latest_file,\n    new_model=True,\n    num_epochs=train_cfg['num_epochs'],\n    gen_epochs=train_cfg['gen_epochs'],\n    batch_size=train_cfg['batch_size'],\n    train_size=train_cfg['train_size'],\n    dropout=train_cfg['dropout'],\n    max_gen_length=train_cfg['max_gen_length'],\n    validation=train_cfg['validation'],\n    is_csv=train_cfg['is_csv'],\n    rnn_layers=model_cfg['rnn_layers'],\n    rnn_size=model_cfg['rnn_size'],\n    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n    max_length=model_cfg['max_length'],\n    dim_embeddings=model_cfg['dim_embeddings'],\n    word_level=model_cfg['word_level'])","719d55fc":"print(textgen.model.summary())","623f2305":"files.download('{}_weights.hdf5'.format(model_name))\nfiles.download('{}_vocab.json'.format(model_name))\nfiles.download('{}_config.json'.format(model_name))","b21a3508":"textgen = textgenrnn(weights_path='6layers30EpochsModel_weights.hdf5',\n                       vocab_path='6layers30EpochsModel_vocab.json',\n                       config_path='6layers30EpochsModel_config.json')\n\ngenerated_characters = 300\n\ntextgen.generate_samples(300)\ntextgen.generate_to_file('lyrics.txt', 300)","bf869da2":"Some lyrics generated by a model created using   [textgenrnn](https:\/\/goo.gl\/E7szXj) :\n\n\n```\n\ni ' m not your friends\nand it rains when you ' re not speaking\nbut you think tim mcgraw\nand i ' m pacing down\ni ' m comfortable\ni ' m not a storm in mind\nyou ' re not speaking\nand i ' m not a saint and i ' m standin ' t know you ' re\ni ' m wonderstruck\nand you ' re gay\n\ni ' ve been giving out\nbut i ' m just another picture to pay\nyou ' re not asking myself , oh , i ' d go back to december , don ' t know you\nit ' s killing me like a chalkboard\nit ' s the one you\ncan ' t you ' re jumping into of you ' re not a last kiss\nand i ' m just a girl , baby , i ' m alone for me\ni ' m not a little troubling\n\nwon ' t you think about a . steps , you roll the stars mind\nyou ' s killing me ? )\nand i ' m say i won ' t stay beautiful at onto the first page\nyou ' s 2 : pretty\nand you said real\n?\nchange makes and oh , who baby , oh , and you talk away\nand you ' s all a minute , ghosts your arms page\nthese senior making me tough , so hello growing up , we were liar , no one someone perfect day when i came\n' re not sorry\nyou ' re an innocent\non the outskirts\n\night , don ' t say a house and he ' round\nshe ' re thinking to december all that baby , all everything now\nand let me when you oh , what to come back my dress\nalways\ni close both young before\nat ?\nyeah\n```\n\n","13b1b443":"Now , after we trained the model ,we can use it to generate fake Taylor Swift lyrics \n\n## Generating lyrics \nWe first pick a random seed , then we will use it to generate lyrics character by character .","6d0ebbf8":"### 3- Make samples and labels :\nMake samples and labels to feed the LSTM RNN","4c96707f":"Print a summary of the model to see some details :","366c911c":"### 2- Mapping characters :\n Make two dictionaries , one to convert chars to ints , the other to convert ints back to chars : \n","4fd44a61":"### Import the dependencies :","7d28bd41":"### Load trained model and use it :","a3eb6a90":"### 1- Convert the lyrics to lowercase :\n","c864b7e8":"## Training \nA model can't do a thing if it did not train.\n\nAs they say **\"No train no gain \"**\n\nFeel free to tweak `model_params` to get a better model","a91d5552":"We saw how easy and convenient it was using  [textgenrnn](https:\/\/goo.gl\/E7szXj) , yes the lyrics still not realistic, but there are much less spelling mistakes than the model that we built from scratch.\n\nanother good thing about  [textgenrnn](https:\/\/goo.gl\/E7szXj) is that one don't have to deal with any dataset processing, just upload the text dataset and set down with a cup of coffee watching your model training and getting better ","6691b70c":"**Load the dataset :**","1ce29a24":"Now, after we learned some essential information about LSTM and RNN , we will start implementing the idea (Taylor Swift Lyrics Generator) \n\nI will use two ways to build the model :\n* From scratch\n* Using a Python module called [textgenrnn](https:\/\/goo.gl\/E7szXj)\n\n## Process The Dataset \nTo train the LSTM model we need a dataset of Taylor songs' lyrics.\nAfter searching for it, I found [this great dataset](https:\/\/goo.gl\/3oUpMG) in Kaggle .\n\n**Let's take a look at it :**\n","eeb6413a":"We will start by determining how many layers our model will has , and how many nodes each layer will has :","d9a13098":"# **Taylor Swift Lyrics Generator**\n![](https:\/\/cdn-images-1.medium.com\/max\/1800\/1*VSXmKKJJZFlUVVZWBwbXVg.jpeg)\n\nA few days ago, I started to learn LSTM RNN (Long Short Term Memory Recurrent Neural Networks), and I thought that it would be a good idea if I make a project using it.\n\nThere is a multitude of applications of LSTM RNN, I decided to go with natural language generation because it will be a good opportunity to learn how to process text data, and it will be entertaining to see texts generated by neural networks, so I got this idea about generating Taylor Swift lyrics.\n","77f30aea":"## Next Steps :\nNow, after you learned how to make a LSTM RNN from scratch to generate texts , and also how to use Pyhton modules such as [textgenrnn](https:\/\/goo.gl\/E7szXj) you can do many things using this knowledge :\n* Try to use other datasets (wikipedia articles , William Shakespeare novevls, etc) to generate novels or articles.\n* Use  LSTM RNN in other applications than text generating .\n* Read more about LSTM RNN","a30c047a":"Flatten the data that is coming from the last hidden layer to input it to the output layer :","0a5c869a":"### LSTM layer VS CuDNNLSTM layer \nThe main difference is that LSTM uses the CPU and CuDNNLSTM uses the GPU , that's why CuDNNLSTM is much faster than LSTM , it is x15 faster.\n\nThis is the reason that made me use CuDNNLTSM instead of LSTM .\n\n**Note :** make sure to change the runtime setting of colab to use its GPU .","238adf04":"## First way : From Scratch ","ddd0e2ee":"### Train the model","7fc32845":"We can see that some files have been downloaded, we can use such files to load the trained weights to be used in untrained models (i.e we don't have to train a model every time we want to use it) \n\n### How to Load the Weights ?\n","3f2a3d60":"Add an input layer :","84b9d34c":"## **What is LSTM Recurrent Neural Networks ?**\nIf you don't know, LSTM recurrent neural networks are networks with loops in them, allowing information to persist, and they have a special type of nodes called LSTM(Long Short Term Memory).\n\nLSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nIf you want to know more about  LSTM Recurrent Neural Networks visit :\n[Understanding LSTM Networks](https:\/\/goo.gl\/dgmxQm) or [Long short-term memoryt](https:\/\/goo.gl\/Dc7kHF)\n","ce82b47f":"Define a sequential model :\n","a9e6f445":"first, import all the needed libraries for our project:","edc45d7d":"After we finished processing the dataset , we will start building our LSTM RNN model .\n\n## Building The Model ","4f3854f2":"Add some hidden layers : ","ee941976":"After getting the wanted data from the dataset , we need to preprocess it.\n\n## Preprocessing The Lyrics\n\n\n","5f4fb8d5":"After we defined the model , we will define the needed callbacks.\n\n### What is a callback ?\nA callback is a function that is called after every epoch\n\nin our case we will call the checkpoint callback , what a checkpoint callback does is saving the weights of the model every time the model gets better.","2cdbf484":"## References :\n* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https:\/\/goo.gl\/Kbpk8S)\n* [Applied Introduction to LSTMs with GPU for text generation](https:\/\/goo.gl\/xnQSJU)\n* [Generating Text Using LSTM RNN](https:\/\/goo.gl\/7GxTxu)\n* [textgenrnn](https:\/\/goo.gl\/E7szXj)\n* [Train a Text-Generating Neural Network for Free with textgenrnn](https:\/\/goo.gl\/biaFCr)\n* [Understanding LSTM Networks](https:\/\/goo.gl\/dgmxQm)\n* [Long short-term memory](https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory)","c86a128e":"### Upload the dataset :","67b92128":"You might noticed that the generated lyrics are not real , and there are many spelling mistakes.\n\nYou can tweak some parameters and add a Dropout layer to avoid overfitting ,then the model could be better at generating tolerable lyrics.\n\nbut if you are lazy and don't want to bother yourself trying these steps , try using [textgenrnn](https:\/\/goo.gl\/E7szXj)","a2c71545":"### Download the trained weights :","a726eb58":"## LSTM Recurrent Neural Networks Applications \nLSTM Recurrent Neural Networks are used in many applicattions , the following are the most popular ones :\n\n\n\n*   Language modeling\n*   Text classification\n*   Dialog systems\n*   **Natural language generation**\n\n[More applications](https:\/\/goo.gl\/eT3bMm)\n","9bbfccec":"### Configure the model :","816146fd":"### 4- Prepare the samples and labels :\nprepare the samples and labels to be ready to go into our model.\n* Reshape the samples\n* Normalize them\n* One hot encode the output targets ","d25241e9":"Add an output layer and define its activation function to be **'softmax'** \n\nand then compile the model with the next params :\n*  loss = 'categorical_crossentropy'\n*  optimizer = 'adam'","4e8e10be":"From what we see , we need to concatenate the lines of each song to get each song by its own in one string.","39b57a7a":"## Second way : Using [textgenrnn](https:\/\/goo.gl\/E7szXj)\n"}}