{"cell_type":{"e9b4191f":"code","afa8290a":"code","bc792c2f":"code","a051af98":"code","6fd771d9":"code","ba634c1f":"code","4e7c8c6c":"code","6dfa9c53":"code","5e931c76":"code","b20834c7":"code","b6a68d47":"code","25365f70":"code","73218ecf":"code","8934c36e":"code","7f30a7f6":"code","3e44ddbe":"code","3cc9dd74":"code","10e06fca":"code","8a05fad0":"code","0637ce17":"code","8dd3d023":"code","229c0666":"code","32374c66":"code","13b27d47":"code","63ee9b30":"code","9a29c464":"code","874ad752":"code","1fa99f68":"code","ff61088d":"code","c72e5384":"code","4ba5d2fa":"code","c62ef535":"code","c9c37db9":"code","ceb206d2":"code","83db3bd0":"code","1a7a8fcd":"code","e496d20e":"code","f04372f5":"code","1f19a46a":"code","4cbdc4b9":"code","5dfde076":"code","6a25ef60":"code","71b9caae":"code","de6c0e57":"code","66aa1ff2":"code","d289e630":"code","59a5ff9d":"code","0d9376dc":"code","90ad6ffd":"code","a22afe56":"code","cca63a0c":"code","d2694d25":"code","1fd588e9":"code","eb1b31d6":"code","7b2318e6":"code","7940d523":"code","fafdbf53":"code","22ebc992":"code","4c40a2e3":"code","c6579945":"code","d753668c":"code","3f898b6d":"code","29147c60":"code","ba9e79ff":"code","ff674ed5":"code","da447859":"code","9092b696":"code","ad61a4f0":"markdown","65bcb5ac":"markdown","264b1298":"markdown","6ed3039e":"markdown","1b84c6d2":"markdown","fb3ff435":"markdown","42aea32d":"markdown","f9b67225":"markdown","97cfce64":"markdown","7b71d49b":"markdown","83f175cf":"markdown","d0ce8b6c":"markdown","3759619e":"markdown","327b937e":"markdown","404deae3":"markdown","a2ee9130":"markdown","c226be3c":"markdown","06fc4f79":"markdown","c823f5cf":"markdown","cf249b2f":"markdown","486a8085":"markdown","67e10f81":"markdown","11075720":"markdown","298876a3":"markdown","ccb63587":"markdown","9d02f584":"markdown","c33d12a2":"markdown","ff5bff61":"markdown","43b2cd41":"markdown","d6e416ea":"markdown","976f5b9d":"markdown","2639bc6e":"markdown"},"source":{"e9b4191f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afa8290a":"# Extrac zip files\nimport zipfile\ndef extract_images(filePath):\n    with zipfile.ZipFile(filePath,\"r\") as z:\n        z.extractall(\".\")\n# Test\nextract_images('\/kaggle\/input\/facebook-recruiting-iv-human-or-bot\/train.csv.zip')\nextract_images('\/kaggle\/input\/facebook-recruiting-iv-human-or-bot\/bids.csv.zip')\nextract_images('\/kaggle\/input\/facebook-recruiting-iv-human-or-bot\/test.csv.zip')","bc792c2f":"train_df = pd.read_csv('\/kaggle\/working\/train.csv')\nbids_df = pd.read_csv('\/kaggle\/working\/bids.csv')\ntest_df = pd.read_csv('\/kaggle\/working\/test.csv')\ntest_df.head()","a051af98":"bids_df.head()","6fd771d9":"train_df.head()","ba634c1f":"# Populate train and test data with bid details\ndef populate_bids(df):\n    df = pd.merge(left=df, right=bids_df, how='left', left_on='bidder_id', right_on='bidder_id')\n    return df","4e7c8c6c":"test = populate_bids(test_df)\ntest.isna().sum()","6dfa9c53":"train = populate_bids(train_df)\ntrain.isna().sum()","5e931c76":"nobids_train = train[pd.isnull(train['bid_id'])]['bidder_id'].unique()\nlen(nobids_train)","b20834c7":"nobids_test = test[pd.isnull(test['bid_id'])]['bidder_id'].unique()\nlen(nobids_test)","b6a68d47":"# Check outcome for these in the train\ntrain[train['bidder_id'].isin(nobids_train)]['outcome']","25365f70":"# Populate train and test data with bid details\ndef populate_bids_with_data_cleanup(df):\n    df = pd.merge(left=df, right=bids_df, how='left', left_on='bidder_id', right_on='bidder_id')\n    nobids = df[pd.isnull(df['bid_id'])]['bidder_id'].unique()\n    # for all the missing value observation, drop it\n    df = df[~df['bidder_id'].isin(nobids)]\n    return df","73218ecf":"test_data = populate_bids_with_data_cleanup(test_df)\ntest_data.isna().sum()","8934c36e":"train_data = populate_bids_with_data_cleanup(train_df)\ntrain_data.isna().sum()","7f30a7f6":"# check the missing value ratio of the country entry in train data\ncount_miss_ratio_train = pd.isnull(train_data['country']).sum()\/len(train_data)*100\nprint(f\" %0.2f %% observations missing country entry in train data.\" %\n      float(count_miss_ratio_train))","3e44ddbe":"# check the missing value ratio of the country entry in test data\ncount_miss_ratio_test = pd.isnull(test_data['country']).sum()\/len(test_data)*100\nprint(f\" %0.2f %% observations missing country entry in test data.\" %\n      float(count_miss_ratio_test))","3cc9dd74":"# Populate train and test data with bid details\ndef populate_bids_with_data_cleanup_final(df):\n    df = pd.merge(left=df, right=bids_df, how='left', left_on='bidder_id', right_on='bidder_id')\n    nobids = df[pd.isnull(df['bid_id'])]['bidder_id'].unique()\n    # for all the missing value observation, drop it\n    df = df[~df['bidder_id'].isin(nobids)]\n    # remove missing country data\n    df = df.dropna()\n    return df","10e06fca":"train = populate_bids_with_data_cleanup_final(train_df)\ntrain.isna().sum()","8a05fad0":"test = populate_bids_with_data_cleanup_final(test_df)\ntest.isna().sum()","0637ce17":"# check the unique number of each feature in train\nprint(f\"total row in train:          {len(train)}\")\nprint(f\"total bids in train:         {len(train['bid_id'].unique())}\")\nprint(f\"total bidder in train:      {len(train['bidder_id'].unique())}\")\nprint(f\"total payment in train:     {len(train['payment_account'].unique())}\")\nprint(f\"total address in train:     {len(train['address'].unique())}\")\nprint(f\"total auction in train:     {len(train['auction'].unique())}\")\nprint(f\"total merchandise in train: {len(train['merchandise'].unique())}\")\nprint(f\"total device in train:      {len(train['device'].unique())}\")\nprint(f\"total country in train:     {len(train['country'].unique())}\")\nprint(f\"total ip in train:          {len(train['ip'].unique())}\")\nprint(f\"total url in train:         {len(train['url'].unique())}\")","8dd3d023":"print(\n    f\"total bids made by robots in data set: {len(train[train['outcome'] == 1])}\")\nprint(\n    f\"total bids made by human in data set:  {len(train[train['outcome'] == 0])}\")\nprint(\"the ratio of made by robot vs. bids made by human in data set: 3:20\")","229c0666":"def get_metrics(bid):\n    print(f\"average bids per robot: %.0f\" % float(len(\n        bid[bid['outcome'] == 1])\/len(bid[bid['outcome'] == 1]['bidder_id'].unique())))\n    print(f\"average bids per human:  %.0f\" % float(len(\n        bid[bid['outcome'] == 0])\/len(bid[bid['outcome'] == 0]['bidder_id'].unique())))\n    print(f\"average bids per auction by robot: %.0f\" % float(\n        len(bid[bid['outcome'] == 1])\/len(bid[bid['outcome'] == 1]['auction'].unique())))\n    print(f\"average bids per auction by human:  %.0f\" % float(\n        len(bid[bid['outcome'] == 0])\/len(bid[bid['outcome'] == 0]['auction'].unique())))\n    print(f\"average bids per device by robot: %.0f\" % float(\n        len(bid[bid['outcome'] == 1])\/len(bid[bid['outcome'] == 1]['device'].unique())))\n    print(f\"average bids per device by human:  %.0f\" % float(\n        len(bid[bid['outcome'] == 0])\/len(bid[bid['outcome'] == 0]['device'].unique())))\n    print(f\"average bids per ip by robots: %.2f\" % float(\n        len(bid[bid['outcome'] == 1])\/len(bid[bid['outcome'] == 1]['ip'].unique())))\n    print(f\"average bids per ip by human:  %.2f\" % float(\n        len(bid[bid['outcome'] == 0])\/len(bid[bid['outcome'] == 0]['ip'].unique())))\n    print(f\"average bids per url by robot: %.2f\" % float(\n        len(bid[bid['outcome'] == 1])\/len(bid[bid['outcome'] == 1]['url'].unique())))\n    print(f\"average bids per url by human:  %.2f\" % float(\n        len(bid[bid['outcome'] == 0])\/len(bid[bid['outcome'] == 0]['url'].unique())))\n\nget_metrics(train)","32374c66":"# bidding time difference per user (bidder_id)\ndef calculate_bidding_time_difference_per_user(df):\n    df = df.sort_values(by=['time'])\n    df['timediffs'] = df.groupby('bidder_id')['time'].transform(pd.Series.diff)\n    return df","13b27d47":"# number of bids a user made per auction\ndef bids_user_made_per_auction(df):\n    bids_per_auction = df.groupby(['auction', 'bidder_id']).size()\n    return bids_per_auction.to_frame()","63ee9b30":"# proportion of bots for each country\ndef proportion_of_bots_for_each_country(df):\n    pbots_country = df[df['outcome'] == 1].groupby('country').size()\/df.groupby('country').size()\n    pbots_country = pbots_country.fillna(0)\n    return pbots_country.to_frame()","9a29c464":"# proportion of bots per device\ndef proportion_of_bots_per_device(df):\n    pbots_device = df[df['outcome'] == 1].groupby('device').size()\/df.groupby('device').size()\n    pbots_device = pbots_device.fillna(0)\n    return pbots_device.to_frame()","874ad752":"# number of unique ip to number of bids ratio\ndef number_of_unique_ip_to_number_of_bids_ratio(df):\n    ip_bids_ratio = df.groupby('bidder_id')['ip'].nunique()\/df.groupby('bidder_id')['bid_id'].nunique()\n    return ip_bids_ratio.to_frame()","1fa99f68":"from scipy import stats\n\ndef ent(data):\n    \"\"\"\n    Calculate the entropy\n\n    Parameters\n    ----------\n    data : dataframe\n        a DataFrame containing original data\n\n    Returns\n    -------\n    output: float\n        computed entropy \n\n    Notes\n    -----\n    entropy is calculated with the following steps:\n        1. compute the probabilities\n        2. fit in the entropy formula\n    \"\"\"\n    p_data = data.value_counts()\/len(data)  # calculates the probabilities\n    # input probabilities to get the entropy\n    entropy = stats.entropy(p_data)\n    return entropy","ff61088d":"# mean per auction url entropy for each user\n# Input a pandas series\ndef mean_per_auctio_url_entropy_for_each_user(df):\n    auction_url_entropy = df.groupby(['auction', 'bidder_id'])['url'].apply(ent)\n    return auction_url_entropy.groupby('bidder_id').mean().reset_index()","c72e5384":"def feature_engineering(df):\n    df = df.sort_values(by=['time'])\n    df['timediffs'] = df.groupby('bidder_id')['time'].transform(pd.Series.diff)\n    df = pd.merge(df, bids_user_made_per_auction(df), on=['auction', 'bidder_id'], how='left')\n    df = pd.merge(df, proportion_of_bots_for_each_country(df), on='country', how='left')\n    df = pd.merge(df, proportion_of_bots_per_device(df), on='device', how='left')\n    df = pd.merge(df, number_of_unique_ip_to_number_of_bids_ratio(df), on='bidder_id', how='left')\n    df = pd.merge(df, mean_per_auctio_url_entropy_for_each_user(df), on='bidder_id', how='left')\n    return df","4ba5d2fa":"train = feature_engineering(train)\ntrain.isnull().sum()","c62ef535":"train.columns","c9c37db9":"# set column names\ntrain.columns = ['bidder_id', 'payment_account', 'address', 'outcome',\n               'bid_id', 'auction', 'merchandise', 'device', 'time', 'country',\n               'ip', 'url', 'timediffs', 'bids_per_auction', 'pbots_country', 'pbots_device',\n               'ip_bids_ratio', 'auction_url_entropy']\ntrain.head()","ceb206d2":"train.timediffs = train.timediffs.fillna(0)\ntrain.head()","83db3bd0":"!pip install beautifultable","1a7a8fcd":"# data visualization package\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from ggplot import *\nfrom beautifultable import BeautifulTable\n# ignore the warning\nimport warnings","e496d20e":"warnings.filterwarnings('ignore')","f04372f5":"bots = train.loc[train.outcome == 1]\nhuman = train.loc[train.outcome == 0]\nbots.shape, human.shape","1f19a46a":"fig, axes = plt.subplots(3, 2, figsize=(14, 12), sharex=False)\nsns.distplot(train['bids_per_auction'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue',\n             kde_kws={'linewidth': 1.5}, ax=axes[0, 0])\nsns.distplot(human['bids_per_auction'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred',\n             kde_kws={'linewidth': 1.5}, ax=axes[0, 0])\n\nsns.distplot(bots['pbots_country'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue',\n             kde_kws={'linewidth': 1.5}, ax=axes[0, 1])\nsns.distplot(human['pbots_country'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred',\n             kde_kws={'linewidth': 1.5}, ax=axes[0, 1])\n\nsns.distplot(bots['pbots_device'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue', label='bots',\n             kde_kws={'linewidth': 1.5}, ax=axes[1, 0])\nsns.distplot(human['pbots_device'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred', label='human',\n             kde_kws={'linewidth': 1.5}, ax=axes[1, 0])\n\nsns.distplot(bots['ip_bids_ratio'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue',\n             kde_kws={'linewidth': 1.5}, ax=axes[1, 1])\nsns.distplot(human['ip_bids_ratio'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred',\n             kde_kws={'linewidth': 1.5}, ax=axes[1, 1])\n\nsns.distplot(bots['auction_url_entropy'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue',\n             kde_kws={'linewidth': 1.5}, ax=axes[2, 0])\nsns.distplot(human['auction_url_entropy'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred',\n             kde_kws={'linewidth': 1.5}, ax=axes[2, 0])\n\nsns.distplot(bots['timediffs'], hist=False, kde=True,\n             bins=int(180\/5), color='darkblue',\n             kde_kws={'linewidth': 1.5}, ax=axes[2, 1])\nsns.distplot(human['timediffs'], hist=False, kde=True,\n             bins=int(180\/5), color='darkred',\n             kde_kws={'linewidth': 1.5}, ax=axes[2, 1])\nplt.legend(['bots', 'human'])\nplt.show()","4cbdc4b9":"# Plot the correlation matrix for the numerical values\ncorr_matrix = train.corr()\nsns.heatmap(corr_matrix.corr(),\n            xticklabels=corr_matrix.corr().columns,\n            yticklabels=corr_matrix.corr().columns,\n            cmap=\"Blues\",\n            fmt='d')","5dfde076":"# for train and test data set split\nfrom sklearn.model_selection import train_test_split\nbid_train, bid_test = train_test_split(train, test_size=0.2)","6a25ef60":"# Balance train data\nbots_train = bid_train.loc[bid_train.outcome == 1]\nhuman_train = bid_train.loc[bid_train.outcome == 0]\nhuman_sample = human_train.sample(n=len(bots_train))\nbid_train_balance = pd.concat([bots_train, human_sample])\nbid_train_balance.head()","71b9caae":"train_columns = ['bids_per_auction','pbots_country','pbots_device','ip_bids_ratio','auction_url_entropy']\nX_train = bid_train_balance[train_columns]\ny_train = bid_train_balance['outcome']\n\nX_test = bid_test[train_columns]\ny_test = bid_test['outcome']\nX_train.shape, y_train.shape","de6c0e57":"# for decision tree model\nfrom sklearn import tree\n# for grid search\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# hyperparameter tuning\ndt = tree.DecisionTreeClassifier()\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': range(3, 6),\n    'max_leaf_nodes': range(10, 15),\n    'min_samples_split': range(2, 6)\n}\n\ndt_cv = GridSearchCV(estimator=dt,\n                     param_grid=param_grid,\n                     cv=5)\ndt_cv.fit(X_train, y_train)\nprint(dt_cv.best_params_)","66aa1ff2":"import time\n# fit the model\n# Decision Tree\nstart_time = time.time()\nkwargs_regularize = dict(criterion='gini',\n                         max_depth=5,\n                         max_leaf_nodes=14,\n                         min_samples_split=2)\ndt = tree.DecisionTreeClassifier(**kwargs_regularize)\ndt.fit(X_train, y_train)\n\ndt_time = time.time() - start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n","d289e630":"%pip install pydotplus","59a5ff9d":"# for decision tree visualizaiton\nfrom six import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dt, out_file=dot_data,\n                filled=True, rounded=True,\n                feature_names=X_train.columns.values,\n                class_names=['human', 'bot'],\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","0d9376dc":"# Check feature importance and display in bar plot.\nprint('Feature importance of Decision Tree Model')\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(5, 5))\nfeat_importances = pd.Series(dt.feature_importances_, index=X_train.columns)\nfeat_importances.nsmallest(5).plot(kind='barh', alpha=0.7)\nfig.savefig('dt_feature.png')","90ad6ffd":"# predict\ny_dt_pred = dt.predict(X_test)","a22afe56":"# for evaluation metric\n# accuracy\nfrom sklearn.metrics import accuracy_score\n# accuracy score\nprint(f\"Decision Tree Accuracy: {accuracy_score(y_dt_pred, y_test):.3f}\")","cca63a0c":"# AUC\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom pylab import rcParams\n\n# Plot ROC in one graph\ny_dt_score = dt.predict_proba(X_test)[:, 1]\nfpr_dt, tpr_dt, _dt = roc_curve(y_test, y_dt_score)\nroc_dt_auc = auc(fpr_dt, tpr_dt)\n\nfig = plt.figure(figsize=(5, 5))\nplt.plot(fpr_dt, tpr_dt, label='DT ROC curve (area = %0.2f)' % roc_dt_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.005])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\nfig.savefig('roc_dt_auc.png')","d2694d25":"print('Classification Report of Decision Tree Model')\nprint(classification_report(y_test, y_dt_pred))","1fd588e9":"# for random search\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50)\nparam_grid = {\n    'max_depth': range(3, 6),\n    'max_leaf_nodes': range(8, 12),\n    'max_features': ['sqrt', 'auto', 'log2']\n}\n\nrf_cv = GridSearchCV(estimator=rf,\n                     param_grid=param_grid,\n                     cv=5)\nrf_cv.fit(X_train, y_train)\nprint(rf_cv.best_params_)","eb1b31d6":"start_time = time.time()\n\nrf = RandomForestClassifier(n_estimators=100, max_depth=5,\n                            max_leaf_nodes=11, max_features='log2',\n                            bootstrap=True, oob_score=True)\nrf.fit(X_train, y_train)\n\nrf_time = time.time() - start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","7b2318e6":"# Check feature importance and display in bar plot.\nprint('Feature importance of Random Forest Model')\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(5, 5))\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nfeat_importances.nsmallest(5).plot(kind='barh', alpha=0.7)\nfig.savefig('rf_feature.png')","7940d523":"y_rf_pred = rf.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_rf_pred, y_test):.3f}\")","fafdbf53":"# Plot ROC in one graph\ny_rf_score = rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _rf = roc_curve(y_test, y_rf_score)\nroc_rf_auc = auc(fpr_rf, tpr_rf)\n\nplt.figure(figsize=(5, 5))\nplt.plot(fpr_rf, tpr_rf, label='RF ROC curve (area = %0.2f)' % roc_rf_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.005])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\nfig.savefig('roc_rf_auc.png')","22ebc992":"print('Classification Report of Random Forest Model')\nprint(classification_report(y_test, y_rf_pred))","4c40a2e3":"# for gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=10)\nparam_grid = {\n    'max_depth': range(3, 6),\n    'max_leaf_nodes': range(8, 11)\n}\n\ngb_cv = GridSearchCV(estimator=gb,\n                     param_grid=param_grid,\n                     cv=5)\ngb_cv.fit(X_train, y_train)\nprint(gb_cv.best_params_)","c6579945":"start_time = time.time()\n\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=5, max_features='sqrt',\n                                max_leaf_nodes=9)\ngb.fit(X_train, y_train)\n\ngb_time = time.time() - start_time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","d753668c":"# Check feature importance and display in bar plot.\nprint('Feature importance of Gradient Boosting Model')\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(5, 5))\nfeat_importances = pd.Series(gb.feature_importances_, index=X_train.columns)\nfeat_importances.nsmallest(5).plot(kind='barh', alpha=0.7)\nfig.savefig('gb_feature.png')","3f898b6d":"y_gb_pred = gb.predict(X_test)\nprint(f\"Gradient Boosting Accuracy: {accuracy_score(y_gb_pred, y_test):.3f}\")","29147c60":"# Plot ROC in one graph\ny_gb_score = gb.predict_proba(X_test)[:, 1]\nfpr_gb, tpr_gb, _gb = roc_curve(y_test, y_gb_score)\nroc_gb_auc = auc(fpr_gb, tpr_gb)\n\nplt.figure(figsize=(5, 5))\nplt.plot(fpr_gb, tpr_gb, label='GB ROC curve (area = %0.2f)' % roc_gb_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.005])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\nfig.savefig('roc_gb_auc.png')","ba9e79ff":"print('Classification Report of Gradient Boosting Model')\nprint(classification_report(y_test, y_gb_pred))","ff674ed5":"# for model comparision\nfrom sklearn import metrics\n\ndt_metrics = [metrics.accuracy_score(y_test, y_dt_pred), metrics.precision_score(y_test, y_dt_pred),\n              metrics.recall_score(y_test, y_dt_pred), metrics.f1_score(\n                  y_test, y_dt_pred),\n              metrics.roc_auc_score(y_test, y_dt_pred), dt_time]\nrf_metrics = [metrics.accuracy_score(y_test, y_rf_pred), metrics.precision_score(y_test, y_rf_pred),\n              metrics.recall_score(y_test, y_rf_pred), metrics.f1_score(\n                  y_test, y_rf_pred),\n              metrics.roc_auc_score(y_test, y_rf_pred), rf_time]\ngb_metrics = [metrics.accuracy_score(y_test, y_gb_pred), metrics.precision_score(y_test, y_gb_pred),\n              metrics.recall_score(y_test, y_gb_pred), metrics.f1_score(\n                  y_test, y_gb_pred),\n              metrics.roc_auc_score(y_test, y_gb_pred), gb_time]","da447859":"fig, ax = plt.subplots(figsize=(10, 8))\nindex = np.arange(5)\nwidth = 0.2\nb1 = plt.bar(index, dt_metrics[0:5], width,\n             alpha=0.4, color='grey', label='decision tree')\nb2 = plt.bar(index+width, rf_metrics[0:5], width,\n             alpha=0.8, color='powderblue', label='random forest')\nb3 = plt.bar(index+2*width, gb_metrics[0:5], width,\n             alpha=0.8, color='pink', label='gradient boosting')\nplt.title('Model Comparison')\nplt.ylabel('score')\nplt.xticks(index+width, ('accuracy', 'precision', 'recall', 'F1', 'ROC AUC'))\nplt.legend(loc=8, ncol=3, mode=\"expand\", borderaxespad=0.)\nplt.show()\nfig.savefig('model_comparison.png')","9092b696":"label = [\"Accuracy_score\", \"Precision_score\",\n         \"Recall_Score\", \"F1_score\", \"ROC_AUC_score\", \"Time(s)\"]\ntable = pd.DataFrame({'Decision Tree': dt_metrics,\n                      'Random Forest': rf_metrics, 'Gradient Boosting': gb_metrics})\ntable = table.transpose()\ntable.columns = label\ntable.transpose().round(3)","ad61a4f0":"**8 Compare Different Models**","65bcb5ac":"Precison vs. Recall","264b1298":"**Evaluation Metric**\n\nAccuracy","6ed3039e":"There exist 29 bidder in training data set having no bid data. After checking with their labels, they were all marked as human. Since we aim to detect robot, we will just ignore them. We will update `populate_bids` function.","1b84c6d2":"**5. Data Visualization**\nCheck whether the new features could help us identify human and robots, plot the distribution of each feature of two different type of bidders.","fb3ff435":"From the above, we can see distinct difference between bids made by human and bots per auction, per device. Features related to number of bids, auction, device draw our attention.","42aea32d":"**AUC**","f9b67225":"Precision vs. Recall","97cfce64":"**Evaluation Metric**","7b71d49b":"AUC","83f175cf":"**Second Model: Random Forest**","d0ce8b6c":"**Evaluation Metrics**\n\nAccuracy","3759619e":"This plot is the correlation matrix heatmap for our engineered features. Dark blue in this heatmap means two variables are highly positively correlated with each other. The white color in this heatmap indicates that two features are highly negatively correlated with each other. From this plot, we can infer that proportion of bots per device, proportion of bots for each country and number of unique ip to number of bids ratio are highly positively correlated with our outcome. While the mean per auction url entropy for each user are highly negatively correlated with our outcome. However, the variable time differ per user does not show clearly corrrelation with our outcome.\n","327b937e":"For the full data set, there is only few of observations missing country entry. We will just ignore the missing country entry data.","404deae3":"**6. Process imblance data**","a2ee9130":"**Undersampling**\n\nFrom data preprocessing part, we see the ratio of the robot bidder and the human bidder is 3:20, thus we need to undersampling the human bidder samples in the trainset to keep the data balance.\n\n","c226be3c":"**1.0 Data loading**\n\nExtract zip files and load data into panda dataframe.","06fc4f79":"**4. Feature Engineering**","c823f5cf":"**Tree Visualization**","cf249b2f":"AUC","486a8085":"These trainning data set has unbalanced number of robots and human:\n\nWe might need to do resampling before training the model.\nfor the evaluation metrices, we will foucus on AUC and precision vs. recall.\nThen checking the features' ratio of robots and human:","67e10f81":"Since the variable timediff (time differ per user) showed a similar distribution between robots and human. It also has no clearly correlation with our outcome. Therefore, this variable may not be very helpful when we are fitting a model. In contrast, we may even have overfitting problems if we incorporate it in our models. Therefore, we will ignore the variable timediff during the modeling process.\n","11075720":"**Data Cleaning**\nSince we can see all the missing values are from the variables that come from bids, we want to check if there exist some of bidder_id do not have any bids.","298876a3":"From this output, we can see the payment and address are one-to-one correspondence to bidder. Since our outpue, whether a bid is made from a human or robot, is label based on bidder, we can igonre these two features for future analysis. Since merchandise and country are compared low in this situation, we can just treat themselves as a feature for analysis.\n\nFor the other features, let's check the ratios to robots and human. Before that, let's check the robots vs. human first.","ccb63587":"**First Model : Decision Tree**","9d02f584":"**Conclusion**\n\nFrom the feature importance, we can see that in the online bidding, human being are more likely come from a random url into a certain aution, however, the robots are more likely come from few specific url. And compared with human beings robots are more likely to use different ip addresses for online bidding. Thus if for a user, it using multi-ip addresses and come from few certain url, we might consider it is a robots.","c33d12a2":"**7. Modeling**","ff5bff61":"**3.0 Data Exploration**","43b2cd41":"**Pecision vs Recall**","d6e416ea":"**Third Model: Gradient Boosting**","976f5b9d":"**2.0 Analyze Dataset**\nThe datasets are provided by two part:\n* train.csv: the base bidder dataset with labels. Besides label, it also contains unique bidder_id followed with payment_account, address and outcome;\n* bids.csv: the dataset contains some more useful information associated with unique key bid_id, corresponding with bidder_id, auction, merchandise, device, time, country, ip, and url. time and url are encrypted information.\n\nSince both of these two files contains the unique key bidder_id, we firstly joined these data by this key. Our object is to try to find each bid is made by a human or robot, thus, we only consider the data which have bids information and labelled. We will left join on train.csv and check the joined data for missing bids information.","2639bc6e":"**Accuracy**"}}