{"cell_type":{"bbe4ff42":"code","635c091d":"code","578fdbe8":"code","3f450a11":"code","99f44c64":"code","6470811f":"code","880d550e":"code","17a57077":"code","bbfda58c":"code","79497e7d":"code","0805b5a0":"code","83a3ca27":"code","a3b2ee28":"code","728f7679":"code","17f37ad0":"code","3803e6e0":"code","06246d27":"code","4f79c600":"code","8180ff93":"code","688d97e6":"code","0ede6f67":"code","3258faaf":"code","122d6d40":"code","f8659fb7":"code","bb252eb4":"code","1aa97503":"code","f44a70b9":"code","d9df7231":"code","2067e926":"code","52be47b6":"code","8b1099d5":"code","71008130":"code","b30f31b9":"code","aef566cc":"code","8da511c6":"code","eb3c95d1":"code","ea76758d":"code","3e905ea3":"code","b44d752c":"code","2b0bd3b2":"code","4e4ab5fc":"code","32ba0749":"code","4178ba37":"code","29ee1e9e":"code","b29f86f6":"code","3c8ad824":"code","3893190d":"code","fcb0e1c0":"code","03bbe298":"code","69068022":"code","69d9f751":"code","3e89b623":"code","d9f5c7d1":"code","3a7895c0":"code","ae9d9800":"code","4b633eb9":"code","a6f68564":"code","858c1a0e":"code","6f3af8e8":"code","4fc281c4":"code","e040850c":"code","f84d181c":"code","44ba28be":"code","afe371b3":"code","588eb123":"code","3b21cf8a":"code","7673a4b8":"code","03e73042":"code","c49a86ec":"code","a14f5c28":"code","efa09f64":"code","cae96324":"code","f076adbf":"code","1003a0d3":"code","b62bd19d":"code","8a0cd561":"code","1023bca8":"code","58c28b6f":"code","cf930d42":"code","48b31fb2":"code","dc8ae69a":"code","d747f270":"code","dc958fb6":"code","bacb4b1e":"code","7ca70e1d":"code","bf706bbe":"code","867d6e48":"code","85de29e8":"code","f37844b6":"code","02438e51":"code","9e7303ab":"code","473bec91":"code","a501b7b2":"code","31e65d2a":"code","56ba4384":"code","03137b32":"code","e471990b":"code","b7d5fab5":"code","530f88bb":"code","e2883ffc":"code","0d757818":"code","1bfc1018":"code","cd05b004":"code","577257fe":"code","69d33055":"code","573a698a":"code","ccce9571":"code","ee7a1631":"code","36aa7fbc":"code","cc4b6af8":"code","4e03be6b":"code","ac98f9b0":"code","285fcfec":"code","a78e58b5":"code","999815ca":"code","ff8d6d1e":"code","b8b03264":"code","fe9b880b":"code","ff7230a8":"code","946bcb4d":"code","788181f6":"code","e4992e77":"code","135b683f":"code","70799547":"code","d8e97e6b":"code","41b7c670":"code","9a419b84":"code","65a8e218":"code","8097a6cd":"code","714925da":"code","a76df41e":"code","84297ea1":"code","aae18df1":"code","33c63740":"code","440cb666":"code","3d7305d6":"markdown","3701aab7":"markdown","3ead598c":"markdown","3b9ce44f":"markdown","9f47d54e":"markdown","49cf2c49":"markdown","f49feb02":"markdown","acb9d4e0":"markdown","8c28ec0e":"markdown","937721d6":"markdown","a6a741f2":"markdown","8222ecf4":"markdown","f1a911c6":"markdown","72af9674":"markdown","320e0aa2":"markdown","2aae90a4":"markdown","80045304":"markdown","34ce8e45":"markdown","bc9e7bf1":"markdown","69fadb36":"markdown","1adfad01":"markdown","fc2d265c":"markdown"},"source":{"bbe4ff42":"import os, sys, logging, random, time\nfrom math import ceil\nimport itertools as it\nfrom collections import OrderedDict, Iterable\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import Perceptron, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\nprint(os.listdir(\"..\/input\"))\nPATH = \"..\/input\/\"","635c091d":"# logger = logging.getLogger()\n# logger.setLevel(logging.CRITICAL)\nl = logging.INFO\nlogging.basicConfig(level=l, format=\"%(levelname)s : %(message)s\")\ninfo = logging.info\n\n# import warnings\n# warnings.filterwarnings('ignore')","578fdbe8":"PROJECT     = \"Kaggle-Titanic_Machine_Learning_From_Disaster\"\nDATA        = \"data\"\nSUBMISSIONS = \"submissions\"\nTRAIN_FILE  = \"train.csv\"\nTEST_FILE   = \"test.csv\"\nN_1         = 20\nN_2         = 10\nN_3         = 30\nN_4         = 50 \nTEST_SIZE   = 25\nCV          = 5","3f450a11":"%matplotlib inline\n# get_ipython().magic('matplotlib inline')\nsns.set()","99f44c64":"################################################################################\n\n# ----------------------------------------------------------\n# 00-first_dataset_tour.py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# Find here a first study of the dataset, in which we try to understand and\n# give meaning to the dataset.\n\n# We are not trying to solve our problem but to be focused on visualization,\n# clenaning and all feature engineering improvements.\n\n# At first we will just study the corelations, the links, the quality and the\n# meaning of our dataset. External research and more general considerations may \n# be included in this work","6470811f":"# our first function designed to init a dataframe form a file\n\ndef init_df(path, file, precast=False) : \n\n    #\u00a0init dataframe\n    df = pd.read_csv(path+file, index_col=0)\n\n    #\u00a0if train df\n    if len(df.columns)  == 11 : \n        df.columns  = pd.Index( [   \"target\", \"pclass\",\"name\", \"sex\", \"age\",\n                                    \"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\n                                    \"embarked\"], dtype=\"object\")\n    #\u00a0if test df \n    elif len(df.columns )  == 10 : \n        df.columns  = pd.Index( [   \"pclass\",\"name\", \"sex\", \"age\",\n                                    \"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\n                                    \"embarked\"], dtype=\"object\")\n    else : \n        raise ValueError(\"invalid numb of columns\")\n\n    #\u00a0if needed, change sex and embarled feature in int dtype\n    if precast : \n        sex_dict        = {\"male\":1, \"female\":0}\n        embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n\n        df[\"sex\"]        = df.sex.map(sex_dict)\n        df[\"embarked\"]   = df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n\n    return df\n\n####\n\n#\u00a0train and test\ntrain_df = init_df(PATH, TRAIN_FILE)\ntest_df = init_df(PATH, TEST_FILE)\n\n#\u00a0for nas : concat train and test df\nboth_df = train_df.copy().append(test_df)\n\n#\u00a0we will work on train to have target feature\ndf = train_df.copy()\n\n#\u00a0it could be good to keep a copy of our original df\nDF = df.copy()\n\ndf.head()","880d550e":"#\u00a0let's have a first tour of our dataframe with some old print functions\ndef study_global_df(df) :     \n    print(\"data frame dimension :       \")\n    print(df.ndim)\n    print(\"\\n\\ndata frame shape :       \")\n    print(df.shape)\n    print(\"\\n\\ndata frame types :      \")\n    print(df.dtypes)\n    print(\"\\n\\ndata frame index :       \") \n    print(df.index)\n    print(\"\\n\\ndata frame columns :     \")\n    print(df.columns)\n    print(\"\\n\\ndata frame info :     \")\n    print(df.info())\n\n\n    \n####\n\nstudy_global_df(df)","17a57077":"df.describe()","bbfda58c":"df.head()","79497e7d":"#\u00a0visualisation\ndef visualize_global(df) : \n    df.hist(grid=True,bins=50, figsize=(10,10))\n    #\u00a0sns.pairplot(df)\n\n    \n####\n\nvisualize_global(df)","0805b5a0":"# in order to have sex and embarked \ndef precast_df(df) : \n        #\u00a0DO NOT FORGET TO MAKE A COPY :):)\n        _df = df.copy()\n        sex_dict        = {\"male\":1, \"female\":0}\n        embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n\n        _df[\"sex\"]        = _df.sex.map(sex_dict)\n        _df[\"embarked\"]   = _df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n\n        return _df        \n    \n####\n\n_df = precast_df(df)\nvisualize_global(_df)","83a3ca27":"df.head()","a3b2ee28":"# pclass, sex, and embarqued are definitively categorcial features, so let's work on this\ndef visualize_global_cat(df) : \n    for feat in [\"embarked\", \"sex\", \"pclass\"] : \n        sns.factorplot(feat,'target', data=df,size=4,aspect=3)\n    \n    #\u00a0other way : \n    # fig, (ax1,ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n    # for ax, feat in zip((ax1,ax2, ax3), [\"embarked\", \"sex\", \"pclass\"]) :\n    #     data = df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    #     sns.barplot(x=feat, y='target', data=data, size=4,aspect=3, ax=ax)\n\n####\n\nvisualize_global_cat(df)","728f7679":"# can we learn something with continuous features ?\ndef visualize_global_continuous(df) : \n    for feat in [\"age\", \"fare\", \"sibsp\", \"parch\" ] : \n        data = pd.concat([pd.cut(df[feat], 11, labels=range(11)), df[\"target\"]], axis=1)\n        #\u00a0data.columns = [feat, \"target\"]\n        sns.factorplot(feat, \"target\", data=data, size=4,aspect=3)\n\n####\n\nvisualize_global_continuous(df)","17f37ad0":"#\u00a0let's go deeper and visualize our categorical features in depth\ndef visualize_depth_cat_1(df) : \n    for feat in [\"embarked\", \"sex\", \"pclass\"] : \n        fig, axs = plt.subplots(1,3,figsize=(15,5))\n        sns.countplot(x=feat, data=df, ax=axs[0])\n        sns.countplot(x='target', hue=feat, data=df, order=[1,0], ax=axs[1])\n        data = df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n        sns.barplot(x=feat, y='target', data=data,ax=axs[2])\n\n####\n\nvisualize_depth_cat_1(df)","3803e6e0":"def visualize_depth_cat_2(df) :\n    for feat in [\"sex\", \"pclass\", \"embarked\"] : \n        sns.factorplot(\"target\", col=feat, col_wrap=4,\n                    data=df, kind=\"count\", size=3.5, aspect=.8)\n        \n####\n\nvisualize_depth_cat_2(df)","06246d27":"def visualize_depth_continuous_2(df) : \n    for feat in  [\"fare\", \"age\", \"sibsp\", \"parch\"] : \n        facet = sns.FacetGrid(df, hue=\"target\",aspect=4)\n        facet.map(sns.kdeplot,feat,shade= True)\n        facet.set(xlim=(0, df[feat].max()))\n        facet.add_legend()\n\n####\n\nvisualize_depth_continuous_2(df)      ","4f79c600":"def just_another_fancy_graph_1(df) : \n    fig, axs = plt.subplots(1,3, figsize = (20,5))\n    feats = [\"embarked\", \"sex\", \"pclass\"]\n\n    for feat, ax in zip(feats, axs) : \n        for f in df[feat].unique() : \n            age = df[~df[\"age\"].isnull()]\n            age = age[age[feat] == f]\n            sns.distplot(age[\"age\"], bins=50, ax=ax)\n            plt.legend(df[feat].unique(),loc='best')\n            plt.title(feat)\n\n####\n\njust_another_fancy_graph_1(df)","8180ff93":"def just_another_fancy_graph_2(df) :\n    g = sns.FacetGrid(df, col=\"sex\", row=\"target\", margin_titles=True)\n    g.map(plt.hist, \"age\",color=\"purple\");\n\n####\n\njust_another_fancy_graph_2(df)","688d97e6":"def just_another_fancy_graph_3(df) :\n    fig, ax = plt.subplots(1,1, figsize=(20,5))\n    sns.boxplot(x=\"embarked\", y=\"age\", hue=\"pclass\", data=df, ax=ax);\n\n####\n\njust_another_fancy_graph_3(df)","0ede6f67":"#\u00a0finally let's see the correlation matrix\n\ndef graph_corr_matrix(df) : \n    \n    fig, ax = plt.subplots(1,1,figsize=(15,15))\n    corr_mat = df.corr()\n    sns.heatmap(corr_mat, cmap=\"coolwarm\", annot=True, fmt='.3g', ax=ax)\n    plt.title(\"correlation matrix\")\n    \n####\n\ngraph_corr_matrix(df)","3258faaf":"def study_nas(df): \n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False).round(3)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n####\n\nstudy_nas(both_df)","122d6d40":"df.head()","f8659fb7":"# 2 nas for embarked and 1 for fare, easy...\n\ndef fill_embarked(df) :\n    _df = df.copy()\n    _df.loc[_df.embarked.isna(), : ]\n    try : \n        _embarked = _df.embarked.value_counts().sort_values(ascending=False).index[0]\n        _df[\"embarked\"] = _df.embarked.fillna(_embarked)\n    except: \n        pass\n    return _df\n\ndef fill_fare(df) : \n    _df = df.copy()\n    _pclass =  int(_df.loc[_df.fare.isna(),\"pclass\"].values)\n    try : \n        val = _df.loc[_df.pclass == _pclass, \"fare\"].median()\n        _df[\"fare\"] = _df.fare.fillna(val)\n    except : \n        pass\n    return _df\n\n####\n\n_both_df = fill_embarked(fill_fare(both_df))\nstudy_nas(_both_df)","bb252eb4":"# but for age it is a much more complex problem! \n# we 20% of nas and know due to obvious logic and due to our data visualization that this a \n#\u00a0very important feature\n# first thing to do could be a statistical fillin strategy based of pclass, sex and embarked\n\ndef fill_age_easy(df) : \n    _df = df.copy()\n    \n    idxs = _df.loc[_df[\"age\"].isna(), : ].index\n\n    #\u00a0using 'for loop' with df is (strongly)  recommanded\n    #\u00a0this 'for  loop' is just here to increase code readability \n    for i in idxs : \n        pers = _df.loc[i, :]\n\n        mask_1 = _df[\"pclass\"]    == pers[\"pclass\"]\n        mask_2 = _df[\"embarked\"]  == pers[\"embarked\"]\n        mask_3 = _df[\"sex\"]       == pers[\"sex\"]\n\n        mask = mask_1 & mask_2 & mask_3\n        sub_df = _df.loc[mask, :]\n\n        if len(sub_df) > 100 : \n            age_mean = sub_df.age.mean()\n            age_std = sub_df.age.std()\n        \n        else : \n            mask = mask_1 & mask_3\n            sub_df = _df.loc[mask, :]\n\n            if len(sub_df) > 100 : \n                age_mean = sub_df.age.mean()\n                age_std = sub_df.age.std()\n\n            else : \n                mask = mask_1 \n                sub_df = _df.loc[mask, :]\n                age_mean = sub_df.age.mean()\n                age_std = sub_df.age.std()\n\n        # define a random age based on the specific norma distr of our filtered samples\n        age = np.random.randint(age_mean - age_std, age_mean + age_std)\n        _df.loc[i, \"age\"] = int(age)\n\n    return _df","1aa97503":"#\u00a0a much more complex way to fill age - but maybe much more\n#\u00a0correct - could to use a ML algo to predict good values of age\n#\u00a0we will not implement this right now but it could be something like this \n\ndef fill_age_hard(df) : \n\n    _df = df.copy()\n\n    # separate target and other features \n    features_list = [\"target\", \"cabin\", \"name\", \"ticket\", \"embarked\"]\n    droped_features = df.loc[:,features_list ]\n    _df = df.drop(features_list, axis=1)\n\n    # sep train and test df\n    _train_df = _df.loc[df[\"age\"].isna(), :]\n    _test_df  = _df.loc[~df[\"age\"].isna(), :]\n    print(_train_df.shape, _test_df.shape)\n\n    # split X_train, X_test...\n    X, y = _train_df.drop(\"age\", axis=1), _train_df.age\n    X_train, X_test, y_train, y_test = train_test_split(X,y)\n    print([i.shape for i in (X_train, X_test, y_train, y_test) ])\n\n    model_list = [LogisticRegression]\n    for m in model_list : \n        grid = GridSearchCV(m(), {}, cv=5)\n        grid.fit(X_train, y_train)\n        acc = accuracy_score(grid.predict(X_test), y_test)\n        print(acc)\n\n    pred_ages = grid.predict(_test_df).astype(np.uint32)\n    _test_df[\"age\"] =  pred_ages\n\n\n    # merge df with droped features \n    for f in droped_features.columns : \n        _df[f] = droped_features[f] \n\n    return _df\n","f44a70b9":"#\u00a0finally we can define our fill_nas function, keeping in mind that our fill_age could be strongly improved\n#\u00a0and that we should have our train df and our test df in args\n\ndef fill_nas(df, df2=None,  \n                 embarked_meth=fill_embarked, \n                 fare_meth=fill_fare,\n                 age_meth=fill_age_easy) :\n\n    #\u00a0merge 2 df if needed\n    if isinstance(df2, Iterable) : \n        idx_1 = df.index\n        idx_2 = df2.index\n        _df = df.copy().append(df2)\n    else : \n        _df = df.copy()\n        \n    #\u00a0 fill embarked, age, and fare\n    _df = embarked_meth(_df)\n    _df = fare_meth(_df)\n    _df = age_meth(_df)\n    \n    #\u00a0if needed re-split train_df and test_df\n    if isinstance(df2, Iterable) : \n        df_1 = _df.loc[idx_1, :]\n        df_2 = _df.loc[idx_2, :]\n        return df_1, df_2\n    else :\n        return _df\n####\n\n#\u00a0please not the 2 examples are the same\n_train_df, _test_df =  fill_nas(train_df, test_df)\n_both_df = fill_nas(both_df)\n#\u00a0you just need to re-split both_df in train_df and test_df\n\ndf = _train_df.copy()\ndf.head()","d9df7231":"study_nas(df)","2067e926":"# how many non unique values for each feature?\ndef count_unique(df) : \n    data = pd.DataFrame([len(df[feat].unique()) for feat in df.columns], columns=[\"unique values\"], index=df.columns)\n    return data.sort_values(by=\"unique values\", ascending=False)\n\n####\n\ncount_unique(df)","52be47b6":"#\u00a0let's have a look to unique - non continuous- values\n\ndef study_unique(df) : \n\n    col = [i for i in df.columns if i not in (\"age\", \"ticket\", \"name\", \"cabin\", \"fare\")]\n    ser = pd.DataFrame( \n            [ (df[i].unique() if len(df[i].unique())<20 else \"too many values\", \n              df[i].dtype) for i in col], index=col, columns=[\"unique\", \"dtype\"])\n    return ser\n\n\n####\n\nstudy_unique(df)","8b1099d5":"def detect_outliers(df) : \n\n    #\u00a0see global fare info\n    print(df.describe().fare)\n\n    print(df.fare.sort_values(ascending=True).iloc[:15])\n\n    #\u00a0plot info\n    fig, axs = plt.subplots(2,1, figsize=(20, 5))\n    df.fare.hist(bins=100, ax=axs[0])\n    _df = df.loc[df.fare <=50.0, :]\n    _df.fare.hist(bins=20, ax=axs[1])\n\n    #\u00a0regul fare >250\n    print(df.fare.sort_values(ascending=False).iloc[:10])\n\n\n####\n\ndetect_outliers(both_df)","71008130":"#\u00a0handle outliers\ndef manage_fare_outliers(df, small=True, high=False) : \n    _df = df.copy()\n\n    if small : \n        #\u00a0regul fare == 0\n        idxs = _df.loc[df[\"fare\"]<1.0 , : ].index\n        \n        for i in idxs : \n            pers = _df.loc[i, :]\n            _pclass = int(pers.pclass)\n            sub_df = _df.loc[_df.pclass == _pclass, :]\n            fare_mean = sub_df.fare.mean()\n            fare_std = sub_df.fare.std()\n\n            _fare = np.random.randint(fare_mean - fare_std, fare_mean + fare_std)\n            _df.loc[i, \"fare\"] = int(_fare)\n\n    if high : \n        _df.loc[_df.fare >260, \"fare\"] = 260\n\n    return _df\n\n####\n\n_df = manage_fare_outliers(df)\n_df.loc[_df.fare == 0, :]","b30f31b9":"detect_outliers(_df)","aef566cc":"df.head()","8da511c6":"#\u00a0we now are going to group every people with the same ticket\n#\u00a0we can consider that there are together, as family or as goup of friends\n#\u00a0this is a very important information because if we are able to \"regroup\" families \/ freinds we could enhance \n#\u00a0significatively our level of information\n\ndef group_ticket(df, threshold=2, reg_fare=False) : \n    \n    _df = df.copy() \n\n    #\u00a0handle wierd tickets \n    _df.loc[df.ticket == \"LINE\", \"ticket\"] = 'LINE -1'\n      \n    _df[\"group_id\"] = np.nan\n    _df[\"group_count\"] = np.nan\n\n    #\u00a0sep nb and letters\n    _df[\"ticket_nb\"]  = _df.ticket.apply(lambda i : int(i) if \" \" not in i else int(i[i.rfind(\" \")+1 :]))\n    _df[\"ticket_let\"] = _df.ticket.apply(lambda i : \"Nan\" if \" \" not in i else i[ : i.rfind(\" \")])\n    # len(df[\"_ticket_nb\"]) == len(df[\"_ticket_nb\"].unique()) \n    # ticket nb non unique!!! snif snif grrr grrrrr\n    \n    #\u00a0we need to simplify this feature\n    print(_df.ticket_let.unique().sort())\n    _df[\"ticket_let\"] = _df.ticket_let.apply(lambda i : i.replace(\".\", \"\"))\n\n    ticket_dict = { 'A\/4'          : \"A4\",\n                    'A\/5'          : \"A5\",\n                    'A\/S'          : \"AS\",\n                    'A4'           : \"A4\",\n                    'A5'           : \"A5\",\n                    'C'            : \"C\",\n                    'CA'           : \"CA\",\n                    'CA\/SOTON'     : \"CA\",\n                    'FC'           : \"FC\",\n                    'FCC'          : \"FCC\",\n                    'Fa'           : \"FA\",\n                    'LINE'         : \"LINE\",\n                    'Nan'          : np.nan,\n                    'P\/PP'         : \"PP\",\n                    'PC'           : \"PC\",\n                    'PP'           : \"PP\",\n                    'SC'           : \"SC\",\n                    'SC\/A4'        : \"A4\",\n                    'SC\/AH'        : \"AH\",\n                    'SC\/AH Basle'  : \"Basle\",\n                    'SC\/PARIS'     : \"PARIS\",\n                    'SC\/Paris'     : \"PARIS\",\n                    'SCO\/W'        : \"SCO\",\n                    'SO\/C'         : \"SOC\",\n                    'SO\/PP'        : \"PP\",\n                    'SOC'          : \"SOC\",\n                    'SOP'          : \"SOP\",\n                    'SOTON\/O2'     : \"STON\",\n                    'SOTON\/OQ'     : \"STON\",\n                    'SP'           : \"SP\",\n                    'STON\/O 2'     : \"STON\",\n                    'STON\/O2'      : \"STON\",\n                    'SW\/PP'        : \"PP\",\n                    'W\/C'          : \"WC\",\n                    'WE\/P'         : \"WEP\",\n                    'WEP'          : \"WEP\"}\n\n    _df[\"ticket_let\"] = _df.ticket_let.map(ticket_dict)\n\n    #\u00a0how many tickets per pers\n    group_count = _df.ticket.value_counts().to_dict(ticket_dict)\n    _group_count = [(i,j) for i,j in group_count.items() if j>=threshold]\n    _group_count.sort(key=lambda i : i[1], reverse=True)\n\n    #\u00a0add group id and group count\n    for i, tup in enumerate(_group_count) : \n        t, c = tup\n        idxs = _df.loc[df.ticket == t, :].index\n        _df.loc[idxs,\"group_id\"] = i\n        _df.loc[idxs,\"group_count\"] = c\n\n    #\u00a0regularize fare if needed\n    if reg_fare : \n        idxs = _df.loc[~_df.group_count.isna(), :].index        \n        _df.loc[idxs, \"fare\"] =  _df.loc[idxs, \"fare\"] \/ _df.loc[idxs, \"group_count\"] \n\n    # fill na group_count = 1\n    _df[\"group_count\"] = _df.group_count.fillna(1)\n    \n    _df = _df.drop(\"ticket\", axis=1)\n    \n    return _df\n\n####\n\n_df = group_ticket(df)\n_df.ticket_let.value_counts(dropna=False)","eb3c95d1":"print(DF.ticket.head())\n_df.head()","ea76758d":"_df.loc[_df.group_count.isna(), : ]","3e905ea3":"sub_df = df.ticket.value_counts().sort_values(ascending=False)\npd.DataFrame(sub_df[sub_df>2], columns=[\"ticket\"])","b44d752c":"def vizualize_group_count_1(df) : \n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"group_count\"\n    sns.countplot(x=feat, data=df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=df, order=[1,0], ax=axis2)\n    data = df[[feat, \"target\"]].groupby(df[feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', order=sorted(df.group_count.unique()), data=data,ax=axis3)\n    \n####\nprint(_df.columns)\nvizualize_group_count_1(_df) \n","2b0bd3b2":"def vizualize_group_count_2(df): \n    feat = \"group_count\"\n    facet = sns.FacetGrid(df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvizualize_group_count_2(_df)","4e4ab5fc":"def vizualize_group_count_3(df): \n    feat= \"group_count\"\n    g = sns.factorplot(\"target\", col=feat, col_wrap=4,\n                        data=df[df[feat].notnull()],\n                        kind=\"count\", size=2.5, aspect=.8)\n    \n####\n\nvizualize_group_count_3(_df)","32ba0749":"#\u00a0what about names? \n#\u00a0obiouvsly we can separate first name, last name and name title\n\ndef sep_names(df) :\n    _df = df.copy()\n    \n    #\u00a0create a tmp feat\n    _df[\"name_len\"] = _df.name.apply(lambda i : len(i))\n    _df[\"_name\"]    = _df.name.apply(lambda i : i.split(\", \"))\n    _df[\"_name\"]    = _df[\"_name\"].apply(lambda i : [    i[0],\n                                                    i[1][: i[1].find(\".\")], \n                                                    i[1][i[1].find(\" \"):]   ]   )\n\n    #\u00a0split last, first, title\n    _df[\"name_last\"]     = _df[\"_name\"].apply(lambda i : i[0])\n    _df[\"title\"]         = _df[\"_name\"].apply(lambda i : i[1])\n    _df[\"name_first\"]    = _df[\"_name\"].apply(lambda i : i[2])\n\n    # countness ?\n    idx = _df.loc[_df.title == \"the Countess\", :].index\n    _df.loc[idx, \"title\"] = \"Countess\"\n    _df.loc[idx, \"name_first\"] = \"Lucy Noel Martha Dyer-Edwards\"\n\n    #\u00a0sep spouce\/second name : \n    _df[\"name_second\"]   = _df.name_first.apply(lambda i : i[i.find(\"(\")+1 : ] if \"(\" in i else np.nan)\n    _df[\"name_first\"]    = _df.name_first.apply(lambda i : i[: i.find(\" (\")] if \"(\" in i else i)\n\n    #\u00a0#################################   \n    # df[\"name_last_count\"] = np.nan\n    ###################################   \n    \n    #\u00a0clean :\n    items = [\"name_first\", \"name_second\", \"name_last\"]\n    def clean(i) : \n        try     : return i.replace(\")\", \"\").replace('\"\"', \"\").replace('\"', \"\").replace(\"'\", '')\n        except  : return i\n\n    for item in items : \n        _df[item] = _df[item].apply(lambda i : clean(i))\n        #\u00a0more pandastic \n        #\u00a0_df[item] = _df[item].map(clean)\n        \n    _df = _df.drop([\"name\", \"_name\"], axis= 1)\n\n    return _df\n\n####\n\n_df = sep_names(df)\n_df.head()","4178ba37":"# 20th most poular names\ndef most_popular_names(df) : \n    _df = sep_names(df)\n    ser = _df.name_last.value_counts().sort_values(ascending=False)[:20]\n    return pd.DataFrame({\"20th most poular names\":ser})\n\n####\n\nmost_popular_names(both_df)","29ee1e9e":"def visualize_name_len_1(df) : \n    _df = sep_names(df)\n    feat = \"name_len\"\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvisualize_name_len_1(df)","b29f86f6":"def visualize_name_count(df) : \n    _df = sep_names(df)\n    feat = \"name_count\"\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n#_df = group_ticket(df)\n#\u00a0visualize_name_count(_df)","3c8ad824":"#\u00a0titles\ndef study_titles(df) : \n    _df = sep_names(df)\n    return pd.DataFrame({\"title_freq\": _df.title.value_counts(normalize=True).round(2), \"title_nb\": _df.title.value_counts(),})\n\n####\n\nstudy_titles(df)","3893190d":"#\u00a0second names?\ndef study_second_names(df) : \n    _df = sep_names(df)\n    fem = pd.Series(_df.loc[_df.sex == \"female\", \"name_second\"].isna().value_counts(), name=\"female\")\n    mal = pd.Series(_df.loc[_df.sex == \"male\", \"name_second\"].isna().value_counts(), name=\"male\")\n    print(\"name_second count by sex\")\n    return pd.DataFrame(dict(male=mal, female=fem))\n\n####\n\nstudy_second_names(df)","fcb0e1c0":"#\u00a0with name, let's build a fancy name, easy to read\ndef fancy_names(df) :  \n    _df = df.copy()\n    names = list()\n    for i in df.index :\n        second = \"\" if _df.loc[i, \"name_second\"] is np.nan else str(_df.loc[i, \"name_second\"]) \n        txt =  _df.loc[i, \"title\" ] + \" \"+ _df.loc[i, \"name_first\"] + \" \" + second + \" \" + _df.loc[i, \"name_last\"]\n        txt = txt.replace(\"  \", \" \").replace('\"', \"\").replace(\"'\", \"\")\n        names.append(txt)\n    _df[\"fancy_name\"] = names\n    return _df\n\n####\n\n_df = fancy_names(sep_names(df))\npd.DataFrame(dict(fancy_name= _df.sort_values(by=\"name_len\", ascending=False).fancy_name.iloc[:20]))","03bbe298":"# let's regroup titles in a smaller list\ndef group_title(df) : \n\n    _df = df.copy()\n    \n    # we do not touch to Mr, Mrs, Miss, Master, Rev, Dr\n    group_dict_1 = {    \"Don\"       : \"Nobl\",  \n                        \"Mme\"       : \"Mrs\",\n                        \"Ms\"        : \"Miss\",\n                        \"Major\"     : \"Mil\",\n                        \"Lady\"      : \"Nobl\",\n                        \"Sir\"       : \"Nobl\",\n                        \"Mlle\"      : \"Miss\",\n                        \"Col\"       : \"Mil\",\n                        \"Capt\"      : \"Mil\",\n                        \"Dona\"      : \"Nobl\",\n                        \"Countess\"  : \"Nobl\",\n                        \"Jonkheer\"  : \"Nobl\",  }\n    \n    group_dict_2 = {\"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"Countess\":   \"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    }\n\n    group_dict = group_dict_2\n    _df[\"title\"] = _df.title.apply(lambda i : group_dict[i] if i in group_dict.keys() else i )\n\n    return _df  \n\n####\n\n_df = group_title(sep_names(df))\n_df.title.value_counts()","69068022":"def visualize_titles_1(df):\n    _df = group_title(sep_names(df))\n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"title\"\n\n    sns.countplot(x=feat, data=_df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=_df, order=[1,0], ax=axis2)\n    data = _df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', data=data,ax=axis3)\n\n####\n\nvisualize_titles_1(df)","69d9f751":"def visualize_titles_2(df):\n    feat = \"title\"\n    _df = group_title(sep_names(df))\n    g = sns.factorplot(\"target\", col=feat, col_wrap=4,\n                    data=_df[_df[feat].notnull()],\n                    kind=\"count\", size=2.5, aspect=.8)\n####\n\nvisualize_titles_2(df)","3e89b623":"#\u00a0we now have a tricky topic \n# what about people who are together, a family for ex, but with various tickets nb\/id?\n#\u00a0we need to tackle this problem, let's do this...\n\ndef group_name(df, threshold=2) : \n\n    try     : \n        _df = sep_names(group_ticket(df))\n    except : \n        _df = df.copy()\n\n    #\u00a0add company\n    _df[\"company\"] = _df.parch + _df.sibsp\n\n    #\u00a0add a specific mask\n    name_count = _df.name_last.value_counts().to_dict()\n    _df[\"name_count\"] = _df.name_last.apply(lambda i : name_count[i])\n    mask = (_df.name_count >1) & ((_df.parch +_df.sibsp) >= threshold)\n    sub_df = _df.loc[mask, :].copy()\n    sub_df.sort_values(by=[\"name_count\", \"name_last\", \"pclass\", \"embarked\", \"age\", \"group_id\"], ascending=False, inplace=True)\n    names_list = sub_df.name_last.unique()\n\n    #\u00a0identify wierd \/ fake family\n    manual_check = list()\n    for i, n in enumerate(names_list) : \n\n        fam = _df.loc[_df.name_last == n , :]\n        idxs = fam.index\n        nb = len(fam)\n\n        is_wierd = True if (   (len(fam.embarked.unique()) != 1) \n                            or (len(fam.pclass.unique())   != 1)\n                            or (len(fam.group_id.unique())   != 1) \n                            or (len(fam.company.unique())  != 1) \n                            or (((fam.company.sum() + nb) \/ nb) != nb )) else False\n        if is_wierd :  manual_check.append(idxs)\n\n    #\u00a0uncomment if you want to print it \n    # for idxs in manual_check : \n    #     sub_df = _df.loc[idxs, :]\n    #     sub_df = sub_df.sort_values(by=[\"name_last\", \"group_id\", \"company\", \"pclass\", \"embarked\", \"age\", \"sex\", ], ascending=False)\n    #     print(sub_df.loc[:, [  \"name_last\", \"name_first\", \"name_second\", \n    #                         \"pclass\", \"embarked\", \"age\",\"sex\",\n    #                         \"cabin_nb\", \"cabin_let\", \"parch\", \"sibsp\", \"company\", \"group_id\"]])\n    #     input()\n    #     print(50 * \"\\n\")\n\n    #\u00a0manualy correct group id \n    try : \n        _df.loc[  1079, \"group_id\"] = 40.0\n        _df.loc[  1025, \"group_id\"] = 211.0\n        _df.loc[ [39, 334], \"group_id\"] = 163.0\n        _df.loc[  530, \"group_id\"] = 92.0\n        _df.loc[ [705, 624], \"group_id\"]  = 209.0\n        _df.loc[ [393,105], \"group_id\"]  = 1000.0\n        _df.loc[ [353, 533, 1229, 774], \"group_id\"] = 1001.0\n        _df.loc[  176, \"group_id\"] = 201.0\n        _df.loc[  1296, \"group_id\"] = 153.0\n        _df.loc[  1197,\"group_id\"] = 149.0\n        _df.loc[  594,\"group_id\"] = 214.0\n        _df.loc[ [1268, 70],\"group_id\"] = 1002.0\n    except: \n        print(\"both_df expected, method failed\")\n\n    #\u00a0drop useless features\n    _df = _df.drop([\"name_count\", \"name_first\", \"name_second\", \"name_last\"], axis=1)\n\n    return df\n\n####\n\n_df = group_name(both_df)\n_df.head()","d9f5c7d1":"#\u00a0finally we have so separate cabin number and cabin letter\n\ndef sep_cabins(df) : \n\n    _df = df.copy()\n    #\u00a0replace cabin by list of str if various cabs for on pers\n    _df[\"_cabin\"]    = _df.cabin.apply(lambda i : [str(i), ])\n    _df[\"_cabin\"]    = _df._cabin.apply(lambda i : i[0].split(\" \"))\n    _df[\"_cabin\"]    = _df._cabin.apply(lambda i : sorted(i))\n\n    #\u00a0count how many cabs by pers\n    _df[\"cabin_count\"]  = _df[\"_cabin\"].apply(lambda i : len(i) if i[0] != \"nan\" else 0)\n\n    #\u00a0drop various cabs and take dirst cab for every people \n    #\u00a0the split letter (deck ? ) from numb\n    _df[\"_cabin\"]    = _df[\"_cabin\"].apply(lambda i : i[0]) \n    _df[\"cabin_let\"] = _df[\"_cabin\"].apply(lambda i : i[0] if i != \"nan\" else np.nan) \n    _df[\"cabin_nb\"]  = _df[\"_cabin\"].apply(lambda i : int(i[1:]) if ((i != \"nan\") and (len(i)>1)) else np.nan) \n\n    #\u00a0encode letter (deck) as an int\n    cabin_let_list  = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\"]\n    cabin_let_dict  = {j:i for i,j in enumerate(cabin_let_list)}\n    _df[\"cabin_let\"] = _df[\"cabin_let\"].apply(lambda i :cabin_let_dict[i] if i is not np.nan else np.nan)\n\n    #\u00a0drop useless features\n    _df = _df.drop([\"cabin\", \"_cabin\"], axis=1, inplace=False)\n\n    return _df\n\n_df = sep_cabins(both_df)\n_df.head()","3a7895c0":"def visualize_cabins_1(df) : \n    _df = sep_cabins(df)\n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"cabin_let\"\n\n    sns.countplot(x=feat, data=_df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=_df, order=[1,0], ax=axis2)\n    data = _df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', order=sorted(_df[feat].unique()), data=data,ax=axis3)\n\n####\n\nvisualize_cabins_1(df)","ae9d9800":"def visualize_cabins_2(df) : \n    _df = sep_cabins(df)\n    feat = \"cabin_let\"\n\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvisualize_cabins_2(df)","4b633eb9":"#\u00a0Not for now, but find bellow some very useful features we will be happy to find to upgrade accuracy score to 86+%\n\ndef is_alone(df) : \n    return None\n\ndef din_not_pay_histicket(df) : \n    return None\n\ndef is_the_dominant_of_the_family(df) : \n    return None\n\ndef is_child(df):\n    return None\n\ndef is_child_with_mother_survivor(df) : \n    return None\n\ndef is_man_just_with_his_wife(df) : \n    return None\n\ndef is_wife_with_men_survivor(df):\n    return None\n\ndef is_child_with_brother_sister_survivor(df):\n    return df","a6f68564":"#\u00a0of course we also can separate age into categories : is_baby (-3), is_little_child (3-7), is_child (7-12), is_pre_ado (12-14), is_ado (15-18), is_young (18-25) is_adult_1 (25-35) \n#\u00a0is_adult_2 (35-45), is_adult_3 (45-55), is pre_old (55-65), is old (65-75) and is_very_old (75-200)...","858c1a0e":"# in order to have a good feature engineering strategy, we have to enhance our dataframe with dummy features\n\ndef add_noises(df) : \n    df[\"n_noise\"] = np.random.randn(len(df))\n    df[\"u_noise\"] = np.random.rand(len(df))\n\n    return df \n\n####\n\n_df = add_noises(df)\n_df.head()","6f3af8e8":"#\u00a0remember to retype all num?\ndef retype_all_num(df) : \n    _df = df.copy()\n    sex_dict        = {\"male\":1, \"female\":0}\n    embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n    _df[\"sex\"]        = _df.sex.map(sex_dict)\n    _df[\"embarked\"]   = _df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n    return _df\n","4fc281c4":"# it will be also helpfull to retype all our features into categorical values\n\ndef retype_all_cat_1(df) :  \n    \n    # add name_count, group_count, name_len, cabin_count\n    # is_alone, is_with_survivor, is_dominant_male\n    \n    # is_baby (-3), is_little_child (3 years - 7) is child (7-12), is_preado(12 - 14) is_ado(15-18), is_young(18-25) is_adult1(25-35) is_adult1(35-45) is_adult1(45-55)\n    #  is pre_old(55-65) is old(65-75) very old(75-200)\n    \n    _df = df.copy()\n    \n    #\u00a0equivalent to hot encoding \/ dummy encoding\n    features_list = (\"cabin_nb\", \"cabin_let\", \"group_id\", \"group_count\")\n    features_list = [(\"as_\"+i, i) for i in features_list]\n    \n    for new, old in features_list : \n        _df[new] = 1\n        idxs = _df.loc[_df[old].isna(), :].index\n        df.loc[idxs, new] = 0\n\n    for new, old in features_list : \n        cat          = pd.Categorical(df[new].unique())\n        df[new]      = df[new].astype(cat)\n\n    #\u00a0fill nas\n    df[\"cabin_nb\"]      = df[\"cabin_nb\"].fillna(-1)\n    df[\"cabin_let\"]     = df[\"cabin_let\"].fillna(-1)\n    df[\"group_id\"]      = df[\"group_id\"].fillna(1234)\n    df[\"group_count\"]   = df[\"group_count\"].fillna(0)\n\n    #\u00a0numerize str features\n    group_dict = {j:i for i, j in enumerate(df.title.unique())} \n    df[\"title\"] = df.title.apply(lambda i : group_dict[i] if i in group_dict.keys() else i )                       \n    \n    sex_dict = {j:i for i, j in enumerate(df.sex.unique())} \n    df[\"sex\"] = df.sex.apply(lambda i : sex_dict[i] if i in sex_dict.keys() else i )                       \n\n    embarked_dict = {j:i for i, j in enumerate(df.embarked.unique())}\n    df[\"embarked\"] = df.embarked.apply(lambda i : embarked_dict[i] if i in embarked_dict.keys() else i )                       \n\n    #\u00a0columns\n    columns = df.columns\n\n    #\u00a0categories\n    # bool_cat          = pd.Categorical([0,1])\n    pclass_cat          = pd.Categorical(df.pclass.unique(), ordered=True) \n    sex_cat             = pd.Categorical(df.sex.unique())\n    embarked_cat        = pd.Categorical(df.embarked.unique())\n    group_count_cat     = pd.Categorical(df.group_count.unique(), ordered=True)\n    title_cat           = pd.Categorical(df.title.unique())\n    #\u00a0cabin_count_cat     = pd.Categorical(df.cabin_count.unique(), ordered=True)\n    cabin_let_cat       = pd.Categorical(df.cabin_let.unique(), ordered=True)\n    parch_cat           = pd.Categorical(df.parch.unique(), ordered=True)\n    sibsp_cat           = pd.Categorical(df.sibsp.unique(), ordered=True)\n    company_cat         = pd.Categorical(df.company.unique(), ordered=True)\n    group_id_cat        = pd.Categorical(df.group_id.unique())\n\n    #\u00a0discrete\n    if \"pclass\" in columns : \n        df['pclass']        = df['pclass'].astype(pclass_cat)\n    if \"sex\" in columns : \n        df['sex']           = df['sex'].astype(sex_cat)\n    if \"embarked\" in columns : \n        df['embarked']      = df['embarked'].astype(embarked_cat)\n    if \"group_count\" in columns :     \n        df['group_count']   = df['group_count'].astype(group_count_cat)\n    if \"title\" in columns :     \n        df['title']         = df['title'].astype(title_cat)\n    #\u00a0if \"cabin_count\" in columns :     \n    #\u00a0   df['cabin_count']   = df['cabin_count'].astype(cabin_count_cat)\n    if \"cabin_let\" in columns :    \n        df['cabin_let']     = df['cabin_let'].astype(cabin_let_cat)\n    if \"parch\" in columns :  \n        df['parch']         = df['parch'].astype(parch_cat)\n    if \"sibsp\" in columns :      \n        df['sibsp']         = df['sibsp'].astype(sibsp_cat)\n    if \"company\" in columns :      \n        df['company']       = df['company'].astype(company_cat)\n    if \"group_id\" in columns :  \n        df['group_id']      = df['group_id'].astype(group_id_cat)\n\n    #\u00a0continous\n    if \"age\" in columns :  \n        df['age']           = pd.cut(df.age,12, labels=range(12))\n    if \"fare\" in columns :  \n        df['fare']          = pd.cut(df.fare,12, labels=range(12))\n    #\u00a0if \"cabin_nb\" in columns :  \n    #\u00a0   df['cabin_nb']      = pd.cut(df.cabin_nb,12, labels=range(12))\n    if \"u_noise\" in columns :  \n        df['u_noise']       = pd.cut(df.u_noise,11, labels=range(11))\n    if \"n_noise\" in columns :  \n        df['n_noise']       = pd.cut(df.n_noise,11, labels=range(11))\n\n    return df\n\n#\u00a0_df = retype_all_cat_1(group_name(df))\n#\u00a0_df.dtypes","e040850c":"def retype_all_cat2(df) :\n    pass\n\n    #####\n    \n    #\u00a0all in one hot dummy var\n    \n    #####","f84d181c":"#\u00a0let's now draw our first feature engineering startegy\n#\u00a0a very basic one, just for naive models\n\ndef num_feature_eng_1(path, train_file, test_file) : \n    \n    #\u00a0init 2 df\n    _train_df = init_df(PATH, TRAIN_FILE)\n    _test_df = init_df(PATH, TEST_FILE)\n    _train_idxs = train_df.index.copy()\n    _test_idxs = test_df.index.copy()\n    \n    #\u00a0merge them\n    _both_df = _train_df.copy().append(_test_df)\n    \n    #\u00a0handle them\n    _both_df = fill_nas(_both_df)\n    _both_df = manage_fare_outliers(_both_df)\n\n    # delete str or nas features\n    _both_df = _both_df.drop([\"cabin\", \"name\", \"ticket\"],axis=1)\n\n    #\u00a0all numerical\n    _both_df = retype_all_num(_both_df)\n    \n    #\u00a0re split \n    _train_df, _test_df = _both_df.loc[_train_idxs, :], _both_df.loc[_test_idxs, :]\n    \n    return _train_df, _test_df \n\n####\n\n# train and test\n_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\ndf.head()","44ba28be":"#\u00a0samething but with more complex strategy\n\ndef num_feature_eng_2(path, train_file, test_file) : \n    \n    #\u00a0init 2 df\n    _train_df = init_df(PATH, TRAIN_FILE)\n    _test_df = init_df(PATH, TEST_FILE)\n    _train_idxs = train_df.index.copy()\n    _test_idxs = test_df.index.copy()\n    \n    #\u00a0merge them\n    _both_df = _train_df.copy().append(_test_df)\n    _both_df = fill_nas(_both_df)\n    _both_df = manage_fare_outliers(_both_df)\n    \n    #\u00a0handle them\n    _both_df = group_ticket(_both_df)\n    _both_df = sep_names(_both_df)\n    _both_df = group_title(_both_df)\n    _both_df = group_name(_both_df)\n    _both_df = sep_cabins(_both_df)\n    \n    #\u00a0drop useless features\n    _both_df = _both_df.drop([\"group_id\", \"ticket_nb\", \"ticket_let\", \"name_last\", \"title\", \"name_first\",\"name_second\", \"cabin_nb\", \"cabin_let\"], axis=1)\n\n    # all numerical\n    _both_df = retype_all_num(_both_df)\n    \n    #\u00a0re split \n    _train_df, _test_df = _both_df.loc[_train_idxs, :], _both_df.loc[_test_idxs, :]\n    \n    return _train_df, _test_df \n\n####\n\n_train_df, _test_df = num_feature_eng_2(PATH, TRAIN_FILE, TEST_FILE)\ndf = _train_df.copy()\n_train_df.dtypes","afe371b3":"#\u00a0we now are building a much more complex feature engineering strategy\n\n####\n#\u00a0we will join train_df and test_df to apply best feat eng strategy and the we will split train_df and test_df\n####\n\ndef cat_feature_eng_1(train_df, test_df) : \n    train_index = train_df.index\n    test_index  = test_df.index\n\n    df = train_df.copy().append(test_df)\n\n    \"\"\"\n    df = fill_nas(df)\n    df = sep_names(df)\n    #\u00a0df = fancy_names(df)\n    df = sep_cabins(df)\n    df = group_title(df)\n    df = group_ticket(df)\n    df = group_name(df)\n    df = add_noises(df)\n    df = retype_all_cat(df)\n    \"\"\"\n    \n    train_df = df.loc[train_index, :]\n    test_df = df.loc[test_index, :]\n    \n    return train_df, test_df","588eb123":"#\u00a0_df, _ = cat_feature_eng_1(train_df, test_df)\n#\u00a0_df.head()","3b21cf8a":"def cat_feature_eng_2(df) : \n    \"\"\"\n    dzdzad\n    zdazdazd\n    azdzaazdza\n    dadazdzad  \n    \"\"\"\n    return df","7673a4b8":"################################################################################\n\n# ----------------------------------------------------------\n# 01-first_naive_models.py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this second part, we will implement our first logistic regression model.\n\n# We will first implement by hand a naive classifier, then a dummy classifier \n# (who does the same job), and finally a basic logistic regression model.\n\n# Rather than looking at the results of a regression we will implement a \n# function that will test the model x times and that will average the results\n# obtained\n","03e73042":"# split our features from our target\n\ndef return_X_y(df) : \n    if \"target\" in df.columns : \n        X = df.drop(\"target\", axis=1)\n        y = df.target\n        return X, y  \n    else : \n        return df \n    \n####\n\n_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())\nprint(N_1)\nprint(CV)\nprint(TEST_SIZE)","c49a86ec":"# split test and train df\/target\n\ndef split(X,y=None, size=0.33) :\n    if isinstance(y, Iterable) : \n        return train_test_split(X, y, test_size=size)\n    else : \n        X,y = return_X_y(X)\n        return train_test_split(X, y, test_size=size)\n\n####\n\nX_tr, X_te, y_tr, y_te = tup = split(X,y)\nX_train, X_test, y_train, y_test = X_tr, X_te, y_tr, y_te \nfor i in tup : print(i.shape)\n    \n","a14f5c28":"# rather than coding a dummy model from scratch, use sklearn DummyClassifier \n\ndef dummy_model(df, param=None) : \n\n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    model = DummyClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test) \n\n    acc = accuracy_score(y_test, y_pred).round(3)\n\n    return acc, model\n\n####\n\nacc, mod = dummy_model(df)\nprint(round(acc,4))\n\nser = pd.Series([dummy_model(df)[0] for i in range(N_2)])\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))\n","efa09f64":"# just for fun, trying to make predictions with a very basic model (no meta \n# params, no features engineering) this one will be our model prediction base\n# it is suposed to be better than our DummyClassifier. If not there is a major\n# issue...\n\ndef basic_model(df, param=None) : \n\n    X,y     = return_X_y(df)\n\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred).round(3)\n    \n    return acc, model\n\n####\n\nacc, mod = basic_model(df)\nprint(round(acc,4))\n\nser = pd.Series([basic_model(df)[0] for i in range(N_2)])\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))\n","cae96324":"# find here an high level function wich is charged of all basic tasks of a GridSearchCV\n\ndef GSCV_basic(model=None,  params=None, df=None) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n        \n    if not model   : model = LogisticRegression()\n    if not params  : params = dict() \n    try            : model = model()\n    except         : pass\n    \n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    grid = GridSearchCV(model, params, \n                        n_jobs=6, \n                        scoring=\"accuracy\",\n                        cv=10)\n\n    grid.fit(X_train, y_train)\n    y_pred = grid.predict(X_test)\n    acc = accuracy_score(y_test, y_pred).round(3)\n    \n    return acc, grid\n\n\n####\n\nacc, mod = GSCV_basic(df=df)\nser = pd.Series([GSCV_basic(df=df)[0] for i in range(N_2)])","f076adbf":"print(round(acc,4))\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))","1003a0d3":"#\u00a0we now are studying quickly wich model is the best\n\nCOLUMNS = [     \"LR\",       \"RC\",\n                \"SVC\",      # \"Nu\",\n                \"KNN\",\n                \"DT\",\n                \"RF\", \n                \"Ada\", #\u00a0\"Per\",      \n                \"MLP\"   ]\n\nMODELS = [      LogisticRegression, RidgeClassifier,\n                LinearSVC, # NuSVC,\n                KNeighborsClassifier,\n                DecisionTreeClassifier, \n                RandomForestClassifier,\n                AdaBoostClassifier, #\u00a0Perceptron, \n                MLPClassifier ]","b62bd19d":"#\u00a0as GSCV_basic function, we build a 'meta ML handler' (overkill you said???)\n\ndef parse_various_models(  n, df=None, params=None,\n                                    models = MODELS, columns= COLUMNS) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n\n    if len(models) != len(columns) : \n        raise ValueError(\"lens not goods\")\n\n    if not params : params = dict()    \n\n    results = [     pd.Series([GSCV_basic(m, df=df)[0] for m in models], \n                        index=columns) for i in range(n)]\n    \n    results = pd.DataFrame(results, columns=columns)\n\n    return results\n\n####\n\nresults = parse_various_models(N_2, df)","8a0cd561":"# print out raw values\nresults.iloc[:10, :]","1023bca8":"# lets have fancy representation of our results\n_results = results.describe().T.sort_values(by=\"50%\", ascending=False)\n_results","58c28b6f":"# graph it \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nresults.boxplot(ax=ax)\nplt.xlabel(\"models\")\nplt.ylabel(\"log_loss score\")\nplt.title(\"benchmark various models, without feat eng or meta params\")","cf930d42":"################################################################################\n\n# ----------------------------------------------------------\n# 02-playing_with_LR..py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this third part we will finally start to make real machine learning. We will first parse various feature engineering\n#\u00a0strategies helped by an other magic function\n\n# We will then benchmark the different classification models as well as the impact of the different meta \n# parametres on the relevance of the basic model: number of folds, preprocessing, scoring method, clip\n# of the predicted values, etc.\n\n# This work is clearly laborious, but its successful execution depends on our ability to really push \n# our model at best.\n","48b31fb2":"#\u00a0here we have various first considerations about feature engineering \n\ndef merge_age_sex(df, age=14) : \n    _df = df.copy() \n    _df[\"childness\"]  = _df.age.apply(lambda x : (0 if x > 17 else (1 if x > 14 else 2)))  \n    _df[\"status\"]     = _df.childness.apply(lambda x : 2 if x > 0 else -1)\n    mask = _df[\"status\"] == -1\n    _df.loc[mask, \"status\"] = (_df.loc[mask, \"sex\"] - 1).abs()\n    _df.drop([\"age\", \"sex\", \"childness\"], axis=1, inplace=True)\n    return _df\n\n\ndef add_childness(df) : \n    _df = df.copy() \n    _df[\"childness\"] = _df.age.apply(lambda x : (0 if x > 17 else (1 if x > 14 else 2)))\n    return _df\n\n\ndef convert_age(df, ages=None, coefs=None) :\n    _df = df.copy() \n    if not ages :  ages     = [ 3, 7,14,15,16,17,20,30,40,50,60,70,80,100]\n    if not coefs : coefs    = [10,10,10, 5, 3, 1, 0, 0, 0, 0, 0, 0, 0,  0]     \n    def converter(x) : \n        for a, c in zip(ages, coefs) : \n            if x <= a :return c\n        raise ValueError(\"error\")\n    _df[\"age\"] = _df.age.apply( lambda x : converter(x))\n    return _df\n\n\ndef regularize_fare(df) : \n    _df = df.copy() \n    #\u00a0g\u00e9rer les fare \u00e0 0 et les fares \u00e0 200+\n    _df.loc[_df.fare >200,\"fare\"] = 220\n    for i in range(1,4) : \n        val = _df.loc[_df[\"pclass\"] == i, \"fare\"].mean()\n        _df.loc[(_df.fare == 0.0) & (_df[\"pclass\"] == i), \"fare\"] = val\n    return _df\n\n\ndef add_family(df) : \n    _df = df.copy() \n    try : \n        _df[\"familly\"] = _df.sibsp + _df.parch\n    except : \n        ValueError(\"impossible\")\n    return _df","dc8ae69a":"#\u00a0various lambda methods \n\nnothing             = lambda df : df\ndrop_age            = lambda df : df.drop([\"age\"], axis=1)\nchildness_del_age   = lambda df : drop_age(add_childness(df))\ndrop_embarked       = lambda df : df.drop([\"embarked\"], axis=1)\ndrop_fare           = lambda df : df.drop([\"fare\"], axis=1)\ndrop_fare_embarked  = lambda df : drop_fare(drop_embarked(df))\ndrop_sibsp          = lambda df : df.drop([\"sibsp\"], axis=1)\ndrop_parch          = lambda df : df.drop([\"parch\"], axis=1)\nfamily_del_both     = lambda df : drop_parch(drop_sibsp(add_family(df)))\nfamily_del_sibsp    = lambda df : drop_sibsp(add_family(df))\nfamily_del_parch    = lambda df : drop_parch(add_family(df))\ndel_sibsp_parch     = lambda df : drop_parch(drop_sibsp(df))","d747f270":"#\u00a0and here our scenarii to benchmark\n\nMETHOD_LIST = []\nCOLUMN_LIST = []\n\n#\u00a0about age\n#\u00a0METHOD_LIST = [nothing, convert_age, merge_age_sex, drop_age, add_childness, childness_del_age]\n#\u00a0COLUMN_LIST = [\"nothing\", \"convert_age\", \"merge_age_sex\", \"drop_age\", \"add_childness\", \"childness_del_age\"]\n\n#\u00a0about embarked\n# METHOD_LIST =     [nothing, drop_embarked]\n# COLUMN_LIST =     [\"nothing\", \"drop_embarked\"]\n\n#\u00a0\u00a0about fare\n#\u00a0METHOD_LIST =   [nothing,   drop_fare,     regularize_fare]\n#\u00a0COLUMN_LIST =   [\"nothing\", \"drop_fare\",   \"regul_fare\",  ]\n\n# #\u00a0about family \n# METHOD_LIST = [nothing, add_family, drop_sibsp, drop_parch, family_del_both, family_del_sibsp, family_del_parch, del_sibsp_parch]\n# COLUMN_LIST = [\"nothing\", \"add_family\", 'drop_sibsp', 'drop_parch', 'family_del_both', 'family_del_sibsp', 'family_del_parch', \"del_sibsp_parch\"]","dc958fb6":"#\u00a0we need to have a tool to compare basic model, vs feature engineered model\n#\u00a0our goal is to select the method wich offers us the best accuracy gain\n#\u00a0of course it will be impossible to gain 5 or 10%, but 0.5, or 1% will be fine \n\ndef df_enhance_gain(method, df=None, model=None, params=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n        \n    grid = GridSearchCV(model, params, cv=3, n_jobs=6, scoring=\"accuracy\")\n\n    # init acc\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    _df = df.copy()\n    _df = method(_df)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n    \n    return round((new_acc - init_acc) \/ init_acc,3)","bacb4b1e":"#\u00a0we now need a tool to handle multiple tests and to give us back a global result dataframe\n\ndef benchmark_various_df_enhance(  n, df=None, params=None, model=None,\n                                methods = METHOD_LIST, cols = COLUMN_LIST) : \n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n\n    if not model : model =  LogisticRegression()\n    if not params : params = dict() \n    if len(methods) != len(cols) : raise ValueError(\"len do not match\")\n\n    results = [ [df_enhance_gain(m, df, model, params) for m in methods] \n                    for i in range(n) ]\n    results = pd.DataFrame(results, columns=cols)\n    return results\n\n####\n\n#\u00a0let's try this\n\nMETHOD_LIST =   [nothing,   drop_fare,     regularize_fare]\nCOLUMN_LIST =   [\"nothing\", \"drop_fare\",   \"regul_fare\",  ]\n\nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","7ca70e1d":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","bf706bbe":"# ok it works! even if results are not good, we have a tool to track each feature engineering impact\n#\u00a0in this case we can se that 'drop_fare' method help us to increase gain mean of 1% and median of ... 0.4% \n#\u00a0Q1 and Q3 are also better with this method, but the std of our results is about 3-4%.\n#\u00a0our accuracy gain is not so good enouth... \n\n# why the hell is there a 'nothing' method wich do nothing? \n# it is supposed to give us a scale of natural std of our results... ","867d6e48":"#\u00a0boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"fare strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding fare feature\")","85de29e8":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"fare strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding fare feature\")","f37844b6":"#\u00a0about embarked\n#\u00a0same method...\n\nMETHOD_LIST =     [nothing, drop_embarked]\nCOLUMN_LIST =     [\"nothing\", \"drop_embarked\"]\n                   \nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","02438e51":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","9e7303ab":"# boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"embarked strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding emabrked feature\")","473bec91":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"embarked strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding emabrked feature\")","a501b7b2":"#\u00a0about age\nMETHOD_LIST = [nothing, convert_age, merge_age_sex, drop_age, add_childness, childness_del_age]\nCOLUMN_LIST = [\"nothing\", \"convert_age\", \"merge_age_sex\", \"drop_age\", \"add_childness\", \"childness_del_age\"]\n\nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","31e65d2a":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","56ba4384":"#\u00a0boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"age strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding age feature\")","03137b32":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"age strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding age feature\")","e471990b":"# we now will try to ehance our output with thresholding our predictions \n\ndef clipping_results(y_pred, x=0.5) : \n    if not isinstance(y_pred, Iterable) : \n        raise ValueError(\"y_pred has to be a pd.Series\")\n    if not(0 <= x <= 1 ) :\n        raise ValueError(\"threshold must be 0.00 --> 0.5\")\n    y_pred = pd.Series(y_pred)\n    y_pred = y_pred.apply(lambda i : 1 if i>=x else 0)\n    return y_pred","b7d5fab5":"# compute accuracy gain for one threshold\n\ndef clipping_results_gain(k, df, model=None, params=None) :     \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model  : model = LogisticRegression()\n    if not params : params = dict()\n    #  info(k)\n    X,y = return_X_y(df)\n    X_tr, X_te, y_tr, y_te = split(X, y)\n    y_test = y_te\n    \n    grid = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict_proba(X_te)[:, 1]\n\n    init_acc = accuracy_score(y_test, clipping_results(y_pred))\n\n    y_pred = clipping_results(y_pred, k)\n    new_acc = accuracy_score(y_test, y_pred)\n\n    return round((new_acc - init_acc) \/ init_acc,3)","530f88bb":"# and benchmark every threshold between 0.0 and 0.5\n\ndef benchmark_various_clipping(   n , df=None, params=None, \n                                        model=None, threshold_list=None) :     \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n\n    if not threshold_list : \n        threshold_list = np.arange(0.2,0.8, 0.05).round(2)\n        # threshold_list = np.arange(0.44,0.66, 0.02).round(2)\n        # threshold_list = [round(i\/1000, 3) for i in range(10,101)]\n        # threshold_list = [round(i\/1000, 3) for i in range(10,500, 5)]\n    results = [ [clipping_results_gain(k, df, model, params) for k in threshold_list]\n                     for _ in range(n)]\n    results = pd.DataFrame(results, columns=threshold_list)\n    return results\n\n####\n\nres = benchmark_various_clipping(N_1, df)","e2883ffc":"#\u00a0print sorted results describe()\nres.describe().T.sort_values(by=\"50%\", axis=0, ascending=False)","0d757818":"#\u00a0boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"clipping levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various clipping strategies\")","1bfc1018":"#\u00a0plot mean, med, Q1,Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:,  [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"clipping levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various clipping strategies\")","cd05b004":"#\u00a0find here a new version of our GSCV function\n#\u00a0but with clipping option\n\ndef GSCV_basic(     model, params,  \n                    X_train, X_test, y_train, y_test, \n                    clipping=None,\n                    n_jobs=6, scoring=\"accuracy\", cv=10): \n\n    try    : model = model()\n    except : pass \n\n    grid = GridSearchCV(model, params, \n                        n_jobs=n_jobs, \n                        scoring=scoring,\n                        cv=cv)\n\n    try : grid.fit(X_train, y_train)\n    except Exception as e : print(e)\n\n    if not clipping : \n        try : y_pred = grid.predict(X_test)\n        except  Exception as e : print(e)\n    else : \n        try :\n            y_pred = grid.predict_proba(X_test)[:, 1]\n            y_pred = clipping_results(y_pred, clipping)\n        except Exception as e: \n            raise e\n\n    try : acc = accuracy_score(y_test, y_pred).round(3)\n    except  Exception as e : print(e)\n\n    return acc, grid\n","577257fe":"#\u00a0find here 3 dict with meta parametres \n\ndefault_params  = {     \"penalty\":[\"l2\"],\n                        \"dual\":[False],\n                        \"tol\":[0.0001],\n                        \"C\":[1.0],\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"liblinear\"],\n                        \"max_iter\":[100],\n                        \"multi_class\":[\"ovr\"],\n                        \"warm_start\":[False],   }\n\nall_params          = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[True, False],\n                        \"tol\":[0.0001, 0.001, 0.1, 1],                   # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.0001, 0.001, 0.01, 0.1, 1, 10, 100],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100, 1000],   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\", \"multinomial\"],\n                        \"warm_start\":[False, True],   }\n\nall_params2     = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[True, False],\n                        \"tol\":[0.0001, 0.001, 0.01],            # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.001, 0.01, 0.1, 1, 10],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100],                   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\", \"multinomial\"],\n                        \"warm_start\":[True, False],   }","69d33055":"def _mean(x) : \n    if not isinstance(x, Iterable) : \n        raise ValueError(\"x must be iter\")\n    return round(float(sum(x) \/ len(x)), 3)\n\n\ndef _med(x) : \n    x = sorted(x)\n    if not (len(x) % 2) : \n        idx     = len(x) \/2\n        idx_u   = ceil(idx)\n        idx_d   = ceil(idx) - 1\n        med = _mean([x[idx_u], x[idx_d]])\n    else :\n        idx = int(len(x)\/2)\n        med = x[idx]\n    return round(med, 3)\n\n\ndef _mix(x) : \n    mea_x = _mean(x)\n    med_x = _med(x)\n    return _mean([mea_x, med_x]) ","573a698a":"#\u00a0in order to be able to parse easly various meta parametres, without wondering if there are\n#\u00a0correct, we need a function which is able to combine a dict of list and one\n#\u00a0other which is able to  select only good parametres\n\ndef combine_param_dict(d) : \n    d = OrderedDict(d)\n    combinations = it.product(*(d[feat] for feat in d))\n    combinations = list(combinations)\n    d = [{i:[j,] for i,j in zip(d.keys(), I)} for I in combinations ]\n    return d\n\n\ndef valid_param_dict(model, list_of_dicts, X_train, X_test, y_train, y_test) : \n    good_dicts = list()\n    try : _model = model()\n    except : _model = model\n    for d in list_of_dicts : \n        try : \n            m = GridSearchCV(_model, d, cv=3)\n            m.fit( X_train, y_train)\n            good_dicts.append(d)\n        except : \n            print(\"params!\", end=\"**\")\n    return good_dicts\n\n####\n\nd = {\"a\" : [\"a\",\"b\",\"c\"], \"b\": [0,1,2,3,4]}\nd = combine_param_dict(d)\nd","ccce9571":"#\u00a0and here we have the heart of our meta parametres search\n#\u00a0is job is to parse all feeded params, to combine them,\n#\u00a0to select only good ones, and to compute the average accuracy score\n\ndef parse_various_params(   n, model, params, df=None, \n                             meth=None, save=True, feat_ing=None,\n                             clipping = None, \n                             n_file=4, name=\"benchmark_various_params\",\n                             path=\"benchmarks\/params\/\",\n                             n_jobs=6, scoring=\"accuracy\",cv=5) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n    if      meth == None   : meth = _mix\n    elif    meth == \"mean\" : meth = _mean\n    elif    meth == \"med\"  : meth = _med\n    elif    meth == \"mix\"  : meth = _mix\n    else                   : raise ValueError(\"not good method\") \n\n    if not feat_ing : feat_ing = \"no feat_ing\"\n    if not name : name = \"benchmark_various_params\"\n    if not path : path = \"benchmarks\/params\/\"\n    if not n_file :  n_file =  1\n\n    name = path+name+str(n_file)+\".csv\"\n\n    if save : \n        txt =   \"init file         \\n\"\n        txt +=  \"model     : {}      \\n\".format(model)\n        txt +=  \"params    : {}      \\n\".format(params)\n        txt +=  \"n         : {}      \\n\".format(n)\n        txt +=  \"clipping  : {}      \\n\".format(clipping)\n        txt +=  \"meth      : {}      \\n\".format(meth)\n        txt +=  \"feat_ing  : {}      \\n\".format(feat_ing)\n        txt +=  \"\\n\\n  ********************************************** \\n\\n\"\n\n        with open(name, \"w\") as f : f.write(txt)\n\n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n    columns = list(params.keys())\n    columns.append(\"acc\")\n    results = list()\n\n    param_dict = combine_param_dict(params)\n    param_dict = valid_param_dict(model, param_dict, X_train, X_test, y_train, y_test)\n\n    for param in param_dict : \n        info(\"testing param : \" + str(param))\n        accs = [GSCV_basic(   model, param, \n                                X_train, X_test, y_train, y_test, \n                                clipping=clipping, \n                                n_jobs=n_jobs, scoring=scoring,cv=cv )[0] \n                                for i in range(n)]\n\n        acc = round(meth(accs), 3)\n        # grid_param = grid.get_params()\n        if save : \n            txt = str(acc) + \",\" + str(param) + \"\\n\"\n            with open(name, \"a\") as f : f.write(txt)\n                \n        serie = {i: j[0] for i,j in param.items()}\n        serie[\"acc\"] = acc\n        results.append(pd.Series(serie))\n\n        info(\"done\")\n\n    results = pd.DataFrame(results, columns =columns )\n    results.sort_values(by=\"acc\", ascending=False, inplace=True)\n    return results","ee7a1631":"#\u00a0let's try a dict params \n\nsmall_dict_params    = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[False, True],\n                        \"tol\":[0.0001, 0.001, 0.01],            # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.001, 0.01, 0.1, 1, 10],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100],                   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\",\"multinomial\"],\n                        \"warm_start\":[True],   }","36aa7fbc":"res = parse_various_params( N_1, # should be 10, 20, 30 or 50\n                           LogisticRegression, \n                           small_dict_params, \n                           save=False)","cc4b6af8":"#\u00a0print sorted results describe()\n_res = res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[:20, :]\nprint(len(res))\n_res","4e03be6b":"_res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[20: , :]","ac98f9b0":"#\u00a0scatter various sub parametres\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.scatter(res.tol,res.acc )\nplt.xlabel(\"tol levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various accuracy gain for tol levels\")","285fcfec":"# scatter various sub parametres\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.scatter(res.C, res.acc)\nplt.xlabel(\"C levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various accuracy gain for C levels\")","a78e58b5":"#\u00a0we now are going to check if our best params dict is 'really' can \n#\u00a0strongly impact our results\n# we now are going to check if our best params dict is 'really' can \n# strongly impact our results\n\nbest_params_top_5 = list()\nfor i in range(5) : \n    best_params = _res.drop(\"acc\",axis=1).iloc[i,:].to_dict()\n    best_params = {i:[j] for i, j in best_params.items()}\n    best_params_top_5.append(best_params)\n\nBEST_PARAMS = dict(     dual            = [False], \n                        penalty         = [\"l2\"], \n                        tol             = [0.001], \n                        multi_class     = [\"multinomial\"], \n                        warm_start      = [True], \n                        solver          = [\"lbfgs\"],\n                        C               = [0.1], \n                        fit_intercept   = [True]    )","999815ca":"#\u00a0compute accuracy gain : with vs without params \n\ndef meta_params_results_gain(model, params, df, re_split=False) : \n    \n    try     : model = model()\n    except  : pass \n\n    grid = GridSearchCV(model, dict(), \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n\n    grid2 = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    # init acc\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    if re_split : \n        X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    \n    grid2.fit(X_tr, y_tr)   \n    y_pred = grid2.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n\n    return round((new_acc - init_acc) \/ init_acc,3)","ff8d6d1e":"#\u00a0benchmark various params, or just one\n\ndef benchmark_various_meta_params(   n, model, params, df=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model : model= LogisticRegression\n    if not params : params = BEST_PARAMS\n    model = model()\n    results = [meta_params_results_gain(model, params, df) for _ in range(n)]\n    #\u00a0results = [[params_results_gain(model, p, df) for _ in range(n)]for p in params]\n    results = pd.DataFrame({\"params\" : results})   \n    #\u00a0results = pd.DataFrame(results, columns=map(str,params))\n    return results\n\n####\n\nres = benchmark_various_meta_params(N_1, LogisticRegression, BEST_PARAMS)","b8b03264":"#\u00a0print sorted results describe()\nres.describe().T.sort_values(by=\"mean\", ascending=False)","fe9b880b":"# boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"best meta parmas\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark best meta params\")","ff7230a8":"# very famous in DL, the pseudo labelling can be a good method to upgrade the accuracy rate\n#\u00a0it works for multi class problems, let's try for our dataset\n\ndef pseudo_labelling(df, model=None, params=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n\n    if not model : model = LogisticRegression()\n        \n    if not params : params = dict()\n\n    X,y = return_X_y(df)\n    X_tr, X_te, y_tr, y_te = split(X, y)\n    y_test = y_te\n    \n    grid = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n\n    init_acc = accuracy_score(y_test, y_pred)\n\n    TR = X_tr.copy()\n    TR[\"target\"] = y_tr\n    TE = X_te.copy()\n    TE[\"target\"] = y_pred\n    \n    new_df = TR.append(TE)\n    new_X,new_y = return_X_y(new_df)\n\n    grid.fit(new_X,new_y)\n\n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_test, y_pred)\n\n    return round((new_acc - init_acc) \/ init_acc,3)\n\n####\n\nres = pd.Series([pseudo_labelling(df) for i in range(N_3)], name=\"pseudo_labelling\")\nres.describe()","946bcb4d":"default_params  =     { \"alpha\":[1.0],\n                        \"normalize\":[False],\n                        \"tol\":[0.001],\n                        \"fit_intercept\":[True],\n                        \"class_weight\":[None],\n                        \"solver\":[\"auto\"],\n                        \"max_iter\":[None],  }\n\nall_params      =     { \"alpha\":np.logspace(-5, 3, 9), \n                        \"normalize\":[False, True],\n                        \"tol\":[1, 0.1, 0.001, 0.0001],\n                        \"fit_intercept\":[True],\n                        \"class_weight\":[None],\n                        \"solver\":['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n                        \"max_iter\":[None],  }\n        ","788181f6":"res = parse_various_params( N_1, #\u00a0should be 10, 20, 30 or 50 \n                           RidgeClassifier, \n                           all_params, \n                           save=False)","e4992e77":"_res = res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[:20, :]\nprint(len(_res))\n_res","135b683f":"_res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[20: , :]","70799547":"#\u00a0select our best params \nBEST_PARAMS_2 = _res.drop(\"acc\",axis=1).iloc[0,:].to_dict()\nBEST_PARAMS_2 = {i:[j] for i, j in BEST_PARAMS_2.items()}\nBEST_PARAMS_2\n","d8e97e6b":"res = benchmark_various_meta_params(N_1, RidgeClassifier, BEST_PARAMS_2)","41b7c670":"#\u00a0print sorted results \nres.describe().T.sort_values(by=\"mean\", ascending=False)","9a419b84":"# boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"best meta parmas\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark best meta params\")","65a8e218":"#\u00a0ok we can see that our meta params gain (+1.3%) is very good compared to other meta params gain","8097a6cd":"_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())","714925da":"_train_df, _test_df = num_feature_eng_2(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())","a76df41e":"def feat_strat_gain(feat_eng_1, feat_eng_2, path=None, \n                    train_file=None, test_file=None, model=None, params=None) : \n    if not path : path = PATH\n    if not train_file : train_file = TRAIN_FILE\n    if not test_file : test_file = TEST_FILE\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n        \n    grid = GridSearchCV(model, params, cv=3, n_jobs=6, scoring=\"accuracy\")\n\n    # init acc\n    _train_df, _test_df = feat_eng_1(path, train_file, test_file)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_train_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    _train_df, _test_df = feat_eng_2(path, train_file, test_file)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_train_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n    \n    return round((new_acc - init_acc) \/ init_acc,3)\n\n####\n\nresults = [feat_strat_gain(num_feature_eng_1, num_feature_eng_2) for _ in range(30)]\nresults = pd.DataFrame({\"feat. strat\" : results})  ","84297ea1":"results.head()","aae18df1":"results.describe()","33c63740":"results.boxplot()","440cb666":"################################################################################\n\n# ----------------------------------------------------------\n# 03-playing_with_RF..py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this 4st part we will play with or Random Forest Classifier and with all feature engineering strategies we can to \n#\u00a0improve our accuracy score. \n","3d7305d6":"**03-playing_with_RF.py**\n---------------------------------------","3701aab7":"**Code conventions**\n--------------------------------------------------------------\n","3ead598c":"##################################################################\n#   Titanic - Learning From Disaster\n<br>\n###############################################################\n\n<br>\n\n\n* autor   : Alexandre Gazagnes\n* date    : 25\/08\/2018\n* commit  : v11\n\n<br>\n\nThought as a complete study of a data science project, this kernel does not only provide a turnkey solution but also provides a detailed study of the different steps needed to provide a good answer. Thus in the image of a mathematical demonstration, the approach and logic interest us more than the final solution (accuracy score of 0.89).\n\n<br>\n\nThe Titanic dataset is a very interesting dataset for several reasons:\n* if \"good\" results can be achieved easily (accuracy score 0.75-0.80), the improvement of these results appears very difficult\n* the dataset provided has a very low number of features, some of which are very complex to deal with\n* the pushing feature engineering approach is the key (as always) in this project, but the complexity and number of feature engineering strategies clearly poses a problem for having an accuracy score greater than 0.85\n\n<br>\n\nHere we will discuss the steps involved in a true Data Science project.\n* import and management of datasets\n* visualization\n* cleaning\n* outlier's management\n* not so basic and advanced engineering feature\n* how to implement a rigorous approach to navigate the feature engineering strategies\n* how to set up a rigorous approach for model selection and meta parametres selection\n* dummy \/ naive models\n* Logistic regression (and how to increase the accuracy score of 7-10% thanks to the feature engineering)\n* ML tricks as result clipping, pseudo-encoding, data increase (very common in DL btw)\n* Random Forests (and how to increase the accuracy score by 10-15% thanks to the feature engineering).\n","3b9ce44f":"**02-playing_with_LR.py**\n---------------------------------------","9f47d54e":"**Parsing various models**\n---------------------------------------","49cf2c49":"**00-first_dataset_tour.py**\n--------------------------------------------------------------\n","f49feb02":"**About global feature eng. strategies impact**\n-------------------------------------------\n","acb9d4e0":"**01-first_naive_models.py**\n---------------------------------------\n","8c28ec0e":"**Dataframe creation **\n---------------------------------------\n","937721d6":"**Import**\n--------------------------------------------------------------\n","a6a741f2":"##\u00a0About functions\nWherever it will be possible, we will use function rather than just normal command lines \u00a0it is not the dominant practice, sorry for that :)\n\n##\u00a0About name scopes\nWhatever the scope of a variable, as far as possible, if a variable is named \"x\" in any scope and we want to work on a copy of \"x\" it will be called \"_x\"\nas far as possible, inside a function, if it is a question of modifying the iniltial dataframe, an internal copy will be made:\n```python\ndef do_something_to(df) : \n    _df = df.copy()\n    _df = _df.map(my_function)\n    return _df\n    \ndf = do_something_to(df)\n```\n\n##\u00a0About 'for loops'\nFor obvious reasons\u00a0'for loops' ('for i in list :' AND list comprehensions) sould not be used with a pd.DataFrame object\u00a0but considering code readability, some 'for loops' can be found in the lines bellow. You are free to delete these awful - but readable - lines :) ","8222ecf4":"**Constants**\n--------------------------------------------------------------\n","f1a911c6":"**Graph settings**\n--------------------------------------------------------------\n","72af9674":"**About feature engineering impact**\n---------------------------------------\n","320e0aa2":"**Features-target and test-train slipt**\n---------------------------------------\n","2aae90a4":"**Logging and warning**\n--------------------------------------------------------------\n","80045304":"**Dummy and naive models**\n-----------------------------------------------------------------\n","34ce8e45":"**Data cleaning**\n--------------------------------------------\n","bc9e7bf1":"**Feature engineering**\n---------------------------------------\n","69fadb36":"**About result's clipping impact**\n---------------------------------------","1adfad01":"**Data exploration and visualization**\n---------------------------------------------------\n","fc2d265c":"**About meta parametres impact**\n---------------------------------------\n"}}