{"cell_type":{"6cbafe3e":"code","70bec4f5":"code","62446d02":"code","0676942c":"code","b1894e2d":"code","44f6dd84":"code","f2d184ae":"code","843dd16c":"code","5ab07f69":"code","4234e34a":"code","a0ca36c3":"code","52d72b1c":"code","865c1264":"code","ebd40fc7":"code","e77eb7da":"code","b932f4ba":"code","83a6c39e":"code","67866abc":"code","ebac9f2d":"code","b3bd4830":"code","7168560d":"code","20f1ab43":"code","c49788ec":"code","fb9b5ad0":"code","8db8820f":"code","fafa3de3":"code","1cc26474":"code","cbb6dad9":"code","22f2033f":"code","5c5d7acb":"code","e169d817":"code","807c3ed1":"code","afbb3444":"code","9ca47ed7":"code","465b650e":"code","37e6a7ab":"code","72c2b47d":"code","98d1a072":"code","6a3c4416":"code","4988b28a":"code","c6e86f4a":"code","1cdd5aef":"code","0426c706":"code","fffa2a80":"code","4023a1c5":"code","8e966dc3":"code","e1e5a1ca":"code","cf0457ce":"code","65decb0f":"code","542e2fd7":"code","5a43cff2":"code","8d01fccf":"code","bbb4ae59":"code","61f8f0c5":"code","de750221":"code","4866c3cb":"code","ef02c5d2":"code","57a80548":"code","c0f4b5d5":"code","e2345834":"code","4cb35e94":"code","ebfb2833":"code","c2b44ab7":"code","e3f9b0ce":"code","08c094bd":"code","3f11d38e":"code","f22414a8":"code","fd6df609":"code","7743e61c":"code","74b7c9ec":"code","038b7980":"code","1ffae22e":"code","83b628d7":"code","b793641a":"code","4c0dabe3":"code","b7a8f1dd":"code","0647ca51":"code","a85ea401":"code","a6afb75f":"code","819d79e0":"code","7d045fee":"code","bda14d2c":"code","3adaa41b":"markdown","9dee075b":"markdown","57ca9e34":"markdown","f22fff4b":"markdown","67a91843":"markdown","12f54eb9":"markdown","6597f271":"markdown","13339b3b":"markdown","e7a24686":"markdown","79d3e3e9":"markdown","977eaa3a":"markdown","18d584a1":"markdown","6d38b50f":"markdown","d6bbe7e9":"markdown","44accd31":"markdown","33661593":"markdown","ce945716":"markdown","721e6def":"markdown","fabf6953":"markdown","595c5d72":"markdown","12249b3d":"markdown","c9d241bc":"markdown","0a47ad0f":"markdown","49ad6d2a":"markdown","09036810":"markdown","b98992b9":"markdown","3750a74b":"markdown","992eac6d":"markdown","6201e00e":"markdown","0a6f38b6":"markdown","e579a108":"markdown","76aa5767":"markdown","b412506c":"markdown","d0740480":"markdown","084b040e":"markdown","2ac81cee":"markdown","ccdcb715":"markdown","d087734f":"markdown","ee4a44ec":"markdown","7bca37f0":"markdown","c088b6d6":"markdown","0e3ae664":"markdown","88bd8eae":"markdown","c4b8f430":"markdown","f44f9668":"markdown","05172307":"markdown","a5ac28bd":"markdown","59c5b71b":"markdown","0b963588":"markdown","cb24f8ee":"markdown","fb5a9915":"markdown","dcfd30a7":"markdown","0e62106a":"markdown","7b1fc669":"markdown","0ce6a562":"markdown","45d01771":"markdown","5583dad9":"markdown","e32c68be":"markdown","946cf6dc":"markdown","727c05d9":"markdown","57d5b0e0":"markdown","d5ccb0db":"markdown"},"source":{"6cbafe3e":"# import all libraries\n\nimport warnings # adds, removes or modifies python library behavior \nwarnings.simplefilter(action='ignore', category=FutureWarning) # turn off future warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) # turn off deprecation warnings\n# Deprecation Warnings: cross_validation, weight_boosting, grid_search,learning_curve\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport logging # tracking events\nimport datetime # classes for dates\nimport time # time definitions\nimport os # operating system\n\nimport scipy.stats as stats # stats contains probability distributions\nimport pylab as pl # combines pyplot and numpy\n\nimport seaborn as sns # statistical data visualization\nimport matplotlib.pyplot as plt # 2D plotting library\n\n# tools for data mining and data analysis\nfrom sklearn import *\n\nfrom xgboost import XGBClassifier # high performance gradient boosting\nimport lightgbm as lgb # fast, distributed, high performance gradient boosting\n\nimport plotly.offline as py # graphing library\npy.init_notebook_mode(connected=True) # plot your graphs offline inside a Jupyter Notebook \nimport plotly.graph_objs as go # web-service for hosting graphs\nimport plotly.tools as tls # web-service for hosting graphs","70bec4f5":"from kaggle.competitions import twosigmanews # imports kaggle module and create an environment\nenv = twosigmanews.make_env()","62446d02":"logging.info('Load data in 2 dataframes: mt_df (market_train_df) & nt_df (news_train_df)')\n(mt_df, nt_df) = env.get_training_data()","0676942c":"days = env.get_prediction_days()\n(mt_obs_df, nt_obs_df, predictions_template_df) = next(days)","b1894e2d":"print(\"market_train_df's shape:\",mt_df.shape)","44f6dd84":"mt_df.dtypes","f2d184ae":"mt_df.isna().sum()","843dd16c":"percent1 = (100 * mt_df.isnull().sum() \/ mt_df.shape[0]).sort_values(ascending=False)\npercent1.plot(kind=\"bar\", figsize = (20,10), fontsize = 20)\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"Value Percent(%)\", fontsize = 20)\nplt.title(\"Total Missing Value by market_obs_df\", fontsize = 20)","5ab07f69":"mt_df.nunique()","4234e34a":"mt_df.head(5)","a0ca36c3":"mt_df.tail(5)","52d72b1c":"mt_df.select_dtypes(include=['float64']).describe()","865c1264":"mt_obs_df.head(5)","ebd40fc7":"mt_obs_df.tail(5)","e77eb7da":"print('Oldest date:', mt_df['time'].min().strftime('%Y-%m-%d'))\nprint('Most recent date:', mt_df['time'].max().strftime('%Y-%m-%d'))\nprint('Total number of different dates:', mt_df['time'].nunique())","b932f4ba":"mt_df['time'].dt.time.describe()","83a6c39e":"mt_df[\"time\"].groupby([mt_df[\"time\"].dt.year, mt_df[\"time\"].dt.month]).count().plot(kind=\"bar\",figsize=(21,5))","67866abc":"print('Total number of unique assetCodes:', mt_df['assetCode'].nunique())","ebac9f2d":"print(mt_df['assetCode'].values)","b3bd4830":"print('Total number of unique assetNames:', mt_df['assetName'].nunique())","7168560d":"print('Total number of unique assetCode & assetNames:', mt_df[['assetName','assetCode']].nunique())","20f1ab43":"print(\"There are {:,} records with assetName = `Unknown` in the training set\".format(mt_df[mt_df['assetName'] == 'Unknown'].size))\nassetNameGB = mt_df[mt_df['assetName'] == 'Unknown'].groupby('assetCode')\nunknownAssets = assetNameGB.size().reset_index('assetCode')\nprint(\"There are {} unique assets without assetName in the training set\".format(unknownAssets.shape[0]))\nunknownAssets.columns = ['assetCode','unknowns']\nunknownAssets.set_index('assetCode')\nunknownAssets.loc[:15,['assetCode','unknowns']].sort_values(by='unknowns', ascending=False).head(10)\n","c49788ec":"print(mt_df['assetName'].values)\nmt_df['assetName'].iloc[0]","fb9b5ad0":"print('Min:', round(mt_df['volume'].min(),0))\nprint('Max:', round(mt_df['volume'].max(),0))\nprint('Mean:', round(mt_df['volume'].mean(),0))\nprint('Median:', round(mt_df['volume'].median(),0))","8db8820f":"mt_df['volume'].plot(kind='hist', bins=[0,200000,400000,600000,800000,1000000]) ","fafa3de3":"mt_df['close'].describe().apply(lambda x: format(x, 'f'))","1cc26474":"mt_df['open'].describe().apply(lambda x: format(x, 'f'))","cbb6dad9":"mt_df['open']","22f2033f":"mt_df['universe'].describe().apply(lambda x: format(x, 'f'))","5c5d7acb":"# plotAsset plots assetCode1 from date1 to date2\ndef plotAsset(assetCode1,date1,date2):\n    asset_df = mt_df[(mt_df['assetCode'] == assetCode1) \n                      & (mt_df['time'] > date1) \n                      & (mt_df['time'] < date2)]\n    # Create a trace\n    trace1 = go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values)\n\n    layout = dict(title = \"Closing prices of {}\".format(assetCode1),\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),)\n    \n    data = [trace1]\n\n    py.iplot(dict(data=data, layout=layout), filename='basic-line')","e169d817":"plotAsset('AAPL.O','2015-01-01','2017-01-01')","807c3ed1":"def Candlestick(assetCode1,date1,date2):\n\n    asset_df = mt_df[(mt_df['assetCode'] == assetCode1) \n                  & (mt_df['time'] > date1) \n                  & (mt_df['time'] < date2)]\n    \n    asset_df['high'] = asset_df['open']\n    asset_df['low'] = asset_df['close']\n\n    for ind, row in asset_df.iterrows():\n        if row['close'] > row['open']:\n            asset_df.loc[ind, 'high'] = row['close']\n            asset_df.loc[ind, 'low'] = row['open']\n\n    trace1 = go.Candlestick(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        open = asset_df['open'].values,\n        low = asset_df['low'].values,\n        high = asset_df['high'].values,\n        close = asset_df['close'].values\n    )\n\n    layout = dict(title = \"Candlestick chart for {}\".format(assetCode1),\n                  xaxis = dict(\n                      title = 'Month',\n                      rangeslider = dict(visible = False)\n                  ),\n                  yaxis = dict(title = 'Price (USD)'))\n    \n    data = [trace1]\n\n    py.iplot(dict(data=data, layout=layout), filename='basic-line')   ","afbb3444":"Candlestick('AAPL.O','2015-01-01','2017-01-01')","9ca47ed7":"print(\"news_train_df's shape:\",nt_df.shape) ","465b650e":"nt_df.dtypes","37e6a7ab":"nt_df.isna().sum()","72c2b47d":"nt_df.nunique()","98d1a072":"nt_df.head(5)","6a3c4416":"nt_df.tail(5)","4988b28a":"nt_df.describe(include='all')","c6e86f4a":"print('Oldest date:', nt_df['time'].min().strftime('%Y-%m-%d'))\nprint('Most recent date:', nt_df['time'].max().strftime('%Y-%m-%d'))\nprint(\"There are {} missing values in the `time` column\".format(nt_df['time'].isna().sum()))\nnt_df['time'].dt.date.describe()","1cdd5aef":"print('Oldest date:', nt_df['sourceTimestamp'].min().strftime('%Y-%m-%d'))\nprint('Most recent date:', nt_df['sourceTimestamp'].max().strftime('%Y-%m-%d'))\nprint(\"There are {} missing values in the `sourceTimestamp` column\".format(nt_df['sourceTimestamp'].isna().sum()))\nnt_df['sourceTimestamp'].dt.date.describe()","0426c706":"print(nt_df.loc[nt_df['time'] == nt_df['sourceTimestamp']].shape[0])","fffa2a80":"print('Oldest date:', nt_df['firstCreated'].min().strftime('%Y-%m-%d'))\nprint('Most recent date:', nt_df['firstCreated'].max().strftime('%Y-%m-%d'))\nprint(\"There are {} missing values in the `firstCreated` column\".format(nt_df['firstCreated'].isna().sum()))\nnt_df['firstCreated'].dt.date.describe()","4023a1c5":"print(\"There are {} missing values in the `sourceId` column\".format(nt_df.sourceId.isna().sum()))\nprint(\"There are {} unique values in the `sourceId` column\".format(nt_df.sourceId.nunique()))\nprint(\"There are {} unique values in the `sourceId` column\".format(nt_df.sourceId.count()))\nprint(nt_df.sourceId.describe())","8e966dc3":"nt_df[nt_df['sourceId']=='d7ad319ee02edea0']","e1e5a1ca":"nt_df[nt_df.duplicated(keep=False)].shape[0]","cf0457ce":"print(\"There are {} missing values in the `headline` column\".format(nt_df['headline'].isna().sum()))\nprint(\"There are {} unique values in the `headline` column\".format(nt_df.headline.nunique()))\nprint(\"There are {} unique values in the `headline` column\".format(nt_df.headline.count()))","65decb0f":"for i in range(0,20):\n    print(nt_df['headline'].iloc[i])","542e2fd7":"print(nt_df['urgency'].describe())\nprint(\"Unique values in the `urgency` column: {}\".format(nt_df['urgency'].unique()))","5a43cff2":"print(nt_df['urgency'].head(5))\nprint(nt_df.groupby(['urgency']).count())\nsns.distplot(nt_df['urgency'])","8d01fccf":"print(nt_df['takeSequence'].head(5))\nsns.distplot(nt_df['takeSequence'])","bbb4ae59":"print(nt_df['provider'].head(5))","61f8f0c5":"print(nt_df['subjects'].head(5))","de750221":"for i in list(range(5)):\n    print(nt_df['subjects'].iloc[i])","4866c3cb":"for i in list(range(5)):\n    print(nt_df['audiences'].iloc[i])","ef02c5d2":"print(nt_df['bodySize'].head(5))\nprint(nt_df['bodySize'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.companyCount)","57a80548":"print(nt_df['companyCount'].head(5))\nsns.distplot(nt_df.companyCount)","c0f4b5d5":"print(\"There are {} missing values in the `headlineTag` column\".format(nt_df['headlineTag'].isna().sum()))\nprint(\"There are {} unique values in the `headlineTag` column\".format(nt_df.headlineTag.nunique()))\nprint(\"There are {} unique values in the `headlineTag` column\".format(nt_df.headlineTag.count()))\nprint(nt_df['headlineTag'].unique())","e2345834":"nt_df['marketCommentary'].unique()","4cb35e94":"print(nt_df['sentenceCount'].head(5))\nprint(nt_df['sentenceCount'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentenceCount)","ebfb2833":"print(nt_df['wordCount'].head(5))\nprint(nt_df['wordCount'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.wordCount)","c2b44ab7":"print(nt_df['assetCodes'].head(5))","e3f9b0ce":"print(nt_df['assetName'].head(5))","08c094bd":"print(nt_df['firstMentionSentence'].head(5))\nprint(nt_df['firstMentionSentence'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.firstMentionSentence)","3f11d38e":"print(nt_df['relevance'].head(5))\nprint(nt_df['relevance'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.relevance)","f22414a8":"print(nt_df['sentimentClass'].unique())\nprint(nt_df['sentimentClass'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentimentClass)","fd6df609":"print(nt_df['sentimentNegative'].unique())\nprint(nt_df['sentimentNegative'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentimentNegative)","7743e61c":"print(nt_df['sentimentNeutral'].unique())\nprint(nt_df['sentimentNeutral'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentimentNeutral)","74b7c9ec":"print(nt_df['sentimentPositive'].head(5))\nprint(nt_df['sentimentPositive'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentimentNeutral)","038b7980":"print(nt_df['sentimentWordCount'].head(5))\nprint(nt_df['sentimentWordCount'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.sentimentWordCount)","1ffae22e":"print(nt_df['noveltyCount12H'].head(5))\nprint(nt_df['noveltyCount12H'].describe().apply(lambda x: format(x, '5.2f')))\nsns.distplot(nt_df.noveltyCount12H)","83b628d7":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\n# days = env.get_prediction_days()","b793641a":"# (market_obs_df, news_obs_df, predictions_template_df) = next(days)\n# predictions_template_df.head()","4c0dabe3":"def make_random_predictions(predictions_df):\n    predictions_df.confidenceValue = 2.0 * np.random.rand(len(predictions_df)) - 1.0\nmake_random_predictions(predictions_template_df)\nenv.predict(predictions_template_df)","b7a8f1dd":"for (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_random_predictions(predictions_template_df)\n    env.predict(predictions_template_df)\nprint('Done!')","0647ca51":"env.write_submission_file()","a85ea401":"market_train = mt_df\nnews_train = nt_df","a6afb75f":"market_train.time = market_train.time.dt.date\nnews_train.time = news_train.time.dt.hour\nnews_train.sourceTimestamp= news_train.sourceTimestamp.dt.hour\nnews_train.firstCreated = news_train.firstCreated.dt.date\nnews_train['assetCodesLen'] = news_train['assetCodes'].map(lambda x: len(eval(x)))\nnews_train['assetCodes'] = news_train['assetCodes'].map(lambda x: list(eval(x))[0])\nkcol = ['firstCreated', 'assetCodes']\nnews_train = news_train.groupby(kcol, as_index=False).mean()\n\nmarket_train = pd.merge(market_train, news_train, \n                        how='left', \n                        left_on=['time', 'assetCode'], \n                        right_on=['firstCreated', 'assetCodes'])\n\nlbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n\nmarket_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n\nfcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', \n                                             'assetName', 'audiences', 'firstCreated', \n                                             'headline', 'headlineTag', 'marketCommentary', \n                                             'provider', 'returnsOpenNextMktres10', 'sourceId',\n                                             'subjects', 'time', 'time_x', 'universe']]","819d79e0":"x1, x2, y1, y2 = model_selection.train_test_split(market_train[fcol], \n                                                  market_train['returnsOpenNextMktres10'], \n                                                  test_size=0.25, \n                                                  random_state=99)\n\ndef lgb_rmse(preds, y): # update to Competition Metric\n    y = np.array(list(y.get_label()))\n    score = np.sqrt(metrics.mean_squared_error(y, preds))\n    return 'RMSE', score, False\n\nparams = {'learning_rate': 0.2, \n          'max_depth': 6, \n          'boosting': 'gbdt',\n          'objective': 'regression',\n          'seed': 2018}\n\nlgb_model = lgb.train(params, \n                      lgb.Dataset(x1, label=y1), \n                      500, \n                      lgb.Dataset(x2, label=y2), \n                      verbose_eval=10,\n                      early_stopping_rounds=20)","7d045fee":"df = pd.DataFrame({'imp': lgb_model.feature_importance(importance_type='gain'), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\n_ = df.plot(kind='barh', x='col', y='imp', figsize=(7,12))\n#plt.savefig('lgb_gain.png')\n\ndf = pd.DataFrame({'imp': lgb_model.feature_importance(importance_type='split'), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\n_ = df.plot(kind='barh', x='col', y='imp', figsize=(7,12))\n# plt.savefig('lgb_split.png')","bda14d2c":"env.write_submission_file()","3adaa41b":"We now look at **number of unique values**","9dee075b":"Since the Total number of unique assetCode & assetNames is 3.780 which is equal to the number of unique *asset Codes* (3.780), we can safely assume that **1 asset code can only have 1 asset name**.  \nWe now check some assetCode values to find out what they look like. Note that we already check that there are no nulls in this field.","57ca9e34":"### <a id=\"3.7\">3.7 *provider* Variable <\/a> \n**Data Description:** provider(category) - identifier for the organization which provided the news item (e.g. RTRS for Reuters News, BSW for Business Wire)","f22fff4b":"[](http:\/\/)### <a id=\"3.21\">3.21 *sentimentPositive* Variable <\/a> \n**Data Description:** sentimentPositive(float32) - probability that the sentiment of the news item was positive for the asset","67a91843":"We now look at a few headlines:","12f54eb9":"# <a id=\"1\">3. Exploration - News Train <\/a>\nThe news data contains information at both the news article level and asset level (in other words, the table is intentionally not normalized).","6597f271":"### <a id=\"3.12\">3.12 *headlineTag* Variable <\/a> \n**Data Description:** headlineTag(object) - the Thomson Reuters headline tag for the news item","13339b3b":"So, number of number of unique *asset Names* (3.511) < unique *asset Codes* (3.780). We now check how many combinations *asset Names* & *asset Codes*  in order to know if the same asset code can have different asset Names","e7a24686":"# Load test data","79d3e3e9":"### <a id=\"3.18\">3.18 *firstMentionSentence* Variable <\/a> \n**Data Description:** firstMentionSentence(int16) - the first sentence, starting with the headline, in which the scored asset is mentioned.\n1: headline\n2: first sentence of the story body\n3: second sentence of the body, etc\n0: the asset being scored was not found in the news item's headline or body text. As a result, the entire news item's text (headline + body) will be used to determine the sentiment score.","977eaa3a":"### <a id=\"4.5\">4.5 Restart the Kernel to run your code again <\/a>\nIn order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n![Restart button](https:\/\/i.imgur.com\/hudu8jF.png)","18d584a1":"## <a id=\"2\">2. Exploration - Market Train <\/a>\n### <a id=\"2.0\">2.0 Snapshot - Application Train <\/a>","6d38b50f":"### <a id=\"2.1\">2.1 *time* Variable <\/a>\nData Description: time(datetime64[ns, UTC]) - the current time (in marketdata, all rows are taken at 22:00 UTC)  \nWe first look at the oldest and most recent dates and at the number of unique dates.","d6bbe7e9":"### <a id=\"4.6\">4.6 the1owl's LGBM<\/a>\nAs a baseline I would recommend [the1owl's my-two-sigma-cents-only](https:\/\/www.kaggle.com\/the1owl\/my-two-sigma-cents-only)","44accd31":"We now look at the distribution","33661593":"### <a id=\"4.2\">4.2 **`predict`** function <\/a> \nStores your predictions for the current prediction day.  Expects the same format as you saw in `predictions_template_df` returned from `get_prediction_days`.\n\nArgs:\n* `predictions_df`: DataFrame which must have the following columns:\n    * `assetCode`: The market asset.\n    * `confidenceValue`: Your confidence whether the asset will increase or decrease in 10 trading days.  All values must be in the range `[-1.0, 1.0]`.\n\nThe `predictions_df` you send **must** contain the exact set of rows which were given to you in the `predictions_template_df` returned from `get_prediction_days`.  The `predict` function does not validate this, but if you are missing any `assetCode`s or add any extraneous `assetCode`s, then your submission will fail.","ce945716":"### <a id=\"3.9\">3.9 *audiences* Variable <\/a> \n**Data Description:** audiences(category) - identifies which desktop news product(s) the news item belongs to. They are typically tailored to specific audiences. (e.g. \"M\" for Money International News Service and \"FB\" for French General News Service)","721e6def":"There are no nulls in the news train data","fabf6953":"### <a id=\"4.3\">4.3 Main Loop <\/a>\nLet's loop through all the days and make our random predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end.","595c5d72":" ### <a id=\"2.6\">2.6 *returnsClose & returnsOpen* Variable <\/a>\n Data Description:  The marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n* Returns are always calculated either:\n    * **open-to-open** (from the opening time of one trading day to the open of another) or \n    * **close-to-close** (from the closing time of one trading day to the open of another).  \n* Returns are either:\n    * **raw**, meaning that the data is not adjusted against any benchmark, or\n    * **market-residualized** (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.  \n* Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.  \n* Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.  \n\n---\n* returnsClosePrevRaw1\n* returnsOpenPrevRaw1\n* returnsClosePrevMktres1\n* returnsOpenPrevMktres1\n* returnsClosePrevRaw10\n* returnsOpenPrevRaw10\n* returnsClosePrevMktres10\n* returnsOpenPrevMktres10\n* returnsOpenNextMktres10\n\nLet's see...","12249b3d":"### <a id=\"4.4\">4.4 *write_submission_file* Function <\/a>\nWrites your predictions to a CSV file (`submission.csv`) in the current working directory.","c9d241bc":"### <a id=\"3.10\">3.10 *bodySize* Variable <\/a> \n**Data Description:** bodySize(int32) - the size of the current version of the story body in characters","0a47ad0f":"### <a id=\"2.7\">2.7 *universe* Variable <\/a>\nData Description: universe(float64) - a boolean indicating whether or not the instrument on that day will be included in scoring. This value is not provided outside of the training data time period. The trading universe on a given date is the set of instruments that are avilable for trading (the scoring function will not consider instruments that are not in the trading universe). The trading universe changes daily.  \n","49ad6d2a":"### <a id=\"3.16\">3.16 *assetCodes* Variable <\/a> \n**Data Description:** assetCodes(category) - list of assets mentioned in the item","09036810":"Only 25 news have been reported as urgency 2. Does it make sence to keep this category? ","b98992b9":"### <a id=\"3.1\">3.1 *time* & *sourceTimestamp* Variables <\/a> \n**Data Description:**   \n* time(datetime64[ns, UTC]) - UTC timestamp showing when the data was available on the feed (second precision)  \n* sourceTimestamp(datetime64[ns, UTC]) - UTC timestamp of this news item when it was created","3750a74b":"### <a id=\"3.5\">3.5 *urgency* Variable <\/a> \n**Data Description:** urgency(int8) - differentiates story types (1: alert, 3: article)","992eac6d":"**Important:** As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission.","6201e00e":"As there is only 1 unique value the above statement proves to be true  \nWe then look at the distribution of rows by date:","0a6f38b6":"### <a id=\"3.23\">3.23 *noveltyCountXXY* Variable <\/a> \n**Data Description:**   \n* noveltyCount12H(int16) - The 12 hour novelty of the content within a news item on a particular asset. It is calculated by comparing it with the asset-specific text over a cache of previous news items that contain the asset.\n* noveltyCount24H(int16) - same as above, but for 24 hours\n* noveltyCount3D(int16) - same as above, but for 3 days\n* noveltyCount5D(int16) - same as above, but for 5 days\n* noveltyCount7D(int16) - same as above, but for 7 days\n* volumeCounts12H(int16) - the 12 hour volume of news for each asset. A cache of previous news items is maintained and the number of news items that mention the asset within each of five historical periods is calculated.\n* volumeCounts24H(int16) - same as above, but for 24 hours\n* volumeCounts3D(int16) - same as above, but for 3 days\n* volumeCounts5D(int16) - same as above, but for 5 days\n* volumeCounts7D(int16) - same as above, but for 7 days","e579a108":" ### <a id=\"2.5\">2.5 *Close & Open* Variable <\/a>\n  Data Description:   \n* close(float64) - the close price for the day (not adjusted for splits or dividends)  \n* open(float64) - the open price for the day (not adjusted for splits or dividends)  \n---\nWe now look at both, close and open values in more detail","76aa5767":"Looking at the data: 6.340.206 unique source ids vs 9.328.750 total source ids we notice that some news appear repeated.  \nWe will now look at the one that appears repeated the most 'd7ad319ee02edea0'    ","b412506c":"### <a id=\"3.13\">3.13 *marketCommentary* Variable <\/a> \n**Data Description:** marketCommentary(bool) - boolean indicator that the item is discussing general market conditions, such as \"After the Bell\" summaries[](http:\/\/)","d0740480":"## <a id=\"4\">4. Workflow <\/a>\n### <a id=\"4.1\">4.1 `get_prediction_days` function <\/a> \n\nGenerator which loops through each \"prediction day\" (trading day) and provides all market and news observations which occurred since the last data you've received.  Once you call **`predict`** to make your future predictions, you can continue on to the next prediction day.\n\nYields:\n* While there are more prediction day(s) and `predict` was called successfully since the last yield, yields a tuple of:\n    * `market_observations_df`: DataFrame with market observations for the next prediction day.\n    * `news_observations_df`: DataFrame with news observations for the next prediction day.\n    * `predictions_template_df`: DataFrame with `assetCode` and `confidenceValue` columns, prefilled with `confidenceValue = 0`, to be filled in and passed back to the `predict` function.\n* If `predict` has not been called since the last yield, yields `None`.","084b040e":"### <a id=\"3.14\">3.14 *sentenceCount* Variable <\/a> \n**Data Description:** sentenceCount(int16) - the total number of sentences in the news item. Can be used in conjunction with firstMentionSentence to determine the relative position of the first mention in the item.","2ac81cee":"### <a id=\"3.8\">3.8 *subjects* Variable <\/a> \n**Data Description:** subjects(category) - topic codes and company identifiers that relate to this news item. Topic codes describe the news item's subject matter. These can cover asset classes, geographies, events, industries\/sectors, and other types.","ccdcb715":"### <a id=\"3.6\">3.6 *takeSequence* Variable <\/a> \n**Data Description:** takeSequence(int16) - the take sequence number of the news item, starting at 1. For a given story, alerts and articles have separate sequences.","d087734f":"According to the competition's data description:\n> all rows are taken at 22:00 UTC  ","ee4a44ec":"**Types of Returns**\n*For example we'll look for the first stock in assetNames*\n1. Returns calculated close-to-close (from the closing time of one trading day to the closing time of anotherc & not adjusted) for 1 day. *returnsClosePrevRaw1*\n2. Returns calculated open-to-open (from the opening time of one trading day to the opening time of another & not adjusted) for 1 day. *returnsOpenPrevRaw1*\n![image.png](attachment:image.png)","7bca37f0":"# Two Sigma: Extended EDA \n**Contents**  \n[1. Introduction](#1)    \n&nbsp;&nbsp;&nbsp;&nbsp; [1.1 End-to-End Usage Example](#1.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [1.2 *get_training_data* Function](#1.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [1.3 Understanding the target](#1.3)  \n[2. Exploration - Market Train](#2)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.0 Snapshot - Market Train](#2.0)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.1 *time* Variable](#2.1)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.2 *Asset Code* Variable](#2.2)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.3 *Asset Name*   Variable](#2.3)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.4 *volume* Variable](#2.4)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.5 *Close & Open* Variable](#2.5)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.6 *returnsClose & returnsOpen*  Variable](#2.6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.7 *universe* Variable](#2.7)   \n[3. Exploration - News Train](#3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [3.0 Snapshot - News Train](#3.0)    \n&nbsp;&nbsp;&nbsp;&nbsp; [3.1 *time* Variable](#3.1)    \n&nbsp;&nbsp;&nbsp;&nbsp; [3.2 *firstCreated* Variable](#3.2)    \n&nbsp;&nbsp;&nbsp;&nbsp; [3.3 *sourceId*   Variable](#3.3)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.4 *headline* Variable](#3.4)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.5 *urgency* Variable](#3.5)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.6 *takeSequence*  Variable](#3.6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.7 *provider* Variable](#3.7)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.8 *subjects* Variable](#3.8)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.9 *audiences* Variable](#3.9)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.10 *bodySize* Variable](#3.10)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.11 *companyCount* Variable](#3.11)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.12 *headlineTag* Variable](#3.12)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.13 *marketCommentary* Variable](#3.13)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.14 *sentenceCount* Variable](#3.14)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.15 *wordCount* Variable](#3.15)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.16 *assetCodes* Variable](#3.16)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.17 *assetName* Variable](#3.17)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.18 *firstMentionSentence* Variable](#3.18)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.19 *relevance* Variable](#3.19)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.20 *sentiment* Variable](#3.20)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.21 *sentimentPositive* Variable](#3.21)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.22 *sentimentWordCount* Variable](#3.22)   \n&nbsp;&nbsp;&nbsp;&nbsp; [3.23 *noveltyCountXXY* Variable](#3.23)   \n[4. Workflow strcuture](#4)    \n&nbsp;&nbsp;&nbsp;&nbsp; [4.1 *get_prediction_days* Function](#4.1)   \n&nbsp;&nbsp;&nbsp;&nbsp; [4.2 *predict* Function](#4.2)   \n&nbsp;&nbsp;&nbsp;&nbsp; [4.3 Main Loop Structure](#4.3)   \n&nbsp;&nbsp;&nbsp;&nbsp; [4.4 *write_submission_file* Function](#4.3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [4.5 Restart the Kernel to run your code again](#4.5)   \n&nbsp;&nbsp;&nbsp;&nbsp; [4.6 the1owl's LGBM](#4.6)  \n\n-----\n## <a id=\"1\">1. Introduction <\/a>\nIn this competition we will predict how stocks change based on market state and news articles.\n\nWe will loop through a long series of trading days; for each day, we will receive an updated state of the market, and a series of news articles which were published since the last trading day, along with impacted stocks and sentiment analysis.  We . will use this information to predict whether each stock will have increased or decreased ten trading days into the future.  Once you make these predictions, you can move on to the next trading day. \n\nThis competition is different from most Kaggle Competitions in that:\n1. We can **only** submit from **Kaggle Kernels**, and we may **not** use **other data sources**, GPU, or internet access.\n1.**two-stage competition**.  \n&nbsp;&nbsp;&nbsp;&nbsp;1. In **Stage One** we can **edit** our Kernel and **improve** our model, where **Public Leaderboard** scores are based on their predictions relative to past market data.   \n&nbsp;&nbsp;&nbsp;&nbsp;2. At the beginning of **Stage Two**, Kernels are locked, and our Kernels will then be **re-run over the next six months**, scoring them based on their predictions relative to live data as those six months unfold.\n1. We must use our custom **`kaggle.competitions.twosigmanews`** Python module.  The purpose of this module is to control the flow of information to ensure that you are not using future data to make predictions for the current trading day.    \n\n**Note that there are no limitations regarding model development.**\n\n### <a id=\"1.1\">1.1 End-to-End Usage Example<\/a>  \n\nIn this Starter Kernel, we'll show how to use the **`twosigmanews`** module to get the training data, get test features and make predictions, and write the submission file.  \n\n```\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\ntrain_my_model(market_train_df, news_train_df)\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():\n  predictions_df = make_my_predictions(market_obs_df, news_obs_df, predictions_template_df)\n  env.predict(predictions_df)\n  \nenv.write_submission_file()\n```  \n\nNote that `train_my_model` and `make_my_predictions` are functions you need to write for the above example to work.","c088b6d6":" ### <a id=\"2.4\">2.4 *Volume* Variable <\/a>\n Data Description: volume(float64) - trading volume in shares for the day  ","0e3ae664":"### <a id=\"3.3\">3.3 *sourceId* Variable <\/a> \n**Data Description:** sourceId(object) - an Id for each news item","88bd8eae":"We now look at the some value samples","c4b8f430":"### <a id=\"3.22\">3.22 *sentimentWordCount* Variable <\/a> \n**Data Description:** sentimentWordCount(int32) - the number of lexical tokens in the sections of the item text that are deemed relevant to the asset. This can be used in conjunction with wordCount to determine the proportion of the news item discussing the asset.  ","f44f9668":"All types seem to be float64 except for:\n\n* *time* -> Datetime\n* *AssetCode* -> object\n* *assetName* -> category  \nWe now look at the number of **NaN** in each field","05172307":"### <a id=\"2.2\">2.2 *Asset Code* Variable <\/a>\nData Description: assetCode(object) - a unique id of an asset  \nWe first check the total number of unique *asset Codes* ","a5ac28bd":"### <a id=\"3.11\">3.11 *companyCount* Variable <\/a> \n**Data Description:** companyCount(int8) - the number of companies explicitly listed in the news item in the subjects field","59c5b71b":"# Load train data","0b963588":"### <a id=\"3.20\">3.20 *sentiment* Variable <\/a> \n**Data Description:**  \n* sentimentClass(int8) - indicates the predominant sentiment class for this news item with respect to the asset. The indicated class is the one with the highest probability.\n* sentimentNegative(float32) - probability that the sentiment of the news item was negative for the asset\n* sentimentNeutral(float32) - probability that the sentiment of the news item was neutral for the asset","cb24f8ee":"We then look at volumes distribution","fb5a9915":"### <a id=\"3.2\">3.2 *firstCreated* Variable <\/a> \n**Data Description:** firstCreated(datetime64[ns, UTC]) - UTC timestamp for the first version of the item","dcfd30a7":"### <a id=\"3.17\">3.17 *assetName* Variable <\/a> \n**Data Description:** assetName(category) - name of the asset","0e62106a":"### <a id=\"1.2\">1.2 *get_training_data* Function<\/a>\n\nReturns the training data DataFrames as a tuple of:\n* `market_train_df`: DataFrame with market training data (In this kernet `mt_df` is used for convinience)\n* `news_train_df`: DataFrame with news training data (In this kernet `mt_df` is used for convinience)\n\nThese DataFrames contain all market and news data from February 2007 to December 2016.  See the [competition's Data tab](https:\/\/www.kaggle.com\/c\/two-sigma-financial-news\/data) for more information on what columns are included in each DataFrame.","7b1fc669":"### <a id=\"2.3\">2.3 *Asset Name* Variable <\/a>\nData Description: assetName(category) - the name that corresponds to a group of assetCodes. These may be \"Unknown\" if the corresponding assetCode does not have any rows in the news data.  \nWe first check the total number of unique *asset Name*","0ce6a562":"### <a id=\"3.4\">3.4 *headline* Variables <\/a> \n**Data Description:** headline(object) - the item's headline","45d01771":"Our market data has a total of 4.072.956 rows and 16 columns. We now look at datatypes.","5583dad9":"By multiplying total number of unique days (2.498) and unique asset codes (3.780) we obtain: **9.442.440 combinations vs dataset size of 4.072.956** which means that not all assets have been reported for all dates.  We now check some assetCode values to find out what they look like. Note that we already check that there are no nulls in this field.","e32c68be":"### <a id=\"3.15\">3.15 *wordCount* Variable <\/a> \n**Data Description:** wordCount(int32) - the total number of lexical tokens (words and punctuation) in the news item","946cf6dc":"### 1.8 Sample -  Apple Inc","727c05d9":"### <a id=\"3.19\">3.19 *relevance* Variable <\/a> \n**Data Description:** relevance(float32) - a decimal number indicating the relevance of the news item to the asset. It ranges from 0 to 1. If the asset is mentioned in the headline, the relevance is set to 1. When the item is an alert (urgency == 1), relevance should be gauged by firstMentionSentence instead.","57d5b0e0":"Both distributions seem normal and no fruther analysis seems required","d5ccb0db":"It seems that in about 80% of the cases both time and sourceTimestamp match"}}