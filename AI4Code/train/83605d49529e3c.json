{"cell_type":{"ef5150c7":"code","1a3277e1":"code","2744a60d":"code","024f883d":"code","ed5bf2ee":"code","11334c7c":"code","9142dd1b":"code","422a95ae":"code","f42b6226":"code","c2b89b14":"code","3b6c0369":"code","abccf58c":"code","3d60ed28":"code","9063c69b":"code","284271fc":"code","a0694883":"code","7712186e":"code","9271ced0":"code","b4fb9f9e":"code","cf1f7207":"markdown","042c32fd":"markdown","f402ea15":"markdown","d846eb66":"markdown","a4cd92ad":"markdown","8fc4cd3e":"markdown"},"source":{"ef5150c7":"# Importing lib\n\nimport numpy as np \nimport pandas as pd ","1a3277e1":"# Reading data\ntrain= pd.read_csv('\/kaggle\/input\/av-janata-hack-payment-default-prediction\/train_20D8GL3.csv')\ntest= pd.read_csv('\/kaggle\/input\/av-janata-hack-payment-default-prediction\/test_O6kKpvt.csv')\nsample= pd.read_csv('\/kaggle\/input\/av-janata-hack-payment-default-prediction\/sample_submission_gm6gE0l.csv')","2744a60d":"#step1: correcting\n#Check data description, these have kinda non-existent labels,so needed fix\n#marriage col- should have 1,2,3 but it has an unlabelled 0 as well. replace 0 with 3(others category).\n#education col- similary education has unlabelled 6 and 0.\n#PAY_0 TO PAY_6 has ambiguity in form of -1 and -2 values.\n\nall_data = [train, test]     #to perform ops on train+test both\nfor df in all_data:\n    df['MARRIAGE'].replace({0 : 3},inplace = True)\n    df[\"EDUCATION\"].replace({6 : 5, 0 : 5}, inplace = True)\n    df[\"PAY_0\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_2\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_3\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_4\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_5\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_6\"].replace({-1 : 0, -2 : 0}, inplace = True)","024f883d":"#step2: completing- dealing with null values\nprint(train.isnull().sum())\nprint('-----------------')\nprint(test.isnull().sum())","ed5bf2ee":"#step3: creating (feature engg)\n#created Age bins out of Age but it didn't turned out to be an important feature, will update soon","11334c7c":"#step4: converting (encoding) + feature scaling here if using models other than tree based.\ncat_cols = [\"SEX\",\"MARRIAGE\",\"EDUCATION\"]\ntrain = pd.get_dummies(train, columns = cat_cols, prefix=['SEX','MARRIAGE','EDUCATION'])\ntest = pd.get_dummies(test, columns = cat_cols, prefix=['SEX','MARRIAGE','EDUCATION'])","9142dd1b":"#final check on the data\nprint(train.shape)\nprint(test.shape)\nprint('--------------------')\ntrain.head()","422a95ae":"#train.columns.tolist()","f42b6226":"#remove ID and target field from features list\nfeatures= [\n 'LIMIT_BAL',\n 'AGE',\n 'PAY_0',\n 'PAY_2',\n 'PAY_3',\n 'PAY_4',\n 'PAY_5',\n 'PAY_6',\n 'BILL_AMT1',\n 'BILL_AMT2',\n 'BILL_AMT3',\n 'BILL_AMT4',\n 'BILL_AMT5',\n 'BILL_AMT6',\n 'PAY_AMT1',\n 'PAY_AMT2',\n 'PAY_AMT3',\n 'PAY_AMT4',\n 'PAY_AMT5',\n 'PAY_AMT6',\n 'SEX_1',\n 'SEX_2',\n 'MARRIAGE_1',\n 'MARRIAGE_2',\n 'MARRIAGE_3',\n 'EDUCATION_1',\n 'EDUCATION_2',\n 'EDUCATION_3',\n 'EDUCATION_4',\n 'EDUCATION_5']","c2b89b14":"# splitting: for local validation, later will train model on all of train set without split\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y= train_test_split(train[features], train.default_payment_next_month,test_size= 0.2, random_state=12)","3b6c0369":"print(train_x.shape)\nprint(train_y.shape)\nprint('---------------')\nprint(test_x.shape)\nprint(test_y.shape)","abccf58c":"#modelling starts here\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import SGDClassifier\n\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score","3d60ed28":"models = [\n    #ensemble\n    AdaBoostClassifier(),\n    ExtraTreesClassifier(),\n    GradientBoostingClassifier(),\n    RandomForestClassifier(),\n    \n    #linear models\n    LogisticRegression(),\n          \n    XGBClassifier(),\n    LGBMClassifier(),\n    CatBoostClassifier()\n         ]","9063c69b":"df_models = pd.DataFrame(columns=['Model_name','ROC'])\n\ni=0\nfor model in models:\n    model.fit(train_x,train_y)\n    pred_y = model.predict(test_x)\n    proba = model.predict_proba(test_x)[:,1]\n    roc_score = roc_auc_score(test_y, proba)\n    name = str(model)\n    print(name[0:name.find(\"(\")])\n    df_models.loc[i,'Model_name']= name[0:name.find(\"(\")]\n \n    df_models.loc[i,'ROC']= roc_score\n    print(confusion_matrix(test_y,pred_y))\n    print(\"------------------------------------------------------------\")\n    i=i+1","284271fc":"df_models.sort_values('ROC', ascending=False)","a0694883":"#for submission: \n# Model blend from all three models and train on all of training dataset this time.\n\nmodel= GradientBoostingClassifier()\nmodel.fit(train[features],train.default_payment_next_month)\npp1= model.predict_proba(test[features])\n\nmodel2= LGBMClassifier()\nmodel2.fit(train[features],train.default_payment_next_month)\npp2= model2.predict_proba(test[features])\n\nmodel3= CatBoostClassifier()\nmodel3.fit(train[features],train.default_payment_next_month)\npp3= model3.predict_proba(test[features])","7712186e":"#using very simple average blend\npp_blend= (pp1 +pp2+pp3)\/3","9271ced0":"pp_blend","b4fb9f9e":"#submission file\nsub = pd.DataFrame({'ID':test['ID'],'default_payment_next_month':pp_blend[:,1]})\nsub.to_csv('blend cat+gradient+lgbm.csv',index=False)","cf1f7207":"### Thank you. Here are few things which could help score better.\n1. Use better validation strategy (stratified k fold)\n2. Use model stacking technique\n3. Use gridsearchcv or randomsearch for hypertuning of parameters\n4. Better feature engineering\n5. Scaling (Minmax scalar) could help linear models like logistic regression to perfom better","042c32fd":"**Data cleaning steps**\n1. Correcting: check for outliers or ambigous data and fix\n2. Completing: deal with missing values\n3. Creating: mainly feature engineering\n4. Converting: Encoding for categorical data","f402ea15":"#### No Null values in the dataset- step 2 done.","d846eb66":"#### All 4 steps of data cleaning process completed. \n### To do: Experiment with different encoding methods like target encoding and do feature engineering in later versions. ","a4cd92ad":"**Solution code for Analytics Vidhya Janata Hack 2020. This is a very basic approach using simple model blending technique. **\n\n**But this can act as a pipeline for any machine learning competition for beginners. Upvote it if you like it.**","8fc4cd3e":"### We have our top 3 models as Gradient boosting classifier, lgbm and catboost.\n#### TO DO: Hypertuning for all these models, pass categorical fields to catboost. Also, feature scaling could help logistic regression perform much better."}}