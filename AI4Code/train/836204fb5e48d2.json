{"cell_type":{"18fa52ef":"code","14492f36":"code","d742dfe4":"code","dbda1803":"code","beb81627":"code","f169ae0e":"code","b371f75c":"code","ad9bb207":"code","79affa6e":"code","b457f584":"markdown","b9eca843":"markdown","69724009":"markdown","dabb7214":"markdown","c4c9e556":"markdown","ccaeaa35":"markdown"},"source":{"18fa52ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14492f36":"# Import helpful libraries\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n#Importing CSV files\nX_train_full=pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/train.csv\")\nX_test_full=pd.read_csv(\"\/kaggle\/input\/ventilator-pressure-prediction\/test.csv\")","d742dfe4":"cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\nprint('Missing:',cols_with_missing)","dbda1803":"def feature_eng(df):\n    df['u_in_first']  = df.groupby('breath_id')['u_in'].transform('first')\n    df['u_in_mean']   = df.groupby('breath_id')['u_in'].transform('mean')\n    df['u_in_median'] = df.groupby('breath_id')['u_in'].transform('median')\n    df['u_in_last']   = df.groupby('breath_id')['u_in'].transform('last')\n    df[\"RCRatio\"] = df.R\/df.C\n    df['u_in_shifted'] = df.groupby('breath_id')['u_in'].shift(2).fillna(method=\"backfill\")\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df.drop(['id','breath_id'], axis=1, inplace=True)\n    return df\n\nX_train_full_fe=feature_eng(X_train_full)\nX_test_full_fe=feature_eng(X_test_full)","beb81627":"y_train_full=X_train_full_fe[['pressure']].to_numpy().reshape(-1, 80)\nX_train_full_fe.drop(['pressure'], axis=1, inplace=True)","f169ae0e":"RS = RobustScaler()\nX_train_full_fe = RS.fit_transform(X_train_full_fe)\nX_test_full_fe = RS.transform(X_test_full_fe)","b371f75c":"X_train_full_fe = X_train_full_fe.reshape(-1, 80, X_train_full_fe.shape[-1])\nX_test_full_fe = X_test_full_fe.reshape(-1, 80, X_train_full_fe.shape[-1])","ad9bb207":"EPOCH = 100\nBATCH_SIZE = 1024\nN_SPLITS=3\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\ntest_preds = []\nfor fold, (train_idx, test_idx) in enumerate(kf.split(X_train_full_fe,y_train_full)):\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = X_train_full_fe[train_idx],X_train_full_fe[test_idx]\n    y_train, y_valid = y_train_full[train_idx],y_train_full[test_idx]\n    model=keras.Sequential([\n        keras.layers.Input(shape=X_train_full_fe.shape[-2:]),\n        keras.layers.Bidirectional(keras.layers.LSTM(200, return_sequences=True)),\n        keras.layers.Bidirectional(keras.layers.LSTM(150, return_sequences=True)),\n        keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True)),\n        keras.layers.Dense(100, activation='relu'),\n        keras.layers.Dense(1), \n    ])\n    model.compile(optimizer='adam',loss='mae')\n    scheduler = ExponentialDecay(1e-3, 400*((len(X_train_full_fe)*0.8)\/BATCH_SIZE), 1e-5)\n    lr = LearningRateScheduler(scheduler, verbose=1)\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr])\n    #model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE)\n    test_preds.append(model.predict(X_test_full_fe).squeeze().reshape(-1, 1).squeeze())","79affa6e":"submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsubmission[\"pressure\"] = sum(test_preds)\/N_SPLITS\nsubmission.to_csv('submission.csv', index=False)\nprint('end')","b457f584":"# 3.Data preparation for LSTM\n","b9eca843":"# 0.Import libraries and CSV files","69724009":"# 1.EDA\n- Here, we are simply checking for missing values.\n- Please refer to my other code for EDA.\nhttps:\/\/www.kaggle.com\/shashimo\/ventilator-very-simple-eda-for-starter","dabb7214":"# 4.LSTM","c4c9e556":"# 2. Feature engeneering","ccaeaa35":"# 5.Make CSVfile"}}