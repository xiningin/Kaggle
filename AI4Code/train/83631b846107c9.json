{"cell_type":{"1624e359":"code","e37fd566":"code","c0309d14":"code","6fcf95b3":"code","2a8a4396":"code","819b0b7b":"code","4afc34d2":"code","3dba626b":"code","f59c89d4":"code","5eaa2829":"code","e3ba3aad":"code","6dbd08cc":"code","7461bc57":"code","b758f5ac":"code","9ac5cf47":"code","b8b691be":"markdown","b16b25c4":"markdown","f105c310":"markdown","f82557dc":"markdown","5e4cf98e":"markdown","bb1e01b0":"markdown","136cf49a":"markdown","70ef55df":"markdown","b5de95c3":"markdown","f6ec648c":"markdown","7eab5fec":"markdown","64b17753":"markdown"},"source":{"1624e359":"!pip install ftfy==5.8\n!pip install gensim==3.8.3\n!pip install scikit-learn==0.23.2\n!pip install scikit-optimize==0.8.1\n!pip install skorch==0.9.0\n!pip install torch==1.7.0","e37fd566":"# toorch module patch fix\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray\n\nimport skopt","c0309d14":"import time\nimport random\n\n# Data handling\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import loguniform\nfrom skopt.space import Integer, Real\n\n# Text modules\nimport re\nimport string\nfrom ftfy import fix_text\n\n# Embeddings\nfrom gensim.models.doc2vec import Doc2Vec\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom skorch import NeuralNetClassifier\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Hyperparameters\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skopt.searchcv import BayesSearchCV\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score","6fcf95b3":"XYtrain = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\")\nXYtest = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\")\nXsubmit = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\")\n\nprint(\"Training data shape:\", XYtrain.shape)\nprint(XYtrain.describe())\n\nprint(\"\\nTest data shape:\", XYtest.shape)\nprint(XYtest.describe())\n\nprint(\"\\nSubmission data shape:\", Xsubmit.shape)\nprint(Xsubmit.describe())\n\nXYtrain.sample(5)","2a8a4396":"print(\"Training data shape initially:\", XYtrain.shape)\nXYtrain = XYtrain.drop_duplicates(keep=\"first\")\nprint(\"Training data shape without duplicates:\", XYtrain.shape)\n\nprint(\"\\nTest data shape initially:\", XYtest.shape)\nXYtest = XYtest.drop_duplicates(keep=\"first\")\nprint(\"Test data shape without duplicates:\", XYtest.shape)","819b0b7b":"Xtrain = XYtrain.loc[:, \"review\"]\nYtrain = XYtrain.loc[:, \"positive\"]\n\nXtest = XYtest.loc[:, \"review\"]\nYtest = XYtest.loc[:, \"positive\"]\n\nXsubmit = Xsubmit.iloc[:, 0]\n\ndel XYtrain\ndel XYtest\n\nprint(\"Xtrain\", Xtrain.shape, \"Ytrain\", Ytrain.shape)\nprint(\"Xtest\", Xtest.shape, \"Ytest\", Ytest.shape)\nprint(\"Xsubmit\", Xsubmit.shape)","4afc34d2":"def preptext(txt):\n    # removing tags\n    txt = txt.replace(\"<br \/>\", \" \")\n    # fixing Mojibakes (See https:\/\/pypi.org\/project\/ftfy\/)\n    txt = fix_text(txt)\n    # converting case\n    txt = txt.lower()\n    # removing punctuation\n    txt = txt.translate(str.maketrans(\"\", \"\", string.punctuation))\n    # removing hyphens\n    txt = txt.replace(\" \u2014 \", \" \")\n    # replacing digits with a tag\n    txt = re.sub(\"\\d+\", \" <number> \", txt)\n    # removing double spaces\n    txt = re.sub(\" +\", \" \", txt)\n    return txt\n\n\ndef clean_split(data):\n    # let's apply this function to clean the dataset\n    data = data.apply(preptext)\n\n    # now I need a vector of words, I'll split those strings\n    data = data.apply(lambda x: x.split())\n\n    print(data.sample(3))\n    return data\n\n\nXtrain = clean_split(Xtrain)\nXtest = clean_split(Xtest)\nXsubmit = clean_split(Xsubmit)","3dba626b":"d2v = Doc2Vec.load(\"..\/input\/sentiment-analysis-pmr3508\/doc2vec\")\n\nprint(\"Embedding Xtrain\")\nsrt = time.perf_counter()\nXtrain = Xtrain.apply(d2v.infer_vector, steps=20)\nXtrain = pd.DataFrame(Xtrain.to_list())\nend = time.perf_counter()\nprint(\"Xtrain shape:\", Xtrain.shape)\nprint(f\"Embedding Xtrain done in {end - srt:.1f}s\\n\")\n\nprint(\"Embedding Xtest\")\nsrt = time.perf_counter()\nXtest = Xtest.apply(d2v.infer_vector, steps=20)\nXtest = pd.DataFrame(Xtest.to_list())\nend = time.perf_counter()\nprint(\"Xtest shape:\", Xtest.shape)\nprint(f\"Embedding Xtest done in {end - srt:.1f}s\\n\")\n\nprint(\"Embedding Xsubmit\")\nsrt = time.perf_counter()\nXsubmit = Xsubmit.apply(d2v.infer_vector, steps=20)\nXsubmit = pd.DataFrame(Xsubmit.to_list())\nend = time.perf_counter()\nprint(\"Xsubmit shape:\", Xsubmit.shape)\nprint(f\"Embedding Xsubmit done in {end - srt:.1f}s\\n\")\n\nXtrain.sample(5)","f59c89d4":"%%time\n\n# Model definition\nlogreg = LogisticRegression(solver=\"liblinear\")\n\n# Hyperparameters\nlogreg_hp = dict(\n    # Inverse of regularization strength; must be a positive float.\n    # Like in support vector machines, smaller values specify\n    # stronger regularization.\n    C=np.linspace(0.01, 10, 100),\n    # Used to specify the norm used in the penalization.\n    penalty=[\"l1\", \"l2\"],\n)\n\n# Research\nlogreg_researcher = RandomizedSearchCV(\n    logreg,\n    logreg_hp,\n    scoring=\"roc_auc\",\n    cv=2,\n    n_iter=50,\n    n_jobs=-1,\n)\nlogreg_results = logreg_researcher.fit(Xtrain, Ytrain)\n\n# Result\nprint(logreg_results.best_params_, logreg_results.best_score_)","5eaa2829":"%%time\n\n# Model definition\nknn = KNeighborsClassifier()\n\n# Hyperparameters\nknn_hp = dict(\n    # Number of neighbors to use.\n    n_neighbors=np.arange(151, 210, 1),\n)\n\n# Research\nknn_researcher = RandomizedSearchCV(\n    knn,\n    knn_hp,\n    scoring=\"roc_auc\",\n    cv=2,\n    n_iter=50,\n    n_jobs=-1,\n)\nknn_results = knn_researcher.fit(Xtrain, Ytrain)\n\n# Result\nprint(knn_results.best_params_, knn_results.best_score_)","e3ba3aad":"%%time\n\n# Model definition\nmlp_1h = MLPClassifier(early_stopping=True)\n\n# Hyperparameters\nmlp_1h_hp = dict(\n    # The ith element represents the number of neurons in the ith\n    # hidden layer.\n    hidden_layer_sizes=[(2 ** i,) for i in np.arange(6, 12)],\n    # L2 penalty (regularization term) parameter.\n    alpha=loguniform(0.000001, 0.1),\n)\n\n# Research\nmlp_1h_researcher = RandomizedSearchCV(\n    mlp_1h,\n    mlp_1h_hp,\n    scoring=\"roc_auc\",\n    cv=2,\n    n_iter=30,\n    n_jobs=-1,\n)\nmlp_1h_results = mlp_1h_researcher.fit(Xtrain, Ytrain)\n\n# Result\nprint(mlp_1h_results.best_params_, mlp_1h_results.best_score_)","6dbd08cc":"%%time\n\n# Model definition\nmlp_2h = MLPClassifier(early_stopping=True)\n\n# Hyperparameters\nmlp_2h_hp = dict(\n    # The ith element represents the number of neurons in the ith\n    # hidden layer.\n    hidden_layer_sizes=[\n        (2 ** i, 2 ** j)\n        for j in np.arange(6, 10)\n        for i in np.arange(6, 10)\n    ],\n    # L2 penalty (regularization term) parameter.\n    alpha=loguniform(0.000001, 0.1),\n)\n\n# Research\nmlp_2h_researcher = RandomizedSearchCV(\n    mlp_2h,\n    mlp_2h_hp,\n    scoring=\"roc_auc\",\n    cv=2,\n    n_iter=30,\n    n_jobs=-1,\n)\nmlp_2h_results = mlp_2h_researcher.fit(Xtrain, Ytrain)\n\n# Result\nprint(mlp_2h_results.best_params_, mlp_2h_results.best_score_)","7461bc57":"%%time\n\n# Model definition\nclass MLPNet(nn.Module):\n    def __init__(self, hidden1_dim=512, hidden2_dim=64, p=0.25):\n        super(MLPNet, self).__init__()\n\n        # 50 -> hidden1_dim -> hidden2_dim -> 2\n        self.fc1 = nn.Linear(50, hidden1_dim)\n        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n        self.fc3 = nn.Linear(hidden2_dim, 2)\n\n        self.dropout = nn.Dropout(p)\n\n    def forward(self, X, **kwargs):\n        fc_out = F.relu(self.fc1(X))\n        fc_out = self.dropout(fc_out)\n\n        fc_out = F.relu(self.fc2(fc_out))\n        fc_out = self.dropout(fc_out)\n\n        fc_out = self.fc3(fc_out)\n        soft_out = F.softmax(fc_out, dim=-1)\n\n        return soft_out\n\n\n# Model definition\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch_2h = MLPNet()\ntorch_2h.to(device)\ntorch_2h = NeuralNetClassifier(\n    torch_2h,\n    max_epochs=20,\n    lr=1e-4,\n    optimizer=optim.Adam,\n    optimizer__weight_decay=1e-4,\n    train_split=False,\n    verbose=0,\n    iterator_train__shuffle=True,\n)\n\n# Hyperparameters\ntorch_2h_hp = dict(\n    module__hidden1_dim=Integer(256, 2048),\n    module__hidden2_dim=Integer(64, 1024),\n    module__p=Real(0.1, 0.75, prior=\"uniform\"),\n    optimizer__weight_decay=Real(1e-10, 1e-2, prior=\"log-uniform\"),\n)\n\n# Research\ntorch_2h_researcher = BayesSearchCV(\n    torch_2h,\n    torch_2h_hp,\n    scoring=\"roc_auc\",\n    cv=2,\n    n_iter=30,\n    n_jobs=-1,\n    verbose=0,\n)\ntorch_2h_results = torch_2h_researcher.fit(\n    torch.tensor(Xtrain.values, dtype=torch.float32),\n    torch.tensor(Ytrain.values, dtype=torch.int64)\n)\n\n# Result\nprint(torch_2h_results.best_params_, torch_2h_results.best_score_)","b758f5ac":"srt = time.perf_counter()\nlogreg_test_score = roc_auc_score(\n    Ytest, logreg_results.predict_proba(Xtest)[:, 1]\n)\nend = time.perf_counter()\nprint(f\"Logistic Regression: {logreg_test_score:.5f}\")\nprint(f\"It took {end - srt:.1f}s\\n\")\n\nsrt = time.perf_counter()\nknn_test_score = roc_auc_score(\n    Ytest, knn_results.predict_proba(Xtest)[:, 1]\n)\nend = time.perf_counter()\nprint(f\"K-nearest Neighbors: {knn_test_score:.5f}\")\nprint(f\"It took {end - srt:.1f}s\\n\")\n\nsrt = time.perf_counter()\nmlp_1h_test_score = roc_auc_score(\n    Ytest, mlp_1h_results.predict_proba(Xtest)[:, 1]\n)\nend = time.perf_counter()\nprint(f\"Multi-layer Perceptron 1 Hidden: {mlp_1h_test_score:.5f}\")\nprint(f\"It took {end - srt:.1f}s\\n\")\n\nsrt = time.perf_counter()\nmlp_2h_test_score = roc_auc_score(\n    Ytest, mlp_2h_results.predict_proba(Xtest)[:, 1]\n)\nend = time.perf_counter()\nprint(f\"Multi-layer Perceptron 2 Hidden: {mlp_2h_test_score:.5f}\")\nprint(f\"It took {end - srt:.1f}s\\n\")\n\nsrt = time.perf_counter()\ntorch_2h_test_score = roc_auc_score(\n    Ytest,\n    torch_2h_results.predict_proba(Xtest.to_numpy(np.float32))[:, 1],\n)\nend = time.perf_counter()\nprint(\n    f\"PyTorch Multi-layer Perceptron 2 Hidden: {torch_2h_test_score:.5f}\"\n)\nprint(f\"It took {end - srt:.1f}s\\n\")","9ac5cf47":"Ysubmit = torch_2h_results.predict_proba(Xsubmit.to_numpy(np.float32))[:, 1]\nYsubmit = pd.DataFrame({\"positive\": Ysubmit})\nYsubmit.to_csv(\"submission.csv\", index=True, index_label=\"Id\")\nYsubmit","b8b691be":"### 3.1.3. Multi-layer Perceptron - 1 Hidden Layer","b16b25c4":"## 3.2. Metric Comparison\n\nBased on ```Test``` dataframe","f105c310":"### 3.1.5. PyTorch Multi-layer Perceptron - 2 Hidden Layers","f82557dc":"They are all ```pandas.Series``` now.\n\n## 2.1. Clean text and vectorize\n\nWe were given the below function to clean and prepare the dataset. This is an important step dealing with NLP, we could make use of NLTK or spaCy, but we rather a manual approach.","5e4cf98e":"# 2. Preprocessing\n\nThe nature of this dataset is different from previous, there might not be ```NAs```, ```NANs``` or ```Nulls```, this time we ought to notice duplicates.","bb1e01b0":"Data is ready!\n\n# 3. Modelling\n\n## 3.1. Models Definition and Hyperparameters Search\n\nNow, using ```sklearn.model_selection.RandomizedSearchCV```, we'll search for the best hyperparameters for each classifier maximizing Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\n### 3.1.1. Logistic Regression","136cf49a":"### 3.1.2. K-nearest Neighbors","70ef55df":"## 2.2. Embed words\n\nIt is a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension $f:\\; \\text{word} \\hookrightarrow \\mathbb{R}^{50}$.\n\nWe'll use a pre-trained Doc2Vec model.","b5de95c3":"# 4. Submission","f6ec648c":"# [Sentiment Analysis](https:\/\/www.kaggle.com\/c\/sentiment-analysis-pmr3508)\n\n[PMR3508](https:\/\/uspdigital.usp.br\/jupiterweb\/obterDisciplina?sgldis=PMR3508) - Machine Learning and Pattern Recognition\n\nProfessor Fabio Gagliardi Cozman\n\nPMR3508-2020-83 - [Vitor Gratiere Torres](https:\/\/github.com\/vitorgt\/PMR3508)","7eab5fec":"This assignment's goal is to tell whether an IMDb review is positive or negative (complementary).\n\nMy analysis takes the following steps:\n\n1. [Import data and python modules](#1.-Import-data-and-python-modules) (Acquisition)\n1. [Preprocessing](#2.-Preprocessing)\n    1. [Clean text and vectorize](#2.1.-Clean-text-and-vectorize) (Preprocessing)\n    1. [Embed words](#2.2.-Embed-words) (Representation)\n1. [Modelling](#3.-Modelling)\n    1. [Models Definition and Hyperparameters Search](#3.1.-Models-Definition-and-Hyperparameters-Search)\n        1. [Logistic Regression](#3.1.1.-Logistic-Regression)\n        1. [K-nearest Neighbors](#3.1.2.-K-nearest-Neighbors)\n        1. [Multi-layer Perceptron - 1 Hidden Layer](#3.1.3.-Multi-layer-Perceptron---1-Hidden-Layer)\n        1. [Multi-layer Perceptron - 2 Hidden Layers](#3.1.4.-Multi-layer-Perceptron---2-Hidden-Layers)\n        1. [PyTorch Multi-layer Perceptron - 2 Hidden Layers](#3.1.5.-PyTorch-Multi-layer-Perceptron---2-Hidden-Layers)\n    1. [Metric Comparison](#3.2.-Metric-Comparison)\n1. [Submission](#4.-Submission)\n\n# 1. Import data and python modules","64b17753":"### 3.1.4. Multi-layer Perceptron - 2 Hidden Layers"}}