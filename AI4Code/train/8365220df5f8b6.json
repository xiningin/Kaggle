{"cell_type":{"5d045209":"code","3df61eaf":"code","4707c379":"code","bfc78067":"code","d1a1c7e0":"code","5bbed33e":"code","e9bd2b38":"code","5d599717":"code","f01a27d1":"code","988e6ec7":"code","5ca2d4ef":"code","a7ee1f10":"code","fa7177f0":"code","4599d3f7":"code","f443bb37":"code","20ab172c":"code","6bac7432":"code","b9734f12":"code","25f357f9":"code","910b17e4":"code","7709aece":"code","9d8fe28b":"code","c02c070d":"code","0c38890c":"code","8676422d":"code","f8a6edb0":"code","3959634b":"code","6a12832c":"code","f9484a7c":"code","c7abf725":"code","994930f2":"code","4eec1827":"code","7900f02b":"code","72396bb3":"code","9a5dcf38":"code","ceff62a6":"code","dae4dec7":"code","717e779e":"code","4c4f8dc2":"code","f2313758":"code","2d63ef8b":"code","6e80d923":"code","f5825df8":"code","a56347f5":"code","58834937":"code","722124de":"code","896ab1ca":"code","69e0c065":"code","b5263873":"code","228ebb96":"code","d09cad49":"code","c41412d3":"code","f2176030":"code","b0386eb6":"code","28ba992c":"code","45dce446":"code","e99348d6":"code","3746fb40":"code","bf5c6be9":"code","b7fb5858":"code","82e25982":"code","730ffb78":"code","bef31541":"code","687e8860":"code","d593e510":"code","493594cd":"code","4fff7603":"code","82746362":"code","b0d2955c":"code","35f4b90f":"code","3a4ac8cd":"code","73e39519":"code","3e90e4b2":"code","5352c1a3":"code","174829ea":"code","ef096f1f":"code","52bec6e3":"code","074ee409":"code","737a32ca":"code","ea575ccf":"code","4763b1c7":"code","6d117d97":"code","089791fd":"code","4b4c3d37":"code","7957bdff":"code","9abe5541":"code","82fe3645":"code","967454e8":"code","7b1229e3":"code","9bd4c65a":"code","64233d9b":"code","f536aed4":"code","2d32f3f9":"code","0ffd67ac":"code","8c991927":"code","b7ee8968":"code","097ca501":"code","81f0ad49":"code","efc1c938":"code","3623396e":"code","e7d4befb":"code","8762fff5":"code","762492c9":"code","d97103f1":"code","597bf47f":"code","bfdb6619":"code","0691207e":"code","de3e5b20":"code","6fe7ac8f":"code","c61de858":"code","a2f1f5b5":"code","4e238579":"code","0adad72c":"code","e5879632":"code","7f6dfcfe":"code","6476eae6":"code","7e3bdcfe":"code","a8ac9a9b":"code","7a5ab7a6":"code","43f81174":"code","6a74dbb1":"code","84a91924":"code","206e30c4":"code","3d312c76":"code","a76fc3d9":"code","9eb5c24f":"code","5134369b":"code","2523330c":"code","18d3b292":"code","a0d4f4b8":"code","97af00aa":"code","596761c5":"code","f1c397e3":"code","29fbbd8f":"code","06d8b020":"code","89df0512":"code","5a6f7f1f":"code","8126d90c":"code","e17e89ac":"code","ddabafb5":"code","0acdb878":"code","0c79f5a9":"code","1220bd11":"code","d9f03aec":"code","cbe8bf16":"code","72c67583":"code","b1403fbe":"code","6cbbecc6":"code","afd1b163":"code","8b789750":"code","524d6cc5":"code","ea38960e":"code","e4fea33b":"code","81f4a30a":"code","d2a6bd38":"code","61875d3f":"code","cc83e996":"code","7e1dc36b":"code","bcc680a7":"code","77e22654":"code","42b78ceb":"code","23ba4331":"code","e11cc203":"code","f903c07a":"code","ae5374e7":"code","87323072":"code","e0a1de55":"code","b2c0f268":"code","4765ed20":"code","e0cf05b0":"code","cf6146a6":"code","3976c6d9":"code","34045929":"code","5270121c":"code","93a4d95d":"code","f4d58d79":"code","cdf17cdd":"code","73139797":"code","782c90ff":"code","66188d46":"code","a2f0d449":"code","e71d22c1":"code","540249e8":"code","c506b2be":"code","4ad0ed8f":"code","b9a3f118":"code","a1b93863":"code","cbe31e0c":"code","7a7f627d":"code","2173674d":"code","fa0e61ca":"code","3031bf72":"code","1f5f9c45":"code","eb1dfc33":"code","459bfd71":"code","fabe15cb":"code","fd227f65":"code","aae4a2a8":"code","dbd202c4":"code","5ccdab07":"code","98112d72":"code","ac8e6100":"code","6bfdae44":"code","07d30479":"code","d6cba426":"code","8debc53b":"code","d2650cbc":"code","db4de801":"markdown","2c097abc":"markdown","664f7c3b":"markdown","ed8ef815":"markdown","a2edfe3e":"markdown","6402b438":"markdown","040cb24c":"markdown","a1b84cfb":"markdown","a9806678":"markdown","aa1da7f6":"markdown","8dec591f":"markdown","cf2bf810":"markdown","2ca9fcbb":"markdown","242099d7":"markdown","887abcaa":"markdown","d34a77ae":"markdown","716ae089":"markdown","02ac3a59":"markdown","9bab78b0":"markdown","0f9cf177":"markdown","72075cac":"markdown","1ef9b560":"markdown","5e94a07e":"markdown","1a5e6e51":"markdown","e5fb99fe":"markdown","17eccd2e":"markdown","71c0757f":"markdown","7d32432e":"markdown","59d3e939":"markdown","f8f9b1e0":"markdown","f50714cb":"markdown","94e9ef63":"markdown","d3ef2af2":"markdown","2d56cd8c":"markdown","153dea20":"markdown","73cfe1c3":"markdown","12272cfb":"markdown","20dccb14":"markdown","72250830":"markdown","98d4087b":"markdown","068dc085":"markdown","7d97b548":"markdown","30ec5232":"markdown","25d10292":"markdown","ac66af1c":"markdown","368abd2c":"markdown","6a0968b5":"markdown","fd33a334":"markdown","13f52bb3":"markdown","0f7e86b0":"markdown","ecf2802b":"markdown","9c84b8e1":"markdown","770e59cd":"markdown","8a469f1b":"markdown","9e10fa17":"markdown","36aa0e96":"markdown","8eafa6cc":"markdown","99346cc4":"markdown","7bd461b1":"markdown","5dcc1e87":"markdown","4f4aa043":"markdown","dbed7dc5":"markdown","15577e9f":"markdown","fbd7fa2e":"markdown","5f955268":"markdown","630d06c1":"markdown","7dc6b3f0":"markdown","902296e8":"markdown","d58e8675":"markdown","ffc17de2":"markdown","d48c675d":"markdown","20c23ed6":"markdown","66e83076":"markdown","c656b2d3":"markdown","85593151":"markdown","8551ccb0":"markdown","a24692bc":"markdown","9f9d7de0":"markdown","b78e563c":"markdown","6baa5349":"markdown","97e65e5e":"markdown","e77468a1":"markdown","a26b44df":"markdown","8a268216":"markdown","adf0d1c2":"markdown"},"source":{"5d045209":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\n# fix the figure size for matplotlin\n#plt.rcParams['figure.figsize'] = [10, 8]","3df61eaf":"# load wrong image wont throw error\n\nimg = cv2.imread('djbfjdbf', 1)\n\nprint(img)","4707c379":"# load correct image\n\nimg = cv2.imread('..\/input\/chicky_512.png', 1)","bfc78067":"plt.imshow(img)\nplt.show()","d1a1c7e0":"# convert to RGB\n\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nprint('Image shape : ', img.shape)\n\nplt.imshow(img)","5bbed33e":"# Load the image in gray scale\n\nimg_gray = cv2.imread('..\/input\/chicky_512.png', 0)\n\nprint('Image shape: ', img_gray.shape)\n\n# since by default it is not in grayscale we will covert it into gray scale\nplt.imshow(img_gray, cmap='gray')","e9bd2b38":"cv2.imwrite('dog.png', img)","5d599717":"resize_img = cv2.resize(img, (600, 400))\n\nplt.imshow(resize_img)","f01a27d1":"ratio_resize_img = cv2.resize(img, (0,0), img, 0.5, 0.5)\n\nplt.imshow(ratio_resize_img)\n\nprint('Image size: ', ratio_resize_img.shape)","988e6ec7":"# flip the image\n\nflip_img = cv2.flip(img, 0)\n\nplt.imshow(flip_img)","5ca2d4ef":"blank_img =  np.zeros(shape=(512, 512, 3))\n\nblank_img.shape","a7ee1f10":"plt.imshow(blank_img)","fa7177f0":"cv2.rectangle(blank_img, pt1=(400, 300), pt2=(200, 100), color=(0, 255, 0), thickness=(5))\n\nplt.imshow(blank_img)","4599d3f7":"# print another rectange\ncv2.rectangle(blank_img, pt1=(100, 480), pt2=(20, 420), color=(54, 120, 80), thickness=(5))\n\nplt.imshow(blank_img)","f443bb37":"# cricle\n\ncv2.circle(blank_img, center=(120, 120), radius=60, color=(255, 0, 0), thickness=5)\n\nplt.imshow(blank_img)","20ab172c":"# Fill the circle or rectange by -1\n\ncv2.circle(blank_img, center=(400, 400), radius=60, color=(255, 0, 0), thickness=-1)\n\nplt.imshow(blank_img)","6bac7432":"# line\n\ncv2.line(blank_img, pt1=(0, 0), pt2=(440, 340), color=(100, 100, 0), thickness=10)\n\nplt.imshow(blank_img)","b9734f12":"# choose font \nfont = cv2.FONT_HERSHEY_COMPLEX\n\ncv2.putText(blank_img, text='Keep learning', org=(0, 100),fontFace=font, \n            fontScale=1.2, color = (0,2,177), thickness=2, lineType=cv2.LINE_4)\n\nplt.imshow(blank_img)","25f357f9":"# new blank image\n \nnew_blank = np.ones(shape=(400, 600, 3), dtype=np.int32)\n\nplt.imshow(new_blank)","910b17e4":"verticies =  np.array([ [100, 200], [250, 300], [400, 250], [500, 100] ], dtype=np.int32)\n\nverticies","7709aece":"# it is in two dimension\n\nverticies.shape","9d8fe28b":"# covert it into three dimension\n\npts = verticies.reshape(-1,1,2)\n\npts.shape, pts","c02c070d":"cv2.polylines(new_blank,[pts], isClosed = True, color = (15, 129, 0), thickness = 3)\n\nplt.imshow(new_blank)","0c38890c":"new_vertices = np.array( [ [250,700], [425, 400], [600, 700] ], np.int32)\nnew_vertices","8676422d":"# reshape\npts_2 = new_vertices.reshape(-1,1,2)\n\n# fillpoly\ncv2.fillPoly(new_blank,[pts_2], (0, 0, 255))\n\nplt.imshow(new_blank)","f8a6edb0":"cv2.rectangle(img, (410, 50), (100, 350), (0, 255, 0), 3)\n\nplt.imshow(img)","3959634b":"img2 = cv2.imread('..\/input\/building.jpg')\n\nplt.imshow(img2)","6a12832c":"to_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n\nplt.imshow(to_rgb)","f9484a7c":"to_hsv = cv2.cvtColor(img2, cv2.COLOR_RGB2HSV)\n\nplt.imshow(to_hsv)","c7abf725":"to_hsL = cv2.cvtColor(img2, cv2.COLOR_RGB2HLS)\n\nplt.imshow(to_hsv)","994930f2":"img_1 = cv2.imread('..\/input\/dog_backpack.png')\nimg_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)\n\nimg_2 = cv2.imread('..\/input\/watermark_no_copy.png')\nimg_2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB)","4eec1827":"plt.imshow(img_1)","7900f02b":"plt.imshow(img_2)","72396bb3":"# shape of both the image\n\nimg_1.shape, img_2.shape","9a5dcf38":"# resize both the image so that we can blend\n\nimg_1 = cv2.resize(img_1, (1000, 1000))\nimg_2 = cv2.resize(img_2, (1000, 1000))\n\n\nimg_1.shape, img_2.shape","ceff62a6":"blend = cv2.addWeighted(src1 = img_1, alpha = 0.8, src2 = img_2, beta = 0.1, gamma = 20)\n\nplt.imshow(blend)","dae4dec7":"# load image again\nimg_1 = cv2.imread('..\/input\/dog_backpack.png')\nimg_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)\n\nimg_2 = cv2.imread('..\/input\/watermark_no_copy.png')\nimg_2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB)","717e779e":"# make img_2 very small\n\nimg_2 = cv2.resize(img_2, (400, 400))\n\nplt.imshow(img_2)","4c4f8dc2":"# assing large image and small image to a variable\nlarge_image = img_1\nsmall_image = img_2","f2313758":"# overy lay small image on the big image\n\nlarge_image[0: small_image.shape[1], 0: small_image.shape[0]] = small_image\n\nplt.imshow(large_image)","2d63ef8b":"# load image again\nimg_1 = cv2.imread('..\/input\/dog_backpack.png')\nimg_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)\n\nimg_2 = cv2.imread('..\/input\/watermark_no_copy.png')\nimg_2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB)\n\n\n# make img_2  small in size\nimg_2 = cv2.resize(img_2, (400, 400))","6e80d923":"# img_1 shape y-axis = 1401, x = 934 \n\nimg_1.shape","f5825df8":"# x_offset = x of img_1x -  x axis of img_2\n\nx_offset =  934 - 600\ny_offset =  1401 - 600","a56347f5":"# create an ROI (Region of interest)\n\nrows, cols, channel = img_2.shape","58834937":"# check rows, columns, shape\n\nrows, cols, channel","722124de":"# grab the ROI\n\nroi =  img_1[y_offset : 1401, x_offset : 943]\n\nplt.imshow(roi)","896ab1ca":"# get the gray scale version of the image\n\nimg_2gray = cv2.cvtColor(img_2, cv2.COLOR_RGB2GRAY)\n\nplt.imshow(img_2gray, cmap='gray')","69e0c065":"# get pure white of mask\n\nmask_inverse = cv2.bitwise_not(img_2gray)\n\nplt.imshow(mask_inverse, cmap='gray')","b5263873":"# but the mask contain no channel\n\nmask_inverse.shape","228ebb96":"# covert it to 3 channel\n\nwhite_bg = np.full(img_2.shape, 255, np.uint8)","d09cad49":"bk = cv2.bitwise_or(white_bg, white_bg, mask = mask_inverse)","c41412d3":"bk.shape","f2176030":"plt.imshow(bk)","b0386eb6":"fg = cv2.bitwise_or(img_2, img_2, mask = mask_inverse)\n\n\nplt.imshow(fg)","28ba992c":"# which part of the large image do we want to blend (it is call Region of interest (ROI))\n\nnp.asarray(roi)\nnp.asarray(fg)\n \nfinal_roi = cv2.bitwise_not(fg, roi)","45dce446":"plt.imshow(final_roi)","e99348d6":"th = cv2.imread('..\/input\/leuvenA.jpg', 0)\n\nplt.imshow(th, cmap='gray')","3746fb40":"ret, thresh1 = cv2.threshold(th, 127, 255, cv2.THRESH_BINARY)\n\n\nret, thresh1","bf5c6be9":"plt.imshow(thresh1, cmap='gray')","b7fb5858":"ret, thresh2 = cv2.threshold(th, 127, 255, cv2.THRESH_BINARY_INV)\n\n\nplt.imshow(thresh2, cmap='gray')","82e25982":"ret, thresh3 = cv2.threshold(th, 127, 255, cv2.THRESH_TRUNC)\n\nplt.imshow(thresh3, cmap='gray')","730ffb78":"ret, thresh4 = cv2.threshold(th, 127, 255, cv2.THRESH_TOZERO)\n\nplt.imshow(thresh4, cmap='gray')","bef31541":"ret, thresh5 = cv2.threshold(th, 127, 255, cv2.THRESH_TOZERO_INV)\n\nplt.imshow(thresh5, cmap='gray')","687e8860":"# function to show big picture\n\ndef big_fig(img):\n    # make the figure size big\n    plt.figure(figsize=(14,14))\n    plt.imshow(img, cmap='gray')\n    plt.show()","d593e510":"cross_word =  cv2.imread('..\/input\/crossword.jpg', 0)\n\nbig_fig(cross_word)","493594cd":"# use binary threshold to make pic either white or black\n\nret, cross_thresh = cv2.threshold(cross_word, 170, 255, cv2.THRESH_BINARY)\n\n# call function to show image\nbig_fig(cross_thresh)","4fff7603":"th2 =  cv2.adaptiveThreshold(cross_thresh, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 8)\n\nbig_fig(th2)","82746362":"# blend cross_thresh and th2\n\nblend_img =  cv2.addWeighted(alpha = 0.2, src1=th2, beta = 0.9, src2 = cross_thresh, gamma = 12)\n\nbig_fig(blend_img)","b0d2955c":"# load image function\n\ndef load_image():\n    brick_img = cv2.imread('..\/input\/bricks.jpg').astype(np.float32) \/ 255\n    brick_img = cv2.cvtColor(brick_img, cv2.COLOR_BGR2RGB)\n\n    return brick_img\n\n# display image functio\ndef display_img(img):\n    plt.figure(figsize=(12,10))\n    plt.imshow(img, cmap='gray')\n    plt.show()","35f4b90f":"# call the functions\n\nloaded_img = load_image()\n\ndisplay_img(loaded_img)","3a4ac8cd":"# gamma value\ngamma = 0.6\n\ngamma_result1 = np.power(loaded_img, gamma)\n\ndisplay_img(gamma_result1)","73e39519":"# load image\ntext_img = load_image()\n\n# write text on the image\ncv2.putText(text_img, text= 'Strong', org = (20, 600),\n            fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 11,\n            color = (255, 0, 0), thickness = 4, lineType = cv2.LINE_AA)\n\ndisplay_img(text_img)","3e90e4b2":"# define a kernel ( we are manually choosing value)\n\n# we can play around with the kernel to see different type of blur in the image\n\nkernel = np.ones(shape=(5,5), dtype=np.float32) \/ 30\n\nkernel","5352c1a3":"# -1 representing the depth of the output image.\n\ndst = cv2.filter2D(text_img, -1, kernel)\n\ndisplay_img(dst)","174829ea":"# load image again\ntext_img2 = load_image()\n\n# write text on the image\ncv2.putText(text_img2, text= 'Strong', org = (20, 600),\n            fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 11,\n            color = (255, 0, 0), thickness = 4, lineType = cv2.LINE_AA)\n\nprint('reset image')","ef096f1f":"text_img2_blur = cv2.blur(text_img2, ksize = (5,5))\n\ndisplay_img(text_img2_blur)","52bec6e3":"# when we increase the kernel size the blurring become more dense\n\ntext_img2_blur = cv2.blur(text_img2, ksize = (10,10))\n\ndisplay_img(text_img2_blur)","074ee409":"# load image again\ntext_img3 = load_image()\n\n# write text on the image\ncv2.putText(text_img3, text= 'Strong', org = (20, 600),\n            fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 11,\n            color = (255, 0, 0), thickness = 4, lineType = cv2.LINE_AA)\n\nprint('reset image')","737a32ca":"# gaussianblur\n\ntext_img3_blur = cv2.GaussianBlur(text_img3, (5,5), 10)\n\ndisplay_img(text_img3_blur)","ea575ccf":"# load image again\ntext_img4 = load_image()\n\n# write text on the image\ncv2.putText(text_img4, text = 'Strong', org = (20, 600),\n            fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 11,\n            color = (255, 0, 0), thickness = 4, lineType = cv2.LINE_AA)\n\nprint('reset image')","4763b1c7":"text_img4_blur = cv2.medianBlur(text_img4, 5)\n\ndisplay_img(text_img4_blur)","6d117d97":"# median blur in another image\n\ndog_img = cv2.imread('..\/input\/sammy.jpg')\ndog_img = cv2.cvtColor(dog_img, cv2.COLOR_BGR2RGB)\n\ndisplay_img(dog_img)","089791fd":"dog_img_blur = cv2.imread('..\/input\/sammy_noise.jpg')\n\nplt.figure(figsize=(15,15))\nplt.imshow(dog_img_blur)","4b4c3d37":"# in above image we can see some noise, we will try to remove it by median blur\n\ndog_img_gaussian = cv2.medianBlur(dog_img_blur, 5)\n\nplt.figure(figsize=(15,15))\nplt.imshow(dog_img_gaussian)","7957bdff":"# load image again\ntext_img5 = load_image()\n\n# write text on the image\ncv2.putText(text_img5, text = 'Strong', org = (20, 600),\n            fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 11,\n            color = (255, 0, 0), thickness = 4, lineType = cv2.LINE_AA)\n\nprint('reset image')","9abe5541":"text_img5_blur = cv2.bilateralFilter(text_img5, 9, 75, 75)\n\ndisplay_img(text_img5_blur)","82fe3645":"# function to generate white background and generate text\n\ndef bg_img_text():\n    bg = np.zeros((500, 600))\n    \n    cv2.putText(bg, text = 'Dope', org=(100, 280), \n                fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n                fontScale = 5, color= (255, 0, 0), thickness = 30)\n    \n    return bg\n\n# display image\ndef bg_load_img(img):\n    plt.figure(figsize=(12,10))\n    plt.imshow(img, cmap='gray')\n    plt.show()","967454e8":"img = bg_img_text()\nbg_load_img(img)","7b1229e3":"# create kernel\n\nkernel = np.ones((5,5), dtype=np.uint8)\n\nkernel","9bd4c65a":"result1 = cv2.erode(img, kernel, iterations=1)\n\nbg_load_img(result1)","64233d9b":"# with iteration 4\nresult2 = cv2.erode(img, kernel, iterations=4)\n\nbg_load_img(result2)","f536aed4":"result3 = cv2.dilate(img, kernel, iterations = 2)\n\nbg_load_img(result3)","2d32f3f9":"# create a background noisy image\n\nimg = bg_img_text()","0ffd67ac":"white_noise = np.random.randint(0, 2, (500, 600))\n\nwhite_noise","8c991927":"bg_load_img(white_noise)","b7ee8968":"white_noise = white_noise * 255","097ca501":"# mix the text image and the noise image\n\nnoise_img = white_noise  + img\n\nbg_load_img(noise_img)","81f0ad49":"# removing noise form the image\n\nopening = cv2.morphologyEx(noise_img, cv2.MORPH_OPEN, kernel)\n\nbg_load_img(opening)","efc1c938":"# create a foreground noisy image\n\nimg = bg_img_text()\n\n# foreground noise\nfg_noise = np.random.randint(0, 2, size=(500, 600))\n\nfg_noise","3623396e":"fg_noise = fg_noise * -255\n\nfg_noise","e7d4befb":"fg_noise_img = img + fg_noise\n\nbg_load_img(fg_noise_img)","8762fff5":"fg_noise_img[fg_noise_img == -255] = 0","762492c9":"bg_load_img(fg_noise_img)","d97103f1":"closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n\nbg_load_img(closing)","597bf47f":"img = bg_img_text()\n\ngradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n\nbg_load_img(gradient)","bfdb6619":"img = cv2.imread('..\/input\/sudoku.jpg', 0)\n\ndisplay_img(img)","0691207e":"# verticle line are more visible\n\nsobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize = 5)\n\ndisplay_img(sobelx)","de3e5b20":"# horizontal line are more visible\nsobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize = 5)\n\ndisplay_img(sobely)","6fe7ac8f":"# capture both horizontal and vetical line\n\nlaplacian = cv2.Laplacian(img, cv2.CV_64F)\n    \ndisplay_img(laplacian)","c61de858":"blend =  cv2.addWeighted(alpha = 0.5, src1=sobelx,gamma = 0.5, src2 = sobely, beta =5)\n\ndisplay_img(blend)","a2f1f5b5":"# threshold\n\nret, th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n\ndisplay_img(th1)","4e238579":"# morpohological operator\n\nkernel =  np.ones((4,4), dtype=np.uint8)\n\nmo = cv2.morphologyEx(blend, cv2.MORPH_GRADIENT, kernel)\n\ndisplay_img(mo)","0adad72c":"# load three images\nhorse = cv2.imread('..\/input\/horse.jpg')\nshow_horse = cv2.cvtColor(horse, cv2.COLOR_BGR2RGB)\n\nraimbow = cv2.imread('..\/input\/rainbow.jpg')\nshow_raimbow = cv2.cvtColor(raimbow, cv2.COLOR_BGR2RGB)\n\nbricks = cv2.imread('..\/input\/bricks.jpg')\nshow_bricks = cv2.cvtColor(bricks, cv2.COLOR_BGR2RGB)","e5879632":"fig, ax = plt.subplots(2,3, figsize=(16,10))\nax[0,0].imshow(horse)\nax[0,1].imshow(raimbow)\nax[0,2].imshow(bricks)\nax[1,0].imshow(show_horse)\nax[1,1].imshow(show_raimbow)\nax[1,2].imshow(show_bricks)\nplt.show()","7f6dfcfe":"hist_values = cv2.calcHist(bricks, channels = [0], mask=None, histSize = [256], ranges = [0, 256])\n\nhist_values.shape ","6476eae6":"plt.plot(hist_values)","7e3bdcfe":"hist_horse = cv2.calcHist(horse, channels = [0], mask =None, histSize = [256], ranges = [0,256])\n\nplt.plot(hist_horse)","a8ac9a9b":"img = horse\n\ncolors = ('b','g','r')\n\nfor i, col in enumerate(colors):\n    hist = cv2.calcHist([img], [i], None, [256],[0,256])\n    plt.plot(hist, color=col)","7a5ab7a6":"# raimbow image\nimg = raimbow\n\n# create mask\nmask = np.zeros(img.shape[:2], dtype=np.uint8)\n\nplt.imshow(mask, cmap='gray')","43f81174":"# in the above mask select some portion out of it\n\nmask[200: 300, 100: 380] = 255\n\nplt.imshow(mask, cmap='gray')","6a74dbb1":"masked_img = cv2.bitwise_and(img, img, mask = mask)","84a91924":"show_masked_img = cv2.bitwise_and(show_raimbow, show_raimbow, mask= mask)","206e30c4":"plt.imshow(show_masked_img)","3d312c76":"hist_mask_value_red = cv2.calcHist([raimbow], channels=[2],mask = mask, histSize=[256], ranges=[0,256])\n\nhist_value_red = cv2.calcHist([raimbow], channels=[2],mask = None, histSize=[256], ranges=[0,256])","a76fc3d9":"plt.plot(hist_mask_value_red, label='with mask RED value')\nplt.plot(hist_value_red, label ='without mask RED value')\nplt.legend()","9eb5c24f":"# new image \ngorilla = cv2.imread('..\/input\/gorilla.jpg', 0)\n\ndisplay_img(gorilla)","5134369b":"hist_values = cv2.calcHist([gorilla], [0], None, [256], [0,256])\n\nplt.plot(hist_values)","2523330c":"eq_hist = cv2.equalizeHist(gorilla)\n\ndisplay_img(eq_hist)","18d3b292":"eq_hist_values = cv2.calcHist([eq_hist], [0], None, [256], [0,256])\n\nplt.plot(eq_hist_values)","a0d4f4b8":"color_gorilla = cv2.imread('..\/input\/gorilla.jpg')\n\nshow_gorilla = cv2.cvtColor(color_gorilla, cv2.COLOR_BGR2RGB)\n\nplt.imshow(show_gorilla)","97af00aa":"# to equalize the histogram of a color image\/ to increase the contrast of an image \n# translate the image to HSV \n\nhsv =  cv2.cvtColor(color_gorilla, cv2.COLOR_BGR2HSV)","596761c5":"# grab the value channel\nhsv[:,:,2].max(), hsv[:,:,2].min()","f1c397e3":"hsv[:,:,2] = cv2.equalizeHist(hsv[:,:,2])","29fbbd8f":"# covert HSV to  RGB\n\neq_color_gorilla = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\nplt.imshow(eq_color_gorilla)","06d8b020":"# load the full image\nfull_img = cv2.imread('..\/input\/sammy.jpg')\n\nfull_img = cv2.cvtColor(full_img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(full_img)","89df0512":"# load the subset of image to match\nface_img = cv2.imread('..\/input\/sammy_face.jpg')\n\nface_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(face_img)","5a6f7f1f":"# shape of both the image\n\nfull_img.shape, face_img.shape","8126d90c":"methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR', 'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']","e17e89ac":"for m in methods:\n    \n    # create a copy of image\n    full_img_cpy = full_img.copy()\n    \n    method = eval(m)\n    \n    # TEMPLATE MATCHNG\n    res = cv2.matchTemplate(full_img_cpy, face_img, method)\n    \n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n    \n    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n        top_left = min_loc\n    \n    else:\n        top_left = max_loc\n        \n    height, width, channel = face_img.shape\n    \n    bottom_right = (top_left[0] + width, top_left[1] + height)\n    \n    # draw rectangle on the detected area\n    cv2.rectangle(full_img_cpy, top_left, bottom_right, (0, 255, 0), 8)\n    \n    # plot and show the image\n    plt.subplot(121)\n    plt.imshow(res)\n    plt.title('Matching Result')\n\n    plt.subplot(122),\n    plt.imshow(full_img_cpy)\n    plt.title('Detected Point')\n    plt.suptitle(m)\n    plt.show()","ddabafb5":"# read image\n\nflat_chess = cv2.imread('..\/input\/flat_chessboard.png')\nflat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_BGR2RGB)\n\nplt.imshow(flat_chess)","0acdb878":"gray_flat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_RGB2GRAY)\n\n\nplt.imshow(gray_flat_chess, cmap='gray')","0c79f5a9":"real_chess =  cv2.imread('..\/input\/real_chessboard.jpg')\n\nreal_chess = cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\n\nplt.imshow(real_chess)","1220bd11":"gray_real_chess = cv2.cvtColor(real_chess, cv2.COLOR_RGB2GRAY)\n\n\nplt.imshow(gray_real_chess, cmap='gray')","d9f03aec":"# covert it to float\ngray = np.float32(gray_flat_chess)\n\n# harris corner\ndst = cv2.cornerHarris(src=gray, blockSize=2, ksize=3, k=0.04)","cbe8bf16":"# just to show the corner we use dialat \n\n# dilate\ndst = cv2.dilate(dst, None)\n\nflat_chess[dst > 0.01 * dst.max()] = [255,0,0]\n\nplt.imshow(flat_chess)","72c67583":"gray2 = np.float32(gray_real_chess)\n\ndst2 = cv2.cornerHarris(gray2, 2, 3, 0.04)","b1403fbe":"dst2 = cv2.dilate(dst2, None)\n\nreal_chess[dst2 > 0.01  * dst2.max()] = [255, 0, 0]\n\nplt.imshow(real_chess)","6cbbecc6":"# Load image \nflat_chess = cv2.imread('..\/input\/flat_chessboard.png')\nflat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_BGR2RGB)\ngray_flat_chess = cv2.cvtColor(flat_chess, cv2.COLOR_RGB2GRAY)\n\nreal_chess =  cv2.imread('..\/input\/real_chessboard.jpg')\nreal_chess = cv2.cvtColor(real_chess, cv2.COLOR_BGR2RGB)\ngray_real_chess = cv2.cvtColor(real_chess, cv2.COLOR_RGB2GRAY)","afd1b163":"corners = cv2.goodFeaturesToTrack(gray_flat_chess, 70, 0.01, 10)","8b789750":"corners = np.int0(corners)\n\nfor i in corners:\n    x, y = i.ravel()\n    cv2.circle(flat_chess, (x,y), 3, (255, 0, 0), -1)\n    \n    \n# show image\nplt.imshow(flat_chess)","524d6cc5":"# detect corner in the real chess image\n\ncorners2 = cv2.goodFeaturesToTrack(gray_real_chess, 70, 0.01, 10)\n\ncorners2 = np.int0(corners2)\n\nfor i in corners2:\n    x, y = i.ravel()\n    cv2.circle(real_chess, (x,y), 3, (255, 0, 0), -1)\n    \n    \n# show image\nplt.imshow(real_chess)","ea38960e":"# load image\n\nimg = cv2.imread('..\/input\/sammy_face.jpg')\n\nplt.imshow(img)","e4fea33b":"# canny edge detector\nedge = cv2.Canny(image = img, threshold1 = 127, threshold2 = 127)\n\nplt.imshow(edge)","81f4a30a":"edge = cv2.Canny(image = img, threshold1 = 0, threshold2 = 255)\n\nplt.imshow(edge)","d2a6bd38":"# find the best threshold value\n\nmed_val = np.median(img)\n\ndisplay(med_val)\n\n# choose lower threshold value to either 0 or 70% of median value, whichever is greater\nlower = int(max(0, 0.7 * med_val))\n\n# upper threshold  to either 130% of the median or 255, which ever is small\nupper = int(min(255, 1.3 * med_val))","61875d3f":"edge = cv2.Canny(image = img, threshold1 = lower, threshold2 = upper)\n\nplt.imshow(edge)","cc83e996":"# Blur and the  apply canny edge detection\n\nblurred_img = cv2.blur(img, (4,4))\n\nedge = cv2.Canny(image = blurred_img, threshold1 = 127, threshold2 = 157)\n\nplt.imshow(edge)","7e1dc36b":"flat_chess = cv2.imread('..\/input\/flat_chessboard.png')\n\nplt.imshow(flat_chess)","bcc680a7":"# findChessBoardCorners is specifically work with chess board type image\n\nfound, corners = cv2.findChessboardCorners(flat_chess, (7,7))\n\nfound","77e22654":"cv2.drawChessboardCorners(flat_chess, (7,7), corners, found)\n\nplt.imshow(flat_chess)","42b78ceb":"dots = cv2.imread('..\/input\/dot_grid.png')\n\nplt.imshow(dots)","23ba4331":"found, corners = cv2.findCirclesGrid(dots, (10,10), cv2.CALIB_CB_SYMMETRIC_GRID)\n\nfound","e11cc203":"cv2.drawChessboardCorners(dots,(10, 10), corners, found)\n\nplt.imshow(dots)","f903c07a":"img = cv2.imread('..\/input\/internal_external.png')\n\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY);\n\nplt.imshow(img, cmap='gray')","ae5374e7":"img.shape","87323072":"# cv2.RETR_CCOMP extract both interal and external contour\ncontours, hierarchy = cv2.findContours(img_gray, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)","e0a1de55":"len(contours), type(hierarchy)","b2c0f268":"# external_contour = np.zeros(img.shape)\n\n# external_contour.shape\n\n# for i in range(len(contours)):\n#     if hierarchy[0][i][3] == -1:\n#         cv2.drawContours(external_contour, contours, i, 255, -1)\n\n#  plt.imshow(external_contour)","4765ed20":"cv2.drawContours(img, contours, -1, (255,0,0), 8)\n\nplt.imshow(img, cmap='gray')","e0cf05b0":"internel_contour = np.zeros(img.shape)\n\ninternel_contour.shape\n\nfor i in range(len(contours)):\n    if hierarchy[0][i][3] != -1:\n        cv2.drawContours(internel_contour, contours, i, 255, -1)\n\nplt.imshow(internel_contour)","cf6146a6":"reeses = cv2.imread('..\/input\/reeses_puffs.png', 0)\n\ndisplay_img(reeses)","3976c6d9":"cereals = cv2.imread('..\/input\/many_cereals.jpg', 0)\n\ndisplay_img(cereals)","34045929":"# create orb instance\norb = cv2.ORB_create()\n\nkp1, des1 = orb.detectAndCompute(reeses, None)\nkp2, des2 = orb.detectAndCompute(cereals, None)","5270121c":"# Create a matching object\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n# Match descriptors.\nmatches = bf.match(des1,des2)\n\n# Sort them in the order of their distance.\nmatches = sorted(matches, key = lambda x:x.distance)\n\n# Draw first 10 matches.\nreeses_match = cv2.drawMatches(reeses,kp1,cereals,kp2,matches[:20],None, flags=2)\n\ndisplay_img(reeses_match)","93a4d95d":"sift = cv2.SIFT_create()\n\nkp1, des1 = orb.detectAndCompute(reeses, None)\nkp2, des2 = orb.detectAndCompute(cereals, None)","f4d58d79":"bf = cv2.BFMatcher()\n\nmatches = bf.knnMatch(des1, des2, k=2)\n\n# ration test to check if the first match and the second match are close to each other or not\n# less distance better match\ngood = []\n\n# for match1, match2 in matches:\n#     # if match 1 distance is less then 75% of match 2 distance\n#     # if descriptor is a good match then apedn it to good \n#     if match1.distance < 0.75 * match2.distance:\n#         good.append(match1)","cdf17cdd":"len(good), len(matches)","73139797":"sift_matchs = cv2.drawMatchesKnn(reeses, kp1, cereals, kp2, matches, None, flags=2)\n\ndisplay_img(sift_matchs)","782c90ff":"sift = cv2.SIFT_create()\n\nkp1, des1 = orb.detectAndCompute(reeses, None)\nkp2, des2 = orb.detectAndCompute(cereals, None)","66188d46":"des1","a2f0d449":"# FLANN \nFLANN_INDEX_KDTREE = 0\nindex_parms = dict(algorithm=FLANN_INDEX_KDTREE, tree=5) \nsearch_parms = dict(checks=50)","e71d22c1":"flann = cv2.FlannBasedMatcher(index_parms, search_parms)\n\nmatches = flann.knnMatch(np.asarray(des1,np.float32),np.asarray(des2,np.float32),k=2)","540249e8":"good = []\n\nfor match1, match2 in matches:\n    if match1.distance < 0.7 * match2.distance:\n        good.append(match1)","c506b2be":"flann_matchs = cv2.drawMatches(reeses, kp1, cereals, kp2, good, None, flags=0)\n\ndisplay_img(flann_matchs)","4ad0ed8f":"sep_coins = cv2.imread('..\/input\/pennies.jpg')\n\n# call function to show image\ndisplay_img(sep_coins)","b9a3f118":"sep_blur = cv2.medianBlur(sep_coins, 25)\n\ndisplay_img(sep_blur)","a1b93863":"gray_sep_coin = cv2.cvtColor(sep_coins, cv2.COLOR_BGR2GRAY)\n\ndisplay_img(gray_sep_coin)","cbe31e0c":"ret, sep_thresh = cv2.threshold(gray_sep_coin, 180, 255, cv2.THRESH_BINARY_INV)\n\ndisplay_img(sep_thresh)","7a7f627d":"contours, hierarchy = cv2.findContours(sep_thresh.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE )\n\ncv2.drawContours(sep_coins, contours, -1, (255, 0 , 0), 10)\n\ndisplay_img(sep_coins)","2173674d":"img = cv2.imread('..\/input\/pennies.jpg')\n\nimg = cv2.medianBlur(img, 35)\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# apply otsu method in threshold \nret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\ndisplay_img(thresh)","fa0e61ca":"# noise removal option\n\nkernel = np.ones((3,3), dtype=np.uint8)\n\n# good way to reduce noise\nopening = cv2.morphologyEx(thresh , cv2.MORPH_OPEN, kernel, iterations=2)\n\ndisplay_img(opening)","3031bf72":"# sure background area\nsure_bg = cv2.dilate(opening,kernel,iterations=3)","1f5f9c45":"# DISTANCE TRANSFORM to seperate the coins\n# close to black will fade way, and closer to white will become brighter\n\ndist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n\nret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)","eb1dfc33":"# Finding unknown region\n\nsure_fg = np.uint8(sure_fg)\n\nunknown = cv2.subtract(sure_bg,sure_fg)","459bfd71":"# Marker labelling\nret, markers = cv2.connectedComponents(sure_fg)\n\n# Add one to all labels so that sure background is not 0, but 1\nmarkers = markers+1\n\n# Now, mark the region of unknown with zero\nmarkers[unknown==255] = 0","fabe15cb":"display_img(markers)","fd227f65":"markers = cv2.watershed(img,markers)\n\nimg[markers == -1] = [255,0,0]\n\ndisplay_img(markers)","aae4a2a8":"contours, hierarchy = cv2.findContours(markers.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE )\n\ncv2.drawContours(img, contours, -1, (255, 0 , 0), 10)\n\ndisplay_img(img)","dbd202c4":"nadia = cv2.imread('..\/input\/Nadia_Murad.jpg', 0)\ndenis = cv2.imread('..\/input\/Denis_Mukwege.jpg', 0)\nsolvay = cv2.imread('..\/input\/solvay_conference.jpg', 0)","5ccdab07":"plt.imshow(nadia, cmap='gray')","98112d72":"plt.imshow(denis, cmap='gray')","ac8e6100":"face_cascade = cv2.CascadeClassifier('..\/input\/haarcascades\/haarcascade_frontalface_default.xml')\nface_cascade","6bfdae44":"def adj_detect_face(cascade_type, img):\n    \n    face_img = img.copy()\n    \n    face_rects = cascade_type.detectMultiScale(face_img, scaleFactor =1.2, minNeighbors = 5)\n    \n    for (x, y, w, h) in face_rects:\n        cv2.rectangle(face_img, (x,y), (x+w, y+h), (255, 0, 0), 10)\n        \n    return face_img","07d30479":"result = adj_detect_face(face_cascade, nadia)\n\nplt.imshow(result, cmap='gray')","d6cba426":"eye_cascade = cv2.CascadeClassifier('..\/input\/haarcascades\/haarcascade_eye.xml')\n\nresult = adj_detect_face(eye_cascade, nadia)\n\nplt.imshow(result, cmap='gray')","8debc53b":"result = adj_detect_face(eye_cascade, denis)\n\nplt.imshow(result, cmap='gray')\n\n# In this case we are not able to dtect the eys because the color of the eyes is not learly visible","d2650cbc":"from sklearn import linear_model","db4de801":"## 6 methods for comparison\n- https:\/\/docs.opencv.org\/master\/df\/dfb\/group__imgproc__object.html#gga3a7850640f1fe1f58fe91a2d7583695dac5babb7dfda59544e3e31ea928f8cb16","2c097abc":"# 2.Brute force Matching with SIFT Descriptors and Ratio Test\nSIFT (Scale-Invariant Feature Transform)\n\nThe SIFT keypoints between two images are matched by identifying their nearest neighbors.\n\nBut in some cases, because of factors such as noise, the second closest match may seem to be closer to the first. In this case, we compute the ratio of closest distance to the second closest distance and check if it is above 0.8. If the ratio is more than 0.8, it means they are rejected.","664f7c3b":"# Thresholding\n* threshold() is used to convert grayscale images to binary image. ","ed8ef815":"# rectangle()","a2edfe3e":"# putText() Add text to the image","6402b438":"# External Counter","040cb24c":"# circle()","a1b84cfb":"# medianBlur()\n* The central element of the image is replaced by the median of all the pixels in the kernel area.\n* This operation processes the edges while removing the noise.\n* Each channel of a multi-channel image is processed independently.","a9806678":"### For examples on live webcam and videos please check it on my GitHub -> https:\/\/github.com\/benai9916\/openCV-in-python","aa1da7f6":"# Blurring and Smoothing\n* Blurring and smoothing often combine with edge detection\n* In **blurring**, we simple reduce the edge content and makes the transition form one color to the other very smooth.","8dec591f":"# equalization for color image","cf2bf810":"# Blending and pasting","2ca9fcbb":"# 7. Feature Matching\n\n### Three method\n1. Brute force Matching with ORB Descriptors\n2. Brute force Matching with SIFT Descriptors and Ratio Test\n3. FLANN based Matcher","242099d7":"## OpenCV\nCompared to other languages like C\/C++, Python is slower. But another important feature of Python is that it can be easily extended with C\/C++. This feature helps us to write computationally intensive codes in C\/C++ and create a Python wrapper for it so that we can use these wrappers as Python modules.\n\n#### This gives us two advantages:\u00a0\n* first, our code is as fast as original C\/C++ code (since it is the actual C++ code working in background) and\u00a0\n* Second, it is very easy to code in Python. This is how OpenCV-Python works, it is a Python wrapper around original C++ implementation.\n\nAnd the support of Numpy makes the task easier. All the OpenCV array structures are converted to-and-from Numpy arrays.\u00a0\n\nSo whatever operations you can do in Numpy, you can combine it with OpenCV, which increases the number of weapons in your arsenal. Besides that, several other libraries like SciPy, Matplotlib which supports Numpy can be used with this.\n\nhttps:\/\/docs.opencv.org\/4.0.0\/d4\/d15\/group__videoio__flags__base.html#gaeb8dd9c89c10a5c63c139bf7c4f5704d","887abcaa":"# 6. Contours Detection\n- A contour is a curve joining all the continuous points having same color or intensity, they represent the shapes of objects found in an image. ","d34a77ae":"# 3. FLANN based Matcher\nFLANN, meaning \"Fast Library for Approximate Nearest Neighbors\", will be much faster but will find an approximate nearest neighbors. It will find a good matching, but not necessarily the best possible one.","716ae089":"# bitwise_or","02ac3a59":"## findCirclesGrid","9bab78b0":"# bitwise_not()","0f9cf177":"# fillpoly()","72075cac":"# Morphological operator\n* Morphological transformations are some simple operations based on the image shape. \n- It is normally performed on binary images. \n- It needs two inputs, one is our original image, second one is called structuring element or kernel which decides the nature of operation. ","1ef9b560":"# 3. Edge detection\n\n## Canny Edge Detection\n - apply Gaussian filter to smooth the image in order to remove the noise","5e94a07e":"# to RGB","1a5e6e51":"# Image Gradient\n- image gradient is a directional change in the intensity or color in the image","e5fb99fe":"# Laplacian Derivatives","17eccd2e":"# line()","71c0757f":"# bilateralFilter()\n* A bilateral filter is used for smoothening images and reducing noise, while preserving edges. ","7d32432e":"# Histogram equalization","59d3e939":"## a) Harris corner Detection","f8f9b1e0":"## cvtColor() Change channel, color","f50714cb":"# Morphological Gradient\nIt is the difference between dilation and erosion of an image.","94e9ef63":"## Gamma correction (to increase or decrease brightness)\n- **Gamma correction** -> apply to an image to make it brighter or darker depending on gamma value\n\n## np.power()","d3ef2af2":"## THRESH_BINARY_INV","2d56cd8c":"# Drawing on image","153dea20":"# imwrite() Save the image","73cfe1c3":"## THRESH_BINARY","12272cfb":"# calcHist()  to calculate histogram","20dccb14":"# Object Detection\n\n## 1. Template matching\n- Template matching is a technique for finding areas of an image that are similar to a patch\n- A patch is a small image with certain features. \n- The goal of template matching is to find the patch\/template in an image.\n- It is the simplest form of object detection\n- we should have the subset of the same image to detect the object ","72250830":"# Sobel\nSobel operators is a joint Gausssian smoothing plus differentiation operation, so it is more resistant to noise. ","98d4087b":"# Creata a mask","068dc085":"# Create mask","7d97b548":"# threshold() \n* thresholding is the simplest method of segmenting images\n\nhttps:\/\/docs.opencv.org\/4.0.0\/d7\/d1b\/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a147222a96556ebc1d948b372bcd7ac59","30ec5232":"# Opening\n- Opening is just another name of erosion followed by dilation. \n- It is useful in removing noise. Here we use the function, cv2.morphologyEx()","25d10292":"## findContours()","ac66af1c":"# Draw a retangle on dog face","368abd2c":"# Blend image of different size","6a0968b5":"## 2. Corner detection\n- A corner can be interpreted as the junction of two edges, where an edge is a sudden change in image brighteness.\n\n### Corner detection algorithm\n- ***Harris Corner Detection*** --\n    Harris Corner Detector is just a mathematical way of determining which windows produce large variations when moved in any direction. With each window, a score R is associated. Based on this score, you can figure out which ones are corners and which ones are not.\n- ***Shi-Tomsi Corner Detection*** -- If we\u2019re scanning the image with a window just as we would with a kernel and we notice that there is an area where there\u2019s a major change no matter in what direction we actually scan, then we have a good intuition that there\u2019s probably a corner there.","fd33a334":"# Histograms","13f52bb3":"# imread() Load image\n* Does not throw any error even if we give the wrong file name.\n* If we print the value and we get \"None\" shows something is wrong","0f7e86b0":"# Convert RGB to HLS, HSV","ecf2802b":"# Internal Counter","9c84b8e1":"# 4. Grid Detection\n\n## findChessboardCorners()","770e59cd":"## THRESH_TOZERO","8a469f1b":"# 1. Brute force Matching with ORB Descriptors\n- Brute-Force matcher is simple. It takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.\n\n### ORB_create()","9e10fa17":"# filter2D","36aa0e96":"# ploylines() Draw polygon","8eafa6cc":"# 9. Watershed Algorithm\n- A marker-based watershed algorithm where you specify which are all valley points are to be merged and which are not.\n- It is an interactive image segmentation.\n- This algorithm is useful in segmenting images into background and foreground.","99346cc4":"## Do UPVOTE if you liked this kernel","7bd461b1":"# equalizeHist() Equalize the histogram\n- when we equalize the histogram we increase the contrast in the image","5dcc1e87":"## THRESH_TRUNC","4f4aa043":"# 1. Erosion erode()\n- The basic idea of erosion is just like soil erosion only, it erodes away the boundaries of foreground object (Always try to keep foreground in white). \n- So what does it do? \n  - The kernel slides through the image (as in 2D convolution). A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero).","dbed7dc5":"# Video","15577e9f":"## THRESH_TOZERO_INV","fbd7fa2e":"# Import Library","5f955268":"# 10. Face Detection","630d06c1":"## Resize by ratio","7dc6b3f0":"# calcHist() with mask","902296e8":"# Closing\n- Closing is reverse of Opening, Dilation followed by Erosion. \n- It is useful in closing small holes inside the foreground objects, or small black points on the object.","d58e8675":"## b) Shi-Tomasi","ffc17de2":"# overlay small image on top of a large image (no blending)\n## Numpy reassignment","d48c675d":"* **we can play around with the kernel to see different type of blur in the image\n**","20c23ed6":"## Matplotlib expect different order of RGB channel when compare with openCV\n* MATPLOTLIB  --> RGB (RED, GREEN, BLUE)\n* OPENCV --> BGR (BLUE, GREEN, RED)","66e83076":"## flip()","c656b2d3":"### steps to find contour using image processing technique\n1. Median blur\n2. Grayscale \n3. Binary Threshold\n4. Find contours","85593151":"# Dilation\n- It is just opposite of erosion. Here, a pixel element is \u20181\u2019 if atleast one pixel under the kernel is \u20181\u2019. \n- So it increases the white region in the image or size of foreground object increases. ","8551ccb0":"# imshow() Show image using Matplotlib","a24692bc":"# resize()","9f9d7de0":"# Find segment using watershed Algorithm","b78e563c":"# to HSV","6baa5349":"## Haar Cascades\nThe algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. \n For this, haar features shown in below image are used. They are just like our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under white rectangle from sum of pixels under black rectangle.\n ![haar_features.jpg](attachment:haar_features.jpg)\n- We need to load pre train haar cascades classifier","97e65e5e":"# adaptiveThreshold()\n* Adaptive thresholding is the method where the threshold value is calculated for smaller regions and therefore, there will be different threshold values for different regions.\n\n### ADAPTIVE_THRESH_GAUSSIAN_C","e77468a1":"# GaussianBlur()\n* The Gaussian filter is a low-pass filter that reduce the high-frequency components.","a26b44df":"# addWeighted() to blend two image\n\n- only work image with the same size","8a268216":"# blur()\n- The function smooths an image using the kernel","adf0d1c2":"# to HLS"}}