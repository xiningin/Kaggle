{"cell_type":{"44989a0e":"code","9117f201":"code","99af6b7f":"code","9404a2be":"code","d0f9c7d1":"code","6e312893":"code","5a684929":"code","d12af2e5":"code","b972e0f6":"code","28a1f553":"code","b8269cd6":"code","b94ac353":"code","4aac17c9":"code","ec332427":"code","21d15a4a":"code","9c7f6598":"code","a6a489c1":"code","ac696866":"code","642ec258":"code","15a4795b":"code","473cdb59":"code","5280e79b":"code","fde848ed":"code","d97e9e12":"code","48e3e810":"code","a629d574":"code","575aedb3":"code","b36b3463":"code","712ef430":"code","5d88d6e3":"code","60c43c04":"code","84afa4ad":"code","dc6abf0a":"code","89952573":"code","bb68b9f4":"code","6c1508a4":"code","f51c5fb4":"code","ad71fbe4":"code","1f1c6fde":"code","e8c7c6f0":"code","b7546121":"code","17722def":"code","363aa5f3":"code","3104c1b3":"code","290d7beb":"code","cefd873b":"code","6fb40e82":"code","6c71d101":"code","849e56ce":"code","999afb9e":"code","14ae3702":"code","8978c2d0":"code","eabbe749":"code","c53a02a4":"code","042caadc":"code","cfb3b537":"code","94e554a7":"code","338e7159":"code","9997e767":"code","68262138":"code","c1fe7268":"code","2171fcf8":"code","153ffac2":"markdown","56748fb7":"markdown","885a3836":"markdown","5395a8b8":"markdown","8e9ba358":"markdown","44290cd7":"markdown","532de471":"markdown","0aa0d87f":"markdown","d2036bca":"markdown","209f206b":"markdown","04adbfad":"markdown","3c0e6a60":"markdown","140da94f":"markdown","f2ce421a":"markdown","b6945657":"markdown","39175049":"markdown","8a6f2308":"markdown","beecff9d":"markdown","a85ece9d":"markdown","e0ecf8d5":"markdown","86ca8667":"markdown","9123bce4":"markdown","ee4cf8d7":"markdown","6b755400":"markdown","ba7c3f07":"markdown","7a2231b7":"markdown","0104881e":"markdown","64a80609":"markdown","df34e804":"markdown","92d8f2c9":"markdown","c15bb32d":"markdown","7c2939f1":"markdown","d64cbf40":"markdown","6c0121e5":"markdown","9a53e8aa":"markdown","388d539f":"markdown","a6e58ae0":"markdown","00309826":"markdown","161354da":"markdown","56bea6e6":"markdown","0f67cc50":"markdown","2a8e846f":"markdown","a27e3eb1":"markdown","0195cfd7":"markdown","aad2acfb":"markdown","4402e17c":"markdown","53d8a5f1":"markdown","276df3e0":"markdown","24605aba":"markdown","e086ab82":"markdown","4e5534dd":"markdown","1e02bfe8":"markdown","bcdee560":"markdown","47c4dda5":"markdown"},"source":{"44989a0e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  #import matplotlib for data viz.\n%matplotlib inline\n\nimport missingno as msno  #visualize missing values in the dataset\nimport seaborn as sns  #used for some specially-formatted plots such as distplot.\nimport scipy.stats as st  #used with seaborn distplot to fit different curves to the target distribution\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)  #seaborn distplot is deprecated but I still want to use it.  Turn off this warning.\n#set the matplotlib style\nplt.style.use(\"fivethirtyeight\")","9117f201":"train_filepath = \"..\/input\/30-days-of-ml\/train.csv\"\ntest_filepath = \"..\/input\/30-days-of-ml\/test.csv\"\ntrain_df = pd.read_csv(train_filepath,index_col=0)\ntest_df = pd.read_csv(test_filepath, index_col=0)\n\n# separate the categorical columns from the rest of the dataset for further exploration later on.\nobject_df_train = train_df.select_dtypes(include=['object'])\n\nobject_df_test = test_df.select_dtypes(include=['object'])\n\n#isolate the numeric variable columsn from the rest of the dataset for further exploration later on.\nnumeric_df_train = train_df.select_dtypes(include=['float','int64'])\n\nnumeric_df_test = test_df.select_dtypes(include=['float','int64'])","99af6b7f":"# train dataset dimensions\nprint(\"There are {} rows and {} columns in the training dataset.\".format(train_df.shape[0],train_df.shape[1]))","9404a2be":"# test dataset dimensions\nprint(\"There are {} rows and {} columns in the test dataset.\".format(test_df.shape[0],test_df.shape[1]))","d0f9c7d1":"#train head\ntrain_df.head()","6e312893":"#train tail\ntrain_df.tail()","5a684929":"#test head\ntest_df.head()","d12af2e5":"#test tail\ntest_df.tail()","b972e0f6":"print(\"There are {} columns in the training dataset.\".format(len(train_df.columns)))\ntrain_df.columns","28a1f553":"print(\"There are {} columns in the test dataset.\".format(len(test_df.columns)))\ntest_df.columns","b8269cd6":"col_diff_list = [x for x in train_df.columns if x not in test_df.columns]\nprint('The column(s) that are in the training dataset but not in the test dataset are: {}'.format(col_diff_list))","b94ac353":"#train dataset describe\ntrain_df.describe()","4aac17c9":"#test dataset describe\ntest_df.describe()","ec332427":"train_df.info()","21d15a4a":"test_df.info()","9c7f6598":"#nullity matrix training dataset\nmsno.matrix(train_df)","a6a489c1":"#nullity matrix test dataset\nmsno.matrix(test_df)","ac696866":"# In statistics, skewness is a degree of asymmetry observed in a probability distribution that deviates from the symmetrical normal distribution (bell\n# curve) in a given set of data.\ntrain_df.skew()","642ec258":"test_df.skew()","15a4795b":"train_df.kurt()","473cdb59":"test_df.kurt()","5280e79b":"#isolate the y variable\ny= train_df['target']\nplt.figure(1)\nplt.title('Target Fit With Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(2)\nplt.title(\"Target Fit With Normal Distribution\")\nsns.distplot(y, kde=False, fit=st.norm)\nplt.figure(3)\nplt.title(\"Target Fit Wit Log Normal Curve\")\nsns.distplot(y, kde=False, fit=st.lognorm)","fde848ed":"#write a function to display correlation data for both training and test datasets\ndef show_correlations(df,df_type = 'train'):\n    '''\n    Write a function that displays a dataframes correlation matrix and correlations with the target variable sorted by absolute value.\n    \n    '''\n    \n    correlations = df.corr()\n    \n    \n    \n    f , ax = plt.subplots(figsize = (14,14))\n\n    if df_type == 'train':\n        plt.title('Correlation of Numeric Variables with Target (Training Dataset)',y=1,size=16)\n    elif df_type == 'test':\n        plt.title('Correlation of Numeric Variables with Target (Test Dataset)',y=1,size=16)\n    else:\n        plt.title('Correlation of Numeric Variables with Target',y=1,size=16)\n    \n    if df_type == 'train':\n        sns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':10})\n        \n        corrs_sorted = correlations['target'].sort_values(ascending=False, key=abs).to_frame(name='Correlations With Target')\n        return(corrs_sorted[~corrs_sorted.index.isin(['target','id'])].style.apply(highlight_abs_max))\n    else:\n        sns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':10})","d97e9e12":"def highlight_abs_max(s):\n    '''\n    highlight the absolute maximum in a Series yellow.\n    '''\n    is_max = s == s.abs().max()\n    return ['background-color: yellow' if v else '' for v in is_max]","48e3e810":"#training dataset\nshow_correlations(numeric_df_train,df_type = 'train')","a629d574":"#test dataset correlations\nshow_correlations(numeric_df_test,df_type = 'test')","575aedb3":"#write a function to handle both categorical and continous distribution vizualizations\ndef visualize_distributions(df,df_type='cat',train_test='train'):\n    '''\n    Function used to visualize all column distributions in a grid\n    using matplotlib.\n    '''\n    plt.figure(1)\n    if df_type == 'cont':\n        plt.subplots(7,2, figsize=(14,14))\n        #if the dataset type is 'continuous' then plot histograms and use 'Numeric' in plot title\n        if train_test == 'train':\n            plt.suptitle(\"Numeric Variable Columns (Training Dataset)\", y=(1.6))\n        else:\n            plt.suptitle(\"Numeric Variable Columns (Test Dataset)\", y=(1.6))\n        numeric_df_minus_id_target = [x for x in df.columns if x != 'id' and x != 'target']\n        for i, item in enumerate(numeric_df_minus_id_target):\n            plt.subplot(7,2,i+1)\n            plt.hist(x=df[item])\n            plt.title(item)\n            plt.grid(True)\n            \n    elif df_type == 'cat':\n        plt.subplots(5,2, figsize=(14,14))\n        if train_test == 'train':\n            plt.suptitle(\"Categorical Variable Columns (Training Dataset)\", y=(1.6))\n        else:\n            plt.suptitle(\"Categorical Variable Columns (Test Dataset)\", y=(1.6))\n        for i, item in enumerate(df.columns):\n            plt.subplot(5,2,i+1)\n            temp_data_obj = df[item].value_counts()\n            plt.bar(x=temp_data_obj.index, height=temp_data_obj.values)\n            plt.title(item)\n            plt.grid(True)\n        \n    plt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.3,\n    wspace=0.35)\n","b36b3463":"#plot categorical variables distributions\nvisualize_distributions(numeric_df_train,df_type='cont',train_test='train')","712ef430":"visualize_distributions(numeric_df_test,df_type='cont',train_test='test')","5d88d6e3":"#plot categorical variables distributions\nvisualize_distributions(object_df_train,df_type='cat',train_test='train')","60c43c04":"visualize_distributions(object_df_test,df_type='cat',train_test='test')","84afa4ad":"#create a dataframe of just categorical columns and the target variable in order to display the target variable distributions by category variable\ncat_cols_plus_target = object_df_train.columns.to_list()\ncat_cols_plus_target.append('target')\ntrain_df_cat = train_df.copy()\n\ntrain_df_cat = train_df_cat[cat_cols_plus_target]","dc6abf0a":"plt.figure(1)\nplt.subplots(5,2, figsize=(14,14))\n\ncol_list = train_df_cat.columns.to_list()\ncol_list.remove('target')\nfor i, item in enumerate(col_list):\n    plt.subplot(5,2,i+1)\n    sns.boxplot(x=train_df_cat[item], y = 'target', data=train_df_cat)\n    plt.grid(True)\n        \nplt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.3,\n    wspace=0.35)","89952573":"unequal_cat_cols = []\nfor i, item in enumerate(object_df_train.columns.to_list()):\n    \n    if not set(object_df_train[item].unique()) == set(object_df_test[item].unique()):\n        print('{} column has a different set of categories in the testing and training datasets.  Needs cleaning.'.format(item))\n        unequal_cat_cols.append(item)\n\nif len(unequal_cat_cols) == 0:\n    print('All categorical columns are good.')\nelse:\n    print('Here are the categorical columns have different categories between training and test.')\n    print(unequal_cat_cols)","bb68b9f4":"high_cardinality_columns_train = [cname for cname in train_df.columns if train_df[cname].nunique() > 15 and train_df[cname].dtype == \"object\"]\nhigh_cardinality_columns_test = [cname for cname in test_df.columns if test_df[cname].nunique() > 15 and test_df[cname].dtype == \"object\"]\n\nif len(high_cardinality_columns_train) > 0 or len(high_cardinality_columns_test) > 0:\n    high_cards = set(high_cardinality_columns_train + high_cardinality_columns_test)\n    print('{} are the columns with high cardinality.'.format(high_cards))\nelse:\n    print('There are no categorical columns with high cardinality.')","6c1508a4":"#check out the datasets again as a reminder:\ntrain_df.head()","f51c5fb4":"#check out the test dataset as a reminder:\ntest_df.head()","ad71fbe4":"object_df_train.head()","1f1c6fde":"numeric_df_train.head()","e8c7c6f0":"#isolate the target variable into y\ny = train_df['target']\n#isolate the x's into a variable called \"all_features\"\nall_features = train_df.drop(['target'], axis=1)","b7546121":"# one-hot encode categorical columns\nX = all_features.copy()\n#the test dataset already has the target variable removed so just make a copy.\nX_test = test_df.copy()\n\n\n#One Hot encoding the categorical variables.\n#If you try to plug categorical variables into most models, you'll get an error.  So, we will need to preprocess these columns.\n#I don't know much about the dataset so I'm not going to assume and natural ordering of the categories in each categorical variable for now.  \n#I have also checked the cardinality and all categorical columns have less than 16 categories so they are relatively low.\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(all_features[object_df_train.columns.to_list()]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test_df[object_df_train.columns.to_list()]))\n\nOH_cols_train.index = all_features.index\nOH_cols_test.index = test_df.index","17722def":"#Remove the categorical columns from the original datasets\nnum_X_train = all_features.drop(object_df_train.columns.to_list(), axis=1)\nnum_X_test = test_df.drop(object_df_train.columns.to_list(), axis=1)","363aa5f3":"#put the one-hot encoding columns together with the numeric columns\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","3104c1b3":"OH_X_train.head()","290d7beb":"OH_X_test.head()","cefd873b":"numeric_df_train = numeric_df_train.drop(columns=['target'],axis=1)","6fb40e82":"numeric_df_train.columns","6c71d101":"mm_scaler = MinMaxScaler()\n#min-max scale the numeric columns only \nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(all_features[numeric_df_train.columns.to_list()]),index = all_features.index, columns = numeric_df_train.columns.to_list())\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df[numeric_df_train.columns.to_list()]),index= test_df.index, columns = numeric_df_train.columns.to_list())","849e56ce":"#now we have a scaled numeric dataframe for both train and test.  Add it to the one-hot encoded dataset from above.\ncat_OH_X_train = OH_X_train.drop(numeric_df_train.columns.to_list(), axis=1)\ncat_OH_X_test = OH_X_test.drop(numeric_df_train.columns.to_list(), axis=1)","999afb9e":"OH_MM_X_train = pd.concat([scaled_cols_train, cat_OH_X_train], axis=1)\nOH_MM_X_test = pd.concat([scaled_cols_test, cat_OH_X_test], axis=1)","14ae3702":"#check numeric variables to ensure they are all on the same scale between 0 - 1.\nOH_MM_X_train.describe()","8978c2d0":"OH_MM_X_test.describe()","eabbe749":"#instead of a 80\/20 split, I am using a much smaller dataset for hyperparameter tuning and model exploration.\nX_train, X_valid, y_train, y_valid = train_test_split(OH_MM_X_train, y, random_state=0,train_size=0.16, test_size=0.04)","c53a02a4":"\n#fit a single default Random Forest model as baseline.\nmodel = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))\n","042caadc":"def get_score(x_t,y_t,x_v,y_v,n_estimator_var):\n    \"\"\"Return the mean squared error for a Random Forest Regressor model.\n\n    \"\"\"\n    model = RandomForestRegressor(n_estimators=n_estimator_var, random_state=1)\n    \n    model.fit(x_t, y_t)\n    \n    preds_valid = model.predict(x_v)\n    print('Done:',n_estimator_var)\n    return(mean_squared_error(y_v, preds_valid, squared=False))","cfb3b537":"# model iteration commented out due to extreme runtime.  A copy of the result dictionary is hard coded below to\n# speed up run time when loading the notebook.\n#results = dict((key, get_score(X_train, y_train, X_valid, y_valid, key)) for key in range(50,1350,50))","94e554a7":"# all results originally gathered using the above get_score() function.  However, due to long runtime, \n# I am just saving the results here so the visualization below does not break.\nresults = {50: 0.7380701264989714, 100: 0.7346610312837812, 150: 0.7335280044475188, 200: 0.7326048634157486, 250: 0.7324100198288928, 300: 0.732170735675222, 350: 0.7319378082855204, 400: 0.7318678511715524, 450: 0.7315206318065365, 500: 0.7315209888399619, 550: 0.7314257963434759, 600: 0.73118585405109, 650: 0.7311601219600095, 700: 0.730976001432203, 750: 0.73103981378051, 800: 0.7309404914314043, 850: 0.7308860415870637, 900: 0.7308730391598484, 950: 0.7308478518349163, 1000: 0.7308373078529284, 1050: 0.7308187615347281, 1100: 0.730831433839403, 1150: 0.7308293618046933, 1200: 0.730812461401183, 1250: 0.7307844216936362, 1300: 0.7307479263706822}","338e7159":"#plotting the results of all get_score() results found above.  Plotting number of trees vs. mean squared error.\nplt.plot(list(results.keys()), list(results.values()))\nplt.title(\"Random Forest Regressor Model N Trees Vs. Mean Squared Error\")\nplt.xlabel(\"N Trees\")\nplt.ylabel(\"Mean Squared Error\")\nplt.show()","9997e767":"#re-define training dataset using 60\/40 split.\nX_train, X_valid, y_train, y_valid = train_test_split(OH_MM_X_train, y, random_state=0,train_size=0.48, test_size=0.12)","68262138":"#re-confirm the training dataset dimensions\nX_train.shape","c1fe7268":"# Define the model using between N Trees:  1050 trees\nmodel = RandomForestRegressor(n_estimators=1050, random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","2171fcf8":"\n# Use the model to generate predictions\npredictions = model.predict(OH_MM_X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': OH_MM_X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)\n","153ffac2":"<br>\n<br>\n\n<a id=\"section-nineteen\"><\/a>\n### Compare Categories In Training And Test\n\nWe already confirmed the training and testing datasets have the same columns.  Now make sure the same category values appear in the testing and training dataset categorical columns","56748fb7":"<a id=\"section-ten\"><\/a>\n### Missing Values","885a3836":"<br>\n\n<a id=\"model5\"><\/a>\n\n### Tuning Results","5395a8b8":"##### Results:\nresults = {50: 0.7380701264989714,\n 100: 0.7346610312837812,\n 150: 0.7335280044475188,\n 200: 0.7326048634157486,\n 250: 0.7324100198288928,\n 300: 0.732170735675222,\n 350: 0.7319378082855204,\n 400: 0.7318678511715524,\n 450: 0.7315206318065365,\n 500: 0.7315209888399619,\n 550: 0.7314257963434759,\n 600: 0.73118585405109,\n 650: 0.7311601219600095,\n 700: 0.730976001432203,\n 750: 0.73103981378051,\n 800: 0.7309404914314043,\n 850: 0.7308860415870637,\n 900: 0.7308730391598484,\n 950: 0.7308478518349163,\n 1000: 0.7308373078529284,\n 1050: 0.7308187615347281,\n 1100: 0.730831433839403,\n 1150: 0.7308293618046933,\n 1200: 0.730812461401183,\n 1250: 0.7307844216936362,\n 1300: 0.7307479263706822}\n \n ##### Best Model:\n ##### It does not appear I have reached the bottom of the curve quite yet.  I'm going to choose 1050 trees as best model for now and will focus on feature engineering for future iterations since most of the leaderboard top models are well within the range of 0.72 - 0.71 mean squared error.  Random Forest without further feature engineering will not cut it for this competition.  Also, there is a RAM limitation when trying to fit for than 1000 trees on this dataset when fully expanded out with dummy variables.  So, just using a small training size and 1050 trees for now and moving on with submission.","8e9ba358":"<a id=\"section-six\"><\/a>\n### Dataset Column Names, Datatypes, Missing Values:","44290cd7":"<br>\n\n<a id=\"model3\"><\/a>\n### Define A Scoring Function","532de471":"<br>\n\n<a id = \"model4\"><\/a>\n   \n### Model Tuning","0aa0d87f":"<a id=\"section-one\"><\/a>\n# Introduction","d2036bca":"<br>\n\n<a id=\"#section-sixteen\"><\/a>\n### Categorical Column Distributions","209f206b":"<br>\n<br>\n\n<a id=\"section-twenty\"><\/a>\n### Check For High\/Low Cardinality Categorical Columns\n\nIn general, one-hot encoding performs worse on categorical variables that have a large number of unique values.  Based on the *Intermediate Machine Learning* Kaggle course, we generally won't use one-hot encoding for variables with more than 15 different values.  Let's check for that.","04adbfad":"<br>\n\n##### Based on the above, we have plenty of training and testing data.  There are no missing data in any of the columns.  The training and testing data have the same columns.\n\n##### Additionally, it appears columns \"cat0\" - \"cat9\" are categorical columns while columns \"cont0\" - \"cont13\" are numeric columns.  \n\n<br>","3c0e6a60":"<a id=\"section-nine\"><\/a>\n### Info:","140da94f":"<a id=\"section-seventeen\"><\/a>\n### Bar Charts","f2ce421a":"<br>\n\n<a id=\"model2\"><\/a>\n### Single Baseline Model Test","b6945657":"<br>\n<br>\n\n<a id=\"preprocess4\"><\/a>\n\n### Min-Max Scaling Numeric Columns","39175049":"<br>\n<br>\n\n<a id=\"model1\"><\/a>\n# First Model","8a6f2308":"##### Based on the above correlations, there are not any strongly correlated continuous variables with the target variable.  The strongest correlation appears to be around 0.47, between cont12 and cont5 which is only mildly correlated. \n\n##### Visually, the training and test datasets seem about the same in terms of correlations.","beecff9d":"<br>\n\n<a id=\"model6\"><\/a>\n\n### Model Selection","a85ece9d":"##### All categories in each categorical column appear to have a median value around 8 which aligns with the target variable overall distribution.  Outliers appear to be mainly on the low end of the target value.","e0ecf8d5":"<a id=\"section-eleven\"><\/a>\n### Target Variable Skewness and Kurtosis:","86ca8667":"1. The training dataset has 300,000 rows and the testing dataset has 200,000 rows.  Plenty of samples to train with. It will likely take a long time to train complex models.\n2. There are no missing values in the training or test datasets.\n3. Training and test datasets have the same columns and data types except the test dataset does not have a 'target' column.\n4. The 'target' variable is has a relatively normal distribution.\n5. The 'target' column has a median of about 8.\n6. The numeric columns have very little skewness or only slight skewness.\n7. The categorical columns almost all have a median of about 8 with a few outliers.\n8. Categorical columns have a relatively low cardinality (i.e. less than 15 unique values).\n9. Correlation is relatively low or mild across all variables.\n10. Not all numeric column values are on the same scale.  Maybe want to scale these variables.","9123bce4":"##### We also already isolated the categorical and numeric columns.  Check these again as a reminder.\n","ee4cf8d7":"##### Visually, the training and test dataset categorical variables appear to be well balanced and have the same approximate distributions","6b755400":"#### There are 26 columns in the training dataset and only 25 columns in the testing dataset.  I understand this is a beginner Kaggle competition and the difference is likely the target dataset.  However, in order to be thorough, I will double-check to make sure all columns are the same except the target dataset in the training dataset.","ba7c3f07":"<br>\n<br>\n\n<a id=\"section-thirteen\"><\/a>\n### Numeric Column Correlation Matrix:","7a2231b7":"<a id=\"section-five\"><\/a>\n### Dataset Head\/Tail:","0104881e":"<br>\n<br>\n\n<a id=\"section-twenty-one\"><\/a>\n# EDA Findings\/Summary","64a80609":"* [Import Libraries](#import)\n* [Introduction](#section-one)\n* [Read In The Data](#section-two)\n* [Exploratory Data Analysis](#section-three)\n    - [Dataset Dimensions](#section-four)\n    - [Dataset Head\/Tail](#section-five)\n    - [Dataset Column Names, Datatypes, Missing Values](#section-six)\n    - [Column Names](#section-seven)\n    - [Describe](#section-eight)\n    - [Info](#section-nine)\n    - [Missing Values](#section-ten)\n    - [Target Variable Skewness and Kurtosis](#section-eleven)\n    - [Target Variable Distribution Plots](#section-twelve)\n    - [Numeric Column Correlation Matrix](#section-thirteen)\n        - [Numeric Columns Correlated With Target Variable Sorted Plus Correlation Matrix](#section-fourteen)\n    - [Numeric Column Distributions](#section-fifteen)\n    - [Categorical Column Distributions](#section-sixteen)\n        - [Bar Charts](#section-seventeen)\n        - [Box Plots](#section-eighteen)\n        - [Compare Categories In Training And Test](#section-nineteen)\n        - [Check For High\/Low Cardinality Categorical Columns](#section-twenty)\n* [EDA Findings\/Summary](#section-twenty-one)\n* [Preprocessing](#preprocess1)\n    - [Split Training Into X and Y](#preprocess2)\n    - [One-Hot Encoding Categorical Variables](#preprocess3)\n    - [Min-Max Scaling Numeric Columns](#preprocess4)\n    - [Train\/Test Split](#preprocess5)\n* [First Model](#model1)\n    - [Single Baseline Model Test](#model2)\n    - [Define A Scoring Function](#model3)\n    - [Model Tuning](#model4)\n    - [Tuning Results](#model5)\n    - [Model Selection](#model6)\n    - [Final Predictions](#model7)","df34e804":"<br>","92d8f2c9":"# Table of Contents","c15bb32d":"<br>\n\n<a id=\"preprocess5\"><\/a>\n### Train\/Test Split","7c2939f1":"<a id=\"section-eight\"><\/a>\n### Describe:","d64cbf40":"<br>\n\n<a id = \"preprocess2\"><\/a>\n### Split Training Into X and Y","6c0121e5":"<br>\n<br>\n\n<a id=\"import\"><\/a>\n# Import Libraries","9a53e8aa":"<br>\n\n<a id=\"section-two\"><\/a>\n# Read In The Data","388d539f":"##### Using the missingno package to visualize a nullity matrix.  \"*A nullity matrix is a data-dense display which lets you quickly pick out patterns in data completion.*\"\n##### See the github repo here [link](https:\/\/github.com\/ResidentMario\/missingno)","a6e58ae0":"<a id='section-seven'><\/a>\n#### Column Names:","00309826":"<br>\n\n##### Based on the above skewness number of 0.176487 and the above charts, I believe the 'target' variable is relatively symmetrical and follows a normal distribution.  We probably don't need to transform it if we want to do a regression.  ","161354da":"##### Most models don't play well with categorical variables.  We will need to encode these.  The most common encoders are ordinal or one-hot encoding (dummy).  Since I already checked the cardinality of all the categorical variables and found relatively low cardinality, I'm going to go ahead and use one-hot encoding.  We also have a large sample size so high-dimensionality is less of a concern.","56bea6e6":"##### \n##### Here is a good blog post about skewness and kurtosis:  [Link](https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/shape-of-data-skewness-and-kurtosis\/)\n##### Here is a link to a good Youtube video about skewness and kurtosis:  [Link](https:\/\/www.youtube.com\/watch?v=lK7nLzxiAQQ)","0f67cc50":"<br>\n\n<a id=\"preprocess3\"><\/a>\n### One-Hot Encoding Categorical Variables","2a8e846f":"### First step is to check the overall characteristics of the dataset","a27e3eb1":"#### Confirmed.  The columns in the training dataset are the same as the test dataset except for the 'target' column.  Now, let's check data types and check for any missing values.","0195cfd7":"<a id=\"section-four\"><\/a>\n### Dataset Dimensions:","aad2acfb":"<br>\n<br>\n\n<a id='preprocess1'><\/a>\n# Preprocessing","4402e17c":"<br>\n<br>\n\n<a id=\"section-fifteen\"><\/a>\n### Numeric Column Distributions","53d8a5f1":"<br>\n\n<a id=\"section-three\"><\/a>\n# Exploratory Data Analysis","276df3e0":"<br>\n\n##### Almost all the variables are very basically symmetrical (skewness less than 0.5).  Importantly, the skewness of the target variable is only 0.176487, indicating a pretty symmetrical target variable distribution.  Only variables cont4, cont8, and cont6 have a slight skewness to their distribution.\n\n##### Visually, the training and test datasets have about the same skewness.\n\n<br>","24605aba":"<a id='section-twelve'><\/a>\n### Target Variable Distribution Plots:","e086ab82":"<a id=\"section-fourteen\"><\/a>\n### Numeric Columns Correlated With Target Variable Sorted Plus Correlation Matrix","4e5534dd":"##### The original dataset for this competition deals with predicting the amount of an insurance claim.  I am expecting a some type of numeric y column with labels so I anticipate some type of regression model will be the first thing I try.\n\n##### In this first notebook, I am going to explore the dataset, do a quick preprocess and then fit a model","1e02bfe8":"<br>\n\n<a id=\"section-eighteen\"><\/a>\n### Box Plots","bcdee560":"#### Wow, a good amount of data in both the training and testing datasets","47c4dda5":"<br>\n\n<a id=\"model7\"><\/a>\n\n### Final Predictions"}}