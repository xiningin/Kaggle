{"cell_type":{"9e51f768":"code","ac251f2e":"code","942360eb":"code","b6c8d3a3":"code","6379dcbd":"code","0b409726":"code","4c7e0610":"code","af947e49":"code","2cf968ca":"code","d1a2af38":"code","910a1e57":"code","6b32994a":"code","46ec6975":"code","1b6e4169":"code","806e1503":"code","90e3798a":"code","3400a0ba":"code","4f91ec17":"code","1c670348":"code","d011d1f1":"code","b363acb4":"code","34c97e10":"code","8df9ed42":"code","a70d7643":"code","758997c7":"code","ca704219":"code","c5341587":"code","c1cb9a62":"code","95c90986":"code","b63146e1":"code","3ce8dc09":"code","98bddc89":"code","a7a8799e":"code","b2dd4a95":"code","3235eeab":"code","6af3c981":"code","1bcccc09":"code","585edcef":"code","d37e9f85":"code","6a98b9e0":"code","83561233":"code","9a75e67e":"code","5ce63d2f":"code","f2fb3227":"code","e3909c72":"code","59bd180a":"code","9d4c7a65":"code","a49f4ba9":"code","80bf14a4":"code","56b30df1":"code","f0d1a518":"code","9027f90b":"code","7ff1a664":"code","22af6944":"code","34d5238d":"code","b20b43ac":"code","13c5b2a2":"code","cd4c3fe4":"code","d6210eed":"code","be296f08":"code","a73c3466":"code","9f240185":"code","9792d111":"markdown","d08a8e04":"markdown","57a1a789":"markdown","2c77ee7b":"markdown","1eccfb65":"markdown","ca8367f9":"markdown","2dd43f2d":"markdown","4524dab7":"markdown","3a59c0a5":"markdown","a2117f78":"markdown","24d249ef":"markdown","5f89845f":"markdown","e47e13cf":"markdown","b71c3fa6":"markdown","064927a2":"markdown","0fa793b7":"markdown","c9bf8537":"markdown"},"source":{"9e51f768":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac251f2e":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# For visualization\nimport matplotlib as mpl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmpl.rcParams[\"patch.force_edgecolor\"]=True\nplt.style.use(\"seaborn-darkgrid\")\n%matplotlib inline\n\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\n\n# Models\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Evaluation\nfrom sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score","942360eb":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic-no-missing-valuescsv\/train_no_missing_values.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic-test-no-missing-valuescsv\/test_no_missing_values.csv\")","b6c8d3a3":"train_df.head()","6379dcbd":"# Make a copy of Passenger column for submission\npassengerId = test_df[\"PassengerId\"]\n# Drop Passenger column from dataframe\ntest_df.drop([\"PassengerId\"], axis=1, inplace=True)\n# Create copy of dataframe to the X_test variable\ntest = test_df.copy()\n# Check the head of dataframe\ntest.head()","0b409726":"# Concatinate SibSp and Parch and create new column\ntrain_df[\"Family\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntest[\"Family\"] = test[\"SibSp\"] + test[\"Parch\"]","4c7e0610":"# Drop Embarked, SibSp, Parch Column\ntrain_df = train_df.drop([\"SibSp\", \"Parch\"], axis=1)\ntest = test.drop([\"SibSp\", \"Parch\"], axis=1)","af947e49":"# Create X\/y\nX = train_df.drop(\"Survived\", axis=1)\ny = train_df['Survived']","2cf968ca":"X.head()","d1a2af38":"col_list = [x for x in X.columns if x!=\"Fare\" and x!=\"Age\"]\ncol_list","910a1e57":"X_dummies = pd.get_dummies(data=X,\n                           prefix=col_list,\n                           columns=col_list,\n                           drop_first=True)\n\ntest_dummies = pd.get_dummies(data=test,\n                              prefix=col_list,\n                              columns=col_list,\n                              drop_first=True)","6b32994a":"# Split X dataset into X_train, X_val sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_dummies, y, test_size=0.3, random_state=42)","46ec6975":"# Scale the data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_val = scaler.transform(X_val)","1b6e4169":"# Scale our test set for submission\ntest_scaled = scaler.transform(test_dummies)","806e1503":"base_gbc = GradientBoostingClassifier()\n\nbase_gbc.fit(scaled_X_train, y_train)","90e3798a":"base_gbc_pred = base_gbc.predict(scaled_X_val)\n\nbase_gbc_acc = round(accuracy_score(y_val, base_gbc_pred)*100, 2)\nprint(f\"Base Accuracy: {base_gbc_acc}\")","3400a0ba":"gbc_model = GradientBoostingClassifier()","4f91ec17":"# Hyperparameters\ngbc_grid_param = {\"n_estimators\": np.arange(10,300,20),\n                  \"min_samples_split\": np.arange(2,20,2),\n                  \"min_samples_leaf\": [1,3,5],\n                  \"max_depth\": [2,3,4],\n                  \"max_features\": [\"auto\",\"sqrt\", \"log2\"]}\n\n# Instantiate model\ngbc_grid = GridSearchCV(gbc_model,\n                        gbc_grid_param,\n                        scoring='accuracy',\n                        verbose=2,\n                        n_jobs=-1,\n                        cv=3)\n\n# Fit the model\ngbc_grid.fit(scaled_X_train, y_train)","1c670348":"gbc_grid.best_params_","d011d1f1":"gbc_grid.best_score_","b363acb4":"grid_gbc_pred = gbc_grid.predict(scaled_X_val)\n\ngrid_gbc_acc = round(accuracy_score(y_val, grid_gbc_pred)*100, 2)\nprint(f\"Grid Gradien Boost Accuracy: {grid_gbc_acc}\")","34c97e10":"base_rfc = RandomForestClassifier()\n\nbase_rfc.fit(scaled_X_train, y_train)","8df9ed42":"base_rfc_pred = base_rfc.predict(scaled_X_val)\n\nbase_rfc_acc = round(accuracy_score(y_val, base_rfc_pred)*100, 2)\nprint(f\"Base Accuracy: {base_rfc_acc}\")","a70d7643":"rfc_model = RandomForestClassifier()","758997c7":"rfc_params = {\"n_estimators\": np.arange(10,300, 30),\n                 \"min_samples_split\": np.arange(2,20, 2),\n                 \"min_samples_leaf\": [1,2,4],\n                 \"max_depth\": [2,3,4],\n                 \"max_features\": [\"auto\", \"sqrt\",\"log2\"]}\n\ngrid_rfc = GridSearchCV(rfc_model,\n                        rfc_params,\n                        scoring=\"accuracy\",\n                        verbose=2,\n                        n_jobs=-1,\n                        cv=3)\n\ngrid_rfc.fit(scaled_X_train, y_train)","ca704219":"grid_rfc.best_params_","c5341587":"grid_rfc.best_score_","c1cb9a62":"grid_rfc_pred = grid_rfc.predict(scaled_X_val)\n\ngrid_rfc_acc = round(accuracy_score(y_val, grid_rfc_pred)*100, 2)\nprint(f\"Base Accuracy: {grid_rfc_acc}\")","95c90986":"train_cat_feature = np.where(X_train.dtypes !=np.float)[0]\n\ntrain_pool = Pool(scaled_X_train,\n                  y_train)\n\ntest_cat_feature = np.where(X_val.dtypes !=np.float)[0]\ntest_pool = Pool(scaled_X_val,\n                 y_val)","b63146e1":"base_catboost = CatBoostClassifier(iterations=1000,\n                                   custom_loss=['Accuracy'],\n                                   loss_function='Logloss',\n                                   verbose=False)\n\nbase_catboost.fit(train_pool, eval_set=test_pool,plot=False)","3ce8dc09":"base_catboost.best_score_","98bddc89":"base_cat_pred = base_catboost.predict(scaled_X_val)\n\nbase_cat_acc = round(accuracy_score(y_val, base_cat_pred)*100, 2)\nprint(f\"Base Accuracy: {base_cat_acc}\")","a7a8799e":"catboost_model = CatBoostClassifier()","b2dd4a95":"param_grid = {'iterations': [100,500,1000],\n          'depth': [4, 5, 6],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'leaf_estimation_iterations': [10],\n          'eval_metric': ['Accuracy'],\n#           'use_best_model': ['True'],\n          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\n\ngrid_catboost = GridSearchCV(catboost_model,\n                             param_grid,\n                             scoring='accuracy',\n                             verbose=2,\n                             n_jobs=-1,\n                             cv=10)\n\ngrid_catboost.fit(scaled_X_train, y_train)","3235eeab":"grid_catboost.best_params_","6af3c981":"grid_catboost_pred = grid_catboost.predict(scaled_X_val)\n\ngrid_catboost_acc = round(accuracy_score(y_val, grid_catboost_pred)*100, 2)\nprint(f\"Grid CatBoost Accuracy: {grid_catboost_acc}\")","1bcccc09":"from sklearn.linear_model import LogisticRegression\n\nbase_lgreg = LogisticRegression(max_iter=1000)\n\nbase_lgreg.fit(scaled_X_train, y_train)","585edcef":"base_lgreg_pred = base_lgreg.predict(scaled_X_val)\n\nbase_lgreg_acc = round(accuracy_score(y_val, base_lgreg_pred)*100, 2)\nprint(f\"Grid CatBoost Accuracy: {base_lgreg_acc}\")","d37e9f85":"lgreg_model = LogisticRegression(max_iter=1000)","6a98b9e0":"param_grid = {\"C\":[100, 10, 1.0, 0.1, 0.01],\n              \"solver\": [\"lbfgs\", \"liblinear\"]}\n\ngrid_lgreg = GridSearchCV(lgreg_model,\n                          param_grid,\n                          scoring='accuracy',\n                          verbose=2,\n                          n_jobs=-1,\n                          cv=5)\n\ngrid_lgreg.fit(scaled_X_train, y_train)","83561233":"grid_lgreg.best_score_","9a75e67e":"grid_lgreg_pred = grid_lgreg.predict(scaled_X_val)\n\ngrid_lgreg_acc = round(accuracy_score(y_val, grid_lgreg_pred)*100, 2)\nprint(f\"Grid Log Regression Accuracy: {grid_lgreg_acc}\")","5ce63d2f":"from xgboost import XGBClassifier","f2fb3227":"base_xgb = XGBClassifier()\n\nbase_xgb.fit(scaled_X_train, y_train)\n","e3909c72":"base_xgb_pred = base_xgb.predict(scaled_X_val)\n\nbase_xgb_acc = round(accuracy_score(y_val, base_xgb_pred)*100, 2)\nprint(f'Base XGBoost Accuracy: {base_xgb_acc}')","59bd180a":"xgb_model = XGBClassifier()","9d4c7a65":"# A parameter grid for XGBoost\nparams =  {\"n_estimators\": [10,100, 1000],\n           \"reg_alpha\": [0, 0.75, 1, 1.25],\n           \"learning_rate\": [0.001, 0.01, 0.1],\n           \"subsample\": [0.5, 0.75, 1],\n           \"max_depth\": [3,7,9]\n        }\n\ngrid_xgb = GridSearchCV(xgb_model,\n                        params,\n                        scoring='accuracy',\n                        verbose=2,\n                        n_jobs=-1,\n                        cv=3)\n\ngrid_xgb.fit(scaled_X_train, y_train)","a49f4ba9":"grid_xgb.best_params_","80bf14a4":"grid_xgb.best_score_","56b30df1":"grid_xgb_pred = grid_xgb.predict(scaled_X_val)\n\ngrid_xgb_acc = round(accuracy_score(y_val, grid_xgb_pred)*100, 2)\nprint(f\"Grid XGBoost Accuracy: {grid_xgb_acc}\")","f0d1a518":"from sklearn.svm import SVC\nsvc_model = SVC()\n\ntest_parameters = {\n    \"C\": [1, 3, 10, 30, 100],\n    \"kernel\": [\"linear\", \"poly\", \"rbf\" , \"sigmoid\"],\n}\ngrid_svc = GridSearchCV(svc_model, test_parameters, scoring=\"f1\", cv=5, n_jobs=-1, verbose=2)\n\ngrid_svc.fit(scaled_X_train, y_train)","9027f90b":"grid_svc.best_score_","7ff1a664":"grid_svc_pred = grid_svc.predict(scaled_X_val)\n\ngrid_svc_acc = round(accuracy_score(y_val, grid_svc_pred)*100, 2)\nprint(f\"Grid SVC Accuracy: {grid_svc_acc}\")","22af6944":"from sklearn.ensemble import VotingClassifier","34d5238d":"ensemble_model = VotingClassifier(estimators=[\n    ('gradientBoost', gbc_grid.best_estimator_),\n    ('randomForest', grid_rfc.best_estimator_),\n    ('logRegression', grid_lgreg.best_estimator_),\n    ('xgBoost', grid_xgb.best_estimator_),\n    ('svc', grid_svc.best_estimator_)], voting='hard',weights=[0.6,0.3,0.3,0.4,0.1])\n\nensemble_model.fit(scaled_X_train, y_train)","b20b43ac":"ensemble_model.score(scaled_X_train, y_train)","13c5b2a2":"ensemble_model_pred = ensemble_model.predict(scaled_X_val) \n\nensemble_model_acc = round(accuracy_score(y_val, ensemble_model_pred)*100, 2)\nprint(f\"Ensemble model Accuracy: {ensemble_model_acc}\")","cd4c3fe4":"sc = StandardScaler()","d6210eed":"scale_X_dummies = sc.fit_transform(X_dummies)\nscaled_test_dummies = sc.transform(test_dummies)","be296f08":"best_ensemble_model = VotingClassifier(estimators=[\n    ('gradientBoost', gbc_grid.best_estimator_),\n    ('randomForest', grid_rfc.best_estimator_),\n    ('logRegression', grid_lgreg.best_estimator_),\n    ('xgBoost', grid_xgb.best_estimator_),\n    ('svc', grid_svc.best_estimator_)], voting='hard', weights=[0.6,0.3,0.3,0.4,0.1])\n\nbest_ensemble_model.fit(scale_X_dummies, y)","a73c3466":"sub_pred = pd.DataFrame({\"PassengerId\": passengerId,\n                         \"Survived\": best_ensemble_model.predict(scaled_test_dummies)})","9f240185":"sub_pred.to_csv(\"submission_ensemble_model.csv\", index=False)","9792d111":"### Submmit to Kaggle","d08a8e04":"#### GridSearchCV for Logistic Regressor","57a1a789":"#### 1. Gradient Boosting Classifier","2c77ee7b":"## Voting from few models and make predictions","1eccfb65":"### Scale the data","ca8367f9":"### CatBoost Classifier","2dd43f2d":"This data has been cleaned in my previous notebook https:\/\/www.kaggle.com\/godzill22\/my-first-project-titanic. In this one I just want to try new method of stacking models I've learnt from Ritesh Patil notebook. For me it was like eye-openning experience, you know that moment when you discover something which can yield much better results.","4524dab7":"### Logistic Regression","3a59c0a5":"#### GridSearchCV for GradientBoostingClassifier","a2117f78":"### Random Forest Regressor","24d249ef":"#### GridSearchCV fro Random Forest Classifier","5f89845f":"### XGBoost Classifier","e47e13cf":"### Random Forest Classifier","b71c3fa6":"### Create and test our model","064927a2":"### Support Vector Classifier SVC","0fa793b7":"Just by using VotingClassifier I improved place on the scoreboard from 14,646 to 6189.\nAfter ajusting  weights I had 4031 place on scoreboard.\n\n\nThank you for reading this notebook. Leave feedback and don't forget to vote or not. :)","c9bf8537":"#### GridSearch Fro CatBoost"}}