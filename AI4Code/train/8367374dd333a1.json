{"cell_type":{"714a82f9":"code","e5fe7a6d":"code","faa5ec21":"code","51c9c324":"code","31cf8c47":"code","450d1ad7":"code","efcfaacd":"code","0ade94df":"code","06c4e7d3":"code","33ab89f4":"code","beaa4d4a":"code","0ae479f2":"code","62550c11":"code","0ff8638c":"code","093f9d35":"code","b47337ba":"code","1fb2ff19":"code","bf301f74":"code","9c8a0a0c":"code","421f3e8c":"code","4825d7aa":"code","167570cc":"code","3d41b1ca":"code","f3e44e20":"code","21b1c5ff":"code","d0988279":"code","61665cdb":"code","0dc044ce":"code","c68b795e":"code","9be58b7a":"code","5a477f6d":"code","7d37ade9":"markdown","c59625c4":"markdown","ebdff03b":"markdown","a3725d32":"markdown","5a8aa19c":"markdown","cdff81a8":"markdown","e8250911":"markdown"},"source":{"714a82f9":"#\u4f7f\u7528\u7684numpy\u6a21\u5757\u4e2d\u7684\u968f\u673a\u51fd\u6570\nimport numpy as np \nimport pandas as pd\n#\u4ecemath\u5373\u6570\u5b66\u5e93\u4e2d\u5bfc\u5165\u7528\u4e8e\u5f00\u6839\u8fd0\u7b97\u7684\u65b9\u6cd5sqrt\nfrom math import sqrt\nimport math\n#\u51fd\u6570\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u504f\u5ea6\nfrom scipy.stats import skew\nfrom scipy import stats\nimport seaborn as sns\n#\u5185\u5d4c\u753b\u9762\nimport matplotlib.pyplot as plt\n#\u5b9e\u73b0\u591a\u5143\u7ebf\u6027\u56de\u5f52\nfrom sklearn import linear_model\n#\u6570\u636e\u9884\u5904\u7406\nfrom sklearn import preprocessing\n#\u5206\u5272\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\nfrom sklearn.model_selection import train_test_split\n#\u6a21\u578b\u8bc4\u4f30\uff0c\u6784\u5efa\u8bc4\u4f30\u51fd\u6570\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport warnings","e5fe7a6d":"#\u8bbe\u7f6e\u80cc\u666f\u6837\u5f0f\nplt.style.use(style='fivethirtyeight')\n#\u8bbe\u7f6e\u7ed8\u56fe\u5206\u8fa8\u7387\nplt.rcParams['figure.figsize'] = (10, 6)","faa5ec21":"# load the datasets into dataframe \u52a0\u8f7d\u6570\u636e\u96c6\u5230\u6570\u636e\u5e27\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","51c9c324":"# \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7f3a\u5931\u6570\u636e\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head(20)","31cf8c47":"# \ntrain = train.drop((missing_data[missing_data['Total']>1]).index,1)\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntrain.isnull().sum().max()","450d1ad7":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(40)","efcfaacd":"test = test.drop((missing_data1[missing_data1['Total']>4]).index,1)","0ade94df":"# \u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u7f3a\u5931\u6570\u636e\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(20)","06c4e7d3":"null_features = (missing_data1[missing_data1['Total']>0]).index\nnull_features","33ab89f4":"for feature in null_features:\n    test[feature] = test[feature].fillna(test[feature].mode()[0])","beaa4d4a":"#\u63cf\u8ff0\ntrain['SalePrice'].describe()","0ae479f2":"#\u6570\u503c\u578b\u6570\u636e\u9884\u5904\u7406\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u9009\u62e9\u7279\u5f81\nnumeric_cols = train.select_dtypes(include = [np.number])\n#\u83b7\u5f97\u4e24\u5217\u4e4b\u95f4\u7684\u76f8\u5173\u6027\ncorr = numeric_cols.corr()\nprint ('The Most Correlated Features with SalePrice:'), print (corr['SalePrice'].sort_values(ascending = False)[:10], '\\n')\nprint ('The Most Uncorrelated Features with SalePrice:'), print (corr['SalePrice'].sort_values(ascending = False)[-5:])","62550c11":"# \u5220\u9664\u7f3a\u5931\u503c\u767e\u5206\u6bd4\u5927\u4e8e80%\u7684\u5217\ntrain_percentage = train.isnull().sum() \/ train.shape[0]\nprint (train_percentage[train_percentage > 0.80])\ntrain = train.drop(train_percentage[train_percentage > 0.80].index, axis = 1)","0ff8638c":"#\u7528\u6d4b\u8bd5\u6570\u636e\u505a\u540c\u6837\u7684\u4e8b\ntest_percentage = test.isnull().sum() \/ test.shape[0]\nprint (test_percentage[test_percentage > 0.80])\ntest = test.drop(test_percentage[test_percentage > 0.80].index, axis = 1)","093f9d35":"# \u7f16\u7801\u5206\u7c7b\u53d8\u91cf\nle = preprocessing.LabelEncoder()\nfor name in train.columns:\n    if train[name].dtypes == 'O':\n        train[name] = train[name].astype(str)\n        le.fit(train[name])\n        train[name] = le.transform(train[name])","b47337ba":"# \u5bf9\u6d4b\u8bd5\u96c6\u505a\u540c\u6837\u7684\u5904\u7406\nfor name in test.columns:\n    if test[name].dtypes == 'O':\n        test[name] = test[name].astype(str)\n        le.fit(test[name])\n        test[name] = le.transform(test[name])","1fb2ff19":"# \u6839\u636e\u53d1\u751f\u7684\u6982\u7387\u6765\u586b\u8865\u7f3a\u5931\u7684\u6570\u503c\nfor column in train.columns:\n    null_vals = train.isnull().values\n    a, b = np.unique(train.values[~null_vals], return_counts = 1)\n    train.loc[train[column].isna(), column] = np.random.choice(a, train[column].isnull().sum(), p = b \/ b.sum())","bf301f74":"# \u5e94\u7528\u5bf9\u6570\u8f6c\u6362\uff0c\u901a\u8fc7\u53d6\u5bf9\u6570\uff08\u7279\u5f81+1\uff09\u6765\u51cf\u5c11\u8d85\u8fc70.75\u7684\u504f\u659c\u5ea6\u3002\nskewed_train = train.apply(lambda x: skew(x.dropna()))\nskewed_train = skewed_train[skewed_train > .75]\ntrain[skewed_train.index] = np.log1p(train[skewed_train.index])","9c8a0a0c":"# \u5904\u7406\u6d4b\u8bd5\u6570\u636e\u4e2d\u7684\u504f\u5ea6\u95ee\u9898\nskewed_test = test.apply(lambda x: skew(x.dropna()))\nskewed_test = skewed_test[skewed_test > .75]\ntest[skewed_test.index] = np.log1p(test[skewed_test.index])","421f3e8c":"# check the number of records and columns in both of datasets \u68c0\u67e5\u4e24\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u548c\u5217\u7684\u6570\u91cf\ntrain.shape, test.shape","4825d7aa":"X = train.drop(['SalePrice', 'Id'], axis = 1)\ny = train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","167570cc":"lr = linear_model.LinearRegression()","3d41b1ca":"model = lr.fit(X_train, y_train)","f3e44e20":"# make predictions based on model \u6839\u636e\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\npredictions = model.predict(X_test)","21b1c5ff":"print ('MAE is:', mean_absolute_error(y_test, predictions))\nprint ('MSE is:', mean_squared_error(y_test, predictions))\nprint ('RMSE is:', sqrt(mean_squared_error(y_test, predictions)))","d0988279":"# alpha helps to show overlapping data    alpha\u6709\u52a9\u4e8e\u663e\u793a\u91cd\u53e0\u7684\u6570\u636e\nplt.scatter(predictions, y_test, alpha = 0.7, color = 'b')\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')","61665cdb":"submission = pd.DataFrame()\nsubmission['Id'] = test['Id'].astype(int)","0dc044ce":"temp = test.select_dtypes(include = [np.number]).drop(['Id'], axis = 1).interpolate()","c68b795e":"predictions = model.predict(temp)","9be58b7a":"predictions = np.exp(predictions)\nsubmission['SalePrice'] = predictions","5a477f6d":"submission.to_csv('submission.csv', index = False)","7d37ade9":"The aobve line code shows that the average sale price of a house is close to 180,000 with most of the values falling within the 130,000 to 215,000 range. Next step is to show the relationship between the columns to examine the correlations between the features and the target.","c59625c4":"# Submission","ebdff03b":"# Linear Regression Model test","a3725d32":"# Modelling","5a8aa19c":"R-squared is the measure of how close the data are to the fitted regression line, in other words it measures the strength of the relationship between the model and the SalePrice on a convenient 0 \u2013 100% scale.","cdff81a8":"# Exploratory Data Analysis","e8250911":"In this initial investigations on data will be performed to to develop an understanding of the data, discover patterns and spot anomalies."}}