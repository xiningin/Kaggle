{"cell_type":{"db08cd22":"code","884f84bb":"code","348e0cac":"code","3e4aa0be":"code","89e76366":"code","5304a358":"code","8f1e69b9":"code","b98096bb":"code","a21c226d":"code","8172055b":"code","ec449efd":"code","387d51a1":"code","24b30e33":"code","d043c898":"code","3d07cf8e":"code","1c867e0b":"code","78dfacd4":"code","3f777ea0":"code","e1c3fce7":"code","303d1ef0":"code","f7da18d4":"code","47617050":"markdown","f05c3820":"markdown","9abb493d":"markdown","aacca260":"markdown","4bbb3b9e":"markdown","9b7a5261":"markdown","77dfecda":"markdown"},"source":{"db08cd22":"!pip install -q pyspark\n!apt-get install -y openjdk-11-jdk-headless -qq > \/dev\/null","884f84bb":"from IPython.core.display import HTML\ndisplay(HTML(\"<style>pre { white-space: pre !important; }<\/style>\"))","348e0cac":"import numpy as np\nimport pandas as pd\n\nimport time\nimport os\n\nfrom tqdm.notebook import tqdm","3e4aa0be":"os.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/java-11-openjdk-amd64\"\n\n!java -version","89e76366":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType, DateType\nfrom pyspark.sql.functions import array, col, explode, lit, create_map\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\nfrom itertools import chain","5304a358":"def count_na(data, normalize=False):\n    if normalize:\n        count = data.count()\n        return data.select([(F.count(F.when(F.isnan(c) | F.col(c).isNull() if t not in ('date', 'timestamp') else F.col(c).isNull(), c))\/count).alias(c) for c, t in data.dtypes])\n    return data.select([(F.count(F.when(F.isnan(c) | F.col(c).isNull() if t not in ('date', 'timestamp') else F.col(c).isNull(), c))).alias(c) for c, t in data.dtypes])","8f1e69b9":"# Printing few file name\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/DailyNAV'):\n    for filename in filenames[:10]:\n        print(os.path.join(dirname, filename))","b98096bb":"spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nsc = spark.sparkContext\nsc","a21c226d":"data_schema = StructType(\n    [StructField('Scheme_Code', IntegerType(), False), \n     StructField('Scheme_Name', StringType(), False), \n     StructField('ISIN Div Payout\/ISIN Growth', StringType(), False),\n     StructField('ISIN Div Reinvestment', StringType(), False),\n     StructField('NAV', DoubleType(), False),\n     StructField('Repurchase Price', DoubleType(), False), \n     StructField('Sale Price', DoubleType(), False), \n     StructField('Date', DateType(), False)]\n)\n\ndata = spark.read.csv('\/kaggle\/input\/DailyNAV', header = True, schema=data_schema)\ndata.cache()\ndata.printSchema()","8172055b":"data.rdd.getNumPartitions()","ec449efd":"data.show()","387d51a1":"%%time\nunique_date = data.select('Date').distinct().orderBy('Date').toPandas()['Date']\nunique_date = pd.to_datetime(unique_date)\nunique_date","24b30e33":"min_date = unique_date.min()\nmax_date = unique_date.max()\n\ndate_range = pd.date_range(min_date, max_date, freq='1D')\nassert set(date_range) == set(unique_date)\n\nmin_date = min_date.strftime(\"%Y-%m-%d\")\nmax_date = max_date.strftime(\"%Y-%m-%d\")\n\nprint(f'Data contains all days between {min_date} and {max_date}')","d043c898":"%%time\ndata_single_day = data.where('Date == \"2020-12-05\"')\ndata_single_day.show()","3d07cf8e":"%%time\nlatest_date = data.selectExpr('max(Date) as Date').collect()[0]['Date'].strftime(\"%Y-%m-%d\")\nprint(f'latest_date: {latest_date}')\n\nlatest_data = data.where(f'Date == \"{latest_date}\"')\nlatest_data.show()","1c867e0b":"%%time\naxis_long_eq = data.where('Scheme_Name == \"Axis Long Term Equity Fund - Direct Plan - Growth Option\"').orderBy('Date')\naxis_long_eq.show()","78dfacd4":"%%time\npandas_data = axis_long_eq.toPandas()\npandas_data['Date'] = pd.to_datetime(pandas_data['Date'])\npandas_data","3f777ea0":"pandas_data.info()","e1c3fce7":"pandas_data.query('NAV == 0')","303d1ef0":"# Observe for one date NAV is 0. Thus we will drop this obervation\npandas_data.loc[pandas_data['NAV'] == 0, 'NAV'] = np.nan","f7da18d4":"pandas_data.set_index('Date')['NAV'].plot(figsize=(20, 10));","47617050":"### Filter for a Mutual Fund","f05c3820":"### Filter on Latest Data","9abb493d":"### Filter for a date","aacca260":"## Loading Dataset","4bbb3b9e":"### Convert into Pandas\n\nCAUTION: pyspark doesn't keep data in memory but pandas does. So, below is Memory intensive command","9b7a5261":"### Validating All Days","77dfecda":"This notebook uses internet to install pyspark\n\nAdvantage of pyspark: We can load all csv file at once and use sql style query. This will enable us to easily filter the data and efficient from the point of view of updation.  \n\nPackage Documentation: https:\/\/spark.apache.org\/docs\/latest\/api\/python\/index.html"}}