{"cell_type":{"56bf9857":"code","c9f32b6f":"code","e3a9b218":"code","c58d405e":"code","b21a9c9c":"code","807fbf43":"code","b9b85327":"code","65272de9":"code","f53b3e6c":"code","47b56ac0":"code","a8c8d304":"code","800b26c9":"code","a5aaedee":"code","87f74ca1":"code","da955561":"code","31d2aacb":"code","cf9cdccb":"code","69af53b9":"code","bfa70295":"code","d5ed1619":"code","9a7ddaa0":"code","5d5e098a":"markdown","55af3310":"markdown","08338679":"markdown","f4bc07d4":"markdown","fbb58e87":"markdown","6adbbc96":"markdown","6234fb86":"markdown","4b23c67c":"markdown","8eef3d52":"markdown","67e3ad56":"markdown","fe4ae3f5":"markdown"},"source":{"56bf9857":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    filepath = dirname\n    list_filename = filenames\n    #for filename in filenames:\n    #    print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9f32b6f":"print(filepath)\nprint(list_filename)","e3a9b218":"#For making list of all the *_OCR.xml files in the data folder use glob or fnmatch+os.listdir()\nimport fnmatch\n\nfile_list = []\nfor file in list_filename:\n    if fnmatch.fnmatch(file,'*_ocr.xml'):\n        file_list.append(file)    ","c58d405e":"#Let's try to open one file and see how well does it help us:\nwith open(os.path.join(dirname,file_list[0]),'r') as f:\n    data = f.read()\nf.close()    ","b21a9c9c":"print(data)","807fbf43":"# The structure of xml(for our interest) is:\n# <TextRegion>\n#     <TextLine>\n#         <Word>\n#             <Unicode> Text <\/Unicode>\n#         <\/Word>\n#     <\/TextLine>\n# <\/TextRegion>","b9b85327":"from lxml import etree, objectify\n\nparser = etree.XMLParser(remove_blank_text=True)\ntree = etree.parse(os.path.join(dirname,file_list[0]))\nroot = tree.getroot()\n\n#This file contains xml name space which come attached with each tag, \n#xmlns makes it difficult for a reader to focus on desired tag, so we remove the namespaces\nfor elem in root.getiterator():\n    if not hasattr(elem.tag, 'find'): continue  \n    i = elem.tag.find('}')\n    if i >= 0:\n        elem.tag = elem.tag[i+1:]\nobjectify.deannotate(root, cleanup_namespaces=True)","65272de9":"#all the tag in xml are added to tag_list \ntag_list = []\nfor element in root.iter():\n    tag_list = element.tag","f53b3e6c":"## check the output if its in the same order as we need it\n# for element in root.iter('Word'):\n#     print('block and line info = ' + str(element.attrib))\n#     for child in element:\n#         #print(child.tag)\n#         if child.tag == 'TextEquiv':\n#             print('for text confidence score =' + str(child.attrib))\n#             for grandchild in child:\n#                 if grandchild.tag == 'Unicode':\n#                     print('text = '+str(grandchild.text))","47b56ac0":"# getting the information in the format in which we need it:\nimport re\nprev_block_Page = ''\nprev_block_Block = ''\nprev_block_Line = ''\nprev_block_Word = ''\nsentence = ''\nsentence2 = ''\nsentence3 = ''\nblock = []\nline = []\nfor element in root.iter('Unicode'):\n    same_page = same_block = same_line = next_word = False\n    parent_node = next(element.iterancestors('Word'))\n    block_list = parent_node.attrib['id'].split('_')\n    if(prev_block_Page == block_list[0] or prev_block_Page == ''):\n        same_page = True\n    if(prev_block_Block == block_list[1] or prev_block_Block == ''):\n        same_block = True        \n    if(prev_block_Line == block_list[2] or prev_block_Line == ''):\n        same_line = True\n    if(prev_block_Word == int(block_list[3][1:])-1 or prev_block_Word == ''):\n        next_word = True\n                    \n    #only same line present in one sentnece:\n    #Here we check if the sentence contains the keywords which we are looking for like 'Date'\n    if same_line and same_block:\n        sentence3 = sentence3 + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text) + ' ' \n    else:\n        line.append(sentence3)\n        sentence3 = ''\n        sentence3 = sentence3 + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text)+ ' '\n    \n    #same block in one line:\n    if same_block:\n        sentence = sentence + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text) + ' '\n    else:\n        block.append(sentence)\n        sentence = ''\n        sentence = sentence + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text)+ ' '  \n      \n    #all text in same line:\n    sentence2 = sentence2 + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text) + ' '\n        \n    prev_block_Page = block_list[0]\n    prev_block_Block = block_list[1]\n    prev_block_Line = block_list[2]\n    prev_block_Word = int(block_list[3][1:])\nprint(line)\nprint(block)    \nprint(sentence2)","a8c8d304":"filename = 'beautiful_data.txt'\nwrite_txt_to_file = open(filename,'a')\nwrite_txt_to_file.write(sentence2)\nwrite_txt_to_file.close()","800b26c9":"import csv\nfield = ['TEXT']\nrow = sentence\nfilename = 'beautiful_data.csv'\nwith open(filename,'a',newline='') as csvfile:\n    csvwriter = csv.writer(csvfile)#creates an object for writing in csv files\n    csvwriter.writerow(sentence)","a5aaedee":"###########################################################################################\n###########                                                                     ###########\n########### This is the summary of complete process, looping over all the files ###########                      \n###########                                                                     ###########\n###########################################################################################\n#Do the whole process on the list of all the files:\n#Reading the file saving the contents in data and proceeding \n\ntxt_filename = 'beautiful_data_summary.txt'\nfor xml_file in file_list:\n    parser = None\n    root = None\n    tree = None\n    new_sentence = ''\n    parser = etree.XMLParser(remove_blank_text=True)\n    xml_file_path = os.path.join(dirname,xml_file)\n    tree = etree.parse(xml_file_path)\n    root = tree.getroot()\n    #remove the namespaces\n    for elem in root.getiterator():\n        if not hasattr(elem.tag, 'find'): continue  \n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i+1:]\n    objectify.deannotate(root, cleanup_namespaces=True)\n    \n    for element in root.iter('Unicode'):\n        new_sentence = new_sentence + re.sub(\"[^0-9a-zA-Z:,]+\", ' ',element.text) + ' '\n    #print('new_sentence = '+new_sentence)    \n    #writing the new_sentence in the file \n    wrtie_txt_to_file = None\n    write_txt_to_file = open(txt_filename,'a')\n    write_txt_to_file.write(new_sentence)\n    write_txt_to_file.write('\\n')\n    write_txt_to_file.close()\n    ","87f74ca1":"import pandas as pd\npd.set_option('display.max_colwidth',3000)\ndf = pd.read_csv('.\/beautiful_data_summary.txt',delimiter='\/n',header=None)\ndf.head(7)","da955561":"import spacy\nimport spacy.cli\nspacy.cli.download(\"en\")\nnlp = spacy.load(\"en\")","31d2aacb":"# detecting date and filling it in date dictionary, directly puttin in a \n#series won't work because in case of multiple entry as list it will take only\n#the last entry:\ndate_dict = {}\n#invoice_no = {}\ncustomer_name_dict = {}\n#total_amt = {}\nfor i in range(0, df.shape[0]):\n  date_list = []\n  name_list = []\n  text = df.iloc[i,:]\n  doc = nlp(str(text))\n  for ent in doc.ents:\n      if ent.label_ == 'DATE':\n          date_list.append(ent.text)\n          #date_dict[i] = ent.text\n          #df['Date'] = ent.text\n      elif ent.label_ == 'PERSON':\n          name_list.append(ent.text)\n\n  date_dict[i] = date_list        \n  customer_name_dict[i] = name_list  ","cf9cdccb":"import pprint\n#pprint.pprint(customer_name_dict)\npprint.pprint(date_dict)","69af53b9":"stopper = 0\nfor key in date_dict:\n    stopper += 1\n    print(key)\n    print(date_dict[key])\n    for iter in range(0,len(date_dict[key])):\n        token = date_dict[key][iter]\n        doc = nlp(str(token))\n        for ent in doc.ents:\n          print(ent.text, ent.label_)\n    if stopper > 2:\n      break\n    print('******************************************************************************************')","bfa70295":"from spacy.matcher import Matcher\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab, validate=True)\ndf[0].astype('string')\nprint(type(df.iloc[5][0]))","d5ed1619":"date_pattern3 = [{'LOWER':{\"REGEX\":'^jan|^feb|^mar|^apr|^may|^jun|^jul|^aug|^sep|^oct|^nov|^dec'}},{'LOWER':{'REGEX':'\\d{1,2}'}},{'LOWER':{'REGEX':'\\d{0-4}'}}]\ndate_pattern4 = [{'LOWER':{\"IN\":['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']}}]\n\n\nmatcher.add('Custom_Date', None, date_pattern4)\ndoc = nlp(df.iloc[i][0])\nmatches = matcher(doc)\nmatches\n\nfor match_id, start, end in matches:\n    string_id = nlp.vocab.strings[match_id]\n    span = doc[start:end]\n    #print(match_id, string_id, start, end, span.text)\n    print(start, end, span.text)","9a7ddaa0":"date_pattern1 = [{'LOWER':{\"REGEX\":'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)'}},{}]\n                           \n\ndef callback_method(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    entity = doc[start:end]\n    print(entity.text)\n    \nmatcher = Matcher(nlp.vocab)\nmatcher.add('Date_Cust2', callback_method,date_pattern1)\ndoc = nlp(df.iloc[5][0])\nmatcher(doc)    ","5d5e098a":"## Observations:","55af3310":"#### **Matching the custom patterns**\n\nLoading the spacy Matcher and making a matcher object from it. The pattern to be matched can be added to 'matcher' and the 'matcher' object can be applied to the spacy doc (containing thet text data) inorder to match with the customized patterns.\n\n","08338679":"### **Unfiltered Results**:\nCode above give us the results in the unfiltered form (for ex: date may contain date and some other numbers tagged as date). Now we need to apply several other techniques to filter the results, one is discussed below. ","f4bc07d4":"## <font color='red'> **Setting up SPACY Pipeline**<\/font>\nNow we have text information from the xml files. Next step is to extract the entities. <br> \n Using SPACY for extracting relevant information ","fbb58e87":"### **POC for filtering results**:\nFollowing code shows how by applying pipeline in series a result can be filtered out. Several random values assigned as dates in the previous result are assigned as 'CARDINAL' value here. It shows how a result can be filtered using the pipeline in series. Here we have used same pipeline for better result a different pipeline (different) model should be used. ","6adbbc96":"## XML Parsing\nXML parsing in python is done either by using **lxml parser along with beautiful soup or using elementtree library**. Choose the one which suits your needs better. \n\nOur aim is to read the text data from XML file and put the information of all the files in a text file. First we read a file and try to get the text from a single file then we loop over all the files to get the text from all the files. ","6234fb86":"# <center><span style='color:red'>**Entity Recognition for OCR using text data**  <\/span><\/center>","4b23c67c":"### Accessing the directory with all the files and all the filenames","8eef3d52":"#### **Call_back function**","67e3ad56":"#### Information in a particular text region (< TextRegion >) is a block of information which is of the same type like address or description. But this block of informaiton may be divided into different lines (< TextLine >). \nWhile reading we need to club information according to text line and text region to make it more reasonable so that sequential property of the text data could be preserved. Opposed to this a normal final_all text in xml would give non sequential data where sequential property of the data will be lost. \nInformation from XML can be extracted row wise, block wise or all the text altogether. Row wise text is smaller in size and contains less sequential information so it will be better to use if for regex comparison. While whole text together or block wise information is expected to be in more sequential manner and thus more suitable for unsupervised learning methods (not supervise because of no labeled data). \n\nNOTE: From xml we are currently not taking the confidence into account, for a better model confidence threshold should be decided\/optimized and used. \n\nParsing the XML file using element tree:","fe4ae3f5":"**Hope this helps you to start with the data set and gives you an idea about what to do and how to do. Do share your amazing ideas and methods for entity extraction from such files.**"}}