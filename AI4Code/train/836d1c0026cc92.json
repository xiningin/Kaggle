{"cell_type":{"69f983eb":"code","071a3d41":"code","a24b2b25":"code","bcdf475c":"code","0ee9ee49":"code","139e6ccb":"code","6260c31e":"code","75883e2d":"code","826f5222":"markdown","1268d3e5":"markdown","d2e23ba9":"markdown","4702744f":"markdown","9c57ccb8":"markdown","766a5f9f":"markdown","868322db":"markdown"},"source":{"69f983eb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom keras.models import Sequential\nfrom keras.layers import LeakyReLU, Dense, Conv1D, MaxPooling1D, Dropout, Permute, Flatten, LSTM\nplt.style.use('ggplot')","071a3d41":"df_train  = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ny_train = df_train['SalePrice']\ndf_train = df_train.drop(columns=['SalePrice'])\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndisplay(df_train.head(10))","a24b2b25":"one_hot_encoded_training_predictors = pd.get_dummies(df_train)\none_hot_encoded_test_predictors = pd.get_dummies(df_test)\ndf_train, df_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\ndf_train = df_train.fillna(df_train.mean())\ndf_test = df_test.fillna(df_test.mean())","bcdf475c":"scaler = StandardScaler()\nids = df_test['Id']\ndf_train = scaler.fit_transform(df_train)\ndf_test = scaler.transform(df_test)\ny_train = np.log(y_train)","0ee9ee49":"time_steps = 1\nnum_features = df_train.shape[-1]\ndf_train = np.reshape(df_train,(-1, time_steps, num_features))\ndf_test = np.reshape(df_test,(-1, time_steps, num_features))","139e6ccb":"model = Sequential()\n\nmodel.add(Permute((2, 1), input_shape=(time_steps, num_features)))\nmodel.add(Conv1D(32, 2))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(64, 2))\nmodel.add(MaxPooling1D(2))\nmodel.add(LSTM(32, return_sequences=False))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.add(LeakyReLU())\nmodel.compile(loss='mean_squared_error', optimizer='adam')","6260c31e":"model.fit(df_train, y_train, epochs=1000, batch_size=64, verbose=2)","75883e2d":"predictions = pd.Series(np.exp(model.predict(df_test)).reshape((-1)), dtype='float64')\nsubmission = pd.DataFrame({\"Id\": ids, \"SalePrice\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","826f5222":"As we can see, there is columns marked as 'object' (mostly strings), so, in order to further analyze the data, we need to deal with this datatype. For this, we will be using Pandas 'get_dummies', which will transform string into dummy integers.\n\nAlso, there is a lot of missing values, as a first analysis, we will be filling those values with the mean value of the column","1268d3e5":"Before inputing the data to a model, we should normalize it, to do so, we will be using StandardScaler, which will transform the data to have zero mean and unit variance","d2e23ba9":"Also, as we will be using neural networks as our model, we need to reshape the data","4702744f":"## Imports","9c57ccb8":"## Opening training and test files to DataFrames and having a quick look at it's structure","766a5f9f":"## Preparing submition","868322db":"## Creating the model\n\nFor the model, we will use a neural network with convolutional layers to extract relevant features"}}