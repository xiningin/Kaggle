{"cell_type":{"47f21789":"code","e1fd8727":"code","345dc4cf":"code","d5818a96":"code","b96ab728":"code","73859556":"code","1e6094d7":"code","887adde2":"code","1e4a3c7e":"code","e3203ca0":"code","8c3ab445":"code","19124851":"code","bbef59e2":"code","c630230c":"code","002d724e":"code","d235431a":"code","be6cf8bb":"code","4ba9c29e":"markdown","4311b243":"markdown","d0a2f878":"markdown","26f27e81":"markdown","bbed4fd7":"markdown"},"source":{"47f21789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1fd8727":"df = pd.read_csv('\/kaggle\/input\/talkingdata-adtracking-fraud-detection\/train_sample.csv')\nprint(df.shape)\nprint(df.info())\ndf.head()","345dc4cf":"df['click_time_parsed'] = pd.to_datetime(df['click_time'])\ndf['year'] = df['click_time_parsed'].dt.year\ndf['month'] = df['click_time_parsed'].dt.month\ndf['day'] = df['click_time_parsed'].dt.day\ndf['hour'] = df['click_time_parsed'].dt.hour\ndf.head()","d5818a96":"df.describe()","b96ab728":"from sklearn.preprocessing import MinMaxScaler\n\ncols = [\"ip\", \"app\", \"device\", \"os\", \"channel\", \"day\", \"hour\"]\n\nscaler = MinMaxScaler()\nscaler.fit(df[cols])\n\nX_min = scaler.data_min_\nX_max = scaler.data_max_\n\nprint(X_min)\nprint(X_max)","73859556":"filepath = \"\/kaggle\/input\/talkingdata-adtracking-fraud-detection\/train_sample.csv\"\n#filepath = \"\/kaggle\/input\/talkingdata-adtracking-fraud-detection\/train.csv\"\n\ntrain_path='train.csv'\nvalid_path='valid.csv'\n\nif os.path.exists(train_path):\n    os.remove(train_path)\n    \nif os.path.exists(valid_path):\n    os.remove(valid_path)","1e6094d7":"import csv\nsplit = 10000\n\nif split:\n    with open(filepath) as f:\n        reader = csv.reader(f, delimiter=',')\n        first_line = True\n        count = 0\n        for row in reader:\n            if first_line:\n                first_line = False\n\n                with open(train_path, 'wt', encoding='utf-8') as train_file:\n                    csv_writer = csv.writer(train_file, delimiter=',')\n                    csv_writer.writerow(row)\n\n                with open(valid_path, 'wt', encoding='utf-8') as valid_file:\n                    csv_writer = csv.writer(valid_file, delimiter=',')\n                    csv_writer.writerow(row)\n\n            else:\n                count +=1\n                if count<=split:\n                    with open(valid_path, 'a', encoding='utf-8') as valid_file:\n                        csv_writer = csv.writer(valid_file)\n                        csv_writer.writerow(row)\n                else:\n                    with open(train_path, 'a', encoding='utf-8') as train_file:\n                        csv_writer = csv.writer(train_file)\n                        csv_writer.writerow(row)","887adde2":"import tensorflow as tf\n\ndataset = tf.data.TextLineDataset(train_path).skip(1)\n\nfor line in dataset.take(5):\n    print(line.numpy())","1e4a3c7e":"from datetime import datetime\n\ndef preprocess(line):\n    defs = [0]*5 + [\"\"]*2 + [tf.constant([], dtype=tf.string)]\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    \n    # parse date\n    day = tf.strings.to_number(tf.strings.substr(fields[5], 8, len=2), tf.int32)\n    hour = tf.strings.to_number(tf.strings.substr(fields[5], 11, len=2), tf.int32)\n    \n    # features\n    features = fields[:-3]\n    features.append(day)\n    features.append(hour)\n    \n    x = tf.stack(features)\n    y = tf.stack(fields[-1])\n    #return x, y\n    return (x - X_min) \/ (X_max - X_min), y=='1'\n    \ntest = b'83230,3,1,13,379,2017-11-06 14:32:21,,0'\npreprocess(test)","e3203ca0":"dataset = tf.data.TextLineDataset(filepath).skip(1)\ndataset = dataset.map(preprocess)\n\nfor line in dataset.take(1):\n    print(line)","8c3ab445":"def csv_reader_dataset(filepath, shuffle_buffer_size=10000, batch_size=256, n_parse_threads=5):\n    dataset = tf.data.TextLineDataset(filepath).skip(1)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    return dataset.batch(batch_size).prefetch(1)","19124851":"tf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ntrain_set = csv_reader_dataset(train_path)\nvalid_set = csv_reader_dataset(valid_path)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(50, activation='relu', input_shape=[7]),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])","bbef59e2":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\n\nmodel.fit(train_set, epochs=100, validation_data=valid_set, callbacks=[early_stopping_cb])","c630230c":"test_path ='\/kaggle\/input\/talkingdata-adtracking-fraud-detection\/test.csv'\n\ndataset = tf.data.TextLineDataset(test_path).skip(1)\nfor line in dataset.take(5):\n    print(line.numpy())","002d724e":"def preprocess_forecast(line):\n    defs = [0]*6 + [\"\"] \n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    \n    # parse date\n    day = tf.strings.to_number(tf.strings.substr(fields[-1], 8, len=2), tf.int32)\n    hour = tf.strings.to_number(tf.strings.substr(fields[-1], 11, len=2), tf.int32)\n    \n    # features\n    features = fields[1:-1]\n    features.append(day)\n    features.append(hour)\n    \n    x = tf.stack(features)\n    return (x - X_min) \/ (X_max - X_min)\n    \n\ndataset = tf.data.TextLineDataset(test_path).skip(1)\ndataset = dataset.map(preprocess_forecast)\n\nfor line in dataset.take(1):\n    print(line)","d235431a":"def model_forecast(model, filepath):\n    dataset = tf.data.TextLineDataset(filepath).skip(1)\n    dataset = dataset.map(preprocess_forecast, num_parallel_calls=5)\n    dataset =  dataset.batch(256).prefetch(1)\n    prediction = model.predict(dataset)\n    return prediction\n\ny_pred = model_forecast(model, test_path)\ny_pred.shape","be6cf8bb":"df = pd.read_csv(test_path)\ndf['is_attributed'] = y_pred\ndf[['click_id','is_attributed']].to_csv('submission.csv', index=False)","4ba9c29e":"# keras Data API","4311b243":"# Explore the sample dataset with pandas","d0a2f878":"# Make Predictions","26f27e81":"# Train - Valid split","bbed4fd7":"# Deep Learning"}}