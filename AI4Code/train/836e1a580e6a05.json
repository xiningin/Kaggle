{"cell_type":{"ffe4b199":"code","d22d1ff0":"code","6541b2ae":"code","46d8984a":"code","6446413c":"code","b967b9a3":"code","c1391341":"code","eca2f371":"code","cca418c7":"code","970382c0":"code","8eb14fd1":"code","dc82b8ea":"code","fcad5fb3":"code","5d58e6a8":"code","3862d952":"code","1c37a86a":"code","0cc1be95":"code","ec960d2c":"code","7b72aadb":"code","cc8ca7bb":"code","0630c12f":"code","67d282f8":"code","9061f1b8":"code","80811969":"code","59b4ae60":"code","36a49eca":"code","e38567b3":"code","c6983724":"code","715931bc":"code","34f59b16":"code","b2e7f758":"code","b6c98aa1":"code","03e5fcea":"code","b00fd37e":"code","51577dea":"code","50fa818a":"code","ca07d4ab":"code","fcba0466":"code","202af857":"code","8d02520b":"code","2db0b41a":"code","74e86c68":"code","4102fab3":"markdown","57dc2c97":"markdown","ecd49370":"markdown","b98503eb":"markdown","e5e6ebb1":"markdown","e50eb209":"markdown","19845796":"markdown","8d363fc9":"markdown","66b0b4ef":"markdown","f459d55b":"markdown","c04fd688":"markdown","45d409c1":"markdown","a0bdac3a":"markdown","db576755":"markdown","8c5080a5":"markdown","b4e82c83":"markdown","7a0a87ea":"markdown","eef79c54":"markdown","43928300":"markdown","cacefdec":"markdown","15345df6":"markdown"},"source":{"ffe4b199":"# Measure how long it takes to run the notebook\nfrom datetime import datetime, timedelta\n\nstart = datetime.now()\nprint('Start time:', start)","d22d1ff0":"# Working with dataframes and arrays\nimport pandas as pd\nimport numpy as np\nimport math\n\n# Ploting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# NLP\nimport nltk\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport gensim\nfrom gensim import corpora\n\n# Modeling\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom mlxtend.preprocessing import DenseTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\n# Cleaner notebook\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Notebook settings\nstop_words = set(stopwords.words('english'))\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)","6541b2ae":"# Link to dataset\n# https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews","46d8984a":"# Description of fields:\n\n# Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n# Age: Positive Integer variable of the reviewers age.\n# Title: String variable for the title of the review.\n# Review Text: String variable for the review body.\n# Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n# Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n# Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n# Division Name: Categorical name of the product high level division.\n# Department Name: Categorical name of the product department name.\n# Class Name: Categorical name of the product class name.","6446413c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b967b9a3":"# Read and preview data\ntry:\n    df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv')\nexcept:\n    df = pd.read_csv('\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')\nprint(df.shape)\nprint(df.columns)\ndf.head()","c1391341":"# Remove the column: 'Unnamed: 0'\ndf.drop('Unnamed: 0', axis=1, inplace=True)","eca2f371":"# Count the number of nulls in each column\ndf.isna().sum()","cca418c7":"# Count the number of unique values in each column\ndf.nunique()","970382c0":"# Clean text data (remove special characters, lowercase, remove stop words, and lemmatize)\n\n# I found this function for text preprocessing online. I think it was towardsdatascience.com,\n# but I can't find\/remember where I found it. If I find the site, I'll post the source to give the author credit.\n\ndef preprocess(raw_text):\n    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n\n    words = letters_only_text.lower().split()\n\n    cleaned_words = []\n    lemmatizer = WordNetLemmatizer()\n\n    for word in words:\n        if word not in stop_words:\n            cleaned_words.append(word)\n    \n    lemmas = []\n    for word in cleaned_words:\n        word = lemmatizer.lemmatize(word)\n        lemmas.append(word)\n    \n    return \" \".join(lemmas)\n\ndf['Review Text'] = df['Review Text'].astype(str)\ndf['clean_review_text'] = df['Review Text'].apply(preprocess)\n\n# Inspect the cleaned text data\ndf[['Review Text','clean_review_text']].head(3)","8eb14fd1":"# Count number of reviews for each product by rating\ndf_ratings = df.pivot_table(values='Review Text'\n                            , index='Clothing ID'\n                            , columns='Rating'\n                            , aggfunc='count'\n                            , margins=True\n                            , margins_name='Total'\n                            , fill_value=0)\n\n# Use the percent of 1 and 2, and 4 and 5 star reviews to gauge if there are polarizing product\ndf_ratings['percent_5_star'] = (df_ratings[5] + df_ratings[4]) \/ df_ratings['Total']\ndf_ratings['percent_1_star'] = (df_ratings[1] + df_ratings[2]) \/ df_ratings['Total']\ndf_ratings['net_polarity'] = df_ratings['percent_5_star'] - df_ratings['percent_1_star']\n\n# Filter out products with less than 30 reviews\ndf_ratings = df_ratings[df_ratings['Total'] >= 30]\n\n# Distribution of product net polarity\nsns.histplot(df_ratings['net_polarity'])\nplt.title('Distribution of Product Net Polarity', fontsize=14)","dc82b8ea":"# Distribution of star ratings by whehter the reviewer recommends the product\nrecommended = df[df['Recommended IND'] == 1]\nnot_recommended = df[df['Recommended IND'] == 0]\n\nplt.subplot(1, 2, 1)\nsns.histplot(recommended['Rating'], color='green')\nplt.title('Recommended Product')\n\nplt.subplot(1, 2, 2)\nsns.histplot(not_recommended['Rating'], color='red')\nplt.title('Not Recommended Product')\nplt.suptitle('Number of Star Rating by Product Recommendation', fontsize=16, verticalalignment='center', horizontalalignment='center')\nplt.tight_layout()\nplt.show()","fcad5fb3":"# Number of characters in the review by whehter the reviewer recommends the product\ndf['num_characters'] = df['clean_review_text'].apply(len)\n\n# Number of words in the review by whehter the reviewer recommends the product\ndf['num_words'] = [len(x.split()) for x in df['clean_review_text'].tolist()]\n\n# Separate reviews that recommend and don't recommend the productd\nrecommend = df[df['Recommended IND'] == 1]\nnot_recommend = df[df['Recommended IND'] == 0]\n\n# Plot\nfig, axs = plt.subplots(2, 2, figsize=(10,5))\n\naxs[0, 0].hist(recommend['num_characters'], color='green')\naxs[0, 0].set_title('Recommend Product: Number of characters')\n\naxs[0, 1].hist(not_recommend['num_characters'], color='red')\naxs[0, 1].set_title('Do Not Recommend Product: Number of characters')\n\naxs[1, 0].hist(recommend['num_words'], color='green')\naxs[1, 0].set_title('Recommend Product: Number of words')\n\naxs[1, 1].hist(not_recommend['num_words'], color='red')\naxs[1, 1].set_title('Do Not Recommend Product: Number of words')\n\nfor ax in axs.flat:\n    ax.set(xlabel='x-label', ylabel='y-label')\n\nplt.suptitle('Distribution of the Number of Charaters \\nand Words in a Review by its Recommendation', fontsize=16, verticalalignment='center', horizontalalignment='center')\nfig.tight_layout()\nplt.show()","5d58e6a8":"# Distribution of Age by Recommendation\n# Not a big difference in age between those that recommend and don't recommend the product\nplt.figure(figsize=(5,8))\nsns.violinplot(y='Age', x='Recommended IND', hue='Recommended IND', data=df, palette=['red', 'green'])\nplt.title('Distribution of Age by Recommendation', fontsize=14)\nplt.show()","3862d952":"# Each age group has a similar range of ratings and recommendations given.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\nn_bins = 10\n\nratings = sorted(list(df.Rating.unique()))\nx1 = [df[df.Rating == i]['Age'].to_numpy() for i in ratings]\nax1.hist(x1, n_bins, histtype='bar')\nax1.set_xlabel('Age')\nax1.legend(ratings)\nax1.set_title('Distribution of Age by Rating', fontsize=12)\n\nrec = sorted(list(df['Recommended IND'].unique()))\nx2 = [df[df['Recommended IND'] == i]['Age'].to_numpy() for i in rec]\nax2.hist(x2, n_bins, histtype='bar')\nax2.set_xlabel('Age')\nax2.legend(rec)\nax2.set_title('Distribution of Age by Recommendation', fontsize=12)\n\nplt.show()","1c37a86a":"# Binning age\nage_bins = pd.qcut(df['Age'], q=6).rename('age_bins')\ndf = pd.concat([df, age_bins], axis=1)\ndf.age_bins.value_counts()","0cc1be95":"# Average rating for each age group\n# The oldest age group (56 to 99 yo) has the highest average rating, but all age groups have pretty similar average ratings.\n(df.groupby('age_bins')\n .agg({'Rating': 'mean', 'clean_review_text': 'count'})\n .reset_index()\n .rename(columns={'Rating': 'Average_Rating', 'clean_review_text': 'Num_reviews', 'age_bins': 'Age Groups'})\n .style.background_gradient(cmap = 'YlGn')\n .format({'Average_Rating':'{:.2f}', 'Num_reviews': '{:.0f}'}))","ec960d2c":"# Percent of ratings that come from each age group\n\n# Most of the 1 ratings (worst) come from the 41 to 48 age group\n# Most of the 5 ratings (best) come from the 56 to 99 age group\nage_ratings = pd.pivot_table(data = df\n              , values = 'clean_review_text'\n              , index = 'age_bins'\n              , columns = 'Rating'\n              , aggfunc = 'count'\n              , fill_value = 0\n              , margins = True)\n\n# Percent of column total\nage_ratings_pct_col = age_ratings.copy()\nfor i in age_ratings.columns:\n    age_ratings_pct_col[i] = age_ratings[i] \/ age_ratings.iloc[-1][i]\n\nprint('Percent of ratings that come from each age group')\nrounded = dict()\nfor i in age_ratings_pct_col.columns:\n    rounded[i] = \"{:.2f}\" # create a dictionary to format the decimals \n\nage_ratings_pct_col.style.background_gradient(axis=1, cmap = 'YlGn').format(rounded)","7b72aadb":"# Store ngrams for reviews that recommend and don't recommend the product in a dictionary\nngrams_dict = dict()\n\nfor i in sorted(df['Recommended IND'].unique(), reverse=True):\n    X = df[df['Recommended IND'] == i]['clean_review_text']\n    for n in range(1, 4):\n        vectorizer = CountVectorizer(max_features = 20, ngram_range = (n,n))\n        DTM = vectorizer.fit_transform(X)\n        frequencies = sum(DTM).toarray()[0]\n        ngrams = pd.DataFrame(frequencies, index = vectorizer.get_feature_names(), columns = ['Frequency'])\n        ngrams.sort_values('Frequency', ascending = False, inplace=True)\n        ngrams = ngrams.reset_index().rename(columns={'index':'Term'})\n        ngrams['ngram'] = n\n        ngrams['recommend'] = i\n        ngrams_dict[str(i) + '_' + str(n)] = ngrams\n\n# Plot ngrams by recommendation\nfig = plt.figure(figsize = (15, 15))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i, j in enumerate(list(ngrams_dict.keys())):\n    i = i + 1\n    to_plot = ngrams_dict[j]\n    to_plot.sort_values('Frequency', inplace=True)\n    not_rec_keys = ['0_1', '0_2', '0_3']\n    if j in not_rec_keys:\n        ax = fig.add_subplot(2, 3, i)\n        ax.barh(to_plot.Term, to_plot.Frequency, color='red')\n        ax.set_title(\"Recommend: \" + str(j[:1]) + \";    Ngram: \" + str(j[-1:]))\n    else:\n        ax = fig.add_subplot(2, 3, i)\n        ax.barh(to_plot.Term, to_plot.Frequency, color='green')\n        ax.set_title(\"Recommend: \" + str(j[:1]) + \";    Ngram: \" + str(j[-1:]))\n\nfor ax in axs.flat:\n    ax.set(xlabel='Frequency', ylabel='Ngram')\nplt.suptitle('20 Most Frequent Ngrams (1, 2, 3) used in Reviews by its Recommendation\\n(Recommend = 1 and Do Not Recommend = 0)', fontsize=20, verticalalignment='center', horizontalalignment='center')\nplt.tight_layout()\nplt.show()","cc8ca7bb":"# List of unique product classes\n# Remove products with less than 10 reviews\nclass_reviews = df.groupby('Class Name').agg({'clean_review_text': 'nunique'})\nclasses = class_reviews[class_reviews.clean_review_text >= 10].index\n\n# Use CountVectorizer to create a document term matrix (DTM) for each product's reviews\nvectorizer = CountVectorizer()\nDTM_clothes = pd.DataFrame()\n\n# Create dictionary to store reviews for each product class\nclothes_dict = dict()\nfor i,j in enumerate(classes):\n    text = df[df['Class Name'] == j]['clean_review_text'] # filter dataframe for that product's reviews\n    texts = str() # Create a string variable to store reviews\n    for s,t in enumerate(text):\n        if s == 0: # Necessary bc w\/o enumerate (texts = texts + ' ' + t), it adds a spaces before the first review\n            texts = t\n        else:\n            texts = texts + ' ' + t # put all of the reviews for each product together in a string\n    bacon = list()\n    bacon.append(texts) # put the string in a list\n    clothes_dict[i] = bacon # Create a key in the dictionary to store the product reviews list\n    DTM = (pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(bacon), columns=vectorizer.get_feature_names()) # Build document term matrix\n             .transpose() # tranpose to move the terms to the index, it will make it easier when joining the DTMs\n             .rename(columns={0: j})) # Name the column with the product class\n    if i == 0:\n        DTM_clothes = DTM\n    else:\n        DTM_clothes = DTM_clothes.join(DTM, how='outer') # put each product's DTM in a dataframe\n\nDTM_clothes.fillna(0, inplace=True) # fill with 0 bc it was an outer join\nDTM_clothes = DTM_clothes.transpose()\n\n# Measure cosine similarity between the product reviews\ncosine = pd.DataFrame(cosine_similarity(DTM_clothes), columns=DTM_clothes.index, index=DTM_clothes.index)\nrounded = dict()\nfor i in cosine.columns:\n    rounded[i] = \"{:.2f}\"\n\ncosine.style.background_gradient(axis=1, cmap = 'YlGn').format(rounded)","0630c12f":"# Find the most similar and dissimilar product reviews for each product class\nsim = []\ndis = []\nfor i in cosine.columns:\n    spam = cosine.sort_values(i, ascending=False)\n    eggs = spam.index[1] # the first value (index 0) will be itself so take the second highest value\n    bacon = spam.index[-1]\n    sim.append(eggs)\n    dis.append(bacon)\n\nsim_dis_reviews = pd.DataFrame({'Most_Similar': sim, 'Most_Dissimilar': dis}\n                               , index = cosine.columns)\n\nsim_dis_reviews","67d282f8":"compound_sentiment = []\nsentences = []\nsid = SentimentIntensityAnalyzer()\n\nfor sentence in df['clean_review_text']:\n    ss = sid.polarity_scores(sentence)\n    compound_sentiment.append(ss['compound'])\n    sentences.append(sentence)\n\nsentiment_vader = pd.DataFrame({'Sentence': sentences,\n                               'Sentiment': compound_sentiment})\n\ndf = pd.concat([df, sentiment_vader.Sentiment], axis=1)\n\nsns.histplot(x='Sentiment', hue='Recommended IND', data=df)\nplt.title('Sentiment of Review by Recommendation', fontsize=12)\nplt.show()","9061f1b8":"def code_sentiment(x):\n    if x > 0.75:\n        return 1\n    else:\n        return 0\n\ntext = df.clean_review_text\nvectorizer = TfidfVectorizer(max_features = 300)\nX = pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(text), index=text.index, columns=vectorizer.get_feature_names())\n\ny = df.Sentiment.apply(code_sentiment)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\nclf_sentiment = LogisticRegression(random_state=1, solver='liblinear').fit(X_train, y_train)\ny_pred = clf_sentiment.predict(X_test)\nprint('Accuracy', accuracy_score(y_test, y_pred))\nprint('='*100)\n\nparams = pd.DataFrame({'Feature': X.columns\n                     , 'Logit': clf_sentiment.coef_[0]})\n\ndef logit_to_prob(x):\n    return math.exp(x) \/ (1 + math.exp(x))\n\nparams['Probability'] = params['Logit'].apply(logit_to_prob)\n\nparams.sort_values('Probability', ascending=False, inplace=True)\npos = list(params.head(20).iloc[:,0])\nneg = list(params.tail(20).iloc[:,0])\n\nprint('Most positive terms:')\nprint(pos)\nprint('='*100)\nprint('Most negative terms:')\nprint(neg)","80811969":"X = df.clean_review_text\nvectorizer = CountVectorizer(max_features = 1000, ngram_range = (2,5))\nDTM = vectorizer.fit_transform(X)\nfrequencies = sum(DTM).toarray()[0]\nngrams = pd.DataFrame(frequencies, index = vectorizer.get_feature_names(), columns = ['Frequency'])\nngrams.sort_values('Frequency', ascending = True, inplace=True)\nngrams = ngrams.reset_index().rename(columns={'index':'Term'})\n\n# Filter ngrams to only those that include words that predict positive or negative sentiment\nterms = list(ngrams.Term)\npos_terms = [s for s in terms if any(x in s for x in pos)]\nneg_terms = [s for s in terms if any(x in s for x in neg)]\n\npos_ngrams = ngrams[ngrams.Term.isin(pos_terms)].head(25)\nneg_ngrams = ngrams[ngrams.Term.isin(neg_terms)].head(25)\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,8))\nax1.barh(pos_ngrams.Term, pos_ngrams.Frequency, color='green')\n# ax1.set_title('Positive Words:\\n25 Most Frequent Ngrams that Follow & Proceed')\nax1.set_title('Positive Words')\n\nax2.barh(neg_ngrams.Term, neg_ngrams.Frequency, color='red')\n# ax2.set_title('Negative Words:\\n25 Most Frequent Ngrams that Follow & Proceed')\nax2.set_title('Negative Words')\n\nfor ax in [ax1, ax2]:\n    ax.set(xlabel='Frequency', ylabel='Ngram')\n\nplt.suptitle('25 Most Frequent Ngrams that Follow & Proceed Positive Words and Negative Words', fontsize=16, verticalalignment='center', horizontalalignment='center')\nplt.tight_layout()\nplt.show()","59b4ae60":"# Topics in reviews that recommend the product\npos = df[df['Recommended IND'] == 1]\n\ntext_clean = []\nfor text in pos['clean_review_text']:\n    text_clean.append(text.split())\n\ndictionary = corpora.Dictionary(text_clean)\ntext_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n\nLda = gensim.models.ldamodel.LdaModel\nldamodel = Lda(text_term_matrix, num_topics=5, id2word = dictionary, passes=30)\n\ntopics = ldamodel.top_topics(text_term_matrix, topn=10)\npos_topics_df = pd.DataFrame()\nfor x in range(len(topics)):\n    pos_topics_df['Topic_' + str(x)] = dict(topics[x][0]).values()\n\npos_topics_df","36a49eca":"# Topics in reviews that do not recommend the product\nneg = df[df['Recommended IND'] == 0]\n\ntext_clean = []\nfor text in neg['clean_review_text']:\n    text_clean.append(text.split())\n\n\ndictionary = corpora.Dictionary(text_clean)\ntext_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n\nLda = gensim.models.ldamodel.LdaModel\nldamodel = Lda(text_term_matrix, num_topics=5, id2word = dictionary, passes=30)\n\ntopics = ldamodel.top_topics(text_term_matrix, topn=10)\nneg_topics_df = pd.DataFrame()\nfor x in range(len(topics)):\n    neg_topics_df['Topic_' + str(x)] = dict(topics[x][0]).values()\n\nneg_topics_df","e38567b3":"def binary_classification(term_features = df.clean_review_text\n                         , other_features = None\n                         , target = df['Recommended IND']\n                         , word_embedding = TfidfVectorizer\n                         , ngram_range = (1, 1)\n                         , max_term_features = 300\n                         , model = LogisticRegression\n                         , balance_classes = True):\n    \"\"\"Returns the F1 score of a binary classification model.\"\"\"\n    if balance_classes:\n        together = pd.concat([target, term_features, other_features], axis=1)\n        \n        class_zero = together[together[target.name] == 0]\n        len_zero = len(class_zero)\n        \n        class_one = together[together[target.name] == 1]\n        len_one = len(class_one)\n        \n        marjority_class = max(len_zero, len_one)\n        \n        if marjority_class == len_zero:\n            down_sample = class_zero.sample(len_one)\n            X_y = pd.concat([down_sample, class_one])\n        else:\n            down_sample = class_one.sample(len_zero)\n            X_y = pd.concat([down_sample, class_zero])\n            \n    else:\n        X_y = pd.concat([target, term_features, other_features], axis=1)\n    \n    terms = X_y[term_features.name]\n    vectorizer = word_embedding(ngram_range = ngram_range, max_features = max_term_features)\n    X_terms = pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(terms), index=terms.index, columns=vectorizer.get_feature_names())\n    \n    if other_features is None:\n        X = X_terms\n    else:\n        X = pd.concat([X_terms, X_y[other_features.name]], axis=1)\n    \n    y = X_y[target.name]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n    \n    try:\n        clf_model = model(random_state=1).fit(X_train, y_train)\n    except:\n        try:\n            clf_model = model().fit(DenseTransformer(X_train), y_train)\n        except:\n            try:\n                clf_model = model().fit(X_train, y_train)\n            except:\n                pass\n    \n    y_pred = clf_model.predict(X_test)\n    \n    model_score = f1_score(y_test, y_pred, average='macro')\n    print(model_score)","c6983724":"# Default parameters -- \n# Text predictors: review text\n# Other predictors: none\n# Word embedding: tf-idf\n# Ngram range: (1, 1)\n# Max term features: 300\n# Classification model: logistic regression\n# Balance classes: true\n# Score: F1 score\nbinary_classification()","715931bc":"# Add sentiment as a predictor\nbinary_classification(other_features = df['Sentiment'])","34f59b16":"# Longer length of ngrams (1,2)\nbinary_classification(ngram_range=(1,2))","b2e7f758":"# Longer length of ngrams (1,3)\nbinary_classification(ngram_range=(1,3))","b6c98aa1":"# Wider term matrix (max_term_features = 500)\nbinary_classification(max_term_features = 500)","03e5fcea":"# Wider term matrix (max_term_features = 1000)\nbinary_classification(max_term_features = 1000)","b00fd37e":"# Wider term matrix (max_term_features = 1000) and longer length of ngrams (1,2)\nbinary_classification(max_term_features = 1000, ngram_range=(1,2))","51577dea":"# Count vectorizer\nbinary_classification(word_embedding=CountVectorizer)","50fa818a":"# Allow for class imbalance\nbinary_classification(balance_classes=False)","ca07d4ab":"# Allow for class imbalance and wider term matrix (max_term_features = 1000)\nbinary_classification(balance_classes=False, max_term_features = 1000)","fcba0466":"# Decision tree classifier\nbinary_classification(model=DecisionTreeClassifier)","202af857":"# Complement naive bayes\nbinary_classification(model=ComplementNB)","8d02520b":"# Balance classes\ntogether = df[['Recommended IND', 'clean_review_text']]\n\nclass_zero = together[together['Recommended IND'] == 0]\nlen_zero = len(class_zero)\n\nclass_one = together[together['Recommended IND'] == 1]\nlen_one = len(class_one)\n\nmarjority_class = max(len_zero, len_one)\n\nif marjority_class == len_zero:\n    down_sample = class_zero.sample(len_one)\n    X_y = pd.concat([down_sample, class_one])\nelse:\n    down_sample = class_one.sample(len_zero)\n    X_y = pd.concat([down_sample, class_zero])\n\n# Create word embedding (tf-idf)\nterms = X_y['clean_review_text']\nvectorizer = TfidfVectorizer(ngram_range = (1, 1), max_features = 1000)\nX = pd.DataFrame.sparse.from_spmatrix(vectorizer.fit_transform(terms), index=terms.index, columns=vectorizer.get_feature_names())\n\n# Separate target variable\ny = X_y['Recommended IND']\n\n# Separate train and validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# Fit model and make a prediction\nclf_final = LogisticRegression(random_state=1).fit(X_train, y_train)\ny_pred = clf_final.predict(X_test)\n\n# Score the model\nf1_macro = f1_score(y_test, y_pred, average='macro')\nprint('F1 macro: {:.2f}'.format(f1_macro))\nprint(classification_report(y_test, y_pred))","2db0b41a":"# Store model coefficients in a dataframe\nrec_params = pd.DataFrame({'Feature': X.columns\n                     , 'Logit': clf_final.coef_[0]})\n\n# Convert coefficients from log odds (logit) to probability\nrec_params['Probability'] = rec_params['Logit'].apply(logit_to_prob)\n\n# Sort and separate terms with the highest and lowest probabilities\nrec_params.sort_values('Probability', ascending=False, inplace=True)\npos_rec = list(rec_params.head(20).iloc[:,0])\nneg_rec = list(rec_params.tail(20).iloc[:,0])\n\n# Plot only the top and bottom most important features\nspam = rec_params[rec_params.Feature.isin(pos_rec)]\neggs = rec_params[rec_params.Feature.isin(neg_rec)]\nspam_and_eggs = pd.concat([spam, eggs])\nplt.figure(figsize=(5, 10))\n\n# Format the colors in the bar plot\nclrs = ['green' if (x > 0) else 'red' for x in spam_and_eggs['Logit'] ]\n\nsns.barplot(x = 'Probability'\n            , y = 'Feature'\n            , data = spam_and_eggs\n            , orient = 'h'\n            , palette=clrs)\nplt.title('Feature Importance (Probability)\\nin Classifying Product Recommendation', fontsize=12)","74e86c68":"end = datetime.now()\nprint('Duration:', end - start)","4102fab3":"# Getting started <a class=\"anchor\" id=\"getting_started\"><\/a>","57dc2c97":"## Most frequent words that follow and preceed the words that predict sentiment\nUse the most positive and negative terms from the sentiment classification model to find the most frequent terms that follow and proceed.","ecd49370":"## Frequent ngrams by recommendation\nFind the 20 most common unigrams, bigrams, and trigrams used in reviews, and separate them by whether they recommend the product.\n\n* Terms like \"fit well\", \"true fit\", and \"fit perfectly\" are common in reviews that recommend the product.\n\n* Terms like \"run small\", \"sadly going back\", and \"first time wore\" are common in reviews that don't recommend the product.","b98503eb":"# Data preprocessing","e5e6ebb1":"# Classification model for product recommendation\nFit a classification model for product recommendation, and try different features, paramters, models, etc. to see which combination produces the best classification score.","e50eb209":"#### The biggest improvements in the classification score come from having a wider term matrix","19845796":"## Ratings\nPlot the distribution of the ratings by whether the product was recommended.\n\nOne interesting thing from the plots is that most of the people that did not recommend the product gave it a rating of 3.","8d363fc9":"## Topic modeling\nSeparate reviews that recommend and don't recommend the product, and use Latent Dirichlet allocation (LDA) to identify topics in the reviews.","66b0b4ef":"# NLP Project with Women's E-Commerce Clothing Reviews","f459d55b":"## Cosine similarity","c04fd688":"# Exploratory data analysis","45d409c1":"## Final model\nFit a classification model with the paramters that produced the best score, and see which features have the most predictive power.","a0bdac3a":"## Product polarity\nCustomer's give the product a rating (1 Worst, to 5 Best). We'll use the rating to find if there polarizing products or products overwhelmingly liked or disliked. Filter out products with less than 30 reviews. Net Polarity = (% 4 and 5 star ratings) - (% 1 and 2 star ratings).\n\nThe higher the net polarity the more universally liked the product is. A net polarity score of 0 means the product is completely polar, and less than 0 is more disliked than liked.","db576755":"## Number of characters and words in the review\nThere's not a big difference in the number of words and characters a review contains whether they do or don't recommend the product.","8c5080a5":"- Is age a good predictor of their rating and recommendation?\n- What is the average rating for each age group?\n- Do most of the 1 star ratings come from a specific age group, 2 star, etc.?","b4e82c83":"## Age of reviewer","7a0a87ea":"The objective of this analysis is to understand what customers like and dislike about the clothing products, mainly by using natural language processing (NLP) to disaggregate their product reviews. Age of the reviewer is also explored for predictive value of product recommendation. The analysis also aims to uncover topics of reviews for those that recommend and don't recommend the product (topic modeling). The bottom of the analysis includes a binary classification model that predicts whether the reviewer recommends the product based on what they wrote in their review.\n\nWhat other natural language processing techniques could be employed on the product reviews to understand what customers think about the products? How would you approach the analysis objective? I'd love to hear other people's thoughts.\n\n-Mason","eef79c54":"# Natural language processing","43928300":"## Sentiment analysis\nUse the nltk library to give each review a sentiment score from -1 (negative) to 1 (positive). Plot the distribution of the sentiment scores.","cacefdec":"Which product classes have the most similar and dissimilar reviews?\n* Reviews for Dresses and Skirts are the most unique reviews (they are most dissimilar to reviews of other products)\n* Reviews for Lounge are most similar to reviews for other products","15345df6":"## Model sentiment (most positive and negative words)\nCode the sentiment scores to be either positive or negative, and fit a logisitic regression model to predict the coded sentiment using the text in the review. Convert the logit coefficients to probability, and find which terms have the highest probability of the review being positive or negative."}}