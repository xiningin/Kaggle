{"cell_type":{"197fa01b":"code","48532cdf":"code","3b9cb041":"code","8b5cfb9f":"code","794e51fa":"code","5f74bdf0":"code","dfe85207":"code","9df89384":"code","15aeb597":"code","18df5c0a":"code","ae0472a0":"code","ef3ae77b":"code","04c6cc2a":"code","6141fef3":"code","3d15861b":"markdown","d8992581":"markdown","e686b90f":"markdown","60902a8b":"markdown","c0a17126":"markdown","56ef6731":"markdown"},"source":{"197fa01b":"import xgboost\nimport numpy as np\nimport optuna\nimport pandas\nimport psutil\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split","48532cdf":"# from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","3b9cb041":"def get_eval_metric(model, X, y_true):\n    y_pred = model.predict_proba(X)[:, 1]\n    return roc_auc_score(y_true, y_pred)","8b5cfb9f":"DEBUG_MODE = False\n\nRANDOM_STATE = 123\nN_TRIALS = 10","794e51fa":"def read_train_data(reduce_memory=True):\n    data_path = \"\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv\"\n    X = pandas.read_csv(data_path, index_col=\"id\")\n    if reduce_memory:\n        X = reduce_memory_usage(X, verbose=True)\n    y = X.pop(\"target\").astype(\"int8\")\n    return X, y","5f74bdf0":"X, y = read_train_data()","dfe85207":"if DEBUG_MODE:\n    X, y = X[:500], y[:500]","9df89384":"def train_model(model_class, params_dict, X_train, y_train, X_val, y_val):   \n    model = model_class(random_state=RANDOM_STATE, **params_dict)\n    model.fit(X_train, y_train, early_stopping_rounds=150, eval_set=[(X_val, y_val)], verbose=False)\n    auc_train = get_eval_metric(model, X=X_train, y_true=y_train)\n    auc_val = get_eval_metric(model, X=X_val, y_true=y_val)\n    print(f\"AUC train={auc_train}\")\n    print(f\"AUC val={auc_val}\")\n    return model, auc_val","15aeb597":"base_params = {\n    \"use_label_encoder\": False,\n    \"n_jobs\": 2,\n    \"tree_method\": \"gpu_hist\",\n    \"gpu_id\": 0,\n    \"predictor\": \"gpu_predictor\",\n    \"eval_metric\" : \"auc\",\n}\n\ndef objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n    \n    trial_params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 5000, 200),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.2, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 1.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n    }\n    model_params = {**trial_params, **base_params}\n    \n    _, auc_val = train_model(xgboost.XGBClassifier, model_params, X_train, y_train, X_val, y_val)\n    \n    return -auc_val","18df5c0a":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)","ae0472a0":"best_model_params = {**base_params, **study.best_params}","ef3ae77b":"best_model_params","04c6cc2a":"import json\n\nwith open(\"\/kaggle\/working\/best_model_params.json\", \"w\") as json_file:\n    json.dump(best_model_params, json_file, indent=2)","6141fef3":"!cat \/kaggle\/working\/best_model_params.json","3d15861b":"## Model","d8992581":"## Create best params file","e686b90f":"## Read data","60902a8b":"## Helper functions","c0a17126":"## Configs\n\n* `DEBUG_MODE`: reduce the number of rows to 500, for faster testing and debugging\n* `RANDOM_STATE`: set fix random state for model\n* `N_TRIALS`: number of Optuna trials\n","56ef6731":"### Hyperparameter Optimisation\n\n"}}