{"cell_type":{"a011f3bb":"code","eacf52f7":"code","e272b868":"code","bb1cc862":"code","b50df82f":"code","1a424083":"code","ffb1cebd":"code","a6fe7e0f":"code","9d4e6603":"code","1689594b":"code","26b81ec9":"code","02842821":"code","72a0c90b":"code","4b3643c3":"code","159674f3":"code","bd2fed30":"code","881e34bd":"code","911b5319":"code","6a4131de":"code","96dfcbf7":"code","e81ef527":"code","47f7a95d":"code","01895233":"code","1638f0a0":"code","48073558":"code","c45a1eb3":"code","f678564e":"code","a4d895ad":"code","36faff82":"markdown","89fb8ce9":"markdown","514d6fca":"markdown","4744790c":"markdown","6b4ae609":"markdown","8adc1e74":"markdown","d8a1b919":"markdown","16134888":"markdown","71ff3b69":"markdown","908ede58":"markdown","1dd49f2d":"markdown","15495d29":"markdown","3e66144e":"markdown","c89611af":"markdown","7b91f0de":"markdown","cd494915":"markdown","cc7cffa6":"markdown","ff1bfa69":"markdown","45bbb0c3":"markdown","b3e2fc57":"markdown","1af1fd31":"markdown","b7932957":"markdown","4d4314a1":"markdown","6d6ce7d1":"markdown","277030e5":"markdown","d8734e45":"markdown","51213437":"markdown","68445f9e":"markdown","8eaf67e9":"markdown","e93499af":"markdown","08f8d60b":"markdown","65a52191":"markdown"},"source":{"a011f3bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eacf52f7":"credit_card_dataset = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","e272b868":"credit_card_dataset.head(10).T","bb1cc862":"credit_card_dataset.shape","b50df82f":"credit_card_dataset.info()","1a424083":"credit_card_dataset.describe()","ffb1cebd":"credit_card_dataset.isnull().sum()","a6fe7e0f":"credit_card_dataset.duplicated().sum()","9d4e6603":"unique = credit_card_dataset.nunique()\nunique","1689594b":"credit_card_dataset.drop_duplicates(keep='first', inplace=True)","26b81ec9":"credit_card_dataset.shape","02842821":"variance = credit_card_dataset.std()\/credit_card_dataset.mean()\nvariance","72a0c90b":"plt.figure(figsize=(30,20))\ncorr = credit_card_dataset.corr()\nsns.heatmap(corr, annot=True, cmap='RdYlGn')\nplt.show()","4b3643c3":"features = credit_card_dataset.drop(['Class'], axis=1)\nlabel = credit_card_dataset['Class']\nfrom sklearn.feature_selection import f_classif\nfval, pval = f_classif(features, label)\nfor i in range(len(pval)):\n    print(features.columns[i], pval[i].round(4))","159674f3":"from sklearn.ensemble import ExtraTreesRegressor\netr = ExtraTreeRegressor()\netr.fit(features, label)","bd2fed30":"plt.figure(figsize=(12,5))\nfeatures_importance = pd.Series(etr.feature_importances_, index=features.columns)\nfeatures_importance.nlargest(20).plot(kind = 'barh')\nplt.show()","881e34bd":"count_classes = pd.value_counts(credit_card_dataset['Class'], sort=True)\nplt.figure(figsize=(12,5))\ncount_classes.plot(kind='bar', rot=0)\nplt.title('Outcome Class Distribution')\nplt.xlabel(\"Class\")\nplt.ylabel(\"No Of Count\")\nplt.show()","911b5319":"normal_trans = credit_card_dataset[credit_card_dataset['Class']==0]\nfraud_trans = credit_card_dataset[credit_card_dataset['Class']==1]\nnormal_trans.shape, fraud_trans.shape","6a4131de":"features = credit_card_dataset.drop(['Class'], axis=1)\nlabel = credit_card_dataset['Class']","96dfcbf7":"from imblearn.combine import SMOTETomek\nsmt = SMOTETomek(random_state=20)\nfeatures_new, label_new = smt.fit_sample(features, label)\nfeatures_new.shape, label_new.shape","e81ef527":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=20, shuffle=False, random_state=None)\nskf.get_n_splits(features_new, label_new)\nfor train_index, test_index in skf.split(features_new, label_new):\n    features_train, features_test = features_new.iloc[train_index], features_new.iloc[test_index]\n    label_train, label_test = label_new.iloc[train_index], label_new.iloc[test_index]","47f7a95d":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier(n_estimators=20, max_depth=8, min_samples_leaf=10,\n                                            min_samples_split=30, random_state=5, oob_score=True)\nrandom_forest_model.fit(features_train, label_train)\nrandom_forest_model.predict(features_test)","01895233":"from sklearn.metrics import recall_score\nrecall_score(label_train, random_forest_model.predict(features_train))","1638f0a0":"from sklearn.metrics import recall_score\nrecall_score(label_test, random_forest_model.predict(features_test))","48073558":"from sklearn.metrics import precision_score\nprecision_score(label_train, random_forest_model.predict(features_train))","c45a1eb3":"from sklearn.metrics import precision_score\nprecision_score(label_test, random_forest_model.predict(features_test))","f678564e":"from sklearn.metrics import accuracy_score\naccuracy_score(label_train, random_forest_model.predict(features_train))","a4d895ad":"from sklearn.metrics import accuracy_score\naccuracy_score(label_test, random_forest_model.predict(features_test))","36faff82":"Here we observe that 1081 rows are duplicate. So we need to handle the dulpicate rows.","89fb8ce9":"This shows that there is no missing data present in the dataset.","514d6fca":"# In this notebook I have built a model using the Ensemble Model Techinique for Credit Card Fraud Detection.\n\n# This is a highly imbalanced dataset and thus we will also see the techniques to convert the imbalanced dataset to balanced dataset.\n\n# Below is a quick view of all the steps performed to buid the model.\n# 1) Importing the dataset\n# 2) Analysing the dataset by viewing the top rows of dataset\n# 3) Exploratory Data Analysis\n         - Checking for null values.\n         - Cheking for duplicate rows.\n         - Handling duplicate rows.\n# 5) Feature Engineering\n         - Variance\n         - Correlation\n         - ANOVA Test\n         - Feature Importance\n# 6) Handling Imbalanced Dataset\n# 7) Splitting data into train and test\n# 8) Applying ML algorithm to build the model\n# 9) Using Metrics to check model performance","4744790c":"Checking the no of records in class 0 and class 1","6b4ae609":"# Importing the dataset","8adc1e74":"Using Variance, ANOVA Test, and Feature Importance for the feature engineering to identify which features are important to build the model.","d8a1b919":"Thus Correlation and ANOVA test shows that all the features are important and thus no feature can be dropped.","16134888":"View no of records in class 0 and class 1","71ff3b69":"# Applying ML Algorithm","908ede58":"# Random Forest","1dd49f2d":"Applying oversampling to balance the dataset","15495d29":"# Exploratory Data Analysis","3e66144e":"Dropping duplicate rows","c89611af":"Handling the duplicate rows is very important as it may lead to drop in performance of the model.","7b91f0de":"Prcision Score","cd494915":"ANOVA Test","cc7cffa6":"So after applying SMOTETomek the dataset is balanced.","ff1bfa69":"We have dropped the duplicate rows.","45bbb0c3":"# Splitting the data into train and test using Stratified K Fold","b3e2fc57":"Recall Score","1af1fd31":"Feature Importance","b7932957":"# Handling imbalanced dataset","4d4314a1":"Here we can see that the data is highly imbalanced.","6d6ce7d1":"As Random Forest works efficiently on large datasets and provide high accuracy on the large datasets, we will use the Random Forest to build the model.\nAlso Random FOrest never overfits.","277030e5":"Accuracy","d8734e45":"Identify columns that have a single value","51213437":"Variance","68445f9e":"# Feature Engineering","8eaf67e9":"Correlation","e93499af":"Imbalance Dataset - Dataset which containes very high difference between the class 0 and class 1 values i.e the classes are not represented equally.\n\nTechiniques to handle the imbalance dataset :\n1) Under-Sampling : balances the data by reducing the size of the class which is abundance.\n2) Over-Sampling : balances the data by incrementing the size of the rare samples.","08f8d60b":"# Handling duplicate rows","65a52191":"# Displaying top 10 rows of the dataset"}}