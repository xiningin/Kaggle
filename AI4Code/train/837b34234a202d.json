{"cell_type":{"8bbb8373":"code","b6c6c4f7":"code","f2aa0fad":"code","e1821c65":"code","fcaf9a24":"code","aaf9edc0":"code","c8945377":"code","cd9f8ed2":"code","5deb8407":"code","4f7ff81a":"code","f006cf5a":"code","cdb8d2f0":"code","5c11eb26":"markdown","4627646c":"markdown","e4cd978c":"markdown","adda9570":"markdown","c209057b":"markdown","7a28f27d":"markdown","d08a081d":"markdown","e0ec5156":"markdown","ae77b46e":"markdown","4390c756":"markdown","6bc29f31":"markdown"},"source":{"8bbb8373":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom fastai import *\nfrom fastai.tabular import *\nfrom sklearn.metrics import cohen_kappa_score","b6c6c4f7":"seed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)","f2aa0fad":"train_csv = pd.read_csv('..\/input\/train\/train.csv', low_memory=False)\ntest_csv = pd.read_csv('..\/input\/test\/test.csv', low_memory=False)\n\ndef preprocess(csv):\n    csv['Description_len'] = [len(str(tt)) for tt in csv['Description']]\n    csv['Name_len'] = [len(str(tt)) for tt in csv['Name']]\n    return csv\n\ntrain_csv = preprocess(train_csv)\ntest_csv = preprocess(test_csv)","e1821c65":"cat_names = ['Type','Breed1','Breed2','Gender','Color1','Color2','State','Color3','FurLength', 'Vaccinated','Dewormed','Sterilized','Health']\ncont_names = ['Age', 'MaturitySize', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'Description_len', 'Name_len']","fcaf9a24":"bs = len(train_csv)\nprocs = [FillMissing, Categorify, Normalize]\ndf = TabularList.from_df(train_csv, path='..\/input', cat_names=cat_names, cont_names=cont_names, procs=procs)\\\n                .no_split()\\\n                .label_from_df(cols='AdoptionSpeed')\ndf_test = TabularList.from_df(test_csv, path='..\/input', cat_names=cat_names, cont_names=cont_names, processor=df.train.x.processor)\ndata = df.add_test(df_test).databunch(num_workers=0, bs=bs)","aaf9edc0":"def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn:Optional[nn.Module]=None):\n    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n    layers = [nn.BatchNorm1d(n_in, track_running_stats=False)] if bn else []\n    if p != 0: layers.append(nn.Dropout(p))\n    layers.append(nn.Linear(n_in, n_out))\n    if actn is not None: layers.append(actn)\n    return layers\n\nclass TabularModel(nn.Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n        super().__init__()\n        ps = ifnone(ps, [0]*len(layers))\n        ps = listify(ps, layers)\n        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont, track_running_stats=False)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n        sizes = self.get_sizes(layers, out_sz)\n        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n        layers = []\n        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n        layers.append(nn.Softmax(dim=1))\n        self.layers = nn.Sequential(*layers)\n\n    def get_sizes(self, layers, out_sz):\n        return [self.n_emb + self.n_cont] + layers + [out_sz]\n\n    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        x = self.layers(x)\n        if self.y_range is not None:\n            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n        return x","c8945377":"unique, counts = np.unique(train_csv['AdoptionSpeed'], return_counts=True)\nE_hist = counts \/ sum(counts)\nww = [sum([E_hist[k]*(k-i)**2 for k in range(5)]) for i in range(5)]\n\nclass KappaLoss(nn.Module):\n    def forward(self, y1, y2, *args):\n        numer = (torch.matmul(y2.float().reshape([-1,1]),torch.tensor(()).new_ones((1, 5)).cuda()) - \n     torch.matmul(torch.tensor(()).new_ones((bs, 1)), torch.tensor(range(5), dtype=torch.float).reshape([1,5])).cuda())**2\n        numer = (numer * y1).sum()\n        denom = torch.matmul(torch.tensor(()).new_ones((bs, 1)), torch.tensor(ww).reshape([1,5])).cuda()\n        denom = (denom * y1).sum()\n        loss =  numer \/ denom\n        return loss","cd9f8ed2":"emb_szs = data.get_emb_szs({})\nmodel = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=[200,100], ps=None, emb_drop=0., y_range=None, use_bn=True)\nlearn = Learner(data, model, loss_func=KappaLoss(), path='\/tmp')","5deb8407":"learn.fit(10, 5e-2)\n\npred = learn.get_preds(ds_type=DatasetType.Train)\ny_pred = [int(np.argmax(row)) for row in pred[0]]\nprint('QWK insample', cohen_kappa_score(y_pred, pred[1], weights='quadratic'))","4f7ff81a":"print('RMSE insample', np.sqrt(np.mean((np.array(y_pred) - np.array(pred[1]))**2)))","f006cf5a":"np.unique(y_pred, return_counts=True)","cdb8d2f0":"pred = learn.get_preds(ds_type=DatasetType.Test)\ny_pred = [int(np.argmax(row)) for row in pred[0]]\n\ntest_csv['AdoptionSpeed'] = y_pred\ntest_csv[['PetID', 'AdoptionSpeed']].to_csv('submission.csv', index=False)","5c11eb26":"Standard fastai model definition with embeddings for categorical variables and two fully connected layers of default size. No regularization applied, to keep everything simple.","4627646c":"This kernel runs fastai tabular NN with default configuration and QWK loss function. The purpose of it is to demonstrate and get feedback on direct QWK loss function usage. Is it a competitive approach vs popular in public kernels regression + thresholds selection approach? I will be happy to hear your thoughts on that.\n\n**Features:**\n\nOnly provided csv files are used. Metadata, sentiments and images are not included. Description and name columns are taken into account through string lengths. \n    \n**LB score:** \n\n0.362","e4cd978c":"This is how the answers are distributed. Some people reported that 0 is never selected. Well, in this approach 0 is sometimes selected. ","adda9570":"fastai data definition code. Everything is standard, except for batch size which is the whole train dataset. It will help with loss function calculation.","c209057b":"Below is the custom loss function implementing QWK. It works only when every batch includes the whole train dataset. Note that `y1` is `5 x N` tensor of predictions and `y2` is `1 x N` tensor of labels. The predictions sum up to 1 (after Softmax), but they are not classification probabilities, but rather a confidence of selecting the corresponding alternative.","7a28f27d":"The following model definition is a copy paste from fastai library, I needed to do 2 changes in the model:\n\n1. Adding nn.Softmax as a last layer\n2. Setting track_running_stats=False for BatchNorm1d layers. This is needed because there are only 10 batches in my whole training, and at the end of training BatchNorm1d layers stored average statistics are not stabilized yet. I run inference on the whole test dataset at once, so it is not an issue.","d08a081d":"The following columns are used","e0ec5156":"Load train and test dataframes and add length columns for Description and Name","ae77b46e":"Submitting results","4390c756":"Disable fastai randomness","6bc29f31":"Learning and giving a score on the training dataset. \n\nI used high learning rate value here in order to fight overfitting to some degree. This kind of configuration without any regularization, without even validation set to keep an eye on it, has a high potential for overfitting. But using only 10 epochs and high learning rate gives decent results out of sample."}}