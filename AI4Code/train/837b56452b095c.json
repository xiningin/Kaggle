{"cell_type":{"aff76b9b":"code","818b5c31":"code","b839f65f":"code","8740a265":"code","ba5f3c47":"code","38312883":"code","24820fd2":"code","4f5730a2":"code","dc1385f9":"code","36b5d8a7":"code","9d362f3d":"code","36bacc56":"code","f885feff":"code","30e9cc19":"code","d145b597":"code","496397de":"code","d97ac263":"code","d272f633":"code","5063db12":"code","d9272ef6":"code","adf18984":"code","aac56b81":"code","df1a9219":"code","a2f8c339":"code","d7e798b8":"code","3f9ca54b":"code","a0cf7f40":"code","3b7241a9":"code","f5f479e4":"code","748c3805":"code","ad1af209":"code","eac37238":"code","2e5354c5":"code","ca294f1f":"code","88fda4c4":"code","cb5beb67":"code","b0cd2423":"code","883a974e":"code","7c8950a0":"code","3352b7fe":"code","0f2ecf24":"code","7ad29bf3":"code","ef291dd3":"code","0c8758e2":"code","593ee466":"code","f6e3e53e":"code","f73cc59d":"code","3feabe68":"code","04d7b9fc":"code","8a4cfe8c":"code","16b6fe04":"code","a019c53f":"code","815e3d1f":"code","45bba2b0":"code","3021b2ec":"code","ef680d57":"code","990b82be":"code","aeb7301f":"code","483cbec3":"code","bff4d663":"code","d05753e4":"code","7cf26863":"code","2864dd15":"code","69ca50be":"code","b2bd329a":"code","9716cec6":"code","96c65bbd":"code","12508a92":"code","7ff20b87":"code","1d9dde02":"code","99f46639":"code","09c320d2":"code","abc2c4bc":"code","0f24f84e":"code","cca6c0fc":"code","9437e310":"code","062e39e5":"code","b12404df":"code","308151ba":"code","2ed1c71d":"code","528f8283":"code","d7a343c8":"code","15f80a7f":"code","2b061ed0":"code","67b96d0b":"code","806ba13c":"code","b571a77c":"code","4762d95a":"code","5678c751":"code","c24f6509":"code","c3fa4b69":"code","d5907558":"code","b3764192":"code","151db7f6":"code","028f1b7e":"code","99c2d2c7":"code","095aa07e":"code","ca7b273f":"code","7585cb60":"code","a0bfd2e0":"code","9b4817de":"code","1f50360f":"code","2ac9009c":"code","453a1f28":"code","3de3d97d":"code","59a1749b":"code","d83e5b5f":"code","14729bfe":"code","01ffbc65":"code","8be1d4e5":"code","f189c066":"code","d10f81a8":"code","b9c9f548":"code","b1a65c06":"code","6552d24b":"code","65558ac3":"code","95761900":"code","08b37388":"code","5c8a14e3":"code","b9ed5376":"markdown","3f09d391":"markdown","3ea1eeae":"markdown","d3358f54":"markdown","860711e6":"markdown","17e65517":"markdown","0844f781":"markdown","e5a6d528":"markdown","25613120":"markdown","a8dfbf28":"markdown","ca76134e":"markdown","40689a2b":"markdown","05d5bbe3":"markdown","4e7d5622":"markdown","50751bde":"markdown","7a3649c6":"markdown","ec51d9d7":"markdown","2dae3069":"markdown","b70eaded":"markdown","4b30ff91":"markdown","520ae310":"markdown","70eb44d9":"markdown","1a41ee55":"markdown","e0f8e910":"markdown","01082df4":"markdown","0441d9f5":"markdown","74444e7a":"markdown","e50178ad":"markdown","16fd2e41":"markdown","d4d5fe68":"markdown","58d3d406":"markdown","504009c9":"markdown","28625347":"markdown","dc78dc61":"markdown","7f7e7155":"markdown","6496a025":"markdown","636273d7":"markdown","8e0a1df6":"markdown","b1cd88e0":"markdown","ac5f43ee":"markdown","77ff7ef4":"markdown","776d2145":"markdown","37c83fce":"markdown","4d2e653a":"markdown","daeb71d8":"markdown","423cbc04":"markdown","97623b21":"markdown","575466b7":"markdown","4486f17f":"markdown","4bb8c977":"markdown","2fe6f64a":"markdown","d17163fc":"markdown","efb65a08":"markdown","69b30833":"markdown","a3dccc07":"markdown","23de8e49":"markdown","b717aefd":"markdown"},"source":{"aff76b9b":"# Importing libraries\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')","818b5c31":"import warnings\nwarnings.filterwarnings('ignore')","b839f65f":"# Loading the train data\ndf = pd.read_csv('\/kaggle\/input\/customer\/Train.csv')\n\n# Looking top 10 rows\ndf.head(10)","8740a265":"# Looking the bigger picture\ndf.info()","ba5f3c47":"# Checking the number of missing values in each column\ndf.isnull().sum()","38312883":"# Removing all those rows that have 3 or more missing values\ndf = df.loc[df.isnull().sum(axis=1)<3]","24820fd2":"# Looking random 10 rows of the data\ndf.sample(10)","4f5730a2":"print('The count of each category\\n',df.Var_1.value_counts())","dc1385f9":"# Checking for null values\ndf.Var_1.isnull().sum()","36b5d8a7":"# Filling the missing values w.r.t other attributes underlying pattern \ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'Yes'),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df['Var_1'])) & (df['Graduated'] == 'No'),\"Var_1\"] = 'Cat_4'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & ((df['Profession'] == 'Lawyer') | (df['Profession'] == 'Artist')),\"Var_1\"] = 'Cat_6'\ndf.loc[ (pd.isnull(df[\"Var_1\"])) & (df['Age'] > 40),\"Var_1\"] = 'Cat_6'","9d362f3d":"# Counting Var_1 in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Var_1\"].value_counts().unstack().round(3)\n\n# Percentage of category of Var_1 in each segment\nax2 = df.pivot_table(columns='Var_1',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","36bacc56":"print('The count of gender\\n',df.Gender.value_counts())","f885feff":"# Checking the count of missing values\ndf.Gender.isnull().sum()","30e9cc19":"# Counting male-female in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Gender\"].value_counts().unstack().round(3)\n\n# Percentage of male-female in each segment\nax2 = df.pivot_table(columns='Gender',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","d145b597":"print('Count of married vs not married\\n',df.Ever_Married.value_counts())","496397de":"# Checking the count of missing values\ndf.Ever_Married.isnull().sum()","d97ac263":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & ((df['Spending_Score'] == 'Average') | (df['Spending_Score'] == 'High')),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Spending_Score'] == 'Low'),\"Ever_Married\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Age'] > 40),\"Ever_Married\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Ever_Married\"])) & (df['Profession'] == 'Healthcare'),\"Ever_Married\"] = 'No'","d272f633":"# Counting married and non-married in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Ever_Married\"].value_counts().unstack().round(3)\n\n# Percentage of married and non-married in each segment\nax2 = df.pivot_table(columns='Ever_Married',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","5063db12":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","d9272ef6":"# Checking the count of missing values\ndf.Age.isnull().sum()","adf18984":"# Looking the distribution of column Age\nplt.figure(figsize=(10,5))\n\nskewness = round(df.Age.skew(),2)\nkurtosis = round(df.Age.kurtosis(),2)\nmean = round(np.mean(df.Age),0)\nmedian = np.median(df.Age)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Age)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(1,2,2)\nsns.distplot(df.Age)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","aac56b81":"# Looking the distribution of column Age w.r.t to each segment\na = df[df.Segmentation =='A'][\"Age\"]\nb = df[df.Segmentation =='B'][\"Age\"]\nc = df[df.Segmentation =='C'][\"Age\"]\nd = df[df.Segmentation =='D'][\"Age\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Age\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","df1a9219":"# Converting the datatype from float to int\ndf['Age'] = df['Age'].astype(int)","a2f8c339":"df.Age.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","d7e798b8":"# Divide people in the 4 age group\ndf['Age_Bin'] = pd.cut(df.Age,bins=[17,30,45,60,90],labels=['17-30','31-45','46-60','60+'])","3f9ca54b":"# Counting different age group in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Age_Bin\"].value_counts().unstack().round(3)\n\n# Percentage of age bins in each segment\nax2 = df.pivot_table(columns='Age_Bin',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","a0cf7f40":"print('Count of each graduate and non-graduate\\n',df.Graduated.value_counts())","3b7241a9":"# Checking the count of missing values\ndf.Graduated.isnull().sum()","f5f479e4":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Spending_Score'] == 'Average'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Profession'] == 'Artist'),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Age'] > 49),\"Graduated\"] = 'Yes'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Var_1'] == 'Cat_4'),\"Graduated\"] = 'No'\ndf.loc[ (pd.isnull(df[\"Graduated\"])) & (df['Ever_Married'] == 'Yes'),\"Graduated\"] = 'Yes'\n\n# Replacing remaining NaN with previous values\ndf['Graduated'] = df['Graduated'].fillna(method='pad')","748c3805":"# Counting graduate and non-graduate in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Graduated\"].value_counts().unstack().round(3)\n\n# Percentage of graduate and non-graduate in each segment\nax2 = df.pivot_table(columns='Graduated',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","ad1af209":"print('Count of each profession\\n',df.Profession.value_counts())","eac37238":"# Checking the count of missing values\ndf.Profession.isnull().sum()","2e5354c5":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Work_Experience'] > 8),\"Profession\"] = 'Homemaker'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Age'] > 70),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Family_Size'] < 3),\"Profession\"] = 'Lawyer'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'Average'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Graduated'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'Yes'),\"Profession\"] = 'Artist'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Ever_Married'] == 'No'),\"Profession\"] = 'Healthcare'\ndf.loc[ (pd.isnull(df[\"Profession\"])) & (df['Spending_Score'] == 'High'),\"Profession\"] = 'Executives'","ca294f1f":"# Count of segments in each profession\nax1 = df.groupby([\"Profession\"])[\"Segmentation\"].value_counts().unstack().round(3)\n\n# Percentage of segments in each profession\nax2 = df.pivot_table(columns='Segmentation',index='Profession',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (16,5))\nlabel = ['Artist','Doctor','Engineer','Entertainment','Executives','Healthcare','Homemaker','Lawyer','Marketing']\nax[0].set_xticklabels(labels = label,rotation = 45)\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (16,5))\nax[1].set_xticklabels(labels = label,rotation = 45)\n\nplt.show()","88fda4c4":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","cb5beb67":"# Checking the count of missing values\ndf.Work_Experience.isnull().sum()","b0cd2423":"# Replacing NaN with previous values\ndf['Work_Experience'] = df['Work_Experience'].fillna(method='pad')","883a974e":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Work_Experience.skew(),2)\nkurtosis = round(df.Work_Experience.kurtosis(),2)\nmean = round(np.mean(df.Work_Experience),0)\nmedian = np.median(df.Work_Experience)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Work_Experience)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Work_Experience)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","7c8950a0":"# Looking the distribution of column Work_Experience w.r.t to each segment\na = df[df.Segmentation =='A'][\"Work_Experience\"]\nb = df[df.Segmentation =='B'][\"Work_Experience\"]\nc = df[df.Segmentation =='C'][\"Work_Experience\"]\nd = df[df.Segmentation =='D'][\"Work_Experience\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Work_Experience\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Work Experience')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","3352b7fe":"# Changing the data type\ndf['Work_Experience'] = df['Work_Experience'].astype(int)","0f2ecf24":"df.Work_Experience.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","7ad29bf3":"# Dividing the people into 3 category of work experience \ndf['Work_Exp_Category'] = pd.cut(df.Work_Experience,bins=[-1,1,7,15],labels=['Low Experience','Medium Experience','High Experience'])","ef291dd3":"# Counting different category of work experience in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Work_Exp_Category\"].value_counts().unstack().round(3)\n\n# Percentage of work experience in each segment\nax2 = df.pivot_table(columns='Work_Exp_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","0c8758e2":"print('Count of spending score\\n',df.Spending_Score.value_counts())","593ee466":"# Checking the count of missing values\ndf.Spending_Score.isnull().sum()","f6e3e53e":"# Counting different category of spending score in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Spending_Score\"].value_counts().unstack().round(3)\n\n# Percentage of spending score in each segment\nax2 = df.pivot_table(columns='Spending_Score',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","f73cc59d":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","3feabe68":"# Checking the count of missing values\ndf.Family_Size.isnull().sum()","04d7b9fc":"# Filling the missing values w.r.t other attributes underlying pattern\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Ever_Married'] == 'Yes'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Var_1'] == 'Cat_6'),\"Family_Size\"] = 2.0\ndf.loc[ (pd.isnull(df[\"Family_Size\"])) & (df['Graduated'] == 'Yes'),\"Family_Size\"] = 2.0\n\n# Fill remaining NaN with previous values\ndf['Family_Size'] = df['Family_Size'].fillna(method='pad')","8a4cfe8c":"# Looking the distribution of column Work Experience\nplt.figure(figsize=(15,10))\n\nskewness = round(df.Family_Size.skew(),2)\nkurtosis = round(df.Family_Size.kurtosis(),2)\nmean = round(np.mean(df.Family_Size),0)\nmedian = np.median(df.Family_Size)\n\nplt.subplot(1,2,1)\nsns.boxplot(y=df.Family_Size)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.distplot(df.Family_Size)\nplt.title('Distribution Plot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.show()","16b6fe04":"# Looking the distribution of column Family Size w.r.t to each segment\na = df[df.Segmentation =='A'][\"Family_Size\"]\nb = df[df.Segmentation =='B'][\"Family_Size\"]\nc = df[df.Segmentation =='C'][\"Family_Size\"]\nd = df[df.Segmentation =='D'][\"Family_Size\"]\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.boxplot(data = df, x = \"Segmentation\", y=\"Family_Size\")\nplt.title('Boxplot')\n\nplt.subplot(1,2,2)\nsns.kdeplot(a,shade= False, label = 'A')\nsns.kdeplot(b,shade= False, label = 'B')\nsns.kdeplot(c,shade= False, label = 'C')\nsns.kdeplot(d,shade= False, label = 'D')\nplt.xlabel('Family Size')\nplt.ylabel('Density')\nplt.title(\"Mean\\n A: {}\\n B: {}\\n C: {}\\n D: {}\".format(round(a.mean(),0),round(b.mean(),0),round(c.mean(),0),round(d.mean(),0)))\n\nplt.show()","a019c53f":"# Changing the data type\ndf['Family_Size'] = df['Family_Size'].astype(int)","815e3d1f":"df.Family_Size.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","45bba2b0":"# Divide family size into 3 category\ndf['Family_Size_Category'] = pd.cut(df.Family_Size,bins=[0,4,6,10],labels=['Small Family','Big Family','Joint Family'])","3021b2ec":"# Counting different category of family size in each segment\nax1 = df.groupby([\"Segmentation\"])[\"Family_Size_Category\"].value_counts().unstack().round(3)\n\n# Percentage of family size in each segment\nax2 = df.pivot_table(columns='Family_Size_Category',index='Segmentation',values='ID',aggfunc='count')\nax2 = ax2.div(ax2.sum(axis=1), axis = 0).round(2)\n\n#count plot\nfig, ax = plt.subplots(1,2)\nax1.plot(kind=\"bar\",ax = ax[0],figsize = (15,4))\nax[0].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[0].set_title(str(ax1))\n\n#stacked bars\nax2.plot(kind=\"bar\",stacked = True,ax = ax[1],figsize = (15,4))\nax[1].set_xticklabels(labels = ['A','B','C','D'],rotation = 0)\nax[1].set_title(str(ax2))\nplt.show()","ef680d57":"print('Count of each category of segmentation\\n',df.Segmentation.value_counts())","990b82be":"segments = df.loc[:,\"Segmentation\"].value_counts()\nplt.xlabel(\"Segment\")\nplt.ylabel('Count')\nsns.barplot(segments.index , segments.values).set_title('Segments')\nplt.show()","aeb7301f":"df.reset_index(drop=True, inplace=True)\ndf.info()","483cbec3":"# number of unique ids\ndf.ID.nunique()","bff4d663":"df.describe(include='all')","d05753e4":"df = df[['ID','Gender', 'Ever_Married', 'Age', 'Age_Bin', 'Graduated', 'Profession', 'Work_Experience', 'Work_Exp_Category',\n         'Spending_Score', 'Family_Size', 'Family_Size_Category','Var_1', 'Segmentation']]\ndf.head(10)","7cf26863":"df1 = df.copy()\ndf1.head()","2864dd15":"# Separating dependent-independent variables\nX = df1.drop('Segmentation',axis=1)\ny = df1['Segmentation']","69ca50be":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf1_trainX, df1_testX, df1_trainY, df1_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","b2bd329a":"# converting binary variables to numeric\ndf1_trainX['Gender'] = df1_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf1_trainX['Ever_Married'] = df1_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_trainX['Graduated'] = df1_trainX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_trainX['Spending_Score'] = df1_trainX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_trainX.Profession,prefix='Profession')\ndf1_trainX = pd.concat([df1_trainX,pf],axis=1)\n\nvr = pd.get_dummies(df1_trainX.Var_1,prefix='Var_1')\ndf1_trainX = pd.concat([df1_trainX,vr],axis=1)\n\n# scaling continuous variables\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1_trainX[['Age','Work_Experience','Family_Size']] = scaler.fit_transform(df1_trainX[['Age','Work_Experience','Family_Size']])\n\ndf1_trainX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","9716cec6":"# converting binary variables to numeric\ndf1_testX['Gender'] = df1_testX['Gender'].replace(('Male','Female'),(1,0))\ndf1_testX['Ever_Married'] = df1_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf1_testX['Graduated'] = df1_testX['Graduated'].replace(('Yes','No'),(1,0))\ndf1_testX['Spending_Score'] = df1_testX['Spending_Score'].replace(('High','Average','Low'),(3,2,1))\n\n# converting nominal variables into dummy variables\npf = pd.get_dummies(df1_testX.Profession,prefix='Profession')\ndf1_testX = pd.concat([df1_testX,pf],axis=1)\n\nvr = pd.get_dummies(df1_testX.Var_1,prefix='Var_1')\ndf1_testX = pd.concat([df1_testX,vr],axis=1)\n\n# scaling continuous variables\ndf1_testX[['Age','Work_Experience','Family_Size']] = scaler.transform(df1_testX[['Age','Work_Experience','Family_Size']])\n\ndf1_testX.drop(['ID','Age_Bin','Profession','Work_Exp_Category','Family_Size_Category','Var_1'], axis=1, inplace=True)","96c65bbd":"df1_trainX.shape, df1_trainY.shape, df1_testX.shape, df1_testY.shape","12508a92":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df1_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","7ff20b87":"df2 = df.copy()\ndf2.head()","1d9dde02":"# Separating dependent-independent variables\nX = df2.drop('Segmentation',axis=1)\ny = df2['Segmentation']","99f46639":"# import the train-test split\nfrom sklearn.model_selection import train_test_split\n\n# divide into train and test sets\ndf2_trainX, df2_testX, df2_trainY, df2_testY = train_test_split(X,y, train_size = 0.7, random_state = 101, stratify=y)","09c320d2":"# Converting binary to numeric\ndf2_trainX['Gender'] = df2_trainX['Gender'].replace(('Male','Female'),(1,0))\ndf2_trainX['Ever_Married'] = df2_trainX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_trainX['Graduated'] = df2_trainX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_trainX.Age_Bin,prefix='Age_Bin')\ndf2_trainX = pd.concat([df2_trainX,ab],axis=1)\n\npf = pd.get_dummies(df2_trainX.Profession,prefix='Profession')\ndf2_trainX = pd.concat([df2_trainX,pf],axis=1)\n\nwe = pd.get_dummies(df2_trainX.Work_Exp_Category,prefix='WorkExp')\ndf2_trainX = pd.concat([df2_trainX,we],axis=1)\n\nsc = pd.get_dummies(df2_trainX.Spending_Score,prefix='Spending')\ndf2_trainX = pd.concat([df2_trainX,sc],axis=1)\n\nfs = pd.get_dummies(df2_trainX.Family_Size_Category,prefix='FamilySize')\ndf2_trainX = pd.concat([df2_trainX,fs],axis=1)\n\nvr = pd.get_dummies(df2_trainX.Var_1,prefix='Var_1')\ndf2_trainX = pd.concat([df2_trainX,vr],axis=1)\n\ndf2_trainX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","abc2c4bc":"# Converting binary to numeric\ndf2_testX['Gender'] = df2_testX['Gender'].replace(('Male','Female'),(1,0))\ndf2_testX['Ever_Married'] = df2_testX['Ever_Married'].replace(('Yes','No'),(1,0))\ndf2_testX['Graduated'] = df2_testX['Graduated'].replace(('Yes','No'),(1,0))\n\n# Converting nominal variables to dummy variables\nab = pd.get_dummies(df2_testX.Age_Bin,prefix='Age_Bin')\ndf2_testX = pd.concat([df2_testX,ab],axis=1)\n\npf = pd.get_dummies(df2_testX.Profession,prefix='Profession')\ndf2_testX = pd.concat([df2_testX,pf],axis=1)\n\nwe = pd.get_dummies(df2_testX.Work_Exp_Category,prefix='WorkExp')\ndf2_testX = pd.concat([df2_testX,we],axis=1)\n\nsc = pd.get_dummies(df2_testX.Spending_Score,prefix='Spending')\ndf2_testX = pd.concat([df2_testX,sc],axis=1)\n\nfs = pd.get_dummies(df2_testX.Family_Size_Category,prefix='FamilySize')\ndf2_testX = pd.concat([df2_testX,fs],axis=1)\n\nvr = pd.get_dummies(df2_testX.Var_1,prefix='Var_1')\ndf2_testX = pd.concat([df2_testX,vr],axis=1)\n\ndf2_testX.drop(['ID','Age','Age_Bin','Profession','Work_Experience','Work_Exp_Category','Spending_Score',\n               'Family_Size','Family_Size_Category','Var_1'],axis=1,inplace=True)","0f24f84e":"df2_trainX.shape, df2_trainY.shape, df2_testX.shape, df2_testY.shape","cca6c0fc":"# Correlation matrix\nplt.figure(figsize=(17,10))\nsns.heatmap(df2_trainX.corr(method='spearman').round(2),linewidth = 0.5,annot=True,cmap=\"YlGnBu\")\nplt.show()","9437e310":"train_knn1_x = df1_trainX.copy()\ntrain_knn1_x.head()","062e39e5":"train_knn1_y = df1_trainY.copy()\ntrain_knn1_y.head()","b12404df":"# import KNeighbors ClaSSifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Instantiate the model\nmodel_knn1 = KNeighborsClassifier(n_neighbors=3)\n\n#fitting the model\nmodel_knn1.fit(train_knn1_x, train_knn1_y)\n\n#checking the training score\nprint('Accuracy on training: ',model_knn1.score(train_knn1_x, train_knn1_y))\n\n# predict the target on the train dataset\nyhat1 = model_knn1.predict(train_knn1_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(train_knn1_y.values, yhat1, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm1)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_knn1_y.values, yhat1))","308151ba":"# Function for checking the optimal number of k\ntrain_accuracy = []\nfor k in range(1,11):\n    model_knn1 = KNeighborsClassifier(n_neighbors=k)\n    model_knn1.fit(train_knn1_x, train_knn1_y)\n    train_accuracy.append(model_knn1.score(train_knn1_x, train_knn1_y))","2ed1c71d":"frame = pd.DataFrame({'no.of k':range(1,11), 'train_acc':train_accuracy})\nframe","528f8283":"# Elbow curve\nplt.figure(figsize=(12,5))\nplt.plot(frame['no.of k'], frame['train_acc'], marker='o')\nplt.xlabel('Number of k')\nplt.ylabel('performance')\nplt.show()","d7a343c8":"# final model\nmodel_knn1 = KNeighborsClassifier(n_neighbors=2)\n\n# fitting the model\nmodel_knn1.fit(train_knn1_x, train_knn1_y)\n\n# Training score\nprint(model_knn1.score(train_knn1_x, train_knn1_y).round(4))","15f80a7f":"test_knn1_x = df1_testX.copy()\ntest_knn1_x.head()","2b061ed0":"test_knn1_y = df1_testY.copy()\ntest_knn1_y.head()","67b96d0b":"y_knn1 = model_knn1.predict(test_knn1_x)\ny_knn1","806ba13c":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_knn1_y.values, y_knn1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_knn1_y.values, y_knn1))","b571a77c":"pd.Series(y_knn1).value_counts()","4762d95a":"train_knn2_x = df2_trainX.copy()\ntrain_knn2_x.head()","5678c751":"train_knn2_y = df2_trainY.copy()\ntrain_knn2_y.head()","c24f6509":"# import KNeighbors ClaSSifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Instantiate the model\nmodel_knn2 = KNeighborsClassifier(n_neighbors=3)\n\n#fitting the model\nmodel_knn2.fit(train_knn2_x, train_knn2_y)\n\n#checking the training score\nprint('Accuracy on training: ',model_knn2.score(train_knn2_x, train_knn2_y))\n\n# predict the target on the train dataset\nyhat2 = model_knn2.predict(train_knn2_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(train_knn2_y.values, yhat2, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm2)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_knn2_y.values, yhat2))","c3fa4b69":"# Function to check the optimal number of k\ntrain_accuracy = []\nfor k in range(1,11):\n    model_knn2 = KNeighborsClassifier(n_neighbors=k)\n    model_knn2.fit(train_knn2_x, train_knn2_y)\n    train_accuracy.append(model_knn2.score(train_knn2_x, train_knn2_y))","d5907558":"frame = pd.DataFrame({'no.of k':range(1,11), 'train_acc':train_accuracy})\nframe","b3764192":"# Elbow plot\nplt.figure(figsize=(12,5))\nplt.plot(frame['no.of k'], frame['train_acc'], marker='o')\nplt.xlabel('number of k')\nplt.ylabel('performance')\nplt.show()","151db7f6":"# final model\nmodel_knn2 = KNeighborsClassifier(n_neighbors=3)\n\n#fitting the model\nmodel_knn2.fit(train_knn2_x, train_knn2_y)\n\n#Training score\nprint(model_knn2.score(train_knn2_x, train_knn2_y).round(4))","028f1b7e":"test_knn2_x = df2_testX.copy()\ntest_knn2_x.head()","99c2d2c7":"test_knn2_y = df2_testY.copy()\ntest_knn2_y.head()","095aa07e":"y_knn2 = model_knn2.predict(test_knn2_x)\ny_knn2","ca7b273f":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------')\nprint(confusion_matrix(test_knn2_y.values, y_knn2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------')\nprint(classification_report(test_knn2_y.values, y_knn2))","7585cb60":"pd.Series(y_knn2).value_counts()","a0bfd2e0":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_knn1_y.values, yhat1))\nprint('\\nTest data')\nprint(classification_report(test_knn1_y.values, y_knn1))","9b4817de":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_knn2_y.values, yhat2))\nprint('\\nTest data')\nprint(classification_report(test_knn2_y.values, y_knn2))","1f50360f":"train_nb1_x = df1_trainX.copy()\ntrain_nb1_x.head()","2ac9009c":"train_nb1_y = df1_trainY.copy()\ntrain_nb1_y.head()","453a1f28":"# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n# instantiate the model\ngnb1 = GaussianNB()\n\n# Train model\nmodel_nb1 = gnb1.fit(train_nb1_x, train_nb1_y)\n\n# Predicting the classes\nyhat3 = gnb1.predict(train_nb1_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(train_nb1_y.values, yhat3, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('\\n\\n-------The confusion matrix for this model is-------')\nprint(cm3)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_nb1_y.values, yhat3))","3de3d97d":"test_nb1_x = df1_testX.copy()\ntest_nb1_x.head()","59a1749b":"test_nb1_y = df1_testY.copy()\ntest_nb1_y.head()","d83e5b5f":"y_nb1 = gnb1.predict(test_nb1_x)\ny_nb1","14729bfe":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_nb1_y.values, y_nb1, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_nb1_y.values, y_nb1))","01ffbc65":"pd.Series(y_nb1).value_counts()","8be1d4e5":"train_nb2_x = df2_trainX.copy()\ntrain_nb2_x.head()","f189c066":"train_nb2_y = df2_trainY.copy()\ntrain_nb2_y.head()","d10f81a8":"# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n# instantiate the model\ngnb2 = GaussianNB()\n\n# Train model\nmodel_nb2 = gnb2.fit(train_nb2_x, train_nb2_y)\n\n# Predicting the classes\nyhat4 = gnb2.predict(train_nb2_x)\n\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(train_nb2_y.values, yhat4, labels=[\"A\",\"B\",\"C\",\"D\"])\nprint('-------The confusion matrix for this model is-------')\nprint(cm4)\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the whole report of the model-------')\nprint(classification_report(train_nb2_y.values, yhat4))","b9c9f548":"test_nb2_x = df2_testX.copy()\ntest_nb2_x.head()","b1a65c06":"test_nb2_y = df2_testY.copy()\ntest_nb2_y.head()","6552d24b":"y_nb2 = gnb2.predict(test_nb2_x)\ny_nb2","65558ac3":"from sklearn.metrics import confusion_matrix\nprint('-------The confusion matrix for test data is-------\\n')\nprint(confusion_matrix(test_nb2_y.values, y_nb2, labels=[\"A\",\"B\",\"C\",\"D\"]))\n\nfrom sklearn.metrics import classification_report\nprint('\\n\\n-------Printing the report of test data-------\\n')\nprint(classification_report(test_nb2_y.values, y_nb2))","95761900":"pd.Series(y_nb2).value_counts()","08b37388":"print('************************  MODEL-1 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_nb1_y.values, yhat3))\nprint('\\nTest data')\nprint(classification_report(test_nb1_y.values, y_nb1))","5c8a14e3":"print('************************  MODEL-2 REPORT  *********************************\\n')\nprint('Train data')\nprint(classification_report(train_nb2_y.values, yhat4))\nprint('\\nTest data')\nprint(classification_report(test_nb2_y.values, y_nb2))","b9ed5376":"###### Graduated","3f09d391":"###### Age","3ea1eeae":"#### Predicting on test set","d3358f54":"**Ways to treat missing values**<br>\nCheck here:https:\/\/www.datasciencenovice.com\/2020\/08\/5-ways-to-treat-missing-values.html","860711e6":"![image.png](attachment:image.png)","17e65517":"##### Observation:\n1. `Age` and `Ever_Married` has a positive correlation of 0.6 which means that people who are married have more age as compared to those who are unmarried.\n2. `Age` and `Profession_Healthcare` has a negative correlation of 0.5 which means all those people whose profession is healthcare are younger in age to those who of other professions people.\n3. `Profession_Healthcare` and `Ever_Married` has negative correlation of 0.42 which means all those peoples whose profession is healthcare are unmarried.(only 13% of healthcare professionals are married).\n4. `Age` and `Profession_Lawyer` has a positive correlation of 0.42 which means all those people whose profession is lawyer are older in age to those of other professions people.\n5. `Ever_Married` and `Spending_Average` has a positive correlation which means those who are married spend averagely.(around 42% married people spent averagely)\n6. `Ever_Married` and `Spending_High` has a little positive correlation which means those who are married spend high.(around 25% of married people spent high)\n7. `Ever_Married` and `Spending_Low` has a negative correlation of 0.67 which means those who are unmarried spent low.(round 99% of unmarried people spent low )\n8. `Age` and `Spending_Score` has a positive correlation of 0.42 which means as age increase the spending power also increase.\n9. `Profession_Executives` and `Spending_High` has positive correlation of 0.40 which means all those peoples whose profession is executive spent high.(around 66% of executives spent high).","0844f781":"#### <font color='red'>Making two different dataframes\n<font color='red'>Now we consider\/make two different dataframes apart from the above main dataframe (namely df) <br>\n- `df1`: Spending Score(ranking), Age(normalise), Work_Experience(normalise), Family Size(normalise)\n- `df2`: Spending Score(dummy variables), Age Bin(dummy variables), Work_Exp_Category(dummy variables), Family_Size_Category(dummy variables)","e5a6d528":"#### Building the model with `second type` of dataframe(df_type2)","25613120":"###### Profession","a8dfbf28":"### Problem Statement\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n<br><br>\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n<br><br>\nYou are required to help the manager to predict the right group of the new customers.<br><br>\nYou can check this link: https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/","ca76134e":"- Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.\n- Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his\/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n- Na\u00efve Bayes classification produces good results when we use it for textual data analysis such as Natural Language Processing.\n- Na\u00efve Bayes models are also known as simple Bayes or independent Bayes. All these names refer to the application of Bayes\u2019 theorem in the classifier\u2019s decision rule. Na\u00efve Bayes classifier applies the Bayes\u2019 theorem in practice. This classifier brings the power of Bayes\u2019 theorem to machine learning.","40689a2b":"## Multi Class Classification \n- A classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multi-class classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. \n- Common examples include image classification (is it a cat, dog, human, etc) or handwritten digit recognition (classifying an image of a handwritten number into a digit from 0 to 9).\n- In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).\n- Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.","05d5bbe3":"###### Work Experience","4e7d5622":"<font color='blue'>Segment A,B and C have major customers from profession:**Artist** while Segment D have major customers from profession:**Healthcare** <br>\n**Homemaker** is least in all the four segment","50751bde":"###### Segmentation","7a3649c6":"<font color='blue'>1. Both the reports are very similar in terms of accuracy, precision, recall and f1-score.<br>\n<font color='blue'>2. `model-2 is little better` as to model-1 because it has better number in all the metrices compared to model-1.<br>\n<font color='blue'>3. So we can't really say which model is best for model building in Naive Bayes Classifier technique. ","ec51d9d7":"###### Var_1","2dae3069":"#### Predicting on test set","b70eaded":"---\n## <font color='orange'>Step III: Model Evaluation","4b30ff91":"###### Gender","520ae310":"## <font color='orange'>Step II: Model Building","70eb44d9":"###### Preprocessing on train data","1a41ee55":"###### Preprocessing in train data","e0f8e910":"---\n## <font color='orange'>Step III: Model Evaluation","01082df4":"#### Predicting on test set","0441d9f5":"#### Training data","74444e7a":"###### Family Size","e50178ad":"### V. k-Nearest Neighbour (k-NN)","16fd2e41":"<font color='blue'>All the 4 segments have around same number of male-female distribution. In all segment male are more than female. <br> \n<font color='blue'>But segment D has highest male percentage as compared to other segments.","d4d5fe68":"<font color='blue'>Segment C has most number of customers who are graduated while segment D has lowest number of graduate customers.","58d3d406":"#### Building the model with `second type` of dataframe(df_type2)","504009c9":"<font color='blue'>Now all the data has been cleaned. There is no missing value and columns are in right format. <br>\n<font color='blue'>All the ids are unique that is there is no duplicate entry.<br>\n<font color='blue'>Created new column: 'Age_Bin', 'Work_Exp_Category' and 'Family_Size_Category'. <br> \n<font color='blue'>Delete only 0.2% of rows. ","28625347":"### Variables Description\n\n           \n| Variable\t            | Definition                                                        |\n|---------------------- |-------------------------------------------------------------------|\n| ID\t                | Unique ID                                                         |\n| Gender\t            | Gender of the customer                                            |\n| Ever_Married\t        | Marital status of the customer                                    |\n| Age\t                | Age of the customer                                               |\n| Graduated\t            | Is the customer a graduate?                                       |\n| Profession\t        | Profession of the customer                                        |\n| Work_Experience\t    | Work Experience in years                                          |\n| Spending_Score\t    | Spending score of the customer                                    |\n| Family_Size\t        | Number of family members for the customer(including the customer) |\n| Var_1\t                | Anonymised Category for the customer                              |\n| Segmentation(target)  | Customer Segment of the customer                                  |","dc78dc61":"###### Ever Married","7f7e7155":"---\n## <font color='orange'>Step II: Model Building","6496a025":"<font color='blue'>1. It is clearly seen that model-1 is better than model-2 w.r.t all the metrices.<br>\n<font color='blue'>2. But the point here to note that there are 2327 entries which are common in train-test data i.e. 2327 test entries(out of total 2627) are already in train data and when we see the result on test set model-1 gave 63% accuracy while model-2 gave 55%.<br>\n<font color='blue'>3. 69% train accuracy with 65% test accuracy is good trade-off by model-1 compared to model-2 which gave 58% train accuracy and 55% test accuracy.<br>\n<font color='blue'>4. So we can conclude that `df_type1 is a better data` for model building in k-NN technique.","636273d7":"##### Why Spearman?\nCheck this: https:\/\/idkwhoneedstohearthis.blogspot.com\/2020\/05\/correlation-why-spearmans.html","8e0a1df6":"###### Preprocessing in test data","b1cd88e0":"###### Spending Score","ac5f43ee":"<font color='blue'>Segment D has people with relatively more experienced than other segments while Segment C has people with low experience","77ff7ef4":"---\n## <font color='orange'>Step I: Importing, Cleaning and EDA","776d2145":"<font color='blue'>We seen that most of the customers in segment C are married while segment D has least number of married customers. It means segment D is a group of customers that are singles and maybe younger in age. ","37c83fce":"<font color='blue'>In each of the segment the count of cat_6 or proportion of cat_6 is very high i.e. most of the entries in the given data belongs to cat_6.","4d2e653a":"#### Building the model with `first type` of dataframe(df_type1)","daeb71d8":"## Algorithms Covered\n\n1. In this notebook, we are going to use **k-NN** and **Naive Bayes** algorithms to solve the same problem.\n\n2. In first notebook, we used **One vs Rest(OvR)** and **One vs One(OvO)** algorithms to solve the problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c101)\n\n3. In second notebook, we used **Decision Tree** and **Random Forest** algorithms to solve the same problem, [click here to see](https:\/\/www.kaggle.com\/mittalvasu95\/multi-class-classification-c102?scriptVersionId=43467747).\n\n**Note**:The EDA process is same in all the three notebooks. The only change is in algorithm to solve the problem.","423cbc04":"<font color='blue'>In the given data it is observed that most of the people have family size of 1 or 2 (i.e. they have small family).<br> But Segment D has more number of  big families as compared to other segments.","97623b21":"<font color='blue'>The mean age of segment D is 33 and we can say that people in this segment are belong to 30s i.e. they are younger and also from 'ever_married' distribution it is seen that segment D has maximum number of customers who are singles indicating they are younger.<br>\n<font color='blue'>Also segment C has mean age of 49 and we also seen that most cutomers in this segment are married. ","575466b7":"![image.png](attachment:image.png)","4486f17f":"#### Is ACCURACY everything? \nIn general, there is no general best measure. The best measure is derived from your needs. `In a sense, it is not a machine learning question, but a business question`. It is common that two people will use the same data set but will choose different metrics due to different goals.\n<br><br>\nAccuracy is a great metric. Actually, most metrics are great and I like to evaluate many metrics. However, at some point you will need to decide between using model A or B. There you should use a single metric that best fits your need.<br><br>\nRead more: https:\/\/towardsdatascience.com\/is-accuracy-everything-96da9afd540d","4bb8c977":"#### Building the model with `first_type` of dataframe(df_type1)","2fe6f64a":"###### Preprocessing on test data","d17163fc":"---\n---\n### VI. Naive Bayes Classifier ","efb65a08":"### <font color = 'blue'>Topics Covered in this notebook<\/font>\n1. Basic cleaning and EDA\n2. k-NN\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm\n3. Naive Bayes Classifier\n    - Model Building with two different dataframes\n    - Model Evaluation\n    - Final comment on which dataframe is good for this algorithm","69b30833":"- KNN is a non-parametric and lazy learning algorithm. \n    - Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. \n    - Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.\n- KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n\n#### The KNN Algorithm\n1. Load the data\n2. Initialize K to your chosen number of neighbors\n3. For each example in the data\n    1. Calculate the distance between the query example and the current example from the data.\n    2. Add the distance and the index of the example to an ordered collection\n4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n5. Pick the first K entries from the sorted collection\n6. Get the labels of the selected K entries\n7. If regression, return the mean of the K labels\n8. If classification, return the mode of the K labels","a3dccc07":"<font color='blue'>Segment D has maximum number of people with low spending score while in Segment C average spending people are more.","23de8e49":"<font color='blue'>1. We have seen that there are `missing values` in the dataset. So we will work on data cleaning.<br>\n<font color='blue'>2. `Create some new attributes` based upon given data\/domain knowledge\/prior experience.<br>\n<font color='blue'>3. Create graphs and `performs EDA` and write observations.","b717aefd":"#### Predicting on test set"}}