{"cell_type":{"8e6e9afa":"code","8c4dfc7f":"code","21d5e0e3":"code","f5969232":"code","f1680577":"code","4d4f1027":"code","cdda1e7a":"code","f78e6899":"code","b47fa46f":"code","36d4df64":"code","0ec2ad8c":"code","e4cc25b9":"code","ea95f89d":"code","0726e492":"code","133ec527":"code","0683528a":"code","cdf3c76b":"code","424c59cd":"code","1d52126a":"code","64bbc703":"code","53d7c15b":"code","9e34b2b6":"code","bce4bd9f":"code","08a99c74":"code","641ced71":"code","b99e9e87":"code","17e00a1d":"code","e574d673":"code","de5c6620":"code","5c2fbf3c":"code","9918d8fa":"code","e3b1b691":"code","c5b83dbf":"code","e294b1ee":"code","ca32f5eb":"code","23864ebc":"code","c552b752":"code","dfb78fee":"code","5afaba8f":"code","b2393820":"code","924719ee":"code","07593038":"code","695fb67e":"code","79d3633e":"code","c6a44a5e":"code","66d74435":"code","20ccf7c4":"code","c539fedf":"code","72eab54c":"code","c70de1c6":"code","a430d3be":"code","4765b431":"code","1fea9c06":"code","45008b48":"code","efd60415":"code","8a82adbe":"code","5d7a14d8":"code","bba863a5":"code","d550a011":"code","0ecb35c4":"code","1e814ffc":"code","6db207a9":"code","1dfcf726":"code","337c416c":"code","cca39057":"markdown","302997cc":"markdown","d894756c":"markdown","38b7d4ef":"markdown","280a7300":"markdown","ec0061c7":"markdown","b023979f":"markdown","e9148635":"markdown","f685e8e3":"markdown","5d2e3c4e":"markdown","c2dfa9ee":"markdown","3ff02d75":"markdown","0fd964a9":"markdown","554411c4":"markdown","92aed923":"markdown","4d636ce1":"markdown","5c230d8e":"markdown","82f71f0d":"markdown","485573f1":"markdown","6052c766":"markdown","98ddaf29":"markdown","f8366183":"markdown","1c164dac":"markdown","0546a2d1":"markdown","162d290c":"markdown","b17e2d53":"markdown","30954169":"markdown","230a4e79":"markdown","da0f170a":"markdown","2a2232e4":"markdown","fb9c9108":"markdown","ba4444f6":"markdown","a6e14a63":"markdown","26abe735":"markdown","1ac4e877":"markdown","1a7b3edd":"markdown","409ae5c8":"markdown","e0f77193":"markdown","a20f9736":"markdown","fdec4500":"markdown","125c8da2":"markdown","defa334a":"markdown","0a1ead67":"markdown","0dc0467a":"markdown","899a4c3c":"markdown","ba8ae46d":"markdown","3c7f8693":"markdown","2a0ec0f5":"markdown","8fde585b":"markdown","c709153e":"markdown","af3c0551":"markdown","a3570293":"markdown","0cc97171":"markdown","f6336c0b":"markdown","520c6103":"markdown","a8268747":"markdown","b96126af":"markdown","c57f06e0":"markdown","6d7fc6a0":"markdown","69da7d61":"markdown","ee63f170":"markdown","d1de014b":"markdown","e1a17d37":"markdown","114e1f2a":"markdown","479238c0":"markdown","ba15f432":"markdown"},"source":{"8e6e9afa":"# Import das bibliotecas\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Constantes\nrand_state = 21","8c4dfc7f":"# Import dos dados de treino e teste\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","21d5e0e3":"# Lista das vari\u00e1veis (para renomear as colunas)\nvar_labels = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n              \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n              \"Hours per week\", \"Country\", \"Income\"]\n\n# Ler os dados de treino e teste, substituir o nome das vari\u00e1veis e indentifcar os valores nulos como ?\ndf_train = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\",\n                       names=var_labels,\n                       header=0,\n                       na_values=\"?\")\n\n# Remove o Income da lista de labels\nvar_labels.pop(-1)\ndf_test = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\",\n                       index_col=0,\n                       names=var_labels,\n                       header=0,\n                       na_values=\"?\")","f5969232":"df_train.head()","f1680577":"df_train.shape","4d4f1027":"df_test.head()","cdda1e7a":"df_test.shape","f78e6899":"df_train.info()","b47fa46f":"df_train[\"Sex\"].value_counts()","36d4df64":"df_train[\"Education\"].value_counts()","0ec2ad8c":"df_train.describe()","e4cc25b9":"!pip install -U pandas-profiling","ea95f89d":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(df_train, title='Exploratory Analysis',html={'style':{'full_width':True}})\nprofile","0726e492":"# Gr\u00e1fico de pizza simples\nplt.pie(df_train[\"Sex\"].value_counts(), labels=df_train[\"Sex\"].unique(),autopct='%1.0f%%')\nplt.title('Sex');","133ec527":"# Gr\u00e1fico de boxes da Idade quebrado renda (Income)\nfig = sns.catplot(x=\"Income\", y=\"Age\", kind=\"boxen\", data=df_train)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Age vs. Income');","0683528a":"# Gr\u00e1fico de boxes do tempo de Educa\u00e7\u00e3o quebrado renda (Income)\nfig = sns.catplot(x=\"Income\", y=\"Education-Num\", kind=\"boxen\", data=df_train)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Education vs. Income');","cdf3c76b":"# Gr\u00e1fico de boxes do Capital Gain quebrado pela renda (Income)\nfig = sns.catplot(x=\"Income\", y=\"Capital Gain\", kind=\"boxen\", data=df_train)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Capital Gain vs. Income');","424c59cd":"# M\u00e9dia do Capital Gain quebrada por Income\ndf_train.groupby([\"Income\"])[\"Capital Gain\"].mean()","1d52126a":"# Gr\u00e1fico de boxes do Capital Loss quebrado pela renda (Income)\nfig = sns.catplot(x=\"Income\", y=\"Capital Loss\", kind=\"boxen\", data=df_train)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Capital Loss vs. Income');","64bbc703":"# M\u00e9dia do Capital Loss quebrada por Income\ndf_train.groupby([\"Income\"])[\"Capital Loss\"].mean()","53d7c15b":"# Gr\u00e1fico de boxes do Horas trabalhadas por semana quebrado pela renda (Income)\nfig = sns.catplot(x=\"Income\", y=\"Hours per week\", kind=\"boxen\", data=df_train)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Hours per week vs. Income');","9e34b2b6":"# M\u00e9dia do Capital Loss quebrada por Income\ndf_train.groupby([\"Income\"])[\"Hours per week\"].mean()","bce4bd9f":"# Gr\u00e1fico de barras da Educa\u00e7\u00e3o quebrado pela renda (Income)\nfig, ax = plt.subplots(figsize=(10, 10))\nfig = sns.countplot(y=\"Education\", data=df_train, hue=\"Income\", order = df_train[\"Education\"].value_counts().index)\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Educa\u00e7\u00e3o vs. Income');\n","08a99c74":"# Gr\u00e1fico de barras da Etnia (Race) quebrado pela renda (Income)\nfig, ax = plt.subplots(figsize=(10, 10))\nfig = sns.countplot(x=\"Race\", data=df_train, hue=\"Income\")\nsns.set(rc={'figure.facecolor':'white'})\nplt.title('Hours per week vs. Income');","641ced71":"df_train = df_train.drop(['fnlwgt', 'Education', 'Country'], axis=1)","b99e9e87":"df_train.head()","17e00a1d":"# Labels\ny = df_train.pop(\"Income\")\n\n# Features\nX = df_train","e574d673":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=rand_state)","de5c6620":"X_train.head()","5c2fbf3c":"y_train.head()","9918d8fa":"print(\"Tamanho dos bancos\")\nprint(\"Treino:\", X_train.shape[0])\nprint(\"Valida\u00e7\u00e3o:\", X_val.shape[0])\n","e3b1b691":"num_features = ['Age', 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week']\n\ncat_features = ['Workclass','Martial Status','Occupation','Relationship','Race','Sex']\n\nprint(\"Features\")\nprint(\"Num\u00e9ricas:\", num_features)\nprint(\"Categ\u00f3ricas:\", cat_features)","c5b83dbf":"# Missing data\nX_train.isnull().sum(axis = 0)","e294b1ee":"from sklearn.impute import SimpleImputer\n\n# Instanciando o Imputer utilizando o mais frequente\nsimple_imputer = SimpleImputer(strategy='most_frequent')","ca32f5eb":"from sklearn.preprocessing import OneHotEncoder\n\n# Inicializa nosso Encoder\none_hot = OneHotEncoder()","23864ebc":"from sklearn.pipeline import Pipeline\n\n# Pipeline para features categ\u00f3ricas\ncat_pipeline = Pipeline(steps = [('imputer', SimpleImputer(strategy = 'most_frequent')),\n                                 ('onehot', OneHotEncoder(drop='if_binary'))])","c552b752":"from sklearn.preprocessing import StandardScaler\n\n# Cria o nosso StandardScaler\nscaler = StandardScaler()","dfb78fee":"# Pipeline para features categ\u00f3ricas\nnum_pipeline = Pipeline(steps = [('scaler', StandardScaler())])","5afaba8f":"from sklearn.compose import ColumnTransformer\n\n# Cria\u00e7\u00e3o do pr\u00e9-processador para cada tipo de feature\npreprocessor = ColumnTransformer(transformers = [('num', num_pipeline, num_features),\n                                                 ('cat', cat_pipeline, cat_features)])","b2393820":"X_train = preprocessor.fit_transform(X_train)\nX_val = preprocessor.fit_transform(X_val)","924719ee":"X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\nX_val = pd.DataFrame.sparse.from_spmatrix(X_val)\nX_train.head()","07593038":"# Import do modelo\nfrom sklearn.linear_model import LogisticRegression\n\n# Instanciando o modelo\nlog_reg = LogisticRegression(random_state = rand_state)\n\n# Parametros do modelo\nlog_reg.get_params()","695fb67e":"# Parametros para serem testados\nparam = {'penalty':['l1','l2', 'elasticnet'],\n         'C' : np.logspace(-4, 4, 20),\n         'solver' : ['lbfgs', 'liblinear', 'sag', 'saga'],\n         'max_iter' : [100, 1000, 2500, 5000]}\n\n# Instanciando o RandomizedSearch com Cross Validation de 10 folds\nrandsearch_log_reg = RandomizedSearchCV(log_reg,\n                                        param_distributions=param,\n                                        cv=10,\n                                        scoring=\"accuracy\",\n                                        verbose=True,\n                                        n_jobs=-1)\n\n\n# Fit do modelo\nrandsearch_log_reg.fit(X_train, y_train)\n\n# Print dos melhor hiper-parametros para o modelo\nprint(\"Best Estimator:\")\nprint(randsearch_log_reg.best_estimator_)\n\n# Print do melhor score\nprint(\"Best Score:\")\nprint(randsearch_log_reg.best_score_)","79d3633e":"# Cria\u00e7\u00e3o de um modelo com os melhores hiperparametros\nlog_reg = randsearch_log_reg.best_estimator_\n\n# Fit do modelo\nlog_reg.fit(X_train, y_train)\n\n# Predi\u00e7\u00e3o nos dados de valida\u00e7\u00e3o\ny_pred1 = log_reg.predict(X_val)\n\n# Acurr\u00e1cia do modelo nos dados de valida\u00e7\u00e3o\naccuracy_score(y_val, y_pred1)","c6a44a5e":"from xgboost import XGBClassifier\n\nxgboost = XGBClassifier(random_state = rand_state)\n\nxgboost.get_params()","66d74435":"# Parametros para serem testados\nparam = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n         \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n         \"min_child_weight\" : [ 1, 3, 5, 7 ],\n         \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n         \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n         \"n_estimators\"     : np.arange(10, 500, 10).tolist()}\n\n# Instanciando o RandomizedSearch com Cross Validation de 10 folds\nrandsearch_xgboost = RandomizedSearchCV(xgboost,\n                              param_distributions=param,\n                              cv=10,\n                              scoring=\"accuracy\",      \n                              verbose=True,\n                              n_jobs=-1)\n\n# Fit do modelo\nrandsearch_xgboost.fit(X_train, y_train)\n\n# Print dos melhor hiper-parametros para o modelo\nprint(\"Best Estimator:\")\nprint(randsearch_xgboost.best_estimator_)\n\n# Print do melhor score\nprint(\"Best Score:\")\nprint(randsearch_xgboost.best_score_)","20ccf7c4":"xgboost = randsearch_xgboost.best_estimator_\nxgboost.fit(X_train, y_train)\ny_pred2 = xgboost.predict(X_val)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_val, y_pred2)","c539fedf":"from sklearn.ensemble import RandomForestClassifier\n\nrandforst = RandomForestClassifier(random_state = rand_state)\n\nrandforst.get_params()","72eab54c":"# Parametros para serem testados\nparam = {'bootstrap': [True, False],\n         'max_depth': [1, 2, 3, 5, 10, 20, 30, 40, 50, 100],\n         'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\n# Instanciando o RandomizedSearch com Cross Validation de 10 folds\nrandsearch_randforst = RandomizedSearchCV(randforst,\n                              param_distributions=param,\n                              cv=10,\n                              scoring=\"accuracy\",      \n                              verbose=True,\n                              n_jobs=-1)\n\n# Fit do modelo\nrandsearch_randforst.fit(X_train, y_train)\n\n# Print dos melhor hiper-parametros para o modelo\nprint(\"Best Estimator:\")\nprint(randsearch_randforst.best_estimator_)\n\n# Print do melhor score\nprint(\"Best Score:\")\nprint(randsearch_randforst.best_score_)","c70de1c6":"randforst = randsearch_randforst.best_estimator_\nrandforst.fit(X_train, y_train)\ny_pred3 = randforst.predict(X_val)\n\naccuracy_score(y_val, y_pred3)","a430d3be":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(max_iter=100, activation=\"relu\", random_state = rand_state, early_stopping=True)\n\nmlp.get_params()","4765b431":"# Lista das hidden layers\nhidden_layers = []\n\n# Nodes de 10 a 100\nnodes_1 = [int(i) for i in np.linspace(10, 100, 10)]\nnodes_2 = [int(i) for i in np.linspace(10, 100, 10)]\nnodes_3 = [int(i) for i in np.linspace(10, 100, 10)]\n  \n# Cria\u00e7\u00e3o dos n\u00f3s de 1 a 3 hidden layers\nfor i in nodes_1:\n    hidden_layers.append(((i,)))\n    for j in nodes_2:\n        hidden_layers.append(((i,j,)))\n        for k in nodes_3:\n            hidden_layers.append(((i,j,k)))\n","1fea9c06":"# Parametros\nparams_mlp = {'hidden_layer_sizes': hidden_layers,\n              \"alpha\": [1e-05, 0.001, 0.1]}\n\n# Instanciando o RandomizedSearch com Cross Validation de 10 folds\nrandsearch_mlp = RandomizedSearchCV(mlp,\n                                    params_mlp,\n                                    cv=10,\n                                    verbose=True,\n                                    n_jobs=-1,\n                                    scoring='accuracy')\n\n# Fit do modelo\nrandsearch_mlp.fit(X_train, y_train)\n\n# Print dos melhor hiper-parametros para o modelo\nprint(\"Best Estimator:\")\nprint(randsearch_mlp.best_estimator_)\n\nprint(\"Best Score:\")\nprint(randsearch_mlp.best_score_)","45008b48":"mlp = randsearch_mlp.best_estimator_\nmlp.fit(X_train, y_train)\ny_pred4 = mlp.predict(X_val)\naccuracy_score(y_val, y_pred3)\n","efd60415":"# Codificando a label dos dados de treino e valida\u00e7\u00e3o para 0 e 1\ny_train_cod = pd.Categorical(pd.factorize(y_train)[0])\ny_val_cod = pd.Categorical(pd.factorize(y_val)[0])\n\n# Codificando a label das predi\u00e7\u00f5es para 0 e 1\ny_pred1_cod = pd.Categorical(pd.factorize(y_pred1)[0])\ny_pred2_cod = pd.Categorical(pd.factorize(y_pred2)[0])\ny_pred3_cod = pd.Categorical(pd.factorize(y_pred3)[0])\ny_pred4_cod = pd.Categorical(pd.factorize(y_pred4)[0])\n","8a82adbe":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_validate\n","5d7a14d8":"# Defini\u00e7\u00e3o de um score\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score),\n           'recall':make_scorer(recall_score), \n           'f1_score':make_scorer(f1_score)}","bba863a5":"def model_evaluation(X, y, folds):\n\n    \n    # Cross-validation com os modelos tunados\n    log_reg_model = cross_validate(log_reg, X, y, cv=folds, scoring=scoring)\n    xgboost_model = cross_validate(xgboost, X, y, cv=folds, scoring=scoring)\n    randforst_model = cross_validate(randforst, X, y, cv=folds, scoring=scoring)\n    mlp_model = cross_validate(mlp, X, y, cv=folds, scoring=scoring)\n    \n    # Data Frame coma a performance do modelo\n    models_scores_table = pd.DataFrame({'Logistic Regression':[log_reg_model['test_accuracy'].mean(),\n                                                               log_reg_model['test_precision'].mean(),\n                                                               log_reg_model['test_recall'].mean(),\n                                                               log_reg_model['test_f1_score'].mean()],\n                                        \n                                        'XGBoost':[xgboost_model['test_accuracy'].mean(),\n                                                   xgboost_model['test_precision'].mean(),\n                                                   xgboost_model['test_recall'].mean(),\n                                                   xgboost_model['test_f1_score'].mean()],\n                                        \n                                        'Random Forest':[randforst_model['test_accuracy'].mean(),\n                                                         randforst_model['test_precision'].mean(),\n                                                         randforst_model['test_recall'].mean(),\n                                                         randforst_model['test_f1_score'].mean()],\n                                        \n                                        'Neural Network':[mlp_model['test_accuracy'].mean(),\n                                                          mlp_model['test_precision'].mean(),\n                                                          mlp_model['test_recall'].mean(),\n                                                          mlp_model['test_f1_score'].mean()]},\n                                       \n                                       index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n    \n    # Adiciona a coluna de melhor score\n    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n    \n    # Retorna o DataFrame\n    return(models_scores_table)\n    \n                                        \nmodel_evaluation(X_train, y_train_cod, 10)                                        ","d550a011":"X_test = df_test.drop(['fnlwgt', 'Education', 'Country'], axis=1)","0ecb35c4":"X_test = preprocessor.fit_transform(X_test)\nX_test = pd.DataFrame.sparse.from_spmatrix(X_test)","1e814ffc":"pred = xgboost.predict(X_test)\npred","6db207a9":"submission = pd.DataFrame()","1dfcf726":"submission[0] = X_test.index\nsubmission[1] = pred\nsubmission.columns = [\"Id\", \"Income\"]\nsubmission.head()","337c416c":"submission.to_csv('submission_final.csv',index = False)","cca39057":"Ap\u00f3s a importa\u00e7\u00e3o, os dados foram colcoados em dois DataFrames, um de treino e outro de test. Al\u00e9m disso, as labels foram renomeadas para facilitar a an\u00e1lise.\n\n**Obs:** a vari\u00e1vel Income foi retirada da lista de labels, j\u00e1 que o banco de dados de teste n\u00e3o tem essa vari\u00e1vel, que \u00e9 justamente nossa vari\u00e1vel alvo.","302997cc":"Aplicamos o pr\u00e9-processador nos dados de treino e valida\u00e7\u00e3o","d894756c":"<a id=\"sec-2\"><\/a>\n## 2. An\u00e1lise explorat\u00f3ria","38b7d4ef":"H\u00e1 15 vari\u00e1veis em nosso banco de dados, al\u00e9m do index do Id do usu\u00e1rio. Dessas, 7 s\u00e3o n\u00famericas e 9 s\u00e3o categ\u00f3ricas. H\u00e1 a seguir h\u00e1 a descri\u00e7\u00e3o das mesmas:\n\n* <code>Id<\/code> identifica o sujeito (observa\u00e7\u00e3o), usado como index. Vari\u00e1vel num\u00e9rica.\n* <code>Age<\/code> idade do indiv\u00edduo. Vari\u00e1vel num\u00e9rica.\n* <code>Workclass<\/code> regime de trabalho do indiv\u00edduo (Private, Self-emp-not-inc, Local-gov...). Vari\u00e1vel categ\u00f3rica.\n* <code>fnlwgt<\/code> significado desconhecido. Vari\u00e1vel num\u00e9rica.\n* <code>Education<\/code> escolaridade do indiv\u00edduo (Bachelors, Some-college, HS-grad...). Vari\u00e1vel categ\u00f3rica.\n* <code>Education-Num<\/code> escolaridade em anos. Vari\u00e1vel num\u00e9rica.\n* <code>Martial Status<\/code> estado civil (Married-civ-spouse, Divorced, Never-married...). Vari\u00e1vel categ\u00f3rica.\n* <code>Occupation<\/code> ocupa\u00e7\u00e3o do indiv\u00edduo(Tech-support, Craft-repair, Other-service...). Vari\u00e1vel categ\u00f3rica.\n* <code>Relationship<\/code> \"papel\" familiar (Wife, Own-child, Husband...). Vari\u00e1vel categ\u00f3rica.\n* <code>Race<\/code> grupo \u00e9tnico (White, Asian-Pac-Islander, Black...). Vari\u00e1vel categ\u00f3rica.\n* <code>Sex<\/code> sexo do indiv\u00edduo (Male e Female). Vari\u00e1vel categ\u00f3rica.\n* <code>Capital Gain<\/code> ganho de capital (heran\u00e7a?). Vari\u00e1vel num\u00e9rica.\n* <code>Capital Loss<\/code> perda de capital durante a vida. Vari\u00e1vel num\u00e9rica.\n* <code>Hours per week<\/code> horas trabalhadas por semana. Vari\u00e1vel num\u00e9rica.\n* <code>Country<\/code> pais do indiv\u00edduo (United-States, Cambodia, England...). Vari\u00e1vel categ\u00f3rica.\n* <code>Income<\/code> renda (maior que 50k ou menor que 50k). Vari\u00e1vel categ\u00f3rica.","280a7300":"Pode-se visualizar os dados de treino (32.560 observa\u00e7\u00f5es) e teste (16.280 observa\u00e7\u00f5es), respectivamente:","ec0061c7":"Tunning dos hiperparametros atrav\u00e9s do RandomSearch usando crossvalidation com 10 folds","b023979f":"Criamos uma fun\u00e7\u00e3o que faz um cross validation e avalia as m\u00e9tricas dos classificadores, colocando-as em um dataframe ","e9148635":"### 2.1 Descri\u00e7\u00e3o das vari\u00e1veis","f685e8e3":"## Random Forest ","5d2e3c4e":"Para lidar com o missing data optou-se por usar o Simple Imputer, j\u00e1 que as features que cont\u00e9m missing s\u00e3o categ\u00f3rias. Para isso, foi utilizado como estrat\u00e9gia o mais frequente","c2dfa9ee":"## Pr\u00e9-processamento de Features Categ\u00f3ricas","3ff02d75":"Tunning dos hiperparametros atrav\u00e9s do RandomSearch usando crossvalidation com 10 folds","0fd964a9":"## 3.1 Feature Selection ","554411c4":"J\u00e1 para as vari\u00e1veis num\u00e9ricas, fizemos a normaliza\u00e7\u00e3o j\u00e1 que h\u00e1 features com ordem de grandezas diferentes","92aed923":"Transforma\u00e7\u00e3o da <code>sperse matrix<\/code> para <code>DataFrame<\/code>","4d636ce1":"### 1.1 Import das bibliotecas","5c230d8e":"### 2.4 Vari\u00e1veis vs. Income","82f71f0d":"Exemplo de gr\u00e1fico para avaliar a frequ\u00eancia de uma vari\u00e1vel categ\u00f3rica:","485573f1":"<a id=\"sec-4\"><\/a>\n## 4. Desenvolvimento dos Classificadores","6052c766":"## Pr\u00e9-processamento de Features Num\u00e9ricas","98ddaf29":"J\u00e1 em rela\u00e7\u00e3o as vari\u00e1veis n\u00famericas, pode-se usar o comando <code>.describe()<\/code> para avaliar estat\u00edsticas b\u00e1sicas das vari\u00e1veis.\n\nDe maneira geral:\n\n* A idade m\u00e9dia dos indiv\u00edduos \u00e9 de 38 anos, ou seja, um p\u00fablico relativamente novo, com um desvio padr\u00e3o de 13,6 anos. Sendo tendo pessoas de 17 a 90 anos.\n* A m\u00e9dia de anos de educa\u00e7\u00e3o \u00e9 de 10 anos, com um desvio padr\u00e3o de 2,6. Variando entre 1 a 16 anos.\n* A m\u00e9dia de horas trabalhadas por semana \u00e9 de 40 horas com desvio padr\u00e3o de 12.3 horas. Desta forma, a m\u00e9dia semanal \u00e9 um regime de 8h di\u00e1rias. ","f8366183":"<a id=\"sec-3\"><\/a>\n# 3. Pr\u00e9-processamento","1c164dac":"Tunning dos hiperparametros atrav\u00e9s do RandomSearch usando crossvalidation com 10 folds","0546a2d1":"Gera\u00e7\u00e3o do report a partir do banco de dados <code>df_train()<\/code>","162d290c":"Atribuimos esses dois procedimento a uma pipeline","b17e2d53":"Utilizamos o <code>One Hot Encoder<\/code> para fazer o enconding das vari\u00e1veis categ\u00f3ricas, transformando elas em num\u00e9ricas","30954169":"Algumas vari\u00e1veis que n\u00e3o s\u00e3o relevantes foram removidas do banco","230a4e79":"Aplica\u00e7\u00e3o do pr\u00e9-processamento nos dados de teste","da0f170a":"### Cria\u00e7\u00e3o do pr\u00e9-processador","2a2232e4":"Nesta etapa, foi feita uma comapra\u00e7\u00e3o entre grupos, separados por pessoas com renda maior ou menor que 50k (vari\u00e1vel target). Os insights foram compilados a seguir:\n\n* A m\u00e9dia de idade das pessoas que tem renda maior que 50k \u00e9 maior que as que tem menos de 50k.\n* O tempo m\u00e9dio de educa\u00e7\u00e3o \u00e9 maior para aqueles com renda maior que 50k.\n* O Ganho de Capital (Capital Gain) \u00e9 parecido, entretanto tende a ser maior para aqueles com renda maior que 50k.\n* A Perda de Capital (Capital Loss) \u00e9 parecida, entretanto aqueles com renda maior que 50k tem uma m\u00e9dia de perda maior.\n* As horas trabalhadas por semana (Hours per week) \u00e9 maior para quem tem renda maior que 50k.\n* Pessoas com maior n\u00edvel educacional tendem a ter maior renda (ex: doutorado e mestrado).\n\nA seguir h\u00e1 os gr\u00e1ficos e an\u00e1lises que geraram estes insights.","fb9c9108":"### 2.2 An\u00e1lise das vari\u00e1veis","ba4444f6":"### 2.3 An\u00e1lise com Pandas Profile Report","a6e14a63":"O **XGBoost** se mostrou melhor em todas as m\u00e9tricas analisadas, principalmente em acurr\u00e1cia (87,47%), que \u00e9 justamente a m\u00e9trica utilziada no leaderboard desta competi\u00e7\u00e3o. O **Random Forest** (acc: 86,62%) foi o segundo  melhor classificador neste banco de dados, seguido pelas **Redes Neurais** (acc: 85,79%) e por fim a **Regress\u00e3o Log\u00edstica** (acc: 85,1%).","26abe735":"Para uma melhor an\u00e1lise explorat\u00f3ria dos dados, foi utilizado o <code>ProfileReport<\/code> do <code>pandas<\/code>. Este report gera uma an\u00e1lise das vari\u00e1veis, intera\u00e7\u00f5es, correla\u00e7\u00f5ese e missing, de uma maneira f\u00e1cil e r\u00e1pida.\n\nPara mais informa\u00e7\u00f5es veja: **[GitHub Pandas Profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling)**","1ac4e877":"<a id=\"sec-5\"><\/a>\n## 5. Comapara\u00e7\u00e3o dos Classificadores","1a7b3edd":"### Aplica\u00e7\u00e3o do pr\u00e9-processador","409ae5c8":"# Classifiers Comparison with Dataset Adult\n\nEstudo da base de dados [Adult](https:\/\/www.kaggle.com\/c\/adult-pmr3508), para a disciplina de Aprendizado de M\u00e1quina - PMR3508.\n\n**Desenvolvido por:** Jo\u00e3o \u00c1lex de S\u00e1 Bugelli - n\u00ba 10281542\n","e0f77193":"DataFrame submission foi criado","a20f9736":"Pode-se verificar os valores \u00fanicos das vari\u00e1veis categ\u00f3ricas com o comando <code>.value_counts()<\/code>. Segue alguns exemplos:","fdec4500":"## Objetivo\n\nEste estudo tem como objetivo desenvolver diferentes classificadores e comparar seu desempenho utilizando a base de dados Adult. Para isto, foram utilizados 4 utilizados: <code>Regress\u00e3o Log\u00edstica<\/code>, <code>XGBoost<\/code>, <code>Random Forest<\/code> e <code>Neural Networks<\/code>","125c8da2":"Neste DataFrame foi colcoado o Id e a predi\u00e7\u00e3o de renda (Income)","defa334a":"## Logistic Regression ","0a1ead67":"## Neural Networks","0dc0467a":"Nesta etapa criamos um pr\u00e9-processador atrav\u00e9s um pipeline para vari\u00e1veis categ\u00f3ricas e num\u00e9ricas. ","899a4c3c":"Predi\u00e7\u00e3o nos dados de teste","ba8ae46d":"Nesta etapa, desenvolvemos um pr\u00e9-processador que lida com o missing data, faz a normaliza\u00e7\u00e3o das features num\u00e9ricas e faz o encoding das vari\u00e1veis categ\u00f3ricas.","3c7f8693":"Neste etapa foi desenvolvido 4 classficadores, fazendo o tunning dos hiperparametros e predi\u00e7\u00e3o nos dados de valida\u00e7\u00e3o. Foram escolhidos os seguintes classificadores: <code>Regress\u00e3o Log\u00edstica<\/code>, <code>XGBoost<\/code>, <code>Random Forest<\/code> e <code>Neural Networks<\/code>","2a0ec0f5":"<a id=\"sec-1\"><\/a>\n## 1. Prepara\u00e7\u00e3o dos Dados","8fde585b":"### 1.2 Import dos dados","c709153e":"## XGBoosting","af3c0551":"Ap\u00f3s a cria\u00e7\u00e3o e tunning dos modelos, nesta etapa foi feita uma compara\u00e7\u00e3o entre os modelos avaliando as principais m\u00e9tricas de classifica\u00e7\u00e3o: Accuracy, Precision, Recall e F1_score ","a3570293":"Nesta etapa, foi feita a prepara\u00e7\u00e3o do ambiente importando as bibliotecas que foram utilizadas e a base de dados Adult, j\u00e1 separada em treino e teste. Al\u00e9m disso, o missing data foi identificado.","0cc97171":"Separamos as vari\u00e1veis em num\u00e9ricas e categ\u00f3ricas","f6336c0b":"Nesta etapa, foi feita uma an\u00e1lise explorat\u00f3ria dos dados, com objetivo de compreender melhor o banco de dados que estamos trabalhando. Para isso, foi feita uma descri\u00e7\u00e3o das vari\u00e1veis, avalio-se seus tipos, estat\u00edsticas descritivas, distribui\u00e7\u00f5es, correla\u00e7\u00f5es, al\u00e9m de alguns cruzamentos com a vari\u00e1vel Income, que \u00e9 nossa vari\u00e1vel alvo neste estudo.","520c6103":"Tunning dos hiperparametros atrav\u00e9s do RandomSearch usando crossvalidation com 10 folds","a8268747":"### Etapas\n\n1. [Prepara\u00e7\u00e3o dos dados](#sec-1)   \n2. [An\u00e1lise Explorat\u00f3ria](#sec-2)\n3. [Pr\u00e9-processamento](#sec-3)\n4. [Desenvolvimento dos Classificadores](#sec-4)\n5. [Comapara\u00e7\u00e3o dos Classificadores](#sec-5)\n5. [Predi\u00e7\u00e3o e Submiss\u00e3o](#sec-6)","b96126af":"Os dados de treino e teste foram importados do Kaggle","c57f06e0":"Por fim, fizemos o pr\u00e9-processamento nos dados de teste e a predi\u00e7\u00e3o utilizando o **XGBoost**, al\u00e9m de gerar o csv de submiss\u00e3o","6d7fc6a0":"## 3.2 Train e Validation Split","69da7d61":"Os dados de predi\u00e7\u00e3o s\u00e3o extra\u00eddos em csv para submiss\u00e3o","ee63f170":"<a id=\"sec-6\"><\/a>\n## 6. Predi\u00e7\u00e3o e Submiss\u00e3o","d1de014b":"Separamos os dados de treino em treino(75%) e valida\u00e7\u00e3o(25%) para realizar os testes dos classificadores","e1a17d37":"Atribuimos esse procedimento a uma pipeline","114e1f2a":"Instala\u00e7\u00e3o do pandas-profiling:","479238c0":"Desta forma, criamos um pr\u00e9-processador para vari\u00e1veis categ\u00f3ricas e num\u00e9ricas, utilizando as pipelines definidas acima e definindo o tipo das vari\u00e1veis atrav\u00e9s dos nomes das features","ba15f432":"## 3.3 Criando um Pr\u00e9-processador"}}