{"cell_type":{"23df4a0c":"code","92c6f82d":"code","6ef32aae":"code","dc9ae934":"code","5542a076":"code","53029c8d":"code","0b855adb":"code","006b577e":"code","27844ea6":"code","4d8bac61":"code","38e49ff8":"code","79e3504c":"code","f01423c5":"code","cf5665e5":"code","414104e4":"code","2c2105dd":"code","238534ff":"code","f9c6e5c4":"code","7cd0b490":"code","779bc5a2":"code","d09b96f1":"code","f8c4dc23":"code","1a6a6fe6":"code","2e6bc95e":"code","92a385af":"code","abf1310d":"code","b9d939eb":"code","68d9d5a7":"code","df08316d":"code","045974f2":"code","e1ed0572":"code","27874d5c":"code","80a7e756":"code","a50b1521":"code","ad653c9a":"code","fe1edc49":"code","1874cf80":"code","7f2f80a5":"code","14a401a6":"code","de1a1d06":"code","f7b84802":"code","6b96b1d6":"code","b05476df":"code","a5e73588":"code","fc7c97b9":"code","df2c8a05":"code","c94001a1":"code","7d5a4e06":"code","155af19a":"code","4362031e":"code","c0bdde3b":"code","22e129d1":"code","e1863bc9":"code","a2bffe35":"code","e44c22d4":"code","afa6cbb3":"code","9620b341":"code","b6bd45ac":"code","254155b9":"code","0358cbcd":"code","2cd0d3a4":"code","94ad0a26":"code","fadef42f":"code","cdf114f3":"code","b371c137":"code","6aba9a3b":"code","9f763b87":"code","82be870c":"code","a2c36b66":"code","ab2d9fca":"code","ac807139":"code","bd532da3":"code","2f248bab":"code","6c47674a":"code","665ab09a":"code","4acd18d2":"code","6031e078":"markdown","df5e74cf":"markdown","3ae17317":"markdown","a88ebe31":"markdown","49bfb41e":"markdown","8d8e64f9":"markdown","5320d566":"markdown","ece3789f":"markdown","62a85b2d":"markdown","27085048":"markdown","7f418300":"markdown","ed884034":"markdown","fa80df1a":"markdown","8de26d65":"markdown","b659703d":"markdown","5cbdcd0d":"markdown","6a5b9c65":"markdown","e8de8695":"markdown","6a2f6899":"markdown","ca6e4840":"markdown","fe6a4b5e":"markdown","8c7867fe":"markdown","73990cfe":"markdown","4a774202":"markdown","d16d561d":"markdown","96bf5e5d":"markdown","2dbba826":"markdown","5c011920":"markdown","7c9c6239":"markdown","124e72a8":"markdown","d3124c8f":"markdown","f89d4b79":"markdown","d113031f":"markdown","ae2ea6d3":"markdown","fe052dba":"markdown","e0514548":"markdown","a1178ad0":"markdown","c5ed7c5e":"markdown","31dc36f1":"markdown","436b07d4":"markdown","1d172ee3":"markdown","43912107":"markdown","f380b0ac":"markdown","3c0194ed":"markdown","d676a129":"markdown","4af98f65":"markdown","d53bf29a":"markdown","a1e8024f":"markdown","fff74a90":"markdown","c4e87e25":"markdown","3a6c9bdb":"markdown","d3182fd9":"markdown","378185d4":"markdown","e7cf2a44":"markdown","6e0c2e59":"markdown","a124f244":"markdown","eb2838b4":"markdown","8a42cd66":"markdown","da43bca9":"markdown","0b1aaf99":"markdown","08190dc5":"markdown","b63db37a":"markdown","550e8f1f":"markdown","cceaee9c":"markdown","e0a1d74f":"markdown","2e902900":"markdown","414e1b7a":"markdown","f0b774cc":"markdown","6bfff928":"markdown","e513d982":"markdown","29a67b60":"markdown","812ff02d":"markdown","2b9789c3":"markdown","95e96b2a":"markdown","dcb31f59":"markdown","19e1521c":"markdown","e2d11ea3":"markdown","ce6b3ed2":"markdown","16c17f96":"markdown","23383173":"markdown","87c671a9":"markdown","4b01cb7f":"markdown","27284588":"markdown","e2831455":"markdown"},"source":{"23df4a0c":"# importing the necessary initial libraries.\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","92c6f82d":"# import the data\n\nvechicle_raw_data = pd.read_csv(\"..\/input\/vechile-dataset\/vehicle.csv\")\n\nvechicle_raw_data.shape","6ef32aae":"vechicle_raw_data.head(10)","dc9ae934":"vechicle_raw_data.info()","5542a076":"vechicle_raw_data.describe().T","53029c8d":"dups = vechicle_raw_data.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))","0b855adb":"### Visuvalizing Boxplot of each columns\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('compactness')\nsns.boxplot(vechicle_raw_data['compactness'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('circularity')\nsns.boxplot(vechicle_raw_data['circularity'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('distance_circularity')\nsns.boxplot(vechicle_raw_data['distance_circularity'],orient='horizondal',color='green')\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('radius_ratio')\nsns.boxplot(vechicle_raw_data['radius_ratio'],orient='horizondal',color='purple')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('pr.axis_aspect_ratio')\nsns.boxplot(vechicle_raw_data['pr.axis_aspect_ratio'],orient='horizondal',color='black')\n  \n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('max.length_aspect_ratio')\nsns.boxplot(vechicle_raw_data['max.length_aspect_ratio'],orient='horizondal',color='yellow')","006b577e":"plt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('scatter_ratio')\nsns.boxplot(vechicle_raw_data['scatter_ratio'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('elongatedness')\nsns.boxplot(vechicle_raw_data['elongatedness'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('pr.axis_rectangularity')\nsns.boxplot(vechicle_raw_data['pr.axis_rectangularity'],orient='horizondal',color='green')\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('max.length_rectangularity')\nsns.boxplot(vechicle_raw_data['max.length_rectangularity'],orient='horizondal',color='purple')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('scaled_variance')\nsns.boxplot(vechicle_raw_data['scaled_variance'],orient='horizondal',color='yellow')\n  \n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('scaled_variance.1')\nsns.boxplot(vechicle_raw_data['scaled_variance.1'],orient='horizondal',color='black')","27844ea6":"plt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('scaled_radius_of_gyration')\nsns.boxplot(vechicle_raw_data['scaled_radius_of_gyration'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('scaled_radius_of_gyration.1')\nsns.boxplot(vechicle_raw_data['scaled_radius_of_gyration.1'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('skewness_about')\nsns.boxplot(vechicle_raw_data['skewness_about'],orient='horizondal',color='green')\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('skewness_about.1')\nsns.boxplot(vechicle_raw_data['skewness_about.1'],orient='horizondal',color='purple')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('skewness_about.2')\nsns.boxplot(vechicle_raw_data['skewness_about.2'],orient='horizondal',color='yellow')\n  \n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('hollows_ratio')\nsns.boxplot(vechicle_raw_data['hollows_ratio'],orient='horizondal',color='black')","4d8bac61":"# Plot the central tendency of the dataset\n_, bp = vechicle_raw_data.boxplot(return_type='both', figsize=(20,10), rot='vertical')\n\nfliers = [flier.get_ydata() for flier in bp[\"fliers\"]]\nboxes = [box.get_ydata() for box in bp[\"boxes\"]]\ncaps = [cap.get_ydata() for cap in bp['caps']]\nwhiskers = [whiskers.get_ydata() for whiskers in bp[\"whiskers\"]]","38e49ff8":"# we define outliers by using Inter Quantile range. \n# Data_point > (Q3 * 1.5) is said to be outlier where Q3 is 75% Quantile !\n\n# finding the IQR for each of the numerical columns\ndef check_outliers(data):\n    vData_num = data.loc[:,data.columns != 'class']\n    Q1 = vData_num.quantile(0.25)\n    Q3 = vData_num.quantile(0.75)\n    IQR = Q3 - Q1\n    count = 0\n    # checking for outliers, True represents outlier\n    vData_num_mod = ((vData_num < (Q1 - 1.5 * IQR)) |(vData_num > (Q3 + 1.5 * IQR)))\n    #iterating over columns to check for no.of outliers in each of the numerical attributes.\n    for col in vData_num_mod:\n        if(1 in vData_num_mod[col].value_counts().index):\n            print(\"No. of outliers in %s: %d\" %( col, vData_num_mod[col].value_counts().iloc[1]))\n            count += 1\n    print(\"\\n\\nNo of attributes with outliers are :\", count)\n    \ncheck_outliers(vechicle_raw_data)","79e3504c":"# creating a datacopy for cleaning the records \n\nvechicle_clean_data = vechicle_raw_data.copy()","f01423c5":"# to replace with median we will loop through each column in the dataframe\n\nfor col in vechicle_clean_data.columns[:-1]:\n    Q1 = vechicle_clean_data[col].quantile(0.25)\n    Q3 = vechicle_clean_data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_value = Q1 - (1.5 * IQR)\n    upper_value = Q3 + (1.5 * IQR)\n    \n    vechicle_clean_data.loc[(vechicle_clean_data[col]< lower_value) | ( vechicle_clean_data[col] > upper_value), col] = vechicle_clean_data[col].median()\n\n# check for outliers\ncheck_outliers(vechicle_clean_data)","cf5665e5":"# Check the dataset after Outlier treatment\nsns.set_style('darkgrid')\nplt.figure(figsize=(30, 30))\nindex = 1\nfor col in vechicle_clean_data.columns[:-1]:\n    plt.subplot(1, len(vechicle_clean_data.columns[:-1]), index)\n    sns.boxplot(y=vechicle_clean_data[col], palette='inferno', fliersize=12)\n    index += 1\nplt.tight_layout()","414104e4":"print(\"Missing values if any (True\/False)? :\", vechicle_raw_data.isnull().values.any())\nvechicle_raw_data.isna().apply(pd.value_counts).T","2c2105dd":"#lets visualize missing values in heatmap\n\nsns.heatmap(vechicle_raw_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","238534ff":"vechicle_raw_data[vechicle_raw_data.isnull().any(axis=1)]","f9c6e5c4":"# creating a meanfiller to replace all column missing values with mean\nmeanFiller = lambda x: x.fillna(x.mean())\n\nvData_num = vechicle_clean_data.loc[:,vechicle_clean_data.columns != 'class']\nvData_cat = vechicle_clean_data.loc[:,vechicle_clean_data.columns == 'class']\nvData_num = vData_num.apply(meanFiller,axis=0)\n\nvechicle_clean_data = pd.concat([vData_num, vData_cat], axis = 1)\nvechicle_clean_data.info()","7cd0b490":"#lets visualize missing values in heatmap\n\nsns.heatmap(vechicle_clean_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","779bc5a2":"print(\"Types of class:\", vechicle_clean_data['class'].unique())\n\nprint(\"\\nValue Counts:\\n\",vechicle_clean_data['class'].value_counts())\n\nsns.countplot(vechicle_clean_data['class'])","d09b96f1":"# we can vsualise target class and see that car class is dominating data set by 50%,shows imabalance\nlabels = ['car','bus','van']\nsize = vechicle_clean_data['class'].value_counts()\ncolors = ['blue', 'orange','green']\nexplode = [0.1, 0.1,0.1]\n\nplt.rcParams['figure.figsize'] = (9, 9)\nplt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')\nplt.title('Class distribution', fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","f8c4dc23":"vechicle_clean_data.groupby([\"class\"]).count() #lets group the classes and we can see car class dominates half of data set","1a6a6fe6":"# we will start with understanding the distrubtion of each attributes using \"hist\".\nplt.style.use('seaborn-whitegrid')\n\nvechicle_clean_data.hist(bins=20, figsize=(60,40), color='lightblue', edgecolor = 'red')\nplt.show()","2e6bc95e":"# understanding the distrubtion of each attributes using distplot.\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(25, 25))\nfor i, column in enumerate(vechicle_clean_data.columns[:-1]):\n    sns.distplot(vechicle_clean_data[column],ax=axes[i\/\/3,i%3])","92a385af":"plt.figure(figsize= (15,15))\nvechicle_clean_data.boxplot()\nplt.xticks(rotation = 90)","abf1310d":"skewValue = vechicle_clean_data.skew()\nprint(\"skewValue of dataframe attributes: \", skewValue)","b9d939eb":"sns.boxplot(x=\"class\", y=\"compactness\", palette=\"ch:r=-.5,l=.75\", data=vechicle_clean_data); #box plot of target clasess","68d9d5a7":"# for the purpose of readability let us set visiblity only for lower triangle.\ng = sns.pairplot(vechicle_clean_data, hue='class')\nfor i, j in zip(*np.triu_indices_from(g.axes, 1)):\n    g.axes[i, j].set_visible(False)","df08316d":"# Heatmap\n#Correlation Matrix\ncorr = vechicle_clean_data.corr() # correlation matrix\nlower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\nmask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\nplt.figure(figsize = (15,15))  # setting the figure size\nsns.set_style(style = 'white')  # Setting it to white so that we do not see the grid lines\nsns.heatmap(lower_triangle, annot= True,cmap='viridis', xticklabels = corr.index,yticklabels = corr.columns,linewidths= 1,mask = mask)   # Da Heatmap\nplt.xticks(rotation = 90)   # Aesthetic purposes\nplt.show()","045974f2":"#splitting dependant and independant attributes.\n\nX = vechicle_clean_data.drop(['class'],axis = 1)\ny = vechicle_clean_data[['class']]\n\nprint(\"shape of independant data: \", X.shape)\nprint(\"shape of dependant data: \", y.shape)\n","e1ed0572":"# encoding the class attribute.\ny.replace({'car':0,'bus':1,'van':2},inplace=True)\n","27874d5c":"# prior to scaling \nplt.plot(X)\nplt.show()","80a7e756":"# Scaling the attributes.\n\nfrom scipy.stats import zscore\nXScaled=X.apply(zscore)\nXScaled.head()","a50b1521":"#after scaling\nplt.plot(XScaled)\nplt.show()","ad653c9a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size=0.30, random_state=3)\n\nprint(\"Shape of X train : \",X_train.shape)\nprint(\"Shape of X_test  : \",X_test.shape)\n\nprint(\"Shape of y_train : \",y_train.shape)\nprint(\"Shape of y_test  : \",y_test.shape)\n\n","fe1edc49":"# building an svm model\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nsvm_model_1 = svm.SVC(gamma=0.025, C=3, kernel= 'linear')\nsvm_model_1.fit(X_train , y_train)","1874cf80":"# getting model accuracies.\n\ny_predict_1 = svm_model_1.predict(X_test)\n\ntrain_score_1 = svm_model_1.score(X_train,y_train)\ntest_score_1 = svm_model_1.score(X_test, y_test)\n\nprint(\"SVM_model_1 score for train set:\", train_score_1*100)\nprint(\"SVM_model_1 score for test set:\", test_score_1*100)\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Method':['SVM (kernel: Linear)'], 'accuracy':test_score_1 })\nresultsDf = resultsDf[['Method', 'accuracy']]\n\nconfusion_matrix_1 = confusion_matrix(y_test,y_predict_1)\nprint(\"\\n\\nConfusion Matrix:\\n   car  bus van\\n\",confusion_matrix_1)\ntarget_names = ['car', 'bus', 'van']\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_predict_1, target_names=target_names))\n#metrics.classification_report(y_test, y_predict_1, target_names=target_names)\nresultsDf","7f2f80a5":"# lets try out with a different kernel\n# we will use the same gamma and C value throught. We will consider hyper parameter tuning later.\n\nsvm_model_2 = svm.SVC(gamma=0.025, C=3, kernel= 'rbf')\nsvm_model_2.fit(X_train , y_train)","14a401a6":"y_predict_2 = svm_model_2.predict(X_test)\n\ntrain_score_2 = svm_model_2.score(X_train,y_train)\ntest_score_2 = svm_model_2.score(X_test, y_test)\n\nprint(\"SVM_model_2 score for train set:\", train_score_2*100)\nprint(\"SVM_model_2 score for test set:\", test_score_2*100)\n\ntempResultsDf = pd.DataFrame({'Method':['SVM (kernel: rbf)'], 'accuracy':test_score_2 })\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\n\nconfusion_matrix_2 = confusion_matrix(y_test,y_predict_2)\n\nprint(\"\\nConfusion Matrix:\\n   car bus van\\n\",confusion_matrix_2)\ntarget_names = ['car', 'bus', 'van']\nprint(\"Classification report:\\n\", classification_report(y_test, y_predict_2, target_names=target_names))\n#metrics.classification_report(y_test, y_predict_1, target_names=target_names)\n\n\nresultsDf","de1a1d06":"#Grid search to tune model parameters for SVC\nfrom sklearn.model_selection import GridSearchCV\n\nc_range = range(5,15)\ngamma_range = [0.001,0.025,0.05,0.04,0.03,0.1,0.5,1,10]\nparams = dict(C=c_range, gamma=gamma_range,kernel=['linear', 'rbf'])\nmodel = GridSearchCV(svm.SVC(), param_grid=params, verbose=1)\nmodel.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\", model.best_params_)","f7b84802":"# Build svm model with C=5 and gamma=0.1 using rbf kernel\n\nsvm_model_3 = svm.SVC(gamma=0.1, C=5, kernel= 'rbf')\nsvm_model_3.fit(X_train , y_train)","6b96b1d6":"y_predict_3 = svm_model_3.predict(X_test)\n\ntrain_score_3 = svm_model_3.score(X_train,y_train)\ntest_score_3 = svm_model_3.score(X_test, y_test)\n\nprint(\"SVM_model_3 score for train set:\", train_score_3*100)\nprint(\"SVM_model_3 score for test set:\", test_score_3*100)\n\ntempResultsDf = pd.DataFrame({'Method':['SVM - Tuned'], 'accuracy':test_score_3 })\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\n\nconfusion_matrix_3 = confusion_matrix(y_test,y_predict_3)\n\nprint(\"\\nConfusion Matrix:\\n   car bus van\\n\",confusion_matrix_3)\ntarget_names = ['car', 'bus', 'van']\nprint(\"Classification report:\\n\", classification_report(y_test, y_predict_3, target_names=target_names))\n#metrics.classification_report(y_test, y_predict_1, target_names=target_names)\n\n\nresultsDf","b05476df":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\nnum_folds = 50\nseed = 7\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = svm.SVC(gamma=0.05, C=10, kernel= 'rbf')\nresults = cross_val_score(model, XScaled, y, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n\nCVScores_Df = pd.DataFrame({'Method':['SVM - Tuned'], 'accuracy':results.mean()*100.0, 'std(+\/-)':results.std()*100.0})\nCVScores_Df = CVScores_Df[['Method', 'accuracy', 'std(+\/-)']]\nCVScores_Df","a5e73588":"sns.distplot(results,kde=True,bins=5)\nplt.xlabel(\"Accuracy\")\nplt.show()\n\n\n\n\n\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \n\np = ((1.0-alpha)\/2.0) * 100              \nlower = max(0.0, np.percentile(results, p))  \n\np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(results, p))\n\n\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","fc7c97b9":"# we have deduced in section 2 that some attributes have very low correlation. \n# Hence we will not perform PCA for those attibutes\n\n# So, we drop the columns 'pr.axis_aspect_ratio','max.length_aspect_ratio', 'skewness_about' and 'skewness_about.1'\n# from PCA analysis.\n\nXScaled_cp = XScaled.drop(['pr.axis_aspect_ratio','max.length_aspect_ratio', 'skewness_about', 'skewness_about.1'],axis=1)\nXScaled_cp.shape","df2c8a05":"covMatrix = np.cov(XScaled_cp,rowvar=False)\nprint(covMatrix)","c94001a1":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=len(XScaled_cp.columns), whiten=False)\npca.fit(XScaled_cp)","7d5a4e06":"print('Features Started with:', len(XScaled_cp.columns))\nprint()\n\nprint('Eigen Values: \\n', pca.explained_variance_)\nprint()\n\nprint('Eigen Vector: \\n', pca.components_)\nprint()\n\npercent_variance = np.asarray([float(format(num, '.3f')) for num in pca.explained_variance_ratio_])\npercent_variance = np.round(np.asarray(percent_variance) * 100, decimals =2)\nprint('Percentage variance explained:\\n ', percent_variance)","155af19a":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nplt.bar(list(range(1,15)),percent_variance,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\n\nplt.subplot(1,2,2)\nplt.step(list(range(1,15)),np.cumsum(percent_variance), where='mid')\nplt.axhline(y=95, color='r', linestyle='-')\nplt.ylabel('Cummulative variance explained')\nplt.xlabel('eigen Value')\nplt.show()\n\n","4362031e":"# 5 features explain 95 % of the variance\npca_rd = PCA(n_components=5)\npca_rd.fit(XScaled_cp)","c0bdde3b":"print(pca_rd.components_)\nprint()\n\nprint(pca_rd.explained_variance_ratio_)\nprint()","22e129d1":"#print(XScaled_cp.columns)\nXScaled_cp_pca = pd.DataFrame(pca_rd.transform(XScaled_cp))\n\nprint(\"shape after dimenationality reduction:\", XScaled_cp_pca.shape)\n\nXScaled_cp_pca.head()","e1863bc9":"sns.pairplot(XScaled_cp_pca, diag_kind='kde') ","a2bffe35":"X_non_pca = XScaled[['pr.axis_aspect_ratio','max.length_aspect_ratio', 'skewness_about', 'skewness_about.1']]\n\nXScaled_pca = pd.merge(XScaled_cp_pca, X_non_pca, right_index=True, left_index=True)\n\nprint(\"Final shape of the data: \", XScaled_pca.shape)\n\nXScaled_pca.head()","e44c22d4":"# for data split we will use the same random state as above, to ensure the accuracy can be compared on same rows.\n# so we give the same random state.\n\nprint(\"shape after pca:\", XScaled_pca.shape)\nX_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(XScaled_pca, y, test_size=0.30, random_state=3)\n\nprint(\"Shape of X train : \",X_pca_train.shape)\nprint(\"Shape of X_test  : \",X_pca_test.shape)\nprint(\"Shape of y train : \",y_pca_train.shape)\nprint(\"Shape of y_test  : \",y_pca_test.shape)","afa6cbb3":"svm_model_4 = svm.SVC(gamma=0.025, C=3, kernel= 'rbf')\nsvm_model_4.fit(X_pca_train , y_pca_train)\n\ny_predict_4 = svm_model_4.predict(X_pca_test)\n\ntrain_score_4 = svm_model_4.score(X_pca_train,y_pca_train)\ntest_score_4 = svm_model_4.score(X_pca_test, y_pca_test)\n\nprint(\"SVM_model_4 score for train set:\", train_score_4*100)\nprint(\"SVM_model_4 score for test set:\", test_score_4*100)\n\n#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['SVM with PCA (kernel: rbf)'], 'accuracy':test_score_4 })\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\n\nconfusion_matrix_4 = confusion_matrix(y_pca_test,y_predict_4)\n\nprint(\"\\n\\nConfusion Matrix:\\n   car bus van\\n\",confusion_matrix_4)\ntarget_names = ['car', 'bus', 'van']\nprint(\"\\nClassification Report:\\n\", classification_report(y_pca_test, y_predict_4, target_names=target_names))\n\nresultsDf","9620b341":"# with tuned parameters\nsvm_model_5 = svm.SVC(gamma=0.05, C=10, kernel= 'rbf')\nsvm_model_5.fit(X_pca_train , y_pca_train)","b6bd45ac":"y_predict_5 = svm_model_5.predict(X_pca_test)\n\ntrain_score_5 = svm_model_5.score(X_pca_train,y_pca_train)\ntest_score_5 = svm_model_5.score(X_pca_test, y_pca_test)\n\nprint(\"SVM_model_5 score for train set:\", train_score_5*100)\nprint(\"SVM_model_5 score for test set:\", test_score_5*100)\n\n#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['SVM with PCA -Tuned'], 'accuracy':test_score_5 })\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\n\nconfusion_matrix_5 = confusion_matrix(y_pca_test,y_predict_5)\n\nprint(\"\\n\\nConfusion Matrix:\\n   car bus van\\n\",confusion_matrix_5)\ntarget_names = ['car', 'bus', 'van']\nprint(\"\\nClassification Report:\\n\", classification_report(y_pca_test, y_predict_5, target_names=target_names))\n\nresultsDf","254155b9":"sns.set_style('whitegrid')\nplt.figure(figsize = (12,10))\nplt.yticks(np.arange(0,100,10))\nsns.barplot(x = list(resultsDf.Method), y = list(resultsDf.accuracy))","0358cbcd":"num_folds = 50\nseed = 7\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = svm.SVC(gamma=0.05, C=10, kernel= 'rbf')\nresults = cross_val_score(model, XScaled_pca, y, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n\ntempCVScores_Df = pd.DataFrame({'Method':['SVM with PCA -tuned'], 'accuracy':results.mean()*100.0, 'std(+\/-)':results.std()*100.0})\nCVScores_Df = pd.concat([CVScores_Df, tempCVScores_Df])\nCVScores_Df = CVScores_Df[['Method', 'accuracy', 'std(+\/-)']]\nCVScores_Df","2cd0d3a4":"sns.distplot(results,kde=True,bins=5)\nplt.xlabel(\"Accuracy\")\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \n\np = ((1.0-alpha)\/2.0) * 100              \nlower = max(0.0, np.percentile(results, p))  \n\np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(results, p))\n\n\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","94ad0a26":"# accuracy scores: \n\nresultsDf","fadef42f":"plt.figure(figsize = (16,4))\nplt.suptitle(\"Confusion Matrices\",fontsize=12)\nplt.subplots_adjust(wspace = 0.8, hspace = 0.8)\n\nplt.subplot(1,3,1)\nplt.title(\"SVM (kernel: Linear) Confusion Matrix\")\nsns.heatmap(confusion_matrix_1, annot = True, cmap = \"Blues\", fmt = 'd', cbar = False, annot_kws = {\"size\": 12})\n\n\nplt.subplot(1,3,2)\nplt.title(\"SVM (kernel: rbf) Confusion Matrix\")\nsns.heatmap(confusion_matrix_2, annot = True, cmap = \"Blues\", fmt = 'd', cbar = False, annot_kws = {\"size\": 12})\n\nplt.subplot(1,3,3)\nplt.title(\"SVM - Tuned Confusion Matrix\")\nsns.heatmap(confusion_matrix_3, annot = True, cmap = \"Blues\", fmt = 'd', cbar = False, annot_kws = {\"size\": 12})\n","cdf114f3":"plt.figure(figsize = (16,4))\nplt.suptitle(\"Confusion Matrices\",fontsize=12)\nplt.subplots_adjust(wspace = 0.8, hspace = 0.8)\n\n\nplt.subplot(1,2,1)\nplt.title(\"SVM with PCA (kernel: rbf) Confusion Matrix\")\nsns.heatmap(confusion_matrix_4, annot = True, cmap = \"Blues\", fmt = 'd', cbar = False, annot_kws = {\"size\": 12})\n\nplt.subplot(1,2,2)\nplt.title(\"SVM with PCA -Tuned Confusion Matrix\")\nsns.heatmap(confusion_matrix_5, annot = True, cmap = \"Blues\", fmt = 'd', cbar = False, annot_kws = {\"size\": 12})\n","b371c137":"# comparing cross validation scores of raw data and with PCA\n\nCVScores_Df","6aba9a3b":"## Confusion matrix\n\nplt.figure(figsize=(10,5))\nclass_label = [\"car\", \"bus\", \"van\"]\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"True Class\")\n\n\nplt.subplot(1,2,1)\nsvm_cm = confusion_matrix(y_test, y_predict_3)\ndf_cm = pd.DataFrame(svm_cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt='d', cbar= False, cmap=\"Blues\")\nplt.title(\"Confusion Matrix -- SVM Tuned (Raw data)\")\n\nplt.subplot(1,2,2)\npca_cm = confusion_matrix(y_pca_test,y_predict_5)\ndf_cm1 = pd.DataFrame(pca_cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm1, annot = True,fmt = \"d\",cbar= False, cmap=\"Blues\")\nplt.title(\"Confusion Matrix -- SVM Tuned (PCA)\")\n\nplt.show()","9f763b87":"target_names = ['car', 'bus', 'van']\nprint(\"\\nClassification Report for SVM -Tuned (RAW data):\\n\", classification_report(y_test, y_predict_3, target_names=target_names))\nprint()\nprint()\nprint(\"\\nClassification Report for SVM -Tuned (PCA):\\n\", classification_report(y_pca_test, y_predict_5, target_names=target_names))","82be870c":"#Finding optimal no. of clusters\nfrom scipy.spatial.distance import cdist\n\nfrom sklearn.cluster import KMeans\n\nclusters = range(1,10)\nmeanDistortions = []\n\n# creating a datacopy for PCA with groups (XScaled_pca)\n\nXScaled_gr_pca = XScaled_pca.copy()\n\nfor k in clusters:\n    km_model= KMeans(n_clusters = k)\n    km_model.fit(XScaled_gr_pca)\n    km_prediction = km_model.predict(XScaled_gr_pca)\n    meanDistortions.append(sum(np.min(cdist(XScaled_gr_pca, km_model.cluster_centers_, 'euclidean'), axis=1)) \/ XScaled_gr_pca.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","a2c36b66":"# Model fitting\nkmeansmodel=KMeans(3)\nkmeansmodel.fit(XScaled_gr_pca)\nkm_prediction=kmeansmodel.predict(XScaled_gr_pca)\n\n#Append the prediction in the group\nXScaled_gr_pca[\"GROUP\"] = km_prediction\nXScaled_gr_pca[\"GROUP\"] = km_prediction\nprint(\"Groups Assigned : \\n\")\nXScaled_gr_pca.head()","ab2d9fca":"XScaled_gr_pca.groupby(['GROUP'])\nXScaled_gr_pca.boxplot(by='GROUP', layout = (3,3),figsize=(15,10))","ac807139":"from sklearn import metrics\n\nscore_km = metrics.silhouette_score(XScaled_gr_pca, kmeansmodel.labels_, metric='euclidean')\nprint(\"The silhouette scores is \"+str(score_km*100))","bd532da3":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\nprint (' Logistic Regression - Before PCA score', model.score(X_test, y_test))\n\nmodel.fit(X_pca_train, y_pca_train)\nprint (' Logistic Regression - After PCA score', model.score(X_pca_test, y_pca_test))\n\n","2f248bab":"num_folds = 10\nseed = 1\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel_scaled = LogisticRegression(solver='lbfgs', multi_class='auto')\nresults = cross_val_score(model_scaled, XScaled, y, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))","6c47674a":"kfold = KFold(n_splits=num_folds, random_state=seed)\nmodel_pca = LogisticRegression(solver='lbfgs', multi_class='auto')\nresults = cross_val_score(model_pca, XScaled_pca, y, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))","665ab09a":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(X_train, y_train)\nprint (' Naive Bayes - Before PCA score', nb.score(X_test, y_test))\n\nnb.fit(X_pca_train, y_pca_train)\nprint (' Naive Bayes - After PCA score', nb.score(X_pca_test, y_pca_test))","4acd18d2":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(criterion = 'entropy' )\n\ndt_model.fit(X_train, y_train)\nprint (' Decisiontree Classifier - Before PCA score', dt_model.score(X_test, y_test))\n\ndt_model.fit(X_pca_train, y_pca_train)\nprint (' Decisiontree Classifier - After PCA score', dt_model.score(X_pca_test, y_pca_test))","6031e078":"### Lets try to tune the parameters and check for the best gamma and C value.","df5e74cf":"# It Seems that Support Vectore Classifier is a better model to classifiy the given silhoutte info as van, bus, car .","3ae17317":"# Observation:\n\nCompactness mostly ranges from 88 to 100\n\nCircularity mostly ranges from 40 to 50\n\nDistance circularity mostly ranges from 70 to 100\n\nRadius_ratio mostly ranges from 140 to 200 with few outliers\n\npr.axis_aspect_ratio mostly ranges from 55 to 65 with some outliers\n\nmax.length_aspect_ration ranges from 8 to 11 with more outliers","a88ebe31":"# Decisiontree Classifier:","49bfb41e":"# Quick Observation :\n\n\n- Most of the data attributes seems to be normally distributed\n\n- scaled valriance 1 and skewness about 1 and 2, scatter_ratio, seems to be right skwed\n\n- pr.axis_rectangularity seems to be haing outliers as there are some gaps found in the bar plot.","8d8e64f9":"# Uni-variate analysis","5320d566":"\n# Notice: We see a slight increase in model accuracy with parameter tuning.\n\n# Observations:\n\n    For class type 'bus', the model has predicted 70\/71 correctly.\n    For class type 'van', the model has predicted 57\/60 correctly.\n    For class type 'car', the model has predicted 119\/123 correctly.\n\n","ece3789f":"# Let us check The Pairplot Of Reduced Dimension After PCA:","62a85b2d":"# As we see, the average model accuracy turns out to be 97.75%. And an accuracy between 90 to 100 can be acheived with 95% confidence.","27085048":"# c. Duplicates","7f418300":"# Going with elbow point 3 that is k=3","ed884034":"# Observation:\n\n- car class median is more than other class type -bus, van","fa80df1a":"# Logistic Regression","8de26d65":"# a. Target attribute analysis.","b659703d":"# 1. Data pre-processing \u2013 Perform all the necessary preprocessing on the data ready to be fed to an Unsupervised algorithm\n\n\n# a. Import Libraries","5cbdcd0d":"# 4. Train a Support vector machine using the train set and get the accuracy on the test set","6a5b9c65":"# Data Description:\nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.\n\n# Domain:\nObject recognition\n\n# Context:\nThe purpose is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles\n\n# Attribute Information:\n\u25cf All the features are geometric features extracted from the silhouette. \u25cf All are numeric in nature.\n\n# Learning Outcomes:\n\u25cf Exploratory Data Analysis \u25cf Reduce number dimensions in the dataset with minimal information loss \u25cf Train a model using Principle Components\n\n# Objective:\nApply dimensionality reduction technique \u2013 PCA and train a model using principle components instead of training the model using just the raw data.\n","e8de8695":"##### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","6a2f6899":"# Observations:\n\n   # The SVM with PCA predicted correclty for all class = 'bus' as evident from recall = 1.00\n   # For class = 'car', the incorrect prediction have increased by 2. The precision and recall reduced by 2% when tuned with PCA\n   # For class = 'van', the no.of incorrect predictions have increased most among the three class types. The precision and recall fell down by 4% & 7% respectively. However note: PCA has reduced dimentionality by 50%.\n    \n    \n   # Since, accuracy has not dropped substancially, we can consider SVM tuned using PCA to be the best model for this case.\n","ca6e4840":"#### Notice, now all attributes have 846 data points. We have replaced missing values with the mean value of each attribute.","fe6a4b5e":"# Apply K fold cross validation on original scaled data (Logistic Regression)","8c7867fe":"### 4.c SVM - tune via GridSearchCV","73990cfe":"# Observations:\n\n    - Most of the attributes seems to be of bimodal\/multimodal distributions (i.e.) we see more than one peaks.\n    \n    - skewness_about.2 and pr.axis_aspect_ratio seems to be normally distrubuted.\n    \n    - Many attributes seems to have long right tails. (ex: scaled_radius_of_gyration.1, scatter_ratio ..)\n    \n    - As seen from boxplots, the scales of the attributes tend to vary, so it would be better to normalize the data.\n    \n    - No presence of outliers as they are already dealt with in previous sections.\n\n","4a774202":"# Bi- Variate Analysis:","d16d561d":"# We get the best paramters as : C = 5, and gamma=0.1 for rbf. Hence let us re-construct the model with these parameters.","96bf5e5d":"# Quick Insights : From Correlation Hetamap:\n\nStrong\/fare Correlation:\n\n      - Scaled Variance & Scaled Variance.1 seems to be strongly correlated with value of 0.98\n      - skewness_about_2 and hollow_ratio seems to be strongly correlated, corr coeff: 0.89\n      - ditance_circularity and radius_ratio seems to have high positive correlation with corr coeff: 0.81\n      - compactness & circularity , radius_ratio & pr.axis_aspect_ratio also seems ver averagely correlated with coeff: 0.67.\n      - scaled _variance and scaled_radius_of_gyration, circularity & distance_circularity also seems to be highly correlated with corr coeff: 0.79\n      - pr.axis_recatngularity and max.length_recatngularity also seems to be strongly correlated with coeff: 0.81 \n      - scatter_ratio and elongatedness seems to be have strong negative correlation val : 0.97\n      - elongatedness and pr.axis_rectangularity seems to have strong negative correlation, val:  0.95\n      \nLittle To No Correlation:\n      \n      -max_length_aspect_ratio & radius_ratio have average correlation with coeff: 0.5\n      - pr.axis_aspect_ratio & max_length_aspect_ratio seems to have very little correlation\n      - scaled_radius_gyration & scaled_radisu_gyration.1 seems to be very little correlated\n      - scaled_radius_gyration.1 & skewness_about seems to be very little correlated\n      - skewness_about & skewness_about.1 not be correlated\n      - skewness_about.1 and skewness_about.2 are not correlated.\n    \n    \n  \n    \n# Observations: Take Away\n    \n    \n    - 4 attributes 'pr.axis_aspect_ratio','max.length_aspect_ratio', 'skewness_about' and 'skewness_about.1' have very low correlation with other attributes. Hence these attributes can be directly considered for modelling.\n    \n    - We see many attributes are higly correclated with more than 0.9. Ex: 'scatter_ratio' and 'scaled variance' have a correlation of 0.98. Hence in such cases we can drop one of the attribute.\n    \n    - From above correlation matrix we can see that there are many features which are highly correlated. if we carefully analyse, we will find that many features are there which having more than 0.9 correlation. so we can decide to get rid of those columns whose correlation is +-0.9 or above.There are 8 such columns:\n\n              max.length_rectangularity\n              scaled_radius_of_gyration\n              skewness_about.2\n              scatter_ratio\n              elongatedness\n              pr.axis_rectangularity\n              scaled_variance\n              scaled_variance.1\n\n    Also, see many attributes have corr of greater than 0.7. Hence it would be difficult for us to deciede which ones to drop. Hence we go for PCA to determine the attributes to retain.\n\n    \n   # Attributes to consider for PCA = 14 (4 attributes left out based on point 1)   \n    \n","2dbba826":"# Observation:\n\nSkewness_about_1 mostly ranges from 145 to 200\n\nScaled_radius_of_gyration_1 mostly ranges from 65 to 75 with some outliers\n\nSkewness_about mostly ranges from 2 to 10 with some outliers\n\nSkewness_about_1 mostly ranges from 5 to 20 with some outliers\n\nSkewness_about_2 mostly ranges from 182 to 192\n\nHollows ratio mostly ranges from 190 to 205","5c011920":"# The data is now ready for model training, with no missing values or outliers. We will also change(to numeric values) the target variable after our analysis in the point 2.","7c9c6239":"No Duplicates found in the given dataset","124e72a8":"# 6. Use PCA from Scikit learn, extract Principal Components that capture about 95% of the variance in the data","d3124c8f":"# Now let us combine the 4 features left out of the PCA with the reduced features and prepare the data for modelling.","f89d4b79":"## Now we see after the outlier treatment (with median) there is no outlier present in our data","d113031f":"# e. Dealing with missing values","ae2ea6d3":"# As we see from the above graph, 95% of the variation in data can be explained using 5 features.","fe052dba":"\n# Observations:\n\n   # - The accuracy of individual models SVM- Tuned(96.85%) falls well within range of the cross validation score of 97.75% with a standard deviation of (+\/- 3.32).\n\n   # - Similarliy, for Model with PCA (94.88%) is appox equal to cross validation score of 94.455% with standard deviation of (+\/- 5.6).\n\n   # - Hence we can conlcude both our models on raw data and with PCA perform well without any overfitting\/underfitting.\n\n","e0514548":"### cross validation score for model after reducing dimentionality using PCA and tuning parameters is 94.33%","a1178ad0":"# After PCA, we reduced the data set to contain 9 features.","c5ed7c5e":"# b. Import and Review the data","31dc36f1":"# Re- apply K fold on PCA components","436b07d4":"# Quick Insights:\nWe can see that :\n\n- circularity, class, hollow_ratio,max.length_rectangularity, , max.length_aspect_ratio, compactness has no missing values rest all features are having some kind of missing values \n- All attributes are of numerical type execpt the target 'class'","1d172ee3":"## As noticed above, since most of the attributes are symmetrically skewed and the no.of missing values are very low, we will replace them with mean.","43912107":"### -------------------------------------------------------------------------------------------------------------------------------------------------------------","f380b0ac":"### ------------------------------------------------------------------------------------------------------------------------------------------------------------","3c0194ed":"# Strategy for Outlier - I am not dropping them, as other columns has significant information on those records, so the strategy for treating the outliers is applying median for those values.","d676a129":"# C. Split data into train and test sets (Note: use random state)","4af98f65":"# Observation:\n\nScatter ratio mostly ranges from 145 to 200\n\nElongatedness mostly ranges from 33 to 47\n\nPr.axis_rectangularity mostly ranges from 19 to 23\n\nMax.length_rectangularity mostly ranges from 138 to 160\n\nScaled_variance mostly ranges from 160 to 220 with very few outliers\n\nScaled_variance mostly ranges from 300 to 600","d53bf29a":"### ------------------------------------------------------------------------------------------------------------------------------------------------------------","a1e8024f":"# The end (optional)\n\n##### this is recap of the summary, just with overall steps)","fff74a90":"# a. Seperating dependant\/independant attributes","c4e87e25":"# Strategy for missing values - replace with mean","3a6c9bdb":"# Let's Look How Some Other Classifier Models Perform Both - On Original Data & PCA treated data sets","d3182fd9":"We start with the base model we created in Step-3","378185d4":"# 3. Split the data into train and test (Suggestion: specify \u201crandom state\u201d if you are using train_test_split from Sklearn)","e7cf2a44":"# 2. Understanding the attributes - Find relationship between different attributes (Independent variables) and choose carefully which all attributes have to be a part of the analysis and whys","6e0c2e59":"# Now lets perform K-fold cross validation and check for 95% confidence intervals","a124f244":"## We are dropping this four attribute before doing the PCA, as these are confirmed feature for our model and after doing the PCA , we will add these four features to the model.\n\n- pr.axis_aspect_ratio\n- max.length_aspect_ratio\n- skewness_about\n- skewness_about.1\n\n","eb2838b4":"# d. Dealing with outliers. ( doing it before missing value, so will know the treatment\/strategy to be given for missing - median or mean )","8a42cd66":"# Observation: \n\n- All the independent attributes are right skewed expect the hollows_ratio attribute (left skewed)","da43bca9":"# Naive Bayes","0b1aaf99":"\n# Observations:\n\n   \n   # - SVM on raw data perform better for Kernel = rbf with an slight advantage of appox ~2% in accuracy.\n   # - When tuned for hyper paramters, gamma=0.1, C=5, kernel= 'rbf' acheived an accuracy of 96.85%, a slight increase on model without tuning.\n   # - Accuracy scores with PCA seems to have dropped slightly by ~2%. But it is to be noted that this has been achived with 50% reduction in dimentions.(PCA has reduced the dimentionality from 18 to 9.)\n\n","08190dc5":"## It is clealry visible from the pairplot above that:\n    \nAfter dimensionality reduction using PCA our attributes have become independent with no correlation among themselves. As most of them have cloud of data points with no lienaer kind of relationship.","b63db37a":"# Observation:\n\nWe notice there a 8 attributes which have outliers.\n\nAlso,the no.of outliers are very low(< 20) w.r.t the total datapoints (846).\n\nHence, we will replace them with the median of the respective attributes.","550e8f1f":"# Steps:","cceaee9c":"# b. Independant attribute analysis","e0a1d74f":"# 5. Perform K-fold cross validation and get the cross validation score of the model (optional)","2e902900":"### 4.b SVM (gamma=0.025, C=3, kernel= 'rbf')","414e1b7a":"\n# Observation:\n\n      The data is not distrubuted equally across the 3 types of classes. Appox Half of the data points are of type \"car\" and remaining from the types \"bus\" and \"van\" in appox. equal proportions.\n    \n      Meaning the model might tend to bias towards class \"car\". We can use down sampling to reduce this bias.\n\n","f0b774cc":"# Additional Part (Classification using un-supervised learning) using the final PCA data","6bfff928":"##### ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","e513d982":"### ----------------------------------------------------------------------------------------------------------------------------------------------------------","29a67b60":"# Model Summary:\n\n# Model                         : SVM tuned using PCA\n# Parameters                : gamma=0.05, C=10, kernel= 'rbf'\n# No.of Features used : 9\n# Model Accuracy        : 94.88%","812ff02d":"# 8. Compare the accuracy scores and cross validation scores of Support vector machines \u2013 one trained using raw data and the other using Principal Components, and mention your findings","2b9789c3":"# b. Scale the independant attributes","95e96b2a":"#### we can see few missing values in several columns","dcb31f59":"Now, lets create a model with tuned parameters.","19e1521c":"# Reducing the dimentionality.","e2d11ea3":"### 4.a SVM (gamma=0.025, C=3, kernel= 'linear')","ce6b3ed2":"# Quick Insights:\n\n    - The data has 19 columns. 18 independant attributes, and a target attribute \"class\"\n    - All independant attributes are numeric.\n    - There are 846 data points in total.\n    - Notice many attributes, have less than 846 datapoints, meaning the data has missing values.\n    - Notice, for most of the attributes mean appox equals the 50 percentile, indicating symmetrical distrubution.\n    - Only radius_ratio, pr.axis_aspect_ratio, max.length_aspect_ratio, scatter_ratio, scaled_variance, scaled_variance.1, skewness_about, skewness_about.1 appear to be skewed to the right.\n    \n\n","16c17f96":"### -----------------------------------------------------------------------------------------------------------------------------------------------------------","23383173":"\n# Recap of the Summary\n\n   #### Data Preprocessing\n\n            As part of data preprocessing we took a glance over number of independent features and number of overall records(rows) available as part of the dataset\n            we then checked for outliers using boxplot, we found out that some of the features had outliers and we used 5 number summary to identify the data set that needs to be considered as outliers and then replaced them with median from the original data set ( we can also replace these data sets with median value of the feature)\n            we found out that some of the independent features had missing values, so in order to fix these we replaced all of the missing values with mean of that corresponding feature\n            we also checked the statistical information of features by using describe() method and noticed that only few features like scatter_ratio, scaled_variance were skewed in nature and rest features looked as normally distributed\n           \n\n   ##### UnderStanding the attributes\/features\n\n            we draw histogram plot of each features to understand the distributions of data, found out that distribution was mixed in nature, few feature sets were normally distributed, some were binomial and random distribution in nature (having two and more kde density curve)\n            then we used pair plot to understand the relationship between independent variables, found out many features were correlated either positively or negatively\n            To cross check we used heatmap to see the correlation\n            We identify 4 attributes need to be in the model, so those there dropped for PCA analysis\n\n   #####  Principal component analysis\n\n            Ideally features which have correlation are considered in PCA, however in this usecase we have used set of indepenent features by dropping the must have four attributes and those will be added later once we do the dimension reducation. \n            Since PCA helps to increase signal to noise ratio ( improves the information content by capturing the info that is availalbe in mathematical space but not used) and also used to reduce the dimension, we considered this for our analysis\n            to start with we standardized the complete set of independent features using zscore\n            Then generated covariance\/correlation matrix , and also generated Principal compoments (eigen vectors) & eigen values\n            we then plot cumulative explained variance of Principal components to understand how each PCs contribute to reduce the variance\n            from the graph and explained variance values we found out that 5 principal components together contribute 95% in reducing the variance\n            we then used 5 Principal component to transform the dataset and stored it in new dataframe annd also added the first four attributes, so total of 9 attributes.\n            we used pairplot to check relationship of these principal components and noticed that there was not relation ( so PCA has done good work)\n\n   ##### Model building\n\n            To build the model we choose Support Vector Classifier algorithm\n                \n                    Method\t             accuracy\n        -\tSVM (kernel: Linear)\t     0.944882\n        -\tSVM (kernel: rbf)\t         0.960630\n        -\tSVM - Tuned\t                 0.968504\n        -\tSVM with PCA (kernel: rbf)\t 0.940945\n        -\tSVM with PCA -Tuned\t         0.944882\n            \n          \n          K-fold cross validation and get the cross validation score\n           \n                  Method\t            accuracy\t std(+\/-)\n        -\tSVM - Tuned\t                97.750000\t3.321577\n        -\tSVM with PCA -tuned\t        94.455882\t5.693589\n\n                \n\n   ##### Final statement\n\n        Principal components ( PCA ) in general will give best results when used in model than the original data set, as PCA makes sure to increase the information content by fetching the information from mathematical space that is not used or fed to the model As PCA works best on features having positive\/negative correlation , Principal components may not yeild good results if these are not taken care in PCA ( choose only feature having relationship between them to generate PCs) In our usecase we took all the features for PCA , and noticed that Naive Bayes algorithm preformed well on Principal components\n        \n        \nThe SVM with PCA predicted correclty for all class = 'bus' as evident from recall = 1.00\nFor class = 'car', the incorrect prediction have increased by 2. The precision and recall reduced by 2% when tuned with PCA\nFor class = 'van', the no.of incorrect predictions have increased most among the three class types. The precision and recall fell down by 4% & 7% respectively. However note: PCA has reduced dimentionality by 50%.\n\nSince, accuracy has not dropped substancially, we can consider SVM tuned using PCA to be the best model for this case.\n\n\nModel Summary:\nModel : SVM tuned using PCA\nParameters : gamma=0.05, C=10, kernel= 'rbf'\nNo.of Features used : 9\nModel Accuracy : 94.88%\n\n","87c671a9":"# Pearson Correlation Coefficient:","4b01cb7f":"### -----------------------------------------------------------------------------------------------------------------------------------------------","27284588":"### -----------------------------------------------------------------------------------------------------------------------------------------------------------","e2831455":"# 7. Repeat steps 3,4 and 5 but this time, use Principal Components instead of the original data. And the accuracy score should be on the same rows of test data that were used earlier."}}