{"cell_type":{"24648e23":"code","d8a7faa2":"code","f30c346d":"code","1e205b2d":"code","6cdc095a":"code","73857651":"code","b3eb16a6":"code","66a9265e":"code","6a3c60d4":"code","5ef0d76e":"code","78b822ef":"code","c567fbd7":"code","b1e01d54":"code","81ba5abb":"code","7ba4347f":"code","cd30abf0":"code","30467bdd":"code","a2b2f600":"code","b60ce7d2":"code","2d193f6e":"code","05a6cc77":"code","78dfdbd7":"code","cc4fbd38":"markdown","31dda549":"markdown","8262a51c":"markdown","a24f578d":"markdown","4eb2831f":"markdown","25d322db":"markdown"},"source":{"24648e23":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom nltk.stem import PorterStemmer\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","d8a7faa2":"true_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","f30c346d":"true_news","1e205b2d":"fake_news","6cdc095a":"true_df = pd.concat([true_news['title'], pd.Series(0, index=true_news.index, name='label')], axis=1)\nfake_df = pd.concat([fake_news['title'], pd.Series(1, index=fake_news.index, name='label')], axis=1)","73857651":"true_df","b3eb16a6":"fake_df","66a9265e":"news_df = pd.concat([true_df, fake_df], axis=0).sample(frac=1.0, random_state=34).reset_index(drop=True)","6a3c60d4":"news_df","5ef0d76e":"ps = PorterStemmer()\n\ndef process_title(title):\n    new_title = title.lower()\n    new_title = re.sub(r'\\$[^\\s]+', 'dollar', new_title)\n    new_title = re.sub(r'[^a-z0-9\\s]', '', new_title)\n    new_title = re.sub(r'[0-9]+', 'number', new_title)\n    new_title = new_title.split(\" \")\n    new_title = list(map(lambda x: ps.stem(x), new_title))\n    new_title = list(map(lambda x: x.strip(), new_title))\n    if '' in new_title:\n        new_title.remove('')\n    return new_title","78b822ef":"titles = news_df['title'].apply(process_title)\n\nlabels = np.array(news_df['label'])","c567fbd7":"titles","b1e01d54":"# Get size of vocabulary\nvocabulary = set()\n\nfor title in titles:\n    for word in title:\n        if word not in vocabulary:\n            vocabulary.add(word)\n\nvocab_length = len(vocabulary)\n\n# Get max length of a sequence\nmax_seq_length = 0\n\nfor title in titles:\n    if len(title) > max_seq_length:\n        max_seq_length = len(title)\n\n# Print results\nprint(\"Vocab length:\", vocab_length)\nprint(\"Max sequence length:\", max_seq_length)","81ba5abb":"tokenizer = Tokenizer(num_words=vocab_length)\ntokenizer.fit_on_texts(titles)\n\nsequences = tokenizer.texts_to_sequences(titles)\n\nword_index = tokenizer.word_index\n\nmodel_inputs = pad_sequences(sequences, maxlen=max_seq_length)","7ba4347f":"model_inputs","cd30abf0":"model_inputs.shape","30467bdd":"X_train, X_test, y_train, y_test = train_test_split(model_inputs, labels)","a2b2f600":"embedding_dim = 64\n\n\ninputs = tf.keras.Input(shape=(max_seq_length,))\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length\n)(inputs)\n\ngru = tf.keras.layers.GRU(units=embedding_dim)(embedding)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(gru)\n\n\nmodel = tf.keras.Model(inputs, outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nbatch_size = 32\nepochs = 3\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)\n    ]\n)","b60ce7d2":"fig = px.line(\n    history.history,\n    y=['loss', 'val_loss'],\n    labels={'x': \"Epoch\", 'y': \"Loss\"},\n    title=\"Loss Over Time\"\n)\n\nfig.show()","2d193f6e":"fig = px.line(\n    history.history,\n    y=['auc', 'val_auc'],\n    labels={'x': \"Epoch\", 'y': \"AUC\"},\n    title=\"AUC Over Time\"\n)\n\nfig.show()","05a6cc77":"model.load_weights('.\/model.h5')","78dfdbd7":"model.evaluate(X_test, y_test)","cc4fbd38":"# Preprocessing","31dda549":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/NULf94GVu44","8262a51c":"# Task for Today  \n\n***\n\n## Fake News Detection  \n\nGiven *titles of various news articles*, let's try to predict whether a given piece of news is **fake** or not.  \n  \nWe will use a TensorFlow GRU RNN to make our predictions.","a24f578d":"# Getting Started","4eb2831f":"# Results","25d322db":"# Training"}}