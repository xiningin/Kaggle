{"cell_type":{"82c2b59a":"code","a9cc8c57":"code","9882bdb0":"code","45eb6ca1":"code","1390e7a2":"code","41f3ec8e":"code","d7687ec5":"code","ab0cee52":"code","6eefe124":"code","8ca166d0":"code","77de54cb":"code","659ce736":"code","0dcba3cf":"code","54e830c7":"code","87f213f4":"code","d05e06be":"code","4c2a9c51":"code","e393fb46":"code","6a7d4e58":"code","f70b1665":"markdown","3290782e":"markdown","540f70b8":"markdown","12db3cd1":"markdown"},"source":{"82c2b59a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom scipy import stats\n\nsns.set_style(style='white')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a9cc8c57":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(train.shape, test.shape)","9882bdb0":"train.head()","45eb6ca1":"train.info()","1390e7a2":"# Dependent variable = categorical\n# Independent variables = numerical or ","41f3ec8e":"fig, ax = plt.subplots(figsize=(5, 3), dpi=100)\ntrain['target'].value_counts().plot(kind='bar', ax=ax)\nfor p in ax.patches:\n    ax.annotate('{:,}'.format(p.get_height()), (p.get_x() + .13, p.get_height() + 3000),  fontsize=10)\n\nax.tick_params(axis='both', rotation=0, labelleft=False)\nax.set_ylim(0, 200000)\nax.grid(axis='y', linestyle='--')\nax.set_title('')\nfig.tight_layout()\nplt.show()","d7687ec5":"# Class imbalance\n# Too many columns, so we will not get much information at once.\nprint(len(train.columns))","ab0cee52":"melted1 = pd.melt(train.iloc[:, np.r_[1, 2:42]], id_vars='target')\nmelted2 = pd.melt(train.iloc[:, np.r_[1, 42:82]], id_vars='target')\nmelted3 = pd.melt(train.iloc[:, np.r_[1, 82:122]], id_vars='target')\nmelted4 = pd.melt(train.iloc[:, np.r_[1, 122:162]], id_vars='target')\nmelted5 = pd.melt(train.iloc[:, np.r_[1, 162:202]], id_vars='target')","6eefe124":"fig, axes = plt.subplots(5, 1, figsize=(25, 15), dpi=100)\n\nfor i, df in enumerate([melted1, melted2, melted3, melted4, melted5]):\n    sns.boxplot(x='variable', y='value', hue='target', data=df, ax=axes.flat[i])\n    axes.flat[i].grid(axis='x', linestyle='--')\n    axes.flat[i].legend(loc='upper right', ncol=2, frameon=True)\nfig.tight_layout()\nplt.show()","8ca166d0":"# Statistical test\n# Check p-values\n# ANOVA","77de54cb":"result = pd.DataFrame(columns=['var_names', 'p-values'])\nresult['var_names'] = train.columns[2:].tolist()\np_vals = []\nfor col in train.columns:\n    if col not in ['ID_code', 'target']:\n        _ = train.loc[:, ['target', col]].pivot(columns='target')\n        statics, p_value = stats.f_oneway(_.iloc[:,0].dropna(), _.iloc[:,1].dropna())\n        p_vals.append(p_value)\nresult['p-values'] = p_vals\nresult = result.assign(disparity=np.log(1.\/result['p-values'].values))\nresult.sort_values(by='disparity', ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(5, 35), dpi=100)\nsns.barplot(y='var_names', x='disparity', data=result, color='lightsalmon', ax=ax)\nplt.show()","659ce736":"print(len(train.columns))\nprint(len(result[result['p-values'] < 0.05]))","0dcba3cf":"logit_mod = sm.Logit(train['target'], train.iloc[:, 2:])\nlogit_res = logit_mod.fit(disp=0)\nprint(logit_res.summary())","54e830c7":"variables = logit_res.pvalues[logit_res.pvalues < 0.05].index.tolist()\nprint(len(variables))","87f213f4":"import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold","d05e06be":"param = {\n    'num_leaves': 5,\n    'max_depth': 15,\n    'save_binary': True,\n    'seed': 42,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'is_unbalance': True,\n}","4c2a9c51":"train_preds = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor train_index, valid_index in skf.split(train[variables], train['target']):\n    train_data = lgb.Dataset(train.loc[train_index, variables], \n                             label=train.loc[train_index, 'target'])\n    valid_data = lgb.Dataset(train.loc[valid_index, variables], \n                             label=train.loc[valid_index, 'target'])\n    \n    bst = lgb.train(param, train_data, num_boost_round=2000, valid_sets=valid_data, \n                    verbose_eval=500, early_stopping_rounds=30)\n    train_preds[valid_index] = bst.predict(train.loc[valid_index, variables], \n                                           num_iteration=bst.best_iteration)\n    test_preds += bst.predict(test[variables], num_iteration=bst.best_iteration) \/ 5\n\nprint('Accuracy {}'.format(accuracy_score(train['target'], np.where(train_preds > 0.5, 1, 0))))\nprint('ROC AUC Score: {}'.format(roc_auc_score(train['target'], train_preds)))","e393fb46":"import itertools\n\nfig, ax = plt.subplots(figsize=(5,5), dpi=100)\nclasses = train.target.unique()\ncm = confusion_matrix(train.target, np.where(train_preds > 0.5, 1, 0))\ncs = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nfig.colorbar(cs)\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\nthresh = cm.max() \/ 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    ax.text(j, i, format(cm[i, j]),\n            horizontalalignment='center',\n            color='white' if cm[i, j] > thresh else 'black')\nax.set_ylabel('True label')\nax.set_xlabel('Predicted label')\nfig.tight_layout()\nplt.show()","6a7d4e58":"submission = pd.DataFrame({'ID_code': test['ID_code'],\n                           'target': test_preds})\nsubmission.to_csv('submission.csv', index=False)","f70b1665":"## Builing a simple model using LightGBM","3290782e":"## Boxplot\n\nWe know that the dependent variable (target) is a binary, 0 and 1. It is a categorical value and I used a box plot to visualize it.\n\nI don't know what the original value on independent variable (features) is because it's pre-processed, but I think the its distribution is a numerical type.","540f70b8":"## ANOVA\n\nI just wanted to see the variance between groups by dividing one variable by 0 and 1.\n\nFor example, the \"var_0\" variable is divided into two based on the value of the target variable, and then compare the variances of the two groups generated.","12db3cd1":"## Logistic Regression\n\nSince the independent variables are numeric and the dependent variables are categorical, statistical tests were performed using logistic regression."}}