{"cell_type":{"6ab797ae":"code","40f9c3eb":"code","15cbb629":"code","e7d47f90":"code","eb830537":"code","ef089e2c":"code","4cf93662":"markdown","472f51d6":"markdown","18cdc2df":"markdown","e206c85b":"markdown","10bc748a":"markdown","3d0d965b":"markdown"},"source":{"6ab797ae":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","40f9c3eb":"train_df = pd.read_csv('..\/input\/train.csv')\n\n# borrowed from other kagglers: https:\/\/www.kaggle.com\/hmendonca\/testing-engineered-features-lb-1-42\n# Find and drop duplicate rows\nt = train_df.iloc[:,2:].duplicated(keep=False)\nduplicated_indices = t[t].index.values\nprint(\"Removed {} duplicated rows: {}\".format(len(duplicated_indices), duplicated_indices))\ntrain_df.iat[duplicated_indices[0], 1] = np.expm1(np.log1p(train_df.target.loc[duplicated_indices]).mean()) # keep and update first with log mean\ntrain_df.drop(duplicated_indices[1:], inplace=True) # drop remaining\n\n# Columns to drop because there is no variation in training set\nzero_std_cols = train_df.drop(\"ID\", axis=1).columns[train_df.std() == 0]\ntrain_df.drop(zero_std_cols, axis=1, inplace=True)\nprint(\"Removed {} constant columns\".format(len(zero_std_cols)))","15cbb629":"# Log-transform all column\ntrain_df = train_df.drop(['ID'], axis=1)\ntrain_df = np.log1p(train_df)","e7d47f90":"X = train_df.drop(['target'], axis=1)\nY = train_df[['target']]\n\n# we will use only a subset of the train data so this kaggle kerenel will run quicker...\nfrom sklearn.model_selection import train_test_split\nX, _, Y, __ = train_test_split(X, Y, test_size=0.33)\nY = np.array(Y).reshape(len(Y))","eb830537":"from skopt.space import Integer, Categorical, Real\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\n\n# set up hyperparameter space\nspace = [Integer(16, 256, name='num_leaves'),\n         Integer(8, 256, name='n_estimators'),\n         Categorical(['gbdt', 'dart', 'goss'], name='boosting_type'),\n         Real(0.001, 1.0, name='learning_rate')]\n\nimport lightgbm\nregressor = lightgbm.LGBMRegressor()\n\nfrom sklearn.model_selection import cross_val_score\n\n@use_named_args(space)\ndef objective(**params):\n    regressor.set_params(**params)\n    return -np.mean(cross_val_score(regressor, X, Y, cv=5, n_jobs=1, scoring='neg_mean_absolute_error'))","ef089e2c":"reg_gp = gp_minimize(objective, space, verbose=True)\n\nprint('best score: {}'.format(reg_gp.fun))\n\nprint('best params:')\nprint('       num_leaves: {}'.format(reg_gp.x[0]))\nprint('     n_estimators: {}'.format(reg_gp.x[1]))\nprint('    boosting_type: {}'.format(reg_gp.x[2]))\nprint('    learning_rate: {}'.format(reg_gp.x[3]))","4cf93662":"now lets set up our optimisation problem...","472f51d6":"Optimizing hyperparameters can be a painfully slowexperience even for some of the most experienced data scientist. The most popular choices currently are Random or Grid searches. In my experience i have found that Grid Searches take an inordinant amount of time due to them trying every possible combination of hyperparameters and Random Searches have no garuntee of finding a good let alone great set of hyperparameters. Lately i've stumbled onto using Bayesian Optimization stratergies which are much quicker than full grid searches and also seem to have a higher chance of converging on a great set of hyperparameters.","18cdc2df":"and finally lets produce the data sets...","e206c85b":"Lets prepare the dataset (using code from other kagglers)...","10bc748a":"now, lets run the optimization process","3d0d965b":"lets log transform the columns..."}}