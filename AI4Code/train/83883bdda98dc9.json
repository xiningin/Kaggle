{"cell_type":{"840f8a05":"code","8e8a2e33":"code","09a3a3dd":"code","f0812261":"code","58e210ac":"code","439d96b0":"code","a8433aba":"code","e866456e":"code","8e36ea59":"code","5fd6cfae":"code","700ee35f":"code","9f9f4ed4":"code","edd7f14b":"code","80c8b219":"code","0d93e296":"code","47628331":"code","37cf346b":"code","94c69f9b":"code","49f38b24":"code","7ff239ba":"code","6283a58d":"code","350d8772":"code","7f9c1374":"code","3568d2ec":"code","24e44c07":"code","c4ef6579":"code","9edce385":"code","41c6d443":"code","b6d85fb0":"code","e6c78824":"code","9ea926f7":"code","8aef1826":"code","984bd8fe":"code","a1bbad42":"code","9d7e7a1e":"code","e5e4607b":"code","bcb319a0":"code","4ef32e94":"code","bef9e255":"code","04bcca56":"code","9e85eaf5":"code","6bfea7a7":"code","8f17bef1":"code","5e9985e7":"code","4e514117":"code","cd761d02":"code","c9189733":"code","6a4d3e7c":"code","7baa9f8f":"code","8cb1ffd0":"code","2123444a":"code","cdc46b13":"code","89e752f1":"code","0b176391":"code","f22e5855":"code","1b2772a8":"code","ac8d6de3":"markdown","c977b70f":"markdown","c9f90832":"markdown","e49d9aae":"markdown","39005da7":"markdown","7b8481a8":"markdown","225230f9":"markdown"},"source":{"840f8a05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline\npd.set_option('display.max_columns',None)","8e8a2e33":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","09a3a3dd":"train.shape","f0812261":"train.SalePrice.describe()","58e210ac":"train.info()","439d96b0":"na_cols = train.columns[train.isna().any()]\nfor col in na_cols:\n    print(col,\":\",np.round(train[col].isna().mean()*100,3),'\\tCount :', train[col].isna().sum())","a8433aba":"for col in na_cols:\n    data = train.copy()\n    data[col] = np.where(data[col].isna(),1,0)\n    print(data.groupby(col)['SalePrice'].median())\n    print(np.round(train[col].isna().mean()*100,3),'%')\n    data.groupby(col)['SalePrice'].median().plot.bar()\n    plt.title(col)\n    plt.show()","e866456e":"num_cols = [col for col in train.columns if train[col].dtype !='O']\nprint(len(num_cols))\ntrain[num_cols].head()","8e36ea59":"for col in num_cols:\n    sns.boxplot(train[col])\n    plt.title(col)\n    plt.xlabel(col)\n    plt.show()  ","5fd6cfae":"year_col = [col for col in num_cols if 'Year' in col or 'Yr' in col]\nyear_col","700ee35f":"print(\"UNIQUE Value Count : \\n\",train[year_col].nunique())\nprint('\\nNULL Value Count : \\n',train[year_col].isna().sum())\nprint('\\nUNIQUE Values :\\n')\nfor col in year_col:\n    print(col,':',train[col].unique())","9f9f4ed4":"train.groupby('YrSold')['SalePrice'].median().plot()\nplt.ylabel('SalePrice')\nplt.title('YrSold Vs. SalePrice')\nplt.show()","edd7f14b":"for col in year_col:\n    if col != 'YrSold':\n        data = train.copy()\n        data[col] = data['YrSold']-data[col]\n        plt.scatter(data[col],data['SalePrice'],)\n        plt.title(col.upper())\n        plt.xlabel(col)\n        plt.ylabel('SalePrice')\n        plt.show()\n        ","80c8b219":"disc_cols = [col for col in num_cols if len(train[col].unique())<25 and col not in year_col]\ndisc_cols","0d93e296":"train[disc_cols].head()","47628331":"for col in disc_cols:\n    data = train.copy()\n    data.groupby(col)['SalePrice'].median().plot.bar()\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\n    plt.title(col.upper())\n    plt.show()","37cf346b":"cont_cols = [col for col in num_cols if col not in disc_cols + year_col + ['Id']]\nfor col in cont_cols:\n    data = train.copy()\n    data[col].hist(bins=25)\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    plt.title(col.upper())\n    plt.show()","94c69f9b":"for col in cont_cols:\n    data = train.copy()\n    if 0 in data[col].unique():\n        pass\n    else:\n        data[col] = np.log(data[col])\n        data['SalePrice'] = np.log(data['SalePrice'])\n        plt.scatter(data[col],data['SalePrice'])\n        plt.xlabel(col)\n        plt.ylabel('SalePrice')\n        plt.title(col.upper())\n        plt.show()","49f38b24":"cat_cols = [col for col in train.columns if train[col].dtypes=='O']\ncat_cols","7ff239ba":"train[cat_cols].head()","6283a58d":"train[cat_cols].nunique()","350d8772":"for col in cat_cols:\n    data = train.copy()\n    data.groupby(col)['SalePrice'].median().plot.bar()\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\n    plt.title(col)\n    plt.show()","7f9c1374":"na_cat_cols = [col for col in cat_cols if train[col].isna().sum()>1]\nfor col in na_cat_cols:\n    print(col,':',np.round(train[col].isna().mean()*100,3),'%')","3568d2ec":"def rep_cat_cols(data, na_col):\n    df = data.copy()\n    df[na_col] = df[na_col].fillna('Missing')\n    return df","24e44c07":"train = rep_cat_cols(train, na_cat_cols)\ntrain[na_cat_cols].isna().sum()","c4ef6579":"na_num_cols = [col for col in num_cols if train[col].isna().sum()>1]\nfor col in na_num_cols:\n    print(col,':',np.round(train[col].isna().mean()*100,3),'%')","9edce385":"for col in na_num_cols:\n    med_val = train[col].median()\n    train[col+'_na'] = np.where(train[col].isna(),1,0)\n    train[col].fillna(med_val, inplace=True)\ntrain[na_num_cols].isna().sum()","41c6d443":"train.head()","b6d85fb0":"for col in year_col:\n    if col!='YrSold':\n        train[col] = train['YrSold']-train[col]","e6c78824":"train[year_col].head()","9ea926f7":"for col in ['LotFrontage','LotArea', '1stFlrSF','GrLivArea','SalePrice']:\n    train[col]=np.log(train[col])","8aef1826":"train.head()","984bd8fe":"for col in cat_cols:\n    temp = train.groupby(col)['SalePrice'].count()\/len(train)\n    temp_df = temp[temp>0.01].index\n    train[col] = np.where(train[col].isin(temp_df),train[col],'Rare_var')","a1bbad42":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor col in cat_cols:\n    train[col] = le.fit_transform(train[col])\ntrain.head()","9d7e7a1e":"features = [col for col in train.columns if col not in ['Id','SalePrice']]\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])","e5e4607b":"train.head(10)","bcb319a0":"X = train.drop(['Id','SalePrice'],axis=1)\ny = train['SalePrice']","4ef32e94":"X","bef9e255":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nselector = SelectFromModel(Lasso(alpha=0.005,random_state=0))\nselector.fit(X,y)\nlen(X.columns[selector.get_support()])","04bcca56":"selector.get_support()","9e85eaf5":"x = X[X.columns[selector.get_support()]]","6bfea7a7":"x.head()","8f17bef1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)","5e9985e7":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train,y_train)\npred = xgb.predict(X_test)\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nprint(mean_absolute_error(np.exp(y_test), np.exp(pred)))\nprint(mean_squared_error(np.exp(y_test), np.exp(pred)))\nprint(np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))","4e514117":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","cd761d02":"na_cols = test.columns[test.isna().any()]\nlen(na_cols)","c9189733":"num_cols = [col for col in test.columns if test[col].dtype !='O']\nlen(num_cols)","6a4d3e7c":"na_cat_cols = [col for col in cat_cols if test[col].isna().sum()>1]\ntest = rep_cat_cols(test, na_cat_cols)\ntest[na_cat_cols].isna().sum()\nna_num_cols = [col for col in num_cols if test[col].isna().sum()>1]\nfor col in na_num_cols:\n    med_val = test[col].median()\n    test[col+'_na'] = np.where(test[col].isna(),1,0)\n    test[col].fillna(med_val, inplace=True)\nfor col in year_col:\n    if col!='YrSold':\n        test[col] = test['YrSold']-test[col]\nfor col in ['LotFrontage','LotArea', '1stFlrSF','GrLivArea']:\n    test[col]=np.log(test[col])\nfor col in cat_cols:\n    temp = train.groupby(col)['SalePrice'].count()\/len(train)\n    temp_df = temp[temp>0.01].index\n    test[col] = np.where(test[col].isin(temp_df),test[col],'Rare_var')\nfor col in cat_cols:\n    test[col] = le.transform(test[col])\ntest[features] = sc.transform(test[features])\nx_test = test.drop(['Id'],axis=1)\nx_test = x_test[X.columns[selector.get_support()]]","7baa9f8f":"x_test.columns[x_test.isna().any()]","8cb1ffd0":"x_test.isna().sum()","2123444a":"x_test['GarageCars'].fillna(x_test['GarageCars'].mode()[0],inplace=True)","cdc46b13":"x_test['BsmtFinSF1'].fillna(x_test['BsmtFinSF1'].median(),inplace=True)","89e752f1":"x_test['TotalBsmtSF'].fillna(x_test['TotalBsmtSF'].median(),inplace=True)","0b176391":"print(x_test.isna().sum())","f22e5855":"submit = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmit.head()","1b2772a8":"prediction = xgb.predict(x_test)\nprediction = np.exp(prediction)\nprint(mean_absolute_error(submit.SalePrice, prediction))\nprint(mean_squared_error(submit.SalePrice, prediction))\nprint(np.sqrt(mean_squared_error(submit.SalePrice, prediction)))","ac8d6de3":"#### Model Prediction","c977b70f":"There are a lot of outliers so it's better to use median values for plotting.","c9f90832":"#### Feature Selection","e49d9aae":"#### Feature Engineering of test data set","39005da7":"#### Feature Engineering","7b8481a8":"There is relation between null values and dependent variable. So we have to replace null values instead of dropping them.","225230f9":"### Data Analysis"}}