{"cell_type":{"dd8225bf":"code","c6bac3f0":"code","d9379cad":"code","e967492b":"code","34373e7b":"code","dcafbc29":"code","4f0e2c46":"code","7da6f01d":"code","1e317075":"code","0c630fc0":"code","aad62062":"code","6bd1ecf6":"code","a50299e4":"code","5c1e5f84":"code","02f6d680":"code","664e2339":"code","0d09d9d4":"code","96a9851c":"code","64baa165":"code","35c30dc7":"code","fb60f50c":"code","8aa82106":"code","f99095a3":"code","6929e6f6":"code","dda08539":"code","e13c4b0a":"code","1e416fb3":"code","3590418a":"code","3fb90dd4":"code","e74727aa":"code","053a5ca5":"code","95c7d091":"code","8b062e24":"code","77c1c317":"code","f1aa29d7":"code","254bc519":"code","979734bb":"code","0546a381":"code","74d10bf5":"code","37b29894":"code","a89c29bb":"code","d3fb87de":"code","1c22b126":"code","4c9aff19":"code","4e4edf6e":"code","da06802e":"code","c199a581":"code","8384b4bf":"code","7470c1d6":"code","48f8fa08":"code","09f328ee":"code","6af6fdfe":"code","03a93d7f":"code","26977bc8":"code","56a842d4":"code","d968c939":"code","0a52c4d3":"code","c1a11061":"code","ae6ea223":"code","30a17bfa":"code","98f7c08d":"code","5e1fbd2d":"code","d05c9e26":"code","3fc7f123":"code","fb4c79fc":"code","d7909bcf":"code","8c588beb":"code","bf95a05f":"code","70580f64":"code","05658104":"code","d0b94bb5":"code","48b58af0":"code","957bbb9d":"code","8fcd51d9":"code","baac3b7b":"code","df519b69":"markdown","7230ca6f":"markdown","d6745e08":"markdown","ad7f973e":"markdown","89926f9e":"markdown","8df568cd":"markdown","35220e14":"markdown","91d152ce":"markdown","d06497e8":"markdown","660ebe98":"markdown","ec73c4e6":"markdown","cffb3756":"markdown","564609cb":"markdown","403e2dd6":"markdown","ca4a3476":"markdown","8d5eab74":"markdown","eebd2652":"markdown","56c1b7d0":"markdown","73c174b3":"markdown","8c6d3af4":"markdown","86fec720":"markdown","2d8d9cd0":"markdown","6a39109e":"markdown","f6040b9a":"markdown","8679761b":"markdown","fb69bdea":"markdown","becd1c60":"markdown","f842c3b9":"markdown"},"source":{"dd8225bf":"import copy\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# WordCloud\nfrom wordcloud import WordCloud\n\n\n# preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# models\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","c6bac3f0":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# Setting up visualisations\nsns.set_style(style='white') \nsns.set(rc={\n    'figure.figsize':(12,7), \n    'axes.facecolor': 'white',\n    'axes.grid': True, 'grid.color': '.9',\n    'axes.linewidth': 1.0,\n    'grid.linestyle': u'-'},font_scale=1.5)\ncustom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\nsns.set_palette(custom_colors)","d9379cad":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId') # Train data\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId') # Test data\ntd = pd.concat([copy.deepcopy(traindf), copy.deepcopy(testdf)], axis=0, sort  = False) # Train & Test data\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # Form for answers","e967492b":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# Missing values\ntd.isnull().sum()\nsns.heatmap(td.isnull(), cbar = False).set_title(\"Missing values heatmap\")","34373e7b":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# We check the train sample for balance\ntraindf['Survived'].value_counts(normalize=True)","dcafbc29":"td.nunique()","4f0e2c46":"td.describe()","7da6f01d":"td['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntd['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n\ntd['Title'] = td['Title'].replace('Ms','Miss')\ntd['Title'] = td['Title'].replace('Mlle','Miss')\ntd['Title'] = td['Title'].replace('Mme','Mrs')\n\ntd['Age'] = td.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n\nmed_fare = td.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ntd['Fare'] = td['Fare'].fillna(med_fare)","1e317075":"# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ntd = td.drop(cols_to_drop, axis=1)          \ntraindf = traindf.fillna(traindf.isnull())","0c630fc0":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nsns.heatmap(traindf.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","aad62062":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n(traindf.Survived.value_counts(normalize=True) * 100).plot.barh().set_title(\"Training Data - Percentage of people survived and Deceased\")","6bd1ecf6":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\nfig_pclass = traindf.Pclass.value_counts().plot.pie().legend(labels=[\"Class 3\",\"Class 1\",\"Class 2\"], loc='center right', bbox_to_anchor=(2.25, 0.5)).set_title(\"Training Data - People travelling in different classes\")","a50299e4":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\npclass_1_survivor_distribution = round((traindf[traindf.Pclass == 1].Survived == 1).value_counts()[1]\/len(traindf[traindf.Pclass == 1]) * 100, 2)\npclass_2_survivor_distribution = round((traindf[traindf.Pclass == 2].Survived == 1).value_counts()[1]\/len(traindf[traindf.Pclass == 2]) * 100, 2)\npclass_3_survivor_distribution = round((traindf[traindf.Pclass == 3].Survived == 1).value_counts()[1]\/len(traindf[traindf.Pclass == 3]) * 100, 2)\npclass_perc_df = pd.DataFrame(\n    { \"Percentage Survived\":{\"Class 1\": pclass_1_survivor_distribution,\"Class 2\": pclass_2_survivor_distribution, \"Class 3\": pclass_3_survivor_distribution},  \n     \"Percentage Not Survived\":{\"Class 1\": 100-pclass_1_survivor_distribution,\"Class 2\": 100-pclass_2_survivor_distribution, \"Class 3\": 100-pclass_3_survivor_distribution}})\npclass_perc_df.plot.bar().set_title(\"Training Data - Percentage of people survived on the basis of class\")\n","5c1e5f84":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\npclass_perc_df","02f6d680":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\nfig_sex = (traindf.Sex.value_counts(normalize = True) * 100).plot.bar()\nmale_pr = round((traindf[traindf.Sex == 'male'].Survived == 1).value_counts()[1]\/len(traindf.Sex) * 100, 2)\nfemale_pr = round((traindf[traindf.Sex == 'female'].Survived == 1).value_counts()[1]\/len(traindf.Sex) * 100, 2)\nsex_perc_df = pd.DataFrame(\n    { \"Percentage Survived\":{\"male\": male_pr,\"female\": female_pr},  \"Percentage Not Survived\":{\"male\": 100-male_pr,\"female\": 100-female_pr}})\nsex_perc_df.plot.barh().set_title(\"Percentage of male and female survived and Deceased\")\nfig_sex","664e2339":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf['Age_Range'] = pd.cut(traindf.Age, [0, 10, 20, 30, 40, 50, 60,70,80])\nsns.countplot(x = \"Age_Range\", hue = \"Survived\", data = traindf, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])","0d09d9d4":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\nsns.distplot(td['Age'].dropna(),color='darkgreen',bins=30)","96a9851c":"td['Family'] = td.Parch + td.SibSp\ntd['Is_Alone'] = td.Family == 0","64baa165":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf['Fare_Category'] = pd.cut(traindf['Fare'], bins=[0,7.90,14.45,31.28,120], labels=['Low','Mid', 'High_Mid','High'])","35c30dc7":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\nx = sns.countplot(x = \"Fare_Category\", hue = \"Survived\", data = traindf, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])\nx.set_title(\"Survival based on fare category\")","fb60f50c":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\np = sns.countplot(x = \"Embarked\", hue = \"Survived\", data = traindf, palette=[\"C1\", \"C0\"])\np.set_xticklabels([\"Southampton\",\"Cherbourg\",\"Queenstown\"])\np.legend(labels = [\"Deceased\", \"Survived\"])\np.set_title(\"Training Data - Survival based on embarking point.\")","8aa82106":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf.Embarked.fillna(traindf.Embarked.mode()[0], inplace = True)","f99095a3":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf['Salutation'] = traindf.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip()) \ntraindf.Salutation.nunique()\nwc = WordCloud(width = 1000,height = 450,background_color = 'white').generate(str(traindf.Salutation.values))\nplt.imshow(wc, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\n\ntraindf.Salutation.value_counts()","6929e6f6":"traindf.info()","dda08539":"# Clone data for FE\ntrain_fe = copy.deepcopy(traindf)\ntarget_fe = train_fe['Survived']\ndel train_fe['Survived']","e13c4b0a":"train_fe = train_fe.fillna(train_fe.isnull())","1e416fb3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train_fe.columns.values.tolist()\nfor col in features:\n    if train_fe[col].dtype in numerics: continue\n    categorical_columns.append(col)\nindexer = {}\nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    _, indexer[col] = pd.factorize(train_fe[col])\n    \nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    train_fe[col] = indexer[col].get_indexer(train_fe[col])","3590418a":"train_fe.info()","3fb90dd4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nX = train_fe\nz = target_fe","e74727aa":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","053a5ca5":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","95c7d091":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","8b062e24":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","77c1c317":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","f1aa29d7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","254bc519":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","979734bb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","0546a381":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Standardization for regression models\ntrain = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","74d10bf5":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","37b29894":"len(coeff_logreg)","a89c29bb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","d3fb87de":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","1c22b126":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()","4c9aff19":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","4e4edf6e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","da06802e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('mean', ascending=False)","c199a581":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","8384b4bf":"#Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('total', ascending=False)","7470c1d6":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ntarget = td.Survived.loc[traindf.index]\ntd = td.drop(['Survived'], axis=1)\ntrain, test = td.loc[traindf.index], td.loc[testdf.index]","48f8fa08":"train.head(3)","09f328ee":"test.head(3)","6af6fdfe":"target[:3]","03a93d7f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","26977bc8":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","56a842d4":"train.info()","d968c939":"test.info()","0a52c4d3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n#%% split training set to validation set\nSEED = 100\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)","c1a11061":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc_random_forest = round(random_forest.score(train, target) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","ae6ea223":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nrandom_forest_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","30a17bfa":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","98f7c08d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_xgb, best)\nparams","5e1fbd2d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nXGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nacc_XGB_Classifier","d05c9e26":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nxgb_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","3fc7f123":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\nplt.show();\nplt.close()","fb4c79fc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(2, 2**11, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","d7909bcf":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_lgb, best)\nparams","8c588beb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nLGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nY_pred = LGB_Classifier.predict(test).astype(int)\nLGB_Classifier.score(train, target)\nacc_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nacc_LGB_Classifier","bf95a05f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nlgb_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","70580f64":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(LGB_Classifier,ax = axes,height = 0.5)\nplt.show();\nplt.close()","05658104":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nmodels = pd.DataFrame({\n    'Model': ['Random Forest',  'XGBClassifier', 'LGBMClassifier'],\n    \n    'Score': [acc_random_forest, acc_XGB_Classifier, acc_LGB_Classifier]})","d0b94bb5":"models","48b58af0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score'], label = 'Score')\nplt.legend()\nplt.title('Score  for 3 popular models')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","957bbb9d":"random_forest_submission.to_csv('submission_RandomForest.csv', index=False)","8fcd51d9":"xgb_submission.to_csv('submission_XGB_Classifier.csv', index=False)","baac3b7b":"lgb_submission.to_csv('submission_LGB_Classifier.csv', index=False)","df519b69":"#### Survived","7230ca6f":"# Titanic by 3 models + EDA","d6745e08":"<a class=\"anchor\" id=\"10.1\"><\/a>\n### 10.1 Random Forest \n##### [Back to Table of Contents](#0.1)","ad7f973e":"## **Acknowledgements**\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n   - https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\n   - https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models","89926f9e":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. EDA & Visualization\n##### [Back to Table of Contents](#0.1)","8df568cd":"#### Pclass","35220e14":"<a class=\"anchor\" id=\"12\"><\/a>\n## 12. Prediction & Output data\n##### [Back to Table of Contents](#0.1)","91d152ce":"<a class=\"anchor\" id=\"10.2\"><\/a>\n### 10.2 XGB\n##### [Back to Table of Contents](#0.1)","d06497e8":"<a class=\"anchor\" id=\"7.4\"><\/a>\n### 7.4 Linear Reagression\n##### [Back to Table of Contents](#0.1)","660ebe98":"#### Correlation Between The Features","ec73c4e6":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE \n##### [Back to Table of Contents](#0.1)","cffb3756":"<a class=\"anchor\" id=\"2\"><\/a>\n## 2. Download datasets \n##### [Back to Table of Contents](#0.1)","564609cb":"<a class=\"anchor\" id=\"7.2\"><\/a>\n### 7.2 XGB\n##### [Back to Table of Contents](#0.1)","403e2dd6":"<a class=\"anchor\" id=\"9\"><\/a>\n## 9. Preparing to modeling\n##### [Back to Table of Contents](#0.1)","ca4a3476":"<a class=\"anchor\" id=\"9.1\"><\/a>\n### 9.1 Data for modeling \n##### [Back to Table of Contents](#0.1)","8d5eab74":"<a class=\"anchor\" id=\"1\"><\/a>\n## 1. Import libraries \n##### [Back to Table of Contents](#0.1)","eebd2652":"<a class=\"anchor\" id=\"10.3\"><\/a>\n### 10.3 LGBM\n##### [Back to Table of Contents](#0.1)","56c1b7d0":"<a class=\"anchor\" id=\"8\"><\/a>\n## 8. Comparison of the all feature importance diagrams\n##### [Back to Table of Contents](#0.1)","73c174b3":"<a class=\"anchor\" id=\"7.1\"><\/a>\n### 7.1 LGBM\n##### [Back to Table of Contents](#0.1)","8c6d3af4":"<a class=\"anchor\" id=\"7\"><\/a>\n## 7. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","86fec720":"<a class=\"anchor\" id=\"10\"><\/a>\n## 10. Tuning models\n##### [Back to Table of Contents](#0.1)","2d8d9cd0":"<a class=\"anchor\" id=\"7.3\"><\/a>\n### 7.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)","6a39109e":"<a class=\"anchor\" id=\"6\"><\/a>\n## 6. Preparing data for building the feature importance diagrams \n##### [Back to Table of Contents](#0.1)","f6040b9a":"<a class=\"anchor\" id=\"9.2\"><\/a>\n### 9.2 Encoding categorical features\n##### [Back to Table of Contents](#0.1)","8679761b":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## **Table of Contents**\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [Data research](#3)\n4. [FE](#4)\n5. [EDA & Visualization](#5)\n6. [Preparing data for building the feature importance diagrams](#6)  \n7. [FE: building the feature importance diagrams](#7)\n  -  [LGBM](#7.1)\n  -  [XGB](#7.2)\n  -  [Logistic Regression](#7.3)\n  -  [Linear Reagression](#7.4)\n8. [Comparison of the all feature importance diagrams](#8)\n9. [Preparing to modeling](#9)\n  - [Data for modeling](#9.1)\n  - [Encoding categorical features](#9.2)\n10. [Tuning models](#10)\n  -  [Random Forest](#10.1)\n  -  [XGB](#10.2)\n  -  [LGBM](#10.3)\n11. [Models evaluation](#11)\n12. [Prediction & Output data](#11)","fb69bdea":"<a class=\"anchor\" id=\"3\"><\/a>\n## 3. Data research\n##### [Back to Table of Contents](#0.1)","becd1c60":"## Preview settings","f842c3b9":"<a class=\"anchor\" id=\"11\"><\/a>\n## 11. Models evaluation\n##### [Back to Table of Contents](#0.1)"}}