{"cell_type":{"e9cbf9f9":"code","8dbc6c54":"code","fd31bc4b":"code","d241f3fd":"code","7528b128":"code","bcc46ca1":"code","af7e4aa5":"code","b7512781":"code","ca3a2142":"code","6948d154":"code","f3311c7e":"code","c4383465":"code","0eb7d0aa":"code","29490701":"code","4438654b":"code","2950a5b9":"code","517780ce":"code","7138fde1":"code","20491d29":"code","b1803993":"code","80ead180":"code","25faf406":"code","904616d5":"code","b35ceddb":"code","84e165e7":"code","50798a5d":"code","420e16eb":"code","787ce8a7":"code","bb660c82":"code","48e7746d":"code","d3a2863b":"code","a5d9a13f":"code","581d808e":"code","d8533a18":"code","cd08cf7b":"code","f1298439":"code","9334f84c":"code","a7b1ad89":"code","4be7caa9":"code","8203be17":"code","04971c8e":"code","4af1b045":"code","7e726da0":"code","09b40a1e":"code","dc88259a":"code","feab0f68":"code","be4182e8":"code","2c3acdf9":"code","af79df64":"code","6c3a2fec":"code","90d34917":"code","905df169":"code","eb99d0f5":"code","b6467085":"code","d0feaacd":"code","9a87d58b":"code","de601940":"code","264110d1":"code","cbea46ad":"code","a82e8248":"code","193369dc":"code","e661529d":"code","7bed6b07":"code","4f25a223":"code","3f57f29c":"code","76853d41":"code","e03055b7":"code","5335a148":"code","284a4d63":"code","7f0ac431":"code","05beffe9":"code","4a4ae86d":"code","fb86714a":"code","308b8c5b":"code","b3e9eef0":"code","cc3f1314":"code","dbd0cef9":"code","01dc3abd":"code","b2058886":"code","94ba05a9":"code","f9efa54c":"code","10f7ce78":"code","c2690111":"code","a74dbd84":"code","e379613e":"code","49461ba3":"code","d8544fcc":"code","2f126ada":"code","82eb70d6":"code","0b6d4451":"code","46f943d8":"code","2b7d7e6b":"code","8b284fd3":"code","1a8a1e18":"code","7602958e":"code","18eb4296":"code","ff1801ee":"code","8dd93748":"markdown","6783ca58":"markdown","a1e607b3":"markdown","c9c90479":"markdown","9c23c866":"markdown","aed11ed7":"markdown","57c6a404":"markdown","a2c07aea":"markdown","83cbcb7e":"markdown","a4b05076":"markdown","4c9c66bd":"markdown","fd834c00":"markdown","cd5c21f4":"markdown","5227d2f7":"markdown","eadb1a5f":"markdown","3d9a02c7":"markdown","3736a719":"markdown","09dadbd3":"markdown","f5ee2a0c":"markdown","a0b08608":"markdown","d1df4f6d":"markdown","8cbd8161":"markdown","c0aeaa87":"markdown","01489ef9":"markdown","87f6df6f":"markdown","76616173":"markdown","df315785":"markdown","30017df0":"markdown","01b20909":"markdown","efd6a161":"markdown","297d6d4b":"markdown","f0a8e4c6":"markdown","be62f533":"markdown","ea970ed5":"markdown","c24154b0":"markdown","2d48082e":"markdown"},"source":{"e9cbf9f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8dbc6c54":"# StratifiedKFold cross validation to make sure the same proportion of both classes maintained during each sampling process\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nplt.figure(figsize = (20, 18))\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom bayes_opt import BayesianOptimization\n# hyperopt is hyperparameter optimization by defining an objective function and declaring a search space\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK","fd31bc4b":"# Loading the training data\noct_data = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\noct_data.shape","d241f3fd":"oct_data.head()","7528b128":"# Drops ID column as it is not required\noct_data.drop([\"id\"], axis=1, inplace=True)","bcc46ca1":"# Checks for data types used in the data set\noct_data.dtypes.unique()","af7e4aa5":"# Check for missing values\nsum(oct_data.isna().sum())","b7512781":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","ca3a2142":"# Compresses the training data as Kaggle kernel resets due to large size of the training data \noct_data = reduce_mem_usage(oct_data)","6948d154":"# Shows the column data types after data compression\noct_data.dtypes","f3311c7e":"# Checks distribution of categorical target variable\noct_data.groupby(['target']).size()","c4383465":"oct_data.groupby(['target']).size().isnull().sum() # no null value in the target column","0eb7d0aa":"oct_data.nunique()\n","29490701":"df = oct_data[['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9','f10','target']]\n\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\n\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()\n","4438654b":"df = oct_data[['f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19','f20','target']]\ntarget = df['target']\n\nfeatures = df.drop('target', axis=1)\n\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","2950a5b9":"df = oct_data[['f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29','f30','target']]\n\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","517780ce":"df = oct_data[['f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39','f40','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","7138fde1":"df = oct_data[['f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49','f50','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","20491d29":"df = oct_data[['f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59','f60','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","b1803993":"df = oct_data[['f61', 'f62','f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69','f70','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","80ead180":"df = oct_data[['f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79','f80','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","25faf406":"df = oct_data[['f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89','f90','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","904616d5":"df = oct_data[['f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99','f100','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","b35ceddb":"\ndf = oct_data[['f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109','f110','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","84e165e7":"df = oct_data[['f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119','f120','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","50798a5d":"df = oct_data[['f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129','f130','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","420e16eb":"df = oct_data[['f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139','f140','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","787ce8a7":"df = oct_data[['f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149','f150','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","bb660c82":"df.head()","48e7746d":"df = oct_data[['f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159','f160','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","d3a2863b":"df.head()","a5d9a13f":"df = oct_data[['f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169','f170','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","581d808e":"df.head()","d8533a18":"\ndf = oct_data[['f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179','f180','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","cd08cf7b":"df = oct_data[['f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189','f190','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","f1298439":"df = oct_data[['f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199','f200','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","9334f84c":"\ndf = oct_data[['f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209','f210','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","a7b1ad89":"df = oct_data[['f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219','f220','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","4be7caa9":"df = oct_data[['f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229','f230','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","8203be17":"df = oct_data[['f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239','f240','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","04971c8e":"df = oct_data[['f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249','f250','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","4af1b045":"df = oct_data[['f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259','f260','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","7e726da0":"df.head()","09b40a1e":"df = oct_data[['f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269','f270','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","dc88259a":"df.head()","feab0f68":"df = oct_data[['f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279','f280','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","be4182e8":"df.head()","2c3acdf9":"\ndf = oct_data[['f281', 'f282', 'f283', 'f284','target']]\ntarget = df['target']\nfeatures = df.drop('target', axis=1)\nselect_univariate = SelectKBest(f_classif, k=4).fit(features, target)\nfeatures_mask = select_univariate.get_support()\nfeatures_mask\nselected_columns = features.columns[features_mask]\nselected_features = features[selected_columns]\nselected_features.head()","af79df64":"df.head()","6c3a2fec":"df=oct_data[[\n'f1','f3','f4','f8','f12', 'f17','f18','f19',\n'f22','f26','f27','f29','f33','f34','f35','f40',\n'f42','f43','f44','f48','f52','f53', 'f56','f58',\n'f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241', 'f243', 'f245','f247',\n'f252','f256', 'f258', 'f260',\n'f265', 'f266', 'f267', 'f269',\n'f274', 'f275','f278','f279',\n'f281', 'f282', 'f283','f284',\n'target']]","90d34917":"del oct_data","905df169":"df.shape","eb99d0f5":"df.head()","b6467085":"df[['f1','f3','f4','f8','f12', 'f17','f18','f19','f26','f27','f29','f33','f34','f35','f40',\n'f42','f44','f48','f52','f53', 'f56','f58','f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241']]= pd.DataFrame(preprocessing.scale(df[['f1','f3','f4','f8','f12', 'f17','f18','f19','f26','f27','f29','f33','f34','f35','f40',\n'f42','f44','f48','f52','f53', 'f56','f58','f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241']]))","d0feaacd":"df[['f1','f3','f4','f8','f12', 'f17','f18','f19','f26','f27','f29','f33','f34','f35','f40',\n'f42','f44','f48','f52','f53', 'f56','f58','f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241']].head()","9a87d58b":"# features and the target\n\ny = df.target\nX = df.drop([\"target\"], axis=1)","de601940":"del df","264110d1":"X.head()","cbea46ad":"y.sample(10)","a82e8248":"# Create StratifiedKFold object.\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state= 40)","193369dc":"# Performs cross validation on XGB Classifier\n\nmodel = XGBClassifier(n_estimators=500,objective='binary:logistic', eval_metric='auc',tree_method='gpu_hist')\nmodel_score = cross_val_score(model, X, y, scoring='roc_auc', cv=skf.split(X, y), n_jobs=-1, verbose=10)","e661529d":"print(model_score.mean())","7bed6b07":"del model_score, model","4f25a223":"parameter_space = {\n    'learning_rate': (0.01, 1.0),\n    'n_estimators': (100, 1000),\n    'max_depth': (2,10),\n    'subsample': (0.4, 1.0),\n    'colsample_bytree' :(0.4, 1.0),\n    'gamma': (0, 5)}\n\ndef xgboost_hyper_param(learning_rate,\n                        n_estimators,\n                        max_depth,\n                        subsample,\n                        colsample_bytree,\n                        gamma):\n\n    max_depth = int(max_depth)\n    n_estimators = int(n_estimators)\n\n    clf = XGBClassifier(\n        tree_method='gpu_hist',\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        objective = 'binary:logistic',\n        eval_metric='auc',\n        gamma=gamma)\n    return np.mean(cross_val_score(clf, X, y, cv=5, scoring='roc_auc'))\n\noptimizer = BayesianOptimization(\n    f=xgboost_hyper_param,\n    pbounds=parameter_space,\n    random_state=100,\n)","3f57f29c":"import warnings\nwarnings.filterwarnings('ignore')","76853d41":"optimizer.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)","e03055b7":"optimizer.res","5335a148":"params_gbm = optimizer.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm","284a4d63":"\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","7f0ac431":"dtrain = xgb.DMatrix(data=X_train, label=y_train)\ndval = xgb.DMatrix(data=X_test, label=y_test)\ndel X_train,y_train,X_test,y_test\nparams = {'colsample_bytree': 0.9717997218792195,\n 'gamma': 4.7790561362268145,\n 'learning_rate': 0.052474259458759866,\n 'max_depth': 6,\n 'subsample': 0.9128209886114327}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"auc\"\nparams[\"tree_method\"] = \"gpu_hist\"\n    \nmodel = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)","05beffe9":"# Loads test data set\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\n\n# Removes ID column as it is not required for prediction\ntest.drop([\"id\"], axis=1, inplace=True)","4a4ae86d":"# Loads submission data set that acts just as a template for submission\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n","fb86714a":"test=test[[\n'f1','f3','f4','f8','f12', 'f17','f18','f19',\n'f22','f26','f27','f29','f33','f34','f35','f40',\n'f42','f43','f44','f48','f52','f53', 'f56','f58',\n'f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241', 'f243', 'f245','f247',\n'f252','f256', 'f258', 'f260',\n'f265', 'f266', 'f267', 'f269',\n'f274', 'f275','f278','f279',\n'f281', 'f282', 'f283','f284']]","308b8c5b":"test[['f1','f3','f4','f8','f12', 'f17','f18','f19','f26','f27','f29','f33','f34','f35','f40',\n'f42','f44','f48','f52','f53', 'f56','f58','f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241']]= pd.DataFrame(preprocessing.scale(test[['f1','f3','f4','f8','f12', 'f17','f18','f19','f26','f27','f29','f33','f34','f35','f40',\n'f42','f44','f48','f52','f53', 'f56','f58','f63','f64','f65', 'f69','f72','f73','f77','f78',\n'f82','f85', 'f86', 'f90',\n'f92', 'f95', 'f96', 'f99',\n'f103','f104', 'f107', 'f108',\n'f112','f114', 'f117', 'f119' ,\n'f125','f127', 'f129', 'f130',\n'f134', 'f136','f138','f139',\n'f141', 'f143', 'f144', 'f150',\n'f152', 'f154', 'f156','f159',\n'f162', 'f163', 'f164', 'f169',\n'f173', 'f174', 'f177', 'f179',\n'f181', 'f184', 'f187', 'f188',\n'f192', 'f195', 'f199','f200',\n'f201', 'f206','f208', 'f210',\n'f211', 'f213', 'f214', 'f219' ,\n'f222','f224','f227','f229',\n'f231', 'f232', 'f239', 'f240',\n'f241']]))","b3e9eef0":"test.head()","cc3f1314":"dtest = xgb.DMatrix(data=test)\npredictions = model.predict(dtest)","dbd0cef9":"submission[\"target\"] = predictions","01dc3abd":"# Checks for sumbission file before saving\nsubmission","b2058886":"# Saves test predictions\nsubmission.to_csv(\".\/submission.csv\", index=False) # 0.84868 score","94ba05a9":"del model, predictions","f9efa54c":"fold_no = 1\nfor train_index, test_index in skf.split(X, y):\n    print('Fold = ',fold_no)\n    y_val = y.iloc[test_index]\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[test_index], label=y.iloc[test_index])\n    fold_no +=1","10f7ce78":"hyperparameter_space = { \n                        'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n                        'max_depth': hp.quniform(\"max_depth\", 2, 6, 1),\n                        'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n                        'reg_alpha' : hp.uniform('reg_alpha', 1e-8, 100),\n                        'reg_lambda' : hp.uniform('reg_lambda', 1e-8, 100),\n                        'gamma': hp.uniform ('gamma', 0.0, 1.0),\n                        'subsample': hp.uniform(\"subsample\", 0.1, 1.0),\n                        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0)\n                       }\n\n\n   ","c2690111":"def optimize_hyppara(hyperparameter_space):\n    # Converts parameter value to int as required by XGBoost\n    hyperparameter_space[\"max_depth\"] = int(hyperparameter_space[\"max_depth\"])\n    hyperparameter_space[\"objective\"] = \"binary:logistic\"\n    hyperparameter_space[\"eval_metric\"] = \"auc\"\n    hyperparameter_space[\"tree_method\"] = \"gpu_hist\"\n    \n    model = xgb.train(\n        hyperparameter_space, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=False)\n    \n    predictions = model.predict(dval)\n    \n    roc_auc = roc_auc_score(y_val, predictions)\n    \n    del predictions, model, hyperparameter_space\n    \n    return {\"loss\": -roc_auc, \"status\": STATUS_OK}","a74dbd84":"# Starts hyperparameters tuning\ntrials = Trials()\nbest_model_params = fmin(fn=optimize_hyppara,space=hyperparameter_space, max_evals=50,algo=tpe.suggest,trials=trials)","e379613e":"best_model_params","49461ba3":"del dtrain, dval,y_val","d8544fcc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ndtrain = xgb.DMatrix(data=X_train, label=y_train)\ndval = xgb.DMatrix(data=X_test, label=y_test)\ndel X_train,y_train,X_test,y_test\nparams = {'colsample_bytree': 0.24949960835732582,\n 'gamma': 0.40334695196873604,\n 'learning_rate': 0.06678598441625683,\n 'max_depth': 5.0,\n 'min_child_weight': 6.0,\n 'reg_alpha': 57.68556763950645,\n 'reg_lambda': 47.221038452153344,\n 'subsample': 0.8481553666497129}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"auc\"\nparams[\"tree_method\"] = \"gpu_hist\"\n    \nmodel = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)","2f126ada":"# Adds other important parameters\nbest_model_params[\"max_depth\"] = int(best_model_params[\"max_depth\"])\nbest_model_params[\"objective\"] = \"binary:logistic\"\nbest_model_params[\"eval_metric\"] = \"auc\"\nbest_model_params[\"tree_method\"] = \"gpu_hist\"","82eb70d6":"dtest = xgb.DMatrix(data=test)\npredictions = model.predict(dtest)","0b6d4451":"submission[\"target\"] = predictions\n\n# Checks for sumbission file before saving\nsubmission\n","46f943d8":"# Saves test predictions\nsubmission.to_csv(\".\/submission.csv\", index=False) # 0.85 score","2b7d7e6b":"del model, dtest, predictions","8b284fd3":"# Gets the model trained over cross validation and predictions \n# against each iteration is stored\n\ntest_predictions = []\n\ndtest = xgb.DMatrix(data=test)\n\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(\"fold\", fold)\n\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[val_index], label=y.iloc[val_index])\n    \n    model = xgb.train(\n        best_model_params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)\n    \n    predictions = model.predict(dtest)\n    \n    test_predictions.append(predictions)\n    \n    del predictions, model, dval, dtrain","1a8a1e18":"test_predictions","7602958e":"del dtest, test\n","18eb4296":"# Predictions stored against each cross validation iteration finally gets aeveraged\n# and target column is set with that averaged predictions\nsubmission[\"target\"] = np.mean(np.column_stack(test_predictions), axis=1)\n\n# Checks for sumbission file before saving\nsubmission\n","ff1801ee":"# Saves test predictions\nsubmission.to_csv(\".\/submission.csv\", index=False)","8dd93748":"#### f152, f154, f156,f159 features","6783ca58":"#### f33,f34,f35,f40 are the features\n","a1e607b3":"#### f211, f213, f214, f219 features","c9c90479":"#### f92, f95, f96, f99 features","9c23c866":"# Feature selection with SelectKBest","aed11ed7":"# b) Hyperopt","57c6a404":"#### f231, f232, f239, f240 features selected features","a2c07aea":"#### f252, 256, f258, f260 features. Categorical features","83cbcb7e":"#### f173, f174, f177, f179 selected features","a4b05076":"# Hyperparameter Tuning with Bayesian Optimization","4c9c66bd":"#### f22,f26,f27,f29 is the feature to consider for model training","fd834c00":"#### f281, f282, f283,f284 categorical features","cd5c21f4":"# StratifiedKFold Cross Validation","5227d2f7":"#### The important features are f12, f17,f18,f19","eadb1a5f":"#### f103,f104, f107, f108 features","3d9a02c7":"#### f265, f266, f267, f269 categorical features","3736a719":"#### 241, 243, f245,247 features. Categorical features","09dadbd3":"#### f72,f73,f77,f78 features","f5ee2a0c":"#### f52,f53 f56,f58 are the features","a0b08608":"#### f1, f3, f4, f8 are the important features","d1df4f6d":"#### f112,f114 f117, f119 features","8cbd8161":"#### f181,f184,f187,f188 selected features","c0aeaa87":"# a) bayes_opt","01489ef9":"#### f141, f143, f144, f150 features","87f6df6f":"#### f63,f64,f65 f69","76616173":"#### f42,f43,f44,f48 are the features","df315785":"#### f162, f163, f164, f169 features\n","30017df0":"#### Ten columns at a time subsetting and applying SelectKBest to get the important features","01b20909":"#### f82,f85, f86, f90 features","efd6a161":"#### f201, f206, f208, f210 features","297d6d4b":"#### f192, f195, f199,f200 features","f0a8e4c6":"#### f274, f275,f278,f279 categorical features","be62f533":"#### f125,f127, f129, f130 features","ea970ed5":"#### f222,f224,f227,f229 are the features","c24154b0":"#### Manual data selection completed and we will train the model with the selected features","2d48082e":"#### f134, f136,f138,f139"}}