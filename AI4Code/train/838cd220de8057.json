{"cell_type":{"a17fc01b":"code","9e9c0a1d":"code","323b2ee8":"code","fb618e88":"code","f5b87faa":"code","b6caafb2":"code","b8ecbf17":"code","f55f448f":"code","3cd7cf5f":"code","efc1eedc":"code","a4de4cd3":"code","6cc183cf":"code","85655f64":"code","eec44395":"code","5b8c718d":"code","a27842c9":"code","e0103903":"code","c793ac64":"code","228ff4a5":"code","ff2000a2":"markdown"},"source":{"a17fc01b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e9c0a1d":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","323b2ee8":"df.head()","fb618e88":"from sklearn.model_selection import train_test_split\n\ndef preprocess_data(df):\n    class0_df = df[df['Class'] == 0] ## majority class, will be used for autoencoder training\n    class1_df = df[df['Class'] == 1]\n    ##\n    class0_arr = class0_df.to_numpy()[:, 1:-1] ## feature set from V1 to Amount (Time is excluded)\n    class1_arr = class1_df.to_numpy()[:, 1:-1]\n    X = class0_arr\n    X_train, X_test = train_test_split(X, test_size=0.1)\n    X_train, X_val = train_test_split(X_train, test_size=0.1)\n\n    scale_mean = np.mean(X_train, axis=0)\n    scale_std = np.std(X_train, axis=0)\n    ## normalization\n    X_train = (X_train - scale_mean)\/scale_std\n    X_val = (X_val - scale_mean)\/scale_std\n    X_test = (X_test - scale_mean)\/scale_std\n    class1_arr = (class1_arr - scale_mean)\/scale_std\n    return X_train, X_val, X_test, scale_mean, scale_std, class1_arr","f5b87faa":"X_train, X_val, X_test, scale_mean, scale_std, class1_arr = preprocess_data(df)\nn_features = X_train.shape[1]","b6caafb2":"n_features","b8ecbf17":"df.Class.hist()","f55f448f":"print(\"Train Size: {}\".format(len(X_train)))\nprint(\"Val Size: {}\".format(len(X_val)))\nprint(\"Test Size: {}\".format(len(X_test)))\nprint(\"No of Features: {}\".format(n_features))","3cd7cf5f":"import tensorflow.keras as keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport numpy as np","efc1eedc":"# model archticeture\n\ndef build_model():\n    model = keras.Sequential([\n        keras.Input(shape=(n_features,)),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.BatchNormalization(),\n        \n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.BatchNormalization(),\n        \n        keras.layers.Dense(16, activation='relu'),\n        keras.layers.BatchNormalization(),\n        \n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.BatchNormalization(),\n        \n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.BatchNormalization(),\n        \n        keras.layers.Dense(n_features)\n    ])\n    \n    return model","a4de4cd3":"# model building\nmodel = build_model()","6cc183cf":"# model compile\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n              loss='mse', metrics=['accuracy'])","85655f64":"# callbacks defined\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 5\n    lrate = initial_lrate * (drop**((1 + epoch)\/epochs_drop))\n    return lrate\n\nlrate_scheduler = LearningRateScheduler(step_decay)\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmodel_chkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\n# model fitting\nmodel.fit(X_train, X_train, batch_size=32, epochs=50, validation_data=(X_val, X_val), callbacks=[early_stop, model_chkpoint, lrate_scheduler])","eec44395":"# Evaluation - If reconstruction loss is greater than a certain threshold, we classify that as fraud class.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\nfrom sklearn.metrics import precision_recall_curve, roc_curve\n\n\ndef reconstruction_error(actual, pred):\n    return np.mean((actual - pred)**2, axis=1)\n\ndef evaluate(model, X, y):\n    X = X.reshape(-1, n_features)\n    out = reconstruction_error(X, model.predict(X))\n    print(\"AUC score: {}\".format(roc_auc_score(y, out)))\n    print(\"PR score: {}\".format(average_precision_score(y, out)))\n    print(\"\\n\\n\")\n    for th in [0.5, 0.6, 0.7, 0.8, 0.9, 1., 1.1 , 1.2, 1.3, 1.4, 1.5, 1.75, 2, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4]:\n        out_th = [1 if (o > th) else 0 for o in out]\n        print(\"TH - {}\".format(th))\n        print(\"Precision: {}\".format(precision_score(y, out_th)))\n        print(\"Recall: {}\".format(recall_score(y, out_th)))\n        print(\"F1score: {}\".format(f1_score(y, out_th)))\n        print(\"\\n\\n\")\n    ","5b8c718d":"final_X_test = np.concatenate((X_test, class1_arr))\nfinal_Y_test = np.concatenate(([0]*len(X_test), [1]*len(class1_arr)))","a27842c9":"# Metrics\nevaluate(model, final_X_test, final_Y_test)","e0103903":"import matplotlib.pyplot as plt\nout = reconstruction_error(final_X_test, model.predict(final_X_test))","c793ac64":"precision, recall, _ = precision_recall_curve(final_Y_test, out)\nplt.plot(recall, precision, lw=2)\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve\")\nplt.show()","228ff4a5":"fpr, tpr, _ = roc_curve(final_Y_test, out)\n                        \nplt.plot(fpr, tpr, lw=2)\n    \nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve\")\nplt.show()","ff2000a2":"## AutoEncoder"}}