{"cell_type":{"6438dbe2":"code","9a1d6e2c":"code","01fe9213":"code","760b9626":"code","5dc18ffa":"code","6e7bcc46":"code","e0d5a37c":"code","e300438d":"code","cea4ec70":"code","1bb25027":"code","96326174":"code","76e29f09":"code","444c2872":"code","7d7dafcb":"code","2193ba1e":"code","f6c8e84b":"code","815efc8b":"code","3c06df95":"code","c061cc95":"code","5e30dde4":"code","70cffe2e":"code","ce998510":"code","228ca5d8":"code","3a6ddd8a":"code","b3daf60a":"code","1cbcdac8":"code","e9b6c36f":"markdown","a77e6f55":"markdown","65f85c43":"markdown","9d6e12dd":"markdown","5f40083f":"markdown","a0726beb":"markdown","86c8c07c":"markdown","e8e12a70":"markdown","92723055":"markdown","07ab26bd":"markdown","defeddf9":"markdown","a01073ca":"markdown","5dc8e836":"markdown","70c7022c":"markdown","7af5278f":"markdown","4a7cc2f1":"markdown","d804a86f":"markdown","4a12a1fd":"markdown","bd718461":"markdown","1f1c0459":"markdown","9a946f81":"markdown","6b04dcd6":"markdown","e844d2ff":"markdown","41acbd19":"markdown","c0ddefa4":"markdown","2c8cd2b0":"markdown","bff2f99e":"markdown"},"source":{"6438dbe2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image","9a1d6e2c":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","01fe9213":"df.head()","760b9626":"df.info()","5dc18ffa":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"The categorical features are : \",categorical)","6e7bcc46":"df = pd.concat(      [df,\n                     pd.get_dummies(df.sex), \n                     pd.get_dummies(df.smoker),\n                     pd.get_dummies(df.region)], axis=1)","e0d5a37c":"df.head()","e300438d":"df.drop(['sex'], axis=1, inplace=True)\ndf.drop(['smoker'], axis=1, inplace=True)\ndf.drop(['region'], axis=1, inplace=True)","cea4ec70":"df.head()","1bb25027":"y = df[\"charges\"]\ndf.drop(['charges'], axis=1, inplace=True)","96326174":"df.isna().any()","76e29f09":"df.isnull().any()","444c2872":"from sklearn.model_selection import train_test_split\n\ntrain_set_x,  test_set_x, train_set_y,test_set_y = train_test_split(df, y, test_size = 0.2, random_state = 0)","7d7dafcb":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(train_set_x)  \ntrain_set_x = scaler.transform(train_set_x)  \ntest_set_x = scaler.transform(test_set_x)","2193ba1e":"print(train_set_y)","f6c8e84b":"print(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)\n","815efc8b":"train_set_y=train_set_y.values.reshape(train_set_y.shape[0],1)\ntest_set_y=test_set_y.values.reshape(test_set_y.shape[0],1)\nprint(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)","3c06df95":"def initialize(dim):\n    w = np.zeros((dim,1))\n    b = 0  \n    return w, b","c061cc95":"def propagate(w, b, X, Y):\n    m = X.shape[1]\n    # FORWARD \n    A = np.dot(w.T,X)+b \n    cost = (1\/(2*m))*np.sum((A - Y) ** 2)\n    # BACKWARD \n    dz = A-Y\n    dw = (1\/m)*np.dot(X,dz.T)\n    db = (1\/m)*np.sum(dz)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n\n    return grads, cost","5e30dde4":"def optimize(w, b, X, Y,print_cost, num_iterations, learning_rate):\n    costs = []\n    for i in range(num_iterations):\n        grads, cost = propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        w = w-learning_rate*dw\n        b = b-learning_rate*db\n\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","70cffe2e":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    Y_prediction = np.dot(w.T,X)+b\n    \n    return Y_prediction ","ce998510":"from sklearn.metrics import r2_score\ndef model(X_train, Y_train, X_test, Y_test,print_cost, num_iterations = 2000, learning_rate = 0.5 ):\n    w, b = initialize(X_train.shape[0])\n\n    parameters, grads, costs = optimize(w, b, X_train, Y_train,print_cost, num_iterations, learning_rate)\n\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    print(\"train accuracy: {} %\".format(r2_score(Y_train.T, Y_prediction_train.T)))\n    print(\"test accuracy: {} %\".format(r2_score(Y_test.T, Y_prediction_test.T)))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","228ca5d8":"d = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = True, num_iterations = 1000, learning_rate = 0.01)","3a6ddd8a":"learning_rates = [0.1, 0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = False, num_iterations = 2000, learning_rate = i)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","b3daf60a":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression().fit(train_set_x,train_set_y)\ny_train_pred = lr.predict(train_set_x)\ny_test_pred = lr.predict(test_set_x)\nprint(lr.score(train_set_x,train_set_y))\nprint(lr.score(test_set_x,test_set_y))","1cbcdac8":"from sklearn.linear_model import Ridge\nreg = Ridge(alpha=.5).fit(train_set_x,train_set_y)\ny_train_pred = lr.predict(train_set_x)\ny_test_pred = lr.predict(test_set_x)\nprint(reg.score(train_set_x,train_set_y))\nprint(reg.score(test_set_x,test_set_y))","e9b6c36f":"remove the old categorical data columns","a77e6f55":"optimizing the weights and bias using the gradieent at each iteration","65f85c43":"check if we have overfitted the results, using L2 regularization (Ridge)","9d6e12dd":"show data frame","5f40083f":"view data","a0726beb":"Calling the model","86c8c07c":"start the propagate, forward and backward to compute the activations\/Cost and gradients respictivly","e8e12a70":"Initialize weights and bias with zeros","92723055":"use the final weights and bias to predict the results for new unseen testing data","07ab26bd":"the main model Implementation","defeddf9":"view data shape","a01073ca":"use get dummies function to fo the hot encoding to convert categorical data to numirical one in a way that is not give extra weight for any above others ","5dc8e836":"show all categorical data that need to be handeled","70c7022c":"Read the csv features file","7af5278f":"scalling the data","4a7cc2f1":"reshape the labels to elemenate rank one arraies","d804a86f":"view data information","4a12a1fd":"Check the results with multible Learning Rate","bd718461":"**Import all needed libraries**","1f1c0459":"In this kernal I will solve the \"*Medical Cost Personal Datasets*\" using Linear Regression.\n\nI will use my own LR implementation vs Scikit learn one\n\n\n**FOR ANY QUESTIONS OR NEEDED EXPLANATIONS JUST COMMENT AND I WILL REPLY ASAP**\n","9a946f81":"big thanks for coursera and deeplearning.ai for the knowledge ","6b04dcd6":"scikit learn LR implementation","e844d2ff":"split data into training and testing","41acbd19":"use charges as target (labels) data and remove it from data frame\nconvert the Yes\/No results to 0\/1","c0ddefa4":"**My LR implementation start here:**\n\n","2c8cd2b0":"Our results and scikit learn results are almost the same\n","bff2f99e":"show data frame"}}