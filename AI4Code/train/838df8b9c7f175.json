{"cell_type":{"9cc5d94c":"code","0fc92696":"code","20275315":"code","3e09a03d":"code","a3e405b1":"code","78475ada":"code","5c6fc085":"code","3a480872":"code","add77717":"code","2e17bf17":"code","001c72d1":"code","5aed4061":"code","677da96f":"code","d2a81c8f":"code","a9929757":"code","90732982":"code","f67dd55d":"code","72f33360":"code","17653b3c":"code","181c61cb":"code","d02a5cb1":"code","ce312ab4":"code","58f3f46e":"code","d1ee31e6":"code","3ed7b87b":"code","975a1896":"code","3a1aa3d4":"code","011ba18e":"code","ef85eadb":"code","0a89dc77":"code","de042cb5":"code","4007f059":"code","16be6e1e":"code","1cbdc7ac":"code","e6e6cf9e":"code","c446a55e":"code","f921a8e1":"code","006bfaaa":"markdown","6d09394c":"markdown","ee651ab2":"markdown","15d8b31e":"markdown","820592e6":"markdown","6d35b199":"markdown","fdfb8942":"markdown","952c5321":"markdown","6328ccfa":"markdown","6eb8d322":"markdown","88d89142":"markdown","62215fca":"markdown","43086c3a":"markdown","748d9be3":"markdown","c84e728c":"markdown","10455a45":"markdown","85be9c46":"markdown","21ffdc09":"markdown","87399e10":"markdown","c0a6b7c0":"markdown","b3e868a4":"markdown","61fe7da6":"markdown","0c0afdec":"markdown"},"source":{"9cc5d94c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\").drop(columns = {\"Id\"})\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\").drop(columns = {\"Id\"})\ntrain.tail()","0fc92696":"#as expected 1460 rows and 80columns\nprint(train.shape)\nprint(test.shape)","20275315":"# no surprise, the train DataFrame has an additional variable : \"SalePrice\"\nprint(train.columns)\nprint(test.columns)","3e09a03d":"# Lets check out the unique values and number of missing values in our DataFrame\nfor i in train:\n    \n    print(\"Missing Values:\" + str(train[i].isnull().sum()))\n    print(train[i].value_counts())\n    print(\"*\"*50)\n# there are very problematic features with a very small range of possible values and on top many missing data (like Alley, FireplaceQu or PoolQC)","a3e405b1":"# lets check out the shape of our target variable distribution\nsns.distplot(train[\"SalePrice\"])","78475ada":"# in numbers:\nprint(train[\"SalePrice\"].skew()) \n#skewness of normal distribution is 0, between -0.5 and 0.5:the data is fairly symmetrical\nprint(train[\"SalePrice\"].kurt()) \n#kurtosis > 3: data includes heavy tails or outliers","5c6fc085":"# models work better with data in a more symmetric or gaussian distribution\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.distplot(train[\"SalePrice\"])\nprint(train[\"SalePrice\"].skew())\nprint(train[\"SalePrice\"].kurt())\n# looks much better :)","3a480872":"# how many missing values has each column?\ntotal_missing = train.isnull().sum().sort_values(ascending = False)\npercentage = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\nmiss_val = pd.concat([total_missing, percentage], axis=1, keys=['Total', 'Percent'])\nmiss_val.head(20)","add77717":"# in these columns NaN means \"None\" like no alley, no fireplace, no garage ect. and will be regarded as such\nstring_col = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MasVnrType\"]\nfor vars in string_col:\n    train[vars] = train[vars].fillna(\"None\")\n    test[vars] = test[vars].fillna(\"None\")\n\n# \"None\" as NaN makes no sense in the following columns\n# .mode()searches for frequent values in a column, [0] tells the function to use the first(most frequent) value!\n# fill the missing values of these other object-type columns with the most frequent value\ncommon_col = [\"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"KitchenQual\"]\nfor vars in common_col:\n    train[vars] = train[vars].fillna(train[vars].mode()[0])\n    test[vars] = test[vars].fillna(test[vars].mode()[0])\n\n#lets clear out what should\/could be 0 but is NaN instaead\nmissing_col = [\"GarageYrBlt\", \"GarageArea\", \"MasVnrArea\"]\nfor vars in missing_col:\n    train[vars] = train[vars].fillna(0)\n    test[vars] = test[vars].fillna(0)\n\n#for the feature LotFrontage I will just take the median\ntrain[\"LotFrontage\"] = train[\"LotFrontage\"].fillna(train[\"LotFrontage\"].median())\ntest[\"LotFrontage\"] = test[\"LotFrontage\"].fillna(test[\"LotFrontage\"].median())","2e17bf17":"# the above operations are base on examining the train-set.. make sure you emaxine your test-data as well\ntest.columns[test.isin([\"Nan\"]).any()]","001c72d1":"# lets have a look again:\n# how many missing values has each column?\ntotal_missing = train.isnull().sum().sort_values(ascending = False)\npercentage = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\nmiss_val = pd.concat([total_missing, percentage], axis=1, keys=['Total', 'Percent'])\nmiss_val.head(6)","5aed4061":"\n# be aware of the \".index\"! without it, the column names will be the index of the freshly created series-objects\nnumerical_col = train.dtypes[train.dtypes != \"object\"].index\nprint(\"Number of numerical features:\" + str(len(numerical_col)))\nobject_col = train.dtypes[train.dtypes == \"object\"].index\nprint(\"Number of object features:\" + str(len(object_col)))","677da96f":"# for a better overview of our numerical columns\nnumerical_col.tolist()\nprint(numerical_col)\nlen(numerical_col)","d2a81c8f":"numerical = train[numerical_col]\n\n# as mentioned, we have numerical features with \"categorical-like-values\" as OverallQual or YearSold etc..\n# ..these will be better visulized in plots\nfig, ax = plt.subplots(nrows=36, ncols=1, figsize = (8,6))\nsales = numerical[\"SalePrice\"]\nfor i, j in zip(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold'], ax):\n    numerical.plot(x = i, y = \"SalePrice\", kind=\"scatter\")\n# I dont know what went wrong with the very first subplot.. the following seem to be normal","a9929757":"# next I will test the correlation of the features to our target\ncorrelation_matrix = numerical.corr()\ncorrelation_matrix = correlation_matrix[\"SalePrice\"].sort_values(ascending = False).to_frame()\ncorrelation_matrix.reset_index().rename(columns = {\"index\": \"CorrColumn\", \"SalePrice\": \"Corr\"})","90732982":"WeakCorr = correlation_matrix[correlation_matrix[\"SalePrice\"] < 0.3]\nWeakCorr = WeakCorr.reset_index().rename(columns = {\"index\": \"WeakCorrCol\"})\nWeakCorr","f67dd55d":"WeakCorr_piv = WeakCorr.pivot(columns = \"WeakCorrCol\", values = \"SalePrice\" ).dropna()\nWeakCorr_piv = WeakCorr_piv.columns.tolist()\n#first of 3 lists","72f33360":"frequency = []\nfor cat in object_col:\n    max_freq = train.loc[:, cat].value_counts().max()\/train.shape[0]\n    frequency.append([cat, max_freq])\nfrequency\n\nfrequency = np.array(frequency)\ndf_frequency = pd.DataFrame(data = frequency, columns = [\"cat\", \"frequency\"])\ndf_frequency.sort_values(by = \"frequency\", ascending = False, inplace = True)\n\n\nplt.figure(figsize = (15,6))\nsns.barplot(x = df_frequency.cat, y = df_frequency[\"frequency\"].astype(np.float))\nplt.xticks(rotation = 75)","17653b3c":"# categorical values with a frequency-vaulue of >80% should not have much information for the target variable\n\ndiscard_cat = df_frequency[df_frequency[\"frequency\"].astype(np.float) > 0.80]\ndiscard_cat.head(24)","181c61cb":"discard_cat_piv = discard_cat.pivot(columns = \"cat\", values = \"frequency\").dropna()\ndiscard_cat_piv = discard_cat_piv.columns.tolist()\n#second of 3 lists","d02a5cb1":"frequency2 = []\nfor num in numerical_col:\n    max_freq2 = train.loc[:, num].value_counts().max()\/train.shape[0]\n    frequency2.append([num, max_freq2])\n\n\nfrequency2 = np.array(frequency2)\nfrequency2\n\n\nnum_frequency = pd.DataFrame(data = frequency2, columns = [\"num\", \"frequency\"])\nnum_frequency.head(10)","ce312ab4":"discard_num = num_frequency[num_frequency[\"frequency\"].astype(np.float) > 0.8]\ndiscard_num.head(15)","58f3f46e":"discard_num_piv = discard_num.pivot(columns = \"num\", values = \"frequency\").dropna()\ndiscard_num_piv = discard_num_piv.columns.tolist()\n#third and final list for my Feature Selection","d1ee31e6":"list(object_col)\ni = object_col\nfor i in train[i]:\n    object_data = train[object_col].join(train[\"SalePrice\"], how = \"outer\")\n    fig, ax = plt.subplots(figsize = (8,6))\n    ax = sns.boxplot(x = train[i], y = train[\"SalePrice\"], data = object_data)\n    plt.xticks(rotation = 75)\n\n# it runs awfully long and takes apparently a lot of the memory\n# if anyone has a better solution, please write a comment!!!","3ed7b87b":"piv_all = [[WeakCorr_piv, discard_num_piv, discard_cat_piv]]\nprint(piv_all)\nprint(\"*\"*50)\nfinal_list = list(set(WeakCorr_piv) | set(discard_num_piv) | set(discard_cat_piv))\nprint(final_list)","975a1896":"train.drop(final_list, axis = 1, inplace = True)\ntrain.head()\n\n# same with test\ntest.drop(final_list, axis = 1, inplace = True)","3a1aa3d4":"# for a label-encoding of specific columns I split the data in two different dataframes, because my function doesnt work with values not included in the following dictionary\ntrain_label = train[[\"BsmtExposure\", \"LotShape\", \"ExterQual\", \"MasVnrType\", \"GarageType\", \"BsmtFinType1\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"BsmtQual\", \"Foundation\", \"GarageFinish\"]]\ntrain_rest = train.drop(columns = {\"BsmtExposure\", \"LotShape\", \"ExterQual\", \"MasVnrType\", \"GarageType\", \"BsmtFinType1\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"BsmtQual\", \"Foundation\", \"GarageFinish\"})\n\n#same with test\ntest_label = test[[\"BsmtExposure\", \"LotShape\", \"ExterQual\", \"MasVnrType\", \"GarageType\", \"BsmtFinType1\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"BsmtQual\", \"Foundation\", \"GarageFinish\"]]\ntest_rest = test.drop(columns = {\"BsmtExposure\", \"LotShape\", \"ExterQual\", \"MasVnrType\", \"GarageType\", \"BsmtFinType1\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"BsmtQual\", \"Foundation\", \"GarageFinish\"})","011ba18e":"# I first tried to map different dictionaries on to our train and test data\nBsmtExposure_dict = {\"Gd\": 3, \"Av\": 2, \"Mn\": 1, \"No\": 0, \"None\": 0}\nLotShape_dict = {\"Reg\": 1, \"IR1\": 2, \"IR2\": 3, \"IR3\": 4}\nExterQual_dict = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1}\nMasVnrType_dict = {\"Stone\": 4, \"BrkFace\": 3, \"None\": 2, \"BrkCmn\": 1}\nGarageType_dict = {\"BuiltIn\": 6, \"Attchd\": 5, \"2Types\": 4, \"Basment\": 3, \"Detchd\": 2, \"CarPort\": 1, \"None\": 0}\nBsmtFinType1_dict = {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"None\": 0}\nHeatingQC_dict = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1}\nKitchenQual_dict = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1}\nFireplaceQu_dict = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1}\nBsmtQual_dict  = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1}\nFoundation_dict = {\"PConc\": 6, \"Stone\": 5, \"Wood\": 4, \"CBlock\": 3, \"BrkTil\": 2, \"Slab\": 1}\nGarageFinish_dict = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\n\n# but it didn\u00b4t work out so i decided to make one big dictionary\nmega_dict = {\"Gd\": 3, \"Av\": 2, \"Mn\": 1, \"No\": 0, \"None\": 0, \"Reg\": 1, \"IR1\": 2, \"IR2\": 3, \"IR3\": 4, \"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1, \"Stone\": 4, \"BrkFace\": 3, \"None\": 2, \"BrkCmn\": 1,\"BuiltIn\": 6, \"Attchd\": 5, \"2Types\": 4, \"Basment\": 3, \"Detchd\": 2, \"CarPort\": 1, \"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"PConc\": 6, \"Stone\": 5, \"Wood\": 4, \"CBlock\": 3, \"BrkTil\": 2, \"Slab\": 1, \"Fin\": 3, \"RFn\": 2, \"Unf\": 1}\n\ntrain_label = train_label.applymap(lambda x: mega_dict[x])\ntrain_label.head()\n\n#same with test\ntest_label = test_label.applymap(lambda x: mega_dict[x])","ef85eadb":"train = train_label.join(train_rest, how = \"outer\")\ntest = test_label.join(test_rest, how = \"outer\")","0a89dc77":"#get the residual object_type columns, make it a list and supply it for the dummy-encoding\ntrain_ohe = train.dtypes[train.dtypes == \"object\"].index\ntest_ohe = test.dtypes[test.dtypes == \"object\"].index\nprint(list(train_ohe))\nprint(list(test_ohe))","de042cb5":"#dummy-encoding\ntrain_dummy = pd.get_dummies(data = train, columns = train_ohe)\ntrain_dummy.head()\n\n#same for test\ntest_dummy = pd.get_dummies(data = test, columns = train_ohe)","4007f059":"#before we make a regression, we divide into target and data which is used to train the model\nX = train_dummy.drop(columns = \"SalePrice\")\ny = train_dummy.SalePrice","16be6e1e":"# next we try a multiple linear regression\n\nimport sklearn.linear_model as lm\nlin_reg = lm.LinearRegression()\nlin_reg.fit(X,y)\nlin_reg.score(X,y)","1cbdc7ac":"print(test_dummy.shape)\nprint(X.shape)\n# like this, lin_reg.predict(test_dummy) will end in an ValueError","e6e6cf9e":"# discarding the 4 additional columns (which is just a single dummy encoded value) in train, does not result in a bad score\nX = train_dummy.drop(columns = \"SalePrice\")[test_dummy.columns]\nlin_reg2 = lm.LinearRegression()\nlin_reg2.fit(X,y)\nlin_reg2.score(X,y)","c446a55e":"# lets try a Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=1000)\n\nmodel.fit(X,y)\nmodel.score(X,y)","f921a8e1":"# and a \nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n\nGBR.fit(X,y)\nGBR.score(X,y)","006bfaaa":"\nNow, what have I done before? \nI examined the categorical features and found some lead in the House Price competition description for the values (Handling Missing Data).\nAfter splitting my features and visualizing them, I selected my data based on a linear correlation-coefficient >0.3 and on feature importance by level frequency.\n\nWe can now go on to the last two steps before we launch the ML-model.\nI want to try differendt regression models and need to convert the categorical features into discret numerical data. Step1: I came up with my own logic (always at least relying on the data description) and created a dictionary (label encoding). The transcription of some cat-features was easy as they have a clear ordinal logic. For others I used boxplots to give them values. Guys, critism is highly recommended -> I really appriciate constructive advice so feel free!\nStep2: The residual categorical features had to be dummy-encoded, because ordering them is beyond my logic.\n\n","6d09394c":"## 7. Applying different ML models","ee651ab2":"### 4.1 Logic for Visualization","15d8b31e":"Before we do a Label Encoding, I want to get a better perspective of the categorical features","820592e6":"After the numerical features we have to deal with categorical features.\nAn interesting approach to deal with categorical features, in particular, dimensionality reduction, is to drop columns with a low \"feature importance by level frequencies\".\nIts a method I first saw in an Notebook from a follow Kaggler, Allunia.\nFeature importance is defined by measuring the frequency of the most common feature-value.\nThe higher the value the less information the feature can offer (like lack of diversity).","6d35b199":"## 5. Application of Feature Selection","fdfb8942":"### 4.3 Visualization of Categorical Features","952c5321":"### 2.2 Target Variable - SalePrice skewness & kurtosis","6328ccfa":"Now, with my three list, I can easily remove those features from the data sets which I claimed to be negligible.","6eb8d322":"### 4.2 Visualization of Numerical Features","88d89142":"This is my Agenda:\n\n1. Importing the first libraries and data\n\n2. Data Exploration\n    2.1 Rough structure of the Train-set and test-set\n    2.2 Target Variable - SalePrice Skew & kurtosis analysis\n3. Missing data and a bit of Feature Engineering\n4. Visualisation and Logic for Feature Selection\n    4.1 Logic for Visualization\n    4.2 Visualization of Numerical Features\n    4.3 Visualization of Categorical Features\n5. Application of Feature Selection\n6. Final Feature Engineering\n    6.1 Label Encoding\n    6.2 Dummy Encoding\n7. Applying different ML models\n","62215fca":"## 4. Visualisation and Logic for Feature Selection","43086c3a":"Next, I use the same logic of feature importance by level frequency for my numerical features as well.","748d9be3":"## 6. Final Feature Engineering","c84e728c":"### 6.1 Label Encoding","10455a45":"# Introduction and Agenda","85be9c46":"Lets take a closer look on weakly (linear) correlated features in our numerical features.\nI decided to label them \"weak\" after Bruce Ratner (2009) who writes that the accepted guidelines for interpreting the correlation coefficient include: <0.3 for a weak correlation coefficient, 0.3<0.7 for a moderate corr. coefficient and 0.7< for a strong corr. coefficient","21ffdc09":"From the boxplots of chapter 4.3 I derived a ordinal logic which I next used for label encoding","87399e10":"### 6.2 Dummy Encoding","c0a6b7c0":"> ## 1. Importing the first libraries and data","b3e868a4":"> ## 2. Data Exploration\n### 2.1 Rough structure of the train-set and test-set","61fe7da6":"For relatoinship-visualization it is very helpful to distinguish into numerical and object type.\nWe will later see that even this distinction is not absolut sufficient because we have numerical features with categorical-like-values as \"OverallQual\".","0c0afdec":"### 3. Missing data and a bit of Feature Engineering"}}