{"cell_type":{"b0c3cb42":"code","d143b81f":"code","bcf537b3":"code","bfd897e7":"code","6384ef81":"code","4572a276":"code","91324d30":"code","e01e9766":"code","9a08340a":"code","76f59cef":"code","b1e17d9f":"code","a2ef6cc5":"code","1defd919":"code","725704fd":"code","2d523f95":"code","99d7a906":"code","af32a92d":"code","c54037f9":"code","ec505f25":"code","dee546af":"code","e2f12ad4":"code","084eb8c0":"code","7934ef46":"code","942a4149":"code","dac7c6eb":"code","10324885":"code","2d7c691a":"code","a6100ced":"code","85d09e1b":"code","adc91655":"code","935c49ce":"code","8767a8e7":"code","2d93b20f":"code","9de79fc4":"code","60b9c772":"code","a4a5d311":"code","87782437":"code","3de3fa84":"code","1317ab1a":"code","0eb5b343":"code","c0fb8dd2":"code","9f0dc64e":"code","74aa01a1":"code","dd4b3c4c":"code","e0ee2bf6":"code","b597deae":"code","a0174585":"code","d4d5f1c6":"code","f18c1595":"code","bdbb9c87":"code","6f18a16f":"code","eefbecdf":"code","5b187278":"code","0f35927e":"code","d6a091e8":"code","1c50f96b":"code","a208b51c":"code","165a7047":"code","8f1f3fb9":"code","27bf2d68":"code","18458dd3":"code","26b9485c":"code","86b4baf4":"code","4de883dd":"code","3c032721":"code","4555caba":"code","5b7e1e8a":"code","f1be5473":"code","2ee4db91":"code","88ad0a99":"code","488ed6fe":"code","283df3b1":"code","5b654a89":"code","62254c33":"code","25b3f26e":"code","3ea92a7a":"code","1c4b72b5":"code","f0c26c01":"code","21667e1c":"code","fcae7b21":"code","5c8af84b":"markdown","ea4a4776":"markdown","dfb8e25c":"markdown","493a24b4":"markdown","576c3490":"markdown","5125a6a5":"markdown","eec1a5cf":"markdown","d7c0d978":"markdown","d12a610a":"markdown","2e2de847":"markdown","a2062298":"markdown","54c11640":"markdown","41c32c9f":"markdown","01c72211":"markdown"},"source":{"b0c3cb42":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\nimport string\nimport warnings\nimport time\nimport imageio\nimport random\nfrom scipy.stats import itemfreq\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nimport operator\nfrom tqdm import tqdm, tqdm_notebook\nfrom collections import defaultdict\nfrom PIL import Image\nfrom skimage import feature as SKImageFeature\nfrom IPython.core.display import HTML\nfrom IPython.display import Image as Image2\nfrom contextlib import contextmanager\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n%matplotlib inline\n\nnp.random.seed(seed=1337)\nwarnings.filterwarnings('ignore')\n# progress bar\ntqdm.pandas(tqdm_notebook)\n\nfrom sklearn.preprocessing import Imputer\n\n\nsplit_char = '\/'","d143b81f":"os.listdir('..\/input')","bcf537b3":"train = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/sample_submission.csv')","bfd897e7":"@contextmanager\ndef timer(task_name=\"timer\"):\n    # a timer cm from https:\/\/www.kaggle.com\/lopuhin\/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    print(\"----{} started\".format(task_name))\n    t0 = time.time()\n    yield\n    print(\"----{} done in {:.0f} seconds\".format(task_name, time.time() - t0))","6384ef81":"debug = False\nif debug:\n    train = train[:500]\n    test = test[:200]","4572a276":"train.columns","91324d30":"import cv2\nimport os\nfrom keras.applications.densenet import preprocess_input, DenseNet121","e01e9766":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","9a08340a":"img_size = 256\nbatch_size = 256","76f59cef":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","b1e17d9f":"pet_ids = train['PetID'].values\nn_batches = int(np.ceil(len(pet_ids) \/ batch_size))\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","a2ef6cc5":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]","1defd919":"pet_ids = test['PetID'].values\nn_batches = int(np.ceil(len(pet_ids) \/ batch_size))\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","725704fd":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]","2d523f95":"train_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","99d7a906":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\nall_ids.shape","af32a92d":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures_df = pd.concat([train_feats, test_feats], axis=0)\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\nimg_features = pd.concat([all_ids, svd_col], axis=1)","c54037f9":"labels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\nlabels_state = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","ec505f25":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\nif debug:\n    train_image_files = train_image_files[:1000]\n    train_metadata_files = train_metadata_files[:1000]\n    train_sentiment_files = train_sentiment_files[:1000]\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","dee546af":"# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ train_df_ids.shape[0]:.3f}')","e2f12ad4":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ test_df_ids.shape[0]:.3f}')","084eb8c0":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    sentiment_filename = f'..\/input\/petfinder-adoption-prediction\/{mode}_sentiment\/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    for ind in range(1,200):\n        metadata_filename = '..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-{}.json'.format(mode, pet_id, ind)\n        try:\n            metadata_file = pet_parser.open_json_file(metadata_filename)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        except FileNotFoundError:\n            break\n    if dfs_metadata:\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    return dfs\n\n\npet_parser = PetFinderParser()","7934ef46":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","942a4149":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","dac7c6eb":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","10324885":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","2d7c691a":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","a6100ced":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","85d09e1b":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","adc91655":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","935c49ce":"# state GDP: https:\/\/en.wikipedia.org\/wiki\/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https:\/\/en.wikipedia.org\/wiki\/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\n\nX_temp[\"state_gdp\"] = X_temp.State.map(state_gdp)\nX_temp[\"state_population\"] = X_temp.State.map(state_population)","8767a8e7":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","2d93b20f":"len(np.asarray(np.array([[1,2,3],[4,5,6]])).shape)","9de79fc4":"def color_analysis(img):\n    # obtain the color palatte of the image \n    palatte = defaultdict(int)\n    for pixel in img.getdata():\n        palatte[pixel] += 1\n    \n    # sort the colors present in the image \n    sorted_x = sorted(palatte.items(), key=operator.itemgetter(1), reverse = True)\n    light_shade, dark_shade, shade_count, pixel_limit = 0, 0, 0, 25\n    for i, x in enumerate(sorted_x[:pixel_limit]):\n        if all(xx <= 20 for xx in x[0][:3]): ## dull : too much darkness \n            dark_shade += x[1]\n        if all(xx >= 240 for xx in x[0][:3]): ## bright : too much whiteness \n            light_shade += x[1]\n        shade_count += x[1]\n        \n    light_percent = round((float(light_shade)\/shade_count)*100, 2)\n    dark_percent = round((float(dark_shade)\/shade_count)*100, 2)\n    return light_percent, dark_percent\n\ndef perform_color_analysis(im, flag):\n    # cut the images into two halves as complete average may give bias results\n    size = im.size\n    halves = (size[0]\/2, size[1]\/2)\n    im1 = im.crop((0, 0, size[0], halves[1]))\n    im2 = im.crop((0, halves[1], size[0], size[1]))\n\n    try:\n        light_percent1, dark_percent1 = color_analysis(im1)\n        light_percent2, dark_percent2 = color_analysis(im2)\n    except Exception as e:\n        return np.NaN\n\n    light_percent = (light_percent1 + light_percent2)\/2 \n    dark_percent = (dark_percent1 + dark_percent2)\/2 \n    if flag == 'black':\n        return dark_percent\n    elif flag == 'white':\n        return light_percent\n    else:\n        return np.NaN\n\ndef get_blurrness_score(image):\n    image = np.asarray(image)\n    if len(image.shape) == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n    return fm\n\ndef average_pixel_width(im):\n    im_array = np.asarray(im.convert(mode='L'))\n    edges_sigma1 = SKImageFeature.canny(im_array, sigma=3)\n    apw = (float(np.sum(edges_sigma1)) \/ (im.size[0]*im.size[1]))\n    return apw * 100\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(img):\n    return img.size\n\ndef get_brightness(img_path):\n    try:\n        img = cv2.imread(img_path, 0)\n        return np.mean(img)\n    except Exception as e:\n        return np.NaN\n    \ndef get_dominant_color(img):\n    img = np.asarray(img)\n    arr = np.float32(img)\n    pixels = arr.reshape((-1, 3))\n\n    n_colors = 5\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n    flags = cv2.KMEANS_RANDOM_CENTERS\n    _, labels, centroids = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n\n    palette = np.uint8(centroids)\n    quantized = palette[labels.flatten()]\n    quantized = quantized.reshape(img.shape)\n\n    dominant_color = palette[np.argmax(itemfreq(labels)[:, -1])]\n    return dominant_color","60b9c772":"from PIL import Image\nreplace_backslash = lambda x: x.replace('\\\\', '\/')\nvectorized_replace_backslash = np.vectorize(replace_backslash)\n\naggs = {\n    'image_blurness': ['sum', 'mean', 'var'],\n    'image_size': ['sum', 'mean', 'var'],\n    'image_width': ['sum', 'mean', 'var'],\n    'image_height': ['sum', 'mean', 'var'],\n}","a4a5d311":"def get_image_features(img_info):\n    img_info[\"image_size\"] = getSize(img_info['image_filename'])\n    \n    try:\n        im = Image.open(img_info['image_filename'])\n    except Exception as e:\n        im = np.NaN\n    \n    if im != np.NaN:\n        dimension = getDimensions(im)\n        img_info[\"image_width\"] = dimension[0]\n        img_info[\"image_height\"] = dimension[1]\n        img_info[\"image_blurness\"] = get_blurrness_score(im)\n    else:\n        img_info[\"image_width\"] = np.NaN\n        img_info[\"image_height\"] = np.NaN\n        img_info[\"image_blurness\"] = np.NaN\n    \n    return img_info","87782437":"train_df_ids = train[['PetID']]\ntrain_image_files = vectorized_replace_backslash(train_image_files)\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_image_features = train_df_imgs.assign(PetID=train_imgs_pets)\n\nwith timer(\"getting image extra image features for train: \"):\n    train_image_features = train_image_features.apply(get_image_features, axis=1)","3de3fa84":"train_image_features","1317ab1a":"agg_train_imgs = train_image_features.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()","0eb5b343":"test_df_ids = test[['PetID']]\ntest_image_files = vectorized_replace_backslash(test_image_files)\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_image_features = test_df_imgs.assign(PetID=test_imgs_pets)\n\nwith timer(\"getting image extra image features for test: \"):\n    test_image_features = test_image_features.apply(get_image_features, axis=1)","c0fb8dd2":"agg_test_imgs = test_image_features.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()","9f0dc64e":"agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)\n\nagg_imgs_columns = list(agg_imgs.columns)\nagg_imgs_columns.remove('PetID')","74aa01a1":"agg_imgs.head(5)","dd4b3c4c":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')","e0ee2bf6":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","b597deae":"n_components = 32 # increase num components to 32\ntext_features = []\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    \n    svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))\n\n    current_text = X_temp[i].str.lower().astype(str)\n    X_temp[i] = X_temp[i].astype(str)\n    X_temp[i + '_num_chars'] = current_text.apply(len) # Count number of Characters\n    X_temp[i + '_num_words'] = current_text.apply(lambda comment: len(comment.split())) # Count number of Words\n    X_temp[i + '_num_unique_words'] = current_text.apply(lambda comment: len(set(w for w in comment.split())))\n    X_temp[i + '_words_vs_unique'] = X_temp[i + '_num_unique_words'] \/ X_temp[i+'_num_words'] * 100 # Count Unique Words\n    X_temp[i + '_count_letters'] = current_text.apply(lambda x: len(str(x)))\n    X_temp[i + \"_count_punctuations\"] = current_text.apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    X_temp[i + \"_mean_word_len\"] = current_text.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n        \n    text_features.append(svd_col)\n    \nX_temp['words_vs_unique_description'] = X_temp['Description_num_unique_words'] \/ X_temp['Description_num_words'] * 100\nX_temp['words_vs_unique_description'] = X_temp['Description_num_unique_words'] \/ X_temp['Description_num_words'] * 100\nX_temp['len_description'] = X_temp['Description'].apply(lambda x: len(x)) \nX_temp['average_description_word_length'] = X_temp['len_description'] \/ X_temp['Description_num_words']","a0174585":"embeddings_index = {}\ntime_start_load_embedding = time.time()\nwith open('..\/input\/fatsttext-common-crawl\/crawl-300d-2M\/crawl-300d-2M.vec', encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    print(\"finished loading embeddings file in: \", time.time() - time_start_load_embedding)","d4d5f1c6":"def sum_embeddings_vector(x):\n    sum_bag_of_words = np.zeros(300)\n    words = x.split(' ')\n    count = 0\n    for w in words:\n        if w in embeddings_index:\n            sum_bag_of_words += embeddings_index.get(w)\n            count += 1\n    if count > 0:\n        sum_bag_of_words \/= count\n    return pd.Series(sum_bag_of_words)","f18c1595":"svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\nfor i in X_text.columns:\n    emb_feat = X_text[i].apply(sum_embeddings_vector)\n    emb_feat = svd_.fit_transform(emb_feat)\n    emb_feat = pd.DataFrame(emb_feat)\n    emb_feat = emb_feat.add_prefix('word_embedding_' + i + '_')\n    text_features.append(emb_feat)","bdbb9c87":"text_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)","6f18a16f":"for i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","eefbecdf":"rescuer_ids = X_temp['RescuerID'].values","5b187278":"X_temp = X_temp.drop(to_drop_columns, axis=1)","0f35927e":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)\nrescuer_ids = rescuer_ids[:len(X_train)]","d6a091e8":"assert len(rescuer_ids) == len(X_train)","1c50f96b":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","a208b51c":"X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","165a7047":"X_train_non_null.shape, X_test_non_null.shape","8f1f3fb9":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","27bf2d68":"# put numerical value to one of bins\ndef to_bins(x, borders):\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _loss(self, coef, X, y, idx):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -quadratic_weighted_kappa(y, X_p)\n        return ll\n\n    def fit(self, X, y):\n        coef = [1.5, 2.0, 2.5, 3.0]\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n        for it1 in range(10):\n            for idx in range(4):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                coef[idx] = a\n                la = self._loss(coef, X, y, idx)\n                coef[idx] = b\n                lb = self._loss(coef, X, y, idx)\n                for it in range(20):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        coef[idx] = a\n                        la = self._loss(coef, X, y, idx)\n                    else:\n                        b = b - (b - a) * golden2\n                        coef[idx] = b\n                        lb = self._loss(coef, X, y, idx)\n        self.coef_ = {'x': coef}\n\n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","18458dd3":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold","26b9485c":"ntrain = X_train_non_null.shape[0]\nntest = X_test_non_null.shape[0]\nNFOLDS = 10","86b4baf4":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","4de883dd":"def get_oof(clf, X, y, X_test, groups):    \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(stratified_group_k_fold(X, y, rescuer_ids, NFOLDS, 1337)):\n        print('Training for fold: ', i + 1)\n        \n        x_tr = X.iloc[train_index, :]\n        y_tr = y[train_index]\n        x_te = X.iloc[test_index, :]\n        y_te = y[test_index]\n\n        clf.train(x_tr, y_tr, x_val=x_te, y_val=y_te)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(X_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","3c032721":"class SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train, **kwargs):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    \nclass XgbWrapper(object):\n    def __init__(self, params=None):\n        self.param = params\n        self.nrounds = params.pop('nrounds', 60000)\n        self.early_stop_rounds = params.pop('early_stop_rounds', 2000)\n\n    def train(self, x_train, y_train, **kwargs):\n        dtrain = xgb.DMatrix(x_train, label=y_train)\n        dvalid = xgb.DMatrix(data=kwargs['x_val'], label=kwargs['y_val'])\n        \n        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n        \n        self.model = xgb.train(dtrain=dtrain, num_boost_round=self.nrounds, evals=watchlist, early_stopping_rounds=self.early_stop_rounds, \n                               verbose_eval=1000, params=self.param)\n\n    def predict(self, x):\n        return self.model.predict(xgb.DMatrix(x), ntree_limit=self.model.best_ntree_limit)\n\n    \nclass LGBWrapper(object):\n    def __init__(self, params=None):\n        self.param = params\n        self.num_rounds = params.pop('nrounds', 60000)\n        self.early_stop_rounds = params.pop('early_stop_rounds', 2000)\n\n    def train(self, x_train, y_train, **kwargs):\n        dtrain = lgb.Dataset(x_train, label=y_train)\n        dvalid = lgb.Dataset(kwargs['x_val'], label=kwargs['y_val'])\n\n        watchlist = [dtrain, dvalid]\n        \n        print('training LightGBM with params: ', self.param)\n        self.model = lgb.train(\n                  self.param,\n                  train_set=dtrain,\n                  num_boost_round=self.num_rounds,\n                  valid_sets=watchlist,\n                  verbose_eval=1000,\n                  early_stopping_rounds=self.early_stop_rounds\n        )\n\n    def predict(self, x):\n        return self.model.predict(x, num_iteration=self.model.best_iteration)","4555caba":"# LightGBM\n\nlgbm_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'nrounds': 50000,\n    'early_stop_rounds': 2000,\n    # trainable params\n    'max_depth': 4,\n    'num_leaves': 46,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 8,\n    'learning_rate': 0.019,\n    'verbose': 0\n}\nlgb_wrapper = LGBWrapper(lgbm_params)\n\nwith timer('Training LightGBM'):\n    lgb_oof_train, lgb_oof_test = get_oof(lgb_wrapper, X_train_non_null.drop(['AdoptionSpeed'], axis=1), X_train_non_null['AdoptionSpeed'].values.astype(int), X_test_non_null, groups=rescuer_ids)","5b7e1e8a":"# Random Forrest\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 150,\n    'max_features': 'auto',\n    'max_depth': 20,\n    'min_samples_leaf': 2,\n}\n\nrf = SklearnWrapper(clf=RandomForestRegressor, seed=1337, params=rf_params)\nwith timer('Training Random Forest'):\n    rf_oof_train, rf_oof_test = get_oof(rf, X_train_non_null.drop(['AdoptionSpeed'], axis=1), X_train_non_null['AdoptionSpeed'].values.astype(int), X_test_non_null, groups=rescuer_ids)","f1be5473":"# extra trees\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 150,\n    'max_features': 'auto',\n    'max_depth': 50,\n    'min_samples_leaf': 2,\n}\net = SklearnWrapper(clf=ExtraTreesRegressor, seed=1337, params=et_params)\n\nwith timer('Training Extra trees'):\n    et_oof_train, et_oof_test = get_oof(et, X_train_non_null.drop(['AdoptionSpeed'], axis=1), X_train_non_null['AdoptionSpeed'].values.astype(int), X_test_non_null, groups=rescuer_ids)","2ee4db91":"# # xgboost\nxgb_params = {\n    'eval_metric': 'rmse',\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n    'seed': 1337,\n    'nrounds': 60000,\n    'early_stop_rounds': 2000,\n    # trainable params\n    'eta': 0.025,\n    'subsample': 0.8,\n    'colsample_bytree': 0.6000000000000001,\n    'gamma': 0.65,\n    'max_depth': 4,\n    'min_child_weight': 5.0,\n    'n_estimators': 1000,\n}\n\nxgbWrapper = XgbWrapper(xgb_params)\n\nwith timer('Training Xgboost'):\n    xgb_oof_train, xgb_oof_test = get_oof(xgbWrapper, X_train_non_null.drop(['AdoptionSpeed'], axis=1), X_train_non_null['AdoptionSpeed'].values.astype(int), X_test_non_null, groups=rescuer_ids)","88ad0a99":"x_train_ensemble = np.concatenate((et_oof_train, rf_oof_train, lgb_oof_train, xgb_oof_train), axis=1)\nx_test_ensemble = np.concatenate((et_oof_test, rf_oof_test, lgb_oof_test, xgb_oof_test), axis=1)","488ed6fe":"# Stack, combine and train ridge regressor\n\nridge_params = {\n    'alpha':50.0, \n    'fit_intercept':True, \n    'normalize':False, \n    'copy_X':True,\n    'max_iter':None, \n    'tol':0.001, \n    'solver':'auto', \n    'random_state':1337\n}\nridge = SklearnWrapper(clf=Ridge, seed=1337, params=ridge_params)\n\nfinal_oof_train = np.zeros((ntrain,))\nfinal_oof_test = np.zeros((ntest,))\nfinal_oof_test_skf = np.empty((NFOLDS, ntest))\n\nfor i, (train_index, test_index) in enumerate(stratified_group_k_fold(x_train_ensemble, X_train_non_null['AdoptionSpeed'].values.astype(int), rescuer_ids, NFOLDS, 1337)):\n    print('Training ridge regressor for fold: ', i + 1)\n    \n    x_tr = x_train_ensemble[train_index]\n    y_tr = X_train_non_null['AdoptionSpeed'].values[train_index]\n    x_te = x_train_ensemble[test_index]\n    y_te = X_train_non_null['AdoptionSpeed'].values[test_index]\n    \n    print(x_tr.shape, y_tr.shape, x_te.shape, y_te.shape)\n\n    ridge.train(x_tr, y_tr, x_val=x_te, y_val=y_te)\n    \n    final_oof_train[test_index] = ridge.predict(x_te)\n    final_oof_test_skf[i, :] = ridge.predict(x_test_ensemble)\n\nfinal_oof_test[:] = final_oof_test_skf.mean(axis=0)\nfinal_oof_train = final_oof_train.reshape(-1, 1)\nfinal_oof_test = final_oof_test.reshape(-1, 1)","283df3b1":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})","5b654a89":"plot_pred(final_oof_train)","62254c33":"plot_pred(final_oof_test.mean(axis=1))","25b3f26e":"optR = OptimizedRounder()\noptR.fit(final_oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(final_oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)","3ea92a7a":"gc.collect()","1c4b72b5":"coefficients_ = coefficients.copy()\ntrain_predictions = optR.predict(final_oof_train, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(final_oof_test, coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","f0c26c01":"Counter(train_predictions)","21667e1c":"Counter(test_predictions)","fcae7b21":"submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","5c8af84b":"## About metadata and sentiment","ea4a4776":"## Image features","dfb8e25c":"### Add various image features","493a24b4":"### Test","576c3490":"### Drop ID, name and rescuerID","5125a6a5":"### Merge image features","eec1a5cf":"### OptimizeRounder from [OptimizedRounder() - Improved](https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved)","d7c0d978":"### Malaysia State GDP Feature","d12a610a":"### Train","2e2de847":"## Train model","a2062298":"## Extract features from json","54c11640":"### TFIDF + Meta text features","41c32c9f":"### group extracted features by PetID:","01c72211":"### merge processed DFs with base train\/test DF:"}}