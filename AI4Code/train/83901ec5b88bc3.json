{"cell_type":{"dfa606f5":"code","5ca93e88":"code","8c6f0705":"code","e65fc09a":"code","5a2147d6":"code","69464150":"code","84bb8821":"code","7d63d14d":"code","a763495c":"code","ddf70458":"code","1fe18975":"code","dec17626":"code","2e07e323":"code","9df894cc":"code","a3df45cb":"code","391fdf06":"code","898e4610":"code","85d9d45c":"code","81b0bb88":"code","2b4135c5":"code","442282e6":"code","d0249ae8":"code","08b42147":"code","a6e3aec4":"code","79578e72":"code","a608cd66":"code","5e8b64e8":"code","270901e6":"code","8c5c26c2":"code","a1e678f5":"code","771fdb57":"code","d3ac1869":"code","b54a1b7f":"code","3d9ba723":"code","ae9d10f2":"code","ff36f778":"markdown","09f30101":"markdown","f56700b1":"markdown","ce3a80bf":"markdown","8757d2bd":"markdown","5f348d5e":"markdown","903b9c1e":"markdown","1917aa41":"markdown"},"source":{"dfa606f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n!pip install scikit-learn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n    roc_auc_score, confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ca93e88":"df = pd.read_csv(\"\/kaggle\/input\/diabetes-data-set\/diabetes.csv\")","8c6f0705":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20): \n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list","e65fc09a":"check_df(df)","5a2147d6":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","69464150":"# Analysis of Categorical Variables\ncat_summary(df, \"Outcome\")","84bb8821":"# Analysis of Numerical Variables\nfor col in num_cols:\n     num_summary(df, col, plot=True)","7d63d14d":"# Analysis of Numerical Variables Based on Target\nfor col in num_cols:\n     target_summary_with_num(df, \"Outcome\", col)","a763495c":"# Examining Correlations\ndf.corr()","ddf70458":"# Correlation Matrix\nf, ax = plt.subplots(figsize=[20, 15])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"magma\")\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","1fe18975":"# Distribution of Dependent Variable\nsns.countplot('Outcome', data=df)\nplt.show()","dec17626":"def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\ndef check_outlier(dataframe, col_name, q1=0.25, q3=0.75):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\n\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers\n\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\n\ndef missing_vs_target(dataframe, target, na_columns):\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    rare_columns = [ col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) < 0.01).sum() > 1]\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n    return dataframe","2e07e323":"zero_columns = [col for col in df.columns if (df[col].min() == 0 and col not in [\"Pregnancies\", \"Outcome\"])]\n\n# We went to each of the stored variables and recorded the observation values containing 0 as 0\nfor col in zero_columns:\n     df[col] = np.where(df[col] == 0, np.nan, df[col])","9df894cc":"# Missing Observation Query\ndf.isnull().sum()","a3df45cb":"na_columns = missing_values_table(df, na_name=True)\n\nmissing_vs_target(df, \"Outcome\", na_columns)","391fdf06":"# Filling the Missing Observations in Categorical Variable Breakdown\ndef median_target(col):\n     temp = df[df[col].notnull()]\n     temp = temp[[col, 'Outcome']].groupby(['Outcome'])[[col]].median().reset_index()\n     return temp\n\nfor col in zero_columns:\n     df.loc[(df['Outcome'] == 0) & (df[col].isnull()), col] = median_target(col)[col][0]\n     df.loc[(df['Outcome'] == 1) & (df[col].isnull()), col] = median_target(col)[col][1]\n\ndf.isnull().sum()","898e4610":"# Outlier Analysis and Suppression Process\nfor col in df.columns:\n     print(col, check_outlier(df, col))\n     if check_outlier(df, col):\n         replace_with_thresholds(df, col)\n","85d9d45c":"check_df(df)","81b0bb88":"# Let's divide the age variable into categories and create a new age variable\ndf.loc[(df[\"Age\"] >= 21) & (df[\"Age\"] < 50), \"NEW_AGE_CAT\"] = \"mature\"\ndf.loc[(df[\"Age\"] >= 50), \"NEW_AGE_CAT\"] = \"senior\"\n\n# BMI below 18.5 is underweight, between 18.5 and 24.9 is normal, 30 and above is obese\ndf['NEW_BMI'] = pd.cut(x=df['BMI'], bins=[0, 18.5, 24.9, 29.9, 100],\n                       labels=[\"Underweight\", \"Healthy\", \"Overweight\", \"Obese\"])\n\n# Convert glucose value to categorical variable\ndf[\"NEW_GLUCOSE\"] = pd.cut(x=df[\"Glucose\"], bins=[0, 140, 200, 300], labels=[\"Normal\", \"Prediabetes\", \"Diabetes\"])\n\n# Creating a categorical variable by considering age and body mass index together\ndf.loc[(df[\"BMI\"] < 18.5) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_BMI_NOM\"] = \"underweightmature\"\ndf.loc[(df[\"BMI\"] < 18.5) & (df[\"Age\"] >= 50), \"NEW_AGE_BMI_NOM\"] = \"underweightsenior\"\n\ndf.loc[((df[\"BMI\"] >= 18.5) & (df[\"BMI\"] < 25)) & (\n        (df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_BMI_NOM\"] = \"healthymature\"\ndf.loc[((df[\"BMI\"] >= 18.5) & (df[\"BMI\"] < 25)) & (df[\"Age\"] >= 50), \"NEW_AGE_BMI_NOM\"] = \"healthysenior\"\n\ndf.loc[((df[\"BMI\"] >= 25) & (df[\"BMI\"] < 30)) & (\n        (df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_BMI_NOM\"] = \"overweightmature\"\ndf.loc[((df[\"BMI\"] >= 25) & (df[\"BMI\"] < 30)) & (df[\"Age\"] >= 50), \"NEW_AGE_BMI_NOM\"] = \"overweightsenior\"\n\ndf.loc[(df[\"BMI\"] > 18.5) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_BMI_NOM\"] = \"obesemature\"\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"Age\"] >= 50), \"NEW_AGE_BMI_NOM\"] = \"obesesenior\"\n\n# Creating a categorical variable by considering age and glucose values \u200b\u200btogether\ndf.loc[(df[\"Glucose\"] < 70) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_GLUCOSE_NOM\"] = \"lowmature\"\ndf.loc[(df[\"Glucose\"] < 70) & (df[\"Age\"] >= 50), \"NEW_AGE_GLUCOSE_NOM\"] = \"lowsenior\"\n\ndf.loc[((df[\"Glucose\"] >= 70) & (df[\"Glucose\"] < 100)) & (\n        (df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_GLUCOSE_NOM\"] = \"normalmature\"\ndf.loc[((df[\"Glucose\"] >= 70) & (df[\"Glucose\"] < 100)) & (df[\"Age\"] >= 50), \"NEW_AGE_GLUCOSE_NOM\"] = \"normalsenior\"\n\ndf.loc[((df[\"Glucose\"] >= 100) & (df[\"Glucose\"] <= 125)) & (\n        (df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_GLUCOSE_NOM\"] = \"hiddenmature\"\ndf.loc[((df[\"Glucose\"] >= 100) & (df[\"Glucose\"] <= 125)) & (df[\"Age\"] >= 50), \"NEW_AGE_GLUCOSE_NOM\"] = \"hiddensenior\"\n\ndf.loc[(df[\"Glucose\"] > 125) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"NEW_AGE_GLUCOSE_NOM\"] = \"highmature\"\ndf.loc[(df[\"Glucose\"] > 125) & (df[\"Age\"] >= 50), \"NEW_AGE_GLUCOSE_NOM\"] = \"highsenior\"\n\n\n# Derive Categorical Variable with Insulin Value\ndef set_insulin(dataframe, col_name=\"Insulin\"):\n    if 16 <= dataframe[col_name] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"\n\n\ndf[\"NEW_INSULIN_SCORE\"] = df.apply(set_insulin, axis=1)\n\n# Enlarging the columns\ndf.columns = [col.upper() for col in df.columns]\n\ncheck_df(df)","2b4135c5":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","442282e6":"# LABEL ENCODING\n\nbinary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and df[col].nunique() == 2]","d0249ae8":"for col in binary_cols:\n    df = label_encoder(df, col)","08b42147":"df.head()","a6e3aec4":"# ONE-HOT ENCODING\n\ndf = pd.get_dummies(df, drop_first=True)\ndf.head()","79578e72":"y = df[\"OUTCOME\"]\nX = df.drop(\"OUTCOME\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nlgr = LogisticRegression(solver='liblinear')\nlgr_model = lgr.fit(X_train, y_train)","a608cd66":"# TRAIN ERROR\ny_pred = lgr_model.predict(X_train)\n\n# Accuracy\naccuracy_score(y_train, y_pred)\n","5e8b64e8":"print(classification_report(y_train, y_pred))","270901e6":"# TEST ERROR\ny_pred = lgr_model.predict(X_test)\ny_prob = lgr_model.predict_proba(X_test)[:, 1]\n\n# Accuracy\naccuracy_score(y_test, y_pred)","8c5c26c2":"# Precision\nprecision_score(y_test, y_pred)","a1e678f5":"# Recall\nrecall_score(y_test, y_pred)","771fdb57":"# F1\nf1_score(y_test, y_pred)","d3ac1869":"print(classification_report(y_test, y_pred))","b54a1b7f":"# ROC Curve\nplot_roc_curve(lgr_model, X_test, y_test)\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.show()","3d9ba723":"# AUC\nroc_auc_score(y_test, y_prob)","ae9d10f2":"# Confusion Matrix\ndef plot_confusion_matrix(y, y_pred):\n     acc = round(accuracy_score(y, y_pred), 2)\n     cm = confusion_matrix(y, y_pred)\n     sns.heatmap(cm, annot=True, fmt=\".0f\")\n     plt.xlabel('y_pred')\n     plt.ylabel('y')\n     plt.title('Accuracy Score: {0}'.format(acc), size=10)\n     plt.show()\n\nplot_confusion_matrix(y_test, y_pred)","ff36f778":"**TASK**\n\nDevelop diabetes prediction model by performing literature search, data preprocessing and feature engineering.","09f30101":"# EDA Analysis","f56700b1":"# Dataset and Story\n\n**Business Problem**\n\nCan you develop a machine learning model that can predict whether people have diabetes when their characteristics are specified?\n\nThe dataset is part of the large dataset held at the National Institutes of Diabetes-Digestive-Kidney Diseases in the USA. Persons aged 21 and over living in Phoenix, the 5th largest city in the State of Arizona in the USA. Data used for diabetes research on Pima Indian women. It consists of 768 observations and 8 numerical independent variables. The target variable is specified as \"outcome\"; 1 indicates positive diabetes test result, 0 indicates negative.\n\n**Variables**\n- Pregnancies: Number of pregnancies\n- Glucose: Glucose.\n- BloodPressure: Blood pressure.\n- SkinThickness: Skin Thickness\n- Insulin: Insulin.\n- BMI: Body mass index.\n- DiabetesPedigreeFunction: A function that calculates our probability of having diabetes based on our ancestry.\n- Age: Age (years)\n- Outcome: Information whether the person has diabetes or not. Have the disease (1) or not (0)\n","ce3a80bf":"# Feature Engineering","8757d2bd":"# Encoding","5f348d5e":"* It is known that variable values other than Pregnancies and Outcome cannot be 0 in a human.\n* Therefore, an action decision should be taken regarding these values. Values that are 0 can be assigned NaN.","903b9c1e":"# Data Preprocessing","1917aa41":"# Modelling"}}