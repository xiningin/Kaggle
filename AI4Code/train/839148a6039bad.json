{"cell_type":{"9a3fc8ec":"code","b5c9e928":"code","bbd66344":"code","047aa868":"code","6a5eaeb9":"code","ff09ae78":"code","76d6a8e5":"code","ce1382aa":"code","59fc0e05":"code","35b5d724":"code","b7177615":"code","f1e02b09":"code","d35e33f8":"code","2c2c211a":"code","23539ed0":"code","55cfe729":"code","fdc4c71d":"code","24a49746":"code","e517872f":"code","c977e9d9":"code","f8350241":"code","f35030cc":"code","aa0e60e9":"code","e5226213":"code","0d9d8d48":"code","33d09ab4":"code","a3510e19":"code","f3b88b39":"code","00a76af9":"code","84913b42":"code","21ace7cc":"code","400ad130":"code","dc53ae99":"code","7cb74c32":"code","99569465":"code","80112382":"code","0197ef7d":"code","36c13445":"code","bacd614e":"code","ac12b661":"code","d3757c36":"code","0f6cb5a7":"code","a5ce6f05":"markdown","c6c74b1b":"markdown","d199a6d2":"markdown","8519234f":"markdown","68851b39":"markdown","5aed9fc8":"markdown","98be67d2":"markdown","eb039bd9":"markdown","3bf5d445":"markdown","a41d5e1b":"markdown","e8842a9e":"markdown","7e15dad1":"markdown","1500bba9":"markdown","20d2606b":"markdown","2712b5f2":"markdown","93afa452":"markdown","972d7960":"markdown","4674b922":"markdown","42cab01f":"markdown","bcd6457f":"markdown","a5a745ce":"markdown","e77d35d5":"markdown","0a65fcb0":"markdown","0b03cdcf":"markdown","bc25c6ff":"markdown","fbc5b1b4":"markdown","cf2a09d9":"markdown","b87eea54":"markdown","dc943567":"markdown","f2153995":"markdown","9928d47d":"markdown","962a6c2b":"markdown","a398f7f6":"markdown","c2bb16fc":"markdown","7bc87256":"markdown","8a498ad0":"markdown","4afac990":"markdown","16d55af6":"markdown","25f8a637":"markdown","d3e58045":"markdown","3baab89a":"markdown"},"source":{"9a3fc8ec":"from IPython.core.display import display, HTML, Javascript\n\ntable_of_contents =\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n    <style>\n    .toc h2{\n        color: #3f4d63;\n        font-weight: 600;\n        font-family: 'Times New Roman', serif;\n        font-size: 28px;\n        margin-bottom: 4px;\n    }\n    \n    .toc ol li{\n        list-style:none;\n        line-height:normal;\n        }\n     \n    .toc li{\n        color: #080808;\n        font-weight: 600;\n        font-family: 'Times New Roman', serif;\n        font-size: 17px;\n        margin-bottom: 2px;\n    }\n\n    .toc ol ol li{\n        color: #4d4d4d;\n        font-weight: 400;\n        font-size: 15px;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        margin-top: 0px;\n        margin-bottom: 0px;\n    } \n    \n    .section_title{\n        background-color: #c22d2d;\n        color: white;\n        font-family: Helvetica;\n        font-size: 25px;\n        padding: 6px 12px;\n        margin-bottom: 5px;\n    }\n    .subsection_title{\n        background: #0f0f0f;\n        color: white;\n        font-family: Helvetica;\n        font-size: 21px;\n        padding: 6px 12px;\n        margin-bottom: 0px;\n    }\n    .subsubsection_title{\n        background: #609c8d;\n        color: white;\n        font-family: Helvetica;\n        font-size: 17px;\n        padding: 6px 12px;\n        margin-bottom: 0px;\n    }\n    .subsubsubsection_title{\n        background: #235f83;\n        color: white;\n        font-family: Helvetica;\n        font-size: 15px;\n        padding: 6px 12px;\n        margin-bottom: 0px;\n    }\n    <\/style>\n    <\/head>\n    <body>\n    <\/body>\n<\/html>\n\"\"\"\n\nHTML(table_of_contents)","b5c9e928":"# for data analysis\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# for viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# usual ML algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n##xgBoost\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#adaBoost\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#gradientBoosting\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold","bbd66344":"# getting\/acquiring our data\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n# we are using combine for operations to run on both train and test which will be used later.\ncombine = [train_df, test_df]\n\n\n# we print values of columns which gives us nothing but our features\nprint(train_df.columns.values)\n\n\n# preview the data\ntrain_df.head()","047aa868":"# our magic library\nimport pandas_profiling \ntrain_df.profile_report()","6a5eaeb9":"## lets not mix our train csv so we create another variable\ntitanic_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n","ff09ae78":"titanic_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().iplot(kind='bar')\ntitanic_df[['Sex','Survived']].groupby(['Sex']).mean().iplot(kind='barh')\ntitanic_df.iplot()","76d6a8e5":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","ce1382aa":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","59fc0e05":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","35b5d724":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","b7177615":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","f1e02b09":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","d35e33f8":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","2c2c211a":"guess_ages = np.zeros((2,3))\nguess_ages","23539ed0":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","55cfe729":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","fdc4c71d":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","24a49746":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","e517872f":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c977e9d9":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","f8350241":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","f35030cc":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","aa0e60e9":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","e5226213":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","0d9d8d48":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","33d09ab4":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","a3510e19":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","f3b88b39":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","00a76af9":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","84913b42":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","21ace7cc":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","400ad130":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","dc53ae99":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","7cb74c32":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","99569465":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","80112382":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","0197ef7d":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","36c13445":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","bacd614e":"xg_boost = XGBClassifier()\nxg_boost.fit(X_train, Y_train)\nY_pred = xg_boost.predict(X_test)\nxg_boost.score(X_train, Y_train)\nacc_xg_boost = round(xg_boost.score(X_train, Y_train) * 100, 2)\nacc_xg_boost","ac12b661":"ada_boost = AdaBoostClassifier()\nada_boost.fit(X_train, Y_train)\nY_pred = ada_boost.predict(X_test)\nacc_ada_boost = round(ada_boost.score(X_train, Y_train) * 100, 2)\nacc_ada_boost","d3757c36":"models = pd.DataFrame({\n    'Model': ['Gradient Boosting','Ada Boosting','XGBoost','Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_gr_boost,acc_ada_boost,acc_xg_boost,acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","0f6cb5a7":"#random_forest = RandomForestClassifier(n_estimators=100)\n#random_forest.fit(X_train, Y_train)\n#Y_pred = random_forest.predict(X_test)\n#random_forest.score(X_train, Y_train)\n#acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n#acc_random_forest\n#Y_pred = random_forest.predict(X_test)\n\n#output = pd.DataFrame({\n #       \"PassengerId\": test_df[\"PassengerId\"],\n #       \"Survived\": Y_pred\n #   })\n\n#output.to_csv('my_submission.csv', index=False)\n#print(\"Your submission was successfully saved!\")","a5ce6f05":"## \u26a1\ufe0f Step 2:\n### \u2705 Get the data (Literally! or we can say \"to acquire\" the data and load it into our DataFrame(pandas).\n### \u2705 Analyze what is present in the dataset.\n  * **Categorize** features (weather they are categorical,numerical etc).\n  * **Indentify** if certain features are mixed (numerical with text for example)\n  * **Typos** (we are dealing with names here, we dont know who uses which title and if they name includes nicknames within etc, we  focus on finding errors and typos whenever dealing with names to avoid problems later in the model)\n<br> \n\n### \u2705 Preview the present data.\n","c6c74b1b":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","d199a6d2":"<a id = \"q11\"><\/a>\n## \ud83d\udc8e Ada Boosting ","8519234f":"### \ud83c\udff3\ufe0f Getting the Data ","68851b39":"<a id = \"q3\"><\/a>\n## \ud83d\udc8e Logistic Regression","5aed9fc8":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","98be67d2":"# <div class=\"section_title\">\ud83e\udd29 Algorithm's Used \n    \n* [KNN- K-nearest-neighbours](#q1)\n* [Suport Vector Machines](#q2)\n* [Logistic Regression](#q3)\n* [Naive Bayes](#q5)\n* [Perceptron](#q5)    \n* [Linear SVC](#q6)\n* [Stochastic Gradient Descent](#q7)\n* [Decision Tree](#q8)\n* [Random Forest](#9)\n* [XG boost](#q10)\n* [Ada Boosting](#q11)    \n* [Gradient Boosting](#q12) \n    \nAlso dont worry if it seems too much at this stage, \nfirst we will explore and analyze our data and then use the above mentioned alogorithms to predict our answers.","eb039bd9":"# <span style=\"color:orange\">\ud83e\udd29 Model Comparison ","3bf5d445":"### \ud83c\udff3\ufe0f Analyzing Data and Previewing Data\n\n**This will be done in a single step ahead, do read.**\n\nTo get better understanding of various categories in the dataset, read [here](https:\/\/www.kaggle.com\/c\/titanic\/data).\n\n\n<br>\n\n\n\n### \ud83e\udd14 Categorical values\nThese are types of data which can be divided into group.\n* In our dataset we have -> Survived, Sex and Embarked.\n\n**Note** We have a value Pclass. Which is also categorical but it has a certain order fixed to it so we call it as **Ordinal**.\n\n### \ud83e\udd14 Numerical Values\nThese are values (numeric ofcourse!), but have an abosolute value to it, a specific measure of some amount.\nHere it would be -> Fare,Age.\n\n**Note** Here too we have another special value called Discrete Data which means it can have only certain values.\nBest example would be roling of 2 dice and the only values we have have is 2,3,4,5,6,7,8,9,10,11 and 12.\n\n<br>\n\n### \u2705 Summary\n* Categorical: Survived, Sex, and Embarked.\n* Ordinal: Pclass.\n* Continous: Age, Fare.\n* Discrete: SibSp, Parch.\n\n","a41d5e1b":"\n## <span style=\"color:orange\">\ud83d\ude0a\ud83d\ude4f\ud83c\udffb Credits\/ Mentions \n### Before starting this i would like to give credits to Mr.Manav Sehgal and his absolutely beautiful explanatory notebook that inspired me to make a my own new, shorter and simplar notebook. Kaggle Team's Alexis Cook's Titanic Tutorial for making me understand concepts in simplest way possible.\n   \n","e8842a9e":"\n\n## \ud83e\udd14 We clearly see the Random forest and Decision tree perform quite the same, but still we still will go with random forest. \u2705\n\n### \ud83e\udd2f Remove \"#\" you want to use this \u2b07\ufe0f  for submission.\n\n","7e15dad1":"<a id = \"q10\"><\/a>\n## \ud83d\udc8e XG Boost","1500bba9":"<a id = \"q9\"><\/a>\n## \ud83d\udc8e Random Forest","20d2606b":"<div class=\"alert alert-block alert-success\">Thank you all for viewing this, I understand you are here looking at the Title of the notebook which focuses on comparison of 12 algorithms, so let's get right to it!\n<\/div>\n    \n","2712b5f2":"## \u26a0\ufe0f Wait! Did you say Interactive plots? CHECK \u2705","93afa452":"<a id = \"q11\"><\/a>\n## \ud83d\udc8e Gradient Boosting","972d7960":"## \u26a1\ufe0f Step 3:\n\n### \u2705 Wrangle Data \n\n**Okay, This is one of the most crucial steps but here, the main aim of this NoteBook was not to explain step by step on how to wrangle our data , there are lot of books existing for that same, but to compare all the Algorithms in short.\nI would suggest you to follow this [link](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Wrangle-data) (just click the link and and give it a few seconds, it will take you directly to that particular section of the of the page), how the data was wrangled and what changes we made**.\n\nThe complete code for Data wrangling is hidden, cause i've used above link for the reference. I suggest you to follow that link if you want to get the details of that part.\n","4674b922":"<div style=\"text-align:center\"> <span style=\"color:Orange; font-size:3.4em;\"> \u2705 Beginners - Solving Tiranic using 12 Algorithms \ud83e\udd29 and Comparison \ud83e\udd2f <span><\/div>","42cab01f":"# <div class=\"section_title\">\ud83d\udce2 Attention \u2b07\ufe0f\n    \n### Now wait, this is getting too long and we have to analyze all of this by ourselfes and there is so so much left! \ud83d\ude30 \n\n\n### What if i told you one line of code and give you all the EDA you need to figure out everything \ud83d\ude32 Literally! I mean plots, Values, Correlation everything! Look at the magic below.\n\n","bcd6457f":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","a5a745ce":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","e77d35d5":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","0a65fcb0":"## \u26a1\ufe0f STEP 1:\n### \u2705 Import the neccessary libraries for data analysis\n### \u2705 Import library for Visualization\n### \u2705 Import Libraries for ML algorithms","0b03cdcf":"<a id = \"q7\"><\/a>\n## \ud83d\udc8e Stochastic Gradient Descent","bc25c6ff":"![Alt Text](https:\/\/media.giphy.com\/media\/uaB5o9l6Wungk\/giphy.gif)","fbc5b1b4":"#  <span style=\"color:orange\">\ud83d\udea9 ReadMe\n### This notebook is a tutorial for beginners who just want to dive into how everything is done, how various models are used to solve this problem and NOT made for increasing public scoring.\n\n\n","cf2a09d9":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","b87eea54":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","dc943567":"<a id = \"q4\"><\/a>\n## \ud83d\udc8e Naive Bayes","f2153995":"<a id = \"q2\"><\/a>\n## \ud83d\udc8e Support Vector Machines ","9928d47d":"## \ud83c\udfc1 Finally! \u2705 Models.....\n\n\n","962a6c2b":"# <span style=\"color:orange\"> \u26a1\ufe0f Thank you for reading this far \ud83d\ude4f\ud83c\udffb\ud83d\ude0a. Dont forget to upvote if it helped you in any way. ","a398f7f6":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","c2bb16fc":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","7bc87256":"gr_boost = GradientBoostingClassifier()\ngr_boost.fit(X_train, Y_train)\nY_pred = ada_boost.predict(X_test)\nacc_gr_boost = round(gr_boost.score(X_train, Y_train) * 100, 2)\nacc_gr_boost","8a498ad0":"\n\n<div class=\"alert alert-block alert-success\">\n\n<\/div>","4afac990":"<a id = \"q8\"><\/a>\n## \ud83d\udc8e Decision Tree","16d55af6":"<a id = \"q1\"><\/a>\n## \ud83d\udc8e KNN - K-nearest-neighbours","25f8a637":"# <span style=\"color:red\">\ud83e\udd14 How is this Notebook different than others? \n    \n### We will **acquire**, **analyze**, make clear **Assumations** and **vizualize** for **each algorithm together** and avoid making a huge notebook of each and every algorithm seperately.\n    \n    \n","d3e58045":"<a id = \"q6\"><\/a>\n## \ud83d\udc8e Linear SVC","3baab89a":"<a id = \"q5\"><\/a>\n## \ud83d\udc8e Perceptron"}}