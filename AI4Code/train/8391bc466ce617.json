{"cell_type":{"448c5bce":"code","ace8fe19":"code","383b582c":"code","452373ba":"code","b9658b07":"code","dd52ce48":"code","c0ae68d4":"code","60453324":"code","12e62d5f":"code","4091c911":"code","4cbe442f":"code","898d8c62":"code","e2b1156d":"code","2b4b4024":"code","9cb22169":"code","e3b97686":"code","03a5f33a":"code","55f9911d":"code","85565e85":"code","ba3849b8":"code","d2ce9140":"code","d56989f1":"code","89b18e66":"code","9c38065d":"code","2149a376":"code","1e506e3b":"code","76faba39":"code","8b1cd06f":"code","8b79eb60":"code","e220bb67":"markdown","d6ef5513":"markdown","b731c1a8":"markdown","46cddd71":"markdown","6dd4e73f":"markdown","2880a6a2":"markdown","6a7470af":"markdown","83baf588":"markdown","8ce67291":"markdown","0ba1331c":"markdown","4209aa47":"markdown","998a7cac":"markdown","b2108681":"markdown","a0cdc081":"markdown","75831f93":"markdown","a0f70f80":"markdown","a5681c6c":"markdown","90d13d53":"markdown","bbb0570f":"markdown","201bd10b":"markdown"},"source":{"448c5bce":"from sklearn import preprocessing, tree\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np","ace8fe19":"#import data\ntrain_raw_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_raw_data = pd.read_csv('..\/input\/titanic\/test.csv')","383b582c":"class TukeyOutlierRemover(BaseEstimator, TransformerMixin):\n\n    __outliers = []\n    __outliers_per_feature = {}\n\n    def __init__(self, n, features):\n        self.n = n\n        self.features = features\n\n    def transform(self, X):\n        return X.drop(self.__outliers, axis=0)\n\n    def fit(self, X, y=None):\n        outliers = []\n        for col in self.features:\n            feature_outliers = []\n            feature_outliers.extend(X[X[col].isna()].index)\n            Q1 = np.percentile(X[col].dropna(), 25)\n            Q3 = np.percentile(X[col].dropna(), 75)\n            IQR = Q3 - Q1\n            outlier_step = 1.5 * IQR\n            feature_outliers.extend(X[(X[col] < Q1 - outlier_step) | (X[col] > Q3 + outlier_step)].index)\n            outliers.extend(feature_outliers)\n            self.__outliers_per_feature[col] = feature_outliers\n        self.__outliers = list(key for key, value in Counter(outliers).items() if value >= self.n)\n        return self\n\n    def get_outliers(self):\n        return self.__outliers\n\n    def get_outliers_per_feature(self, feature=None):\n        if feature is not None:\n            return self.__outliers_per_feature[feature]\n        else:\n            return self.__outliers_per_feature\n","452373ba":"outlierRemover = TukeyOutlierRemover(3, ['Age','SibSp','Parch','Fare'])\ntrain_raw_data = outlierRemover.fit_transform(train_raw_data)\nprint(outlierRemover.get_outliers())","b9658b07":"joint_raw_data = pd.concat(objs=[train_raw_data, test_raw_data], axis=0, sort=False).reset_index(drop=True)","dd52ce48":"#OHE\n#input column values must be int\ndef train_ohe(data): \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore')\n    ohe.fit(data.drop(['PassengerId', 'Survived'], axis=1))\n    return ohe\n\ndef transform_ohe(data, ohe): \n    columns_to_remove = ['PassengerId']\n    if 'Survived' in data.columns:\n        columns_to_remove.append('Survived')\n    ohe_labels = ohe.transform(data.drop(columns_to_remove, axis=1, inplace=False))\n    new_data = pd.concat([data['PassengerId'].reset_index(drop=True),pd.DataFrame(ohe_labels)],axis=1)\n    if 'Survived' in columns_to_remove:\n        new_data = pd.concat([new_data, data['Survived'].reset_index(drop=True)],axis=1)\n    return new_data","c0ae68d4":"def generate_binary_columns(data):\n#    data['is_Alone'] = (data['cat_Family_Members']==0)\n#    data['is_big_Family'] = (data['cat_Family_Members']==3)\n    data['is_Child'] = (data['Age']<=5)\n    data['is_Elder'] = (data['Age']>=60)\n    data['is_Officer'] = (data['Title']=='Officer')\n    data['is_Royalty'] = (data['Title']=='Royalty')\n    data['is_Male'] = (data['Title']=='Mr')\n    data['is_Female'] = ((data['Title']=='Miss') | (data['Title']=='Mrs'))\n    data['is_First_Cabin'] = (data['Cabin']==0)\n    data['is_Last_Cabin'] = (data['Cabin']==3)\n    return data\n","60453324":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\" : \"Mrs\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\n\ndef feature_eng(data):\n    def get_ticket_prefix(ticket_data):\n        def take_prefix(ticket):\n            ticket = ticket.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')\n            return ticket[0]\n        ticket_data = ticket_data.apply(lambda x: take_prefix(x) if not x.isdigit() else \"X\")\n        return ticket_data\n\n    data['Cabin'] = data['Cabin'].fillna('X').str[0:1]\n#    data['Cabin'] = data['Cabin'].replace(['F', 'G', 'T', 'A'], 'Internal')\n#    data['Cabin'] = data['Cabin'].map( {'F': 'Internal', 'G': 'Internal', 'T': 'Internal', 'A': 'Internal'})\n    data['Title'] = data['Name'].apply(lambda x : x.split(\",\")[1].split(\".\")[0].strip())\n    data['Title'] = data['Title'].map(Title_Dictionary)\n#    data['Title'] = data['Title'].map({'Master': 'Rare', 'Royalty': 'Rare', 'Officer': 'Rare'})\n    data['Family_Members'] = data['SibSp'].astype(int) + data['Parch'].astype(int) + 1\n#    data['cat_Family_Members'] = pd.cut(x=data['Family_Members'], right=False, bins=(1,2,3,5,99), labels=[0,1,2,3])\n    data['cat_Age'] = pd.cut(x=data['Age'].dropna(), right=True, bins=(0,3,12,60,100), labels=[0,1,2,3])\n    data['log_Fare'] = data['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\n    data['Pclass'] = data['Pclass'].astype(\"category\")                                                  \n    data['Sex'] = data['Sex'].map( {'female': 0, 'male': 1})\n    #data['Ticket'] = get_ticket_prefix(data['Ticket'])\n    return data\n","12e62d5f":"def to_integer(data, feature):\n    le = preprocessing.LabelEncoder()\n    le.fit(data[feature])\n    data[feature] = le.transform(data[feature])\n    \ndef to_numeric_feature(data):\n    columns_to_dummie = ['Title', 'Embarked'] # ##duas juntas\n    dum = pd.get_dummies(data[columns_to_dummie], prefix=columns_to_dummie)\n    return pd.concat([data, dum], axis=1) ","4091c911":"joint_data = feature_eng(joint_raw_data)\n\n#to_integer(joint_data, 'Title')\n#to_integer(joint_data, 'Cabin')\n#to_integer(joint_data, 'Embarked')\n\n#joint_data = generate_binary_columns(joint_data)\njoint_data = to_numeric_feature(joint_data)\n\n# drop high std features\n#joint_data.drop(labels=['Title_Royalty', 'Cabin_G','Cabin_T', 'Cabin_F', 'Cabin_A'], inplace=True, axis=1)","4cbe442f":"joint_data.head()","898d8c62":"class DropFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, features_to_remove):\n        self.features_to_remove = features_to_remove\n\n    def transform(self, X):\n        X = X.drop(labels=self.features_to_remove, axis=1).astype(float)\n        #print(X.head())\n        return X\n\n    def fit(self, X, y=None):\n        return self\n\n\nclass DfNanFiller(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n\n    def transform(self, X):\n        transformed_X = X.copy()\n        for feature in self.features:\n            if X[feature].dtype is np.float64:\n                transformed_X[feature] = X[feature].fillna(X[feature].dropna().median())\n            else:\n                transformed_X[feature] = X[feature].fillna(X[feature].mode(dropna=True)[0])\n        return transformed_X\n\n    def fit(self, X, y=None):\n        return self\n\n\nclass FeatureNanFiller(BaseEstimator, TransformerMixin):\n    def __init__(self, nanFeature, base_features):\n        self.nanFeature = nanFeature\n        self.base_features = base_features\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if len(self.nanFeature) == 0:\n            return X\n        transformed_X = X.copy()\n        nan_indexes = X[X[self.nanFeature].isna()].index\n        for i in nan_indexes:\n            median = X[self.nanFeature][self.__get_df_slicer(X, i)].median()\n            if not np.isnan(median):\n                transformed_X.loc[i, self.nanFeature] = median\n            else:\n                transformed_X.loc[i, self.nanFeature] = X[self.nanFeature].median()\n        return transformed_X\n\n    def __get_slice_expression(self, X, feature, i):\n        return (X[feature] == X.loc[i, feature])\n\n    def __get_df_slicer(self, X, index):\n        expression = self.__get_slice_expression(X, self.base_features[0], index)\n        for i in range(1, len(self.base_features)):\n            expression = expression & self.__get_slice_expression(X, self.base_features[i], index)\n        return expression","e2b1156d":"from sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import normalize, quantile_transform, FunctionTransformer, MinMaxScaler, StandardScaler","2b4b4024":"transformators = []\ntransformators.append((\"dfNaFiller\", DfNanFiller(['Title', 'Cabin', 'Embarked'])))\ntransformators.append((\"ageNaFiller\", FeatureNanFiller('Age', ['Family_Members', 'Title', 'Pclass'])))\ntransformators.append((\"fareNaFiller\", FeatureNanFiller('Fare', ['Cabin', 'Pclass'])))\n#transformators.append((\"dropFeatures\", DropFeatures(['Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'cat_Age', 'Embarked'])))\ntransformators.append((\"dropFeatures\", DropFeatures(['Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'cat_Age', 'Cabin', 'Title', 'Embarked'])))\n#transformators.append((\"printHead\", PrintHead()))\n\ntransformators.append((\"quantilesTransform\", FunctionTransformer(quantile_transform)))\n#transformators.append((\"unitNorm\", FunctionTransformer(normalize)))\n#transformators.append((\"minMax\", MinMaxScaler()))\n#transformators.append((\"scaler\", StandardScaler()))\n\npreprocess_pipeline = Pipeline(transformators)\n","9cb22169":"from sklearn import ensemble, linear_model, model_selection\n","e3b97686":"from sklearn import ensemble, linear_model, model_selection, neighbors, neural_network, svm\n\ndef divide_data(data):\n    ids = data['PassengerId'].astype({\"PassengerId\": int})\n    X = data.drop(labels=['PassengerId', 'Survived'], axis=1)\n    if not data['Survived'].isna().any():\n        Y = data['Survived'].astype({\"Survived\": int})\n        return ids, X, Y\n    else:\n        return ids, X\n","03a5f33a":"train_data = joint_data[joint_data['PassengerId'].isin(train_raw_data['PassengerId'])]\ntest_data = joint_data[joint_data['PassengerId'].isin(test_raw_data['PassengerId'])]\n\npassenger_ids_training, X_training, Y_training = divide_data(train_data)\npassenger_ids_test, X_test = divide_data(test_data)\n","55f9911d":"def create_pipeline(preprocess_pipeline, model_name, estimator):\n    classification_pipeline = Pipeline([(model_name, estimator)])\n    full_pipeline = Pipeline([(\"preprocess\", preprocess_pipeline),\n                            (\"classfication\", classification_pipeline)])\n    return full_pipeline","85565e85":"def preprocess_without_pipeline(data):\n    naFiller = DfNanFiller(['Title', 'Cabin', 'Embarked'])\n    ageFeatureNanFiller = FeatureNanFiller('Age', ['Family_Members', 'Title', 'Pclass'])\n    fareNaFiller = FeatureNanFiller('Fare', ['Cabin', 'Pclass'])\n    dropFeatures = DropFeatures(['Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'cat_Age', 'Cabin', 'Title', 'Embarked'])\n    minMax = MinMaxScaler()\n    quantiles = FunctionTransformer(quantile_transform)\n    \n    data = naFiller.fit_transform(data)\n    data = ageFeatureNanFiller.fit_transform(data)\n    data = fareNaFiller.fit_transform(data)\n    data = dropFeatures.fit_transform(data)\n    data = minMax.fit_transform(data)\n    #data = quantiles.fit_transform(data)\n    return data\n","ba3849b8":"def compare_models(proprocess_pipeline, estimators, X, Y, metrics=['accuracy','precision','recall','f1']):\n    scores = {}\n    score_means = {}\n    score_std = {}\n    for estimator in estimators:\n        name = type(estimator).__name__\n        classifier_pipeline = create_pipeline(preprocess_pipeline, name, estimator)\n       # scores[name] = model_selection.cross_validate(estimator, X, n_jobs=-1, y=Y, scoring=metrics, cv=kfold, return_train_score=False)\n        scores[name] = model_selection.cross_validate(classifier_pipeline, X, n_jobs=-1, y=Y, scoring=metrics, cv=kfold, return_train_score=False)\n        score_means[name] = {key:values.mean() for key,values in scores[name].items()}\n        score_std[name] = {key:values.std() for key,values in scores[name].items()}        \n    return scores, score_means, score_std\n\ndef print_scores_as_table(score_means, score_std, print_performance_metrics=True):\n    for classifier_name, score_mean_values in score_means.items():\n        score_std_values = score_std[classifier_name]\n        print(classifier_name)\n        print(\"   metric \\t\\tmean \\t\\tstd\")\n        if not print_performance_metrics:\n            score_mean_values.pop('fit_time', False)\n            score_mean_values.pop('score_time', False)\n            score_std_values.pop('fit_time', False)\n            score_std_values.pop('score_time', False)\n            for metric,mean in score_mean_values.items():\n                print(metric+\" = \"+str(mean)+\"   \"+str(score_std_values[metric]))\n        print(\"--------------------------------\")\n\n    ","d2ce9140":"kfold = model_selection.StratifiedKFold(n_splits=10)\nrandom_state = 2\nn_estimators=1000\n\nestimators = []\nestimators.append(svm.SVC(random_state=random_state, gamma='auto'))\nestimators.append(tree.DecisionTreeClassifier(random_state=random_state))\nestimators.append(neural_network.MLPClassifier(random_state=random_state))\nestimators.append(neighbors.KNeighborsClassifier())\nlog_reg = linear_model.LogisticRegression(random_state=random_state, max_iter=1000, solver='lbfgs')\nestimators.append(log_reg)\nestimators.append(ensemble.RandomForestClassifier(random_state=random_state, n_estimators=n_estimators))\nestimators.append(ensemble.ExtraTreesClassifier(random_state=random_state, n_estimators=n_estimators))\nestimators.append(ensemble.GradientBoostingClassifier(random_state=random_state, n_estimators=n_estimators))\nestimators.append(ensemble.AdaBoostClassifier(log_reg ,random_state=random_state,learning_rate=0.1))\n\nscores, score_means, score_std = compare_models(preprocess_pipeline, estimators, X_training, Y_training)\nprint_scores_as_table(score_means, score_std, False)\n","d56989f1":"def train_and_save_submission(models, training_X, training_Y, test_data, test_passenger_ids, name=None):    \n    for model in models:\n        algorithm = model.fit(training_X, training_Y)\n        predictions = model.predict(test_data)\n\n        submission = pd.DataFrame({ 'PassengerId': test_passenger_ids.astype(int), 'Survived': predictions.astype(int) })\n        if name is None:\n            name = type(model).__name__\n        submission.to_csv(name+\".csv\", index=False)\n        ","89b18e66":"k = 10\nn_jobs = -1","9c38065d":"X_training2 = preprocess_without_pipeline(X_training)\nX_test2 = preprocess_without_pipeline(X_test)","2149a376":"gridsRFC = ensemble.RandomForestClassifier()\n\nparam_grid = {\n    'n_estimators': [15, 30, 100, 200],\n    'max_depth': [10, 50, 100, 200, 300],\n    'min_samples_split': [2, 10, 20, 30],\n    'min_samples_leaf': [2, 3, 4, 6],\n    'max_features': ['sqrt', 'log2'],\n    'criterion' : [\"gini\", \"entropy\"],\n    'max_leaf_nodes': [30, 50, 100, 200, 300],\n    'oob_score' : [True, False]\n#    'classfication__min_impurity_decrease': [0], #[0, 0.1, 0.5, 1],\n#    'classfication__min_weight_fraction_leaf': [0, 0.001] #[0, 0.1, 0.25, 0.5]\n}\n\nrandomSearch = model_selection.RandomizedSearchCV(estimator = gridsRFC, param_distributions = param_grid, n_iter = 1000, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrandomSearch.fit(X_training2, Y_training)\nprint(randomSearch.best_params_)\n\n","1e506e3b":"print(randomSearch.best_score_)\nprint(randomSearch.best_estimator_)\n\ntrain_and_save_submission([randomSearch.best_estimator_], X_training2, Y_training, X_test2, passenger_ids_test, \"gridsRFC\")\n","76faba39":"import matplotlib.pyplot as plt\n#import warnings\n#warnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef plot_learning_curve(estimator, X, y, train_sizes=np.linspace(.1, 1.0, 5), cv=None, metric='accuracy'):\n    plt.figure()\n    plt.title(\"Learning Curves\")\n    plt.xlabel(\"Training samples\")\n    plt.ylabel(metric)\n    train_sizes, train_scores, test_scores = model_selection.learning_curve(estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=metric)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training accuracy\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross validation accuracy\")\n    plt.legend(loc=\"best\")\n    return plt","8b1cd06f":"g = plot_learning_curve(randomSearch.best_estimator_, X_training2, Y_training, cv=kfold)","8b79eb60":"g = plot_learning_curve(randomSearch.best_estimator_, X_training2, Y_training, cv=kfold, metric='brier_score_loss')","e220bb67":"****### 5.1[](http:\/\/). Random Forest","d6ef5513":"## 1. Importing Data and Joining Data","b731c1a8":"## 6. Learning Curve","46cddd71":"### 4.3. Preprocess Without Pipeline (for testing)","6dd4e73f":"###  2.1. OHE (Bonus)","2880a6a2":"### 2.5. Run PreProcessing on Joint Data","6a7470af":"## 5. Submitting Algorithm Models","83baf588":"****### 6.2. Random Forest","8ce67291":"### 2.4. Convert to Numeric","0ba1331c":"##  2. Pre-processing Data\n","4209aa47":"## 4. Modelling\n","998a7cac":"## 5. Hyper-parameter Tunning","b2108681":"### 1.1. Removing outliers from training data","a0cdc081":"### 6.1. Common Function","75831f93":"### 4.3. Trying learning algorithms","a0f70f80":"### 4.1. Importing and Preparing Data","a5681c6c":"### 1.2. Joining Data","90d13d53":"##  3. Pipeline for Preprocessing data\n","bbb0570f":"### 4.2. Classification Pipeline","201bd10b":"### 2.3. Feature Engineering"}}