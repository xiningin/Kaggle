{"cell_type":{"f4a8c007":"code","e1382a43":"code","a4be110f":"code","24006edd":"code","010a8d0d":"code","f30e5e66":"code","79c0290d":"code","ad60ff01":"code","54f5b701":"code","0244fcef":"code","f09241e5":"code","a8dd4e43":"code","b4c32343":"code","8a31e212":"code","4558cbe6":"code","2e90b36f":"code","8a4c7381":"code","f15edd75":"code","3ad9d4fc":"code","d53bcdc3":"code","349ff4e3":"code","4a2c4285":"code","d438cd32":"code","cde8c30f":"code","1bb317bf":"code","8ac4839d":"code","33baaa53":"code","c52cc5c5":"code","32075dcf":"markdown","f45136cc":"markdown","ea9c96c8":"markdown","45e948b4":"markdown","ad9659cf":"markdown","7e04d89a":"markdown","9a790dac":"markdown","a99e5c6a":"markdown","09f7923e":"markdown","d51ea8d3":"markdown","10041261":"markdown","deee97e3":"markdown","91dbbfb1":"markdown","7ede7a00":"markdown","75b954e9":"markdown"},"source":{"f4a8c007":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.tools.plotting import scatter_matrix\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e1382a43":"#Load datasets\ndf_data = pd.read_csv('..\/input\/data.csv')\n#df_genre = pd.read_csv('..\/input\/data_2genre.csv')\n\n#Joins datasets\n#df_genre['label'] = df_genre['label'].apply(lambda x: 'pop' if x == 1 else 'classical')\n\n#frames = [df_data, df_genre]\n#df = pd.concat(frames)\n\ndf = df_data\ndf.head(3)","a4be110f":"_ = df[\"label\"].value_counts().plot.pie( autopct='%.2f', figsize=(10, 10),fontsize=20)","24006edd":"df.describe()","010a8d0d":"df.hist(bins=50, figsize=(20,15))\nplt.show()","f30e5e66":"df.drop([\"filename\"], axis=1, inplace=True)","79c0290d":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.4)\nfor train_index, cross_index in split.split(df, df[\"label\"]):\n    strat_train_set = df.iloc[train_index]\n    strat_cross_set = df.iloc[cross_index]\n    \nsplit_cross = StratifiedShuffleSplit(n_splits=1, test_size=0.5)\nfor test_index, valid_index in split_cross.split(df, df[\"label\"]):\n    strat_test_set = df.iloc[test_index]\n    strat_valid_set = df.iloc[valid_index]","ad60ff01":"music = strat_train_set.copy()","54f5b701":"_ = music[\"label\"].value_counts().plot.pie( autopct='%.2f', figsize=(6, 6))","0244fcef":"attributes = [\"beats\", \"tempo\", \"spectral_centroid\",\n              \"spectral_bandwidth\", \"rolloff\", \"zero_crossing_rate\" ]\nsm = scatter_matrix(music[attributes], figsize=(20, 15), diagonal = \"kde\");\n\n#Hide all ticks\n[s.set_xticks(()) for s in sm.reshape(-1)];\n[s.set_yticks(()) for s in sm.reshape(-1)];\n\nfor ax in sm.ravel():\n    ax.set_xlabel(ax.get_xlabel(), fontsize = 14)\n    ax.set_ylabel(ax.get_ylabel(), fontsize = 14)","f09241e5":"music = strat_train_set.drop(\"label\", axis=1)\nmusic_labels = strat_train_set[\"label\"].copy()","a8dd4e43":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: %0.2f\" % scores.mean())\n    print(\"Standard deviation: %0.2f\" % (scores.std() * 2))","b4c32343":"music = scale(music);","8a31e212":"folds = 10\nn_jobs=-1\nverbose=5","4558cbe6":"clf = SVC(kernel='linear', C=0.01)\nscores = cross_val_score(clf, music, music_labels, cv=folds, n_jobs=n_jobs, verbose=verbose)\ndisplay_scores(scores)","2e90b36f":"clf = SVC(kernel=\"poly\", degree=5, coef0=1, C=5)\nscores = cross_val_score(clf, music, music_labels, cv=folds, n_jobs=n_jobs, verbose=verbose)\ndisplay_scores(scores)","8a4c7381":"clf = SVC(kernel=\"rbf\", gamma=5, C=1)\nscores = cross_val_score(clf, music, music_labels, cv=folds, n_jobs=n_jobs, verbose=verbose)\ndisplay_scores(scores)","f15edd75":"clf = KNeighborsClassifier(n_neighbors=3)\nscores = cross_val_score(clf, music, music_labels, cv=folds, n_jobs=n_jobs, verbose=verbose)\ndisplay_scores(scores)","3ad9d4fc":"music_valid = strat_valid_set.drop(\"label\", axis=1)\nmusic_valid_labels = strat_valid_set[\"label\"].copy()\n\nmusic_valid = scale(music_valid);\n\nparam_grid = [{'kernel':[\"poly\"], 'degree': [1,2,3,4,5],'gamma': [0.01, 0.1, 0.5], 'coef0': [0, 0.1, 1], 'C': [0.001, 0.01, 1, 5]}]\nsvm_poly = SVC()\ngrid_search = GridSearchCV(svm_poly, param_grid, cv=folds, n_jobs=n_jobs)\ngrid_search.fit(music_valid, music_valid_labels)","d53bcdc3":"grid_search.best_params_","349ff4e3":"def compute_ratio(matrix, score):\n    np.set_printoptions(precision=2)\n\n    FP = matrix[0,1]\n    FN = matrix[1,0]\n    TP = matrix[1,1]\n    TN = matrix[0,0]\n\n    FP = FP.astype(float)\n    FN = FN.astype(float)\n    TP = TP.astype(float)\n    TN = TN.astype(float)\n    \n    # Sensitivity or true positive rate\n    TPR = TP\/(TP+FN)\n    # Specificity or true negative rate\n    TNR = TN\/(TN+FP) \n        \n    print(\"Accuracy:   %0.3f\" % score)    \n    print(\"Specificity or true negative rate (AVG):   %0.3f\" % TNR)\n    print(\"Sensitivity or true positive rate (AVG):   %0.3f\" % TPR)","4a2c4285":"X_test = strat_test_set.drop(\"label\", axis=1)\ny_test = strat_test_set[\"label\"].copy()\nX_test_prepared = scale(X_test)","d438cd32":"clf = grid_search.best_estimator_\n\ny_expected = music_valid_labels\ny_predic = clf.predict(music_valid)\n    \n# Computes the accuracy (the fraction) of correct predictions.\nscore = metrics.accuracy_score(y_expected, y_predic)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_expected, y_predic)\ncompute_ratio(cnf_matrix, score)","cde8c30f":"y_expected = music_labels\ny_predic = clf.predict(music)\n    \n# Computes the accuracy (the fraction) of correct predictions.\nscore = metrics.accuracy_score(y_expected, y_predic)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_expected, y_predic)\ncompute_ratio(cnf_matrix, score)","1bb317bf":"clf = grid_search.best_estimator_\n\ny_expected = y_test\ny_predic = clf.predict(X_test_prepared)\n    \n# Computes the accuracy (the fraction) of correct predictions.\nscore = metrics.accuracy_score(y_expected, y_predic)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_expected, y_predic)\ncompute_ratio(cnf_matrix, score)","8ac4839d":"df_pca= df\ndf_pca['label']=pd.Categorical(df_pca['label'])\nmy_color=df['label'].cat.codes\ndf_pca = df_pca.drop('label', 1)\n\nplt.figure(1);\n# Get current size\nfig_size = plt.rcParams[\"figure.figsize\"]\n\n# Set figure width to 12 and height to 9\nfig_size[0] = 12\nfig_size[1] = 9\nplt.rcParams[\"figure.figsize\"] = fig_size","33baaa53":"y = my_color\nX = df.drop('label', 1)\n\n\n#In general a good idea is to scale the data\nscaler = StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)    \n\npca = PCA()\nx_new = pca.fit_transform(X)\n\ndef myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0\/(xs.max() - xs.min())\n    scaley = 1.0\/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = y)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.xlabel(\"PC{}\".format(1))\nplt.ylabel(\"PC{}\".format(2))\nplt.grid()\n\n#Call the function. Use only the 2 PCs.\nmyplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\nplt.show()","c52cc5c5":"print(np.array2string(pca.explained_variance_ratio_, formatter={'float_kind':lambda x: \"%.2f\" % (x*100)}))","32075dcf":"#### Accuracy test set","f45136cc":"### Standard correlation coefficient","ea9c96c8":"Dataset keeps the same proportion after stratified.","45e948b4":"#### Amount of variance does each PC explain","ad9659cf":"### Fine-Tune - Grid Search","7e04d89a":"### Plot 2D PCA","9a790dac":"#### SVM Linear","a99e5c6a":"### Cross-Validation\n\n\"A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k \u201cfolds\u201d\"","09f7923e":"###  Principal Component Analysis","d51ea8d3":"### Accuracy","10041261":"#### SVM Polynomial","deee97e3":"#### KNNeighbors","91dbbfb1":"#### Accuracy train set","7ede7a00":"#### SVM RBF","75b954e9":"#### Accuracy validation set"}}