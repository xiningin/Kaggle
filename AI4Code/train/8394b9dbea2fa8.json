{"cell_type":{"cabaded9":"code","cc6268e7":"code","c30f30d8":"code","348d28af":"code","ef571aee":"code","5b54ef09":"code","cba8c20d":"code","ed9011db":"code","13b3cd0d":"code","8d810b72":"code","6b243d08":"code","75a6536f":"code","39b1b046":"code","49341369":"code","593ed455":"code","82cf359a":"code","78752b48":"code","2fd801a9":"code","df0267f7":"code","c61bb045":"code","bb85caed":"code","ac6f60df":"code","c5690e45":"code","0064e65f":"code","9c787f2c":"code","93e37376":"code","f8e56487":"markdown","90ec9718":"markdown","514419ad":"markdown","859fcaca":"markdown","dfa1795d":"markdown","5775cbeb":"markdown","a68e4d3d":"markdown","f2323f53":"markdown"},"source":{"cabaded9":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport pickle\nimport optuna\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport re\nimport nltk\nimport string\nfrom textblob import TextBlob\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import BayesianRidge, Lasso, ElasticNet","cc6268e7":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","c30f30d8":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","348d28af":"_, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.boxplot(x='target', data=train_df, ax=ax[0])\nsns.histplot(x='target', data=train_df, ax=ax[1])\nax[0].title.set_text('Box Plot - target')\nax[1].title.set_text('Hist Plot - target')","ef571aee":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(\"Ytrain: {}\".format(Ytrain.shape))","5b54ef09":"def decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","cba8c20d":"def dialog_parser(text):\n    \n    tokenized = nltk.word_tokenize(text)\n    \n    # let's set up some lists to hold our pieces of narrative and dialog\n    parsed_dialog = []\n    parsed_narrative = []\n    \n    # and this list will be a bucket for the text we're currently exploring\n    current = []\n\n    # now let's set up values that will help us loop through the text\n    length = len(tokenized)\n    found_q = False\n    counter = 0\n    quote_open, quote_close = '``', \"''\"\n\n    # now we'll start our loop saying that as long as our sentence is...\n    while counter < length:\n        word = tokenized[counter]\n\n        # until we find a quotation mark, we're working with narrative\n        if quote_open not in word and quote_close not in word:\n            current.append(word)\n\n        # here's what we do when we find a closed quote\n        else:\n            # we append the narrative we've collected & clear our our\n            # current variable\n            parsed_narrative.append(current)\n            current = []\n            \n            # now current is ready to hold dialog and we're working on\n            # a piece of dialog\n            current.append(word)\n            found_q = True\n\n            # while we're in the quote, we're going to increment the counter\n            # and append to current in this while loop\n            while found_q and counter < length-1:\n                counter += 1\n                if quote_close not in tokenized[counter]:\n                    current.append(tokenized[counter])\n                else:\n                    # if we find a closing quote, we add our dialog to the\n                    # appropriate list, clear current and flip our found_q\n                    # variable to False\n                    current.append(tokenized[counter])\n                    parsed_dialog.append(current)\n                    current = []\n                    found_q = False\n\n        # increment the counter to move us through the text\n        counter += 1\n    \n    if len(parsed_narrative) == 0:\n        parsed_narrative.append(current)\n    \n    mean_dialog_word_len = 0\n    \n    if len(parsed_dialog) > 0:\n        for text in parsed_dialog:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_dialog_word_len += len(join_text.split())\n        \n        mean_dialog_word_len \/= float(len(parsed_dialog))\n    \n    mean_narrative_word_len = 0\n    \n    if len(parsed_narrative) > 0:\n        for text in parsed_narrative:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_narrative_word_len += len(join_text.split())\n        \n        mean_narrative_word_len \/= float(len(parsed_narrative))\n\n    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len","ed9011db":"def remove_punctuations(text):\n    punct = []\n    punct += list(string.punctuation)\n    punct += '\u2019'\n    punct += '-'\n    punct += ','\n    punct += '.'\n    punct += '?'\n    punct += '!'\n    punct.remove('\"')\n    \n    for punctuation in punct:\n        text = text.replace(punctuation, '')\n    return text","13b3cd0d":"def lemmatize_words(text):\n    lemmatizer = WordNetLemmatizer()\n    wordnet_map = {\n        \"N\": wordnet.NOUN, \n        \"V\": wordnet.VERB, \n        \"J\": wordnet.ADJ, \n        \"R\": wordnet.ADV\n    }\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])","8d810b72":"combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n\ndel train_df\ndel test_df\ngc.collect()\n\ncombined_df.head()","6b243d08":"punct = []\npunct += list(string.punctuation)\npunct += '\u2019'\npunct += '-'\npunct += ','\npunct += '.'\npunct += '?'\npunct += '!'\n\n\ncombined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ncombined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ncombined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\ncombined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in set(stopwords.words('english'))]))\ncombined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in punct]))\ncombined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ncombined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ncombined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ncombined_df.head()","75a6536f":"# Convert to lower case\ncombined_df['Processed_excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n\n# Remove double spaces\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n\n# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: decontraction(x))\n\n# Remove punctuations\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(remove_punctuations)\n\n# Lemmatize words\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: lemmatize_words(text))\n\ncombined_df.head()","39b1b046":"combined_df[\"excerpt_num_dialog\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[0])\ncombined_df[\"excerpt_num_narrative\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[1])\ncombined_df[\"excerpt_dialog_mean_word_len\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[2])\ncombined_df[\"excerpt_narrative_mean_word_len\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[3])\ncombined_df['excerpt_polarity'] = combined_df['Processed_excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\ncombined_df['excerpt_subjectivity'] = combined_df['Processed_excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\ncombined_df.head()","49341369":"df = combined_df[:Ytrain.shape[0]].copy()\ndf['target'] = Ytrain\nplt.figure(figsize=(15,12))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")","593ed455":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,5), \n                        analyzer='word', max_df=0.95, min_df=3, \n                        use_idf=1, sublinear_tf=1, \n                        max_features=500, strip_accents='ascii')\n\nfeatures = tfidf.fit_transform(combined_df.Processed_excerpt).toarray()\n\nfeatures_df = pd.DataFrame(features, \n                           columns=tfidf.get_feature_names(), \n                           index=combined_df.index)\n\ncombined_df = pd.merge(combined_df, \n                       features_df, \n                       left_index=True, \n                       right_index=True)\n\ncombined_df.shape","82cf359a":"for i in range(3):\n    print(f\"\\nOriginal Excerpt: \\n{combined_df.iloc[i]['excerpt']} \\n\\nProcessed Excerpt: \\n{combined_df.iloc[i]['Processed_excerpt']}\\n\")\n    print(\"=\"*150)","78752b48":"def sent2vec(text):\n    words = nltk.word_tokenize(text)\n    words = [w for w in words if w.isalpha()]\n    \n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    \n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    \n    return v \/ np.sqrt((v ** 2).sum())","2fd801a9":"with open(\"..\/input\/gloveembeddings\/Glove_840B_300d_Embeddings.txt\", 'rb') as handle: \n    data = handle.read()\n\nprocessed_data = pickle.loads(data)\nembeddings_index = processed_data['embeddings_index']\nprint('Word vectors found: {}'.format(len(embeddings_index)))\n\ndel processed_data\ngc.collect()","df0267f7":"glove_vec = [sent2vec(x) for x in tqdm(combined_df[\"Processed_excerpt\"].values)]\ncol_list = ['glove_'+str(i) for i in range(300)]\nglove_vec_df = pd.DataFrame(np.array(glove_vec), columns=col_list, index=combined_df.index)\nprint(f\"glove_vec_df: {glove_vec_df.shape}\\n\")\nglove_vec_df.head()","c61bb045":"combined_df = pd.merge(combined_df, glove_vec_df, how=\"inner\", on=\"id\", sort=False)\ncombined_df.drop(['excerpt','Processed_excerpt'], inplace=True, axis=1)\nprint(f\"combined_df: {combined_df.shape}\\n\")\ncombined_df.head()","bb85caed":"Xtrain = combined_df[:Ytrain.shape[0]].copy()\nXtest = combined_df[Ytrain.shape[0]:].copy()\nprint(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")\n\ndel combined_df\ngc.collect()","ac6f60df":"for col in tqdm(Xtrain.columns):\n    transformer = QuantileTransformer(n_quantiles=1000, \n                                      random_state=10, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(Xtrain[col].values)\n    vec_len_test = len(Xtest[col].values)\n\n    raw_vec = Xtrain[col].values.reshape(vec_len, 1)\n    test_vec = Xtest[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    Xtrain[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    Xtest[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(\"Xtrain: {} \\nYtrain: {} \\nXtest: {}\".format(Xtrain.shape, Ytrain.shape, Xtest.shape))","c5690e45":"FOLD = 5\nNUM_SEED = 3\nCOUNTER = 0\n\nnp.random.seed(0)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\n#oof_score_ridge = 0\noof_score_lasso = 0\noof_score_enet = 0\n#oof_score_gbr = 0\noof_score_lgb = 0\noof_score_xgb = 0\n\n#y_pred_final_ridge = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_lasso = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_enet = np.zeros((Xtest.shape[0], NUM_SEED))\n#y_pred_final_gbr = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_lgb = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_xgb = np.zeros((Xtest.shape[0], NUM_SEED))\n\n#y_pred_meta_ridge = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_lasso = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_enet = np.zeros((Ytrain.shape[0], NUM_SEED))\n#y_pred_meta_gbr = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_lgb = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_xgb = np.zeros((Ytrain.shape[0], NUM_SEED))","0064e65f":"print(\"Model Name \\t\\t\\tSeed \\t\\tFold \\t\\tOOF Score \\t\\tAggregate OOF Score\")\n\nfor sidx, seed in enumerate(seeds):\n    #seed_score_ridge = 0\n    seed_score_lasso = 0\n    seed_score_enet = 0\n    #seed_score_gbr = 0\n    seed_score_lgb = 0\n    seed_score_xgb = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        COUNTER += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain[val]\n        \n        \n        #====================================================================\n        #                           Bayesian Ridge\n        #====================================================================\n        '''\n        ridge_model = BayesianRidge()\n        ridge_model.fit(train_x, train_y)\n        \n        y_pred = ridge_model.predict(val_x)\n        y_pred_meta_ridge[val, sidx] = y_pred\n        y_pred_final_ridge[:, sidx] += ridge_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_ridge += score\n        seed_score_ridge += score\n        print(f\"\\nBayesian Ridge \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        '''\n        \n        #====================================================================\n        #                                 Lasso\n        #====================================================================\n        \n        lasso_model = Lasso(alpha=0.025, max_iter=2000, random_state=0)\n        lasso_model.fit(train_x, train_y)\n        \n        y_pred = lasso_model.predict(val_x)\n        y_pred_meta_lasso[val, sidx] = y_pred\n        y_pred_final_lasso[:, sidx] += lasso_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_lasso += score\n        seed_score_lasso += score\n        print(f\"Lasso         \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                              ElasticNet\n        #====================================================================\n        \n        enet_model = ElasticNet(alpha=0.025, max_iter=2000, random_state=0)\n        enet_model.fit(train_x, train_y)\n        \n        y_pred = enet_model.predict(val_x)\n        y_pred_meta_enet[val, sidx] = y_pred\n        y_pred_final_enet[:, sidx] += enet_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_enet += score\n        seed_score_enet += score\n        print(f\"ElasticNet \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                     HistGradientBoostingRegressor\n        #====================================================================\n        '''\n        gbr_model = HistGradientBoostingRegressor(max_depth=6, max_leaf_nodes=52, random_state=0)\n        gbr_model.fit(train_x, train_y)\n        \n        y_pred = gbr_model.predict(val_x)\n        y_pred_meta_gbr[val, sidx] = y_pred\n        y_pred_final_gbr[:, sidx] += gbr_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_gbr += score\n        seed_score_gbr += score\n        print(f\"GradientBoostingRegressor \\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        '''\n        \n        #====================================================================\n        #                                LightGBM\n        #====================================================================\n        \n        params = {}\n        params[\"objective\"] = 'regression'\n        params[\"metric\"] = 'rmse'\n        params[\"boosting\"] = 'gbdt'\n        params[\"learning_rate\"] = 0.0204\n        params[\"lambda_l2\"] = 0.225\n        params[\"num_leaves\"] = 52\n        params[\"max_depth\"] = 6\n        params[\"feature_fraction\"] = 0.75\n        params[\"bagging_fraction\"] = 0.65\n        params[\"bagging_freq\"] = 10\n        params[\"min_data_in_leaf\"] = 15\n        params[\"verbosity\"] = -1\n        num_rounds = 5000\n        \n        lgtrain = lgb.Dataset(train_x, label=train_y.ravel())\n        lgvalidation = lgb.Dataset(val_x, label=val_y.ravel())\n\n        lgb_model = lgb.train(params, lgtrain, num_rounds, \n                              valid_sets=[lgtrain, lgvalidation], \n                              early_stopping_rounds=100, verbose_eval=False)\n\n        y_pred = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n        y_pred_meta_lgb[val, sidx] = y_pred\n        y_pred_final_lgb[:, sidx] += lgb_model.predict(Xtest, num_iteration=lgb_model.best_iteration)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_lgb += score\n        seed_score_lgb += score\n        print(f\"LightGBM       \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                                XGBoost\n        #====================================================================\n        \n        xgb_model = XGBRegressor(\n            objective='reg:squarederror',\n            eval_metric='rmse',\n            booster='gbtree',\n            sample_type='uniform',\n            tree_method='hist',\n            grow_policy='lossguide',\n            num_round=5000,\n            #max_depth=11, \n            max_leaves=55,\n            learning_rate=0.074,\n            subsample=0.74,\n            colsample_bytree=0.675,\n            min_child_weight=7,\n            reg_lambda=0.152,\n            verbosity=0\n        )\n\n        xgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], \n                      early_stopping_rounds=200, verbose=False)\n\n        y_pred = xgb_model.predict(val_x, iteration_range=(0, xgb_model.best_iteration))\n        y_pred_meta_xgb[val, sidx] += y_pred\n        y_pred_final_xgb[:, sidx] += xgb_model.predict(Xtest, iteration_range=(0, xgb_model.best_iteration))\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_xgb += score\n        seed_score_xgb += score\n        print(f\"XGBoost     \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\\n\")\n        \n    print(\"=\"*110)\n    #print(f\"Bayesian Ridge \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_ridge \/ FOLD)}\")\n    print(f\"Lasso         \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_lasso \/ FOLD)}\")\n    print(f\"ElasticNet \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_enet \/ FOLD)}\")\n    #print(f\"GradientBoostingRegressor \\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_gbr \/ FOLD)}\")\n    print(f\"LightGBM       \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_lgb \/ FOLD)}\")\n    print(f\"XGBoost     \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_xgb \/ FOLD)}\")\n    print(\"=\"*110)","9c787f2c":"#y_pred_final_ridge = y_pred_final_ridge \/ float(FOLD)\ny_pred_final_lasso = y_pred_final_lasso \/ float(FOLD)\ny_pred_final_enet = y_pred_final_enet \/ float(FOLD)\n#y_pred_final_gbr = y_pred_final_gbr \/ float(FOLD)\ny_pred_final_lgb = y_pred_final_lgb \/ float(FOLD)\ny_pred_final_xgb = y_pred_final_xgb \/ float(FOLD)\n\n#oof_score_ridge \/= float(COUNTER)\noof_score_lasso \/= float(COUNTER)\noof_score_enet \/= float(COUNTER)\n#oof_score_gbr \/= float(COUNTER)\noof_score_lgb \/= float(COUNTER)\noof_score_xgb \/= float(COUNTER)\n\n#print(f\"Bayesian Ridge | Aggregate OOF Score: {oof_score_ridge}\")\nprint(f\"Lasso | Aggregate OOF Score: {oof_score_lasso}\")\nprint(f\"ElasticNet | Aggregate OOF Score: {oof_score_enet}\")\n#print(f\"GradientBoostingRegressor | Aggregate OOF Score: {oof_score_gbr}\")\nprint(f\"LightGBM | Aggregate OOF Score: {oof_score_lgb}\")\nprint(f\"XGBoost | Aggregate OOF Score: {oof_score_xgb}\")","93e37376":"y_pred_final = (y_pred_final_lgb * 0.65) + (y_pred_final_enet * 0.15) + (y_pred_final_xgb * 0.1) + (y_pred_final_lasso * 0.1) \n\nsubmit_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","f8e56487":"## Quantile Transformation","90ec9718":"## Import libraries","514419ad":"## Glove Embeddings","859fcaca":"## Extract target label","dfa1795d":"## Load source datasets","5775cbeb":"## Base Models\n\n* **BayesianRidge** (Unused)\n* **Lasso**\n* **ElasticNet**\n* **HistGradientBoostingRegressor** (Unused)\n* **LightGBM**\n* **XGBoost**","a68e4d3d":"## Feature Engineering","f2323f53":"## Create submission file"}}