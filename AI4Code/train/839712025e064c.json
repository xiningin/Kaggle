{"cell_type":{"1d5efa96":"code","8b660582":"code","60e2608c":"code","80acde99":"code","8e707ee8":"code","2ac572a7":"code","6a327213":"code","e206ea4e":"code","9dc51e5b":"code","7a69c015":"code","541b0297":"code","63462c0a":"code","80738537":"code","9a6ea0d1":"code","69d0c8e6":"code","825a63ee":"code","1bcc0faf":"code","054de9f9":"code","7650ebb4":"code","6ad6cab7":"code","7b04ff75":"code","d59a9a33":"code","d8b8bdff":"code","0cfd3c10":"code","7785e8aa":"code","d008a6b9":"code","502ef087":"code","197988e0":"markdown","15af3d8a":"markdown","c20ff5f9":"markdown","4c67288d":"markdown","415eccf4":"markdown","184a2118":"markdown","ef083d59":"markdown"},"source":{"1d5efa96":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\n\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# fastai\nimport fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed","8b660582":"NNBATCHSIZE = 96\nGROUP_BATCH_SIZE = 4000\nSEED = 80085","60e2608c":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n        \nseed_all(SEED)","80acde99":"# plotting functions\n\ndef plot_train(train_df, label, is_train=True):\n    label = f\"{label} {'Training' if is_train else 'Testing'}\" \n    plt.figure(figsize=(20,5)); res = 1\n    plt.plot(range(0,train_df.shape[0],res),train_df.signal[0::res])\n    for i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\n    for j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\n    plt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \n    plt.title(label + ' Data Signal - 10 batches',size=20)\n    plt.show()\n    \n    \ndef plot_test(test_df, label):\n    plt.figure(figsize=(20,5))\n    let = ['A','B','C','D','E','F','G','H','I','J']\n    r = test_df.signal.rolling(30000).mean()\n    plt.plot(test_df.time.values,r)\n    for i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\n    for i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\n    for k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\n    for k in range(10): plt.text(505+k*10,4,let[k],size=16)\n    plt.title(f'{label} Test Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\n    plt.show()","8e707ee8":"orig_df = pd.read_csv(\n    '\/kaggle\/input\/liverpool-ion-switching\/train.csv', \n    dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32}\n)\n\norig_test = pd.read_csv(\n    '\/kaggle\/input\/liverpool-ion-switching\/test.csv', \n    dtype={'time': np.float32, 'signal': np.float32}\n)\n\nclean_df = pd.read_csv(\n    '\/kaggle\/input\/data-without-drift\/train_clean.csv', \n    dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32}\n)","2ac572a7":"# read data\ndef read_data(orig=False):\n    sub  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\n    if orig:\n        train = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n        test  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv', dtype={'time': np.float32, 'signal': np.float32})\n    else:\n        train = pd.read_csv('\/kaggle\/input\/data-without-drift\/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n        test  = pd.read_csv('\/kaggle\/input\/data-without-drift\/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})    \n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    #print(df)\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    from sklearn.preprocessing import StandardScaler\n    \n    scaler = StandardScaler()\n    train = scaler.fit_transform(train.values.reshape(-1, 1)).flatten()\n    test = scaler.transform(test.values.reshape(-1, 1)).flatten()\n    \n    return train, test, scaler\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows, prefix):\n    train_cols = []\n    for window in windows:    \n        df[prefix + '_signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df[prefix + '_signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n        train_cols.append(prefix + '_signal_shift_pos_' + str(window))\n        train_cols.append(prefix + '_signal_shift_neg_' + str(window))\n    return df, train_cols\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size, prefix=''):\n    # create batches\n    df = batching(df, batch_size=batch_size)\n\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df, cols = lag_with_pct_change(df, list(range(1, 8)), prefix)\n    return df, cols\n\ndef split(GROUP_BATCH_SIZE=4000, SPLITS=5, noisy=False):\n    print('Reading Data Started...')\n    orig_train, orig_test, sample_submission = read_data(orig=True)\n    clean_train, clean_test, sample_submission = read_data(orig=False)\n    \n    orig_train['signal'], clean_train['signal'], scaler = normalize(orig_train['signal'], clean_train['signal'])\n    print('Reading and Normalizing Data Completed')\n    \n    print('Feature Engineering Started...')\n    orig_train, orig_train_cols = run_feat_engineering(orig_train, batch_size=GROUP_BATCH_SIZE, prefix='orig')\n    clean_train = batching(clean_train, batch_size=GROUP_BATCH_SIZE)\n    \n    orig_train = orig_train.rename({\n        'signal': 'orig_signal'\n    }, axis='columns')\n    clean_train = clean_train.rename({\n        'signal': 'clean_signal',\n    }, axis='columns')\n    orig_train_cols.append('orig_signal')\n    \n    features = [col for col in clean_train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    clean_train = clean_train[features] \n    \n    joint_df = pd.concat([orig_train, clean_train], axis=1)\n    print(joint_df.columns)\n    print('Feature Engineering Completed...')\n\n    X = np.array(list(joint_df.groupby('group').apply(lambda x: x[orig_train_cols].values))).astype(np.float32)\n    y = np.array(list(joint_df.groupby('group').apply(lambda x: x['clean_signal'].values))).astype(np.float32)\n    \n    \n    # preprocess the original test data too, so we can run prediction and match against clean test data.\n    orig_test['signal'] = scaler.transform(orig_test['signal'].values.reshape(-1, 1)).flatten()\n    orig_test, orig_test_cols = run_feat_engineering(orig_test, batch_size=GROUP_BATCH_SIZE, prefix='orig_test')\n    orig_test_cols.append('signal')\n    X_test = np.array(list(orig_test.groupby('group').apply(lambda x: x[orig_test_cols].values))).astype(np.float32)\n\n    return X, np.expand_dims(y, axis=-1), X_test, clean_test, scaler","6a327213":"%%time\nX, y, X_test, clean_test, scaler = split()","e206ea4e":"X.shape, y.shape, X_test.shape, clean_test.shape","9dc51e5b":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\nX_train.shape, y_train.shape","7a69c015":"from torch.utils.data import Dataset, DataLoader\nclass IonDataset(Dataset):\n    def __init__(self, data, labels=None):\n        self.data = data\n        self.train_mode = labels is not None\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data[idx]\n        \n        if self.train_mode:\n            labels = self.labels[idx]\n            return [data.astype(np.float32), labels.astype(np.float32)]\n        else:\n            return data.astype(np.float32)","541b0297":"# from https:\/\/www.kaggle.com\/hanjoonchoe\/wavenet-lstm-pytorch-ignite-ver        \nclass Wave_Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,dilation_rates):\n        super(Wave_Block,self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n        \n        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n        dilation_rates = [2**i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n            \n    def forward(self,x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i+1](x)\n            res = torch.add(res, x)\n        return res\n    \n    \nclass Encoder(nn.Module):\n    def __init__(self, nc, hidden_size, input_size):\n        super().__init__()\n        self.lstm1 = nn.GRU(\n            input_size=input_size, \n            hidden_size=hidden_size, \n            num_layers=2,\n            dropout=0.2,\n            batch_first=True, \n            bidirectional=True\n        )\n        \n        self.encoder = nn.Sequential(\n            Wave_Block(nc, 64, 4),\n            nn.BatchNorm1d(64),\n            Wave_Block(64, input_size, 1),\n            nn.BatchNorm1d(input_size)\n        )\n        \n    def forward(self, x):\n        # ---- Encoder ----\n        x = x.permute(0, 2, 1)\n        x = self.encoder(x)\n        \n        # ---- Bottleneck ----\n        x = x.permute(0, 2, 1)\n        x, _ = self.lstm1(x)\n        \n        return x\n    \nclass Model(nn.Module):\n    def __init__(self, nc, hidden_size=128):\n        super().__init__()\n        input_size = 128\n\n        self.encoder = Encoder(nc, hidden_size, input_size)\n        \n        self.decoder = nn.Sequential(\n            Wave_Block(hidden_size*2, 64, 1),\n            nn.BatchNorm1d(64),\n            Wave_Block(64, nc, 1),\n            nn.BatchNorm1d(nc)\n        )\n        \n        self.fc = nn.Linear(nc, 1)\n            \n    def forward(self,x):\n        x = self.encoder(x)        \n        \n        # ---- Decoder ----\n        x = x.permute(0, 2, 1)\n        x = self.decoder(x)\n        \n        # ---- Output ----\n        x = x.permute(0, 2, 1)\n        x = self.fc(x)\n        \n        return x","63462c0a":"epochs = 150\nfolds_to_train = [0]\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","80738537":"time2train = 25 * len(folds_to_train) * epochs \/ 3600\nf\"{time2train:.02f} hours\"","9a6ea0d1":"torch.cuda.empty_cache()\n\ntrain_dataset = IonDataset(X_train, y_train)\nvalid_dataset = IonDataset(X_val, y_val)\n\nnc = X_train.shape[-1]\nmodel = Model(nc)\n\ndb = DataBunch.create(train_dataset, valid_dataset, bs=NNBATCHSIZE)\nlearn = Learner(db, model, loss_func=nn.MSELoss())\nlearn.callbacks.append(ShowGraph(learn))\nlearn.callbacks.append(CSVLogger(learn, filename=f\"history\"))\n# learn.callbacks.append(SaveModelCallback(learn, name=f'model_fold_{index}'))","69d0c8e6":"# learn.lr_find()\n# learn.recorder.plot()\nlearn.fit_one_cycle(epochs, max_lr=1e-2)\nlearn.save('denoiser')","825a63ee":"def run_pred(dataset, scaler):\n    pred_list = []\n    pred_loader = DataLoader(IonDataset(dataset, None), NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n\n    with torch.no_grad():\n        for x in tqdm(pred_loader):\n            x = x.to(device)\n\n            predictions = model(x)\n            predictions = predictions.view(-1, predictions.shape[-1])\n            pred_list.extend(predictions.cpu().numpy().tolist())\n            \n    vals = np.array(pred_list)\n    vals = scaler.inverse_transform(vals.reshape(-1, 1)).flatten()\n    return vals","1bcc0faf":"vals = run_pred(X, scaler)","054de9f9":"# inverse transform to get the values back\ndenoised_df = orig_df.copy()\ndenoised_df['signal'] = vals\ndenoised_df.to_csv('denoised_df.csv', index=False)\n\nfrom sklearn.metrics import mean_squared_error\ndiff = abs(clean_df['signal'] - denoised_df['signal'])\nmse = mean_squared_error(clean_df['signal'], denoised_df['signal'])\nprint(f\"Mean: {diff.mean()}, std: {diff.std()}, mse: {mse}\")","7650ebb4":"plot_train(orig_df, label=\"Orig\", is_train=True)","6ad6cab7":"plot_train(clean_df, label=\"Cleaned\", is_train=True)","7b04ff75":"plot_train(denoised_df, label=\"Denoised\", is_train=True)","d59a9a33":"vals = run_pred(X_test, scaler)","d8b8bdff":"denoised_test_df = clean_test.copy()\ndenoised_test_df['signal'] = vals\ndenoised_test_df.to_csv('denoised_test_df.csv', index=False)","0cfd3c10":"diff = abs(clean_test['signal'] - denoised_test_df['signal'])\nmse = mean_squared_error(clean_test['signal'], denoised_test_df['signal'])\nprint(f\"Mean: {diff.mean()}, std: {diff.std()}, mse: {mse}\")","7785e8aa":"plot_test(orig_test, label=\"Orig\")","d008a6b9":"plot_test(clean_test, label=\"Clean\")","502ef087":"plot_test(denoised_test_df, label=\"Denoised\")","197988e0":"# Train set denoising","15af3d8a":"# Test set denoising","c20ff5f9":"# Preprocessing\nMuch of the code has been borrowed from up there, with little modifications thrown here and there :p","4c67288d":"# Dataset","415eccf4":"# This is an attempt towards automatic drift correction and denoising the signals using LSTM Autoencoders.\n#####  Refered kernels:\n1. Chris Deotte: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n1. TJ Klein: https:\/\/www.kaggle.com\/friedchips\/clean-removal-of-data-drift\n1. Mobassir: https:\/\/www.kaggle.com\/mobassir\/understanding-ion-switching-with-modeling","184a2118":"# Denoising Model: Wavenet-LSTM-Autoencoder","ef083d59":"# Training"}}