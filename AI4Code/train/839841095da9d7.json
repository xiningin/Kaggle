{"cell_type":{"64b94957":"code","c30639c0":"code","477d60f4":"code","b56ac93a":"code","a5d83a67":"code","4dab055a":"code","d0c03527":"code","719fde0a":"code","c02149b8":"code","16dab26e":"code","e0a34425":"code","50c66e52":"code","7537c1bf":"code","c53a09c6":"code","ec8abb30":"code","43a83732":"code","068b52b1":"code","625711fc":"code","afa1d186":"code","e6674b83":"code","860f872d":"code","ec130287":"code","9f084fc9":"code","4783b35a":"code","e08271e1":"code","3627a528":"code","52bcd626":"code","313abb75":"code","695b4659":"code","5cc5e0e1":"markdown","3387539a":"markdown","b1359a1b":"markdown","2935d66d":"markdown"},"source":{"64b94957":"import sys\n!cp ..\/input\/rapids\/rapids.0.13.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","c30639c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage import measure\nimport seaborn as sns\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nfrom nilearn import image\nfrom nilearn import plotting\nfrom nilearn import datasets\nfrom nilearn import surface\nimport h5py\nimport plotly.graph_objs as go\nfrom IPython.display import Image, display\nimport random\nimport os\nimport cudf\nimport cupy as cp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","477d60f4":"%%time\n# Reading all the required files using pandas\nloading_data = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/loading.csv\")\ntrain_data = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/sample_submission.csv')\nfnc = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/fnc.csv')\nICN_numbers = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nreveal_ID_site2 = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/reveal_ID_site2.csv')","b56ac93a":"%%time\n# Reading all the required files using cudf\nloading_data = cudf.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/loading.csv\")\ntrain_data = cudf.read_csv('\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv')\nsample_submission = cudf.read_csv('\/kaggle\/input\/trends-assessment-prediction\/sample_submission.csv')\nfnc = cudf.read_csv('\/kaggle\/input\/trends-assessment-prediction\/fnc.csv')\nICN_numbers = cudf.read_csv('\/kaggle\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nreveal_ID_site2 = cudf.read_csv('\/kaggle\/input\/trends-assessment-prediction\/reveal_ID_site2.csv')","a5d83a67":"# Understanding the structure of the given files\nprint('The shape of loading data is =',loading_data.shape)\nprint('The shape of training scores is =',train_data.shape)\nprint('The shape of sample submission is =',sample_submission.shape)\nprint('The shape of fnc data is =',fnc.shape)\nprint('The shape of ICN numbers data is =',ICN_numbers.shape)\nprint('The shape of reveal Id site2 is =',reveal_ID_site2.shape)\nprint('')","4dab055a":"# Data Pre-Processing\n\n# Checking for missing data\n# Training Data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint('')\nprint('\\n','Missing Training Data','\\n',missing_train_data.head())\n\n# Loading Data\ntotal = loading_data.isnull().sum().sort_values(ascending = False)\npercent = (loading_data.isnull().sum()\/loading_data.isnull().count()*100).sort_values(ascending = False)\nmissing_loading_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint('')\nprint('Missing Loading Data','\\n',missing_loading_data.head())# No null values\n\n# fnc Data\ntotal = fnc.isnull().sum().sort_values(ascending = False)\npercent = (fnc.isnull().sum()\/fnc.isnull().count()*100).sort_values(ascending = False)\nmissing_fnc_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint('')\nprint('Missing FNC Data','\\n',missing_fnc_data.head())# No null values","d0c03527":"#Using a heat-map to understand correlation between vaiables\nmap_data =  loading_data.drop(['Id'], axis=1)\nplt.figure(figsize = (30, 10))\nsns.heatmap(map_data.corr(), annot = True)\nplt.yticks(rotation=0) \nplt.show()","719fde0a":"#Using a bar plot to understand the overall distribution\n#Helper Fuction to plot a bar graph\ndef plot_bar(df, feature, title='', show_percent = False, size=2):\n    f, ax = plt.subplots(1,1, figsize=(5*size,2*size))\n    total = float(len(df))\n    sns.barplot(np.round(df[feature].value_counts().index).astype(int), df[feature].value_counts().values, alpha=0.8,orient='o', \n                palette='Set1')\n\n    plt.title(title)\n    if show_percent:\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()\/2.0,\n                    height + 3,\n                    '{:1.2f}%'.format(100*height\/total),\n                    ha=\"center\", rotation=0) \n    plt.xlabel(feature, fontsize=12, )\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xticks(rotation=0)\n    plt.show()\nplot_bar(train_data, 'age', 'Count and Percentage Plot', show_percent=True, size=4)","c02149b8":"smri = '\/kaggle\/input\/ch2better\/ch2better.nii'# for better clarity\nfmri_mask ='\/kaggle\/input\/trends-assessment-prediction\/fMRI_mask.nii'\nmask_img = nl.image.load_img(fmri_mask)\n\ndef load_subject(filename, mask_img):\n    subject_data = None\n    with h5py.File(filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_img = nl.image.new_img_like(mask_img, subject_data, affine=mask_img.affine, copy_header=True)\n\n    return subject_img","16dab26e":"files = random.choices(os.listdir('\/kaggle\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('\/kaggle\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    nlplt.plot_prob_atlas(subject_img, bg_img=smri, view_type='filled_contours',\n                          draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n    print(\"-\"*50)","e0a34425":"import os\nfrom time import time\nimport math\nimport random\nimport gc\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam,SGD\nfrom keras.callbacks import Callback\nfrom numpy.random import seed\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom time import gmtime, strftime\nimport cudf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(1994)","50c66e52":"%%time\n# Re-importing using cudf for code clarity\ntrain = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtype={'Id':str}).dropna().reset_index(drop=True) # to make things easy\nreveal_ID = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv', dtype={'Id':str})\nICN_numbers = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nloading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtype={'Id':str})\nfnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv', dtype={'Id':str})\nsample_submission = pd.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv', dtype={'Id':str})\n# dtype={'Id':str}->converting Id to string\n#reset_index(drop=True)>We can use the drop parameter to avoid the old index being added as a column","7537c1bf":"%%time\n# Re-importing using cudf for code clarity\ntrain = cudf.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtypes={'Id':str}).dropna().reset_index(drop=True) # to make things easy\nreveal_ID = cudf.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv', dtypes={'Id':str})\nICN_numbers = cudf.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nloading = cudf.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtypes={'Id':str})\nfnc = cudf.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv', dtypes={'Id':str})\nsample_submission = cudf.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv', dtypes={'Id':str})\n# dtype={'Id':str}->converting Id to string\n#reset_index(drop=True)>We can use the drop parameter to avoid the old index being added as a column","c53a09c6":"# Config\nID = 'Id'\ny_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nSEED = 26#https:\/\/www.geeksforgeeks.org\/random-seed-in-python\/","ec8abb30":"#lambda operator or lambda function is a way to create small anonymous functions, i.e. functions without a name\n#lambda argument_list: expression\n\n#>>> sum = lambda x, y : x + y\n#sum(3,4)\n#7\n\n# >>> def sum(x,y):\n# ...     return x + y\n# ... \n# >>> sum(3,4)\n# 7\n\n#.apply()\n#Pandas.apply allow the users to pass a function and apply it on every single value of the Pandas series\n\n#The astype() function is used to cast a pandas object to a specified data type.\n\n# It performs a blocking garbage collection of all generations. All objects, regardless of how long they have been in memory, \n# are considered for collection; however, objects that are referenced in managed code are not collected.\n# Use this method to force the system to try to reclaim the maximum amount of available memory.\n\nsample_submission['ID_num'] = sample_submission[ID].apply(lambda x: int(x.split('_')[0]))\ntest = pd.DataFrame({ID: sample_submission['ID_num'].unique().astype(str)})\ndel sample_submission['ID_num']; gc.collect()","43a83732":"#Removing unecessary fnc columns\nfnc[fnc.columns[1:]] =fnc[fnc.columns[1:]]\/600","068b52b1":"# merge\ntrain = train.merge(loading, on=ID, how='left')\ntrain = train.merge(fnc, on=ID, how='left')\n\ntest = test.merge(loading, on=ID, how='left')\ntest = test.merge(fnc, on=ID, how='left')","625711fc":"print('Loading Columns : ', len(loading.columns))\nprint()\nprint('FNC Columns : ', len(fnc.columns))\nprint()\nprint('Train Columns : ', len(train.columns))","afa1d186":"#Normalizing data to enable smooth operation of optimizer and loss function (time saving)\nfrom sklearn import preprocessing\ndef scaler(df):\n    for i in range(5, len(df.columns)-5):\n        col = df.iloc[:,i]\n        col = preprocessing.minmax_scale(col)\n    return df\ntrain = scaler(train)\ntrain = train.dropna(how='all').dropna(how='all', axis=1)\n#\u201caxis 0\u201d represents rows and \u201caxis 1\u201d represents columns","e6674b83":"X_train = train.drop('Id', axis=1).drop(y_cols, axis=1)\ny_train = train.drop('Id', axis=1)[y_cols]\nX_test = test.drop('Id', axis=1)\ny_test=test.drop('Id', axis=1)","860f872d":"# class myCallback(tf.keras.callbacks.Callback):\n#   def on_epoch_end(self, epoch, logs={}):\n#     if(logs.get('loss')<0.4):\n#       print(\"\\nReached 60% accuracy so cancelling training!\")\n#       self.model.stop_training = True\n\n# callbacks = myCallback()","ec130287":"def metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))\n","9f084fc9":"epochs= 50\nbatch_size = 128\nverbose = 1\nvalidation_split = 0.3\ninput_dim = X_train.shape[1]\nn_out = y_train.shape[1]\nactivation='selu'\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(2048,activation=activation,input_shape=(input_dim,)),#Normalized input layer\n    keras.layers.Dense(1164,activation=activation),#Fully connected layers\n    keras.layers.Dropout(0.8),\n    keras.layers.Dense(512,activation=activation),\n    keras.layers.Dense(128,activation=activation),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(64,activation=activation),\n    keras.layers.Dense(32,activation=activation),\n    keras.layers.Dense(16,activation=activation),\n    keras.layers.Dense(n_out)#Output Layer\n])\n\n\n\ninitial_learning_rate = 1e-3\nstep = tf.Variable(0, trainable=False)\nlearning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,decay_steps=100000,decay_rate=0.96,staircase=True)\nlr_schedule = learning_rate_fn(step)\n\nmodel.compile(loss='mse',\n              optimizer=Adam(learning_rate=lr_schedule),\n              metrics=['accuracy'])\n\nes= tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose =1 ,mode='min', patience =10)\nrp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min',factor =0.2,min_lr =1e-6,patience = 10)\n\nhist_1=model.fit(X_train, y_train,\n                        batch_size = batch_size,epochs=epochs,\n                        callbacks = [es,rp],\n                        verbose=verbose, validation_split=validation_split)\n\nmodel.summary()","4783b35a":"predicted_set = model.predict(X_test)\npredicted_set = pd.DataFrame(predicted_set)\npredicted_set.columns = y_train.columns\npredicted_set.head(10)","e08271e1":"d=hist_1.history\nval_mse=[]\nval_loss=[]\nloss=[]\nmse=[]\nval_mse.append(d['val_accuracy'])\nval_loss.append(d['val_loss'])\nloss.append(d['loss'])\nmse.append(d['accuracy'])","3627a528":"plt.figure(figsize=(8,6))\nplt.title('Learning Curve')\nplt.plot(d['val_loss'])\nplt.plot(d['loss'])\nplt.legend(['val_loss','train_loss'])\nplt.tight_layout()\nplt.grid()\nplt.ylabel('Loss')\nplt.xlabel('Epoch');","52bcd626":"plt.figure(figsize=(8,6))\nplt.title('Learning Curve')\nplt.plot(d['val_accuracy'])\nplt.plot(d['accuracy'])\nplt.legend(['val_accuracy ','accuracy'])\nplt.tight_layout()\nplt.grid()\nplt.ylabel('accuracy')\nplt.xlabel('Epoch');","313abb75":"df = pd.DataFrame()\nfor e in y_cols:\n    tmp = pd.DataFrame()\n    tmp[ID] = [f'{c}_{e}' for c in test[ID].values]\n    tmp['Predicted'] = predicted_set[e]\n    df = pd.concat([df, tmp])","695b4659":"submission = pd.merge(sample_submission, df, on = 'Id')[['Id', 'Predicted_y']]\nsubmission.columns = ['Id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","5cc5e0e1":"# Deep Neural Network for Predicting Age Using Keras","3387539a":"# Exploratory Data Analysis","b1359a1b":"# Using nilearn to Visuvalize a few Spatial Maps","2935d66d":"References:\n\nSpatial Image Visuvalization: https:\/\/www.kaggle.com\/soham1024\/visualization-using-nilearn , https:\/\/nilearn.github.io\/\n\nFor combining the features: https:\/\/www.kaggle.com\/tunguz\/rapids-randomforest-on-trends-neuroimaging\n\nDeep Learning : https:\/\/www.youtube.com\/results?search_query=jeff+heaton+deep\n\nKeras baseline model : https:\/\/www.kaggle.com\/kmatsuyama\/simple-nn-baseline-using-keras\n\nThis is my first notebook in kaggle. I have read through multiple notebooks to understand the essence of this competition. I am still learning.\nThis notebook will be modified frequently. \n\nPositive critisims are most welcomed. I look forward to improving my skills. \n\nIncase anyone feels a part of their work is present and not sited feel free to add it to the comments, I will add it to the references list.\n\nHope this notebook would be useful!"}}