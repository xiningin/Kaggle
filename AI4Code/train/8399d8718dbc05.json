{"cell_type":{"0c8b2f73":"code","eec17f28":"code","ff939ce2":"code","cde8a739":"code","175ed588":"code","fe24f7e2":"code","5fd19555":"code","7ac8458b":"code","288cdf8f":"code","3d214389":"code","967fe8e9":"code","c0d98569":"code","e9747e42":"code","82b91ebb":"code","31c987d5":"code","691c1d4b":"code","c981e4c2":"code","8c04ebdb":"code","4e935a06":"code","371aa2ca":"code","9e027773":"code","2363c199":"code","6a43d7f2":"code","79039156":"code","94577c72":"code","3fbbf1dd":"code","bef62076":"code","4753b035":"code","58c2d9e8":"code","ce00e79e":"code","1eba2222":"code","dded9bd3":"code","f17a76a2":"code","6a0ae6a6":"code","649fbdea":"code","d07dd4e9":"code","dfa42f1b":"code","c52cea20":"code","fbe0cc36":"code","51af87c2":"code","ef41eea7":"code","f384c0ab":"code","4bdd6997":"code","b7f0b1d5":"code","e6e7eb4b":"code","23666cc0":"code","dc5c5f08":"code","a944ed2c":"code","603eeccf":"code","c1721afc":"code","21d77cc2":"code","a4403da4":"code","deddd036":"code","1098b929":"code","c50070ab":"code","d8e23a6d":"code","d0cc6962":"code","ec9762b1":"code","b88c5d09":"code","bc95ae13":"code","867ab803":"code","d26cecf1":"code","29539199":"code","72cd4755":"code","4cdc8884":"code","bf0e382f":"code","42223cc4":"code","de1336a8":"code","076b53ad":"code","35d59820":"code","5a47bc37":"code","ae2dc8a7":"code","33c034ff":"code","b910ea55":"code","8de1b496":"code","fa5c3da4":"code","87704eac":"code","ead02ec5":"code","2264d6ea":"code","ef3a0d8d":"code","e81bb499":"code","97dbfc78":"code","d66d462b":"code","4f0653a2":"code","9184569f":"code","1d01fcaf":"code","b00d514a":"code","3a61448b":"code","fe40cc2e":"code","88ee9c79":"code","bc0833b2":"code","3d616e83":"code","01182f4e":"code","493eeca8":"code","0459cef2":"code","1f664632":"code","9d426bd8":"code","c72a4658":"code","e57b8dfa":"code","4eafa0d6":"code","cba54a89":"code","6a96d627":"code","512ec6eb":"code","e2e5b707":"code","5cd351de":"code","1e8d5866":"code","c73aa391":"code","17520bd1":"code","3cd25437":"code","0ba724ce":"code","5a3942c2":"code","c1073bf4":"code","7559d961":"code","8700e4e4":"code","bc7b576d":"code","560ccd3b":"code","b2759b9d":"code","8ab57f2a":"code","93c49b88":"code","d5950d6e":"code","7c1f1615":"code","e2934607":"code","092f4141":"code","712350f5":"code","b9d43466":"code","9b4af41f":"code","388da1e1":"code","ab679f44":"markdown","c0e3190e":"markdown","25d0341b":"markdown","598dca15":"markdown","5e6c0ddd":"markdown","aca74d11":"markdown","24fe392a":"markdown"},"source":{"0c8b2f73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eec17f28":"# linear algebra\nimport numpy as np \n# data processing\nimport pandas as pd \n# data visualization\nimport seaborn as sn\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","ff939ce2":"titanic_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n# separately store ID of test datasets, \n# this will be using at the end of the task to predict.\nTestPassengerID = test['PassengerId']\n","cde8a739":"titanic_df.shape","175ed588":"titanic_df.info()","fe24f7e2":"titanic_df.describe()","5fd19555":"titanic_df.head(4)","7ac8458b":"titanic_df.columns.values","288cdf8f":"#Training Data Set\nAmitPandey = titanic_df.isnull().sum().sort_values(ascending=False)\npercent_1 = titanic_df.isnull().sum()\/titanic_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([AmitPandey, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","3d214389":"#Test Data Set\nAPandey = test.isnull().sum().sort_values(ascending=False)\npercent_1 = test.isnull().sum()\/test.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([APandey, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","967fe8e9":"###Imputing missing values in train and test dataset\nCleaning = [titanic_df,test]\nfor dataset in Cleaning:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","c0d98569":"#Checking Again for Missing Values \ntitanic_df.isnull().sum()","e9747e42":"#Checking Again for Missing Values \ntest.isnull().sum()","82b91ebb":"drop_column = [\"PassengerId\",\"Cabin\"]\ntitanic_df.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)","31c987d5":"print(\"Dimension of train:\",titanic_df.shape)\nprint(\"Dimension of test:\",test.shape)","691c1d4b":"sn.barplot(x='Sex', y='Survived', data=titanic_df,palette=\"rocket\")\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by bifurcation of Sex\", fontsize=20)\nplt.show()\ntitanic_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c981e4c2":"sn.barplot(x='Pclass', y='Survived', data=titanic_df,palette=\"deep\")\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by bifurcation of Pclass\", fontsize=16)\nplt.show()\ntitanic_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8c04ebdb":"sn.barplot(x='Embarked', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by bifurcation of Embarked\", fontsize=16)\nplt.show()\ntitanic_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4e935a06":"sn.barplot(x='Sex', y='Survived', hue='Pclass', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by Pclass and Sex\", fontsize=15)\nplt.show()","371aa2ca":"titanic_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9e027773":"titanic_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2363c199":"titanic_df.describe()","6a43d7f2":"#Create a new Column \"Title\"\nTotal = [titanic_df, test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Unknown\": 5}\nfor dataset in Total:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","79039156":"titanic_df.Title.value_counts()","94577c72":"sn.barplot(x='Title', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by Title\", fontsize=15)\nplt.show()","3fbbf1dd":"#Create a new Column \"Title\"\n#Grouped together Miss \/Mrs & Master together as there is high survival chance\nCleaning = [titanic_df, test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 2, \"Master\": 2, \"Unknown\": 3}\nfor dataset in Total:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Jonkheer', 'Dona'], 'Unknown')\n    dataset['Title'] = dataset['Title'].replace('Sir', 'Mr')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Lady', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)","bef62076":"#Merged Miss\/ Mrs & Master based on my research & movie watching that they were main survivor [ You can say this as domain knowledge]\nsn.barplot(x='Title', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by Title\", fontsize=15)\nplt.show()","4753b035":"#Introducing a new column as \"FamilySize\"\ntitanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] ","58c2d9e8":"titanic_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).agg('mean')","ce00e79e":"#Introducing a new column \"IsAlone\"\ntitanic_df['IsAlone'] = 0\ntitanic_df.loc[titanic_df['FamilySize'] == 0, 'IsAlone'] = 1\ntitanic_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","1eba2222":"cols = ['Survived', 'Parch', 'SibSp', 'Embarked','IsAlone', 'FamilySize']\nnr_rows = 2\nnr_cols = 3\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sn.countplot(titanic_df[cols[i]], hue=titanic_df[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=18, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()","dded9bd3":"#Fare Column\ntitanic_df[[\"Fare\", \"Survived\"]].groupby(['Survived'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f17a76a2":"#Premium Fare means better chance of Survival\ntitanic_df.groupby(['Sex','Survived'])[['Fare']].agg(['min','mean','max'])","6a0ae6a6":"#Based on my own understanding of data, created 4 category in \"Fare\" column\ntitanic_df.loc[ titanic_df['Fare'] <= 7.89, 'Fare'] = 0\ntitanic_df.loc[(titanic_df['Fare'] > 7.89) & (titanic_df['Fare'] <= 14.45), 'Fare'] = 1\ntitanic_df.loc[(titanic_df['Fare'] > 14.45) & (titanic_df['Fare'] <= 31.47), 'Fare'] = 2\ntitanic_df.loc[ titanic_df['Fare'] > 31.47, 'Fare'] = 3\ntitanic_df['Fare'] = titanic_df['Fare'].astype(int)","649fbdea":"sn.barplot(x='Fare', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Visualizing Survival by Fare Category\", fontsize=15)\nplt.show()","d07dd4e9":"Bhuppi = sn.FacetGrid(titanic_df, col='Survived')\nBhuppi.map(plt.hist, 'Fare', bins=20)\nplt.show()","dfa42f1b":"sn.barplot(x='Sex', y='Survived', hue='Fare', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival with bifurcation of Fare and Sex\")\nplt.show()","c52cea20":"#Let's understand the relation between \"Age\" & \"Survived\nVinu = sn.FacetGrid(titanic_df, col='Survived')\nVinu .map(plt.hist, 'Age', bins=20)\nplt.show()","fbe0cc36":"g = sn.FacetGrid(titanic_df, col=\"Survived\")\ng.map_dataframe(sn.scatterplot, x=\"Age\", y=\"Fare\")\ng.set_axis_labels(\"Age\", \"Fare\")\ng.add_legend()","51af87c2":"#Seems less age and higher class is a better combination to survive\n#bins=np.arange(0, 80, 10)\nPrabhU = sn.FacetGrid(titanic_df, row='Sex', col='Pclass', hue='Survived', margin_titles=True, height=3, aspect=1.1)\nPrabhU.map(sn.distplot, 'Age', kde=False, bins=4, hist_kws=dict(alpha=0.6))\nPrabhU.add_legend()  \nplt.show()","ef41eea7":"#Categorize Age Feature\ntitanic_df.loc[ titanic_df['Age'] <= 23, 'Age'] = 1\ntitanic_df.loc[(titanic_df['Age'] > 23) & (titanic_df['Age'] <= 28), 'Age'] = 2\ntitanic_df.loc[(titanic_df['Age'] > 28) & (titanic_df['Age'] <= 36), 'Age'] = 3\ntitanic_df.loc[ titanic_df['Age'] > 36, 'Age'] = 4\ntitanic_df['Age'] = titanic_df['Age'].astype(int)","f384c0ab":"sn.barplot(x='Age', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival with bifurcation of Age\", fontsize=16)\nplt.show()\ntitanic_df[['Age', 'Survived']].groupby(['Age'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4bdd6997":"titanic_df.Age.value_counts()","b7f0b1d5":"sn.barplot(x='Pclass', y='Survived', hue='Age', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as bifurcation of Age and Sex\")\nplt.show()","e6e7eb4b":"sn.barplot(x='FamilySize', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Age\", fontsize=16)\nplt.show()\ntitanic_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","23666cc0":"titanic_df.SibSp.value_counts()","dc5c5f08":"test.SibSp.value_counts()","a944ed2c":"# Creating a categorical variable for Family Sizes\ntitanic_df['FamilyCategory'] = ''\ntitanic_df['FamilyCategory'].loc[(titanic_df['SibSp'] == 0)] = 'Without_Siblings_Spouses'\ntitanic_df['FamilyCategory'].loc[(titanic_df['SibSp'] > 0) & (titanic_df['SibSp'] <= 2 )] = 'Small'\ntitanic_df['FamilyCategory'].loc[(titanic_df['SibSp'] > 2) & (titanic_df['SibSp'] <= 4 )] = 'Medium'\ntitanic_df['FamilyCategory'].loc[(titanic_df['SibSp'] > 4)] = 'Large'","603eeccf":"sn.barplot(x='FamilyCategory', y='Survived', data=titanic_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of FamilyCategory\", fontsize=16)\nplt.show()\ntitanic_df[['FamilyCategory', 'Survived']].groupby(['FamilyCategory'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c1721afc":"#creating a new column \"Age*Class\"\ntitanic_df['Age*Class'] = titanic_df.Age * titanic_df.Pclass","21d77cc2":"titanic_df[[\"Age*Class\", \"Survived\"]].groupby(['Age*Class'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a4403da4":"pd.crosstab([titanic_df.Survived], [titanic_df.Sex,titanic_df['Age*Class']], margins=True).style.background_gradient(cmap='viridis')","deddd036":"pd.crosstab([titanic_df.Survived], [titanic_df.Sex,titanic_df['IsAlone']], margins=True).style.background_gradient(cmap='viridis')","1098b929":"titanic_df.head(4)","c50070ab":"test.loc[ test['Fare'] <= 7.89, 'Fare'] = 0\ntest.loc[(test['Fare'] > 7.89) & (test['Fare'] <= 14.45), 'Fare'] = 1\ntest.loc[(test['Fare'] > 14.45) & (test['Fare'] <= 31.47), 'Fare'] = 2\ntest.loc[ test['Fare'] > 31.47, 'Fare'] = 3\ntest['Fare'] = test['Fare'].astype(int)","d8e23a6d":"test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1","d0cc6962":"test.loc[ test['Age'] <= 23, 'Age'] = 1\ntest.loc[(test['Age'] > 23) & (test['Age'] <= 28), 'Age'] = 2\ntest.loc[(test['Age'] > 28) & (test['Age'] <= 36), 'Age'] = 3\ntest.loc[ test['Age'] > 36, 'Age'] = 4\ntest['Age'] = titanic_df['Age'].astype(int)","ec9762b1":"# Creating a categorical variable for Family Sizes\ntest['FamilyCategory'] = ''\ntest['FamilyCategory'].loc[(test['SibSp'] == 0)] = 'Without_Siblings_Spouses'\ntest['FamilyCategory'].loc[(test['SibSp'] > 0) & (test['SibSp'] <= 2 )] = 'Small'\ntest['FamilyCategory'].loc[(test['SibSp'] > 2) & (test['SibSp'] <= 4 )] = 'Medium'\ntest['FamilyCategory'].loc[(test['SibSp'] > 4)] = 'Large'","b88c5d09":"test['Age*Class'] = test.Age * test.Pclass","bc95ae13":"test.head(2)","867ab803":"titanic_df.head(2)","d26cecf1":"#code categorical data\nfrom sklearn import preprocessing\nTotal = [titanic_df,test]\nlabel = preprocessing.LabelEncoder()\nfor dataset in Total:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['Age'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['Fare'])\n    dataset['FamilyCategory_Code'] = label.fit_transform(dataset['FamilyCategory'])\n    dataset['IsAlone_Code'] = label.fit_transform(dataset['IsAlone'])\n    dataset['Age*Class_Code'] = label.fit_transform(dataset['Age*Class'])\n    dataset['Pclass_Code'] = label.fit_transform(dataset['Pclass'])","29539199":"Train_Ready = titanic_df.copy()\nTest_Ready = test.copy()","72cd4755":"#Dropping redundant columns and also including columns to drop based on my model accuracy findings like [\"IsALone_Code\",\"FamilyCategory_Code\"]\n# Trying by excluding [\"Parch feature from the data set\"]\ndrop_column = [\"Sex\",\"Name\",\"Ticket\",\"Embarked\",\"Title\",\"Age\",\"Fare\",\"FamilyCategory\",\"IsAlone\",\"Age*Class\",\"Pclass\",\"IsAlone_Code\",\"FamilyCategory_Code\"]\ntitanic_df.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)","4cdc8884":"titanic_df.shape","bf0e382f":"test.shape","42223cc4":"Train_Ready = titanic_df.copy()\nTest_Ready = test.copy()","de1336a8":"Train_Ready.info()","076b53ad":"Test_Ready.info()","35d59820":"# #Dropping redundant columns\n# #Will drop feature with least importance & improve my model accuracy- 1st will drop IsAlone_Code\n# #Will drop feature with least importance & improve my model accuracy- 2nd will drop FamilyCategory_Code feature\n# drop_column = [\"FamilyCategory_Code\"]\n# titanic_df.drop(drop_column, axis=1, inplace = True)\n# test.drop(drop_column, axis=1, inplace = True)","5a47bc37":"# titanic_df.head(4)","ae2dc8a7":"#Pearson Correlation of Features\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsn.heatmap(titanic_df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","33c034ff":"# Train_Ready = titanic_df.copy()\n# Test_Ready = test.copy()","b910ea55":"F_X_train = Train_Ready.drop(\"Survived\",axis=1)\nF_y_train = titanic_df[\"Survived\"]\nF_X_test  = Test_Ready","8de1b496":"print(\"Dimension of X_train:\",F_X_train.shape)\nprint(\"Dimension of y_train:\",F_y_train.shape)\nprint(\"Dimension of X_test:\",F_X_test.shape)","fa5c3da4":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","87704eac":"X_train = sc.fit_transform(F_X_train)\nX_test = sc.fit_transform(F_X_test)\ny_train = F_y_train","ead02ec5":"#Stochastic Gradient Descent (SGD)\nsgd = linear_model.SGDClassifier(penalty='l1',max_iter=1000, tol=1e-3)\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nsgd.score(X_train, y_train)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd ","2264d6ea":"#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=1000,criterion='gini', max_features='auto')\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","ef3a0d8d":"#Logistic Regression\nlogreg = LogisticRegression(penalty='l1',solver='liblinear',max_iter=500)\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","e81bb499":"#K Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","97dbfc78":"#K Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","d66d462b":"#Gaussian Naive Bayes\ngaussian = GaussianNB() \ngaussian.fit(X_train, y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","4f0653a2":"#Perceptron\nperceptron = Perceptron(max_iter=20)\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","9184569f":"#Linear Support Vector Machine\nlinear_svc = LinearSVC(penalty='l2',max_iter=1000)\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc","1d01fcaf":"#Decision Tree\ndecision_tree = DecisionTreeClassifier(criterion='entropy',max_features=\"auto\") \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","b00d514a":"#GradientBoostingClassifier\nGB_Clf = GradientBoostingClassifier() \nGB_Clf.fit(X_train, y_train)  \nY_pred = GB_Clf.predict(X_test)  \nacc_GB_Clf = round(GB_Clf.score(X_train, y_train) * 100, 2)\nacc_GB_Clf","3a61448b":"#SGDClassifier\nSGD_Clf = SGDClassifier() \nSGD_Clf.fit(X_train, y_train)  \nY_pred = SGD_Clf.predict(X_test)  \nacc_SGD_Clf = round(SGD_Clf.score(X_train, y_train) * 100, 2)\nacc_SGD_Clf","fe40cc2e":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree','GradientBoostingClassifier','SGDClassifier'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree,acc_GB_Clf,acc_SGD_Clf]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","88ee9c79":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(decision_tree, X_train, y_train, cv=3)\ncm=confusion_matrix(y_train, predictions)\nsn.heatmap(cm, annot=True, fmt='.2f',\n           xticklabels= [\"Not Survived\", \"Survived\"],\n           yticklabels = [\"Not Survived\", \"Survived\"] )\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","bc0833b2":"# The code below will perform K-Fold Cross Validation on our random forest model, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train,y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","3d616e83":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\n# Create a dataframe to store the features and their corresponding importances\nfeature_rank = pd.DataFrame( { \"feature\": F_X_train.columns,\"importance\":np.round(random_forest.feature_importances_,3)})\n## Sorting the features based on their importances with most important feature at top.\nfeature_rank = feature_rank.sort_values(\"importance\", ascending =False)\nplt.figure(figsize=(8, 6))\n# plot the values\nsn.barplot( y = \"feature\", x = \"importance\", data = feature_rank )","01182f4e":"feature_rank[\"cumsum\"] = feature_rank.importance.cumsum() * 100 \nfeature_rank.head(30)","493eeca8":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\n\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n","0459cef2":"#Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","1f664632":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\ncm=confusion_matrix(y_train, predictions)\nsn.heatmap(cm, annot=True, fmt='.2f',\n           xticklabels= [\"Not Survived\", \"Survived\"],\n           yticklabels = [\"Not Survived\", \"Survived\"] )\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","9d426bd8":"# from sklearn.model_selection import GridSearchCV\n# ## Configuring parameters and values for searched\n# tuned_parameters = [{ \"criterion\" : [\"gini\", \"entropy\"],'max_depth': [4,5,6,8,10],\n#                      'n_estimators': [100, 200, 500],'max_features': ['auto', 'sqrt','log2', None]}]\n\n# ## Initializing the RF classifier\n# radm_clf = RandomForestClassifier()\n# ## Configuring search with the tunable parameters\n# clf = GridSearchCV(radm_clf,tuned_parameters,cv=5,scoring='accuracy')\n# ## Fitting the training set\n# clf.fit(X_train, y_train)","c72a4658":"# #Mean cross-validated score of the best_estimator\n# clf.best_score_","e57b8dfa":"# #Parameter setting that gave the best results on the hold out data.\n# clf.best_params_","4eafa0d6":"## Initializing the Random Forest Model with the optimal values arrived using hyper parameter tuning\nradm_clf = RandomForestClassifier(criterion= 'entropy', max_depth= 4, max_features= None,n_estimators=500)\n## Fitting the model with the training set\nradm_clf.fit(X_train,y_train )","cba54a89":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(radm_clf, X_train, y_train, cv=3)\ncm=confusion_matrix(y_train, predictions)\nsn.heatmap(cm, annot=True, fmt='.2f',\n           xticklabels= [\"Not Survived\", \"Survived\"],\n           yticklabels = [\"Not Survived\", \"Survived\"] )\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","6a96d627":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\n# Create a dataframe to store the features and their corresponding importances\nfeature_rank = pd.DataFrame( { \"feature\": F_X_train.columns,\"importance\":np.round(radm_clf.feature_importances_,3)})\n## Sorting the features based on their importances with most important feature at top.\nfeature_rank = feature_rank.sort_values(\"importance\", ascending =False)\nplt.figure(figsize=(8, 6))\n# plot the values\nsn.barplot( y = \"feature\", x = \"importance\", data = feature_rank )","512ec6eb":"# #Let's Try More\n# from sklearn.model_selection import GridSearchCV\n# ## Configuring parameters and values for searched\n# tuned_parameters = [{'criterion': ['entropy', 'gini'],\n#                      'max_depth': [2,5,10,15,20,25],\n#                      'max_features': ['auto', 'sqrt','log2', None],\n#                      'min_samples_leaf': [4, 6, 8, 12],\n#                      'min_samples_split': [5, 7, 10, 14],\n#                      'n_estimators': [100, 200, 500,1000, 1500]}]\n\n# ## Initializing the RF classifier\n# radm_clf = RandomForestClassifier()\n# ## Configuring search with the tunable parameters\n# clf = GridSearchCV(radm_clf,tuned_parameters,cv=5,scoring='accuracy')\n# ## Fitting the training set\n# clf.fit(X_train, y_train)","e2e5b707":"# #Mean cross-validated score of the best_estimator\n# clf.best_score_","5cd351de":"# #Parameter setting that gave the best results on the hold out data.\n# clf.best_params_","1e8d5866":"# ## Initializing the Random Forest Model with the optimal values arrived using hyper parameter tuning\n# radm_clf = RandomForestClassifier(criterion= 'entropy', max_depth= 5, max_features= \"sqrt\", n_estimators= 500)\n# ## Fitting the model with the training set\n# radm_clf.fit(X_train,y_train )","c73aa391":"# from sklearn.model_selection import cross_val_predict\n# from sklearn.metrics import confusion_matrix\n# predictions = cross_val_predict(radm_clf, X_train, y_train, cv=3)\n# cm=confusion_matrix(y_train, predictions)\n# sn.heatmap(cm, annot=True, fmt='.2f',\n#            xticklabels= [\"Not Survived\", \"Survived\"],\n#            yticklabels = [\"Not Survived\", \"Survived\"] )\n# plt.ylabel(\"True label\")\n# plt.xlabel(\"Predicted label\")\n# plt.show()","17520bd1":"# import seaborn as sns\n\n# table = pd.pivot_table(pd.DataFrame(model.cv_results_),\n#     values='mean_test_score', index='param_n_estimators', \n#                        columns='param_criterion')\n     \n# sns.heatmap(table)","3cd25437":"# from sklearn.model_selection import cross_val_predict\n# from sklearn.metrics import confusion_matrix\n# predictions = cross_val_predict(model, X_train, y_train, cv=3)\n# cm=confusion_matrix(y_train, predictions)\n# sn.heatmap(cm, annot=True, fmt='.2f',\n#            xticklabels= [\"Not Survived\", \"Survived\"],\n#            yticklabels = [\"Not Survived\", \"Survived\"] )\n# plt.ylabel(\"True label\")\n# plt.xlabel(\"Predicted label\")\n# plt.show()","0ba724ce":"# import numpy as np\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# import seaborn as sn\n# # Create a dataframe to store the features and their corresponding importances\n# feature_rank = pd.DataFrame( { \"feature\": F_X_train.columns,\"importance\":np.round(random_forest.feature_importances_,3)})\n# ## Sorting the features based on their importances with most important feature at top.\n# feature_rank = feature_rank.sort_values(\"importance\", ascending =False)\n# plt.figure(figsize=(8, 6))\n# # plot the values\n# sn.barplot( y = \"feature\", x = \"importance\", data = feature_rank )","5a3942c2":"# feature_rank[\"cumsum\"] = feature_rank.importance.cumsum() * 100 \n# feature_rank.head(30)","c1073bf4":"# from sklearn.ensemble import GradientBoostingClassifier","7559d961":"# gboost_clf = GradientBoostingClassifier( n_estimators=500,max_depth=10)\n# ## Fitting gradient boosting model to training set\n# gboost_clf.fit(X_train,y_train)","8700e4e4":"# from sklearn.model_selection import cross_val_score\n# gboost_clf = GradientBoostingClassifier( n_estimators=500,max_depth=10)\n# cv_scores = cross_val_score( gboost_clf, X_train, y_train,cv = 10, scoring = 'accuracy' )","bc7b576d":"# print(cv_scores)\n# print(\"Mean Accuracy: \", np.mean(cv_scores), \"with standard deviation of:\", np.std(cv_scores))","560ccd3b":"# gboost_clf.fit(X_train,y_train )\n# predictions  = gboost_clf.predict(X_test )","b2759b9d":"# from sklearn.model_selection import cross_val_predict\n# from sklearn.metrics import confusion_matrix\n# predictions = cross_val_predict(gboost_clf, X_train, y_train, cv=10)\n# cm=confusion_matrix(y_train, predictions)\n# sn.heatmap(cm, annot=True, fmt='.2f',\n#            xticklabels= [\"Not Survived\", \"Survived\"],\n#            yticklabels = [\"Not Survived\", \"Survived\"] )\n# plt.ylabel(\"True label\")\n# plt.xlabel(\"Predicted label\")\n# plt.show()","8ab57f2a":"# import numpy as np\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# import seaborn as sn\n# # Create a dataframe to store the features and their corresponding importances\n# feature_rank = pd.DataFrame( { \"Feature\": F_X_train.columns,\"Importance\":np.round(gboost_clf.feature_importances_,3)})\n# ## Sorting the features based on their importances with most important feature at top.\n# feature_rank = feature_rank.sort_values(\"Importance\", ascending =False)\n# plt.figure(figsize=(8, 6))\n# # plot the values\n# sn.barplot( y = \"Feature\", x = \"Importance\", data = feature_rank )","93c49b88":"# predictions = cross_val_predict(radm_clf, X_train, y_train, cv=20)","d5950d6e":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","7c1f1615":"#Combining precision and recall into one score is called the F-score.\nfrom sklearn.metrics import f1_score\nf1_score(y_train, predictions)","e2934607":"#Precision Recall Curve\n#For each person the Random Forest algorithm has to classify, it computes a probability based on a function and \n#it classifies the person as survived (when the score is bigger the than threshold) or as not survived (when the score is smaller than the threshold). \n#That\u2019s why the threshold plays an important part.\nfrom sklearn.metrics import precision_recall_curve\n# getting the probabilities of our predictions\ny_scores = radm_clf.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","092f4141":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","712350f5":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('ROC Curve - False Positive Rate \\n Precision Recall Curve - Recall', fontsize=15)\n    plt.ylabel('ROC Curve - True Positive Rate \\n Precision Recall Curve - Precision', fontsize=15)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","b9d43466":"#The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","9b4af41f":"Y_prediction = radm_clf.predict(X_test)\nY_prediction","388da1e1":"submission = pd.DataFrame({\n        \"PassengerId\": TestPassengerID,\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","ab679f44":"# Will use the code of the hyperparamter tuning","c0e3190e":"# Importing Data Set","25d0341b":"# Missing Values Identification & Treatment","598dca15":"# Will drop columns \"PassengerId\" & \"Cabin\". We don't require IDs for prediction & \"Cabin\" have more than 75% missing values","5e6c0ddd":"# Feature Importance will be used to remove some impurities","aca74d11":"# Models for Classification","24fe392a":"# Preparing Test Data"}}