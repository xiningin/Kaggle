{"cell_type":{"84606271":"code","b97f4165":"code","1c246a18":"code","63058c62":"code","d4cb3a66":"code","f887cfad":"code","b3d837b4":"code","678a37c6":"code","96864e99":"code","e3d442bc":"code","9163b3fb":"code","735f336e":"code","46423b89":"code","3f1cd264":"code","b93a7bfd":"code","547817d2":"code","2a4491f3":"code","f11a0fa7":"code","b4c2d995":"code","c83b2e26":"code","c0e7ac9d":"code","04caef9b":"code","230f02cf":"code","217746cf":"code","76b49918":"code","ddd60be6":"code","339b5257":"code","ea2761fc":"code","84034aa9":"code","181a9c62":"code","8ac94cc3":"code","46555410":"code","4ff4289f":"code","752463ac":"code","5b7cdcac":"code","17762441":"code","12759cd2":"code","a81afee1":"code","b5b80ded":"code","54a601c2":"code","3c1bb917":"code","144d2d93":"code","2554fe55":"code","5b50836c":"code","e69480bc":"code","859bb7e1":"code","fa8d7cc3":"code","5dbf0f52":"code","b88dd4e3":"code","2b5542b2":"code","cab8b228":"code","a51435af":"code","b61e7a7e":"code","8e93e4e6":"code","3dc74bf1":"code","b98a658a":"code","6e297234":"code","3c7554ed":"code","77098ef1":"code","d0172aef":"code","d7647261":"code","e2783671":"code","e068ffa3":"code","9ece3913":"code","2e978dad":"code","2e23aa5b":"code","fb15a326":"code","f2dab5b5":"code","eaec1253":"code","1440cf1d":"code","360eb359":"code","8dc79628":"code","52e2bd51":"code","eb078a68":"code","4621885c":"code","8895f877":"code","0cc33d6d":"code","e9cc47d2":"code","cf37d60c":"code","6c075b3b":"code","596741a9":"code","07752d97":"markdown","42da43bd":"markdown","d3147827":"markdown","fc837e54":"markdown","0ed03fc0":"markdown","eda233ef":"markdown","1d6117a8":"markdown","ceb33010":"markdown","66f441ca":"markdown","54d5fe5c":"markdown","ce7347a0":"markdown","40041ebd":"markdown","78f95d49":"markdown","5d39f2dc":"markdown","34702c02":"markdown","3a3408a0":"markdown","cdab5849":"markdown","d99b31e8":"markdown","d2d48d2b":"markdown","ad6f4402":"markdown","010770f2":"markdown","c4a61bd0":"markdown","217ee11d":"markdown","3c5ad3ab":"markdown","9409d131":"markdown","72eb6720":"markdown","a9d10e22":"markdown","2a27518c":"markdown","ec91f59b":"markdown","5eeba3d9":"markdown","a21bb4f0":"markdown","91cb7f9e":"markdown","94fc42a0":"markdown","3035b210":"markdown","b25bac8d":"markdown","967521a5":"markdown","b6dddbda":"markdown","72454102":"markdown","3dbdb3be":"markdown","5ba0393a":"markdown","909ec587":"markdown","506eadc4":"markdown","926f15b6":"markdown","f9eb634d":"markdown","7f35a033":"markdown","6f464eac":"markdown","052bf2ba":"markdown","ef10c71d":"markdown","1a0bb922":"markdown","a0d9537f":"markdown","cdfcd864":"markdown","36036bd9":"markdown","1e1f677f":"markdown","1c3d78db":"markdown","e1009473":"markdown","e2c2720f":"markdown","a7713479":"markdown","306abb8c":"markdown","ee5ce1a3":"markdown","0fc9c401":"markdown","8e553f4b":"markdown","c4cc595f":"markdown","36093b2b":"markdown","63d118bc":"markdown","3ec7ba70":"markdown","0206d6e5":"markdown","1ef43ae9":"markdown","d2a245a2":"markdown","42ddece4":"markdown","a6ce8969":"markdown","895655cc":"markdown","4f7ebe98":"markdown","ce935422":"markdown","3c5f9adb":"markdown","aa33e7bd":"markdown","7d3dc533":"markdown","d49ebc6d":"markdown","8e4f6292":"markdown","56e55932":"markdown","37394ba1":"markdown","22ca5dbd":"markdown","e18f8990":"markdown"},"source":{"84606271":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b97f4165":"# I import some extra libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette('viridis')\nsns.set_style('whitegrid')\n%matplotlib inline\nnp.random.seed(42)","1c246a18":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","63058c62":"train_df.info()\ntrain_df.head()","d4cb3a66":"def get_df_by_group(df, group):\n    df_groupedby = df.groupby(group).agg({'PassengerId':'count', 'Survived': 'sum'}).rename(columns={'PassengerId': 'NumPassengers'})\n    df_groupedby['Rate'] = df_groupedby['Survived'] \/ df_groupedby['NumPassengers'] \n    return df_groupedby\n\ndf = train_df.copy()","f887cfad":"train_groupby_sex = get_df_by_group(df, ['Sex'])\ntrain_groupby_sex","b3d837b4":"f, (ax1, ax2) = plt.subplots(1, 2)\nf.set_figwidth(16)\nf.set_figheight(6)\nsns.barplot(x=train_groupby_sex.index, y='Survived', data=train_groupby_sex, ax=ax1)\nsns.barplot(x=train_groupby_sex.index, y='NumPassengers', data=train_groupby_sex, ax=ax2)\nax1.set_title('Passengers Survived Per Sex')\nax2.set_title('Passengers Embarked Per Sex') \nax1.plot()\nax2.plot()","678a37c6":"train_groupby_pclass = get_df_by_group(df, ['Pclass'])\ntrain_groupby_pclass","96864e99":"f, (ax1, ax2) = plt.subplots(1, 2)\nf.set_figwidth(16)\nf.set_figheight(6)\nsns.barplot(x=train_groupby_pclass.index, y='Survived', data=train_groupby_pclass, ax=ax1)\nsns.barplot(x=train_groupby_pclass.index, y='NumPassengers', data=train_groupby_pclass, ax=ax2)\nax1.set_title('Passengers Survived Per Class')\nax2.set_title('Passengers Embarked Per Class') \nax1.plot()\nax2.plot()","e3d442bc":"#Embarked had a couple of null values as we saw when we called train_df.info(). Let's check the most common values and choose a good approach to fill the nan values\ndf['Embarked'].value_counts()","9163b3fb":"#As S is the most common value by far, we will fill the couple of Nan values with the most common one\ndef fill_embarked_nan(df):\n    return df['Embarked'].fillna(value='S')\n\ndf['Embarked'] = fill_embarked_nan(df)\ndf['Embarked'].isnull().any()","735f336e":"class_median_age_series = df.groupby(['Pclass'])['Age'].median()\nclass_median_age_series","46423b89":"def fill_age_nan(df):\n    return df[['Age', 'Pclass']].apply(lambda x: class_median_age_series.get(x['Pclass']) if(pd.isnull(x['Age'])) else x['Age'], axis=1)\ndf['Age'] = fill_age_nan(df)","3f1cd264":"def ageclass_by_age(age):\n    if age < 10:\n        return '< 10'\n    elif (age >= 10 and age < 20):\n        return '>= 10 and < 20'\n    elif (age >= 20 and age < 35):\n        return '>= 20 and < 35'\n    elif (age >= 35 and age < 50):\n        return '>= 35 and < 50'\n    elif (age >= 50 and age < 65):\n        return '>= 50 and < 65'\n    else:\n        return '> 65'\n\ndef convert_age_to_ageclass(df):\n     return df['Age'].apply(ageclass_by_age)\n    \ndf['Age'] = convert_age_to_ageclass(df)","b93a7bfd":"train_groupby_age = get_df_by_group(df, ['Age'])\ntrain_groupby_age","547817d2":"plt.figure(figsize=(10,4))\nf = sns.distplot(df[df['Pclass']==1]['Fare'], bins=30)\nf.plot()","2a4491f3":"plt.figure(figsize=(10,4))\nf = sns.distplot(df[df['Pclass']==2]['Fare'], bins=30)\nf.plot()","f11a0fa7":"plt.figure(figsize=(10,4))\nf = sns.distplot(df[df['Pclass']==3]['Fare'], bins=30)\nf.plot()","b4c2d995":"def extract_title_from_name(df):\n    return df['Name'].apply(lambda x: (x[x.index(',') + 1:x.index('.')]).strip())\n\ndf['Title'] = extract_title_from_name(df)\ndf['Title'].value_counts()","c83b2e26":"def reduce_list_titles(title):\n    if(title in ['Mrs', 'Miss', 'Master', 'Mr', 'Dr']):\n        title = title\n    elif(title in ['Ms','Mme']):\n        title = 'Mrs'\n    elif(title in ['Mlle', 'Lady']):\n        title = 'Miss'\n    elif(title in ['Don']):\n        title = 'Mr'\n    else:\n        title = 'Other'\n    return title\n\ndef convert_title_to_reduced_list(df):\n    return df['Title'].apply(reduce_list_titles)\n\ndf['Title'] = convert_title_to_reduced_list(df)","c0e7ac9d":"train_groupby_title = get_df_by_group(df, ['Title'])\ntrain_groupby_title.sort_values(by='Rate', ascending=False)","04caef9b":"def convert_family_size_cat(df):\n    family_members=df['Parch'] + df['SibSp']\n    if(family_members == 0):\n        return 'NO_FAMILY'\n    if(family_members <= 3 and family_members >=0):\n        return 'SMALL_FAMILY'\n    else:\n        return 'BIG_FAMILY'\n    \ndef convert_parch_and_sibsp_to_family_cat(df):\n     return df.apply(convert_family_size_cat, axis=1)\n    \ndf['Family'] = convert_parch_and_sibsp_to_family_cat(df)","230f02cf":"train_groupby_family = get_df_by_group(df, ['Family'])\ntrain_groupby_family.sort_values(by='Family', ascending=False)","217746cf":"def fill_cabin_nan(df):\n    return df['Cabin'].apply(lambda x: str(x)[0] if(pd.notnull(x)) else str('O'))\n\ndf['Cabin'] = fill_cabin_nan(df)","76b49918":"train_groupby_cabin = get_df_by_group(df, ['Cabin'])\ntrain_groupby_cabin.sort_values(by='Rate', ascending=False)","ddd60be6":"grouped_df = df.groupby(['Sex', 'Age', 'Pclass', 'Family', 'Title']).agg({'PassengerId':'count', 'Survived': 'sum'}).rename(columns={'PassengerId': 'NumPassengers'})\ngrouped_df['Rate'] = grouped_df['Survived'] \/ grouped_df['NumPassengers']\ngrouped_df = grouped_df.reset_index()\ngrouped_df.head(10)","339b5257":"grouped_df['Sex-AgeRange-Title'] = grouped_df['Sex'] + ' - ' + grouped_df['Age'].astype(str) + ' - ' + grouped_df['Title']\ngrouped_df['Class-HasFamily'] = 'Class_' + grouped_df['Pclass'].astype(str) + ' - ' + grouped_df['Family']\npivot_df = grouped_df.pivot(index='Sex-AgeRange-Title', columns='Class-HasFamily', values='Rate')\nplt.figure(figsize=(16, 8))\nsns.heatmap(pivot_df, annot=True)","ea2761fc":"def convert_to_categories_and_fill_nan(X):\n    df = pd.DataFrame(X, columns=cat_attributes)\n    df['Title'] = extract_title_from_name(df)\n    df['Title'] = convert_title_to_reduced_list(df)\n    df['Embarked'] = fill_embarked_nan(df)\n    df['Age'] = fill_age_nan(df)\n    df['Age'] = convert_age_to_ageclass(df)\n    df['Family'] = convert_parch_and_sibsp_to_family_cat(df)\n    df['Cabin'] = fill_cabin_nan(df)\n    df.drop(['Name', 'SibSp', 'Parch'], axis=1, inplace=True)\n    return df.values\n","84034aa9":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# I print here one last time the first 5 rows to decide the attributes that I will use in the model\ntrain_df.head()","181a9c62":"cat_attributes = ['Pclass', 'Age', 'Sex', 'Embarked', 'Cabin', 'Name', 'SibSp', 'Parch']\nnum_attributes = ['Fare']","8ac94cc3":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('std_scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('convert_to_categories_and_fill_nan', FunctionTransformer(convert_to_categories_and_fill_nan, validate=False)),\n    ('cat_encoder', OneHotEncoder())\n])","46555410":"full_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attributes),\n    ('cat', cat_pipeline, cat_attributes)\n])","4ff4289f":"X_train = train_df[['Fare','Sex', 'Pclass', 'Age', 'SibSp', 'Parch', 'Embarked', 'Cabin', 'Name']].copy()\nX_test = test_df[['Fare','Sex', 'Pclass', 'Age', 'SibSp', 'Parch','Embarked', 'Cabin', 'Name']].copy()\ny_train = train_df['Survived']\nX_train.info()","752463ac":"X_concat = pd.concat([X_train, X_test])\nX_concat_prepared = full_pipeline.fit_transform(X_concat)\nX_train_prepared = X_concat_prepared[:891]\nX_test_prepared = X_concat_prepared[891:]\nX_train_prepared[:10]","5b7cdcac":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, accuracy_score","17762441":"logistic_reg = LogisticRegression(solver='lbfgs',max_iter=1000,random_state=101)\ny_logistic_score = cross_val_score(logistic_reg, X_train_prepared, y_train, cv=10, scoring='accuracy')\ny_logistic_score.mean()","12759cd2":"# To get the confusion matrix, we don't want to get the score but the actual predictions, so we can pass it later to the confusion_matrix function together with the actual \n# labels to get the matrix\ny_logistic_pred = cross_val_predict(logistic_reg, X_train_prepared, y_train, cv=10)","a81afee1":"confusion_matrix(y_train, y_logistic_pred)","b5b80ded":"precision_score(y_train, y_logistic_pred)","54a601c2":"recall_score(y_train, y_logistic_pred)","3c1bb917":"f1_score(y_train, y_logistic_pred)","144d2d93":"print(classification_report(y_train,y_logistic_pred))","2554fe55":"y_decision_function = cross_val_predict(logistic_reg, X_train_prepared, y_train, cv=10, method='decision_function')","5b50836c":"threshold = 2\nprint(confusion_matrix(y_train, y_decision_function > threshold))\nprint(precision_score(y_train, y_decision_function > threshold))\nprint(recall_score(y_train, y_decision_function > threshold))","e69480bc":"threshold = -2\nprint(confusion_matrix(y_train, y_decision_function > threshold))\nprint(precision_score(y_train, y_decision_function > threshold))\nprint(recall_score(y_train, y_decision_function > threshold))","859bb7e1":"precisions, recalls, thresholds = precision_recall_curve(y_train, y_decision_function)","fa8d7cc3":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.legend(loc=\"upper left\", fontsize=16)\n    plt.ylim([0, 1])\n    \nplt.figure(figsize=(16, 8))\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)","5dbf0f52":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n\nplt.figure(figsize=(16, 8))\nplot_precision_vs_recall(precisions, recalls)","b88dd4e3":"def plot_roc_curve(fpr, tpr,  label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1], [0,1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n\nfpr, tpr, threshold = roc_curve(y_train, y_decision_function)\nplt.figure(figsize=(16, 8))\nplot_roc_curve(fpr, tpr)","2b5542b2":"roc_auc_score(y_train, y_decision_function)","cab8b228":"from sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint\n\n# This is the combination of Hyperparams that we are going to test\nparam_grid = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.01,0.1,0.9,0.95]\n}\n\nlogistic_reg = LogisticRegression()\n\n# As parameters for the GridSearchCV we provide the LogisticRegression instance, the param dictionary to test, \n# the number of folds that the train set should be split on for cross validation, and the scoring method.\ngrid_search = GridSearchCV(logistic_reg, param_grid, cv=5,scoring='accuracy')\n\ngrid_search.fit(X_train_prepared, y_train)","a51435af":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","b61e7a7e":"means = grid_search.cv_results_['mean_test_score']\nstds = grid_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","8e93e4e6":"from sklearn.neighbors import KNeighborsClassifier\n\nparam_grid = {\n    'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n}\n\ngrid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\ngrid_search_knn.fit(X_train_prepared, y_train)","3dc74bf1":"print(grid_search_knn.best_params_)\nprint(grid_search_knn.best_score_)","b98a658a":"means = grid_search_knn.cv_results_['mean_test_score']\nstds = grid_search_knn.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search_knn.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","6e297234":"from sklearn.svm import SVC\n\nparam_grid = [\n    {'kernel':['rbf'], 'C':[0.7,0.8,1], 'gamma':[0.07, 0.08, 0.09]},\n    {'kernel':['poly'],'C':[0.1,1,10,100], 'gamma':['auto']}\n  ]\n\ngrid_search_svc = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\ngrid_search_svc.fit(X_train_prepared, y_train)","3c7554ed":"print(grid_search_svc.best_params_)\nprint(grid_search_svc.best_score_)","77098ef1":"means = grid_search_svc.cv_results_['mean_test_score']\nstds = grid_search_svc.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search_knn.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","d0172aef":"from sklearn.tree import DecisionTreeClassifier\n\nparam_grid = {\n    'max_features':[8,9,10,11,12,13],\n    'max_leaf_nodes':[7,8,9,10,11],\n    'min_samples_split':[3,4,5]\n}\n\ngrid_search_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\ngrid_search_tree.fit(X_train_prepared, y_train)","d7647261":"print(grid_search_tree.best_params_)\nprint(grid_search_tree.best_score_)","e2783671":"means = grid_search_tree.cv_results_['mean_test_score']\nstds = grid_search_tree.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search_tree.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","e068ffa3":"feature_importance_list = grid_search_tree.best_estimator_.feature_importances_\nlist_categories = full_pipeline.named_transformers_['cat']['cat_encoder'].categories_\nflat_list = [item for sublist in list_categories for item in sublist]\nattribute_list = num_attributes + flat_list\ndf_attribute_importance = pd.DataFrame({'attribute_name': attribute_list, 'importance': feature_importance_list})\ndf_attribute_importance.sort_values('importance', ascending=False)","9ece3913":"from sklearn.ensemble import AdaBoostClassifier\n\nada_boost_clf = AdaBoostClassifier(DecisionTreeClassifier(), random_state=42)\n\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2,3],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngrid_search_ada = GridSearchCV(ada_boost_clf, param_grid, cv=5, scoring='accuracy', verbose=1)\ngrid_search_ada.fit(X_train_prepared, y_train)","2e978dad":"print(grid_search_ada.best_params_)\nprint(grid_search_ada.best_score_)","2e23aa5b":"means = grid_search_ada.cv_results_['mean_test_score']\nstds = grid_search_ada.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search_ada.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","fb15a326":"feature_importance_list = grid_search_ada.best_estimator_.feature_importances_\nlist_categories = full_pipeline.named_transformers_['cat']['cat_encoder'].categories_\nflat_list = [item for sublist in list_categories for item in sublist]\nattribute_list = num_attributes + flat_list\ndf_attribute_importance = pd.DataFrame({'attribute_name': attribute_list, 'importance': feature_importance_list})\ndf_attribute_importance.sort_values('importance', ascending=False)","f2dab5b5":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost_clf = GradientBoostingClassifier(DecisionTreeClassifier(), random_state=42)\n\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [300, 400, 500],\n              'learning_rate': [0.2, 0.15, 0.1],\n              'max_depth': [3, 4, 6, 8],\n              'min_samples_leaf': [60, 80, 100],\n              'max_features': [0.5, 0.3, 0.1] \n              }\ngrid_search_gradient_boost = GridSearchCV(gradient_boost_clf, param_grid, cv=5, scoring='accuracy', verbose=1)\ngrid_search_gradient_boost.fit(X_train_prepared, y_train)","eaec1253":"print(grid_search_gradient_boost.best_params_)\nprint(grid_search_gradient_boost.best_score_)","1440cf1d":"means = grid_search_gradient_boost.cv_results_['mean_test_score']\nstds = grid_search_gradient_boost.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, grid_search_gradient_boost.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","360eb359":"feature_importance_list = grid_search_gradient_boost.best_estimator_.feature_importances_\nlist_categories = full_pipeline.named_transformers_['cat']['cat_encoder'].categories_\nflat_list = [item for sublist in list_categories for item in sublist]\nattribute_list = num_attributes + flat_list\ndf_attribute_importance = pd.DataFrame({'attribute_name': attribute_list, 'importance': feature_importance_list})\ndf_attribute_importance.sort_values('importance', ascending=False)","8dc79628":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80,120,140],\n    'max_features':[4,5,6,7,8],\n    'min_samples_leaf':[2,3,4],\n    'min_samples_split':[8,10,12,14,16],\n    'n_estimators': [100,200,300,500,1000]\n}\n\nforest2_clf = RandomForestClassifier()\nrnd_search = RandomizedSearchCV(forest2_clf, param_distributions=param_grid,\n                                n_iter=500, cv=10, scoring='accuracy', random_state=101)\nrnd_search.fit(X_train_prepared, y_train)","52e2bd51":"rnd_search.best_params_","eb078a68":"rnd_search.best_score_","4621885c":"means = rnd_search.cv_results_['mean_test_score']\nstds = rnd_search.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, rnd_search.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"\n          % (mean, std * 2, params))","8895f877":"feature_importance_list = rnd_search.best_estimator_.feature_importances_","0cc33d6d":"list_categories = full_pipeline.named_transformers_['cat']['cat_encoder'].categories_\nflat_list = [item for sublist in list_categories for item in sublist]\nattribute_list = num_attributes + flat_list\ndf_attribute_importance = pd.DataFrame({'attribute_name': attribute_list, 'importance': feature_importance_list})\ndf_attribute_importance.sort_values('importance', ascending=False)","e9cc47d2":"best_estimator = rnd_search.best_estimator_","cf37d60c":"y_test_predictions = best_estimator.predict(X_test_prepared)","6c075b3b":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_test_predictions\n    })\nsubmission.head()","596741a9":"submission.to_csv('submission5.csv', index=False)","07752d97":"I name here the attributes that I will use in my model. I name some numberic attributes like 'SibSp' and 'Parch' as categoricals as I want those to go through the categorical transformations that I will do in the pipeline","42da43bd":"# Motivation for this Kernel","d3147827":"Finally I use ColumnTransformer Pipeline to define which attributes will be transformed in each pipeline","fc837e54":"## Testing the results of Logistic Regression Model","0ed03fc0":"Pipelines are useful to make a sequence of transformation repeteable. \n\nThey are also useful in case you want to parameterize your transformation functions and then use those params as hyperparams of the model using RandomizedSearchCV or GridSearchCV to find the best combination.","eda233ef":"And I can also print every single combination tested and the score of obtained by that combination","1d6117a8":"## Per Cabin","ceb33010":"## K Nearest Neighbors","66f441ca":"I want to reduce the number of titles to a more manageable number. There are some titles that seem to be the same than I have tried to unify under the same name. Ie Lady and Miss. And there are others that have very little occurrences like Sir and Major. For those I have crated a big category called other.","54d5fe5c":"Now, the opposite happened, the number of False Negatives decreased a lot and the model is able to predict many more survivals, however this is at the expense of decreasing the precission as there are a much higher number of False Positives","ce7347a0":"Finally, We can calculate the area under the curve. A good model would get a result as close as possible to 1, while 0.5 would be a pure random model","40041ebd":"Embarked had some nan values, we need to fill the gaps","78f95d49":"## Per Family Members","5d39f2dc":"## Creating the methods to call from pipelines","34702c02":"# Testing the model with the test set and publishing result","3a3408a0":"For Age I also want to segment the age in ranges that I will use as categories for the model","cdab5849":"I am just going to plot the distribution of Fares per category to have an idea of the different values of it. Later I will look at the correlations","d99b31e8":"We have all values for Fare. I will use this value as is. Maybe I could improve the model if making a category of this value like I did for Age?","d2d48d2b":"## Per Sex","ad6f4402":"Title can be extracted from the name of each passenger. Below I created a function to extract the Titles.","010770f2":"Cabin has a big number of nan, but the known information can still be valuable to determine if a passenger survived or not. The strategy that I choose to fill nan in this case was to fill all nan with the same value 'O' (other)","c4a61bd0":"I am not going to spend too long in the Exploratory analysis and data preparation. For those interested in a good exploratory analysis, there are kernels like https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions.\n\nFor the data preparation I am going to pack every transformation that I do in a function so I can use pipelines later to call this functions.","217ee11d":"#### ROC AUC Score","3c5ad3ab":"Now, I am going to import all the classes that I will use for hte pipeline and different transformations that I will do.","9409d131":"First, we are going to set the threshold to a value higher than 0 and see how this modifies precision and recall","72eb6720":"## Per Age Range","a9d10e22":"And we can also print the score that we get with each of the combinations of hyperparams tested","2a27518c":"We also have the Nan values for age. We will use median per Pclass to fill those gaps ","ec91f59b":"# Data Transformation: Scikit pipelines","5eeba3d9":"F1 Score, combines precision and recally into a single metric. It is useful when you want a single metric that combines both classifiers.\n\n``F1 = 2 \/ ( (1 \/ precision) + (1 \/ recall) ) = 2 * ( (precission * recall) \/ (precission + recall) ) = TP \/ (TP + ( (FN + FP) \/ 2) ) ``","a21bb4f0":"### Using Decision Function to control threshold evaluate precision and recall","91cb7f9e":"Here I define the two pipelines that I will use. One for numeric attributes and one for categorical attributes:\n- num_pipeline defines 2 transformations:\n    - SimpleImputer will just replace nan with the most frequent value. In this case the num_pipeline only will be applied to Fare and it will be use the most common value to fill the gaps.\n    - StandardScaler Standardize features by removing the mean and scaling to unit variance\n- cat_pipeline: defines 2 transformations for the categorical attributes too:\n    - convert_to_categories_and_fill_nan: this is our custom function defined above\n    - OneHotEncoder: This converts categorical values into one-hot vectors.","94fc42a0":"## Per Embarked","3035b210":"This, increased a lot the precision and there are just 4 False Positives, however, it also increased a lot the number of False Negatives reducing a lot the Recall","b25bac8d":"In addition, we can also print precision vs recall curve. This is useful to choose correctly a value with a good recall just before precission falls abruptly. ","967521a5":"## Per Fare","b6dddbda":"Finally, I am going to use my best estimator in the test set to submit my prediction for the competition","72454102":"After exploratory analysis of survival based on the number of members in a family, it seems that having a small family increases the chances of survival but having a big family (more than 3 members) reduces it. So I have created 3 categories based in the attributes Parch and SibSp.","3dbdb3be":"We start by concating the X_train and X_test sets to process both of them at the same time by the pipeline. The reason to do this is because not all categorical values could be represented in both sets which would give different number of attributes when applying the OneHotEncoder.\n\nAfter running the pipeline, we split the np array again between the train data and the test data.","5ba0393a":"# Loading and Inspecting the data","909ec587":"Precission is a way to look at the accuracy of the positive predictions. The formula to calculate it is \n\n``PRECISSION = TP \/ (TP + FP)``","506eadc4":"#### Confusion Matrix","926f15b6":"I am also printing the receiver operating characteristic curve (ROC). ROC curve plots the true positive rate vs false positive rate****","f9eb634d":"We can plot how the precision and recall change depending on the threshold chosen","7f35a033":"## Creating the pipelines to format the data","6f464eac":"Classification report builds a report that contains some of the metrics that I just computed above","052bf2ba":"Finally, I am going to train a Random Forest model and I will use RandomizedSearchCV to test a number of hyperparameters combinations. The reason to use RandomizedSearchCV instead of GridSearchCV is that we are testing a very high number of combinations, randomizedSearchCV will just test a number of random combinations instead of all possible combinations.","ef10c71d":"## Heatmap of the Survival grouping Categories","1a0bb922":"First model I am going to try is LogisticRegression. I am going to use cross validation to check the accuracy of the model.\n\ncross_val_score performs k-fold validation which splits the training set in n distinct subsets (specified by the param cv) and picking a different fold for evaluation every time training in the other n-1 folds.","a0d9537f":"# Hyperparam Tunning using RandomizedSearchCV for Random Forest ","cdfcd864":"#### F1 Score","36036bd9":"# Hyperparam Tunning with GridSearchCV","1e1f677f":"In this Kernel I am going to create the models to make a prediction for the famous Titanic dataset. I believe that this Kernel can be helpful for people looking for the following topics:\n\n- How to use Pipelines for data preparation\n\n- Using K-Fold cross validation to test different models\n\n- Evaluating results using confusion_matrix, precision_score, recall_score, f1_score, plotting precission\/recall chart...\n\n- Testing and tuning models like LogisticRegression, SVM and RandomForest\n\n- Using GridSearchCV to test different convination of Hyperparams to tune our model\n\n- Using RandomizedSearchCV when there are many combinations of Hyperparms to limit the time that the model takes to find the best combination\n\nThe model published from this kernel got 0.80861 in the Titanic Competition (top 11%)\n    ","1c3d78db":"## Per Title","e1009473":"#### Precission curve per threshold vs Recall curve per threshold ","e2c2720f":"Now we are going to use the pipelines. First I get the train and test sets with just the attributes we are going to use in the pipelines and we also split the labels of the train set into the y_train variable.","a7713479":"## AdaBoostClassifier","306abb8c":"A good model, should make the curve as close as possible to the top-left corner.","ee5ce1a3":"#### Classification Report","0fc9c401":"#### Precision Score","8e553f4b":"# Exploratory analysis and data preparation","c4cc595f":"I will use this method in my pipelines to call above methods that make all the transformations of the different attributes. In this method, I start by converting the numpy array into pandas DataFrame and at the very end I return np array again as this is what the pipeline expects. \nI would appreciate if any of the experts in kaggle can tell me if this is a good practice or not.","36093b2b":"#### Precision vs Recall curve","63d118bc":"Also called Sensitivity or True positive Rate, this is the ratio of possitive instances that are correctly detected by the classifier. \n\n``RECALL = TP \/ (TP + FN)``","3ec7ba70":"I can get the feature importance of each feature, sort it, and print them in a table to analyze which features are the most important in the model","0206d6e5":"# Logistic Regression with K-Fold Cross Validation","1ef43ae9":"Now, I am going to train again a LogisticRegression, this time, using GridSearchCV to find the best combination of Hyper Params. GridSearchCV tests all the parameters in the param_grid and does cross validation to get the combination with the best score.","d2a245a2":"Precission and Recall can be controlled by modifiying the threshold of the decision function that decides the predicted value. However this is a tradeoff, if we increase precission, Recall will decrease and viceversa.","42ddece4":"We can now find the combination of hyperparms that gives the best accuracy and the score that we got with that combination","a6ce8969":"## Logistic Regression","895655cc":"To visualize better the survival rates per category I am going to create a heatmap that shows the survival rate per Sex, Age, Pclass, Family and Title.","4f7ebe98":"## Gradient Boosting Classifier","ce935422":"## Using the pipelines to prepared the train and test data","3c5f9adb":"Second, we are going to set the threshold to a value lower than 0 and see how precission and recall change this time","aa33e7bd":"We can check the best hyperparam combination tested","7d3dc533":"## Per Class","d49ebc6d":"Confusion matrix is a better way to evaluate the performance of a classifier. In a confusion matrix we are going to count the number of instances of each class that are counted as the correct class and as the incorrect class:\n- In the top row we will have the instaces that are negative and in the bottom row the instances that are positive, in this case the top row would be the not survivals and the bottom one the survivals\n- In the left column we have what the model predicted as negative and in the right one the ones predicted as positive. Thus, the left column is what the model predicted as not survivals, and the right one what the model predicted as survivals.\n\nTherefore, the left-top and right-bottom cells are what the model got right. Top-right corner would be called \"False Positives\", which in this case are not survivals that the model predicted as survivals. Bottom-left corner would be False Negatives, in this case, survivals that were predicted as non-survivals by the model.","8e4f6292":"#### Recall Score","56e55932":"## SVM Classifier","37394ba1":"## Decision Tree Classifier","22ca5dbd":"Also the best score that was obtained by the above hyperparam combination","e18f8990":"#### ROC Curve"}}