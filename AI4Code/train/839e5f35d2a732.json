{"cell_type":{"bcaf11b4":"code","d05d62eb":"code","d3cc0cb2":"code","7e10fdcf":"code","1bfcc684":"code","607b2f4d":"code","7a5d6753":"code","f2ba9d4c":"code","6ded7708":"code","82f13f16":"code","3c4712b7":"code","e896de81":"code","b7235dd4":"code","9091c1f8":"code","1a07c861":"markdown","6ee5d7c7":"markdown","5f4acd30":"markdown","936c028e":"markdown","06dd8285":"markdown","ee07373c":"markdown","019d9ed1":"markdown","47c9e4bd":"markdown","6c1be7ce":"markdown","9b0344b5":"markdown","4870d3ae":"markdown","b932d0fb":"markdown","273f245f":"markdown","a6ffe673":"markdown","ce5fc47b":"markdown","7999cef6":"markdown","a116a3fd":"markdown"},"source":{"bcaf11b4":"import pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","d05d62eb":"#Start time record for process\nstart_time = time.time()","d3cc0cb2":"print(\"**************************Import raw data********************************\")","7e10fdcf":"raw_data = pd.read_csv(\"..\/input\/multistage-continuousflow-manufacturing-process\/continuous_factory_process.csv\", index_col=\"time_stamp\")\nprint(raw_data.info())\nprint(raw_data.head())","1bfcc684":"#Drop setpoint data\nprint(\"*************************Remove set point from raw data*****************************\")\nset_point = [42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114]\nset_point_name = list(raw_data.columns[set_point])\nset_data = raw_data.drop(columns = set_point_name, axis=1)\nprint(set_data.info())","607b2f4d":"#Count 0 value in data\nprint(\"***************************Check 0 more than 30% in columns**************************\")\nfor i in range(len(set_data.columns[:])):\n    CC = set_data.values[:,i]\n    CCN = (((np.count_nonzero(CC == 0))\/14088)*100)\n    if (CCN>30):\n        DROP = (set_data.columns[i])\n        print (\"    Column Number \",i,\" Name \",DROP, \" Value =\",\"{:.2f}\".format(CCN ))\n","7a5d6753":"#Drop columns 0 more than 30% of output\nDrop_list = [42,46,47,48,52,55,74]\nDrop_list_name = list(set_data.columns[Drop_list])\nAraw_data = set_data.drop(columns= Drop_list_name, axis=1 )\n#Araw_list = list(Araw_data.columns)\nprint(\"********************************Columns after drop*******************************\")\nprint(\"    \",Araw_data.info())","f2ba9d4c":"#Data for model A\nInput_A = Araw_data.values[:,0:41]\nInput_AN = Araw_data.columns[0:41]\nInput_AA = pd.DataFrame(data=Input_A, columns=Input_AN)\nInput_AAN = pd.DataFrame(data=Input_A) #Non name\nOutput_A = Araw_data.values[:,41:50]\nOutput_AN = Araw_data.columns[41:50]\nOutput_AA = pd.DataFrame(data=Output_A, columns=Output_AN)\nprint(\"Input for model A\")\nprint(Input_AA.info())\nprint(\"Output for model A\")\nprint(Output_AA.info())","6ded7708":"#Data for model B\nIN  = [0,1,50,51,52,53,54,55,56,57,58,59,60,61,62,63]\nInput_B = Araw_data.values[:,IN]\nInput_BN = Araw_data.columns[IN]\nInput_BAN = pd.DataFrame(data=Input_B) #Non name\nInput_BA = pd.DataFrame(data=Input_B, columns=Input_BN)\nOutput_B = Araw_data.values[:,64:78]\nOutput_BN = Araw_data.columns[64:78]\nOutput_BA = pd.DataFrame(data=Output_B, columns=Output_BN)\nprint(\"Input for model B\")\nprint(Input_BA.info())\nprint(\"Output for model B\")\nprint(Output_BA.info())","82f13f16":"#Data for model C\nINC  = [0,1,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]\nInput_C = Araw_data.values[:,INC]\nInput_CN = Araw_data.columns[INC]\nInput_CA = pd.DataFrame(data=Input_C, columns=Input_CN)\nInput_CAN = pd.DataFrame(data=Input_C) #Non name\nOutput_CA = Output_BA\nprint(\"Input for model C\")\nprint(Input_CA.info())\nprint(\"Output for model C\")\nprint(Output_CA.info())","3c4712b7":"#Feature selection and prediction for model A\nprint(\"********************************Prediction model A (Stage 1)*******************************\")\nfor n in range (len(Output_AN)):\n    Yi = Output_AA.values [:,n];\n    # Backward Elimination #for feature selection\n    cols = list(Input_AAN.columns)\n    pmax = 1\n    while (len(cols) > 0):\n       p = []\n       X_1 = Input_AAN[cols]\n       X_1 = sm.add_constant(X_1)\n       model = sm.OLS(Yi, X_1).fit()\n       p = pd.Series(model.pvalues.values[1:], index=cols)\n       pmax = max(p)\n       feature_with_p_max = p.idxmax()\n       if (pmax > 0.05):\n          cols.remove(feature_with_p_max)\n       else:\n          break\n    selected_features = cols\n    #print(Output_AN[n], \"(Selected feature) =\", list(Input_AA.columns[cols])) #list of selected feature model A\n    print(Output_AN[n],\"     Total feature\",\"(\",len(Input_AN),\")\",\"Selected feature\",len(selected_features))\n    x_selected = Input_AA.values[:, selected_features]\n    x_train, x_test, y_train, y_test = train_test_split(x_selected, Yi, test_size=0.3)\n\n    # SVM-poly\n    svr_poly = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2, kernel='poly', degree=3))\n    svr_poly = svr_poly.fit(x_train, y_train)\n    Svr_poly = abs(svr_poly.score(x_test, y_test))\n    Svr_poly = \"{:.2f}\".format(Svr_poly * 100)\n\n    # Decision tree\n    clf = tree.DecisionTreeRegressor(max_features='auto')\n    clf = clf.fit(x_train, y_train)\n    CRF = abs(clf.score(x_test, y_test))\n    CRF = \"{:.2f}\".format(CRF * 100)\n\n    # KNN\n    neigh = KNeighborsRegressor(n_neighbors=5, weights='distance', algorithm='auto', leaf_size=30, p=2,\n                                metric='minkowski', metric_params=None, n_jobs=None)\n    neigh = neigh.fit(x_train, y_train)\n    KNN = abs(neigh.score(x_test, y_test))\n    KNN = \"{:.2f}\".format(KNN * 100)\n\n    # Accuracy\n    print(\"             %Prediction accuracy,\", \"Tree \", CRF, \" SVM-Poly \", Svr_poly, \"KNN \", KNN)","e896de81":"#Feature selection and prediction for model A\nprint(\"********************************Prediction model B (Stage 2)*******************************\")\nfor m in range (len(Output_BN)):\n    YBi = Output_BA.values [:,m];\n    # Backward Elimination #for feature selection\n    colsB = list(Input_BAN.columns)\n    pBmax = 1\n    while (len(colsB) > 0):\n       pB = []\n       X_2 = Input_AAN[colsB]\n       X_2 = sm.add_constant(X_2)\n       model = sm.OLS(YBi, X_2).fit()\n       pB = pd.Series(model.pvalues.values[1:], index=colsB)\n       pBmax = max(pB)\n       feature_with_p_maxB = pB.idxmax()\n       if (pBmax > 0.05):\n          colsB.remove(feature_with_p_maxB)\n       else:\n          break\n    selected_featuresB = colsB\n    #print(Output_BN[m], \"(Selected feature) =\", list(Input_BA.columns[colsB])) #list of selected feature model B\n    print(Output_BN[m],\"     Total feature\",\"(\",len(Input_BN),\")\",\"Selected feature\",len(selected_featuresB))\n    x_selectedB = Input_AA.values[:, selected_featuresB]\n    x_trainB, x_testB, y_trainB, y_testB = train_test_split(x_selectedB, YBi, test_size=0.3)\n\n    # SVM-poly\n    svr_polyB = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2, kernel='poly', degree=3))\n    svr_polyB = svr_polyB.fit(x_trainB, y_trainB)\n    Svr_polyB = abs(svr_polyB.score(x_testB, y_testB))\n    Svr_polyB = \"{:.2f}\".format(Svr_polyB * 100)\n\n    # Decision tree\n    clfB = tree.DecisionTreeRegressor(max_features='auto')\n    clfB = clfB.fit(x_trainB, y_trainB)\n    CRFB = abs(clfB.score(x_testB, y_testB))\n    CRFB = \"{:.2f}\".format(CRFB * 100)\n\n    # KNN\n    neighB = KNeighborsRegressor(n_neighbors=5, weights='distance', algorithm='auto', leaf_size=30, p=2,\n                                metric='minkowski', metric_params=None, n_jobs=None)\n    neighB = neighB.fit(x_trainB, y_trainB)\n    KNNB = abs(neighB.score(x_testB, y_testB))\n    KNNB = \"{:.2f}\".format(KNNB * 100)\n\n    # Accuracy\n    print(\"             %Prediction accuracy,\", \"Tree \", CRFB, \" SVM-Poly \", Svr_polyB, \"KNN \", KNNB)","b7235dd4":"print(\"*********************Prediction model C (Output Stage 1 sent to input stage 2 )*************\")\nfor r in range (len(Output_BN)):\n    YCi = Output_CA.values [:,r];\n    # Backward Elimination #for feature selection\n    colsC = list(Input_CAN.columns)\n    pCmax = 1\n    while (len(colsC) > 0):\n       pC = []\n       X_3 = Input_CAN[colsC]\n       X_3 = sm.add_constant(X_3)\n       model = sm.OLS(YCi, X_3).fit()\n       pC = pd.Series(model.pvalues.values[1:], index=colsC)\n       pCmax = max(pC)\n       feature_with_p_maxC = pC.idxmax()\n       if (pCmax > 0.05):\n          colsC.remove(feature_with_p_maxC)\n       else:\n          break\n    selected_featuresC = colsC\n    #print(Output_BN[r], \"(Selected feature) =\", list(Input_CA.columns[colsC])) #list of selected feature model C\n    print(Output_BN[r],\"     Total feature\",\"(\",len(Input_CN),\")\",\"Selected feature\",len(selected_featuresC))\n    x_selectedC = Input_CA.values[:, selected_featuresC]\n    x_trainC, x_testC, y_trainC, y_testC = train_test_split(x_selectedC, YCi, test_size=0.3)\n\n    # SVM-poly\n    svr_polyC = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2, kernel='poly', degree=3))\n    svr_polyC = svr_polyC.fit(x_trainC, y_trainC)\n    Svr_polyC = abs(svr_polyC.score(x_testC, y_testC))\n    Svr_polyC = \"{:.2f}\".format(Svr_polyC * 100)\n\n    # Decision tree\n    clfC = tree.DecisionTreeRegressor(max_features='auto')\n    clfC = clfC.fit(x_trainC, y_trainC)\n    CRFC = abs(clfC.score(x_testC, y_testC))\n    CRFC = \"{:.2f}\".format(CRFC * 100)\n\n    # KNN\n    neighC = KNeighborsRegressor(n_neighbors=5, weights='distance', algorithm='auto', leaf_size=30, p=2,\n                                metric='minkowski', metric_params=None, n_jobs=None)\n    neighC = neighC.fit(x_trainC, y_trainC)\n    KNNC = abs(neighC.score(x_testC, y_testC))\n    KNNC = \"{:.2f}\".format(KNNC * 100)\n\n    # Accuracy\n    print(\"             %Prediction accuracy,\", \"Tree \", CRFC, \" SVM-Poly \", Svr_polyC, \"KNN \", KNNC)","9091c1f8":"#Finish process time\nfinish_time = time.time()\ntime = finish_time-start_time\nprint('Process time', \"{:.2f}\".format(time),\" sec\")","1a07c861":"* Model B used ambient condition columns, machine 4-5 columns, exit zone condition columns to an input ","6ee5d7c7":"**Model B**\n\n> Feature selected and Data split\n1. Model B feature selection and data split were the same as model A. The only difference is variable names. \n\n>Model B accuracy\n1. The difference from model A. The accuracy show \"Poly-SVM\" is the highest average accuracy  but is a slight difference form K-nearest neighbor. \n2. The \"Decision tree\" is the most change ranking from highest to lowest rank. The reason that happened maybe occurred value of each point in same columns of output is a high difference.","5f4acd30":"Again in raw data, is seen some columns is many of zero values, maybe it can drop those columns from the prediction model. \n* I decided to use for loop to count and show the columns have %0 more than 30%\n* For loop was iteration in length set_data.columns \n* CC is values of set_data in index (column) i\n* CC is np. array, thus I use np.count_nonzero function to count 0 and divided by the total data in column i for create %0\n* \"if\" function was used for selected the column have %0 more than 30. 30% is not from the theory. It set by myself. \n* After the column has %0 more than 30 was selected. Those columns will show in the \"DROP\" variable.","936c028e":"Import raw data file (CSV) process was used pandas read CSV file. The timestamp column was used for the index of data\n* Raw data info was print to shown data type, number of columns, missing (null) in columns\n* Raw data head was print to shown columns head and some values in columns","06dd8285":"After the data preparing process (remove setpoint\/%0 columns). Data were split into input and output for models A, B, and C. Data splitting process used columns index for reference to split.\n* Model A used ambient condition columns, machine 1-3 columns, combination condition columns to an input ","ee07373c":"# Let's start the prediction session","019d9ed1":"# Let's strat data preparation","47c9e4bd":"**Model C**\n\n> Feature selected and Data split\n1. Model C feature selection and data split were the same as model A. The only difference is variable names. \n\n>Model C accuracy\n1. The average accuracy of all algorithm has seemed higher than model B. It means stage 1 output strongly effects to stage 2 output.\n2. In model C, The highest average accuracy is \"K-nearest neighbor\"\n","6c1be7ce":"From raw data, in the output session is seen data have setpoint columns (I think is only a reference value, not output). I decide to remove them from raw data.\nFor remove setpoint columns\n* From raw_data.info() above, I collected the index of setpoint columns and store it in the set_point variable\n* The set_point_name variable is a list of setpoint columns name, it used for select columns drop in the set_data variable\n* Print (set_data.info()) is shown info of data after dropped","9b0344b5":"**First import library** \n* Pandas for reading CSV and crate data frame\n* Statsmodels.api for the feature selection process\n* Numpy for array data management\n* Scikit-learn for management train\/test dataset and create a prediction model\n* Time for record process time\n* Warning for ignoring some systems warning","4870d3ae":"After I get the column's name above. Those columns removal process was started. List of remove columns was stored in the \"Drop_list and Drop_list_name\" variables. After removed data was stored in the \"Araw_data\" and print to show info of data.","b932d0fb":"**Process time** was show how long for calculate prediction models","273f245f":"Start record time before import raw data file","a6ffe673":"# **Prediction Multi-Stage Continuous-Flow Manufacturing Process**\n**Problem description**\n    The data of the input and output was collected from the multi-flow manufacturing line. The data was stored in CSV file type, name \"continuous_factory_process\". The manufacturing line was separated into 2 stages.\n*  **Stage 1**: Have 3 machines, all machine has parallel run.  After the product (Not sure they are a product or something) finished the process by machine. It is then sent to combine with products from other machines at the conveyor. At conveyor 15 output location was measured\n* **Stage 2**: Output of stage 1 was feed into stage 2 by the conveyor. Stage 2 have 2 machines, it was a series run. After the product finished the process by machine 5, the output was measured in 15 location.\n* **The goal of problem** 1. Prediction output of stage 1 2. Prediction output of stage 2 ","ce5fc47b":"**Model A**\n> Feature selection\n1. Feature selection: I used the p-value method to find a correlation of input and each output columns in the \"for loop\"\n2. Correlation was calculate using sm.OLS fitting model\n3. The p-value was used to remove the input columns has a low correlation to output columns. The p-values are one of the statistical indicator values. P-value 0.05 was coming from the significant level at 95% confidence interval (It generally used 0.05). If columns have a p-value of more than 0.05, they were removed.\n4. List of input effects to each output was stored in the \"Selected_features\" variable. That list does not show because difficult to see in the console. Number of total and selected feature was print.\n5. After getting the effect input on output, those inputs was select for the creation model in the \"x_selected\" variable\n\n> Data split and prediction algorithm\n1. Data (Input is x_selected, Output is Yi ) was split into train and test set in ratio 70:30 by split function from scikit-learn library\n2. The \"Decision tree\", \"Poly-Support vector machine\", and \"K-nearest neighbor\" were employed for the prediction output of the model. All algorithm is regression model.\n\n>Support vector machine\n* Support vector machine (SVM) is one of popular machine learning algorithm. The SVM is widely used in the classification problem but is can use for linear\/non-linear problems too. In this prediction model, SVM was selected because it is both flexible and works well especially when the information is complex.  The Poly-SVM algorithm is one of SVM. TThe Poly-SVM adds polynomial degree 3 into the model (For trial run degree 3 and 4 is no difference in accuracy). Input data of Poly-SVM was standardization before the training (for remove feature for use polynomial)\n\n>Decision trees\n* In this prediction model, the Decision trees algorithm was selected because of fast calculation and high accuracy.\n\n> K-nearest neighbor\n* In this prediction model, the K-nearest neighbor algorithm was selected because of it simple function, powerful and no training involved.\n* K-nearest neighbor is modify weights parameter only in scikit-learn library, using \"distance\". \n\n> Accuracy\n* The % prediction accuracy was presented by the score function.\n1. The accuracy show \"Decision tree\" is the highest  average accuracy. The accuracy around 40%+ except for Stage1.Output.Measurement4.U.Actual Stage1.Output.Measurement10.U.Actual.\n2. The \"Poly-SVM\" is the lowest is average accuracy comparision to another algorithm. Maybe the Poly-SVM algorithm is not suitable for this problem. \n3. The \"K-nearest neighbor\" is low accuracy too. The hihest accuracy around 50% at Stage1.Output.Measurement9.U.Actual.\n","7999cef6":"* Model C used ambient condition columns, stage 1 output (actual vale not from prediction), machine 4-5 columns, exit zone condition columns to an input ","a116a3fd":"# Prediction concept\nI decided to crate 3 models for predict this problem\n1. Model A: This model was used to predict only stage 1 output. The ambient condition, machine 1-3 parameters, and combination zone parameters were employed to be an input of this model\n2. Model B: This model is the same concept as model A. The model was used to predict stage 2 output only. The ambient condition, machine 4-5 parameters, and exit zone parameters were employed to be an input of this model.\n3. Model C: This model is the same concept as model B (same output) but different input parameters. From the problem description, it saw the output at stage 1 was feed into stage 2 maybe the output at stage 1 is can use to be an input for predict output at stage 2. Thus in model C, I will use output at stage 1, ambient condition, machine 4-5 parameters, and exit zone parameters to input this model."}}