{"cell_type":{"7c9cadda":"code","576de65c":"code","4249ad1c":"code","e7080840":"code","48cfa6d5":"code","a60fe5d2":"code","6f4ee9f1":"code","98ff0beb":"code","1d271110":"code","6022c6f1":"code","dc63ec67":"code","bfe9610c":"code","1719b6fa":"code","4ae3ee33":"code","78719796":"code","1d8a1746":"code","22cbc31b":"code","833154da":"code","c7e6b621":"code","973714a8":"code","8c2a6817":"code","67164c55":"code","5a615fb4":"code","b471b313":"code","6b0cfcb3":"code","7bfadadc":"code","5c9412a9":"code","c761fcdb":"code","6dab2f13":"code","1651836a":"code","39c388b1":"code","bfea9eaa":"markdown"},"source":{"7c9cadda":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom scipy.sparse import hstack","576de65c":"train = pd.read_csv('..\/input\/train.tsv', delimiter='\\t').fillna(' ')\ntest = pd.read_csv('..\/input\/test.tsv', delimiter='\\t').fillna(' ')\nsampleSubmission = pd.read_csv('..\/input\/sampleSubmission.csv')","4249ad1c":"train.head()","e7080840":"test.head()","48cfa6d5":"sampleSubmission.head()","a60fe5d2":"train.Sentiment.unique()","6f4ee9f1":"Sentiments = pd.get_dummies(train.Sentiment)\nSentiments.head()","98ff0beb":"Sentiments[0].values","1d271110":"train_text = train['Phrase']\ntest_text = test['Phrase']\nall_text = pd.concat([train_text, test_text])","6022c6f1":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 3),\n    max_features=18000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)","dc63ec67":"char_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(1, 7),\n    max_features=60000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)","bfe9610c":"train_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n","1719b6fa":"predictions = []\nsubmission = pd.DataFrame.from_dict({'PhraseId': test['PhraseId']})\nfor i in range(5):\n    train_target = Sentiments[i].values\n    classifier = LogisticRegression(C=2.65, solver='sag')\n\n    #cv_prediction = cross_val_predict(classifier, train_features, train_target, cv=3, method='predict_proba')\n    #predictions.append(cv_prediction[:, 1])\n    #print('CV score for class {} is {}'.format(i, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[str(i)] = classifier.predict_proba(test_features)[:, 1]","4ae3ee33":"predictions_2 = []\nsubmission_2 = pd.DataFrame.from_dict({'PhraseId': test['PhraseId']})\nfor i in range(5):\n    train_target = Sentiments[i].values\n    classifier = LogisticRegression(C=2.6, solver='sag')\n\n    #cv_prediction = cross_val_predict(classifier, train_features, train_target, cv=3, method='predict_proba')\n    #predictions_2.append(cv_prediction[:, 1])\n    #print('CV score for class {} is {}'.format(i, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission_2[str(i)] = classifier.predict_proba(test_features)[:, 1]","78719796":"predictions_words = []\nsubmission_words = pd.DataFrame.from_dict({'PhraseId': test['PhraseId']})\nfor i in range(5):\n    train_target = Sentiments[i].values\n    classifier = LogisticRegression(C=2.6, solver='sag')\n\n    #cv_prediction = cross_val_predict(classifier, train_word_features, train_target, cv=3, method='predict_proba')\n    #predictions_words.append(cv_prediction[:, 1])\n    #print('CV score for class {} is {}'.format(i, cv_score))\n\n    classifier.fit(train_word_features, train_target)\n    submission_words[str(i)] = classifier.predict_proba(test_word_features)[:, 1]","1d8a1746":"predictions_chars = []\nsubmission_chars = pd.DataFrame.from_dict({'PhraseId': test['PhraseId']})\nfor i in range(5):\n    train_target = Sentiments[i].values\n    classifier = LogisticRegression(C=2.6, solver='sag')\n\n    #cv_prediction = cross_val_predict(classifier, train_char_features, train_target, cv=3, method='predict_proba')\n    #predictions_chars.append(cv_prediction[:, 1])\n    #print('CV score for class {} is {}'.format(i, cv_score))\n\n    classifier.fit(train_char_features, train_target)\n    submission_chars[str(i)] = classifier.predict_proba(test_char_features)[:, 1]","22cbc31b":"predictions_chars[0]","833154da":"predictions_words[0]","c7e6b621":"submission.head()","973714a8":"submission_2.head()","8c2a6817":"submission_chars.head()","67164c55":"submission_words.head()","5a615fb4":"print('Total CV score is {}'.format(np.mean(scores)))","b471b313":"submission.head()","6b0cfcb3":"submission[['0', '1', '2', '3', '4']].values","7bfadadc":"blend_1 = 0.5*submission[['0', '1', '2', '3', '4']].values+0.5*submission_2[['0', '1', '2', '3', '4']].values\nblend_2 = (0.35*submission[['0', '1', '2', '3', '4']].values+0.35*submission_2[['0', '1', '2', '3', '4']].values+\n           0.15*submission_words[['0', '1', '2', '3', '4']].values+0.15*submission_chars[['0', '1', '2', '3', '4']].values)","5c9412a9":"predictions_1 = np.round(np.argmax(blend_1, axis=1)).astype(int)\npredictions_2 = np.round(np.argmax(blend_2, axis=1)).astype(int)","c761fcdb":"predictions_1","6dab2f13":"predictions_2","1651836a":"sampleSubmission['Sentiment'] = predictions_1\nsampleSubmission.to_csv(\"LR_Prediction_1.csv\", index=False)\nsampleSubmission['Sentiment'] = predictions_2\nsampleSubmission.to_csv(\"LR_Prediction_2.csv\", index=False)","39c388b1":"sampleSubmission.head()","bfea9eaa":"I wanted to see if a simple n-gram approach with Logistic regression can get me somewhere in this competition as well. So far it seems to be working pretty well, but the script needs to be optimized further."}}