{"cell_type":{"1c0f680b":"code","10cb1e91":"code","ebd882f0":"code","2099e2af":"code","2c61ec57":"code","8b63638f":"code","7640d4c8":"code","a77f1151":"code","ff372526":"code","d50bbf5c":"code","4eb0d92d":"code","299fabf9":"code","d009cb12":"code","6b4a47b7":"code","9b145310":"code","af9aeaaf":"code","a5510e6c":"code","7a204bff":"code","61849c03":"code","619241b5":"code","e1262ffe":"code","2d8102b9":"code","377eef68":"code","21076ffd":"code","ac2116f1":"code","21101efa":"code","1abae807":"code","c6fb0d14":"code","eb8ea4d5":"code","ecd6c4a9":"code","ef27eaa2":"code","68beb287":"code","9f6d251b":"code","e477d1b3":"code","97ede479":"code","0660de4a":"code","10e7866f":"code","782c0711":"code","b6a148cd":"code","c7ed4dea":"code","d764fcb7":"code","6f8b7aef":"code","22ec9c52":"code","6063d2c8":"code","59349be5":"code","64b31577":"code","65986b36":"code","c8f2bd50":"code","848cac25":"code","437a54b5":"code","d1980c99":"code","e44abcfe":"code","885683b9":"code","6f645949":"code","99b4170d":"code","cccb9c4b":"code","ce3f3843":"code","290c94f8":"code","9256b64a":"code","dcc771ea":"code","264a8e1b":"code","e9114446":"code","1e7dbf96":"code","59bea1ec":"code","967a7607":"code","05450759":"code","882a0bc7":"code","18006945":"code","2b205dc2":"code","6eabfee0":"code","18fc9664":"code","53a15a9e":"code","c96cd557":"code","f583801d":"code","87802950":"code","96c05261":"code","ddd4b683":"code","fecee13a":"code","3bd0caeb":"code","6c6344e1":"code","ffd2f0dd":"code","60ffd453":"code","6d200b4f":"code","d4496ad7":"markdown","5dd076c4":"markdown","ccdc53ea":"markdown","1168748c":"markdown","e19b1f2e":"markdown","cb886c6b":"markdown","24f38049":"markdown","647033ea":"markdown","4e2a1fe7":"markdown","24105678":"markdown","44a48a9a":"markdown","d89cc7dd":"markdown","af597143":"markdown","37b92e14":"markdown","4712d56b":"markdown","fc0ab0f6":"markdown","f53c04d6":"markdown","54c7d8b8":"markdown","dccc23a6":"markdown","b5b6c6e0":"markdown","3546d1db":"markdown","addcd156":"markdown","e0ba5542":"markdown","3af5441c":"markdown","2aec27ab":"markdown","00dc1a0d":"markdown","85e624e1":"markdown","e7f93662":"markdown","afcd3a39":"markdown","be4ddc50":"markdown","48956e6d":"markdown","f8b1d392":"markdown","78a4fff6":"markdown","78e79606":"markdown","92061275":"markdown","dae3f7d3":"markdown","5a8fdbd0":"markdown","74c0ddd6":"markdown","40f3c30c":"markdown"},"source":{"1c0f680b":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_numeric_dtype, is_object_dtype\n\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","10cb1e91":"train_df","ebd882f0":"def get_cat_num_features(df):\n    \n    num_features = []\n    cat_features = []\n    \n    for col in df.columns:\n        if is_numeric_dtype(df[col]):\n            num_features.append(col)\n                \n        if is_object_dtype(df[col]):\n            cat_features.append(col)\n            \n    return num_features, cat_features\n\ndef get_unique_df(features):\n    unique_df = pd.DataFrame(columns=['Feature', 'Unique', 'Count'])\n    for col in features.columns:\n        v = features[col].unique()\n        l = len(v)\n        unique_df = unique_df.append({'Feature':col, \n                                     'Unique':v,\n                                     'Count':l}, ignore_index=True)\n    return unique_df\n\ndef get_null_df(features):\n    col_null_df = pd.DataFrame(columns = ['Column', 'Type', 'Total NaN', '%'])\n    col_null = features.columns[features.isna().any()].to_list()\n    L = len(features)\n    for col in col_null:\n        T = 0\n        if is_numeric_dtype(features[col]):\n            T = \"Numerical\"  \n        else:\n            T = \"Categorical\"\n        nulls = len(features[features[col].isna() == True][col])   \n        col_null_df = col_null_df.append({'Column': col, \n                                          'Type': T,\n                                          'Total NaN': nulls,\n                                          '%': (nulls \/ L)*100\n                                         }, ignore_index=True)\n        \n    return col_null_df\n\ndef summary(data):\n    \n    print(\"Samples --> \", len(data))\n    print()\n    target = data['SalePrice']\n    features = data.drop(['SalePrice'], axis=1)\n    print(\"Features --> \", len(features.columns))\n    print(\"\\n\",features.columns)\n    \n    num_features, cat_features = get_cat_num_features(features)\n      \n    print()\n    print(\"\\nNumerical Features --> \", len(num_features))\n    print()\n    print(num_features)\n    print()\n    print(\"Categorical Features -->\", len(cat_features))\n    print()\n    print(cat_features)\n    print()\n    print(\"*************************************************\")\n    stats = features.describe().T\n    \n    print()\n    print(\"Value counts of each categorical feature\\n\")\n    for col in cat_features:\n        print(col)\n        print(features[col].value_counts())\n        print()\n        \n    unique_df = get_unique_df(features)\n    \n    col_null_df = get_null_df(features)\n    \n    return {'features':features, \n            'target': target, \n            'stats': stats, \n            'unique_df':unique_df,\n            'col_null_df': col_null_df}","2099e2af":"df_summary = summary(train_df)","2c61ec57":"# Features with only 1 unique value\ndf_summary['unique_df'][df_summary['unique_df']['Count'] == 1]","8b63638f":"# Features with null values\ndf_summary['col_null_df']","7640d4c8":"# stats of the numerical feature\ndf_summary['stats']","a77f1151":"target = df_summary['target']\ntarget","ff372526":"df = df_summary['features']\n\ncleaned_df = df.drop_duplicates(subset=['Id'])\nnew_test_df = test_df.drop_duplicates(subset=['Id'])\n\nprint(\"Total Duplicates were \", len(df) - len(cleaned_df))","d50bbf5c":"# col_null_df = df_summary['col_null_df']\n# col_null_df[col_null_df['%']>=80]","4eb0d92d":"# null_cols = col_null_df[col_null_df['%']>=80]['Column'].to_list()\n\n# cleaned_df.drop(null_cols, axis=1, inplace=True)\n# new_test_df.drop(null_cols, axis=1, inplace=True)\n\n# print(cleaned_df.shape)\n# print(new_test_df.shape)","299fabf9":"cleaned_df.drop(['Id'], axis=1, inplace=True)\nnew_test_df.drop(['Id'], axis=1, inplace=True)\n\nprint(cleaned_df.shape)\nprint(new_test_df.shape)","d009cb12":"df_summary['unique_df'][df_summary['unique_df']['Count']==1]","6b4a47b7":"# col_null_df[col_null_df['%'] < 80]\ncol_null_df = df_summary[\"col_null_df\"]","9b145310":"# null values in test_df\n\ntest_null_df = get_null_df(new_test_df)\ntest_null_df","af9aeaaf":"def imputation(null_df, df):\n    \n    for ind, row in null_df.iterrows():\n        col = row['Column']\n        if row['Type'] == 'Categorical':\n            df[col].fillna('NotAvail', inplace=True)\n        else:\n            df[col].fillna(df[col].median(), inplace=True)\n    \n    return df\n\n# null_df = col_null_df[col_null_df['%'] < 80]\nnull_df = col_null_df\n\ncleaned_df = imputation(null_df, cleaned_df)\nnew_test_df = imputation(test_null_df, new_test_df)\n\nprint(cleaned_df.shape)\nprint(new_test_df.shape)","a5510e6c":"cleaned_df.columns[cleaned_df.isna().any()]","7a204bff":"new_test_df.columns[new_test_df.isna().any()]","61849c03":"print(\"Min & Max of YearBuilt \", cleaned_df['YearBuilt'].min(), cleaned_df['YearBuilt'].max())\nprint(\"\\nMin & Max of GarageYrBlt \", cleaned_df['GarageYrBlt'].min(), cleaned_df['GarageYrBlt'].max())\nprint(\"\\nMin & Max of YrSold \", cleaned_df['YrSold'].min(), cleaned_df['YrSold'].max())\nprint(\"\\nMin & Max of YearRemodAdd \", cleaned_df['YearRemodAdd'].min(), cleaned_df['YearRemodAdd'].max())","619241b5":"cleaned_df[cleaned_df['YearBuilt']<1900]['YearBuilt'].count()","e1262ffe":"cleaned_df[cleaned_df['YearBuilt']==1900]['YearBuilt'].count()","2d8102b9":"bins =  [1800] + [i for i in range(1900, 2020, 10)]\nbins","377eef68":"def create_new_features(cleaned_df):\n    cleaned_df['Remod_Built_Age'] = cleaned_df['YearRemodAdd'] - cleaned_df['YearBuilt']\n    cleaned_df['Sold_Built_Age'] = cleaned_df['YrSold'] - cleaned_df['YearBuilt']\n    cleaned_df['Remod_Sold_Age'] = cleaned_df['YrSold'] - cleaned_df['YearRemodAdd']\n\n    cleaned_df['MSSubClass'].replace({20:\"1-STORY 1946 & NEWER\",\n                                   30:\"1-STORY 1945 & OLDER\",\n                                   40:\"1-STORY W\/FINISHED\",\n                                   45:\"1-1\/2 STORY - UNFINISHED\",\n                                   50:\"1-1\/2 STORY FINISHED\",\n                                   60:\"2-STORY 1946 & NEWER\",\n                                   70:\"2-STORY 1945 & OLDER\",\n                                   75:\"2-1\/2 STORY ALL AGES\",\n                                   80:\"SPLIT OR MULTI-LEVEL\",\n                                   85:\"SPLIT FOYER\",\n                                   90:\"DUPLEX\",\n                                   120:\"1-STORY PUD\",\n                                   150:\"1-1\/2 STORY PUD\",\n                                   160:\"2-STORY PUD\",\n                                   180:\"PUD - MULTILEVEL\",\n                                   190:\"2 FAMILY CONVERSION\"                         \n                                  },inplace=True)\n\n    cleaned_df['MoSold'].replace({1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",\n                             7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n                            ,inplace=True)\n\n    return cleaned_df\n\n# Function for binning\ndef binning(df, col, bins, test=None):\n    df[col] = df[col].astype(int)\n    binned_values = list()\n    for ind, i in enumerate(df[col]):\n        for j, k in enumerate(bins):\n            if i <= k and i!= 1800:\n                binned_values.append('{}-{}'.format(bins[j-1], k))\n                if col == 'GarageYrBlt' and ind==1458 and test!= None:\n                    binned_values.append('{}-{}'.format(bins[j-1], k))\n                    \n                    \n                break\n            if i <= k and i == 1800:\n                binned_values.append('{}'.format(k))\n                break\n    df[col] = binned_values\n    return df","21076ffd":"create_new_features(cleaned_df)\ncreate_new_features(new_test_df)\n\nyr_cols = ['YearBuilt', 'GarageYrBlt', 'YrSold', 'YearRemodAdd']\n\nfor col in yr_cols:\n    binning(cleaned_df, col, bins)\n    binning(new_test_df, col, bins, 1)\n\nprint(cleaned_df.shape)\nprint(new_test_df.shape)","ac2116f1":"def calc_interquartile(df, column):\n    \n    #calculating the first and third quartile\n    first_quartile, third_quartile = np.percentile(df[column], 25), np.percentile(df[column], 75)\n    \n    #calculate the interquartilerange\n    iqr = third_quartile - first_quartile\n    \n    # outlier cutoff (1.5 is a generally taken as a threshold thats why i am also taking it)\n    cutoff = iqr*1.5\n    \n    #calculate the lower and upper limits\n    lower, upper = first_quartile - cutoff , third_quartile + cutoff\n    \n    #remove the outliers from the columns\n    upper_outliers = df[df[column] > upper]\n    lower_outliers = df[df[column] < lower]\n    \n    return lower, upper, lower_outliers.shape[0]+upper_outliers.shape[0]\n\n\ndef get_outliers(df, num_feat):\n    \n    outlier_df = pd.DataFrame(columns=['Feature', 'Total Outliers','Upper limit', 'Lower limit'])\n    \n    for col in num_feat:\n        lower, upper, total = calc_interquartile(df, col)\n        if total != 0 and (upper !=0 and lower!=0):\n            outlier_df = outlier_df.append({'Feature':col, 'Total Outliers': total,\n                                       'Upper limit': upper, 'Lower limit':lower}, ignore_index=True)\n        \n    return outlier_df\n\nnum_feat, _ = get_cat_num_features(cleaned_df)\n\noutlier_df = get_outliers(cleaned_df, num_feat)\noutlier_df","21101efa":"def remove_outliers(df, outlier_df, num_feat):\n    \n    for col in outlier_df['Feature'].to_list():\n        upper = outlier_df[outlier_df['Feature']== col ]['Upper limit'].values[0]\n        lower = outlier_df[outlier_df['Feature']== col ]['Lower limit'].values[0]\n        \n        df[col] = np.where(df[col]>upper, upper, df[col])\n        df[col] = np.where(df[col]<lower, lower, df[col])\n        \n    return df\n\ncleaned_df = remove_outliers(cleaned_df, outlier_df, num_feat)","1abae807":"get_outliers(cleaned_df, num_feat)","c6fb0d14":"print(cleaned_df.shape)\nprint(new_test_df.shape)","eb8ea4d5":"unique_df = get_unique_df(cleaned_df)\nunique_df","ecd6c4a9":"# columns with single unique values\nunique_df[unique_df['Count']==1]","ef27eaa2":"cols = unique_df[unique_df['Count']==1]['Feature'].to_list()\n\ncleaned_df.drop(cols, axis=1, inplace=True)\nnew_test_df.drop(cols, axis=1, inplace=True)\n\nget_unique_df(cleaned_df)[get_unique_df(cleaned_df)['Count']==1]","68beb287":"print(cleaned_df.shape)\nprint(new_test_df.shape)","9f6d251b":"# BOX COX TRANSFORMATION\n\n# from scipy import stats\n# trans_df = cleaned_df.copy()\n# num_feat, _ = get_cat_num_features(cleaned_df)\n\n# def transformed_feat(trans_df, num_feat):\n    \n#     for col in num_feat:\n#         try:\n#             trans_df[col], _ = stats.boxcox(cleaned_df[col])\n#         except:\n#             # if there are observations which 0 or negative, shit their values to 0.001 to make them above 0\n#             trans_df[col] = np.where(trans_df[col]<=0, 0.001, trans_df[col])\n#             trans_df[col], _ = stats.boxcox(trans_df[col])\n        \n#     return trans_df\n\n# trans_df = transformed_feat(trans_df, num_feat)\n# new_test_df = transformed_feat(new_test_df, num_feat)","e477d1b3":"from sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='yeo-johnson')","97ede479":"trans_df = cleaned_df.copy()\nnum_feat, _ = get_cat_num_features(cleaned_df)\n\ndef transformed_feat(trans_df, new_test_df, num_feat):\n    \n    for col in num_feat:\n        t = scaler.fit_transform(np.array(cleaned_df[col]).reshape(-1,1))\n        trans_df[col] = t.reshape(-1)\n        t = scaler.transform(np.array(new_test_df[col]).reshape(-1, 1))\n        new_test_df[col] = t.reshape(-1)\n        \n    return trans_df, new_test_df\n\ntrans_df, new_test_df = transformed_feat(trans_df, new_test_df, num_feat)","0660de4a":"print(trans_df.shape)\nprint(new_test_df.shape)","10e7866f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")","782c0711":"num_feat, cat_feat = get_cat_num_features(trans_df)","b6a148cd":"#  Scatterplot for numerical features\n\nplt.figure(figsize=(20,90))\nfor i in range(len(num_feat)):\n    plt.subplot(12, 3, i+1)\n    sns.scatterplot(x=trans_df[num_feat[i]], y=target)\n\nplt.show()","c7ed4dea":"#  Bar Plot for Categorical Features\nfor col in cat_feat:\n    plt.figure(figsize=(30, 10))\n    sns.barplot(x=trans_df[col], y=target)\n    plt.show()","d764fcb7":"# Box Plot of Numerical Features\n\nplt.figure(figsize=(20,90))\nfor i in range(len(num_feat)):\n    plt.subplot(12, 3, i+1)\n    sns.boxplot(y=trans_df[num_feat[i]])\n\nplt.show()","6f8b7aef":"# Distribution Plots\nfor i in num_feat:\n    sns.displot(x=trans_df[i], kde=True)\n    plt.show()","22ec9c52":"from sklearn.preprocessing import LabelEncoder","6063d2c8":"yr_cols.remove('YrSold')\nyr_cols","59349be5":"ordinal_feat = ['LotShape','LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n               'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'FireplaceQu',\n               'GarageQual', 'GarageCond' , 'Utilities', \"PoolQC\"]\n\nnominal_feat = ['MSSubClass', 'MSZoning','Street', 'LotConfig', 'Neighborhood',\n               'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n               'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'Electrical', 'Functional',\n               'GarageType', 'GarageFinish', 'PavedDrive','SaleType', 'SaleCondition'\n               , 'LandContour', 'MoSold', \"Alley\", \"Fence\", \"MiscFeature\"]","64b31577":"ord_dict = {\"LotShape\": ['Reg','IR1','IR2','IR3', 'NotAvail'],\n            \"LandSlope\" : [\"Gtl\", \"Mod\", \"Sev\",'NotAvail' ],\n            \"ExterQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", 'NotAvail' ],\n            \"ExterCond\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", 'NotAvail' ],\n            \"BsmtQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"BsmtCond\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"BsmtExposure\": [\"Gd\", \"Av\", \"Mn\", \"No\", \"NA\", 'NotAvail'],\n            \"BsmtFinType1\":[ \"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NA\", 'NotAvail'],\n            \"BsmtFinType2\":[ \"GLQ\",\"ALQ\",\"BLQ\",\"Rec\",\"LwQ\",\"Unf\",\"NA\", 'NotAvail'],\n            \"HeatingQC\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"KitchenQual\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"FireplaceQu\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"GarageQual\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"GarageCond\": [  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ],\n            \"Utilities\":  [ \"AllPub\", \"NoSewr\", \"NoSeWa\",\"ELO\", \"NotAvail\"],\n            \"PoolQC\":[  \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"NA\", 'NotAvail' ]\n           \n           }","65986b36":"enc_df = trans_df.copy()\ntest_enc_df = new_test_df.copy()\n\ndef encode_feat(nom_feat, ord_feat, yr_cols, df, t_df=pd.DataFrame()):\n    \n    # Label encoding ordinal features\n    le = LabelEncoder()\n    \n    for col in ord_feat:\n        le.fit(ord_dict[col])\n        df[col] = le.transform(df[col])\n        if len(t_df) != 0:\n            t_df[col] = le.transform(t_df[col])\n            \n    for col in yr_cols:\n        df[col] = le.fit_transform(df[col])\n        if len(t_df) != 0:\n            t_df[col] = le.transform(t_df[col])\n    \n    # dummy encoding nominal features\n    for col in nom_feat:\n        dum = pd.get_dummies(df[col], prefix=col)\n        df = pd.concat([df, dum], axis=1)\n        df.drop([col], axis=1, inplace=True)\n        \n        if len(t_df) != 0:\n            t_df = pd.concat([t_df, dum], axis=1)\n            t_df.drop([col], axis=1, inplace=True)\n     \n    if len(t_df) != 0:\n        return df, t_df\n    else:\n        return df\n\n\nenc_df, test_enc_df = encode_feat(nominal_feat, ordinal_feat, yr_cols, enc_df, test_enc_df) ","c8f2bd50":"print(enc_df.shape)\nprint(test_enc_df.shape)","848cac25":"test_enc_df = test_enc_df.iloc[:-1, :]\ntest_enc_df","437a54b5":"print(enc_df.shape)\nprint(test_enc_df.shape)","d1980c99":"from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet, BayesianRidge,RANSACRegressor,HuberRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, make_scorer\nfrom sklearn.pipeline import Pipeline","e44abcfe":"sc = ('Scaler', StandardScaler())\nest =[]\nest.append(('LinearRegression', Pipeline([sc, ('LinearRegression', LinearRegression())])))\nest.append(('Ridge', Pipeline([sc, ('Ridge', Ridge())])))\nest.append(('Lasso', Pipeline([sc, ('Lasso', Lasso())])))\nest.append(('BayesianRidge', Pipeline([sc, ('BayesianRidge', BayesianRidge())])))\nest.append(('ElasticNet', Pipeline([sc,('Elastic', ElasticNet())])))\nest.append(('SGD', Pipeline([sc,('SGD', SGDRegressor())])))\nest.append(('Huber', Pipeline([sc,('Huber', HuberRegressor())])))\nest.append(('RANSAC', Pipeline([sc,('RANSAC', RANSACRegressor())])))\nest.append(('GradientBoosting', Pipeline([sc,('GradientBoosting',GradientBoostingRegressor())])))\nest.append(('AdaBoost', Pipeline([sc, ('AdaBoost', AdaBoostRegressor())])))\nest.append(('ExtraTree', Pipeline([sc,('ExtraTrees', ExtraTreesRegressor())])))\nest.append(('RandomForest', Pipeline([sc,('RandomForest', RandomForestRegressor())]))) \nest.append(('Bagging', Pipeline([sc,('Bagging', BaggingRegressor())])))\nest.append(('KNeighbors', Pipeline([sc,('KNeighbors', KNeighborsRegressor())])))\nest.append(('DecisionTree', Pipeline([sc,('DecisionTree', DecisionTreeRegressor())])))\nest.append(('XGB', Pipeline([sc,('XGB', XGBRegressor())])))","885683b9":"import warnings\nwarnings.filterwarnings(action='ignore')\nseed = 4\nsplits = 7\nmodels_score ={}\nfor i in est:\n    kfold = KFold(n_splits=splits, random_state=seed, shuffle=True)\n    results = cross_val_score(i[1], enc_df, target, cv=kfold)\n    models_score.update({i[0] : results.mean()})\n    \nsorted(models_score.items(), key= lambda v:v[1], reverse=True)","6f645949":"base_model_scores = sorted(models_score.items(), key= lambda v:v[1], reverse=True)","99b4170d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,  f_classif","cccb9c4b":"num_feat, cat_feat = get_cat_num_features(trans_df)\nnum_df = trans_df[num_feat]\ncat_df = trans_df[cat_feat]\n\nprint(\"Total Numerical Features = \", len(num_feat))\nprint(\"Total Categorical Features = \", len(cat_feat))","ce3f3843":"# define feature selection\nnum_fs = SelectKBest(score_func=f_regression, k=20)\n# apply feature selection\nnum_fs.fit(num_df, target)\n# get the column indices\ncols  = num_fs.get_support(indices=True)\nbest_num_df = num_df.iloc[:,cols]\n\nbest_num_df","290c94f8":"import warnings\nwarnings.filterwarnings(action='ignore')\n\ndef lab_encode_feat(df):\n    \n    # Label encoding ordinal features\n    le = LabelEncoder()\n    \n    for col in df.columns:\n        df[col] = le.fit_transform(df[col])\n        \n    return df\n\nlab_encode_feat(cat_df) \ncat_df","9256b64a":"cat_fs = SelectKBest(score_func=f_classif, k=30)\ncat_fs.fit(cat_df, target)\ncols = cat_fs.get_support(indices=True)\nbest_cat_df = cat_df.iloc[:, cols]\n\nbest_cat_df","dcc771ea":"best_cols = best_num_df.columns.to_list() + best_cat_df.columns.to_list()\nbest_cols","264a8e1b":"best_feat_df = trans_df[best_cols]\nbest_feat_df","e9114446":"best_cat_df.columns.to_list()","1e7dbf96":"# strong nominal and ordinal columns\nnom_cols = []\nord_cols = []\nfor col in best_cat_df.columns.to_list():\n    if col in nominal_feat:\n        nom_cols.append(col)\n    else:\n        ord_cols.append(col)","59bea1ec":"nom_cols","967a7607":"ord_cols","05450759":"ord_cols.remove('YearBuilt')\nord_cols.remove('YearRemodAdd')\nord_cols.remove('GarageYrBlt')","882a0bc7":"best_feat_df = encode_feat(nom_cols, ord_cols, yr_cols, best_feat_df)\nbest_feat_df","18006945":"import warnings\nwarnings.filterwarnings(action='ignore')\nseed = 4\nsplits = 7\nmodels_score ={}\nfor i in est:\n    kfold = KFold(n_splits=splits, random_state=seed, shuffle=True)\n    results = cross_val_score(i[1], best_feat_df, target, cv=kfold)\n    models_score.update({i[0] : results.mean()})\n    \nsorted(models_score.items(), key= lambda v:v[1], reverse=True)","2b205dc2":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaled_df = StandardScaler().fit_transform(enc_df)\npca = PCA(n_components=0.99, svd_solver='full')\npca_enc_df = pca.fit_transform(scaled_df)\n\npca_enc_df.shape","6eabfee0":"est =[]\nest.append(('LinearRegression', Pipeline([('LinearRegression', LinearRegression())])))\nest.append(('Ridge', Pipeline([('Ridge', Ridge())])))\nest.append(('Lasso', Pipeline([ ('Lasso', Lasso())])))\nest.append(('BayesianRidge', Pipeline([('BayesianRidge', BayesianRidge())])))\nest.append(('ElasticNet', Pipeline([('Elastic', ElasticNet())])))\nest.append(('SGD', Pipeline([('SGD', SGDRegressor())])))\nest.append(('Huber', Pipeline([('Huber', HuberRegressor())])))\nest.append(('RANSAC', Pipeline([('RANSAC', RANSACRegressor())])))\nest.append(('GradientBoosting', Pipeline([('GradientBoosting',GradientBoostingRegressor())])))\nest.append(('AdaBoost', Pipeline([('AdaBoost', AdaBoostRegressor())])))\nest.append(('ExtraTree', Pipeline([('ExtraTrees', ExtraTreesRegressor())])))\nest.append(('RandomForest', Pipeline([('RandomForest', RandomForestRegressor())]))) \nest.append(('Bagging', Pipeline([('Bagging', BaggingRegressor())])))\nest.append(('KNeighbors', Pipeline([('KNeighbors', KNeighborsRegressor())])))\nest.append(('DecisionTree', Pipeline([('DecisionTree', DecisionTreeRegressor())])))\nest.append(('XGB', Pipeline([('XGB', XGBRegressor())])))","18fc9664":"import warnings\nwarnings.filterwarnings(action='ignore')\nseed = 4\nsplits = 7\nmodels_score ={}\nfor i in est:\n    kfold = KFold(n_splits=splits, random_state=seed, shuffle=True)\n    results = cross_val_score(i[1], pca_enc_df, target, cv=kfold)\n    models_score.update({i[0] : results.mean()})\n    \nsorted(models_score.items(), key= lambda v:v[1], reverse=True)","53a15a9e":"# Top model with scores\nbase_model_scores[0]","c96cd557":"from sklearn.model_selection import GridSearchCV","f583801d":"# %%time\n\n# sc = ('Scaler', StandardScaler())\n# h_est = []\n# h_est.append(('GBR', Pipeline([sc,('GBR',GradientBoostingRegressor())])))\n\n# best = []\n# seed = 4\n\n# parameters = {\n              \n#               'GBR': {'GBR__learning_rate': [0.1, 0.01],\n#                          'GBR__max_depth': [4,6,8],\n#                       'GBR__n_estimators': [400, 500, 600]}\n#              }\n\n\n# for i in h_est:\n#     kfold = KFold(n_splits=3, random_state=seed, shuffle=True)\n#     grid = GridSearchCV(estimator=i[1], param_grid = parameters[i[0]], cv = kfold, n_jobs=-1)\n#     grid.fit(enc_df, target)\n#     best.append((i[0], grid.best_score_,  grid.best_params_))","87802950":"# best","96c05261":"# param_dict = best[0][2]\n# param = {}\n# for i in param_dict:\n#     key = i.split('__')[1]\n#     param.update({key:param_dict[i]})\n    \n# param","ddd4b683":"# std_scaler = StandardScaler()\n# std_scaler.fit(enc_df)\n# scaled_df = std_scaler.transform(enc_df)\n\n# model = GradientBoostingRegressor(learning_rate= param['learning_rate'], \n#                                   max_depth= param['max_depth'], \n#                                   n_estimators=param['n_estimators'])\n# model.fit(scaled_df, target)\n\n# top_model = XGBRegressor(learning_rate=0.1,  \n#                                   n_estimators= 1000)\n# top_model.fit(scaled_df, target)","fecee13a":"top_model_name = base_model_scores[0][0]\ntop_model = dict(est)[top_model_name][0]\n\nstd_scaler = StandardScaler()\nstd_scaler.fit(enc_df)\nscaled_df = std_scaler.transform(enc_df)\n\ntop_model.fit(scaled_df, target)","3bd0caeb":"test_scaled_df = std_scaler.transform(test_enc_df)\npredictions = top_model.predict(test_scaled_df)","6c6344e1":"submission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = test_df['Id']\nsubmission['SalePrice'] = predictions\n\nsubmission.to_csv('submission.csv', index=False)","ffd2f0dd":"feat_imp_df = pd.DataFrame(columns=['Features', 'Value'])\nfeat_imp_df['Features'] = enc_df.columns.to_list()\nfeat_imp_df['Value'] = top_model.feature_importances_\n\nfeat_imp_df[feat_imp_df['Value']>0].sort_values(by=['Value'], ascending=False).head(10)","60ffd453":"cols = feat_imp_df[feat_imp_df['Value']>0].sort_values(by=['Value'], ascending=False).head(10)['Features'].to_list()","6d200b4f":"#  Scatterplot for numerical features\n\nplt.figure(figsize=(20,90))\nfor i in range(len(cols)):\n    plt.subplot(12, 3, i+1)\n    sns.scatterplot(x=enc_df[cols[i]], y=target)\n\nplt.show()","d4496ad7":"### Encoding categorical features","5dd076c4":"# MODEL TRAINING WITH STRONG FEATURES","ccdc53ea":"# PREDICTION ON TEST DATA\n\n*****************************\n\nWe already performed the same preprocessing steps on test dataset **( test_enc_df )** along with the training dataset. Hence we will just pass it through the top_model .","1168748c":"# LOADING DATA ","e19b1f2e":"# STORY TELLING FROM THE ABOVE PLOTS","cb886c6b":"### Selecting best 20 numerical features","24f38049":"# RESULT ANALYSIS \n\n***************\n\n   - Let's analyse the results from the training data","647033ea":"### Dictionary of all the ordinal categorical features","4e2a1fe7":"### Drop Columns with more than 80% null values\n*******************\n\n* In this notebook version 2, I am not dropping the values with 80% null values.\n\n* I dropped them in the version 1. There was not much change in the scores obtained by the model. \n\n* Thats why I have commented out the below code snippets","24105678":"# FEATURE SELECTION\n*****************************\n\n* Selecting strong numerical features using Pearson\u2019s Correlation Coefficient\n* Selecting strong categorical using ANOVA ","44a48a9a":"### Drop Duplicates","d89cc7dd":"### Drop columns with single unique value","af597143":"### Top 10 features with their scores","37b92e14":"### Drop Columns with single unique values again after outlier analysis","4712d56b":"# FEATURE TRANSFORMATION\n\n***************************\n\n* Changing the distribution of numerical features to Gaussian (Normal)\n* We will apply power transform (Yeo-Johnson) on the features. ","fc0ab0f6":"### Select best 30 categorical features","f53c04d6":"# FEATURE ENGINEERING ANALYSIS\n********************\n\n* Feature engineering is an art and requires different combinations of techniques to attain better performance.\n\n* Following cells shows different analysis that I made from the different combinations of techniques.","54c7d8b8":"# HYPERPARAMETER TUNING\n******************\n\n* Our best model is gradient boosting classifier.\n\n* We will tune its parameter that is `learning_rate` , `max_depth` and `n_estimators`.\n\n* Well the default values of the parameters on which we got the results are following:-\n\n    * learning_rate=0.1       \n    * n_estimators=100        \n    * max_depth=3.\n    * min_samples_split=2.\n    * min_samples_leaf=1.\n    * subsample=1.0\n    \n    \n* Well tuning the min_samples_split, min_samples_leaf and subsample results to nothing. Hence we won't tune these parameters","dccc23a6":"* Houses with a good overall condition, with a greater living room which is above ground along with a greater basement area are expensive houses. \n\n* People are preferring those houses that has furnished basement with a greater area with a garage having capacity of storing more cars.\n\n* It seems that most of the people prefer to buy two story houses, with greater floor area. \n\n* Big houses or a mansions with a greater area are even more expensive.\n\n* It seems that people are preferring the houses that has been recently remodeled or reconstructed.\n\n***************\n## MORAL OF THE STORY\n\n* Buy a **big 2-floored** house with a **good overall condition**, with a **greater furnished basement area** along with a **wide garage** and which has been remodelled recently.","b5b6c6e0":"## VERSION 6 NOTEBOOK FEATURE ENGINEERING ANALYSIS\n\n************************\nI have performed various combinations of feature engineering and evaluated their results. The following results are on the basis of **IQR outlier removal and BoxCox feature transformation**\n\n\n\n* **NO FEATURE TRANSFORMATION & NO OUTLIER REMOVAL**\n\n\n      ('GradientBoosting', 0.8608838908601799),\n\n      ('ExtraTree', 0.8513076801447276),\n\n      ('RandomForest', 0.8483765546873913)\n          \n          \n* **WITH FEATURE TRANSFORMATION & NO OUTLIER REMOVAL**\n\n\n      ('ExtraTree', 0.8516058854676427),\n\n      ('GradientBoosting', 0.8491665618535139),\n\n      ('RandomForest', 0.8462603323781929)\n         \n     \n* **NO FEATURE TRANSFORMATION & WITH OUTLIER REMOVAL**\n\n\n      ('ExtraTree', 0.8709907177784123),\n\n      ('GradientBoosting', 0.8695859492594143),\n\n      ('RandomForest', 0.8579828682121151)\n         \n         \n* **WITH FEATURE TRANSFORMATION & WITH OUTLIER REMOVAL**\n\n\n      ('GradientBoosting', 0.8735160942186653),\n\n      ('ExtraTree', 0.8655515278510605),\n\n      ('RandomForest', 0.8585987968429459)\n     \n *************************\n \n* Ensemble Trees have given the best results followed by the linear models with regularization (ridge, lasso etc.).\n\n* Simple linear models have performed very poor. \n\n* On the basis of the r2 scores of different models, we can see we got **better results when we removed the outliers** although feature transformation didn't put much impact. This is because the trees doesn't take normality (gaussian distribution) into consideration. So even if you don't scale or normalize your data and directly put it into the emsemble trees, it would behave the same. Thats why **feature transformation** had a very minor impact.\n\n* Also in the initial version of this notebook, I didn't perform the binning on year related features. In this notebook I performed it and kind of analysed the results and I found that it also had no impact on the results. The results were same. This could mean that year related columns were not impacting the prices (target variable) much.\n\n*****************************","3546d1db":"## LATEST VERSION NOTEBOOK FEATURE ENGINEERING ANALYSIS ","addcd156":"* From the above dataframe, we can see the top 10 features which are impacting the house prices.\n\n    * **OverallQual**  - Rates the overall material and finish of the house. Discrete feature (0 to 10)\n    \n    * **GrLivArea** - Above grade (ground) living area square feet\n    \n    * **TotalBsmtSF** - Total square feet of basement area\n    \n    * **GarageCars** - Size of garage in car capacity\n    \n    * **BsmtFinSF1** - Basement finished square feet area\n    \n    * **2ndFlrSF** - Second floor square feet\n    \n    * **1stFlrSF** - First floor square feet\n    \n    * **Remod_Sold_Age** - This is the feature we created. Difference between YrSold and YearRemodAdd\n    \n         - YrSold - Year in which the house is sold\n    \n         - YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n         \n    * **LotArea** - Lot size in square feet. A lot area is the total area of a property.\n    \n    * **GarageArea** - Size of the garage","e0ba5542":"# FEATURE EXTRACTION\n***********\n\n* Using PCA to perform dimensionality reduction.\n* Don't forget to scale your data before doing PCA.","3af5441c":"# VISUALIZE\n\n*************************\n\n- Scatterplot and distribution of numerical features\n- BarCharts of categorical features\n- Box plots to check the outliers","2aec27ab":"# MODEL TRAINING & EVALUATION","00dc1a0d":"* We will make categories \n    - 1800-1900\n    - 1900-1910\n    - 1910-1920\n    \n    ...\n    - 2000-2010\n    \n* Column 'YrSold' has all the values between 2006 to 2010 so all of its will fall into a single category 2000-2010. \n\n* Hence we will delete YrSold in further steps coz it won't be giving us any information (because of no variance)","85e624e1":"### Training the model with best parameters","e7f93662":"### Impute NaN Values","afcd3a39":"***************\n### UPDATED\n***************\n* In the previous version (6th) , I have used the best parameters to train the model, but it seemed to me that the tree kind of overfitted and didn't generalise well on the test set because my score which was `0.23` before became `0.26` on the scoreboard.\n\n* In the 11th version, I sticked to the default parameters of the Gradient Boosting but applied yeo-johnson feature transformation and my score improved from `0.23` to `0.16`.\n\n* Hence I am commenting out the following code.","be4ddc50":"### Create New Features\n********************\n\n* There are columns with the year. Years are not informative, so we need to transform it.\n\n* We will perform **binning** on those year column to divide it into decades and will make it categorical. \n\n* We will also calculate the age of house on the basis of different columns.","48956e6d":"### Visualising these columns with respect to the price","f8b1d392":"# ENCODING\n****************\n\n* Some of the categorical features are nominal and some are ordinal. We need to encode them separately.\n\n* For ordinal data, use label encoding and for nominal data, use dummy encoding.\n\n* In dummy encoding, we create separate columns for each category in a feature. Hence we need to make sure that two different features does not contain the same categories. Otherwise, there will be multiple columns with the same name.\n\n* Features like `Condition1 and Condition2` , `Exterior1st and Exterior2nd` are nominal features and contains the same categories.\n\n* In that case, while performing dummy encoding we will change the column name by putting a prefix of the original column.","78a4fff6":"# PREPROCESSING\n*************************\n\n* Drop Duplicates\n* Drop Columns with more than 80% null values\n* Drop uninformative columns\n* Drop Columns with single unique values\n* Inpute Null Values\n* Create New Features\n* Outlier Analysis and Removal\n* Drop Columns with single unique values again after outlier analysis","78e79606":"# STEPS FOR REGRESSION PREDICTIVE ANALYSIS\n\n**********************************\n\n* **LOADING DATA**\n\n    - Loading both train and test dataset and applying preprocessing steps together\n    \n********************\n\n* **SUMMARY OF DATA**\n\n    - Total Samples\n    - Total Features\n    - Total Categorical Features\n    - Total Numerical Features\n    - Stats of Numerical Features\n    - Value Count of Categorical Features\n    - Unique Values DataFrame\n    - Null Values DataFrame\n    \n***********************\n\n*  **PREPROCESSING**\n\n    * Drop Duplicates\n    * Drop Columns with more than 80% null values\n    * Drop uninformative columns\n    * Drop Columns with single unique values\n    * Inpute Null Values\n    * Create New Features\n    * Outlier Analysis and Removal\n    * Drop Columns with single unique values again after outlier analysis\n    \n*************************\n\n* **VISUALIZE**\n\n    - Scatterplot of numerical features\n    - Distribution of numerical features\n    - BarCharts of categorical features\n    - Box plots to check the outliers\n    \n***********************\n\n* **FEATURE TRANSFORMATION**\n\n    - Changing the distribution of numerical features to Gaussian (Normal)\n    \n****************\n\n*  **ENCODING**\n\n    - Some of the categorical features are nominal and some are ordinal. We need to encode them separately.\n    - For **ordinal** features, we will do **label encoding**\n    - For **nominal** features, we will do **dummy encoding**\n    \n*************\n\n* **MODEL TRAINING & EVALUATION**\n\n    - Perform Scaling\n    \n        - MinMax Scaling\n        - Variance Scaling (Standard Scaler)\n        \n    - Fitting Different Regression Models\n    \n        - Linear Regression\n        - Polynomial Regression (with interaction features)\n        - Ridge Regression\n        - Lasso Regression\n        - SGD Regression\n        - Elastic Regression\n        - Bayesian Ridge\n        - Huber Regression (robust to outliers)\n        - RANSAC Regression (robust to outliers)\n        - XGB Regressor\n        - Ensemble Regressor \n            - Random Forest\n            - Gradient Boosting\n            - AdaBoosting\n            - Bagging Regressor\n            - ExtraTreesRegressor\n         \n       \n*******************\n\n* **FEATURE SELECTION**\n\n    * Selecting strong numerical features using Pearson\u2019s Correlation Coefficient\n    * Selecting strong categorical using ANOVA \n\n*****************************\n\n* **FEATURE EXTRACTION**\n\n    * Using PCA to perform dimensionality reduction.\n    * Don't forget to scale your data before doing PCA.\n    \n******************\n\n* **MODEL TRAINING & EVALUATION WITH STRONG FEATURES ONLY**\n\n    - Using the same models as stated above.\n    \n*******************\n\n* **CONCLUSION**\n\n    - Which model performed the best one with using all the features or the one with the strong features only ?\n    \n*************\n\n* **HYPERPARAMETER TUNING**\n\n    - Tuning the parameters of the best model.\n    \n******************************\n\n* **PREDICTION**\n\n    - Prediction on the test dataset using the top scorer model\n    - Saving the results in submission.csv\n    \n************************\n\n* **FEATURE ENGINEERING ANALYSIS**\n\n    - Comparison of the scores of the different feature engineering steps.\n**************************\n\n* **RESULT ANALYSIS**\n\n    - Analysis of the results given by the model.\n    \n****************\n\n* **STORY TELLING FROM THE RESULT ANALYSIS**\n\n    - Simple interpretation of the results in layman language.\n    \n*****************\n****************","92061275":"* Intially I used **BOXCOX** transformation that requires all the values to be greater than 0, so I shifted the value to 0.02 in the columns and then applied **BOXCOX** transformation to it. It gave me a score of `0.23` in the scoreboard *(lesser the score, the better the model behaved on test set)*.\n\n* But when I applied **Yeo-Johnson Transformation** which can work on both negative and positive values, my score became `0.13` in the scoreboard which is great. That means **TRANSFORMATION** did play a great role in better generalization (performance on unknown dataset). \n\n* Hence we can say that a **combination of outlier removal (IQR), performing binning on year columns and yeo-johnson feature transformation** has led to better result that the earlier combination of *IQR outlier removal, binning and boxcox feature transformation* .\n\n* Let's see which features really helped in making decisions.","dae3f7d3":"# CONCLUSION\n*****************\n\n* We performed both **Feature Selection** and **Feature Extraction** and we can see, when we used all the features we got better results as compared to using only strong features.\n\n* Feature Selection was better than the Feature Extraction (PCA). Hence feature selection through f-test ANOVA and pearson correlation test is best here to select strong features than the dimensionality reduction method using PCA.\n\n* Although the difference between the results of both (feature selection and with all the features) isn't much, so we can use only the strong features also. Here I am taking all the features.\n\n* Hence we will use all the features and the top model with the best score to predict house prices   ","5a8fdbd0":"### Drop uninformative columns","74c0ddd6":"# SUMMARY OF DATA\n*******************\n- Total Samples\n- Total Features\n- Total Categorical Features\n- Total Numerical Features\n- Stats of Numerical Features\n- Value Count of Categorical Features\n- Unique Values DataFrame\n- Null Values DataFrame","40f3c30c":"### Outlier Analysis and Removal\n\n*******************************\n\n* Using IQR "}}