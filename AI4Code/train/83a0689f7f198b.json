{"cell_type":{"24062a5c":"code","b17dc7a1":"code","659e1405":"code","ce095e27":"code","3e4ae07a":"code","2c66ac9e":"code","645cef97":"code","872d8db6":"code","3b00b0e5":"code","7e226f88":"code","e1bb2d7c":"code","acb91513":"code","7027d434":"code","49e4e467":"code","b81eef83":"code","7e7a2036":"code","e09a665d":"code","cba4133a":"code","aae12c6c":"code","07dac2a8":"code","7331ce95":"code","79c73d1f":"code","53324a22":"code","79b71db6":"code","ec6725cb":"code","5b992e81":"code","79007584":"markdown","0aa02807":"markdown","549fc6c7":"markdown","006440ad":"markdown","b83f635b":"markdown","1ea4933f":"markdown","1de0f134":"markdown","1aa04f4d":"markdown","59b24d75":"markdown","cc01d4cb":"markdown","7fb8169d":"markdown","539b7cad":"markdown","f585d577":"markdown","62dbd546":"markdown","a2e912c6":"markdown","918d5356":"markdown","53c8fde8":"markdown","de9d96a2":"markdown","abe1344d":"markdown","b23e6f0a":"markdown","d6ec3bb8":"markdown","8f25c48c":"markdown","9b900cb7":"markdown","549023e5":"markdown","b6458192":"markdown","280bfaf2":"markdown"},"source":{"24062a5c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\/ieee-fraud-detection\"))","b17dc7a1":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\n## Function to reduce the DF size\n# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","659e1405":"df_trans = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ndf_test_trans = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\n\ndf_id = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ndf_test_id = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\n\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id\n","ce095e27":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","3e4ae07a":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3','C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9',  'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_30', \n                   'id_31', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']+['V%d'%i for i in range(1,338,1)]","2c66ac9e":"cols_to_drop = [col for col in df_train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionDT')\n\ntrain = df_train.drop(cols_to_drop, axis=1)\ntest = df_test.drop(cols_to_drop, axis=1)","645cef97":"# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = preprocessing.LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n\n# Encoding - count encoding separately for train and test\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","872d8db6":"test['isFraud'] = 'test'\ndf = pd.concat([train, test], axis=0, sort=False)\ndf = df.reset_index()\ndf = df.drop('index', axis=1)\ndel train,test","3b00b0e5":"columns_a = ['TransactionAmt', 'id_02', 'D15']\ncolumns_b = ['card1', 'card4', 'addr1']\n\nfor col_a in columns_a:\n    for col_b in columns_b:\n        df[f'{col_a}_to_mean_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('mean')\n        df[f'{col_a}_to_std_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('std')\n\n# New feature - decimal part of the transaction amount.\ndf['TransactionAmt_decimal'] = ((df['TransactionAmt'] - df['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - log of transaction amount.\ndf['TransactionAmt'] = np.log(df['TransactionAmt'])\ndf['TransactionAmt'] = np.log(df['TransactionAmt'])\n\n# New feature - day of week in which a transaction happened.\nimport datetime\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE,'%Y-%m-%d')\ndf['Date'] = df['TransactionDT'].apply(\\\n    lambda x:(startdate+datetime.timedelta(seconds=x)))\ndf['Transaction_day_of_week'] = df['Date'].dt.dayofweek\ndf['Transaction_hour'] = df['Date'].dt.hour\ndf['Transaction_days'] = df['Date'].dt.day\ndel df['Date']\n\nfor col in ['Transaction_day_of_week','Transaction_hour','Transaction_days']:\n    from sklearn.preprocessing import minmax_scale\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n    \n# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n    df[feature + '_count_full'] = df[feature].map(df[feature].value_counts(dropna=False))","7e226f88":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","e1bb2d7c":"# Label Encoding\nfor f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        print(f)\n        try:\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n            df_train[f] = lbl.transform(list(df_train[f].values))\n            df_test[f] = lbl.transform(list(df_test[f].values))   \n\n        except:\n            print('drop %s'%f)\n            df_train = df_train.drop(f, axis=1)\n            df_test = df_test.drop(f, axis=1)","acb91513":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","7027d434":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","49e4e467":"mas_v = df.columns[df.columns.str.startswith('V')]\nmas_v","b81eef83":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)","7e7a2036":"df = reduce_mem_usage(df)","e09a665d":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","cba4133a":"df_train.shape","aae12c6c":"X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n                                                      'TransactionDT', \n                                                      #'Card_ID'\n                                                     ],\n                                                     axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ], \n                                                   axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","07dac2a8":"column_names = X_train.columns\n'TransactionID' in column_names","7331ce95":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sample_submission.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):#0829\u53bb\u9664ID\u7684\u5206\u7ec4[column_names[1:]]\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True, \n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]#X_train.iloc[tr_idx, 1:], X_train.iloc[val_idx, 1:]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean \/ FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}\n","79c73d1f":"# Set algoritm parameters\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=27)\n\n# Print best parameters\nbest_params = space_eval(space, best)","53324a22":"print(\"BEST PARAMS: \", best_params)\n\nbest_params['max_depth'] = int(best_params['max_depth'])","79b71db6":"clf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params,\n    tree_method='gpu_hist'\n)\n\nclf.fit(X_train[column_names[1:]], y_train)\n\ny_preds = clf.predict_proba(X_test[column_names[1:]])[:,1] ","ec6725cb":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","5b992e81":"\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('submission.csv')","79007584":"# Trainning and Predicting with best Parameters","0aa02807":"# Encoding categorical features","549fc6c7":"# Running the optimizer","006440ad":"I will set all functions in the cell bellow.","b83f635b":"# \u4e00\u4e9b\u7279\u5f81\u5b57\u6bb5\u7684\u4f18\u5316","1ea4933f":"## Importing necessary libraries","1de0f134":"# Seting X and y","1aa04f4d":"# Defining the HyperOpt function with parameters space and model","59b24d75":"df_train = id_split(df_train)\ndf_test = id_split(df_test)","cc01d4cb":"## Seting y_pred to csv","7fb8169d":"## I'm working in this kernel yet.\n# <font color=\"red\">Please if this kernel were useful for you, please <b>UPVOTE<\/b> =)<\/font>","539b7cad":"# Concating dfs to get PCA of V features","f585d577":"# reducing memory usage","62dbd546":"# Best parameters","a2e912c6":"# Importing train datasets","918d5356":"# Getting PCA ","53c8fde8":"from sklearn.impute import SimpleImputer\n\nmissing_cols = []\nfor i,j in enumerate(X_train.dtypes):\n    if (X_train[X_train.columns[i]].isna().sum()>0) or (X_test[X_test.columns[i]].isna().sum()>0):\n        missing_cols.append(X_train.columns[i])\n\n        if (np.isinf(X_test[X_test.columns[i]]).sum()>0) or (np.isinf(X_train[X_train.columns[i]]).sum()>0):\n            X_train.loc[np.isinf(X_train[X_train.columns[i]]),X_train.columns[i]] = np.nan\n            X_test.loc[np.isinf(X_test[X_test.columns[i]]),X_test.columns[i]] = np.nan\n        print(j,X_train.columns[i],X_train[X_train.columns[i]].isna().sum()\/len(X_train),np.isinf(X_train[X_train.columns[i]]).sum())\n        print(X_test.columns[i],X_test[X_test.columns[i]].isna().sum()\/len(X_test),np.isinf(X_test[X_test.columns[i]]).sum())\n","de9d96a2":"def id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe","abe1344d":"sample_submission.describe()","b23e6f0a":"# **Logistic Regession Model**[](http:\/\/)\n\u60b2\u5267\u7684\u662f\u9700\u8981\u5904\u7406\u7a7a\u503c","d6ec3bb8":"# Some feature engineering","8f25c48c":"## Predicting X test","9b900cb7":"import os\nos.listdir()","549023e5":"# Top 20 Feature importance","b6458192":"X_train[X_train.columns[i]]","280bfaf2":"# Seting train and test back"}}