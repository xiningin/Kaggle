{"cell_type":{"2291bf29":"code","d60a1803":"code","d32955f2":"code","a00916a6":"code","b4096ccf":"code","405eba83":"code","da8b6d48":"code","78d6c10e":"code","c07bcfe2":"code","c1b2df55":"code","b8211177":"code","64991010":"code","3249489e":"markdown","13e9f8f5":"markdown","dc7cc314":"markdown","e29ffd38":"markdown","0a2c126d":"markdown"},"source":{"2291bf29":"# example of loading the mnist dataset\nfrom keras.datasets import mnist\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\n# load dataset\n(trainX, trainy), (testX, testy) = mnist.load_data()","d60a1803":"# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))","d32955f2":"for i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(trainX[i])\nplt.show()","a00916a6":"# reshape dataset to have a single channel\ntrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\ntestX = testX.reshape((testX.shape[0], 28, 28, 1))","b4096ccf":"from tensorflow.keras.utils import to_categorical\n# one hot encode target values\ntrainY = to_categorical(trainy)\ntestY = to_categorical(testy)","405eba83":"def prep_pixels(train, test):\n    # convert from integers to floats\n    train_norm = train.astype('float32')\n    test_norm = test.astype('float32')\n    # normalize to range 0-1\n    train_norm = train_norm \/ 255.0\n    test_norm = test_norm \/ 255.0\n    # return normalized images\n    return train_norm, test_norm\n\n","da8b6d48":"prep_pixels(trainX, testX)","78d6c10e":"#define cnn model\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","c07bcfe2":"def evaluate_model(dataX, dataY, n_folds=5):\n    scores, histories = list(), list()\n    # prepare cross validation\n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n    # enumerate splits\n    for train_ix, test_ix in kfold.split(dataX):\n        # define model\n        model = define_model()\n        # select rows for train and test\n        trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n        # fit model\n        history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n        # evaluate model\n        _, acc = model.evaluate(testX, testY, verbose=0)\n        print('> %.3f' % (acc * 100.0))\n        # stores scores\n        scores.append(acc)\n        histories.append(history)\n    return scores, histories","c1b2df55":"from sklearn.model_selection import KFold\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.models import Sequential\nevaluate_model(trainX, trainY, n_folds=5)","b8211177":"def summarize_diagnostics(histories):\n    for i in range(len(histories)):\n        # plot loss\n        pyplot.subplot(2, 1, 1)\n        pyplot.title('Cross Entropy Loss')\n        pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n        pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n        # plot accuracy\n        pyplot.subplot(2, 1, 2)\n        pyplot.title('Classification Accuracy')\n        pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n        pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n        pyplot.show()","64991010":"\n# summarize model performance\ndef summarize_performance(scores):\n    # print summary\n    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n    # box and whisker plots of results\n    pyplot.boxplot(scores)\n    pyplot.show()","3249489e":"# Present Results\nOnce the model has been evaluated, we can present the results.\n\nThere are two key aspects to present: the diagnostics of the learning behavior of the model during training and the estimation of the model performance. These can be implemented using separate functions.\n\nFirst, the diagnostics involve creating a line plot showing model performance on the train and test set during each fold of the k-fold cross-validation. These plots are valuable for getting an idea of whether a model is overfitting, underfitting, or has a good fit for the dataset.\n\nWe will create a single figure with two subplots, one for loss and one for accuracy. Blue lines will indicate model performance on the training dataset and orange lines will indicate performance on the hold out test dataset. The summarize_diagnostics() function below creates and shows this plot given the collected training histories.","13e9f8f5":"We also know that there are 10 classes and that classes are represented as unique integers.\n\nWe can, therefore, use a one hot encoding for the class element of each sample, transforming the integer into a 10 element binary vector with a 1 for the index of the class value, and 0 values for all other classes. We can achieve this with the to_categorical() utility function.","dc7cc314":"Next, the classification accuracy scores collected during each fold can be summarized by calculating the mean and standard deviation. This provides an estimate of the average expected performance of the model trained on this dataset, with an estimate of the average variance in the mean. We will also summarize the distribution of scores by creating and showing a box and whisker plot.\n\nThe summarize_performance() function below implements this for a given list of scores collected during model evaluation.","e29ffd38":"# Define Model\nNext, we need to define a baseline convolutional neural network model for the problem.\n\nThe model has two main aspects: the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make a prediction.\n\nFor the convolutional front-end, we can start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.\n\nGiven that the problem is a multi-class classification task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.\n\nAll layers will use the ReLU activation function and the He weight initialization scheme, both best practices.\n\nWe will use a conservative configuration for the stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum of 0.9. The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we have the same number of examples in each of the 10 classes.","0a2c126d":"# Prepare Pixel Data\nWe know that the pixel values for each image in the dataset are unsigned integers in the range between black and white, or 0 and 255.\n\nWe do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n\nA good starting point is to normalize the pixel values of grayscale images, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value."}}