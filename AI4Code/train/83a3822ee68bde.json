{"cell_type":{"40f1dc11":"code","dcfd8b1f":"code","ccaf33ad":"code","4ea12d1d":"code","aeeaa189":"code","81d83b41":"code","f76c4c31":"code","ad74ddc9":"code","9aab147b":"code","48e0a577":"code","767c330d":"code","19131f6b":"code","01374a6b":"code","3cd4d37b":"code","26a5702a":"code","48e03ccb":"code","93f0b3dd":"code","9920531f":"code","23422fb8":"code","05839d00":"markdown","f26b1980":"markdown","625d428c":"markdown","9e146d80":"markdown","b24124a3":"markdown","55be739d":"markdown","aa7a7406":"markdown","46d48d66":"markdown","f5a0a8f5":"markdown","8f662479":"markdown","30a18a1d":"markdown"},"source":{"40f1dc11":"import datatable as dt\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","dcfd8b1f":"N_SPLITS = 5\nN_ESTIMATORS = 20000\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE = 1000\nSEED = 42","ccaf33ad":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything(SEED)","4ea12d1d":"train = dt.fread('..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread('..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\ntrain = train[train.columns[1:]]\ntest = test[test.columns[1:]]\n\nTARGET = 'target'\ntrain[TARGET] = train[TARGET].astype('uint8')","aeeaa189":"features = [col for col in train.columns if 'f' in col]\n\ncont_features =[]\ndisc_features =[]\n\nfor col in features:\n    if train[col].dtype=='float64':\n        cont_features.append(col)\n    else:\n        disc_features.append(col)","81d83b41":"train[cont_features] = train[cont_features].astype('float32')\ntrain[disc_features] = train[disc_features].astype('uint8')\n\ntest[cont_features] = test[cont_features].astype('float32')\ntest[disc_features] = test[disc_features].astype('uint8')","f76c4c31":"target_train = train['target']\ntrain = train.drop('target', axis=1) ","ad74ddc9":"cols = disc_features.copy()\ncols.remove('f22')\ncols.remove('f43')\ntrain['disc_sum'] = train[cols].sum(axis=1)\ntest['disc_sum'] = test[cols].sum(axis=1)\n\ndisc_features += ['disc_sum']","9aab147b":"cols_ovr = [f'{col}_ovr' for col in cont_features]\ntrain[cols_ovr] = (train[cont_features] > train[cont_features].mean()).astype('uint8')\ntest[cols_ovr] = (test[cont_features] > test[cont_features].mean()).astype('uint8')\n\ndisc_features += cols_ovr","48e0a577":"features = disc_features + cont_features","767c330d":"display(train.info())\ndisplay(train[features].head())","19131f6b":"display(test.info())\ndisplay(test[features].head())","01374a6b":"'''from sklearn.decomposition import PCA\npca = PCA(n_components=400, random_state=1)\npca.fit(train)\nx_pca = pca.transform(train)\ntest_pca = pca.transform(test)\nx_pca = pd.DataFrame(x_pca)\nx_pca.head()'''","3cd4d37b":"'''from sklearn.linear_model import LogisticRegression\nn_splits = 2\nkf = StratifiedKFold(n_splits=n_splits,random_state=1,shuffle=True)\n\nlr = LogisticRegression(n_jobs = -1, random_state = 42, C = 5, max_iter = 2000)\ny_test_pred = np.zeros(test_pca.shape[0])\nprint(y_test_pred.shape)\nfor i, (train_idx, test_idx) in enumerate(kf.split(x_pca, target_train)):\n    \n    x_train = x_pca.iloc[train_idx]\n    x_val = x_pca.iloc[test_idx]\n    \n    y_train = target_train.iloc[train_idx]\n    y_val = target_train.iloc[test_idx]\n    lr.fit(x_train, y_train)\n    \n    y_pred = lr.predict_proba(x_val)[:, 1]\n    y_test_pred += lr.predict_proba(test_pca)[:, 1]\/n_splits\n\n    auc = roc_auc_score(y_val, y_pred)\n    print('Fold', i, 'AUC :', auc)'''","26a5702a":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 8e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'categorical_feature': len(disc_features),\n}","48e03ccb":"'''lgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nx_pca = train\ntest_pca =  test\n\nn_splits = 2\n\nkf = StratifiedKFold(n_splits=n_splits,random_state=1,shuffle=True)\n\nfor i, (train_idx, test_idx) in enumerate(kf.split(x_pca, target_train)):\n    print(f\"===== fold {i} =====\")\n    x_train = x_pca.iloc[train_idx]\n    x_val = x_pca.iloc[test_idx]\n    \n    y_train = target_train.iloc[train_idx]\n    y_val = target_train.iloc[test_idx]\n    \n    start = time.time()\n    model = lgb.LGBMClassifier(**lgb_params)\n    model.fit(\n        x_train, \n        y_train,\n        eval_set=[(x_val, y_val)],\n        eval_metric='auc',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n    )\n    \n    fi_tmp = pd.DataFrame()\n    fi_tmp['feature'] = model.feature_name_\n    fi_tmp['importance'] = model.feature_importances_\n    fi_tmp['fold'] = i\n    fi_tmp['seed'] = SEED\n    lgb_importances = lgb_importances.append(fi_tmp)\n\n    lgb_oof[test_idx] = model.predict_proba(x_val)[:, -1]\n    lgb_pred += model.predict_proba(test_pca)[:, -1]\/n_splits\n\n    elapsed = time.time() - start\n    auc = roc_auc_score(y_val, lgb_oof[test_idx])\n    print(f\"fold {i} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof lgb roc = {roc_auc_score(train[TARGET], lgb_oof)}\")\n\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)'''","93f0b3dd":"\n\nx_train, x_val, y_train, y_val = train_test_split(\n    train, target_train, test_size=0.2, random_state=42)\n\nlgb_oof = np.zeros(x_val.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nstart = time.time()\nmodel = lgb.LGBMClassifier(**lgb_params)\nmodel.fit(\n    x_train, \n    y_train,\n    eval_set=[(x_val, y_val)],\n    eval_metric='auc',\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n    verbose=VERBOSE,\n)\n\nfi_tmp = pd.DataFrame()\nfi_tmp['feature'] = model.feature_name_\nfi_tmp['importance'] = model.feature_importances_\nfi_tmp['seed'] = SEED\nlgb_importances = lgb_importances.append(fi_tmp)\n\nlgb_oof = model.predict_proba(x_val)[:, -1]\nlgb_pred += model.predict_proba(test)[:, -1]\n\nelapsed = time.time() - start\nauc = roc_auc_score(y_val, lgb_oof)\nprint(f\"lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof lgb roc = {roc_auc_score(y_val, lgb_oof)}\")\n\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)","9920531f":"submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsubmission[TARGET] = lgb_pred\nsubmission.to_csv(\"submission.csv\", index=False)","23422fb8":"'''order = list(lgb_importances.groupby('feature').mean().sort_values('importance', ascending=False).index)\n\nfig = plt.figure(figsize=(16, 32), tight_layout=True)\nsns.barplot(x=\"importance\", y=\"feature\", data=lgb_importances.groupby('feature').mean().reset_index(), order=order)\nplt.title(\"LightGBM feature importances\")'''","05839d00":"## Feature importance","f26b1980":"# Libraries\n---","625d428c":"# LightGBM\n---","9e146d80":"# Logistic Regression\n\nFor this section we use a simple Logistic regression paires with k-fold. This last technique is not useful really since we have enough data but it's nice to play with it. ","b24124a3":"# PCA\n\nHe utilize PCA in order to make the data more compact and not run out of memory. However, this made it lower it's accuracy so we commented it. ","55be739d":"# Datasets\n---","aa7a7406":"This is commented because we have enough data and don't really benefit too much from Kfold.","46d48d66":"## Cross validation","f5a0a8f5":"**Please if anyone sees this Notebook upvote it :) it's free**","8f662479":"# Parameters\n---","30a18a1d":"# Submission"}}