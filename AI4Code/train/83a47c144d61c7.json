{"cell_type":{"6a40de88":"code","5af6fc93":"code","808ad21e":"code","0d1c5b03":"code","73f985b9":"code","355737c4":"code","d0dea79b":"code","df7d3fa3":"code","3fb620e9":"code","3f2ec27f":"code","d6563804":"code","8db65841":"code","6069c976":"code","c1c2ced5":"code","20b8acd2":"code","e9a125ca":"code","88ed879b":"code","fb6f5a7c":"code","1fc0ca01":"code","e17ccab6":"code","89d36b29":"code","fdce3b59":"code","89fdb0dc":"code","cee5062d":"code","7da05eb9":"code","dbea14f0":"code","a028b47f":"code","b3c2bef9":"code","e1d5054b":"code","210184b2":"code","70e1ad2d":"code","2b2a9f7e":"code","f17fc09b":"code","89225eb4":"code","f8eb657a":"code","78342c3d":"code","6e60e2ab":"code","812b81f0":"code","4cf90a36":"code","155a367b":"markdown","7acb7343":"markdown","a7d3d62b":"markdown","ead7b3bb":"markdown","523eada5":"markdown","4ae67a08":"markdown","302bc837":"markdown","06f1d428":"markdown","282d8409":"markdown","31538517":"markdown","4e76e98f":"markdown","801cf435":"markdown","03bb8354":"markdown","cec78067":"markdown","5d94e133":"markdown","cd09512e":"markdown","701a1f62":"markdown","d0e6b3f7":"markdown","aea42e8d":"markdown","4e26570b":"markdown","1a690a88":"markdown","fe339668":"markdown","4a0a1511":"markdown","4f0cea6a":"markdown","1a7adc67":"markdown","1cc798b8":"markdown","9ffe782f":"markdown","9e49c2f4":"markdown","bf13baf8":"markdown","ce97a4af":"markdown","2d593a6a":"markdown","275e16ca":"markdown","aa220c33":"markdown","19b74976":"markdown","73edbe6c":"markdown","569d6fd5":"markdown","96c8ee04":"markdown","3a7435cc":"markdown","678ff384":"markdown","948a6e64":"markdown","ae984011":"markdown","50b28fde":"markdown","be2b076f":"markdown","93d39d06":"markdown"},"source":{"6a40de88":"import tensorflow as tf\nprint(tf.version.VERSION)","5af6fc93":"!git clone --depth 1 -b v2.4.0 https:\/\/github.com\/tensorflow\/models.git","808ad21e":"# install requirements to use tensorflow\/models repository\n!pip install -Uqr models\/official\/requirements.txt\n# you may have to restart the runtime afterwards, also ignore any ERRORS popping up at this step","0d1c5b03":"!pip install --upgrade -q wandb","73f985b9":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb-api\")\n","355737c4":"import wandb\nfrom wandb.keras import WandbCallback\nwandb.login(key=wandb_api)\n","d0dea79b":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.append('models')\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\nfrom official.nlp import optimization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","df7d3fa3":"print(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","3fb620e9":"# TO LOAD DATA FROM ARCHIVE LINK\n# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n\n# df = pd.read_csv('https:\/\/archive.org\/download\/quora_dataset_train.csv\/quora_dataset_train.csv.zip', \n#                  compression='zip',\n#                  low_memory=False)\n# print(df.shape)","3f2ec27f":"# TO LOAD DATA FROM KAGGLE\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\nprint(df.shape)","d6563804":"df.head(10)\n# label 0 == non toxic\n# label 1 == toxic ","8db65841":"print(df['target'].value_counts())\ndf['target'].value_counts().plot.bar()\nplt.yscale('log');\nplt.title('Distribution of Labels')","6069c976":"print('Average word length of questions in dataset is {0:.0f}.'.format(np.mean(df['question_text'].apply(lambda x: len(x.split())))))\nprint('Max word length of questions in dataset is {0:.0f}.'.format(np.max(df['question_text'].apply(lambda x: len(x.split())))))\nprint('Average character length of questions in dataset is {0:.0f}.'.format(np.mean(df['question_text'].apply(lambda x: len(x)))))","c1c2ced5":"# Since the dataset is very imbalanced we will keep the same distribution in both train and test set by stratifying it based on the labels\n# using small portions of the data as the over all dataset would take ages to train, feel free to include more data by changing train_size \ntrain_df, remaining = train_test_split(df, random_state=42, train_size=0.1, stratify=df.target.values)\nvalid_df, _ = train_test_split(remaining, random_state=42, train_size=0.01, stratify=remaining.target.values)\nprint(train_df.shape)\nprint(valid_df.shape)","20b8acd2":"print(\"FOR TRAIN SET\\n\")\nprint('Average word length of questions in train set is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('Max word length of questions in train set is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('Average character length of questions in train set is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\nprint('Label Distribution in train set is \\n{}.'.format(train_df['target'].value_counts()))\nprint(\"\\n\\nFOR VALIDATION SET\\n\")\nprint('Average word length of questions in valid set is {0:.0f}.'.format(np.mean(valid_df['question_text'].apply(lambda x: len(x.split())))))\nprint('Max word length of questions in valid set is {0:.0f}.'.format(np.max(valid_df['question_text'].apply(lambda x: len(x.split())))))\nprint('Average character length of questions in valid set is {0:.0f}.'.format(np.mean(valid_df['question_text'].apply(lambda x: len(x)))))\nprint('Label Distribution in validation set is \\n{}.'.format(valid_df['target'].value_counts()))","e9a125ca":"# TRAIN SET \ntrain_df['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in words')","88ed879b":"# VALIDATION SET\nvalid_df['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in words')","fb6f5a7c":"# TRAIN SET\ntrain_df['question_text'].apply(lambda x: len(x)).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters')","1fc0ca01":"# VALIDATION SET\nvalid_df['question_text'].apply(lambda x: len(x)).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters')","e17ccab6":"# we want the dataset to be created and processed on the cpu \nwith tf.device('\/cpu:0'):\n    train_data = tf.data.Dataset.from_tensor_slices((train_df['question_text'].values, train_df['target'].values))\n    valid_data = tf.data.Dataset.from_tensor_slices((valid_df['question_text'].values, valid_df['target'].values))\n    # lets look at 3 samples from train set\n    for text,label in train_data.take(3):\n        print(text)\n        print(label)\n","89d36b29":"print(len(train_data))\nprint(len(valid_data))","fdce3b59":"# Setting some parameters\n\nconfig = {'label_list' : [0, 1], # Label categories\n          'max_seq_length' : 128, # maximum length of (token) input sequences\n          'train_batch_size' : 32,\n          'learning_rate': 2e-5,\n          'epochs':5,\n          'optimizer': 'adam',\n          'dropout': 0.5,\n          'train_samples': len(train_data),\n          'valid_samples': len(valid_data),\n          'train_split':0.1,\n          'valid_split': 0.01\n         }","89fdb0dc":"# Get BERT layer and tokenizer:\n# All details here: https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\n\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2',\n                            trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # checks if the bert layer we are using is uncased or not\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","cee5062d":"input_string = \"hello world, it is a wonderful day for learning\"\nprint(tokenizer.wordpiece_tokenizer.tokenize(input_string))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize(input_string)))","7da05eb9":"# This provides a function to convert row to input features and label, \n# this uses the classifier_data_lib which is a class defined in the tensorflow model garden we installed earlier\ndef create_feature(text, label, label_list=config['label_list'], max_seq_length=config['max_seq_length'], tokenizer=tokenizer):\n    \"\"\"\n    converts the datapoint into usable features for BERT using the classifier_data_lib\n\n    Parameters:\n    text: Input text string\n    label: label associated with the text\n    label_list: (list) all possible labels\n    max_seq_length: (int) maximum sequence length set for bert\n    tokenizer: the tokenizer object instantiated by the files in model assets\n\n    Returns:\n    feature.input_ids: The token ids for the input text string\n    feature.input_masks: The padding mask generated \n    feature.segment_ids: essentially here a vector of 0s since classification\n    feature.label_id: the corresponding label id from lable_list [0, 1] here\n\n    \"\"\"\n    # since we only have 1 sentence for classification purpose, textr_b is None\n    example = classifier_data_lib.InputExample(guid = None,\n                                            text_a = text.numpy(), \n                                            text_b = None, \n                                            label = label.numpy())\n    # since only 1 example, the index=0\n    feature = classifier_data_lib.convert_single_example(0, example, label_list,\n                                    max_seq_length, tokenizer)\n\n    return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)","dbea14f0":"def create_feature_map(text, label):\n    \"\"\"\n    A tensorflow function wrapper to apply the transformation on the dataset.\n    Parameters:\n    Text: the input text string.\n    label: the classification ground truth label associated with the input string\n\n    Returns:\n    A tuple of a dictionary and a corresponding label_id with it. The dictionary \n    contains the input_word_ids, input_mask, input_type_ids  \n    \"\"\"\n    input_ids, input_mask, segment_ids, label_id = tf.py_function(create_feature, inp=[text, label], \n                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])\n    max_seq_length = config['max_seq_length']\n\n    # py_func doesn't set the shape of the returned tensors.\n    input_ids.set_shape([max_seq_length])\n    input_mask.set_shape([max_seq_length])\n    segment_ids.set_shape([max_seq_length])\n    label_id.set_shape([])\n\n    x = {\n        'input_word_ids': input_ids,\n        'input_mask': input_mask,\n        'input_type_ids': segment_ids\n    }\n    return (x, label_id)\n\n    # the final datapoint passed to the model is of the format a dictionary as x and labels.\n    # the dictionary have keys which should obv match","a028b47f":"# Now we will simply apply the transformation to our train and test datasets\nwith tf.device('\/cpu:0'):\n  # train\n  train_data = (train_data.map(create_feature_map,\n                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n                          .shuffle(1000)\n                          .batch(32, drop_remainder=True)\n                          .prefetch(tf.data.experimental.AUTOTUNE))\n\n  # valid\n  valid_data = (valid_data.map(create_feature_map, \n                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                          .batch(32, drop_remainder=True)\n                          .prefetch(tf.data.experimental.AUTOTUNE)) ","b3c2bef9":"# train data spec, we can finally see the input datapoint is now converted to the BERT specific input tensor\ntrain_data.element_spec","e1d5054b":"# valid data spec\nvalid_data.element_spec","210184b2":"# Building the model, input ---> BERT Layer ---> Classification Head\ndef create_model():\n    \n    input_word_ids = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                       name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                   name=\"input_mask\")\n    input_type_ids = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                    name=\"input_type_ids\")\n\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    # for classification we only care about the pooled-output\n    # at this point we can play around with the classification head based on the downstream tasks and its complexity\n\n    drop = tf.keras.layers.Dropout(config['dropout'])(pooled_output)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(drop)\n\n    # inputs coming from the function\n    model = tf.keras.Model(\n      inputs={\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}, \n      outputs=output)\n\n    return model\n  ","70e1ad2d":"# Calling the create model function to get the keras based functional model\nmodel = create_model()","2b2a9f7e":"# using adam with a lr of 2*(10^-5), loss as binary cross entropy as only 2 classes and similarly binary accuracy\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy(),\n                       tf.keras.metrics.PrecisionAtRecall(0.5),\n                       tf.keras.metrics.Precision(),\n                       tf.keras.metrics.Recall()])\nmodel.summary()","f17fc09b":"tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76, )","89225eb4":"# Update CONFIG dict with the name of the model.\nconfig['model_name'] = 'BERT_EN_UNCASED'\nprint('Training configuration: ', config)\n\n# Initialize W&B run\nrun = wandb.init(project='Finetune-BERT-Text-Classification', \n                 config=config,\n                 group='BERT_EN_UNCASED', \n                 job_type='train')","f8eb657a":"# Train model\n# setting low epochs as It starts to overfit with this limited data, please feel free to change\nepochs = config['epochs']\nhistory = model.fit(train_data,\n                    validation_data=valid_data,\n                    epochs=epochs,\n                    verbose=1,\n                    callbacks = [WandbCallback()])\nrun.finish()","78342c3d":"# Initialize a new run for the evaluation-job\nrun = wandb.init(project='Finetune-BERT-Text-Classification', \n                 config=config,\n                 group='BERT_EN_UNCASED', \n                 job_type='evaluate')\n\n\n\n# Model Evaluation on validation set\nevaluation_results = model.evaluate(valid_data,return_dict=True)\n\n# Log scores using wandb.log()\nwandb.log(evaluation_results)\n\n# Finish the run\nrun.finish()","6e60e2ab":"\ndef create_graphs(history):\n    train_accuracy = history.history['binary_accuracy']\n    val_accuracy = history.history['val_binary_accuracy'],\n    train_losses = history.history['loss'],\n    val_losses = history.history['val_loss']\n    fig1 = plt.figure()\n    plt.plot(train_accuracy, 'r', linewidth=3.0, marker='o')\n    plt.plot(val_accuracy, 'b', linewidth=3.0, marker='o')\n    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n    plt.xlabel('Epochs ', fontsize=16)\n    plt.ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy Curves', fontsize=16)\n    plt.show()\n\n    fig2 = plt.figure()\n    plt.plot(train_losses, 'r', linewidth=3.0, marker='o')\n    plt.plot(val_losses, 'b', linewidth=3.0, marker='o')\n    plt.legend(['Training loss', 'Validation Loss'], fontsize=18)\n    plt.xlabel('Epochs ', fontsize=16)\n    plt.ylabel('Loss', fontsize=16)\n    plt.title('Loss Curves', fontsize=16)\n    plt.show()","812b81f0":"#create_graphs(history)","4cf90a36":"# Save model\nmodel.save(f\"{config['model_name']}.h5\")\n\n# Initialize a new W&B run for saving the model, changing the job_type\nrun = wandb.init(project='Finetune-BERT-Text-Classification', \n                 config=config,\n                 group='BERT_EN_UNCASED', \n                 job_type='save')\n\n\n# Save model as Model Artifact\nartifact = wandb.Artifact(name=f\"{config['model_name']}\", type='model')\nartifact.add_file(f\"{config['model_name']}.h5\")\nrun.log_artifact(artifact)\n\n# Finish W&B run\nrun.finish()","155a367b":"#### W&B Experiment Tracking\nIn order to start the expirment tracking, we will be creating 'runs' on W&B, \n\n`wandb.init()`: It initializes the run with basic project information\nparameters: \n- project: The project name, this will create a new project tab where all the experiments for this project will be tracked\n- config: A dictionary of all parameters and hyper-parameters we wish to track\n- group: optional, but would help us to group by different parameters later on\n- job_type: to describe the job type, it would help in grouping different experiments later. eg \"train\", \"evaluate\" etc","7acb7343":"The official tfhub page states that\n\"All parameters in the module are trainable, and fine-tuning all parameters is the recommended practice.\" Therefore we will go ahead and train teh entire model without freezing anything","a7d3d62b":"## Some Initial Imports and Checks","ead7b3bb":"We will be using the uncased BERT present in the tfhub. In order to prepare the text to be given to the BERT layer, we need to first tokenize our words. The tokenizer here is present as a model asset and will do uncasing for us as well.","523eada5":"There are two outputs from the BERT Layer: \n- A pooled_output of shape [batch_size, 768] with representations for the entire input sequences.  \n- A sequence_output of shape [batch_size, max_seq_length, 768] with representations for each input token (in context).\n\nFor the classification task, we are only concerned with the pooled_output.","4ae67a08":"You want to use [`Dataset.map`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#map) to apply this function to each element of the dataset. [`Dataset.map`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#map) runs in graph mode.\n\n- Graph tensors do not have a value.\n- In graph mode you can only use TensorFlow Ops and functions.\n\nSo you can't `.map` this function directly: You need to wrap it in a [`tf.py_function`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/py_function). The [`tf.py_function`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/py_function) will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function.","302bc837":"Since we just trained for a very limited data and less epoch these graphs generated here are not as \nrepresenetative, so leaving this here more interms of a place holder for the reader to experiment with.","06f1d428":"#### Logging into wandb.\nFirst things first, we need to create a free account on [W&B](https:\/\/wandb.ai\/site)\n\nThen let us access our [authorization API key](https:\/\/wandb.ai\/authorize) and add it to kaggle's secret key for hassle free authentication.\n\nMore details about Kaggle's Secret key feature on ---> https:\/\/www.kaggle.com\/product-feedback\/114053","282d8409":"Even the distribution of question length in words and characters is very similar. It looks like a good train test split so far.","31538517":"## Let Us Train !","4e76e98f":"One drawback of the tf hub is that we import the entire module as a layer in keras as a result of which we dont see the parameters and layers in the model summary.","801cf435":"We will be using GPU accelerated Kernel for this tutorial as we would require a GPU to fine-tune BERT.\n\n## Prerequisites:\n- Willingness to learn: Growth Mindset is all you need \n- Some basic idea about Tensorflow\/Keras \n- Some Python to follow along with the code ","03bb8354":"### Wrapping the Python Function into a TensorFlow op for Eager Execution","cec78067":"- The data we will use is the dataset provided on the [Quora Insincere Questions Classification competition on Kaggle](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/data). \n\n- Please feel free to download the train set from kaggle or use the link below to download the train.csv from that competition [https:\/\/archive.org\/download\/quora_dataset_train.csv\/quora_dataset_train.csv.zip](https:\/\/archive.org\/download\/quora_dataset_train.csv\/quora_dataset_train.csv.zip). \n\n- Let us decompress and read the data into a pandas DataFrame.","5d94e133":"### References:\nW&B usage and intro: https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases ","cd09512e":"# Taming the Data","701a1f62":"In order to Log all the different metrics we will use a simple callback provided by W&B\n\n`WandCallback()` : https:\/\/docs.wandb.ai\/guides\/integrations\/keras\n\nYes, Its as simple as adding a callback :D","d0e6b3f7":"## Initial Set Up","aea42e8d":"<h1> Fine-Tune BERT for Text Classification with TensorFlow<\/h1>","4e26570b":"## Lets Get That Data Ready: Tokenize and Preprocess Text for BERT","1a690a88":"Hopefully, this was useful for you, and by now, you have a small kickstart on training and utilizing BERT for a variety of downstream tasks like classification, Named Entity Recognition, Sentence filling, and many more.\n\nYou can check out and get the entire code as a notebook and run it on colab from this [Github Repo](https:\/\/github.com\/au1206\/Fine_Tuning_BERT).\n\nIf this was helpful, consider sharing it with more people so they can also learn about it.\n\nComing up Next:\n- BERT Annotated Paper\n- Write up on Transformers and its workings \n\nFor some annotated, reader-friendly research papers on advanced concepts and tutorials like these please visit https:\/\/au1206.github.io\/.\n\nIf you made it this far, please consider leaving feedback so I can improve and also if you liked it consider upvoting. \n\n(UPDATE): [BERT Annotated Paper and Beyond...](https:\/\/au1206.github.io\/annotated%20paper\/BERT\/)","fe339668":"<div align=\"center\">\n    <img width=\"512px\" src='https:\/\/drive.google.com\/uc?id=1mBqrfxng42SgSXvK62V1C67Or_vgrsVm' \/>\n    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model<\/p>\n<\/div>","4a0a1511":"### Install TensorFlow and TensorFlow Model Garden","4f0cea6a":"### Checking out some of the training samples and their tokenized ids","1a7adc67":"**Note**: After installing the required Python packages, you'll need to restart the Colab Runtime Engine \n(Run ---> Restart and clear all cell outputs...)","1cc798b8":"### Get to Know your data: Some Basic EDA","9ffe782f":"Setting all parameters in form of a dictionary so any changes if needed can be made here","9e49c2f4":"[Artifact dashboard](https:\/\/wandb.ai\/akshayuppal12\/Finetune-BERT-Text-Classification\/artifacts\/model\/BERT_EN_UNCASED\/48ffa3e14aba242a5113): Model versioning and more.","bf13baf8":"# Lets Model Our Way to Glory!!!","ce97a4af":"### Lets Look at some Graphs\n\nThese Graphs will mainly be useful when we are training for more epochs and more data.\nAll these graphs are actually directly logged on the [wandb dashboard](https:\/\/wandb.ai\/akshayuppal12\/Finetune-BERT-Text-Classification\/runs\/29thnm00?workspace=user-akshayuppal12) created for this run. To still give out a method to generate graphs explicitly here is some very basic code.","2d593a6a":"## Let the Data Flow: Creating the final input pipeline using `tf.data`","275e16ca":"### Quick Sneak Peek into the W&B Dashboard\n\n**Things to note:**\n- Grouping of experiments and runs.\n- Visualizations of all training logs and metrics.\n- Visualizations for system metrics, could be useful when training on cloud instances or physical GPU machines\n- Hyperparmeter tracking in the tabular form.\n- Artifacts: Model versioning and storage.\n\n<img src=\"https:\/\/i.imgur.com\/CD7iPK1.gif\">\n\n","aa220c33":"Each line of the dataset is composed of the review text and its label.\nData preprocessing consists of transforming text to BERT input features:\n`input_word_ids`, `input_mask`, `segment_ids\/input_type_ids`\n\n- **Input Word Ids:** Output of our tokenizer, converting each sentence into a set of token ids.\n\n- **Input Masks:** Since we are padding all the sequences to 128(max sequence length), it is important that we create some sort of mask to make sure those paddings do not interfere with the actual text tokens. Therefore we need a generate input mask blocking the paddings. The mask has 1 for real tokens and 0 for padding tokens. Only real\ntokens are attended to.\n\n- **Segment Ids:** For out task of text classification, since there is only one sequence, the segment_ids\/input_type_ids is essentially just a vector of 0s. \n\n\n\nBert was trained on two tasks:\n- fill in randomly masked words from a sentence.\n- given two sentences,  which sentence came first. \n","19b74976":"### Lets Evaluate\n\nLet us do an evaluation on the validation set and log the scores using weights and biases.\n\n**wandb.log()**: Log a dict of scalars (metrics like accuracy and loss) and any other type of wandb object.\nHere we will pass the evaluation dictionary as it is and log it.","73edbe6c":"The resulting `tf.data.Datasets` return `(features, labels)` pairs, as expected by [`keras.Model.fit`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#fit):","569d6fd5":"A Healthy practice for any ML practioner is to do a clean experiment tracking such that reasults are reproducable and trackable. For this kernel we will be looking into [Weights and Biases](https:\/\/wandb.ai\/site) for experiment tracking.\n\nHere are Four main things that W&B offers:\n- Experiment Tracking: Tracking ML Experiments and logging various parameters and metrics on a clean dashboard.\n- Sweeps: Hyper-parameter tuning. You can run multiple experiments with different hyper-parameters and track them.\n- Artifacts: Storing Datasets, models and other files for version tracking.\n- Reports: we can create reports on experiments and project levels.\n\n\nWe will be looking into, **Experiment Trancking** and **Artifacts**","96c8ee04":"**NOTE: ANYTHING BEFORE THIS CELL SHOULD ONLY BE RUN ONCE, ONLY DURING THE INITIAL SETUP**","3a7435cc":"### Saving the models and model Versioning","678ff384":"#### W&B Artifacts\nFor saving the models and making it easier to track different experiments, I will be using wandb.artifacts.\nW&B Artifacts are a way to save your datasets and models.\n\nWithin a run, there are three steps for creating and saving a model Artifact.\n\n- Create an empty Artifact with wandb.Artifact().\n- Add your model file to the Artifact with wandb.add_file().\n- Call wandb.log_artifact() to save the Artifact\n","948a6e64":"## Create The Model","ae984011":"## Lets BERT: Get the Pre-trained BERT Model from TensorFlow Hub","50b28fde":"## Lets Get the Dataset","be2b076f":"So it looks like the train and validation set are similar in terms of class imbalance and the various lengths in the question texts.","93d39d06":"Cloning the Github Repo for tensorflow models \n-  \u2013depth 1, during cloning, Git will only get the latest copy of the relevant files. It can save you a lot of space and time.\n\n- -b lets us clone a specific branch only.\n\n**Please match it with your tensorflow 2.x version.**"}}