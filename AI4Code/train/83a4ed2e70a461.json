{"cell_type":{"d2429d22":"code","49da51fb":"code","848fe463":"code","c55c98fb":"code","5fbf4a7d":"code","e535c157":"code","e7b7f024":"code","ac8a543d":"code","f3e783cd":"code","c1e8831a":"code","d1f91295":"code","0ad06652":"code","8a4ca036":"code","aaf179f9":"code","41664125":"code","b7ae2e6d":"code","180643ac":"code","940398da":"code","f88c74fc":"code","3cde93e3":"code","a34bebbe":"code","5fd04b6a":"code","5e7c84ae":"code","9d717802":"code","f2f97e42":"code","65ddd323":"code","14e09621":"code","56789031":"code","49778e43":"code","fac70358":"code","4daa0be5":"code","6446952f":"code","424af7f6":"markdown"},"source":{"d2429d22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","49da51fb":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nimport seaborn as sns\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","848fe463":"# Load the wholesale customers dataset\ntry:\n    data = pd.read_csv(\"..\/input\/creating-customer-segments\/customers.csv\")\n    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n    print(\"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape))\nexcept:\n    print(\"Dataset could not be loaded. Is the dataset missing?\")\n","c55c98fb":"###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################","5fbf4a7d":"def pca_results(good_data,pca):\n    '''Create a dataframe of the PCA results\n    Included dimension feature weights and explained variance\n    Visualized the PCA results\n    '''\n    #dimension indexing\n    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n    \n    #PCA components\n    components = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n    components.index = dimensions\n    \n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n    \n    # Create a bar plot visualization\n    fig, ax = plt.subplots(figsize = (14,8))\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)\n    \n    # Display the explained variance ratios\n    for i, ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n%.4f\"%(ev))\n\n    #Return a concatenated DataFrame\n    return pd.concat([variance_ratios, components], axis = 1)","e535c157":"def cluster_results(reduced_data, preds, centers, pca_samples):\n    '''\n    Visualizes the PCA-reduced cluster data in two dimensions\n    Adds cues for cluster centers and student-selected sample data\n    '''\n\n    predictions = pd.DataFrame(preds, columns = ['Cluster'])\n    plot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n    # Generate the cluster plot\n    fig, ax = plt.subplots(figsize = (14,8))\n\n    # Color map\n    cmap = cm.get_cmap('gist_rainbow')\n\n    # Color the points based on assigned cluster\n    for i, cluster in plot_data.groupby('Cluster'):   \n        cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n                     color = cmap((i)*1.0\/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n    # Plot centers with indicators\n    for i, c in enumerate(centers):\n        ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n                   alpha = 1, linewidth = 2, marker = 'o', s=200);\n    \n        ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n        \n    # Plot transformed sample points \n    ax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n        s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n    # Set plot title\n    ax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");\n","e7b7f024":"def biplot(good_data, reduced_data, pca):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https:\/\/github.com\/teddyroland\/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 7.0, 8.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=18)\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n    return ax","ac8a543d":"def channel_results(reduced_data, outliers, pca_samples):\n    '''\n    Visualizes the PCA-reduced cluster data in two dimensions using the full dataset\n    Data is labeled by \"Channel\" and cues added for student-selected sample data\n    '''\n\n    # Check that the dataset is loadable\n    try:\n        full_data = pd.read_csv(\"..\/input\/customers.csv\")\n    except:\n        print(\"Dataset could not be loaded. Is the file missing?\")       \n        return False\n    # Create the Channel DataFrame\n    channel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n    channel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n    labeled = pd.concat([reduced_data, channel], axis = 1)\n    \n    # Generate the cluster plot\n    fig, ax = plt.subplots(figsize = (14,8))\n\n    # Color map\n    cmap = cm.get_cmap('gist_rainbow')\n    # Color the points based on assigned Channel\n    labels = ['Hotel\/Restaurant\/Cafe', 'Retailer']\n    grouped = labeled.groupby('Channel')\n    for i, channel in grouped:   \n        channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n                     color = cmap((i-1)*1.0\/2), label = labels[i-1], s=30);\n        \n    # Plot transformed sample points   \n    for i, sample in enumerate(pca_samples):\n        ax.scatter(x = sample[0], y = sample[1], \\\n               s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n        ax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n    # Set plot title\n    ax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","f3e783cd":"#print some sample data\ndisplay(data.head())","c1e8831a":"display(data.info())","d1f91295":"display(data.describe())","0ad06652":"#samples\nnp.random.seed(2018)\nindices = np.random.randint(low=0, high = 441,size=3)\nprint(\"Indices of Samples => {}\".format(indices))\n","8a4ca036":"#create a dataframe of the chosen samples\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\nprint(\"\\nChosen samples of wholesale customers dataset:\")\ndisplay(samples)","aaf179f9":"# Scale the data using the natural logarithm\nlog_data = np.log(data)\n\n# Scale the sample data using the natural logarithm\nlog_samples = np.log(samples)\n\n# Produce a scatter matrix for each pair of newly-transformed features\n_ = sns.pairplot(log_data, diag_kind = 'kde')","41664125":"# Display the log-transformed sample data\ndisplay(log_samples)","b7ae2e6d":"# Display the correlation heatmap\ncorr = data.corr()\n\nplt.figure(figsize = (10,5))\nax = sns.heatmap(corr, annot=True)\nax.legend(loc=0, prop={'size': 15})","180643ac":"# Display the correlation heatmap\nlog_corr = log_data.corr()\n\nf = plt.figure(figsize = (16,8))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax1 = sns.heatmap(corr, annot=True, mask=mask, cbar_kws={'label': 'Before Log Normalization'})\n\nmask2 = np.zeros_like(corr)\nmask2[np.tril_indices_from(mask2)] = True\nwith sns.axes_style(\"white\"):\n    ax2 = sns.heatmap(log_corr, annot=True, mask=mask2, cmap=\"YlGnBu\", cbar_kws={'label': 'After Log Normalization'})","940398da":"# Produce a scatter matrix for each pair of features in the data\n_ = sns.pairplot(data, diag_kind = 'kde')","f88c74fc":"plt.figure(figsize = (20,8))\n_ = sns.barplot(data=data, palette=\"Set2\")","3cde93e3":"outliers_list=[]\n# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n    \n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data[feature], 25)\n    \n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data[feature], 75)\n    \n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outliers = list(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index.values)\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    outliers_list.extend(outliers)\n    \nprint(\"List of Outliers -> {}\".format(outliers_list))\nduplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\nduplicate_outliers_list.sort()\nprint(\"\\nList of Common Outliers -> {}\".format(duplicate_outliers_list))\n\n# Select the indices for data points you wish to remove\noutliers  = duplicate_outliers_list\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)","a34bebbe":"#apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components = 6, random_state=0)\npca.fit(good_data)","5fd04b6a":"#transform the log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)\nprint(\"Explained Variance Ratio => {}\\n\".format(pca.explained_variance_ratio_))\nprint(\"Explained Variance Ratio(csum) => {}\\n\".format(pca.explained_variance_ratio_.cumsum()))\n# Generate PCA results plot\npca_results = pca_results(good_data, pca)","5e7c84ae":"# Display sample log-data after having a PCA transformation applied\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))","9d717802":"# Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components = 2, random_state=0)\npca.fit(good_data)\n\n# Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_data)\n\n# Transform log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])","f2f97e42":"# Display sample log-data after applying PCA transformation in two dimensions\ndisplay(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))","65ddd323":"# Create a biplot\nbiplot(good_data, reduced_data, pca)","14e09621":"def sil_coeff(no_clusters):\n    # Apply your clustering algorithm of choice to the reduced data \n    clusterer_1 = KMeans(n_clusters=no_clusters, random_state=0 )\n    clusterer_1.fit(reduced_data)\n    \n    # Predict the cluster for each data point\n    preds_1 = clusterer_1.predict(reduced_data)\n    \n    # Find the cluster centers\n    centers_1 = clusterer_1.cluster_centers_\n    \n    # Predict the cluster for each transformed sample data point\n    sample_preds_1 = clusterer_1.predict(pca_samples)\n    \n    # Calculate the mean silhouette coefficient for the number of clusters chosen\n    score = silhouette_score(reduced_data, preds_1)\n    \n    print(\"silhouette coefficient for `{}` clusters => {:.4f}\".format(no_clusters, score))\n    \nclusters_range = range(2,15)\nfor i in clusters_range:\n    sil_coeff(i)","56789031":"# Display the results of the clustering from implementation for 2 clusters\nclusterer = KMeans(n_clusters = 2)\nclusterer.fit(reduced_data)\npreds = clusterer.predict(reduced_data)\ncenters = clusterer.cluster_centers_\nsample_preds = clusterer.predict(pca_samples)\n\ncluster_results(reduced_data, preds, centers, pca_samples)","49778e43":"# Inverse transform the centers\nlog_centers = pca.inverse_transform(centers)\n\n# Exponentiate the centers\ntrue_centers = np.exp(log_centers)\n\n# Display the true centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)","fac70358":"display(data.mean(axis=0))","4daa0be5":"display(samples)","6446952f":"# Display the predictions\nfor i, pred in enumerate(sample_preds):\n    print(\"Sample point\", i, \"predicted to be in Cluster\", pred)","424af7f6":"https:\/\/www.kaggle.com\/samratp\/creating-customer-segments-unsupervised-learning"}}