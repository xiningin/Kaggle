{"cell_type":{"ff356492":"code","f32e5ec3":"code","e9042e94":"code","6f0cf2c5":"code","121e074d":"code","175e331d":"code","4fc92d0a":"code","26549e97":"code","aa560083":"code","be186d26":"code","bc433934":"code","e7535335":"code","3ed2e2c5":"code","009e2ae7":"code","b129694a":"markdown","48feb101":"markdown","82bee309":"markdown","86c4cb77":"markdown","bdea936f":"markdown","42a4ccb1":"markdown","a18aaed5":"markdown","d1e39f83":"markdown"},"source":{"ff356492":"# Install libraries\n!pip install '..\/input\/offline-packages\/Keras_Applications-1.0.8-py3-none-any.whl'\n!pip install '..\/input\/offline-packages\/efficientnet-1.1.1-py3-none-any.whl'","f32e5ec3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\n\nimport os\nfilenames = os.listdir('..\/input\/petfinder-pawpularity-score')\nprint(filenames)","e9042e94":"Q = 30\nfeature_folds = 5\nbatch_size = 16\nepochs  = 10\nseed  = 4261\nverbose = 1\nLR  = 0.0005\nCHANNELS = 3\nIMG_SIZE = 384\n# SetAutoTune\nAUTOTUNE = tf.data.experimental.AUTOTUNE  \n\nroot_dir = '..\/input\/petfinder-pawpularity-score\/'\ntrain_meta = pd.read_csv(root_dir + 'train.csv')\ntest_meta = pd.read_csv(root_dir + 'test.csv')\ntrain_dir = root_dir + 'train\/'\ntest_dir = root_dir + 'test\/'","6f0cf2c5":"train_meta.head()","121e074d":"train_meta['Id'] = train_meta['Id'].apply(lambda x: train_dir + x + '.jpg')\n\n# Set a specific label to be able to perform stratification\ntrain_meta['stratify_label'] = pd.qcut(train_meta['Pawpularity'], q = Q, labels = range(Q))\n\n# Label value to be used for feature model 'classification' training.\ntrain_meta['target_value'] = train_meta['Pawpularity'] \/ 100.\n\n# Summary\nprint('train_meta:{}'.format(train_meta.shape))\ntrain_meta.head()","175e331d":"test_meta['Id'] = test_meta['Id'].apply(lambda x: test_dir + x + '.jpg')\ntest_meta['Pawpularity'] = 0\n\nprint('test_meta:{}'.format(test_meta.shape))\ntest_meta.head()","4fc92d0a":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    # On google colab (tpu_address = 'grpc:\/\/' + os.environ['COLAB_TPU_ADDR'], pass tpu_address as param in below fn)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    #If no TPU, uncomment below to check for GPU\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","26549e97":"def build_augmenter(is_labelled):\n    def augment(img):\n        # Only use basic augmentations...too much augmentation hurts performance\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        \n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        # Read Image\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n        \n        # Normalize and Resize\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        \n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, batch_size = 32, is_labelled = False, augment = False, repeat = False, shuffle = False):\n    decode_fn = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset","aa560083":"def unfreeze_model(model):\n    # Unfreeze layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n        else:\n            layer.trainable = False\n\ndef create_model(): \n    # Create and Compile Model and show Summary\n#     effnet_model = tf.keras.applications.EfficientNetB1(weights='imagenet', \n#                                                             include_top=False, \n#                                                             input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), pooling='avg')\n    \n    effnet_model = efn.EfficientNetB2(include_top = False, \n                                      classes = None, \n                                      input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), \n                                      weights = '..\/input\/weights\/efficientnet-b2_noisy-student_notop.h5',\n                                      pooling = 'avg')\n\n    # Set all layers to Trainable except BN layers\n    unfreeze_model(effnet_model)\n    \n    X = tf.keras.layers.Dropout(0.25)(effnet_model.output)\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(X)\n    \n    # Create Final Model\n    model = tf.keras.Model(inputs = effnet_model.input, outputs = output)\n\n    # Compile\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR), \n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = [tf.keras.metrics.RootMeanSquaredError('rmse')])        \n    \n    return model","be186d26":"training_dataset = create_dataset(train_meta,\n                                  batch_size  = batch_size, \n                                  is_labelled = True, \n                                  augment = True,\n                                  repeat  = False, \n                                  shuffle = False)","bc433934":"from tensorflow.keras import losses, optimizers , metrics\nfrom tensorflow.keras import callbacks\n\ndef get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size \n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return callbacks.LearningRateScheduler(lrfn, verbose=True)\n\ndef model_callback(fold):\n    ckpt = tf.keras.callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n                                              verbose = 1, \n                                              monitor = 'val_rmse',\n                                              mode = 'min', \n                                              save_weights_only = True,\n                                              save_best_only = True)\n    \n    return [ckpt, get_lr_callback(batch_size)]","e7535335":"sample_images, _ = next(iter(training_dataset))\n\nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(16, 10))\nfor i, image in enumerate(sample_images[:6]):\n    print(image.shape)\n    ax = plt.subplot(3, 4, 2 * i + 1)\n    plt.title(\"Input Image\")\n    plt.imshow(image.numpy().squeeze())\n    plt.axis(\"off\")\n\n#     ax = plt.subplot(3, 4, 2 * i + 2)\n#     resized_image = learnable_resizer(image[None, ...])\n#     plt.title(\"Resized Image\")\n#     plt.imshow(resized_image.numpy().squeeze())\n#     plt.axis(\"off\")","3ed2e2c5":"import gc\nfrom sklearn.model_selection import StratifiedKFold\n\n# OOF RMSE Placeholder\nall_val_loss = []\nkfold = StratifiedKFold(n_splits = feature_folds, \n                        shuffle = True, random_state = seed)\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_meta.index,\n                                                            train_meta['stratify_label'])):\n#     if fold == 1:\n    print(f'\\nFold {fold}\\n')\n    # Pre model.fit cleanup\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Create Model\n    model = create_model()\n#     for i in range(len(model.weights)):\n#         model.weights[i]._handle_name = model.weights[i].name + str(i)\n\n    # Create TF Datasets\n    trn = train_meta.iloc[train_index]\n    val = train_meta.iloc[val_index]\n    training_dataset = create_dataset(trn, \n                                      batch_size  = batch_size, \n                                      is_labelled = True, \n                                      augment     = True, \n                                      repeat      = True, \n                                      shuffle     = True)\n    validation_dataset = create_dataset(val, \n                                        batch_size  = batch_size, \n                                        is_labelled = True,\n                                        augment     = False, \n                                        repeat      = True,\n                                        shuffle     = False)\n    # Fit Model\n    history = model.fit(training_dataset,\n                        epochs = epochs,\n                        steps_per_epoch  = trn.shape[0] \/\/ batch_size,\n                        validation_steps = val.shape[0] \/\/ batch_size,\n                        callbacks = model_callback(fold),\n                        validation_data = validation_dataset,\n                        verbose = 1)   \n\n#         # Validation Information\n#         best_val_loss = min(history.history['val_rmse'])\n#         all_val_loss.append(best_val_loss)\n#         print(f'\\nValidation RMSE: {best_val_loss}\\n')","009e2ae7":"submission_df = pd.read_csv('{}sample_submission.csv'.format(root_dir))\npred = 0\n\nfor fold_index in range(feature_folds):\n    model = create_model()\n    model.load_weights('feature_model_{}.h5'.format(fold_index))\n    \n    cb_test_set = create_dataset(test_meta, \n                             batch_size  = batch_size,\n                             is_labelled = False,\n                             repeat      = False, \n                             shuffle     = False)\n    pred = pred + model.predict(cb_test_set)*100\n    \nsubmission_df['Pawpularity'] = pred\/5\nsubmission_df.to_csv('submission.csv', index = False)\nsubmission_df.head(10)","b129694a":"## Check images in dataset","48feb101":"## Define callbacks","82bee309":"## Create and train models","86c4cb77":"## Config","bdea936f":"## Create Tf Dataset","42a4ccb1":"## Define strategy","a18aaed5":"## Create submission file\nWe will calculate average of predictions from all 5 models to get our final prediction.","d1e39f83":"## Define Model"}}