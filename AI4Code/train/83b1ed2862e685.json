{"cell_type":{"bb221c56":"code","c95eece1":"code","6caefbc9":"code","44efe03f":"code","b16c4c27":"code","279b45f5":"code","910f8055":"code","0c572626":"code","cb5f6729":"code","00560dfa":"code","4a06a7d2":"code","633db2d3":"code","8547c517":"code","42c07fc5":"code","b016bae7":"code","d954b0b6":"code","b42e6f76":"code","1ca86171":"code","70751934":"code","5fbca8eb":"code","97f6e9e4":"code","e2d7ea84":"code","502e616f":"code","c2ec6332":"code","90233736":"code","b7b378ed":"code","f28a9b90":"code","f539be63":"code","ee22c1d2":"code","40a30636":"code","187fb986":"code","b83c09f2":"code","b12b1603":"code","15492c58":"code","867eb08c":"code","67a2106e":"code","0c4a8957":"code","3d819dea":"code","b6c42f3f":"code","1ee45cbc":"code","44e12345":"markdown","53c1f113":"markdown","35aeb6d4":"markdown","b83faafc":"markdown","46fcb4b7":"markdown","41fc952b":"markdown","4ba6eefa":"markdown","cd0dbb02":"markdown","090ebb3d":"markdown","7ac70ace":"markdown","5c1f8333":"markdown","65f31ab2":"markdown","7747169b":"markdown","0904c7e0":"markdown","6663e0db":"markdown","60139b65":"markdown","cb50ee31":"markdown","4fb84443":"markdown","4fc5ac83":"markdown","6b5f41b6":"markdown","244b0088":"markdown","96f0ba96":"markdown","3f49b46d":"markdown","9223d4b4":"markdown","27f85668":"markdown","f4d3fc71":"markdown","92df0ed6":"markdown","7b9bc7b2":"markdown","a52daecd":"markdown","23541c8d":"markdown","b436b3ae":"markdown","b2d25b36":"markdown","c9a46574":"markdown"},"source":{"bb221c56":"!pip install --upgrade git+https:\/\/github.com\/goolig\/dsClass.git\nfrom dsClass.path_helper import *\nimport dsClass\nget_file_path(\"srganB-epoch300.data-00000-of-00001\")\n#srganA-epoch300.data-00000-of-00001\n#srgan-pic1.jpg\n#model-20170512-110547.ckpt-250000.data-00000-of-00001","c95eece1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","6caefbc9":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","44efe03f":"import os\ncwd = os.getcwd()\n\nfrom dsClass.align_custom import AlignCustom\nfrom dsClass.face_feature import FaceFeature\nfrom dsClass.mtcnn_detect import MTCNNDetect\nfrom dsClass.tf_graph import FaceRecGraph\nfrom dsClass.path_helper import *\n\nimport sys\nimport glob\nimport cv2\nimport json\nimport numpy as np\nimport pandas as pd\nimport time\nimport scipy\nimport urllib\nimport matplotlib.pyplot as plt","b16c4c27":"from IPython.display import YouTubeVideo\nYouTubeVideo('NuhCoO6GO5U')","279b45f5":"def annotate_face(rect, recog_data, frame):\n    \"\"\"\n    Draw a box around the face and label the person\n    :param rect : the face bounding box\n    :param recog_data : tuple of person name and confidence percentage\n    :param frame : the frame to draw on\n    \"\"\"\n    shrtname = short_name(recog_data[0])\n    acc = round(recog_data[1], 1)\n    bbox_color = (255, 255, 255) if \"Unknown\" in recog_data[0] else (124,252,0)  # RGB\n\n    #draw bounding box \/ fancy border for the face\n    #cv2.rectangle(frame,(rect[0],rect[1]),(rect[0] + rect[2],rect[1]+rect[3]),bbox_color) \n    draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), bbox_color, 1, 10, 10)  \n    anot_text = shrtname + \"-\" + str(acc) + \"%\"\n    cv2.putText(frame, anot_text,\n                (rect[0]-4,rect[1]-4), cv2.FONT_HERSHEY_SIMPLEX,0.35,\n                bbox_color, 1, cv2.LINE_AA)                        \n\n    \ndef short_name(name):\n    \"\"\"e.g: Jaime Lannister -> Jaime.L\"\"\"\n    if not name.startswith(\"Unknown\"):\n        name_split = name.split(\" \")\n        short_name = name_split[0]\n        if len(name_split) > 1:\n            short_name = short_name + \".\" + name_split[1][0]\n        return(short_name)\n    else:\n        return name\n    \n    \ndef draw_border(img, pt1, pt2, color, thickness, r, d):\n    \"\"\"\n    Fancy box drawing function by Dan Masek\n    Code in: https:\/\/www.codemade.io\/fast-and-accurate-face-tracking-in-live-video-with-python\/\n    \"\"\"\n    x1, y1 = pt1\n    x2, y2 = pt2\n \n    # Top left drawing\n    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n \n    # Top right drawing\n    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n \n    # Bottom left drawing\n    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n \n    # Bottom right drawing\n    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness) \n    \n    \ndef read_image_from_url(url2read):\n    if url2read.startswith('http'):\n        req = urllib.request.urlopen(url2read)\n        arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n        img = cv2.imdecode(arr, -1) # 'Load it as it is'\n    else:\n        img = cv2.imread(url2read)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return(img)","910f8055":"dict_faces = dict()\ndict_faces[\"Jaime Lannister\"] = [\"https:\/\/s2.r29static.com\/\/bin\/entry\/97f\/340x408,85\/1832698\/image.jpg\",\n                                \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/b\/b4\/Jaime_Lannister-Nikolaj_Coster-Waldau.jpg\/220px-Jaime_Lannister-Nikolaj_Coster-Waldau.jpg\",\n                                 \"https:\/\/upload.wikimedia.org\/wikipedia\/pt\/thumb\/0\/06\/Nikolaj-Coster-Waldau-Game-of-Thrones.jpg\/220px-Nikolaj-Coster-Waldau-Game-of-Thrones.jpg\",\n                                 \"https:\/\/purewows3.imgix.net\/images\/articles\/2017_09\/jaime-lannister-season-7-game-of-thrones-finale1.jpg?auto=format,compress&cs=strip&fit=min&w=728&h=404\",\n                                 \"https:\/\/cdn.newsday.com\/polopoly_fs\/1.13944684.1502107079!\/httpImage\/image.jpeg_gen\/derivatives\/landscape_768\/image.jpeg\",\n                                 \"https:\/\/www.cheatsheet.com\/wp-content\/uploads\/2017\/08\/Jaime-Lannister-Game-of-Thrones.png\",\n                                 \"https:\/\/fsmedia.imgix.net\/9c\/c0\/27\/10\/15e0\/44a4\/8ecb\/9339993b563d\/nikolaj-coster-waldau-as-jaime-lannister-in-game-of-thrones-season-7.png?rect=0%2C0%2C1159%2C580&dpr=2&auto=format%2Ccompress&w=650\",\n                                 \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQrIQuBKKUocAizwfWtIdhAcvfowLJatKqqDsO3ywYdh3rv-mBk\"]","0c572626":"# Check urls and print last image\n# if any image causes problems (like error 403 forbidden) then remove it and choose another\nfor p in dict_faces.keys():\n    urls = dict_faces[p]\n    for url2read in urls:\n        print('\\rChecking: %s' % url2read[:100], ' '*100, end='')\n        img = read_image_from_url(url2read)\nplt.imshow(img);","cb5f6729":"os.chdir(\"\/kaggle\/working\/\")\n!wget https:\/\/github.com\/opencv\/opencv\/raw\/master\/data\/haarcascades\/haarcascade_frontalface_default.xml\n!ls","00560dfa":"#Create the haar cascade\nface_cascade = cv2.CascadeClassifier('\/kaggle\/working\/haarcascade_frontalface_default.xml')  # if not running on kaggle, remove the initial '\/'\n\ndef find_faces_in_image(orig_img, scaleFactor, minNeighbors, minSize, maxSize):\n    orig_img_copy = orig_img.copy()\n    gray = cv2.cvtColor(orig_img_copy, cv2.COLOR_BGR2GRAY)\n    #plt.imshow(gray) \n    \n    # Detect faces in the image\n    faces = face_cascade.detectMultiScale(\n        gray,           \n        scaleFactor=scaleFactor, \n        minNeighbors=minNeighbors,  \n        minSize=minSize, \n        maxSize=maxSize \n    )\n    \n    print(\"Found {0} faces!\".format(len(faces)))\n\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(orig_img_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    plt.imshow(orig_img_copy)","4a06a7d2":"scaleFactor = 1.3\nminNeighbors = 5\nminSize = (60, 60)   \nmaxSize = (70, 70)\nfind_faces_in_image(img, scaleFactor, minNeighbors, minSize, maxSize)","633db2d3":"def find_faces_in_frame_of_video(frame, scaleFactor, minNeighbors, minSize, maxSize):\n    \"\"\"Find faces in an image and returns the bounding boxes of them\"\"\"","8547c517":"def video_file_recog_haar(src_filename, output_filename='\/kaggle\/working\/output_haar.mp4', framerate=None, scaleFactor=1.3, minNeighbors=5, minSize=60, maxSize=70):\n    print(\"[INFO] Reading video file...\")\n    if glob.glob(src_filename):\n        vs = cv2.VideoCapture(src_filename); #get input from file\n    else:\n        print(\"file does not exist\")\n        return\n    \n    print(\"[INFO] Initializing video writer...\")\n    frame_width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # DIVX, XVID, MJPG, X264, WMV1, WMV2, mp4v\n    if framerate is None:\n        framerate = vs.get(cv2.CAP_PROP_FPS) # use same as input video, or can set to 20.0 \/ 30.0\n    out = cv2.VideoWriter(output_filename, fourcc, framerate, (frame_width,frame_height))\n    \n    recog_list = []\n    frame_counter = 0\n    t0 = time.time()\n    while True:        \n        ret, frame = vs.read();\n        if ret:\n            frame_counter += 1\n            if frame_counter%(30\/framerate)==0:\n                min_face_size = 60 #min face size is set to 60x60\n                rects = find_faces_in_frame_of_video(frame, scaleFactor, minNeighbors, (minSize, minSize), (maxSize,maxSize))\n                print(\"\\rNumber of faces found in frame \" + str(frame_counter) + \":\",len(rects), end='')\n                aligns = []\n                positions = []\n                for (i, rect) in enumerate(rects):\n                    draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), (255,255,255), 1, 10, 10)\n                    cv2.putText(frame,\"Unknown\",\n                                        (rect[0]-4,rect[1]-4),cv2.FONT_HERSHEY_SIMPLEX,0.35,\n                                        (255,255,255),1,cv2.LINE_AA)\n\n\n                out.write(frame)\n        else:  # end of video, no more frames\n            break\n    \n    elapsed_time = time.time() - t0\n    print()\n    print(\"[exp msg] elapsed time for going over the video: \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n    vs.release()\n    out.release()\n    cv2.destroyAllWindows()\n    print(\"Done\")","42c07fc5":"video_file_recog_haar(src_filename = get_file_path(\"Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4\"), framerate=10)","b016bae7":"#Global Variables\nperson_embeddings = None","d954b0b6":"def augment_image(img):\n    \"\"\"Takes an image array and returns a list of its augemented versions\"\"\"\n    aug_images = []\n    #https:\/\/docs.opencv.org\/2.4\/modules\/core\/doc\/operations_on_arrays.html?highlight=flip#cv2.flip\n    # for more examples see https:\/\/github.com\/aleju\/imgaug\n    return(aug_images)\n\n\ndef get_person_imgs(urls, min_face_size=40):\n    \"\"\"\n    Given a list of URLs of a person images, this function will extract and align the faces within the image and detect its pose (left\/right\/center)\n    \"\"\"\n    person_imgs = {\"Left\" : [], \"Right\": [], \"Center\": []};\n    person_imgs_count = {\"Left\" : 0, \"Right\": 0, \"Center\": 0};\n    \n    counter_break = 0\n    while True:    \n        for url2read in urls:\n            #print(file)\n            #ret, frame = vs.read()\n            #img = cv2.imread(file)\n            img = read_image_from_url(url2read) # ****** file = url2read\n            if img is None:\n                print(\"********************* image was not loaded ***********************\")\n                continue\n\n            # Augmenting the data - add a flipped version of the image to add more data\n            frames = [img]\n            frames.extend(augment_image(img))\n\n            for frame in frames:\n                if True: #ret:\n                    rects, landmarks = face_detect.detect_face(frame, min_face_size)\n                    #print(\"rects\", rects)\n                    for (i, rect) in enumerate(rects):\n                        aligned_frame, pos = aligner.align(160, frame,landmarks[i]);\n                        #print(pos)\n                        person_imgs_count[pos]+=1\n                        if len(aligned_frame) == 160 and len(aligned_frame[0]) == 160:\n                            person_imgs[pos].append(aligned_frame)\n                            #cv2.imshow(\"Captured face\", aligned_frame)\n                            #cv2.imwrite(\"..\/data2\/frame%d.jpg\" % count, aligned_frame)\n                else:\n                    break\n            \n        if person_imgs_count[\"Left\"] == 0 or person_imgs_count[\"Right\"] == 0 or person_imgs_count[\"Center\"] == 0:\n            counter_break+=1\n            if counter_break > 0:\n                print(person_imgs_count) \n                assert 0==1, \"Must get all poses of a face: Left, Right and Center, try adding more images\"\n                return None\n        else:\n            break\n                            \n    print(person_imgs_count)    \n    return(person_imgs)  ","b42e6f76":"def extract_embeddings_from_images(min_face_size=40, embeddings_filename='\/kaggle\/working\/facerec_128D.txt'):\n    \"\"\" \n    Go over all urls, extract and align faces, feed each face to the embeddings net,\n    and saves an embedding vector for each person-position pair.\n    Save all embeddings to a .txt file\n    \"\"\"\n    print()\n    print(\"[INFO] Extracting data from images ...\")\n    data_set = dict()\n\n    for new_name in dict_faces.keys():\n        person_features = {\"Left\" : [], \"Right\": [], \"Center\": []};\n        print(\"Extracting:\", new_name)\n        print(\"number of img files:\",len(dict_faces[new_name]))\n        person_imgs = get_person_imgs(dict_faces[new_name], min_face_size=min_face_size) \n        if person_imgs is None:\n            print(\"extraction of:\",new_name, \" failed\")\n            continue\n        \n        print(\"extracted person_imgs from:\",new_name)\n        print(\"-------------------------------------\")\n\n        for pos in person_imgs: # there are some exceptions here, but I'll just leave it as this to keep it simple\n            person_features[pos] = [np.mean(extract_feature.get_features(person_imgs[pos]), axis=0).tolist()]\n        data_set[new_name] = person_features;\n    \n    global person_embeddings\n    person_embeddings = data_set\n    with open(embeddings_filename, 'w+') as f:\n        f.write(json.dumps(data_set))\n    \n\ndef load_embeddings_from_file(embeddings_filename='\/kaggle\/working\/facerec_128D.txt'):\n    global person_embeddings\n    with open(embeddings_filename, 'r') as f:\n        person_embeddings = json.loads(f.read());\n\n        \ndef identifyPerson(features_arr, position, thres = 0.6, percent_thres = 70):\n    '''\n    :param features_arr: a list of 128d Features of a face\n    :param position: face position types (Left\/Right\/Center)\n    :param thres: distance threshold\n    :param percent_thres : minimum confidence required to identify a person\n    :return: tuple of person name and confidence of detection\n    '''\n    assert person_embeddings is not None, \"Must load or extract persons embeddings in order to recgonize persons\"\n    result = \"Unknown\"\n    smallest = sys.maxsize  # initialize with a large number\n    for person in person_embeddings.keys():\n        person_data = person_embeddings[person][position]\n        for data in person_data:  # in our case there's only one embedding per person-position pair\n            distance = scipy.spatial.distance.euclidean(data, features_arr)  # same as: np.sqrt(np.sum(np.square(data-features_arr)))\n            #distance = scipy.spatial.distance.cosine(data, features_arr)  # if using cosine distance it is recommended to lower the thres to ~0.4\n            \n            if(distance < smallest):\n                smallest = distance\n                result = person\n    percentage =  min(100, 100 * thres \/ smallest)\n    if percentage <= percent_thres:\n        result = \"Unknown (%s)\" % result.split(' ')[0]  # show highest score person for debug purposes\n    return (result, percentage)","1ca86171":"model_path = '..\/input\/model-20170512-110547.ckpt-250000' \nos.chdir(\"\/kaggle\/input\/\")  # if not on kaggle environment then omit the initial '\/'","70751934":"# initalize\nFRGraph = FaceRecGraph();\naligner = AlignCustom();\nextract_feature = FaceFeature(FRGraph, model_path = model_path);\nface_detect = MTCNNDetect(FRGraph, scale_factor=2); #scale_factor, rescales image for faster detection","5fbca8eb":"#load_embeddings_from_file()\nextract_embeddings_from_images(min_face_size=40)","97f6e9e4":"url = 'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg'\nframe = read_image_from_url(url)\nrects, landmarks = face_detect.detect_face(frame, minsize=40);  # min face size is set to 80x80\n\nfor rect in rects:\n    draw_border(frame, (rect[0],rect[1]), (rect[0] + rect[2],rect[1]+rect[3]), (255,255,255), 2, 10, 10)\n    \nplt.subplots(figsize=(15,10))\nplt.imshow(frame);","e2d7ea84":"idx = 0\nrect = rects[idx]\nplt.imshow(frame[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]])\nfor k in range(int(len(landmarks[idx]) \/ 2)):\n    plt.plot(landmarks[idx][k]-rect[0], landmarks[idx][k+5]-rect[1], 'r+', markersize=20)","502e616f":"for (i, rect) in enumerate(rects):\n    aligned_frame, pos = aligner.align(160, frame, landmarks[i])\n    plt.subplot(121)\n    plt.imshow(frame[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]])\n    plt.title('Original')\n    plt.subplot(122)\n    plt.imshow(aligned_frame)\n    plt.title('Aligned')\n    plt.suptitle('Position: %s' % pos)\n    plt.show()","c2ec6332":"features_vector = extract_feature.get_features([aligned_frame])","90233736":"features_vector.shape","b7b378ed":"features_vector","f28a9b90":"def frame_face_recog(frame, min_face_size=80, percent_thres = 70, verbose=False):\n    \"\"\" \n    Detect faces in a frame, try to recgonize them, and draws a box around the face with predicted person + % confidence\n    :param frame : the frame to indentify faces in. an array with shape of width X height X channels\n    :param min_face_size : minimum size of face to detect. integer. e.g: value of 80 is set to 80x80 pixels\n    :param verbose : True to print debug information while running\n    \n    Alters inplace the frame with predictions annotations\n    returns a list of (person,confidence) tuples\n    \"\"\"\n    aligner_resize_to = 160  # the aligner function will rescale image to X by X pixels before sent to be embedded.\n    \n    # Detect all faces in frame and get their bounding-rectangles and landmarks\n    # <your code here>\n    \n    # Go through each face in frame and perform:\n    #  1) align the face (using aligner.align() function). remember that aligner.align() returns the aligned-face and face-pose (left\/right\/center)\n    #  2) extract aligned face features (embeddings)\n    #  3) find the person the face belongs to\n    #  4) draw a box around the face and label the person\n    recog_list = []\n    # <your code here>\n            \n    return recog_list","f539be63":"url = 'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg'\nframe = read_image_from_url(url)\nframe_face_recog(frame)\nplt.subplots(figsize=(20,15))\nplt.imshow(frame);","ee22c1d2":"def video_file_recog(src_filename, output_filename='\/kaggle\/working\/output.mp4', percent_thres = 70, verbose=False):\n    print(\"[INFO] Reading video file...\")\n    if glob.glob(src_filename):\n        vs = cv2.VideoCapture(src_filename); #get input from file\n    else:\n        print(\"file does not exist\")\n        return\n    \n    print(\"[INFO] Initializing video writer...\")\n    frame_width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # DIVX, XVID, MJPG, X264, WMV1, WMV2, mp4v\n    framerate = vs.get(cv2.CAP_PROP_FPS) # use same as input video, or can set to 20.0 \/ 30.0\n    out = cv2.VideoWriter(output_filename, fourcc, framerate, (frame_width,frame_height))\n    \n    recog_list = []\n    frame_counter = 0\n    t0 = time.time()\n    while True:        \n        ret, frame = vs.read();\n        if ret:\n            frame_counter += 1\n            print('\\rProcessing Frame %i\/%i' % (frame_counter, total_frames), end=' ')\n            recog_data = frame_face_recog(frame, min_face_size=40, percent_thres=percent_thres, verbose=verbose)\n            recog_list.extend(recog_data)\n            #cv2.imshow(\"Frame\",frame)\n            #cv2.imwrite(\"..\/data3\/frame%d.jpg\" % count, frame)\n            out.write(frame)\n        else:  # end of video, no more frames\n            break\n    \n    elapsed_time = time.time() - t0\n    print()\n    print(\"[exp msg] elapsed time for going over the video: \" + str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n    vs.release()\n    out.release()\n    cv2.destroyAllWindows()\n    \n    known_counter = len([1 for recog in recog_list if recog[1] > percent_thres])\n    unknown_counter = len(recog_list) - known_counter\n    print(\"known_counter:\", known_counter, \"unknown_counter:\", unknown_counter)\n    \n    print()\n    print(\"Done\")","40a30636":"video_file_recog(src_filename = get_file_path(\"Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4\"), percent_thres=70, verbose=False)","187fb986":"# the output file from opencv is just video frames (Without audio). lets add the original audio track to the output movie\n# if ffmpeg is not installed then it will return False\n#vid2vid_audio_transfer('Game of Thrones 7x07 - Epic Daenerys Dragonpit Entrance.mp4', 'output.mp4', 'output_w_audio.mp4');","b83c09f2":"!ls ..\/working\/ -ashl","b12b1603":"#from srganUnified import SRGAN\nfrom dsClass.srganUnified import SRGAN\n\nimport tensorflow as tf\nos.chdir(cwd)  # reset position to root folder","15492c58":"def infer(x_test, ground_truth, titles, save_results=False, display=True, model_file='\/kaggle\/input\/srganA-epoch300'):\n    \"\"\" x_test should be in shape of (batch_size, 24, 24, 3). images in BGR (not RGB)\n    and ground_truth is same as x_test, just (batch_size, 96, 96, 3)\n    ground_truth is only used for displaying, and not for inferring.\n    \"\"\"\n    x = tf.placeholder(tf.float32, [None, 24, 24, 3])\n    is_training = tf.placeholder(tf.bool, [])\n\n    print('Initializing Model')\n    model = SRGAN(x, is_training, batch_size=len(x_test), infer=True)\n\n    print('Loading model checkpoint')\n    # Restore the SRGAN network\n    saver = tf.train.Saver()\n    saver.restore(sess, model_file)\n\n    print('Inferring')\n    # Infer\n    raw = x_test.astype('float32')\n    fake = sess.run(\n        model.imitation,\n        feed_dict={x: raw, is_training: False})\n    save_img([raw, fake, ground_truth], ['Input', 'Output', 'Ground Truth'], titles, save=save_results, display=display)\n    print('Done')\n\n    \ndef save_img(imgs, label, titles, save=False, display=True):\n    for i in range(len(imgs[0])):\n        seq_ = \"{0:04d}\".format(i+1)\n        fig = plt.figure()\n        for j, img in enumerate(imgs):\n            im = np.uint8((img[i]+1)*127.5)\n            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n            fig.add_subplot(1, len(imgs), j+1)\n            plt.imshow(im)\n            plt.tick_params(labelbottom='off')\n            plt.tick_params(labelleft='off')\n            plt.gca().get_xaxis().set_ticks_position('none')\n            plt.gca().get_yaxis().set_ticks_position('none')\n            plt.xlabel(label[j])\n            if j==1:\n                plt.title(titles[i])\n    \n        path = os.path.join('result', '{}.jpg'.format(titles[i]))\n        if save:\n            plt.savefig(path)\n        if display:\n            plt.show()\n        if save:\n            plt.close()","867eb08c":"def downscale_func(x):\n        if len(x.shape)==3:\n            x = np.expand_dims(x, axis=0)\n        K = 4\n        arr = np.zeros([K, K, 3, 3])\n        arr[:, :, 0, 0] = 1.0 \/ K ** 2\n        arr[:, :, 1, 1] = 1.0 \/ K ** 2\n        arr[:, :, 2, 2] = 1.0 \/ K ** 2\n        weight = tf.constant(arr, dtype=tf.float32)\n        downscaled = tf.nn.conv2d(\n            x, weight, strides=[1, K, K, 1], padding='SAME')\n        return downscaled\n\n\ndef process_file(filename, downscale='cv2'):\n    \"\"\" downscale can be 'cv2' \/ 'conv' \n    \"\"\"\n    img = cv2.imread(filename)\n    return process_img_arr(img[:,:,::-1], filename, downscale)\n\n\ndef process_img_arr(img, name, downscale='cv2'):\n    face = img[:,:,::-1].copy()\n      \n    if face.shape[0] > 96:\n        ground_truth = cv2.resize(face, (96, 96))\n    else:\n        ground_truth = face\n    \n    if downscale=='cv2':\n        face = cv2.resize(face, (24, 24))\n    elif downscale=='conv':\n        gt4conv = cv2.resize(face, (96, 96))\n        downs = downscale_func(gt4conv.astype('float32'))\n        face = sess.run(downs)\n    \n    ground_truth = ground_truth \/ 127.5 - 1\n    face = face \/ 127.5 - 1\n    input_ = np.zeros((1, 24, 24, 3))\n    input_[0] = face\n    \n    return input_, ground_truth","67a2106e":"face_urls = [\n             '\/kaggle\/input\/srgan-pic1.jpg',\n             '\/kaggle\/input\/srgan-pic2.jpg',\n             '\/kaggle\/input\/srgan-pic3.jpg',\n             'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/best-game-of-thrones-season-8-fan-theories-1554917935.jpg',\n             'https:\/\/i.redd.it\/mm9sgp28ri811.jpg',\n             'https:\/\/cdn.pastemagazine.com\/www\/articles\/CERSEI-LANNISTER-quotes-list.jpg',\n]","0c4a8957":"def urls2imgarr(urls, crop_faces=True):\n    \"\"\" \n    Go over all urls, fetch the images, and if crop_faces is True then extract and crop all faces in image.\n    returns a list of image arrays. each item in the list is of shape of (width, height, 3). width and height of images can vary, they will be resized later on in process_img_arr()\n    \"\"\"\n    all_faces = []\n    # <your code here>\n    \n    return all_faces\n\nall_faces = urls2imgarr(face_urls, crop_faces=True)\nprint('Fetched total of %i faces' % len(all_faces))","3d819dea":"tf.reset_default_graph()\nsess = tf.Session()\ninit = tf.global_variables_initializer() \nsess.run(init)\n\nx_test = []\nground_truth = []\ntitles = []\nfor (i, img_arr) in enumerate(all_faces):\n    x, gt = process_img_arr(img_arr, i, downscale='conv')\n    x_test.append(x)\n    ground_truth.append(gt)\n    titles.append(i)\n\nx_test = np.concatenate(x_test)","b6c42f3f":"x_test.shape","1ee45cbc":"tf.reset_default_graph()\nsess = tf.Session()\ninit = tf.global_variables_initializer() \nsess.run(init)\n\ninfer(x_test, ground_truth, titles, save_results=False, display=True, model_file='\/kaggle\/input\/srganA-epoch300')","44e12345":"Heavily based on https:\/\/github.com\/tadax\/srgan, which is an implementation of the SRGAN model proposed in the paper (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, https:\/\/arxiv.org\/abs\/1609.04802) with TensorFlow.  \nTrained on Labeled Faces in the Wild - http:\/\/vis-www.cs.umass.edu\/lfw\/","53c1f113":"### Feature extraction","35aeb6d4":"## Questions and Instructions - Part 1\n\n### Haar Cascade\n- Change paramaters of (scaleFactor, minNeighbors, minSize, maxSize) to find all faces in the GOT image, using find_faces_in_image().\n    - gray is the input grayscale image.\n    - scaleFactor is the parameter specifying how much the image size is reduced at each image scale. It is used to create the scale pyramid.\n    - minNeighbors is a parameter specifying how many neighbors each candidate rectangle should have, to retain it. A higher number gives \n      lower false   positives.\n    - minSize is the minimum rectangle size to be considered a face.\n    - More help can be found in: https:\/\/docs.opencv.org\/2.4\/modules\/objdetect\/doc\/cascade_classification.html\n- Which parameters did the best work?\n- complete function find_faces_in_frame_of_video() to run face detection using  video_file_recog_haar() on the GOT video\n-  Change paramaters of (scaleFactor, minNeighbors, minSize, maxSize) to find as many TRUE faces as possible in video using\n    video_file_recog_haar()\n    - To download output video file (output_haar.mp4): click on the download icon near its name in the right pane. click once and wait, as sometimes it takes it a bit of time to process the download.\n    - Which parameters did the best work?\n- Check your parameters with framerate of 30 when you think it is good enough\n\n### MTCNN and Face Vector Search\n- RUN Network Based Detection and Recognition code\n- Download movie output.mp4 and check who was recognized and how many times?\n- Add more individulas to the database so you could recgnize more individuals in video (notice you need images with center, right, left angles)\n- What led you to choose the images you chose? what was your thought process?\n- Try to augment the images using the augment_image function, does that improves the accuracy?\n   - For help check opencv image manipulaions and https:\/\/github.com\/aleju\/imgaug\n- Try to change min_face_size and see if you can recgnize faces in more frames\n- How would you increase the accuracy of the recognition?\n- How would you increase the consistency of the recognition (like when the same face is recognized in one frame and not in the next)?\n- How would you make the entire process run faster?\n- what will happen if we will reshape the face images (that the aligner returns) to something other than 160x160 pixels?\n- what should you do if you want to methodically test your model on the given video file?\n- in second 55, Cersei isn't recognized well? which images you should include in person-images to improve that?","b83faafc":"Based and Inspired by:\n- **https:\/\/github.com\/vudung45\/FaceRec\n- Augmentation code: https:\/\/github.com\/vxy10\/ImageAugmentation\n- Fancy borders: https:\/\/www.codemade.io\/fast-and-accurate-face-tracking-in-live-video-with-python\/","46fcb4b7":"## Generate Face DB","41fc952b":"## RUN","4ba6eefa":"### Align Face","cd0dbb02":"# Part 2 - Super Resolution\n\n- OBJECTIVE - reconstruct a face from a low resolution image","090ebb3d":"## Fetching the data\nfeed in urls of images of faces","7ac70ace":"### Mini assignment\nComplete the following frame_face_recog() function so we can do face detection on the video","5c1f8333":"# Part 1 - Objective - Face Detection in Video\n\nDetect and recognize the faces in the following youtube video:","65f31ab2":"### RUN Video generation","7747169b":"## Mini assignment\nComplete the following urls2imgarr() function.  \nuse the face detection methods you've used in part 1","0904c7e0":"## Detection and Recgnition","6663e0db":"## Detect Faces Using MTCNN","60139b65":"----------------","cb50ee31":"Description:\n- Images from Video Capture -> detect faces' regions -> crop those faces and align them \n- each cropped face is categorized in 3 types: Center, Left, Right \n- Extract 128D vectors( face features)\n- Search for matching subjects in the dataset based on the types of face positions. \n- The preexisitng face 128D vector with the shortest distance to the 128D vector of the face on screen is most likely a match\n(Distance threshold is 0.6, percentage threshold is 70%)\n    ","4fb84443":"## Part by Part Walkthrough","4fc5ac83":"### test your function","6b5f41b6":"### Mini Assignemnt\nChange paramaters of (scaleFactor, minNeighbors, minSize, maxSize) above to find all faces in the GOT image,\nuse information in: https:\/\/docs.opencv.org\/2.4\/modules\/objdetect\/doc\/cascade_classification.html","244b0088":"### Face Detection","96f0ba96":"## Face Recognition in Video using MTCNN, aligner and embedded vectors","3f49b46d":"## Face Detection in Video using Haar","9223d4b4":"### Mini assignment\nComplete the following find_faces_in_frame_of_video() function so we can do face detection on the video","27f85668":"## Helper functions","f4d3fc71":"> > ## Setup and main functions","92df0ed6":"### Landmarks","7b9bc7b2":"## Questions and Instructions - Part 2\n\n- Complete urls2imgarr() function. use the face detection methods you've used in part 1.\n- what happens when we are not cropping the images to only the faces? why?\n- which downscaling method brings better results ('conv' or 'cv2')? why do you think that the downscaling method affects the performance of the model?\n- go to Labeled Faces in the Wild website and inspect it a bit (http:\/\/vis-www.cs.umass.edu\/lfw\/)\n  - what are the caveats of this dataset? \n  - what we should look out for when using it?\n- run the inference one time with '\/kaggle\/input\/srganA-epoch300' as the model_file, and again with '\/kaggle\/input\/srganB-epoch300'\n  - do you spot any differences (they are quite subtle...)?\n  - both srganA and srganB models have the same architechture. What do you think is the difference between them? (hint: relates to the question about the LFW dataset)\n- add images urls of your own to the list and re-run the model to see how it performs on various types of faces","a52daecd":"![example](https:\/\/github.com\/tadax\/srgan\/raw\/master\/results\/000000010.jpg)","23541c8d":"### Generate Faces DB functions\n- Extract and crop faces from each given image\n- convert faces to embedded vector of 128D","b436b3ae":"### Generate face database","b2d25b36":"### Initialize","c9a46574":"## Detect Faces Using Haar Cascades"}}