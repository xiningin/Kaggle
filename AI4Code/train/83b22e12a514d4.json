{"cell_type":{"b66cb090":"code","e1058e09":"code","093b5f6a":"code","461709da":"code","61420d57":"code","966fe42f":"code","f9fe75ba":"code","b00875cc":"code","2056ed37":"code","eaddb816":"code","17f7c3c7":"code","e7442daf":"code","66ca4c53":"code","b3fff911":"code","e3a693f1":"code","c06f04be":"code","ac0d7bd6":"code","ec60617c":"code","69a52f6d":"code","b3deb468":"code","fba7d6b7":"code","55a764e4":"code","70918abe":"code","5ad0c18f":"code","a0a3af1d":"code","2fd235e2":"code","5d8e05fc":"code","f22ef57d":"code","02ae40d3":"code","40c4cea5":"code","b244433d":"code","6c20108d":"code","3107210c":"code","52c1ad8f":"markdown"},"source":{"b66cb090":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e1058e09":"# !pip install urduhack tensorflow_gpu","093b5f6a":"import tensorflow as tf\nfrom urduhack import normalize\nfrom urduhack.preprocess import remove_punctuation, normalize_whitespace\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input, Dropout\nfrom tensorflow.keras.layers import Concatenate, Conv1D, MaxPool1D, LSTM, BatchNormalization\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel","461709da":"FILTER_SIZES = [3, 4]\nNUM_FILTERS = 4\nMAX_SEQUENCE_LENGTH = 512","61420d57":"imdb_df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-translated-urdu-reviews\/urdu_imdb_dataset.csv\")","966fe42f":"imdb_df.head()","f9fe75ba":"imdb_df.info()","b00875cc":"imdb_df.describe()","2056ed37":"imdb_df.columns","eaddb816":"plt.hist(imdb_df[\"sentiment\"]);","17f7c3c7":"imdb_df[\"review\"][0]","e7442daf":"# Removing punctuations and unnecessary whitespaces from the text\nimdb_df[\"review\"] = imdb_df[\"review\"].apply(remove_punctuation)\nimdb_df[\"review\"] = imdb_df[\"review\"].apply(normalize_whitespace)\nimdb_df[\"review\"] = imdb_df[\"review\"].apply(normalize)","66ca4c53":"#clean and normalized text\nimdb_df[\"review\"][0]","b3fff911":"tokenizer = XLMRobertaTokenizer.from_pretrained(\"jplu\/tf-xlm-roberta-base\")\nxlm_roberta = TFXLMRobertaModel.from_pretrained(\"jplu\/tf-xlm-roberta-base\")\nxlm_roberta.trainable = False","e3a693f1":"tokens = tokenizer.tokenize(imdb_df[\"review\"][0])\ntokens","c06f04be":"token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(imdb_df[\"review\"][0]))\ntoken_ids","ac0d7bd6":"#sentence output gives token level vector representation\n#pooled output gives sentence level vector representation\nsentence_output, pooled_output = xlm_roberta(np.array([token_ids]))\n\nprint(sentence_output.shape, pooled_output.shape)","ec60617c":"for i in range(2):\n    print(str(tokens[i]) + \": \" + str(sentence_output[:, i, :]))","69a52f6d":"texts = imdb_df[\"review\"].values.tolist()","b3deb468":"#tokenizer.encode directly converts text to tokens to ids and adds special_tokens(cls_tokens, unk_token, sep_tokens) as well as padding\nencoded = []\nfor text in texts:\n    input_ids = tokenizer.encode(text, max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True, add_special_tokens=True)\n    encoded.append(input_ids)","fba7d6b7":"data = np.array(encoded)","55a764e4":"labels = imdb_df[\"sentiment\"].values.tolist()\nfrom sklearn.preprocessing import LabelEncoder\nen = LabelEncoder()\nlabels = en.fit_transform(labels)\nlabels = tf.keras.utils.to_categorical(labels)","70918abe":"n_classes = len(np.unique(labels))","5ad0c18f":"VALIDATION_SPLIT = 0.15\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]","a0a3af1d":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nsequence_ouput, _ = xlm_roberta(sequence_input)\n\nconv_blocks = []\nfor k_size in FILTER_SIZES:\n    conv = Conv1D(filters=NUM_FILTERS,\n                         kernel_size=k_size,\n                         padding=\"valid\",\n                         activation=\"relu\",\n                         strides=1)(sequence_ouput)\n    conv = Dropout(0.25)(conv)\n    conv = MaxPooling1D(pool_size=MAX_SEQUENCE_LENGTH - k_size + 1)(conv)\n    conv = Flatten()(conv)\n    conv_blocks.append(conv)\nconcat = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\nx = Dropout(0.2)(concat)\nx = Dense(512, activation=\"relu\")(x)\npreds = Dense(n_classes, activation=\"softmax\")(x)","2fd235e2":"model = tf.keras.Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n              metrics=['acc'])","5d8e05fc":"model.summary()","f22ef57d":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, mode='auto')\nhistory = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n                    epochs=10, batch_size=16, callbacks=[es])","02ae40d3":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show();\n    \nplot_graphs(history, \"acc\");\nplot_graphs(history, \"loss\");","40c4cea5":"text = \"\u0627\u06af\u0631 \u0622\u067e \u06c1\u06cc\u0644 \u0628\u0648\u0627\u0626\u06d2 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u06a9\u06be\u0648\u0626\u06d2 \u06c1\u0648\u0626\u06d2 \u0635\u0646\u062f\u0648\u0642 \u06a9\u06d2 \u0686\u06be\u0627\u067e\u0648\u06ba \u06a9\u0648 \u0639\u0628\u0648\u0631 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 \u067e\u0631\u0644 \u06c1\u0627\u0631\u0628\u0631 \u0627\u0648\u0631 \u06c1\u062f\u0627\u06cc\u062a \u06a9\u0627\u0631 \u062c\u0648 \u062c\u0627\u0646\u0633\u0679\u0646 \u06a9\u06d2 \u0627\u067e\u0646\u06d2 \u062f\u06cc \u0631\u0648\u06a9\u06cc\u0679\u06cc\u0626\u0631 \u06a9\u0648 \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062a\u0648 \u060c \u0622\u067e \u0627\u0633 \u0641\u0644\u0645 \u06a9\u06cc \u062a\u062e\u0644\u06cc\u0642 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0631\u0627\u0633\u062a\u06d2 \u0645\u06cc\u06ba \u0679\u06be\u06cc\u06a9 \u06c1\u0648\u062c\u0627\u0626\u06cc\u06ba \u06af\u06d2\u06d4 \u06cc\u06c1 \u0627\u0686\u06be\u06cc \u0637\u0631\u062d \u0633\u06d2 \u0686\u0644 \u0631\u06c1\u0627 \u06c1\u06d2 \u060c \u0645\u062d\u0628 \u0648\u0637\u0646 \u062c\u0646\u06af \u0639\u0638\u06cc\u0645 \u062f\u0648\u0645 \u06a9\u06cc \u0645\u06c1\u0645 \u062c\u0648\u0626\u06cc \u0633\u0627\u0626\u0646\u0633 \u0641\u0627\u0626\u06cc \/ \u0641\u0646\u062a\u0627\u0633\u06cc \u062a\u0635\u0648\u0641 \u06a9\u0627 \u0627\u06cc\u06a9 \u06a9\u0645 \u062c\u0645\u06c1\u0648\u0631\u06cc\u06c1 \u0627\u0648\u0631 \u06c1\u06cc\u0631\u0648 \u06c1\u06d2 \u062c\u0633 \u06a9\u06cc \u062c\u0691\u06cc\u06ba \u0627\u06a9\u0679\u06be\u0627 \u06a9\u0631\u0646\u0627 \u0628\u06c1\u062a \u0622\u0633\u0627\u0646 \u06c1\u06d2\u06d4 \u0631\u0627\u062c\u0631\u0632 \/ \u06a9\u06cc\u067e\u0679\u0646 \u0627\u0645\u0631\u06cc\u06a9\u06c1 \u060c \u0628\u06c1\u062a \u0627\u0686\u06be\u0627 \u0622\u062f\u0645\u06cc \u06c1\u06d2 \u060c \u062d\u0642\u06cc\u0642\u062a \u0645\u06cc\u06ba \u060c \u06a9\u06c1 \u0648\u06c1 \u06a9\u0628\u06be\u06cc \u06a9\u0628\u06be\u06cc \u062a\u06be\u0648\u0691\u0627 \u0633\u0627 \u06a9\u0645\u0632\u0648\u0631 \u06c1\u0648\u062a\u0627 \u06c1\u06d2 - \u0648\u06c1 \u06cc\u0642\u06cc\u0646\u0627 a \u0627\u06cc\u06a9 \u0628\u06c1\u062a \u0628\u0691\u0627 \u0631\u0648\u0644 \u0645\u0627\u0688\u0644 \u06c1\u06d2 \u060c \u0644\u06cc\u06a9\u0646 \u0622\u067e \u0627\u067e\u0646\u06d2 \u0622\u067e \u06a9\u0648 \u06cc\u06c1 \u062e\u0648\u0627\u06c1\u0634 \u067e\u0627\u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba \u06a9\u06c1 \u0627\u0633 \u06a9\u06d2 \u067e\u0627\u0633 \u062a\u06be\u0648\u0691\u0627 \u0633\u0627 \u0648\u0644\u0648\u0631\u0627\u0626\u0646 \u06cc\u0627 \u0679\u0648\u0646\u06cc \u0627\u0633\u0679\u0627\u0631\u06a9 \u06a9\u06cc \u0633\u0646\u06cc\u0627\u0631\u06a9 \u062a\u06be\u06cc\u06d4 (\u0641\u0644\u0645 \u0645\u06cc\u06ba \u0645\u062c\u0645\u0648\u0639\u06cc \u0637\u0648\u0631 \u067e\u0631 \u0645\u0632\u0627\u062d \u0633\u06d2 \u062a\u06be\u0648\u0691\u0627 \u0633\u0627 \u0645\u062e\u062a\u0635\u0631 \u06c1\u06d2 \u060c \u062f\u0631\u0627\u0635\u0644 \u060c \u06a9\u06cc\u067e\u0679\u0646 \u0627\u0645\u0631\u06cc\u06a9\u06c1 \u06a9\u06cc \u0631\u06af \u0679\u06cc\u06af \u0622\u0641 \u0633\u067e\u0627\u06c1\u06cc\u0648\u06ba \u06a9\u06cc \u0679\u06cc\u0645 \u0632\u06cc\u0627\u062f\u06c1 \u062a\u0631 \u0642\u06c1\u0642\u06c1\u0648\u06ba \u062f\u06cc\u062a\u06cc \u06c1\u06d2 \u060c \u0644\u06cc\u06a9\u0646 \u0627\u0646 \u0645\u06cc\u06ba \u0627\u062a\u0646\u0627 \u0632\u06cc\u0627\u062f\u06c1 \u0633\u06a9\u0631\u06cc\u0646 \u0679\u0627\u0626\u0645 \u0646\u06c1\u06cc\u06ba \u0645\u0644\u062a\u0627 \u06c1\u06d2\u06d4)\"","b244433d":"text = remove_punctuation(text)\ntext = normalize_whitespace(text)\ntext = normalize(text)","6c20108d":"encoded = tokenizer.encode(text, add_special_tokens=True, max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True)\nencoded = np.asarray([encoded])\npredictions = model.predict(encoded)","3107210c":"print(np.argmax(predictions))","52c1ad8f":"## Vector Visualization"}}