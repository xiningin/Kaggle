{"cell_type":{"8e350ade":"code","e5b1c85f":"code","e5322106":"code","7e85aa8f":"code","01806c99":"code","d1a87b09":"code","d9a57f3f":"code","917f8912":"code","036b397d":"code","b07e3437":"code","219d1325":"code","277c238d":"code","714c9b5c":"code","2de4b8ec":"code","4ea988ca":"code","694b19e1":"code","3d2cbcb3":"code","7d1234b7":"code","b9a51295":"code","f3f17ede":"code","2564de04":"code","a7f7d595":"code","31634252":"code","7e2417a1":"code","6f91dfae":"code","262c8c1f":"code","93898546":"code","a31452f0":"code","2c2fbff6":"code","94671eae":"markdown","a687d90f":"markdown"},"source":{"8e350ade":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5b1c85f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e5322106":"# define column names for easy indexing\nindex_names = ['unit_nr', 'time_cycles']\nsetting_names = ['setting_1', 'setting_2', 'setting_3']\nsensor_names = ['s_{}'.format(i) for i in range(1,22)] \ncol_names = index_names + setting_names + sensor_names\n\n# read data\ntrain = pd.read_csv('train_FD001.txt', sep='\\s+', header=None, names=col_names)\ntest = pd.read_csv('test_FD001.txt', sep='\\s+', header=None, names=col_names)\ny_test = pd.read_csv('RUL_FD001.txt', sep='\\s+', header=None, names=['RUL'])\n\ntrain.head()","7e85aa8f":"test.head()","01806c99":"def add_remaining_useful_life(df):\n    # Get the total number of cycles for each unit\n    grouped_by_unit = df.groupby(by=\"unit_nr\")\n    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n    \n    # Merge the max cycle back into the original frame\n    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n    \n    # Calculate remaining useful life for each row\n    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n    result_frame[\"RUL\"] = remaining_useful_life\n    \n    # drop max_cycle as it's no longer needed\n    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n    return result_frame\n\ntrain = add_remaining_useful_life(train)\ntrain[index_names+['RUL']].head()","d1a87b09":"drop_labels = index_names+setting_names\n#dropping the columns except the sensor datas\nX_train = train.drop(drop_labels, axis=1)\ny_train = X_train.pop('RUL')","d9a57f3f":"# Since the true RUL values for the test set are only provided for the last time cycle of each enginge, the test set is subsetted to represent the same.\n# Reducing from 13095 rows to 100 rows\nX_test = test.groupby('unit_nr').last().reset_index().drop(drop_labels, axis=1)","917f8912":"X_train.shape","036b397d":"from sklearn.model_selection import train_test_split\n\nX_trains, X_val, y_trains, y_val = train_test_split(X_train, y_train, test_size=0.2)","b07e3437":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping","219d1325":"model = keras.Sequential()\nmodel.add(Dense(21, activation='relu', input_shape=(21,)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1))","277c238d":"initial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=100000,\n    decay_rate=0.96,\n    staircase=True)","714c9b5c":"optimizer_A=tf.keras.optimizers.Adam(learning_rate = 0.001)\nmodel.compile(optimizer=optimizer_A,loss='mean_absolute_error',metrics=['accuracy'])","2de4b8ec":"model.summary()","4ea988ca":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=3, min_lr=1e-7, verbose=1)","694b19e1":"history = model.fit(x=X_train,y=y_train,\n                    validation_data = (X_val,y_val),\n                    epochs = 50,\n                    shuffle = True,\n                    callbacks=[reduce_lr])","3d2cbcb3":"import matplotlib.pyplot as plt\nloss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,51)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss for Adam')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","7d1234b7":"import matplotlib.pyplot as plt\nacc_train = history.history['accuracy']\nacc_val = history.history['val_accuracy']\nepochs = range(1,51)\nplt.plot(epochs, acc_train, 'r', label='Training accuracy')\nplt.plot(epochs, acc_val, 'g', label='validation accuracy')\nplt.title('Training and Validation Accuracy for Custom Arch.')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b9a51295":"y_pred = model.predict(X_test)\nprint(y_pred)","f3f17ede":"import sklearn\n\nprint(sklearn.metrics.r2_score(y_test, y_pred))\n#print(sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred))\nprint(sklearn.metrics.mean_absolute_error(y_test, y_pred))","2564de04":"!pip install autokeras","a7f7d595":"from autokeras import StructuredDataRegressor","31634252":"search = StructuredDataRegressor(max_trials=15, loss='mean_absolute_error') #no of trial and errors allowed\nsearch.fit(x=X_train, y=y_train, verbose=1) #fitting the model","7e2417a1":"mae, acc = search.evaluate(X_test, y_test, verbose=1)","6f91dfae":"yhat = search.predict(X_test)","262c8c1f":"print(sklearn.metrics.r2_score(y_test, yhat))\n#print(sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred))\nprint(sklearn.metrics.mean_absolute_error(y_test, yhat))","93898546":"model1 = search.export_model()","a31452f0":"model1.summary()","2c2fbff6":"model1.save('model1.tf') #saving the model","94671eae":"Normal Model","a687d90f":"Neural Architecture using AutoKeras"}}