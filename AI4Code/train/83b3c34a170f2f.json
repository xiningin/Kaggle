{"cell_type":{"7f158744":"code","2fe0f632":"code","cf563785":"code","2e7e005f":"code","ad8ddefa":"code","10623f6b":"code","4f10eaac":"code","dcb64881":"code","3712c8cd":"code","1bd66f50":"code","29e71e47":"code","817e2d6c":"code","810acb10":"code","f1477dab":"code","a1a8f094":"code","ba4070e7":"code","02687507":"code","b9a0473c":"code","fcbb3195":"code","16e9aed5":"code","97b88623":"code","3cb6627f":"code","c33e5e32":"code","90381d6e":"code","439346b4":"code","116f4238":"code","a20a7e03":"code","f36cb5e9":"code","036ce070":"code","78fc2bdd":"code","a3920f4c":"code","e8d23a25":"code","b1550118":"code","4ec08237":"code","f25bf594":"code","836e856c":"code","c7114941":"code","8313aa2d":"code","db431f48":"code","fbc2f146":"code","ee727349":"code","81f3707e":"code","0b5fe454":"code","b658a1d7":"code","0c7b3886":"code","b7086db1":"code","75e26b29":"code","946d2de3":"code","59a9603a":"code","de937010":"markdown","f70b4fc3":"markdown","b737c6a2":"markdown"},"source":{"7f158744":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix\nimport datetime\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.pipeline import Pipeline \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu' )  #kaggle notebook run on gpu(CUDA)\ndevice","2fe0f632":"# train and test data concatenate \ntrain=pd.read_csv('\/kaggle\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/train.csv')\ntrain.columns=train.columns.str.lower() \ntest=pd.read_csv('\/kaggle\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/test.csv')\ntest.columns=test.columns.str.lower() \ndata=pd.concat([train,test],axis=0)\ndata=data.reset_index(drop=True)\ndata.target=data.target.astype('O')\ndata.drop(data.txn_source[data.txn_source=='R'].index,inplace=True) ","cf563785":"data.head()","2e7e005f":"data.isnull().sum()","ad8ddefa":"data.info()","10623f6b":"# using label encoder some columns\n# data is encrypted so that we can better understand it.\n# data is masked\n\nle=LabelEncoder()\ndata[['cst_nr','cc_nr','txn_entry','city','country','mc_name','mc_id','mcc_code']]=data[['cst_nr','cc_nr','txn_entry','city','country','mc_name','mc_id','mcc_code']].apply(lambda x: le.fit_transform(x).astype('O'))\ndata.day_of_week=data.day_of_week.astype('O')\ndata.day_of_month=data.day_of_month.astype('O')\ndata.txn_entry.replace(2,3,inplace=True)\ndata.drop('day_of_week',axis=1,inplace=True)\ndata.drop('mc_id',axis=1,inplace=True)\ndata.drop(['txn_trm'],axis=1,inplace=True)","4f10eaac":"# I am increasing the number of fraud targets in the model by re-inserting my test results from  lstm model , which I went to before and which captured 90% auc value, into the model.\n# result=pd.read_csv('\/kaggle\/input\/545454\/result54 (bestoutputs).csv')\n# dum=list(result[result.Predicted>0.5].Id.values)\n# data.loc[data.id.isin(dum),'day_of_month']=data.loc[data.id.isin(dum),'day_of_month']-11\n# data=pd.concat([data,data[data.id.isin(dum)].fillna(1)])\n# data.drop(['id','txn_trm'],axis=1,inplace=True)","dcb64881":"# from pandas_profiling import ProfileReport\n# profile = ProfileReport(data,title=\"Pandas Profiling Report\", minimal=True)\n# profile.to_widgets()","3712c8cd":"print(data.shape)\nprint(data.target.value_counts())","1bd66f50":"# visualization target value counts\nsns.countplot(x=data.target)","29e71e47":"data.isnull().sum()","817e2d6c":"# I bring it to the appropriate form so that I can apply pd.datetime to the txn_time variable\ndata=pd.concat([data[data.target.notnull()].drop_duplicates(keep='first'),data[data.target.isnull()]])\ndata.txn_time=pd.to_datetime(data.txn_time.map(lambda x : str((int(x)))[:-2]+':'+str((int(x)))[-2:] if (len(str(int(x)))>2) else ('00:00'if x==0.0 else '0:0'+str((int(x)))  )))\n\n\n#I find the number of transactions from the card in 30-minute periods and add it to the data as a separate variable.\ndum1=data.set_index('txn_time',drop=False).groupby([pd.Grouper(freq='30Min'),'cc_nr','day_of_month','txn_source','mc_name']).agg({'txn_amnt': lambda  x: len(x)}).rename(columns={'txn_amnt':'freq'}).reset_index()\ndum1=dum1.groupby(['cc_nr','day_of_month','txn_source','mc_name'])['freq'].sum().reset_index()\n\ndum1.freq=dum1.apply(lambda x : 1 if x.freq >150 else 0 ,axis=1)\ndata=data.merge(dum1,how='left',on=['cc_nr','day_of_month','txn_source','mc_name'])","810acb10":"# txn_amnt divide intervals and tag  \ndata['txn_amnt_new']=data.txn_amnt.map(lambda x:  0   if 0<=x<5 \n                                                   else ( 1 if  5<=x<10 \n                                                       else ( 2  if 10<=x<20 \n                                                           else ( 3  if 20<=x<40 \n                                                                else ( 4  if 40<=x<60 \n                                                                     else ( 5  if 60<=x<80 \n                                                                          else ( 6  if 80<=x<100 \n                                                                               else ( 7  if 100<=x<200 \n                                                                                    else ( 8  if 200<=x<500 \n                                                                                         else ( 9 if 500<=x<1000 \n                                                                                              else ( 10  if 1000<=x<1500\n                                                                                                   else ( 11  if 1500<=x<2000 \n                                                                                                        else ( 12 if 2000<=x<5000 \n                                                                                                             else ( 13  ))))))))))))))\n","f1477dab":"data['txn_time_q']=data.txn_time.map(lambda x :  x.replace(minute=0)  if x.minute<15 else ( x.replace(minute=15) if (15<=x.minute) and (x.minute<30) else (  x.replace(minute=30) if (30<=x.minute) and (x.minute<45)  else x.replace(minute=45)  )))\ndata.txn_time_q=data.txn_time_q.map(lambda x : x.time())\ndata.txn_time_q=data.txn_time_q.map(lambda x :float(datetime.timedelta(hours=x.hour, minutes=x.minute, seconds=x.second).total_seconds())\/60)\ndata['time_q']=data.txn_time_q.map(lambda x :  0 if x<(15*24) else ( 1 if (15*24<=x) & (x<15*48)  else ( 2 if (15*48<=x) & (x<15*72) else 3) ))","a1a8f094":"dum=data.groupby(['mcc_code','time_q']).agg({'mcc_code':'count','target':'sum'})\ndum['rate_time_mcc']=dum.target\/dum.mcc_code\ndum.drop(['mcc_code','target'],axis=1,inplace=True)\ndum=dum.reset_index()\ndata=data.merge(dum,on=['mcc_code','time_q'])","ba4070e7":"#not time converted to minutes  \nfig,ax=plt.subplots(2,2,figsize=(15,10))\nsns.histplot(x=data[data.target==1].txn_time,bins=40,ax=ax[0,0],kde=True).set_title('fraud');\nsns.histplot(x=data[data.target==0].txn_time,bins=40,ax=ax[0,1],kde=True).set_title('non-fraud');\nsns.countplot(x=data.txn_source,ax=ax[1,1])\nsns.countplot(x=data.txn_entry,ax=ax[1,0])","02687507":"plt.figure(figsize=(10,5))\nfig,ax=plt.subplots(2,2,figsize=(15,10))\ndum=data.groupby('txn_time').agg({'txn_amnt':'mean'})\nsns.lineplot(x=dum.index,y=dum.txn_amnt,ax=ax[0,1])\ndum=data.groupby('day_of_month').agg({'txn_amnt':'mean'})\nsns.lineplot(x=dum.index,y=dum.txn_amnt,ax=ax[0,0])\nsns.countplot(x=data.txn_amnt_new,ax=ax[1,0])\nsns.countplot(x=data.txn_entry,hue=data.target,ax=ax[1,1])","b9a0473c":"# divided the time in 15 minute periods and then wrote it in minutes\n","fcbb3195":"# log10 transformation for txn_amnt\n# data.txn_amnt=np.log1p(data.txn_amnt)\n# data.txn_amnt=np.exp(data.txn_amnt)","16e9aed5":"# rate_cst_trgt\n# Fraud rates by cst_nr\ndum=data[data.target.notnull()].groupby(['cst_nr']).agg({'cst_nr':'count','target':'sum'})  \ndum['rate_cst_trgt']=(dum.target\/dum.cst_nr)  #brings the rate\ndata=data.merge(dum.drop(['cst_nr','target'],axis=1),how='left',on=['cst_nr'])\ndata.rate_cst_trgt=data.apply(lambda x: 0 if pd.isna(x.rate_cst_trgt) else x.rate_cst_trgt ,axis=1)# null with some row i fill those rows with 0\ndata.rate_cst_trgt=data.apply(lambda x: 1 if x.rate_cst_trgt>0.3 else 0 ,axis=1) #If the ratio is higher than 0.4, I throw a value of 0 if it is lower.\n\n# rate_cc_trgt\n# Fraud rates by cc_nr\ndum=data[data.target.notnull()].groupby(['cc_nr']).agg({'cc_nr':'count','target':'sum'})\ndum['rate_cc_nr_trgt']=(dum.target\/dum.cc_nr) #brings the rate\ndata=data.merge(dum.drop(['cc_nr','target'],axis=1),how='left',on=['cc_nr'])\ndata.rate_cc_nr_trgt=data.apply(lambda x: 0 if pd.isna(x.rate_cc_nr_trgt) else x.rate_cc_nr_trgt ,axis=1) #null with some row i fill those rows with 0\ndata.rate_cc_nr_trgt=data.apply(lambda x: 1 if x.rate_cc_nr_trgt>0.3 else 0 ,axis=1) #If the ratio is higher than 0.4, I throw a value of 0 if it is lower.\n\n#I write the time values in the txn_time variable in minutes, for example, 1080th minute of the day = 18:00\ndata.txn_time=data.txn_time.map(lambda x : x.time())      \ndata.txn_time=data.txn_time.map(lambda x :float(datetime.timedelta(hours=x.hour, minutes=x.minute, seconds=x.second).total_seconds())\/60)","97b88623":"# for a credit card I assigned the time difference between transactions during the day as a variable\ndum2=data.groupby(['cst_nr','cc_nr','day_of_month','txn_source','txn_entry','city','mc_name','mcc_code']).agg({'txn_time':lambda x: sorted([i  for i in x])}).rename(columns={'txn_time':'freq_time'})\ndum2.freq_time=dum2.freq_time.map(lambda x :  np.append(0,np.diff(sorted(x))) if len(x)>1 else 'null' )\ndum2=dum2.explode('freq_time').reset_index()\n# ------------------------------------------------------------------------------------------------\n\ndata=data.sort_values(['cst_nr','cc_nr','day_of_month','txn_source','txn_entry','city','mc_name','mcc_code']).reset_index(drop=True)\ndata['freq_t']=dum2.freq_time\n\ndata=pd.concat([data[data.id.isnull()],data[data.id.notnull()].sort_values('id')],axis=0).reset_index(drop=True)\ndata.drop('id',axis=1,inplace=True)\ndata.freq_t=data.freq_t.replace('null',0)\ndata.drop('day_of_month',axis=1,inplace=True)","3cb6627f":"data","c33e5e32":"data_dum=data.iloc[:,:-1].copy()\n# NLP-like data preprocessing\n# Since there are too many categorical variables in our data and too many labels within the categorical variables,\n# we make the lstm model by thinking the model as a word as if we were doing sentiment analysis.\nfor i in data_dum.columns[1:]:\n    data_dum[i]=data_dum[i].map(lambda x : str(x)+'_'+i )\ndata2=data_dum.drop('target',axis=1)\n# ----------------------------------------------------------------------------------------------------------------------------------------------\ndata3=data2.apply(lambda x :  x.values,axis=1)\ntoken_list=np.concatenate(data2.values)\ncorpus=Counter(token_list)     \ncorpus_=sorted(corpus,key=corpus.get,reverse=True)\none_hot_dict={w:i+1 for i,w in enumerate(corpus_)}\n# ----------------------------------------------------------------------------------------------------------------------------------------------\ndum=pd.Series(one_hot_dict)\ndata4=pd.DataFrame(dum.loc[np.concatenate(data3.values)].values.reshape(data2.shape))\ndata4=pd.DataFrame(data4.apply(lambda x: x.values,axis=1))\ndata4['target']=data.target","90381d6e":"data4","439346b4":"class fraudLSTM(nn.Module):\n    def __init__(self,batch_size,hidden_size,vocab_size,embed_size,p,pad):\n        super(fraudLSTM,self).__init__()\n        \n        self.hidden=hidden_size\n        self.embed=nn.Embedding(vocab_size,embed_size)\n        self.embed_size=embed_size\n        self.num_layers=2\n        self.p=p\n        self.pad=pad\n        self.lstm=nn.LSTM(input_size=self.embed_size,\n                         hidden_size=self.hidden,\n                         num_layers=self.num_layers,\n                         batch_first=True)\n        \n\n        \n        \n        self.linear=nn.Linear(self.hidden,1)\n#         # self.linear2=nn.Linear(512,128)\n#         # self.linear3=nn.Linear(128,1)\n\n        \n        self.drop=nn.Dropout(self.p)\n        self.sigmoid=nn.Sigmoid()\n        self.relu=nn.ReLU()\n    \n\n    \n    def forward(self,x,hidden):\n        \n        batch=x.shape[0]\n        x=x.view(batch,-1)\n        x=self.embed(x)\n        x,hidden=self.lstm(x)\n        x=x.contiguous().view(-1, self.hidden)\n        x=self.drop(x)\n        x=self.relu(x)\n        x=self.linear(x)\n#         x=self.drop(x)\n#         x=self.relu(x)\n#         x=self.linear2(x) \n#         x=self.drop(x)\n#         x=self.relu(x)\n#         x=self.linear3(x) \n\n\n        x=self.sigmoid(x)\n        out=x.view(batch,self.pad, -1)   \n        return out[:,-1],h\n    \n    def init_hidden(self,b):\n\n        h0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        c0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        hidden = (h0,c0)\n        return hidden \n\n","116f4238":"# clip = 5\nepochs=14\nlr=0.001\nhidden_size=512\nvocab_size=len(one_hot_dict)+1\nembed_size=400\np=0.5\nbatch_size=1024\n\n\n\n\npad=data2.shape[1]\nmodel=fraudLSTM(batch_size,hidden_size,vocab_size,embed_size,p,pad).to(device)\ncriterion=nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=0.00001)\nimport torch.optim as optim\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\ntarget=data4[data4.target.notnull()].target.astype('float')\nx_train,x_valid,y_train,y_valid=train_test_split(torch.Tensor(data4[data4.target.notnull()][0]), torch.Tensor(target),test_size=0.05,random_state=4)\n\n\ntrain_data = TensorDataset(x_train, y_train)\nvalid_data = TensorDataset(x_valid, y_valid)\n\ntrain_loader=DataLoader(dataset=train_data,\n                        shuffle=False,\n                        batch_size=batch_size)\n\nvalid_loader=DataLoader(dataset=valid_data,\n                        shuffle=False,\n                        batch_size=batch_size)\n\n\n\nvalid_loss_min = np.Inf\ntrain_losses=[]\nvalid_losses=[]\n\nfor epoch in range(epochs):\n    train_loss=0\n    train_pred=[]\n    train_pred_prob=[]\n\n    labels=[]\n    model.train()\n    h = model.init_hidden(batch_size)\n\n    for e,(inputs,label) in enumerate(train_loader):\n        \n        inputs=inputs.type(torch.LongTensor).to(device)\n        label=label.to(device).reshape(-1,1)\n        h = tuple([each.data for each in h])\n\n        pred,h=model(inputs,h)\n        loss=criterion(pred,label)\n        train_loss+=loss.item()\n        \n        model.zero_grad()\n        loss.backward()\n        labels.append(np.array(label.cpu()))\n        train_pred_prob.append(pred)\n        train_pred.append(np.array([ 1 if i >0.5 else 0 for i in (pred)]))\n        optimizer.step()\n\n    train_losses.append(train_loss\/e)    \n    roc_auc_score_train=roc_auc_score(np.concatenate(labels),np.concatenate(train_pred))    \n\n    model.eval() \n    valid_loss=0\n    valid_preds=[]\n    valid_preds_prob=[]\n\n    valid_labels=[]\n    \n    for t,(input_valid,label_valid) in enumerate(valid_loader):\n        \n        input_valid=input_valid.type(torch.LongTensor).to(device)\n        label_valid=label_valid.to(device).reshape(-1,1)\n        h = tuple([each.data for each in h])\n        pred_valid,h=model(input_valid,h)\n\n        loss_valid=criterion(pred_valid,label_valid)\n        valid_loss+=loss_valid.item()\n        \n        valid_labels.append(np.array(label_valid.cpu()))\n        valid_preds_prob.append(pred_valid)\n        valid_preds.append(np.array([ 1 if i >0.5 else 0 for i in (pred_valid)]))\n        \n    \n    valid_losses.append(valid_loss\/t)\n    roc_auc_score_valid=roc_auc_score(np.concatenate(valid_labels),np.concatenate(valid_preds))\n    \n    if (valid_loss\/t) <= valid_loss_min:\n        torch.save(model.state_dict(), '\/kaggle\/working\/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,(valid_loss\/t)))\n        valid_loss_min = (valid_loss\/t)\n\n    print('epochs {} Train_roc_auc_score {:.4f},train error {:.4f} , valid_roc_auc_score {:.4f} valid_error {:.4f}'.format(epoch,roc_auc_score_train,(train_loss\/e),roc_auc_score_valid,(valid_loss\/t)))","a20a7e03":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()","f36cb5e9":"#lstm_outputs result from lstm\nmodel.load_state_dict(torch.load( '\/kaggle\/working\/state_dict.pt'))\nlstm_submission_preds=[] \n\nh = model.init_hidden(batch_size)\nfor seq in torch.Tensor(data4[0])[-len(data4[data4.target.isnull()]):]:\n    model.eval()\n    with torch.no_grad():\n        h = tuple([each.data for each in h])\n        output,h= model(seq.type(torch.LongTensor).reshape(1,-1).to(device),h)\n        lstm_submission_preds.append(output.detach().cpu().item())\n(np.array(lstm_submission_preds)>0.5).sum()  ","036ce070":"# lstm_submission_preds save\nsubmission=pd.read_csv('\/kaggle\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/sample_submission.csv')\nsubmission['Predicted']=np.array(lstm_submission_preds)\nsubmission.iloc[:,:2].set_index('Id').to_csv('\/kaggle\/working\/result84.csv')","78fc2bdd":"cm = confusion_matrix( np.concatenate(valid_preds).reshape(-1,1),np.concatenate(valid_labels))\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0','Actual Positive:1'], \n                                 index=[ 'Predict Negative:0','Predict Positive:1'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","a3920f4c":"# target=data4[data4.target.notnull()].target.astype('float')\n# all_data=TensorDataset(torch.Tensor(data4[data4.target.notnull()][0]))\n# all_loader=DataLoader(dataset=all_data,\n#                         shuffle=False,\n#                         batch_size=batch_size)\n# all_results=[]\n\n# for all_input in all_loader:\n#     model.eval()\n#     with torch.no_grad():\n#         all_input=all_input.type(torch.LongTensor).to(device)\n#         h = tuple([each.data for each in h])\n#         all_pred,h=model(all_input,h)\n#         all_results.append(np.array(all_pred.cpu()))","e8d23a25":"data_copy=data.copy()\n# data=data_copy.copy()\ndata","b1550118":"#rate_mcc_code_trgt\ndum=data[data.target.notnull()].groupby(['mcc_code']).agg({'mcc_code':'count','target':'sum'})\ndum['rate_mcc_code_trgt']=(dum.target\/dum.mcc_code)#brings the rate\ndata=data.merge(dum.drop(['mcc_code','target'],axis=1),how='left',on=['mcc_code'])\ndata.rate_mcc_code_trgt=data.apply(lambda x: 0 if pd.isna(x.rate_mcc_code_trgt) else x.rate_mcc_code_trgt ,axis=1)\n\n#rate_txn_amnt_new\ndum=data[data.target.notnull()].groupby(['txn_amnt_new']).agg({'txn_amnt_new':'count','target':'sum'})\ndum['rate_txn_amnt_new']=(dum.target\/dum.txn_amnt_new)\ndata=data.merge(dum.drop(['txn_amnt_new','target'],axis=1),how='left',on=['txn_amnt_new'])\ndata.rate_txn_amnt_new=data.apply(lambda x: 0 if pd.isna(x.rate_txn_amnt_new) else x.rate_txn_amnt_new ,axis=1)\n#rate_mc_name_trgt\ndum=data[data.target.notnull()].groupby(['mc_name']).agg({'mc_name':'count','target':'sum'})\ndum['rate_mc_name_trgt']=(dum.target\/dum.mc_name)\ndata=data.merge(dum.drop(['mc_name','target'],axis=1),how='left',on=['mc_name'])\ndata.rate_mc_name_trgt=data.apply(lambda x: 0 if pd.isna(x.rate_mc_name_trgt) else x.rate_mc_name_trgt ,axis=1)\n#rate_city_trgt\ndum=data[data.target.notnull()].groupby(['city']).agg({'city':'count','target':'sum'})\ndum['rate_city_trgt']=(dum.target\/dum.city)\ndata=data.merge(dum.drop(['city','target'],axis=1),how='left',on=['city'])\ndata.rate_city_trgt=data.apply(lambda x: 0 if pd.isna(x.rate_city_trgt) else x.rate_city_trgt ,axis=1)","4ec08237":"# clustering for mc_name\nfrom sklearn.cluster import KMeans\ndff=data[['txn_time','txn_amnt','mc_name']]\ndff.set_index(dff.mc_name,inplace=True)\ndff.drop('mc_name',axis=1,inplace=True)\nKmean = KMeans(n_clusters=200)\nKmean.fit(dff)\ndff['mc_name_label']=Kmean.labels_\ndff=dff.reset_index()\ndff=dff[['mc_name_label','mc_name']]\ndata=pd.concat([data,dff.set_index(data.index)['mc_name_label']],axis=1)\ndata.mc_name_label=data.mc_name_label.astype('O')","f25bf594":"data","836e856c":"data.drop(['cst_nr','cc_nr','city','mc_name'],axis=1,inplace=True)\n# out=pd.read_csv('\/kaggle\/input\/outputsall\/outputs_all.csv')['0']\n# data['pred']=out\n# data['pred']=np.append(np.concatenate(all_results),lstm_submission_preds)\ndata.txn_entry=data.txn_entry.astype('O')\ndata.txn_amnt_new=data.txn_amnt_new.astype('O')\ndata.rate_cst_trgt=data.rate_cst_trgt.astype('O')\ndata.rate_cc_nr_trgt=data.rate_cc_nr_trgt.astype('O')\ndata.txn_time=data.txn_time.astype('int')","c7114941":"data.info()","8313aa2d":"target=data[data.target.notnull()].target.values\ndata.drop('target',axis=1,inplace=True)\ndata_dum=pd.get_dummies(data)\ndata_dum","db431f48":"#parameters tuning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score,accuracy_score,log_loss\nimport optuna\nimport xgboost as xgb\nsc=StandardScaler()\n# data_dum=sc.fit_transform(data_dum)\ndata_end=data_dum[:len(target)]\nx_train,x_valid,y_train,y_valid=train_test_split(data_end,target,test_size=0.20,random_state=120)","fbc2f146":"import optuna\nfrom sklearn import metrics\ndef objective(trial,data=data_end,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data_end, target, test_size=0.20,random_state=4)\n    param = {'use_label_encoder':False,            \n              'eval_metric':'logloss',\n#             'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n             'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n             'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n             'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n             'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n             'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n             'n_estimators': 10,\n             'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n             'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = xgb.XGBClassifier(**param)  \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    auc = roc_auc_score(test_y.astype('int'), preds)\n    \n    return auc\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\ngc.collect()","ee727349":"Best trial: {'lambda': 0.041208786089502875, 'alpha': 0.007881292771586742, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.009, 'max_depth': 20, 'random_state': 24, 'min_child_weight': 250}","81f3707e":"best_params=study.best_trial.params\nbest_params.update({'use_label_encoder': False,\n                    'n_estimators': 5,\n                    'objective':'binary:logistic',\n                    'eval_metric':'logloss'})\n\nif torch.cuda.is_available()==True :\n    best_params.update({'tree_method':'gpu_hist'})\nbest_params","0b5fe454":"# sorted(metrics.SCORERS.keys())","b658a1d7":"# Validation\nfrom sklearn import model_selection  \ncv=model_selection.KFold(n_splits=10,shuffle=True,random_state=120) \nxgb_tuned=xgb.XGBClassifier(**best_params )\nval_auc_score=(model_selection.cross_validate(xgb_tuned,data_end,\n                                              target.astype('int'),\n                                              cv=cv,\n                                              scoring='roc_auc'))['test_score'].mean()\n\nprint('val_auc_score: ',val_auc_score)\n    ","0c7b3886":"# XGB model predicts\nxgb_model = xgb.XGBClassifier(\n                            **best_params\n                             ).fit(x_train,y_train,eval_set=[(x_valid,y_valid)],early_stopping_rounds=100,verbose=False)\n\n\nval_pred=xgb_model.predict(x_valid)\nprint('roc_auc_score: ',roc_auc_score(y_valid.astype('int'), val_pred))\nxgb_submission_pred=xgb_model.predict_proba(data_dum[len(target):])[:,1]\nprint(np.array([1 if i>0.5 else 0 for i in xgb_submission_pred]).sum())","b7086db1":"# lstm_submission_preds save\n# submission=pd.read_csv('\/kaggle\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/sample_submission.csv')\n# submission['Predicted']=xgb_pred\n# submission.iloc[:,:2].set_index('Id').to_csv('\/kaggle\/working\/result80.csv')","75e26b29":"cm = confusion_matrix(xgb_model.predict(x_valid),y_valid.astype('int'))\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative:0','Actual Positive:1'], \n                                 index=[ 'Predict Negative:0','Predict Positive:1'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","946d2de3":"# Simple mean of LSTM vs XGBOOSt model predictions\ndum=pd.DataFrame({'xgb_pred':xgb_submission_pred,\n            'lstm_pred':lstm_submission_preds})\nsubmits=dum.mean(axis=1)  \nsubmission=pd.read_csv('\/kaggle\/input\/turkiye-is-bankasi-machine-learning-challenge-4\/MLChallenge4\/sample_submission.csv')\nsubmission['Predicted']=np.array(submits)\nsubmission.iloc[:,:2].set_index('Id').to_csv('\/kaggle\/working\/result83.csv')","59a9603a":"submission","de937010":"# Data preprocessing ","f70b4fc3":"# NOW LSTM MODEL T\u0130ME !!!\n","b737c6a2":"# XGBOOST MODEL"}}