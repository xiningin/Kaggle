{"cell_type":{"3bf45fc8":"code","0b89c07a":"code","ffabd474":"code","40fc231f":"code","53e42dfa":"code","9e10e80e":"code","4f623539":"code","ddba56c8":"code","e352e573":"code","a76918e4":"code","405caee3":"code","7c06206d":"code","705ecfe4":"code","f6ac7378":"code","f450a602":"code","3eb22a9e":"code","71a6b919":"code","00c3e30d":"code","c5c7fe68":"code","4a5d5958":"code","6477a1bd":"markdown","a47264ce":"markdown","72a0acad":"markdown","faa3161a":"markdown","f4159d8b":"markdown","4d500d10":"markdown","058be836":"markdown"},"source":{"3bf45fc8":"# import necessary libraries\nimport os\nimport re\nimport pandas as pd \nimport numpy as np\nfrom datetime import timedelta\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport geopy.distance\nfrom pprint import pprint\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nimport string\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")\nfrom wordcloud import WordCloud\nfrom collections import Counter","0b89c07a":"# Data Cleaning Function\nporter = PorterStemmer()\n\n# add the word size to the stopword\ndef get_clean_text(x):\n    \n    x = x.lower()\n    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n    x = x.encode('ascii', 'ignore').decode()\n    x = re.sub(r'https*\\S+', ' ', x)\n    x = re.sub(r'@\\S+', ' ', x)\n    x = re.sub(r'#\\S+', ' ', x)\n    x = re.sub(r'\\'\\w+', '', x)\n    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    x = re.sub(r'\\s{2,}', ' ', x)\n\n    x = ' '.join([porter.stem(word) for word in x.split(' ')])\n    return x","ffabd474":"# Function to plot ngrams\ndef plot_ngrams(text, n=2, topk=15):\n\n    text = get_clean_text(text)\n    tokens = text.split()\n    \n    # get the ngrams \n    ngram_phrases = ngrams(tokens, n)\n    \n    # Get the most common ones \n    most_common = Counter(ngram_phrases).most_common(topk)\n    \n    # Make word and count lists \n    words, counts = [], []\n    for phrase, count in most_common:\n        word = ' '.join(phrase)\n        words.append(word)\n        counts.append(count)\n    \n    # Plot the barplot \n    plt.figure(figsize=(10, 6))\n    title = \"Most Common \" + str(n) + \"-grams in the text\"\n    plt.title(title)\n    ax = plt.bar(words, counts)\n    plt.xlabel(\"n-grams found in the text\")\n    plt.ylabel(\"Ngram frequencies\")\n    plt.xticks(rotation=90)\n    plt.show()","40fc231f":"# Function to collect all the text \ndef get_text(df, column):\n    text = ''\n    \n    # iterate through the data \n    for i in range(df.shape[0]):\n        text += df.iloc[i][column] \n        text += ' '\n    \n    return text","53e42dfa":"# Read the data \ndata = pd.read_csv('..\/input\/nigeria-newsfeed\/Nigeria2019_Newsfeed.csv')\ndata.shape","9e10e80e":"data.head()","4f623539":"# Check columns \ndata.columns","ddba56c8":"data['Newsfeed_IncidentType'].unique()","e352e573":"# Collect all the text \nall_short_description = get_text(data, 'Newsfeed_Description')\nall_long_description = get_text(data, 'Newsfeed_Description2')\nlen(all_short_description), len(all_long_description)","a76918e4":"# Clean the short description \nclean_short_text = get_clean_text(all_short_description)","405caee3":"clean_short_text","7c06206d":"# Let's make a word cloud to see the most popular words in the text \nword_cloud = WordCloud(collocations=False, \n                       width=600, \n                       height=600,\n                       background_color = 'white').generate(clean_short_text)\n\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","705ecfe4":"# Let's look at the most common words \nplot_ngrams(clean_short_text, n=2, topk=15)","f6ac7378":"# Let's look at the most common words \nplot_ngrams(clean_short_text, n=3, topk=15)","f450a602":"# Drop the null values in the incident type column \ndata_copy = data.copy()\ndata.dropna(subset=['Newsfeed_IncidentType'], inplace=True)","3eb22a9e":"# Assigning and individual id to each event type\ndata['incident_id'] = data['Newsfeed_IncidentType'].factorize()[0]\nincident_category = data[['Newsfeed_IncidentType', 'incident_id']].drop_duplicates().sort_values('incident_id')\nincident_category","71a6b919":"# Convert into a label dictionary \ncategory_labels = dict(incident_category.values)\nprint(category_labels)\n\nprint(\"=======\"*15) # Line break display\n\n# Similarly, we can create an inverse of the previous one to convert labels to categories\ncategory_reverse = dict(incident_category[['incident_id', 'Newsfeed_IncidentType']].values)\nprint(category_reverse)","00c3e30d":"# Apply the clean text function on the the descriptions\ndata['clean_desc'] = data['Newsfeed_Description'].apply(get_clean_text)\ndata['clean_desc']","c5c7fe68":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Creating an instance of the Tfidf vectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, \n                        min_df=5, \n                        norm = 'l2', \n                        encoding='latin-1', \n                        ngram_range=(1, 2))\n\n\n# Extracting the features by fitting the Vectorizer on Combined Data\nfeat = tfidf.fit_transform(data['clean_desc']).toarray()\nlabels = data['incident_id']    # Series containing all the post labels\nprint(feat.shape)","4a5d5958":"from sklearn.feature_selection import chi2\n\n# chisq2 statistical test\nN = 5    # Number of examples to be listed\nfor f, i in sorted(category_labels.items()):\n    chi2_feat = chi2(feat, labels == i)\n    indices = np.argsort(chi2_feat[0])\n    feat_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [w for w in feat_names if len(w.split(' ')) == 1]\n    bigrams = [w for w in feat_names if len(w.split(' ')) == 2]\n    print(\"\\nDescription Category '{}':\".format(f))\n    print(\"Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))\n    print(\"Most correlated bigrams:\\n\\t. {}\".format('\\n\\t. '.join(bigrams[-N:])))","6477a1bd":"### Check the most correlated words","a47264ce":"# Newsfeed Data Analysis\n\nWhat trends, patterns, and information can you find when analyzing the Newsfeed data-frame provided? How did you analyze the data? What was your methodology? What would you suggest pursuing if you had more time (e.g. a week or two weeks).","72a0acad":"The long description has a lot of text and would require more time for processing so I will use the short description at the moment. ","faa3161a":"We can identify the various words that are correlated with different types of incidents. ","f4159d8b":"## Data Analysis","4d500d10":"### Analysis of the Newsfeed description","058be836":"## Utility Functions \n\nThese are the functions needed for text analysis."}}