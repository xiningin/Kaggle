{"cell_type":{"b931a053":"code","07f0a830":"code","97424bce":"code","701fb1cf":"code","9dcdfb80":"code","5b3166aa":"code","fd6442d1":"code","028c3e7a":"code","2bd60e58":"code","66f00bbd":"code","2b5f94a2":"code","8499993a":"code","ca2e3725":"code","339d6564":"code","f17535f4":"code","6b42711b":"code","aec965f7":"code","46a45d26":"code","cfcc9548":"markdown","33be8570":"markdown","a30bf07d":"markdown","17df7360":"markdown","5b9d2197":"markdown","4898e42e":"markdown","74d7f094":"markdown","7b3d8679":"markdown","83d4a023":"markdown","0ada2f9b":"markdown"},"source":{"b931a053":"# First we will import the important libraries that we will need in order to run our code\nimport numpy as np \nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# So running this will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","07f0a830":"#load train dataframe\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","97424bce":"#load test dataframe\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","701fb1cf":"#Get a general overview of the dataframe\ntrain_data.describe()","9dcdfb80":"#Knowing that 1 means survived and 0 means deceased, we can see the percentage of survival through:\nsurvived = train_data[(train_data.Survived == 1)].count()\ndied = train_data[(train_data.Survived == 0)].count()\nprint(\"The chances of survival is\", (survived[1]\/(survived[1]+died[1]))*100, \"%\")","5b3166aa":"#Count the number of occurrences of men and women, respectively, using value_counts\ntrain_data['Sex'].value_counts() ","fd6442d1":"#Calculate the percentage of women that survived\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"Of those 314 women\", rate_women, \"% survived\")","028c3e7a":"#Calculate the percentage of men that survived\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"Of these 577 men\", rate_men, \"% survived\")","2bd60e58":"#Adding the mean Age to the dataframe, where no Age is present for both dataframes\nm_age_train = train_data[\"Age\"].mean() #finding mean age\ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(m_age_train) #fill all the places where there is no value, using fillna, with the mean age\n\nm_age_test = test_data[\"Age\"].mean() #finding mean age\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(m_age_test) #fill all the places where there is no value, using fillna, with the mean age","66f00bbd":"#Adding the mean Fare to the dataframe, where no Fare is present for both dataframes\nm_fare_train = train_data[\"Fare\"].mean() #finding mean fare price\ntrain_data[\"Fare\"] = train_data[\"Fare\"].fillna(m_fare_train) #fill all the places where there is no value, using fillna, with the mean price\n\nm_fare_test = test_data[\"Fare\"].mean() #finding mean fare price\ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(m_fare_test) #fill all the places where there is no value, using fillna, with the mean price","2b5f94a2":"#Adding the frequent Embarked class to the dataframe, where no Embarked is present for both dataframes\ne_most_train = train_data[\"Embarked\"].describe()\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(e_most_train[2]) #as the most common is placed at the second position, we simply add 2 instead of adding the actual letter.\n\ne_most_test = test_data[\"Embarked\"].describe()\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].fillna(e_most_test[2]) #as the most common is placed at the second position, we simply add 2 instead of adding the actual letter.","8499993a":"#Transform Sex to categorical for both dataframes\ntrain_data[\"Sex_cat\"] = train_data[\"Sex\"].astype(\"category\")\ntest_data[\"Sex_cat\"] = test_data[\"Sex\"].astype(\"category\")\n\n#Transform Embarked to categorical for both dataframes\ntrain_data[\"Embarked_cat\"] = train_data[\"Embarked\"].astype(\"category\")\ntest_data[\"Embarked_cat\"] = test_data[\"Embarked\"].astype(\"category\")","ca2e3725":"#Change the Sex categorical to codes for both dataframes\ntrain_data[\"Sex_codes\"] = train_data[\"Sex_cat\"].cat.codes\ntest_data[\"Sex_codes\"] = test_data[\"Sex_cat\"].cat.codes\n\n#Change the Embarked categorical to codes for both dataframes\ntrain_data[\"Embarked_codes\"] = train_data[\"Embarked_cat\"].cat.codes\ntest_data[\"Embarked_codes\"] = test_data[\"Embarked_cat\"].cat.codes","339d6564":"#Adding new feature Family_size_total from combining SibSp and Parch\ntrain_data[\"Family_size_total\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"Family_size_total\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","f17535f4":"#Family size to groups sorted by number of total familymembers.\ntrain_data[\"Family_size_total\"] = train_data[\"Family_size_total\"].astype(int) #changing them to integers for both dataframes\ntest_data[\"Family_size_total\"] = test_data[\"Family_size_total\"].astype(int)\ndef family_range(df): #if the amount of familty_size_total is \n    df[\"Family_size_total\"].loc[df[\"Family_size_total\"] <= 1 ] = 0 # <= 1, give it index 0\n    df[\"Family_size_total\"].loc[(df[\"Family_size_total\"] >= 2) & (df[\"Family_size_total\"] <= 4)] = 1 # <= >= 2 but <= 4, give it index 1\n    df[\"Family_size_total\"].loc[df[\"Family_size_total\"] >= 5] = 2 # >= 5, give it index 2","6b42711b":"#See what colums our new dataframe's contain\ntrain_data.head()","aec965f7":"# Get our y (only for train - Kaggle doesn't give us the test target)\ny_train = train_data[\"Survived\"]\n\n# Get our X (train and test)\nfeatures = [\"Pclass\", \"Sex_codes\", \"SibSp\", \"Parch\", \"Embarked_codes\", \"Age\", \"Fare\", \"Family_size_total\"]\nX_test = test_data[features].copy()\nX_train = train_data[features].copy()\n\n#Final prediction\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train) #fit our train data to our model\n\n#Get predictions\npredictions = model.predict(X_test)\n\n# Write to a file (for submission to Kaggle)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","46a45d26":"output","cfcc9548":"We can see that onboard the Titanic, there were 577 men and 314 women.","33be8570":"The code above calculates the percentage of female passengers who survived.","a30bf07d":"# Importing data\n\nFirst we will upload our train and test dataset, in order to create our prediction. \nIt's important to have a train and test set, in order to see if the predicted values in the train set corresponds with the test set.","17df7360":"Our prediction then looks like the following:","5b9d2197":"# Final predictions\n\nIn order to predict who died and who survived, we will build what's known as a random forest model. This model is constructed of several \"trees\" that will individually consider each passenger's data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in the four different columns that we specify in our features (Pclass\", \"Sex_codes\", \"SibSp\", \"Parch\", \"Embarked_codes\", \"Age\", \"Fare\" and \"Family_size_total) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file my_submission.csv.","4898e42e":"# Overview of data\n\nWe will loosely look at some different parameters within our dataframe","74d7f094":"This gives us a general overview of eg. mean and standard deviation across our datasets. Next we will look at percentage of survival and the seperation of men and women onboard. ","7b3d8679":"# Portfolio 1b - Titanic survival\n\nMaria Louise Hougesen (WZH513) & Bodil Djern\u00e6s Henriksen (ZCB736)\n\nIn the following portfolio, we will build a Machine Learning model as to predict the possibility of survival pr person onboard the Titanic. As with Machine Learning, we will first import the data, then look at different relationships, clean the dataframe and lastly build our model based on different variables. ","83d4a023":"# Cleaning and creating variables\n\nAs our goal with this portfolio is to predict who died and who survived the sinking of Titanic, we will start working with our dataframe. We wish to create our predictions based on more than a single column. So by considering multiple columns, we will be able to predict more complex patterns that can potentially yield us a better-informed prediction. \nSo in order to do this, we will fill certain values that we wish to use for our prediction ","0ada2f9b":"The code above calculates the percentage of male passengers who survived.\n\nFrom this we can see that almost 75% of the women on board survived, whereas only 19% of the men did, which is consistent with the time given that they evacuated women and children first."}}