{"cell_type":{"0eaaa9a1":"code","f5ba1061":"code","276b3045":"code","5201a4eb":"code","0e466879":"code","ee1382dc":"code","3c3fa2ef":"code","0f20ce02":"code","3ef8752f":"code","04a8a00f":"code","309403f6":"code","e2da854c":"code","057e4cdb":"code","119e6cb4":"code","bbce342d":"code","9e742f6b":"code","99512d4e":"code","4e723f1f":"code","7f3e2320":"markdown","1e6753a4":"markdown","af4a392b":"markdown","5cd32531":"markdown","b5e39afd":"markdown","f75dc0dc":"markdown","20f72563":"markdown","2eed60ce":"markdown","1aac3c60":"markdown","950424be":"markdown","ca5a4992":"markdown","4b8c3b95":"markdown","e4041d8f":"markdown","3093f11b":"markdown","c4db144f":"markdown","6eaf87e4":"markdown","d27124b6":"markdown","002e7f2d":"markdown"},"source":{"0eaaa9a1":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\nknn = KNeighborsClassifier(n_neighbors=4)","f5ba1061":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='accuracy',\n           cv=0)\n\nsfs1 = sfs1.fit(X, y)","276b3045":"sfs1.subsets_","5201a4eb":"feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width')\nsfs1 = sfs1.fit(X, y, custom_feature_names=feature_names)\nsfs1.subsets_","0e466879":"sfs1.k_feature_idx_","ee1382dc":"sfs1.k_feature_names_","3c3fa2ef":"sfs1.k_score_","0f20ce02":"# Sequential Forward Selection\nsfs = SFS(knn, \n          k_features=3, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          cv=4,\n          n_jobs=-1)\nsfs = sfs.fit(X, y)\n\nprint('\\nSequential Forward Selection (k=3):')\nprint(sfs.k_feature_idx_)\nprint('CV Score:')\nprint(sfs.k_score_)\n\n###################################################\n\n# Sequential Backward Selection\nsbs = SFS(knn, \n          k_features=3, \n          forward=False, \n          floating=False, \n          scoring='accuracy',\n          cv=4,\n          n_jobs=-1)\nsbs = sbs.fit(X, y)\n\nprint('\\nSequential Backward Selection (k=3):')\nprint(sbs.k_feature_idx_)\nprint('CV Score:')\nprint(sbs.k_score_)\n\n###################################################\n\n# Sequential Forward Floating Selection\nsffs = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=True, \n           scoring='accuracy',\n           cv=4,\n           n_jobs=-1)\nsffs = sffs.fit(X, y)\n\nprint('\\nSequential Forward Floating Selection (k=3):')\nprint(sffs.k_feature_idx_)\nprint('CV Score:')\nprint(sffs.k_score_)\n\n###################################################\n\n# Sequential Backward Floating Selection\nsbfs = SFS(knn, \n           k_features=3, \n           forward=False, \n           floating=True, \n           scoring='accuracy',\n           cv=4,\n           n_jobs=-1)\nsbfs = sbfs.fit(X, y)\n\nprint('\\nSequential Backward Floating Selection (k=3):')\nprint(sbfs.k_feature_idx_)\nprint('CV Score:')\nprint(sbfs.k_score_)","3ef8752f":"import pandas as pd\npd.DataFrame.from_dict(sfs.get_metric_dict()).T","04a8a00f":"pd.DataFrame.from_dict(sbs.get_metric_dict()).T","309403f6":"pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T","e2da854c":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\n\nsfs = SFS(knn, \n          k_features=4, \n          forward=True, \n          floating=False, \n          scoring='accuracy',\n          verbose=2,\n          cv=5)\n\nsfs = sfs.fit(X, y)\n\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","057e4cdb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target\n\nlr = LinearRegression()\n\nsfs = SFS(lr, \n          k_features=13, \n          forward=True, \n          floating=False, \n          scoring='neg_mean_squared_error',\n          cv=10)\n\nsfs = sfs.fit(X, y)\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","119e6cb4":"from sklearn.datasets import load_iris\nfrom mlxtend.evaluate import PredefinedHoldoutSplit\nimport numpy as np\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nrng = np.random.RandomState(123)\nmy_validation_indices = rng.permutation(np.arange(150))[:30]\nprint(my_validation_indices)\n","bbce342d":"from sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\n\n\nknn = KNeighborsClassifier(n_neighbors=4)\npiter = PredefinedHoldoutSplit(my_validation_indices)\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='accuracy',\n           cv=piter)\n\nsfs1 = sfs1.fit(X, y)","9e742f6b":"# Initialize the dataset\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.33, random_state=1)\n\nknn = KNeighborsClassifier(n_neighbors=4)","99512d4e":"# Select the \"best\" three features via\n# 5-fold cross-validation on the training set.\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(knn, \n           k_features=3, \n           forward=True, \n           floating=False, \n           scoring='accuracy',\n           cv=5)\nsfs1 = sfs1.fit(X_train, y_train)\nprint('Selected features:', sfs1.k_feature_idx_)","4e723f1f":"# generate the new subsets based on the selected features\n# note that the transform call is equivalent to X_train[:, sfs1.k_feature_idx_]\n\nX_train_sfs = sfs1.transform(X_train)\nX_test_sfs = sfs1.transform(X_test)\n\n# Fit the estimator using the new feature subset\n# and make a prediction on the test data\nknn.fit(X_train_sfs, y_train)\ny_pred = knn.predict(X_test_sfs)\n\n# Compute the accuracy of the prediction\nacc = float((y_test == y_pred).sum()) \/ y_pred.shape[0]\nprint('Test set accuracy: %.2f %%' % (acc * 100))","7f3e2320":"# Example 5 - Sequential Feature Selection for Regression\nSequentialFeatureSelector also supports scikit-learn's estimators for regression like other classifiaction examples.","1e6753a4":"Here, we get similar results regardless of the algorithms used. Because we selected best 3 out of 4","af4a392b":"# Example 6 - Feature Selection with Fixed Train\/Validation Splits\nVoluntarily, you can use PredefinedHoldoutSplit class to specify your own, fixed training and validation split.","5cd32531":"# Example 4 - Plotting the results\nUsing plot_sequential_feature_selection, we can also visualize the results using matplotlib figures.","b5e39afd":"Via the subsets_ attribute, we can take a look at the selected feature indices at each step:","f75dc0dc":"here 'feature_names' entry is simply a string notation of the 'feature_idx' in this case.\n\nwe can also provide custom feature names via the fit method's custom_feature_names parameter:","20f72563":"We choose to select the best 3 features from the Iris dataset via Sequential Forward Selection (SFS). Here, we have valued forward with True and floating with False. As cv=0, any cross-validation had not been done, therefore, the performance (here: 'accuracy') is computed entirely on the training set.","2eed60ce":"# Example 1 - A simple Sequential Forward Selection example\nInitializing a simple classifier from scikit-learn:","1aac3c60":"Both SFS and SBFS found the same \"best\" 3 features, however, the intermediate steps where obviously different.\n\nThe ci_bound column in the DataFrames above represents the confidence interval around the computed cross-validation scores. By default, a confidence interval of 95% is used, but we can use different confidence bounds via the confidence_interval parameter. E.g., the confidence bounds for a 90% confidence interval can be obtained as follows:","950424be":"Now, let's compare it to the Sequential Backward Selector:","ca5a4992":"**k_score_** is used to show the prediction score of features","4b8c3b95":"Furthermore, we can access the indices of the 3 best features directly via the k_feature_idx_ attribute:","e4041d8f":"And similarly, to obtain the names of these features, given that we provided an argument to the custom_feature_names parameter, we can refer to the sfs1.k_feature_names_ attribute:","3093f11b":"We can visualize the output from the feature selection in a pandas DataFrame format using the get_metric_dict method of the SequentialFeatureSelector object. Standard deviation and standard errors of the cross-validation scores can be seen in the columns std_dev and std_err.\n\nBelow, we see the DataFrame of the Sequential Forward Selector from Example 2:","c4db144f":"# Example 3 - Visualizing the results in DataFrames","6eaf87e4":"**forward** and **floating** parameters are used to toggle between **SFS, SBS, SFFS, and SBFS** as shown below. here, we are performing (stratified) 4-fold cross-validation for more robust estimates in contrast to Example 1. Via n_jobs=-1, we choose to run the cross-validation on all our available CPU cores.","d27124b6":"# Example 2 - Toggling between SFS, SBS, SFFS, and SBFS","002e7f2d":"# Example 7 - Using the Selected Feature Subset For Making New Predictions"}}