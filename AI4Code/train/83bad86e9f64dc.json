{"cell_type":{"d0efe92f":"code","473de1e8":"code","ffb91339":"code","4e6a78f5":"code","2d93908e":"code","9cbe73a5":"code","bc894fa4":"code","a41470a5":"code","bf2ba670":"code","7e2b2c14":"code","0d6e2f96":"code","7a153127":"code","88e21e3f":"code","8501307b":"code","9536301e":"code","a1c83a90":"code","78f24144":"code","b58bf8c5":"code","361e8c9a":"code","d6f2597c":"code","c2d984bb":"code","3caafc0e":"code","d8091ca5":"code","eccce2c2":"code","ebae561b":"code","fdcf0fa3":"code","327b85ef":"code","0a040446":"code","fce393d8":"code","9a2c7e4d":"code","b564e6e0":"code","6bbad711":"code","b8ac5a2a":"code","f5c810ae":"code","ba778152":"code","393c80c6":"code","e80e98d0":"code","658a7870":"code","31aec591":"code","a19bdd4d":"code","f6d59b57":"code","3959b067":"code","e19d1938":"code","bb8d6c85":"code","6ce6af2b":"code","f3e0af4a":"code","4904db10":"code","58a157a0":"code","5e163cd9":"code","a4ef7f2f":"code","669219c2":"code","20a70ef6":"code","d8649c5e":"code","07e9a818":"code","64596973":"code","172bf75b":"code","90092e59":"code","229430db":"code","6538b0e0":"code","ee429dbb":"code","ac0cf3a3":"code","7ec8652f":"markdown","aecd9ce0":"markdown","b8a68db0":"markdown","b0b2ff2d":"markdown","a32cccc6":"markdown","ae209bd1":"markdown","77c89af4":"markdown","011b572e":"markdown","f650d93a":"markdown","8567cf4a":"markdown","5703a397":"markdown","89b10f0e":"markdown","2e223617":"markdown"},"source":{"d0efe92f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","473de1e8":"!mkdir tmp\n%cd tmp","ffb91339":"!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install numpy==1.17\n!pip install PyYAML==5.3.1\n!pip install git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI","4e6a78f5":"!git clone https:\/\/github.com\/NVIDIA\/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex","2d93908e":"from pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nimport json\nimport urllib\nimport PIL.Image as Image\nimport cv2\nimport torch\nimport torchvision\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nrcParams['figure.figsize'] = 16, 10\nnp.random.seed(42)","9cbe73a5":"!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n\n%cd ..\/\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","bc894fa4":"!pip install -q --upgrade wandb\n# Login \nimport wandb\nwandb.login()","a41470a5":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n#customize iPython writefile so we can write variables\nfrom IPython.core.magic import register_line_cell_magic\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","bf2ba670":"TRAIN_PATH = '\/kaggle\/input\/siim-covid19-resized-384512-and-640px\/SIIM-COVID19-Resized\/img_sz_640\/train\/'\nIMG_SIZE = 640\nBATCH_SIZE = 16   # 16 if yolov5x\nEPOCHS = 30","7e2b2c14":"%cd ..\/\n%cd ..\/\n# Load image level csv file\ndf = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/train_image_level.csv')\n\n# Modify values in the id column\ndf['id'] = df.apply(lambda row: row.id.split('_')[0], axis=1)\n# Add absolute path\ndf['path'] = df.apply(lambda row: TRAIN_PATH+row.id+'.jpg', axis=1)\n# Get image level labels\ndf['image_level'] = df.apply(lambda row: row.label.split(' ')[0], axis=1)\n\ndf.head(5)","0d6e2f96":"meta_df = pd.read_csv('\/kaggle\/input\/siim-covid19-resized-384512-and-640px\/SIIM-COVID19-Resized\/img_sz_640\/meta_sz_640.csv')\ntrain_meta_df = meta_df.loc[meta_df.split == 'train']\ntrain_meta_df = train_meta_df.drop('split', axis=1)\ntrain_meta_df.columns = ['id', 'dim0', 'dim1']\n\ntrain_meta_df.head(2)","7a153127":"# Merge both the dataframes\ndf = df.merge(train_meta_df, on='id',how=\"left\")\ndf.head(2)","88e21e3f":"# Create train and validation split.\ntrain_df, valid_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df.image_level.values)\n\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\n\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)","8501307b":"print(f'Size of dataset: {len(df)}, training images: {len(train_df)}. validation images: {len(valid_df)}')","9536301e":"\nos.makedirs('working\/tmp\/covid\/images\/train', exist_ok=True)\nos.makedirs('working\/tmp\/covid\/images\/valid', exist_ok=True)\n\nos.makedirs('working\/tmp\/covid\/labels\/train', exist_ok=True)\nos.makedirs('working\/tmp\/covid\/labels\/valid', exist_ok=True)\n\n! ls working\/tmp\/covid\/images","a1c83a90":"os.listdir('working\/tmp')","78f24144":"# Move the images to relevant split folder.\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    if row.split == 'train':\n        copyfile(row.path, f'working\/tmp\/covid\/images\/train\/{row.id}.jpg')\n    else:\n        copyfile(row.path, f'working\/tmp\/covid\/images\/valid\/{row.id}.jpg')","b58bf8c5":"# Create .yaml file \nimport yaml\n\ndata_yaml = dict(\n    train = '..\/covid\/images\/train',\n    val = '..\/covid\/images\/valid',\n    nc = 1,\n    names = ['opacity']\n)\n\n# Note that I am creating the file in the yolov5\/data\/ directory.\nwith open('working\/tmp\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat working\/tmp\/yolov5\/data\/data.yaml","361e8c9a":"# Get the raw bounding box by parsing the row value of the label column.\n# Ref: https:\/\/www.kaggle.com\/yujiariyasu\/plot-3positive-classes\ndef get_bbox(row):\n    bboxes = []\n    bbox = []\n    for i, l in enumerate(row.label.split(' ')):\n        if (i % 6 == 0) | (i % 6 == 1):\n            continue\n        bbox.append(float(l))\n        if i % 6 == 5:\n            bboxes.append(bbox)\n            bbox = []  \n            \n    return bboxes\n\n# Scale the bounding boxes according to the size of the resized image. \ndef scale_bbox(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE\/row.dim1\n    scale_y = IMG_SIZE\/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        x = int(np.round(bbox[0]*scale_x, 4))\n        y = int(np.round(bbox[1]*scale_y, 4))\n        x1 = int(np.round(bbox[2]*(scale_x), 4))\n        y1= int(np.round(bbox[3]*scale_y, 4))\n\n        scaled_bboxes.append([x, y, x1, y1]) # xmin, ymin, xmax, ymax\n        \n    return scaled_bboxes\n\n# Convert the bounding boxes in YOLO format.\ndef get_yolo_format_bbox(img_w, img_h, bboxes):\n    yolo_boxes = []\n    for bbox in bboxes:\n        w = bbox[2] - bbox[0] # xmax - xmin\n        h = bbox[3] - bbox[1] # ymax - ymin\n        xc = bbox[0] + int(np.round(w\/2)) # xmin + width\/2\n        yc = bbox[1] + int(np.round(h\/2)) # ymin + height\/2\n        \n        yolo_boxes.append([xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]) # x_center y_center width height\n    \n    return yolo_boxes","d6f2597c":"# Prepare the txt files for bounding box\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    # Get image id\n    img_id = row.id\n    # Get split\n    split = row.split\n    # Get image-level label\n    label = row.image_level\n    \n    if row.split=='train':\n        file_name = f'working\/tmp\/covid\/labels\/train\/{row.id}.txt'\n    else:\n        file_name = f'working\/tmp\/covid\/labels\/valid\/{row.id}.txt'\n        \n    \n    if label=='opacity':\n        # Get bboxes\n        bboxes = get_bbox(row)\n        # Scale bounding boxes\n        scale_bboxes = scale_bbox(row, bboxes)\n        # Format for YOLOv5\n        yolo_bboxes = get_yolo_format_bbox(IMG_SIZE, IMG_SIZE, scale_bboxes)\n        \n        with open(file_name, 'w') as f:\n            for bbox in yolo_bboxes:\n                bbox = [0]+bbox\n                bbox = [str(i) for i in bbox]\n                bbox = ' '.join(bbox)\n                f.write(bbox)\n                f.write('\\n')","c2d984bb":"# %cd working\/tmp\/yolov5\/\n%cd \/kaggle\/working\/tmp\/yolov5\/","3caafc0e":"os.listdir('\/kaggle\/working\/tmp\/yolov5\/data\/hyps')","d8091ca5":"import yaml\n\nwith open('\/kaggle\/working\/tmp\/yolov5\/data\/hyps\/hyp.scratch.yaml') as file:\n    # The FullLoader parameter handles the conversion from YAML\n    # scalar values to Python the dictionary format\n    fruits_list = yaml.load(file, Loader=yaml.FullLoader)\n\n    print(fruits_list)","eccce2c2":"fruits_list[\"lrf\"]= 0.032\nfruits_list[\"box\"]= 0.1\nfruits_list[\"cls\"]= 1.0\nfruits_list[\"cls_pw\"]= 0.5\nfruits_list[\"obj\"]= 2.0\nfruits_list[\"obj_pw\"]= 0.5\nfruits_list[\"anchors\"]= 0\nfruits_list[\"translate\"]= 0.2\nfruits_list[\"scale\"]= 0.6\nfruits_list[\"flipud\"]= 0.2\nfruits_list[\"fliplr\"]= 0.5","ebae561b":"# dont change mosaic.\n# apply rotation\n# apply fliplr","fdcf0fa3":"with open(\"\/kaggle\/working\/tmp\/yolov5\/data\/hyps\/hyp.scratch.yaml\", 'w') as file:\n    documents = yaml.dump(fruits_list, file)","327b85ef":"with open('\/kaggle\/working\/tmp\/yolov5\/data\/hyps\/hyp.scratch.yaml') as file:\n    # The FullLoader parameter handles the conversion from YAML\n    # scalar values to Python the dictionary format\n    fruits_list = yaml.load(file, Loader=yaml.FullLoader)\n\n    print(fruits_list)","0a040446":"!python train.py    --img {IMG_SIZE} \\\n                    --batch {BATCH_SIZE} \\\n                    --epochs {150} \\\n                    --data data.yaml \\\n                    --weights yolov5l.pt \\\n                    --cfg models\/yolov5l.yaml\\\n                    --save_period 1 \\\n                    --project kaggle-siim-covid-yolov5l-clas1-mod8\n\n# here you can choose which model you want. Till now i have observed yolov5x gives best results","fce393d8":"os.listdir(\"\/kaggle\/working\/tmp\/yolov5\/runs\")","9a2c7e4d":"!python train.py    --img {IMG_SIZE} \\\n                    --batch {BATCH_SIZE} \\\n                    --epochs {120} \\\n                    --data data.yaml \\\n                    --weights \/kaggle\/working\/tmp\/yolov5\/artifacts\/run_2yubez04_model:v29\/best.pt \\\n                    --save_period 1 \\\n                    --project kaggle-siim-covid-yolov5l-clas1-mod7","b564e6e0":"run = wandb.init()","6bbad711":"artifact = run.use_artifact(\"39ajinkya\/kaggle-siim-covid-yolov5l-t3-clas1\/run_1qerm3x5_model:v24\")\n# artifact = run.use_artifact(\"39ajinkya\/kaggle-siim-covid-yolov5l-clas1-mod6\/run_2yubez04_model:v29\")\nartifact_dir = artifact.download()","b8ac5a2a":"# artifact = run.use_artifact(\"39ajinkya\/kaggle-siim-covid-yolov5x-t1-clas1\/run_lkm1qq0s_model:v19\")\n# artifact_dir = artifact.download()","f5c810ae":"# run_1qerm3x5_model:v24","ba778152":"run.join()","393c80c6":"os.listdir(\"\/kaggle\/working\/tmp\/yolov5\/artifacts\")","e80e98d0":"# MODEL_PATH = \"artifacts\/run_2xb4vetk_model:v29\/best.pt\"\n# MODEL_PATH = \"artifacts\/run_1qerm3x5_model:v24\/last.pt\"\nMODEL_PATH = \"artifacts\/run_2yubez04_model:v29\/best.pt\"","658a7870":"# MODEL_PATH = 'kaggle-siim-covid-yolov5l-t3-clas2\/exp\/weights\/best.pt'\nTEST_PATH = '..\/..\/..\/input\/siim-covid19-resized-to-512px-png\/test\/'","31aec591":"!python detect.py --weights {MODEL_PATH} \\\n                  --source {TEST_PATH} \\\n                  --img {IMG_SIZE} \\\n                  --conf 0.3 \\\n                  --iou-thres 0.5 \\\n                  --max-det 3 \\\n                  --save-txt \\\n                  --save-conf","a19bdd4d":"# a = wandb.restore('39ajinkya\/kaggle-siim-covid-yolov5x-t1-clas1\/lkm1qq0s')\n# # 39ajinkya\/kaggle-siim-covid-yolov5x-t1-clas1\/lkm1qq0s","f6d59b57":"!python train.py --resume wandb-artifact:\/\/39ajinkya\/kaggle-siim-covid-yolov5l-clas1-mod6\/2yubez04 \\\n                 --epochs {150}\\\n# #!python train.py --resume MODEL_PATH                 ","3959b067":"os.listdir(\"\/kaggle\/working\/tmp\/yolov5\/artifacts\")","e19d1938":"# os.listdir('\/kaggle\/tmp\/yolov5\/runs\/detect\/exp\/labels')","bb8d6c85":"PRED_PATH = 'runs\/detect\/exp\/labels'  # it can be exp\/exp2\/exp3 depending on your count of running detect.py. First run will save results in exp\n!ls {PRED_PATH}","6ce6af2b":"# Visualize predicted coordinates.\n%cat runs\/detect\/exp3\/labels\/ba91d37ee459.txt","f3e0af4a":"prediction_files = os.listdir(PRED_PATH)\nprint('Number of test images predicted as opaque: ', len(prediction_files))","4904db10":"# The submisison requires xmin, ymin, xmax, ymax format. \n# YOLOv5 returns x_center, y_center, width, height\ndef correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w\/2))\n        xmax = xc + int(np.round(w\/2))\n        ymin = yc - int(np.round(h\/2))\n        ymax = yc + int(np.round(h\/2))\n        \n        correct_bboxes.append([xmin, xmax, ymin, ymax])\n        \n    return correct_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes","58a157a0":"# Read the submisison file\nsub_df = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')\nsub_df.tail()","5e163cd9":"# Prediction loop for submission\npredictions = []\n\nfor i in tqdm(range(len(sub_df))):\n    row = sub_df.loc[i]\n    id_name = row.id.split('_')[0]\n    id_level = row.id.split('_')[-1]\n    \n    if id_level == 'study':\n        # do study-level classification\n        predictions.append(\"Negative 1 0 0 1 1\") # dummy prediction\n        \n    elif id_level == 'image':\n        # we can do image-level classification here.\n        # also we can rely on the object detector's classification head.\n        # for this example submisison we will use YOLO's classification head. \n        # since we already ran the inference we know which test images belong to opacity.\n        if f'{id_name}.txt' in prediction_files:\n            # opacity label\n            confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}\/{id_name}.txt')\n            bboxes = correct_bbox_format(bboxes)\n            pred_string = ''\n            for j, conf in enumerate(confidence):\n                pred_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '\n            predictions.append(pred_string[:-1]) \n        else:\n            predictions.append(\"None 1 0 0 1 1\")","a4ef7f2f":"sub_df['PredictionString'] = predictions\nsub_df.to_csv('\/kaggle\/working\/submission.csv', index=False)\nsub_df.tail()","669219c2":"sub_df.loc[sub_df['PredictionString'] == \"None 1 0 0 1 1\"]","20a70ef6":"os.listdir('\/kaggle\/working')","d8649c5e":"# #creating a coco format dataset.\n# df.iloc[0].path","07e9a818":"# annotation = dict()\n\n# for i in tqdm(range(len(df))):\n#     row = df.loc[i]\n#     # Get image id\n#     img_id = row.id\n#     # Get split\n#     split = row.split\n#     # Get image-level label\n#     label = row.image_level\n    \n#     file_name = row.path\n        \n    \n#     if label=='opacity':\n#         # Get bboxes\n#         bboxes = get_bbox(row)\n#         # Scale bounding boxes\n#         scale_bboxes = scale_bbox(row, bboxes)\n#         # Format for YOLOv5\n#         yolo_bboxes = get_yolo_format_bbox(IMG_SIZE, IMG_SIZE, scale_bboxes)\n        \n#         l = []\n#         for bbox in yolo_bboxes:\n#             l.append({'bbox':bbox,'label':'opacity'})\n#         annotation[file_name] = l","64596973":"# annotation","172bf75b":"# del dataset","90092e59":"# image_path = 'kaggle\/input\/siim-covid19-resized-to-256px-jpg\/train\/*'\n# glob.glob(image_path)","229430db":"# import glob\n# import fiftyone as fo\n\n# image_path = 'kaggle\/input\/siim-covid19-resized-to-256px-jpg\/train\/*'\n\n# # Ex: your custom label format\n\n# # Create dataset\n# dataset = fo.Dataset(name=\"siim-covid-19-6\")\n\n# # Persist the dataset on disk in order to \n# # be able to load it in one line in the future\n# dataset.persistent = True\n\n# # Add your samples to the dataset\n# for filepath in annotation:\n#     sample = fo.Sample(filepath=filepath)\n#     sample.tags.append('images')\n#     # Convert detections to FiftyOne format\n#     detections = []\n#     for obj in annotation[filepath]:\n#         label = obj[\"label\"]\n\n#         # Bounding box coordinates should be relative values\n#         # in [0, 1] in the following format:\n#         # [top-left-x, top-left-y, width, height]\n#         bounding_box = obj[\"bbox\"]\n        \n#         detections.append(\n#             fo.Detection(label=label, bounding_box=bounding_box)\n#         )\n\n#     # Store detections in a field name of your choice\n#     sample[\"train\"] = fo.Detections(detections=detections)\n\n#     dataset.add_sample(sample)","6538b0e0":"# view = dataset.match_tags('images')\n# for sample in view:\n#     print(sample)","ee429dbb":"# export_dir = \"\/path\/for\/coco-detection-dataset-1\"\n# label_field = \"ground_truth\"  # for example\n\n# # Export the dataset\n# dataset.export(\n#     export_dir=export_dir,\n#     dataset_type=fo.types.COCODetectionDataset,\n#     label_field=label_field,\n# )","ac0cf3a3":"# !pip install fiftyone","7ec8652f":"**Put proper path in cell below and run detect.py to generate results**.","aecd9ce0":"loading train_image_level.csv and making some modifications.\n\nAdding absolute path of images, adding image level labels","b8a68db0":"Loading pretrained model artifacts from wandb. You can ignore this if you have just trained model. Just change save path","b0b2ff2d":"### Modifying train_csv(because images are resized), adding absolute path, splitting data ","a32cccc6":"### Cloning the github repository.","ae209bd1":"WANDB login (model artifacts will be stored on wandb account)","77c89af4":"yaml file will be used for training","011b572e":"meta_df is stored in modified dataset folder. It specified dimension ratio by which images are shrunk. ","f650d93a":"Change this TRAIN_PATH as per convinience. It is path to modified training dataset which ","8567cf4a":"## training model and test images prediction","5703a397":"creating a coco format dataset.","89b10f0e":"Now your model will be stored as an artifact on wandb account and also in yolov5 directory here. You can find command to predict results over testing data","2e223617":"## Store results in submission.csv file  "}}