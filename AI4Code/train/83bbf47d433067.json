{"cell_type":{"ea09c548":"code","c55b9a19":"code","5cbd4313":"code","61da59b3":"code","aa3f797a":"code","05b29c02":"code","96431c7f":"code","dcf9d7d2":"code","34ebfbb4":"code","04213adf":"code","9e32c23b":"code","20e1916e":"code","dac08892":"code","da554ff7":"code","45deb79a":"code","73836181":"code","b0eb05ea":"code","d098ebb5":"code","9260875c":"code","0bd5a951":"code","e7874bed":"code","16649cdc":"code","3ab0c3b5":"code","0a77dba0":"code","73dd5311":"code","11f07700":"code","c2695d43":"code","8cff87ef":"code","85bf59b8":"code","570f059e":"code","e8bb990f":"code","7d9796e4":"code","5af2bfba":"code","4149f2ef":"code","b4b7e88f":"code","eac71a0d":"markdown","f1ab508e":"markdown","b878ee35":"markdown","9f666ab3":"markdown","a6d2a60e":"markdown","33890e76":"markdown","a7c519ed":"markdown","e2a3f1fa":"markdown","40a04309":"markdown"},"source":{"ea09c548":"!pip install segmentation-models-pytorch \n!pip install sewar ","c55b9a19":"import sewar\nimport segmentation_models_pytorch  as smp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport cv2\nfrom tqdm import tqdm\nfrom torch.nn import functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport gc\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom albumentations import (HorizontalFlip, VerticalFlip, \n                            ShiftScaleRotate, Normalize, Resize, \n                            Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensorV2\n","5cbd4313":"train_df = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/train.csv')\ntrain_df","61da59b3":"sub_df = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/sample_submission.csv')\nsub_df","aa3f797a":"TEST_IMGS_PATH = \"..\/input\/sartorius-cell-instance-segmentation\/test\/\"\nTRAIN_IMGS_PATH = \"..\/input\/sartorius-cell-instance-segmentation\/train\/\"\n\nIMGS_WIDTH = 704\nIMGS_HEIGHT = 520\n\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)\n\nTARGET_IMGS_HEIGHT=512\nTARGET_IMGS_WIDTH=512\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using : \",DEVICE)","05b29c02":"def rle_decode(x,shape,color=1):\n    \n    out = np.zeros((shape[0]*shape[1],shape[2]))\n    x=[int(i) for i in x.split(\" \")]\n    for i in range(0,len(x),2):\n        out[ x[i]:(x[i]+x[i+1]) ]=color\n\n    return np.reshape(out,shape)","96431c7f":"def rle_encode(x):\n    out=[]\n    x=x.flatten()\n    for i in range(0,x.shape[0]-1):\n        if(x[i]==1):\n            count=1\n            out.append(str(i))\n            i+=1\n            while(x[i]==1):\n                count+=1\n                i+=1\n            out.append(str(count))\n    return \" \".join(out)","dcf9d7d2":"class SartoriusCellDataset(Dataset):\n    def __init__(self,train_df,train_imgs_path,transforms):\n        self.train_df=train_df\n        self.image_ids=np.unique(train_df['id']).tolist()\n        self.train_imgs_path=train_imgs_path\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.image_ids)\n    def __getitem__(self,idx):\n        \n        image = cv2.cvtColor( cv2.imread( self.train_imgs_path +  self.image_ids[idx] + \".png\"),cv2.COLOR_BGR2RGB)\n        mask = np.zeros((image.shape[0],image.shape[1],1),dtype=np.float32)\n        rle_masks=self.train_df[train_df[\"id\"]==self.image_ids[idx]]['annotation'].tolist()\n        \n        for rle_mask in rle_masks:\n            mask+=rle_decode(rle_mask,(image.shape[0],image.shape[1],1)).astype(np.float32)\n        mask = mask.clip(0, 1)\n        \n        if self.transforms:\n            aug = self.transforms(image=image,mask=mask)\n            image,mask=aug['image'],aug['mask']\n            \n        return image,mask.reshape((1,image.shape[1],image.shape[2]))","34ebfbb4":"transforms = Compose([Resize(TARGET_IMGS_HEIGHT,TARGET_IMGS_WIDTH),\n                    Normalize(mean=RESNET_MEAN,std=RESNET_STD),\n                    VerticalFlip(p=0.5),\n                    HorizontalFlip(p=0.5),\n                    ToTensorV2()])\ntrain_ds = SartoriusCellDataset(train_df,\n                          TRAIN_IMGS_PATH,\n                          transforms)","04213adf":"\nmodel = smp.DeepLabV3Plus(\"resnet34\",\n                  encoder_weights=\"imagenet\", \n                )\n","9e32c23b":"if torch.cuda.is_available():\n    model.load_state_dict(torch.load(\"..\/input\/deeplabv3plus-resnet34-sartorius\/best_model.pth\"))\nelse:\n    model.load_state_dict(torch.load(\"..\/input\/deeplabv3plus-resnet34-sartorius\/best_model.pth\",map_location=torch.device('cpu')))\n\nmodel.to(DEVICE)","20e1916e":"BATCH_SIZE=32\ntrain_loader=DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)","dac08892":"for batch_idx,batch in enumerate(train_loader):\n    images,masks=batch\n    if torch.cuda.is_available():\n        images,masks=images.cuda(),masks.cuda()\n    preds=model(images)\n    print(preds.shape)\n    fig,axs=plt.subplots(16,2,figsize=(10,80))\n    images,masks=images.cpu(),masks.cpu()\n    preds=preds.cpu().detach().numpy()\n    print(preds[0].max(),preds[0].min())\n    for i in range(16):\n        #axs[i][0].imshow(images[i].reshape(512,512,3))\n        axs[i][0].imshow(masks[i].reshape(512,512,1))\n        axs[i][0].title.set_text(\"Ground truth\")\n        #axs[i][1].imshow(images[i].reshape(512,512,3))\n        axs[i][1].imshow(preds[i].reshape(512,512,1))\n        axs[i][1].title.set_text(\"Prediction\")\n\n    plt.subplots_adjust(wspace=0.1)\n\n    plt.show()\n    break","da554ff7":"sample_pred_1=preds[1].reshape((512,512,1))\nplt.imshow(sample_pred_1)","45deb79a":"thresholds=[0.05,0.1,0.5,0.7]\nfig,axs=plt.subplots(4,2,figsize=(8,16))\n\nfor idx,t in enumerate(thresholds):\n    axs[idx][0].imshow(masks[1].reshape(512,512,1))\n    axs[idx][0].title.set_text(\"Ground Truth\")\n    thresh_img=cv2.threshold(sample_pred_1,t,1,cv2.THRESH_BINARY)[1]\n    axs[idx][1].imshow(thresh_img)\n    axs[idx][1].title.set_text(\"Pred,Treshold : \"+str(t)+\",UQI score : \"+\"{:.4f}\".format(sewar.full_ref.uqi(sample_pred_1.reshape((512,512)),thresh_img)))","73836181":"sample_pred_2=preds[2].reshape((512,512,1))\nplt.imshow(sample_pred_2) ","b0eb05ea":"thresholds=[0.05,0.1,0.5,0.7]\nfig,axs=plt.subplots(4,2,figsize=(8,16))\n\nfor idx,t in enumerate(thresholds):\n    axs[idx][0].imshow(masks[2].reshape(512,512,1))\n    axs[idx][0].title.set_text(\"Ground Truth\")\n    thresh_img=cv2.threshold(sample_pred_2,t,1,cv2.THRESH_BINARY)[1]\n    axs[idx][1].imshow(thresh_img)\n    axs[idx][1].title.set_text(\"Pred,Treshold : \"+str(t)+\",UQI score : \"+\"{:.4f}\".format(sewar.full_ref.uqi(sample_pred_2.reshape((512,512)),thresh_img)))","d098ebb5":"sample_pred_3=preds[7].reshape((512,512,1))\nplt.imshow(sample_pred_3)","9260875c":"thresholds=[0.05,0.1,0.5,0.7,1]\nfig,axs=plt.subplots(5,2,figsize=(8,20))\n\nfor idx,t in enumerate(thresholds):\n    axs[idx][0].imshow(masks[7].reshape(512,512,1))\n    axs[idx][0].title.set_text(\"Ground Truth\")\n    thresh_img=cv2.threshold(sample_pred_3,t,1,cv2.THRESH_BINARY)[1]\n    axs[idx][1].imshow(thresh_img)\n    axs[idx][1].title.set_text(\"Pred,Treshold : \"+str(t)+\",UQI score : \"+\"{:.4f}\".format(sewar.full_ref.uqi(sample_pred_3.reshape((512,512)),thresh_img)))\nplt.subplots_adjust(hspace=0.3)","0bd5a951":"def preds_postprocess(mask,threshold=0.6,min_size=300):\n    mask=cv2.threshold(mask,threshold,1,cv2.THRESH_BINARY)[1]\n    n_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions=[]\n    for c in range(1,n_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            a_prediction = np.zeros((512, 512), np.float32)\n            a_prediction[p] = 1\n            predictions.append(a_prediction)\n    return predictions\n\npost_preds=preds_postprocess(sample_pred_1)\nprint(len(post_preds))\nprint(post_preds[0].shape)\nprint(np.unique(post_preds[0]))","e7874bed":"fig,axs=plt.subplots(1,3,figsize=(15,5))\naxs[0].imshow(masks[1].reshape((512,512,1)))\naxs[0].title.set_text(\"Ground Truth\")\naxs[1].imshow(sample_pred_1.reshape((512,512,1)))\naxs[1].title.set_text(\"Prediction\")\naxs[2].imshow(cv2.threshold(sample_pred_1.reshape((512,512,1)),0.5,1,cv2.THRESH_BINARY)[1])\naxs[2].title.set_text(\"Thresholded Prediction\")","16649cdc":"print(\"RLE Test\")\nencode_sample_pred = post_preds[2]\nfig,axs=plt.subplots(1,2)\nreconstr_pred = rle_decode(rle_encode(encode_sample_pred),(512,512,1))\naxs[0].imshow(encode_sample_pred)\naxs[1].imshow(reconstr_pred)\n\naxs[0].title.set_text(\"Original Image\")\naxs[1].title.set_text(\"Reconstructed Image\")","3ab0c3b5":"def read_img(path,resize_shape=(512,512)):\n    img=cv2.cvtColor( cv2.imread(path),cv2.COLOR_BGR2RGB)\n    img=cv2.resize(img,resize_shape)\n    return img.astype(np.double)\n\ndef np_to_torch(img,img_shape=(512,512)):\n    return torch.tensor(np.expand_dims(img, axis=0).astype(np.float32)).reshape(1,3,img_shape[0],img_shape[1])\n\ndef torch_to_plt(img,shape=(512,512,1)):\n    return img.detach().numpy().astype(np.uint8).reshape(shape)","0a77dba0":"test_transforms = Compose([Resize(512,512),Normalize(mean=RESNET_MEAN,std=RESNET_STD), ToTensorV2()])","73dd5311":"#For some reason doesnt work \u10da(\u2565\ufe4f\u2565\u10da)\nPREDS_THRESHOLD = 0.5\nmodel.cpu()\nsingle_mask_preds=[]\nfor id in sub_df['id'].tolist():\n    \n    plt.figure(figsize=(32,32))\n    \n    image = read_img(TEST_IMGS_PATH + id +\".png\").astype(np.float32)\n    \n    #Original\n    original_image=test_transforms(image=image)[\"image\"]\n    original_pred = model(torch.unsqueeze(original_image,0)).detach().numpy()\n    plt.subplot(1,8,1)\n    plt.imshow(original_pred.reshape(512,512,1))\n    \n    #Vertical flip \n    v_flip_image=test_transforms(image=np.flipud(image).astype(np.float32))[\"image\"]\n    v_flip_pred=np.flipud(model(torch.unsqueeze(v_flip_image,0)).detach().numpy())\n    plt.subplot(1,8,2)\n    plt.imshow(np.flipud(v_flip_pred.reshape(512,512,1)))\n    \n    #Horizontal flip \n    h_flip_image=test_transforms(image=np.fliplr(image).astype(np.float32))[\"image\"]\n    h_flip_pred=np.fliplr(model(torch.unsqueeze(h_flip_image,0)).detach().numpy())\n    plt.subplot(1,8,3)\n    plt.imshow(np.fliplr(h_flip_pred.reshape(512,512,1)))\n    \n    #Diagonal\n    v_h_flip_image=test_transforms(image=np.fliplr(np.flipud(image)).astype(np.float32))[\"image\"]\n    v_h_flip_pred=np.fliplr(np.flipud(model(torch.unsqueeze(v_h_flip_image,0)).detach().numpy()))\n    plt.subplot(1,8,4)\n    plt.imshow(np.fliplr(np.flipud(v_h_flip_pred.reshape(512,512,1))))\n    \n    pred = (original_pred+v_flip_pred+h_flip_pred+v_h_flip_pred) \/ 4\n    single_mask_preds.append(pred)\n    \nsingle_mask_preds=np.asarray(single_mask_preds)\nthresh_preds= single_mask_preds > PREDS_THRESHOLD","11f07700":"PREDS_THRESHOLD = 0.5\nmodel.cpu()\nmodel.eval()\nsingle_mask_preds=[]\nfor id in sub_df['id'].tolist():\n    \n    plt.figure(figsize=(32,32))\n    \n    image = read_img(TEST_IMGS_PATH + id +\".png\").astype(np.float32)\n    \n    #Original\n    image=test_transforms(image=image)[\"image\"]\n    pred = model(torch.unsqueeze(image,0)).detach().numpy()\n    plt.subplot(1,8,1)\n    plt.imshow(pred.reshape(512,512,1))\n    single_mask_preds.append(pred)\n    \nsingle_mask_preds=np.asarray(single_mask_preds)\nthresh_preds= single_mask_preds > PREDS_THRESHOLD","c2695d43":"plt.figure(figsize=(80,80))\nplt.subplot(1,10,1)\nplt.imshow(thresh_preds[0].reshape(512,512,1))\n\nplt.subplot(1,10,2)\n\nplt.imshow(thresh_preds[1].reshape(512,512,1))\n\nplt.subplot(1,10,3)\n\nplt.imshow(thresh_preds[2].reshape(512,512,1))","8cff87ef":"def remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","85bf59b8":"def preds_postprocess(mask,threshold=0.5,min_size=300):\n    mask=cv2.threshold(mask,threshold,1,cv2.THRESH_BINARY)[1]\n    n_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions=[]\n    for c in range(1,n_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            a_prediction = np.zeros((520, 704), np.float32)\n            a_prediction[p] = 1\n            predictions.append(a_prediction)\n    return predictions","570f059e":"sample_sub = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/sample_submission.csv')\nsample_sub","e8bb990f":"test_transforms = Compose([Resize(512,512),Normalize(mean=RESNET_MEAN,std=RESNET_STD), ToTensorV2()])","7d9796e4":"pred_list=[]\nfor idx,id in enumerate (np.unique(sample_sub['id']).tolist()):\n    \n    image=read_img(TEST_IMGS_PATH +str(id) + \".png\")\n    image=test_transforms(image=image)[\"image\"]\n    single_mask_pred=model(image.unsqueeze(0)).detach().squeeze(0).numpy().reshape(512,512)\n\n    single_mask_pred=cv2.resize(single_mask_pred,(704,520),interpolation = cv2.INTER_AREA)\n\n    img_preds=preds_postprocess(single_mask_pred)\n    masks=[]\n    for img_pred in img_preds:\n        fixed_mask=remove_overlapping_pixels(img_pred,masks)\n        masks.append(fixed_mask)\n        pred_list.append((str(id),rle_encode(fixed_mask)))\n        \nsub_df = pd.DataFrame(sub_list,columns=['id','predicted'])\nsub_df","5af2bfba":"def build_masks(df, image_id, input_shape):\n    height, width = input_shape\n    labels = df[df[\"id\"] == image_id][\"predicted\"].tolist()\n    mask = np.zeros((height, width,1))\n    for label in labels:\n        mask += rle_decode(label, shape=(height, width,1))\n    mask = mask.clip(0, 1)\n    return mask\nfig,axs=plt.subplots(1,3,figsize=(30,30))\nfor n,id in enumerate(np.unique(sub_df['id']).tolist()):\n    sample_img=plt.imread(TEST_IMGS_PATH+id+\".png\")\n    sample_masks=build_masks(sub_df,id,input_shape=(520, 704))\n\n    masked = np.ma.masked_where(sample_masks == 0, sample_masks)\n\n    axs[n].imshow(sample_img,cmap=\"seismic\")\n    axs[n].imshow(masked,alpha=0.6,cmap=\"bone\")\n\nplt.show()","4149f2ef":"sub_df.to_csv('submission.csv')","b4b7e88f":"sub_df","eac71a0d":"`\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2591\u2591\u258c GUD.\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2590\u2591\u2591\u2591\u258c\n\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2580\u2592\u2590\u2592\u2591\u2591\u2591\u258c\n\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2580\u2584\u2591\u2591\u2591\u2584\u2584\u2580\u2580\u2592\u2592\u2592\u2592\u258c\u2592\u2592\u2591\u2591\u258c\n\u2591\u2591\u2591\u2591\u2590\u2592\u2591\u2591\u2591\u2580\u2584\u2580\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2588\n\u2591\u2591\u2591\u2591\u258c\u2592\u2591\u2591\u2591\u2591\u2592\u2580\u2584\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2580\u2584\n\u2591\u2591\u2591\u2591\u2590\u2592\u2591\u2591\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u258c\u2592\u2590\u2592\u2592\u2592\u2592\u2592\u2580\u2584\n\u2591\u2591\u2591\u2591\u258c\u2580\u2584\u2591\u2591\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2590\u2592\u2592\u2592\u258c\u2592\u258c\u2592\u2584\u2584\u2592\u2592\u2590\n\u2591\u2591\u2591\u258c\u258c\u2592\u2592\u2580\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2590\u2592\u2592\u2592\u2592\u2592\u2588\u2584\u2588\u258c\u2592\u2592\u258c\n\u2591\u2584\u2580\u2592\u2590\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2584\u2580\u2588\u258c\u2592\u2592\u2592\u2592\u2592\u2580\u2580\u2592\u2592\u2590\u2591\u2591\u2591\u2584\n\u2580\u2592\u2592\u2592\u2592\u258c\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2584\u2592\u2590\u2588\u2588\u2588\u258c\u2584\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2584\u2580\u2580\u2580\u2580\n\u2592\u2592\u2592\u2592\u2592\u2590\u2592\u2592\u2592\u2592\u2592\u2584\u2580\u2592\u2592\u2592\u2580\u2580\u2580\u2592\u2592\u2592\u2592\u2584\u2588\u2580\u2591\u2591\u2592\u258c\u2580\u2580\u2584\u2584\n\u2592\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2584\u2584\u2580\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2590\u2592\u2580\u2584\u2580\u2584\u2591\u2591\u2591\u2591\u2580\n\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2588\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2584\u2592\u2592\u2592\u2592\u2584\u2580\u2592\u2592\u2592\u258c\u2591\u2591\u2580\u2584\n\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2580\u2584\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2580\u2580\u2580\u2580\u2592\u2592\u2592\u2584\u2580`","f1ab508e":"# THE TRAINING NOTEBOOK (PART 1 CAN BE FOUND HERE)\n# [Sartorius-Cell-Segmentation-DeepLabv3-Training](https:\/\/www.kaggle.com\/albertozorzetto\/sartorius-cell-segmentation-deeplabv3-training)","b878ee35":"# Inference test on training data","9f666ab3":"# Test-Time Data Augmentation (TTA) INFERENCE","a6d2a60e":"`\u2588\u2580\u2580\u2584\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2580\u2588\n\u2591\u2588\u2591\u2591\u2591\u2580\u2584\u2591\u2584\u2584\u2584\u2584\u2584\u2591\u2584\u2580\u2591\u2591\u2591\u2588\n\u2591\u2591\u2580\u2584\u2591\u2591\u2591\u2580\u2591\u2591\u2591\u2591\u2591\u2580\u2591\u2591\u2591\u2584\u2580\n\u2591\u2591\u2591\u2591\u258c\u2591\u2584\u2584\u2591\u2591\u2591\u2584\u2584\u2591\u2590\u2580\u2580\n\u2591\u2591\u2591\u2590\u2591\u2591\u2588\u2584\u2591\u2591\u2591\u2584\u2588\u2591\u2591\u258c\u2584\u2584\u2580\u2580\u2580\u2580\u2588\n\u2591\u2591\u2591\u258c\u2584\u2584\u2580\u2580\u2591\u2584\u2591\u2580\u2580\u2584\u2584\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2588\n\u2584\u2580\u2580\u2590\u2580\u2580\u2591\u2584\u2584\u2584\u2584\u2584\u2591\u2580\u2580\u258c\u2584\u2584\u2584\u2591\u2591\u2591\u2588\n\u2588\u2591\u2591\u2591\u2580\u2584\u2591\u2588\u2591\u2591\u2591\u2588\u2591\u2584\u2580\u2591\u2591\u2591\u2591\u2588\u2580\u2580\u2580\n\u2591\u2580\u2584\u2591\u2591\u2580\u2591\u2591\u2580\u2580\u2580\u2591\u2591\u2580\u2591\u2591\u2591\u2584\u2588\u2580\n\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2584\u2591\u2580\u2584\n\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2584\u2580\u2588\u2591\u2591\u2588\u2591\u2591\u2588\n\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2584\u2588\u2591\u2591\u2584\u2580\n\u2591\u2591\u2591\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2580\n\u2591\u2591\u2591\u2580\u2584\u2584\u2580\u2580\u2584\u2584\u2580\u2580\u2584\u2584\u2584\u2588\u2580\n`","33890e76":"Aaaaaand , submission \u1555( \u141b )\u1557","a7c519ed":"# Post-process and sub","e2a3f1fa":"### Apparently 0.5 threshold is better quite often ","40a04309":"# Loading the model"}}