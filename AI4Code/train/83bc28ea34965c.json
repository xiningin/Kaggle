{"cell_type":{"83646529":"code","11f9b497":"code","e8513059":"code","6a0d34fd":"code","9e3f71b4":"code","bb14291b":"code","8235c4b6":"code","1a1c4760":"code","9032c028":"code","da52d699":"code","8b2f113d":"code","f5c7f69d":"code","f4c4b266":"code","9900613e":"code","2e016ebf":"code","64842a1f":"code","f8e4c47f":"code","1f6e94c6":"code","77127ea2":"code","55e7be73":"code","16b35abc":"code","ec3a1b66":"code","37cb6454":"code","0d812304":"code","fa21553a":"code","487f0690":"code","fc153702":"code","0b60d305":"code","fbbf4ded":"code","7014096c":"code","c69045a3":"code","72962832":"code","07e2eb13":"code","e4c96728":"code","1c499d6f":"code","b4e67d2f":"code","472df370":"code","1580421d":"code","8f336257":"code","3c784a57":"code","e3f443a9":"code","4c942777":"code","25da7328":"code","184ec99c":"code","38ab5b3b":"code","f6b842d7":"code","57f08524":"code","19cd262a":"code","a56e18a8":"code","f1c86995":"code","d110c43b":"code","931fe58c":"code","26f3835a":"code","cad66942":"code","466fe1e2":"code","d86050f4":"code","53c6f997":"code","785ea83c":"code","b01485e0":"code","bc4ab140":"code","f2633f94":"code","4ebed2b7":"code","56d18968":"code","26132757":"code","070cc5af":"code","ae25ef05":"code","8e7f7335":"code","7f9fa172":"code","6f3065c5":"code","041e6c74":"code","56042195":"code","19d7e748":"code","8d458f5e":"code","32ef02e2":"code","79af8a58":"code","8246eeda":"code","8a188c43":"code","e3f070b4":"code","415fbe46":"code","2ca5a406":"code","476360d7":"code","abc10209":"code","2623dd70":"code","db5656fd":"code","69e34732":"code","dee76161":"code","e27cb7bd":"code","ac79314f":"code","a06bea83":"code","376e79b1":"markdown","c0a555e9":"markdown","e5472a4f":"markdown","29438bba":"markdown","32fada9d":"markdown","64ca068c":"markdown","9860b09e":"markdown","b8348772":"markdown","c85d1223":"markdown","38e1512b":"markdown","17f78b08":"markdown","bf9a1b14":"markdown","6c1bae6b":"markdown","cdfed4a3":"markdown","79e33599":"markdown","8ee8789d":"markdown","228da8de":"markdown","0f2d454c":"markdown","e0ff10b7":"markdown","8fa79884":"markdown","c57ea713":"markdown","41e4a5de":"markdown","2902d752":"markdown","5262f6cf":"markdown","12694bbf":"markdown","62b71ced":"markdown","23ac2799":"markdown","baafa3ff":"markdown","f7161aeb":"markdown","fcd06098":"markdown","ce91703c":"markdown","6249e043":"markdown","41dadd88":"markdown","6476de22":"markdown","2cd609dd":"markdown","b12f3432":"markdown","798a18d5":"markdown","4b391241":"markdown","a85f7819":"markdown","2ea5e037":"markdown","59b776d6":"markdown","35e1d5e2":"markdown","c72f5c72":"markdown","eb6cf080":"markdown","b275f636":"markdown","3f067f4a":"markdown","46898a68":"markdown","3306289f":"markdown","4f758181":"markdown","f63fcb4d":"markdown","58cf728e":"markdown","65b41d25":"markdown","81e17469":"markdown","91515255":"markdown","00ab4e1b":"markdown","49f0e834":"markdown","6fba1066":"markdown","2df0fb22":"markdown","aebbec55":"markdown","4fe2b907":"markdown","df45c7c9":"markdown","9aceb6e3":"markdown","fdfa4462":"markdown","71f28aff":"markdown","67b59409":"markdown","065afafb":"markdown","5e9fe1c8":"markdown"},"source":{"83646529":"## Ignore warning\nimport warnings \nwarnings.filterwarnings('ignore') \n\n\n# Data processing and analysis libraries\nimport numpy as np\nimport pandas as pd\nimport re\n\n\n# Data visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n\n# Configure visualisations\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\n\n\n# Classification algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\n# Data preprocessing :\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n\n\n# Modeling helper functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n# Classification metrices\nfrom sklearn.metrics import accuracy_score","11f9b497":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e8513059":"# Load train and Test set\n\n%time\n\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nIDtest = test_df['PassengerId']","6a0d34fd":"print(f'The shape of the training set : ', (train_df.shape))\nprint(f'The shape of the test set : ', (test_df.shape))\nprint(f'The shape of the submission set : ', (submission_df.shape))","9e3f71b4":"train_df.head()","bb14291b":"test_df.head()","8235c4b6":"train_df.info()","1a1c4760":"test_df.info()","9032c028":"# missing values in training set\n\nvar1 = [col for col in train_df.columns if train_df[col].isnull().sum() != 0]\n\nprint(train_df[var1].isnull().sum())","da52d699":"# missing values in test set\n\nvar2 = [col for col in test_df.columns if test_df[col].isnull().sum() != 0]\n\nprint(test_df[var2].isnull().sum())","8b2f113d":"train_df.describe()","f5c7f69d":"test_df.describe()","f4c4b266":"# find categorical variables in training set\n\ncategorical1 = [var for var in train_df.columns if train_df[var].dtype =='O']\n\nprint('There are {} categorical variables in training set.\\n'.format(len(categorical1)))\n\nprint('The categorical variables are :', categorical1)","9900613e":"# find numerical variables in training set\n\nnumerical1 = [var for var in train_df.columns if train_df[var].dtype !='O']\n\nprint('There are {} numerical variables in training set.\\n'.format(len(numerical1)))\n\nprint('The numerical variables are :', numerical1)","2e016ebf":"# find categorical variables in test set\n\ncategorical2 = [var for var in test_df.columns if test_df[var].dtype =='O']\n\nprint('There are {} categorical variables in test set.\\n'.format(len(categorical2)))\n\nprint('The categorical variables are :', categorical2)","64842a1f":"# find numerical variables in test set\n\nnumerical2 = [var for var in test_df.columns if test_df[var].dtype !='O']\n\nprint('There are {} numerical variables in test set.\\n'.format(len(numerical2)))\n\nprint('The numerical variables are :', numerical2)","f8e4c47f":"# view missing values in training set\n\nmsno.matrix(train_df, figsize = (30,10))","1f6e94c6":"# view missing values in test set\n\nmsno.matrix(test_df, figsize = (30,10))","77127ea2":"train_df['Survived'].value_counts()","55e7be73":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train_df['Survived'], data = train_df, palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","16b35abc":"train_df.groupby('Survived')['Sex'].value_counts()","ec3a1b66":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train_df['Survived'], data = train_df, hue='Sex', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","37cb6454":"females = train_df[train_df['Sex'] == 'female']\nfemales.head()","0d812304":"females['Survived'].value_counts()\/len(females)","fa21553a":"males = train_df[train_df['Sex'] == 'male']\nmales.head()","487f0690":"males['Survived'].value_counts()\/len(males)","fc153702":"# create the first of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 1)   # (rows, columns, panel number)\nlabels1 = females['Survived'].value_counts().index\nsize1 = females['Survived'].value_counts()\ncolors1=['cyan','pink']\nplt.pie(size1, labels = labels1, colors = colors1, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of females who survived', fontsize = 20)\nplt.legend(['1:Survived', '0:Not Survived'], loc=0)\nplt.show()\n\n# create the second of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 2)   # (rows, columns, panel number)\nlabels2 = males['Survived'].value_counts().index\nsize2 = males['Survived'].value_counts()\ncolors2=['pink','cyan']\nplt.pie(size2, labels = labels2, colors = colors2, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of males who survived', fontsize = 20)\nplt.legend(['0:Not Survived','1:Survived'])\nplt.show()","0b60d305":"train_df['Sex'].value_counts()","fbbf4ded":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train_df['Sex'], data=train_df, palette = 'bone')\ngraph.set_title('Distribution of sex among passengers', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","7014096c":"train_df['Sex'].value_counts()\/len(train_df)","c69045a3":"plt.figure(figsize=(8,6))\nlabels = train_df['Sex'].value_counts().index\nsize = train_df['Sex'].value_counts()\ncolors=['cyan','pink']\nplt.pie(size, labels = labels, shadow = True, colors=colors, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage distribution of sex among passengers', fontsize = 20)\nplt.legend()\nplt.show()","72962832":"train_df.groupby('Pclass')['Sex'].value_counts()","07e2eb13":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train_df['Pclass'], data=train_df, palette = 'bone')\ngraph.set_title('Number of people in different classes', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","e4c96728":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train_df['Pclass'], data=train_df, hue='Survived', palette = 'bone')\ngraph.set_title('Distribution of people segregated by survival', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","1c499d6f":"# percentage of survivors per class\nsns.factorplot('Pclass', 'Survived', data = train_df)","b4e67d2f":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train_df['Embarked'], data=train_df, palette = 'bone')\ngraph.set_title('Number of people across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","472df370":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train_df['Embarked'], data=train_df, hue='Survived', palette = 'bone')\ngraph.set_title('Number of people who survived across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","1580421d":"x = train_df['Age']\nplt.figure(figsize=(8,6))\nplt.hist(x, bins=25, color='g')\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.title('Age distribution of passengers', fontsize = 20)\nplt.show()","8f336257":"plt.figure(figsize=(8,6))\ntrain_df.Age[train_df.Pclass == 1].plot(kind='kde')    \ntrain_df.Age[train_df.Pclass == 2].plot(kind='kde')\ntrain_df.Age[train_df.Pclass == 3].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;","3c784a57":"train_df.hist(bins=10,figsize=(12,8),grid=False);","e3f443a9":"g = sns.FacetGrid(train_df, col=\"Sex\", row=\"Survived\", margin_titles=True)\ng.map(plt.hist, \"Age\", color=\"green\");","4c942777":"corr = train_df.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, vmax=.8, linewidths=0.01, square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","25da7328":"#correlation of features with target variable\ntrain_df.corr()[\"Survived\"]","184ec99c":"g = sns.factorplot(x=\"Age\", y=\"Embarked\",\n                    hue=\"Sex\", row=\"Pclass\",\n                    data=train_df[train_df.Embarked.notnull()],\n                    orient=\"h\", size=2, aspect=3.5, \n                   palette={'male':\"purple\", 'female':\"blue\"},\n                    kind=\"violin\", split=True, cut=0, bw=.2);","38ab5b3b":"#Lets check which rows have null Embarked column\ntrain_df[train_df['Embarked'].isnull()]","f6b842d7":"plt.figure(figsize=(8,6))\nsns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train_df)\nplt.show()","57f08524":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna('C')","19cd262a":"#there is an empty fare column in test set\ntest_df.describe()","a56e18a8":"test_df[test_df['Fare'].isnull()]","f1c86995":"#we can replace missing value in fare by taking median of all fares of those passengers \n#who share 3rd Passenger class and Embarked from 'S' \ndef fill_missing_fare(df):\n    median_fare=df[(df['Pclass'] == 3) & (df['Embarked'] == 'S')]['Fare'].median()\n#'S'\n       #print(median_fare)\n    df[\"Fare\"] = df[\"Fare\"].fillna(median_fare)\n    return df\n\ntest_df=fill_missing_fare(test_df)","d110c43b":"train_df[\"Deck\"]=train_df.Cabin.str[0]\ntest_df[\"Deck\"]=test_df.Cabin.str[0]\ntrain_df[\"Deck\"].unique() # 0 is for null values","931fe58c":"g = sns.factorplot(\"Survived\", col=\"Deck\", col_wrap=4,\n                    data=train_df[train_df.Deck.notnull()],\n                    kind=\"count\", size=2.5, aspect=.8);","26f3835a":"train_df = train_df.assign(Deck=train_df.Deck.astype(object)).sort_values(\"Deck\")\ng = sns.FacetGrid(train_df, col=\"Pclass\", sharex=False,\n                  gridspec_kws={\"width_ratios\": [5, 3, 3]})\ng.map(sns.boxplot, \"Deck\", \"Age\");","cad66942":"train_df.Deck.fillna('Z', inplace=True)\ntest_df.Deck.fillna('Z', inplace=True)\ntrain_df[\"Deck\"].unique() # Z is for null values","466fe1e2":"# Create a family size variable including the passenger themselves\ntrain_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]+1\ntest_df[\"FamilySize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]+1\nprint(train_df[\"FamilySize\"].value_counts())","d86050f4":"# Discretize family size\ntrain_df.loc[train_df[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\ntrain_df.loc[(train_df[\"FamilySize\"] > 1)  &  (train_df[\"FamilySize\"] < 5) , \"FsizeD\"] = 'small'\ntrain_df.loc[train_df[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\n\ntest_df.loc[test_df[\"FamilySize\"] == 1, \"FsizeD\"] = 'singleton'\ntest_df.loc[(test_df[\"FamilySize\"] >1) & (test_df[\"FamilySize\"] <5) , \"FsizeD\"] = 'small'\ntest_df.loc[test_df[\"FamilySize\"] >4, \"FsizeD\"] = 'large'\n","53c6f997":"print(train_df[\"FsizeD\"].unique())\nprint(train_df[\"FsizeD\"].value_counts())","785ea83c":"sns.factorplot(x=\"FsizeD\", y=\"Survived\", data=train_df);","b01485e0":"#Create feature for length of name \n# The apply method generates a new series\n\ntrain_df[\"NameLength\"] = train_df[\"Name\"].apply(lambda x: len(x))\ntest_df[\"NameLength\"] = test_df[\"Name\"].apply(lambda x: len(x))\nbins = [0, 20, 40, 57, 85]\ngroup_names = ['short', 'okay', 'good', 'long']\ntrain_df['NlengthD'] = pd.cut(train_df['NameLength'], bins, labels=group_names)\ntest_df['NlengthD'] = pd.cut(test_df['NameLength'], bins, labels=group_names)\n","bc4ab140":"sns.factorplot(x=\"NlengthD\", y=\"Survived\", data=train_df)\nprint(train_df[\"NlengthD\"].unique())","f2633f94":"import re\n\n#A function to get the title from a name.\ndef get_title(name):\n    \"\"\"Use a regular expression to search for a title.  \n       Titles always consist of capital and lowercase letters, and end with a period\"\"\"\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    #If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","4ebed2b7":"#Get all the titles and print how often each one occurs.\ntitles = train_df[\"Name\"].apply(get_title)\nprint(pd.value_counts(titles))\n","56d18968":"#Add in the title column.\ntrain_df[\"Title\"] = titles","26132757":"# Titles with very low cell counts to be combined to \"rare\" level\nrare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n","070cc5af":"# Also reassign mlle, ms, and mme accordingly\ntrain_df.loc[train_df[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\ntrain_df.loc[train_df[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\ntrain_df.loc[train_df[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\ntrain_df.loc[train_df[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\ntrain_df.loc[train_df[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n","ae25ef05":"titles = test_df[\"Name\"].apply(get_title)\nprint(pd.value_counts(titles))","8e7f7335":"#Add in the title column.\ntest_df[\"Title\"] = titles\n","7f9fa172":"# Titles with very low cell counts to be combined to \"rare\" level\nrare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']","6f3065c5":"# Also reassign mlle, ms, and mme accordingly\ntest_df.loc[test_df[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\ntest_df.loc[test_df[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\ntest_df.loc[test_df[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\ntest_df.loc[test_df[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\ntest_df.loc[test_df[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n","041e6c74":"test_df[\"Title\"].value_counts()","56042195":"train_df[\"Ticket\"].tail()","19d7e748":"train_df[\"TicketNumber\"] = train_df[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\ntrain_df[\"TicketNumber\"] = train_df[\"TicketNumber\"].apply(pd.to_numeric)","8d458f5e":"test_df[\"TicketNumber\"] = test_df[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\ntest_df[\"TicketNumber\"] = test_df[\"TicketNumber\"].apply(pd.to_numeric)","32ef02e2":"#some rows in ticket column dont have numeric value so we got NaN there\ntrain_df[train_df[\"TicketNumber\"].isnull()]","79af8a58":"train_df.TicketNumber.fillna(train_df[\"TicketNumber\"].median(), inplace=True)\ntest_df.TicketNumber.fillna(test_df[\"TicketNumber\"].median(), inplace=True)","8246eeda":"labelenc=LabelEncoder()\n\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    train_df[col]=labelenc.fit_transform(train_df[col])\n    test_df[col]=labelenc.fit_transform(test_df[col])","8a188c43":"train_df.head()","e3f070b4":"with sns.plotting_context(\"notebook\",font_scale=1.5):\n    sns.set_style(\"whitegrid\")\n    sns.distplot(train_df[\"Age\"].dropna(),\n                 bins=80,\n                 kde=False,\n                 color=\"red\")\n    plt.title(\"Age Distribution\")\n    plt.ylabel(\"Count\");","415fbe46":"from sklearn.ensemble import RandomForestRegressor\n#predicting missing values in age using Random Forest\ndef fill_missing_age(df):\n    \n    #Feature set\n    age_df = df[['Age','Embarked','Fare', 'Parch', 'SibSp',\n                 'TicketNumber', 'Title','Pclass','FamilySize',\n                 'FsizeD','NameLength',\"NlengthD\",'Deck']]\n    # Split sets into train and test\n    train  = age_df.loc[ (df.Age.notnull()) ]# known Age values\n    test = age_df.loc[ (df.Age.isnull()) ]# null Ages\n    \n    # All age values are stored in a target array\n    y = train.values[:, 0]\n    \n    # All the other values are stored in the feature array\n    X = train.values[:, 1::]\n    \n    # Create and fit a model\n    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n    rtr.fit(X, y)\n    \n    # Use the fitted model to predict the missing values\n    predictedAges = rtr.predict(test.values[:, 1::])\n    \n    # Assign those predictions to the full data set\n    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n    \n    return df","2ca5a406":"train_df=fill_missing_age(train_df)\ntest_df=fill_missing_age(test_df)","476360d7":"from sklearn import preprocessing\n\nstd_scale = preprocessing.StandardScaler().fit(train_df[['Age', 'Fare']])\ntrain_df[['Age', 'Fare']] = std_scale.transform(train_df[['Age', 'Fare']])\n\n\nstd_scale = preprocessing.StandardScaler().fit(test_df[['Age', 'Fare']])\ntest_df[['Age', 'Fare']] = std_scale.transform(test_df[['Age', 'Fare']])","abc10209":"train_df.corr()[\"Survived\"]","2623dd70":"# Declare feature vector and target variable\nX_train = train_df.drop(labels = ['Survived'],axis = 1)\ny_train = train_df['Survived']\nX_test = test_df\n","db5656fd":"# Initializing Support Vector classifier\nclf_svc = SVC(C = 50, degree = 1, gamma = \"auto\", kernel = \"rbf\", probability = True)\n\n# Initializing Multi-layer perceptron  classifier\nclf_mlp = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n                            learning_rate = \"constant\", max_iter = 2000, random_state = 1000)\n\n# Initialing Nu Support Vector classifier\nclf_nusvc = NuSVC(degree = 1, kernel = \"rbf\", nu = 0.25, probability = True)\n\n# Initializing Random Forest classifier\nclf_rfc = RandomForestClassifier(n_estimators = 500, criterion = \"gini\", max_depth = 10,\n                                     max_features = \"auto\", min_samples_leaf = 0.005,\n                                     min_samples_split = 0.005, n_jobs = -1, random_state = 1000)","69e34732":"classifiers = [('svc', clf_svc),\n               ('mlp', clf_mlp),                             \n               ('nusvc', clf_nusvc),\n               ('rfc', clf_rfc)]","dee76161":"clf = StackingClassifier(estimators=classifiers, \n                         final_estimator=LogisticRegression(),\n                         stack_method='auto',\n                         n_jobs=-1,\n                         passthrough=False)","e27cb7bd":"predictors=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\",\"NlengthD\",\n              \"FsizeD\", \"Title\",\"Deck\",\"NameLength\",\"TicketNumber\"]\n\nclf.fit(X_train[predictors],y_train)","ac79314f":"test_predictions=clf.predict(X_test[predictors])\n","a06bea83":"test_predictions=test_predictions.astype(int)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": test_predictions\n    })\n\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","376e79b1":"So, we are right that `Age`, `Cabin` and `Embarked` contain missing values in training set.","c0a555e9":"<a class=\"anchor\" id=\"0\"><\/a>\n# **Stacked Classifier : Top 10 % on LB** \n\n\n\n## **Introduction**\n\n\nPrashant Banerjee\n\n\nApril 2020\n\n\nThis notebook gives a very simple and basic introduction to an ensemble learning technique known as **stacking**. The objective of this notebook is to provide an intuitive understanding and implement **stacking**. We have used the famous titanic dataset for the illustration purposes.\n\n\nThere is an excellent notebook on titanic survival. It is -\n\n\n[Titanic Survival Prediction End to End ML Pipeline](https:\/\/www.kaggle.com\/poonaml\/titanic-survival-prediction-end-to-end-ml-pipeline) by **Poonam Ligade**. Nice data exploration.\n\n\nI have adapted several lines of code from the above notebook.\n\n\nNow let's begin our journey to understand stacking. So, let's dive in.","e5472a4f":"![Stacked Classifier](https:\/\/miro.medium.com\/max\/2044\/1*5O5_Men2op_sZsK6TTjD9g.png)","29438bba":"### **What's in the name?**","32fada9d":"# **6. Data Preprocessing** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Table of Contents](#0.1)\n","64ca068c":"### **Correlation of features with target**","9860b09e":"- Now, let's get to implementation of stacking or stacked classifier.\n\n- The first step is to import the libraries and dataset","b8348772":"# **12. Stacked Classifier** <a class=\"anchor\" id=\"12\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nTo stack the above classifiers, we will use the [StackingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) from scikit-learn library.\n\n\nWe can also use the [StackingCVClassifier](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/) from MLXTEND for the same purpose. We can take a look at the [official documentation](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/) since it goes in detail over useful examples of how to implement the StackingCVClassifier.\n","c85d1223":"## **3.1 Import Libraries** <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Notebook Contents](#0.1)\n","38e1512b":"In this notebook, we have demonstrated the stacked classifier.\n\nNow we will come to the end of this kernel. I hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","17f78b08":"# **7. Feature Engineering** <a class=\"anchor\" id=\"7\"><\/a>\n\n[Table of Contents](#0.1)\n\n","bf9a1b14":"`Age`, `Fare` and `Cabin` contain missing values in test set.","6c1bae6b":"## **5.3 Sex** <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Table of Contents](#0.1)\n","cdfed4a3":"### **Preview test set**","79e33599":"We can see that for 1st class median line is coming around fare $80 for embarked value 'C'. So we can replace NA values in Embarked column with 'C'.","8ee8789d":"### **Ticket column**","228da8de":"So, `Pclass` has got highest negative correlation with `Survived` and `Fare` has got highest positive correlation with `Survived`.","0f2d454c":"We can see that port of embarkment plays a major role in survival probability.","e0ff10b7":"Here, missing values occur in `Age`,`Fare` and `Cabin`. We will see it later.","8fa79884":"## **3.2 Load data** <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Notebook Contents](#0.1)\n\n","c57ea713":"# **2. Stacking is prone to overfitting** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- This type of Stacking is prone to overfitting due to information leakage.\n\n- To prevent information leakage into the training set from the target set, the level one predictions should come from a subset of the training data that was not used to train the level one classifiers.\n\n- This can be applied by applying k-fold cross validation technique. In this technique, the training data is split into k-folds. Then the first k-1 folds are used to train the level one classifiers. The validation fold is then used to generate a subset of the level one predictions. The process is repeated for each unique group to generate the level one predictions.\n\n- The figure below illustrates this process -","41e4a5de":"## **5.1 Missing values** <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Notebook Contents](#0.1)","2902d752":"### **Types of Variables**\n\n\nNow, we will classify the variables into categorical and numerical variables.","5262f6cf":"Now females have higher probability of survival than males.\n\nLet' check it","12694bbf":"### **View statistical properties**","62b71ced":"# **5. Data Visualization** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Notebook Contents](#0.1)\n","23ac2799":"# **11. Individual Classifiers** <a class=\"anchor\" id=\"11\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nFor the purpose of illustration, we will train a **Support Vector Classifier (SVC)**, **Multi-layer Perceptron (MLP) classifier**, **Nu-Support Vector classifier (NuSVC)** and a **Random Forest (RF) classifier** \u2014 classifiers available in Scikit-learn. \n","baafa3ff":"## **5.6 Age** <a class=\"anchor\" id=\"5.6\"><\/a>\n\n[Table of Contents](#0.1) ","f7161aeb":"## **5.4 Pclass** <a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Table of Contents](#0.1)\n","fcd06098":"### **View concise summary of test set**","ce91703c":"As expected females have higher probability of survival (value 1) 74.20% than males 18.89%.\n\nLet's visualize it.","6249e043":"# **8. Categorical Encoding** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Table of Contents](#0.1)","41dadd88":"## **5.5 Embarked** <a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Table of Contents](#0.1)","6476de22":"[Go to Top](#0)","2cd609dd":"## **5.2 Survived**  <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Notebook Contents](#0.1)","b12f3432":"We can see that majority of passengers are aged between 20 and 40.","798a18d5":"![Stacked Classifier](https:\/\/www.researchgate.net\/profile\/David_Powers2\/publication\/264125265\/figure\/fig1\/AS:295914087436290@1447562824204\/Fusion-system-based-on-stacking.png)","4b391241":"### **View concise summary of training set**","a85f7819":"### **Fare Column**","2ea5e037":"# **9. Feature Scaling** <a class=\"anchor\" id=\"9\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nWe can see that Age, Fare are measured on different scales, so we need to do Feature Scaling first before we proceed with making predictions with **stacked classifier**.","59b776d6":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> keep me motivated.**\n","35e1d5e2":"### **Embarked Column**","c72f5c72":"![k-fold Cross Validation Techniques](https:\/\/miro.medium.com\/max\/2972\/1*RP0pkQEOSrw9_EjFu4w3gg.png)","eb6cf080":"# **10. Declare feature vector and target label** <a class=\"anchor\" id=\"10\"><\/a>\n\n[Table of Contents](#0.1)\n","b275f636":"## **5.7 Visualizations about training set** <a class=\"anchor\" id=\"5.7\"><\/a>\n\n[Table of Contents](#0.1) ","3f067f4a":"# **1. Introduction to Stacking** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- [Stacking](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/) is an ensemble machine learning technique to combine multiple individual classification models via a meta-classifier. \n\n- But, wait what is a meta-classifier?\n\n- Let's visualize the schematic representation of meta classifier below.\n","46898a68":"## **5.8 Correlation Heatmap** <a class=\"anchor\" id=\"5.8\"><\/a>\n\n[Table of Contents](#0.1) ","3306289f":"### **Check the shape of the datasets**","4f758181":"We suspect missing values in `Age`,`Cabin` and `Embarked` in training set. We will explore it later.","f63fcb4d":"It is important to fill missing values, because some machine learning algorithms can't accept them eg SVM.\n\n\nBut filling missing values with mean\/median\/mode is also a prediction which may not be 100% accurate, instead we can use models like Decision Trees and Random Forest which handle missing values very well.","58cf728e":"### **Do the same with test set**","65b41d25":"From the above diagram, we can conclude that stacking can be thought of as a two step process.\n\n### **Step 1** : In the first step, the individual classification models are trained based on the complete training set and their individual outputs are stored. These individual classification models are referred to as **Level One or Base Classifiers**.\n\n\n### **Step 2** : In the second step, the predictions of individual classifiers (referred to as **Level One or Base Classifiers**) are used as new features to train a new classifier. This new classifier is called **Meta Classifier**. The meta-classifier can be any classifier of our choice. \n\n\nThe meta-classifier is fitted based on the outputs -- **meta-features** -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\nThe figure below shows how three different classifiers get trained. Their predictions get stacked and are used as features to train the meta-classifier which makes the final prediction.","81e17469":"**PassengerId** **62** and **830** have missing embarked values. Both have **Passenger class 1** and **fare $80**.\n\n\nNow, lets plot a graph to visualize and try to guess from where they embarked.","91515255":"### **Preview training set**","00ab4e1b":"### **Deck- Where exactly were passenger on the ship?**","49f0e834":"Here 0 stands for not survived and 1 stands for survived.\n\nSo, 549 people survived and 342 people did not survive.\n\nLet's visualize it by plotting.","6fba1066":"### **Check for missing values**","2df0fb22":"How Big is your family?","aebbec55":"Here 0 stands for not survived and 1 stands for survived.\n\nSo, we can see that Pclass plays a major role in survival.\n\nMajority of people survived in Pclass 1 while a large number of people do not survive in Pclass 3.","4fe2b907":"We can see that `Age` and `Fare` are measured on very different scaling. So we need to do feature scaling before predictions.","df45c7c9":"# **3. Basic Set Up** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Notebook Contents](#0.1)\n\n","9aceb6e3":"# **4. Data Exploration** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Notebook Contents](#0.1)\n","fdfa4462":"## **6.1 Missing Values Imputation** <a class=\"anchor\" id=\"6.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n","71f28aff":"### **Age Column**\n\nAge seems to be promising feature. So it doesnt make sense to simply fill null values out with median\/mean\/mode.\n\nWe will use Random Forest algorithm to predict ages.","67b59409":"Let's check the percentage of survival for males and females separately.","065afafb":"### **Do you have longer names?**","5e9fe1c8":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Notebook Contents**\n\n- [Part 1 - Introduction to Stacking](#1)\n- [Part 2 - Stacking is prone to Overfitting](#2)\n- [Part 3 - Basic Set Up](#3)\n   - [3.1 Import libraries](#3.1)\n   - [3.2 Load data](#3.2)\n- [Part 4 - Data Exploration](#4)\n- [Part 5 - Data Visualization](#5)\n- [Part 6 - Data Preprocessing](#6)\n- [Part 7 - Feature Engineering](#7)\n- [Part 8 - Categorical Encoding](#8)\n- [Part 9 - Feature Scaling](#9)\n- [Part 10 - Declare feature vector and target variable](#10)\n- [Part 11 - Individual Classifier](#11)\n- [Part 12 - Stacked Classifier](#12)\n\n"}}