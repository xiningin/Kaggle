{"cell_type":{"1f69026f":"code","40da26da":"code","00beb49b":"code","1f19d37f":"code","85bf0f07":"code","15f03a56":"code","04ab51dc":"code","198e013e":"code","54141c32":"code","25a76cee":"code","716e1e45":"code","b6951291":"code","8562c1bf":"code","43ad24b2":"code","e448954c":"code","22292cd6":"code","e4cfc120":"code","8ad6ae1c":"code","4edf7ca7":"code","25e87eae":"code","8409ef8f":"code","642a1d6d":"code","45fce40a":"code","4e8b32fb":"code","891bfe4d":"code","8ae2d6b6":"code","ea8d050b":"code","ed45b191":"code","93ea92d4":"markdown","e3f3d289":"markdown","620fa3ca":"markdown","3e39c253":"markdown","18773181":"markdown","6622e929":"markdown","592a0d76":"markdown","59477857":"markdown","8b59693f":"markdown","398afdb7":"markdown","0579e9ea":"markdown","16814671":"markdown","42f9c019":"markdown","a965b163":"markdown","c84469f1":"markdown","fb2b9b47":"markdown","470efb9e":"markdown","6d460077":"markdown","e4a05499":"markdown"},"source":{"1f69026f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/Data\"))\n\n# Any results you write to the current directory are saved as output.","40da26da":"from pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime as dt\nimport urllib.request, json\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler","00beb49b":"data_source = 'kaggle'\ndf = pd.read_csv(\"..\/input\/Data\/Stocks\/hpq.us.txt\",delimiter=',',usecols=['Date','Open','High','Low','Close'])\nprint('Loaded data from the Kaggle repository')","1f19d37f":"df.shape","85bf0f07":"df.head()","15f03a56":"# Sort DataFrame by date\ndf = df.sort_values('Date')\n\n# Double check the result\ndf.head()","04ab51dc":"range(0,df.shape[0],500)","198e013e":"df['Date'].loc[::500]","54141c32":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),(df['Low']+df['High'])\/2.0)\nplt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid Price',fontsize=18)\nplt.show()","25a76cee":"# First calculate the mid prices from the highest and lowest\nhigh_prices = df.loc[:,'High'].as_matrix()\nlow_prices = df.loc[:,'Low'].as_matrix()\nmid_prices = (high_prices+low_prices)\/2.0","716e1e45":"train_data = mid_prices[:11000]\ntest_data = mid_prices[11000:]","b6951291":"# Scale the data to be between 0 and 1\n# When scaling remember! You normalize both test and train data with respect to training data\n# Because you are not supposed to have access to test data\nscaler = MinMaxScaler()\ntrain_data = train_data.reshape(-1,1)\ntest_data = test_data.reshape(-1,1)","8562c1bf":"# Train the Scaler with training data and smooth data\nsmoothing_window_size = 2500\nfor di in range(0,10000,smoothing_window_size):\n    scaler.fit(train_data[di:di+smoothing_window_size,:])\n    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n\n# You normalize the last bit of remaining data\nscaler.fit(train_data[di+smoothing_window_size:,:])\ntrain_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])","43ad24b2":"# Reshape both train and test data\ntrain_data = train_data.reshape(-1)\n\n# Normalize test data\ntest_data = scaler.transform(test_data).reshape(-1)","e448954c":"# Now perform exponential moving average smoothing\n# So the data will have a smoother curve than the original ragged data\nEMA = 0.0\ngamma = 0.1\nfor ti in range(11000):\n    EMA = gamma*train_data[ti] + (1-gamma)*EMA\n    train_data[ti] = EMA\n\n# Used for visualization and test purposes\nall_mid_data = np.concatenate([train_data,test_data],axis=0)","22292cd6":"window_size = 100\nN = train_data.size\nstd_avg_predictions = []\nstd_avg_x = []\nmse_errors = []\n\nfor pred_idx in range(window_size,N):\n\n    if pred_idx >= N:\n        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n    else:\n        date = df.loc[pred_idx,'Date']\n\n    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n    std_avg_x.append(date)\n\nprint('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))","e4cfc120":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\nplt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Mid Price')\nplt.legend(fontsize=18)\nplt.show()","8ad6ae1c":"window_size = 100\nN = train_data.size\n\nrun_avg_predictions = []\nrun_avg_x = []\n\nmse_errors = []\n\nrunning_mean = 0.0\nrun_avg_predictions.append(running_mean)\n\ndecay = 0.5\n\nfor pred_idx in range(1,N):\n\n    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n    run_avg_predictions.append(running_mean)\n    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n    run_avg_x.append(date)\n\nprint('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))\n","4edf7ca7":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\nplt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n# plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Mid Price')\nplt.legend(fontsize=18)\nplt.show()","25e87eae":"class DataGeneratorSeq(object):\n\n    def __init__(self,prices,batch_size,num_unroll):\n        self._prices = prices\n        self._prices_length = len(self._prices) - num_unroll\n        self._batch_size = batch_size\n        self._num_unroll = num_unroll\n        self._segments = self._prices_length \/\/self._batch_size\n        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n\n    def next_batch(self):\n\n        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n\n        for b in range(self._batch_size):\n            if self._cursor[b]+1>=self._prices_length:\n                #self._cursor[b] = b * self._segments\n                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n\n            batch_data[b] = self._prices[self._cursor[b]]\n            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n\n            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n\n        return batch_data,batch_labels\n\n    def unroll_batches(self):\n\n        unroll_data,unroll_labels = [],[]\n        init_data, init_label = None,None\n        for ui in range(self._num_unroll):\n\n            data, labels = self.next_batch()    \n\n            unroll_data.append(data)\n            unroll_labels.append(labels)\n\n        return unroll_data, unroll_labels\n\n    def reset_indices(self):\n        for b in range(self._batch_size):\n            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n\n\n\ndg = DataGeneratorSeq(train_data,5,5)\nu_data, u_labels = dg.unroll_batches()\n\nfor ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n    print('\\n\\nUnrolled index %d'%ui)\n    dat_ind = dat\n    lbl_ind = lbl\n    print('\\tInputs: ',dat )\n    print('\\n\\tOutput:',lbl)","8409ef8f":"D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\nnum_unrollings = 50 # Number of time steps you look into the future.\nbatch_size = 500 # Number of samples in a batch\nnum_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\nn_layers = len(num_nodes) # number of layers\ndropout = 0.2 # dropout amount\n\ntf.reset_default_graph() # This is important in case you run this multiple times","642a1d6d":"# Input data.\ntrain_inputs, train_outputs = [],[]\n\n# You unroll the input over time defining placeholders for each time step\nfor ui in range(num_unrollings):\n    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))","45fce40a":"lstm_cells = [\n    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n                            state_is_tuple=True,\n                            initializer= tf.contrib.layers.xavier_initializer()\n                           )\n for li in range(n_layers)]\n\ndrop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n) for lstm in lstm_cells]\ndrop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n\nw = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\nb = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))","4e8b32fb":"# Create cell state and hidden state variables to maintain the state of the LSTM\nc, h = [],[]\ninitial_state = []\nfor li in range(n_layers):\n    c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n    h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n\n# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n# a specific format. Read more at: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/dynamic_rnn\nall_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n\n# all_outputs is [seq_length, batch_size, num_nodes]\nall_lstm_outputs, state = tf.nn.dynamic_rnn(\n    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n    time_major = True, dtype=tf.float32)\n\nall_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n\nall_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n\nsplit_outputs = tf.split(all_outputs,num_unrollings,axis=0)","891bfe4d":"# When calculating the loss you need to be careful about the exact form, because you calculate\n# loss of all the unrolled steps at the same time\n# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n\nprint('Defining training Loss')\nloss = 0.0\nwith tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n    for ui in range(num_unrollings):\n        loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n\nprint('Learning rate decay operations')\nglobal_step = tf.Variable(0, trainable=False)\ninc_gstep = tf.assign(global_step,global_step + 1)\ntf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\ntf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n\nlearning_rate = tf.maximum(\n    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n    tf_min_learning_rate)\n\n# Optimizer.\nprint('TF Optimization operations')\noptimizer = tf.train.AdamOptimizer(learning_rate)\ngradients, v = zip(*optimizer.compute_gradients(loss))\ngradients, _ = tf.clip_by_global_norm(gradients, 5.0)\noptimizer = optimizer.apply_gradients(\n    zip(gradients, v))\n\nprint('\\tAll done')","8ae2d6b6":"print('Defining prediction related TF functions')\n\nsample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n\n# Maintaining LSTM state for prediction stage\nsample_c, sample_h, initial_sample_state = [],[],[]\nfor li in range(n_layers):\n    sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n    sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n    initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n\nreset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n\nsample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n                                   initial_state=tuple(initial_sample_state),\n                                   time_major = True,\n                                   dtype=tf.float32)\n\nwith tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n    sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n\nprint('\\tAll done')","ea8d050b":"epochs = 30\nvalid_summary = 1 # Interval you make test predictions\n\nn_predict_once = 50 # Number of steps you continously predict for\n\ntrain_seq_length = train_data.size # Full length of the training data\n\ntrain_mse_ot = [] # Accumulate Train losses\ntest_mse_ot = [] # Accumulate Test loss\npredictions_over_time = [] # Accumulate predictions\n\nsession = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n# Used for decaying learning rate\nloss_nondecrease_count = 0\nloss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n\nprint('Initialized')\naverage_loss = 0\n\n# Define data generator\ndata_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n\nx_axis_seq = []\n\n# Points you start your test predictions from\ntest_points_seq = np.arange(11000,12000,50).tolist()\n\nfor ep in range(epochs):       \n\n    # ========================= Training =====================================\n    for step in range(train_seq_length\/\/batch_size):\n        \n        u_data, u_labels = data_gen.unroll_batches()\n\n        feed_dict = {}\n        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)): \n            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n\n        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n\n        average_loss += l\n\n    # ============================ Validation ==============================\n    if (ep+1) % valid_summary == 0:\n        average_loss = average_loss\/(valid_summary*(train_seq_length\/\/batch_size))\n\n      # The average loss\n        if (ep+1)%valid_summary==0:\n            print('Average loss at step %d: %f' % (ep+1, average_loss))\n\n        train_mse_ot.append(average_loss)\n\n        average_loss = 0 # reset loss\n\n        predictions_seq = []\n\n        mse_test_loss_seq = []\n        \n\n      # ===================== Updating State and Making Predicitons ========================\n        for w_i in test_points_seq:\n            mse_test_loss = 0.0\n            our_predictions = []\n\n            if (ep+1)-valid_summary==0:\n                # Only calculate x_axis values in the first validation epoch\n                x_axis=[]\n\n        # Feed in the recent past behavior of stock prices\n        # to make predictions from that point onwards\n            for tr_i in range(w_i-num_unrollings+1,w_i-1):\n                current_price = all_mid_data[tr_i]\n                feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n                _ = session.run(sample_prediction,feed_dict=feed_dict)\n\n            feed_dict = {}\n\n            current_price = all_mid_data[w_i-1]\n\n            feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n\n        # Make predictions for this many steps\n        # Each prediction uses previous prediciton as it's current input\n            for pred_i in range(n_predict_once):\n                pred = session.run(sample_prediction,feed_dict=feed_dict)\n                our_predictions.append(np.asscalar(pred))\n                feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n                if (ep+1)-valid_summary==0:\n                    # Only calculate x_axis values in the first validation epoch\n                    x_axis.append(w_i+pred_i)\n\n                mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n\n        session.run(reset_sample_states)\n\n        predictions_seq.append(np.array(our_predictions))\n\n        mse_test_loss \/= n_predict_once\n        mse_test_loss_seq.append(mse_test_loss)\n\n        if (ep+1)-valid_summary==0:\n            x_axis_seq.append(x_axis)\n\n    current_test_mse = np.mean(mse_test_loss_seq)\n\n      # Learning rate decay logic\n    if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n        loss_nondecrease_count += 1\n    else:\n        loss_nondecrease_count = 0\n\n    if loss_nondecrease_count > loss_nondecrease_threshold :\n        session.run(inc_gstep)\n        loss_nondecrease_count = 0\n        print('\\tDecreasing learning rate by 0.5')\n\n    test_mse_ot.append(current_test_mse)\n    print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n    predictions_over_time.append(predictions_seq)\n    print('\\tFinished Predictions')\n","ed45b191":"best_prediction_epoch = 28 # replace this with the epoch that you got the best results when running the plotting code\n\nplt.figure(figsize = (18,18))\nplt.subplot(2,1,1)\nplt.plot(range(df.shape[0]),all_mid_data,color='b')\n\n# Plotting how the predictions change over time\n# Plot older predictions with low alpha and newer predictions with high alpha\nstart_alpha = 0.25\nalpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)\/len(predictions_over_time[::3]))\nfor p_i,p in enumerate(predictions_over_time[::3]):\n    for xval,yval in zip(x_axis_seq,p):\n        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n\nplt.title('Evolution of Test Predictions Over Time',fontsize=18)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid Price',fontsize=18)\nplt.xlim(11000,12500)\n\nplt.subplot(2,1,2)\n\n# Predicting the best test prediction you got\nplt.plot(range(df.shape[0]),all_mid_data,color='b')\nfor xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n    plt.plot(xval,yval,color='r')\n\nplt.title('Best Test Predictions Over Time',fontsize=18)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid Price',fontsize=18)\nplt.xlim(11000,12500)\nplt.show()","93ea92d4":"**Prediction Related Calculations**","e3f3d289":"**Visualizing the Predictions**","620fa3ca":"**LSTM**","3e39c253":"### **Exponential Moving Average**","18773181":"**Loss Calculation and Optimizer**","6622e929":"Different time periods of data have different value ranges, we normalize the data by splitting the full series into windows. If we don't do this, the earlier data will be close to 0 and will not add much value to the learning process. Here you choose a window size of 2500.","592a0d76":"Reshape the data back to the shape of [data_size]","59477857":"### <b> Normalizing the data <\/b>","8b59693f":"### <b> Importing all the libraries <\/b>","398afdb7":"### <b> loading the dataset <\/b>","0579e9ea":"### <b> Data Visualization <\/b>","16814671":"**Running the LSTM**\n\n\nHere we will train and predict stock price movements for several epochs and see whether the predictions get better or worse over time. \n\nWe follow the following procedure.\n\n1. Define a test set of starting points (test_points_seq) on the time series to evaluate the model on \n2. For each epoch\n    *        For full sequence length of training data\n                    Unroll a set of num_unrollings batches\n                    Train the neural network with the unrolled batches\n    * Calculate the average training loss\n    * For each starting point in the test set\n                        Update the LSTM state by iterating through the previous num_unrollings data points found before the test point\n                        Make predictions for n_predict_once steps continuously, using the previous prediction as the current input\n                        Calculate the MSE loss between the n_predict_once points predicted and the true stock prices at those time stamps","42f9c019":"### **Standard Average **","a965b163":"**Calculating LSTM output and Feeding it to the regression layer to get final prediction**","c84469f1":"We can now smooth the data using the exponential moving average. This helps us to get rid of the inherent raggedness of the data in stock prices and produce a smoother curve.\n\n**Note:**  We should only smooth training data.","fb2b9b47":"**Defining Parameters of the LSTM and Regression layer**\n\n\nWe will have a three layers of LSTMs and a linear regression layer, denoted by w and b, that takes the output of the last Long Short-Term Memory cell and output the prediction for the next time step. We can use the MultiRNNCell in TensorFlow to encapsulate the three LSTMCell objects we created. Additionally, we can have the dropout implemented LSTM cells, as they improve performance and reduce overfitting.","470efb9e":" MinMaxScalar scales all the data to be in the region of 0 and 1. Reshape the training and test data to be in the shape [data_size, num_features].","6d460077":"### <b> Data Exploration <\/b>","e4a05499":"### <b> Splitting Data into a Training set and a Test set<\/b>"}}