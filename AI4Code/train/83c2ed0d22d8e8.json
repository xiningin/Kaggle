{"cell_type":{"9b4fc4d7":"code","49962d56":"code","b3334ed0":"code","2bb7f404":"code","4886a850":"code","eb55e595":"code","9b855a83":"code","0eede4d1":"code","b94c3c2f":"code","3a615f88":"code","51ce4ba1":"code","d54909c9":"markdown"},"source":{"9b4fc4d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","49962d56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn import metrics\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import *\nfrom keras.optimizers import *\nimport keras.backend as K\nfrom keras.callbacks import *\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom smart_open import smart_open\nimport datetime \nfrom keras.utils import multi_gpu_model\n\nimport os\nimport time\nimport gc\nimport re\nimport random\n#from unidecode import unidecode","b3334ed0":"def convertCats2sentence(x,cols):\n    \n    ref = [[col.lower() + '=' + (str(val)).lower() for val in x[col]] for col in cols]\n\n    # reformat list to be sentence lines, docs for doc2vec\n    \n    tmp = pd.DataFrame.from_records(ref)\n    \n    tmp = tmp.transpose()\n    \n    X = tmp.values.tolist()\n    return X\n\n\n#filename = '..\/input\/train.csv'\n\n#object_key = 'train.csv'\n\n#path = 's3:\/\/{}:{}@{}\/{}'.format(aws_key, aws_secret, bucket_name, object_key)\n\nchunksize = 100000\nheader = pd.read_csv('..\/input\/microsoft-malware-prediction\/train.csv',nrows=1)\ncols = list(set(header.head(0)) - set(['Census_PrimaryDiskTotalCapacity',\n            'Census_SystemVolumeTotalCapacity','HasDetections']))\n\nn =  8921484 - 1 \ns = 500000 #desired sample size # low just for show...\nskip = sorted(random.sample(range(1,n+1),n-s)) \n\nX = []\nt0 = time.time()\nsteps = 1\n#smart_open(path)\nfor chunk in pd.read_csv('..\/input\/microsoft-malware-prediction\/train.csv', chunksize=chunksize,dtype='category',skiprows=skip):\n    # chunk is a dataframe\n    print('step...'+str(steps))\n    X.extend(convertCats2sentence(chunk,cols))\n    steps = steps + 1\n    \nt1 = time.time()\nprint(t1-t0)\nprint('train converted...')\ngc.collect()","2bb7f404":"embed_size = 300\nmaxlen = len(cols)\nmax_features = None\nwindow_size = 40\nt0 = time.time()\nmodel = Word2Vec(X,size=embed_size,window=window_size,min_count=1)\nt1 = time.time()\nprint(t1-t0)\nprint('model fit...')\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features, filters='')\ntokenizer.fit_on_texts(X)\n\n","4886a850":"# read in all training data...for model training\n\nn =  8921484 - 1 \ns = 1000000 #desired sample size # low just for show\nskip = sorted(random.sample(range(1,n+1),n-s)) \n\nX = []\nt0 = time.time()\nsteps = 1\n#smart_open(path)\ngc.collect()\nfor chunk in pd.read_csv('..\/input\/microsoft-malware-prediction\/train.csv', chunksize=chunksize,dtype='category',skiprows=skip):\n    # chunk is a dataframe\n    print('step...'+str(steps))\n    X.extend(convertCats2sentence(chunk,cols))\n    steps = steps + 1\n    \nt1 = time.time()\nprint(t1-t0)\nprint('train converted...')\n\n\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=maxlen)\n\nword_index = tokenizer.word_index\nmax_features = len(word_index)+1\n\ntrain = pd.read_csv('..\/input\/microsoft-malware-prediction\/train.csv',usecols=['HasDetections'],skiprows=skip)\nY = train['HasDetections']\ndel train\ngc.collect()","eb55e595":"def load_wv(model, word_index):\n    embedding_matrix = np.random.normal(model.wv[model.wv.vocab].mean(), model.wv[model.wv.vocab].std(), \n                                        (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n            \n        try: \n            embedding_vector = model.wv[word]\n            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        except:\n            print(word, ' not found')\n            \n            \n    return embedding_matrix \n\nembedding_matrix = load_wv(model, word_index)","9b855a83":"embedding_matrix.shape","0eede4d1":"#object_key = 'test.csv'\n\n#path = 's3:\/\/{}:{}@{}\/{}'.format(aws_key, aws_secret, bucket_name, object_key)\n\n#t0 = time.time()\n#X_test = []\n#for chunk in pd.read_csv('test.csv', chunksize=chunksize,dtype='category'):\n    # chunk is a dataframe\n #   X_test.extend(convertCats2sentence(chunk,cols))\n    \n#t1 = time.time()\n#print(t1-t0)\n#print('test converted...')\n\n#X_test = tokenizer.texts_to_sequences(X_test)\n#X_test = pad_sequences(X_test, maxlen=maxlen)\n\nsub = pd.read_csv('..\/input\/microsoft-malware-prediction\/test.csv', usecols=['MachineIdentifier'])\n\n#gc.collect()","b94c3c2f":"def squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)\/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x \/ scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, \n                 kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n\n\ndef capsule():\n    K.clear_session()       \n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, \n                  weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(rate=0.2)(x)\n    x = Bidirectional(CuDNNLSTM(100, return_sequences=True, \n                                kernel_initializer=glorot_normal(seed=12300), \n                                recurrent_initializer=orthogonal(gain=1.0, \n                                                                 seed=10000)))(x)\n\n    x = Capsule(num_capsule=10, dim_capsule=5, routings=2, share_weights=True)(x)\n    x = Flatten()(x)\n\n    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n    x = Dropout(0.12)(x)\n    x = BatchNormalization()(x)\n    \n    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n    x = Dropout(0.2)(x)\n    x = BatchNormalization()(x)\n\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n    \n    return model\n\ndef auc(y_true, y_pred):\n    \n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=1)\n\n    return metrics.auc(fpr, tpr)","3a615f88":"kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\nbestscore = []\ny_test = np.zeros((sub.shape[0], ))\nfor i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n    filepath=\"weights_best.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n                                 verbose=2, save_best_only=True, mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, \n                                  min_lr=0.0001, verbose=2)\n    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, \n                                  patience=2, verbose=2, mode='auto')\n    callbacks = [checkpoint, reduce_lr]\n    with tf.device('\/cpu:0'):\n        model = capsule()\n    if i == 0:print(model.summary()) \n    \n    \n    #parallel_model = multi_gpu_model(model, gpus=2)\n    #parallel_model.compile(loss='binary_crossentropy', optimizer=Adam())\n    #parallel_model.fit(X_train, Y_train, batch_size=512, epochs=6, \n    #                   validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks )\n    #parallel_model.load_weights(filepath)\n    model.fit(X_train, Y_train, batch_size=512, epochs=20, \n                       validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks )\n    y_pred = model.predict([X_val], batch_size=1024, verbose=2)\n    #sub['HasDetections']  = np.squeeze(model.predict([X_test], batch_size=1024, verbose=2))\n    \n   # predict in batches to avoid memory overhead\n    \n    t0 = time.time()\n    yhat = []\n    step = 1\n    for X_test in pd.read_csv('..\/input\/microsoft-malware-prediction\/test.csv', chunksize=chunksize,dtype='category'):\n        X_test = pad_sequences(tokenizer.texts_to_sequences(\n            convertCats2sentence(X_test,cols)),maxlen=maxlen)\n        \n        yhat.extend(np.squeeze(model.predict([X_test],batch_size=1024,verbose=2)))\n        #print('predicted partition ',step,'...')\n        #step = step + 1\n        \n    \n    t1 = time.time()\n    print('test predicted....',t1-t0)\n\n    y_test += np.array(yhat)\/5\n    auc_val = auc(np.squeeze(Y_val), np.squeeze(y_pred))\n    \n    print('AUC: {:.4f}'.format(auc_val))\n    bestscore.append(auc_val)\n    gc.collect() # this is necessary to prevent crashing when using full dataset\n    \n    #now = datetime.datetime.now()\n    \n    #object_key = 'submission_' + str(i) + '_' + str(now) + '.csv'\n\n    #path = 's3:\/\/{}:{}@{}\/{}'.format(aws_key, aws_secret, bucket_name, object_key)\n\n    #sub.to_csv(smart_open(path,mode='w'), index=False)\n    ","51ce4ba1":"sub['HasDetections']  = y_test.reshape((-1, 1))\ntmp = pd.read_csv('..\/input\/lgbyhat\/lgb_submission.csv')\ntmp.columns = ['MachineIdentifier','HD']\nsub = pd.merge(sub,tmp)\nsub['mean'] = sub.mean(axis=1)\nsub.drop(['HD','HasDetections'],axis=1,inplace=True)\nsub.columns = ['MachineIdentifier','HasDetections']\nsub.to_csv(\"submission.csv\", index=False)","d54909c9":"The basic idea is to convert all entries to a variable key string pairing and treat each row as a blurb of text. The order doesn't necessarily matter so I widended the word2vec window to enclose the whole variable set. The kfold performance is around 0.72-0.73 but not reflected on the leaderboard. Memory bandwidth was an issue. I ended up running this script on AWS with a GPU instance (2 GPU's, 244 GB ram, g3.8xlarge). I ended up going north of 150 GB so needed to run the better GPU instance. I randomly selected a subset of samples to train the word2vec impression. The model fit was then performed with a second random subsample or the full dataset. I ended up crashing my aws instance due to GPU memory or general ram saturation. The random subsampling helped the code finish. I pulled the keras implementation from a previous competition which you can find online; everything is a pull of a pull after all. Basic s3 bucket syntax is below. I used smart_open to read from my s3 bucket. In the end I found loading the zip datasets directly to my jupyter instance were faster than reading from s3, and didn't stall. \n\nThe below code will run for a small sample set but not enough memory with kaggle to load all test data. This code was ran on AWS in about 3-4 hours when using 3 million samples to train word2vec and an additional 3 million to train neural network. I doubt I'll make a big move in the leaderboard but thought this was an interesting implementation for working with categorical features. I'd seen the 'treat all categoricals as strings' in an online word2vec forum. \n\nhttps:\/\/towardsdatascience.com\/a-non-nlp-application-of-word2vec-c637e35d3668\nhttps:\/\/towardsdatascience.com\/multi-state-lstms-for-categorical-features-66cc974df1dc\n\nAdded an average with https:\/\/www.kaggle.com\/stanislavblinov\/my-first-public-kernel-yet-another-lgbm\/output\n"}}