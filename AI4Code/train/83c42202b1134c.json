{"cell_type":{"5e97be2f":"code","4581e568":"code","da1b79d3":"code","5963d572":"code","5ef333f1":"code","10c69a77":"code","1a8776bc":"code","88f8f84e":"code","185bf584":"code","564d7388":"code","67e30837":"code","810e18c8":"code","593f2f75":"code","c415a77c":"code","cbe6c4b8":"code","f6792393":"code","68ed3a1f":"code","a9883d20":"code","06c7b66d":"code","26e21c51":"code","e2a6461e":"code","ca986a65":"code","600f4881":"code","fbd11501":"code","1744944d":"code","763ab653":"code","43365c5f":"code","9cf71cea":"code","56d26668":"code","5c35e9b2":"code","6030af55":"markdown","18b06e84":"markdown","549762e1":"markdown","70b9e676":"markdown","28061f02":"markdown","46e788cf":"markdown","f4932e26":"markdown","655c338d":"markdown","97733d57":"markdown","e23811f3":"markdown","4a22c1e0":"markdown"},"source":{"5e97be2f":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))\nimport operator\n\n#export\nfrom pathlib import Path\nfrom IPython.core.debugger import set_trace\nfrom fastai import datasets\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor, nn, optim\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler\nimport torch.nn.functional as F\n\ndef test(a,b,cmp,cname=None):\n    if cname is None: cname=cmp.__name__\n    assert cmp(a,b), f\"{cname}:\\n{a}\\n{b}\"\n\ndef test_eq(a,b): test(a,b,operator.eq, '==')\ndef near(a,b): return torch.allclose(a,b,1e-3,1e-5)\ndef test_near(a,b): test(a,b,near)\n    \n\nMNIST_URL='http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl'\n\ndef get_data():\n    path = datasets.download_data(MNIST_URL, ext='.gz')\n    with gzip.open(path, 'rb') as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n    return map(tensor, (x_train, y_train, x_valid, y_valid))\n\ndef normalize(x,m,s): return (x-m)\/s","4581e568":"x_train, y_train, x_valid, y_valid = get_data()\n\nn,m = x_train.shape\nc   = y_train.max()+1\nnh  = 50\nn,m,c,nh","da1b79d3":"class Model(nn.Module):\n    def __init__(self, ni, nh, no):\n        super().__init__()\n        self.layers = [nn.Linear(ni,nh), nn.ReLU(), nn.Linear(nh,no)]\n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x     \n\nmodel = Model(m,nh,10)\npred  = model(x_train); pred.shape","5963d572":"def log_softmax(x): return (x.exp()\/(x.exp().sum(-1,keepdim=True))).log()\nsm_pred = log_softmax(pred); sm_pred.shape, sm_pred[:3]","5ef333f1":"def nll(inp, targ): return -inp[range(targ.shape[0]), targ].mean()\nloss = nll(sm_pred, y_train); loss","10c69a77":"# Followings all are same\ndef log_softmax(x): return x - x.exp().sum(-1, keepdim=True).log()\ntest_near(nll(log_softmax(pred), y_train), loss)\n\ndef log_softmax(x): return x - x.logsumexp(-1, keepdim=True)\ntest_near(nll(log_softmax(pred), y_train), loss)\n\ntest_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)\ntest_near(F.cross_entropy(pred, y_train), loss)","1a8776bc":"loss_func = F.cross_entropy\ndef acc(out, yb): return (torch.argmax(out, -1)==yb).float().mean()","88f8f84e":"bs=64\nxb, yb = x_train[:bs], y_train[:bs]\npreds = model(xb)\nloss_func(preds, yb), acc(preds, yb)","185bf584":"lr=0.5; epochs=1\n\nfor e in range(epochs):\n    for i in range((n-1)\/\/bs+1):\n        xb = x_train[bs*i:bs*(i+1)]\n        yb = y_train[bs*i:bs*(i+1)]\n        loss = loss_func(model(xb), yb)\n        \n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad * lr\n                    l.weight.grad.zero_()\n                    l.bias.grad.zero_()\n\nloss_func(model(xb), yb), acc(model(xb), yb)","564d7388":"class Model(nn.Module):\n    def __init__(self, ni, nh, no):\n        super().__init__()\n        self.l1, self.l2 = nn.Linear(ni,nh), nn.Linear(nh,no)\n    def __call__(self, x): return self.l2(F.relu(self.l1(x)))","67e30837":"model = Model(m,nh,10)\nfor name, l in model.named_children(): print(f\"{name}: {l}\")","810e18c8":"model","593f2f75":"def fit():\n    for e in range(epochs):\n        for i in range((n-1)\/\/bs + 1):\n            xb = x_train[bs*i:bs*(i+1)]\n            yb = y_train[bs*i:bs*(i+1)]\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            \n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad*lr\n                model.zero_grad()\n                \nfit()\nloss_func(model(xb), yb), acc(model(xb), yb)","c415a77c":"class DummyModule():\n    def __init__(self,ni,nh,no):\n        self._modules = {}\n        self.l1, self.l2 = nn.Linear(ni,nh), nn.Linear(nh,no)\n    def __setattr__(self,k,v):\n        if not k.startswith(\"_\"): self._modules[k] = v\n        super().__setattr__(k,v)\n    def __repr__(self): return f'{self._modules}'\n        \n    def parameters(self):\n        for l in self._modules.values():\n            for p in l.parameters(): yield p\n\nmdl = DummyModule(m,nh,10); mdl","cbe6c4b8":"[o.shape for o in mdl.parameters()]","f6792393":"layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i, l in enumerate(self.layers): self.add_module(f'layer{i}', l)\n    def __call__(self, x):\n        for l in self.layers : x = l(x)\n        return x\n\nmodel = Model(layers); model","68ed3a1f":"class SeqModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n    def __call__(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\nmodel = SeqModel(layers); model\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)); model\n\nfit()\nloss_func(model(xb), yb), acc(model(xb), yb)","a9883d20":"class Optimizer():\n    def __init__(self, params, lr=0.5): \n        self.params, self.lr = list(params), lr\n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= p.grad*lr\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()","06c7b66d":"model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\nopt   = Optimizer(model.parameters())\n\nfor e in range(epochs):\n    for i in range((n-1)\/\/bs +1):\n        xb = x_train[bs*i:bs*(i+1)]\n        yb = y_train[bs*i:bs*(i+1)]\n        loss = loss_func(model(xb), yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nloss,accuracy = loss_func(model(xb),yb), acc(model(xb),yb); loss, accuracy","26e21c51":"from torch import optim\n\ndef get_model():\n    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n    return model, optim.SGD(model.parameters(), lr)\n\nmodel, opt = get_model()\nloss_func(model(xb), yb)","e2a6461e":"class Dataset():\n    def __init__(self,x,y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i], self.y[i]\n    \ntrain_ds, valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\nassert len(train_ds) == len(x_train)\nassert len(valid_ds) == len(x_valid)\n\nfor e in range(epochs):\n    for i in range((n-1)\/\/bs +1):\n        xb,yb = train_ds[i*bs:(i+1)*bs]\n        loss_func(model(xb), yb).backward()\n        opt.step()\n        opt.zero_grad()\n\nloss,accuracy = loss_func(model(xb),yb), acc(model(xb),yb); loss,accuracy","ca986a65":"class DataLoader():\n    def __init__(self,ds,bs): self.ds,self.bs = ds,bs\n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,-1)); yb[0]","600f4881":"model, opt = get_model()\n\ndef fit():\n    for e in range(epochs):\n        for xb,yb in train_dl:\n            loss_func(model(xb),yb).backward()\n            opt.step(); opt.zero_grad()\nfit()\nloss,accuracy = loss_func(model(xb),yb), acc(model(xb),yb); loss,accuracy","fbd11501":"class Sampler():\n    def __init__(self,ds,bs,shuffle=False):\n        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n    def __iter__(self):\n        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n        for i in range(0,self.n,self.bs): yield self.idxs[i:i+self.bs]\n            \nsmall_ds = Dataset(*train_ds[:10])\n\ns1, s2 = Sampler(small_ds,3), Sampler(small_ds,3,True)\n[o for o in s1], [o for o in s2]","1744944d":"for oo in s1:\n    for o in oo:\n        print(o)","763ab653":"for oo in s1:\n    for o in oo:\n        print(o)\n    break","43365c5f":"for oo in s1:\n    for o in oo:\n        print(o)\n        break","9cf71cea":"def collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs), torch.stack(ys)\n\nclass DataLoader():\n    def __init__(self,ds,sampler,collate_fn=collate):\n        self.ds,self.sampler,self.collate_fn=ds,sampler,collate_fn\n    def __iter__(self):\n        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])\n            \ntrain_samp = Sampler(train_ds,bs,True)\nvalid_samp = Sampler(valid_ds,bs)\ntrain_dl   = DataLoader(train_ds,train_samp)\nvalid_dl   = DataLoader(valid_ds,valid_samp)\nmodel,opt = get_model()\nfit()\nloss_func(model(xb),yb), acc(model(xb),yb)","56d26668":"from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n\ntrain_dl = DataLoader(train_ds,bs,sampler=RandomSampler(train_ds),collate_fn=collate)\nvalid_dl = DataLoader(valid_ds,bs,sampler=SequentialSampler(valid_ds),collate_fn=collate)\nmodel,opt = get_model()\nfit(); loss_func(model(xb),yb), acc(model(xb),yb)","5c35e9b2":"def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for e in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss_func(model(xb),yb).backward()\n            opt.step(); opt.zero_grad()\n        \n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc=0.,0.\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                tot_loss += loss_func(pred,yb)\n                tot_acc  += acc(pred,yb)\n        nv = len(valid_dl)\n        print(e, tot_loss\/nv, tot_acc\/nv)\n    return tot_loss\/nv, tot_acc\/nv\n\ndef get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds,bs  ,True ,**kwargs),\n            DataLoader(valid_ds,bs*2,      **kwargs))\n\nmodel,opt = get_model()\nloss,accu = fit(5,model,loss_func,opt,*get_dls(train_ds,valid_ds,bs))","6030af55":"## Valid","18b06e84":"## Dataset and DataLoader","549762e1":"## PyTorch DataLoader","70b9e676":"# Lesson video & note\n\nhttps:\/\/course.fast.ai\/videos\/?lesson=8                     \nhttps:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl2\/03_minibatch_training.ipynb","28061f02":"## Registering Modules","46e788cf":"## Using param and optim","f4932e26":"## Optim","655c338d":"## Basic Training Loop","97733d57":"## Cross Entropy Loss","e23811f3":"## nn.ModuleList and nn.Sequential","4a22c1e0":"## Random Sampling"}}