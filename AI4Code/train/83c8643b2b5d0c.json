{"cell_type":{"c22b2076":"code","e362d4bf":"code","7727d30b":"code","f66986c4":"code","362e3788":"code","2ae4c2a6":"code","f8cc6eea":"code","6a6eaeb8":"code","f549c9bf":"code","dc75a96e":"code","14c9ad8f":"code","3a8e4097":"code","41768425":"code","18745d86":"code","68a85ea2":"code","fe519fd5":"code","0903bfc6":"code","3b6dfbe0":"code","881f0222":"code","fa4578bc":"code","804f9940":"code","837acc05":"code","172e1ba3":"code","aea3cafb":"code","a523404b":"code","455d6e18":"markdown","5ca63a23":"markdown","47b2bbf4":"markdown","d5eb75f8":"markdown","a13c8839":"markdown","9d0551ea":"markdown","2f08813e":"markdown"},"source":{"c22b2076":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nsns.set()\n\n","e362d4bf":"data = pd.read_csv(\"..\/input\/titanic-extended\/full.csv\")","7727d30b":"data.head(10) #view the first few column of the dataset","f66986c4":"data.shape #view the shape of the dataset (m,n)","362e3788":"sns.countplot(x=\"Survived\", hue = \"Sex\", data=data) #analysis with visulisation","2ae4c2a6":"sns.countplot(x=\"Survived\", hue = \"Pclass\", data=data)","f8cc6eea":"data[\"Age\"].plot.hist()","6a6eaeb8":"data.info()","f549c9bf":"data.isnull().sum() #lists the number of null values in each feature","dc75a96e":"data.head(5)","14c9ad8f":"data.drop([\"Hometown\",\n           \"Age_wiki\",\n           \"Name_wiki\", \n           \"Ticket\",\n           \"Cabin\",\n           \"Destination\",\n           \"Body\",\n           \"WikiId\", \n           \"Name\",\"Boarded\",\n           \"Lifeboat\",\n           \"Name\",\n           \"PassengerId\"],axis=1, inplace= True)","3a8e4097":"data.head(5)\n","41768425":"data.dropna(inplace=True) #drop all the data entries with missing values","18745d86":"data.isnull().sum()","68a85ea2":"sex = pd.get_dummies(data[\"Sex\"], drop_first=True)\nembark = pd.get_dummies(data[\"Embarked\"], drop_first=True)\nclss = pd.get_dummies(data[\"Class\"], drop_first=True)\ndata.drop('Pclass', axis=1,inplace=True)","fe519fd5":"data=pd.concat([data,sex,embark,clss], axis=1)\ndata.head(5)","0903bfc6":"data.drop([\"Sex\",\"Embarked\",'Class'],axis=1,inplace=True) ","3b6dfbe0":"data.head(5)","881f0222":"x=data.drop(\"Survived\",axis=1) #features\ny=data[\"Survived\"] #target variable","fa4578bc":"\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=42)","804f9940":"#build the model and train the model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(x_train, y_train)","837acc05":"#perform predictions\npredic = model.predict(x_test)","172e1ba3":"#analyse the accuracy of the model\nreport = classification_report(y_test,predic)","aea3cafb":"print(report)","a523404b":"\naccuracy_score(y_test, predic)","455d6e18":"# Split the data for training","5ca63a23":"**Input Data**","47b2bbf4":"# **Perform data analysis**","d5eb75f8":"# Perfom data cleaning","a13c8839":"Drop column with high number of null values and duplicate columns","9d0551ea":"# **Importing all the important Libraries**","2f08813e":"**handling categorial variables******"}}