{"cell_type":{"8877b034":"code","154cf6ff":"code","ff3c71ed":"code","906c2ade":"code","93d651a9":"code","e96e2c75":"code","7b03fdc2":"code","8bd2f496":"code","9e2c3811":"code","1f7e9705":"code","31189681":"code","dcc5d92e":"code","18ffa892":"code","6f99f265":"code","b49213b2":"code","fc4e391f":"code","687959d2":"code","8cba6d27":"code","4713eaef":"code","5c4d0004":"code","02f0950c":"code","746e7328":"code","17a2380e":"code","3480d19a":"code","a2c30a27":"code","6609de02":"code","6bcac3fd":"code","65e63d83":"code","3188784c":"code","f4544dab":"code","76288ca1":"code","219b00cf":"markdown","7a2b72eb":"markdown","49b244c3":"markdown","7da4b646":"markdown","a989b462":"markdown","edb311bf":"markdown","d5fa64e3":"markdown","617e84b1":"markdown","8e21b75b":"markdown"},"source":{"8877b034":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport os\nroot_path = '\/kaggle\/input\/commonlitreadabilityprize\/'\nos.listdir(root_path)","154cf6ff":"train = pd.read_csv(root_path + \"\/train.csv\")\ntest = pd.read_csv(root_path + \"\/test.csv\")\nsample_submission = pd.read_csv(root_path + \"\/sample_submission.csv\")\n\ndisplay(train.info())\nprint(\"\\n\\n\")\ndisplay(test.info())\nprint(\"\\n\\n\")\ndisplay(sample_submission.info())","ff3c71ed":"display(train.sample(), test.sample(), sample_submission.sample())","906c2ade":"original_cols = ['id', 'url_legal', 'license', 'excerpt', 'target', 'standard_error']\n\ntrain_stats = (pd.concat([train[original_cols].apply(lambda x: x.nunique(), axis = 0)\n                          .rename(\"distinct_values\").to_frame(),\n                          train[original_cols].apply(lambda x: x.notna().sum(), axis = 0)\n                          .rename(\"not_nan_values\").to_frame()], 1)\n              .reset_index().rename({'index': 'variable'}, axis = 1))\n\ntrain_stats['distinct_over_notnan_percentage'] = (train_stats['distinct_values']\/train_stats['not_nan_values']).round(2)\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 6), gridspec_kw={'width_ratios': [1.2, 1]})\n\nplt.style.use('fivethirtyeight')\n\nsns.set_context(rc = {'patch.linewidth': 2.0})\nsns.barplot(x = 'variable', \n            y = 'distinct_values',\n            data = train_stats,\n            edgecolor = 'black',\n            linewidth = 2,\n            palette=\"pastel\",\n            ax = ax[0])\n\nfor index, row in train_stats.iterrows():\n    value = row.distinct_values\n    ax[0].text(index, value+20, value, color='black', ha=\"center\", \n               fontsize = 15, fontweight = 'bold')\n\nax[0].grid(True)\nax[0].legend(fontsize=18)\nax[0].set_title('Distinct values for each variable', fontsize = 18, fontweight = 'bold')\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\nax[0].set_xlabel('')\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation = 35, fontsize = 13, color = 'black')\nax[0].set_ylabel('distinct_values', fontsize = 18, color ='black')\nplt.subplots_adjust(hspace = 0.3)\n\nsns.barplot(x = 'variable', \n            y = 'not_nan_values',\n            data = train_stats,\n            palette=\"pastel\",\n            linewidth = 2,\n            edgecolor = 'black',\n            ax = ax[1])\n\nfor index, row in train_stats.iterrows():\n    value = row.not_nan_values\n    ax[1].text(index, value+20, value, color='black', ha=\"center\", fontsize = 15, fontweight = 'bold')\n    \nax[1].grid(True)\nax[1].set_title('Not NaN values for each variable', fontsize = 18, fontweight = 'bold')\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)\nax[1].set_xlabel('')\nax[1].set_xticklabels(ax[0].get_xticklabels(), rotation = 35, fontsize = 13, color = 'black')\nax[1].set_ylabel('not_nan_values', fontsize = 18, color ='black')\n\nfig,ax = plt.subplots(1, 1, figsize = (20, 8))\n\nbbox=[-0.2, 0, 1.2, 0.9]\nax.axis('off')\nax.title.set_text('')\nccolors = plt.cm.BuPu(np.full(len(train_stats.columns), 0.1))\n\nmpl_table = ax.table(cellText = train_stats.values, bbox=bbox, colLabels=train_stats.columns, colColours=ccolors)\nmpl_table.auto_set_font_size(False)\nmpl_table.auto_set_column_width(col=list(range(len(train_stats.columns))))\nmpl_table.set_fontsize(18)","93d651a9":"assert len(train.loc[(train.url_legal.notna()) & (train.license.isna())] ) == 0","e96e2c75":"plt.style.use('ggplot')\nfig, ax = plt.subplots(2, 1, figsize = (20, 12))\n\nfig.suptitle(\"Distribution when url_legal is NaN or not\", fontsize = 20, fontweight = 'bold')\nplt.title(\"in private set we won't have url_legal nor license\", fontsize = 12)\n\nsns.histplot(data = train.loc[train.url_legal.notna()], x = 'target', \n             ax = ax[0], kde=True, bins = 50,\n             stat = 'density', color = 'red', edgecolor = 'black',\n             alpha = 0.3, label = 'url_legal not NaN', \n             linewidth = 3, line_kws= {'linewidth': 3})\n\nsns.histplot(data = train.loc[train.url_legal.isna()], x = 'target', \n             ax = ax[0], kde=True, bins = 50, stat = 'density', \n             alpha = 0.3,  label = 'url_legal NaN', edgecolor = 'black',\n             linewidth = 3, line_kws= {'linewidth': 3})\n\nax[0].legend(fontsize=18)\nax[0].set_xlabel('target', fontsize = 18)\nax[0].set_ylabel('Density', fontsize = 18)\nax[0].set_title('target distribution with url_legal NaN or not', fontsize = 15)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nsns.histplot(data = train.loc[train.url_legal.notna()], x = 'standard_error', \n             ax = ax[1], kde=True, bins = 50,\n             stat = 'density', color = 'red',\n             alpha = 0.3, label = 'url_legal not NaN',\n             edgecolor = 'black',\n             linewidth = 3, line_kws= {'linewidth': 3})\n\nsns.histplot(data = train.loc[train.url_legal.isna()], x = 'standard_error', \n             ax = ax[1], kde=True, bins = 50, stat = 'density', \n             alpha = 0.3,  label = 'url_legal NaN',\n             edgecolor = 'black',\n             linewidth = 3, line_kws= {'linewidth': 3})\n\nax[1].legend(fontsize=18)\nax[1].set_xlabel('standard_error', fontsize = 18)\nax[1].set_ylabel('Density', fontsize = 18)\nax[1].set_xlim(0.4, 0.7)\nax[1].set_title('standard_error distribution with url_legal NaN or not', \n                fontsize = 15)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)\n\nplt.subplots_adjust(hspace = 0.3)","7b03fdc2":"# target,standard_error distributions including also violin plots\n# aggiungere text con percentili e violinplot\n\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 6), gridspec_kw={'width_ratios': [1.5, 0.6]})\n\npercentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\npercentiles = train['target'].quantile(percentiles_asked).tolist()\n\nsns.histplot(data = train, x = 'target', ax = ax[0], kde=False, bins = 50,\n             stat = 'density', \n             alpha = 0.5, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n             #line_kws= {'linewidth': 5, 'color': 'red', 'alpha': 0.6}\n            )\n\nsns.kdeplot(data = train, x = 'target', ax = ax[0], alpha = 0.01, fill = True, \n            linewidth = 5, color = 'blue')\n\nfor m, percentile in enumerate(percentiles):\n        ax[0].axvline(percentile, alpha = 0.35, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        ax[0].text(percentile-0.16, 0.43, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n        \nmean = train.target.mean().round(2)\nmedian = train.target.median().round(2)\nst_dev = train.target.std().round(2)\n\nax[0].text(-4.4, 0.4, \"mean: {}\".format(mean), size = 12, alpha = 1)\nax[0].text(-4.4, 0.36, \"median: {}\".format(median), size = 12, alpha = 1)\nax[0].text(-4.4, 0.32, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n\n#https:\/\/stackoverflow.com\/questions\/49926147\/how-to-modify-edge-color-of-violinplot-using-seaborn\/55131881 \n#per cambiare colore linea esterna\nsns.violinplot(y='target', data = train, ax=ax[1], inner = 'quartile',)\nfor l in ax[1].lines:\n    l.set_linestyle('--')\n    l.set_color('yellow')\n    l.set_alpha(0.2)\n\nax[0].set_ylabel('Density', fontsize = 15)\nax[1].set_ylabel('target', fontsize = 15)\nax[0].set_xlabel('target', fontsize = 15)\nax[0].set_title('hist-kde plot', fontsize = 16)\nax[1].set_title('violin plot with quartiles', fontsize = 16)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n#plt.subplots_adjust(hspace = 0.8)\nfig.suptitle('Distribution of variable target', fontsize = 20, fontweight = 'bold')","8bd2f496":"# target,standard_error distributions including also violin plots\n# aggiungere text con percentili e violinplot\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 6), gridspec_kw={'width_ratios': [1.5, 0.6]})\n\npercentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\npercentiles = train['standard_error'].quantile(percentiles_asked).tolist()\nprint(percentiles)\n\nsns.histplot(data = train, x = 'standard_error', ax = ax[0], kde=False, bins = 50,\n             stat = 'density', \n             alpha = 0.5, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n            )\n\nsns.kdeplot(data = train, x = 'standard_error', ax = ax[0], alpha = 0.01, fill = True, \n            linewidth = 3, color = 'blue')\n\n#for m, percentile in enumerate(percentiles):\n#        ax[0].axvline(percentile, alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n#        ax[0].text(percentile-0.01, 16.5, \"{}\".format(percentiles_asked[m]), size = 12, alpha = 1)\n\nsns.violinplot(y='standard_error', data = train, ax=ax[1], inner = 'quartile')\nfor l in ax[1].lines:\n    l.set_linestyle('--')\n    l.set_color('yellow')\n    l.set_alpha(0.3)\n    \nmean = train.standard_error.mean().round(2)\nmedian = train.standard_error.median().round(2)\nst_dev = train.standard_error.std().round(2)\n\nax[0].text(0.0, 16, \"mean: {}\".format(mean), size = 12, alpha = 1)\nax[0].text(0.0, 14, \"median: {}\".format(median), size = 12, alpha = 1)\nax[0].text(0.0, 12, \"std deviation: {}\".format(st_dev), size = 12, alpha = 1)\n\nax[0].set_ylabel('Density', fontsize = 15)\nax[1].set_ylabel('standard_error', fontsize = 15)\nax[0].set_xlabel('standard_error', fontsize = 15)\nax[0].set_title('hist-kde plot', fontsize = 16)\nax[1].set_title('violin plot with quartiles', fontsize = 16)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n#ax[0].set_xlim(0.4, 0.7)\nax[0].set_ylim(0, 17)\nfig.suptitle('Distribution of variable standard_error', fontsize = 20, fontweight = 'bold')\nplt.subplots_adjust(hspace = 0.6)\n\nax[0].annotate('crazy outlier in standard_error', xy=(0.0, 0),  xycoords='data',\n            xytext=(0.4, 0.2), textcoords='axes fraction', fontsize = 14,\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            horizontalalignment='right', verticalalignment='top',\n            )","9e2c3811":"from sklearn.preprocessing import StandardScaler\n\nst_sc = StandardScaler()\n# comparison between the 2 after standardizing\n\n# target,standard_error distributions including also violin plots\n# aggiungere text con percentili e violinplot\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 6))#, gridspec_kw={'height_ratios': [0.6, 1.5]})\n\n#sns.histplot(x = train['standard_error'],  bins = 20,\n#            ax = ax[0], alpha = 0.25, fill = True, label = 'standard_error', \n#            linewidth = 3, color = 'blue')\n#\n#sns.histplot(x = train['target'], bins = 10,\n#            ax = ax[0], alpha = 0.25, fill = True, label = 'target', \n#            linewidth = 3, color = 'red', common_norm = True)\n\nsns.kdeplot(x = st_sc.fit_transform(train[['standard_error']])[:, 0], \n            ax = ax, alpha = 0.25, fill = True, label = 'standard_error_normalized', \n            linewidth = 3, color = 'blue')\n\nsns.kdeplot(x = st_sc.fit_transform(train[['target']])[:, 0], \n            ax = ax, alpha = 0.25, fill = True, label = 'target_normalized', \n            linewidth = 3, color = 'red')\n\nax.legend(fontsize = 20, loc = 'upper left')\n\nax.annotate('crazy outlier in standard_error', xy=(-14, 0),  xycoords='data',\n            xytext=(0.3, 0.1), textcoords='axes fraction', fontsize = 14,\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            horizontalalignment='right', verticalalignment='top',\n            )\n\nax.set_ylabel('Density', fontsize = 15)\nfig.suptitle('Distribution of target and standard_error after normalization', \n             fontsize = 20, fontweight = 'bold')\n\nplt.subplots_adjust(hspace = 0.6)","1f7e9705":"# score based on number of letters\/words\nfig, ax = plt.subplots(2, 1, figsize = (20, 14))\nsns.scatterplot(y = 'target', \n            x = 'standard_error',\n            data = train, \n            ax = ax[0], \n            color = 'blue', \n            sizes = [5],\n               alpha = 0.3)\n\nax[0].set_title(\"target vs standard_error\")\nax[0].set_ylabel('target', fontsize = 15)\nax[0].set_xlabel('standard_error', fontsize = 15)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nfig.suptitle(\"target vs standard_error\", fontsize = 20, fontweight = 'bold')\n\nax[0].set_ylim(-4, 2)\nax[0].set_xlim(0, 1)\n\nax[0].annotate('crazy outlier in standard_error', xy=(0, 0),  xycoords='data',\n            xytext=(0.4, 0.95), textcoords='axes fraction',fontsize = 14,\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            horizontalalignment='right', verticalalignment='top',\n            )\n\nx = st_sc.fit_transform(train[['standard_error']])[:, 0]\ny = st_sc.fit_transform(train[['target']])[:, 0]\n\nsns.scatterplot(y = y, \n            x = x,\n            ax = ax[1], \n            color = 'blue', \n            sizes = [5],\n               alpha = 0.3)\n\nax[1].set_title(\"target vs standard_error after normalization\")\nax[1].set_ylabel('target', fontsize = 15)\nax[1].set_xlabel('standard_error', fontsize = 15)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)","31189681":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression(fit_intercept = True)\n\nlr.fit(y = train['target'], X = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len().to_frame())\nfitted = lr.predict(X = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len().to_frame())\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 8))\nsns.scatterplot(y = train['target'], x = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\", regex=True).str.len(),\n                ax = ax, color= 'blue', sizes= [5], alpha= 0.3)\n\nsns.lineplot(x = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\", regex = True).str.len(),\n             y = fitted, ax = ax, linewidth = 5, color = 'red')\n\nax.set_ylabel('target', fontsize = 15)\nax.set_xlabel('number of letters in excerpt', fontsize = 15)\nax.set_title('Negative correlation between number of letters in excerpt and target', fontsize = 13)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\n\nfig.suptitle(\"Number of letters in excerpt vs target (fitted with Sklearn)\", fontsize = 20, fontweight = 'bold')\n","dcc5d92e":"fig, axes = plt.subplots(3, 2, figsize = (16, 16))\n\nax = axes.ravel()\n\nsns.regplot(y = train['target'], x = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len(),\n            ax = ax[0], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, \n            line_kws = {'color': 'red', 'linewidth': 3, 'alpha': 0.5})\n\nsns.regplot(y = train['standard_error'], x = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len(),\n            ax = ax[1], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, \n            line_kws = {'color': 'red', 'linewidth': 3, 'alpha': 0.5})\n\n\n\nlr = LinearRegression()\n\n\nax[0].set_ylabel('target', fontsize = 15)\nax[0].set_xlabel('Number of letters in Excerpt', fontsize = 15)\nax[0].set_title('Number of letters in excerpt vs target', fontsize = 16)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nax[1].set_ylabel('standard_error', fontsize = 15)\nax[1].set_xlabel('Number of letters in Excerpt', fontsize = 15)\nax[1].set_title('Number of letters in excerpt vs standard_error', fontsize = 16)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)\nax[1].set_ylim(0.425, 0.65)\n\n\nsns.regplot(y = train['target'], x = train['excerpt'].str.split().str.len(),\n                ax = ax[2], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\nsns.regplot(y = train['standard_error'], x = train['excerpt'].str.split().str.len(),\n                ax = ax[3], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\n\nax[2].set_ylabel('target', fontsize = 15)\nax[2].set_xlabel('Number of words in Excerpt', fontsize = 15)\nax[2].set_title('Number of words in excerpt vs target', fontsize = 16)\nax[2].tick_params(axis='both', which='major', labelsize=14)\nax[2].tick_params(axis='both', which='minor', labelsize=14)\n\nax[3].set_ylabel('standard_error', fontsize = 15)\nax[3].set_xlabel('Number of words in Excerpt', fontsize = 15)\nax[3].set_title('Number of words in excerpt vs standard_error', fontsize = 16)\nax[3].tick_params(axis='both', which='major', labelsize=14)\nax[3].tick_params(axis='both', which='minor', labelsize=14)\nax[3].set_ylim(0.425, 0.65)\n\nsns.regplot(y = train['target'], x = train['excerpt'].str.split(pat='[.!?]+').str.len(),\n                ax = ax[4], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\nsns.regplot(y = train['standard_error'], x = train['excerpt'].str.split(pat='[.!?]+').str.len(),\n                ax = ax[5], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\n\nax[4].set_ylabel('target', fontsize = 15)\nax[4].set_xlabel('Number of sentences in Excerpt', fontsize = 15)\nax[4].set_title('Number of sentences in excerpt vs target', fontsize = 16)\nax[4].tick_params(axis='both', which='major', labelsize=14)\nax[4].tick_params(axis='both', which='minor', labelsize=14)\n\nax[5].set_ylabel('standard_error', fontsize = 15)\nax[5].set_xlabel('Number of sentences in Excerpt', fontsize = 15)\nax[5].set_title('Number of sentences in excerpt vs standard_error', fontsize = 16)\nax[5].tick_params(axis='both', which='major', labelsize=14)\nax[5].tick_params(axis='both', which='minor', labelsize=14)\nax[5].set_ylim(0.425, 0.65)\n\nplt.subplots_adjust(hspace = 0.3)\n\nfig.suptitle(\"Number of letters\/words\/sentences in excerpt vs target\/standard_error (using sns.regplot)\", \n             fontsize = 20, fontweight = 'bold')","18ffa892":"corr_df = train.copy()\n\ncorr_df['n_letters_excerpt'] = corr_df['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len()\ncorr_df['n_words_excerpt'] = corr_df['excerpt'].str.split().str.len()\ncorr_df['n_sentences_excerpt'] = corr_df['excerpt'].str.split(pat = '[.!?]+').str.len()\n\ncorr_df = corr_df[['target', 'standard_error', 'n_letters_excerpt', 'n_words_excerpt', 'n_sentences_excerpt']]\n\ncorr_matrix = round(corr_df.corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \nfig, ax = plt.subplots(1, 1, figsize = (10, 10))\ncolors = sns.color_palette('rocket', 21)\nlevels = np.linspace(-1, 1, 21)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            cmap = cmap_plot, norm = norm, annot_kws={\"size\": 15, \"color\": 'black'})\nax.hlines([0, 1, 2, 3, 4], *ax.get_xlim(), color = 'black')\nax.vlines([0, 1, 2, 3, 4], *ax.get_ylim(), color = 'black')\nax.xaxis.set_ticks_position('bottom')\nax.set_title('Distinct values for each variable', fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax.set_yticklabels(ax.get_yticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax.xaxis.label.set_size(14)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(0.5, 2.5, 'o',\n        ms=circle_rad * 2, mec='w', mfc='none', mew=4)\n\nfig.suptitle('Correlation Matrix for {}'.format('train.csv'), \n             fontsize = 20, color = 'black', fontweight = 'bold')\nplt.title(\"Just to sum up: as seen before there's a negative correlation between number of letters and target\", fontsize = 12)\nfig.show()\n    ","6f99f265":"corr_df['average_word_length'] = (corr_df['n_letters_excerpt']\/corr_df['n_words_excerpt']).round(3)\ncorr_df['average_sentence_length'] = (corr_df['n_sentences_excerpt']\/corr_df['n_words_excerpt']).round(3)\n\ncorr_matrix = round(corr_df.corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \nfig, ax = plt.subplots(1, 1, figsize = (8, 8))\ncolors = sns.color_palette('rocket', 21)\nlevels = np.linspace(-1, 1, 21)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax, \n            cmap = cmap_plot, norm = norm, annot_kws={\"size\": 15, \"color\": 'black'})\nax.hlines([0, 1, 2, 3, 4, 5, 6], *ax.get_xlim(), color = 'black')\nax.vlines([0, 1, 2, 3, 4, 5, 6], *ax.get_ylim(), color = 'black')\nax.xaxis.set_ticks_position('bottom')\nax.set_title('Distinct values for each variable', fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, fontsize = 12, color = 'black')\nax.set_yticklabels(ax.get_yticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax.xaxis.label.set_size(14)\n\ncircle_rad = 25  # This is the radius, in points\nax.plot(0.5, 5.5, 'o',\n        ms=circle_rad * 2, mec='w', mfc='none', mew=4)\n\nfig.suptitle('Correlation Matrix for {}, after creating average_word_length'.format('train.csv'), \n             fontsize = 20, color = 'black', fontweight = 'bold')\nplt.title(\"An even more negative correlation between average_length and target\", fontsize = 12)\nfig.show()\n    ","b49213b2":"from sklearn.feature_extraction.text import CountVectorizer \ncvect = CountVectorizer(binary = True)\nbag_of_words = cvect.fit_transform(train.excerpt)\nprint(\"Total Number of words (no stemming nor lemmatization): {}\".format(bag_of_words.shape[1]))","fc4e391f":"counts = bag_of_words.sum(axis = 0)\ndf_counts = pd.DataFrame({'counts': np.squeeze(np.asarray(counts)), 'word': cvect.get_feature_names()})\ndisplay(df_counts.sort_values('counts', ascending = False, ignore_index = True))","687959d2":"most_rare = df_counts.iloc[int(0.9*len(df_counts)):]['word'].tolist()\nprint(\"Some 'rare' words: \\n\")\nnp.random.choice(most_rare, 5).tolist()","8cba6d27":"# score based on number of letters\/words\n\ndef how_many_rare(excerpt, most_rare, list_like = True):\n    if list_like:\n        #list_like may take some time (< 1 minute)\n        return len([i for i in excerpt.split() if i in most_rare])\n    else:\n        return len(set(excerpt.split()).intersection(set(most_rare)))\n\ntrain['rare_words'] = train.apply(lambda x: how_many_rare(x.excerpt, most_rare), 1)\ntrain['rare_words_over_nwords'] = train['rare_words']\/train.excerpt.str.split().apply(lambda x: len(x))\n\n\nfig, ax = plt.subplots(2, 1, figsize = (12, 12))\nsns.regplot(y = train['target'], x = train['rare_words'],\n                ax = ax[0], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\nax[0].set_ylabel('target', fontsize = 15)\nax[0].set_xlabel('', fontsize = 15)\nax[0].set_title('Number of rare words in excerpt vs target', fontsize = 16)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nsns.regplot(y = train['standard_error'], x = train['rare_words'],\n                ax = ax[1], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\nax[1].set_ylabel('standard_error', fontsize = 15)\nax[1].set_xlabel('', fontsize = 15)\nax[1].set_title('Number of rare words in excerpt vs standard_error', fontsize = 16)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)\nax[1].set_ylim(0.4, 0.7)\n\nfig.suptitle(\"Number of 'rare words' vs target\", fontsize = 20, fontweight = 'bold')","4713eaef":"test","5c4d0004":"test['n_letters_excerpt'] = test['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len()\ntest['n_words_excerpt'] = test['excerpt'].str.split().str.len()\ntest['n_sentences_excerpt'] = test['excerpt'].str.split(pat = '[.!?]+').str.len()\n","02f0950c":"test","746e7328":"# target,standard_error distributions including also violin plots\n# aggiungere text con percentili e violinplot\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 6))\n\npercentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\npercentiles = corr_df['n_letters_excerpt'].quantile(percentiles_asked).tolist()\n\nsns.histplot(data = corr_df, x = 'n_letters_excerpt', ax = ax, kde=False, bins = 50,\n             stat = 'density', \n             alpha = 0.2, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n            )\n\nsns.kdeplot(data =corr_df, x = 'n_letters_excerpt', ax = ax, alpha = 0.01, fill = True, \n            linewidth = 3, color = 'blue')\n\nfor idx, row in test.sort_values('n_letters_excerpt', ignore_index=True).iterrows():\n    \n    ax.vlines(x = row['n_letters_excerpt'], ymin = 0, ymax = 0.0045, colors = 'red', linewidth = 2, label = row.id,\n             linestyles = 'dashed')\n    if idx <2:\n        ax.text(x = row['n_letters_excerpt']-40, y = 0.0047, s=row.id, fontsize = 9)\n    if idx >4:\n        ax.text(x = row['n_letters_excerpt'], y = 0.0047, s=row.id, fontsize = 9)\n    if idx == 2:\n        ax.text(x = row['n_letters_excerpt']-40, y = 0.0046, s=row.id, fontsize = 9)\n    if idx == 3:\n        ax.text(x = row['n_letters_excerpt']-30, y = 0.00475, s=row.id, fontsize = 9)\n    if idx == 4:\n        ax.text(x = row['n_letters_excerpt']-30, y = 0.0046, s=row.id, fontsize = 9)\n    \n\nax.set_ylabel('Density', fontsize = 15)\nax.set_xlabel('n_letters_excerpt', fontsize = 15)\nax.set_ylim(0.0, 0.005)\nax.set_title('hist-kde plot', fontsize = 16)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\n\nfig.suptitle('Distribution of number of letters in excerpt: train vs test with corresponding ids', fontsize = 20, fontweight = 'bold')\nplt.subplots_adjust(hspace = 0.6)\n\n","17a2380e":"# target,standard_error distributions including also violin plots\n# aggiungere text con percentili e violinplot\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(1, 1, figsize = (16, 6))\n\npercentiles_asked = [0.1, 0.25, 0.5, 0.75, 0.9]\npercentiles = corr_df['n_words_excerpt'].quantile(percentiles_asked).tolist()\n\nsns.histplot(data = corr_df, x = 'n_words_excerpt', ax = ax, kde=False, bins = 25,\n             stat = 'density', \n             alpha = 0.2, \n             fill = True,\n             linewidth = 3,\n             edgecolor='black',\n             color = 'red',\n            )\n\nsns.kdeplot(data =corr_df, x = 'n_words_excerpt', ax = ax, alpha = 0.01, fill = True, \n            linewidth = 3, color = 'blue')\n\nfor idx, row in test.sort_values('n_words_excerpt', ignore_index=True).iterrows():\n    \n    ax.vlines(x = row['n_words_excerpt'], ymin = 0, ymax = 0.025, colors = 'red', linewidth = 2, label = row.id,\n             linestyles = 'dashed')\n    \n    if idx == 0:\n        ax.text(x = row['n_words_excerpt']-5, y = 0.026, s=row.id, fontsize = 9)\n    if idx == 1:\n        ax.text(x = row['n_words_excerpt']-2, y = 0.028, s=row.id, fontsize = 9)\n    if idx == 5:\n        ax.text(x = row['n_words_excerpt']-3, y = 0.026, s=row.id, fontsize = 9)\n    if idx == 6:\n        ax.text(x = row['n_words_excerpt']-1, y = 0.028, s=row.id, fontsize = 9)\n    if (idx>=2) & (idx<=4):\n        ax.text(x = row['n_words_excerpt']-3, y = 0.026, s=row.id, fontsize = 9)\n\nax.set_ylabel('Density', fontsize = 15)\nax.set_xlabel('n_words_excerpt', fontsize = 15)\nax.set_ylim(0.0, 0.03)\nax.set_title('hist-kde plot', fontsize = 16)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params(axis='both', which='minor', labelsize=14)\n\nfig.suptitle('Distribution of number of words in excerpt: train vs test with corresponding ids', fontsize = 20, fontweight = 'bold')\nplt.subplots_adjust(hspace = 0.6)\n\n","3480d19a":"train['sentence'] = train.excerpt.str.split(pat = '[.!?]+')","a2c30a27":"def tokenize_sentences(x: list):\n    tags = []\n    for sentence in x:\n        pos_tag = [i[1] for i in nltk.pos_tag(nltk.word_tokenize(sentence), tagset= 'universal')]\n        tags+=pos_tag\n    return tags\n\ndef tokenize_sentences(x: list):\n    tags = []\n    for sentence in x:\n        pos_tag = [token.pos_ for token in nlp(sentence)]\n        tags+=pos_tag\n    return tags","6609de02":"start_time = time.time()\ntrain['pos_tag'] = train.sentence.apply(lambda x: tokenize_sentences(x))\nprint(\"Time Elapsed: {}\".format(time.time()-start_time))","6bcac3fd":"train['number_of_verbs'] = train['pos_tag'].apply(lambda x: len([i for i in x if i == 'VERB'])\/len(x))","65e63d83":"# score based on number of letters\/words\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 12), gridspec_kw={'width_ratios': [2, 1]})\nsns.regplot(y = train['target'], x = train['number_of_verbs'],\n                ax = ax[0], scatter_kws = {'color': 'blue', 'sizes': [5], 'alpha': 0.3}, line_kws = {'color': 'red', \n                                                                      'linewidth': 3, 'alpha': 0.5})\n\nax[0].set_ylabel('target', fontsize = 15)\nax[0].set_xlabel('', fontsize = 15)\nax[0].set_title('Percentage of verbs in excerpt vs target', fontsize = 16)\nax[0].tick_params(axis='both', which='major', labelsize=14)\nax[0].tick_params(axis='both', which='minor', labelsize=14)\n\nfig.suptitle(\"Percentage of verbs vs target\/standard_error\", fontsize = 20, fontweight = 'bold')\nplt.title(\"More verbs seem to make the comprehension easier\", fontsize = 12, fontweight = 'bold')\n\ncorr_df = train.copy()\n\ncorr_df = corr_df[['target', 'standard_error', 'number_of_verbs']]\n\ncorr_matrix = round(corr_df.corr(), 2)\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    \ncolors = sns.color_palette('rocket', 21)\nlevels = np.linspace(-1, 1, 21)\ncmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\nsns.heatmap(corr_matrix, mask=mask, annot=True, ax = ax[1], \n            cmap = cmap_plot, norm = norm, annot_kws={\"size\": 15, \"color\": 'black'})\nax[1].hlines([0, 1, 2, 3, 4, 5], *ax[1].get_xlim(), color = 'black')\nax[1].vlines([0, 1, 2, 3, 4, 5], *ax[1].get_ylim(), color = 'black')\nax[1].xaxis.set_ticks_position('bottom')\nax[1].set_title('Correlation matrix', fontsize = 20)\nax[1].tick_params(axis='both', which='major', labelsize=14)\nax[1].tick_params(axis='both', which='minor', labelsize=14)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax[1].set_yticklabels(ax[1].get_yticklabels(), rotation = 35, fontsize = 15, color = 'black')\nax[1].xaxis.label.set_size(14)\n\ncircle_rad = 25  # This is the radius, in points\nax[1].plot(0.5, 2.5, 'o',\n        ms=circle_rad * 2, mec='w', mfc='none', mew=4)\n","3188784c":"train['n_letters_excerpt'] = train['excerpt'].str.replace(\"[^a-zA-Z0-9-]\", \"\").str.len()\ntrain['n_words_excerpt'] = train['excerpt'].str.split().str.len()\ntrain['n_sentences_excerpt'] = train['excerpt'].str.split(pat = '[.!?]+').str.len()","f4544dab":"?px.scatter_3d","76288ca1":"import plotly.express as px\nfig = px.scatter_3d(train.assign(size=2), x='n_words_excerpt', y='number_of_verbs', z='target', size = 'size', color = 'target', opacity=0.7)\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","219b00cf":"<a id = \"test\"><\/a>\n\n<h5 style=\"background-color:#e6f7ff;\" align = 'center'style=\"background-color:#e6f7ff;\" align = 'center'> <i>test.csv<\/i> <\/h5>","7a2b72eb":"<a id = \"files\"><\/a>\n\n<h5 style=\"background-color:#e6f7ff;\" align = 'center'><i>Files available and some info<\/i><\/h5>","49b244c3":"I choose the 10% most rare words","7da4b646":"<h1> CommonLit Readability Prize EDA <\/h1>\n<br>\n\nIn this notebook I'll perform some exploratory data analysis, trying to update it often. \n\n<h4 style=\"background-color:#e6f7ff;\" align = 'center'><i>Table of Contents<\/i><\/h4>\n\n- [files available and some info](#files)\n- [train.csv](#train)\n- [test.csv](#test)\n","a989b462":"**Higher standard error for more extreme values of target**","edb311bf":"<a id =\"train\"><\/a>\n<h5 style=\"background-color:#e6f7ff;\" align = 'center'style=\"background-color:#e6f7ff;\" align = 'center'> <i>train.csv<\/i> <\/h5>","d5fa64e3":"I expect more difficult scores when the number of rare words is higher... ","617e84b1":"url_legal and license will be missing in the private test set. ","8e21b75b":"There is a little shift to the right in the `target` distribution when `url_legal` is not NaN."}}