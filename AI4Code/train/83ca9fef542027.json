{"cell_type":{"d3c47a22":"code","f0bff3f2":"code","dea7ec94":"code","0ab872ef":"code","1f12ca0f":"code","2214260a":"code","b2842582":"code","60f441aa":"code","4dd28ac5":"code","373dbfaa":"code","b7fefb87":"code","9e9f9349":"code","111c3759":"code","1f3e57a9":"code","ae0c0fd3":"code","cbb0257f":"code","e934674c":"code","cc61cc33":"code","26b7cd59":"code","6590065d":"code","e1fe00f8":"code","75c24a71":"code","8e0e9655":"code","00382484":"code","f74aa8aa":"code","78bd997d":"code","981bc7ed":"code","acf63c2b":"code","9e3054d1":"code","686e4459":"code","e74d7684":"code","fbfe4053":"code","5bd4f045":"code","67b8cf0c":"code","a9d0e90a":"code","8e394e4d":"markdown","004291df":"markdown","d30f2b42":"markdown","411d951d":"markdown","85decfed":"markdown","67bbc938":"markdown","2f35696e":"markdown","73b38560":"markdown","2acfb5a1":"markdown","40735870":"markdown","16ea522e":"markdown","ca3e949c":"markdown","cf99ddf1":"markdown","ae688870":"markdown","7763a98e":"markdown","302fa394":"markdown","7e7a7c28":"markdown","ca6319dc":"markdown"},"source":{"d3c47a22":"cd \/kaggle\/input\/mask-rcnn","f0bff3f2":"!pip3 install -r requirements.txt","dea7ec94":"cp -r \/kaggle\/input\/mask-rcnn\/Mask_RCNN \/kaggle\/working\/","0ab872ef":"cd \/kaggle\/working\/Mask_RCNN\/Mask_RCNN","1f12ca0f":"import tensorflow as tf\ntf.__version__","2214260a":"!python setup.py install","b2842582":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n\nfrom mrcnn.config import Config\nfrom mrcnn.model import MaskRCNN\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle","60f441aa":"# define the test configuration\nclass TestConfig(Config):\n    NAME = \"test\"\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    NUM_CLASSES = 1 + 80\n    \nrcnn = MaskRCNN(mode='inference', model_dir='\/kaggle\/working\/Mask_RCNN\/Mask_RCNN\/', \n                config=TestConfig())","4dd28ac5":"# draw an image with detected objects\ndef draw_image_with_boxes(filename, boxes_list):\n    # load the image\n    data = plt.imread(filename)\n    # plot the image\n    plt.imshow(data)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for box in boxes_list:\n         # get coordinates\n        y1, x1, y2, x2 = box\n         # calculate width and height of the box\n        width, height = x2 - x1, y2 - y1\n         # create the shape\n        rect = Rectangle((x1, y1), width, height, fill=False, color='red') \n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    plt.show()","373dbfaa":"WEIGHTS = '\/kaggle\/input\/mask-rcnn\/mask_rcnn_coco.h5'\nIMG_PATH = '\/kaggle\/input\/mask-rcnn\/elephant.jpg'","b7fefb87":"rcnn.load_weights(WEIGHTS, by_name=True)\n\nimg = load_img(IMG_PATH)\n\nimg = img_to_array(img)\n\nresults = rcnn.detect([img], verbose=0)","9e9f9349":"draw_image_with_boxes(IMG_PATH, results[0]['rois'])","111c3759":"# define 81 classes that the coco model knowns about\nclass_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', \n'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', \n'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', \n'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', \n'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', \n'scissors', 'teddy bear', 'hair drier', 'toothbrush']","1f3e57a9":"from mrcnn.visualize import display_instances\n\n# get dictionary for first prediction \nr = results[0]\n\ndisplay_instances(img, r['rois'], r['masks'], \n                  r['class_ids'], class_names, r['scores'])\n","ae0c0fd3":"r.keys()","cbb0257f":"r['scores']","e934674c":"ANNOT_PATH = '\/kaggle\/input\/mask-rcnn\/kangaroo\/kangaroo\/annots\/'\nKANGAROO_PATH = '\/kaggle\/input\/mask-rcnn\/kangaroo\/kangaroo\/images\/'\nROOT_DIR = '\/kaggle\/input\/mask-rcnn\/kangaroo\/kangaroo\/'","cc61cc33":"import xml.dom.minidom\n\ndom = xml.dom.minidom.parse(ANNOT_PATH + '00001.xml') \npretty_xml_as_string = dom.toprettyxml()\n\nprint(pretty_xml_as_string)\n","26b7cd59":"from xml.etree import ElementTree\n\ndef extract_boxes(filename):\n    # load and parse the file\n    tree = ElementTree.parse(filename)\n    # get the root of the document\n    root = tree.getroot()\n    \n    boxes = list()\n    \n    for box in root.findall('.\/\/bndbox'):\n        xmin = int(box.find('xmin').text) \n        ymin = int(box.find('ymin').text) \n        xmax = int(box.find('xmax').text) \n        ymax = int(box.find('ymax').text) \n\n        coors = [xmin, ymin, xmax, ymax] \n        boxes.append(coors)\n\n    width = int(root.find('.\/\/size\/width').text) \n    height = int(root.find('.\/\/size\/height').text) \n    \n    return boxes, width, height\n\nboxes, w, h = extract_boxes(ANNOT_PATH + '00001.xml')\nprint(boxes, w, h)","6590065d":"from mrcnn.utils import Dataset\n\nclass KangarooDataset(Dataset):\n    \n    def extract_boxes(self, filename):\n        # load and parse the file\n        tree = ElementTree.parse(filename)\n        # get the root of the document\n        root = tree.getroot()\n\n        boxes = list()\n\n        for box in root.findall('.\/\/bndbox'):\n            xmin = int(box.find('xmin').text) \n            ymin = int(box.find('ymin').text) \n            xmax = int(box.find('xmax').text) \n            ymax = int(box.find('ymax').text) \n\n            coors = [xmin, ymin, xmax, ymax] \n            boxes.append(coors)\n\n        width = int(root.find('.\/\/size\/width').text) \n        height = int(root.find('.\/\/size\/height').text) \n\n        return boxes, width, height\n    \n    \n    def load_dataset(self, img_path, annot_path, is_train=True):\n        self.add_class(\"dataset\", 1, \"kangaroo\")\n        \n        for filename in os.listdir(img_path):\n            image_id = filename[:-4]\n            \n            # skip bad images\n            if image_id in ['00090']: \n                continue\n            \n            if is_train and int(image_id) >= 150:\n                continue\n            if not is_train and int(image_id) < 150:\n                continue\n            \n            img_p = img_path + filename\n            ann_p = annot_path + image_id + '.xml'\n            \n            self.add_image('dataset', image_id=image_id, path=img_p, \n                           annotation=ann_p)\n\n    \n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        path = info['annotation']\n        \n        boxes, w, h = self.extract_boxes(path)\n        \n        masks = np.zeros([h, w, len(boxes)], dtype='uint8')\n        \n        class_ids = list()\n        for i in range(len(boxes)):\n            box = boxes[i]\n            row_s, row_e = box[1], box[3]\n            col_s, col_e = box[0], box[2]\n            \n            masks[row_s:row_e, col_s:col_e, i] = 1\n            class_ids.append(self.class_names.index('kangaroo'))\n        \n        return masks, np.asarray(class_ids, dtype='int32')\n    \n    def image_reference(self, image_id): \n        info = self.image_info[image_id] \n        return info['path']","e1fe00f8":"train_set = KangarooDataset() \n\ntrain_set.load_dataset(KANGAROO_PATH, ANNOT_PATH, is_train=True) \n\ntrain_set.prepare()\n\nprint('Train: %d' % len(train_set.image_ids))","75c24a71":"test_set = KangarooDataset() \n\ntest_set.load_dataset(KANGAROO_PATH, ANNOT_PATH, is_train=False) \n\ntest_set.prepare()\n\nprint('Test: %d' % len(test_set.image_ids))","8e0e9655":"image_id = 0\n\nimage = train_set.load_image(image_id)\nprint(image.shape)\n\nmask, class_ids = train_set.load_mask(image_id)\nprint(mask.shape)","00382484":"_ = plt.figure(figsize=(15,8))\n_ = plt.imshow(image)\n_ = plt.imshow(mask[:,:,0], cmap='gray', alpha=0.5)\n_ = plt.show()","f74aa8aa":"plt.figure(figsize=(20,15))\nfor i in range(9):\n    plt.subplot(330 + 1 + i)\n    plt.axis('off')\n    \n    image = train_set.load_image(i)\n    \n    mask, _ = train_set.load_mask(i)\n    \n    plt.imshow(image)\n    for j in range(mask.shape[2]):\n        plt.imshow(mask[:,:,j], cmap='gray', alpha=0.3)\n\nplt.show()","78bd997d":"for image_id in train_set.image_ids:\n  \n    info = train_set.image_info[image_id]\n  \n    print(info)","981bc7ed":"from mrcnn.visualize import display_instances\nfrom mrcnn.utils import extract_bboxes\n\nimage_id = 15\n\nimage = train_set.load_image(image_id)\n\nmask, class_ids = train_set.load_mask(image_id)\n\nbbox = extract_bboxes(mask)\n\ndisplay_instances(image, bbox, mask, class_ids, train_set.class_names)","acf63c2b":"from mrcnn.config import Config\n\nclass KangarooConfig(Config):\n    \n    NAME = \"kangaroo_cfg\"\n    NUM_CLASSES = 1 + 1\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 3 \n    \n    TRAIN_ROIS_PER_IMAGE = 32\n    MAX_GT_INSTANCES = 7\n    DETECTION_MAX_INSTANCES = 7\n    \n    STEPS_PER_EPOCH = 131","9e3054d1":"config = KangarooConfig()\nconfig.display()\n\nmodel = MaskRCNN(mode='training', model_dir='\/kaggle\/working\/Mask_RCNN\/Mask_RCNN\/', \n                 config=config)\n\n# load weights (mscoco) and exclude the output layers \nmodel.load_weights('\/kaggle\/input\/mask-rcnn\/mask_rcnn_coco.h5', by_name=True, \n                   exclude=[\"mrcnn_class_logits\",\"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])","686e4459":"# train weights (output layers or \u272cheads\u272c)\nmodel.train(train_set, test_set, learning_rate=config.LEARNING_RATE, \n            epochs=1, layers='heads')","e74d7684":"WEIGHTS_PATH = '\/kaggle\/working\/Mask_RCNN\/Mask_RCNN\/kangaroo_cfg20210224T0711\/'","fbfe4053":"from mrcnn.utils import compute_ap\nfrom mrcnn.model import load_image_gt\nfrom mrcnn.model import mold_image\n\nclass PredictionConfig(Config):\n    NAME = \"kangaroo_cfg\"\n    \n    NUM_CLASSES = 1 + 1\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    \ndef evaluate_model(dataset, model, cfg):\n    APs = list()\n    for image_id in dataset.image_ids:\n        # load image, bounding boxes and masks for the image id\n        image, _, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, \n                                                                image_id,use_mini_mask=False)\n        \n        # convert pixel values (e.g. center)\n        scaled_image = mold_image(image, cfg)\n        \n        # convert image into one sample\n        sample = np.expand_dims(scaled_image, 0)\n        \n        yhat = model.detect(sample, verbose=0)\n        \n        # extract results for first sample\n        r = yhat[0]\n        \n        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, \n                                 r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r[\"masks\"])\n        APs.append(AP)\n    \n    mAP = np.mean(APs)\n    return mAP","5bd4f045":"cfg = PredictionConfig()\n\nmodel = MaskRCNN(mode='inference', model_dir='\/kaggle\/working\/Mask_RCNN\/Mask_RCNN\/', \n                 config=cfg)\n\nmodel.load_weights(WEIGHTS_PATH+'mask_rcnn_kangaroo_cfg_0000.h5', by_name=True)\n\ntrain_mAP = evaluate_model(train_set, model, cfg)\n\nprint(\"Train mAP: %.3f\" % train_mAP)\n\ntest_mAP = evaluate_model(test_set, model, cfg)\n\nprint(\"Test mAP: %.3f\" % test_mAP)","67b8cf0c":"from mrcnn.model import mold_image\n\ndef plot_actual_vs_predicted(dataset, model, cfg, n_images=5):\n    for i in range(n_images):\n        image = dataset.load_image(i)\n        mask, _ = dataset.load_mask(i)\n        scaled_image = mold_image(image, cfg)\n        sample = np.expand_dims(scaled_image, 0)\n        yhat = model.detect(sample, verbose=0)[0]\n        \n        plt.figure(figsize=(20,15))\n        plt.subplot(n_images, 2, i*2+1)\n        plt.axis('off')\n        plt.imshow(image)\n        \n        if (i==0):\n            plt.title('Actual')\n        \n        for j in range(mask.shape[2]):\n            plt.imshow(mask[:,:,j], cmap='gray', alpha=0.3)            \n        \n        plt.subplot(n_images, 2, i*2+2)\n        plt.axis('off')\n        plt.imshow(image)\n        \n        if (i==0):\n            plt.title('Predicted')\n            \n        ax = plt.gca()\n        \n        for box in yhat['rois']:\n            y1, x1, y2, x2 = box\n            width, height = x2 - x1, y2 - y1\n            rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n            ax.add_patch(rect)\n            \n        plt.show()","a9d0e90a":"plot_actual_vs_predicted(train_set, model, cfg)","8e394e4d":"<h3><center>1. Importing Libraries<\/center><\/h3>","004291df":"<h3><center>7. Detect Kangaroos in New Photos<\/center><\/h3>","d30f2b42":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\n We can see that the annotation file contains a size element that describes the shape of the photograph, and object elements describe the bounding boxes for the kangaroo objects in the photograph.   \n    <\/div>","411d951d":"<h3><center>3. Load Weights & Detect<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe next step is to load the weights. Now we can make a prediction for our image. <br><br>  We can then make a prediction with the model. Instead of calling predict() as we would on a normal Keras model, will call the detect() function and pass it the single image.<br><br>\n    The result contains a dictionary for each image that we passed into the detect() function.The keys of the dictionary of note are as follows:\n    <br>\n    <ul>\n        <li>\u2018rois\u2019: The bound boxes or regions-of-interest (ROI) for detected objects.\n            <li>\u2018masks\u2019: The masks for the detected objects.\n                <li>\u2018class ids\u2019: The class integers for the detected objects.\n                    <li>\u2018scores\u2019: The probability or confidence for each predicted class.\n    <\/ul>\n<\/div>","85decfed":"<h3><center>1. Parse Annotation File<\/center><\/h3>","67bbc938":"<h3><center>4. Extract Boundary Boxes<\/center><\/h3>","2f35696e":"<h3><center>5. Training Output layers<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">","73b38560":"<h2><center>PART II : Develop New Model<\/center><\/h2>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe Mask R-CNN introduced in the 2018 paper titled Mask R-CNN is the most recent variation of the family of models and supports both object detection and object segmentation. Object segmentation not only involves localizing objects in the image but also specifies a mask for the image, indicating exactly which pixels in the image belong to the object.\n    <br><br>\nThe Mask R-CNN is designed to learn to predict both bounding boxes for objects as well as masks for those detected objects, and the kangaroo dataset does not provide masks. As such, we will use the dataset to learn a kangaroo object detection task, and ignore the masks and not focus on the image segmentation capabilities of the model.\n    <\/div>","2acfb5a1":"<h2><center>Part I : Object Detection Application<\/center><\/h2>","40735870":"<h3><center>6. Evaluate Model<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe performance of a model for an object recognition task is often evaluated using the mean absolute precision, or mAP. We are predicting bounding boxes so we can determine whether a bounding box prediction is good or not based on how well the predicted and actual bounding boxes overlap. This can be calculated by dividing the area of the overlap by the total area of both bounding boxes, or the intersection divided by the union, referred to as intersection over union, or IoU. A perfect bounding box prediction will have an IoU of 1. It is standard to assume a positive prediction of a bounding box if the IoU is greater than 0.5, e.g. they overlap by 50% or more. Precision refers to the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all bounding boxes predicted. Recall is the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all objects in the photo.<br><br>\n    The average or mean of the average precision (AP) across all of the images in a dataset is called the mean average precision, or mAP. The mask-rcnn library provides a mrcnn.utils.compute_ap to calculate the AP and other metrics for a given images.\n    <\/div>","16ea522e":"<h3><center>Matterport Mask R-CNN Project<\/center><\/h3>\n<h3><center>1. Introduction<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nMask R-CNN is a sophisticated model to implement, especially as compared to a simple or even state-of-the-art deep convolutional neural network model. Source code is available for each version of the R-CNN model, provided in separate GitHub repositories with prototype models based on the Caffe deep learning framework.\n<br><br>\nThe best of breed third-party implementations of Mask R-CNN is the Mask R-CNN Project developed by Matterport. The project is open source released under a permissive license (i.e. MIT license) and the code has been widely used on a variety of projects and Kaggle competitions\n<br><br>\n<ul>\n    There are perhaps three main use cases for using the Mask R-CNN model with the Matterport library; they are:\n    <li>Object Detection Application: Use a pre-trained model for object detection on new images.\n    <li>New Model via Transfer Learning: Use a pre-trained model as a starting point in developing a model for a new object detection dataset.\n    <li>New Model from Scratch: Develop a new model from scratch for an object detection dataset.\n    <\/ul>\n    <\/div>","ca3e949c":"<h3><center>4. Object Detection<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe Mask RCNN API provides a function called display instances() that will take the array of pixel values for the loaded image and the aspects of the prediction dictionary, such as the bounding boxes, scores, and class labels, and will plot the photo with all of these annotations.<br><br>\n    The function also needs a mapping of ids to class labels. The pre-trained model was fit with a dataset that had 80 (81 including background) class labels\n    <\/div>","cf99ddf1":"<h3><center>5. Configuration for training<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe pre-defined model architecture and weights can be loaded. This can be achieved by calling the load_weights().<br><br>\nClass-specific output layers are removed using exclude argument.<br><br>\nWe can also specify what layers to train. In this case, we will only train the heads, that is the output layers of the model.\n    <\/div>","ae688870":"<h4>Debugging<\/h4>","7763a98e":"<h3>Setting up Environment<\/h3>","302fa394":"<h3><center>2. Configure Model<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    First, the model must be defined via an instance of the MaskRCNN class. This class requires a configuration object as a parameter. The configuration object defines how the model might be used during training or inference. In this case, the configuration will only specify the number of images per batch, which will be one, and the number of classes to predict. You can see the full extent of the configuration object and the properties that you can override in the config.py file.<\/div>","7e7a7c28":"<h3><center>3. Test Kangaroo Dataset object<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe first useful test is to confirm that the images and masks can be loaded correctly. We can test this by creating a dataset and loading an image via a call to the load image() function with an image id, then load the mask for the image via a call to the load mask() function with the same image id.    \n    <\/div>","ca6319dc":"<h3><center>2. Develop Kangaroo Dataset object<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nTrain, validation, and test datasets are managed by a mrcnn.utils.Dataset object. We can define a new class that extends the mrcnn.utils.Dataset. We can define a function load_dataset() to load the data<br><br>\n    To use a Dataset object, it is instantiated, then our custom load function must be called, then finally the built-in prepare() function is called.<br><br>\n    Classes are defined by calling the built-in add class() function and specifying the source (the name of the dataset), the class id or integer for the class (e.g. 1 for the first class as 0 is reserved for the background class), and the class name\n(e.g. kangaroo).<br><br>\n    Objects are defined by a call to the built-in add image() function and specifying the source (the name of the dataset), a unique image id (e.g. the filename without the file extension like 00001), and the path for where the image can be loaded\n    <\/div>"}}