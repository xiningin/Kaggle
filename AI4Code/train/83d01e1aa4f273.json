{"cell_type":{"1866842c":"code","05782581":"code","a102b6ab":"code","cbfc84f0":"code","9ccc25a2":"code","daca1b20":"code","e80f6357":"code","4fcaafc4":"code","c4922a78":"code","4fce0134":"code","06b9d7ab":"code","ef206a6b":"code","0bd34125":"code","a98a3264":"code","972c151f":"code","795b665e":"code","54af819f":"code","9d7e62b2":"code","08aa8502":"code","760e7995":"code","b625201c":"code","916f5b5c":"code","b8031d89":"code","31b073df":"code","115850ed":"code","09e10060":"code","b431ce72":"code","29ed6198":"code","42295e1f":"code","f3d82584":"code","80ea6000":"code","e6c411ea":"code","86d3e3af":"code","03174069":"code","8afdc09d":"code","ab000d5a":"code","3e87d35d":"code","6d8c3b2e":"code","91959eaa":"code","3f7594ed":"code","ed632dc9":"markdown","48f5b9dc":"markdown","326a732a":"markdown","3c303252":"markdown","edea6769":"markdown"},"source":{"1866842c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05782581":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt","a102b6ab":"BASE = \"..\/input\/tabular-playground-series-feb-2021\"\ntest = pd.read_csv(BASE + '\/test.csv')\n\ntrain = pd.read_csv(BASE + '\/train.csv')\n\nsample_sub = pd.read_csv(BASE + '\/sample_submission.csv')","cbfc84f0":"train = train.drop(['cat4', 'cont4'], axis = 1) \ntest = test.drop(['cat4', 'cont4'], axis=1)","9ccc25a2":"columns = test.columns[1:]\ncolumns","daca1b20":"target = train['target'].values","e80f6357":"train.shape","4fcaafc4":"test.shape","c4922a78":"train.describe()","4fce0134":"cat_features = columns[:9]\ncat_features","06b9d7ab":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])","ef206a6b":"cont_features = columns[9:]\ncont_features","0bd34125":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\ntrain_features = train[cont_features]\ntest_features = test[cont_features]\n\nct = ColumnTransformer([\n        ('somename', MinMaxScaler(), ['cont0', 'cont1', 'cont2', 'cont3', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'])\n    ], remainder='passthrough')\nft = ColumnTransformer([\n        ('someothername', MinMaxScaler(), ['cont0', 'cont1', 'cont2', 'cont3','cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'])\n    ], remainder='passthrough')\n\ntrain[cont_features] = ct.fit_transform(train_features)\n\ntest[cont_features] = ft.fit_transform(test_features)","a98a3264":"train.head()","972c151f":"test.head()","795b665e":"train.head()\ntrain.describe()","54af819f":"train.head()","9d7e62b2":"test.head()","08aa8502":"train.iloc[0:][columns]","760e7995":"train_cat_cols = [f\"cat{i}\" for i in range(10) if i is not 4]\ntrain_num_col = [f\"cont{i}\" for i in range(14) if i is not 4]","b625201c":"train_num_col","916f5b5c":"!pip install --upgrade xgboost\nimport xgboost as xgb\nxgb.__version__","b8031d89":"xgb_params= {\n        \"objective\": \"reg:squarederror\",\n        \"max_depth\": 6,\n        \"learning_rate\": 0.01,\n        \"colsample_bytree\": 0.4,\n        \"subsample\": 0.6,\n        \"reg_alpha\" : 6,\n        \"min_child_weight\": 100,\n        \"n_jobs\": 2,\n        \"seed\": 2021,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n    }","31b073df":"catBoost_params = {\n    \"n_estimator\": 850,\n    \"tast_type\" : \"GPU\",\n    \"loss_function\": 'RMSE',\n    \"eval_metric\": 'RMSE',\n    \"metric_period\": 1000,\n    \"use_best_model\": True,\n    \"random_seed\": 2021\n}","115850ed":"lgbm_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": -1,\n    \"cat_feature\": [x for x in range(len(train_cat_cols))],\n    \"early_stopping_round\": 150,\n    \"reg_alpha\": 6.147694913504962,\n    \"reg_lambda\": 0.002457826062076097,\n    \"colsample_bytree\": 0.3,\n    \"learning_rate\": 0.01,\n    \"max_depth\": 30,\n    \"num_leaves\": 100,\n    \"min_child_samples\": 275,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 40.0,\n    \"max_bin\": 512,\n    \"min_data_per_group\": 100,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.7,\n    \"cat_l2\": 12.0,\n}","09e10060":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n","b431ce72":"from catboost import CatBoostRegressor","29ed6198":"train_oof = np.zeros((300000,))\nlight_trainPredict = np.zeros((300000,))\ncat_trainPredict = np.zeros((300000,))\n\ncat_preds = 0\nlight_preds = 0\n\n\ntest_preds = 0\ntrain_oof.shape","42295e1f":"test_0 = test[columns]\ntest","f3d82584":"Test = xgb.DMatrix(test[columns])","80ea6000":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        ## we are going to use cat boost\n        \n        cat_model = CatBoostRegressor(learning_rate = 0.1,\n                                      iterations = 2000,\n                                      task_type=\"GPU\",\n                                      loss_function= 'RMSE',\n                                      eval_metric='RMSE',\n                                      metric_period=1000, \n                                      use_best_model=True,\n                                      random_seed=42,\n                                      allow_writing_files = False,\n                                      od_type = 'Iter',\n                                      bagging_temperature = 0.8,\n                                      depth = 6,\n                                      od_wait = 20,)\n        cat_model.fit(\n        train_df,\n        train_target,\n        eval_set=[(val_df, val_target)],\n        plot=True,\n        early_stopping_rounds=250\n        )\n        cat_valpredict = cat_model.predict(val_df)\n        cat_testPredict = cat_model.predict(test_0)\n        \n        cat_trainPredict[val_ind] = cat_valpredict\n        cat_preds += cat_testPredict\/NUM_FOLDS\n        print(mean_squared_error(cat_valpredict, val_target, squared=False))\n        \n        ## we are going to use lightboost as well\n        \n        light_model = LGBMRegressor(**lgbm_params ) \n        light_model.fit(\n        train_df, \n        train_target,\n        eval_set=[(val_df, val_target)],\n        verbose=100,\n    )\n        light_valpredict = light_model.predict(val_df)\n        light_testPredict = light_model.predict(test_0)\n        \n        light_trainPredict[val_ind] = light_valpredict\n        light_preds += light_testPredict\/NUM_FOLDS\n        print(mean_squared_error(light_valpredict, val_target, squared=False))\n        \n        \n        ## xgboost model with Dmatrix which is used for optimization\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 3600)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(Test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(mean_squared_error(temp_oof, val_target, squared=False))","e6c411ea":"mean_squared_error(train_oof, target, squared=False)","86d3e3af":"mean_squared_error(light_trainPredict, target, squared=False)","03174069":"mean_squared_error(cat_trainPredict, target, squared=False)","8afdc09d":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)","ab000d5a":"train_model = [train_oof,light_trainPredict,cat_trainPredict]\ntest_model = [ test_preds,cat_preds, light_preds]","3e87d35d":"np.save('light_oof', light_trainPredict)\nnp.save('cat_oof', cat_trainPredict)\nnp.save('light_test_preds', light_preds)\nnp.save('cat_test_preds', cat_preds)","6d8c3b2e":"sample_sub['target'] = test_preds\nsample_sub.to_csv('xgBoost.csv', index=False)\n\n\n","91959eaa":"sample_sub['target'] = light_preds\nsample_sub.to_csv('light_sub.csv',index=False)","3f7594ed":"sample_sub['target'] = cat_preds\nsample_sub.to_csv('cat_sub.csv',index=False)","ed632dc9":"# Let's get the xgboost model\n","48f5b9dc":"# A bit of feature engineering \n","326a732a":"# Let's see how the individual models perform and then we can make a ensemble according to that\n\n## CV score for \n\n### XGBOOST = 0.8422094126465101\n\n### CatBOOST = 0.8512973171285753\n\n### lightBOOST = 0.84181677324358\n\n\n## LeaderBoard Score...","3c303252":"# Let's do this\n\n## Importing the libraries \n","edea6769":"# LABEL ENCODING"}}