{"cell_type":{"c85a38f1":"code","cff8709a":"code","e4412f43":"code","b6bb4f1b":"code","6c019a4e":"code","096ca053":"code","69990286":"code","60b3c12d":"code","b284360e":"code","b47b7d52":"code","3fb44554":"code","f59eef16":"code","cf365a7b":"code","a4f829b7":"code","4b68579d":"code","28ced2ee":"code","16e83361":"code","557a77ea":"code","cab429fd":"code","dafc1021":"code","4779c1d6":"code","373d3c66":"code","ed41537e":"code","20d0376d":"code","b01311e1":"code","e6a910a6":"code","609fc34e":"code","fb2e3e4b":"code","30cdd053":"code","951e0b9d":"code","4c63df8c":"code","b032b8ec":"code","aa5d5827":"code","da7ded4e":"code","64f331da":"code","1b800b0e":"code","1da6ffff":"code","8ecb3d1c":"code","f4864cee":"code","4c4fc3e1":"code","18e90e5e":"code","125ce6c2":"code","d9dc2d20":"code","16a1dfd7":"code","ed314082":"code","81bd37f3":"code","4fd3da5f":"code","e040c7d3":"code","d0f363d1":"code","c19b5917":"code","16691479":"code","5a9bf57b":"code","fd7bb28c":"code","d13bc181":"code","7538e4c5":"code","5b745878":"code","e25d8adf":"code","5fee1d5f":"code","50636ce1":"code","71fafc2e":"code","4808e134":"code","36539925":"code","3cede2ec":"code","58719597":"code","1c6109d2":"code","43245ac1":"code","e4e5ef92":"code","e661cd25":"code","ad0d66b8":"code","6498f015":"code","0ac7dfda":"code","401674db":"code","55f2d332":"code","cd788ae0":"code","b8cf2e6d":"code","02e32e08":"code","3089c418":"code","bf557fc9":"code","d1dfec81":"code","40a2f361":"code","5b014b66":"code","1d184962":"code","ed5804c8":"code","7087bd7b":"code","159264f9":"code","0d90d665":"code","42e17918":"code","1fb1f105":"code","38d802ce":"code","d8da8f32":"code","475ff1e4":"code","78d61df0":"code","48825332":"code","6a066d6b":"code","95ea6941":"code","df1f7a32":"code","baa0cc00":"code","21c8d228":"code","cea4aaa1":"code","92911218":"code","cc8aa769":"code","d4f80f8a":"code","3ff9f40a":"code","44871f73":"code","b5facfca":"code","a7e0716d":"code","aa511df6":"code","5de99055":"code","ce033b1a":"code","235fced5":"code","df03cd8f":"code","b50a0384":"code","39f75bc2":"code","9cb54dea":"code","059008eb":"code","d66f82f7":"code","72847796":"code","5436189c":"code","9f22fa49":"code","025491af":"code","dcddb138":"code","4e42444c":"code","72e93e4f":"code","df398d65":"code","2ea97e55":"code","ac694521":"code","b525ddfb":"code","80108700":"markdown","8cf60409":"markdown","66ff7e63":"markdown","9bfc9187":"markdown","2fb2b117":"markdown","3ffd603c":"markdown","fabd6f22":"markdown","c8f77d72":"markdown","3086abe9":"markdown","67d941e3":"markdown","d1432c26":"markdown","534c24af":"markdown","13dece2e":"markdown","d5a5eca4":"markdown","506ed417":"markdown","c4715268":"markdown","7db5ce32":"markdown","0d9c8dae":"markdown","4c1a8b57":"markdown","fa4682ac":"markdown","60c12512":"markdown","92fa1fed":"markdown","543324b7":"markdown","0be0dbda":"markdown"},"source":{"c85a38f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns; sns.set()\n\n# Disabling warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cff8709a":"data = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\ndf = data.copy()","e4412f43":"display(df.head())\ndisplay(df.tail())","b6bb4f1b":"df.info()","6c019a4e":"df.describe().T","096ca053":"sns.swarmplot(x=\"sex\", y=\"charges\", data=df)\nplt.title('Medical Costs by Gender', color = 'blue', fontsize=15)\nplt.show()\n\nsns.swarmplot(x=\"smoker\", y=\"charges\", data=df)\nplt.title('Medical Costs by Smoking', color = 'blue', fontsize=15)\nplt.show()\n\nsns.swarmplot(x=\"smoker\", y=\"bmi\", data=df)\nplt.title('Bmi by Smoking', color = 'blue', fontsize=15)\nplt.show()","69990286":"sns.boxplot(x=\"sex\", y=\"charges\",hue=\"smoker\", data=df)\nplt.title('Medical Costs by Gender and Smoking', color = 'blue', fontsize=15)\nplt.show()\n\nsns.boxplot(x=\"smoker\", y=\"bmi\", data=df)\nplt.title('Bmi by Smoking', color = 'blue', fontsize=15)\nplt.show()","60b3c12d":"df['sex'] = [1 if each=='male' else 0 for each in df.sex]\ndf['smoker']=[1 if each=='yes' else 0 for each in df.smoker]\ndf = pd.get_dummies(df, columns = [\"region\"], prefix = [\"region\"], drop_first=False)\ndf.head()","b284360e":"from sklearn.neighbors import LocalOutlierFactor","b47b7d52":"clf = LocalOutlierFactor()\nclf.fit_predict(df)","3fb44554":"df_scores = clf.negative_outlier_factor_","f59eef16":"# When we sort the outlier scores, it is seen that the top 5 values seem to outstand compared to the rest.\n# In this framework, the 6th observation is selected as the threshold and the outliers will be replaced by this threshold.\nnp.sort(df_scores)[:20]","cf365a7b":"# The threshold score is set as a filter.\nthreshold_score = np.sort(df_scores)[5]\ndf[df_scores == threshold_score]\noutlier_tf = df_scores < threshold_score","a4f829b7":"# the threshold observation\nthreshold_observation = df[df_scores == threshold_score]\nthreshold_observation","4b68579d":"outliers = df[outlier_tf]\noutliers","28ced2ee":"# We convert these into an array to handle the outliers with to_records() method.\nres = outliers.to_records(index = False)\nres","16e83361":"# We replace the values of outlier observations with the values of the threshold observation. \nres[:] = threshold_observation.to_records(index = False)\ndf[outlier_tf] = pd.DataFrame(res, index = df[outlier_tf].index)\ndf[outlier_tf]","557a77ea":"df.head()","cab429fd":"import statsmodels.api as sm \nimport statsmodels.formula.api as smf \nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict \nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","dafc1021":"X = df.drop([\"charges\"],axis = 1)\ny = df['charges']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 42)","4779c1d6":"# Multilinear Regression model with skilearn \nlr = LinearRegression()\nmodel1 = lr.fit(X_train, y_train)","373d3c66":"# Coefficients\nmodel1.coef_ ","ed41537e":"# Intercept\nmodel1.intercept_","20d0376d":"# R2 score of test data\nmodel1.score(X_test,y_test)","b01311e1":"# RMSE score of test data\nrmse = np.sqrt(mean_squared_error(y_test, model1.predict(X_test)))\nrmse","e6a910a6":"y_pred = model1.predict(X_test)\ny_test_ =np.array(range(0,len(y_test)))\nplt.plot(y_test_,y_test,color=\"r\")\nplt.plot(y_test_,y_pred,color=\"blue\")\nplt.show()","609fc34e":"# R2 average of test data after cross validation\nmlin_final_r2 = cross_val_score(model1, X_train, y_train, cv = 10, scoring = \"r2\").mean()\nmlin_final_r2","fb2e3e4b":"# RMSE average score of test data after cross validation\nmlin_final_rmse = np.sqrt(-cross_val_score(model1, \n                X_test, \n                y_test, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()\nmlin_final_rmse","30cdd053":"from sklearn.cross_decomposition import PLSRegression, PLSSVD","951e0b9d":"pls_model = PLSRegression().fit(X_train, y_train)","4c63df8c":"# PLS model coefficients\npls_model.coef_","b032b8ec":"# PLS model predictions based on train data\ny_pred = pls_model.predict(X_train)","aa5d5827":"# PLS RMSE score for train data\nnp.sqrt(mean_squared_error(y_train, y_pred))","da7ded4e":"# PLS R2 for train data\nr2_score(y_train, y_pred)","64f331da":"# PLS prediction based on test data\ny_pred = pls_model.predict(X_test)","1b800b0e":"# PLS RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","1da6ffff":"# PLS R2 for test data\nr2_score(y_test, y_pred)","8ecb3d1c":"# Illustraion of change in RMSE score as the model adds one additional component to the model in each loop.\ncv_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n\nRMSE = []\n\nfor i in np.arange(1, X_train.shape[1] + 1):\n    pls = PLSRegression(n_components=i)\n    score = np.sqrt(-1*cross_val_score(pls, X_train, y_train, cv=cv_10, scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)\n\nplt.plot(np.arange(1, X_train.shape[1] + 1), np.array(RMSE), '-v', c = \"r\")\nplt.xlabel('Number of Components')\nplt.ylabel('RMSE')\nplt.title('Components and RMSE');","f4864cee":"# PLS model with two components\npls_model2 = PLSRegression(n_components = 2).fit(X_train, y_train)","4c4fc3e1":"# PLS prediction based on test data after cross validation\ny_pred2 = pls_model2.predict(X_test)","18e90e5e":"# PLS RMSE test score after cross validation\npls_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred2))\npls_final_rmse","125ce6c2":"# PLS R2 test score after cross validation\npls_final_r2 = r2_score(y_test, y_pred2)\npls_final_r2","d9dc2d20":"from sklearn.linear_model import Ridge","16a1dfd7":"ridge_model = Ridge(alpha = 0.1).fit(X_train, y_train)\nridge_model","ed314082":"ridge_model.coef_","81bd37f3":"# Illustration of how weights of independent variables approaches to 0 as the alpha value increases. \n\nlambdas = 10**np.linspace(10,-2,100)*0.5\n\nridge_model = Ridge()\ncoefficients = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha = i)\n    ridge_model.fit(X_train, y_train) \n    coefficients.append(ridge_model.coef_)\n        \nax = plt.gca()\nax.plot(lambdas, coefficients) \nax.set_xscale('log') \n\nplt.xlabel('Lambda(Alpha) Values')\nplt.ylabel('Coefficients')\nplt.title('Ridge Coefficients');","4fd3da5f":"# Ridge prediction based on test data\ny_pred = ridge_model.predict(X_test)","e040c7d3":"# Ridge RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","d0f363d1":"# Ridge R2 \nr2_score(y_test, y_pred)","c19b5917":"from sklearn.linear_model import RidgeCV","16691479":"# Ridge instantiation of cross validation model and model details\nridge_cv = RidgeCV(alphas = lambdas, \n                   scoring = \"neg_mean_squared_error\",\n                   normalize = True)\nridge_cv.fit(X_train, y_train)\nridge_model","5a9bf57b":"# Ridge cross validation alpha score\nridge_cv.alpha_","fd7bb28c":"# Ridge tuned model after cross validation\nridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n                   normalize = True).fit(X_train,y_train)","d13bc181":"# Ridge model coefficients after cross validation\nridge_tuned.coef_","7538e4c5":"# Ridge RMSE test score after cross validation\nridge_final_rmse = np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(X_test)))\nridge_final_rmse","5b745878":"# Ridge R2 after cross validation\nridge_final_r2 = r2_score(y_test, ridge_tuned.predict(X_test))\nridge_final_r2","e25d8adf":"from sklearn.linear_model import Lasso","5fee1d5f":"lasso_model = Lasso(alpha = 1.0).fit(X_train, y_train)\nlasso_model","50636ce1":"# Lasso model coefficients\nlasso_model.coef_","71fafc2e":"# The weight of independent variables comes to value of zero as the alpha score changes. \n\nlasso = Lasso()\nlambdas = 10**np.linspace(10,-2,100)*0.5 \ncoefficients = []\n\nfor i in lambdas:\n    lasso.set_params(alpha=i)\n    lasso.fit(X_train, y_train)\n    coefficients.append(lasso.coef_)\n    \nax = plt.gca()\nax.plot(lambdas*2, coefficients)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","4808e134":"# Lasso model prediction based on test data\ny_pred = lasso_model.predict(X_test)","36539925":"# Lasso RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","3cede2ec":"# Lasso R2\nr2_score(y_test, y_pred)","58719597":"from sklearn.linear_model import LassoCV","1c6109d2":"lasso_cv_model = LassoCV(alphas = None, \n                         cv = 10, \n                         max_iter = 10000, \n                         normalize = True)","43245ac1":"# Lasso cross validation model details\nlasso_cv_model.fit(X_train,y_train)","e4e5ef92":"# Lasso cross validation model alpha score\nlasso_cv_model.alpha_","e661cd25":"# Lasso tuned model after cross validation\nlasso_tuned = Lasso(alpha = lasso_cv_model.alpha_)\nlasso_tuned.fit(X_train, y_train)","ad0d66b8":"# Lasso predictions of tuned model base on test data\ny_pred = lasso_tuned.predict(X_test)","6498f015":"# Lasso model coefficients after cross validation\nlasso_tuned.coef_","0ac7dfda":"# Lasso RMSE test score after cross validation\nlasso_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nlasso_final_rmse","401674db":"# Lasso R2 after cross validation\nlasso_final_r2 = r2_score(y_test, y_pred)\nlasso_final_r2","55f2d332":"print(f\"\"\"Multilinear Regression RMSE: {mlin_final_rmse}, R2: {mlin_final_r2}\nPLS Regression RMSE: {pls_final_rmse}, R2: {pls_final_r2}\nRidge Regression RMSE: {ridge_final_rmse}, R2: {ridge_final_r2}\nLasso Regression RMSE: {lasso_final_rmse}, R2: {lasso_final_r2}\"\"\")","cd788ae0":"from sklearn.preprocessing import PolynomialFeatures","b8cf2e6d":"# we can change the degree value for model tuning, but it is worthwhile to note that higher degree levels may lead to overfitting.\npoly_features = PolynomialFeatures(degree=3)","02e32e08":"x_train_poly = poly_features.fit_transform(X_train)","3089c418":"poly_model = LinearRegression()\npoly_model.fit(x_train_poly, y_train)","bf557fc9":"y_train_pred = poly_model.predict(x_train_poly)","d1dfec81":"# Polynomial Regression RMSE and R2 score for train data\nrmse_train = np.sqrt(mean_squared_error(y_train,y_train_pred))\nr2_train = r2_score(y_train, y_train_pred)\nprint(rmse_train,r2_train)","40a2f361":"y_test_pred = poly_model.predict(poly_features.fit_transform(X_test))","5b014b66":"# Polynomial Regression RMSE and R2 score for test data\npoly_rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred))\npoly_r2_final = r2_score(y_test, y_test_pred)\nprint(poly_rmse_final,poly_r2_final)","1d184962":"y_test_ =np.array(range(0,len(y_test_pred)))\nplt.plot(y_test_,y_test,color=\"r\")\nplt.plot(y_test_,y_test_pred,color=\"blue\")\nplt.show()","ed5804c8":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score","7087bd7b":"rf_model = RandomForestRegressor(random_state = 42)","159264f9":"rf_model.fit(X_train, y_train)\n","0d90d665":"rf_model.predict(X_test)[0:5]","42e17918":"y_pred = rf_model.predict(X_test)","1fb1f105":"# RF RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","38d802ce":"# RF R2 score for test data\nr2_score(y_test, y_pred)","d8da8f32":"rf_params = {'max_depth': list(range(1,10)),\n            'max_features': [2,3,5,7],\n            'n_estimators' : [100, 200, 500, 1000, 1500]}","475ff1e4":"rf_model = RandomForestRegressor(random_state = 42)","78d61df0":"rf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                            n_jobs = -1,\n                            verbose = 2)","48825332":"rf_cv_model.fit(X_train, y_train)","6a066d6b":"rf_cv_model.best_params_","95ea6941":"rf_tuned = RandomForestRegressor(max_depth  = 5, \n                                 max_features = 7, \n                                 n_estimators =1000)","df1f7a32":"rf_tuned.fit(X_train, y_train)","baa0cc00":"y_pred = rf_tuned.predict(X_test)","21c8d228":"# RF RMSE test score after model tuning\nrf_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_rmse_final","cea4aaa1":"# RF R2 for test data after model tuning\nrf_r2_final = r2_score(y_test, y_pred)\nrf_r2_final","92911218":"# Importance level of independent variables through RF\nImportance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Importance Levels of Variables\")","cc8aa769":"import xgboost as xgb","d4f80f8a":"DM_train = xgb.DMatrix(data = X_train, label = y_train)\nDM_test = xgb.DMatrix(data = X_test, label = y_test)","3ff9f40a":"from xgboost import XGBRegressor","44871f73":"xgb_model = XGBRegressor().fit(X_train, y_train)\n","b5facfca":"# XGBoost RMSE test score\ny_pred = xgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","a7e0716d":"# XGBoost R2 for test data\nr2_score(y_test, y_pred)","aa511df6":"xgb_model","5de99055":"xgb_grid = {\n     'colsample_bytree': [0.4, 0.5,0.6,0.9,1], \n     'n_estimators':[100, 200, 500, 1000],\n     'max_depth': [2,3,4,5,6],\n     'learning_rate': [0.1, 0.01, 0.5]\n}","ce033b1a":"xgb = XGBRegressor()\n\nxgb_cv = GridSearchCV(xgb, \n                      param_grid = xgb_grid, \n                      cv = 10, \n                      n_jobs = -1,\n                      verbose = 2)\n\n\nxgb_cv.fit(X_train, y_train)","235fced5":"xgb_cv.best_params_","df03cd8f":"xgb_tuned = XGBRegressor(colsample_bytree = 0.9, \n                         learning_rate = 0.01, \n                         max_depth = 3, \n                         n_estimators = 500) \n\nxgb_tuned = xgb_tuned.fit(X_train,y_train)","b50a0384":"# XGBoost RMSE test score after model tuning\ny_pred = xgb_tuned.predict(X_test)\nxg_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\nxg_rmse_final","39f75bc2":"# XGBoost R2 after model tuning\nxg_r2_final = r2_score(y_test, y_pred)\nxg_r2_final","9cb54dea":"from lightgbm import LGBMRegressor","059008eb":"lgbm = LGBMRegressor()\nlgbm_model = lgbm.fit(X_train, y_train)","d66f82f7":"y_pred = lgbm_model.predict(X_test, \n                            num_iteration = lgbm_model.best_iteration_)","72847796":"# LGBM RMSE test score\nnp.sqrt(mean_squared_error(y_test, y_pred))","5436189c":"# LGBM R2 for test data\nr2_score(y_test, y_pred)","9f22fa49":"lgbm_model","025491af":"lgbm_grid = {\n    'colsample_bytree': [0.4, 0.5,0.6,0.9,1],\n    'learning_rate': [0.01, 0.1, 0.5,1],\n    'n_estimators': [20, 40, 100, 200, 500,1000],\n    'max_depth': [1,2,3,4,5,6,7,8] }\n\nlgbm = LGBMRegressor()\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_grid, cv=10, n_jobs = -1, verbose = 2)","dcddb138":"lgbm_cv_model.fit(X_train, y_train)","4e42444c":"lgbm_cv_model.best_params_","72e93e4f":"lgbm_tuned = LGBMRegressor(learning_rate = 0.01, \n                           max_depth = 3, \n                           n_estimators = 500,\n                          colsample_bytree = 0.9)\n\nlgbm_tuned = lgbm_tuned.fit(X_train,y_train)","df398d65":"y_pred = lgbm_tuned.predict(X_test)","2ea97e55":"# LGBM RMSE test score after model tuning\nlgbm_rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\nlgbm_rmse_final","ac694521":"# LGBM R2 for test data after model tuning\nlgbm_r2_final = r2_score(y_test, y_pred)\nlgbm_r2_final","b525ddfb":"print(f\"\"\"Polynomial Regression RMSE: {poly_rmse_final}, R2: {poly_r2_final}\nRF Regression RMSE: {rf_rmse_final}, R2: {rf_r2_final}\nXGBoost Regression RMSE: {xg_rmse_final}, R2: {xg_r2_final}\nLightGBM Regression RMSE: {lgbm_rmse_final}, R2: {lgbm_r2_final}\"\"\")","80108700":"### As it can be seen from the graph above that smoking, bmi and age are the most important independent variables according to the RF regression results. Actually, this result is not a surprise since we can also observe these in the graphs of EDA.\n### An alternative study can be conducted only with these 3 variables and the results can be compared with the study including all of the variables. ","8cf60409":"### Multilinear Regression CV","66ff7e63":"## Converting categorical variables","9bfc9187":"### PLS Model Tuning","2fb2b117":"## Light GBM","3ffd603c":"### As it can be seen from above that non-linear models give better RMSE and R2 scores compared to linear models. Although the selected 4 models have close scores, Light GBM has the highest R2 and lowest RMSE scores. ","fabd6f22":"### Lasso Regression CV","c8f77d72":"## Random Forest","3086abe9":"# Data preprocessing","67d941e3":"## Train-test splitting","d1432c26":"\n## Outliers\n- As it can be seen from the general statistical information of the data set that min & max values of charges and bmi, the standard deviation of charges and the difference between the mean and media of the charges give an impression that there may be some outliers in the data set. The swarm plots and box plots show also that there are some outliers. \n- I will use Local Outlier Factor method to detect the outliers and replace them with an observation which is selected as the threshold.","534c24af":"## PLS Regression","13dece2e":"### Light GBM Model Tuning","d5a5eca4":"### As it is seen above that different multilinear regression models have very close RMSE and R2 scores and they are really not bad. On the other hand, it would be wise to check some of the non-linear models as well since the realation between the charges, and age and bmi are not completely linear. This can be seen through the pairplots and regression lines in my study. \n### In this framework, I will look into some of the selected non-linear models below. ","506ed417":"# General Information about the data\n\n- There are 7 features\/variables in the data namely:\n\n    - age: age of primary beneficiary\n    - sex: insurance contractor gender (female\/male)\n    - bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n    - children: Number of children covered by health insurance (Number of dependents)\n    - smoker: Smoking (whether the beneficiary is a smoker or not)\n    - region: the beneficiary's residential area in the US (northeast, southeast, southwest, northwest)\n    - charges: Individual medical costs billed by health insurance\n- The age, bmi index, children and charges are ratio variables.\n- The sex, smoker and region are nominal categorical variables.\n    \n# Purpose of the study\n\n- The data give information about the profile of the medical insurance beneficiaries and their charged medical costs. The final aim is to predict potential medical costs of a beneficiary.\n\n- I have already done EDA and data visualization in another Kaggle notebook. \nhttps:\/\/www.kaggle.com\/brooksrattigan\/medical-cost-eda-data-visualization\n\n- The dependent variable is medical costs of the beneficiaries and the rest of the variables will be used as independent variables.\n\n- The dependent variable is a continous ratio variable. In this respect, the aim of this study is to conduct different regression models. For this purpose;\n    - The outliers will be handled. \n    - The categorical variables will be transformed so that the categories will have numerical values.\n    - The selected regression models will be used and the RMSE and R2 scores of each model will be compared.","c4715268":"## Ridge Regression","7db5ce32":"## Polynomial Regression","0d9c8dae":"### Ridge Regression CV","4c1a8b57":"### XGBoost Model Tuning","fa4682ac":"## Multilinear Regression","60c12512":"# Conclusion:\n### In this study, I have worked on different regression models to find out the best model which is able predict the medical costs of a medical insurance beneficiary with the given information. \n### Multilinear regression models give fair results with respect to RMSE and R2 scores. \n### On the other hand, non-linear regression models give better results. In fact, EDA of the data has some indications that the relation between charges, and age and bmi is not completely linear. Among the selected 4 non-linear regression models Light GBM has given the best RMSE and R2 scores.\n### Random Forest analysis presents us another fact that smoking, bmi and age are the top 3 features among 8 independent variables. This is not really suprising tough as we can sense that through EDA as well. \n### In this framework, an alternative analysis may be conducted only with these 3 independent variables and the results can be compared. ","92fa1fed":"### RF Model Tuning","543324b7":"## Lasso Regression","0be0dbda":"## XGBoost"}}