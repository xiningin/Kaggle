{"cell_type":{"2caf7b9a":"code","5f28758f":"code","d2524400":"code","acaffa7d":"code","59907eb8":"code","6b2d4e17":"code","ac272e8f":"code","453f1b1d":"code","41a48145":"code","2e63dd65":"code","e7df1c4b":"code","769b5fc1":"code","28b750cc":"code","e6db190d":"code","c901e124":"code","6bc81300":"code","b1cd8248":"code","0022fbac":"code","7b90c186":"code","a4e36c59":"code","5e9bb1bd":"code","0f4d00c8":"code","95f6c9f5":"code","b13fa9fd":"code","eb3d6ee2":"code","b0f4311c":"code","9733fffe":"code","03488079":"markdown","0c0d15ca":"markdown","5b8650a5":"markdown","aa90e268":"markdown","68f5e29f":"markdown","b11e00fd":"markdown","8a06cd50":"markdown","3ee17aca":"markdown","806ed103":"markdown","66027a45":"markdown","ce1602e4":"markdown","022ee989":"markdown","6fd6f706":"markdown","bfda3cfc":"markdown","840c6ac1":"markdown","326bee61":"markdown","264b59b7":"markdown"},"source":{"2caf7b9a":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5f28758f":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","d2524400":"path = '\/kaggle\/input\/jane-street-market-prediction\/'\nos.listdir(path)","acaffa7d":"def plot_timeseries(data, feature):\n    fig = plt.figure(figsize=(10, 6))\n    x = range(len(data))\n    y = data[feature]\n    plt.plot(x, y)\n    plt.grid()","59907eb8":"def memory_reduction(df):\n    \"\"\" Iterate through all the columns of the dataframe df \n        and modify the data type to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","6b2d4e17":"train_data = pd.read_csv(path+'train.csv')\n#test_data = pd.read_csv(path+'example_test.csv')","ac272e8f":"print('number of train samples:', len(train_data))\n#print('number of features:', len(feature_data))","453f1b1d":"train_data[['weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].describe()","41a48145":"plot_timeseries(train_data, 'feature_4')","2e63dd65":"cols_with_missing_train = [col for col in train_data.columns if train_data[col].isnull().any()]","e7df1c4b":"print('train columns with missing data:', cols_with_missing_train[0:4])","769b5fc1":"mean = train_data[cols_with_missing_train].mean()\n#train_data[cols_with_missing_train] = train_data[cols_with_missing_train].fillna(0, inplace=False)\ntrain_data[cols_with_missing_train] = train_data[cols_with_missing_train].fillna(mean, inplace=False)","28b750cc":"#train_data = memory_reduction(train_data)","e6db190d":"#train_data['resp_mean'] = train_data[['resp_4', 'resp']].mean(axis=1)","c901e124":"train_temp = train_data[train_data['weight'] != 0]\ntrain_temp['action'] = (train_temp['resp'] > 0) * 1","6bc81300":"X_train = train_temp.loc[:, train_temp.columns.str.contains('feature')]\ny_train = train_temp.loc[:, 'action']","b1cd8248":"features = ['feature_'+str(i) for i in range(130)]","0022fbac":"mean = X_train[features].mean(axis=0)\nX_train[features] = X_train[features].astype('float32')\nX_train[features] -= X_train[features].mean(axis=0)\nstd = X_train[features].std(axis=0)\nX_train[features] \/= X_train[features].std(axis=0)","7b90c186":"X_train.describe()","a4e36c59":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=2020)","5e9bb1bd":"print('number of train samples', len(X_train))\nprint('number of val samples', len(X_val))","0f4d00c8":"# param_grid = {'objective': ['binary:logistic'],\n#               'learning_rate': [1\/(10**i) for i in range(1, 2)],\n#               'max_depth': [i for i in range(9, 11)],\n#               'n_estimators': [i*100 for i in range(8, 10)],\n#               'random_state': [2020],\n#              'tree_method': ['gpu_hist']}\n# grid = GridSearchCV(XGBClassifier(), param_grid=param_grid, cv=6)\n# grid.fit(X_train, y_train)\n# best_params = grid.best_params_\n# print('Best score of cross validation: {:.2f}'.format(grid.best_score_))\n# print('Best parameters:', best_params)","95f6c9f5":"model_XGB = XGBClassifier(objective='binary:logistic',\n                          n_estimators=900,\n                          learning_rate=0.1,\n                          random_state=2020,\n                          max_depth=9,\n                          tree_method = 'gpu_hist')\nmodel_XGB.fit(X_train, y_train)","b13fa9fd":"preds_val = model_XGB.predict(X_val)\naccuracy_score(y_val, preds_val)","eb3d6ee2":"importance = model_XGB.feature_importances_","b0f4311c":"fig = plt.figure(figsize=(10, 30))\nx = X_train.columns.values\nplt.barh(x, 100*importance, orientation='horizontal')\nplt.title('Feature Importance', loc='left')\nplt.xlabel('Percentage')\nplt.grid()\nplt.show()","9733fffe":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    \n    #Predict Target\n    y_preds = model_XGB.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","03488079":"# Create Environment","0c0d15ca":"\n# Prepare Data\nWe focus on the samples with weight grather than zero. And define the binar target based on the feature resp.","5b8650a5":"# Load Data","aa90e268":"# Libraries","68f5e29f":"# Path","b11e00fd":"# Feature Importance\nWe want to know useful are the features for predicting a target variable.","8a06cd50":"# Intro\nWelcome to the [Jane Street Market Prediction](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data) competition.\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23304\/logos\/header.png)\n\nThis is a starter notebook and will help you to begin with the competition.\n\nWe pass a simple feature engineering to handle missing values and start with a simple XGB Classifier.\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Thank you. <\/span>","3ee17aca":"Scale data:","806ed103":"# Reduce Memory","66027a45":"We use a memory reduction function based on this [notebook](https:\/\/www.kaggle.com\/unrool\/starter-notebook-with-mem-reducing).","ce1602e4":"# Model\nWe use a simple XGB classifier.","022ee989":"The features are numericals. There are several techniques to fill the missing data, i.e. set them to zero oder the mean value.","6fd6f706":"# EDA","bfda3cfc":"# Handle Missing Values","840c6ac1":"# Split Train And Validation Data","326bee61":"# Functions","264b59b7":"# Submission"}}