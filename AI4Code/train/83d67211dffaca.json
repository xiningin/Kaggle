{"cell_type":{"a9c5d151":"code","d86d11e1":"code","8fd6fede":"code","16c7b3a5":"code","d6985ff6":"code","5b327c77":"code","277edc7d":"code","e4d8a6b9":"code","76d50234":"code","40669975":"code","561f58d2":"code","525b362c":"code","3da3a664":"code","31fa4382":"code","64bc6901":"code","5349e562":"code","c2c19c53":"code","86c44497":"code","2a1eac90":"code","965cf9df":"code","a6a4e1f5":"code","39be6721":"code","e7abf11d":"code","61ca2643":"code","eb669421":"code","21b3b0ad":"code","a7c51d97":"code","0aa2d6d6":"code","25628044":"code","8c7f58e6":"code","1b1d63a5":"code","6f67e99b":"code","bb9483cc":"code","0af4b876":"code","409daee6":"code","7decfdeb":"code","cccde25f":"code","01ec19a1":"code","dfce77ee":"code","5838295b":"code","f766850b":"code","2711a2f7":"code","45561ebb":"code","5ce6e741":"code","b2017b95":"code","83c183f8":"code","01c01abb":"markdown"},"source":{"a9c5d151":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Model, load_model\nfrom keras.layers import Input ,BatchNormalization , Activation \nfrom keras.layers.convolutional import Conv2D, UpSampling2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers \nfrom sklearn.model_selection import train_test_split\nimport os\nimport nibabel as nib\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\nimport glob\nimport skimage.io as io\nimport skimage.color as color\nimport random as r\nimport math\nfrom nilearn import plotting\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d86d11e1":"import SimpleITK as sitk\nfrom tqdm import tqdm\nimport cv2\nfrom tensorflow.keras.utils import plot_model\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *","8fd6fede":"Flair= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_flair.nii')\nSeg= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_seg.nii')\nT1= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t1.nii')\nT1ce= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t1ce.nii')\nT2= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t2.nii')","16c7b3a5":"plotting.plot_anat(Flair,cut_coords=[-100, 100, 100],title=\"Flair\",draw_cross =False)","d6985ff6":"plotting.plot_anat(T1,cut_coords=[-100, 100, 100],title=\"T1\",draw_cross =False)","5b327c77":"plotting.plot_anat(T1ce,cut_coords=[-100, 100, 100],title=\"T1ce\",draw_cross =False)","277edc7d":"plotting.plot_anat(Seg,cut_coords=[-100, 100, 100],title=\"Seg\",draw_cross =False)","e4d8a6b9":"plotting.plot_anat(T2,cut_coords=[-100, 100, 100],title=\"T2\",draw_cross =False)","76d50234":"import cv2 as cv\nfrom scipy.signal.signaltools import wiener\n\n# def binary_threshold(img):\n#     img1 = img.copy().astype(np.uint8)\n#     _, thres = cv.threshold(img1,0,255,cv.THRESH_OTSU)\n#     return thres\n\n# def otsu_threshold(img):\n#     img1 = wiener(img, (3,3))\n#     img1 = img1.astype(np.uint8)\n#     _, thres = cv.threshold(img1, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n#     thres = cv.bitwise_and(thres, binary_threshold(img).astype(np.uint8))\n#     return thres\ndef normalize(image):\n    image1 = image.copy()\n    min_ = np.min(image1)\n    max_ = np.max(image1)\n    scale = max_ - min_\n    image1 = (image1 - min_) \/ scale\n    return image1\n\ndef normalise_image(image):\n    norm = np.zeros((192,192))\n    norm_image = cv.normalize(image, norm, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n    norm_image = norm_image.astype(np.uint8)\n    return norm_image\n\n\n\n# def zscore_normalise(img: np.ndarray) -> np.ndarray:\n#     img1 = img.copy()\n#     slices = (img1 != 0)\n#     img1[slices] = (img1[slices] - np.mean(img1[slices]) \/ np.std(img1[slices]))\n#     return img1\n\ndef min_max_preprocess(image, low_perc=1, high_perc=99):\n    non_zeros = image > 0\n    low, high = np.percentile(image[non_zeros], [low_perc, high_perc])\n    image1 = np.clip(image, low, high)\n    image1 = normalise_image(image1)\n    return image1","40669975":"Path= '..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData'\np=os.listdir(Path)\nInput_Data= []\ndef Data_Preprocessing(modalities_dir):\n    all_modalities = []    \n    for modality in modalities_dir:      \n        nifti_file   = nib.load(modality)\n        brain_numpy  = np.asarray(nifti_file.dataobj)\n        #normalise = normalise_image(brain_numpy)\n        #normalised = min_max_preprocess(normalise)\n        all_modalities.append(brain_numpy)\n    brain_affine   = nifti_file.affine\n    all_modalities = np.array(all_modalities)\n    all_modalities = np.rint(all_modalities).astype(np.int16)\n    all_modalities = all_modalities[:, :, :, :]\n    all_modalities = np.transpose(all_modalities)\n    return all_modalities\nfor i in p[:20]:\n    brain_dir = os.path.normpath(Path+'\/'+i)\n    flair     = glob.glob(os.path.join(brain_dir, '*_flair*.nii'))\n    t1        = glob.glob(os.path.join(brain_dir, '*_t1*.nii'))\n    t1ce      = glob.glob(os.path.join(brain_dir, '*_t1ce*.nii'))\n    t2        = glob.glob(os.path.join(brain_dir, '*_t2*.nii'))\n    gt        = glob.glob( os.path.join(brain_dir, '*_seg*.nii'))\n    modalities_dir = [flair[0], t1[0], t1ce[0], t2[0], gt[0]]\n    P_Data = Data_Preprocessing(modalities_dir)\n    Input_Data.append(P_Data)\n","561f58d2":"fig = plt.figure(figsize=(5,5))\nimmmg = Input_Data[1][100,:,:,3]\nimgplot = plt.imshow(immmg, 'gray')\nplt.show()","525b362c":"def Data_Concatenate(Input_Data):\n    counter=0\n    Output= []\n    for i in range(5):\n        print('$')\n        c=0\n        counter=0\n        for ii in range(len(Input_Data)):\n            if (counter != len(Input_Data)):\n                a= Input_Data[counter][:,:,:,i]\n               # print('a={}'.format(a.shape))\n                b= Input_Data[counter+1][:,:,:,i]\n               # print('b={}'.format(b.shape))\n                if(counter==0):\n                    c= np.concatenate((a, b), axis=0)\n                    print('c1={}'.format(c.shape))\n                    counter= counter+2\n                else:\n                    c1= np.concatenate((a, b), axis=0)\n                    c= np.concatenate((c, c1), axis=0)\n                    print('c2={}'.format(c.shape))\n                    counter= counter+2\n        c= c[:,:,:,np.newaxis]\n        Output.append(c)\n    return Output\n\nInData= Data_Concatenate(Input_Data)","3da3a664":"AIO= concatenate(InData, axis=3)\nAIO=np.array(AIO,dtype='float32')\nTR=np.array(AIO[:,:,:,1],dtype='float32')\nTRL=np.array(AIO[:,:,:,4],dtype='float32')\n","31fa4382":"\nX_train , X_test, Y_train, Y_test = train_test_split(TR, TRL, test_size=0.15, random_state=32)\nAIO=TRL=0\n","64bc6901":"def build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\n","5349e562":"input_layer = Input((240, 240, 1))\noutput_layer = build_model(input_layer, 16)\nmodel = Model(input_layer, output_layer)","c2c19c53":"model.summary()","86c44497":"plot_model(model, \n           show_shapes = True, \n           show_layer_names = True, \n           rankdir = 'TB', \n           expand_nested = False, \n           dpi = 60)","2a1eac90":"# Computing Dice_Coefficient\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# Computing Precision \ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives \/ (possible_positives + K.epsilon())\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives \/ (possible_negatives + K.epsilon())","965cf9df":"# Compiling the model \nAdam=optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=Adam, loss='binary_crossentropy', metrics=['accuracy',dice_coef,precision,sensitivity,specificity])","a6a4e1f5":"# Fitting the model over the data\nimport time\nstart_time = time.time()\nhistory = model.fit(X_train,Y_train,batch_size=32,epochs=40,validation_split=0.20,verbose=1,initial_epoch=0)\nend_time = time.time()\nprint(end_time - start_time)","39be6721":"# Evaluating the model on the training and testing data \nmodel.evaluate(x=X_train, y=Y_train, batch_size=32 , verbose=1, sample_weight=None, steps=None)\nmodel.evaluate(x=X_test, y=Y_test, batch_size=32, verbose=1, sample_weight=None, steps=None)","e7abf11d":"# Accuracy vs Epoch\ndef Accuracy_Graph(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    #plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()\n    \n# Dice Similarity Coefficient vs Epoch\ndef Dice_coefficient_Graph(history):\n\n    plt.plot(history.history['dice_coef'])\n    plt.plot(history.history['val_dice_coef'])\n    #plt.title('Dice_Coefficient')\n    plt.ylabel('Dice_Coefficient')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()\n# Loss vs Epoch\ndef Loss_Graph(history):\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    #plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()","61ca2643":"# Plotting the Graphs of Accuracy, Dice_coefficient, Loss at each epoch on Training and Testing data\nAccuracy_Graph(history)\nDice_coefficient_Graph(history)\nLoss_Graph(history)","eb669421":"from keras.utils import plot_model\nplot_model(model,to_file='model.png')","21b3b0ad":"model.save('.\/BraTs2020.h5')\nmodel.save_weights('BraTs2020.hdf5')","a7c51d97":"model.load_weights('.\/BraTs2020.h5')","0aa2d6d6":"fig = plt.figure(figsize=(5,5))\nimmmg = TR[250,:,:]\nimgplot = plt.imshow(immmg,'gray')\nplt.show()","25628044":"pref_Tumor = model.predict(TR)","8c7f58e6":"fig = plt.figure(figsize=(5,5))\nimmmg = pref_Tumor[250,:,:]\nimgplot = plt.imshow(immmg,'gray')\nplt.show()","1b1d63a5":"plt.figure(figsize=(15,10))\n\n\nplt.subplot(341)\nplt.title('Sample 1')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[250,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[250,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(342)\nplt.title('Sample 2')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[550,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[550,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(343)\nplt.title('Sample 3')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[400,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[400,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(344)\nplt.title('Sample 4')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[690,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[690,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(345)\nplt.title('Sample 5')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[850,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[850,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(346)\nplt.title('Sample 6')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1450,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1450,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(347)\nplt.title('Sample 7')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1800,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1800,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(348)\nplt.title('Sample 8')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[60,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[60,:,:]),alpha=0.3,cmap='Reds')","6f67e99b":"import pandas as pd\nhist_df = pd.DataFrame(history.history)\nhist_df['epoch_id'] = [i for i in range (1,len(hist_df)+1)]\nhist_df=hist_df.reindex(columns= ['epoch_id', 'loss', 'accuracy','val_loss','val_accuracy'])\nprint(hist_df)\n","bb9483cc":"def Convolution(input_tensor,filters):\n    \n    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1))(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x) \n    return x\n\ndef model(input_shape):\n    \n    inputs = Input((input_shape))\n    \n    conv_1 = Convolution(inputs,32)\n    maxp_1 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_1)\n    \n    conv_2 = Convolution(maxp_1,64)\n    maxp_2 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_2)\n    \n    conv_3 = Convolution(maxp_2,128)\n    maxp_3 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_3)\n    \n    conv_4 = Convolution(maxp_3,256)\n    maxp_4 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_4)\n    \n    conv_5 = Convolution(maxp_4,512)\n    upsample_6 = UpSampling2D((2, 2)) (conv_5)\n    \n    conv_6 = Convolution(upsample_6,256)\n    upsample_7 = UpSampling2D((2, 2)) (conv_6)\n    \n    upsample_7 = concatenate([upsample_7, conv_3])\n    \n    conv_7 = Convolution(upsample_7,128)\n    upsample_8 = UpSampling2D((2, 2)) (conv_7)\n    \n    conv_8 = Convolution(upsample_8,64)\n    upsample_9 = UpSampling2D((2, 2)) (conv_8)\n    \n    upsample_9 = concatenate([upsample_9, conv_1])\n    \n    conv_9 = Convolution(upsample_9,32)\n    outputs = Conv2D(1, (1, 1), activation='sigmoid') (conv_9)\n    \n    model = Model(inputs=[inputs], outputs=[outputs]) \n    \n    return model","0af4b876":"# Loding the Light weighted CNN\nmodel2 = model(input_shape = (240,240,1))\nmodel2.summary()","409daee6":"# Compiling the model \nAdam=optimizers.Adam(lr=0.001)\nmodel2.compile(optimizer=Adam, loss='binary_crossentropy', metrics=['accuracy',dice_coef,precision,sensitivity,specificity])","7decfdeb":"# Fitting the model over the data\nimport time\nstart_time = time.time()\nhistory = model2.fit(X_train,Y_train,batch_size=32,epochs=40,validation_split=0.20,verbose=1,initial_epoch=0)\nend_time = time.time()\nprint(end_time - start_time)","cccde25f":"# Evaluating the model on the training and testing data \nmodel2.evaluate(x=X_train, y=Y_train, batch_size=32 , verbose=1, sample_weight=None, steps=None)\nmodel2.evaluate(x=X_test, y=Y_test, batch_size=32, verbose=1, sample_weight=None, steps=None)","01ec19a1":"# Plotting the Graphs of Accuracy, Dice_coefficient, Loss at each epoch on Training and Testing data\nAccuracy_Graph(history)\nDice_coefficient_Graph(history)\nLoss_Graph(history)","dfce77ee":"from keras.utils import plot_model\nplot_model(model2,to_file='model.png')","5838295b":"model2.save('.\/BraTs2020.h5')\nmodel2.save_weights('BraTs2020.hdf5')","f766850b":"model2.load_weights('.\/BraTs2020.h5')","2711a2f7":"fig = plt.figure(figsize=(5,5))\nimmmg = TR[250,:,:]\nimgplot = plt.imshow(immmg)\nplt.show()","45561ebb":"pref_Tumor = model2.predict(TR)","5ce6e741":"fig = plt.figure(figsize=(5,5))\nimmmg = pref_Tumor[250,:,:,0]\nimgplot = plt.imshow(immmg)\nplt.show()","b2017b95":"plt.figure(figsize=(15,10))\n\n\nplt.subplot(341)\nplt.title('Sample 1')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[250,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[250,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(342)\nplt.title('Sample 2')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[550,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[550,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(343)\nplt.title('Sample 3')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[400,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[400,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(344)\nplt.title('Sample 4')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[690,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[690,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(345)\nplt.title('Sample 5')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[850,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[850,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(346)\nplt.title('Sample 6')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1450,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1450,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(347)\nplt.title('Sample 7')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1800,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1800,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(348)\nplt.title('Sample 8')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[60,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[60,:,:]),alpha=0.3,cmap='Reds')","83c183f8":"import pandas as pd\nhist_df = pd.DataFrame(history.history)\nhist_df['epoch_id'] = [i for i in range (1,len(hist_df)+1)]\nhist_df=hist_df.reindex(columns= ['epoch_id', 'loss', 'accuracy','val_loss','val_accuracy'])\nprint(hist_df)","01c01abb":"**Unet**"}}