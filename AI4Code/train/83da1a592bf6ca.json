{"cell_type":{"f34c48b7":"code","019c20ec":"code","818dc0ac":"code","68d82838":"code","23d25828":"code","69a5755d":"code","5e621f7b":"code","d0b817a5":"code","f833db28":"code","2697597b":"code","c92a357f":"code","03508d4a":"code","bd2deb03":"code","a41495c0":"code","b9f9bfd0":"code","f0c84d23":"code","60037256":"code","200aa3e4":"code","31823f5d":"code","a9381c06":"code","d8fd3ea8":"markdown","e80fcd91":"markdown","0ca76dde":"markdown","5d28b7ed":"markdown","9185ddad":"markdown","22d1eb60":"markdown","42440f14":"markdown"},"source":{"f34c48b7":"import nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","019c20ec":"# Run only if this is the first ever time using nltk\n# nltk.download()","818dc0ac":"df = pd.read_csv(\"..\/input\/tweets-data\/tweets.csv\")\ndf","68d82838":"[nltk.sent_tokenize(item) for item in df[\"text\"].values]","23d25828":"tokens = [nltk.word_tokenize(item) for item in df[\"text\"].values]\ntokens","69a5755d":"import string\nimport re\n\nregex = re.compile(f'[{re.escape(string.punctuation)}]')\n\ntokens_without_punctuation = [regex.sub(u'', word) for words in tokens for word in words if not regex.sub(u'', word) == u'']\ntokens_without_punctuation[:10]","5e621f7b":"from nltk.corpus import stopwords\n\nstop_words = stopwords.words(\"english\")\nstop_words.append(\"via\")\nstop_words\n\nwords = [token for token in tokens_without_punctuation if token not in stop_words]\nwords[:15]","d0b817a5":"import re\n    \nregex = re.compile('http\\S+')\n\ntokens_without_links = [regex.sub(u'', word) for word in words if not regex.sub(u'', word) == u'' and not word.startswith(\"tc\")]\ntokens_without_links[:20]","f833db28":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n\nstemmed_words = [stemmer.stem(word) for word in tokens_without_links]\nstemmed_words[:10]","2697597b":"from collections import Counter\n\ncounter = Counter(stemmed_words)\ncounter.most_common(20)","c92a357f":"def plot_words(words, values):\n    indexes = np.arange(len(words))\n    plt.xticks(indexes, words, rotation=90)\n    plt.bar(indexes, values)","03508d4a":"plot_words(counter.keys(), counter.values())","bd2deb03":"most_common_words = [word for word, _ in counter.most_common(20)]\nmost_common_values = [count for _, count in counter.most_common(20)]","a41495c0":"plot_words(most_common_words, most_common_values)","b9f9bfd0":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_without_links]\nlemmatized_words[:20]","f0c84d23":"lemmatized_counter = Counter(lemmatized_words)\nlemmatized_counter.most_common(20)","60037256":"plot_words(lemmatized_counter.keys(), lemmatized_counter.values())","200aa3e4":"most_common_words = [word for word, _ in lemmatized_counter.most_common(20)]\nmost_common_values = [count for _, count in lemmatized_counter.most_common(20)]","31823f5d":"plot_words(most_common_words, most_common_values)","a9381c06":"pd.DataFrame({\n    \"words\": lemmatized_words\n}).to_csv(\"words.csv\", index=False, encoding=\"UTF-8\")","d8fd3ea8":"### Tokenize words","e80fcd91":"### Stem words","0ca76dde":"### Remove stop words","5d28b7ed":"### Remove links","9185ddad":"### Lemmatize words","22d1eb60":"## Clean text data","42440f14":"### Remove punctuation"}}