{"cell_type":{"8ff6f485":"code","f6fb9535":"code","fe59fca5":"code","dfa9e337":"code","af3e1256":"code","13298d9b":"code","eeed7d29":"code","bc88ca67":"code","32540279":"code","6dcf4563":"code","4063c14a":"code","0badc101":"code","43af37cb":"code","e466859f":"code","1f5dcce2":"code","2b23760f":"code","011259b4":"code","4ff95b79":"code","b730374a":"code","5dd24a2c":"code","b8a49fe8":"code","8a458e66":"code","a4996cbd":"code","d6b87c36":"code","65611b93":"code","5d55a1e3":"code","5b0352e0":"code","9ebbd1a7":"code","2c638a35":"code","14c8f998":"code","876e801d":"code","fb7616d5":"code","abc960d3":"markdown","34c6dc7a":"markdown","03136508":"markdown","fc2058bd":"markdown","e74a97b8":"markdown","14e14bde":"markdown","f94853b5":"markdown","c4f44824":"markdown","40a631d6":"markdown","7acf53e6":"markdown","59d7a50c":"markdown","ded33f72":"markdown","1bc60812":"markdown","558aa380":"markdown","0dd861cb":"markdown","6f645c2d":"markdown","f9807e11":"markdown","910c53e0":"markdown","924750be":"markdown","fdf27161":"markdown","1c289b35":"markdown","5da52e1e":"markdown","a29c86fd":"markdown","ca75c350":"markdown","1953b216":"markdown","5b00bdd0":"markdown","d85e6c6e":"markdown","b076fb99":"markdown","cbbdc2b9":"markdown","f919ba8c":"markdown"},"source":{"8ff6f485":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f6fb9535":"df = pd.read_csv(\"..\/input\/titanic_data.csv\")\ndf.head()","fe59fca5":"df.columns","dfa9e337":"drop_rows = [\"PassengerId\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\ncdf = df.drop(drop_rows, axis=1)","af3e1256":"cdf.head()","13298d9b":"cdf.describe()","eeed7d29":"cdf.groupby(\"Survived\").mean()","bc88ca67":"cdf.groupby(df[\"Age\"].isnull()).mean()","32540279":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in [\"Age\", \"Fare\"]:\n    died = list(cdf[cdf[\"Survived\"]==0][i].dropna())\n    survived = list(cdf[cdf[\"Survived\"]==1][i].dropna())\n    xmin= min(min(died), min(survived))\n    xmax= max(max(died), max(survived))\n    width = (xmax-xmin)\/40\n    sns.distplot(died, color=\"r\", kde=False, bins=np.arange(xmin,xmax,width))\n    sns.distplot(survived, color=\"g\", kde=False, bins=np.arange(xmin,xmax,width))\n    plt.legend([\"Did not Survived\", \"Survived\"])\n    plt.title(\"Overlaid histogram for {}\".format(i))\n    plt.show()","6dcf4563":"for i, col in enumerate([\"Pclass\", \"SibSp\", \"Parch\"]):\n    plt.figure(i)\n    sns.catplot(x=col, y=\"Survived\", data=cdf, kind=\"point\", aspect=2)","4063c14a":"cdf[\"Family\"] = cdf[\"SibSp\"] + cdf[\"Parch\"]\ncdf.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)\nsns.catplot(x=\"Family\", y=\"Survived\", data=cdf, kind=\"point\", aspect=2)","0badc101":"cdf[\"Age\"].fillna(cdf[\"Age\"].mean(), inplace=True)\ncdf.isnull().sum()","43af37cb":"cdf.head(10)","e466859f":"rows_to_drop = [\"PassengerId\", \"Pclass\", \"Name\", \"Age\", \"SibSp\", \"Parch\", \n                \"Fare\"]\nidf = df.drop(rows_to_drop, axis=1)\nidf.head()","1f5dcce2":"idf.info()","2b23760f":"idf.groupby(idf[\"Cabin\"].isnull()).mean()","011259b4":"idf[\"Cabin_ind\"] = np.where(idf[\"Cabin\"].isnull(), 0, 1)\nidf.head()","4ff95b79":"for i, col in enumerate([\"Cabin_ind\", \"Sex\", \"Embarked\"]):\n    plt.figure(i)\n    sns.catplot(x=col, y=\"Survived\", data=idf, kind=\"point\", aspect=2)","b730374a":"idf.pivot_table(\"Survived\", index=\"Sex\", columns=\"Embarked\", aggfunc=\"count\")","5dd24a2c":"idf.pivot_table(\"Survived\", index=\"Cabin_ind\", columns=\"Embarked\", \n                aggfunc=\"count\")","b8a49fe8":"idf.head()","8a458e66":"gender_num = {\"male\": 0, \"female\": 1}\nidf[\"Sex\"] = idf[\"Sex\"].map(gender_num)\nidf.head()","a4996cbd":"idf.drop([\"Cabin\", \"Embarked\"], axis=1, inplace=True)\nidf.head()","d6b87c36":"df.drop([\"Name\", \"Ticket\", \"Embarked\"], axis=1, inplace=True)\ndf.head()","65611b93":"df[\"Sex\"] = idf[\"Sex\"]\ndf[\"Age\"] = cdf[\"Age\"]\ndf[\"Cabin\"] = idf[\"Cabin_ind\"]\ndf.head()","5d55a1e3":"from sklearn.model_selection import train_test_split\nx = df.drop(\"Survived\", axis=1)\ny = df[\"Survived\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.33, random_state=42)\n","5b0352e0":"for dataset in [y_train, y_test]:\n    print(round(len(dataset)\/len(x), 2))","9ebbd1a7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nrf = RandomForestClassifier()\nscores = cross_val_score(rf,x, y, cv=5)\nscores","2c638a35":"def print_result(result):\n    print(\"BEST PARAMS:{}\\n\".format(result.best_params_))\n    \n    means = result.cv_results_[\"mean_test_score\"]\n    std = result.cv_results_[\"std_test_score\"]\n    for mean, std, params in zip(means, std, result.cv_results_[\"params\"]):\n        print(\"{} (+\/-{}) for {}\".format(round(mean, 3), round(std, 3), params))","14c8f998":"parameters = {\n    \"n_estimators\": [5, 50, 100], \n    \"max_depth\":[2, 10, 20, None]\n}\n\ncv = GridSearchCV(rf, parameters, cv=5)\ncv.fit(x, y)\nprint_result(cv)","876e801d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nrf = RandomForestClassifier(n_estimators=100, max_depth=20)\nrf.fit(x_train,y_train)\ny_hat = rf.predict(x_test)\nprint(\"The accuracy score: \",accuracy_score(y_test, y_hat))\n","fb7616d5":"print(\"The f1_score:\\n\", classification_report(y_test, y_hat))","abc960d3":"* Name - Name of the passenger\n* Pcalss- Ticket Class\n* Sex - Sex of the passenger\n* Age - Age in years\n* SibSp - Number of siblings and spouses aboard\n* Parch - Number of parents and children aboard\n* Ticket - Ticket number\n* Fare - Passenger fare\n* Cabin - Cabin number \n* Embarked - Port of embarkation ( C=Cherbourg, Q=Queenstown, S=Southampton)","34c6dc7a":"Let's convert the sex feature into numeric.","03136508":"Let's plot the continuous features which are age and fare.\n\nWe can see that, there isn't much difference in age between people that survived and those that did not.\n\nWe could see there's a heavy concentration of people between 20 and 30, maybe extending out to 40, but the distribution between survived versus not survived is relatively similar except for the very low end. \n\nNow let's look at fare. We noticed a fairly drastic difference in the averages. The average is about $48 for those that survived, and about $22 for those that did not.\n\nSo fare can probably help us predict whether somebody survived, but it's not quite as cut and dry as the average indicate. The average is likely being impacted by some outliers. ","fc2058bd":"Let's find the best parameters for our model.\nA function just to help us print out the results a little more cleanly. ","e74a97b8":"Let's do groupby for cabin to see the missing values.\nThis is only returning the survived field because the survived field is the only feature left in the data frame that is continuous right now.\n\nWe can see over 66% of the people who have non missing cabin values survived, well less than 30% of those who had a missing cabin value survived.\n\nit's very clear that cabin is not missing at random, it's actually very strong indicator whether somebody survived or not. \n\nSo one easy hypothesis here is just that people without an assigned cabin literally don't have a cabin. Maybe they were stuck somewhere in the bows of the ship and that's why so few survived. ","14e14bde":"Now let's go ahead and print out the first five rows.  \nSo now you can see a much cleaner data frame where we only have the continuous features here.","f94853b5":"Let's fill in the missing values in age. We are going to just replace the missing values with average value for that age. This way it satisfies the model by making sure that there's a value in there, but by replacing it with the average value, it's not biasing the model towards one outcome or another. ","c4f44824":"In row 5, we have 29.699, and that's going to be the average value. And you can tell, because the rest of the values are integers, and this one is a float.","40a631d6":"By looking at the info of our data, we can notice that cabin only has 204 non-null features and then embarked only has 889 non-null features while we have 891 non-null features for other features, that means we have alot of missing values for cabin and just a couple for embarked","7acf53e6":"So we can say around 84% accuracy but depending on the subset trained and tested on, that could be as low as 72% or as high as 85%. ","59d7a50c":"So the way we're going to handle that is just create a binary indicator that just says whether a person had a cabin or not. ","ded33f72":"The first thing is to actually look at the data.\nWe have personal information here in the name feature, but this is public information so that's okay. ","1bc60812":"let's look at the relationship between port and whether they had cabins or not.\n\nhere we can see that for Queenstown and Southampton there're drastically more people without cabins than with cabins. So, for Queenstown there's about fifteen times more people without cabins than with cabins, and for South Hampton there's about three and a half times more people without cabins than with cabins. But, then when we look at Cherbourg, it's relatively close, only about 50% more people had no cabin versus having a cabin. Given that we know people that had cabins were much more likely to survive, this would explain why Cherbourg had a much higher survival rate. \n\nwe have now seen that fewer people from Southampton survived, because so many more men boarded there, and a much higher ratio of people survived from Cherbourg, because so many people that boarded there had cabins. ","558aa380":"Now, it's telling us that the best hyper parameter values are max depth of 20, with 100 estimators. ","0dd861cb":"Explore Sex, Cabin and embark","6f645c2d":"Let's look at the shapes of our train and test sets.","f9807e11":"We want to understand whether the missing values in age are at random, for instance if it was never reported for certain people, or if it's missing in a systematic way, e.g. they didn't ask the age of anybody in first class. Because this will inform how we handle these missing values. \n\nPeople without age reported were less likely to survive, had a higher class number, had fewer parents or children, and a lower fare. So based on all this information, we could theorize that age wasn't recorded for people in the bowels of the ship that maybe were traveling alone. \n\nBut in summary, nothing really jumps out here that would require us to treat these missing values any specific way.","910c53e0":"So we're going to start with the best hyper parameter combination that we found. ","924750be":"Now let's explore the categorical features","fdf27161":"Let's plot our categorical feature.\n\nSo, looking at this first plot, this says that people without cabins had a 30% survival rate, and those who did have cabins were around 66%. \n\nWe see that more than 70% of the women survived, while only 20% of men survived, so it's really clear that this has really powerful splitting power. \n\nby looking at the embarked plot, It's unlikely that where people boarded caused them to survive or not, more than likely this correlated with other features that are already being accounted for in our data. For instance, perhaps a higher ratio of men boarded in South Hampton, or maybe more people that boarded in Cherbourg had cabins, and thus were more likely to survive. ","1c289b35":"Let's look at the shape of the data. It can be seen that under age the count was only  714, whereas it is 891 for the other five features. The reason is that there is missing values with age.\n\nFor our Survived feature, which is our target, that's a binary(0 or 1), since it's binary we can use the mean to tell us what percent of people survived in this dataset. Here it tells us 38.4% of people survived. It is always useful to know that when we are doing classification problem.","5da52e1e":"We can actually explore these hypothesis using pivot tables.","a29c86fd":"The features name, passenger ID, and ticket aren't going to be of much use here in determining whether a passenger survived. So we're going to just drop those. \nIn addition to name, passenger ID, and ticket, sex, cabin, and embarked are all non-numeric features which we'll be exploring later on","ca75c350":"the actual Cabin feature is repetitive and unnecessary. We discovered that there are different survival rates for the different levels of the Embarked feature. However, this was not a causal relationship. Other features like percent of men boarding at each port or percent of people that had cabins were the real causal factors, and since those are already accounted for in this model, we can drop the Embarked feature.  ","1953b216":"Now let's look at Pclass, SibSp, and Parch. \nThese features are ordinal.\n\nSo for first class passengers, roughly 63% of those people survived, and then these vertical bars represent the error.\n\nBetter class, first class is more likely to survive than third class.\npeople with more siblings are also less likely to survive.\nthose with more parents or children are also less likely to survive.","5b00bdd0":"Now our data is cleaned and ready for modelling and prediction.\nlet's first split our data into train and test sets.","d85e6c6e":"so this is telling us that 95 people that boarded in Cherbourg were male while only 73 were female. Now you can see that for Cherbourg and Queenstown the number of men versus women boarding is fairly close, but in South Hampton, you can see that more than double the amount of men boarded as women. \n\nGiven the fact that we know men were much less likely to survive than women, this would explain why South Hampton had the lowest survival rate of all the ports. ","b076fb99":"Let's add the cleaned features in the actual dataframe.","cbbdc2b9":"Now, it seems like SibSp and Parch all have to do with family members aboard, and when you have more, you're less likely to survive. So it seems like perhaps we could merge that into a single feature.","f919ba8c":"Now let's see which features might be strong indicators of whether somebody survived or not.\nOne way to do that is to just groupby the two levels of survive, and generate the average value of the other features at those two levels of the target variable.\n\nSo the average age of a person that didn't survive is 30.6 and the average age for somebody who did survive is 28.3. \n\nThe average fare for somebody who didn't survive is around 22 dollars, while the average fare for somebody who did survive is over 48 dollars. \n\nThe average Pclass for somebody who didn't survive is around 2.5 (2nd and 3rd class), while the average Pclass for somebody who did survive is over 1.9(1st and 2nd class)."}}