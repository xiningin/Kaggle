{"cell_type":{"0cb577b5":"code","f3417bb5":"code","fb43c30a":"code","65174e18":"code","c4212f46":"code","1a2189c3":"code","016c1a14":"code","88d96f25":"code","81dabcd8":"code","5b456f98":"code","fd2c0f92":"code","36c88824":"code","d3beb3ad":"code","0622ccc1":"code","2647e385":"code","84814eb5":"code","75a5b3e4":"code","912b4099":"code","810d9891":"code","96f20253":"code","1175c388":"code","a57e603e":"code","8137f83a":"code","fdfad491":"code","6c38707c":"code","ea254c9e":"code","1e1e6f07":"code","157454fa":"code","1ebf9d01":"code","92f77374":"code","7622be9d":"code","ecf0abc4":"code","31a6c374":"code","35da3afc":"code","a80186c1":"code","bbc98644":"code","74c80477":"code","039b801c":"code","e9270069":"code","aeaf936e":"code","de0d9906":"code","dda07ac2":"code","5f183cf5":"code","8e2bbc30":"code","878e0014":"code","042d129c":"code","a0f4398b":"code","2606cfb5":"code","75617ae7":"code","233aebec":"code","29698cdd":"code","86512f5f":"code","5b3826db":"code","438569d3":"code","13e659e2":"code","2ee3e5b5":"code","03eaec00":"code","94b9b5f9":"code","706c2f68":"code","af595f48":"code","8e00a754":"code","06b86741":"code","bf31e7c6":"code","0ca0e3a4":"code","49d07e78":"code","d137e8ea":"code","87a861a1":"code","425e02a6":"code","950bb7a0":"code","0e0494ee":"code","a42def55":"code","7d039ff3":"code","12152800":"code","432b08e7":"code","44cb489f":"code","9aa0d3a2":"code","84e4cb4d":"code","40fc6641":"code","500b9e74":"code","8f27508b":"code","297073a6":"code","d13d62bd":"code","15b55c6c":"code","413394cd":"code","09914b0c":"code","1aef048d":"code","994ca9f4":"code","53d673fe":"code","6f35e42e":"code","ea06ed90":"code","e27aba0a":"code","0745a349":"code","d7b3b9c5":"code","8b213625":"code","a2c2da9f":"code","e5ad4b7f":"code","62243b08":"code","345f23da":"code","ffeae749":"code","cdb93998":"code","993a383f":"code","1e800cfe":"code","1859d75b":"code","4ba68281":"code","f45c7672":"code","e95eb170":"code","bf890350":"code","92bd38c2":"code","cf2a287e":"code","3a0213f6":"code","20d80e94":"code","a946479d":"code","dcbd6ffc":"code","c8aed06c":"code","9f414119":"code","bbee9900":"code","21b85897":"code","b05bd642":"code","55709a2a":"code","d8a89487":"code","36624d13":"code","6b060a4f":"code","f9f72b1b":"code","114ea047":"code","b136d4e8":"code","93731d11":"code","0af1cf62":"code","17e68fa1":"code","3a8c1335":"code","244d24d0":"code","b93f53ac":"code","a7393578":"markdown"},"source":{"0cb577b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport nltk\nfrom string import punctuation\n# Any results you write to the current directory are saved as output.","f3417bb5":"sent=\"India is a republic country. We are proud Indians\"","fb43c30a":"text1=nltk.word_tokenize(sent)","65174e18":"vocab=sorted(set(text1))","c4212f46":"print(vocab)","1a2189c3":"# print(punctuation(vocab))\nprint(list(punctuation))","016c1a14":"vocab1=[]\nfor i in vocab:\n    if i not in punctuation:\n        vocab1.append(i)\nprint(vocab1)","88d96f25":"from nltk.stem.snowball import SnowballStemmer\nstemmed_vocab=[]\nstemObj = SnowballStemmer(\"english\")\nfor i in vocab1:\n    stemmed_vocab.append(stemObj.stem(i))\nprint(stemmed_vocab)","81dabcd8":"from nltk.stem.wordnet import WordNetLemmatizer\nlemmaObj =  WordNetLemmatizer()\nlemmaObj.lemmatize(\"went\",pos='v')","5b456f98":"# lemmaObj =  WordNetLemmatizer()\n# pos_list=worpos_tag(vocab1)\n# print(pos_list)\n# lemma_vocab=[]\n# for i,pos in pos_list:\n#     lemma_vocab.append(lemmaObj.lemmatize(i,pos=pos))\n# print(lemma_vocab)","fd2c0f92":"from nltk.corpus import stopwords\nwo_stop_words=[]\nstop_words=set(stopwords.words(\"english\"))\nfor i in vocab1:\n    if i not in stop_words:\n        wo_stop_words.append(i)\nprint(wo_stop_words)","36c88824":"vocab_wo_punct=vocab1","d3beb3ad":"from nltk import pos_tag\npos_list = pos_tag(vocab_wo_punct)\nprint(pos_list)\n","0622ccc1":"vocab_no_punct=vocab1","2647e385":"from nltk import FreqDist\nFreqDist(vocab_no_punct).plot()","84814eb5":"from nltk import ngrams\nfrom nltk import ConditionalFreqDist\n#use 2 for bigrams\nbigrams = ngrams(vocab_no_punct,2)\nprint(list(bigrams))\nnltk.ConditionalFreqDist(list(bigrams))","75a5b3e4":"# from nltk.corpus import brown\n# cfd = nltk.ConditionalFreqDist(\n#           (genre, word)\n#            for genre in brown.categories()\n#            for word in brown.words(categories=genre))","912b4099":"# cfd.tabulate().shape","810d9891":"# t1 = \"This is Anand here\"\n# t2 = \"we have now breached the inner circle\"\n# t3 = \"I dont think i want to not wish you a merry christmas\"\n# text = [t1,t2,t3]\n# # words = [\"we\",\"the\"]`a`\n# nltk.ConditionalFreqDist()","96f20253":"from nltk import ngrams\n#use 2 for trigrams\ntrigrams = ngrams(vocab_no_punct,3)\nprint(list(trigrams))","1175c388":"from nltk import ngrams\n#use 2 for bigrams\nfourgrams = ngrams(vocab_no_punct,4)\nprint(list(fourgrams))","a57e603e":"text=\"I saw John coming. He was with Mary. I talked to John and Mary. \\\nJohn said he met Mary on the way. John and Mary were going to school.\"\n\nnltk.FreqDist(nltk.word_tokenize(text)).plot()","8137f83a":"from nltk.book import *\n","fdfad491":"from nltk import Text\nvocab_no_punct.append(\"India\")\ntext_nltk = Text(vocab_no_punct)\ntype(text_nltk)","6c38707c":"text_nltk.dispersion_plot(['India', 'Indians', 'We', 'a', 'are','country', 'is', 'proud', 'republic'])","ea254c9e":"import nltk\nimport string\nfrom nltk import word_tokenize\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk import ngrams","1e1e6f07":"file = open(\"..\/input\/nlp-text-sample\/nlp wikipedia sample.txt\",'r')\n# lst\ntext = ''\nfor i in file.readlines():\n    text += i\n# print(text)","157454fa":"trimmed_text = text.strip()\n# print(trimmed_text)","1ebf9d01":"converted_text = trimmed_text.lower()\n# print(converted_text)","92f77374":"tokenized_list = word_tokenize(converted_text)\n# print(tokenized_list)","7622be9d":"punct_tokenized_list = wordpunct_tokenize(converted_text)\n# print(punct_tokenized_list)","ecf0abc4":"vocab_set = set(tokenized_list)\n# print(vocab_list)","31a6c374":"set_wo_stopwords = vocab_set - set(stopwords.words(\"english\"))\n# print(set_wo_stopwords)","35da3afc":"set_wo_punctuation = set_wo_stopwords - set(punctuation)\n# print(set_wo_punctuation)","a80186c1":"stemmed_list= []\nstemObj = SnowballStemmer(\"english\")\nfor i in set_wo_punctuation:\n    stemmed_list.append(stemObj.stem(i))\n# print(stemmed_list)","bbc98644":"pos_tag_list = pos_tag(set_wo_punctuation)\nprint(pos_tag_list)","74c80477":"def parts_of_speech(pos):\n    if pos.startswith(\"N\"):\n        return wordnet.NOUN\n    elif pos.startswith(\"J\"):\n        return wordnet.ADJ\n    elif pos.startswith(\"V\"):\n        return wordnet.VERB\n    elif pos.startswith(\"R\"):\n        return wordnet.ADV\n    elif pos.startswith(\"S\"):\n        return wordnet.ADJ_SAT\n    else:\n        return ''","039b801c":"lemma_list = []\nlemmaObj =  WordNetLemmatizer()\nfor word,pos in pos_tag_list:\n    get_pos = parts_of_speech(pos)\n    if get_pos != '':\n        lemma_list.append(lemmaObj.lemmatize(word, pos = get_pos))\n    else:\n        lemma_list.append(word)\n# print(lemma_list)","e9270069":"bigrams = ngrams(set_wo_punctuation,2)\n# print(list(bigrams))","aeaf936e":"from nltk import RegexpParser\ngrammar = r\"\"\"NP: {<DT|PP\\$>?<JJ>*<NN>}\n           VP: {<NNP>+}\"\"\"\nregParser = RegexpParser(grammar)","de0d9906":"#  print(regParser.parse(pos_tag(punct_tokenized_list)))","dda07ac2":"barack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician who served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017. A member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\n","5f183cf5":"from nltk import ne_chunk\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\ntokenised_barack = word_tokenize(barack)\npos_list = pos_tag(tokenised_barack)\nne_ch = ne_chunk(pos_list)\n# ne_ch.draw()\nprint(ne_ch)","8e2bbc30":"from nltk import RegexpParser\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\n\ngrammar = r\"\"\"Place: {<NNP><NNPS>+}\n           Date: {<NNP><CD><,><CD>}\n           Person: {<NNP>+}\n           \"\"\"\n\ntokenised_barack = word_tokenize(barack)\npos_list = pos_tag(tokenised_barack)\nregParser = RegexpParser(grammar)\nreg_lines = regParser.parse(pos_list)\n# print(reg_lines)","878e0014":"#default tagger\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import brown\nfrom nltk.tag import DefaultTagger\n\ntagged_sentences = brown.tagged_words(categories = \"news\") \n\ntags = [tag for word,tag in tagged_sentences]\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\ntokenised_barack = word_tokenize(barack)\ndefault_tag = nltk.FreqDist(tags).max()\ndefault_tagger = nltk.DefaultTagger(default_tag)\ntagged_barack = default_tagger.tag(tokenised_barack)\n# print(tagged_barack)\n# print(default_tagger.evaluate([pos_list]))","042d129c":"#regexp tagger\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import brown\nfrom nltk.tag import RegexpTagger\n\npatterns = [\n     (r'.*ing$', 'VBG'),               # gerunds\n     (r'.*ed$', 'VBD'),                # simple past\n     (r'.*es$', 'VBZ'),                # 3rd singular present\n     (r'.*ould$', 'MD'),               # modals\n     (r'.*\\'s$', 'NN$'),               # possessive nouns\n     (r'.*s$', 'NNS'),                 # plural nouns\n     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n     (r'.*', 'NN')                     # nouns (default)\n]\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\n\ntokenised_barack = word_tokenize(barack)\nregexp_tagger = nltk.RegexpTagger(patterns)\ntagged_barack = regexp_tagger.tag(tokenised_barack)\n# print(tagged_barack)\n# regexp_tagger.evaluate([pos_list])\n","a0f4398b":"#lookup tagger\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import brown\nfrom nltk.tag import UnigramTagger\n\nfd = nltk.FreqDist(brown.words(categories='news'))\ncfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\nmost_freq_words = fd.most_common(100)\nlikely_tags = dict((word, cfd[word].max()) for (word, tag) in most_freq_words)\n\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\n\ntokenised_barack = word_tokenize(barack)\n\nunigram_tagger = nltk.UnigramTagger(model=likely_tags)\ntagged_barack = unigram_tagger.tag(tokenised_barack)\n# print(tagged_barack)\n# unigram_tagger.evaluate([pos_list])","2606cfb5":"from nltk import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import brown\nfrom nltk.tag import UnigramTagger\n\nbrown_tagged_sents = brown.tagged_sents(categories='news')\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008).\"\"\"\n\ntokenised_barack = word_tokenize(barack)\n\nprint(brown_tagged_sents)\n\nunigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\ntagged_barack = unigram_tagger.tag(tokenised_barack)\n# print(tagged_barack)\n# print(unigram_tagger.evaluate([pos_list]))","75617ae7":"# training_list = ","233aebec":"#n-gram tagging\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import brown\nfrom nltk.tag import BigramTagger\n\nbrown_tagged_sents = brown.tagged_sents(categories='news')\n\nbarack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\nwho served as the 44th President of \nthe United States from January 20, 2009, to January 20, 2017.\nA member of the Democratic Party, he was the \nfirst African American to assume the presidency and previously\nserved as a United States Senator from Illinois (2005\u20132008). President of the United States Donald John Trump (born June 14, 1946) succeeded him\"\"\"\n\ntrump =\"\"\"President of the United States Donald John Trump (born June 14, 1946) is the 45th President of the United States. \nBefore entering politics, \nhe was a businessman and television personality.\"\"\"\n\ntrump_pos_list = pos_tag(word_tokenize(trump))\n\n# print(trump_pos_list)\n\ntokenised_barack = word_tokenize(barack)\nbigrams = ngrams(tokenised_barack,2)\nbigram_tagger = nltk.NgramTagger(n=3,train=[pos_list])\ntagged_trump = bigram_tagger.tag(word_tokenize(barack))\nprint(tagged_trump)\nprint(bigram_tagger.evaluate([trump_pos_list]))","29698cdd":"# pos_list","86512f5f":"words = [w for w,_ in brown.tagged_words(categories=\"news\")]","5b3826db":"common_words=[]\nfor word in word_tokenize(barack):\n    if word in words:\n        common_words.append(word)\n# common_words","438569d3":"'Illinois' in words","13e659e2":"unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\ntagged_barack = unigram_tagger.tag(tokenised_barack)\n# print(tagged_barack)\nunigram_tagger.evaluate([pos_list])","2ee3e5b5":"# pos_list","03eaec00":"words.index(\"United\")","94b9b5f9":"words[7155],words[7156],words[7157],words[7158]","706c2f68":"from nltk.corpus import wordnet as wn","af595f48":"wn.synset('dog.n.01').hypernyms()","8e00a754":"wn.synset('cat.n.01').hypernyms()","06b86741":"wn.synset('animal.n.01').hypernyms()","bf31e7c6":"dog = wn.synset('dog.n.01')\ndog.lemmas()","0ca0e3a4":"dog.hyponyms()","49d07e78":"#path similariy method shortest path in the taxonomy\ncat = wn.synset('cat.n.01')\ncat.path_similarity(dog)\n#dog and cat synset similarity","d137e8ea":"#wu and palmer similarity method based on their Least Common Subsumer and the maximum depth in the taxonomy\ncat.wup_similarity(dog)","87a861a1":"animal = wn.synset('animal.n.01')\nanimal.path_similarity(wn.synset('bird.n.01'))","425e02a6":"animal.wup_similarity(wn.synset('bird.n.01'))","950bb7a0":"#access to all synsets\n# list(wn.all_synsets('n'))","0e0494ee":"#MORPHY\nwn.morphy(\"working\" , wn.VERB)","a42def55":"wn.morphy(\"denied\" , wn.VERB)","7d039ff3":"wn.morphy(\"abaci\")","12152800":"#synonyms\nsynonyms = []\nfor syn in wn.synsets(\"good\"):\n    for word in syn.lemmas():\n        if word.name() != \"good\":\n            synonyms.append(word.name())\n#print(synonyms)","432b08e7":"#antonyms\nantonyms = []\nfor syn in wn.synsets(\"good\"):\n    for word in syn.lemmas():\n        if word.name() != \"good\" and word.antonyms() :\n            antonyms.append( word.antonyms()[0].name())\n# print(antonyms)","44cb489f":"from nltk.sentiment import SentimentIntensityAnalyzer\nsa = SentimentIntensityAnalyzer()\nprint(sa.polarity_scores(\"very bad\"))","9aa0d3a2":"import pandas as pd \nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","84e4cb4d":"sms_data = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')","40fc6641":"sms_data.head()","500b9e74":"sms_data.columns","8f27508b":"cols = sms_data.columns[:2]","297073a6":"data = sms_data[cols]\ndata.shape","d13d62bd":"data = data.rename(columns={\"v1\":\"Value\",\"v2\":\"Text\"})","15b55c6c":"data.head()","413394cd":"data.Value.value_counts()","09914b0c":"data['Value_num'] = data.Value.map({\"spam\":1,\"ham\":0})","1aef048d":"# X = data[\"\"] ","994ca9f4":"# data[data[\"Value\"]==\"spam\"]","53d673fe":"from string import punctuation\nfrom re import *\nimport nltk\nfrom nltk import word_tokenize\n","6f35e42e":"punctuation = list(punctuation)\n# print(punctuation)","ea06ed90":"#no of punctuations\ndata[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n# len(re.findall(r\"[^\\w+&&^\\s]\",'Go until jurong point, crazy.. Available only ...'))\n","e27aba0a":"#no of punctuations\ndata[\"Phonenumbers\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[0-9]{10}\",x)))\n# print(len(re.findall(r\"[0-9]{10}\",\"9999999999\")))\n","0745a349":"#links\nis_link = lambda x: 1 if re.search(r\"https?:\/\/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\",x)!=None else 0\ndata[\"Links\"] = data[\"Text\"].apply(is_link)\n# print(re.search(r\"(http|https)?(:\/\/)?(www.)?.*(.)[A-Za-z]+\",\"http:\/\/www.a.cm\"))\n# print(is_link(\"http:\/\/hello.c\"))","d7b3b9c5":"\nis_link = lambda x : list(map(str.isupper,x.split())).count(True) \nupper_case = lambda y,n : n+1 if y.isupper() else n\ndata[\"Uppercase\"] = data[\"Text\"].apply(is_link)","8b213625":"def unusual_words(text):\n    text_vocab = set(w.lower() for w in text if w.isalpha())\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n    unusual = text_vocab - english_vocab\n    return len(sorted(unusual))","a2c2da9f":"data[\"unusualwords\"] = data[\"Text\"].apply(lambda x: unusual_words(word_tokenize(x)))","e5ad4b7f":"data[14:25]","62243b08":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf= TfidfVectorizer(stop_words=\"english\",strip_accents='ascii',max_features=300)\ntf_idf_matrix = tf_idf.fit_transform(data[\"Text\"])","345f23da":"data_extra_features = pd.concat([data,pd.DataFrame(tf_idf_matrix.toarray(),columns=tf_idf.get_feature_names())],axis=1)","ffeae749":"X=data_extra_features\nfeatures = X.columns.drop([\"Value\",\"Text\",\"Value_num\"])\ntarget = [\"Value\"]\nX_train,X_test,y_train,y_test = train_test_split(X[features],X[target])","cdb93998":"X_train.shape","993a383f":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\npred = dt.predict(X_test)\nprint(accuracy_score(y_train, dt.predict(X_train)))\nprint(accuracy_score(y_test, pred))\nprint(dt.tree_.node_count)","1e800cfe":"my_submission=pd.DataFrame({'ID': test.Id, 'SalePrice': pred})\nmy_submission.to_csv('submission.csv',index=False)","1859d75b":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train,y_train)\npred_mnb = mnb.predict(X_test)\n\nprint(accuracy_score(y_test, pred_mnb))","4ba68281":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\npred_lr = lr.predict(X_test)\n\nprint(accuracy_score(y_test, pred_lr))","f45c7672":"X = data[\"Text\"]\ny =  data[\"Value_num\"]","e95eb170":"#vectorize the train and test matrices for label Text\ncnt_vect = CountVectorizer()\nX_cnt_vect = cnt_vect.fit_transform(X)\n","bf890350":"#bag of words\n# print(cnt_vect.get_feature_names()[:20])\n# print(cnt_vect.get_feature_names()[-20:])\nprint(X_cnt_vect[:5,:1000])","92bd38c2":"tfidf_vect = TfidfVectorizer(strip_accents='unicode', stop_words=\"english\",)\nX_tfidf_vect = tfidf_vect.fit_transform(X)","cf2a287e":"from nltk.stem import SnowballStemmer\nimport re","3a0213f6":"l = tfidf_vect.get_feature_names()\nfeature_names = tfidf_vect.get_feature_names()\nstemmed_list= []\nstemObj = SnowballStemmer(\"english\")\nfor i in l:\n    if re.search(r\".*[A-Za-z].*\",i):\n        stemmed_list.append(stemObj.stem(i))\nlist(set(stemmed_list))[-20:]\n# print(X_tfidf_vect[:5,:1000])\n# print(len(feature_names))","20d80e94":"#  X_tfidf_vect.toarray()","a946479d":"print(X_cnt_vect.shape)\nprint(X_tfidf_vect.shape)\nprint(y.shape)","dcbd6ffc":"#create train X\nX_train,X_test,y_train,y_test = train_test_split(X_cnt_vect,y, test_size = 0.25,random_state=42)","c8aed06c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","9f414119":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)","bbee9900":"pred_cnt_vect = dt.predict(X_test)\n# print(pred_cnt_vect)","21b85897":"accuracy_score(y_test, pred_cnt_vect)","b05bd642":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train,y_train)\npred_mnb_tfidf = mnb.predict(X_test)\n# print(pred_mnb_tfidf)\naccuracy_score(y_test, pred_mnb_tfidf)","55709a2a":"train_toks = pd.DataFrame(X_train.toarray(),columns=tfidf_vect.get_feature_names())\n# feature_set = pd.DataFrame(X_train.toarray(),columns=tfidf_vect.get_feature_names())\n# def maxent_classifier_method(algo):\n#      mxclassifier = MaxentClassifier.train(train_toks,algorithm=algo, trace=0, max_iter=1000)\n# maxent_classifier_method(\"GIS\")\n# train_toks\n# dict(tfidf_vect.get_feature_names(),X_train.toarray())\nfeature_set = train_toks.to_dict()","d8a89487":"print(feature_set)","36624d13":"X_train,X_test,y_train,y_test = train_test_split(X_tfidf_vect,y, test_size = 0.25,random_state=42)","6b060a4f":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\n\npred_tfidf_vect = dt.predict(X_test)\n# print(pred_cnt_vect)\n\naccuracy_score(y_test, pred_tfidf_vect)","f9f72b1b":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train,y_train)\npred_mnb_tfidf = mnb.predict(X_test)\n# print(pred_mnb_tfidf)\naccuracy_score(y_test, pred_mnb_tfidf)","114ea047":"pred_mnb_tfidf = mnb.predict(X_test)\n# print(pred_mnb_tfidf)","b136d4e8":"accuracy_score(y_test, pred_mnb_tfidf)","93731d11":"print(X_train.shape)\nprint(y_train.shape)","0af1cf62":"from nltk.classify import MaxentClassifier","17e68fa1":"def maxent_classifier_method(algo):\n    mxclassifier = MaxentClassifier.train(data,algorithm=algo, trace=0, max_iter=1000)\nmaxent_classifier_method(\"GIS\")","3a8c1335":"# X_test.shape\n# for i in X_test:\n#     print(i[0])\nX_test[0:5,0:1000]","244d24d0":"import nltk\nfrom nltk.chat.util import Chat, reflections\nfrom sys import version_info\nfrom string import punctuation \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\npairs = [\n    [\n        r\"How can I avail internet reservation facility through credit cards?\",\n        ['Recently internet reservation facility has started on Indian Railways. The web site http:\/\/www.irctc.co.in is operational, wherein you can get the railway reservation done through Credit Cards.For more on Reservation through credit cards click here  Internet Reservation',]\n    ],\n    [\n        r'Why are PNR and reservation availability queries not available after certain timings at night?',\n        ['The online PNR and seat availability queries are fetched from the computerized reservation applications. These online reservation applications are shut down daily around 2330 hrs to 0030 hrs IST. Due to the dynamic changes taking place in the PNR status updation and the availability positions, these two types of queries have to be fetched from the online reservation applications, hence the non- availability of them after certain timings. The sheer size of these databases does not allow them to be copied over network lines.Please note that the web site is functional 24 hrs. a day and other queries (trains between any two stations, fare queries, etc.) are functional throughout the day.',]\n    ],\n    [\n    r'How can I avail the enquiries, through SMS on mobile phones?',\n    ['Now all the enquiries offered on the web site www.indianrail.gov.in are available on your mobile phone through SMS facility. For more information on the mobile service providers and the key words to be used on the mobile, please click here, SMS help . Please note that we are giving the backend service only for the SMS queries. For more information and help on key words and SMS facility, kindly contact the mobile service provider according to the table.',]\n    ],\n    [\n    r'Why do sometimes the fonts, colors schemes and java scripts behave differently in some browser or browsers?',\n    ['This web site is best viewed with Microsoft Internet Explorer 6.0 and above. It might not give desired results with other browsers. All the pages, color schemes and scripts have been tested for IE 6.0 and above. ',]\n\n    ],\n    [\n    r'Where can I get the latest arrival and departure timings of trains, when they get delayed?',\n    ['The latest arrival and departure timings of delayed trains, alongwith diverted routes etc. will be made available shortly on this web site only.',]\n    ],\n    [\n    r'Where can I lodge complaint against any type of grievances in the Trains, Platforms, officials for problems on this web site and give suggestions?',\n    ['The complaint software is presently under development. We try our best to forward your grievances to the concerned department. However please note that this is not always possible. Please note that all your complaints and suggestions for the improvement of the web site http:\/\/www.indianrail.gov.in  can be put on the Feedback & suggestions page. Please note that, in case of any problems, give the query type (hyper link), the inputs which you gave, and the exact error message generated by this web site. All this will help us in solving the problems quickly. In the absence of such inputs, we cannot solve the problems.',]\n    ],\n\n]\ndef unique(list1):\n    # intilize a null list\n    unique_list = []\n    # traverse for all elements\n    for x in list1:\n        # check if exists in unique_list or not\n        if x not in unique_list:\n            unique_list.append(x)\n    return(unique_list)\nlemmatiser = WordNetLemmatizer()\ndef preprocessing (sent) :\n    rem_words = ['get', 'avail', 'who' , 'where', 'how' , 'what', 'why' , 'when', 'I', 'can']\n    ##print(sent)\n    # remove punctuation\n    # convert to lower\n    for p in list(punctuation):\n        sent=sent.replace(p,'')\n    sent=sent.lower().split()\n    #remove stop words \n    stop_words = set(stopwords.words('english'))\n    sent = [w for w in sent if not w in stop_words]\n    sent = [w for w in sent if not w in rem_words]\n    # lemmitise \n    #[item.upper() for item in mylis]\n    sent = [lemmatiser.lemmatize(item, pos=\"v\") for item  in sent ]\n    return(unique(sent)) \n\n\ndef tellme_bot():\n    ##print(\"Hi how can I help you today?\")\n    while(1):\n        #print('\\n' *10 )\n        response = input(\"Tell Me. [q to quit]>\")\n        if response == 'q':\n            break\n        i=0\n        chosen = len(pairs) \n        matches = 0\n        list_response=preprocessing(response) \n        #print(list_response)\n        ##print(list_response)\n        while ( i < len(pairs) ):\n            #print(\"The idx is : \" + str(i) )\n            #print('----------------------------------------------------')\n            loc_matches = 0 \n            #x=pairs[i][0]\n            x=pairs[i][0] + \"  \".join(pairs[i][1])\n            #y=x[0]\n            #print(x)\n            list_pair=preprocessing(x)\n            #print(list_pair)\n            for word in list_pair:\n                if word in list_response:\n                    #print (word+ ' is a  word in pairs')\n                    loc_matches=loc_matches+1\n            #print('loc matches :'+ str(i) + \" \" +str(loc_matches) ) \n            if ( loc_matches > matches ):\n                chosen = i \n                matches = loc_matches\n            i = i + 1 \n        if ( chosen <len(pairs) ) :\n            ans=pairs[chosen][1]\n            print(ans[0] ) \n        else :\n            print(\"Unable to answer this question\" ) \n        break\ntellme_bot()","b93f53ac":"#10 Ways Sugar Harms Your Health\n# https:\/\/www.atkins.com\/how-it-works\/library\/articles\/10-ways-sugar-harms-your-health\n\ndocs1=\"Sugar causes blood glucose to spike and plummet. Unstable blood sugar often leads to mood swings, fatigue, headaches and cravings for more sugar. Cravings set the stage for a cycle of addiction in which every new hit of sugar makes you feel better temporarily but, a few hours later, results in more cravings and hunger. On the flip side, those who avoid sugar often report having little or no cravings for sugary things and feeling emotionally balanced and energized.\"\ndocs2=\"Sugar increases the risk of obesity, diabetes and heart disease. Large-scale studies have shown that the more high-glycemic foods (those that quickly affect blood sugar), including foods containing sugar, a person consumes, the higher his risk for becoming obese and for developing diabetes and heart disease1. Emerging research is also suggesting connections between high-glycemic diets and many different forms of cancer.\"\ndocs3=\"Sugar interferes with immune function. Research on human subjects is scant, but animal studies have shown that sugar suppresses immune response5. More research is needed to understand the exact mechanisms; however, we do know that bacteria and yeast feed on sugar and that, when these organisms get out of balance in the body, infections and illness are more likely.\"\ndocs4=\"A high-sugar diet often results in chromium deficiency. Its sort of a catch-22. If you consume a lot of sugar and other refined carbohydrates, you probably dont get enough of the trace mineral chromium, and one of chromiums main functions is to help regulate blood sugar. Scientists estimate that 90 percent of Americans dont get enough chromium. Chromium is found in a variety of animal foods, seafood and plant foods. Refining starches and other carbohydrates rob these foods of their chromium supplies.\"\ndocs5=\"Sugar accelerates aging. It even contributes to that telltale sign of aging: sagging skin. Some of the sugar you consume, after hitting your bloodstream, ends up attaching itself to proteins, in a process called glycation. These new molecular structures contribute to the loss of elasticity found in aging body tissues, from your skin to your organs and arteries7. The more sugar circulating in your blood, the faster this damage takes hold.\"\ndocs6=\"Sugar causes tooth decay. With all the other life-threatening effects of sugar, we sometimes forget the most basic damage it does. When it sits on your teeth, it creates decay more efficiently than any other food substance8. For a strong visual reminder, next time the Tooth Fairy visits, try the old tooth-in-a-glass-of-Coke experiment\u2014the results will surely convince you that sugar isnt good for your pearly whites.\"\ndocs7=\"Sugar can cause gum disease, which can lead to heart disease. Increasing evidence shows that chronic infections, such as those that result from periodontal problems, play a role in the development of coronary artery disease9. The most popular theory is that the connection is related to widespread effects from the bodys inflammatory response to infection.\"\ndocs7=\"Sugar affects behavior and cognition in children. Though it has been confirmed by millions of parents, most researchers have not been able to show the effect of sugar on childrens behavior. A possible problem with the research is that most of it compared the effects of a sugar-sweetened drink to one containing an artificial sweetener10. It may be that kids react to both real sugar and sugar substitutes, therefore showing no differences in behavior. What about kids ability to learn? Between 1979 and 1983, 803 New York City public schools reduced the amount of sucrose (table sugar) and eliminated artificial colors, flavors and two preservatives from school lunches and breakfasts. The diet policy changes were followed by a 15.7 percent increase in a national academic ranking (previously, the greatest improvement ever seen had been 1.7 percent).\"\ndocs8=\"Sugar increases stress. When were under stress, our stress hormone levels rise; these chemicals are the bodys fight-or-flight emergency crew, sent out to prepare the body for an attack or an escape. These chemicals are also called into action when blood sugar is low. For example, after a blood-sugar spike (say, from eating a piece of birthday cake), theres a compensatory dive, which causes the body to release stress hormones such as adrenaline, epinephrine and cortisol. One of the main things these hormones do is raise blood sugar, providing the body with a quick energy boost. The problem is, these helpful hormones can make us feel anxious, irritable and shaky.\"\ndocs9=\"Sugar takes the place of important nutrients. According to USDA data, people who consume the most sugar have the lowest intakes of essential nutrients\u2013\u2013especially vitamin A, vitamin C, folate, vitamin B-12, calcium, phosphorous, magnesium and iron. Ironically, those who consume the most sugar are children and teenagers, the individuals who need these nutrients most12.\"\ndocs10=\"Slashing Sugar. Now that you know the negative impacts refined sugar can have on your body and mind, youll want to be more careful about the foods you choose. And the first step is getting educated about where sugar lurks\u2014believe it or not, a food neednt even taste all that sweet for it to be loaded with sugar. When it comes to convenience and packaged foods, let the ingredients label be your guide, and be aware that just because something boasts that it is low in carbs or a diet food, doesnt mean its free of sugar. Atkins products never contain added sugar.\"\n\n\n# compile documents\ndoc_complete=[docs1,docs2,docs3, docs4,docs5,docs6,docs7,docs8,docs9,docs10, ]\n#print(doc_complete)\n\nfrom nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(doc).split() for doc in doc_complete] \n#print(doc_clean)\n# Importing Gensim\nimport gensim\nfrom gensim import corpora\n\n# Creating the term dictionary of our courpus, where every unique term is assigned an index. \ndictionary = corpora.Dictionary(doc_clean)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n#print(doc_term_matrix)\n\n# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Trainign LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=300)\n\n#Result\n#print(ldamodel.print_topics(num_topics=5, num_words=5))\ntopics = ldamodel.print_topics(num_topics=5, num_words=5)\nfor  i in topics :\n    print (i) \n    \n","a7393578":"Trimming the text of unwanted spaces that occur at the beginning and end of the text\n\nConvert the text into either lower or uppercase\n\nTokenizing the text and determine its vocabulary\n\n\nRemove stopwords from the text\n\nRemove punctuations\u00a0\n\nNormalize the text using stemming and\/or lemmatization\n\nCreate n-grams from text\n"}}