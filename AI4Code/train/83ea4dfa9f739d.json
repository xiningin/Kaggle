{"cell_type":{"e6c758e4":"code","cd40fecf":"code","feff26e8":"code","9089af33":"code","b9c1e3a7":"code","a6131519":"code","db5dbe82":"code","65bfb74b":"code","64342b82":"code","b1fe79c2":"code","7b8fb04e":"code","38654152":"code","91abe9a9":"code","dafc8fd0":"code","81d8441c":"code","a8a24e6f":"code","1770ad1d":"code","1d930bba":"code","00e9fcc9":"code","2c2294f1":"code","0a632c87":"code","4c6b9331":"code","3d994536":"code","5331abd9":"code","5fc5629f":"code","bf5a8140":"code","e1bcc4a5":"code","4d054be7":"code","abef93cd":"code","3612b1dc":"code","d39d9e97":"markdown","2ae422c6":"markdown"},"source":{"e6c758e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd40fecf":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()","feff26e8":"train.isnull().any()","9089af33":"train.isna().sum()","b9c1e3a7":"train.info()","a6131519":"train.shape","db5dbe82":"train.describe()","65bfb74b":"sns.heatmap(train.isna(), cbar=False, cmap = 'plasma')\n#cbar = color bar shown just right of the heatmap\n#cmap = color map ","64342b82":"sns.set_style('whitegrid')\nsns.countplot(x = 'Survived', data = train)","b1fe79c2":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Sex', data=train,  palette='hls')  ","7b8fb04e":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Pclass', data=train,  palette='hls') ","38654152":"sns.distplot(train['Age'].dropna(),kde=False)","91abe9a9":"sns.countplot(x='SibSp', data=train)","dafc8fd0":"sns.countplot(x='SibSp', hue='Survived', data=train)","81d8441c":"sns.countplot(x='Embarked', hue='Survived', data=train)","a8a24e6f":"sns.countplot(x='Parch', hue='Survived', data=train)","1770ad1d":"sns.distplot(train['Fare'], kde=False)","1d930bba":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","00e9fcc9":"def new_age(row):\n    age, pcl = row[0], row[1]\n    if pd.isnull(age):\n        if pcl == 1:\n            return 37\n        elif pcl == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return age\n#the line in box represents the average age in each Pclass\n#from boxplot we can take average of pclass1,2,3 as 37, 29, 24 respectively\n\npcl = train['Pclass'].unique()\ntrain['Age'] = train[['Age','Pclass']].apply(new_age, axis=1)","2c2294f1":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","0a632c87":"train.dropna(axis=0, subset=['Embarked'], inplace=True)\nembarked = {'S':0, 'C':1, 'Q':2}\ngen = {'male':0, 'female':1}\ntrain['Sex'] = train['Sex'].map(gen)\ntrain['Embarked'] = train['Embarked'].map(embarked)","4c6b9331":"corelation = train.corr().round(3)\ncorelation","3d994536":"plt.figure(figsize=(15,15))\nsns.heatmap(corelation, annot = True)","5331abd9":"X = train[['Pclass','Sex','Age','SibSp','Embarked','Parch']]\ny = train['Survived']","5fc5629f":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y, test_size=0.3, random_state=42)\nX.isnull().any()","bf5a8140":"from sklearn.linear_model import LogisticRegression\n\nmodellr = LogisticRegression()\nmodellr.fit(train_X, train_y)\npred = modellr.predict(val_X)","e1bcc4a5":"from sklearn.metrics import accuracy_score\naccuracy_score(val_y,pred)","4d054be7":"from sklearn import tree\nmodeldt = tree.DecisionTreeClassifier()\nmodeldt.fit(train_X, train_y)\npreddt = modeldt.predict(val_X)\naccuracy_score(val_y,preddt)","abef93cd":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(train_X,train_y).predict(val_X)\naccuracy_score(val_y,y_pred)","3612b1dc":"from sklearn.ensemble import RandomForestClassifier\nestimators = [100,200,500,750,1000]\ndepth = [3,5,7,9]\nfor i in estimators:\n    for j in depth:\n        model = RandomForestClassifier(n_estimators=i, max_depth=j, random_state=42)\n        model.fit(train_X, train_y)\n        predictions = model.predict(val_X)\n        print(i,j,accuracy_score(val_y,predictions))","d39d9e97":"**Too much values are missing in the cabin data. We might drop the column later or change it to another feature.**","2ae422c6":"**n_estimators = 100 and max_depth = 5 giving best accuracy so we can use these parameter values in Random Forest Classifier**"}}