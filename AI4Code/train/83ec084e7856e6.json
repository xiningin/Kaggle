{"cell_type":{"b0e48383":"code","7a9df5af":"code","a99d0eea":"code","cad42320":"code","819bea41":"code","2018d71a":"code","f53683d2":"code","3ebe8d34":"code","1cbd7879":"code","2c563bdd":"code","af8bdceb":"code","798b7a48":"code","8f50798b":"code","80b81d74":"code","980c949b":"code","4f027483":"code","75e11e0e":"code","12b11742":"code","dfe2b305":"markdown","035d093b":"markdown","e8426450":"markdown","f5bcb454":"markdown","07c9c065":"markdown","b66cf16d":"markdown","0b97a339":"markdown","4ee4ea61":"markdown","7eaafc62":"markdown","be3856ad":"markdown"},"source":{"b0e48383":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a9df5af":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,InputLayer, Conv2DTranspose, Dropout, BatchNormalization, Input, Concatenate, Activation, concatenate ,RepeatVector ,Reshape ,UpSampling2D\nfrom keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport cv2\nimport PIL\nfrom skimage import transform\nfrom PIL import Image\nimport random\nimport h5py\nimport os\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n","a99d0eea":"images_gray = np.load(\"..\/input\/image-colorization\/l\/gray_scale.npy\")\nimages_ab1 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab1.npy\")\nimages_ab2 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab2.npy\")\nimages_ab3 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab3.npy\")\n\n","cad42320":"#taking training data of 100\nX_train = (images_gray[:250,:,:].astype('float')).reshape(250,224,224,1) #reshaping the input gray images\nY = (images_ab1[:250,:,:].astype('float'))\nX_test = (images_gray[300:550,:,:].astype('float')).reshape(250,224,224,1)\nY_test = (images_ab1[300:550,:,:].astype('float'))","819bea41":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\ninception = InceptionResNetV2(weights=None, include_top=True)\ninception.load_weights('..\/input\/image-colorization\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninception.graph = tf.get_default_graph()","2018d71a":"#To generate embeddings of 1000*1 by passing input images through InceptionResNetV2\ndef create_inception_embedding(grayscaled_rgb):\n    grayscaled_rgb_resized = []\n    for i in grayscaled_rgb:\n        i = resize(i, (299, 299, 3), mode='constant')\n        grayscaled_rgb_resized.append(i)\n    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb_resized)\n    return embed\nimport tensorflow as tf","f53683d2":"#creating embeddings for Train data\nincept_em = create_inception_embedding(X_train)\nembeddings = RepeatVector(28 * 28)(incept_em)\nlayer_embedding_train = Reshape(([28, 28, 1000]))(embeddings)","3ebe8d34":"print(layer_embedding_train.shape)","1cbd7879":"model_path = \".\/color_model.h5\"\ncheckpoint = ModelCheckpoint(model_path,\n                            monitor = \"val_loss\",\n                            mode=\"min\",\n                            save_best_only = True,\n                            verbose = 1)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","2c563bdd":"#Encoder\nembed_input = Input(shape=(28, 28, 1000))\nencoder_input = Input(shape=(224, 224, 1,))\nencoder_1 = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\nencoder_2 = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_1)\nencoder_3 = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_2)\nencoder_4 = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_3)\nencoder_5 = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_4)\nencoder_6 = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_5)\nencoder_7 = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_6)\nencoder_output= Conv2D(256, (3,3), activation='relu', padding='same')(encoder_7)\n#Fusion layer\nfusion_output = concatenate([encoder_output, embed_input], axis=3) \nfusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\nfusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n#Decoder layer\ndecoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(2, (3, 3), activation='relu', padding='same')(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\nmodel = Model(inputs=[encoder_input,embed_input], outputs=decoder_output)\nmodel.summary()","af8bdceb":"model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\nhistory = model.fit(x=[X_train,layer_embedding_train] ,y=Y,callbacks = [checkpoint,es], batch_size=5, epochs=2500,steps_per_epoch=1)","798b7a48":"plt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n","8f50798b":"plt.plot(history.history['acc'])\nplt.title('Training Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.show()\n","80b81d74":"# creating embeddings for test data\nincept_em = create_inception_embedding(X_test)\nembeddings = RepeatVector(28 * 28)(incept_em)\nlayer_embedding_test = Reshape(([28, 28, 1000]))(embeddings)","980c949b":"loss, acc = model.evaluate([X_test,layer_embedding_test] ,Y_test,steps=3)\nprint()\nprint(\"Test accuracy = \", acc)","4f027483":"output = model.predict([X_test,layer_embedding_test],steps=3)\n","75e11e0e":"def get_LAB(image_l, image_ab  ):\n       \n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n \n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb\ndef get_LAB1(image_l  ):\n    image_ab =  np.ones((224,224,2))*128\n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n \n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb","12b11742":"for i in range(50,100):\n    pred = get_LAB(X_test[i],output[i])\n    real = get_LAB(X_test[i],Y_test[i])\n    original = get_LAB1(X_test[i])\n    f, axarr = plt.subplots(1,3)\n    axarr[0].title.set_text('Black and white')  \n    axarr[1].title.set_text('Prediction')  \n    axarr[2].title.set_text('original')  \n    axarr[0].imshow(original)\n    axarr[1].imshow(pred)\n    axarr[2].imshow(real)\n    \n    \n    ","dfe2b305":"# Resulting Images ","035d093b":"# Creating samples for training and testing  ","e8426450":"# Training.....","f5bcb454":"> > # THE MODEL","07c9c065":"# The prediction (results)","b66cf16d":"# Loading pretrained Model ..... Inception-ResNet-v2","0b97a339":"# Accuracy on training Data","4ee4ea61":"# Converting LAB to RGB for the predicted Images ","7eaafc62":"#  Loading the Data......","be3856ad":"# Loss"}}