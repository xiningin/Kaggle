{"cell_type":{"961e6307":"code","7856efe3":"code","e42b47a1":"code","53cac150":"code","58bc7395":"code","af210e8e":"code","2d2644cc":"code","af5a4b42":"code","018e3a5c":"code","ea493dd8":"code","02f9f032":"code","0e7656ca":"code","6bbf4b32":"code","feaa216c":"code","dbc07193":"code","67cea61e":"code","42b8b9f2":"code","f7ea5df4":"code","9719766f":"markdown","8ffb4e16":"markdown","5c0a3de8":"markdown","6b33ca56":"markdown","935c14f9":"markdown","09bfda9e":"markdown"},"source":{"961e6307":"import numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import shuffle","7856efe3":"def plot_decision_boundary(X, model):\n    h = .02 \n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                       np.arange(y_min, y_max, h))\n\n\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)","e42b47a1":"def plot_decision_boundary_for_two_datasets(model_dataset_1, model_dataset_2):\n    fig = plt.figure(figsize=(12,4))\n    plt.subplot(1, 2, 1)\n    plt.scatter(X1[:,0], X1[:,1], s=100, c=Y1, alpha=0.5)\n    plot_decision_boundary(X1, model_dataset_1)\n    plt.title('Dataset 1')\n    plt.subplot(1, 2, 2)\n    plt.scatter(X2[:,0], X2[:,1], s=100, c=Y2, alpha=0.5)\n    plot_decision_boundary(X2, model_dataset_2)\n    plt.title('Dataset 2')","53cac150":"def create_dataset_1():\n    np.random.seed(10)\n\n    N = 500\n    D = 2\n    X = np.random.randn(N, D)\n\n    delta = 1.75\n    X[:125] += np.array([delta, delta])\n    X[125:250] += np.array([delta, -delta])\n    X[250:375] += np.array([-delta, delta])\n    X[375:] += np.array([-delta, -delta])\n    Y = np.array([0] * 125 + [1]*125 + [1]*125 + [0] * 125)\n    \n    return X, Y","58bc7395":"def create_dataset_2():\n    np.random.seed(10)\n\n    N = 500\n    D = 2\n    X = np.random.randn(N, D)\n\n    R_smaller = 5\n    R_larger = 10\n\n    R1 = np.random.randn(N\/\/2) + R_smaller\n    theta = 2 * np.pi * np.random.random(N\/\/2)\n    X[:250] = np.concatenate([[R1 * np.cos(theta)], [R1*np.sin(theta)]]).T\n\n\n    R2 = np.random.randn(N\/\/2) + R_larger\n    theta = 2 * np.pi * np.random.random(N\/\/2)\n    X[250:] = np.concatenate([[R2 * np.cos(theta)], [R2*np.sin(theta)]]).T\n\n    Y = np.array([0] * (N\/\/2) + [1] * (N\/\/2))\n    \n    return X, Y","af210e8e":"X1, Y1 = create_dataset_1()\nX2, Y2 = create_dataset_2()","2d2644cc":"fig = plt.figure(figsize=(12,4))\nax1=plt.subplot(1, 2, 1)\nplt.scatter(X1[:,0], X1[:,1], s=100, c=Y1, alpha=0.5)\nax2=plt.subplot(1, 2, 2)\nplt.scatter(X2[:,0], X2[:,1], s=100, c=Y2, alpha=0.5)","af5a4b42":"dt_ds1 = DecisionTreeClassifier()\ndt_ds1.fit(X1, Y1)\nprint(\"Score for Dataset 1:\", dt_ds1.score(X1, Y1))\ndt_ds2 = DecisionTreeClassifier()\ndt_ds2.fit(X2, Y2)\nprint(\"Score for Dataset 2\", dt_ds2.score(X2, Y2))","018e3a5c":"plot_decision_boundary_for_two_datasets(dt_ds1, dt_ds2)","ea493dd8":"dt3_ds1 = DecisionTreeClassifier(criterion='entropy', max_depth=3)\ndt3_ds1.fit(X1, Y1)\nprint(\"Score for Dataset 1:\", dt3_ds1.score(X1, Y1))\ndt3_ds2 = DecisionTreeClassifier(criterion='entropy', max_depth=3)\ndt3_ds2.fit(X2, Y2)\nprint(\"Score for Dataset 2\", dt3_ds2.score(X2, Y2))","02f9f032":"plot_decision_boundary_for_two_datasets(dt3_ds1, dt3_ds2)","0e7656ca":"dt5_ds1 = DecisionTreeClassifier(criterion='entropy', max_depth=5)\ndt5_ds1.fit(X1, Y1)\nprint(\"Score for Dataset 1:\", dt5_ds1.score(X1, Y1))\ndt5_ds2 = DecisionTreeClassifier(criterion='entropy', max_depth=5)\ndt5_ds2.fit(X2, Y2)\nprint(\"Score for Dataset 2\", dt5_ds2.score(X2, Y2))","6bbf4b32":"plot_decision_boundary_for_two_datasets(dt5_ds1, dt5_ds2)","feaa216c":"logreg_ds1 = LogisticRegression()\nlogreg_ds1.fit(X1, Y1)\nprint(\"Score for Dataset 1:\", logreg_ds1.score(X1, Y1))\nlogreg_ds2 = LogisticRegression()\nlogreg_ds2.fit(X2, Y2)\nprint(\"Score for Dataset 2\", logreg_ds2.score(X2, Y2))","dbc07193":"plot_decision_boundary_for_two_datasets(logreg_ds1, logreg_ds2)","67cea61e":"class BaggedTreeClassifier:\n    def __init__(self, M):\n        self.M = M\n        \n    def fit(self, X, Y):\n        N = len(X)\n        self.models = []\n        for m in range(self.M):\n            idx = np.random.choice(N, size=N, replace=True)\n            Xb, Yb = X[idx], Y[idx]\n            model = DecisionTreeClassifier(max_depth=5)\n            model.fit(Xb, Yb)\n            self.models.append(model)\n    \n    def predict(self,X):\n        predictions = np.zeros(len(X))\n        for model in self.models:\n            predictions += model.predict(X)\n        return np.round(predictions \/ self.M)\n    \n    def score(self,X,Y):\n        result = self.predict(X)\n        return np.mean(result == Y)","42b8b9f2":"btree_ds1 = BaggedTreeClassifier(M=200)\nbtree_ds1.fit(X1, Y1)\nprint(\"Score for Dataset 1:\", btree_ds1.score(X1, Y1))\nbtree_ds2 = BaggedTreeClassifier(M=200)\nbtree_ds2.fit(X2, Y2)\nprint(\"Score for Dataset 2\", btree_ds2.score(X2, Y2))","f7ea5df4":"plot_decision_boundary_for_two_datasets(btree_ds1, btree_ds2)","9719766f":"# Decision Tree 3 Max Depth","8ffb4e16":"# Logistic Regression","5c0a3de8":"# Decision Tree","6b33ca56":"# Bagged Decision Tree","935c14f9":"# Decision Tree 5 Max Depth","09bfda9e":"# Create Dataset"}}