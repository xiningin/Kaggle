{"cell_type":{"5f352950":"code","3fbb3804":"code","ed7eab34":"code","8bc96885":"code","d54ce5e9":"code","8b1b3a97":"code","c14a2b46":"code","3cfd2dba":"code","e44715de":"code","b7ceface":"code","95ab4142":"code","888dbe02":"code","c033d290":"code","c7848500":"code","249d3a72":"code","adaa72de":"code","fa12e0e8":"code","d8c251f6":"code","210ebdd7":"code","0e51c59e":"code","ae2667ae":"code","c12b52fa":"code","ec5c8425":"code","42a987df":"code","3ed50a8c":"code","1d755e87":"code","2e78e2c4":"code","ac08cd01":"code","2de9c6a3":"code","713603f4":"code","6a777252":"code","3b9a6a78":"code","48ba8493":"code","ed14e134":"code","298c5e7a":"code","01933553":"code","aa9cd1c0":"code","4284ed16":"code","559d28b4":"code","2a918462":"code","3f4181f2":"code","463fc12b":"code","7edfe49e":"code","cbb22d81":"code","a48ae119":"code","eb3e045a":"code","38a8879b":"code","4b38434e":"code","8d9cd820":"code","207823fc":"code","e1be0737":"code","7d0a7ece":"code","66b54925":"code","d4c9e45f":"code","12e8efda":"code","d5128c98":"code","21d506ea":"code","5c71e06f":"code","53cc06de":"markdown","ba296d09":"markdown","87bc64f0":"markdown","1342180c":"markdown","f9223ddd":"markdown","6e99e25d":"markdown","b9f37980":"markdown","ca68ff66":"markdown","e9b2c573":"markdown","a6d79b88":"markdown","d6bd5682":"markdown","ffec2d73":"markdown","2c77f0a4":"markdown","d2876aff":"markdown","3ab76180":"markdown","d96164d5":"markdown","b92093d5":"markdown","fc0f13e6":"markdown","bf395468":"markdown","e904ea88":"markdown","75b0ae68":"markdown","bea2ebad":"markdown","7d97196f":"markdown","b4f6433f":"markdown","12a4f5b3":"markdown","7ac05d25":"markdown","671fd653":"markdown","25b5c4a8":"markdown","50bf5fb7":"markdown","277f9813":"markdown","977b51fc":"markdown","a72f366f":"markdown","b0b05446":"markdown","28a9d9f0":"markdown","8c176ba6":"markdown","a6353203":"markdown"},"source":{"5f352950":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport tensorflow as tf\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3fbb3804":"DATA_PATH = '\/kaggle\/input\/nlp-getting-started\/'\nSEED = 42\nDROPOUT = 0.5\nEPOCHS = 3\nLEARN_RATE = 1e-4\nSPLIT = 0.2\nBATCH_SIZE = 32","ed7eab34":"train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nds = ['train', 'test', 'sample submission']\nprint(\"Training set has {} rows and {} columns.\".format(train.shape[0], train.shape[1]))\nprint(\"Test set has {} rows and {} columns.\".format(test.shape[0], test.shape[1]))\n\nprint()\nprint(train.columns)\nprint(test.columns)","8bc96885":"train[train.target == 0].head()","d54ce5e9":"train.head()","8b1b3a97":"print('Train Keyword Distribution:\\n\\n')\nprint(train.keyword.value_counts())\nprint('\\n', '-' * 50, '\\n')\nprint('Test Keyword Distribution:\\n\\n')\nprint(test.keyword.value_counts())","c14a2b46":"temp = train['target'].value_counts(dropna = False).reset_index()\ntemp.columns = ['target', 'counts']\n\ncountplt = sns.countplot(x = 'target', data = train, hue = train['target'])\ncountplt.set_xticklabels(['0: Not Disaster (4342)', '1: Disaster (3271)'])","3cfd2dba":"train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain.drop(columns=['target_mean'], inplace=True)","e44715de":"print('Count NaN:')\nprint(train.isnull().sum(), '\\n')\nprint('Percentage NaN:')\nprint(train.isnull().sum()\/ len(train))","b7ceface":"print('Count NaN:')\nprint(test.isnull().sum(), '\\n')\nprint('Percentage NaN:')\nprint(test.isnull().sum()\/ len(test))","95ab4142":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len = train[train['target'] == 0]['text'].str.len()\nax1.hist(train_len,color='blue')\nax1.set_title('Not A Disaster')\ntrain_len = train[train['target'] == 1]['text'].str.len()\nax2.hist(train_len,color='red')\nax2.set_title('Disaster')\nfig.suptitle('Characters in Train Set\\'s Text')\nplt.show()","888dbe02":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntrain_len = train[train['target'] == 0]['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='blue')\nax1.set_title('Not A Disaster')\ntrain_len = train[train['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='red')\nax2.set_title('Disaster')\nfig.suptitle('Words in Train Set\\'s Text')\nplt.show()","c033d290":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain.loc[train['id'].isin(ids_with_target_error),'target'] = 0","c7848500":"train.loc[train['keyword'].notnull(), 'text'] = train['keyword'] + ' ' + train['text']\ntest.loc[test['keyword'].notnull(), 'text'] = test['keyword'] + ' ' + test['text']\n\n# view\ntrain[train['keyword'].notnull()].head()","249d3a72":"train = train.drop(['id', 'keyword', 'location'], axis=1)\ntest = test.drop(['keyword', 'location'], axis=1) # keep id\n\ntrain.head()","adaa72de":"import string\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n# NLTK Tweet Tokenizer for now\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(strip_handles=True)\n\ncorpus = []\n\n# clean up text\ndef clean_text(text):\n    \"\"\"\n    Copied from other notebooks\n    \"\"\"\n    # expand acronyms\n    \n    # special characters\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    \n    # emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    \n    \"\"\"\n    Our Stuff\n    \"\"\"\n    # remove numbers\n    text = re.sub(r'[0-9]', '', text)\n    \n    # remove punctuation and special chars (keep '!')\n    for p in string.punctuation.replace('!', ''):\n        text = text.replace(p, '')\n        \n    # remove urls\n    text = re.sub(r'http\\S+', '', text)\n    \n    # tokenize\n    text = tknzr.tokenize(text)\n    \n    # remove stopwords\n    text = [w.lower() for w in text if not w in stop_words]\n    corpus.append(text)\n    \n    # join back\n    text = ' '.join(text)\n    \n    return text","fa12e0e8":"%%time\ntrain['text'] = train['text'].apply(lambda s: clean_text(s))\ntest['text'] = test['text'].apply(lambda s: clean_text(s))\n\n# see some cleaned data\ntrain.sample(10)","d8c251f6":"texts = train['text'].to_numpy()\nword_freq = {}\n\nfor text in texts:\n    for word in text.split():\n        word_freq[word] = word_freq.get(word, 0) + 1","210ebdd7":"# # remove words occuring < 3 times\n# freq_threshold = 3\n# for i, text in enumerate(texts):\n#     for word in text.split():\n#         if word_freq[word] < freq_threshold:\n#             print(word)\n#             texts[i].replace(word, '')","0e51c59e":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 40\n\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nnum_words = len(word_index) + 1\nprint('Found %s unique tokens.' % (num_words - 1))\n\n# pad \ndata = pad_sequences(\n    sequences, \n    maxlen=MAX_SEQUENCE_LENGTH,\n    padding='post', \n    truncating='post'\n)\n\nlabels = train['target'].to_numpy()\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","ae2667ae":"x_train = data\ny_train = labels","c12b52fa":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n\nvectorizer = CountVectorizer()\nx_train_vectorized = vectorizer.fit_transform(train['text'])\n\n# print vocabulary\nprint(vectorizer.get_feature_names()[2500:2600])","ec5c8425":"y_train_NB = np.array(train['target'])","42a987df":"# alpha is smoothing param\nmodel_NB = BernoulliNB(alpha=1.0)\nmodel_NB.fit(x_train_vectorized, y_train_NB)\n\n# prepare test\nx_test_NB = vectorizer.transform(test['text'])","3ed50a8c":"# read fasttext twitter embeddings\nembeddings_df = pd.read_pickle('\/kaggle\/input\/fasttext-twitter-derived-embeddings\/twitter_derived_embeddings')\nembeddings_df.head()","1d755e87":"EMBEDDING_DIM = 400\n\nfasttext_embedding_idx = {}\nfor idx, row in embeddings_df.iterrows():\n    word = row[0]\n    embeddings = np.asarray(row[1], 'float32')\n    fasttext_embedding_idx[word] = embeddings\n\n# print only 20\nfasttext_embedding_idx['earthquake'][:20]","2e78e2c4":"# NOTE: comment out if using fasttext\n# glove_embedding_idx = {}\n# EMBEDDING_DIM = 100\n# with open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.100d.txt','r') as f:\n#     for line in f:\n#         values=line.split()\n#         word=values[0]\n#         vectors=np.asarray(values[1:],'float32')\n#         glove_embedding_idx[word] = vectors\n# f.close()","ac08cd01":"iv = 0\noov = 0\n\nembedding_idx = fasttext_embedding_idx # swap between embeddings\n\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embedding_idx.get(word)\n    if embedding_vector is not None:\n        iv += 1\n        # words not found in the embedding space are all zeros\n        embedding_matrix[i] = embedding_vector\n    else: oov += 1\n        \nprint('%i tokens in vocab, %i tokens out of vocab' % (iv, oov)) # TODO: must reduce out of vocab","2de9c6a3":"# create the embedding layer, this will not be trainable!\nfrom tensorflow.keras.layers import Embedding\n\nmodel_NN = tf.keras.Sequential()\n\nembedding_layer = Embedding(\n    num_words, \n    EMBEDDING_DIM,\n    weights = [embedding_matrix],\n    input_length=MAX_SEQUENCE_LENGTH,\n    trainable=False\n)","713603f4":"model_NN = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'),\n    embedding_layer,\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(DROPOUT),\n    tf.keras.layers.Dense(1, activation='sigmoid') # add sigmoid to get [0,1]\n])\n\nloss = tf.keras.losses.BinaryCrossentropy(\n    from_logits=False, \n    name='binary_crossentropy'\n)\n\noptimizer = tf.keras.optimizers.Adam(LEARN_RATE)\n\nmodel_NN.compile(\n    loss=loss,\n    optimizer=optimizer,\n    metrics=['accuracy']\n)","6a777252":"model_NN.fit(x_train, y_train, epochs=EPOCHS, validation_split=SPLIT, batch_size=BATCH_SIZE)","3b9a6a78":"test_tokenizer = Tokenizer()\ntest_texts = test['text'].to_numpy()\ntest_tokenizer.fit_on_texts(texts)\ntest_sequences = test_tokenizer.texts_to_sequences(test_texts)\n\ntest_word_index = test_tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\ntest_embedding_matrix = np.zeros((len(test_word_index) + 1, EMBEDDING_DIM))\nfor word, i in test_word_index.items():\n    embedding_vector = embedding_idx.get(word)\n    if embedding_vector is not None:\n        # words not found in the embedding space are all zeros\n        test_embedding_matrix[i] = embedding_vector","48ba8493":"raw_lstm_preds = model_NN.predict(test_data)\nlstm_preds = raw_lstm_preds.round().astype(int)","ed14e134":"train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))","298c5e7a":"import tensorflow_hub as hub","01933553":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n    \nimport tokenization","aa9cd1c0":"%%time \nmax_seq_length = 180\nbert_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/1\"\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32)\nbert_inputs = [input_word_ids, input_mask, segment_ids]\n\nbert_layer = hub.KerasLayer(bert_url, trainable=True)\npooled_output, sequence_output = bert_layer(bert_inputs)","4284ed16":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","559d28b4":"# vocab_file is a url, do_lower_case is a tf.Variable bool, and tokenizer is an object\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","2a918462":"x_train_bert = bert_encode(train['text'].values, tokenizer, max_len=max_seq_length)\nx_test_bert = bert_encode(test['text'].values, tokenizer, max_len=max_seq_length)\ny_train_bert = train['target'].values","3f4181f2":"#dropout = tf.keras.layers.Dropout(DROPOUT)(sequence_output[:, 0, :])\ndense = tf.keras.layers.Dense(64, activation='relu')(sequence_output[:, 0, :])\npred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n\n# callbacks, adaptive learning rate\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n\nmodel_NN_bert = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n\noptimizer = tf.keras.optimizers.Adam(LEARN_RATE)\n\nmodel_NN_bert.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel_NN_bert.summary()","463fc12b":"model_NN_bert.fit(\n    x_train_bert,\n    y_train_bert,\n    validation_split=SPLIT,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE\n)","7edfe49e":"bert_preds = model_NN_bert.predict(x_test_bert)","cbb22d81":"# probabilities [0,1]\n# print(model_NB.classes_)\nnb_preds_prob = model_NB.predict_proba(x_test_NB)[:, 1]","a48ae119":"nb_preds = model_NB.predict(x_test_NB)\nsample_submission['target'] = nb_preds\nsample_submission.head()","eb3e045a":"np.allclose(nb_preds, nb_preds_prob.round().astype(int))","38a8879b":"sample_submission.to_csv('submission_NB.csv', index=False)","4b38434e":"sample_submission['target'] = lstm_preds\nsample_submission.head()","8d9cd820":"sample_submission.to_csv('submission_NN.csv', index=False)","207823fc":"sample_submission['target'] = bert_preds.round().astype(int)\nsample_submission.head()","e1be0737":"sample_submission.to_csv('submission_NN_bert.csv', index=False)","7d0a7ece":"nb_preds_prob[:5]","66b54925":"raw_lstm_preds = raw_lstm_preds[:, 0]\nraw_lstm_preds[:5]","d4c9e45f":"bert_preds = bert_preds[:, 0]\nbert_preds[:5]","12e8efda":"ensemble_preds = .5*nb_preds_prob + .5*bert_preds","d5128c98":"ensemble_preds[:5]","21d506ea":"sample_submission['target'] = ensemble_preds.round().astype(int)\nsample_submission.head()","5c71e06f":"sample_submission.to_csv('submission_ensemble.csv', index=False)","53cc06de":"### BERT Usage\nLayer from TensorFlow Hub. [See Here](https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/1) for explanation and tutorial\n","ba296d09":"#### Neural Net Submission","87bc64f0":"**References:**\n* Official tokenization script created by the Google team: https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n* Bert_Encode function from Kaggle's Bert-Starter-Inference: https:\/\/www.kaggle.com\/user123454321\/bert-starter-inference\n* All pre-trained BERT models from Tensorflow Hub: https:\/\/tfhub.dev\/s?q=bert","1342180c":"### Number of Words in `text` - Train Set","f9223ddd":"Train","6e99e25d":"### Target Distribution in Keywords","b9f37980":"### Read and Inspect Data","ca68ff66":"#### Notebook Credit\n* https:\/\/www.kaggle.com\/ratan123\/in-depth-guide-to-google-s-bert\/output\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n* https:\/\/towardsdatascience.com\/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n\n**Note:** Make sure GPU is on!","e9b2c573":"#### Predict","a6d79b88":"### Keywords\nNot including NaNs","d6bd5682":"#### Ensemble Submission (Naive Bayes + LSTM + BERT NN)","ffec2d73":"#### Naive Bayes Submission","2c77f0a4":"#### Remove infrequent words","d2876aff":"### Class Balance \n* (4342 0's and 3271 1's)","3ab76180":"#### Prepend keyword to text - Run only once!","d96164d5":"Disasters","b92093d5":"### Result of Naive Bayes\nSee [Version 8](https:\/\/www.kaggle.com\/grantgasser\/eda-naive-bayes-twitter-embeddings-nn?scriptVersionId=30306784), where NB model scored **.804**","fc0f13e6":"## Data Cleaning and Preparation","bf395468":"### Missing Values (NaNs)","e904ea88":"#### GloVe Twitter embeddings\nFound [here](https:\/\/www.kaggle.com\/jdpaletto\/glove-global-vectors-for-word-representation)","75b0ae68":"Clearly not disasters..","bea2ebad":"### Tokenizing and Removing punctutation\nThoughts:\n   * use some regex code from [here](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\/notebook)\n   * use the Regex Tokenizer","7d97196f":"Build Model","b4f6433f":"Re-read data to nullify pre-processing","12a4f5b3":"### Number of Characters in `text` - Train Set","7ac05d25":"#### BERT Neural Net Submission","671fd653":"## Pre-Trained Embeddings\n\n#### Fasttext embeddings\nThese embeddings were created using a [created](https:\/\/fredericgodin.com\/research\/twitter-word-embeddings\/) by Fr\u00e9deric Godin at Ghent University in Belgium. [This blog post](https:\/\/fredericgodin.com\/research\/twitter-word-embeddings\/) explains the work. The fasttext embedding model is too large to load in to kaggle (~15GB) so we created a specialized embedding data set that only contains words of our vocabulary.","25b5c4a8":"## BERT and TFHub","50bf5fb7":"#### Create embeddings layer","277f9813":"## Naive Bayes (Non-Neural Baseline Method)\nThe goal of this step is to learn about naive bayes and establish a baseline non-neural network model\n\n* Great Naive Bayes [Explanation and Tutorial](https:\/\/towardsdatascience.com\/algorithms-for-text-classification-part-1-naive-bayes-3ff1d116fdd8)\n* Another [Explanation and Tutorial](https:\/\/stackabuse.com\/the-naive-bayes-algorithm-in-python-with-scikit-learn\/)","977b51fc":"## Enter submission","a72f366f":"### Fix mislabelled samples\n* Copied from [here](https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data)\n* My hesitation with this is that if samples are mislabbeled in the training set, the are likely samples in the test set mislabelled...","b0b05446":"### Fit and predict (Naive Bayes)","28a9d9f0":"# Natural Language Processing Project (NU MSAI)\n### Authors: Grant Gasser, Sundar Thevar, Blaine Rothrock, Zhili Wang","8c176ba6":"Create Model: Simple Bi-directional LSTM","a6353203":"#### Train the model"}}