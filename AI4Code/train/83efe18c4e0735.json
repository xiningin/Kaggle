{"cell_type":{"efe515b5":"code","561e26e9":"code","f5847657":"code","7bd12561":"code","3a3d9e86":"code","0e996048":"code","7556ccec":"code","a8428d0b":"code","84b66dbb":"code","e6c74453":"code","2ab268b5":"code","26d59306":"code","aaf21d48":"code","8cb2d201":"code","4162cb74":"code","033722c5":"code","108556ca":"code","2e97e4ee":"code","98522dc5":"code","1c73dfec":"code","e249815a":"code","9baf8224":"code","96b867e1":"code","9c4f485d":"markdown","3d07236e":"markdown","47db0aa7":"markdown","da609560":"markdown","d9f24b25":"markdown"},"source":{"efe515b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.0","561e26e9":"from nltk import word_tokenize\nimport nltk\n\nstop_words = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsample = pd.read_csv('..\/input\/sample_submission.csv')\n\ntrain.head()","f5847657":"train.author.unique()","7bd12561":"test.head()","3a3d9e86":"sample.head()","0e996048":"train.text[:10]","7556ccec":"print(\"Input data has {} rows and {} columns\".format(len(train), \n                                                    len(train.columns)))","a8428d0b":"print(\"Out of {} rows, {} are EAP, {} are HPL, {} are MWS\".format(len(train), \n                                                                 len(train[train[\"author\"]== \"EAP\"]),\n                                                                 len(train[train[\"author\"]==\"HPL\"]), \n                                                                 len(train[train[\"author\"]==\"MWS\"])))","84b66dbb":"train.isnull().sum()","e6c74453":"import string\nimport re\n\ndef cleaned_text(text):\n    text_nopunct = \"\".join([char.lower() for char in text if char not in string.punctuation])\n    tokenized = re.split(\"\\W+\", text_nopunct)\n    stem_text = [ps.stem(word) for word in tokenized if word not in stop_words]\n    return stem_text \n\ntrain[\"text_cleaned\"] = train[\"text\"].apply(lambda x:cleaned_text(x))\ntest[\"text_cleaned\"] = test[\"text\"].apply(lambda x:cleaned_text(x))\ntrain.head()","2ab268b5":"test.head()","26d59306":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(analyzer=cleaned_text)\nx_tfidf = tfidf.fit(train[\"text_cleaned\"])\n\ntfidf_train = x_tfidf.transform(train[\"text_cleaned\"])\ntfidf_test = x_tfidf.transform(test[\"text_cleaned\"]) \n\nprint(tfidf_train.shape)\nprint(tfidf_test.shape)\n#print(tfidf.get_feature_names())","aaf21d48":"df_tfidf_train = pd.DataFrame(tfidf_train.toarray())\ndf_tfidf_test = pd.DataFrame(tfidf_test.toarray())\n\ndf_tfidf_train.columns = tfidf.get_feature_names()\ndf_tfidf_test.columns = tfidf.get_feature_names()\n\ndf_tfidf_train.head()","8cb2d201":"def count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round((count\/(len(text)-text.count(\" \")))*100, 3)\n\ntrain[\"body_length\"] = train[\"text\"].apply(lambda x:len(x)-x.count(\" \"))\ntrain[\"punctuation%\"] = train[\"text\"].apply(lambda x: count_punc(x))\n\ntest[\"body_length\"] = test[\"text\"].apply(lambda x:len(x)-x.count(\" \"))\ntest[\"punctuation%\"] = test[\"text\"].apply(lambda x: count_punc(x))\n\ntrain.head()","4162cb74":"test.head()","033722c5":"import matplotlib.pyplot as plt\n\nbins = np.linspace(0,200, 40)\nplt.hist(train[train[\"author\"]==\"EAP\"][\"body_length\"], bins, alpha=0.5, normed=True, label=\"EAP\")\nplt.hist(train[train[\"author\"]=='HPL'][\"body_length\"], bins, alpha=0.5, normed=True, label='HPL')\nplt.hist(train[train[\"author\"]=='MWS'][\"body_length\"], bins, alpha=0.5, normed=True, label='MWS')\nplt.legend(loc=\"best\")\nplt.show()","108556ca":"bins = np.linspace(0,200, 40)\nplt.hist(train[train[\"author\"]==\"EAP\"][\"punctuation%\"], bins, alpha=0.5, normed=True, label=\"EAP\")\nplt.hist(train[train[\"author\"]=='HPL'][\"punctuation%\"], bins, alpha=0.5, normed=True, label='HPL')\nplt.hist(train[train[\"author\"]=='MWS'][\"punctuation%\"], bins, alpha=0.5, normed=True, label='MWS')\nplt.legend(loc=\"best\")\nplt.show()","2e97e4ee":"df_train = pd.concat([train[[\"body_length\", \"punctuation%\"]].reset_index(drop=True), df_tfidf_train], axis=1)\ndf_test = pd.concat([test[[\"body_length\", \"punctuation%\"]].reset_index(drop=True), df_tfidf_test], axis=1)\ndf_train.head()","98522dc5":"df_test.head()","1c73dfec":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df_train, train[\"author\"], test_size=0.2)\n\ndef train_rf(n_est, depth):\n    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n    rf_model = rf.fit(x_train, y_train)\n    y_pred = rf_model.predict(x_test)\n    score = accuracy_score(y_test, y_pred)\n    print(\"n_estimator: {} \/ depth: {} \/ accuracy score: {}\".format(n_est, depth, score))\n    \nfor i in [10,50,100,150]:\n    for k in [10,20,30,40,None]:\n        train_rf(i, k)","e249815a":"rf = RandomForestClassifier(n_estimators=50, max_depth=None, n_jobs=-1)\nrf_model = rf.fit(df_train, train[\"author\"])\ny_pred = rf_model.predict(df_test)","9baf8224":"df_pro = rf.predict_proba(df_test)\ndf_prob = pd.DataFrame(df_pro)\ndf_prob.columns = [\"EAP\",\"HPL\", \"MWS\"]\ndf_prob.head()\n","96b867e1":"\ndf = pd.concat([test[\"id\"], df_prob], axis=1)\ndf.head(10)","9c4f485d":"We can add two more features, the length of the text and the number of punctuations are used in the text. \nThey might be different for each author, one more punctuation than the other.","3d07236e":"How much missing data is there?","47db0aa7":"How many rows for each author","da609560":"Remove our punctuations in our text","d9f24b25":"The shape of the dataset"}}