{"cell_type":{"c44a0e90":"code","b7e15964":"code","706e1f67":"code","ba296cf7":"code","0e9b0419":"code","96583c7b":"code","bbb2bee2":"code","571da55d":"code","b76298f3":"code","a3726251":"code","f863ef89":"code","2f2638ee":"code","415de89e":"code","e8cee599":"code","b4a9834d":"code","92e34c53":"code","d7dbd8c3":"code","a35b532e":"code","dff7746c":"code","39c517a4":"code","276cd5b3":"code","1edabb2b":"code","f85bb2a3":"code","a35e00ad":"code","de64e898":"code","5b342f71":"code","6692d46e":"code","41022ba8":"code","97e6d316":"code","c191e68d":"code","3fd8f1b7":"code","ca0e9463":"code","d5d7b7e0":"code","504296ca":"code","43d5ec91":"code","9781e27b":"code","d978be3f":"code","f61234e4":"code","479a5bc6":"code","17ca5755":"code","2cabab24":"code","9863e789":"code","606cce9b":"code","be43b2c4":"code","7402e8b8":"code","a182d949":"code","b9a54d29":"code","d5656651":"code","ca4a4bb1":"code","832dbb49":"code","d2825edd":"code","eb5f8a0b":"markdown","da0bfcd7":"markdown","f7bc18db":"markdown","44e8a7d9":"markdown","be17911a":"markdown","da685777":"markdown","cf5b1b77":"markdown","227ea32a":"markdown"},"source":{"c44a0e90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b7e15964":"traindf = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntestdf = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(\"Number of rows and columns in train data : \",traindf.shape)\nprint(\"Number of rows and columns in test data : \",testdf.shape)","706e1f67":"traindf.head()","ba296cf7":"traindf[\"lang_abv\"].value_counts()","0e9b0419":"plt.figure(figsize=(10,5))\nsns.countplot(x='label', data=traindf,\n                   order=list(traindf['label'].value_counts().sort_index().index) ,\n                   color='black')","96583c7b":"print(f'Number of different Langauges: {len(traindf[\"language\"].unique())}')\nplt.figure(figsize=(20,5))\nsns.countplot(x='language', data=traindf,\n                   order=list(traindf['language'].value_counts().sort_index().index) ,\n                   color='black')","bbb2bee2":"plt.figure(figsize=(20,5))\nsns.countplot(traindf['language'], hue = traindf['label'] ,\n                   color='black')","571da55d":"langdf=pd.DataFrame()\nlangdf['Name']=traindf.language.value_counts().index\nlangdf['Count']=traindf.language.value_counts().values\n\nlangdf_test=pd.DataFrame()\nlangdf_test['Name']=testdf.language.value_counts().index\nlangdf_test['Count']=testdf.language.value_counts().values\n\nlangdf['Key'] = 'train'\nlangdf_test['Key'] = 'test'\nDF = pd.concat([langdf,langdf_test],keys=['train','test'])\nDF.groupby(['Name','Key']).sum().unstack('Key').plot(kind='bar',figsize=(20, 5),color='black')","b76298f3":"traindf[\"lang_abv\"]= traindf[\"lang_abv\"].replace(\"zh\",\"zh-tw\") \ntestdf[\"lang_abv\"]= testdf[\"lang_abv\"].replace(\"zh\",\"zh-tw\") ","a3726251":"pip install googletrans","f863ef89":"import googletrans\nprint(googletrans.LANGUAGES)","2f2638ee":"from googletrans import Translator\ntranslator = Translator()\nresult = translator.translate('Main acha hoon', src='hi')\nprint(result.src)\nprint(result.dest)\nprint(result.origin)\nprint(result.text)\nprint(result.pronunciation)","415de89e":"def To_English(language,textstring):\n    if language!=\"en\":\n        translator = Translator()\n        return translator.translate(textstring,dest = \"en\").text\n    else:\n        return textstring","e8cee599":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","b4a9834d":"strategy","92e34c53":"def Translate(x):\n    translator = Translator()\n    return translator.translate(x).text","d7dbd8c3":"#traindf.premise[traindf.lang_abv!= 'en']=traindf.premise[traindf.lang_abv!= 'en'].apply(lambda x: Translate(x))","a35b532e":"#traindf.hypothesis[traindf.lang_abv!= 'en']=traindf.hypothesis[traindf.lang_abv!= 'en'].apply(lambda x: Translate(x))","dff7746c":"#traindf.to_csv(\"traindf.csv\",index=True)","39c517a4":"#testdf.hypothesis[testdf.lang_abv!= 'en']=testdf.hypothesis[testdf.lang_abv!= 'en'].apply(lambda x: Translate(x))","276cd5b3":"#testdf.premise[testdf.lang_abv!= 'en']=testdf.premise[testdf.lang_abv!= 'en'].apply(lambda x: Translate(x))","1edabb2b":"#testdf.to_csv(\"testdf.csv\",index=True)","f85bb2a3":"Updatedtraindf = pd.read_csv(\"..\/input\/translated-data\/traindf.csv\")\nUpdatedtestdf = pd.read_csv(\"..\/input\/translated-data\/testdf.csv\")","a35e00ad":"Updatedtraindf.head()","de64e898":"LangDf = pd.DataFrame()\nLangDf['premise'] = Updatedtraindf['premise']\nLangDf['hypothesis'] = Updatedtraindf['hypothesis']","5b342f71":"import nltk\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')","6692d46e":"print(stop_words)","41022ba8":"LangDf['premise'][0]","97e6d316":"import random\nfrom random import shuffle\nrandom.seed(1)\n\n# import these modules \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize, word_tokenize\n#cleaning up text\nimport re\ndef Preprocess_text(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"\u2019\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    \n    #Removing stop words and convert words to base forms\n    clean_line=LemmaSentence(clean_line)\n    return clean_line\n\ndef LemmaSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    New_sentence=[]\n    updated_word_list = list(set([word for word in token_words if word not in stop_words]))\n    for word in token_words:\n        lemmatizer = WordNetLemmatizer()\n        New_sentence.append(lemmatizer.lemmatize(word))\n        New_sentence.append(\" \")\n        \n    return \"\".join(New_sentence)","c191e68d":"########################################################################\n# Synonym replacement\n# Replace n words in the sentence with synonyms from wordnet\n########################################################################\n\n#for the first time you use wordnet\n#import nltk\n#nltk.download('wordnet')\nfrom nltk.corpus import wordnet \n\ndef synonym_replacement(words, n):\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            #print(\"replaced\", random_word, \"with\", synonym)\n            num_replaced += 1\n        if num_replaced >= n: #only replace up to n words\n            break\n\n#this is stupid but we need it, trust me\n    sentence = ' '.join(new_words)\n    new_words = sentence.split(' ')\n\n    return new_words\n\ndef get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    if word in synonyms:\n        synonyms.remove(word)\n    return list(synonyms)\n\n########################################################################\n# Random deletion\n# Randomly delete words from the sentence with probability p\n########################################################################\n\ndef random_deletion(words, p):\n\n#obviously, if there's only one word, don't delete it\n    if len(words) == 1:\n        return words\n\n#randomly delete words with probability p\n    new_words = []\n    for word in words:\n        r = random.uniform(0, 1)\n        if r > p:\n            new_words.append(word)\n\n#if you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n    return new_words\n\n########################################################################\n# Random swap\n# Randomly swap two words in the sentence n times\n########################################################################\n\ndef random_swap(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        new_words = swap_word(new_words)\n    return new_words\n\ndef swap_word(new_words):\n    random_idx_1 = random.randint(0, len(new_words)-1)\n    random_idx_2 = random_idx_1\n    counter = 0\n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words)-1)\n        counter += 1\n        if counter > 3:\n            return new_words\n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n    return new_words\n\n########################################################################\n# Random insertion\n# Randomly insert n words into the sentence\n########################################################################\n\ndef random_insertion(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        add_word(new_words)\n    return new_words\n\ndef add_word(new_words):\n    synonyms = []\n    counter = 0\n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return\n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)\n\n########################################################################\n# main data augmentation function\n########################################################################\n\ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=7):\n    words = sentence.split(' ')\n    words = [word for word in words if word is not '']\n    num_words = len(words)\n    augmented_sentences = []\n    num_new_per_technique = int(num_aug\/4)+1\n    n_sr = max(1, int(alpha_sr*num_words))\n    n_ri = max(1, int(alpha_ri*num_words))\n    n_rs = max(1, int(alpha_rs*num_words))\n\n#sr\n    for _ in range(num_new_per_technique):\n        a_words = synonym_replacement(words, n_sr)\n        augmented_sentences.append(' '.join(a_words))\n\n#ri\n    for _ in range(num_new_per_technique):\n        a_words = random_insertion(words, n_ri)\n        augmented_sentences.append(' '.join(a_words))\n\n#rs\n    for _ in range(num_new_per_technique):\n        a_words = random_swap(words, n_rs)\n        augmented_sentences.append(' '.join(a_words))\n\n    #rd\n    for _ in range(num_new_per_technique):\n        a_words = random_deletion(words, p_rd)\n        augmented_sentences.append(' '.join(a_words))\n\n    augmented_sentences = [sentence for sentence in augmented_sentences]\n    shuffle(augmented_sentences)\n\n    #trim so that we have the desired number of augmented sentences\n    if num_aug >= 1:\n        augmented_sentences = augmented_sentences[:num_aug]\n    else:\n        keep_prob = num_aug \/ len(augmented_sentences)\n        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n    #append the original sentence\n    augmented_sentences.append(sentence)\n    return augmented_sentences","3fd8f1b7":"print(f\"Actual text - {Preprocess_text(LangDf['premise'][1])}\")\nprint(\"Augmented text -\")\nfor x in eda(Preprocess_text(LangDf['premise'][1])):\n    print(x)","ca0e9463":"Updatedtraindf=Updatedtraindf.drop([\"id\",\"language\",\"lang_abv\",\"Unnamed: 0\"],axis=1)","d5d7b7e0":"print(Updatedtraindf.shape)\nUpdatedtraindf.head()","504296ca":"augmenteddf = pd.DataFrame(columns = ['premise', 'hypothesis', 'label']) ","43d5ec91":"rows=[]\ndef augment_data(premise,hypothesis,label):\n     # Pass a series in append() to append a row in dataframe  \n    rows.append([premise,hypothesis,label])\n    for x in eda(premise):\n        rows.append([x,hypothesis,label])\n    for y in eda(hypothesis):\n        rows.append([premise,y,label])    ","9781e27b":"for i in Updatedtraindf.index: \n    augment_data(Preprocess_text(Updatedtraindf['premise'][i]),Preprocess_text(Updatedtraindf['hypothesis'][i]), Updatedtraindf['label'][i])\n    \naugmenteddf=pd.DataFrame(rows, columns=['premise', 'hypothesis', 'label'])","d978be3f":"print(Updatedtraindf.shape)\nprint(augmenteddf.shape)\n","f61234e4":"from sklearn.utils import shuffle\naugmenteddf = shuffle(augmenteddf)","479a5bc6":"augmenteddf.to_csv(\"AugmentedTrain.csv\",index=False)","17ca5755":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs\n","2cabab24":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","9863e789":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","606cce9b":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntrain_input = bert_encode(augmenteddf.premise.values, augmenteddf.hypothesis.values, tokenizer)","be43b2c4":"max_len = 30\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","7402e8b8":"with strategy.scope():\n    model = build_model()\n    model.summary()","a182d949":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping=EarlyStopping(monitor='val_accuracy',mode='max',patience=5,min_delta=0.01)\nmodel.fit(train_input, augmenteddf.label.values, epochs = 10, verbose = 1, batch_size = 64, validation_split = 0.3,callbacks=[early_stopping])","b9a54d29":"test_input=bert_encode(Updatedtestdf.premise.values, Updatedtestdf.hypothesis.values, tokenizer)","d5656651":"predictions = [np.argmax(i) for i in model.predict(test_input)]","ca4a4bb1":"submission = Updatedtestdf.id.copy().to_frame()\nsubmission['prediction'] = predictions","832dbb49":"submission.head()","d2825edd":"submission.to_csv('submission.csv', header=True, index=False) ","eb5f8a0b":"For a given sentence in the training set, we randomly choose and perform one of the following operations:\n1. Synonym Replacement (SR): <br\/>\nRandomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.<br\/>\n2. Random Insertion (RI): <br\/>\nFind a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.<br\/>\n3. Random Swap (RS):<br\/>\nRandomly choose two words in the sentence and swap their positions. Do this n times.<br\/>\n4. Random Deletion (RD):<br\/>\nRandomly remove each word in the sentence with probability p","da0bfcd7":"<h5>Using Google translate API for handling data from different Languages<\/h5>\nFor more Info please refer - https:\/\/stackabuse.com\/text-translation-with-google-translate-api-in-python\/","f7bc18db":"<center><H1> In Progress <\/h1><\/center>","44e8a7d9":"Please find the augmented data <a href=\"https:\/\/www.kaggle.com\/krsna540\/translated-data\"> here <\/a>","be17911a":"<h5>References:<\/h5>\n\n* https:\/\/arxiv.org\/pdf\/1901.11196.pdf\n* https:\/\/github.com\/jasonwei20\/eda_nlp\n* https:\/\/arxiv.org\/abs\/1706.03762\n* https:\/\/github.com\/google-research\/bert\n* https:\/\/openai.com\/blog\/better-language-models\/\n* https:\/\/arxiv.org\/pdf\/1906.08237.pdf\n* https:\/\/blog.einstein.ai\/introducing-a-conditional-transformer-language-model-for-controllable-generation\/\n* https:\/\/developer.nvidia.com\/blog\/training-bert-with-gpus\/\n* https:\/\/www.microsoft.com\/en-us\/research\/blog\/turing-nlg-a-17-billion-parameter-language-model-by-microsoft\/","da685777":"Labels are also well distributed across multiple langauges","cf5b1b77":"Please find the translated data <a href=\"https:\/\/www.kaggle.com\/krsna540\/translated-data\">here<\/a>","227ea32a":"Well Balanced on Label Distribution"}}