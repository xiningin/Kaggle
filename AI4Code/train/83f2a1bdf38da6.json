{"cell_type":{"8d19a038":"code","e086f884":"code","de2556c4":"code","9d40fc04":"code","ba6f447c":"code","abcf241d":"code","460d1651":"code","84de9c48":"code","4e470732":"code","c735401e":"code","c280026c":"code","13f0575c":"code","73f2f8c9":"code","a8082b50":"code","1274bdbd":"code","3b29a036":"code","ed9890a2":"code","7838a5e5":"code","0453ae78":"code","aaf0068c":"code","41412e90":"code","09158f35":"code","67f89e40":"code","e0e67ca9":"code","59518927":"code","ebb0d9ce":"code","1de51a15":"markdown","bb56b251":"markdown","2fad2216":"markdown","5d4acebf":"markdown","a34a0ef2":"markdown","ec4977e6":"markdown","179754a5":"markdown","641dd6b6":"markdown","a748da3b":"markdown","bab0bec7":"markdown","9c0c23e2":"markdown","aac1c28e":"markdown","64430dc9":"markdown","99ebcb39":"markdown","422591b1":"markdown","96a1c77a":"markdown","0f199a10":"markdown","545236fd":"markdown","e18eb749":"markdown"},"source":{"8d19a038":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","e086f884":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","de2556c4":"df.info()","9d40fc04":"from sklearn.model_selection import train_test_split\n\ndef get_preprocessed_df(df=None):\n    df_copy = df.copy()\n    df_copy.drop('Time', axis=1, inplace=True)\n    return df_copy","ba6f447c":"def get_train_test_dataset(df=None):\n    df_copy = get_preprocessed_df(df)\n    \n    X_features = df_copy.iloc[:, :-1]\n    y_target = df_copy.iloc[:, -1]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=2019, stratify=y_target)\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_train_test_dataset(df)","abcf241d":"print('Label Ratio of Training Data:')\nprint(y_train.value_counts()\/y_train.shape[0]*100)\nprint('Label Ratio of Test Data:')\nprint(y_test.value_counts()\/y_test.shape[0]*100)","460d1651":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score\n\ndef get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    roc_auc = roc_auc_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","84de9c48":"def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n    model.fit(ftr_train, tgt_train)\n    pred = model.predict(ftr_test)\n    get_clf_eval(tgt_test, pred)","4e470732":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression()\nget_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)","c735401e":"from lightgbm import LGBMClassifier\n\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=1, boost_from_average=False)\nget_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)","c280026c":"import seaborn as sns\n\nplt.figure(figsize=(8,4))\nplt.xticks(range(0, 30000, 1000), rotation=60)\nsns.distplot(df['Amount'])","13f0575c":"from sklearn.preprocessing import StandardScaler\n\ndef get_preprocessed_df (df=None):\n    df_copy = df.copy()\n    scaler = StandardScaler()\n    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))\n    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n    return df_copy","73f2f8c9":"X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('Logistic Regression')\nlr_clf = LogisticRegression()\nget_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n\nprint('LightGBM')\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=1, boost_from_average=False)\nget_model_train_eval(lgbm_clf,ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)","a8082b50":"def get_preprocessed_df (df=None):\n    df_copy = df.copy()\n    amount_n = np.log1p(df_copy['Amount'])\n    df_copy.insert(0, 'Amount_Scaled', amount_n)\n    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n    return df_copy","1274bdbd":"X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('Logistic Regression')\nlr_clf = LogisticRegression()\nget_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n\nprint('LightGBM')\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=1, boost_from_average=False)\nget_model_train_eval(lgbm_clf,ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)","3b29a036":"plt.figure(figsize=(9, 9))\ncorr = df.corr()\nsns.heatmap(corr, cmap='RdBu')","ed9890a2":"def get_outlier(df=None, column=None, weight=1.5):\n    fraud = df[df['Class']==1][column]\n    quantile_25 = np.percentile(fraud.values, 25)\n    quantile_75 = np.percentile(fraud.values, 75)\n    \n    iqr = quantile_75 - quantile_25\n    iqr_weight = iqr * weight\n    lowest_val = quantile_25 - iqr_weight\n    highest_val = quantile_75 + iqr_weight\n    \n    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n    return outlier_index","7838a5e5":"outlier_index = get_outlier(df=df, column='V14', weight=1.5)","0453ae78":"print('outlier index: ', outlier_index)","aaf0068c":"def get_preprocessed_df (df=None):\n    df_copy = df.copy()\n    amount_n = np.log1p(df_copy['Amount'])\n    df_copy.insert(0, 'Amount_Scaled', amount_n)\n    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n    outlier_index = get_outlier(df=df, column='V14', weight=1.5)\n    df_copy.drop(outlier_index, axis=0, inplace=True)\n    return df_copy","41412e90":"X_train, X_test, y_train, y_test = get_train_test_dataset(df)\n\nprint('Logistic Regression')\nlr_clf = LogisticRegression()\nget_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n\nprint('LightGBM')\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=1, boost_from_average=False)\nget_model_train_eval(lgbm_clf,ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)","09158f35":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=2019)\nX_train_over, y_train_over = smote.fit_sample(X_train, y_train)\nprint('Feature\/Label Dataset before SMOTE: ', X_train.shape, y_train.shape)\nprint('Feature\/Label Dataset after SMOTE: ', X_train_over.shape, y_train_over.shape)\nprint('Label Distribution after SMOTE: \\n', pd.Series(y_train_over).value_counts())","67f89e40":"lr_clf = LogisticRegression()\nget_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)","e0e67ca9":"from sklearn.metrics import precision_recall_curve\n\ndef precision_recall_curve_plot(y_test , pred_proba_c1):\n    # threshold ndarray\uc640 \uc774 threshold\uc5d0 \ub530\ub978 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 ndarray \ucd94\ucd9c. \n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n    \n    # X\ucd95\uc744 threshold\uac12\uc73c\ub85c, Y\ucd95\uc740 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 \uac12\uc73c\ub85c \uac01\uac01 Plot \uc218\ud589. \uc815\ubc00\ub3c4\ub294 \uc810\uc120\uc73c\ub85c \ud45c\uc2dc\n    plt.figure(figsize=(8,6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n    \n    # threshold \uac12 X \ucd95\uc758 Scale\uc744 0.1 \ub2e8\uc704\ub85c \ubcc0\uacbd\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n    \n    # x\ucd95, y\ucd95 label\uacfc legend, \uadf8\ub9ac\uace0 grid \uc124\uc815\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","59518927":"precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:, 1])","ebb0d9ce":"lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=1, boost_from_average=False)\nget_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)","1de51a15":"- Creating Function","bb56b251":"LightGBM shows better performance than Logistic Regression","2fad2216":"- Prediction Performance Evaluation","5d4acebf":"- Split Data into Train and Test Data","a34a0ef2":"A little improved!","ec4977e6":"- LightGBM with using SMOTE","179754a5":"- Apply SMOTE Over-sampling\n\nonly to train data set","641dd6b6":"Too sensitive threshold value!","a748da3b":"- Data Preprocessing","bab0bec7":"- Logistic Regression","9c0c23e2":"Performance is not improved even though Amount values were standardized!\n\nLet's transform Amount based on Log!","aac1c28e":"High Recall but Low Precision!\n\nLet's chek precision-recall curve!","64430dc9":"- Check and Eliminate more important Outliers","99ebcb39":"- Transformation of data distribution and Model learning\/prediction\/evaluation","422591b1":"- Logistic Regression with using SMOTE","96a1c77a":"- Data Loading","0f199a10":"Features with high negative correlation with Class are V14 and V17.","545236fd":"Most instances have Amount values less than 1000.\n\nAmount values need to be transformed.","e18eb749":"- LightGBM"}}