{"cell_type":{"719de9b8":"code","1f7fa6c1":"code","521244f2":"code","33c44ebc":"code","047e71e3":"code","f75c1f79":"code","f60392ce":"code","a9325452":"code","3220a990":"code","72c7c9de":"code","ee33c36f":"code","647eea8f":"code","5dbe3f3d":"code","2607e246":"code","4c401ce8":"code","89c105e8":"code","bb29a70b":"code","9da4429f":"code","3263c98b":"code","b9c03ddf":"code","d72ad81f":"code","e615f305":"code","ff75f382":"code","98e969e9":"code","bb5e5d5b":"code","aa985c92":"code","670714c7":"code","6b8a5e82":"code","92d97447":"code","1a7e733f":"code","c54caaeb":"code","bd1ee325":"code","9639d330":"code","3006bcba":"code","b26adfc8":"code","a2d272fd":"code","fc1746b0":"code","c957f60e":"code","216a61d6":"code","7604b4b9":"code","89773957":"code","2920e57e":"code","39ed23d5":"code","ac540c6b":"code","031df75b":"code","920dc518":"code","5e80d413":"code","bf48b837":"code","5882c776":"code","444ae7b8":"code","5d694903":"code","56013f0a":"code","750a09f5":"code","a0d6e6c9":"code","48d450be":"code","4f4e41b2":"code","46f64056":"code","ae2fc086":"code","94a26c38":"code","b58b72a9":"code","34328a2b":"code","f367d885":"code","02d8aa79":"code","12e26158":"code","621e3ff6":"code","58f54b40":"code","5b1dce09":"code","03e0b87b":"code","90c8557c":"code","0670af2d":"code","661102d6":"code","416675fa":"code","d02b5db5":"code","72cb852f":"code","d66d780a":"code","03a83c5d":"code","90b88e42":"code","3372874c":"code","6274fb1a":"code","8eac7707":"code","11fbc157":"code","0a94f4a0":"code","6438bbe4":"code","5a598a53":"code","7877d16d":"code","ed7178fe":"code","22de9e3b":"code","9e04bfbe":"code","66871655":"code","b9933de4":"code","17439df9":"code","6d957f66":"code","f31e3b9e":"code","39599ca7":"code","d840ce65":"code","2a186c29":"code","d4482d07":"code","a74c0db9":"code","aae8067d":"code","f8778dd5":"code","daebca76":"code","bf44093c":"code","a29b1c34":"code","d5ab208c":"code","808c58b4":"code","c00879a4":"code","2f349aa8":"code","685d4228":"code","ab01432b":"code","e5f50cae":"code","a88e1aa1":"code","1611d7a2":"code","65fbd17a":"code","828e719c":"code","ad18c34f":"code","bc369fa7":"code","c5dece75":"code","8253da8e":"code","73cf26ee":"code","6a697ce6":"code","81294fb4":"code","86f001df":"code","fb846e1c":"code","457ac354":"code","47576336":"code","a80458db":"code","7b0337ae":"code","5b23c988":"code","46706101":"code","8cd0207a":"code","8649ecd4":"code","d106c9fa":"code","72b9f294":"code","1875e475":"code","2a0b0078":"code","39ae325b":"code","15e08cfa":"code","5080609e":"code","015bf599":"code","9ca8dcff":"code","c6b8d112":"code","d09e09c7":"code","711333f0":"code","90cfbc2a":"code","2dbddf51":"code","7049323a":"code","1d68c0f0":"code","89b170cf":"markdown","3d4adfb9":"markdown","fac883f9":"markdown","dfd6f596":"markdown","9c03f5fe":"markdown","87e91bc3":"markdown","50ca3aed":"markdown","5e5d425e":"markdown","72c5137b":"markdown","2379afd5":"markdown","d76980b3":"markdown","2fe86491":"markdown","9837575d":"markdown","1a5d3ee0":"markdown","61ba5989":"markdown","1714b940":"markdown","e991d1f3":"markdown","cf5f249a":"markdown","d213ff24":"markdown","3a383ea4":"markdown","12cd174b":"markdown","5cdcc6e0":"markdown","634a3d1f":"markdown","08c4eb3c":"markdown","b7a9602b":"markdown","36fac416":"markdown","5f333890":"markdown","11bdcd9a":"markdown","549135a6":"markdown","3844cd76":"markdown","567aa394":"markdown","b2b3fc8e":"markdown","5a53fe2b":"markdown","3aecdbb0":"markdown","734761a3":"markdown","808fa5cf":"markdown","7195ca26":"markdown","c5daf8ed":"markdown","68474dab":"markdown","0a150499":"markdown","0464a028":"markdown","bd8a6b4c":"markdown","cfe74e9f":"markdown","a71a6ab3":"markdown","72647822":"markdown","d1fa598f":"markdown","3e63f276":"markdown","3a1380b7":"markdown","47ee2cb2":"markdown","57018fd0":"markdown","c8ad4025":"markdown","b48aa594":"markdown","6c0c1f74":"markdown","ced9f441":"markdown","ee6cb664":"markdown","43b8f0be":"markdown","b9f2ce7f":"markdown","e684a041":"markdown","10a293b0":"markdown","68a3e0a2":"markdown","90f13b52":"markdown","efa6b108":"markdown","a1c44de9":"markdown","dd0698cf":"markdown","ed44e016":"markdown","0e8e4db7":"markdown","fc0d3687":"markdown","246c1126":"markdown","5ecb997d":"markdown","ef87afc1":"markdown","ecac5332":"markdown","6457bbac":"markdown","9b5f67e5":"markdown","c2046045":"markdown","25a008e1":"markdown","ef080f6d":"markdown","6794b415":"markdown","841e7dcc":"markdown","4f922356":"markdown","49d5f356":"markdown","cc8f0374":"markdown","c2b9a308":"markdown","2ad3146e":"markdown","4699cc34":"markdown","654a9784":"markdown","e8720b67":"markdown","ae61b452":"markdown","b61775d4":"markdown","f6b4491c":"markdown","b2a13922":"markdown","6fbb63cf":"markdown","7328d8a3":"markdown","af0805bc":"markdown","faedc6fb":"markdown","63237825":"markdown","e035d219":"markdown","7abcf3ec":"markdown","40004a38":"markdown","2d852be0":"markdown","586133c8":"markdown"},"source":{"719de9b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1f7fa6c1":"import pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier","521244f2":"job_data = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv', index_col = 'job_id')\njob_data","33c44ebc":"# checking the shape of the data\njob_data.shape","047e71e3":"# 0 is not fake and 1 is fake\n# checking for Imbalance\njob_data['fraudulent'].value_counts()","f75c1f79":"print(\"percentage of data with class a 0: \",job_data['fraudulent'].value_counts()[0] \/job_data.shape[0] *100)\nprint(\"percentage of data with class a 1: \",job_data['fraudulent'].value_counts()[1] \/job_data.shape[0] *100)\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"fraudulent\", data=job_data)\nax.set_title(\"count plot of the classes\")","f60392ce":"job_data.info()","a9325452":"import matplotlib.pyplot as plt\ntotal= job_data.isnull().sum()\nmissing_percent =  job_data.isnull().sum()* 100 \/ len(job_data)\nmissing_data = pd.concat([total,missing_percent],axis=1,keys=['Total','Percentage'])\nf,ax = plt.subplots(figsize=(15,6))\nxlocs=plt.xticks(rotation='90')\nbars = plt.bar(missing_data.index,missing_data['Total'])\n\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .005, round((yval*100)\/len(job_data),2), color='red',fontweight='bold')","3220a990":"categorical_feature = []\nfor col in job_data.columns:\n    print(f'Unique rows in {col}:', job_data[col].nunique())\n    if job_data[col].nunique() < 15:\n        categorical_feature.append(col)\nprint('Categorical feature:',categorical_feature)\nprint('Total cat feature',len(categorical_feature))","72c7c9de":"# job_data.dropna(thresh = threshold, axis=1)\njob_data.drop(columns = ['salary_range'],axis=1,inplace = True)\njob_data['department'].fillna('other',inplace=True)\njob_data.head()","ee33c36f":"job_data.shape","647eea8f":"# total= job_data.isnull().sum()\n# missing_percent =  job_data.isnull().sum()* 100 \/ len(job_data)\n# missing_data = pd.concat([total,missing_percent],axis=1,keys=['Total','Percentage'])\n# f,ax = plt.subplots(figsize=(15,6))\n# xlocs=plt.xticks(rotation='90')\n# bars = plt.bar(missing_data.index,missing_data['Total'])\n\n# for bar in bars:\n#     yval = bar.get_height()\n#     plt.text(bar.get_x(), yval + .005, round((yval*100)\/len(job_data),2), color='red',fontweight='bold')","5dbe3f3d":"# empty_columns = job_data.loc[:,job_data.isnull().sum()>0]\n# empty_columns","2607e246":"# filling he categorical missing data\nfor feature in categorical_feature:\n#     print(\"feature name: \",feature)\n    if job_data[feature].isnull().sum() > 0 :\n        print(\"find the empyt value in feature:\",feature)\n        job_data[feature].fillna(value= job_data[feature].mode()[0],inplace=True)","4c401ce8":"# filling the non categorical data\nnon_categorical_data =  list(set(job_data.columns) - set(categorical_feature))\nfor feature in non_categorical_data:\n    if job_data[feature].isnull().sum() > 0 :\n        print(\"find the empyt value in feature:\",feature)\n        job_data[feature].fillna(value= 'Not specified',inplace=True)","89c105e8":"job_data.isnull().sum(), job_data.shape","bb29a70b":"job_data.describe()","9da4429f":"job_data.head()","3263c98b":"categorical_feature.pop()","b9c03ddf":"# categorical feature effect on the fraudulent classes\nplt.figure(1,figsize=(20,8))\nsns.countplot(hue=job_data.fraudulent,x=job_data.employment_type);\nplt.title('Which type of jobs have more fraudulent postings');","d72ad81f":"plt.figure(1,figsize=(20,8))\nsns.countplot(hue=job_data.fraudulent,x=job_data.required_experience);\nplt.title('Which required experience of jobs have more fraudulent postings');","e615f305":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.required_education)\nplt.legend(loc='upper right')\nplt.title('Which required education of jobs have more fraudulent postings')","ff75f382":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.telecommuting)\nplt.legend(loc='upper right')\nplt.title('How telecommuting jobs effect contribute towards the fraudulent postings.')","98e969e9":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.has_company_logo)\nplt.legend(loc='upper right')\nplt.title('Company logo presence effect on fraudulent postings')","bb5e5d5b":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.has_questions)\nplt.legend(loc='upper right')\nplt.title('Screening Question effect fraudulent postings')","aa985c92":"processed_country = []\n# not_specified_country_index = []\n# processed_state = []\n# not_specified_state_index = []\n# processed_city = []\ndef location_separator(splitted_location_name):\n    for idx, j in enumerate(splitted_location_name):\n        if j.isspace() :\n            if idx == 0:\n                processed_country.append('not given')\n#             elif idx == 1:\n# #                 not_specified_state_index.append(idx)\n#                 processed_state.append('not given')\n#             else:\n#                 processed_city.append('not given')\n        else:\n            if idx == 0:\n                processed_country.append(j.replace(\" \", \"\"))\n#             elif idx == 1:\n#                 processed_state.append(j.replace(\" \", \"\"))\n#             else:\n#                 processed_city.append(j.replace(\" \", \"\")) ","670714c7":"processed_location = []\nfor idx,value in enumerate(job_data.location.to_list()):\n    if '\\t' in value:\n        processed_location_name = value.replace('\\t','')\n        processed_location.append(processed_location_name)\n        splitted_location_name = processed_location_name.split(',')\n        location_separator(splitted_location_name) \n    else:\n        \n        if value == 'Not specified':\n            value = 'NA, NA, NA'\n        processed_location.append(value)\n        location_separator(value.split(','))","6b8a5e82":"len(processed_country),len(processed_location)","92d97447":"state_name = []\nimport re\nfor idx,value in enumerate(job_data.location.to_list()):\n    value = value.replace('\\t','')\n    if len(value.split(',')) > 3:\n        state_value = list(filter(str.strip, value.split(',')))\n        state_value = [re.sub('[0-9]','',word) for word in state_value if len(word.replace(\" \", \"\"))>1]\n        state_value = list(filter(str.strip,state_value))\n        state_value = list(set(state_value))\n        state_value = state_value[:3]\n        state_name.append(state_value[1].replace(\" \", \"\"))  \n    elif len(value.split(',')) == 1:\n        state_name.append('NOT GIVEN')      \n    else:\n        if len(value.split(',')[1].replace(\" \", \"\"))==1 or value.split(',')[1].isspace():\n#             print(\"original_value: \",value,\"<---list_value:--> \",value.split(','))\n#             print('idx',idx,'lenght: ',len(value.split(',')))\n            state_name.append('NOT GIVEN')\n        elif value.split(',')[1].replace(\" \", \"\").isdigit():\n            state_name.append('NOT GIVEN')\n        else:\n#             print(\"original_value: \",value,\"<---list_value:--> \",value.split(','))\n#             print('idx',idx,'lenght: ',len(value.split(',')))\n            \n            state_name.append(value.split(',')[1].replace(\" \", \"\")) ","1a7e733f":"len(state_name)","c54caaeb":"# job_data[job_data.duplicated(keep=False)].reset_index()\n# state_name\n# len('01')","bd1ee325":"city_name = []\nimport re\nfor idx,value in enumerate(job_data.location.to_list()):\n    value = value.replace('\\t','')\n    if len(value.split(',')) > 3:\n        state_value = list(filter(str.strip, value.split(',')))\n        state_value = [re.sub('[0-9]','',word) for word in state_value if len(word.replace(\" \", \"\"))>1]\n        state_value = list(filter(str.strip,state_value))\n        state_value = list(set(state_value))\n        city_name.append(state_value[2].replace(\" \", \"\"))  \n    elif len(value.split(',')) == 1:\n        city_name.append('NOT GIVEN')      \n    else:\n        if len(value.split(',')[2].replace(\" \", \"\"))==1 or value.split(',')[2].isspace():\n            city_name.append('NOT GIVEN')\n        elif value.split(',')[2].replace(\" \", \"\").isdigit():\n            city_name.append('NOT GIVEN')\n        else:\n            city_name.append(value.split(',')[2].replace(\" \", \"\"))\n    ","9639d330":"len(processed_country), len(state_name),len(city_name)","3006bcba":"processed_job_data = job_data.copy()\nprocessed_job_data['country'] = processed_country\nprocessed_job_data['state'] = state_name\nprocessed_job_data['city'] = city_name\nprocessed_job_data.drop(columns=['location'],axis=1,inplace=True)\n","b26adfc8":"fraud_data = processed_job_data[processed_job_data.fraudulent==1]\n# fraud_data","a2d272fd":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.department, data=fraud_data, order= fraud_data.department.value_counts().iloc[:].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the fraudulent postings.')","fc1746b0":"no_fraud_data = processed_job_data[processed_job_data.fraudulent==0]\n# no_fraud_data","c957f60e":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.department, data=no_fraud_data, order= no_fraud_data.department.value_counts().iloc[:100].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","216a61d6":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.industry,data=no_fraud_data,order=no_fraud_data.industry.value_counts().iloc[:50].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","7604b4b9":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.industry,data=fraud_data,order=fraud_data.industry.value_counts().iloc[:50].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","89773957":"processed_job_data.columns","2920e57e":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=processed_job_data.fraudulent,x=processed_job_data.function ,data=processed_job_data, order = processed_job_data.function.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('function effect on fraudulent postings')","39ed23d5":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.country ,data=fraud_data, order = fraud_data.country.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('state effect on fraudulent postings')","ac540c6b":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.state ,data=fraud_data, order = fraud_data.state.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('state effect on fraudulent postings')","031df75b":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.country ,data=no_fraud_data, order = no_fraud_data.country.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('Country effect on non fraudulent postings')","920dc518":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.state ,data=no_fraud_data, order = no_fraud_data.state.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('state effect on non fraudulent postings')","5e80d413":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.title ,data=fraud_data, order = fraud_data.title.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('title effect on fraudulent postings')","bf48b837":"no_fraud_data.title.value_counts()","5882c776":" fraud_data.title.value_counts()","444ae7b8":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.title ,data=no_fraud_data, order = no_fraud_data.title.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('title effect on non fraudulent postings')","5d694903":"# import re\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","56013f0a":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","750a09f5":"processed_job_data.columns","a0d6e6c9":"preprocessed_description = []\nfor sentance in tqdm(processed_job_data['description'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    preprocessed_description.append(sent)  ","48d450be":"processed_job_data.drop(['description'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_description'] = preprocessed_description","4f4e41b2":"preprocessed_benefits = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['benefits'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_benefits.append(sent)  ","46f64056":"processed_job_data.drop(['benefits'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_benefits'] = preprocessed_benefits\n","ae2fc086":"preprocessed_requirements = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['requirements'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_requirements.append(sent)  ","94a26c38":"processed_job_data.drop(['requirements'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_requirements'] = preprocessed_requirements","b58b72a9":"preprocessed_title = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['title'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = sent.strip()\n    sent = decontracted(sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_title.append(sent)  ","34328a2b":"processed_job_data.drop(['title'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_title'] = preprocessed_title","f367d885":"preprocessed_company_profile = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['company_profile'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = sent.strip()\n    sent = decontracted(sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_company_profile.append(sent)  ","02d8aa79":"processed_job_data.drop(['company_profile'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_company_profile'] = preprocessed_company_profile","12e26158":"processed_job_data","621e3ff6":"processed_job_data.drop(['department'],axis=1,inplace=True)","58f54b40":"# processed_job_data\nprocessed_job_data.reset_index(inplace=True)\nprocessed_job_data.drop(['job_id'],axis=1,inplace=True)","5b1dce09":"y = processed_job_data['fraudulent'].values\nX = processed_job_data.drop(['fraudulent'], axis=1)\nX.head()","03e0b87b":"!pip install imbalanced-learn","90c8557c":"# check version number\nimport imblearn\nprint(imblearn.__version__)","0670af2d":"# # balancig the Imbalanced dataset https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n# X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)\n\n# print(X_train.shape)\n# print(y_train.shape)\n# print(X_cv.shape)\n# print(y_cv.shape)\n# print(X_test.shape)\n# print(y_test.shape)","661102d6":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['employment_type']),lowercase=False, binary=True)\n# X_train_employment_type_one_hot = vectorizer.fit_transform(X_train['employment_type'].values)\n# X_cv_employment_type_one_hot = vectorizer.transform(X_cv['employment_type'].values)\n# X_test_employment_type_one_hot = vectorizer.transform(X_test['employment_type'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_employment_type_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_employment_type_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_employment_type_one_hot.shape)","416675fa":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_experience']),lowercase=False, binary=True)\n# X_train_required_experience_one_hot = vectorizer.fit_transform(X_train['required_experience'].values)\n# X_cv_required_experience_one_hot = vectorizer.transform(X_cv['required_experience'].values)\n# X_test_required_experience_one_hot = vectorizer.transform(X_test['required_experience'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_required_experience_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_experience_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_required_experience_one_hot.shape)","d02b5db5":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_education']),lowercase=False, binary=True)\n# X_train_required_education_one_hot = vectorizer.fit_transform(X_train['required_education'].values)\n# X_cv_required_education_one_hot = vectorizer.transform(X_cv['required_education'].values)\n# X_test_required_education_one_hot = vectorizer.transform(X_test['required_education'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_required_education_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_education_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_required_education_one_hot.shape)","72cb852f":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['industry']),lowercase=False, binary=True)\n# X_train_industry_one_hot = vectorizer.fit_transform(X_train['industry'].values)\n# X_cv_industry_one_hot = vectorizer.transform(X_cv['industry'].values)\n# X_test_industry_one_hot = vectorizer.transform(X_test['industry'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_industry_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_industry_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_industry_one_hot.shape)","d66d780a":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['function']),lowercase=False, binary=True)\n# X_train_function_one_hot = vectorizer.fit_transform(X_train['function'].values)\n# X_cv_function_one_hot = vectorizer.transform(X_cv['function'].values)\n# X_test_function_one_hot = vectorizer.transform(X_test['function'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_function_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_function_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_function_one_hot.shape)","03a83c5d":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['country']),lowercase=False, binary=True)\n# X_train_country_one_hot = vectorizer.fit_transform(X_train['country'].values)\n# X_cv_country_one_hot = vectorizer.transform(X_cv['country'].values)\n# X_test_country_one_hot = vectorizer.transform(X_test['country'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_country_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_country_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_country_one_hot.shape)","90b88e42":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['state']),lowercase=False, binary=True)\n# X_train_state_one_hot = vectorizer.fit_transform(X_train['state'].values)\n# X_cv_state_one_hot = vectorizer.transform(X_cv['state'].values)\n# X_test_state_one_hot = vectorizer.transform(X_test['state'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_state_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_state_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_state_one_hot.shape)","3372874c":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_description_tfidf = vectorizer.fit_transform(X_train['preprocessed_description'])\n# X_cv_description_tfidf = vectorizer.transform(X_cv['preprocessed_description'])\n# X_test_description_tfidf = vectorizer.transform(X_test['preprocessed_description'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_description_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_description_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_description_tfidf.shape)","6274fb1a":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_denifits_tfidf = vectorizer.fit_transform(X_train['preprocessed_benefits'])\n# X_cv_denifits_tfidf = vectorizer.transform(X_cv['preprocessed_benefits'])\n# X_test_denifits_tfidf = vectorizer.transform(X_test['preprocessed_benefits'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_denifits_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_denifits_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_denifits_tfidf.shape)","8eac7707":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_requirements_tfidf = vectorizer.fit_transform(X_train['preprocessed_requirements'])\n# X_cv_requirements_tfidf = vectorizer.transform(X_cv['preprocessed_requirements'])\n# X_test_requirements_tfidf = vectorizer.transform(X_test['preprocessed_requirements'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_requirements_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_requirements_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_requirements_tfidf.shape)","11fbc157":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_title_tfidf = vectorizer.fit_transform(X_train['preprocessed_title'])\n# X_cv_title_tfidf = vectorizer.transform(X_cv['preprocessed_title'])\n# X_test_title_tfidf = vectorizer.transform(X_test['preprocessed_title'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_title_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_title_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_title_tfidf.shape)","0a94f4a0":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_profile_tfidf = vectorizer.fit_transform(X_train['preprocessed_company_profile'])\n# X_cv_profile_tfidf = vectorizer.transform(X_cv['preprocessed_company_profile'])\n# X_test_profile_tfidf = vectorizer.transform(X_test['preprocessed_company_profile'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_profile_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_profile_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_profile_tfidf.shape)","6438bbe4":"X.columns","5a598a53":"# # merge two sparse matrices: https:\/\/stackoverflow.com\/a\/19710648\/4084039\n\n# # with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\n# # X = hstack((categories_one_hot, sub_categories_one_hot, text_bow, price_standardized))\n# X_tr = hstack((X_train_employment_type_one_hot,X_train_required_experience_one_hot,X_train_required_education_one_hot,X_train_industry_one_hot,\n#               X_train_function_one_hot,X_train_country_one_hot,X_train_state_one_hot,X_train_description_tfidf,\n#               X_train_denifits_tfidf,X_train_requirements_tfidf,X_train_title_tfidf,X_train_profile_tfidf,\n#               X_train['telecommuting'].values.reshape(-1,1),X_train['has_company_logo'].values.reshape(-1,1),\n#                X_train['has_questions'].values.reshape(-1,1))).tocsr()\n\n# X_cr = hstack((X_cv_employment_type_one_hot,X_cv_required_experience_one_hot,X_cv_required_education_one_hot,\n#               X_cv_industry_one_hot,X_cv_function_one_hot,X_cv_country_one_hot,X_cv_state_one_hot,\n#               X_cv_description_tfidf,X_cv_denifits_tfidf,X_cv_requirements_tfidf,X_cv_title_tfidf,\n#               X_cv_profile_tfidf,X_cv['telecommuting'].values.reshape(-1,1),\n#               X_cv['has_company_logo'].values.reshape(-1,1),X_cv['has_questions'].values.reshape(-1,1))).tocsr()\n\n# X_te = hstack((X_test_employment_type_one_hot,X_test_required_experience_one_hot,X_test_required_education_one_hot,\n#               X_test_industry_one_hot,X_test_function_one_hot,X_test_country_one_hot,X_test_state_one_hot,\n#               X_test_description_tfidf,X_test_denifits_tfidf,X_test_requirements_tfidf,X_test_title_tfidf,\n#               X_test_profile_tfidf,X_test['telecommuting'].values.reshape(-1,1),\n#               X_test['has_company_logo'].values.reshape(-1,1),X_test['has_questions'].values.reshape(-1,1))).tocsr()\n\n","7877d16d":"# #For memory issue batch wise prediction\n# def batch_predict(clf, data):\n#     y_data_pred = []\n#     tr_loop = data.shape[0] - data.shape[0]%1000\n#     for i in range(0, tr_loop, 1000):\n#         y_data_pred.extend(clf.predict(data[i:i+1000])[:,1])\n#     if data.shape[0]%1000 !=0:\n#         y_data_pred.extend(clf.predict(data[tr_loop:])[:,1])\n#     return y_data_pred\n\n# def find_best_threshold(threshould, fpr, tpr):\n#     t = threshould[np.argmax(tpr*(1-fpr))]\n#     # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n#     print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n#     return t\n\n# def predict_with_best_t(proba, threshould):\n#     predictions = []\n#     for i in proba:\n#         if i>=threshould:\n#             predictions.append(1)\n#         else:\n#             predictions.append(0)\n#     return predictions","ed7178fe":"# # Please write all the code with proper documentation\n# #selecting the hyperparameter using RandomSearch\n# from sklearn.metrics import f1_score, make_scorer\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import roc_curve\n# from xgboost import XGBClassifier\n# from sklearn.linear_model import SGDClassifier\n\n# # from sklearn.model_selection import GridSearchCV\n\n# log = SGDClassifier(loss = 'log', class_weight= 'balanced')\n# # parameters = {'alpha':sp_randint(50, 100)}\n# parameters = {'alpha':[10**x for x in range(-4,4)]}\n# clf = GridSearchCV(log, parameters, cv=3, scoring=make_scorer(f1_score),return_train_score=True)\n# clf.fit(X_tr, y_train)\n\n# results = pd.DataFrame.from_dict(clf.cv_results_)\n# # results3 = results3.sort_values(['param_alpha'])\n\n# train_f1= results['mean_train_score']\n# train_f1_std= results['std_train_score']\n# cv_f1 = results['mean_test_score'] \n# cv_f1_std= results['std_test_score']\n# # alpha3 =  results3['param_alpha']\n# # alpha3 = alpha3.astype(np.int64) # https:\/\/stackoverflow.com\/questions\/46995041\/why-does-this-array-has-no-attribute-log10?rq=1\n# alpha = np.log10(parameters['alpha'])\n\n# plt.plot(alpha, train_f1, label='Train F1')\n# # this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# # plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\n# plt.plot(alpha, cv_f1, label='CV F1')\n# # this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# # plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\n# plt.scatter(alpha, train_f1, label='Train F1 points')\n# plt.scatter(alpha, cv_f1, label='CV F1 points')\n\n\n\n# plt.legend()\n# plt.xlabel(\"alpha: hyperparameter\")\n# plt.ylabel(\"f1\")\n# plt.title(\"Hyper parameter Vs f1 score plot\")\n# plt.grid()\n# plt.show()\n\n# results.head()","22de9e3b":"# # Please write all the code with proper documentation\n# best_alpha = 0.001\n\n\n# log_reg = SGDClassifier(loss = 'log',alpha=best_alpha, class_weight= 'balanced')\n# log_reg.fit(X_tr, y_train)\n# # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# # not the predicted outputs\n\n# y_train_pred = log_reg.predict(X_tr) \n# y_cv_pred = log_reg.predict(X_cr)    \n# y_test_pred = log_reg.predict(X_te)\n\n# # train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n# # test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n# # plt.plot(train_fpr_3, train_tpr_3, label=\"train AUC =\"+str(auc(train_fpr_3, train_tpr_3)))\n# # plt.plot(test_fpr_3, test_tpr_3, label=\"test AUC =\"+str(auc(test_fpr_3, test_tpr_3)))\n# # plt.legend()\n# # plt.xlabel(\"alpha_3: hyperparameter\")\n# # plt.ylabel(\"AUC\")\n# # plt.title(\"ERROR PLOTS\")\n# # plt.grid()\n# # plt.show()","9e04bfbe":"# y_train_pred","66871655":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Train confusion matrix\")\n\n# f1_score_ = f1_score(y_train,y_train_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_train, y_train_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set, columns=np.unique(y_test), index = np.unique(y_test))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","b9933de4":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"cv confusion matrix\")\n# f1_score_ = f1_score(y_cv,y_cv_pred)\n# print(\"F1_score\",f1_score_)\n# cm_cv_set = confusion_matrix(y_cv,y_cv_pred)\n# print(cm_cv_set)\n# df_cm = pd.DataFrame(cm_cv_set, columns=np.unique(y_cv), index = np.unique(y_cv))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","17439df9":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Test confusion matrix\")\n# f1_score_ = f1_score(y_test,y_test_pred)\n# print(\"F1_score\",f1_score_)\n# cm_test_set = confusion_matrix(y_test,y_test_pred)\n# print(cm_test_set)\n# df_cm = pd.DataFrame(cm_test_set, columns=np.unique(y_test), index = np.unique(y_test))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","6d957f66":"# #selecting the hyperparameter using GridSearch\n# # https:\/\/www.kaggle.com\/arindambanerjee\/grid-search-simplified\n# from sklearn.metrics import f1_score, make_scorer\n# from sklearn.model_selection import GridSearchCV\n\n# xg_clf = XGBClassifier()\n# parameters = {'n_estimators':[5, 10, 50, 100, 200],'max_depth':[2,3, 4, 5, 6, 7]}\n# xg_clf_ = GridSearchCV(xg_clf, parameters, n_jobs= -1, verbose=10, cv=2, scoring=make_scorer(f1_score),return_train_score=True)\n# xg_clf_.fit(X_tr, y_train)\n\n# results2 = pd.DataFrame.from_dict(xg_clf_.cv_results_)\n# # results4 = results4.sort_values(['param_alpha'])\n# max_depth_list = list(xg_clf_.cv_results_['param_max_depth'].data)\n# n_estimator_list = list(xg_clf_.cv_results_['param_n_estimators'].data)\n\n\n# sns.set_style(\"whitegrid\")\n# plt.figure(figsize=(16,6))\n# plt.subplot(1,2,1)\n\n# data = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':xg_clf_.cv_results_['mean_train_score']})\n# data = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('AUC for Training data')\n# plt.subplot(1,2,2)\n\n# # Testing Heatmap\n# data = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':xg_clf_.cv_results_['mean_test_score']})\n# data = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\n# plt.show()\n\n# results2.head()","f31e3b9e":"# # Please write all the code with proper documentation\n# n_estimators = 50\n# max_depth = 6\n\n\n# xg_clf = XGBClassifier(max_depth = max_depth, n_estimators = n_estimators , n_jobs=1)\n# xg_clf.fit(X_tr, y_train)\n# # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# # not the predicted outputs\n\n# y_train_pred = xg_clf.predict(X_tr)    \n# y_cv_pred = xg_clf.predict(X_cr)    \n# y_test_pred = xg_clf.predict(X_te)\n\n# # train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n# # test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n","39599ca7":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Train confusion matrix\")\n# f1_score_ = f1_score(y_train,y_train_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_train, y_train_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","d840ce65":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"cv confusion matrix\")\n# f1_score_ = f1_score(y_cv,y_cv_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_cv, y_cv_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","2a186c29":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Test confusion matrix\")\n# f1_score_1 = f1_score(y_test,y_test_pred)\n# print(\"F1_score\",f1_score_1)\n# cm_test_set = confusion_matrix(y_test, y_test_pred)\n# print(cm_test_set)\n# df_cm = pd.DataFrame(cm_test_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","d4482d07":"# balancig the Imbalanced dataset https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18\nfrom sklearn.model_selection import train_test_split\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.33, stratify=y)\n# X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)\n\nprint(X_train_.shape)\nprint(y_train_.shape)\n# print(X_cv.shape)\n# print(y_cv.shape)\nprint(X_test_.shape)\nprint(y_test_.shape)","a74c0db9":"# X_train_","aae8067d":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['employment_type']),lowercase=False, binary=True)\nX_train_employment_type_one_hot = vectorizer.fit_transform(X_train_['employment_type'].values)\n# X_cv_employment_type_one_hot = vectorizer.transform(X_cv['employment_type'].values)\nX_test_employment_type_one_hot = vectorizer.transform(X_test_['employment_type'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_employment_type_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_employment_type_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_employment_type_one_hot.shape)","f8778dd5":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_experience']),lowercase=False, binary=True)\nX_train_required_experience_one_hot = vectorizer.fit_transform(X_train_['required_experience'].values)\n# X_cv_required_experience_one_hot = vectorizer.transform(X_cv['required_experience'].values)\nX_test_required_experience_one_hot = vectorizer.transform(X_test_['required_experience'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_required_experience_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_experience_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_required_experience_one_hot.shape)","daebca76":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_education']),lowercase=False, binary=True)\nX_train_required_education_one_hot = vectorizer.fit_transform(X_train_['required_education'].values)\n# X_cv_required_education_one_hot = vectorizer.transform(X_cv['required_education'].values)\nX_test_required_education_one_hot = vectorizer.transform(X_test_['required_education'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_required_education_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_education_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_required_education_one_hot.shape)","bf44093c":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['industry']),lowercase=False, binary=True)\nX_train_industry_one_hot = vectorizer.fit_transform(X_train_['industry'].values)\n# X_cv_industry_one_hot = vectorizer.transform(X_cv['industry'].values)\nX_test_industry_one_hot = vectorizer.transform(X_test_['industry'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_industry_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_industry_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_industry_one_hot.shape)","a29b1c34":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['function']),lowercase=False, binary=True)\nX_train_function_one_hot = vectorizer.fit_transform(X_train_['function'].values)\n# X_cv_function_one_hot = vectorizer.transform(X_cv['function'].values)\nX_test_function_one_hot = vectorizer.transform(X_test_['function'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_function_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_function_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_function_one_hot.shape)","d5ab208c":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['country']),lowercase=False, binary=True)\nX_train_country_one_hot = vectorizer.fit_transform(X_train_['country'].values)\n# X_cv_country_one_hot = vectorizer.transform(X_cv['country'].values)\nX_test_country_one_hot = vectorizer.transform(X_test_['country'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_country_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_country_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_country_one_hot.shape)","808c58b4":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['state']),lowercase=False, binary=True)\nX_train_state_one_hot = vectorizer.fit_transform(X_train_['state'].values)\n# X_cv_state_one_hot = vectorizer.transform(X_cv['state'].values)\nX_test_state_one_hot = vectorizer.transform(X_test_['state'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_state_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_state_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_state_one_hot.shape)","c00879a4":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_description_tfidf = vectorizer.fit_transform(X_train_['preprocessed_description'])\n# X_cv_description_tfidf = vectorizer.transform(X_cv['preprocessed_description'])\nX_test_description_tfidf = vectorizer.transform(X_test_['preprocessed_description'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_description_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_description_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_description_tfidf.shape)","2f349aa8":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_denifits_tfidf = vectorizer.fit_transform(X_train_['preprocessed_benefits'])\n# X_cv_denifits_tfidf = vectorizer.transform(X_cv['preprocessed_benefits'])\nX_test_denifits_tfidf = vectorizer.transform(X_test_['preprocessed_benefits'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_denifits_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_denifits_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_denifits_tfidf.shape)","685d4228":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_requirements_tfidf = vectorizer.fit_transform(X_train_['preprocessed_requirements'])\n# X_cv_requirements_tfidf = vectorizer.transform(X_cv['preprocessed_requirements'])\nX_test_requirements_tfidf = vectorizer.transform(X_test_['preprocessed_requirements'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_requirements_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_requirements_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_requirements_tfidf.shape)","ab01432b":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_title_tfidf = vectorizer.fit_transform(X_train_['preprocessed_title'])\n# X_cv_title_tfidf = vectorizer.transform(X_cv['preprocessed_title'])\nX_test_title_tfidf = vectorizer.transform(X_test_['preprocessed_title'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_title_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_title_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_title_tfidf.shape)","e5f50cae":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_profile_tfidf = vectorizer.fit_transform(X_train_['preprocessed_company_profile'])\n# X_cv_profile_tfidf = vectorizer.transform(X_cv['preprocessed_company_profile'])\nX_test_profile_tfidf = vectorizer.transform(X_test_['preprocessed_company_profile'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_profile_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_profile_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_profile_tfidf.shape)","a88e1aa1":"# merge two sparse matrices: https:\/\/stackoverflow.com\/a\/19710648\/4084039\n\n# with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\n# X = hstack((categories_one_hot, sub_categories_one_hot, text_bow, price_standardized))\nX_tr = hstack((X_train_employment_type_one_hot,X_train_required_experience_one_hot,X_train_required_education_one_hot,X_train_industry_one_hot,\n              X_train_function_one_hot,X_train_country_one_hot,X_train_state_one_hot,X_train_description_tfidf,\n              X_train_denifits_tfidf,X_train_requirements_tfidf,X_train_title_tfidf,X_train_profile_tfidf,\n              X_train_['telecommuting'].values.reshape(-1,1),X_train_['has_company_logo'].values.reshape(-1,1),\n               X_train_['has_questions'].values.reshape(-1,1))).tocsr()\n\nX_te = hstack((X_test_employment_type_one_hot,X_test_required_experience_one_hot,X_test_required_education_one_hot,\n              X_test_industry_one_hot,X_test_function_one_hot,X_test_country_one_hot,X_test_state_one_hot,\n              X_test_description_tfidf,X_test_denifits_tfidf,X_test_requirements_tfidf,X_test_title_tfidf,\n              X_test_profile_tfidf,X_test_['telecommuting'].values.reshape(-1,1),\n              X_test_['has_company_logo'].values.reshape(-1,1),X_test_['has_questions'].values.reshape(-1,1))).tocsr()\n\n","1611d7a2":"X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_tr, y_train_)\ny_train_upsample.mean()","65fbd17a":"from collections import Counter\nprint(Counter(y_train_upsample))","828e719c":"kf = KFold(n_splits=10, random_state=42, shuffle=False)","ad18c34f":"params = {'alpha':[0.0001,0.001,0.01,0.1,1]}\ndef score_model(model, params, cv=None):\n    \"\"\"\n    Creates folds manually, and upsamples within each fold.\n    Returns an array of validation (recall) scores\n    \"\"\"\n    if cv is None:\n        cv = KFold(n_splits=5, random_state=42)\n\n    smoter = SMOTE(random_state=42)\n    \n    scores = []\n\n    for train_fold_index, val_fold_index in cv.split(X_tr, y_train_):\n        # Get the training data\n        X_train_fold, y_train_fold = X_tr[train_fold_index], y_train_[train_fold_index]\n        # Get the validation data\n        X_val_fold, y_val_fold = X_tr[val_fold_index], y_train_[val_fold_index]\n\n        # Upsample only the data in the training section\n        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n                                                                           y_train_fold)\n        # Fit the model on the upsampled training data\n        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n        # Score the model on the (non-upsampled) validation data\n        score = f1_score(y_val_fold, model_obj.predict(X_val_fold))\n        scores.append(score)\n    return np.array(scores)\n\n\n# Example of the model in action\n# score_model(RandomForestClassifier, example_params, cv=kf)","bc369fa7":"score_tracker = []\nfor alpha in params['alpha']:\n        example_params = {\n            'alpha': alpha,\n            'random_state': 13,\n            'loss': 'log',\n        }\n        example_params['f1_score'] = score_model(SGDClassifier, \n                                               example_params, cv=kf).mean()\n        score_tracker.append(example_params)\n     \n# What's the best model?\n# print(score_tracker)\nsorted(score_tracker, key=lambda x: x['f1_score'], reverse=True)[0]","c5dece75":"# clf = SGDClassifier(alpha=0.0001,loss='log',random_state=13)\n# clf.fit(X_train_upsample, y_train_upsample)\nprint(\"Train f1 score\",f1_score(y_train_,clf.predict(X_tr))),(\"Test f1 score: \",f1_score(y_test_,clf.predict(X_te)))","8253da8e":"params = {'n_estimators':[5, 10, 50, 100, 200],'max_depth':[2,3, 4, 5, 6, 7]}\ndef score_model(model, params, cv=None):\n    \"\"\"\n    Creates folds manually, and upsamples within each fold.\n    Returns an array of validation (recall) scores\n    \"\"\"\n    if cv is None:\n        cv = KFold(n_splits=5, random_state=42)\n\n    smoter = SMOTE(random_state=42)\n    \n    scores = []\n\n    for train_fold_index, val_fold_index in cv.split(X_tr, y_train_):\n        # Get the training data\n        X_train_fold, y_train_fold = X_tr[train_fold_index], y_train_[train_fold_index]\n#         print('X_train_fold',X_train_fold.shape)\n        # Get the validation data\n        X_val_fold, y_val_fold = X_tr[val_fold_index], y_train_[val_fold_index]\n#         print('X_val_fold',X_val_fold.shape)\n\n        # Upsample only the data in the training section\n        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n                                                                           y_train_fold)\n#         print(\"X_train_fold_upsample\",X_train_fold_upsample.shape)\n        # Fit the model on the upsampled training data\n#         print(model)\n#         clf = model()\n#         print(params)\n        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n        # Score the model on the (non-upsampled) validation data\n#         print('X_val_fold',X_val_fold.shape)\n#         print('y_val_fold',y_val_fold.shape)\n        score = f1_score(y_val_fold, model_obj.predict(X_val_fold))\n        scores.append(score)\n    return np.array(scores)\n\n# Example of the model in action\n# score_model(RandomForestClassifier, example_params, cv=kf)","73cf26ee":"# score_tracker = []\n# for n_estimator in params['n_estimators']:\n#     for max_depth in params['max_depth']:\n#         example_params = {\n#             'n_estimators': n_estimator,\n#             'max_depth': max_depth,\n#             'random_state': 13,\n            \n#         }\n#         example_params['f1_score'] = score_model(XGBClassifier, \n#                                                example_params, cv=kf).mean()\n#         score_tracker.append(example_params)\n     \n# # What's the best model?\n# print(score_tracker)\n# sorted(score_tracker, key=lambda x: x['f1_score'], reverse=True)[0]","6a697ce6":"from imblearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\n","81294fb4":"kf = KFold(n_splits=5, shuffle=False)\n# from sklearn.neighbors import KNeighborsClassifier\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('classifier',SGDClassifier())])\n\nimba_pipeline","86f001df":"imba_pipeline.get_params().keys()","fb846e1c":"# y_train_upsample,clf.predict(X_train_upsample)\n# cross_val_score(imba_pipeline, X_tr, y_train_, scoring=make_scorer(f1_score), cv=kf)","457ac354":"params = {\n    'alpha': [0.0001,0.001,0.01,0.1,1],\n    'loss':['log'],\n    'penalty':['l1','l2'],\n    'random_state': [13]\n}\nnew_params = {'classifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","47576336":"# estimator.get_params().keys()","a80458db":"grid_imba.cv_results_['mean_test_score'], grid_imba.cv_results_['mean_train_score']","7b0337ae":"#selecting the hyperparameter using GridSearch\n# https:\/\/www.kaggle.com\/arindambanerjee\/grid-search-simplified\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame.from_dict(grid_imba.cv_results_)\n\n# results4 = results4.sort_values(['param_alpha'])\nalpha = list(grid_imba.cv_results_['param_classifier__alpha'].data)\n# loss = list(grid_imba.cv_results_['param_classifier__loss'].data)\n\ntrain_f1= results['mean_train_score']\n# train_auc_std= results['std_train_score']\ncv_f1 = results['mean_test_score'] \n# cv_auc_std= results['std_test_score']\n# K =  results['param_classifier__alpha']\nalpha = np.log10(alpha)\nprint(params['alpha'])\nprint(alpha_)\nprint(train_f1)\n\nplt.plot(alpha, train_f1, label='Train f1 score')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_f1, label='CV f1 score')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\n# plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_f1, label='Train f1 score')\nplt.scatter(alpha, cv_f1, label='CV f1 score')\n\n\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"F1 SCORE\")\nplt.title(\"Hyper parameter Vs F1 plot\")\nplt.grid()\nplt.show()\n\n\nsns.set_style(\"whitegrid\")\n# plt.figure(figsize=(16,6))\n# plt.subplot(1,2,1)\n\n# data = pd.DataFrame(data={'alpha':alpha, 'loss':loss, 'f1_score':grid_imba.cv_results_['mean_train_score']})\n\n# data = data.reset_index().pivot_table(index='alpha', columns='loss', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Training data')\n# plt.subplot(1,2,2)\n\n# # Testing Heatmap\n# data = pd.DataFrame(data={'alpha':alpha, 'loss':loss, 'f1_score':grid_imba.cv_results_['mean_test_score']})\n# data = data.reset_index().pivot_table(index='alpha', columns='loss', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\n# plt.show()\n\nresults.head()","5b23c988":"grid_imba.best_score_","46706101":"grid_imba.best_params_","8cd0207a":"clf = SGDClassifier(alpha=0.001,loss='log',random_state=13)\nclf.fit(X_train_upsample, y_train_upsample)\nprint(\"Train f1 score\",f1_score(y_train_upsample,clf.predict(X_train_upsample))),(\"Test f1 score: \",f1_score(y_test_,clf.predict(X_te)))","8649ecd4":"y_test_predict = grid_imba.best_estimator_.predict(X_te)\ny_train_predict = grid_imba.best_estimator_.predict(X_tr)","d106c9fa":"f1_score(y_test_, y_test_predict)","72b9f294":"f1_score(y_train_, y_train_predict)","1875e475":"f1_score(y_train_upsample,clf.predict(X_train_upsample))","2a0b0078":"kf = KFold(n_splits=5, shuffle=False)\nfrom sklearn.neighbors import KNeighborsClassifier\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('xgbclassifier',XGBClassifier())])\n\nimba_pipeline","39ae325b":"params = {\n    'n_estimators':[5, 10, 50, 100, 200],\n    'max_depth':[2,3, 4, 5, 6, 7]\n}\nnew_params = {'xgbclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","15e08cfa":"grid_imba.cv_results_['mean_test_score'], grid_imba.cv_results_['mean_train_score']","5080609e":"#selecting the hyperparameter using GridSearch\n# https:\/\/www.kaggle.com\/arindambanerjee\/grid-search-simplified\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\nresults2 = pd.DataFrame.from_dict(grid_imba.cv_results_)\n\n# results4 = results4.sort_values(['param_alpha'])\nmax_depth_list = list(grid_imba.cv_results_['param_xgbclassifier__max_depth'].data)\nn_estimator_list = list(grid_imba.cv_results_['param_xgbclassifier__n_estimators'].data)\n\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,1)\n\ndata = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':grid_imba.cv_results_['mean_train_score']})\ndata = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\nsns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Training data')\nplt.subplot(1,2,2)\n\n# Testing Heatmap\ndata = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':grid_imba.cv_results_['mean_test_score']})\ndata = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\nsns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\nplt.show()\n\nresults2.head()","015bf599":"grid_imba.best_score_","9ca8dcff":"grid_imba.best_params_","c6b8d112":"y_test_predict = grid_imba.best_estimator_.predict(X_te)\ny_train_predict = grid_imba.best_estimator_.predict(X_tr)","d09e09c7":"f1_score(y_test_, y_test_predict)","711333f0":"f1_score(y_train_, y_train_predict)","90cfbc2a":"f1_score(y_train_upsample,clf.predict(X_train_upsample))","2dbddf51":"from sklearn.metrics import recall_score, roc_auc_score\nprint(roc_auc_score(y_train_,grid_imba.predict_proba(X_tr)[:,1]))\n# print ('Cross validation AUC for Random Forest model : ',np.mean(cross_val_score(grid_imba,X_tr,y_train_,scoring='roc_auc',cv=10)))\nprint(roc_auc_score(y_test_,grid_imba.predict_proba(X_te)[:,1]))","7049323a":"kf = KFold(n_splits=5, shuffle=False)\n# from sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('lgbclassifier',lgb.LGBMClassifier())])\n\nimba_pipeline","1d68c0f0":"params = {'num_leaves':[6,8,12,16],'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n          'n_estimators':[5, 10, 50, 100, 200],'max_depth':[3,5,10]}\n\n# params = {\n#     'n_estimators':[5, 10, 50, 100, 200],\n#     'max_depth':[2,3, 4, 5, 6, 7]\n# }\nnew_params = {'lgbclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","89b170cf":"### Which industry has most common in fraud job post","3d4adfb9":"### Observation\nIn the non fraudulent job post also more than 59% of total job post do not specify there department so here we cannot clearly say that if the department is specified as other then the job post is fraud.","fac883f9":"## Onehot encoding ","dfd6f596":"### Checking the missing value ","9c03f5fe":"### required_education","87e91bc3":"### Logistic regression","50ca3aed":"By looking at the count of number of data point belongs to each class we can clearly see that the dataset is highly imbalaced in nature and hence we have to balance it using various balancing techniques","5e5d425e":"### title","72c5137b":"### Company profile","2379afd5":"### Description","d76980b3":"# Business problem\nWe want to Identify wheater the job posting given to us is the geniune or a fraudulent","2fe86491":"### Employment type on job post fraud","9837575d":"### Imputating the categorical feature\nnow to all the categorical data we will replace it with the most frequent occuring data","1a5d3ee0":"### Checking the data balance after Imputation","61ba5989":"### Observation\nThere are around 18k data with 17 feature","1714b940":"### state","e991d1f3":"### Which industry is most common in the non fraud job post","cf5f249a":"we can se that we have total of 6 categorical feature I am not counting the fraudulent col as that is our target class.","d213ff24":"### State","3a383ea4":"## Onehot encoding of the categorical data","12cd174b":"### Observation\n\nLooking at the count plot of the industry type in the non fraudulent job post we can see that the high number of job post does not specify the industry count.","5cdcc6e0":"### Identiyfing the categorical features ","634a3d1f":"### lightgbm","08c4eb3c":"### Benifits","b7a9602b":"### function","36fac416":"### Presence of company logo on job post on fraud post","5f333890":"### Employment type","11bdcd9a":"#### Getting the state name from the location","549135a6":"### Requirements","3844cd76":"### Imputating the non categorical feature","567aa394":"### Requirements","b2b3fc8e":"# Loading the data","5a53fe2b":"### Observation\nBy lookig at the plot of department of fraudulent post we can clearly see that the most fraud post there is no deparment specified more than 64% of total fraudlent post dont specify there department.","3aecdbb0":"## Vectorizing the processed text data","734761a3":"### Title","808fa5cf":"### Which department is most common in the non fraud job post","7195ca26":"### Which function is most common in the fraud job post and non fraud job post","c5daf8ed":"### DATA CLEANING AND FEATURE ENGINEEING OF LOCATION COLUMN","68474dab":"### Industry","0a150499":"## Observation\n\nWe can see in the plot that the job post which have education requirement as bachelors degree contribute more to the fraudulent post","0464a028":"### Which state is the most common in the non fraud job post","bd8a6b4c":"### Separating all the fraud and no fraud data for some analysis on our newly created feature","cfe74e9f":"### industry","a71a6ab3":"### Benifits","72647822":"# EDA","d1fa598f":"### Benefits","3e63f276":"### Required Experience","3a1380b7":"### Observation\nWe cannot make a descision after looking at the countplot of the function in case of the job post to be fraud or geniuine.","47ee2cb2":"### Observation\nThe fraudulent post also do not specify the industry type and it is vast in number but if we see carefully the number of post with oil and energy , Accounting etc is higher than that of non fraudulent post hence we can use industry as feature.","57018fd0":"## Observation\n\nThe mid senior level work exprience job posting have more fraudulent job post then any other","c8ad4025":"#### Getting the city name from the location","b48aa594":"### Observation\nMost geniune job post have the title as the english teacher.","6c0c1f74":"### Observation\nIn the plot of titles in the fraud post we can see the titles like URGENT, Data Entry or customer care have most the count towards fraud post.","ced9f441":"### Eemployment type","ee6cb664":"### country","43b8f0be":"Now since the dataset contains missing value and it has 17 columns and feature we have to first identitfy which of them is catgorical and which of them are not to do so I am going to count the number of unique value in the cols and will set some threshold and if the col have unique values less than the threshold means that column is categorical","b9f2ce7f":"### Which state is the most common in the fraud job post","e684a041":"### Which country is most common in the fraud job post","10a293b0":"## Imputation of missing values","68a3e0a2":"### Observation\nJob post which have the company logo in it has less number of faudulent casses then the one which do not have the company logo which is like a natural thing to see.","90f13b52":"### Which country is most common in the non fraud job post","efa6b108":"### How balance the dataset is","a1c44de9":"# Machine learning problem\nThis one is a classical Classifiation problem were we have to predict weater the given input belongs to the class 0 or class 1 here the input is the job post and the we have to clasify is it a geniune opening or a fraud opening.","dd0698cf":"### Observation\nWe can see that is the screening questions are present then there is less number of fraudulent job compare to the job posting where not screening questions are present.","ed44e016":"### preprocessed_description","0e8e4db7":"### Observation\nThis is intereseting the to see that most of the authentic job post in the us does not specify the state in there location.","fc0d3687":"#### Which Department is the most common in the fraud job posting","246c1126":"### Observation\nIn US also the texas is the state were most fraud job post were present","5ecb997d":"#### Getting the country name from the job location","ef87afc1":"### function","ecac5332":"## Univarient Analysis","6457bbac":"Mejority of the columns in this data has missing value as we can see the percent of missing value in each columns\/feature so we have to decide either we have to imuputate the missing data or we have to drop the feature","9b5f67e5":"### Observation\nFor the non telecomunicating position there is fraudulent post then the telecomunicating position.","c2046045":"# Performance metric\nThe performance metric that I am going to use it in this is F1 score.","25a008e1":"### Required Education on job post fraud","ef080f6d":"### Required Experience on job post fraud","6794b415":"### Observation\nUS is the country with maximum fraud post.","841e7dcc":"### required_experience","4f922356":"## Splitting the data","49d5f356":"### company profile","cc8f0374":"## Text Preprocessing","c2b9a308":"### Which title of the job is common in the most fraud job post","2ad3146e":"# Lets try balancing the data","4699cc34":"### Requirement","654a9784":"### Required Education","e8720b67":"### Which title of the job is common in the most non fraud job post","ae61b452":"### Title","b61775d4":"### Which title of the job is common in the most fraud job post","f6b4491c":"# importing the libraries","b2a13922":"### Company Profile","6fbb63cf":"### XGBOOST Classifier","7328d8a3":"### Telecommuting Education on job post fraud","af0805bc":"Now we are done with our missing value and now we have no missing data feature so we can move futher in our analysis","faedc6fb":"### Presence of screening question on job post fraud","63237825":"Now It is time to imputate the missing value data. You must remember that we have calculated the percenteage of mising value of data so we set a threshold using that and drop the column having percentage more than threshold.","e035d219":"## Vectorizing the preprocessed text data","7abcf3ec":"### Description","40004a38":"### Observation\n\nBy observing the count plot of the employment type we can make a conclusion that expect of employment type full time there is no other types that contribute to the fraudulent job post.","2d852be0":"### Observation\n Also US is the country with most geniune job post hence we can say that most of the job post were posted in the US then any other country in the world.","586133c8":"### Country"}}