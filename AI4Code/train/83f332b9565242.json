{"cell_type":{"ec1250c4":"code","a72f79bd":"code","855916b0":"code","f75b1539":"code","f3578365":"code","cd8da51b":"code","213291d7":"code","010b3258":"code","77b7437f":"code","dfc7009b":"code","fa7cbafd":"code","467868b8":"code","2d76edec":"markdown","265f5ac4":"markdown","f5368498":"markdown","6f8dc699":"markdown","cc83dd77":"markdown","241fc87d":"markdown","295936ec":"markdown","9ef0a70b":"markdown","4792572e":"markdown","cd55260d":"markdown","87af2dc4":"markdown"},"source":{"ec1250c4":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\n\npd.set_option('display.max_columns', 200)","a72f79bd":"train_df = pd.read_csv('..\/input\/train.csv')\n\ntest_df = pd.read_csv('..\/input\/test.csv')","855916b0":"train_df.head()","f75b1539":"test_df.head()","f3578365":"train_df.target.value_counts()","cd8da51b":"#parameters were obtained using the same structure presented in the following kernel:\n\n#   https:\/\/www.kaggle.com\/fayzur\/lgb-bayesian-parameters-finding-rank-average\n\nparam = {\n    'num_leaves': 18,\n     'max_bin': 63,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 49,\n     'save_binary': True,\n     'seed': 1337,\n     'feature_fraction_seed': 1337,\n     'bagging_seed': 1337,\n     'drop_seed': 1337,\n     'data_random_seed': 1337,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}\n","213291d7":"nfold = 10","010b3258":"target = 'target'\npredictors = train_df.columns.values.tolist()[2:]","77b7437f":"gc.collect()","dfc7009b":"skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    nround = 8523\n    clf = lgb.train(param, xg_train, nround, valid_sets = [xg_valid], verbose_eval=250)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    \n    predictions += clf.predict(test_df[predictors], num_iteration=nround) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","fa7cbafd":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df[:10]","467868b8":"sub_df.to_csv(\"Customer_Transaction.csv\", index=False)","2d76edec":"The problem is unbalance!\nWe can build a quick model on this dataset considering unbalance to see how far we can go without Feature engineering! ","265f5ac4":"The above parameters were obtained using the same structure presented in the following kernel:\n\nhttps:\/\/www.kaggle.com\/fayzur\/lgb-bayesian-parameters-finding-rank-average","f5368498":"Upvote if it is useful :)","6f8dc699":"Test dataset:","cc83dd77":"<a id=\"1\"><\/a> <br>\n## 2. Training the model","241fc87d":"Number of Kfolds:","295936ec":"We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:","9ef0a70b":"<a id=\"0\"><\/a> <br>\n## 1. Loading the data","4792572e":"<a id=\"2\"><\/a> <br>\n## 2. Submission","cd55260d":"Distribution of target variable","87af2dc4":"# Santander Customer Transaction Prediction\n\nThis kernel uses LGBM model to predict Customer Transaction.\n\n**For LightGBM parameters optimization, please find my other kernel below, where I show how to take advantage of Bayesian Optimization to find optimal paramer ofr LightGBM:**\n\nhttps:\/\/www.kaggle.com\/fayzur\/lgb-bayesian-parameters-finding-rank-average\n\n\n## Notebook  Content\n1. [Loading the data](#0) <br>    \n1. [Training the model](#1)\n1. [Submission](#2)"}}