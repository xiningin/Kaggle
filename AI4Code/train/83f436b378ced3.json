{"cell_type":{"a45d13ed":"code","e29d20a4":"code","46825c10":"code","1fd4d3d2":"code","c308c5f3":"code","a39acd0f":"code","bb7882a2":"code","810145d8":"code","129e10b4":"code","9fd073a0":"code","b8f4f7f3":"code","b38ff4fc":"code","f3d4bc3a":"code","237d3735":"code","e8a94286":"code","44ee246f":"code","5f6276b7":"code","76337521":"code","5dca9445":"code","3925c126":"code","3b036d25":"code","b941fbc8":"code","e0201b06":"code","50d7c627":"code","665ca889":"code","9869713d":"code","45e093d2":"code","e5097b64":"code","ef377c4d":"code","16559718":"code","e7986ced":"code","f24752fc":"code","b47c95e3":"code","bc9c1b6a":"code","06784ca9":"code","b8d0863e":"code","a75b7122":"code","4daf0f49":"code","df132895":"code","6786d664":"code","7ad8b276":"code","b76b6217":"code","a8f88a08":"code","84d7d4e7":"code","8ca78384":"code","a470a3c6":"code","697e7636":"code","0c2b0bad":"code","a364c506":"code","46b42235":"markdown","2683886c":"markdown","e0731b9d":"markdown","acd0df5c":"markdown","81620400":"markdown","c76b1f72":"markdown","36d5fb06":"markdown","5505a975":"markdown","71971502":"markdown","22cad920":"markdown","d4df600d":"markdown","683a0b82":"markdown","401fd2fe":"markdown","0ccaa284":"markdown","cd9f254c":"markdown","00859cf6":"markdown","229e1f65":"markdown","654718fb":"markdown","7c13563e":"markdown","10feac1a":"markdown","63cbbcb2":"markdown","3a78944e":"markdown"},"source":{"a45d13ed":"# import os\n# __print__ = print\n# def print(string):\n#     os.system(f'echo \\\"{string}\\\"')\n#     __print__(string)","e29d20a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\n\nfrom time import time\n\nfrom collections import Counter\nfrom itertools import combinations\n\nfrom sklearn.model_selection import cross_val_score,cross_validate, train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n\nfrom category_encoders import CountEncoder, CatBoostEncoder, TargetEncoder\n\nfrom mlxtend.classifier import StackingCVClassifier, StackingClassifier\nfrom mlxtend.feature_selection import ColumnSelector\n\n# import lightgbm\nfrom lightgbm import LGBMClassifier, plot_importance\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n\nfrom tqdm import tqdm\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\ndef show_sys_var():\n    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                             key= lambda x: -x[1])[:10]:\n        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n        \ndef mem_reduce(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    return df\n\n\npd.options.display.max_seq_items = 500\n\n# Any results you write to the current directory are saved as output.\n","46825c10":"X = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\nX_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\n\ny = X['Cover_Type'] # this is the target\nX = X.drop('Cover_Type', axis = 1)\n\ntest_index = X_test.index\nnum_train = len(X)\n\nprint('Train set shape : ', X.shape)\nprint('Test set shape : ', X_test.shape)\n\n#renaming to avoid long names in the feature engineering\nX.rename({'Horizontal_Distance_To_Roadways':'HDR',\n              'Horizontal_Distance_To_Hydrology':'HDH',\n              'Horizontal_Distance_To_Fire_Points':'HDF',\n              'Vertical_Distance_To_Hydrology':'VDH'}, axis=\"columns\", inplace=True)\nX_test.rename({'Horizontal_Distance_To_Roadways':'HDR',\n              'Horizontal_Distance_To_Hydrology':'HDH',\n              'Horizontal_Distance_To_Fire_Points':'HDF',\n              'Vertical_Distance_To_Hydrology':'VDH'}, axis=\"columns\", inplace=True)\n\n\nraw_columns = X.columns\ncategorial_feat = [] \nbinary_feat = [] \nX.head()\n\n","1fd4d3d2":"# Helper function to generate submission files.\ndef to_submission(preds, file_name):\n    output = pd.DataFrame({'Id': test_index,\n                           'Cover_Type': preds})\n    output.to_csv(file_name+'.csv', index=False)\n","c308c5f3":"# Some getters to eacily grab new models later.\n\ndef get_XGB(n_estimators= 500, random_state= 2019):\n    return XGBClassifier( n_estimators=n_estimators, \n                    learning_rate= 0.1, \n                    max_depth= 200,  \n                    objective= 'binary:logistic',\n                    random_state= random_state,\n                    n_jobs=-1)\n\ndef get_LGBM(n_estimators=500, random_state= 2019):\n    return LGBMClassifier(n_estimators=n_estimators,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= random_state,\n                     n_jobs=-1)\n\ndef get_RF(n_estimators = 750, random_state = 2019):\n    return RandomForestClassifier(n_estimators = n_estimators, \n                            max_depth = None, \n                            random_state=random_state,\n                            n_jobs=-1)\n\ndef get_EXT(n_estimators = 750, random_state = 2019):\n    return ExtraTreesClassifier(n_estimators = n_estimators, \n                            max_depth = None, \n                            random_state=random_state,\n                            n_jobs=-1)\n\ndef get_CB (n_estimators = 200, random_state = 2019, cat_features = None):\n    return CatBoostClassifier(n_estimators = n_estimators, \n                              max_depth = None,\n                              learning_rate=0.3,\n                              random_state=random_state, \n                              cat_features = cat_features,\n                              verbose=False)\n\ndef get_LR():\n    return LogisticRegression(max_iter=1000, \n                       n_jobs=-1,\n                       solver= 'lbfgs', #?\n                       multi_class = 'multinomial')\n\n\n","a39acd0f":"print('Missing Label? ', y.isnull().any())\nprint('Missing train data? ', X.isnull().any().any())\nprint('Missing test data? ', X_test.isnull().any().any())","bb7882a2":"print (X.dtypes.value_counts())\nprint (X_test.dtypes.value_counts())","810145d8":"#transform Soil_Type into categorial\ndef categorify(df, col_string_search, remove_original=False):\n    cols = df.columns\n    for key_str in col_string_search:\n        new_col_name = key_str+'_cat'\n        df[new_col_name]=0\n        for col in cols:\n            if ~str(col).find(key_str):\n                binary_feat.append(col)\n                df[new_col_name]= df[new_col_name]+int(str(col).lstrip(key_str))*df[col]\n                if remove_original:\n                    df.drop(col, axis=1, inplace=True)\n        categorial_feat.append(new_col_name)\n#         df[new_col_name] = df[new_col_name].astype('category')\n        \n#keeping track of the categorial features\n    return df","129e10b4":"cols_to_categorify = ['Soil_Type', 'Wilderness_Area']\n\nall_data = X.append(X_test)\nall_data = categorify(all_data, cols_to_categorify, remove_original=False)\n\nX= all_data.loc[:num_train]\nX_test= all_data.loc[num_train+1:]\n\n\nX_test.head()","9fd073a0":"X.describe()","b8f4f7f3":"# for col in X.columns:\nplt.figure(figsize=(15,5))\nsns.distplot(X.Hillshade_3pm)\nplt.show()","b38ff4fc":"print(X.Hillshade_3pm[(X.Hillshade_3pm<130).to_numpy() &  (X.Hillshade_3pm>120).to_numpy()].value_counts())\nprint((X.Hillshade_3pm==0).sum())\nprint((X_test.Hillshade_3pm==0).sum())\n","f3d4bc3a":"# checking which features correlates with Hillshade_3pm\ncorr = X[X.Hillshade_3pm!=0].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr,annot=True)","237d3735":"#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nall_data = X.append(X_test)\n\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_zero.shape\n\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n# res = cross_val_score(RandomForestRegressor(n_estimators=100), HS_train.drop('Hillshade_3pm',axis=1), HS_train.Hillshade_3pm, n_jobs=-1, verbose=True)\n# print(res) --> Output:  #[0.9996774  0.99989463 0.9999186 ]\n##actually, the CV is so close to perfect that there is actually no new information here..keeping it for .. sanity ?\n\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\nall_data.loc[HS_zero.index,'Hillshade_3pm'] = out\n\nX['Hillshade_3pm']= all_data.loc[:num_train,'Hillshade_3pm']\nX_test['Hillshade_3pm']= all_data.loc[num_train+1:,'Hillshade_3pm']\n\ndel(HS_train)","e8a94286":"baseline_cols = X.columns","44ee246f":"\nt = time()\n\npca = PCA(n_components=0.99).fit(all_data)\ntrans = pca.transform(all_data)\nprint(trans.shape)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    X[col_name] = trans[:num_train, i]\n    X_test[col_name] = trans[num_train:, i]\n\nprint('duration: '+ str(time()-t))\n","5f6276b7":"# Adding Gaussian Mixture features to perform some unsupervised learning hints from the full data\n# https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\n\nt = time()\ncomponents = 10 # TODO check other numbers.  with 10 labels there are a few ones with 0 importances. \ngmix = GaussianMixture(n_components=components) \ngaussian = gmix.fit_predict(StandardScaler().fit_transform(all_data))\n\nX['GM'] = gaussian[:num_train]\nX_test['GM'] = gaussian[num_train:]\n\ncategorial_feat.append('GM')\n\nfor i in range(components):\n    X['GM'+str(i)] = gaussian[:num_train]==i  \n    X_test['GM'+str(i)] = gaussian[num_train:]==i\n    binary_feat.append('GM'+str(i))\n\nprint('duration: '+ str(time()-t))","76337521":"sns.violinplot(x='GM',y=y, data=X)\n","5dca9445":"X.head()","3925c126":"# Helper function to generate some basic FE\ndef quick_fe(df, cols, operations, max_combination=2):\n    \n    if max_combination>=2:\n        for col1, col2 in combinations(cols, 2):\n            for ope in operations:\n                if ope=='add': df[col1 + \"_add_\" + col2] = df[col1]+df[col2]\n                elif ope=='minus': df[col1 + \"_minus_\" + col2] = df[col1]-df[col2]\n                elif ope=='minabs': df[col1 + \"_minabs_\" + col2] = abs(df[col1]-df[col2])\n                elif ope=='time': df[col1 + \"_time_\" + col2] = df[col1]*df[col2]\n    if max_combination>=3:\n        for col1, col2, col3 in combinations(cols, 3):\n            for ope in operations:\n                if ope=='add': df[col1 + \"_\" + col2 + \"_\" + col3+ \"_add\" ] = df[col1]+df[col2]+df[col3]\n                elif ope=='time': df[col1 + \"_\" + col2+ \"_\" + col3+ \"_time\" ] = df[col1]*df[col2]*df[col3]\n    return df\n\nX.head()","3b036d25":"\n# group all the FE features\ndef feature_eng(dataset):\n    # https:\/\/www.kaggle.com\/nadare\/eda-feature-engineering-and-modeling-4th-359#nadare's-kernel\n    #https:\/\/www.kaggle.com\/lukeimurfather\/adversarial-validation-train-vs-test-distribution\n    #https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\n    \n    dataset['Distance_hyd'] = (dataset['HDH']**2+dataset['VDH']**2)**0.5\n\n    cols_to_combine = ['Elevation','HDH', 'HDF', 'HDR', 'VDH']\n    dataset = quick_fe(dataset, cols_to_combine, ['add','time','minus', 'minabs'], max_combination=3)\n\n#     cols_to_combine = ['Elevation', 'VDH']\n#     dataset = quick_fe(dataset, cols_to_combine, ['add','time','minus', 'minabs'], max_combination=2)\n\n    dataset['Mean_Distance']=(dataset.HDF + \n                               dataset.Distance_hyd + \n                               dataset.HDR) \/ 3 \n    dataset['Elevation_Adj_distanceH'] = dataset['Elevation'] - 0.25*dataset['Distance_hyd']\n    dataset['Elevation_Adj_distanceV'] = dataset['Elevation'] - 0.19*dataset['HDH']\n\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    dataset = quick_fe(dataset,hillshade_col, ['add','minus'], max_combination=3)\n\n    dataset[\"Hillshade_std\"] = dataset[hillshade_col].std(axis=1)\n    dataset[\"Hillshade_max\"] = dataset[hillshade_col].max(axis=1)\n    dataset[\"Hillshade_min\"] = dataset[hillshade_col].min(axis=1)\n   \n    #Aspect\n    dataset['Aspect'] = dataset['Aspect'].astype(int) % 360\n\n    dataset['Sin_Aspect'] = np.sin(np.radians(dataset['Aspect']))\n    dataset['Cos_Aspect'] = np.cos(np.radians(dataset['Aspect']))\n    \n    dataset['Slope_hyd'] = np.arctan(dataset['VDH']\/(dataset['HDH']+0.001))\n    dataset.Slope_hyd=dataset.Slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    dataset['Sin_Slope_hyd'] = np.sin(np.radians(dataset['Slope_hyd']))\n    dataset['Cos_Slope_hyd'] = np.cos(np.radians(dataset['Slope_hyd']))\n\n    dataset['Sin_Slope'] = np.sin(np.radians(dataset['Slope']))\n    dataset['Cos_Slope'] = np.cos(np.radians(dataset['Slope']))\n    \n    # extremely stony = 4, very stony = 3, stony = 2, rubbly = 1, None = 0\n    Soil_to_stony = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1,\n                1, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n                0, 4, 4, 4, 4, 4, 3, 4, 4, 4, \n                4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n    dataset['Stonyness'] = [Soil_to_stony[x] for x in (dataset['Soil_Type_cat'].astype(int)-1)]\n    dataset.loc[:,'Extremely_Stony']= dataset['Stonyness']==4\n    dataset.loc[:,'Very_Stony']= dataset['Stonyness']==3\n    dataset.loc[:,'Stony']= dataset['Stonyness']==2\n    dataset.loc[:,'Rubbly']= dataset['Stonyness']==1\n    dataset.loc[:,'Stony_NA']= dataset['Stonyness']==0\n    \n    return dataset\n\ncategorial_feat.append('Stonyness')\nbinary_feat.append('Extremely_Stony')\nbinary_feat.append('Very_Stony')\nbinary_feat.append('Stony')\nbinary_feat.append('Rubbly')\nbinary_feat.append('Stony_NA')\n\nX = feature_eng(X)\nX_test = feature_eng(X_test)\n\ncolumns = X.columns","b941fbc8":"def combine_features(df, col1, col2):\n    new_name= str(col1)+'_'+str(col2)\n    mixed = df[col1].astype(str)+'_'+df[col2].astype(str)\n#     print(mixed.head())\n    label_enc = LabelEncoder()\n    df[new_name] = label_enc.fit_transform(mixed)\n    return df","e0201b06":"plt.figure(figsize=(15,8))\nsns.violinplot(x='GM',y='Wilderness_Area_cat', data=X)","50d7c627":"all_data = X.append(X_test)\nall_data = combine_features(all_data, 'Stonyness', 'Wilderness_Area_cat')\nall_data = combine_features(all_data, 'Soil_Type_cat', 'Wilderness_Area_cat')\nall_data = combine_features(all_data, 'GM', 'Wilderness_Area_cat')\nall_data = combine_features(all_data, 'GM', 'Soil_Type_cat')\nX = all_data[:num_train]\nX_test = all_data[num_train:]\ncategorial_feat.append('Stonyness_Wilderness_Area_cat')\ncategorial_feat.append('Soil_Type_cat_Wilderness_Area_cat')\ncategorial_feat.append('GM_Wilderness_Area_cat')\ncategorial_feat.append('GM_Soil_Type_cat')","665ca889":"plt.figure(figsize=(15,8))\nsns.violinplot(x=y,y=all_data[:num_train].Stonyness_Wilderness_Area_cat)","9869713d":"plt.figure(figsize=(15,8))\nsns.violinplot(x='Soil_Type_cat',y='Wilderness_Area_cat', data=X)","45e093d2":"all_data = X.append(X_test)\n\ncount_enc = CountEncoder()\ncount_encoded = count_enc.fit_transform(all_data[categorial_feat].astype(str))\n\nall_data = all_data.join(count_encoded.add_suffix(\"_count\"))\n\nX = all_data[:num_train]\nX_test = all_data[num_train:]\n\ndel(all_data)","e5097b64":"X.describe()","ef377c4d":"# Create the encoder itself\ntarget_enc = TargetEncoder(cols=categorial_feat)\n\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(X[categorial_feat], y) \n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\nX = X.join(target_enc.transform(X[categorial_feat]).add_suffix('_target'))\nX_test = X_test.join(target_enc.transform(X_test[categorial_feat]).add_suffix('_target'))\n","16559718":"#TODO : target encode high importance values?","e7986ced":"columns = X.columns\nX_test.head()","f24752fc":"X = mem_reduce(X)\nX_test = mem_reduce(X_test)","b47c95e3":"all_features = X.columns\nadded_features = X.columns.drop(baseline_cols).drop(binary_feat, errors='ignore') #droping binary and keep only categorial..maybe should not ?\nall_minus_binary_features = X.drop(binary_feat, axis=1).columns\ncategorized_baseline = baseline_cols.drop(binary_feat, errors='ignore')","bc9c1b6a":"#Change here to modify the remaining  notebook classifier of reference.\n# some constant to be low for kernel editing\/ code checking and higher for commit\/real training.\n\nCLASSIFIER = get_CB  #get_LGBM   # The classifier to test the feature with\nESTIMATORS = 75      # number of estimators for the classifier testing features (don't take too high or the testing will take ages)\nMODEL_FACTOR = 1     # a model factor to scale the finale model for offline commit (large) or debugging\/coding (small)\nCV = 6 #2            # CV\n\ndef get_clf(n_estimators=ESTIMATORS, random_state= 2019):\n    return CLASSIFIER( n_estimators=n_estimators, random_state = random_state)\n\ndef check_selected_features_score(features, n_estimators=ESTIMATORS, verbose=False):\n    return np.mean(cross_val_score(get_clf(n_estimators=n_estimators), \n                                   pd.DataFrame(X[features]), \n                                   y, \n                                   cv=CV, \n                                   verbose=verbose))","06784ca9":"raw_score = check_selected_features_score(raw_columns)\nraw_cat_score = check_selected_features_score(categorized_baseline)\nno_bin_score = check_selected_features_score(all_minus_binary_features)\nfull_feature_score = check_selected_features_score(all_features)\n\nprint('Raw score : ', raw_score)\nprint('Categorized Raw score : ', raw_cat_score)\nprint('No binary Features score : ', no_bin_score)\nprint('All Features score : ', full_feature_score)\n","b8d0863e":"def only_add_improving_features():\n    score= pd.Series(index=added_features)\n    for feat in added_features:\n        cols = X[baseline_cols.drop(binary_feat, errors='ignore')].columns.to_list()\n        cols.append(feat)\n        score[feat] = check_selected_features_score(cols)\n        print(feat, score[feat], score[feat]>raw_cat_score )\n\n    cols =  X[baseline_cols.drop(binary_feat, errors='ignore')].columns\n    improving_features = added_features[score>raw_cat_score]\n    return cols.append(improving_features)","a75b7122":"# improving_raw_features = only_add_improving_features()\n\n# print(improving_raw_features)","4daf0f49":"\ndef check_individual_score_and_add():\n    score= pd.Series(index= all_minus_binary_features)\n    for feat in all_minus_binary_features:\n        score[feat] = check_selected_features_score(feat)\n        print(feat, score[feat])\n    score=score.sort_values(ascending=False)\n    #check score by adding one by one. keep only improving feature\n    hi_score = 0.0\n    cols=[]\n    for feat in score.index:\n        cols.append(feat)\n        res = check_selected_features_score(cols)\n        print(feat, res, res>hi_score)\n        if res>hi_score:\n            hi_score=res\n        else:\n            cols.pop()\n    return cols\n    ","df132895":"individual_features_add = pd.Index(check_individual_score_and_add())\nprint(individual_features_add)","6786d664":"\n# probably would be better to have a randomized approach that add\/remove features not in a definite order\n\n#add features one by one from scratch, possibly 'num_it' times.\ndef additive_feature_selection(num_it=1):\n    \n    hi_score = 0.0\n    kept_cols = []\n    for i in range(num_it):\n        print( 'ADDITIVE FEATURE stage ', num_it)\n        features = all_features.drop(kept_cols)\n        for feat in features:\n            kept_cols.append(feat)\n            res = check_selected_features_score(kept_cols)\n            print(feat, res, res>hi_score)\n            if res>hi_score:\n                hi_score=res\n            else:\n                kept_cols.pop()\n    \n        print( 'Adding mode finale score : ', hi_score)\n    return kept_cols \n\n#remove features one by one from all features, possibly 'num_it' times.\ndef dropping_feature_selection(num_it=1):\n    kept_cols = all_features\n    for i in range(num_it):\n        all_remaining_features = kept_cols\n        hi_score = check_selected_features_score(kept_cols)\n        print( 'Dropping mode baseline score : ', hi_score)\n\n        for j, feat in reversed(list(enumerate(all_remaining_features))): #reversed to start from fancy added features\n            test_cols=kept_cols.drop(feat)\n            res = check_selected_features_score(test_cols)\n            print(feat, res, res>=hi_score)\n            if res>=hi_score:\n                hi_score=res\n                kept_cols=test_cols\n        print( 'Dropping mode FINALE score : ', hi_score)\n    return kept_cols ","7ad8b276":"# selected_feat_add = pd.Index(additive_feature_selection(num_it=1))\n\n# print(selected_feat_add)","b76b6217":"# selected_feat_drop =  pd.Index(dropping_feature_selection(num_it=1))\n# print(selected_feat_drop)","a8f88a08":"from sklearn.feature_selection import RFECV\n\nselector = RFECV(get_clf(), step=1, min_features_to_select=2, cv=CV, scoring='accuracy', verbose=False)\n# selector = selector.fit(X, y)\n","84d7d4e7":"# print(\"Optimal number of features : %d\" % selector.n_features_)\n\n# # Plot number of features VS. cross-validation scores\n# plt.figure()\n# plt.xlabel(\"Number of features selected\")\n# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n# plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n# plt.show()\n","8ca78384":"# rfecv_selected = all_features[selector.support_]\n\n# print(rfecv_selected)","a470a3c6":"\n# print('Raw score : ',check_selected_features_score(raw_columns, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# print('Categorized Raw score : ', check_selected_features_score(categorized_baseline, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# print('No binary Features score : ', check_selected_features_score(all_minus_binary_features, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# print('All Features score : ', check_selected_features_score(X.columns, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# # print('improving_raw_features', check_selected_features_score(improving_raw_features, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# print('individual_features_add', check_selected_features_score(individual_features_add, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# # print('selected_feat_add', check_selected_features_score(selected_feat_add, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# # print('selected_feat_drop', check_selected_features_score(selected_feat_drop, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# print('rfecv_selected', check_selected_features_score(rfecv_selected, n_estimators=ESTIMATORS*MODEL_FACTOR))\n# gc.collect()\n","697e7636":"\ndef get_pipeline(features, n_estimators= ESTIMATORS*MODEL_FACTOR, random_state=2019):\n    selector = ColumnSelector([(col not in features) for col in X.columns])\n    return make_pipeline(selector, get_clf(n_estimators= n_estimators, \n                                           random_state=random_state))\n","0c2b0bad":"#single model\n# single_model = get_pipeline(rfecv_selected, n_estimators= individual_features_add_estimators)\n# single_model.fit(X,y)\n# single_model_pred = single_model.predict(X_test)\n# to_submission(single_model_pred, 'single_model_pred')","a364c506":"# clf = StackingCVClassifier(classifiers=[get_pipeline(improving_raw_features ,n_estimators= 500),\n#                                         get_pipeline(individual_features_add,n_estimators= 500),\n#                                         get_pipeline(selected_feat_add,n_estimators= selected_feat_add_estimators),\n#                                         get_pipeline(selected_feat_drop,n_estimators= selected_feat_drop_estimators),\n#                                         get_pipeline(rfecv_selected,n_estimators= rfecv_selected_estimators)],\n#                            meta_classifier=get_LGBM(),\n#                            cv=CV, \n#                            random_state=666, \n#                            use_probas=True, \n#                            use_features_in_secondary=True, \n#                            verbose=True)\n# # clf.fit(X, y)\n# # print('Predicting test values...')\n# # y_pred = clf.predict(X_test)\n# # print('Saving predictions...')\n# # to_submission(y_pred, 'lgbm_stack_preds_sub')\n# print('Done')\n                                        \n# gc.collect()","46b42235":"This notebook follows my notebooks:\n* https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/ that looked for hyper-paramters (although it was without all the FE so the parameters may not be ideal anymore)\n* https:\/\/www.kaggle.com\/arateris\/stacked-classifiers-for-forest-cover where I was playing with various FE and original stacking\n* https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover with more stacking and features, but using the imbalanced accuracy that I remove here.\n\n\nThere was a lot of inspirations from other notebooks which I will mention on the way as much as I remember.","2683886c":"## Adding and removing from scratch\nadditive\n- Start from raw categorized data\n- add each added features one by one successfully if improving the CV score.\n\ndroping\n- Start from all features (non-binary)\n- Remove individuals that reduce CV score\n","e0731b9d":"## Target encoding ","acd0df5c":"## Check baseline scores with small estimator for speed\n","81620400":"## Check individual feature score\n- Check each feature score alone\n- Sort features by importance\n- Start with best feature,\n- Add each feature successively if they improve CV score.","c76b1f72":"Note : large difference between train and test size. Will need to check input distributions.","36d5fb06":"## Count Encoding","5505a975":"#Import datasets","71971502":"# TODO\n* Check joint add\/drop features (especially the one feature encoding, target encoding etc.)\n* Check binary vs categorized features. ","22cad920":"## Checking scores with stronger classifier","d4df600d":"## Add PCA features and Gaussian Mixture\n","683a0b82":"## Fixing Hillshade_3pm\nPloting all the histograms I saw one weird thing in the Hillshade_3pm","401fd2fe":"(Now quite old) List of classifiers and hyper-parameters\n- XGBClassifier\n-- Params: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 10} ?\n-- n_estimators = 719, max_depth = 464 https:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2  https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n- RFClassifier\n-- {'max_depth': 100, 'max_features': 0.3, 'n_estimators': 2000}  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n-- Params: {n_estimators = 719, max_features = 0.3, max_depth = 464, min_samples_split = 2, min_samples_leaf = 1, bootstrap = False} https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n- ExtraTrees\n-- Params : n_estimators = 750, max_features = 0.3, max_depth = None,  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n- LGBM \n-- Params : n_estimators=400,  num_leaves=100  ?  https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\n-- {'learning_rate': 0.5, 'max_depth': 25, 'n_estimators': 500}  https:\/\/www.kaggle.com\/arateris\/xgb-rf-with-gridsearch-for-forest-classifier\/\n- ADABoost \n-- Params : {max_depth  = 464, min_samples_split = 2, min_samples_leaf = 1,}  https:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2\n\n","0ccaa284":"## Categorial combining","cd9f254c":"## Categories vs OHE\nOriginally transformed the binary classes to categorial features but maybe not a good idea so in this version I won't keep it for training at the end. \nI keep it here to do frequency encoding and may try to use it again later.","00859cf6":"--> Everything in numeric. \n\nSoil_type and Wilderness_area are categorial data already put as one hot encoded.","229e1f65":"## Check by adding only new features that bring in gains\n- Start from raw categorized data,\n- Add each feature individually to the base if check if it improved CV score.","654718fb":"# Feature selection","7c13563e":"Hill_shade_3pm is missing ~30 values at 126. \nHill_shade_3pm has 88 values equal to 0 which probably should not. 1250 zeros in the test set. --> this is not so many values, but still prefer fixing it to avoid spreading bad data in the feature engineering and training.","10feac1a":"# Feature engineering","63cbbcb2":"--> No missing data.","3a78944e":"# Model generation"}}