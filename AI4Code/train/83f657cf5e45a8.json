{"cell_type":{"acb55971":"code","1741a445":"code","031481dc":"code","8b9a2574":"code","1ab5cb31":"code","4d7f1ca4":"code","53e12f3e":"code","13a02dda":"code","8a78f515":"code","866e3d7c":"code","b1feacca":"code","6dde7574":"code","9126d7f8":"code","493242f2":"code","893cb00d":"code","c25c51b4":"code","c74864b0":"code","2be32b30":"code","e84459a9":"code","a56a8b0d":"code","db19955f":"code","0084d8fa":"code","c94a2203":"code","d0fe9118":"markdown","0ae9387c":"markdown","4dee6498":"markdown","b4c2b048":"markdown","c12908a4":"markdown","b3c9ce65":"markdown","0661bbdb":"markdown","59413c0f":"markdown","c4536a4b":"markdown","e7023aeb":"markdown","d35a1bc7":"markdown","a57f1d9f":"markdown","5b3d23a3":"markdown","8f0ac600":"markdown","eb0e2467":"markdown","a51f0d53":"markdown","fb3e615b":"markdown"},"source":{"acb55971":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","1741a445":"from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout, BatchNormalization\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","031481dc":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","8b9a2574":"train.head()","1ab5cb31":"y_train = train['label']\nX_train = train.drop(['label'], axis=1)","4d7f1ca4":"X_train.isnull().any().describe()","53e12f3e":"test.isnull().any().describe()","13a02dda":"X_train.shape","8a78f515":"X_train = X_train\/255.\ntest = test\/255.","866e3d7c":"X_train.shape","b1feacca":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","6dde7574":"y_train.head()","9126d7f8":"y_train = to_categorical(y_train, num_classes = 10)  #num_classes is basically the number of variables we have in our y_train dataframe. This case we have numbers from 0 to 9.","493242f2":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=48) #random_state = 48 is just any random seed you can set.","893cb00d":"plt.imshow(X_train[65][:,:,0])","c25c51b4":"model = tf.keras.models.Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='Same', activation='relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', strides=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.4))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=(5,5), padding='Same', activation='relu', strides=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.4))\n\nmodel.add(Dense(10, activation='softmax'))","c74864b0":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","2be32b30":"imageDataGen = ImageDataGenerator(width_shift_range=0.1, # randomly shift images horizontally\n                                  height_shift_range=0.1, # randomly shift images vertically\n                                  rotation_range=10,   # randomly rotate images in the range provided.\n                                  zoom_range=0.1) # Randomly zoom image \n\nimageDataGen.fit(X_train)","e84459a9":"reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)","a56a8b0d":"history = model.fit_generator(imageDataGen.flow(X_train, y_train, batch_size=64), \n                    verbose = 2,\n                    validation_data=(X_val, y_val), \n                    epochs=50, \n                    steps_per_epoch=X_train.shape[0] \/\/ 64,\n                    callbacks= [reduce_lr])","db19955f":"from sklearn.metrics import confusion_matrix\n# Predict the values from the validation dataset\ny_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Blues\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","0084d8fa":"pred = model.predict(test)\npred = np.argmax(pred, axis=1)\npred = pd.Series(pred, name=\"Label\")","c94a2203":"final_res = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),pred],axis = 1)\nfinal_res.to_csv(\"MNIST_try_v8.csv\",index=False)","d0fe9118":"So as we can see we have 42000 values in this train dataset. Since we will be working with CNN models, its best to normalize the pixel values, since CNN converges faster on [0...1]","0ae9387c":"Confusion Matrix on the validation data set.","4dee6498":"# **Importing the required packages**","b4c2b048":"Now we will split our Training and Validation Data. ","c12908a4":"**Fitting the model.**\n\nI used a batch_size of 64. I didn't come up with this number directly. I initialially tried 32, 48, 64, 80, 128, and the best result was displayed with a size of 64.","b3c9ce65":"I'm measuring the loss on categorical_crossentropy which is typically used with >2 classes classification.\n\nI chose the Adam optimizer with the default hyperparameters as it is very effective. It combines the best properties of the AdaGrad and RMSProp algorithms that can handle sparse gradients on noisy problems.\n\nThe metric function \"accuracy\" is used is to evaluate the performance our model. ","0661bbdb":"If you find this notebook useful, kindly upvote and give me a boost also! Thanks for reading through my notebook. Cheers!","59413c0f":"LearningRateScheduler is a callback function. Basically what it does is it gets the updated learning rate value from the scheduler function with the current epoch and applies the updated learning rate on the optimizer. Helps us in reducing the learning rate.","c4536a4b":"Let's Check for any missing values now.","e7023aeb":"**I have used Sequential API available in the tensorflow keras library.**\nAfter many tries I finally decided to use 32 filters for the first 3 Conv2D layers and 64 for the last 3 Conv2D layers with different kernel sizes. Each kernel filter matrix is applied on the whole image.\n\nNext between each Conv2D layer I have applied Batch normalization which is proposed as a technique to help coordinate the update of multiple layers in the model. It does this scaling the output of the layer, specifically by standardizing the activations of each input variable per mini-batch, such as the activations of a node from the previous layer. \n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored for each training sample. I used a different range of dropout rates [10...50] and best result I achieved was with 40%.\n\n'ReLu' is an activation function max(0,x). It adds non-linearity to our network.\n\nThe flatten layer is to transform the entire output to a 1D vector.\n\nIn the end a fully connected layer with Softmax activation function is applied to outputs distribution of probability of each class.","d35a1bc7":"Making the submission.","a57f1d9f":"As we can see it's categorical value [0...9] We will change these scalars to one_hot vectors","5b3d23a3":"So its 784 pixels that is 28x28 pixels. So Let's reshape our data.","8f0ac600":"# **Model Creation**","eb0e2467":"**Data Augmentation**\n\nIn order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make our existing dataset even larger without even collecting it.Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nI firstly trained a model without data augmentation and could only reach upto 99.31%. After using data augmentation the accuracy crossed 99.61%.","a51f0d53":"As we can see we have pixel values (784 to be precise) and one dependent variable label which is our output number. So lets separate our train and test parameters","fb3e615b":"# **Importing the data**"}}