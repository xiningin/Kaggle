{"cell_type":{"c3338822":"code","9175d96e":"code","5c93e20b":"code","ae13f287":"code","9444ce1b":"code","066730b4":"code","eec97ee1":"code","13d365a4":"code","df83fad0":"code","24e200cb":"code","6b343807":"code","c50725d5":"code","760cce8f":"code","b69b1dd8":"code","47c667eb":"code","8c8a3df9":"code","316f45fa":"code","d0eb97b1":"code","0a491bda":"code","6d337f28":"code","1c646cf1":"code","daf9c63e":"code","9bd88d6a":"markdown","48ea42c9":"markdown","905ffc5c":"markdown","3b6e2139":"markdown","aff9fb51":"markdown"},"source":{"c3338822":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nimport math","9175d96e":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nfrom torch.autograd import Variable\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","5c93e20b":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('Training on CPU...')\nelse:\n    print('Training on GPU...')","ae13f287":"train_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain_df.head()","9444ce1b":"sns.countplot(train_df['label'].value_counts())","066730b4":"train_df.isnull().any().sum(), test_df.isnull().any().sum()","eec97ee1":"class MNISTData(torch.utils.data.Dataset):\n    def __init__(self, data, transform=None):\n        self.data = data\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n                \n        image = item[1:].values.astype(np.uint8).reshape((28, 28))\n        label = item[0]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image, label","13d365a4":"# Seperate the features and labels\ntargets_np = train_df.label.values\nfeatures_np = train_df.loc[:, train_df.columns != 'label'].values\/255\n\n# Split into training and test set\nfeatures_train, features_test, target_train, target_test = train_test_split(features_np, targets_np, test_size=0.2, random_state=42)","df83fad0":"# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(target_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(target_test).type(torch.LongTensor) # data type is long","24e200cb":"# Set batch size\nbatch_size = 256\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n","6b343807":"BATCH_SIZE = 20\nVALID_SIZE = 0.15 # percentage of data for validation\n\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),\n   # transforms.RandomRotation(0, 0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntransform_valid = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\n# Importing data that will be used for training and validation\ndataset = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\n\n# Creating datasets for training and validation\n#train_data = MNISTData(train, transform=transform_train)\n#test_data = MNISTData(test, transform=transform_valid)\n\n\n\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)#, sampler=train_sampler)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)#, sampler=valid_sampler)","c50725d5":"# Viewing data examples used for training\nfig, axis = plt.subplots(2, 10, figsize=(15, 10))\nimages, labels = next(iter(train_loader))\n\nfor i, ax in enumerate(axis.flat):\n    with torch.no_grad():\n        image, label = images[i], labels[i]\n\n        ax.imshow(image.view(28, 28), cmap='binary') # add image\n        ax.set(title = f\"{label}\") # add label","760cce8f":"test_data","b69b1dd8":"images, labels = next(iter(test_loader))\n\n","47c667eb":"images.shape","8c8a3df9":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128, 10),\n        )\n                \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        \n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nmodel = Net()\nmodel = model.cuda()\nprint(model)","316f45fa":"LEARNING_RATE = 0.001680\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n\n","d0eb97b1":"epochs = 150\nvalid_loss_min = np.Inf\ntrain_losses, valid_losses = [], []\nhistory_accuracy = []\n\nfor e in range(1, 50):\n    running_loss = 0\n\n    for images, labels in train_loader:\n        if train_on_gpu:\n            images, labels = images.cuda(), labels.cuda()\n        # Clear the gradients, do this because gradients are accumulated.\n        optimizer.zero_grad()\n        \n        # Forward pass, get our log-probabilities.\n        ps = model(images).cuda()\n\n\n        # Calculate the loss with the logps and the labels.\n        loss = criterion(ps, labels)\n        \n        # Turning loss back.\n        loss.backward()\n        \n        # Take an update step and few the new weights.\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        valid_loss = 0\n        accuracy = 0\n        \n        # Turn off gradients for validation, saves memory and computations.\n        with torch.no_grad():\n            model.eval() # change the network to evaluation mode\n            for images, labels in test_loader:\n                if train_on_gpu:\n                    images, labels = images.cuda(), labels.cuda()\n                # Forward pass, get our log-probabilities.\n                #log_ps = model(images)\n                ps = model(images).cuda()\n                \n                # Calculating probabilities for each class.\n                #ps = torch.exp(log_ps)\n                \n                # Capturing the class more likely.\n                _, top_class = ps.topk(1, dim=1)\n                \n                # Verifying the prediction with the labels provided.\n                equals = top_class == labels.view(*top_class.shape)\n                \n                valid_loss += criterion(ps, labels)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n                \n        model.train() # change the network to training mode\n        \n        train_losses.append(running_loss\/len(train_loader))\n        valid_losses.append(valid_loss\/len(test_loader))\n        history_accuracy.append(accuracy\/len(test_loader))\n        \n        network_learned = valid_loss < valid_loss_min\n\n        if e == 1 or e % 5 == 0 or network_learned:\n            print(f\"Epoch: {e}\/{epochs}.. \",\n                  f\"Training Loss: {running_loss\/len(train_loader):.3f}.. \",\n                  f\"Validation Loss: {valid_loss\/len(test_loader):.3f}.. \",\n                  f\"Test Accuracy: {accuracy\/len(test_loader):.3f}\")\n        \n        if network_learned:\n            valid_loss_min = valid_loss\n            torch.save(model.state_dict(), 'model_mtl_mnist.pt')\n            print('Detected network improvement, saving current model')","0a491bda":"# Viewing training information\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label='Training Loss')\nplt.plot(valid_losses, label='Validation Loss')\nplt.legend(frameon=False)","6d337f28":"plt.plot(history_accuracy, label='Validation Accuracy')\nplt.legend(frameon=False)","1c646cf1":"def view_classify(img, ps):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    ax2.set_yticklabels(np.arange(10))\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()","daf9c63e":"%matplotlib inline\ndef make_prediction(data):\n    images, labels = next(iter(data))\n\n    img = images[42].view(1, 784)\n    # Turn off gradients to speed up this part\n    with torch.no_grad():\n        logps = model(img)\n\n    # Output of the network are log-probabilities, need to take exponential for probabilities\n    ps = torch.exp(logps)\n    view_classify(img.view(1, 28, 28), ps)\nmake_prediction(test_loader)","9bd88d6a":"![](https:\/\/camo.githubusercontent.com\/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275\/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)","48ea42c9":"This notebook will give you a step by step guide on how to crease a **Convolutional Neural Networks** using PyTorch to classify the MNIST data.","905ffc5c":"# **Introduction**","3b6e2139":"**Hope you enjoy!**","aff9fb51":"**TABLE OF CONTENTS**\n\n1. Imports\n2. Data Clearning and Scaling\n3. Data Visualization\n2. Example Neural Network\n3. Convolutional Neural network\n4. Training and Validation Graphs\n5. Submission"}}