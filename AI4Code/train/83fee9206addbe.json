{"cell_type":{"d0877440":"code","00e8bae7":"code","9960b8e9":"code","231c0377":"code","a80779f9":"code","08609958":"code","2d529069":"code","9216b089":"code","9b38c255":"code","bbdb4fc2":"code","d14e5470":"code","62765b6e":"markdown","54a8663c":"markdown"},"source":{"d0877440":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00e8bae7":"#Importing the dataset\ndataset = pd.read_csv('\/kaggle\/input\/iris-flower-dataset\/IRIS.csv')","9960b8e9":"#To display first 5 rows of the dataset\ndataset.head()","231c0377":"#Checking the data format in the dataset\ndataset.info()","a80779f9":"#Checking if the missing values are present in the dataset\ndataset.isnull().sum()","08609958":"#Seperating the dependant and independent data into different variables\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","2d529069":"#Splitting the data into train and test datasets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","9216b089":"#Training the Dataset using Support Vector Classification Model\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","9b38c255":"#Predicting the dependant values using test dataset\ny_pred = classifier.predict(X_test)","bbdb4fc2":"#Comparing the actual values with predicting values\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","d14e5470":"#Creating confusion matrix to check no of incorrect predictions and checking the accuracy of the predicted values against the actual values\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","62765b6e":"While splitting the data into train and test dataset, the test_size of 0.2 represents that we'll be using 20% of randomly chosen data to test our model whereas rest 80% of data will be used to train our model","54a8663c":"We can see the from the result above that our dataset has no missing values, which is a good thing since we won't need to do any extra steps to take care of them."}}