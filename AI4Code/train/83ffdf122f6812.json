{"cell_type":{"31296152":"code","ce7e1cac":"code","8a407f5b":"code","8fbc742a":"code","4925edca":"code","3670c1be":"code","9afb9104":"code","f661ced8":"code","f1dc1cae":"code","6dfafe7c":"code","17b1deaa":"code","642db239":"code","15abe255":"code","a15006a3":"code","b9493c12":"code","2726f12f":"code","d8779fe7":"code","ecb135a2":"code","20ac5066":"code","07ca7e8b":"code","9e4c0f91":"code","5053e0e4":"code","ca5d5e93":"code","1ebac965":"code","21b6e4f6":"code","d618d0c0":"code","170410fe":"code","8c6b6ff8":"code","cf22bb82":"code","01b9fd9d":"code","b50b422b":"code","d5d2f70b":"code","62dd4466":"code","40c49182":"code","a9ad44af":"code","8e27ef0e":"code","cd4c0cbb":"code","a424c677":"code","70bda8cf":"code","8daf4f37":"code","dafbae38":"code","4fa35f8d":"code","0e59e0db":"code","8779f758":"code","0c713c71":"code","7de157f7":"code","e4662465":"code","9e12089e":"code","2b70b03b":"code","3cfb0fc1":"code","8e5bf99b":"code","47330d48":"code","88a6e24a":"markdown","16c0e56b":"markdown","7b771401":"markdown","e3aa139d":"markdown","17751931":"markdown","a3cf027f":"markdown","afb17d16":"markdown","da86ee94":"markdown","458dff6e":"markdown","9092f2b4":"markdown","17f3de9e":"markdown","8517f59b":"markdown","c9d82891":"markdown","eec37709":"markdown","0ce3cdd2":"markdown","2dcf5b3e":"markdown","0b9837c3":"markdown","f01c2624":"markdown","60ebb12d":"markdown","42fbac00":"markdown","bdf912f3":"markdown","042315ba":"markdown","a907201b":"markdown","710c27c4":"markdown","dcd2dbfa":"markdown","d7640371":"markdown","a05a82b8":"markdown","9ac135ef":"markdown","52ff6dc9":"markdown","302d2bc8":"markdown","8af17f84":"markdown","458d1bc8":"markdown","05501920":"markdown","73eedbb4":"markdown","f10e7cc7":"markdown","4de28684":"markdown","e9e4038a":"markdown","0fe5cb27":"markdown","6888f8ae":"markdown","58c6e893":"markdown","5cfa7703":"markdown","43f50585":"markdown","1d4c5daf":"markdown","87e5e8bd":"markdown","a7fa9aea":"markdown","0e1e032a":"markdown","9e9cb134":"markdown","32a8d793":"markdown","9bae7594":"markdown","e764bac9":"markdown","489553af":"markdown","b142d0dd":"markdown","37a08339":"markdown","6754fdfe":"markdown","0eb4028e":"markdown","c45d01e5":"markdown","0c7d8471":"markdown","fc1f741b":"markdown","f5956419":"markdown","f87aa992":"markdown","d03072c3":"markdown","64af06b0":"markdown","fe857861":"markdown","64d6908a":"markdown","a55b25df":"markdown","5cd3f630":"markdown","bbc2a7a3":"markdown"},"source":{"31296152":"#Basic libraries\nimport pandas as pd \nimport numpy as np \n\n#Visualization libraries\nimport matplotlib.pyplot as plt \nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom textblob import TextBlob\nfrom plotly import tools\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10, 5]\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n#NLTK libraries\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Machine Learning libraries\nimport sklearn \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n \n\n#Metrics libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n#Miscellanous libraries\nfrom collections import Counter\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Deep learning libraries\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout","ce7e1cac":"#reading the fake and true datasets\nfake_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\n\n# print shape of fake dataset with rows and columns and information \nprint (\"The shape of the  data is (row, column):\"+ str(fake_news.shape))\nprint (fake_news.info())\nprint(\"\\n --------------------------------------- \\n\")\n\n# print shape of true dataset with rows and columns and information\nprint (\"The shape of the  data is (row, column):\"+ str(true_news.shape))\nprint (true_news.info())","8a407f5b":"#Target variable for fake news\nfake_news['output']=0\n\n#Target variable for true news\ntrue_news['output']=1","8fbc742a":"#Concatenating and dropping for fake news\nfake_news['news']=fake_news['title']+fake_news['text']\nfake_news=fake_news.drop(['title', 'text'], axis=1)\n\n#Concatenating and dropping for true news\ntrue_news['news']=true_news['title']+true_news['text']\ntrue_news=true_news.drop(['title', 'text'], axis=1)\n\n#Rearranging the columns\nfake_news = fake_news[['subject', 'date', 'news','output']]\ntrue_news = true_news[['subject', 'date', 'news','output']]","4925edca":"fake_news['date'].value_counts()","3670c1be":"#Removing links and the headline from the date column\nfake_news=fake_news[~fake_news.date.str.contains(\"http\")]\nfake_news=fake_news[~fake_news.date.str.contains(\"HOST\")]\n\n'''You can also execute the below code to get the result \nwhich allows only string which has the months and rest are filtered'''\n#fake_news=fake_news[fake_news.date.str.contains(\"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\")]\n","9afb9104":"#Converting the date to datetime format\nfake_news['date'] = pd.to_datetime(fake_news['date'])\ntrue_news['date'] = pd.to_datetime(true_news['date'])","f661ced8":"frames = [fake_news, true_news]\nnews_dataset = pd.concat(frames)\nnews_dataset","f1dc1cae":"#Creating a copy \nclean_news=news_dataset.copy()","6dfafe7c":"def review_cleaning(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","17b1deaa":"clean_news['news']=clean_news['news'].apply(lambda x:review_cleaning(x))\nclean_news.head()","642db239":"stop = stopwords.words('english')\nclean_news['news'] = clean_news['news'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nclean_news.head()","15abe255":"#Plotting the frequency plot\nax = sns.countplot(x=\"subject\", data=clean_news,\n                   facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                   edgecolor=sns.color_palette(\"dark\", 3))\n\n#Setting labels and font size\nax.set(xlabel='Type of news', ylabel='Number of news',title='Count of news type')\nax.xaxis.get_label().set_fontsize(15)\nax.yaxis.get_label().set_fontsize(15)","a15006a3":"g = sns.catplot(x=\"subject\", col=\"output\",\n                data=clean_news, kind=\"count\",\n                height=4, aspect=2)\n\n#Rotating the xlabels\ng.set_xticklabels(rotation=45)","b9493c12":"ax=sns.countplot(x=\"output\", data=clean_news)\n\n#Setting labels and font size\nax.set(xlabel='Output', ylabel='Count of fake\/true',title='Count of fake and true news')\nax.xaxis.get_label().set_fontsize(15)\nax.yaxis.get_label().set_fontsize(15)","2726f12f":"#Extracting the features from the news\nclean_news['polarity'] = clean_news['news'].map(lambda text: TextBlob(text).sentiment.polarity)\nclean_news['review_len'] = clean_news['news'].astype(str).apply(len)\nclean_news['word_count'] = clean_news['news'].apply(lambda x: len(str(x).split()))\n\n#Plotting the distribution of the extracted feature\nplt.figure(figsize = (20, 5))\nplt.style.use('seaborn-white')\nplt.subplot(131)\nsns.distplot(clean_news['polarity'])\nfig = plt.gcf()\nplt.subplot(132)\nsns.distplot(clean_news['review_len'])\nfig = plt.gcf()\nplt.subplot(133)\nsns.distplot(clean_news['word_count'])\nfig = plt.gcf()","d8779fe7":"#Function to get top n words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n#Calling function and return only top 20 words\ncommon_words = get_top_n_words(clean_news['news'], 20)\n\n#Printing the word and frequency\nfor word, freq in common_words:\n    print(word, freq)\n\n#Creating the dataframe of word and frequency\ndf1 = pd.DataFrame(common_words, columns = ['news' , 'count'])\n\n#Group by words and plot the sum\ndf1.groupby('news').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in news')\n","ecb135a2":"#Function to get top bigram words\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n#Calling function and return only top 20 words\ncommon_words = get_top_n_bigram(clean_news['news'], 20)\n\n#Printing the word and frequency\nfor word, freq in common_words:\n    print(word, freq)\n    \n#Creating the dataframe of word and frequency\ndf3 = pd.DataFrame(common_words, columns = ['news' , 'count'])\n\n#Group by words and plot the sum\ndf3.groupby('news').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in news')\n","20ac5066":"#Function to get top trigram words\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n#Calling function and return only top 20 words\ncommon_words = get_top_n_trigram(clean_news['news'], 20)\n\n#Printing word and their respective frequencies\nfor word, freq in common_words:\n    print(word, freq)\n\n#Creating a dataframe with words and count\ndf6 = pd.DataFrame(common_words, columns = ['news' , 'count'])\n\n#Grouping the words and plotting their frequencies\ndf6.groupby('news').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in news')\n","07ca7e8b":"text = fake_news[\"news\"]\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","9e4c0f91":"text = true_news[\"news\"]\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","5053e0e4":"#Creating the count of output based on date\nfake=fake_news.groupby(['date'])['output'].count()\nfake=pd.DataFrame(fake)\n\ntrue=true_news.groupby(['date'])['output'].count()\ntrue=pd.DataFrame(true)\n\n#Plotting the time series graph\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n         x=true.index,\n         y=true['output'],\n         name='True',\n    line=dict(color='blue'),\n    opacity=0.8))\n\nfig.add_trace(go.Scatter(\n         x=fake.index,\n         y=fake['output'],\n         name='Fake',\n    line=dict(color='red'),\n    opacity=0.8))\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\n        \n    \nfig.update_layout(title_text='True and Fake News',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n\nfig.show()","ca5d5e93":"#Extracting 'reviews' for processing\nnews_features=clean_news.copy()\nnews_features=news_features[['news']].reset_index(drop=True)\nnews_features.head()","1ebac965":"stop_words = set(stopwords.words(\"english\"))\n#Performing stemming on the review dataframe\nps = PorterStemmer()\n\n#splitting and adding the stemmed words except stopwords\ncorpus = []\nfor i in range(0, len(news_features)):\n    news = re.sub('[^a-zA-Z]', ' ', news_features['news'][i])\n    news= news.lower()\n    news = news.split()\n    news = [ps.stem(word) for word in news if not word in stop_words]\n    news = ' '.join(news)\n    corpus.append(news)   ","21b6e4f6":"corpus[1]","d618d0c0":"tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(2,2))\n# TF-IDF feature matrix\nX= tfidf_vectorizer.fit_transform(news_features['news'])\nX.shape","170410fe":"#Getting the target variable\ny=clean_news['output']","8c6b6ff8":"print(f'Original dataset shape : {Counter(y)}')","cf22bb82":"## Divide the dataset into Train and Test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","01b9fd9d":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i in range (cm.shape[0]):\n        for j in range (cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b50b422b":"#creating the objects\nlogreg_cv = LogisticRegression(random_state=0)\ndt_cv=DecisionTreeClassifier()\nknn_cv=KNeighborsClassifier()\nnb_cv=MultinomialNB(alpha=0.1) \ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'Naive Bayes'}\ncv_models=[logreg_cv,dt_cv,knn_cv,nb_cv]\n\n#Printing the accuracy\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring ='accuracy').mean()))","d5d2f70b":"param_grid = {'C': np.logspace(-4, 4, 50),\n             'penalty':['l1', 'l2']}\nclf = GridSearchCV(LogisticRegression(random_state=0), param_grid,cv=5, verbose=0,n_jobs=-1)\nbest_model = clf.fit(X_train,y_train)\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","62dd4466":"logreg = LogisticRegression(C=24.420530945486497, random_state=0)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","40c49182":"cm = metrics.confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cm, classes=['Fake','True'])","a9ad44af":"print(\"Classification Report:\\n\",classification_report(y_test, y_pred))","8e27ef0e":"logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","cd4c0cbb":"corpus[1]","a424c677":"#Setting up vocabulary size\nvoc_size=10000\n\n#One hot encoding \nonehot_repr=[one_hot(words,voc_size)for words in corpus] ","70bda8cf":"#Setting sentence length\nsent_length=5000\n\n#Padding the sentences\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","8daf4f37":"embedded_docs[1]","dafbae38":"#Creating the lstm model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100)) #Adding 100 lstm neurons in the layer\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\n\n#Compiling the model\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","4fa35f8d":"len(embedded_docs),y.shape","0e59e0db":"# Converting the X and y as array\nX_final=np.array(embedded_docs)\ny_final=np.array(y)\n\n#Check shape of X and y final\nX_final.shape,y_final.shape","8779f758":"# Train test split of the X and y final\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n# Fitting with 10 epochs and 64 batch size\nmodel.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)","0c713c71":"# Predicting from test data\ny_pred=model.predict_classes(X_test)\n\n#Creating confusion matrix\n#confusion_matrix(y_test,y_pred)\ncm = metrics.confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cm,classes=['Fake','True'])","7de157f7":"#Checking for accuracy\naccuracy_score(y_test,y_pred)","e4662465":"# Creating classification report \nprint(classification_report(y_test,y_pred))","9e12089e":"# Creating bidirectional lstm model\nembedding_vector_features=40\nmodel1=Sequential()\nmodel1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel1.add(Bidirectional(LSTM(100))) # Bidirectional LSTM layer\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(1,activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","2b70b03b":"# Fitting the model\nmodel1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)","3cfb0fc1":"# Predicting from test dataset\ny_pred1=model1.predict_classes(X_test)\n\n#Confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred1)\nplot_confusion_matrix(cm,classes=['Fake','True'])","8e5bf99b":"#Calculating Accuracy score\naccuracy_score(y_test,y_pred1)","47330d48":"# Creating classification report \nprint(classification_report(y_test,y_pred1))","88a6e24a":"**Insights:**\n* There is an important news which ruled the US media-'Black lives matter' post the demise of Floyd. We can see that news has been covered in our data. There were lot of fake news revolved around the death.\n* Rest of the news are about US politics ","16c0e56b":"We have removed all the stop words in the review column","7b771401":"## Logistic Regression with Hyperparameter Tuning\nWe use regularization parameter and penality for parameter tuning. let's see which one to plug.","e3aa139d":"### Confusion Matrix\nLet's look at the true positive and true negative classified by the model","17751931":"If you notice, we had links and news headline inside the date column which can give us trouble when converting to datetime format. So let's remove those records from the column ","a3cf027f":"We have removed all punctuation in our news column","afb17d16":"## Count of news subject based on true or fake \nLets look at the count based on the fake\/true outcome.","da86ee94":"Our dataset is nearly a balanced one. So let's leave balancing it.","458dff6e":"## Train-test split(75:25)\nUsing train test split function we are splitting the dataset into 75:25 ratio for train and test set respectively.","9092f2b4":"## N-gram analysis\n\n### Top 20 words in News\nLet's look at the top 20 words from the news which could give us a brief idea on what news are popular in our dataset","17f3de9e":"Only fake news dataset had an issue with date column,Now let's proceed with converting the date column to datetime format","8517f59b":"## Fitting and Evaluation of Model\nLet's now fit the bidirectional LSTM model to the data we have with the same parameters we had before","c9d82891":"## Model Selection\nFirst select the best peforming model by using cross validaton. Let's consider all the classification algorithm and perform the model selection process\n\n**Note:** I'm not including SVM in this algorithm because it took alot of time to train in my device","eec37709":"We have got an accuracy of 96%. That's awesome !","0ce3cdd2":"<a id=\"section-four\"><\/a>\n# Stemming & Vectorizing\n\n## Stemming the reviews\nStemming is a method of deriving root word from the inflected word. Here we extract the reviews and convert the words in reviews to its root word. for example,\n\n* Going->go\n* Finally->fina\n\nIf you notice, the root words doesn't need to carry a semantic meaning. There is another technique knows as Lemmatization where it converts the words into root words which has a semantic meaning. Simce it takes time. I'm using stemming","2dcf5b3e":"From the classification report we can see the accuracy value is nearly around 98%. We have to concentrate on precision score and it is 98%","0b9837c3":"All our scores are 98%. Certainly unreal to get such values. There are only changes in the support.","f01c2624":"**Insights:**\n* Most of the polarity are neutral, neither it shows some bad news nor much happy news\n* The word count is between 0-1000 and the length of the news are between 0-5000 and few near 10000 words which could be an article","60ebb12d":"As we have considered 5000 words, we can confirm that we have 5000 columns from the shape.","42fbac00":"## Text Processing\nThis is an important phase for any text analysis application.There will be many unuseful content in the news which can be an obstacle when feeding to a machine learning model.Unless we remove them the machine learning model doesn't work efficiently. Lets go step by step.\n\n## News-Punctuation Cleaning\nLet's begin our text processing by removing the punctuations","bdf912f3":"## Evaluation of model\nNow, let's predict the output for our test data and evaluate the predicted values with y_test","042315ba":"**Insights:**\n* As feared, I think the model will be biased in it's results considering the amount of trump news\n* We can see the north korea news as well, I think it will be about the dispute between US and NK\n* There are also few news from fox news as well","a907201b":"## Appending two datasets\nWhen we are providing a dataset for the model, we have to provide it as a single file. So it's better to append both true and fake news data and preprocess it further and perform EDA","710c27c4":"From the selected params, we get accuracy. Let's plug and chug","dcd2dbfa":"**Insights:**\n* True news doesn't involve much trump instead on Republican Party and Russia\n* There are news about Budget,military which comes under government news","d7640371":"## LSTM Model\nAt first we are going to develop the base model and compile it. The first layer will be the embedding layer which has the input of vocabulary size, vector features and sentence length. Later we add 30% dropout layer to prevent overfitting and the LSTM layer which has 100 neurons in the layer.In final layer we use sigmoid activation function. Later we compile the model using adam optimizer and binary cross entropy as loss function since we have only two outputs.\n\nTo understand how LSTM works please check this [link](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/). To give a small overview on how LSTM works,it remembers only the important sequence of words and forgets the insignificant words which doesn't add value in the prediction","a05a82b8":"## Count of fake news and true news\nLet's check the count of fake and true news and confirm whether our data is balanced or not","9ac135ef":"## Importing the dataset\nLet's welcome our dataset and see what's inside the box","52ff6dc9":"## News-Stop words\nA stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\nWe would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. Source: [Geeks for Geeks](https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/)\n\nFor our project, we are considering the english stop words and removing those words","302d2bc8":"## Concatenating title and text of news\nNews has to be classified based on the tile and text jointly. Treating the title and content of news separately doesn't reap us any benefit. So, lets concatenate both the columns in both datasets","8af17f84":"We have got an accuracy of 98%. That's better than LSTM !","458d1bc8":"<a id=\"section-six\"><\/a>\n# Deep learning-LSTM\nHere in this part we use neural network to predict whether the given news is fake or not.\n\n\nWe aren't gonna use normal neural network like ANN to clasify but LSTM(long short term memory) which helps in containing sequence information.Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n\n\n## One hot for Embedding layers\nBefore jumping into creating a layer let's take some vocabulary size. There might be a question why vocabulary size ? it is because we will be one hot encoding the sentences in the corpus for embedding layers. While onehot encoding the words in sentences will take the index from the vocabulary size. Let's fix the vocabulary size to 10000","05501920":"**Insights:**\n* Our dataset has more political news than any other news followed by world news\n* We have some repeated class names which expresses same meaning such as news,politics,government news etc which is similar to the alternative","73eedbb4":"## Padding embedded documents\nAll the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our LSTM model, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary. Here we take the common length as 5000 and perform padding using pad_sequence() function . Also we are going to 'pre' pad so that zeros are added before the sentences to make the sentence of equal length","f10e7cc7":"We can see all the sentences are of equal length with the addition of zeros infront of the sentences and making all the sentences of length 5000","4de28684":"## TFIDF(Term Frequency \u2014 Inverse Document Frequency)\nTF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.\n\nHere we are splitting as bigram (two words) and consider their combined weight.Also we are taking only the top 5000 words from the news.","e9e4038a":"### Checking for balance of data\nWe should be careful about when handling imbalance data. If it is imbalanced, the model will be biased towards the higher frequency class and returns max output","0fe5cb27":"**Insights:**\n* We have a pretty much balanced data \n* But the count of fake news is higher than the true news but not on a greater extent","6888f8ae":"We have got 98% accuracy. As already discussed before this is a biased dataset and we can easily get such higher accuracy without any effort in processing it. But for classification problems we need to get confusion matrix and check f1 score rather than accuracy","58c6e893":"## Time series analysis- Fake\/True news\nLet's look at the timeline of true and fake news that were circulated in the media.","5cfa7703":"**Insights:**\n* All the top 20 news are about the US government \n* Especially it's about Trump and US followed by obama\n* We can understand that the news are from reuters.\n\n### Top 2 words in the news\nNow let's expand our search to top 2 words from the news","43f50585":"<a id=\"section-five\"><\/a>\n# Model Building: Fake News Classifier\nAs we have successfully processed the text data, not it is just a normal machine learning problem. Where from the sparse matrix we predict the classes in target feature.","1d4c5daf":"<a id=\"section-seven\"><\/a>\n# Conclusion\nWe have done a mainstream work on processing the data and building the model. We could have indulged in changing the ngrams while vectorizing the text data . We took 2 words and vectorized it. You can check [Shreta's work](https:\/\/www.kaggle.com\/sreshta140\/is-it-authentic-or-not) on the same dataset where she got better results by considering both 1 and 2 word and also way better results with the help of LSTM and Bi-LSTM network.Let's discuss on the general insights from the dataset.\n\n* Most of the fake news are surrounded among Election news and about Trump. Considering the US elections 2020. There are chances to spread fake news and application of these technology will be heavily required.\n* Fake news are currently rooted during this pandemic situation to play politics and to scare people and force them to buy goods\n* Most of the news are from Reuters. We don't know whether this news media is politically influenced. So we should always consider the source of news to find if the news is fake or true.\n\nYou can check [Josu\u00e9 Nascimento's](https:\/\/www.kaggle.com\/josutk\/only-one-word-99-2) work where he has explained why this dataset is more biased\n\nYou can also check out my other notebooks [here](https:\/\/www.kaggle.com\/notebooks).","87e5e8bd":"From the classification report we can see the accuracy value is nearly around 96%. We have to concentrate on precision score and it is 96% which is great.","a7fa9aea":"We should consider the AUC score here which is 98%. Very well. All metrics are performing good. The more far left the curve is better our model We can adjust our threshold based on our ROC curve to get results based on model requirements","0e1e032a":"From the results, we can see logistic regression outdone the rest of the algorithms followed by Naive Bayes and Decision Tree.That's great. So let's go with logistic regression with hyperparameter tuning.","9e9cb134":"Let's split our new X and y variable into train and test and proceed with fitting the model to the data. We have considered 10 epochs and 64 as batch size. It can be varied to get better results.","32a8d793":"<a id=\"section-three\"><\/a>\n# Story Generation and Visualization from news\nIn this section we will complete do exploratory data analysis on news such as ngram analysis and understand which are all the words,context which are most likely found in fake news","9bae7594":"## Fitting the LSTM Model\nBefore fitting to the model, let's consider the padded embedded object as X and y as y itself and convert them into an array.","e764bac9":"### Classification Report\nConsidering Fake news, we should seriously consider precision score (False positive). We can't afford the mistakes when the model classifies fake news as true which will lead to chaos","489553af":"We can see all the words in the sentences are transformed into their index from the vocabulary we created.","b142d0dd":"# Table of Contents:\n1. [Introduction](#section-one)\n\n2. [Preprocessing and Cleaning](#section-two)\n\n3. [Story Generation and Visualization from news](#section-three)\n\n4. [Stemming & Vectorizing](#section-four)\n\n5. [Model Building: Fake news Classifier](#section-five)\n\n6. [Deep Learning-LSTM](#section-six)\n\n7. [Conclusion](#section-seven)","37a08339":"<a id=\"section-two\"><\/a>\n# Preprocessing and Cleaning\nWe have to perform certain preprocessing steps before performing EDA and giving the data to the model.Let's begin with creating the output column\n\n## Creating the target column \nLet's create the target column for both fake and true news. Here we are gonna denote the target value as '0' incase of fake news and '1' incase of true news","6754fdfe":"## Deriving new features from the news\nLets extract more features from the news feature such as\n1. Polarity: The measure which signifies the sentiment of th news\n2. Review length: Length of the news(number of letters and spaces)\n3. Word Count: Number of words in the news","0eb4028e":"<a id=\"section-one\"><\/a>\n# Introduction\n\nNews medium has become a channel to pass on the information of what's happening on world to the people living. Often people perceive whatever conveyed in the news to be true. There were circumstances where even the news channels acknowledged that their news is not true as they wrote. But some news have a significant impact not only to the people or government but also the economy. One news can shift the curves up and down depending on the emotions of people and political situation. It is important to identify the fake news from the real true news. The problem has been taken over and resolved with the help of Natural Language Processing tools which help us identify fake or true news based on the historical data. The news are now in safe hands !\n\n## Problem statement\nThe authenticity of Information has become a longstanding issue affecting businesses and society, both for printed and digital media. On social networks, the reach and effects of information spread occur at such a fast pace and so amplified that distorted, inaccurate or false information acquires a tremendous potential to cause real world impacts, within minutes, for millions of users. Recently, several public concerns about this problem and some approaches to mitigate the problem were expressed. . The sensationalism of not-so-accurate eye catching and intriguing headlines aimed at retaining the attention of audiences to sell information has persisted all throughout the history of all kinds of information broadcast. On social networking websites, the reach and effects of information spread are however significantly amplified and occur at such a fast pace, that distorted, inaccurate or false information acquires a tremendous potential to cause real impacts, within minutes, for millions of user\n\n## Objective\n1. Our sole objective is to classify the news  from the dataset to fake or true news. \n2. Extensive EDA of news\n3. Selecting and building a powerful model for classification","c45d01e5":"**Insights:**\n* Most of the fake news revolves around Donald Trump and America\n* There are also fake news about privacy, internet etc.,","0c7d8471":"**Insights:**\n* Fake news are all over the category except politics and world news \n* True news are present only in politics and world news and the count is high\n* THIS IS A HIGHLY BIASED DATASET and we can expect higher accuracy which doesn't signify it is a good model considering the poor quality of dataset","fc1f741b":"## Converting the date columns to datetime format\nWe can use pd.datetime to convert our date columns to date format we desire. But there was a problem,especially in fake_news date column. Let's check the value_counts() to see what lies inside","f5956419":"## Import Libraries\nLet's import all necessary libraries for the analysis and along with it let's bring down our dataset","f87aa992":"**Insights:**\n* True news got their dominance since Aug 2017. As they are seen at a very higher rates.That is a good sign\n* There are few outliers in true news where it was higher than the fake news(Nov 9, 2016 and Apr 7, 2017)\n* Our dataset has more fake news than the true one as we can see that we don't have true news data for whole 2015, So the fake news classification will be pretty accurate than the true news getting classified","d03072c3":"## WordCloud of Fake and True News\nLet's look at the word cloud for both fake and true news","64af06b0":"## ROC-AUC Curve\n\nThis is a very important curve where we decide on which threshold to setup based upon the objective criteria","fe857861":"Check out the diagonal elements(5799+5202), they are correctly predicted records and rest are incorrectly classified by the algorithm. Our model has done well(results are good by the data is biased :P)","64d6908a":"## Dataset Details\nThis metadata has two csv files where one dataset contains fake news and the other contains true\/real news and has nearly **23481 fake news and 21417 true news**\n\n**Description of columns in the file:**\n\n* title- contains news headlines\n* text-contains news content\/article\n* subject- type of news\n* date- date the news was published","a55b25df":"This is how a line looks like now, as computer cannot understand words and their sentiment we need to convert these words into 1's and 0's. To encode it we use TFIDF","5cd3f630":"## Bidirectional LSTM\n\nBi-LSTM is an extension of normal LSTM with two independent RNN's together. The normal LSTM is uni directional where it cannot know the future words whereas in Bi-LSTM we can predict the future use of words as there is a backward information passed on from the other RNN layer in reverse. \n\nThere is only one change made in the code compared to the LSTM, here we use Bidirectional() function and call LSTM inside.","bbc2a7a3":"## Count of news subject\nLet's start by looking at the count of news types in our dataset"}}