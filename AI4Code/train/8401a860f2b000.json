{"cell_type":{"11c72b65":"code","ccdfdd45":"code","6260cf8c":"code","89606357":"code","7c8be1f5":"code","467ba9b0":"code","377e04c1":"code","acda7616":"code","d74946a4":"code","9ff1ff32":"code","7107efd6":"code","22ee0c1e":"code","9ae84162":"code","e64bab87":"code","66a168f5":"code","fd261cd8":"code","2b6eb2e1":"code","36e20e61":"code","419c26dd":"code","a86c5cda":"code","f1016947":"code","00e2d342":"markdown","57706ab2":"markdown","0f726598":"markdown","46d66043":"markdown","30c16460":"markdown","c4aff82a":"markdown","a4c24d76":"markdown"},"source":{"11c72b65":"%matplotlib inline\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nfrom skimage.segmentation import mark_boundaries\nDATA_DIR = os.path.join('..', 'input')","ccdfdd45":"class_str = \"\"\"car, 33\nmotorbicycle, 34\nbicycle, 35\nperson, 36\nrider, 37\ntruck, 38\nbus, 39\ntricycle, 40\nothers, 0\nrover, 1\nsky, 17\ncar_groups, 161\nmotorbicycle_group, 162\nbicycle_group, 163\nperson_group, 164\nrider_group, 165\ntruck_group, 166\nbus_group, 167\ntricycle_group, 168\nroad, 49\nsiderwalk, 50\ntraffic_cone, 65\nroad_pile, 66\nfence, 67\ntraffic_light, 81\npole, 82\ntraffic_sign, 83\nwall, 84\ndustbin, 85\nbillboard, 86\nbuilding, 97\nbridge, 98\ntunnel, 99\noverpass, 100\nvegatation, 113\nunlabeled, 255\"\"\"\nclass_dict = {v.split(', ')[0]: int(v.split(', ')[-1]) for v in class_str.split('\\n')}\n# we will just try to find moving things\ncar_classes = [ 'bus',  'car', 'bus_group', 'car_groups', 'truck', 'truck_group']\ncar_idx = [v for k,v in class_dict.items() if k in car_classes]\ndef read_label_image(in_path):\n    idx_image = imread(in_path)\/\/1000\n    return np.isin(idx_image.ravel(), car_idx).reshape(idx_image.shape).astype(np.float32)","6260cf8c":"group_df = pd.read_csv('..\/input\/label-analysis\/label_breakdown.csv', index_col = 0)\n# fix the paths\ngroup_df['color'] = group_df['color'].map(lambda x: x.replace('\/input\/', '\/input\/cvpr-2018-autonomous-driving\/'))\ngroup_df['label'] = group_df['label'].map(lambda x: x.replace('\/input\/', '\/input\/cvpr-2018-autonomous-driving\/'))\ngroup_df.sample(3)","89606357":"def total_car_vol(in_row):\n    out_val = 0.0\n    for k in car_classes:\n        out_val += in_row[k]\n    return out_val\ngroup_df['total_vehicle'] = group_df.apply(total_car_vol,1)\ngroup_df['total_vehicle'].plot.hist(bins = 50, normed = True)\ntrain_df = group_df.sort_values('total_vehicle', ascending = False).head(1000)\ntrain_df['total_vehicle'].plot.hist(bins = 50, normed = True)\nprint(train_df.shape[0], 'rows')","7c8be1f5":"sample_rows = 6\nfig, m_axs = plt.subplots(sample_rows, 3, figsize = (20, 6*sample_rows))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2, ax3), (_, c_row) in zip(m_axs, train_df.sample(sample_rows).iterrows()):\n    c_img = imread(c_row['color'])\n    l_img = read_label_image(c_row['label'])\n    ax1.imshow(c_img)\n    ax1.set_title('Color')\n    \n    ax2.imshow(l_img, cmap = 'nipy_spectral')\n    ax2.set_title('Labels')\n    xd, yd = np.where(l_img)\n    bound_img = mark_boundaries(image = c_img, label_img = l_img, color = (1,0,0), background_label = 255, mode = 'thick')\n    ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n    ax3.set_title('Cropped Overlay')","467ba9b0":"from sklearn.model_selection import train_test_split\ntrain_split_df, valid_split_df = train_test_split(train_df, random_state = 2018, test_size = 0.25)\nprint('Training Images', train_split_df.shape[0])\nprint('Holdout Images', valid_split_df.shape[0])","377e04c1":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import preprocess_input\nIMG_SIZE = (1024, 1024) # many of the ojbects are small so 512x512 lets us see them\nimg_gen_args = dict(samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.05, \n                              width_shift_range = 0.02, \n                              rotation_range = 3, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range = 0.05)\nrgb_gen = ImageDataGenerator(preprocessing_function = preprocess_input, **img_gen_args)\nlab_gen = ImageDataGenerator(**img_gen_args)","acda7616":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, seed = None, **dflow_args):\n    base_dir = os.path.dirname(in_df[path_col].values[0])\n    print('## Ignore next message from keras, values are replaced anyways: seed: {}'.format(seed))\n    df_gen = img_data_gen.flow_from_directory(base_dir, \n                                     class_mode = 'sparse',\n                                              seed = seed,\n                                    **dflow_args)\n    df_gen.filenames = in_df[path_col].values\n    df_gen.classes = np.stack(in_df[y_col].values)\n    df_gen.samples = in_df.shape[0]\n    df_gen.n = in_df.shape[0]\n    df_gen._set_index_array()\n    df_gen.directory = '' # since we have the full path\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n    return df_gen","d74946a4":"import keras.preprocessing.image as KPImage\nfrom PIL import Image\nclass pil_image_awesome():\n    @staticmethod\n    def open(in_path):\n        if 'instanceIds' in in_path:\n            # we only want to keep the positive labels not the background\n            return Image.fromarray(read_label_image(in_path))\n        else:\n            return Image.open(in_path)\n    fromarray = Image.fromarray\nKPImage.pil_image = pil_image_awesome","9ff1ff32":"from skimage.filters.rank import maximum\nfrom scipy.ndimage import zoom\ndef lab_read_func(in_path):\n    bin_img = (imread(in_path)>1000).astype(np.uint8)\n    x_dim, y_dim = bin_img.shape\n    max_label_img = maximum(bin_img, np.ones((x_dim\/\/IMG_SIZE[0], y_dim\/\/IMG_SIZE[1])))\n    return np.expand_dims(zoom(max_label_img, (IMG_SIZE[0]\/x_dim, IMG_SIZE[1]\/y_dim), order = 3), -1)\n\n\ndef train_and_lab_gen_func(in_df, batch_size = 8, seed = None):\n    if seed is None:\n        seed = np.random.choice(range(1000))\n    train_rgb_gen = flow_from_dataframe(rgb_gen, in_df, \n                             path_col = 'color',\n                            y_col = 'id', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = batch_size,\n                                   seed = seed)\n    train_lab_gen = flow_from_dataframe(lab_gen, in_df, \n                             path_col = 'label',\n                            y_col = 'id', \n                            target_size = IMG_SIZE,\n                             color_mode = 'grayscale',\n                            batch_size = batch_size,\n                                   seed = seed)\n    for (x, _), (y, _) in zip(train_rgb_gen, train_lab_gen):\n        yield x, y\n    \ntrain_and_lab_gen = train_and_lab_gen_func(train_split_df, batch_size = 8)\nvalid_and_lab_gen = train_and_lab_gen_func(valid_split_df, batch_size = 8)","7107efd6":"(rgb_batch, lab_batch) = next(valid_and_lab_gen)\n\nsample_rows = 4\nfig, m_axs = plt.subplots(sample_rows, 3, figsize = (20, 6*sample_rows))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2, ax3), rgb_img, lab_img in zip(m_axs, rgb_batch, lab_batch):\n    # undoing the vgg correction is tedious\n    r_rgb_img = np.clip(rgb_img+110, 0, 255).astype(np.uint8)\n    ax1.imshow(r_rgb_img)\n    ax1.set_title('Color')\n    ax2.imshow(lab_img[:,:,0], cmap = 'nipy_spectral')\n    ax2.set_title('Labels')\n    if lab_img.max()>0.1:\n        xd, yd = np.where(lab_img[:,:,0]>0)\n        bound_img = mark_boundaries(image = r_rgb_img, label_img = lab_img[:,:,0], \n                                    color = (1,0,0), background_label = 255, mode = 'thick')\n        ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n        ax3.set_title('Cropped Overlay')","22ee0c1e":"out_depth = 128\nscale_factor = 2","9ae84162":"import tensorflow as tf\ndef add_simple_grid_tf(in_layer,  # type: tf.Tensor\n                       x_cent=0.0,  # type: tf.Tensor\n                       y_cent=0.0,  # type: tf.Tensor\n                       x_wid=1.0,  # type: tf.Tensor\n                       y_wid=1.0,  # type: tf.Tensor\n                       z_cent=None,  # type: Optional[tf.Tensor]\n                       concat=False\n                       ):\n    # type: (...) -> tf.Tensor\n    \"\"\"\n    Adds spatial grids to images for making segmentation easier\n    :param in_layer: the base image to use for x,y dimensions\n    :param x_cent: the x mid coordinate\n    :param y_cent: the y mid coordinate\n    :param x_wid: the width in x (pixel spacing)\n    :param y_wid: the width in y (pixel spacing)\n    :param z_cent: the center location in z\n    :return:\n    \"\"\"\n    with tf.variable_scope('add_grid'):\n        batch_size = tf.shape(in_layer)[0]\n        xg_wid = tf.shape(in_layer)[1]\n        yg_wid = tf.shape(in_layer)[2]\n        x_min = x_cent - x_wid\n        x_max = x_cent + x_wid\n        y_min = y_cent - y_wid\n        y_max = y_cent + y_wid\n\n        if z_cent is None:\n            xx, yy = tf.meshgrid(tf.linspace(x_min, x_max, xg_wid),\n                                 tf.linspace(y_min, y_max, yg_wid),\n                                 indexing='ij')\n        else:\n            xx, yy, zz = tf.meshgrid(tf.linspace(x_min, x_max, xg_wid),\n                                     tf.linspace(y_min, y_max, yg_wid),\n                                     tf.linspace(z_cent, z_cent, 1),\n                                     indexing='ij')\n\n        xx = tf.reshape(xx, (xg_wid, yg_wid, 1))\n        yy = tf.reshape(yy, (xg_wid, yg_wid, 1))\n        if z_cent is None:\n            xy_vec = tf.expand_dims(tf.concat([xx, yy], -1), 0)\n        else:\n            zz = tf.reshape(zz, (xg_wid, yg_wid, 1))\n            xy_vec = tf.expand_dims(tf.concat([xx, yy, zz], -1), 0)\n        txy_vec = tf.tile(xy_vec, [batch_size, 1, 1, 1])\n        if concat:\n            return tf.concat([in_layer, txy_vec], -1)\n        else:\n            return txy_vec","e64bab87":"from keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Dropout, Flatten, Reshape, Dense, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\n\n# Build U-Net model\ninputs = Input((None, None)+(3,))\ns = BatchNormalization()(inputs) # we can learn the normalization step\ns = Dropout(0.5)(s)\n\nc1 = Conv2D(scale_factor*8, (3, 3), activation='relu', padding='same') (s)\nc1 = Conv2D(scale_factor*8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(scale_factor*16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(scale_factor*16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(scale_factor*32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(scale_factor*32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(scale_factor*64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(scale_factor*64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\n\nc5 = Conv2D(scale_factor*128, (3, 3), activation='relu', padding='same') (p4)\nc5 = Conv2D(scale_factor*128, (3, 3), activation='relu', padding='same') (c5)\n\n# spatial layers and some post processing before combining with real layers\nnew_c5 = Lambda(add_simple_grid_tf, name = 'JustSpatialDimensions')(c5)\nnew_c5 = BatchNormalization()(new_c5)\nnew_c5 = Conv2D(out_depth\/\/2, (1, 1), activation='relu', padding='same')(new_c5)\nnew_c5 = concatenate([new_c5, c5], name = 'AddingSpatialComponents')\nnew_c5 = Conv2D(out_depth, (3, 3), activation='relu', padding='same')(new_c5)\nnew_c5 = Conv2D(out_depth, (1, 1), activation='relu', padding='same')(new_c5)\n\nu6 = Conv2DTranspose(scale_factor*64, (2, 2), strides=(2, 2), padding='same') (new_c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(scale_factor*64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(scale_factor*64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(scale_factor*32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(scale_factor*32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(scale_factor*32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(scale_factor*16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(scale_factor*16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(scale_factor*16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(scale_factor*8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(scale_factor*8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(scale_factor*8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nprint(model.predict(rgb_batch[:1]).shape, 'make sure model works')\nmodel.summary()","66a168f5":"import tensorflow as tf\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\n# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec))\n\nsmooth = 1.\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef dice_bce_loss(y_true, y_pred):\n    return 0.5*binary_crossentropy(y_true, y_pred)-dice_coef(y_true, y_pred)\n\nmodel.compile(optimizer = 'adam', \n                   loss = dice_bce_loss, \n                   metrics = [dice_coef, 'binary_accuracy', 'mse'])","fd261cd8":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('unet')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","2b6eb2e1":"# reset the generators so they all have different seeds when multiprocessing lets loose\nfrom IPython.display import clear_output\nbatch_size = 8\ntrain_and_lab_gen = train_and_lab_gen_func(train_split_df, batch_size = batch_size)\nvalid_and_lab_gen = train_and_lab_gen_func(valid_split_df, batch_size = batch_size)\nmodel.fit_generator(train_and_lab_gen, \n                    steps_per_epoch = 2048\/\/batch_size,\n                    validation_data = valid_and_lab_gen,\n                    validation_steps = 256\/\/batch_size,\n                    epochs = 2, \n                    workers = 2,\n                    max_queue_size=3,\n                    use_multiprocessing = True,\n                    callbacks = callbacks_list)\nclear_output()","36e20e61":"model.load_weights(weight_path)\nmodel.save('vehicle_unet.h5')","419c26dd":"# Show the performance on a small batch since we delete the other messages\neval_out =  model.evaluate_generator(valid_and_lab_gen, steps=8)\nclear_output()","a86c5cda":"print('Loss: %2.2f, DICE: %2.2f, Accuracy %2.2f%%, Mean Squared Error: %2.2f' % (eval_out[0], eval_out[1], eval_out[2]*100, eval_out[3]))","f1016947":"(rgb_batch, lab_batch) = next(valid_and_lab_gen)\nsample_rows = 8\nfig, m_axs = plt.subplots(sample_rows, 5, figsize = (20, 6*sample_rows), dpi = 120)\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2, ax2_pred, ax3, ax3_pred), rgb_img, lab_img in zip(m_axs, rgb_batch, lab_batch):\n    # undoing the vgg correction is tedious\n    r_rgb_img = np.clip(rgb_img+110, 0, 255).astype(np.uint8)\n    lab_pred = model.predict(np.expand_dims(rgb_img, 0))[0]\n    \n    ax1.imshow(r_rgb_img)\n    ax1.set_title('Color')\n    ax2.imshow(lab_img[:,:,0], cmap = 'bone_r')\n    ax2.set_title('Labels')\n    ax2_pred.imshow(lab_pred[:,:,0], cmap = 'bone_r')\n    ax2_pred.set_title('Pred Labels')\n    if lab_img.max()>0.1:\n        xd, yd = np.where(lab_img[:,:,0]>0)\n        bound_img = mark_boundaries(image = r_rgb_img, label_img = lab_img[:,:,0], \n                                    color = (1,0,0), background_label = 255, mode = 'thick')\n        ax3.imshow(bound_img[xd.min():xd.max(), yd.min():yd.max(),:])\n        ax3.set_title('Cropped Overlay')\n        bound_pred = mark_boundaries(image = r_rgb_img, label_img = (lab_pred[:,:,0]>0.5).astype(int), \n                                    color = (1,0,0), background_label = 0, mode = 'thick')\n        ax3_pred.imshow(bound_pred[xd.min():xd.max(), yd.min():yd.max(),:])\n        ax3_pred.set_title('Cropped Prediction')\nfig.savefig('trained_model.png')","00e2d342":"# Showing the results\nHere we can preview the output of the model on a few examples","57706ab2":"## Replace PIL with scikit-image \nThis lets us handle the 16bit numbers well in the instanceIds image. This is incredibly, incredibly hacky, please do not use this code outside of this kernel.","0f726598":"1. # Let's train with very vehicle images\nHere we select a group of images with lots of vehicle in them to make the dataset less imbalanced.","46d66043":"# Add a spatial information\nWe use the meshgrid functionality to add a linearly spaced $xx$ and $yy$ dimension to the image so the CNN knows where it is\n","30c16460":"# Create the generators\nWe want to generate parallel streams of images and labels","c4aff82a":"# Overview\nThe notebook aims to organize the data and hack Keras so that we can train a model in a fairly simple way. The aim here is to get a model working that can reliably segment the images into objects and then we can make a model that handles grouping the objects into categories based on the labels. As you will see the Keras requires a fair bit of hackery to get it to load images from a dataframe and then get it to read the label images correctly (uint16 isn't supported well). Once that is done, training a U-Net model is really easy.\n\n## Focus\nThe focus here is to get the vehicles as accurately as possible without looking at the other classes. We can also try to differentiate between the various class","a4c24d76":"# Explore the training set\nHere we can show the training data image by image to see what exactly we are supposed to detect with the model"}}