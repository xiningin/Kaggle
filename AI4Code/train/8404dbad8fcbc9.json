{"cell_type":{"89096a4c":"code","5720f0ec":"code","1331135b":"code","4e194e22":"code","ca82ac09":"code","bb9f46bc":"code","1de1d41d":"code","be788596":"code","ee15b968":"code","7fb76a6c":"code","d985cd49":"code","ab88acf9":"code","1b5e4820":"code","c0e1b03d":"code","b21f2a3f":"code","6b9694a8":"code","6f5de648":"code","6bb65a03":"code","66612f80":"code","67d8ff5a":"code","6236991e":"code","8f0ec332":"code","2e3109d2":"code","ecb9fa91":"code","ede83a64":"code","403cf630":"code","4ead5e57":"code","083f3480":"code","bb601e04":"code","f733b42a":"code","02f1dfa9":"code","5912f467":"code","ef58f824":"code","7aaaf2c3":"code","bd4c8b2b":"code","08140649":"code","199011f6":"code","cf7d67a7":"code","087630a2":"code","95bc42ea":"code","994ac779":"code","f032adfe":"code","52dfc325":"markdown","f25c9931":"markdown","31bc875e":"markdown","34f5637c":"markdown","654c5786":"markdown","d191472d":"markdown","7298f515":"markdown","61269ccc":"markdown","2533b841":"markdown","119f721c":"markdown","7ab1ae59":"markdown","0dd456f9":"markdown","d702b833":"markdown","bb605a08":"markdown","1c4ac24d":"markdown","f609eb14":"markdown","b209b8e2":"markdown","3ec9f3e6":"markdown","84f41534":"markdown","0a1d0465":"markdown","ab9f0eac":"markdown","cee9b264":"markdown","8c4d3b82":"markdown","d39ea270":"markdown","f6d297be":"markdown","2d6f0246":"markdown"},"source":{"89096a4c":"!pip install seaborn --upgrade","5720f0ec":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.float_format = '{:.1f}'.format  # 1 decimal only for float numbers display\nnp.set_printoptions(suppress=True)","1331135b":"sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\ncategories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","4e194e22":"sales.head()","ca82ac09":"sales_train = sales.join(other=shops, on=\"shop_id\", how=\"inner\", rsuffix=\"_\").join(items, on=\"item_id\", how=\"inner\", rsuffix=\"_\").join(categories, on=\"item_category_id\", how=\"inner\", rsuffix=\"_\")\nsales_train.drop(['shop_id_', 'item_id_', 'item_category_id_'], axis=1, inplace=True)","bb9f46bc":"sales_train.head()","1de1d41d":"sales_train.info()","be788596":"sales_train.describe()","ee15b968":"len(sales_train[sales_train['item_cnt_day'] < 0])","7fb76a6c":"sales_train = sales_train[(sales_train['item_cnt_day'] > 0) & (sales_train['item_price'] > 0)]\nsales_train.isna().sum()","d985cd49":"plt.scatter(sales_train.index, sales_train['item_cnt_day'])\nplt.show()","ab88acf9":"p = 99\npercentile = np.percentile(sales_train['item_cnt_day'], p)\nprint(f\"The item_cnt_day {p}th percentile is equal to {percentile}\")","1b5e4820":"p = 99.9\npercentile = np.percentile(sales_train['item_cnt_day'], p)\nprint(f\"The item_cnt_day {p}th percentile is equal to {percentile}\")","c0e1b03d":"outliers_num = len(sales_train[sales_train['item_cnt_day'] > percentile])\nsales_train = sales_train[sales_train['item_cnt_day'] < percentile]\nprint(f\"We removed {outliers_num} outliers from the data\")","b21f2a3f":"print(sales_train['item_id'].nunique())\nprint(sales_train['shop_id'].nunique())\nprint(len(test))","6b9694a8":"print(f\"Min date from train set: {sales_train['date'].min().date()}\")\nprint(f\"Max date from train set: {sales_train['date'].max().date()}\") \n\nsales_train['month'] = sales_train['date'].dt.month\nsales_train['year'] = sales_train['date'].dt.year","6f5de648":"N = 15\n\nitems_total_sold = sales_train.groupby('item_id').sum()\nitems_total_sold.reset_index(inplace=True)\n\nidxs = items_total_sold['item_cnt_day'].values.argsort()[::-1][0:N]\ntemp = items_total_sold['item_cnt_day'].to_numpy()\nmax_sold = [temp[idx] for idx in idxs]\n\nitem_ids = items_total_sold.loc[idxs,'item_id'].values\nitem_names = items.loc[item_ids, 'item_name']\n\nfig, ax = plt.subplots(figsize=(16,12))\nbarplot = sns.barplot(x=max_sold, y=item_names)\nfor p in barplot.patches:\n    barplot.text(p.get_width(), p.get_y()+0.55*p.get_height(),\n                 '{:1.0f}'.format(p.get_width()),\n                 ha='center', va='center')\nbarplot.set(xlabel=\"Number of items sold\", ylabel=\"Items\", title=f\"Top {N} items sold\")\nplt.show()","6bb65a03":"sales_train[\"transaction_price\"] = sales_train[\"item_cnt_day\"] * sales_train[\"item_price\"]\ntotal_revenue_shops = sales_train.groupby(\"shop_id\").agg({'transaction_price': ['sum']})\ntotal_shops = total_revenue_shops['transaction_price']['sum'].sort_values(ascending=False)\nindex = total_shops.index\n\nsns.set_theme(context=\"notebook\", style=\"whitegrid\", font_scale=1.3)\n\nfig, ax = plt.subplots(figsize=(20,10))\nbarplot = sns.barplot(x=index, y=total_shops, order=index)\nbarplot.set(xlabel=\"Shop id\", ylabel=\"Total revenues\", title=\"Shops's rank in term of revenues\")\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","66612f80":"avg_total_sales_day = sales_train.groupby('date').agg({'item_cnt_day': ['sum']}).mean().values[0]\nprint(f\"In average, total sales per day is about {round(avg_total_sales_day)} items sold.\")","67d8ff5a":"total_sales_day = sales_train.groupby('date').agg({'item_cnt_day': ['sum']})['item_cnt_day']['sum']\n\nfig, ax = plt.subplots(figsize=(20,10))\nlineplot = sns.lineplot(x=total_sales_day.index, y=total_sales_day)\nlineplot.set(xlabel=\"Date\", ylabel=\"Total sales\", title=\"Total day sales through time among all shops\")\nplt.show()","6236991e":"avg_total_sales_month = sales_train.groupby(['year', 'month']).agg({'item_cnt_day': ['sum']}).mean().values[0]\nprint(f\"In average, total sales per month is about {round(avg_total_sales_month)} items sold.\")","8f0ec332":"y = sales_train.groupby(['year','month']).agg({'item_cnt_day': ['sum']})['item_cnt_day']['sum'].values\n\ndate = []\nyears=['2013','2014','2015']\nfor year in years:\n    for month in range(1,13):\n        if month<10:\n            date.append(year+'-0'+str(month))\n        else:\n            date.append(year+'-'+str(month))\n\nfig, ax = plt.subplots(figsize=(20,10))\nlineplot = sns.lineplot(x=date, y=y)\nlineplot.set(xlabel=\"Date\", ylabel=\"Total sales\", title=\"Total month sales through time among all shops\")\nlineplot.set_xticklabels(date, rotation=50)\nplt.show()","2e3109d2":"sales_train.groupby('date').agg({'item_cnt_day': ['sum']}).tail(12)","ecb9fa91":"N = 10\n\ntotal_categories_sold = sales_train.groupby('item_category_id').sum()\nitems_sold_by_category = total_categories_sold['item_cnt_day'].nlargest(n=N)\nidxs = items_sold_by_category.index\n\ncategories_names = categories.loc[idxs, 'item_category_name']\n\nfig, ax = plt.subplots(figsize=(16,8))\nbarplot = sns.barplot(x=categories_names, y=items_sold_by_category)\nbarplot.set(xlabel=\"Category name\", ylabel=\"Total items sold\", title=f\"Top {N} most popular categories\")\nfor label in barplot.get_xticklabels():\n    label.set_rotation(50)\nfor p in barplot.patches:\n    barplot.annotate(format(p.get_height(), '1.0f'), \n                    (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                     ha='center', va='center', \n                     xytext=(0, 7), \n                     textcoords='offset points')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","ede83a64":"data_sales = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)['item_cnt_day'].sum()\ndata_sales.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)\ndata_sales = pd.pivot_table(data_sales, index=['shop_id','item_id'],columns=['date_block_num'], fill_value=0)","403cf630":"data_sales","4ead5e57":"df_sales = data_sales.merge(test, how=\"right\", on=['shop_id','item_id'])\nper = round(df_sales.isna().sum()[2] \/ len(df_sales), 2)\ndf_sales","083f3480":"print(f\"The percentage of (shop_id, item_id) combinations with NaN values is {per}\")","bb601e04":"df_sales.fillna(0, inplace=True)\ndf_sales.drop(['shop_id', 'item_id', 'ID'], axis=1, inplace=True)\ndf_sales.head()","f733b42a":"from sklearn.model_selection import train_test_split\n\nX, y = df_sales.drop(labels=[('item_cnt_month',33)], axis=1).values, df_sales.values[:,-1]\nX_test = df_sales.drop(labels=[('item_cnt_month',0)], axis=1).values\n\n# We need to reshape the data accordingly to Keras input shape (bach_size, timesteps, features)\nX = X.reshape(X.shape[0], X.shape[1], 1)\ny = y.reshape(y.shape[0], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, shuffle=True, random_state=0)\n\nprint(\"X_train shape : \", X_train.shape)\nprint(\"y_train shape : \", y_train.shape)\nprint(\"X_val shape : \", X_val.shape)\nprint(\"y_val shape : \", y_val.shape)\nprint(\"X_test shape : \", X_test.shape)","02f1dfa9":"from tensorflow.keras import Sequential, Input\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ntf.keras.backend.set_floatx('float64')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, mode='auto')\n\ndef plot_train_val_curves(hist):\n    fig, (ax1, ax2) = plt.subplots(2, figsize=(15,8), sharex=True)\n    fig.suptitle('Loss and MSE train and validation curves')\n    ax1.plot(hist.history['loss'], label='train')\n    ax1.plot(hist.history['val_loss'], label='validation')\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n    ax2.plot(hist.history['mean_squared_error'], label='train')\n    ax2.plot(hist.history['val_mean_squared_error'], label='validation')\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"MSE\")\n    ax2.legend()\n    plt.show()\n\ndef LSTM_model(shape, units=[64,64], dropout=0.3):\n    model = Sequential()\n    model.add(Input(shape=shape, dtype='float64'))\n    model.add(LSTM(units=units[0], activation='tanh', return_sequences=True))\n    model.add(Dropout(dropout))\n    model.add(LSTM(units=units[1], activation='tanh'))\n    model.add(Dropout(dropout))\n    model.add(Dense(1, activation=None, kernel_initializer=tf.initializers.zeros()))\n    return model","5912f467":"lstm_model = LSTM_model(shape=(X_train.shape[1],X_train.shape[2]), units=[128,128], dropout=0.4)\nlstm_model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=['mean_squared_error'])\nlstm_model.summary()\nhistory = lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=4096, epochs=10, callbacks=[early_stopping])\nlstm_model.save(\"lstm_model\", overwrite=True)","ef58f824":"plot_train_val_curves(history)","7aaaf2c3":"predictions = lstm_model.predict(X_test)\npredictions = predictions.clip(0,20)","bd4c8b2b":"per = round(predictions[predictions > 1].sum() \/ len(predictions), 2) * 100\nprint(f\"{per} % of sales predictions are higher than 1\")","08140649":"test_submission = pd.DataFrame({'ID': test['ID'], 'item_cnt_month': predictions.flatten()})\ntest_submission.to_csv('test_submission.csv', index=False) ","199011f6":"from sklearn.preprocessing import MinMaxScaler\n\n# We scale item_price in our train and test set. This will stabilize training and make it faster.\nscaler = MinMaxScaler()\nsales_train['item_price'] = scaler.fit_transform(sales_train['item_price'].values.reshape(-1,1))","cf7d67a7":"data_prices = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)['item_price'].mean()\ndata_prices.rename(columns={'item_price': 'item_price_mean'}, inplace=True)\ndata_prices = pd.pivot_table(data_prices, index=['shop_id','item_id'], columns=['date_block_num'], fill_value=0)\ndata_prices","087630a2":"df_prices = data_prices.merge(test, how=\"right\", on=['shop_id','item_id']).fillna(0)\ndf_prices.drop(['shop_id', 'item_id', 'ID'], axis=1, inplace=True)\n\n# get each feature for training data and labels \nX_sales = df_sales.drop(labels=[('item_cnt_month',33)], axis=1).values\nX_prices = df_prices.drop(labels=[('item_price_mean',33)], axis=1).values\ny = df_sales.values[:,-1]\n\n# reshape arrays\nX_sales = X_sales.reshape(X_sales.shape[0], X_sales.shape[1], 1)\nX_prices = X_prices.reshape(X_prices.shape[0], X_prices.shape[1], 1)\n# combine sales and prices in the same numpy array\nX = np.append(X_sales, X_prices, axis=2)\ny = y.reshape(y.shape[0], 1)\n\n# construct test data (X_test)\nX_sales_test = df_sales.drop(labels=[('item_cnt_month',0)], axis=1).values\nX_prices_test = df_prices.drop(labels=[('item_price_mean',0)], axis=1).values\nX_sales_test = X_sales_test.reshape(X_sales_test.shape[0], X_sales_test.shape[1], 1)\nX_prices_test = X_prices_test.reshape(X_prices_test.shape[0], X_prices_test.shape[1], 1)\nX_test = np.append(X_sales_test, X_prices_test, axis=2)\n\n# split data in train and validation sets \nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, shuffle=True, random_state=0)\n\nprint(\"X shape : \", X.shape)\nprint(\"y shape : \", y.shape)\nprint(\"X_train shape : \", X_train.shape)\nprint(\"y_train shape : \", y_train.shape)\nprint(\"X_val shape : \", X_val.shape)\nprint(\"y_val shape : \", y_val.shape)\nprint(\"X_test shape : \", X_test.shape)","95bc42ea":"mult_lstm_model = LSTM_model(shape=(X_train.shape[1],X_train.shape[2]), units=[128,128], dropout=0.4)\nmult_lstm_model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=['mean_squared_error'])\nmult_lstm_model.summary()\nhistory = mult_lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=4096, epochs=10, callbacks=[early_stopping])\nmult_lstm_model.save(\"mult_lstm_model\", overwrite=True)","994ac779":"plot_train_val_curves(history)","f032adfe":"predictions = mult_lstm_model.predict(X_test)\npredictions = predictions.clip(0,20)\ntest_submission = pd.DataFrame({'ID': test['ID'], 'item_cnt_month': predictions.flatten()})\ntest_submission.to_csv('test_submission.csv', index=False) ","52dfc325":"We can confirm the trends described earlier in this plot and also say that there is a continued decline in total month sales over time. For instance, total month sales were about 120 000 items sold in january 2013, then went down to near 100 000 in january 2014 and finally only 80 000 in january 2015. A reason that could explain the general decrease is the fact that the list of shops and products slightly changes every month as written in the \"Data\" section of this kaggle project. Maybe some high-performance shops have left the list of shops or\/and some popular items have left the list of items.   \n\nMoreover, it seems like there is a huge drop in sales since october 2015 (going from 68 000 to 28 000 next month, almost a division of sales by a factor of 2,5 !). It's pretty unusual as this period is normally very conductive to sales as Christmas Eve approaches. That happened because of missing data between 2015-11-10 and 2015-12-01 (so 20 missing days of data) as suggested by the day sales plot. Maybe an error during concatenation of data occured when building the dataset. \n\nTo conclude, the trend in sales is clearly downwards. ","f25c9931":"We want to upgrade the current version of seaborn in this kaggle kernel to the latest one.","31bc875e":"Finally, we export submission DataFrame in csv file without writting the default index in it (ID column is already an index). ","34f5637c":"# Predictions\n\n## Univariate LSTM Model","654c5786":"We want to remove item_cnt_day outliers from our data. We inspect the scatter plot and print somes percentiles to decide which threshold we shoud use to remove data.  ","d191472d":"That's interesting, 48% of rows of our merged DataFrame are (shop_id, item_id) combinations which were not find in the data DataFrame (training set) so their item_cnt_month cells are full empty with NaN values. We must fill them with 0 as we can't do anything better right now. Intuitively, putting 0 sales for every (item_id shop_id) combination that isn't in training data means that every item not listed in a shop hasn't been sold.  \nWe also delete shop_id, item_id and ID columns, they are not useful for our model. Indeed we want to create an LSTM model which will be feed on time series data, i.e. monthly sales for each combination. ","7298f515":"## How sales behave globally ?","61269ccc":"## What are the 15 most sold items ?","2533b841":"Why is validation loss and MSE a little bit lower than training loss and MSE ? At first look, that's counter intuitive but it is actually normal in this case when we think about it. Our LSTM architecture includes 2 dropout layers that are used during training loop but are NOT in validation loop (Keras default behavior). Regularization methods often sacrifice training accuracy to improve validation\/testing accuracy. This fact can explain by itself why it gives more accurate predictions on validation data and so a lower loss. Another reason is that training loss is measured during each epoch while validation loss is measured after each epoch. Training loss is continually reported over the course of an entire epoch, however, validation metrics are computed over the validation set only once the current training epoch is completed. A third reason would be that training and validation distributions are different but that's not our case as we shuffled our data before splitting it. ","119f721c":"# EDA","7ab1ae59":"We can say from this plot that movies popularity is way ahead from the others categories with 634 171 items sold in total. DVD quality (720x576 pixels resolution) is less good than Blu-Ray quality (1920x1080 pixels resolution) but it's still the most prefered category. It could be explained by multiple reasons including the price difference between those two categories (DVD is cheaper than Blu-Ray), the fact that Blu-Ray doesn't have a movie catalog as large as the DVDs (especially old titles) and also because there is still a significant part of russian families that don't have a full HD compatible TV to be able to play Blu-Ray movies.  \nThen we find PC games (standard and additional editions) as well as console games (PS3, XBOX 360, PS4 but not Xbox One, Sony seems to have one the latest generation of game console) and also music CD and albums. ","0dd456f9":"Let's build an univariate time series predictive model that will be able to forecast the next month sales based on month sales sequences. \nWe group data by month, shop_id, item_id and aggregate sales by month using sum function. We then use pivot table to present data in an interpretable way (shop_id, item_id as index and month as columns). ","d702b833":"# Predict Future Sales","bb605a08":"We can observe that the most sold item is nothing but \"Branded package T-shirt 1C Interest white (34 * 42)\" (using google traduction of course, I can't understand russian yet). It makes sense, white t-shirt is a very basic cloth whether for men or women that everyone should have in their wardrobe. The rest of most sold items are video games such as Diablo III, a Massive Online Multiplayer game, Grand Theft Auto V (on 3 different platforms : PS3, Xbox360, PC) a famous RPG, Battlefied, an Online multiplayer game, and many other popular games.  \n1C is one of the largest video game development, publishing and distribution studios in Russia so it's not surprising that almost only video games are among the highest ranked. ","1c4ac24d":"## What are the 10 most popular categories ?","f609eb14":"We can observe that some item_cnt_day values are negative. It seems that when an item is returned to the shop by a customer, a new row is created with a negative count to compensate for adding the item count when the customer first bought it. For the moment, we will just remove those rows from the DataFrame.","b209b8e2":"## Multivariate LSTM model","3ec9f3e6":"There is potentially 21807 x 60 (shop_id, item_id) combinations so 1 308 420 different combinations and that's pretty huge. Hopefully, we only need to forecast the combinations in the test file (214 200 combinations) and we also have to keep in mind that not all items are in all shops. ","84f41534":"We predict November 2015 sales using our trained model. We replace negative predictions by zero values (no sales) and large values by 20, in another words, we clip predictions between 0 and 20. As we have seen at the beginning of this notebook, the distribution of sales lies in this area (outliers are above). ","0a1d0465":"We must now merge the test dataset with our data DataFrame because we only need to predict the (shop_id,item_id) combinations contained in the test dataset.","ab9f0eac":"We make a join to join sales_train with shops, items and item categories DataFrames so we can get all the data we are looking for in the same DataFrame. ","cee9b264":"This time, we are going to build a multivariate LSTM model by adding another important feature : item_price. We will aggregate monthly item_price using mean function. This feature will be a good indicator of the price variation and will help as a predictor to get more accurate forecast sales. We will use the same data processing stages that we used for the last model. We will get at the end 2 features for each timestep (1 timestep = 1 month) in our time series data.","8c4d3b82":"We prepare our train\/validation\/test sets properly. Our splitting strategy is the following :\n1. We train our time series model from 0 to 32 date_block_num (from January 2013 to September 2015) and try to predict 33 (October 2015 sales) in order to learn patterns from data.\n2. To better evaluate the model's performance (at each epoch), we split our data between train and validation sets (90% training and 10% validation). We use validation MSE (Mean Square Error) metric to monitor performance during training. \n3. We make predictions of November 2015 sales using our trained LSTM model with test set from 1 to 33 date_block_num (from February 2013 to October 2015). \n\nEach sets must have the same dimensions (so same number of timesteps as we want to build an univariate model here, i.e features=1). ","d39ea270":"It means that 99% of item_cnt_day values are less or equal to 5 (so lies between 0 and 5)","f6d297be":"## Shops ranking according to revenue","2d6f0246":"We can notice that there is a seasonal pattern in total sales that seems to repeat each year. Indeed, there is a huge increase during november until late december of total sales (sales peaks). We can interpret this phenomenon as the increase of consumer purchases for Christmas period and New Year celebration that could also be strengthen by discounts during this period. The highest peaks match with days just before New Year celebration which are in terms of total sales, approximatively 4 times higher than average."}}