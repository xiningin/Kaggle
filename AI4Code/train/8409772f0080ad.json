{"cell_type":{"811a24df":"code","8c353656":"code","199e3431":"code","2976a70b":"code","67a545e3":"code","ba75a236":"code","2dac3131":"code","997739f8":"code","9cb6f553":"code","515f8814":"code","04e488b3":"code","55ca83d4":"code","af24f15b":"code","9da7b93f":"code","438a51d6":"code","891c14f3":"code","f39a7f29":"code","213aeafa":"code","a670ac83":"code","08f62a3d":"code","9fcbc47c":"code","611604b5":"code","58641436":"code","3ec7709c":"code","d3a1a78f":"code","32b74334":"code","33b5f557":"code","ab6d2fbf":"code","353c9385":"code","2405099f":"code","eca14509":"code","0d87fc30":"code","385f5d9b":"code","419c2eda":"code","1a72507c":"code","9acda3a3":"code","07c4a0c2":"code","a0997304":"code","1cb48d30":"code","8639ea6c":"markdown","963baf19":"markdown","2053e8de":"markdown","3ba8a40d":"markdown","22eee196":"markdown","53fc13e9":"markdown","49728b6b":"markdown","59894fea":"markdown","bad835d8":"markdown","0418ff1f":"markdown","6320fcd5":"markdown","90cda01f":"markdown","b7e7d01e":"markdown","9a75ae07":"markdown","749ceb4e":"markdown","7e9f6d66":"markdown","33a2e20b":"markdown","339a7bd9":"markdown","f64b8948":"markdown","57cb9091":"markdown","ccc7c0b7":"markdown","d1756749":"markdown","cc74e319":"markdown","2a417935":"markdown","f1c144d7":"markdown","d1c82eaf":"markdown","2f562aed":"markdown","eb1b8652":"markdown","1964f14a":"markdown","3d1d050a":"markdown","4b5b249d":"markdown","d003f816":"markdown","fca78d79":"markdown","1435c647":"markdown"},"source":{"811a24df":"!pip install torch torchvision feather-format kornia pyarrow --upgrade   > \/dev\/null\n!pip install git+https:\/\/github.com\/fastai\/fastai_dev                    > \/dev\/null","8c353656":"from fastai2.basics           import *\nfrom fastai2.vision.all       import *\nfrom fastai2.medical.imaging  import *\nfrom fastai2.callback.tracker import *\nfrom fastai2.callback.all     import *\n\nnp.set_printoptions(linewidth=120)\nmatplotlib.rcParams['image.cmap'] = 'bone'","199e3431":"path = Path('..\/input\/rsna-intracranial-hemorrhage-detection\/')\npath_trn = path\/'stage_1_train_images'\npath_tst = path\/'stage_1_test_images'\n\npath_inp = Path('..\/input')\npath_xtra = path_inp\/'rsna-hemorrhage-jpg'\npath_meta = path_xtra\/'meta'\/'meta'\npath_jpg = path_xtra\/'train_jpg'\/'train_jpg'","2976a70b":"df_comb = pd.read_feather(path_meta\/'comb.fth').set_index('SOPInstanceUID')\ndf_tst  = pd.read_feather(path_meta\/'df_tst.fth').set_index('SOPInstanceUID')\ndf_samp = pd.read_feather(path_meta\/'wgt_sample.fth').set_index('SOPInstanceUID')\nbins = (path_meta\/'bins.pkl').load()","67a545e3":"set_seed(42)\npatients = df_comb.PatientID.unique()\npat_mask = np.random.random(len(patients))<0.8\npat_trn = patients[pat_mask]","ba75a236":"def split_data(df):\n    idx = L.range(df)\n    mask = df.PatientID.isin(pat_trn)\n    return idx[mask],idx[~mask]\n\nsplits = split_data(df_samp)","2dac3131":"df_trn = df_samp.iloc[splits[0]]\np1 = L.range(df_samp)[df_samp.PatientID==df_trn.PatientID[0]]\nassert len(p1) == len(set(p1) & set(splits[0]))","997739f8":"def filename(o): return os.path.splitext(os.path.basename(o))[0]\n\nfns = L(list(df_samp.fname)).map(filename)\nfn = fns[0]\nfn","9cb6f553":"def fn2image(fn): return PILCTScan.create((path_jpg\/fn).with_suffix('.jpg'))\nfn2image(fn).show();","515f8814":"htypes = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\ndef fn2label(fn): return df_comb.loc[fn][htypes].values.astype(np.float32)\nfn2label(fn)","04e488b3":"bs,nw = 128,4","55ca83d4":"tfms = [[fn2image], [fn2label,EncodedMultiCategorize(htypes)]]\ndsrc = DataSource(fns, tfms, splits=splits)\nnrm = Normalize(tensor([0.6]),tensor([0.25]))\naug = aug_transforms(p_lighting=0.)\nbatch_tfms = [IntToFloatTensor(), nrm, Cuda(), *aug]","af24f15b":"def get_data(bs, sz):\n    return dsrc.databunch(bs=bs, num_workers=nw, after_item=[ToTensor],\n                          after_batch=batch_tfms+[AffineCoordTfm(size=sz)])","9da7b93f":"dbch = get_data(128, 96)\nxb,yb = to_cpu(dbch.one_batch())\ndbch.show_batch(max_n=4, figsize=(9,6))\nxb.mean(),xb.std(),xb.shape,len(dbch.train_dl)","438a51d6":"def accuracy_any(inp, targ, thresh=0.5, sigmoid=True):\n    inp,targ = flatten_check(inp[:,0],targ[:,0])\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()","891c14f3":"def get_loss(scale=1.0):\n    loss_weights = tensor(2.0, 1, 1, 1, 1, 1).cuda()*scale\n    return BaseLoss(nn.BCEWithLogitsLoss, pos_weight=loss_weights, floatify=True, flatten=False, \n        is_2d=False, activation=torch.sigmoid)","f39a7f29":"loss_func = get_loss(0.14*2)\nopt_func = partial(Adam, wd=0.01, eps=1e-3)\nmetrics=[accuracy_multi,accuracy_any]","213aeafa":"def get_learner():\n    dbch = get_data(128,128)\n    learn = cnn_learner(dbch, xresnet50, loss_func=loss_func, opt_func=opt_func, metrics=metrics)\n    return learn.to_fp16()","a670ac83":"learn = get_learner()","08f62a3d":"# lrf = learn.lr_find()","9fcbc47c":"def do_fit(bs,sz,epochs,lr, freeze=True):\n    learn.dbunch = get_data(bs, sz)\n    if freeze:\n        if learn.opt is not None: learn.opt.clear_state()\n        learn.freeze()\n        learn.fit_one_cycle(1, slice(lr))\n    learn.unfreeze()\n    learn.fit_one_cycle(epochs, slice(lr))","611604b5":"# do_fit(128, 96, 4, 1e-2)","58641436":"# do_fit(128, 160, 3, 1e-3)","3ec7709c":"fns = L(list(df_comb.fname)).map(filename)\nsplits = split_data(df_comb)","d3a1a78f":"def fix_pxrepr(dcm):\n    if dcm.PixelRepresentation != 0 or dcm.RescaleIntercept<-100: return\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000","32b74334":"def dcm_tfm(fn): \n    fn = (path_trn\/fn).with_suffix('.dcm')\n    try:\n        x = fn.dcmread()\n        fix_pxrepr(x)\n    except Exception as e:\n        print(fn,e)\n        raise SkipItemException\n    if x.Rows != 512 or x.Columns != 512: x.zoom_to((512,512))\n    px = x.scaled_px\n    return TensorImage(px.to_3chan(dicom_windows.brain,dicom_windows.subdural, bins=bins))","33b5f557":"dcm = dcm_tfm(fns[0])\nshow_images(dcm)\ndcm.shape","ab6d2fbf":"tfms = [[dcm_tfm], [fn2label,EncodedMultiCategorize(htypes)]]\ndsrc = DataSource(fns, tfms, splits=splits)\nbatch_tfms = [nrm, Cuda(), *aug]","353c9385":"def get_data(bs, sz):\n    return dsrc.databunch(bs=bs, num_workers=nw, after_batch=batch_tfms+[AffineCoordTfm(size=sz)])","2405099f":"dbch = get_data(64,256)\nx,y = to_cpu(dbch.one_batch())\ndbch.show_batch(max_n=4)\nx.shape","eca14509":"learn.loss_func = get_loss(1.0)","0d87fc30":"def fit_tune(bs, sz, epochs, lr):\n    dbch = get_data(bs, sz)\n    learn.dbunch = dbch\n    learn.opt.clear_state()\n    learn.unfreeze()\n    learn.fit_one_cycle(epochs, slice(lr))","385f5d9b":"# fit_tune(64, 256, 2, 1e-3)","419c2eda":"test_fns = [(path_tst\/f'{filename(o)}.dcm').absolute() for o in df_tst.fname.values]","1a72507c":"tst = test_dl(dbch, test_fns)\nx = tst.one_batch()[0]\nx.min(),x.max()","9acda3a3":"preds,targs = learn.get_preds(dl=tst)\npreds_clipped = preds.clamp(.0001, .999)","07c4a0c2":"ids = []\nlabels = []\n\nfor idx,pred in zip(df_tst.index, preds_clipped):\n    for i,label in enumerate(htypes):\n        ids.append(f\"{idx}_{label}\")\n        predicted_probability = '{0:1.10f}'.format(pred[i].item())\n        labels.append(predicted_probability)","a0997304":"# df_csv = pd.DataFrame({'ID': ids, 'Label': labels})\n# df_csv.to_csv(f'submission.csv', index=False)\n# df_csv.head()","1cb48d30":"from IPython.display import FileLink, FileLinks\n# FileLink('submission.csv')","8639ea6c":"We can use that to take just the patients in a dataframe that match that mask:","963baf19":"We need to create a `DataBunch` that contains our sample data, so we need a function to convert a filename (pointing at a DICOM file) into a path to our sample JPEG files:","2053e8de":"These functions are copied nearly verbatim from our [earlier cleanup notebook](https:\/\/www.kaggle.com\/jhoward\/cleaning-the-data-for-rapid-prototyping-fastai), so have a look there for details.","3ba8a40d":"We have some slight changes to our data source","22eee196":"Let's try it out!","53fc13e9":"Now let's fine tune this model on the full dataset. We'll need all the filenames now, not just the sample.","49728b6b":"The loss function in this competition is weighted, so let's train using that loss function too.","59894fea":"Now we can pre-train at different sizes.","bad835d8":"## Train vs valid","0418ff1f":"Run the code below if you want a link to download the submission file.","6320fcd5":"If you have a larger GPU or more workers, change batchsize and number-of-workers here:","90cda01f":"First we read in the metadata files (linked in the introduction).","b7e7d01e":"In the notebook \"[Cleaning the data for rapid prototyping](https:\/\/www.kaggle.com\/jhoward\/cleaning-the-data-for-rapid-prototyping-fastai)\" I showed how to create a small, fast, ready-to-use dataset for prototyping our models. The dataset created in that notebook, along with the metadata files it uses, are now [available here](https:\/\/www.kaggle.com\/jhoward\/rsna-hemorrhage-jpg).\n\nSo let's use them to create a model! In this notebook we'll see the whole journey from pre-training using progressive resizing on our prototyping sample, through to fine-tuning on the full dataset, and then submitting to the competition.\n\nI'm intentionally not doing any tricky modeling in this notebook, because I want to show the power of simple techniques and simples architectures. You should take this as a starting point and experiment! e.g. try data augmentation methods, architectures, preprocessing approaches, using the DICOM metadata, and so forth...\n\nWe'll be using the fastai.medical.imaging library here - for more information about this see the notebook [Some DICOM gotchas to be aware of](https:\/\/www.kaggle.com\/jhoward\/some-dicom-gotchas-to-be-aware-of-fastai). We'll also use the same basic setup that's in the notebook.\n\n*Update: I'm out of GPU hours and Kaggle isn't freezing when running the current version of the notebook. To see a complete run, see [this version](https:\/\/www.kaggle.com\/jhoward\/from-prototyping-to-submission-fastai?scriptVersionId=22577538). I've commented out the GPU calls in this run so I can run it end to end.*","9a75ae07":"Leslie Smith's famous LR finder will give us a reasonable learning rate suggestion.","749ceb4e":"We need to remove the sample scaling from our loss function, since we're using the full dataset.","7e9f6d66":"## Pretrain on sample","33a2e20b":"We also need to be able to grab the labels from this, which we can do by simply indexing into our sample `DataFrame`.","339a7bd9":"Now we're ready to create our learner. We can use mixed precision (fp16) by simply adding a call to `to_fp16()`!","f64b8948":"## Scale up to full dataset","57cb9091":"## Prepare for submission","ccc7c0b7":"Let's double-check that for a patient in the training set that their images are all in the first split.","d1756749":"We're going to use fastai's new [Transform Pipeline API](http:\/\/dev.fast.ai\/pets.tutorial.html) to create the DataBunch, since this is extremely flexible, which is great for intermediate and advanced Kagglers. (Beginners will probably want to stick with the Data Blocks API). We create two transform pipelines, one to open the image file, and one to look up the label and create a tensor of categories.","cc74e319":"We will grab our sample filenames for the initial pretraining.","2a417935":"## Prepare sample DataBunch","f1c144d7":"To get better validation measures, we should split on patients, not just on studies, since that's how the test set is created.\n\nHere's a list of random patients:","d1c82eaf":"Now we're ready to submit. We can use the handy `test_dl` function to get an inference `DataLoader` ready, then we can check it looks OK.","2f562aed":"Here's our main routine for changing the size of the images in our DataBunch, doing one fine-tuning of the final layers, and then training the whole model for a few epochs.","eb1b8652":"Now we can test it out:","1964f14a":"Let's track the accuracy of the *any* label as our main metric, since it's easy to interpret.","3d1d050a":"We'll scale the loss initially to account for our sampling (since the original data had 14% rows with a positive label, and we resampled it to 50\/50).","4b5b249d":"To support progressive resizing (one of the most useful tricks in the deep learning practitioner's toolbox!) we create a function that returns a dataset resized to a requested size:","d003f816":"I'm too lazy to write a function that creates a submission file, so this code is stolen from Radek, with minor changes.","fca78d79":"We pass that to `get_preds` to get our predictions, and then clamp them just in case we have some extreme values.","1435c647":"We can now fine-tune the final layers."}}