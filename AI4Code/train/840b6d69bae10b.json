{"cell_type":{"baee21da":"code","7870b26d":"code","7c837164":"code","e701a81d":"code","a4b1abb9":"code","d19c3c1e":"code","79805691":"code","59697033":"code","df135fd0":"code","6ffb1650":"code","a40b3309":"code","a4c7b469":"code","8e66eb9d":"code","8a81330a":"code","2888f49d":"code","611eb87a":"code","bb1856a9":"code","33e5ab1f":"markdown","36a938e0":"markdown","a68427c4":"markdown","95f0386d":"markdown","f7bc961f":"markdown","fcd291c6":"markdown","0b8c95c0":"markdown","5b7b0b5d":"markdown","f984b51c":"markdown","4b5f9902":"markdown","8deef0b1":"markdown","9784ce32":"markdown","60c5781d":"markdown","e58fa119":"markdown","91eb2512":"markdown","031a3140":"markdown","02b97368":"markdown","52dcd6b0":"markdown"},"source":{"baee21da":"#Let's import important libraries to analyze the data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelEncoder\nwarnings.filterwarnings('ignore')\n%matplotlib inline","7870b26d":"#read data from the file\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n#print 5 rows\ntrain_data.head(5)","7c837164":"train_data.shape\n#quick review \ntrain_data.describe()","e701a81d":"def check_dublicates (df):\n    unic = len(set(df.PassengerId))\n    total = df.shape[0]\n    is_dublicates = unic - total\n    if is_dublicates:\n        return 'There are {is_dublicates} dublicate records'\ncheck_dublicates(train_data)\ncheck_dublicates(test_data)","a4b1abb9":"all_data = pd.concat([train_data, test_data], ignore_index=True, sort = False)","d19c3c1e":"#define and fill an empty column with some values\nmissing_report = all_data[all_data.columns[all_data.isnull().sum()!=0]]\n(missing_report.isnull().sum() \/ missing_report.shape[0]).sort_values(ascending=False)","79805691":"all_data = all_data.drop(['Cabin', 'Ticket', 'Name'], axis=1)\nall_data.head(5)","59697033":"#[val for val in categorical if val in checking_categorical_data (joined_data )]\nimport operator\nres ={col: all_data[ all_data[col]==0 ][col].count() \/ all_data[col].shape[0] \n     for col in all_data if all_data[ all_data[col]==0 ][col].count() \/ all_data[col].shape[0] > 0.8}\ndict(sorted(res.items(), key=operator.itemgetter(1),reverse=True))","df135fd0":"#check it out dataset \nmissing_report = all_data[all_data.columns[all_data.isnull().sum()!=0]]\n(missing_report.isnull().sum() \/ missing_report.shape[0]).sort_values(ascending=False)","6ffb1650":"empty_columns = ['Age', 'Embarked', 'Fare'] \nfor col in empty_columns:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n#check it out dataset \nmissing_report = all_data[all_data.columns[all_data.isnull().sum()!=0]]\n(missing_report.isnull().sum() \/ missing_report.shape[0]).sort_values(ascending=False)","a40b3309":"all_data.nunique()","a4c7b469":"def checking_categorical_data ( dataframe ):\n    list_ord = []\n    for feature in dataframe.columns:\n        if 1.*dataframe[feature].nunique()\/dataframe[feature].count() < 0.01:\n            list_ord.append(feature)\n    return list_ord\n\nres = checking_categorical_data(all_data)\nprint(res)","8e66eb9d":"all_data.head(5)","8a81330a":"def normalization_columns(dataframe, feature_to_encode):\n    for col in feature_to_encode:\n        min_max_scaler = preprocessing.MinMaxScaler()\n        x = dataframe[[col]].values.astype(float)\n        x_scaled = min_max_scaler.fit_transform(x)\n        dataframe[col] = pd.DataFrame(x_scaled)\n    return dataframe\n\nnumeric_columns = ['Age', 'Fare', 'SibSp', 'Parch'] \nall_data = normalization_columns(all_data, numeric_columns)\nall_data.head(5)","2888f49d":"all_data.shape","611eb87a":"def one_hot_encoding(dataframe, feature_to_encode):\n    for cat in feature_to_encode:\n        dummies = pd.get_dummies(dataframe[[cat]].astype(str))\n        dataframe = pd.concat([dataframe, dummies], axis=1)\n        dataframe.drop(cat, axis=1, inplace=True)\n    return dataframe\n\nall_data = one_hot_encoding(all_data,['Pclass', 'Sex', 'Embarked'])\nall_data.head()","bb1856a9":"print (f\"Data processing is completed and result is the all_data has {len(all_data.columns)} column of total {len(all_data.index)} rows\")","33e5ab1f":"Read and analyse the dataset","36a938e0":"# Data processing","a68427c4":"You can ","95f0386d":"# Quick review","f7bc961f":"We need to define the columns contain zeros with 0.8 and more percentages and to drop.  ","fcd291c6":"Proce\nYou can see only Sex and Fare are real but others are categorical data and table above is uninformative. Therefore, you should convert and normalize its to feed neural network. For this reason, we can do:\n* remove the uninformative features;\n* define and fill an empty column with some values;\n* real data should be normalized to interval [0-1];\n* categorical data should be encoded (one-hot encoded).","0b8c95c0":"# Autotuning neural network for prediction ","5b7b0b5d":"# Optimisers for fitting neural network","f984b51c":"You can see above list of categorial parameters but I'll remove 'SibSp', 'Parch' because ones define number of siblings or spouses and of parents children aboard the Titanic. Define the real values for normalization  ","4b5f9902":"# Neural network based on intuitive architecture: a first attemp","8deef0b1":"Take a look at Survived, Age, Embarked, Fare each one of them has missing values.\nLet's fill in each missing value by an appropriate mode of them. ","9784ce32":"I try to use neural network for this task but neural network needs a set of hyperparameters such as a layer number, activation function, optimisers and etc. In theory, you can not use feature engineering to create new features and you don't have to dive into data analysis because neural network is universal approximator(https:\/\/en.wikipedia.org\/wiki\/Universal_approximation_theorem).\nI want to check it and to use neural network model with auto tuning algorithm for building the optimal hyperparameters and getting the best result for prediction. Let's start!    ","60c5781d":"We have to concat train with test because after data processing we will use all dataset for prediction. Firstly, we concat test with train for data processing. Secondly, we split processed data into train and test and fit neural network with train data after then we will evaluate neural network with using test.     ","e58fa119":"Now, we need encoding the categorical_data such as ['Pclass', 'Sex', 'Embarked']","91eb2512":"You can see Cabin almost does not contain data and you may drop it. At the same time Ticket and Name are \nuninformative data too and we may drop its.   ","031a3140":"Fortunately, we don't have the columns with zero values with 0.8 and more percentages.","02b97368":"We could be used the different techniques of defining the types of parameters. For example, the categorical variables are defined by frequency of same values in colunms (if dataframe[feature].nunique()\/dataframe[feature].count() < 0.05), but not at this time. In this case, it need to consider each parameters, because we face to different type of categorical variable such as ordinal and nominal.","52dcd6b0":"To time create neural network but main problem is how to define a hyperparameters for our model. \n# The number of hidden layers and nodes per layer\nHow to choose the number of hidden layers and nodes in a feedforward neural network? Therea are many ideas how to define hidden layers and nodes (https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#) and you should know its but all things are personal experience of the observer like this: \n* one layer - Only capable of representing linear separable functions or decisions.\n* two layers - Can approximate any function that contains a continuous mapping\nfrom one finite space to another.\n* three layers - Can represent an arbitrary decision boundary to arbitrary accuracy\nwith rational activation functions and can approximate any smooth\nmapping to any accuracy.\nLet's try rule of thumb that helps for supervised learning problems. You can usually prevent over-fitting if you keep your number of neurons below (https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#):\nYou can usually prevent over-fitting if you keep your number of neurons below:\n# <center> $N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$\n$alpha$ - an arbitrary scaling factor usually 2-10,\n$N_s$ - number of samples in training data set,\n$N_i$ - number of input neurons,\n$N_o$ - number of output neurons. Others recommend setting \u03b1 to a value between 5 and 10, but I find a value of 2 will often work without overfitting. You can think of \u03b1 as the effective branching factor or number of nonzero weights for each neuron. Dropout layers will bring the \"effective\" branching factor way down from the actual mean branching factor for your network."}}