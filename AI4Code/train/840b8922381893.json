{"cell_type":{"1b65a772":"code","4ac1a77b":"code","96a33108":"code","15cfffd9":"code","806a1170":"code","0db31b38":"code","d05e2081":"code","183b3698":"code","c73b7af2":"code","92a8c1f6":"code","85cbafdd":"code","2d19c4c3":"code","f9d1bd84":"code","717d7b37":"code","48547699":"code","947bc96d":"code","14d9455b":"code","47199f18":"code","7e1bf682":"code","6abd1f8d":"code","f75b1ace":"code","a1a3c2be":"code","a406fb11":"code","9d68f656":"code","7e7837fc":"code","661ec4e1":"code","60e344bc":"code","e4cbc189":"code","162f50bc":"code","ef8ff286":"code","cda584f7":"code","cbb7f608":"code","b7c832fb":"code","49cc271b":"code","ff70a0ed":"code","614f42a2":"code","b8bcca6f":"code","a6fd02e0":"code","116979a5":"code","28bfdf93":"code","2c46d9d3":"code","94cd61ca":"code","470ae45e":"code","4ad1b9c1":"code","11991bba":"code","76d7d904":"code","23e66b35":"code","824599dd":"code","46256027":"code","c89ce179":"markdown","804f2dfb":"markdown","c33d5811":"markdown","4e511376":"markdown","7e233d47":"markdown","992c8ac0":"markdown","60732628":"markdown","6257f15b":"markdown","4c0e5251":"markdown","946d4e39":"markdown","198f922a":"markdown","fc151760":"markdown","163bd611":"markdown","1b0a4789":"markdown","2a1aff09":"markdown","bd181256":"markdown"},"source":{"1b65a772":"!pip install tensorflow-gpu==2.3.1","4ac1a77b":"import tensorflow as tf\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","96a33108":"print(tf.__version__)","15cfffd9":"f= open('..\/input\/deathnotescript\/Deathnote-script.txt' , encoding = 'utf-8')\nd= \" \"\nx = []\nfor i in f:\n    x.append(i)\n\nxx = d.join(x)\n#xx = x.split(\"\\n\")\n\ndata = xx.split(\"\\n\")\n#print(yy[:10])","806a1170":"data","0db31b38":"data[0 : 10]","d05e2081":"len(data)","183b3698":"#join data\nfinal_data = \" \".join(data)","c73b7af2":"final_data[:87]\n","92a8c1f6":"def clean_text(doc):\n    tokens = doc.split(\" \")   #white space sep\n    punc = str.maketrans(\"\",\"\",string.punctuation) # all punc\n    tokens = [w.translate(punc) for w in tokens]  #remove punc\n    tokens = [word for word in tokens if word.isalpha()]  #only alpha\n    tokens = [word.lower() for word in tokens]  #lower\n    return tokens","85cbafdd":"tokens = clean_text(final_data)","2d19c4c3":"tokens[:30]","f9d1bd84":"token_length = len(tokens)   #length\ntoken_length","717d7b37":"unique_token_len = len(set(tokens))   #length of unique word in tokens(also called voc size)\nunique_token_len","48547699":"#50 - ip\n#1 - op\n\ninput_length = 50+1\nlines = []\n\nfor i in range(input_length , len(tokens)):\n    seq = tokens[i-input_length : i]  #0 to inp length\n    line = \" \".join(seq)   # join to make inp sequence\n    lines.append(line)    # append in list\n    \nprint(len(lines))\n","947bc96d":"lines[0]","14d9455b":"tokens[50]","47199f18":"lines[1]","7e1bf682":"tokens[51]","6abd1f8d":"from tensorflow.keras.models import  Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense,LSTM ,Dropout, Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","f75b1ace":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)   #tokens applied on lines\nseq = tokenizer.texts_to_sequences(lines)","a1a3c2be":"seq       #list of int values for the words","a406fb11":"len(seq)","9d68f656":"seq = np.array(seq)","7e7837fc":"seq.shape","661ec4e1":"x, y = seq[: , :-1] , seq[: , -1]","60e344bc":"x[0]     #input","e4cbc189":"y[0]     #output","162f50bc":"tokenizer.word_index     #index for each word","ef8ff286":"voc_size = len(tokenizer.word_index) + 1 # as it was started from 0\nvoc_size","cda584f7":"y = to_categorical(y , num_classes =voc_size )\nx.shape[1]","cbb7f608":"y","b7c832fb":"seq_length = x.shape[1]\nseq_length","49cc271b":"\nmodel = Sequential()\n#model.add(Embedding(inp  , op , inp length))\nmodel.add(Embedding(voc_size , 50 , input_length = seq_length))\nmodel.add(LSTM(100 , return_sequences = True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(80))\nmodel.add(Dense(100 , activation= 'relu'))\nmodel.add(Dense(voc_size , activation = 'softmax'))\n","ff70a0ed":"model.summary()","614f42a2":"checkpoint_path = \"lstm-model-DN-v1.ckpt\"\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","b8bcca6f":"model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' ,metrics = ['accuracy'])","a6fd02e0":"#train\n#with tensorflow.device('\/gpu:1'):\nhistory = model.fit(x,y,batch_size = 256 , epochs = 500 ,callbacks=[cp_callback]) \n","116979a5":"plt.plot(history.history['accuracy'])","28bfdf93":"plt.plot(history.history['loss'])","2c46d9d3":"#train\n#with tensorflow.device('\/gpu:1'):\nhistory = model.fit(x,y,batch_size = 256 , epochs = 200 ,callbacks=[cp_callback]) \n","94cd61ca":"# Loads the weights\nmodel.load_weights(checkpoint_path)","470ae45e":"len(lines)","4ad1b9c1":"# predict func\ndef generate_text(model , tokenizer , text_seq_length , seed_text , n_words):\n    final_ans = []\n    for n  in range(n_words):\n        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n        encoded = pad_sequences([encoded] , maxlen = text_seq_length , truncating = 'pre')\n        \n        y_pred = model.predict_classes(encoded)\n        \n        pred_word =  \" \"\n        \n        for word , index in tokenizer.word_index.items():\n            if index == y_pred:\n                pred_word = word\n                break\n        seed_text = seed_text + \" \" + pred_word\n        final_ans.append(pred_word)\n    return \" \".join(final_ans)","11991bba":"inp = lines[3455]\nprint(\"Input: \" , inp)\nprint(\"\")\nprint(\"Output: \" ,generate_text(model , tokenizer , seq_length , inp , 50))\n","76d7d904":"inp = lines[720]\nprint(\"Input: \" , inp)\nprint(\"\")\nprint(\"Output: \" ,generate_text(model , tokenizer , seq_length , inp , 10))\n","23e66b35":"inp = lines[6698]\nprint(\"Input: \" , inp)\nprint(\"\")\nprint(\"Output: \" ,generate_text(model , tokenizer , seq_length , inp , 30))\n","824599dd":"inp = lines[8878]\nprint(\"Input: \" , inp)\nprint(\"\")\nprint(\"Output: \" ,generate_text(model , tokenizer , seq_length , inp , 40))\n","46256027":"#### If you liked this notebook , please upvote this notebook . This gives me motivation to build other notebooks :) ####","c89ce179":"## Tokenizing","804f2dfb":"## Predict","c33d5811":"## Tf version","4e511376":"## If not satisfied with the results -- train again","7e233d47":"## Vocab Size","992c8ac0":"## Creata Data Sequence","60732628":"## Model Architecture","6257f15b":"## Create X and Y","4c0e5251":"## Libs","946d4e39":"## Load Model","198f922a":"## Convert into array","fc151760":"## Import Libraries","163bd611":"## Check 1st input ","1b0a4789":"## Open the text file (data)","2a1aff09":"## Clean text","bd181256":"## Import Libs"}}