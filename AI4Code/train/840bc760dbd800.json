{"cell_type":{"4843a3db":"code","73bff028":"code","053b7852":"code","0fb08e18":"code","b1a24820":"code","6d7de5b8":"code","b0bbc776":"code","7b80c196":"code","8f3fcc24":"code","7d55122f":"code","871a4fc6":"code","6921840b":"code","3dbcb703":"code","c47b04ac":"code","87a6fd42":"code","7dd51ef0":"code","d694f83f":"code","f48cbfd9":"code","8286e927":"code","6fa3f4e0":"code","5414908f":"code","b9d275f8":"code","b5e49e2b":"code","b20cb273":"code","eec07bb2":"code","35933d60":"code","36da8336":"code","aa955992":"markdown","5c934b83":"markdown","49552b3f":"markdown","417e1cad":"markdown","4a64138d":"markdown","7c9aaa39":"markdown","0100a2bc":"markdown","36c76a3e":"markdown","80b64529":"markdown","a1817b8e":"markdown","52447d88":"markdown","e54a3255":"markdown","696f6fc9":"markdown","650534a2":"markdown"},"source":{"4843a3db":"import torch \nfrom torch import nn \nimport torch.nn.functional as F\nimport numpy as np \nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport pytorch_lightning as pl \nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup, AdamW\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt","73bff028":"#taking only the id,excerpt,target,standard_error\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\",usecols=[\"id\",\"excerpt\",\"target\",\"standard_error\"])\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)","053b7852":"plt.scatter(df['target'], df['standard_error'])","0fb08e18":"# remove outlier\ndf = df[df['standard_error']!=0]\nplt.scatter(df['target'], df['standard_error'])","b1a24820":"#any null rows\nprint(\"TRAIN NULLS: \\n\",df.isnull().sum())\nprint(\"TEST NULLS: \\n\",df.isnull().sum()) ","6d7de5b8":"#remove \\n and replace \\'s with 'sfrom the text\ndef prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","b0bbc776":"max_words = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\nprint(\"maximum words in instance:\",max_words)","7b80c196":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n# read training data\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf = df[df['standard_error']!=0]\n\n# create folds\ndf = create_folds(df, num_splits=5)","8f3fcc24":"BATCH_SIZE = 16\nEPOCHS = 200\nNUM_TRAIN_STEPS = int((df.shape[0]\/BATCH_SIZE)*EPOCHS)\nNUM_WARMUP_STEPS = 0\nFOLDS = df.kfold.unique()\nNUM_FOLDS = df.kfold.nunique() ","7d55122f":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1e-8\n        \n    def forward(self,output,target):\n        return torch.sqrt(F.mse_loss(output,target)+self.eps)","871a4fc6":"class BertModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = transformers.AutoModel.from_pretrained(\"..\/input\/bert-base-uncased\")\n        #self.model = transformers.AutoModel.from_pretrained(\"..\/input\/huggingface-bert\/bert-large-uncased\")\n        #self.model = transformers.AutoModel.from_pretrained(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")\n        self.drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(768,2)  # output to 2 dimensions, targets and errors\n        \n        # convolutional layer\n        self.conv1 = nn.Conv1d(205, 128, kernel_size=3, stride=1, padding=3)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=3)\n        self.conv3 = nn.Conv1d(64, 1, kernel_size=3, stride=1, padding=3)\n        self.ReLU = nn.ReLU()\n        self.pool = nn.MaxPool1d(3)\n        #self.fc_conv = nn.Linear(257,2)\n        self.fc_conv = nn.Linear(30,2)\n        \n        #self.fc = nn.Linear(1024,2)\n        \n        # batch normalization\n        self.bn1 = nn.BatchNorm1d(128)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.bn3 = nn.BatchNorm1d(1)\n        \n    \n    def forward(self,inputs):\n        out = self.model(**inputs) # output from BERT model\n        last_hiddens = out[0]\n        #print(last_hiddens.size())\n        #out = self.drop(last_hiddens[:,0,:].squeeze(1))\n        out = self.conv1(last_hiddens)\n        out = self.bn1(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        out = self.conv2(out)\n        out = self.bn2(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        out = self.conv3(out)\n        out = self.bn3(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        #print(out.size())\n        return self.fc_conv(out)\n    \n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, # original : 0.01\n            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5) # original : 5e-5\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)\n        return [optimizer],[scheduler] \n    \n    def loss_fn(self,output,target):\n        return RMSELoss()(output.view(-1,2),target.view(-1,2))\n    \n    def training_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        return loss\n    \n    def validation_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        self.log(\"val_loss\",loss,prog_bar=True)","6921840b":"class BertDataset(Dataset):\n    def __init__(self,texts,labels,max_len):\n        super().__init__()\n        self.texts = texts\n        self.max_len = max_len\n        self.labels = labels\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n        #self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/huggingface-bert\/bert-large-uncased\")\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self,idx):\n        text = \" \".join(self.texts[idx].split())\n        label = self.labels[idx]\n        inputs = self.tokenizer(text,return_tensors=\"pt\",max_length = self.max_len, padding=\"max_length\",truncation=True)\n        return {\n            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n                      \"attention_mask\":inputs[\"attention_mask\"][0],},\n            \"label\":torch.tensor(label,dtype=torch.float)\n        }","3dbcb703":"# fold = 0\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","c47b04ac":"# fold = 1\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","87a6fd42":"# fold = 2\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","7dd51ef0":"# fold = 3\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","d694f83f":"# fold = 4\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","f48cbfd9":"# for fold in FOLDS:\n#     print(\"Fold :\",fold)\n#     train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n#     train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n#     valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n#     train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n#     valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n#     bert_model = BertModel() \n#     trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n#     trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n#     trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","8286e927":"target_prediction = np.zeros(df.shape[0]) \nerror_prediction = np.zeros(df.shape[0]) \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor fold in FOLDS:\n    print(\"Fold:\",fold)\n    loaded_model = BertModel.load_from_checkpoint(f\"..\/input\/bert-trained-models\/checkpoint_{fold}fold.ckpt\",map_location=device)\n    loaded_model.to(device)\n    loaded_model.eval() \n    #using the same BertDataset module of train, here dummy labels are provided\n    check_dataset = BertDataset(df.excerpt.values,\n                                labels = (np.array([df.target.values,df.standard_error.values]).T),\n                                max_len=max_words)\n    check_dataloader = DataLoader(check_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    out_target = []\n    out_error = []\n    for batch in check_dataloader:\n        x  = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        for key in x.keys():\n            x[key] = x[key].to(device)\n        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n        out = loaded_model(x)\n        out = torch.squeeze(out, dim=1)\n        #print(out.size())\n        out_target_t = out[:,0]\n        out_error_t = out[:,1]\n        out_target.extend(out_target_t.cpu().detach().numpy())\n        out_error.extend(out_error_t.cpu().detach().numpy())\n        label_target = labels[:,0]\n        label_error = labels[:,0]\n        #print(out,labels)\n    target_prediction += np.hstack(out_target)\n    error_prediction += np.hstack(out_error)\n    #target_label += np.hstack(label_target)\n    #error_label += np.hstack(label_error)","6fa3f4e0":"plt.scatter(target_prediction\/5, df['target'])","5414908f":"plt.scatter(target_prediction\/5, error_prediction\/5)","b9d275f8":"np.ones([test_df.shape[0],2])","b5e49e2b":"test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\ntest_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)","b20cb273":"target_prediction = np.zeros(test_df.shape[0]) \nerror_prediction = np.zeros(test_df.shape[0]) \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor fold in FOLDS:\n    print(\"Fold:\",fold)\n    loaded_model = BertModel.load_from_checkpoint(f\"..\/input\/bert-trained-models\/checkpoint_{fold}fold.ckpt\",map_location=device)\n    loaded_model.to(device)\n    loaded_model.eval() \n    #using the same BertDataset module of train, here dummy labels are provided\n    test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\n    test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    out_target = []\n    out_error = []\n    for batch in test_dataloader:\n        x  = batch[\"inputs\"]\n        for key in x.keys():\n            x[key] = x[key].to(device)\n        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n        out = loaded_model(x)\n        out = torch.squeeze(out, dim=1)\n        out_target_t = out[:,0]\n        out_error_t = out[:,1]\n        out_target.extend(out_target_t.cpu().detach().numpy())\n        out_error.extend(out_error_t.cpu().detach().numpy())\n    target_prediction += np.hstack(out_target)\n    error_prediction += np.hstack(out_error)","eec07bb2":"target_prediction","35933d60":"test_df[\"target\"] = target_prediction\/NUM_FOLDS\nsub = test_df.drop(\"excerpt\",axis=1) \nsub.to_csv(\"submission.csv\",index=False)","36da8336":"sub","aa955992":"# check with training data","5c934b83":"<a href=\".\/checkpoint_1fold.ckpt\"> checkpoint_1fold.ckpt <\/a>","49552b3f":"<a href=\".\/checkpoint_2fold.ckpt\"> checkpoint_2fold.ckpt <\/a>","417e1cad":"# Data and Process","4a64138d":"# About this implementation\n- Submission for this competition is limitted for three hours. If you include the training phase that takes long time, your submission will not be accepted.\n- So my other strategy is\n    1. Model training on kaggle notebook\n    1. Download the models to my local machine\n    1. Upload the models to kaggle\n    1. Add the models to my notebook\n    1. Run all codes except for the training phase\n- If this strategy is accepted, code execution time limit is not problem.","7c9aaa39":"<a href=\".\/checkpoint_4fold.ckpt\"> checkpoint_4fold.ckpt <\/a>","0100a2bc":"# Trainer\n- I already trained the models and incluce this notebook.\n- So skip this section when commit or submission","36c76a3e":"<a href=\".\/checkpoint_0fold.ckpt\"> checkpoint_0fold.ckpt <\/a>","80b64529":"# Create Folds ","a1817b8e":"<a href=\".\/checkpoint_3fold.ckpt\"> checkpoint_3fold.ckpt <\/a>","52447d88":"# Load Weights and Inference ","e54a3255":"# Tokenize Dataset and Dataloader","696f6fc9":"# Bert Model and Training Module","650534a2":"# Try it!"}}