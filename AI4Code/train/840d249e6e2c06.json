{"cell_type":{"446c005f":"code","5cf62107":"code","924e9095":"code","ea332aab":"code","8d540e54":"code","a4e387e9":"code","c83d1448":"code","b4a0457f":"code","fca09d41":"code","145a871a":"code","c8af674d":"code","5bc9959e":"code","a2fea28e":"code","c26c94bf":"code","7b264b0e":"code","72fb1b97":"code","489e81c0":"code","75c3c4de":"code","f5520f4f":"code","0c1ce613":"code","2094eecf":"code","06d4b95a":"code","287852d4":"code","823ad140":"code","740c8f85":"code","a279fa56":"code","12d4c30d":"markdown","ebf1435d":"markdown","6e6081be":"markdown","c1922414":"markdown","c3aa1977":"markdown","6a55e48d":"markdown","d0732a56":"markdown","de705fe8":"markdown","07eee49e":"markdown","6fedf65f":"markdown","e717065b":"markdown","118159cf":"markdown","3587869b":"markdown","7210f4f4":"markdown","84f52fae":"markdown","bab075f9":"markdown","d2555b74":"markdown","ae9a7bc0":"markdown","eb2ff6f2":"markdown","16e825c7":"markdown","4b6bd6c3":"markdown","6199bd96":"markdown","98c9d265":"markdown","59f59c40":"markdown"},"source":{"446c005f":"import pandas as pd\n\ndata_true = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\ndata_fake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\n\ndata_true['label'] = 1\ndata_fake['label'] = 0\ndata = pd.concat([data_true, data_fake], axis = 0)\ndata = data.sample(frac=1).reset_index(drop=True)\n\nprint(data_true.shape), print(data_fake.shape), print(data.shape)","5cf62107":"data_true.head()","924e9095":"data_fake.head()","ea332aab":"print(\"Total number of news: \", len(data))\nprint(\"Number of true news: \", len(data_true))\nprint(\"Number of fake news: \", len(data_fake))","8d540e54":"X_text = data['text']\ny_label = data['label']","a4e387e9":"from sklearn.model_selection import train_test_split\n\nX, X_test, y, y_test = train_test_split(X_text, y_label, test_size=0.2, random_state=13) # split to train and test data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13) # split to train and validation data","c83d1448":"y_train.mean()","b4a0457f":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer()\ntfv.fit(X_train)\n\nX_train_tfv = tfv.transform(X_train)\nX_valid_tfv = tfv.transform(X_val)","fca09d41":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nlr = LogisticRegression()\nlr.fit(X_train_tfv, y_train)\nlr_preds = lr.predict(X_valid_tfv)\nlr_preds_proba = lr.predict_proba(X_valid_tfv)\n\nprint(\"The prediction accuracy is {0}\".format(np.mean(y_val==lr_preds)))\nprint(\"The loss is {0}\".format(round(log_loss(y_val, lr_preds_proba), 2)))","145a871a":"from collections import defaultdict\nfrom nltk.corpus import stopwords\n\nstopwords = stopwords.words('english')\n\n\n# create a corpuse of words in the data.\ndef create_corpuse(data, column_name):\n    words_by_frequency = defaultdict(int)\n\n    for title in data['{0}'.format(column_name)]:\n        for word in title.lower().split(' '):\n            if word not in stopwords:\n                words_by_frequency[word] += 1\n\n    return sorted(words_by_frequency.items(), key=lambda x: x[1], reverse=True)\n\ncounter_words_title_true = create_corpuse(data_true, 'title')\ncounter_words_title_fake = create_corpuse(data_fake, 'title')\n\ncounter_words_text_true = create_corpuse(data_true, 'text')\ncounter_words_text_fake = create_corpuse(data_fake, 'text')","c8af674d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# plot the most frequent words in the data.\ndef most_frequent_words_barplot(true_words, fake_words, part = 'titles'):\n    fig, ax = plt.subplots(1, 2, figsize = (16, 8)) \n    sns.barplot(x=[x[1] for x in true_words[:30]], y =[x[0] for x in true_words[:30]], palette='Blues', ax = ax[0])\n    sns.barplot(x=[x[1] for x in fake_words[:30]], y =[x[0] for x in fake_words[:30]], palette='Reds', ax = ax[1])\n\n    ax[0].title.set_text(\"30 most common words in true news articles {0}\".format(part))\n    ax[1].title.set_text(\"30 most common words in fake news articles {0}\".format(part))","5bc9959e":"# Most frequent words in titles of true vs fake news.\nmost_frequent_words_barplot(counter_words_title_true, counter_words_title_fake)","a2fea28e":"# Most frequent words in texts of true vs fake news.\nmost_frequent_words_barplot(counter_words_text_true, counter_words_text_fake)","c26c94bf":"# get number of words in the data\ndef text_length(text):\n    word_count = 0\n    for word in text.split(\" \"):\n        word_count += 1\n    return word_count\n\ntrue_words_count = data_true['text'].apply(text_length)\nfake_words_count = data_fake['text'].apply(text_length)\n\nwords_count = pd.DataFrame({'True': true_words_count, 'Fake': true_words_count})\n\nplt.figure(figsize=(10, 6))\nwords_count.boxplot()\nplt.xlabel('News category')\nplt.ylabel('Distribution of words in the news text')","7b264b0e":"true_duplicated = data_true['text'].duplicated()\nfake_duplicated = data_fake['text'].duplicated()\n\nsum(true_duplicated), sum(fake_duplicated)","72fb1b97":"num_unique_words_true = len(counter_words_text_true)\nnum_unique_words_fake = len(counter_words_text_fake)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(y=[num_unique_words_true, num_unique_words_fake], x=['True', 'Fake'], palette='Reds')\nplt.xlabel('News category')\nplt.ylabel('Number of unique words in news text')","489e81c0":"import re\n\n# get the number of 'element' in the data\ndef find_text_elements(data, column_name, element):\n    count_element = 0\n    re_element = re.compile(element)\n    for text in data['{0}'.format(column_name)]:\n        count_element += len(re.findall(re_element, text.lower()))\n    return count_element","75c3c4de":"true_twitter_user_names = find_text_elements(data_true, 'text', element = '@[0-9A-Za-z]*')\nfake_twitter_user_names = find_text_elements(data_fake, 'text', element = '@[0-9A-Za-z]*')\n\nplt.figure(figsize=(10, 6))\nsns.barplot(y=[true_twitter_user_names, fake_twitter_user_names], x=['True', 'Fake'], palette='Reds')\nplt.xlabel('News category')\nplt.ylabel('Number of twitter users in news text')","f5520f4f":"true_sites_web_links = find_text_elements(data_true, 'text', element = 'http|www\\\\.|\\\\.com')\nfake_sites_web_links = find_text_elements(data_fake, 'text', element = 'http|www\\\\.|\\\\.com')\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(y=[true_sites_web_links, fake_sites_web_links], x=['True', 'Fake'], palette='Reds')\nplt.xlabel('News category')\nplt.ylabel('Number of web-link references in news text')","0c1ce613":"# some popular news agencies names randomly chosen by me\narticles = {'the washington post': [0, 0], 'cnn': [0, 0], 'bbc': [0, 0], 'reuters': [0, 0], \n            'fox news': [0, 0], 'the new york times': [0, 0], 'nbc': [0, 0]}\nfor key, _ in articles.items():\n    true_value = find_text_elements(data_true, 'text', element=key)\n    fake_value = find_text_elements(data_fake, 'text', element=key)\n    articles[key] = true_value, fake_value\n    \narticles","2094eecf":"articles_count = pd.DataFrame(articles)\n\n\nplt.figure(figsize=(10, 6))\narticles_count.T.plot(kind='barh', colormap='autumn')\nplt.xlabel('Number of times the articles are mentioned');\nplt.legend(['True', 'Fake']);","06d4b95a":"from tensorflow import keras\n\nnum_words = 10000\n\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words+1)\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\nX_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seq,\n                                                    maxlen=200,\n                                                    padding='post',\n                                                    truncating='post')\nX_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seq,\n                                                    maxlen=200,\n                                                    padding='post',\n                                                    truncating='post')","287852d4":"X_train_padded.shape, len(y_train)","823ad140":"X_val_padded.shape, len(y_val)","740c8f85":"model = keras.Sequential([\n    keras.layers.Embedding(num_words+1, 16, input_length=200),\n    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'],\n             optimizer='adam')\n\nhistory = model.fit(X_train_padded, y_train, \n                    validation_data=(X_val_padded, y_val), epochs = 10)","a279fa56":"fig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\n\nax[0].set_title('Model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'val'])\n\nax[1].set_title('Model loss')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'val'])","12d4c30d":"The two plots above don't show any apparent difference between the titles of the news. The same can be said for the text. Though I noticed that the true news text mentiones the 'reuters'-as a source for the news quite frequentley.","ebf1435d":"We see that fake news reffer to wildly known news aggencies more frequently than the true news. I think, this can be partly explained by the desire of those fake news to be seem more reliable by mentioning the famous agencies names even if they are connected to the essence of the article only remotley. \nFrom another point I suppose, that the true news are represnted by those famous agencies and they rarely want to share the spotlight by mentioning a competitive agency as their source. Instead they use their reporters to get the reliable news.\n\nAt the same time the things are not so clear with 'reuters'. So, an assumption is that either 'reuters' is very reliable news agency and the others are not or the data should be biased. The latter will partly explain the high prediction results for the base model.","6e6081be":"A good idea is to investigate to which sources news refer to. In particular we will look at the usage of twitter or some external web-site links. In addition to this we can look if some other news agencies names are used apart of 'reuters'.","c1922414":"It seems to be the case that the fake news are using\/citing twitter far more extensively than the true news. So, there seems to be a bias in the data.","c3aa1977":"# Model evaluation","6a55e48d":"There are almost an equal quantity of fake vs. true news.","d0732a56":"--------------------------------------\nWith the growth of speach freedom and various forms of communacation certain groups of people\/agenceis want to promote thier own perspective of the world. Unfortunetley, in doing so sometimes they are trying to manipulate people opinions by generating fake news. It is done to make people believe in what can promote interest of news generating agencies.\n\nFrom the perspective of the users it is important to understand wether they can trust the news or they should be suspicous of it to avoid becoming a victim of some sort of scum or disinformation.","de705fe8":"# Data exploration","07eee49e":"Again the fake news refer to different web-sites far more often than the true news.","6fedf65f":"Our dataset consists of two parts: fake and true news datasets. The respective number of news in each dataset is $23481, 21417$.\n\nIn the concatinated dataset there are $2$ categories of labels to classify the news as either true or false based on the news content.","e717065b":"The model starts to slightly overfit at the $5$th epoch.","118159cf":"# Base model","3587869b":"Let's see if there is a difference between the lengths of the text in the true\/fake news.","7210f4f4":"Though during the exploratory analysis we saw that the data seems to be biased in the way it is formed we decided to create an LSTM-model too to see if it captures the underlying features better than the simple logistic regression.\n\nNote that we are not doing any data cleaning in this case becuase already by the nature of the data peculiarities we would expect to get a model with quite high prediction accuracy.","84f52fae":"The high prediction accuracy for the base model surprised me and made me suspicous about the data. Let's investigate it to see wheather there are any peculiarities in the data.\n\nIn particular at this moment we should find out if there is some feature that helps to easily differentiate between true and fake data. Now we will try to see if:\n- There is a significant difference between the words usage in true vs fake news.\n- There is a significant difference between the lenghts of the text in true vs fake news.\n- There is a significant difference of topics of the true vs fake news.","bab075f9":"# Reading the data","d2555b74":"Again fake news use more unique tokens than the true news. This can be connected with the fact that fake news use lots of external links to twitter users and web-sites (as we found out below).","ae9a7bc0":"--------------------------------------\n**Dataset**: In this project we deal with fake and real news dataset.   \n**Dataset\nResearch question**: The main question we wanted to answer is: wheather the given news is a true news or a fake one?  \n**Methods and Findings**:  To answer to this question we have created a base model using logistic regression method. The base model serves as a reference for the further improvement by the LSTM-model. However, the results of the base model were surprisingly good. Hence, we initiated a detailed exploratory analysis of the data which revealed that there are some problems in the way the data has been formed.","eb2ff6f2":"To partly resolve this phenomenon we looked at the presence of the duplicated texts and found out that there are over $6000$ dupliacte text in the fake news. ","16e825c7":"Surprisingly fake news have more words than true ones. Hypothetically, this can be explained by the desire of fake news to convince the audience in their reliablity and hence, they use more words and tricks to do it. However, at the same the true news should give an extensive explanation of their articles to the audience. Hence, some mystery occures here.","4b6bd6c3":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQfrEUPl6MA-wBBVSQr86WCOrG1uNtqn_NEKQ&usqp=CAU\" alt=\"Heat beating\" style=\"margin-top:3rem;\"> <\/div>","6199bd96":"The mean of the labels in the train data shows that if we use a model that naively assigns to each observation $1$ i.e. classifies every news as 'fake' we would classify $48$ percent of the news correctly.\n\nLet's try to improve this accuracy.","98c9d265":"What popular news agencies the news use?","59f59c40":"Number of unique words can be investigated too."}}