{"cell_type":{"29c32a53":"code","7fe31311":"code","c377959f":"code","9c840322":"code","703f7226":"code","69136de7":"code","ed3cbc3e":"code","8a361ac9":"code","a8016494":"code","bd5c7768":"code","567a7427":"code","b23e86e6":"code","a5230a7c":"code","e68f477b":"code","a9ec24dc":"code","97aa4146":"code","0754b170":"code","c0d4e48a":"code","8bb2e79d":"code","790c0575":"code","220a5eed":"code","e835321a":"code","787920c4":"markdown","5810b88f":"markdown","95c7b3e2":"markdown","eff6cf7a":"markdown","6db8fb87":"markdown","7d1895f4":"markdown","b3a56c8b":"markdown","e3d7505e":"markdown","e5339475":"markdown","9a5ff962":"markdown","8f2066b7":"markdown","412b691b":"markdown","2c09664a":"markdown","09c6040d":"markdown","44869a14":"markdown","d270aa84":"markdown","4c8c2239":"markdown","ff866850":"markdown","fe2897ea":"markdown","56c024a5":"markdown","11688eb2":"markdown","7b58a5b7":"markdown","7b2321ac":"markdown","aeb32ca8":"markdown","9551ab4b":"markdown","c4e493ff":"markdown"},"source":{"29c32a53":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format='svg'\n%matplotlib inline ","7fe31311":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split#, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score","c377959f":"clinical_data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nclinical_data.head()","9c840322":"clinical_data.info()","703f7226":"clinical_data[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']].describe()","69136de7":"plt.figure()\ncorr_death = clinical_data.corr('pearson')['DEATH_EVENT'].drop('DEATH_EVENT')\nsorted_idx_corr_death = corr_death.argsort()\nplt.barh(clinical_data.columns[sorted_idx_corr_death], corr_death[sorted_idx_corr_death])\nplt.title('Pearson correlation with DEATH_EVENT')\nplt.xlabel('corr_value')\nplt.ylabel('variable')\nplt.show()","ed3cbc3e":"X = clinical_data.drop(['DEATH_EVENT'], axis = 1)\ny = clinical_data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n\nprint(X_train.shape)\nprint(X_test.shape)","8a361ac9":"xgb = GradientBoostingClassifier(max_depth=2, min_samples_split=0.5, n_estimators=50,random_state=1)\nxgb.fit(X_train, y_train)","a8016494":"y_pred = xgb.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\ncm = confusion_matrix(y_pred, y_test)\n\ndef create_confusion_graph(cm, title='Confusion matrix'):\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, cmap='Blues')\n    \n    ax.set_xticks([0,1])\n    ax.set_yticks([0,1])\n    ax.set_xticklabels(['True','False'])\n    ax.set_yticklabels(['True','False'])\n\n    plt.xlabel('Predicted')\n    plt.ylabel('Real')\n\n    for i in range(len(cm)):\n        for j in range(len(cm[0])):\n            text = ax.text(j, i, cm[i, j],\n                           ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.title(title)\n    \n    return fig\n\ncreate_confusion_graph(cm, 'Confusion matrix for the first gradient boost classifier')\nplt.show()","bd5c7768":"sorted_idx = xgb.feature_importances_.argsort()\n\nplt.barh(y=X.columns[sorted_idx], width=xgb.feature_importances_[sorted_idx])\n\nplt.title('Gini importance')\nplt.xlabel('Gini importance')\nplt.ylabel('feature')\nplt.show()","567a7427":"from sklearn.inspection import permutation_importance\nresult = permutation_importance(xgb, X, y, scoring='accuracy', random_state=1)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.set_size_inches((7,7))\nplt.show()","b23e86e6":"from sklearn.inspection import plot_partial_dependence\nplot_partial_dependence(xgb, X, features=['time','ejection_fraction','serum_creatinine'], n_cols=3, response_method='predict_proba', method='brute')\nplt.title('Partial dependece plot for the first classifier')\nplt.show()","a5230a7c":"clinical_data[(clinical_data['time'] >= 150) & (clinical_data['time'] <= 170)]['DEATH_EVENT'].value_counts()","e68f477b":"X_test[xgb.predict(X_test) != y_test][['time', 'ejection_fraction', 'serum_creatinine']]","a9ec24dc":"X_no_time = X.drop(['time'], axis = 1)\nX_train_no_time, X_test_no_time, y_train, y_test = train_test_split(X_no_time, y, test_size = 0.2, random_state = 2)\nprint(X_train_no_time.shape)\nprint(y_train.shape)","97aa4146":"xgb_no_time = GradientBoostingClassifier(learning_rate=0.01, max_depth=2,\n                           min_samples_leaf=0.1, min_samples_split=0.5,\n                           random_state=1)\nxgb_no_time.fit(X_train_no_time, y_train)","0754b170":"y_pred = xgb_no_time.predict(X_test_no_time)\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\ncreate_confusion_graph(confusion_matrix(y_pred, y_test))\nplt.show()","c0d4e48a":"sorted_idx_no_time = xgb_no_time.feature_importances_.argsort()\n\nplt.barh(y=X.columns[sorted_idx_no_time], width=xgb_no_time.feature_importances_[sorted_idx_no_time])\nplt.title('Gini importance')\nplt.xlabel('Gini importance')\nplt.ylabel('feature')\nplt.show()","8bb2e79d":"result = permutation_importance(xgb_no_time, X_no_time, y, scoring='accuracy', random_state=1)\nsorted_idx_no_time = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx_no_time].T,\n           vert=False, labels=X_test_no_time.columns[sorted_idx_no_time])\nax.set_title(\"Permutation Importances (test set)\")\nfig.set_size_inches((7,7))\nplt.show()","790c0575":"plot_partial_dependence(xgb_no_time, X_no_time, ['ejection_fraction','age', 'serum_creatinine'], grid_resolution=500, response_method='predict_proba', method='brute')\nplt.title('Partial dependence plot for the second classifier')\nplt.show()","220a5eed":"'''param_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 150],\n    'max_depth': [2,3,5],\n    'min_samples_split': [0.01, 0.1, 0.5],    \n}\n\ngrid_search = GridSearchCV(GradientBoostingClassifier(random_state=1), cv=5, param_grid=param_grid, scoring='roc_auc', verbose = 1, n_jobs = 2)\ngrid_search.fit(X_train, y_train)\n\nprint('Best params found: \\n\\t', grid_search.best_params_)\nprint('Best ROC_AUC score found: \\n\\t', grid_search.best_score_)\nprint('Best estimator: \\n\\t', grid_search.best_estimator_)\n'''\n\nprint('grid search - first model')","e835321a":"'''param_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 150],\n    'max_depth': [2,3,5,10],\n    'min_samples_split': [0.0, 0.1, 0.5],\n    'min_samples_leaf': [0.0, 0.1, 0.5],\n    'min_impurity_decrease': [0.0, 0.1,0.5]\n}\n\ngrid_search = GridSearchCV(GradientBoostingClassifier(random_state=1), cv=5, param_grid=param_grid, scoring='roc_auc', verbose = 1, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\n\nprint('Best params found: \\n\\t', grid_search.best_params_)\nprint('Best ROC_AUC score found: \\n\\t', grid_search.best_score_)\nprint('Best estimator: \\n\\t', grid_search.best_estimator_)'''\n\nprint('grid search - second model')","787920c4":"First we separe the target and the feature columns and then split all the data as train and test sets. The test size was setted as 20% of the original data.","5810b88f":"Then we use a GradientBoostClassifier to train with the train set. The hyperparameters of this classifier was found using a GridSearch (the parameter grid used is shown at the end of this document).","95c7b3e2":"## Machine Learning Process","eff6cf7a":"* The only 3 samples that had a wrong prediction in the test set are showed below. All of them have a `DEATH_EVENT` value of 1 and get a predicition value of 0. We can see below that they did not match with what was shown in the partial dependent plot and, because of that, the model could not predict correctly.","6db8fb87":"The patial dependence plot also is shown below with these three variables. The results of `ejection_fraction` and `serum_creatinine` look like those obtained earlier, but now much more accentuated. With respect to `age`, the partial dependence plot shows that an age greater than 70 increases the probability of death, for this model.","7d1895f4":"The data has 299 samples and 13 column. As can be seen there is no null value in the data and all the columns has numerical values, from which `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking` and `DEATH_EVENT` has binary values. Below some statistics measures for the non-binary columns are given.","b3a56c8b":"As can been seen, not coincidentally, the variables with the greater Pearson correlation are the more importants, according with de Gini measure. To confirm this, a permutation importance inspection was done, and the results, which confirms the Gini measure, are seen below.","e3d7505e":"# Heart Failure Prediction with and without time - XGB model (95%) and interpretability of results","e5339475":"## Is that all?\nThe model created in the last section did a pretty good job (95% accuracy). But, we saw that his predictions was based mostly on the `time` feature. Although this is not necessarily bad, the `time` feature, which tell a patient's follow-up period (in days), is not a biological feature. If we remove the `time` feature, would the model work well? And would it make its predictions based on which variables?","9a5ff962":"## A new gradient model without `time`","8f2066b7":"## Conclusion\n\nWith all that has been presented, we can conclude that, for these samples, the factors of follow-up period, level of serum creatinine in the blood, and ejection fraction (percentage of blood leaving the heart at each contraction) were the most important to predict a death event due to a heart failure.","412b691b":"## Loading and inspecting data","2c09664a":"Then we create a new model. The hyperparameters of this model was found with the second grid search from the final of this document.","09c6040d":"The objective of the task is to predict the target column `DEATH_EVENT`, which indicates death due to a heart failure, based on the information provided by the other 12 columns. Pearson correlation indicator can be a good start for this purpose.  Below a graph show us the result of this indicator when applied between the target column and all the other variables. As we can see `time` has a strong negative correlation with the target column. Followed by it `serum_creatinine`, `ejection_fraction`, `age` and `serum_sodium` also have considerable correlation with the target, when compared with the other columns.","44869a14":"## Grid search parameters","d270aa84":"Next we will see if these correlations are identified and\/or useful for the machine learning algorithm.","4c8c2239":"With the best hyperparameters found, the maximum accuracy found was 80%. As expected, without `time`, the model has more difficult learning.","ff866850":"## Importing libraries","fe2897ea":"With the results confirmed, we can say that, for this classifier, the most important feature was `time` with a huge advantage if compared with the others features. Both `ejection_faction` and `serum_creatinine` also seems considerable important to this model. Using these three variables a partial dependece plot, which show us the average effect of the features on the target variable, is shown below. From him we can see clearly that the model learn what the Pearson correlation measure told us in the past.","56c024a5":"To answer the second question made before, we going to make the same procedure of analysing the results. Below a Gini importance graph is shown again. Without `time`, the more importants features, according with the Gini measure are `serum_creatinine` and `ejection_fraction`, which were part of the first three previously. Interestingly, now the model seems to consider only three variables to make its predictions, the two already mentioned and `age`.","11688eb2":"The permutation importance measurement below confirms what was shown with the Gini importance measurement.","7b58a5b7":"The accuracy and the confusion matrix of this classifier are showed below.","7b2321ac":"### Interpretability of the first model","aeb32ca8":"First we remove the `time` feature from the previous X data. Then we split the new set.","9551ab4b":"Next we will see a analysis of the results of the first model. Gradient boost models in `scikit_learn` has an attribute called `feature_importance`, that tell us \"how much that feature reduced the criterion of a split\", (this is known as the Gini importance). The graph below show us this measure.","c4e493ff":"From this graph is also shown: \n* Patients with follow-up period (`time`) less than 50, `ejection_fraction < 20` and\/or `serum_creatinine > 7.5` has a higher probability of dying.\n* There is an increase in the curve of the `time` variable between the values of 150 and 170. This happens because all of the patients who had a quantity of days of follow-up period at this interval died. This is shown below."}}