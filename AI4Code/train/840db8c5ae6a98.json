{"cell_type":{"3f1b5c59":"code","ff249195":"code","732cf0b6":"code","1afdc0b5":"code","dc60513e":"code","7191c0ec":"code","f4f86edf":"code","cdad7410":"code","749e2ab5":"code","f23721c6":"code","9df094a8":"code","6b14fb40":"code","3c8c8acd":"code","d93d909a":"code","11564ebc":"code","e9800a13":"code","cbe64da7":"code","eb8c41ad":"code","b844e808":"code","838ffe69":"code","55807f74":"code","05b50f19":"code","85f945bf":"code","3befbf70":"code","0c1f321d":"code","cde4d874":"code","ac5f98f5":"code","4f33d0d9":"code","38216324":"code","8f165024":"code","d5f28721":"code","f682dcbe":"code","ddadca68":"code","aa286066":"code","b0bf4754":"markdown","b5334463":"markdown"},"source":{"3f1b5c59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ff249195":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","732cf0b6":"!pip install transformers\n\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nbert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#bert_model.save_weights(\"bert_model.h5\")\nimport pickle\npkl_filename = \"bert_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_model, file)\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#tokenizer.save_weights(\"tokenizer.h5\")\nimport pickle\npkl_filename = \"bert_tokenizer.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_tokenizer, file)\n","1afdc0b5":"\nimport os\n#%%capture\n!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","dc60513e":"#%%capture\n!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","7191c0ec":"#%%capture\n!wget -O lucene.tar.gz https:\/\/www.dropbox.com\/s\/j55t617yhvmegy8\/lucene-index-covid-2020-04-10.tar.gz?dl=0\n!tar xvfz lucene.tar.gz\nminDate = '2020\/04\/10'  \nluceneDir = '\/kaggle\/working\/lucene-index-covid-2020-04-10\/'","f4f86edf":"from IPython.core.display import display, HTML\nimport json\ndef show_query(query):\n    \"\"\"HTML print format for the searched query\"\"\"\n    return HTML('<br\/><div style=\"font-family: Times New Roman; font-size: 20px;'\n                'padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>')\n\ndef show_document(idx, doc):\n    \"\"\"HTML print format for document fields\"\"\"\n    have_body_text = 'body_text' in json.loads(doc.raw)\n    body_text = ' Full text available.' if have_body_text else ''\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               f'<b>Document {idx}:<\/b> {doc.docid} ({doc.score:1.2f}) -- ' +\n               f'{doc.lucene_document.get(\"authors\")} et al. ' +\n              f'{doc.lucene_document.get(\"journal\")}. ' +\n              f'{doc.lucene_document.get(\"publish_time\")}. ' +\n               f'{doc.lucene_document.get(\"title\")}. ' +\n               f'<a href=\"https:\/\/doi.org\/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}<\/a>.'\n               + f'{body_text}<\/div>')\n\ndef show_query_results(query, searcher, top_k=10):\n    \"\"\"HTML print format for the searched query\"\"\"\n    output_query = searcher.search(query)\n#    print(\"output_query\",output_query)\n    display(show_query(query))\n    for i, k in enumerate(output_query[:top_k]):\n        display(show_document(i+1, k))\n    return output_query[:top_k]   ","cdad7410":"import json\ndef query_id(query_result):\n    #print(len(query_result))\n    #print(\"query_result\",query_result)\n    files_list=[]\n    for i in range(len(query_result)):\n      doc_json = json.loads(query_result[i].raw)\n      print(\"doc_json\",doc_json)\n      paper_id = 'paper_id' in doc_json    \n      if paper_id :\n        print(doc_json['paper_id'])\n        files_list.append(doc_json)\n        #print(doc_json)\n      else:\n        print(doc_json['sha'])\n    #print(\"files_list\",files_list)\n    return files_list","749e2ab5":"import glob\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n#all_json=files_list\n#print(\"all_json type\", type(all_json))\n#print(\"length of json:\",len(all_json))\n\nclass FileReader:\n    def __init__(self, file):        \n #         print(\"file is=\",file)\n          content = file\n #         print(\"content is\", type(content))\n          self.paper_id = content['paper_id']\n          self.abstract = []\n          self.body_text = []\n          self.abstract_section=[]\n          self.body_section=[]\n          # Abstract\n          try:\n              for entry in content['abstract']:\n                  self.abstract.append(entry['text'])\n                  self.abstract_section.append(entry['section'])\n          except KeyError:pass    \n          # Body text\n          for entry in content['body_text']:\n              self.body_text.append(entry['text'])\n              self.body_section.append(entry['section'])\n          self.abstract = '\\n'.join(self.abstract)\n          self.body_text = '\\n'.join(self.body_text)\n          self.abstract_section = '\\n'.join(self.abstract_section)\n          self.body_section = '\\n'.join(self.body_section)\n            \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}++++++ {self.body_text[:200]}+++++'\n        \n#first_row = FileReader(all_json[0])\n#print(\"first_row===\",first_row)\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\ndef generate_clean_df(files_list):\n    all_json=files_list\n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],'body_section':[],'abstract_section':[], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': [],'source_x':[],'publish_time':[]}\n\n    for idx, entry in enumerate(all_json):\n        content = FileReader(entry) \n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n        dict_['body_section'].append(content.body_section)\n        dict_['abstract_section'].append(content.abstract_section)\n    df_covid = pd.DataFrame(dict_, columns=['paper_id','abstract','abstract_section','body_section','body_text'])\n    #print(df_covid.head())\n    text_dict = df_covid.to_dict()\n    len_text = len(text_dict[\"paper_id\"])\n    paper_id_list  = []\n    body_text_list = []\n    body_section_list =[]\n    section_list =[]\n    for i in tqdm(range(0,len_text)):\n      paper_id = df_covid['paper_id'][i]\n      body_text = df_covid['body_text'][i].split(\"\\n\")\n      body_section = df_covid['body_section'][i].split('\\n')\n      for i in tqdm(range(0,len(body_text))):\n        paper_id_list.append(paper_id)\n        body_text_list.append(body_text[i])\n        body_section_list.append(body_section[i])\n        section_list.append(\"BODY\")\n\n    df_paragraph = pd.DataFrame({\"paper_id\":paper_id_list,\"section\":section_list,\"paragraph\":body_text_list,\"subsection\":body_section_list})\n    df_paragraph.to_csv(\"paragraph.csv\")\n    return df_paragraph\n","f23721c6":"#files_list=query_id(query_result)\n#cl_df=generate_clean_df(files_list)","9df094a8":"!wget https:\/\/github.com\/facebookresearch\/fastText\/archive\/v0.9.1.zip\n!unzip v0.9.1.zip\n!cd fastText-0.9.1\n","6b14fb40":"! ls -l\n! pip install \/kaggle\/working\/fastText-0.9.1\/.","3c8c8acd":"! git clone https:\/\/github.com\/epfml\/sent2vec.git\n#! cd \/content\/sent2vec\/\n! pip install \/kaggle\/working\/sent2vec\/.","d93d909a":"pkl_filename = \"\/kaggle\/working\/bert_model.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    bert_model = pickle.load(file)\n    \npkl_filename = \"\/kaggle\/working\/bert_tokenizer.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    tokenizer = pickle.load(file)\n","11564ebc":"import numpy as np\ndef bertsquadpred(bert_model, document, question):\n    input_ids = tokenizer.encode(question, document)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    num_seg_b = len(input_ids) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    assert len(segment_ids) == len(input_ids)\n    n_ids = len(segment_ids)\n    #print(\"n_ids\",n_ids)\n    #print(n_ids)\n    if n_ids < 512:\n        start_scores, end_scores = bert_model(torch.tensor([input_ids]), \n                                 token_type_ids=torch.tensor([segment_ids]))\n    else:        \n        start_scores, end_scores = bert_model(torch.tensor([input_ids[:512]]), \n                                 token_type_ids=torch.tensor([segment_ids[:512]]))\n    #print(\"start_scores\",start_scores)\n    #print(\"end_scores\",end_scores)\n    start_scores = start_scores[:,1:-1]\n    end_scores = end_scores[:,1:-1]\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    #print(\"answer_start, answer_end\",answer_start, answer_end)\n    answer = ''\n    \n    t_count = 0    \n    for i in range(answer_start, answer_end + 2):\n        if tokens[i] == '[SEP]' or tokens[i] == '[CLS]':\n            continue\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        else:\n            if t_count == 0:\n                answer +=  tokens[i]\n            else:\n                answer += ' ' + tokens[i]\n        t_count+=1\n            \n    full_txt = ''\n    for t in tokens:\n        if t[0:2] == '##':\n            full_txt += t[2:]\n        else:\n            full_txt += ' ' + t\n            \n    abs_returned = full_txt.split('[SEP]')[1]\n            \n    #print(abs_returned)\n    ans={}\n    ans['answer'] = answer\n    #print(answer)\n    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n        ans['confidence'] = -1.0\n    else:\n        confidence = torch.max(start_scores) + torch.max(end_scores)\n        confidence = np.log(confidence.item())\n        ans['confidence'] = confidence\/(1.0+confidence)\n    ans['start'] = answer_start\n    ans['end'] = answer_end\n    ans['paragraph_bert'] = abs_returned\n    return ans","e9800a13":"import sent2vec\n#from nltk import word_tokenize\n#from nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\n\n#model_path = \"\/content\/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\nmodel_path = \"\/kaggle\/input\/biosentvec\/BioSentVec_CORD19-bigram_d700.bin\"\n#model_path = \"\/content\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel = sent2vec.Sent2vecModel()\ntry:\n    model.load_model(model_path)\nexcept Exception as e:\n    print(e)\nprint('model successfully loaded')","cbe64da7":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef test_query(cl_df):\n    para_vector_dict = {}\n    subsection_vector_dict={}\n    para_list = []\n    #data=[]\n    f_data=[]\n    count = 0\n    n_return = 2\n    for id, group in cl_df.groupby(['paper_id']):\n        para = group[\"paragraph\"].values\n        paper_id1=group[\"paper_id\"].values\n        paper_id=group[\"paper_id\"].values\n        paper_id=paper_id.tolist()       \n        print(\"\\n paper_id\",set(paper_id))         \n        subsection = group[\"subsection\"].values       \n        \n        \n        paras = [p for p in para if isinstance(p,str)]\n        subsections = [s for s in subsection if isinstance (s, str)]\n        #print(\"subsections=\",type(subsections))\n        #print(type(paras))\n        paras_count = len(paras)\n        #print(paras_count)    \n        if paras_count==0:\n            continue\n        #paras_text = \" \".join(paras)\n        paragraph_dict={}\n        paragraph_list=[]\n        k=0\n        for sub_para in paras:\n          paragraph_dict[k]=model.embed_sentence(sub_para)\n          paragraph_list.append(sub_para)\n          k+=1    \n        \n        keys = list(paragraph_dict.keys())\n        \n        vectors = np.array(list(paragraph_dict.values()))\n        #print(\"vectors are\",vectors)\n\n        nsamples, nx, ny = vectors.shape\n        para_vectors = vectors.reshape((nsamples,nx*ny))\n\n        from scipy.spatial import distance\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        para_matrix_query = cosine_similarity(para_vectors, query_vector.reshape(1,-1))\n        para_indexes = np.argsort(para_matrix_query.reshape(1,-1)[0])[::-1][:5]\n        #print(\"paragraph_index\",para_indexes)\n        short_listed_paras = [paragraph_list[i] for i in para_indexes]\n        #[main_list[x] for x in indexes]\n        #print(\"subsections==\",subsections)\n        subsection_head=[subsections[i] for i in para_indexes]\n        #print(\"subsection_head=\",subsection_head)\n        print(\"\\n query is:\",query)\n        \n        from itertools import chain \n        answer_squad_list=[]\n        answer_squad_dict={}\n        for sub_head,sub_para in zip(subsection_head,short_listed_paras):\n          #for subsection_header in subsection_head:        \n            #print(sub_head ,\":\", sub_para)\n            #print(\" answer is\",answer)\n            answer_vector = model.embed_sentence(sub_para)\n            cosine_sim = 1 - distance.cosine(query_vector, answer_vector)\n            #print('cosine similarity:', cosine_sim)\n            answer_squad = bertsquadpred(bert_model, sub_para, query)\n            #print(\"answer is\",answer_squad['answer'])\n            #print(\"-\"*100)\n            answer_squad_list.append(answer_squad['answer'])\n        print(\"\\n answer:\",\" \".join(answer_squad_list))\n        print(\"-\"*90)\n        #data={'paper_id':group['paper_id'].unique(),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':(group['paper_id'].nunique()),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':set(paper_id),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #f_data.append(data)\n    #df = pd.DataFrame(f_data) \n    #df.to_csv(\"\/kaggle\/working\/t.csv\", index=False)\n    #print(df)                                                                                                              \n                                                                                                                  \n        \n    ","eb8c41ad":"#Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n#Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n#Seasonality of transmission.\n#Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n#Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n#Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n#Natural history of the virus and shedding of it from an infected person\n#Implementation of diagnostics and products to improve clinical processes\n#Disease models, including animal models for infection, disease and transmission\n#Tools and studies to monitor phenotypic change and potential adaptation of the virus\n#Immune response and immunity\n#Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n#Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n#Role of the environment in transmission","b844e808":"#query=\"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\"\nquery=\"Natural history of the virus and shedding of it from an infected person\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n#cl_df.to_csv(\"\/kaggle\/working\/cl_df.csv\",index=False)\n#query_vector = model.embed_sentence(query)","838ffe69":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","55807f74":"#query=\"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\"\nquery=\"Seasonality of transmission of covid-19 or the novel coronavirus\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n#cl_df.to_csv(\"\/kaggle\/working\/cl_df.csv\",index=False)\n#query_vector = model.embed_sentence(query)","05b50f19":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","85f945bf":"query=\"Persistence and stability on a multitude of substrates and sources  like nasal discharge, sputum, urine, fecal matter, blood\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n#cl_df.to_csv(\"\/kaggle\/working\/cl_df.csv\",index=False)\n#query_vector = model.embed_sentence(query)\n","3befbf70":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","0c1f321d":"\nquery=\"Tools and studies to monitor phenotypic change and potential adaptation of the virus ?\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n#cl_df.to_csv(\"\/kaggle\/working\/cl_df.csv\",index=False)\n#query_vector = model.embed_sentence(query)\n","cde4d874":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","ac5f98f5":"query=\"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings?\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n","4f33d0d9":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","38216324":"query=\"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n","8f165024":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","d5f28721":"query=\"Role of the environment in transmission\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n","f682dcbe":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","ddadca68":"query=\"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"\nfrom pyserini.search import pysearch\nsearcher = pysearch.SimpleSearcher(luceneDir)\nquery_result = show_query_results(query, searcher, top_k=10)\nfiles_list=query_id(query_result)\ncl_df=generate_clean_df(files_list)\n","aa286066":"query_vector = model.embed_sentence(query)\ntest_query(cl_df)","b0bf4754":"Extracting the Paper_id of the information of the top searched results","b5334463":"Note: This work is highly inspired from few other kaggle kernels , github sources and other data science resources. Any traces of replications, which may appear , is purely co-incidental. Due respect & credit to all my fellow kagglers. Thanks !!\n\nAnd this notebook is WIP"}}