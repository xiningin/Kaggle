{"cell_type":{"c2974562":"code","da195542":"code","efa5c8d2":"code","2c825ec4":"code","8165263d":"code","b1b5e875":"code","b8bea7f2":"code","8f550411":"code","7af4e499":"code","1c4b27df":"code","3a9a3f0c":"code","187b7dbc":"code","74db534f":"code","e80d106e":"code","89912499":"code","6b822a0d":"code","11b9e1d8":"code","59c90cd9":"code","1ccdbe7d":"code","4a1f3b27":"code","50462ae2":"code","bd18d452":"code","2ac077d0":"code","8b76f90b":"code","14b84180":"code","6521eaef":"code","99b64eb7":"code","ed18a56b":"code","2c2ac43b":"code","d0040828":"code","3f10fe6f":"code","4ae93bd8":"code","dee595a4":"code","970e71f5":"code","fb1d0b04":"code","088708c1":"code","4339f4df":"code","68da4f9d":"code","a9c16c88":"code","8e628ccc":"code","5300ad0d":"code","40ce9c8d":"code","03d96bb9":"code","572992e9":"code","cd87a719":"code","85719872":"code","137de4ea":"code","ef968c75":"code","b5cdafdf":"code","9b05abb1":"markdown","7a84b534":"markdown","19927ca0":"markdown"},"source":{"c2974562":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('vader_lexicon')","da195542":"#internet to be turned on\n#import nltk\n#nltk.download('punkt')\n#nltk.download('stopwords')\n#nltk.download('vader_lexicon')\n#pip install emoji\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy  as np\nimport nltk\nfrom datetime import datetime, timedelta\nimport string\nimport emoji\nimport re\nimport seaborn as sns\nsns.set(style='darkgrid')\nfrom wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","efa5c8d2":"data = pd.read_csv(\"..\/input\/nmimschat\/NMIMSchat.csv\")\ndata['Gender'][data['Gender']=='f'] = 0\ndata['Gender'][data['Gender']=='m'] = 1  \ndata['Gender'] = data['Gender'].astype(int)\n\ndname = data.groupby(['Name'])['DaysTexted','DaysBeen','ActiveDays','Gender'].mean()\ndname['Consistency'] = 100*dname['DaysTexted']\/dname['DaysBeen']\ndname['TimesTexted'] = data.groupby(['Name'])['Name'].count()\ndname['Frequentness'] = 10*dname['TimesTexted']\/dname['DaysBeen']\ndname['Agressiveness'] = 10*dname['TimesTexted']\/dname['ActiveDays']\n#dname.sort_values(by='Consistency',ascending=False)\n#dname.sort_values(by='TimesTexted',ascending=False)\nprint('\ud83d\ude0e '+'Aman Jain')","2c825ec4":"dname","8165263d":"print('Top 10 Popular folks (TimesTexted)')\ndname.sort_values(by='TimesTexted',ascending=False).head(10).iloc[:,5:6]","b1b5e875":"print('Top 10 Consistent folks (DaysTexted\/DaysBeen)')\ndname.sort_values(by='Consistency',ascending=False).head(10).iloc[:,4:5]","b8bea7f2":"print('Top 10 most Frequently texting folks (TimesTexted\/DaysBeen)')\ndname.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,6:7]","8f550411":"names = pd.DataFrame(data.groupby(['Name']).size(),columns={'TextTimes'}).reset_index()\nnames['avgWordspertext'] = 0\nnames['minWordspertext'] = 0\nnames['maxWordspertext'] = 0\nnames['evocab'] = 0\nnames['totalemojis'] = 0\nnames['top5emojis'] = 0\nnames['vocab'] = 0\nnames['top5words'] = 0\ndef nltk_sentiment(sentence):\n      nltk_sentiment = SentimentIntensityAnalyzer()\n      score = nltk_sentiment.polarity_scores(sentence)\n      return score\ndef getResult(pos, neu, neg):\n    if (pos > neu and pos > neg):\n        return (\"Positive\")\n    elif (neg > neu and neg > pos):\n        return (\"Negative\")\n    else:\n        return('Neutral')\n\ndfHFreqs = pd.DataFrame(data.groupby(['Hour'])['Hour'].count())\ndfHFreqs.columns = ['Group']\ndfHFreqs = dfHFreqs.reset_index()\ndfHFreqs.columns = ['Hour','Group']\ns = dfHFreqs['Group'].sum()\ndfHFreqs['Group'] = 100*dfHFreqs['Group']\/s\n\nnames['Pos'] = 0\nnames['Neu'] = 0\nnames['Neg'] = 0\nnames['avgTime'] = 0\nstop = stopwords.words('english')\nvStopWords = [\"guys\",\"groups\",\"thanks\",\"congratulations\",\"birthday\",\"congrats\",\"happy\",\"group\",\"thats\",\"dont\",\"also\",\"like\",\"https\",\"from\",\"all\",\"also\",\"and\",\"any\",\"are\",\"but\",\"can\",\"cant\",\"cry\",\"due\",\"etc\",\"few\",\"for\",\"get\",\"had\",\"has\",\"hasnt\",\"have\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"inc\",\"into\",\"its\",\"ltd\",\"may\",\"nor\",\"not\",\"now\",\"off\",\"once\",\"one\",\"only\",\"onto\",\"our\",\"ours\",\"out\",\"over\",\"own\",\"part\",\"per\",\"put\",\"see\",\"seem\",\"she\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"thence\",\"there\",\"these\",\"they\",\"this\",\"those\",\"though\",\"thus\",\"too\",\"top\",\"upon\",\"very\",\"via\",\"was\",\"were\",\"what\",\"when\",\"which\",\"while\",\"who\",\"whoever\",\"whom\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"would\",\"yet\",\"you\",\"your\",\"yours\",\"the\"]\nstop = stop + vStopWords\nylabel='% of total time spent'\nxlabel='Time clock in 24 hours'","7af4e499":"for name in names['Name']:\n  data1 = data[data['Name']==name]\n  dstr = ' '.join(data1['Text'])\n  dlist = data1['Text'].to_list()\n  \n  #names['avgTime'][names['Name']==name] = data1['Time'].mean().strftime(\"%I:%M %p\")\n\n  L1 = []\n  for l in dlist:\n    L1.append(len(l.split()))\n  names['avgWordspertext'][names['Name']==name] = np.mean(L1)\n  names['minWordspertext'][names['Name']==name] = np.min(L1)\n  names['maxWordspertext'][names['Name']==name] = np.max(L1)\n  \n  LE = []\n  LE = [c for c in dstr if c in emoji.UNICODE_EMOJI]\n  dfE = pd.DataFrame({'Emoji':LE})\n  dfEFreqs = pd.DataFrame(dfE.groupby(['Emoji'])['Emoji'].count())\n  dfEFreqs.columns = ['Freq']\n  dfEFreqs = dfEFreqs.reset_index()\n  dfEFreqs.columns = ['Emoji','Freq']\n  names['evocab'][names['Name']==name] = len(dfEFreqs)\n  names['totalemojis'][names['Name']==name] = dfEFreqs['Freq'].sum()\n  dfEFreqs = dfEFreqs.sort_values('Freq',ascending=False)\n  names['top5emojis'][names['Name']==name] = ' '.join(dfEFreqs['Emoji'][0:5])\n\n  demoji = dstr.encode('ascii', 'ignore').decode('ascii')\n  demoji = re.sub(r'[`!?~@#$%^&*()_+-=<>,.:;]', '', demoji)\n  demoji = re.sub(r'[\u2013]', '', demoji)\n  demoji = re.sub(r'[\\[\\]\\(\\)\\{\\}]', '', demoji)\n  demoji = re.sub(r'[\\t\\\"\\'\\\/\\\\]', '', demoji)\n  lstAllWords = demoji.split()\n\n  lstTmpWords=[]\n  for strWord in lstAllWords:\n      if len(strWord)>3:\n          lstTmpWords.append(strWord)\n  lstAllWords = lstTmpWords\n  del lstTmpWords\n\n  for i in range(0,len(lstAllWords)):\n      lstAllWords[i] = str.lower(lstAllWords[i])\n\n  dfWords = pd.DataFrame({'Words':lstAllWords})\n  dfWords = dfWords[-dfWords['Words'].isin(stop)]\n  dfWords = dfWords[-dfWords['Words'].isin(emoji.UNICODE_EMOJI.keys())]\n\n  dfFreqs = pd.DataFrame(dfWords.groupby(['Words'])['Words'].count())\n  dfFreqs.columns = ['Freq']\n  dfFreqs = dfFreqs.reset_index()\n  dfFreqs.columns = ['Word','Freq']\n  names['vocab'][names['Name']==name] = len(dfFreqs)\n  dfFreqs = dfFreqs.sort_values('Freq',ascending=False)\n  names['top5words'][names['Name']==name] = ' '.join(dfFreqs['Word'][0:5])\n\n  print('\ud83d\ude0e '+name)\n  d = {}\n  for a, x in dfFreqs[0:10].values:\n      d[a] = x \n  wordcloud = WordCloud(background_color=\"white\")\n  wordcloud.generate_from_frequencies(frequencies=d)\n  plt.figure()\n  plt.imshow(wordcloud, interpolation=\"bilinear\")\n  plt.axis(\"off\")\n  plt.show()\n\n  lstLines = sent_tokenize(dstr)\n  lstLines = [t.lower() for t in lstLines]\n  lstLines = [t.translate(str.maketrans('','',string.punctuation)) for t in lstLines]\n  saResults = [nltk_sentiment(t) for t in lstLines]\n  # create dataframe\n  df = pd.DataFrame(lstLines, columns=['Lines'])\n  df['Pos']=[t['pos'] for t in saResults]\n  df['Neu']=[t['neu'] for t in saResults]\n  df['Neg']=[t['neg'] for t in saResults]\n  #df['Result']= [getResult(t['pos'],t['neu'],t['neg']) for t in saResults]\n  names['Pos'][names['Name']==name] = df['Pos'].mean()\n  names['Neu'][names['Name']==name] = df['Neu'].mean()\n  names['Neg'][names['Name']==name] = df['Neg'].mean()  \n    \n","1c4b27df":"names = names.set_index('Name')","3a9a3f0c":"print('Top 10 Emoji-using folks')\nnames.sort_values(by='totalemojis',ascending=False).head(10).iloc[:,5:6]","187b7dbc":"print('Top 10 follks with hishest Emoji vocab')\nnames.sort_values(by='evocab',ascending=False).head(10).iloc[:,4:5]","74db534f":"print('Top 10 folks who write LONG texts')\npcs = names[names['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=False).head(10).iloc[:,1:2]","e80d106e":"print('Top 10 folks who write SHORT texts')\npcs = names[names['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=True).head(10).iloc[:,1:2]","89912499":"print('Top 10 Positively texting folks')\npcs = names[names['TextTimes']>30]\npcs.sort_values(by='Pos',ascending=False).head(10).iloc[:,9:10]","6b822a0d":"print('Top 10 Neutraly texting folks')\npcs = names[names['TextTimes']>30]\npcs.sort_values(by='Neu',ascending=False).head(10).iloc[:,10:11]","11b9e1d8":"print('Top 10 folks with diverse vocab \\n(maximum different words)')\nnames.sort_values(by='vocab',ascending=False).head(10).iloc[:,7:8]","59c90cd9":"print('Top 5 emojis most used by Top 10 popular folks')\nnames.sort_values(by='TextTimes',ascending=False).head(10).iloc[:,6:7]","1ccdbe7d":"names['Frequentness'] = dname['Frequentness']\nprint('Top 5 most used words by Top 10 frequently texting folks')\nnames.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,8:9]","4a1f3b27":"names['Gender'] = dname['Gender']","50462ae2":"print('Number of Men and Women in the group')\npcs = names.groupby(['Gender']).size()\nprint('Women:'+str(pcs[0])+'\\nMen:'+str(pcs[1]))","bd18d452":"print('Top Popular Women (TimesTexted)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='TimesTexted',ascending=False).head(10).iloc[:,5:6]","2ac077d0":"print('Top Consistent Women (DaysTexted\/DaysBeen)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='Consistency',ascending=False).head(10).iloc[:,4:5]\n","8b76f90b":"print('Top most Frequently texting Women (TimesTexted\/DaysBeen)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,6:7]\n","14b84180":"print('Top most Emoji-using Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='totalemojis',ascending=False).head(10).iloc[:,5:6]","6521eaef":"print('Top Women with hishest Emoji vocab')\npcs = names[names['Gender']==0]\npcs.sort_values(by='evocab',ascending=False).head(10).iloc[:,4:5]\n","99b64eb7":"print('Top Women who write LONG texts')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=False).head(10).iloc[:,1:2]\n","ed18a56b":"print('Top Women who write SHORT texts')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=True).head(10).iloc[:,1:2]\n","2c2ac43b":"print('Top Positively texting women')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>30]\npcs.sort_values(by='Pos',ascending=False).head(10).iloc[:,9:10]","d0040828":"print('Top Neutraly texting Women')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>30]\npcs.sort_values(by='Neu',ascending=False).head(10).iloc[:,10:11]\n","3f10fe6f":"print('Top Women with diverse vocab \\n(maximum different words)')\npcs = names[names['Gender']==0]\npcs.sort_values(by='vocab',ascending=False).head(10).iloc[:,7:8]","4ae93bd8":"print('Top 5 emojis most used by Top popular Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='TextTimes',ascending=False).head(10).iloc[:,6:7]","dee595a4":"print('Top 5 most used words by Top frequently texting Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,8:9]","970e71f5":"for name in names.index:\n    data1 = data[data['Name']==name]\n    dstr = ' '.join(data1['Text'])\n    dlist = data1['Text'].to_list()\n\n    dfHPFreqs = pd.DataFrame(data[data['Name']==name].groupby(['Hour'])['Hour'].count())\n    dfHPFreqs.columns = [name]\n    dfHPFreqs = dfHPFreqs.reset_index()\n    dfHPFreqs.columns = ['Hour',name]\n    s = dfHPFreqs[name].sum()\n    dfHPFreqs[name] = 100*dfHPFreqs[name]\/s\n    dfC = pd.merge(dfHFreqs,dfHPFreqs,how='left')\n    dfC = dfC.fillna(0)\n    title = 'Texting Pattern: \ud83d\ude0e'+name\n    plt.figure()\n    ax = dfC.iloc[:, 2].plot(legend=True,figsize=(12,6),title=title,color='r')\n    dfC.iloc[:,1].plot(legend=True,figsize=(12,6),color='y')\n    ax.autoscale(axis='x',tight=True)\n    ax.set(xlabel=xlabel, ylabel=ylabel);","fb1d0b04":"dstr = ' '.join(data['Text'])\ndlist = data['Text'].to_list()\nL1 = []\nfor l in dlist:\n  L1.append(len(l.split()))\nLE = []\nLE = [c for c in dstr if c in emoji.UNICODE_EMOJI]\ndfE = pd.DataFrame({'Emoji':LE})\ndfEFreqs = pd.DataFrame(dfE.groupby(['Emoji'])['Emoji'].count())\ndfEFreqs.columns = ['Freq']\ndfEFreqs = dfEFreqs.reset_index()\ndfEFreqs.columns = ['Emoji','Freq']\nevocab = len(dfEFreqs)\ntotalemojis = dfEFreqs['Freq'].sum()\ndfEFreqs = dfEFreqs.sort_values('Freq',ascending=False)\ntop5emojis =  ' '.join(dfEFreqs['Emoji'][0:5])","088708c1":"print('Group Emoji Vocab: '+str(evocab)+'\\n(different emojis used)') ","4339f4df":"print('Total Emojis used in the group: '+str(totalemojis)) ","68da4f9d":"print('Top 10 emojis used in the group')\ndfEFreqs.set_index('Emoji').head(10)","a9c16c88":"demoji = dstr.encode('ascii', 'ignore').decode('ascii')\ndemoji = re.sub(r'[`!?~@#$%^&*()_+-=<>,.:;]', '', demoji)\ndemoji = re.sub(r'[\u2013]', '', demoji)\ndemoji = re.sub(r'[\\[\\]\\(\\)\\{\\}]', '', demoji)\ndemoji = re.sub(r'[\\t\\\"\\'\\\/\\\\]', '', demoji)\nlstAllWords = demoji.split()\ntotalwords = len(lstAllWords)\nlstTmpWords=[]\nfor strWord in lstAllWords:\n    if len(strWord)>3:\n        lstTmpWords.append(strWord)\nlstAllWords = lstTmpWords\ndel lstTmpWords\n\nfor i in range(0,len(lstAllWords)):\n    lstAllWords[i] = str.lower(lstAllWords[i])\n\ndfWords = pd.DataFrame({'Words':lstAllWords})\ndfWords = dfWords[-dfWords['Words'].isin(stop)]\ndfWords = dfWords[-dfWords['Words'].isin(emoji.UNICODE_EMOJI.keys())]\n\ndfFreqs = pd.DataFrame(dfWords.groupby(['Words'])['Words'].count())\ndfFreqs.columns = ['Freq']\ndfFreqs = dfFreqs.reset_index()\ndfFreqs.columns = ['Word','Freq']\nvocab = len(dfFreqs)\n#totalwords = dfFreqs['Freq'].sum()\ndfFreqs = dfFreqs.sort_values('Freq',ascending=False)\n#names['top5words'][names['Name']==name] = ' '.join(dfFreqs['Word'][0:10])","8e628ccc":"print('Group Vocab: '+str(vocab)+'\\n(different words used)') ","5300ad0d":"print('Total Words used in the group: '+str(totalwords)) ","40ce9c8d":"#100*evocab\/vocab\nprint(str(int(np.round(100*totalemojis\/totalwords)))+'% of words used were emojis')","03d96bb9":"print('Top 10 words used in the group')\ndfFreqs.set_index('Word').head(10)","572992e9":"d = {}\nfor a, x in dfFreqs[0:10].values:\n    d[a] = x \nwordcloud = WordCloud(background_color=\"white\")\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n","cd87a719":"lstLines = sent_tokenize(dstr)\nlstLines = [t.lower() for t in lstLines]\nlstLines = [t.translate(str.maketrans('','',string.punctuation)) for t in lstLines]\nsaResults = [nltk_sentiment(t) for t in lstLines]\n# create dataframe\ndf = pd.DataFrame(lstLines, columns=['Lines'])\ndf['Pos']=[t['pos'] for t in saResults]\ndf['Neu']=[t['neu'] for t in saResults]\ndf['Neg']=[t['neg'] for t in saResults]\n#df['Result']= [getResult(t['pos'],t['neu'],t['neg']) for t in saResults]\npos = df['Pos'].mean()\nneu = df['Neu'].mean()\nneg = df['Neg'].mean()  ","85719872":"print('Sentiments of the group')\nprint(str(round(pos*100,2))+'% Positive')\nprint(str(round(neu*100,2))+'% Neutral')\nprint(str(round(neg*100,2))+'% Negative')","137de4ea":"dfHFreqs = dfHFreqs.set_index('Hour')\ntitle='Group Texting Pattern'\nplt.figure()\nax = dfHFreqs.plot(legend=True,figsize=(12,6),title=title,color='r')\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel);","ef968c75":"mc = pd.read_csv(\"..\/input\/nmimschat\/monthcount.csv\")","b5cdafdf":"for s in mc['Name']:\n  data1 = mc[mc['Name']==s]\n  data1 = data1.transpose()\n  data1 = data1[1:]\n  data1.columns = ['TextedTimes']\n  plt.figure()\n  ax = data1.plot(legend=True,figsize=(15,6),title='Month wise Texting Frequency : '+s,color='r')\n  ax.autoscale(axis='x',tight=True)\n  ax.set(xlabel='Month', ylabel='Times Texted');","9b05abb1":"On scrolling down, you will find:\n\n**1. For Everyone**\n* Top 10 Favorite words (wordcloud)\n* Texting Pattern during 24 hours (compared with group) \n\n\n**2. Top 10 folks** \n* Top 10 Popular folks\n* Top 10 most Emoji-using folks\n* Top 10 folks who write LONG texts\n* Top 10 folks who write SHORT texts\n* Top 10 Consistent folks\n* Top 10 most Frequently texting folks\n* Top 10 follks with hishest Emoji vocab\n* Top 10 Positively texting folks\n* Top 10 Neutraly texting folks\n* Top 10 folks with diverse vocab\n* Top 5 emojis most used by Top 10 popular folks\n* Top 5 most used words by Top 10 frequently texting folks\n\n\n*As Women as not even half of Men, here goes one exclusively for Women*\n\n**3. Top Women**\n* Top Popular Women \n* Top Women who write SHORT texts\n* Top emojis most used by Top popular women\n* Top Women with diverse vocab\n* Top Positively texting Women\n* Top Consistent women\n* Top most Frequently texting women\n* Top Women with hishest Emoji vocab\n* Top Women who write LONG texts\n* Top Neutraly texting Women\n* Top most used words by Top frequently texting women\n\n\n**4. Group Characteristics** \n\n* Group Texting Pattern\n* Top 10 words used in the group\n* Top 10 emojis used in the group\n\n\n","7a84b534":"*   Done by **Aman Jain**\n*   **[ LinkedIn](https:\/\/www.linkedin.com\/in\/amanacden\/)**   **[Instagram](http:\/\/instagram.com\/amanacden)**","19927ca0":"**NMIMS DSA Chat Analysis**\n\n(Mar 11, 2019 - May 31, 2020)"}}