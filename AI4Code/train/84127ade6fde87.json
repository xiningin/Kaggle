{"cell_type":{"f3b94d01":"code","75b2cc1a":"code","a2a4151b":"code","adcfee09":"code","338a4959":"code","81f398c9":"code","56bf9f57":"code","4b39971d":"code","e357e15a":"markdown","764c68ed":"markdown","0f61dfdf":"markdown","a5242d16":"markdown","5e231c4d":"markdown","d3998d64":"markdown","1cd5aefd":"markdown","52afd360":"markdown","3f5c46be":"markdown","b6e6a02d":"markdown","aa089410":"markdown","170ac58d":"markdown","781cf349":"markdown","f3992c51":"markdown","607ee4cd":"markdown","f68a17d4":"markdown","c52a9509":"markdown","df32b934":"markdown","ae2c6e81":"markdown","e413f35b":"markdown","cbb030b8":"markdown","9bbf51fd":"markdown","982b7dcb":"markdown","dff13e63":"markdown","ca5cdfea":"markdown","c26779c9":"markdown","1e87931a":"markdown","5b26f6aa":"markdown","03e5b1a3":"markdown","c222cabc":"markdown","68881c86":"markdown","1edb9168":"markdown","af130b4a":"markdown","de46c7ce":"markdown","3b1ec42a":"markdown","c031b479":"markdown","09da1e1b":"markdown","0d25746e":"markdown","9f9baa38":"markdown","e2f6992a":"markdown","3f8c5f8b":"markdown","68065dc4":"markdown","41ced03b":"markdown","809b2877":"markdown","63707548":"markdown","87cfd492":"markdown","01d3f86d":"markdown","375d9408":"markdown","fecdef80":"markdown","3ecedfe7":"markdown"},"source":{"f3b94d01":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nimport numpy as np\nimport pandas as pd\nimport torch\ntorch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)","75b2cc1a":"with open('\/kaggle\/input\/13437.txt', encoding='utf8') as f:\n    text = f.read()","a2a4151b":"lines = text.split('\\n')\nline = lines[504]\nline","adcfee09":"letter_t = torch.zeros(len(line), 128)  # 128 hardcoded due to the limits of ASCII\nletter_t.shape","338a4959":"for i, letter in enumerate(line.lower().strip()):\n    letter_index = ord(letter) if ord(letter) < 128 else 0  # The text uses directional double\n                                                            # quotes, which are not valid ASCII,\n                                                            # so we screen them out here.\n    letter_t[i][letter_index] = 1","81f398c9":"def clean_words(input_str):\n    punctuation = '.,;:\"!?\u201d\u201c_-'\n    word_list = input_str.lower().replace('\\n',' ').split()\n    word_list = [word.strip(punctuation) for word in word_list]\n    return word_list\n\nwords_in_line = clean_words(line)\nline, words_in_line","56bf9f57":"word_list = sorted(set(clean_words(text)))\nword2index_dict = {word: i for (i, word) in enumerate(word_list)}\n\nlen(word2index_dict), word2index_dict['grandmother']","4b39971d":"word_t = torch.zeros(len(words_in_line), len(word2index_dict))\nfor i, word in enumerate(words_in_line):\n    word_index = word2index_dict[word]\n    word_t[i][word_index] = 1\n    print('{:2} {:4} {}'.format(i, word_index, word))\n\nprint(word_t.shape)","e357e15a":"For most things, our mapping is just splitting by words. But the rarer parts\u2014the capitalized Impossible and the name Bennet\u2014are composed of subunits.","764c68ed":"At this point, we need to parse through the characters in the text and provide a one-hot encoding for each of them. Each character will be represented by a vector of length equal to the number of different characters in the encoding. This vector will contain all zeros except a one at the index corresponding to the location of the character in the encoding.","0f61dfdf":"We have one-hot encoded our sentence into a representation that a neural network could digest. Word-level encoding can be done the same way by establishing a vocabu- lary and one-hot encoding sentences\u2014sequences of words\u2014along the rows of our tensor. Since a vocabulary has many words, this will produce very wide encoded vectors, which may not be practical. We will see in the next section that there is a more efficient way to represent text at the word level, using embeddings. For now, let\u2019s stick with one-hot encodings and see what happens.","a5242d16":"![image.png](attachment:image.png)","5e231c4d":"?Im|pos|s|ible|,|?Mr|.|?B|en|net|,|?impossible|,|?when|?I|?am|?not|?acquainted|?with|?him\n\nThis is from a SentencePiece tokenizer trained on a machine translation dataset.","d3998d64":"The choice between character-level and word-level encoding leaves us to make a trade-off. In many languages, there are significantly fewer characters than words: representing characters has us representing just a few classes, while representing words requires us to represent a very large number of classes and, in any practical application, deal with words that are not in the dictionary. On the other hand, words convey much more meaning than individual characters, so a representation of words is considerably more informative by itself. Given the stark contrast between these two options, it is perhaps unsurprising that intermediate ways have been sought, found, and applied with great success: for example, the byte pair encoding method starts with a dictionary of individual letters but then iteratively adds the most frequently observed pairs to the dictionary until it reaches a prescribed dictionary size. Our example sentence might then be split into tokens like this:","1cd5aefd":"**NOTE** 128 characters are clearly not enough to account for all the glyphs, accents, ligatures, and so on that are needed to properly represent written text in languages other than English. To this end, a number of encodings have been developed that use a larger number of bits as code for a wider range of characters. That wider range of characters was standardized as Unicode, which maps all known characters to numbers, with the representation in bits of those numbers provided by a specific encoding. Popular encodings are UTF-8, UTF-16, and UTF-32, in which the numbers are a sequence of 8-, 16-, or 32-bit integers, respectively. Strings in Python 3.x are Unicode strings.","52afd360":"Let\u2019s load Best Russian Short Stories by Leonid Andreyev et al. from the Project Gutenberg website:\nhttp:\/\/www.gutenberg.org\/cache\/epub\/13437\/pg13437.txt. We\u2019ll just save the file and read it in\n(code\/p1ch4\/5_text_jane_austen.ipynb).","3f5c46be":"Next, let\u2019s build a mapping of words to indexes in our encoding:","b6e6a02d":"## Converting text to numbers","aa089410":"## One-hot encoding whole words","170ac58d":"* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-dog-detection\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-gan-horse-zebra\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch3-tensors\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-working-with-images\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-3d-images-volumetric-data\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-tabular-data\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-time-series","781cf349":"We certainly could do some work to deduplicate words, condense alternate spellings, collapse past and future tenses into a single token, and that kind of thing. Still, a general-purpose English-language encoding would be huge. Even worse, every time we encountered a new word, we would have to add a new column to the vector, which would mean adding a new set of weights to the model to account for that new vocabulary entry\u2014which would be painful from a training perspective.","f3992c51":"We are going to one-hot encode our characters. It is instrumental to limit the one-hot encoding to a character set that is useful for the text being analyzed. In our case, since we loaded text in English, it is safe to use ASCII and deal with a small encoding. We could also make all of the characters lowercase, to reduce the number of different characters in our encoding. Similarly, we could screen out punctuation, numbers, or other characters that aren\u2019t relevant to our expected kinds of text. This may or may not make a practical difference to a neural network, depending on the task at hand.","607ee4cd":"## Text embeddings as a blueprint","f68a17d4":"Conclusion","c52a9509":"Let\u2019s create a tensor that can hold the total number of one-hot-encoded characters for the whole line:","df32b934":"We\u2019ll define clean_words , which takes text and returns it in lowercase and stripped of punctuation. When we call it on our \u201cImpossible, Mr. Bennet\u201d line , we get the following:","ae2c6e81":"As you\u2019ve probably already guessed, this kind of work can be automated. By processing a large corpus of organic text, embeddings similar to the one we just discussed can be generated. The main differences are that there are 100 to 1,000 elements in the embedding vector and that axes do not map directly to concepts: rather, conceptually similar words map in neighboring regions of an embedding space whose axes are arbitrary floating-point dimensions.","e413f35b":"As we start embedding words, we can map apple to a number in the fruit and red quadrant. Likewise, we can easily map tangerine, lemon, lychee, and kiwi (to round out our list of colorful fruits). Then we can start on flowers, and assign rose, poppy, daffodil, lily, and ... Hmm. Not many brown flowers out there. Well, sunflower can get flower, yellow, and brown, and then daisy can get flower, white, and yellow. Perhaps we should update kiwi to map close to fruit, brown, and green. For dogs and color, we can embed redbone near red; uh, fox perhaps for orange; golden retriever for yellow, poodle for white, and... most kinds of dogs are brown.","cbb030b8":"One example is word2vec: https:\/\/code.google.com\/archive\/p\/word2vec.","9bbf51fd":"More contemporary embedding models\u2014with BERT and GPT-2 making headlines even in mainstream media\u2014are much more elaborate and are context sensitive: that is, the mapping of a word in the vocabulary to a vector is not fixed but depends on the surrounding sentence. Yet they are often used just like the simpler classic embeddings we\u2019ve touched on here.","982b7dcb":"## One-hot-encoding characters","dff13e63":"There are two particularly intuitive levels at which networks operate on text: at the\ncharacter level, by processing one character at a time, and at the word level, where\nindividual words are the finest-grained entities to be seen by the network. The tech-\nnique with which we encode text information into tensor form is the same whether we\noperate at the character level or the word level. And it\u2019s not magic, either. We stum-\nbled upon it earlier: one-hot encoding.","ca5cdfea":"All data from book **Deep Learning with PyTorch** https:\/\/pytorch.org\/deep-learning-with-pytorch","c26779c9":"How can we compress our encoding down to a more manageable size and put a cap on the size growth? Well, instead of vectors of many zeros and a single one, we can use vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can indeed represent a large number of words. The trick is to find an effective way to map individual words into this 100-dimensional space in a way that facilitates downstream learning. This is called an embedding.","1e87931a":"# Representing text","5b26f6aa":"Note that word2index_dict is now a dictionary with words as keys and an integer as a value. We will use it to efficiently find the index of a word as we one-hot encode it. Let\u2019s now focus on our sentence: we break it up into words and one-hot encode it \u2014 that is, we populate a tensor with one one-hot-encoded vector per word. We create an empty vector and assign the one-hot-encoded values of the word in the sentence:","03e5b1a3":"One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also having consistent spatial relationships with other words. For example, if we were to take the embedding vector for apple and begin to add and subtract the vectors for other words, we could begin to perform analogies like apple - red - sweet + yellow + sour and end up with a vector very similar to the one for lemon.","c222cabc":"![image.png](attachment:image.png)","68881c86":"Our goal in this section is to turn text into something a neural network can pro-\ncess: a tensor of numbers, just like our previous cases. If we can do that and later\nchoose the right architecture for our text-processing job, we\u2019ll be in the position of\ndoing NLP with PyTorch. We see right away how powerful this all is: we can achieve state-of-the-art performance on a number of tasks in different domains with the same\nPyTorch tools; we just need to cast our problem in the right form. The first part of this\njob is reshaping the data.","1edb9168":"While the exact algorithms used are a bit out of scope for what we\u2019re wanting to focus on here, we\u2019d just like to mention that embeddings are often generated using neural networks, trying to predict a word from nearby words (the context) in a sentence. In this case, we could start from one-hot-encoded words and use a (usually rather shallow) neural network to generate the embedding. Once the embedding was available, we could use it for downstream tasks.","af130b4a":"Embeddings are an essential tool for when a large number of entries in the vocabulary have to be represented by numeric vectors. But we won\u2019t be using text and text embeddings in this book, so you might wonder why we introduce them here. We believe that how text is represented and processed can also be seen as an example for dealing with categorical data in general. Embeddings are useful wherever one-hot encoding becomes cumbersome. Indeed, in the form described previously, they are an efficient way of representing one-hot encoding immediately followed by multiplication with the matrix containing the embedding vectors as rows.","de46c7ce":"We\u2019ve covered a lot of ground in this chapter. We learned to load the most common types of data and shape them for consumption by a neural network. Of course, there are more data formats in the wild than we could hope to describe in a single volume. Some, like medical histories, are too complex to cover here. Others, like audio and video, were deemed less crucial for the path of this book. If you\u2019re interested, however, we provide short examples of audio and video tensor creation in bonus Jupyter Notebooks provided on the book\u2019s website (www.manning.com\/books\/deep-learning-with-pytorch) and in our code repository (https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code\/tree\/master\/p1ch4).","3b1ec42a":"When we are interested in co-occurrences of observations, the word embeddings we saw earlier can serve as a blueprint, too. For example, recommender systems\u2014customers who liked our book also bought ...\u2014use the items the customer already interacted with as the context for predicting what else will spark interest. Similarly, processing text is perhaps the most common, well-explored task dealing with sequences; so, for example, when working on tasks with time series, we might look for inspiration in what is done in natural language processing.","c031b479":"In non-text applications, we usually do not have the ability to construct the embeddings beforehand, but we will start with the random numbers we eschewed earlier and consider improving them part of our learning problem. This is a standard technique\u2014so much so that embeddings are a prominent alternative to one-hot encodings for any categorical data. On the flip side, even when we deal with text, improving the prelearned embeddings while solving the problem at hand has become a common practice.","09da1e1b":"Now that we\u2019re familiar with tensors and how to store data in them, we can move on to the next step towards the goal of the book: teaching you to train deep neural networks! The next chapter covers the mechanics of learning for simple linear models.","0d25746e":"There\u2019s one more detail we need to take care of before we proceed: encoding. This is \u0430 pretty vast subject, and we will just touch on it. Every written character is represented \u0431\u044b a code: a sequence of bits of appropriate length so that each character can be \u0443\u043d\u0438\u043a\u0443\u0435\u043b\u0438 identified. The simplest such encoding is ASCII (American Standard \u041a\u043e\u0434\u0435 \u0444\u043e\u0440 Information Interchange), which dates back to the 1960s. ASCII encodes 128 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0430 using 128 integers. For instance, the letter a corresponds to binary 1100001 or \u0434\u0435\u0446\u0438\u043c\u0430\u043b\u044c 97, the letter b to binary 1100010 or decimal 98, and so on. The encoding fits 8 bits, which was a big bonus in 1965.","9f9baa38":"Nadkarni et al., \u201cNatural language processing: an introduction,\u201d JAMIA, http:\/\/mng.bz\/8pJP. See also\nhttps:\/\/en.wikipedia.org\/wiki\/Natural-language_processing.","e2f6992a":"Deep learning has taken the field of natural language processing (NLP) by storm, par-\nticularly using models that repeatedly consume a combination of new input and previ-\nous model output. These models are called recurrent neural networks (RNNs), and they\nhave been applied with great success to text categorization, text generation, and auto-\nmated translation systems. More recently, a class of networks called transformers with a\nmore flexible way to incorporate past information has made a big splash. Previous\nNLP workloads were characterized by sophisticated multistage pipelines that included\nrules encoding the grammar of a language. 5 Now, state-of-the-art work trains networks\nend to end on large corpora starting from scratch, letting those rules emerge from the\ndata. For the last several years, the most-used automated translation systems available\nas services on the internet have been based on deep learning.","3f8c5f8b":"Well, if we were to design a solution to this problem by hand, we might decide to build our embedding space by choosing to map basic nouns and adjectives along the axes. We can generate a 2D space where axes map to nouns\u2014fruit (0.0-0.33), flower (0.33-0.66), and dog (0.66-1.0)\u2014and adjectives\u2014red (0.0-0.2), orange (0.2-0.4), yellow (0.4-0.6), white (0.6-0.8), and brown (0.8-1.0). Our goal is to take actual fruit, flowers, and dogs and lay them out in the embedding.","68065dc4":"At this point, tensor represents one sentence of length 12 in an encoding space of size 10,160, the number of words in our dictionary. Figure 4.6 compares the gist of our two options for splitting text (and using the embeddings we\u2019ll look at in the next section).","41ced03b":"Let\u2019s start with a character-level example. First, let\u2019s get some text to process. An\namazing resource here is Project Gutenberg (www.gutenberg.org), a volunteer effort\nto digitize and archive cultural work and make it available for free in open formats,\nincluding plain text files. If we\u2019re aiming at larger-scale corpora, the Wikipedia corpus\nstands out: it\u2019s the complete collection of Wikipedia articles, containing 1.9 billion\nwords and more than 4.4 million articles. Several other corpora can be found at the\nEnglish Corpora website (www.english-corpora.org).","809b2877":"Note that letter_t holds a one-hot-encoded character per row. Now we just have to set a one on each row in the correct position so that each row represents the correct character. The index where the one has to be set corresponds to the index of the character in the encoding:","63707548":"In principle, we could simply iterate over our vocabulary and generate a set of 100 random floating-point numbers for each word. This would work, in that we could cram a very large vocabulary into just 100 numbers, but it would forgo any concept of distance between words based on meaning or context. A model using this word embedding would have to deal with very little structure in its input vectors. An ideal solution would be to generate the embedding in such a way that words used in similar contexts mapped to nearby regions of the embedding.","87cfd492":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","01d3f86d":"One-hot encoding is a very useful technique for representing categorical data in tensors. However, as we have anticipated, one-hot encoding starts to break down when the number of items to encode is effectively unbound, as with words in a corpus. In just one book, we had over 7,000 items!","375d9408":"Now our embeddings look like figure 4.7. While doing this manually isn\u2019t really feasible for a large corpus, note that although we had an embedding size of 2, we described 15 different words besides the base 8 and could probably cram in quite a few more if we took the time to be creative about it.","fecdef80":"## Text embeddings","3ecedfe7":"We first split our text into a list of lines and pick an arbitrary line to focus on:"}}