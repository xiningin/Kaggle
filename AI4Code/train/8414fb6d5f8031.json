{"cell_type":{"88ff4393":"code","f5978d89":"code","1473b616":"code","d95cb739":"code","e9d8aac9":"code","48596128":"code","80ca1343":"code","c9daddbf":"code","e60613cd":"code","abe9c9cb":"code","943a261c":"code","14510b87":"code","9dfcd87b":"code","ddfa8137":"code","9a8bdc71":"code","db29685c":"code","d04729eb":"code","898bd92a":"code","ffa7ba7c":"code","630d1dfb":"code","04fd36da":"code","78dd1c2f":"code","0de7f3cf":"code","50872743":"code","260d6824":"code","6d520dbe":"code","2c37c847":"code","d2d52e33":"code","4286c757":"code","d0805362":"code","d8ada78f":"code","6d979f7d":"code","570bc6c2":"code","b20f37f9":"code","34474d71":"code","11629634":"code","e758ba05":"code","6b1a384d":"code","6508aabc":"code","fbdd5244":"code","e4efebc3":"code","dac07776":"markdown"},"source":{"88ff4393":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","f5978d89":"combined_news_djia = pd.read_csv('..\/input\/Combined_News_DJIA.csv')\ndjia_table = pd.read_csv('..\/input\/DJIA_table.csv')\nredditnews = pd.read_csv('..\/input\/RedditNews.csv')","1473b616":"combined_news_djia.head()","d95cb739":"djia_table.head()","e9d8aac9":"redditnews.head()","48596128":"import matplotlib\nimport matplotlib.pylab as plt\nmatplotlib.style.use(\"ggplot\")\n\ndjia_table[['Date', 'Close']].plot(color='blue', figsize=(10,7))\nplt.show()","80ca1343":"data = combined_news_djia.merge(djia_table[['Date', 'Close']], how='left', on='Date')\ndata['Close_diff'] = data['Close'].diff()\ndata.head()","c9daddbf":"data.dtypes","e60613cd":"import re\n\ndef analyzer(sentence):\n    sentence = str(sentence)\n    sentence = sentence.lower()\n    sentence = re.sub(re.compile('[\\s\\t\\r\\n]'), ' ', sentence)\n    sentence = re.sub(re.compile('[^a-z\\s]'), ' ', sentence)\n    sentence = re.sub(re.compile('[\\s]+'), ' ', sentence)\n    return sentence\n\ndates, closes, close_diffs,close_abs_diffs,  labels, news = [], [], [], [], [], []\nfor i, row in data.iterrows():\n    dates.append(row['Date'])\n    closes.append(row['Close'])\n    close_diffs.append(row['Close_diff'])\n    close_abs_diffs.append(abs(row['Close_diff']))\n    labels.append(row['Label'])\n    news.append(' '.join(row['Top1':'Top25'].apply(analyzer).as_matrix()))","abe9c9cb":"len(dates)","943a261c":"stop_words = ['b', 'a', 'am' ,'an' ,'and', 'are', 'as', 'at', 'be', 'but', 'by', 'did', 'do', 'does', 'for', 'from', 'had', 'has', 'have', 'how', 'i', 'if', 'in', 'is', 'it', 'its', 'me', 'my', 'no', 'not', 'of', 'on', 'or', 'to', 'over', 'same', 'so', 'some', 'such', 'than', 'that', 'the', 'then', 'there', 'they', 'this', 'to', 'too', 'very', 'was', 'we', 'what', 'when', 'where', 'which', 'who', 'why', 'with', 'would', 'you', 'your']\n\ndef analyzer2(sentence):\n    tmp = []\n    words = sentence.split(' ')\n    for word in words:\n        if word not in stop_words and len(word) > 1:\n            tmp.append(word)\n    return ' '.join(tmp).strip()\n    \nnews_tmp = []\nfor sentence in news:\n    news_tmp.append(analyzer2(sentence))\nnews = news_tmp\nlen(news)","14510b87":"# BoW, tf-idf, Word2Vec, SCDV\n\nfeatures_num = 200\nmin_word_count = 10\ncontext = 5\ndownsampling = 1e-3\nepoch_num = 10","9dfcd87b":"# BoW\n\nimport time\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nst = time.time()\ncorpus = news\ncount_vectorizer = CountVectorizer(min_df=min_word_count, binary=True)\nbows = count_vectorizer.fit_transform(corpus)\ned = time.time()\nprint(ed-st)\n\nbows.shape","ddfa8137":"from sklearn.manifold import TSNE\n\nst = time.time()\ntsne_bow = TSNE(n_components=2).fit_transform(bows.toarray())\ned = time.time()\nprint(ed-st)","9a8bdc71":"tsne_bow_df = pd.DataFrame({\n    'x': tsne_bow[:, 0],\n    'y': tsne_bow[:, 1],\n    'label': labels,\n    'close_diff': close_diffs,\n    'close_abs_diff': close_abs_diffs\n})","db29685c":"tsne_bow_df.plot.scatter(x='x', y='y', c='label', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","d04729eb":"tsne_bow_df.plot.scatter(x='x', y='y', c='close_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","898bd92a":"tsne_bow_df.plot.scatter(x='x', y='y', c='close_abs_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","ffa7ba7c":"# tf-idf \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nst = time.time()\ncorpus = news\ntfidf_vectorizer = TfidfVectorizer(min_df=min_word_count)\ntfidfs = tfidf_vectorizer.fit_transform(corpus)\ned = time.time()\nprint(ed-st)\n\ntfidfs.shape","630d1dfb":"st = time.time()\ntsne_tfidf = TSNE(n_components=2).fit_transform(tfidfs.toarray())\ned = time.time()\nprint(ed-st)","04fd36da":"tsne_tfidf_df = pd.DataFrame({\n    'x': tsne_tfidf[:, 0],\n    'y': tsne_tfidf[:, 1],\n    'label': labels,\n    'close_diff': close_diffs,\n    'close_abs_diff': close_abs_diffs\n})","78dd1c2f":"tsne_tfidf_df.plot.scatter(x='x', y='y', c='label', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","0de7f3cf":"tsne_tfidf_df.plot.scatter(x='x', y='y', c='close_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","50872743":"tsne_tfidf_df.plot.scatter(x='x', y='y', c='close_abs_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","260d6824":"# Word2Vec\n\nfrom gensim.models import Word2Vec\n\nst = time.time()\ncorpus = []\nfor sentence in news:\n    corpus.append(sentence.split(' '))\nword2vecs = Word2Vec(\n    sentences = corpus,\n     iter = epoch_num,\n     size = features_num,\n     min_count = min_word_count,\n     window = context,\n     sample = downsampling,\n)\n#path = '.\/{}_features_word2vecs'.format(features_num)\n#word2vecs.save(path)\n#word2vecs = Word2Vec.load(path)\ned = time.time()\nprint(ed-st)","6d520dbe":"def to_word2vec_avg(sentence):\n    score = np.zeros(features_num, dtype=np.float32)\n    sentences = sentence.split(' ')\n    for word in sentences:\n        if word in word2vecs:\n            score += word2vecs[word]\n    score \/= len(sentences)\n    return score\n\nword2vec_avgs = []\nfor sentence in news:\n    word2vec_avgs.append(to_word2vec_avg(sentence))\nword2vec_avgs = np.array(word2vec_avgs)\nword2vec_avgs.shape","2c37c847":"st = time.time()\ntsne_word2vec_avgs = TSNE(n_components=2).fit_transform(word2vec_avgs)\ned = time.time()\nprint(ed-st)","d2d52e33":"tsne_word2vec_avg_df = pd.DataFrame({\n    'x': tsne_word2vec_avgs[:, 0],\n    'y': tsne_word2vec_avgs[:, 1],\n    'label': labels,\n    'close_diff': close_diffs,\n    'close_abs_diff': close_abs_diffs\n})","4286c757":"tsne_word2vec_avg_df.plot.scatter(x='x', y='y', c='label', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","d0805362":"tsne_word2vec_avg_df.plot.scatter(x='x', y='y', c='close_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","d8ada78f":"tsne_word2vec_avg_df.plot.scatter(x='x', y='y', c='close_abs_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","6d979f7d":"# SCDV-Word2Vec\n# https:\/\/arxiv.org\/pdf\/1612.06778.pdf\n\nfrom sklearn.mixture import GaussianMixture\n\nst = time.time()\nword_vectors = word2vecs.wv.syn0\nclusters_num = 60\ngmm = GaussianMixture(n_components=clusters_num, covariance_type='tied', max_iter=50)\ngmm.fit(word_vectors)\ned = time.time()\nprint(ed-st)","570bc6c2":"idf_dic = dict(zip(tfidf_vectorizer.get_feature_names(), tfidf_vectorizer._tfidf.idf_))\nassign_dic = dict(zip(word2vecs.wv.index2word, gmm.predict(word_vectors)))\nsoft_assign_dic = dict(zip(word2vecs.wv.index2word, gmm.predict_proba(word_vectors)))","b20f37f9":"st = time.time()\nword_topic_vecs = {}\nfor word in assign_dic:\n    word_topic_vecs[word] = np.zeros(features_num*clusters_num, dtype=np.float32)\n    for i in range(0, clusters_num):\n        try:\n            word_topic_vecs[word][i*features_num:(i+1)*features_num] = word2vecs[word]*soft_assign_dic[word][i]*idf_dic[word]\n        except:\n            continue\ned = time.time()\nprint(ed-st)","34474d71":"scdvs = np.zeros((len(news), clusters_num*features_num), dtype=np.float32)\n\na_min = 0\na_max = 0\n\nfor i, sentence in enumerate(news):\n    tmp = np.zeros(clusters_num*features_num, dtype=np.float32)\n    words = sentence.split(' ')\n    for word in words:\n        if word in word_topic_vecs:\n            tmp += word_topic_vecs[word]\n    norm = np.sqrt(np.sum(tmp**2))\n    if norm > 0:\n        tmp \/= norm\n    a_min += min(tmp)\n    a_max += max(tmp)\n    scdvs[i] = tmp\n\np = 0.04\na_min = a_min*1.0 \/ len(news)\na_max = a_max*1.0 \/ len(news)\nthres = (abs(a_min)+abs(a_max)) \/ 2\nthres *= p\n\nscdvs[abs(scdvs) < thres] = 0\nscdvs.shape","11629634":"st = time.time()\ntsne_scdv = TSNE(n_components=2).fit_transform(scdvs)\ned = time.time()\nprint(ed-st)","e758ba05":"tsne_scdv_df = pd.DataFrame({\n    'x': tsne_scdv[:, 0],\n    'y': tsne_scdv[:, 1],\n    'label': labels,\n    'close_diff': close_diffs,\n    'close_abs_diff': close_abs_diffs\n})","6b1a384d":"tsne_scdv_df.plot.scatter(x='x', y='y', c='label', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","6508aabc":"tsne_scdv_df.plot.scatter(x='x', y='y', c='close_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","fbdd5244":"tsne_scdv_df.plot.scatter(x='x', y='y', c='close_abs_diff', cmap='bwr', figsize=(15, 10), s=20)\nplt.show()","e4efebc3":"!echo 'uh...'","dac07776":"* Practice of using [SCDV](https:\/\/arxiv.org\/pdf\/1612.06778.pdf)\n* Visualization of the relationship between distributed representations of documents and prices\n* Distributed representations of documents are BoW, tf-idf, Word2Vec(avg), SCDV"}}