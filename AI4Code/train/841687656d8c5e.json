{"cell_type":{"08330a9f":"code","db5b56bd":"code","45e22222":"code","d2fecf3e":"code","a342dae4":"code","883c40ff":"code","214d43f7":"code","35858c64":"code","0a7d36fc":"code","df96ff13":"code","03cf3fb1":"code","f1daeee3":"code","cd3d7168":"code","dde534de":"code","63de390f":"code","a8b0f001":"code","b7b6ee51":"code","d4e05295":"code","2632ab33":"code","5aee2ca8":"code","2e989ff4":"code","ad6eb3d0":"code","920c7fcb":"code","56b53dbb":"code","ae8e1071":"code","3931a9ae":"code","5b531727":"code","e387ba6f":"code","0bdd8c47":"code","e1861961":"code","38f4f242":"code","d114363a":"code","4936d734":"code","36f808bd":"code","b94285bf":"code","f8872eec":"code","fc9edc89":"code","af9404d0":"code","96ed3d88":"code","1cae9e4a":"code","67eed775":"markdown","c91513c5":"markdown","b594656f":"markdown","1959e3f6":"markdown","3745d33b":"markdown","b226ab8a":"markdown","5205afdc":"markdown","090b53de":"markdown","cf790662":"markdown","13c0f8d1":"markdown","97af7a40":"markdown","d9af6468":"markdown","8170e400":"markdown","e18b4e77":"markdown","44ba0b1a":"markdown","7c18c911":"markdown","59fd9c99":"markdown","010998df":"markdown","e8f3005e":"markdown","79706ec0":"markdown","523ef6f0":"markdown","0f86abea":"markdown","125f2230":"markdown","5312a640":"markdown","5d6c7a14":"markdown","a5751fba":"markdown"},"source":{"08330a9f":"pip install spacy tweet-preprocessor azure-ai-textanalytics --pre","db5b56bd":"#DS\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\nimport json\nimport re\n\n#spacy\nimport spacy\n\n#gensim\nfrom gensim.utils import simple_preprocess\n\n#nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\n\n#String manipulation\nimport string\n\n#Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Microsoft Azure\nfrom azure.ai.textanalytics import TextAnalyticsClient\nfrom azure.core.credentials import AzureKeyCredential\n\n#API Access\nfrom azure_authentificate import AzureAuth\n\n#Twit preprocessor Last update may 2020\nimport preprocessor as pt\n\n#Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\n#pickle \nimport pickle\n\n#tf\nimport tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint\n\nnltk.download('stopwords', quiet=True);","45e22222":"tensorflow.version.VERSION","d2fecf3e":"PATH = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'\ndf = pd.read_csv(PATH, names=['Target', 'Msg_ID', 'Date', 'flag', 'User_Name', 'text'], encoding='latin-1')","a342dae4":"#Convert list of list to list.\ndef list_list_to_list(x):\n    return [item for sublist in x for item in sublist]","883c40ff":"class Preprocess:\n    \n    #Global static var\n    contractions = { \n        \"ain't\": \"am not\",\n        \"aren't\": \"are not\",\n        \"can't\": \"cannot\",\n        \"can't've\": \"cannot have\",\n        \"'cause\": \"because\",\n        \"could've\": \"could have\",\n        \"couldn't\": \"could not\",\n        \"couldn't've\": \"could not have\",\n        \"didn't\": \"did not\",\n        \"doesn't\": \"does not\",\n        \"don't\": \"do not\",\n        \"hadn't\": \"had not\",\n        \"hadn't've\": \"had not have\",\n        \"hasn't\": \"has not\",\n        \"haven't\": \"have not\",\n        \"he'd\": \"he would\",\n        \"he'd've\": \"he would have\",\n        \"he'll\": \"he will\",\n        \"he'll've\": \"he will have\",\n        \"he's\": \"he is\",\n        \"how'd\": \"how did\",\n        \"how'd'y\": \"how do you\",\n        \"how'll\": \"how will\",\n        \"how's\": \"how does\",\n        \"i'd\": \"i would\",\n        \"i'd've\": \"i would have\",\n        \"i'll\": \"i will\",\n        \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\",\n        \"i've\": \"i have\",\n        \"isn't\": \"is not\",\n        \"it'd\": \"it would\",\n        \"it'd've\": \"it would have\",\n        \"it'll\": \"it will\",\n        \"it'll've\": \"it will have\",\n        \"it's\": \"it is\",\n        \"let's\": \"let us\",\n        \"ma'am\": \"madam\",\n        \"mayn't\": \"may not\",\n        \"might've\": \"might have\",\n        \"mightn't\": \"might not\",\n        \"mightn't've\": \"might not have\",\n        \"must've\": \"must have\",\n        \"mustn't\": \"must not\",\n        \"mustn't've\": \"must not have\",\n        \"needn't\": \"need not\",\n        \"needn't've\": \"need not have\",\n        \"o'clock\": \"of the clock\",\n        \"oughtn't\": \"ought not\",\n        \"oughtn't've\": \"ought not have\",\n        \"shan't\": \"shall not\",\n        \"sha'n't\": \"shall not\",\n        \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\",\n        \"she'd've\": \"she would have\",\n        \"she'll\": \"she will\",\n        \"she'll've\": \"she will have\",\n        \"she's\": \"she is\",\n        \"should've\": \"should have\",\n        \"shouldn't\": \"should not\",\n        \"shouldn't've\": \"should not have\",\n        \"so've\": \"so have\",\n        \"so's\": \"so is\",\n        \"that'd\": \"that would\",\n        \"that'd've\": \"that would have\",\n        \"that's\": \"that is\",\n        \"there'd\": \"there would\",\n        \"there'd've\": \"there would have\",\n        \"there's\": \"there is\",\n        \"they'd\": \"they would\",\n        \"they'd've\": \"they would have\",\n        \"they'll\": \"they will\",\n        \"they'll've\": \"they will have\",\n        \"they're\": \"they are\",\n        \"they've\": \"they have\",\n        \"to've\": \"to have\",\n        \"wasn't\": \"was not\",\n        \" u \": \" you \",\n        \" ur \": \" your \",\n        \" n \": \" and \"\n    }\n\n    def __init__(self, df, sample_size):\n        df = self.RemoveInvalidTwit(df)\n        self.df = self.Sample(df, sample_size)\n        self.UpdateTargetValue()\n        self.DropUnusedColumns()\n        self.BasicPreprocess()\n        self.df['preprocess_text'].apply(lambda x: self.Contraction(x))\n        self.RemoveSW()\n        self.Lemma_and_postags()\n\n    def Sample(self, df, size):\n        #Create empty DataFrame\n        sample = []\n    \n        #Get all Target from our dataset\n        labels = df['Target'].value_counts().index.to_list()\n\n        for value in labels:\n            sample.append(df[df['Target'] == value].sample(n=size, random_state=42))\n            \n        return pd.concat(sample)\n            \n    def RemoveInvalidTwit(self, df):\n        TWIT_SIZE = 160 #In 2021 it's 280 characters\n        \n        return df[(df['text'].map(len) > 0) & (df['text'].map(len) <= 160)]\n    \n    def UpdateTargetValue(self):\n        \"\"\" Convert value 4 to 1 because we only have\n         2 values : 0 - Negative, 1 - Positive Twit\"\"\"\n        self.df['Target'] = self.df['Target'].replace(4, 1)\n    \n    def DropUnusedColumns(self):\n        \"\"\" Remove useless columns from our dataset \"\"\"\n        self.df = self.df.drop(['Date', 'flag', 'User_Name'], axis=1)\n        \n    def BasicPreprocess(self):\n        \"\"\" This function is the preprocessing function for our Twit.\n        \n        It remove all empty twit,\n        convert all our text in lowercase, remove digits, remove emojis, links, #,\n        remove punctuation, and finally remove newlines\n        \"\"\"\n        \n        #remove empty reviews\n        not_empty = [x for x in self.df[\"text\"].tolist() if len(x) != 0]\n        \n        #Lowercase\n        lower = [x.lower() for x in not_empty]\n        \n        #Suppress Digits\n        remove_digit = str.maketrans('', '', string.digits)\n        no_digit = [x.translate(remove_digit) for x in lower]\n        \n        preprotwit = [pt.clean(x) for x in no_digit]\n        \n        #Suppress punctuation\n        translator = str.maketrans('', '', string.punctuation)\n        punctuation = [x.translate(translator) for x in preprotwit]\n        \n        #Suppress newlines     \n        self.df['preprocess_text'] = [x.replace('\\n', '') for x in punctuation]\n        \n    def RemoveSW(self):\n        \"\"\" Remove StopWords \"\"\"\n        stop_words = stopwords.words(\"english\")\n        \n        #Extended list of stopwords\n        stop_words.extend(['im'])\n        \n        #Create Bow without stopwords\n        self.df['text_tokenize'] = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in self.df.preprocess_text]\n\n    def Contraction(self, x):\n        for key in self.contractions:\n            value = self.contractions[key]\n            x = x.replace(key, value)\n            \n        return (x)\n            \n    def Lemma_and_postags(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n        #Initialisation\n        lemma = WordNetLemmatizer()\n        \n        texts_out = []\n        nlp = spacy.load('en', disable=['parser', 'ner'])\n        for sent in self.df['text_tokenize']:\n            doc = nlp(\" \".join(sent)) \n            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n  \n        self.df['text_tokenize'] = texts_out\n        self.df['text_rdy_for_model'] = [' '.join(x) for x in texts_out]\n\n    def plot_freqDist(self):\n        ## Creating FreqDist,keeping the 30 most common tokens\n        fdist_most_commun = FreqDist(list_list_to_list(self.df['text_tokenize'])).most_common(30)\n        fdist_less_commun = FreqDist(list_list_to_list(self.df['text_tokenize'])).most_common()[-30:]\n        \n        ## Setting figure, ax into variables\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n\n         ## Conversion to Pandas series via Python Dictionary for easier plotting\n        df_fdist_most_commun = pd.Series(dict(fdist_most_commun))\n        df_fdist_less_commun = pd.Series(dict(fdist_less_commun))\n        \n        ## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n        ax1.set_title('Most common words')\n        sns.barplot(x=df_fdist_most_commun.values, y=df_fdist_most_commun.index, ax=ax1)\n        \n        ax2.set_title('less common words')\n        sns.barplot(x=df_fdist_less_commun.values, y=df_fdist_less_commun.index, ax=ax2)\n        \n        plt.show()","214d43f7":"def authenticate_client(endpoint, key):\n    \"\"\" Prepare our client for azure cognitive API \"\"\"\n    ta_credential = AzureKeyCredential(key)\n    text_analytics_client = TextAnalyticsClient(\n            endpoint=endpoint, \n            credential=ta_credential)\n    \n    return text_analytics_client","35858c64":"def model_perfs_plot(history, epochs):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(epochs)\n\n    plt.figure(figsize=(12, 12))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()","0a7d36fc":"nlp_azure_cognitive = Preprocess(df, 500)","df96ff13":"nlp_azure_cognitive.df.head(10)","03cf3fb1":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\nsns.set_style('darkgrid')\n        \nlist_nb_characters = [len(x) for x in df.text]\nlist_nb_of_words = [len(x.split()) for x in df.text]\n\nax1.set_title('number of characters per Twit')\nsns.histplot(list_nb_characters, ax = ax1)\nax1.set_xlim([0, 175])\n\nax2.set_title('number of words per Twit')\nsns.histplot(list_nb_of_words, ax = ax2)\nax2.set_xlim([0, 40])\n\nplt.show()","f1daeee3":"nlp_azure_cognitive.plot_freqDist()","cd3d7168":"nlp_azure_cognitive.df[['Target','text']].to_csv('1000_Tweets_Sample.csv', index=False)","dde534de":"client = authenticate_client(AzureAuth.config('endpoint'), AzureAuth.config('key_1'))\nnlp_azure_cognitive.df.shape","63de390f":"sentence = 'Hello, my name is Charles and i hope you like my notebook !'\n\nres = client.analyze_sentiment(documents=[sentence])","a8b0f001":"print('Scores : {}'.format(res[0].confidence_scores))","b7b6ee51":"print('Sentiment associated : {}'.format(res[0].sentiment))","d4e05295":"def get_sentiment(client, sentence):\n    res = client.analyze_sentiment(documents=[sentence])\n    return 0 if res[0].confidence_scores.negative > res[0].confidence_scores.positive else 1\n\nnlp_azure_cognitive.df['azure_prediction'] = nlp_azure_cognitive.df.apply(lambda x: get_sentiment(client, x.text), axis=1)","2632ab33":"nlp_azure_cognitive.df.head(5)","5aee2ca8":"Total = nlp_azure_cognitive.df.shape[0]\nTotal_Error = nlp_azure_cognitive.df[nlp_azure_cognitive.df['azure_prediction'] != nlp_azure_cognitive.df['Target']].shape[0]\n\nPercent =   100 - Total_Error * 100 \/ Total\n\nprint('Accuracy : {} %'.format(Percent))","2e989ff4":"nlp_cnn = Preprocess(df, 42500)","ad6eb3d0":"nlp_cnn.df['Target'].value_counts()","920c7fcb":"text = nlp_cnn.df['preprocess_text'].to_list()\ny = nlp_cnn.df['Target'].to_list()","56b53dbb":"#tokenizer to read all the words present in our corpus\ntoken = Tokenizer()\ntoken.fit_on_texts(text)\n\nvocab_size  = len(token.word_index) + 1\nprint('vocabulary_size : ', vocab_size)\n\nencoded_text = token.texts_to_sequences(text)\nprint(encoded_text[0])","ae8e1071":"#Saving to json\nwith open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(token.to_json(), ensure_ascii=False))\n    f.close()\n    \n# saving to .pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)","3931a9ae":"max_length = 120\nX = pad_sequences(encoded_text, maxlen=max_length, padding='post')\nX.shape","5b531727":"#splitting the dataset into train and test dataset\nX = np.array(X)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3, stratify = y)","e387ba6f":"%%time\nglove_vectors = dict()\n\nfile = open('..\/input\/twitter-glove-200\/glove.twitter.27B.200d.txt', encoding='utf-8')\n\nfor line in file:\n    values = line.split()\n    word = values[0]\n    vectors = np.asarray(values[1: ])\n    glove_vectors[word] = vectors\n    \nfile.close()\n\nprint('Glove Vector lenght : ', len(glove_vectors))\n\nword_vector_matrix = np.zeros((vocab_size, 200))\n\nfor word, index in token.word_index.items():\n    vector = glove_vectors.get(word)\n    if vector is not None:\n        word_vector_matrix[index] = vector","0bdd8c47":"#CONFIG\nVEC_SIZE = 200\nEPOCHS = 15\n\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, VEC_SIZE, input_length=max_length, weights = [word_vector_matrix], trainable = False))\n\nmodel.add(Conv1D(128, 8, activation = 'relu'))\n\n#here 64 is number of filters and 8 is size of filters\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(learning_rate = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.summary()","e1861961":"%%time\ncallbacks = [\n    ModelCheckpoint(\"cnn_200_save_at_{epoch}.h5\", save_best_only=True, verbose=1),\n]\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = EPOCHS,\n                    callbacks = callbacks,\n                    validation_data = (X_test, y_test))","38f4f242":"model_perfs_plot(history, EPOCHS)","d114363a":"#CONFIG\nEPOCHS = 3\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, VEC_SIZE, input_length=X.shape[1], weights = [word_vector_matrix]))\nmodel.add(LSTM(64, input_shape=X.shape, recurrent_dropout=0.8, return_sequences=True))\n\nmodel.add(Conv1D(64, 8, activation = 'relu'))\n\n#here 64 is number of filters and 8 is size of filters\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.6))\n\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","4936d734":"%%time\n\ncallbacks = [\n    ModelCheckpoint(\"rnn_200_save_at_{epoch}.h5\", save_best_only=True, verbose=1),\n]\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = EPOCHS,\n                    callbacks = callbacks,\n                    validation_data = (X_test, y_test))","36f808bd":"model_perfs_plot(history, EPOCHS)","b94285bf":"%%time\nglove_vectors = dict()\n\nfile = open('..\/input\/twitter-glove-200\/glove.twitter.27B.50d.txt', encoding='utf-8')\n\nfor line in file:\n    values = line.split()\n    word = values[0]\n    vectors = np.asarray(values[1: ])\n    glove_vectors[word] = vectors\n    \nfile.close()\n\nprint('Glove Vector lenght : ', len(glove_vectors))\n\nword_vector_matrix = np.zeros((vocab_size, 50))\n\nfor word, index in token.word_index.items():\n    vector = glove_vectors.get(word)\n    if vector is not None:\n        word_vector_matrix[index] = vector","f8872eec":"#CONFIG\nVEC_SIZE = 50\nEPOCHS = 15\n\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, VEC_SIZE, input_length=max_length, weights = [word_vector_matrix], trainable = False))\n\nmodel.add(Conv1D(128, 8, activation = 'relu'))\n\n#here 64 is number of filters and 8 is size of filters\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(learning_rate = 0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.summary()","fc9edc89":"%%time\ncallbacks = [\n    ModelCheckpoint(\"cnn_50_save_at_{epoch}.h5\", save_best_only=True, verbose=1),\n]\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = EPOCHS,\n                    callbacks = callbacks,\n                    validation_data = (X_test, y_test))","af9404d0":"model_perfs_plot(history, EPOCHS)","96ed3d88":"#CONFIG\nEPOCHS = 3\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, VEC_SIZE, input_length=X.shape[1], weights = [word_vector_matrix]))\nmodel.add(LSTM(64, input_shape=X.shape, recurrent_dropout=0.8, return_sequences=True))\n\nmodel.add(Conv1D(64, 8, activation = 'relu'))\n\n#here 64 is number of filters and 8 is size of filters\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.6))\n\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","1cae9e4a":"%%time\n\ncallbacks = [\n    ModelCheckpoint(\"rnn_50_save_at_{epoch}.h5\", save_best_only=True, verbose=1),\n]\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs = EPOCHS,\n                    callbacks = callbacks,\n                    validation_data = (X_test, y_test))","67eed775":"## **Results** : ","c91513c5":"### 4.2.2 **RNN**","b594656f":"#### Export my sample to compare azure cognitive API with Azure ML Classic","1959e3f6":"### 4.2.1 **CNN**","3745d33b":"## 1.1 **Preprocessing**","b226ab8a":"## 4.3 **GloVe 50-Dimensional Word Vectors**","5205afdc":"**~70 %** it's not that bad ! let's see if we can do better with our own model.","090b53de":"# 1. **Functions**\n\n#### In this part you will find most of the functions used in this notebook","cf790662":"## 4.1 **Data Preparation**","13c0f8d1":"Let's use it on our sample to evaluate its performance on tweets!","97af7a40":"## 1.3 **Plots**","d9af6468":"## **How does it work?**\n\nWith the Azure Cognitive API you just have to send a list with all the sentences you want to analyze and it will return the sentiment associated to each of your sentences as a list as well as their score and Voil\u00e0 !","8170e400":"### 4.3.1 **CNN**","e18b4e77":"# 3. **Microsoft Azure cognitive API**\nIn this part we will use the Azure cognitive API and evaluate its performance on a small sample (~1000)","44ba0b1a":"# 5. **Microsoft Machine Learning Studio**\nI used the sample generate in the part 2.","7c18c911":"**When the dataset has been processed our class contains a dataset with the following columns**:\n\n* **Target** : Our Target, the sentiment associate with the message.\n* **Msg_ID** : message(text) ID \n* **text** : text\n* **preprocess_text** : text preprocess\n* **text_tokenize** : text tokenize\n* **text_rdy_for_model** : our text preprocess, lemmatized, etc.. it is ready to be given to our model","59fd9c99":"# 4. **Deep Learning**","010998df":"# 2. **Exploratory Analysis**","e8f3005e":"## 5.1 **Model**\n\n<img src=\"https:\/\/i.ibb.co\/7zVz2rP\/p7-Azure-ML-classic.png\">\n\n## 5.2 **Train Results**\n\n<img src=\"https:\/\/i.ibb.co\/4SK7jvS\/p7-Azure-ML-classic-CLEARR.png\">\n\n## 5.3 **Test Results**\n\n<img src=\"https:\/\/i.ibb.co\/1TChfqv\/p7-Azure-ML-classic-Scored-dataset-to-compare-clean.png\">","79706ec0":"## 4.2 **GloVe 200-Dimensional Word Vectors**","523ef6f0":"## 4.3.2 **RNN**","0f86abea":"****************************************","125f2230":"**The target value can only be 0 or 1 :**\n* 0 -> Negative sentiment \n* 1 -> Positive sentiment\n\n**What does the Class Preprocess do ?**\n1. Create a balanced sample\n2. Update Target Value\n3. Clear our DataFrame\n4. Preprocess the text (adapt for twitter : remove #, URL, lowercase, punctuation....)\n5. Lemmatization and keep allowed postags only.","5312a640":"# Install Requirements ","5d6c7a14":"## 1.2 **Microsoft Azure**","a5751fba":"**save my Tokenizer to .json and .pickle format**"}}