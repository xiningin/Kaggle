{"cell_type":{"531b2fec":"code","df17c7e7":"code","b3a72b92":"code","1939959c":"code","27c8818d":"code","4e101b18":"code","f8f0ba86":"code","93780ef9":"code","50eaacb9":"code","cdff8bb5":"code","db0b07ee":"code","db3cec88":"code","2cea83ac":"code","ad4577e0":"code","666a1a97":"code","31adb312":"code","4dcce4e8":"code","6deef9d4":"code","302c9de3":"code","6b40fef1":"code","36d38bc2":"code","b602bbd7":"code","8ab818fa":"code","c17bf2b6":"code","1340fc11":"code","6105bef6":"code","d5cb2ce2":"code","f0a45ef7":"code","beeaf30a":"markdown","46b90880":"markdown","8cdd2307":"markdown","5e2f05cc":"markdown","a03a76d2":"markdown","8ca5c4cc":"markdown","d567e214":"markdown","16e768bb":"markdown","95c38b3e":"markdown","7b2c524b":"markdown","add315e8":"markdown","11accca4":"markdown","73c47732":"markdown","fa977505":"markdown","c3208859":"markdown","7890aa92":"markdown","da269a63":"markdown","4808d07f":"markdown","5b8b5860":"markdown","2e94526d":"markdown","0be53821":"markdown","cb5c1ad6":"markdown","ac6440f8":"markdown","7fefa4be":"markdown","4d256387":"markdown","2a77d5a6":"markdown","e6f57170":"markdown","eddd1053":"markdown","b4f3b3b2":"markdown","00924b2c":"markdown","5daba7cc":"markdown","c2165338":"markdown","fa8cf9d4":"markdown","561e6690":"markdown","e7cde4a2":"markdown"},"source":{"531b2fec":"!pip install seaborn --upgrade","df17c7e7":"# load the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.gridspec as gridspec\n","b3a72b92":"# Read the dataset\nkaggle_2020 = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\", skiprows = 1)\n","1939959c":"# Rename columns\n\ncol_dict = {0: 'Duration',\n 1: 'Age Group',\n 2: 'Gender',\n 3: 'Country',\n 4: 'Education Level',\n 5: 'Job Title',\n 6: 'Programming Experience',\n 7: 'Python',\n 8: 'R',\n 9: 'SQL',\n 10: 'C',\n 11: 'C++',\n 12: 'Java',\n 13: 'Javascript',\n 14: 'Julia',\n 15: 'Swift',\n 16: 'Bash',\n 17: 'MATLAB',\n 18: 'Progamming None',\n 19: 'Programming Other',\n 20: 'Recommended Language',\n 21: 'Jupyter (JupyterLab, Jupyter Notebooks, etc)',\n 22: 'RStudio',\n 23: 'Visual Studio \/ Visual Studio Code',\n 24: 'Click to write Choice 13',\n 25: 'PyCharm',\n 26: 'Spyder',\n 27: 'Notepad++',\n 28: 'Sublime Text',\n 29: 'Vim \/ Emacs',\n 30: 'MATLAB IDE',\n 31: 'IDE None',\n 32: 'IDE Other',\n 33: 'Kaggle Notebooks',\n 34: 'Colab Notebooks',\n 35: 'Azure Notebooks',\n 36: 'Paperspace \/ Gradient',\n 37: 'Binder \/ JupyterHub',\n 38: 'Code Ocean',\n 39: 'IBM Watson Studio',\n 40: 'Amazon Sagemaker Studio',\n 41: 'Amazon EMR Notebooks',\n 42: 'Google Cloud AI Platform Notebooks',\n 43: 'Google Cloud Datalab Notebooks',\n 44: 'Databricks Collaborative Notebooks',\n 45: 'Hosted Notebooks None',\n 46: 'Hosted Notebook Other',\n 47: 'Most often used Cloud Platform',\n 48: 'GPUs',\n 49: 'TPUs',\n 50: 'Hardware None',\n 51: 'Hardware Other',\n 52: 'TPU usage experience',\n 53: 'Matplotlib',\n 54: 'Seaborn',\n 55: 'Plotly \/ Plotly Express',\n 56: 'Ggplot \/ ggplot2',\n 57: 'Shiny',\n 58: 'D3 js',\n 59: 'Altair',\n 60: 'Bokeh',\n 61: 'Geoplotlib',\n 62: 'Leaflet \/ Folium',\n 63: 'Visualization None',\n 64: 'Visualization Other',\n 65: 'ML Experience',\n 66: 'Scikit-learn',\n 67: 'TensorFlow',\n 68: 'Keras',\n 69: 'PyTorch',\n 70: 'Fast.ai',\n 71: 'MXNet',\n 72: 'Xgboost',\n 73: 'LightGBM',\n 74: 'CatBoost',\n 75: 'Prophet',\n 76: 'H2O 3',\n 77: 'Caret',\n 78: 'Tidymodels',\n 79: 'JAX',\n 80: 'ML None',\n 81: 'ML Other',\n 82: 'Linear or Logistic Regression',\n 83: 'Decision Trees or Random Forests',\n 84: 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n 85: 'Bayesian Approaches',\n 86: 'Evolutionary Approaches',\n 87: 'Dense Neural Networks (MLPs, etc)',\n 88: 'Convolutional Neural Networks',\n 89: 'Generative Adversarial Networks',\n 90: 'Recurrent Neural Networks',\n 91: 'Transformer Networks (BERT, gpt-3, etc)',\n 92: 'ML Algorithms None',\n 93: 'ML Algo Other',\n 94: 'General purpose image\/video tools (PIL, cv2, skimage, etc)',\n 95: 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n 96: 'Object detection methods (YOLOv3, RetinaNet, etc)',\n 97: 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n 98: 'Generative Networks (GAN, VAE, etc)',\n 99: 'CV None',\n 100: 'CV Other',\n 101: 'Word embeddings\/vectors (GLoVe, fastText, word2vec)',\n 102: 'Encoder-decorder models (seq2seq, vanilla transformers)',\n 103: 'Contextualized embeddings (ELMo, CoVe)',\n 104: 'Transformer language models (GPT-3, BERT, XLnet, etc)',\n 105: 'NLP None',\n 106: 'NLP Other',\n 107: 'Company Size',\n 108: 'Data Science Team Size',\n 109: 'ML Application in Company',\n 110: 'Analyze and understand data to influence product or business decisions',\n 111: 'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n 112: 'Build prototypes to explore applying machine learning to new areas',\n 113: 'Build and\/or run a machine learning service that operationally improves my product or workflows',\n 114: 'Experimentation and iteration to improve existing ML models',\n 115: 'Do research that advances the state of the art of machine learning',\n 116: 'None of these activities are an important part of my role at work',\n 117: 'Work Activity Other',\n 118: 'Annual Compensation',\n 119: 'ML Spend',\n 120: 'Amazon Web Services (AWS)',\n 121: 'Microsoft Azure',\n 122: 'Google Cloud Platform (GCP)',\n 123: 'IBM Cloud \/ Red Hat',\n 124: 'Oracle Cloud',\n 125: 'SAP Cloud',\n 126: 'Salesforce Cloud',\n 127: 'VMware Cloud',\n 128: 'Alibaba Cloud',\n 129: 'Tencent Cloud',\n 130: 'Cloud None',\n 131: 'Cloud Other',\n 132: 'Amazon EC2',\n 133: 'AWS Lambda',\n 134: 'Amazon Elastic Container Service',\n 135: 'Azure Cloud Services',\n 136: 'Microsoft Azure Container Instances',\n 137: 'Azure Functions',\n 138: 'Google Cloud Compute Engine',\n 139: 'Google Cloud Functions',\n 140: 'Google Cloud Run',\n 141: 'Google Cloud App Engine',\n 142: 'Cloud Products None',\n 143: 'Cloud Product Other',\n 144: 'Amazon SageMaker',\n 145: 'Amazon Forecast',\n 146: 'Amazon Rekognition',\n 147: 'Azure Machine Learning Studio',\n 148: 'Azure Cognitive Services',\n 149: 'Google Cloud AI Platform \/ Google Cloud ML Engine',\n 150: 'Google Cloud Video AI',\n 151: 'Google Cloud Natural Language',\n 152: 'Google Cloud Vision AI',\n 153: 'ML Products None',\n 154: 'ML Prod Other',\n 155: 'MySQL',\n 156: 'PostgresSQL',\n 157: 'SQLite',\n 158: 'Oracle Database',\n 159: 'MongoDB',\n 160: 'Snowflake',\n 161: 'IBM Db2',\n 162: 'Microsoft SQL Server',\n 163: 'Microsoft Access',\n 164: 'Microsoft Azure Data Lake Storage',\n 165: 'Amazon Redshift',\n 166: 'Amazon Athena',\n 167: 'Amazon DynamoDB',\n 168: 'Google Cloud BigQuery',\n 169: 'Google Cloud SQL',\n 170: 'Google Cloud Firestore',\n 171: 'DW None',\n 172: 'DW Other',\n 173: 'Most Used Database',\n 174: 'Amazon QuickSight',\n 175: 'Microsoft Power BI',\n 176: 'Google Data Studio',\n 177: 'Looker',\n 178: 'Tableau',\n 179: 'Salesforce',\n 180: 'Einstein Analytics',\n 181: 'Qlik',\n 182: 'Domo',\n 183: 'TIBCO Spotfire',\n 184: 'Alteryx',\n 185: 'Sisense',\n 186: 'SAP Analytics Cloud',\n 187: 'BI None',\n 188: 'BI Other',\n 189: 'Most Used BI Tool',\n 190: 'Automated data augmentation (e.g. imgaug, albumentations)',\n 191: 'Automated feature engineering\/selection (e.g. tpot, boruta_py)',\n 192: 'Automated model selection (e.g. auto-sklearn, xcessiv)',\n 193: 'Automated model architecture searches (e.g. darts, enas)',\n 194: 'Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)',\n 195: 'Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)',\n 196: 'Auto ML None',\n 197: 'Auto ML Other',\n 198: 'Google Cloud AutoML',\n 199: 'H20 Driverless AI',\n 200: 'Databricks AutoML',\n 201: 'DataRobot AutoML',\n 202: 'Tpot',\n 203: 'Auto-Keras',\n 204: 'Auto-Sklearn',\n 205: 'Auto_ml',\n 206: 'Xcessiv',\n 207: 'MLbox',\n 208: 'Auto ML Tools None',\n 209: 'Auto ML Tools Other',\n 210: 'Neptune.ai',\n 211: 'Weights & Biases',\n 212: 'Comet.ml',\n 213: 'Sacred + Omniboard',\n 214: 'TensorBoard',\n 215: 'Guild.ai',\n 216: 'Polyaxon',\n 217: 'Trains',\n 218: 'Domino Model Monitor',\n 219: 'ML Tools None',\n 220: 'ML Tools Other',\n 221: 'Plotly Dash',\n 222: 'Streamlit',\n 223: 'NBViewer',\n 224: 'GitHub',\n 225: 'Personal blog',\n 226: 'Kaggle',\n 227: 'Colab',\n 228: 'Shiny Deploy',\n 229: 'I do not share my work publicly',\n 230: 'Deploy Other',\n 231: 'Coursera',\n 232: 'edX',\n 233: 'Kaggle Learn Courses',\n 234: 'DataCamp',\n 235: 'Fast.ai Learning',\n 236: 'Udacity',\n 237: 'Udemy',\n 238: 'LinkedIn Learning',\n 239: 'Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',\n 240: 'University Courses (resulting in a university degree)',\n 241: 'Learn ML None',\n 242: 'Learn ML Other',\n 243: 'Primary Analysis Tool',\n 244: 'Twitter (data science influencers)',\n 245: \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\",\n 246: 'Reddit (r\/machinelearning, etc)',\n 247: 'Kaggle (notebooks, forums, etc)',\n 248: 'Course Forums (forums.fast.ai, Coursera forums, etc)',\n 249: 'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)',\n 250: 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)',\n 251: 'Blogs (Towards Data Science, Analytics Vidhya, etc)',\n 252: 'Journal Publications (peer-reviewed journals, conference proceedings, etc)',\n 253: 'Slack Communities (ods.ai, kagglenoobs, etc)',\n 254: 'Media None',\n 255: 'Media Other',\n 256: 'Aspire Amazon Web Services (AWS)',\n 257: 'Aspire Microsoft Azure',\n 258: 'Aspire Google Cloud Platform (GCP)',\n 259: 'Aspire IBM Cloud \/ Red Hat',\n 260: 'Aspire Oracle Cloud',\n 261: 'Aspire SAP Cloud',\n 262: 'Aspire VMware Cloud',\n 263: 'Aspire Salesforce Cloud',\n 264: 'Aspire Alibaba Cloud',\n 265: 'Aspire Tencent Cloud',\n 266: 'Aspire Cloud None',\n 267: 'Aspire Cloud Other',\n 268: 'Aspire Amazon EC2',\n 269: 'Aspire AWS Lambda',\n 270: 'Aspire Amazon Elastic Container Service',\n 271: 'Aspire Azure Cloud Services',\n 272: 'Aspire Microsoft Azure Container Instances',\n 273: 'Aspire Azure Functions',\n 274: 'Aspire Google Cloud Compute Engine',\n 275: 'Aspire Google Cloud Functions',\n 276: 'Aspire Google Cloud Run',\n 277: 'Aspire Google Cloud App Engine',\n 278: 'Aspire Cloud Products None',\n 279: 'Aspire Cloud Products Other',\n 280: 'Aspire Amazon SageMaker',\n 281: 'Aspire Amazon Forecast',\n 282: 'Aspire Amazon Rekognition',\n 283: 'Aspire Azure Machine Learning Studio',\n 284: 'Aspire Azure Cognitive Services',\n 285: 'Aspire Google Cloud AI Platform \/ Google Cloud ML Engine',\n 286: 'Aspire Google Cloud Video AI',\n 287: 'Aspire Google Cloud Natural Language',\n 288: 'Aspire Google Cloud Vision AI',\n 289: 'Aspire ML None',\n 290: 'Aspire ML Other',\n 291: 'Aspire MySQL',\n 292: 'Aspire PostgresSQL',\n 293: 'Aspire SQLite',\n 294: 'Aspire Oracle Database',\n 295: 'Aspire MongoDB',\n 296: 'Aspire Snowflake',\n 297: 'Aspire IBM Db2',\n 298: 'Aspire Microsoft SQL Server',\n 299: 'Aspire Microsoft Access',\n 300: 'Aspire Microsoft Azure Data Lake Storage',\n 301: 'Aspire Amazon Redshift',\n 302: 'Aspire Amazon Athena',\n 303: 'Aspire Amazon DynamoDB',\n 304: 'Aspire Google Cloud BigQuery',\n 305: 'Aspire Google Cloud SQL',\n 306: 'Aspire Google Cloud Firestore',\n 307: 'Aspire DW None',\n 308: 'Aspire DW Other',\n 309: 'Aspire Microsoft Power BI',\n 310: 'Aspire Amazon QuickSight',\n 311: 'Aspire Google Data Studio',\n 312: 'Aspire Looker',\n 313: 'Aspire Tableau',\n 314: 'Aspire Salesforce',\n 315: 'Aspire Einstein Analytics',\n 316: 'Aspire Qlik',\n 317: 'Aspire Domo',\n 318: 'Aspire TIBCO Spotfire',\n 319: 'Aspire Alteryx',\n 320: 'Aspire Sisense',\n 321: 'Aspire SAP Analytics Cloud',\n 322: 'Aspire BI None',\n 323: 'Aspire BI Other',\n 324: 'Aspire Automated data augmentation (e.g. imgaug, albumentations)',\n 325: 'Aspire Automated feature engineering\/selection (e.g. tpot, boruta_py)',\n 326: 'Aspire Automated model selection (e.g. auto-sklearn, xcessiv)',\n 327: 'Aspire Automated model architecture searches (e.g. darts, enas)',\n 328: 'Aspire Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)',\n 329: 'Aspire Automation of full ML pipelines (e.g. Google Cloud AutoML, H20 Driverless AI)',\n 330: 'Aspire AutoML None',\n 331: 'Aspire AutoML Other',\n 332: 'Aspire Google Cloud AutoML',\n 333: 'Aspire H20 Driverless AI',\n 334: 'Aspire Databricks AutoML',\n 335: 'Aspire DataRobot AutoML',\n 336: 'Aspire Tpot',\n 337: 'Aspire Auto-Keras',\n 338: 'Aspire Auto-Sklearn',\n 339: 'Aspire Auto_ml',\n 340: 'Aspire Xcessiv',\n 341: 'Aspire MLbox',\n 342: 'Aspire AutoML B None',\n 343: 'Aspire AutoML B Other',\n 344: 'Aspire Neptune.ai',\n 345: 'Aspire Weights & Biases',\n 346: 'Aspire Comet.ml',\n 347: 'Aspire Sacred + Omniboard',\n 348: 'Aspire TensorBoard',\n 349: 'Aspire Guild.ai',\n 350: 'Aspire Polyaxon',\n 351: 'Aspire Trains',\n 352: 'Aspire Domino Model Monitor',\n 353: 'Aspire ML Tools  None',\n 354: 'Aspire ML Tools Other'}\n\nkaggle_2020.columns = np.arange(len(kaggle_2020.columns))\nkaggle_2020 = kaggle_2020.rename(columns = col_dict)\n\n# Drop mising salaries\nkaggle_2020.dropna(subset=[\"Annual Compensation\"], inplace = True)\n# kaggle_2020[\"Annual Compensation\"].value_counts(dropna = False)\n","27c8818d":"# Bucket Salaries\n\nsalary_dict = {\n    'Missing' : 'Missing' ,\n'$0-999' : 'Less than 10k' ,\n'1,000-1,999' : 'Less than 10k' ,\n'2,000-2,999' : 'Less than 10k' ,\n'3,000-3,999' : 'Less than 10k' ,\n'4,000-4,999' : 'Less than 10k' ,\n'5,000-7,499' : 'Less than 10k' ,\n'7,500-9,999' : 'Less than 10k' ,\n'10,000-14,999' : '10k to 50k' ,\n'15,000-19,999' : '10k to 50k' ,\n'20,000-24,999' : '10k to 50k' ,\n'25,000-29,999' : '10k to 50k' ,\n'30,000-39,999' : '10k to 50k' ,\n'40,000-49,999' : '10k to 50k' ,\n'50,000-59,999' : '50k to 80k' ,\n'60,000-69,999' : '50k to 80k' ,\n'70,000-79,999' : '50k to 80k' ,\n'100,000-124,999' : '80k to 150k' ,\n'125,000-149,999' : '80k to 150k' ,\n'80,000-89,999' : '80k to 150k' ,\n'90,000-99,999' : '80k to 150k' ,\n'> $500,000' : '150k and above' ,\n'150,000-199,999' : '150k and above' ,\n'200,000-249,999' : '150k and above' ,\n'250,000-299,999' : '150k and above' ,\n'300,000-500,000' : '150k and above'\n}\n\nkaggle_2020[\"Earnings\"] = kaggle_2020[\"Annual Compensation\"].replace(salary_dict)\n\n# Create order list\nchart_order = CategoricalDtype(\n    [\"Less than 10k\", \"10k to 50k\", \"50k to 80k\", \"80k to 150k\", \"150k and above\"], \n    ordered=True\n)\n\nchart_order_list  = [\"Less than 10k\", \"10k to 50k\", \"50k to 80k\", \"80k to 150k\", \"150k and above\"]\n\nagg_earnings = kaggle_2020[\"Earnings\"].value_counts(sort = False).to_frame().rename(columns = {\"Earnings\" : \"count\"} )\n\n","4e101b18":"# Create Data Wrangling and Dashboard functions\n\n\n# For Y\/N fields\ndef cross_tab_row(input_df, varname, index_var, out_text):\n    \n    out_df = pd.crosstab(columns = input_df[varname], index = input_df[index_var],\n                         values = input_df[index_var], aggfunc = \"count\").fillna(0)\n    \n    out_df.columns = np.arange(len(out_df.columns))\n    out_df.rename(columns = {0 : \"count\"}, inplace = True)\n\n    overall_val = input_df[varname].dropna().value_counts()[0] \/ input_df.shape[0]\n    out_df = out_df.div(agg_earnings).reset_index().rename(columns = {\"index\" : index_var}).reset_index(drop = True)\n    out_df[varname] = out_df[\"count\"] \/ overall_val * 100\n    out_df[varname].fillna(0, inplace = True)\n    out_df.drop(columns = {\"count\"}, inplace = True)\n    index_range = max(out_df[varname]) - min(out_df[varname])\n    min_val = min(out_df[varname])\n    max_val = max(out_df[varname])\n    out_df[index_var] = out_df[index_var].astype(chart_order)\n    out_df.sort_values(index_var, inplace = True)\n    \n    return out_df, overall_val, index_range, min_val, max_val, out_text\n\n\n# For Multiple Choice Fields\ndef cross_tab_mat(input_df, varname, index_var):\n    out_list = []\n    agg_earnings_mat = pd.crosstab(index = input_df[index_var], columns = input_df[varname], values = input_df[varname], aggfunc = \"count\", margins = True, margins_name = \"Overall\", normalize = \"columns\").fillna(0)\n    agg_earnings_mat = agg_earnings_mat.loc[agg_earnings_mat[\"Overall\"] >= 0.05]\n    overall_vals = agg_earnings_mat[[\"Overall\"]]\n    agg_earnings_mat = agg_earnings_mat.T.drop(\"Overall\")\n    cols_list = agg_earnings_mat.columns.to_list()\n    for col in cols_list:\n        base_val = overall_vals.loc[col, \"Overall\"]\n        out_df = agg_earnings_mat[[col]].reset_index()\n        out_df[col] = out_df[col]\/ base_val * 100\n        index_range = max(out_df[col]) - min(out_df[col])\n        min_val = min(out_df[col])\n        max_val = max(out_df[col])\n        out_df[\"Earnings\"] = out_df[\"Earnings\"].astype(chart_order)\n        out_df.sort_values(\"Earnings\", inplace = True)\n        row_list = [out_df, base_val, index_range, min_val, max_val, col]\n        out_list.append(row_list)\n    \n    return out_list\n\n\n# setup fonts\nheadfont = {'fontname':'Lato'}\nsubphfont = {'fontname':'Liberation Serif'}\naxisfont = {'fontname':'Liberation Sans Narrow'}\n\n# Create Chart Functions\ndef create_plot(input_list, subptitle_inp):\n    \n    N = len(input_list)\n    plt.close()\n    #set style\n    sns.set_style(\"white\")\n    fig = plt.figure(constrained_layout=True, figsize = (12 ,(2.5 * N)))\n    fig.set_dpi(100)\n    fig.set_constrained_layout_pads(hspace=0.2, wspace=0.1)\n    gs = fig.add_gridspec(nrows = N, ncols = 3)\n    \n    fig.suptitle(subptitle_inp, fontsize=20, x = 0.0, ha = \"left\", y = 1.04, va = \"top\", **headfont, fontweight = \"bold\")\n    fig.text(x = 0.85, y = 1.03, s = \"Incidence\", fontsize=20, fontweight = \"medium\", **headfont)\n\n    # create the axes\n    axs = []\n    paxs = []\n    \n    for i in range(N):\n        # Set plot title\n        plot_title = input_list[i][-1]\n        # Get plot data\n        input_df = input_list[i][0]\n        # Set limits\n        min_idx = int(input_list[i][3] \/ 10) * 10\n        max_idx = int(input_list[i][4] \/ 10) * 10 + 30\n        ylim_min = min(60, min_idx)\n        ylim_max = max(140, max_idx)\n        # Set colors\n        idx_range = input_list[i][2]\n        max_earn_idx = float(input_list[i][0].loc[input_list[i][0][\"Earnings\"] == \"150k and above\"].iloc[:,1])\n        min_earn_idx = float(input_list[i][0].loc[input_list[i][0][\"Earnings\"] == \"Less than 10k\"].iloc[:,1])\n        if idx_range < 30:\n            lcolor = \"royalblue\"\n            fill_color = \"aqua\"\n        elif ((max_earn_idx < 100 and min_earn_idx < 100) or (max_earn_idx > 100 and min_earn_idx > 100)):\n            lcolor = \"royalblue\"\n            fill_color = \"aqua\"\n        elif max_earn_idx < 100:\n            lcolor = \"red\"\n            fill_color = \"tomato\"\n        else:\n            lcolor = \"forestgreen\"\n            fill_color = \"lime\"\n    \n        \n        axs.append(fig.add_subplot(gs[i, :-1]))\n        axs[i].set_title(plot_title, fontsize=16, x = 0.05, y = 0.9, ha = \"left\", **subphfont, fontweight = \"medium\")\n        x = np.arange(5)\n        y = input_df.iloc[:,-1]\n        plt.xticks(np.arange(5), chart_order_list, ha = \"center\", fontsize = 12, **axisfont)\n        plt.grid(color = \"gray\", linewidth = 1.5, linestyle = \"-\", axis = \"y\", alpha = 0.1)\n        plt.yticks(fontsize = 12, **axisfont)\n        axs[i].margins(x = 0.05, y = 0)\n        axs[i].plot(x, y, color=lcolor, lw=2)\n        plt.ylim(bottom = ylim_min, top = ylim_max)\n        axs[i].fill_between(x,100, y, color = fill_color, alpha = 0.6)\n        \n#         if i != N-1:\n#             plt.setp(axs[i].get_xticklabels(), visible=False)\n\n        # Add pie charts\n        paxs.append(fig.add_subplot(gs[i,-1]))\n        col_pal = plt.cm.cool\n        paxs[i].axis('equal')\n        pie_val = input_list[i][1]\n        group_size = [pie_val,1-pie_val] \n        mypie, _ = paxs[i].pie(group_size, radius=1.2, colors=[col_pal(pie_val), \"white\"], startangle = 90)\n        plt.setp( mypie, width=0.8, edgecolor='white')\n        paxs[i].text(x = 0., y = 0.8, s = \"{:.0%}\".format(pie_val), fontsize=18, **axisfont)\n\n    sns.despine()\n    plt.show();\n","f8f0ba86":"# Company Size\nkaggle_2020[\"Company Size\"].value_counts(dropna = False)\n\ncompany_size_order = CategoricalDtype(\n    [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"], \n    ordered=True\n)\n\ncompany_size_list = [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"] \n\nkaggle_2020[\"Company Size\"] = kaggle_2020[\"Company Size\"].astype(company_size_order)\nkaggle_2020.sort_values(\"Company Size\", inplace = True)\ncsize_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Company Size\"]], \"Earnings\", \"Company Size\")\ncreate_plot(csize_list, \"Company Size\")","93780ef9":"# Data Science Team Size\n\nkaggle_2020[\"Data Science Team Size\"] = kaggle_2020[\"Data Science Team Size\"].apply(lambda x: \"10-19\" if x in [\"10-14\", \"15-19\"] else x)\n\nds_team_size_order = CategoricalDtype(\n    [\"0\", \"1-2\", \"3-4\", \"5-9\", \"10-19\", \"20+\"], \n    ordered=True\n)\n\nds_team_list = [\"0\", \"1-2\", \"3-4\", \"5-9\", \"10-19\", \"20+\"]\n\nkaggle_2020[\"Data Science Team Size\"] = kaggle_2020[\"Data Science Team Size\"].astype(ds_team_size_order)\nkaggle_2020.sort_values(\"Data Science Team Size\", inplace = True)\nds_team_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Data Science Team Size\"]], \"Earnings\", \"Data Science Team Size\")\ncreate_plot(ds_team_list, \"Data Science Team Size\")","50eaacb9":"# ML Usage in Company\n\nml_usage_order = CategoricalDtype([\"No (we do not use ML methods)\", \n                                  \"We use ML methods for generating insights (but do not put working models into production)\",\n                                  \"We are exploring ML methods (and may one day put a model into production)\",\n                                  \"We recently started using ML methods (i.e., models in production for less than 2 years)\",\n                                  \"We have well established ML methods (i.e., models in production for more than 2 years)\",\n                                  \"I do not know\"], ordered=True\n                                 )\n\nml_usage_list = [\"No (we do not use ML methods)\", \n                \"We use ML methods for generating insights (but do not put working models into production)\",\n                \"We are exploring ML methods (and may one day put a model into production)\",\n                \"We recently started using ML methods (i.e., models in production for less than 2 years)\",\n                \"We have well established ML methods (i.e., models in production for more than 2 years)\",\n                \"I do not know\"]\n\nkaggle_2020[\"ML Application in Company\"] = kaggle_2020[\"ML Application in Company\"].astype(ml_usage_order)\nkaggle_2020.sort_values(\"ML Application in Company\", inplace = True)\nml_use_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Application in Company\"]], \"Earnings\", \"ML Application in Company\")\ncreate_plot(ml_use_list, \"ML Usage in Company\")","cdff8bb5":"# Country\n\ncountry_dict = {\n'India' : 'India',\n'United States of America' : 'USA',\n'Other' : 'Other',\n'Brazil' : 'Latin America',\n'Japan' : 'East Asia',\n'Russia' : 'Eastern Europe',\n'United Kingdom of Great Britain and Northern Ireland' : 'UK, Canada and Australia',\n'Nigeria' : 'Africa',\n'China' : 'East Asia',\n'Germany' : 'Western Europe',\n'Turkey' : 'Eastern Europe',\n'Spain' : 'Western Europe',\n'France' : 'Western Europe',\n'Canada' : 'UK, Canada and Australia',\n'Indonesia' : 'South East Asia',\n'Pakistan' : 'South Asia',\n'Italy' : 'Western Europe',\n'Taiwan' : 'East Asia',\n'Australia' : 'UK, Canada and Australia',\n'Mexico' : 'Latin America',\n'South Korea' : 'East Asia',\n'Egypt' : 'Africa',\n'Colombia' : 'Latin America',\n'Ukraine' : 'Eastern Europe',\n'Iran, Islamic Republic of...' : 'West Asia',\n'Kenya' : 'Africa',\n'Netherlands' : 'Western Europe',\n'Singapore' : 'South East Asia',\n'Poland' : 'Eastern Europe',\n'Viet Nam' : 'South East Asia',\n'Bangladesh' : 'South Asia',\n'South Africa' : 'Africa',\n'Argentina' : 'Latin America',\n'Morocco' : 'Africa',\n'Malaysia' : 'South East Asia',\n'Thailand' : 'South East Asia',\n'Portugal' : 'Western Europe',\n'Greece' : 'Eastern Europe',\n'Philippines' : 'South East Asia',\n'Tunisia' : 'Africa',\n'Israel' : 'West Asia',\n'Peru' : 'Latin America',\n'Chile' : 'Latin America',\n'Sweden' : 'Western Europe',\n'Saudi Arabia' : 'West Asia',\n'Republic of Korea' : 'East Asia',\n'Sri Lanka' : 'South Asia',\n'Switzerland' : 'Western Europe',\n'Nepal' : 'South Asia',\n'Romania' : 'Eastern Europe',\n'Belgium' : 'Western Europe',\n'United Arab Emirates' : 'West Asia',\n'Belarus' : 'Eastern Europe',\n'Ireland' : 'Western Europe',\n'Ghana' : 'Africa'\n}\nkaggle_2020[\"Region\"] = kaggle_2020[\"Country\"].replace(country_dict)\ncountry_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Region\"]], \"Earnings\", \"Region\")\ncountry_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(country_list, \"Country\")","db0b07ee":"# Role at work\n\nwork_role = ['Analyze and understand data to influence product or business decisions',\n'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n'Build prototypes to explore applying machine learning to new areas',\n'Build and\/or run a machine learning service that operationally improves my product or workflows',\n'Experimentation and iteration to improve existing ML models',\n'Do research that advances the state of the art of machine learning',\n'None of these activities are an important part of my role at work',\n'Work Activity Other'\n]\n\nwork_role_out = ['Analyze data',\n'Build and\/or run the data infrastructure',\n'Build prototypes',\n'Build and\/or run a ML service',\n'Improve existing ML models',\n'Research',\n'None of these',\n'Work Activity Other'\n]\n\nwork_role_list = []\ni = 0\nfor list_val in work_role:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", work_role_out[i])\n    if ct_out[1] >= 0.1:\n        work_role_list.append(ct_out)\n    i +=1\n\n# sort output list\nwork_role_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(work_role_list, \"Major Role at Work\")","db3cec88":"# Job Title\n\nkaggle_2020[\"Job Title\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"Job Title\"] = kaggle_2020[\"Job Title\"].apply(lambda x: \"Unemployed, Missing and Others\" if x in [\"Currently not employed\", \"Missing\", \"Other\"] else x)\njob_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Job Title\"]], \"Earnings\", \"Job Title\")\njob_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(job_list, \"Job Title\")","2cea83ac":"# ML Experience\n\nkaggle_2020[\"ML Experience\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].apply(lambda x: \"None or Missing\" if x in [\"Missing\", \"I do not use machine learning methods\"] else x)\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].apply(lambda x: \"More than 5 years\" if x in [\"5-10 years\", \"10-20 years\", \"20 or more years\"] else x)\n\nml_exp_order = CategoricalDtype(\n    [\"None or Missing\", \"Under 1 year\", \"1-2 years\",\"2-3 years\", \"3-4 years\", \"4-5 years\", \"More than 5 years\"], \n    ordered=True\n)\nml_exp_list = [\"None or Missing\", \"Under 1 year\", \"1-2 years\",\"2-3 years\", \"3-4 years\", \"4-5 years\", \"More than 5 years\"]\n\nkaggle_2020[\"ML Experience\"] = kaggle_2020[\"ML Experience\"].astype(ml_exp_order)\nkaggle_2020.sort_values(\"ML Experience\", inplace = True)\nml_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Experience\"]], \"Earnings\", \"ML Experience\")\ncreate_plot(ml_list, \"ML Experience\")","ad4577e0":"# Programming Skills\n\nmajor_progs = ['Python','R','SQL']\nminor_progs = ['C', 'C++', 'Java', 'Javascript', 'Julia', 'Swift', 'Bash','MATLAB', 'Progamming None', 'Programming Other']\n\nmajor_prog_list = []\nfor list_val in major_progs:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", list_val)\n    if ct_out[1] >= 0.1:\n        major_prog_list.append(ct_out)\n\n# sort output list\nmajor_prog_list.sort(key = lambda x:x[1], reverse = True)\n\nminor_prog_list = []\nfor list_val in minor_progs:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", list_val)\n    if ct_out[1] >= 0.1:\n        minor_prog_list.append(ct_out)\n\n# sort output list\nminor_prog_list.sort(key = lambda x:x[1], reverse = True)\n","666a1a97":"create_plot(major_prog_list, \"Major Programming Skills\")","31adb312":"# Visualization\n\nvisual_list = ['Matplotlib', 'Seaborn', 'Plotly \/ Plotly Express', 'Ggplot \/ ggplot2', 'Shiny', 'D3 js', 'Altair',\n               'Bokeh', 'Geoplotlib', 'Leaflet \/ Folium', 'Visualization None', 'Visualization Other']\nvisual_out = ['Matplotlib', 'Seaborn', 'Plotly', 'Ggplot', 'Shiny', 'D3 js', 'Altair',\n              'Bokeh', 'Geoplotlib', 'Leaflet \/ Folium', 'Visualization None', 'Visualization Other']\n\nvisual_usage_list = []\ni = 0\nfor list_val in visual_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", visual_out[i])\n    if ct_out[1] >= 0.1:\n        visual_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nvisual_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(visual_usage_list, \"Visualization Tools\")\n\n","4dcce4e8":"# Big Data Products\n\nbig_data_list = ['MySQL', 'PostgresSQL', 'SQLite', 'Oracle Database', 'MongoDB', 'Snowflake', 'IBM Db2', 'Microsoft SQL Server',\n'Microsoft Access', 'Microsoft Azure Data Lake Storage', 'Amazon Redshift', 'Amazon Athena', 'Amazon DynamoDB',\n'Google Cloud BigQuery', 'Google Cloud SQL', 'Google Cloud Firestore', 'DW None', 'DW Other']\nbig_data_out = ['MySQL', 'PostgresSQL', 'SQLite', 'Oracle Database', 'MongoDB', 'Snowflake', 'IBM Db2', 'Microsoft SQL Server',\n'Microsoft Access', 'Microsoft Azure Data Lake Storage', 'Amazon Redshift', 'Amazon Athena', 'Amazon DynamoDB',\n'Google Cloud BigQuery', 'Google Cloud SQL', 'Google Cloud Firestore', 'DW None', 'DW Other']\n\n\nbig_data_usage_list = []\ni = 0\nfor list_val in big_data_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", big_data_out[i])\n    if ct_out[1] >= 0.1:\n        big_data_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nbig_data_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(big_data_usage_list, \"Big Data Products\")","6deef9d4":"# BI Tools\n\nbi_tools_list = ['Amazon QuickSight', 'Microsoft Power BI', 'Google Data Studio', 'Looker', 'Tableau', 'Salesforce',\n'Einstein Analytics', 'Qlik', 'Domo', 'TIBCO Spotfire', 'Alteryx', 'Sisense', 'SAP Analytics Cloud',\n'BI None', 'BI Other']\nbi_tools_out = ['Amazon QuickSight', 'Microsoft Power BI', 'Google Data Studio', 'Looker', 'Tableau', 'Salesforce',\n'Einstein Analytics', 'Qlik', 'Domo', 'TIBCO Spotfire', 'Alteryx', 'Sisense', 'SAP Analytics Cloud',\n'BI None', 'BI Other'\n]\n\n\nbi_tools_usage_list = []\ni = 0\nfor list_val in bi_tools_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", bi_tools_out[i])\n    if ct_out[1] >= 0.1:\n        bi_tools_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nbi_tools_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(bi_tools_usage_list, \"BI Tools\")","302c9de3":"# Primary Analysis Tool\n\nkaggle_2020[\"Primary Analysis Tool\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"Primary Analysis Tool\"] = kaggle_2020[\"Primary Analysis Tool\"].apply(lambda x: \"Missing and Others\" if x in [\"Missing\", \"Other\"] else x)\nanalysis_tool_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Primary Analysis Tool\"]], \"Earnings\", \"Primary Analysis Tool\")\nanalysis_tool_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(analysis_tool_list, \"Primary Analysis Tool\")","6b40fef1":"# ML Frameworks\n\nml_frame = ['Scikit-learn', 'TensorFlow', 'Keras', 'PyTorch', 'Fast.ai', 'MXNet', 'Xgboost', 'LightGBM', 'CatBoost', 'Prophet',\n'H2O 3', 'Caret', 'Tidymodels', 'JAX', 'ML None', 'ML Other']\n\nml_frame_out = ['Scikit-learn', 'TensorFlow', 'Keras', 'PyTorch', 'Fast.ai', 'MXNet', 'Xgboost', 'LightGBM', 'CatBoost', 'Prophet',\n'H2O 3', 'Caret', 'Tidymodels', 'JAX', 'ML None', 'ML Other'\n]\n\nml_frame_list = []\ni = 0\nfor list_val in ml_frame:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_frame_out[i])\n    if ct_out[1] >= 0.1:\n        ml_frame_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_frame_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_frame_list, \"ML Frameworks\")","36d38bc2":"# ML Algorithms\n\nml_algo = ['Linear or Logistic Regression','Decision Trees or Random Forests', 'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks (MLPs, etc)', 'Convolutional Neural Networks',\n'Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks (BERT, gpt-3, etc)',\n'ML Algorithms None','ML Algo Other']\n\nml_algo_out = ['Linear \/ Logistic Regression','Decision Trees \/ Random Forests', 'Gradient Boosting Machines',\n'Bayesian Approaches', 'Evolutionary Approaches', 'Dense Neural Networks \/ MLPs', 'Convolutional Neural Networks',\n'Generative Adversarial Networks','Recurrent Neural Networks','Transformer Networks','ML Algorithms None','ML Algo Other'\n]\n\nml_algo_list = []\ni = 0\nfor list_val in ml_algo:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_algo_out[i])\n    if ct_out[1] >= 0.1:\n        ml_algo_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_algo_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_algo_list, \"ML Algorithms\")","b602bbd7":"# Computer Vision Methods\n\ncv_methods = ['General purpose image\/video tools (PIL, cv2, skimage, etc)', 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n'Object detection methods (YOLOv3, RetinaNet, etc)', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n'Generative Networks (GAN, VAE, etc)', 'CV None', 'CV Other',\n]\n\ncv_methods_out = ['General purpose image\/video tools (PIL, cv2, skimage, etc)', 'Image segmentation methods (U-Net, Mask R-CNN, etc)',\n'Object detection methods (YOLOv3, RetinaNet, etc)', 'Other networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)',\n'Generative Networks (GAN, VAE, etc)', 'CV None', 'CV Other']\n\n\ncv_methods_list = []\ni = 0\nfor list_val in cv_methods:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", cv_methods_out[i])\n    if ct_out[1] >= 0.1:\n        cv_methods_list.append(ct_out)\n    i +=1\n\n# sort output list\ncv_methods_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(cv_methods_list, \"Computer Vision Methods\")\n","8ab818fa":"# NLP Tools\n\nnlp_methods = ['Word embeddings\/vectors (GLoVe, fastText, word2vec)', 'Encoder-decorder models (seq2seq, vanilla transformers)',\n               'Contextualized embeddings (ELMo, CoVe)', 'Transformer language models (GPT-3, BERT, XLnet, etc)',\n               'NLP None', 'NLP Other']\n\nnlp_methods_out = ['Word vectors  - GLoVe, fastText, word2vec',\n'Encoder-decorder models - seq2seq, vanilla transformers',\n'Contextualized embeddings - ELMo, CoVe',\n'Transformer language models -  (GPT-3, BERT, XLnet, etc',\n'NLP None', 'NLP Other']\n\n\nnlp_methods_list = []\ni = 0\nfor list_val in nlp_methods:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", nlp_methods_out[i])\n    if ct_out[1] >= 0.1:\n        nlp_methods_list.append(ct_out)\n    i +=1\n\n# sort output list\nnlp_methods_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(nlp_methods_list, \"NLP Methods\")\n\n","c17bf2b6":"# Edication\nedu_dict = {\n    \"Master\u2019s degree\" : \"Master\u2019s degree\", \"Bachelor\u2019s degree\" : \"Bachelor\u2019s degree\",\n    \"Doctoral degree\" : \"Doctoral degree\", \n    \"Professional degree\" : \"Professional degree\",\n    \"Some college\/university study without earning a bachelor\u2019s degree\" : \"Unfinished College\",\n    \"Others\": \"Others\", \"I prefer not to answer\" : \"Others\", \n    \"No formal education past high school\"  : \"Others\"\n}\n\nkaggle_2020[\"Education\"] = kaggle_2020[\"Education Level\"].replace(edu_dict)\neducation_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"Education\"]], \"Earnings\", \"Education\")\neducation_list.sort(key=lambda x: x[1], reverse = True)\ncreate_plot(education_list, \"Education Level\")","1340fc11":"# ML Spend\n\nkaggle_2020[\"ML Spend\"].fillna(\"Missing\", inplace = True)\nkaggle_2020[\"ML Spend\"] = kaggle_2020[\"ML Spend\"].apply(lambda x: \"$0 or Missing\" if x in [\"Missing\", \"$0 ($USD)\"] else x)\nkaggle_2020[\"ML Spend\"] = kaggle_2020[\"ML Spend\"].apply(lambda x: \"$100,000 or more\" if x in [\"$100,000 or more ($USD)\"] else x)\n\n\nml_spend_order = CategoricalDtype(\n    [\"$0 or Missing\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"$100,000 or more\"], \n    ordered=True\n)\n\nml_spend_list = [\"$0 or Missing\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"$100,000 or more\"]\n\nkaggle_2020[\"ML Spend Order\"] = kaggle_2020[\"ML Spend\"].astype(ml_spend_order)\nkaggle_2020.sort_values(\"ML Spend Order\", inplace = True)\nml_spend_order_list = cross_tab_mat(kaggle_2020[[\"Earnings\", \"ML Spend Order\"]], \"Earnings\", \"ML Spend Order\")\ncreate_plot(ml_spend_order_list, \"ML Spend\")\n","6105bef6":"# ML Learning Platform\n\nml_learn_list = ['Coursera', 'edX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai Learning', 'Udacity', 'Udemy',\n'LinkedIn Learning','Cloud-certification programs (direct from AWS, Azure, GCP, or similar)',\n'University Courses (resulting in a university degree)','Learn ML None',\n'Learn ML Other']\nml_learn_out = ['Coursera', 'edX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai Learning', 'Udacity', 'Udemy',\n'LinkedIn Learning','Cloud-certification - AWS, Azure, GCP',\n'University Courses','None',\n'Others',\n]\n\nml_learn_usage_list = []\ni = 0\nfor list_val in ml_learn_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ml_learn_out[i])\n    if ct_out[1] >= 0.1:\n        ml_learn_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nml_learn_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ml_learn_usage_list, \"Learning Platform\")","d5cb2ce2":"# Knowledge Sharing\n\nsharing_list = ['Plotly Dash', 'Streamlit', 'NBViewer', 'GitHub', 'Personal blog', 'Kaggle', 'Colab',\n'Shiny Deploy', 'I do not share my work publicly', 'Deploy Other']\nsharing_out = ['Plotly Dash', 'Streamlit', 'NBViewer', 'GitHub', 'Personal blog', 'Kaggle', 'Colab',\n'Shiny Deploy', 'I do not share my work publicly', 'Other'\n]\n\nsharing_usage_list = []\ni = 0\nfor list_val in sharing_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", sharing_out[i])\n    if ct_out[1] >= 0.1:\n        sharing_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nsharing_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(sharing_usage_list, \"Knowledge Sharing\")","f0a45ef7":"# Data Science Media\n\nds_media_list = ['Twitter (data science influencers)', \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\",\n'Reddit (r\/machinelearning, etc)', 'Kaggle (notebooks, forums, etc)', 'Course Forums (forums.fast.ai, Coursera forums, etc)',\n'YouTube (Kaggle YouTube, Cloud AI Adventures, etc)', 'Podcasts (Chai Time Data Science, O\u2019Reilly Data Show, etc)',\n'Blogs (Towards Data Science, Analytics Vidhya, etc)', 'Journal Publications (peer-reviewed journals, conference proceedings, etc)',\n'Slack Communities (ods.ai, kagglenoobs, etc)', 'Media None', 'Media Other',\n]\n\n\nds_media_out = ['Twitter', \"Email newsletters\", 'Reddit', 'Kaggle', 'Course Forums',\n'YouTube', 'Podcasts', 'Blogs', 'Journal Publications', 'Slack', 'Media None', 'Media Other'\n]\n\nds_media_usage_list = []\ni = 0\nfor list_val in ds_media_list:\n    ct_out = cross_tab_row(kaggle_2020[[\"Earnings\", list_val]], list_val, \"Earnings\", ds_media_out[i])\n    if ct_out[1] >= 0.1:\n        ds_media_usage_list.append(ct_out)\n    i +=1\n\n# sort output list\nds_media_usage_list.sort(key = lambda x:x[1], reverse = True)\n\ncreate_plot(ds_media_usage_list, \"Data Science Media\")","beeaf30a":"* As with neural networks, CV algorithms too are being adopted quickly. ","46b90880":"\n\nTIME magazine officially called[ 2020, the worst year ever](https:\/\/time.com\/5917394\/2020-in-review\/). The Covid pandemic ravaged economies around the world. At the same time bringing economic destruction on a scale hitherto unknown. \n\nConcurrently, companies in the tech space boomed. The combined size of `Google, Amazon, Microsoft, Apple and Facebook` is now more than those of the remaining 495 in the S&P 500. There is an explosion in the interest in education, upskilling and re-skilling across the world. Data Science was called the sexiest job of the 21st century earlier. After the pandemic, it has simply gotten irresistible. \n\nIt is tempting to learn data science. A cursory Google search will throw up numerous data science courses. Each promising us to deliver us to the provmised land faster than most televangelists. \n\n<img src=\"https:\/\/i.ytimg.com\/vi\/k3h-KfsD_iw\/maxresdefault.jpg\" width=\"600\">\n\n\n<div><p> <\/div>\n  \n\n\n","8cdd2307":"* Natural Language Processing is expected to be the next game changer. \n* If you are comfortable with regression and classification, NLP seems to be the next step. ","5e2f05cc":"* Since very few analysts earn big bucks, around a third of the respondents reported no BI experience. \n* The usage of BI tools drop in the higher earnings bucket. \n* Tableau and Power BI are the tools of choice of you want Data Analyst roles. \n\n#### \ud83d\udccc  **The BI tool usage probably follow the Job roles.**","a03a76d2":"* Costly courses seem to pay off.\n* The free courses might be popular, but they don't seem to add value\n\n`This data is spend over last five years. And also includes company sponsored learning, so the high spends are not that surprising.`\n\n#### \ud83d\udccc **You get what you pay for. You have to spend money to earn money**","8ca5c4cc":"* Regression and Forests are everywhere. \n* It is surprising to see Neural Networks being used so widely. Shows how quickly technologies become commonplace. \n\n","d567e214":"You might get the kicks out of trying out *new stuff*, but to earn, you need to work *and excel* in a company with an established Data Science \/ Machine Learning practice. \n\n\n* A company with an established practice will help weed out impractical or unfeasible ideas. \n* You will meet great mentors, who will guide you in your journey. \n \n#### \ud83d\udccc **Working in a company that has a stable ML practice can be valuable, especially in the early stages of your career**  ","16e768bb":"There are some clear winners and losers when in comes to job location. \n\n* America remains the land of opportunity. \n* UK, Canada and Australia are the next destinations.\n* Most other places including Western Europe drop off in in the higher earnings bucket. \n\n`This might be explained by a lack of large companies involved in the field in Western Europe`\n\n* India is super competitive. So very few top paying jobs are available there. \n\n#### \ud83d\udccc **Move to USA if you want to earn serious money in Data Science**","95c38b3e":"So you have moved to the right place. What do you do when you get there? Let us explore what kind of work pays you. ","7b2c524b":"# How to keep Learning\n\nData Science is fluid, so the professionals need to keep themselves continually updated. \n\nHere we look at their sharing and learning methods. ","add315e8":"# What to do?","11accca4":"While designations *might be deceptive*, they largely follow a trend similar to the Work Role. \n\n* Not all ML \/ DS roles pay equally. \n* Product Managment roles as expected, pay better compared to research and engineering roles\n* I am curious to find out more about the Miscellaneous Category. \n\n#### \ud83d\udccc *Actual work pays*","73c47732":"# Acquiring Skills\n\nNow that we know what we need to pick up, where do we learn them from?","fa977505":"\n* Almost half the Data Sciene world has a master's degree\n\n![Masters%20Degree.jpg](attachment:Masters%20Degree.jpg)\n\n* With Indians constituting around a quarter of the respondents, Bachelor's Degree does not pay that much. \n\n* Doctoral programs pay off. \n\n#### \ud83d\udccc *If you want to succeed in Data Science, study*","c3208859":"* Scikit learn is your first step to Data Science. So get comfortable with it. \n* TF and Keras too are finding wide usage. \n* PyTorch is the new shiny toy in the ML world. So you might see a lot of resumes with PyTorch proficiency next year. \n\n#### \ud83d\udccc **Scikit Learn is to ML what matplotlib is to visualization**\n","7890aa92":"# ML Skills","da269a63":"\n* As discussed earlier, Python is ubiquitous.\n* R is not a bad second language to learn. \n* SQL remains the every green tool. \n* With greater emphasis on execution, you can't go wrong with SQL. \n\n#### \ud83d\udccc Everyone and their cat is learning Python, you too will have to","4808d07f":"\nAs with any other profession, we cannot exist in silos. We may have impressions of Data Scientists being super nerdy folks working in isolation tapping away at the keyboard. But in reality, Data Science is a collaborative effort. And you need the right environment to grow and prosper. So if you are planning to switch jobs in 2021, here are few factors that you mind want to consider. ","5b8b5860":"# Learn and Earn in 2021\n\n[](http:\/\/)","2e94526d":"\n* Free and low priced courses like Kaggle, Udemy are popular at the lower end. \n* Mid priced courses like DataCamp and LinkedIn learning are popular with mid-range earners. \n* Coursera is the most poplular of the large MOOCs with almost half of all respondents using it. \n* Almost a quarter of the respondents went to the University. \n\n#### \ud83d\udccc **The platform choice reflects the spends**","0be53821":"* The traditional SQL daabases (mySQL, MS-SQL and PostgreSQL still dominate. \n* MongoDB and noSQL databases are taking time to rise up the usage charts. \n\n#### \ud83d\udccc**Databases don't change much. Do they?**","cb5c1ad6":"\n\n* GitHub is the platform of choice. \n* A fifth of the respondents don't share. And their proportion increases with earnings. \n* Unfortunately both Kaggle and Colab suffer because of this. \n\n#### \ud83d\udccc **The high earners need not be selfish. They simply might be busy with the additional work that comes with additional pay and responsibilites.** \n","ac6440f8":"* Matplotlib and Seaborn are becoming too commonplace.\n* ggplot follows the R trend. If you are from the R world, learn ggplot. \n\n#### \ud83d\udccc **Ploltly perhaps the next big visualization tool in the Python universe.**\n","7fefa4be":"As with company size of the company, there is a <span style=\"background:lawngreen; font-weight:medium; color:white\">direct relationship<\/span> between the data science team size and earnings. This might be because - \n\n`You will get access to not only a wide variety of team members, but will also be sharing responsibilities and can therefore get more opportunities to network and learn`\n\n#### \ud83d\udccc**It makes sense to work in a large Data Science team**\n","4d256387":"# Methodology\n\nBefore we start, let us set up our evaluation criteria. \n\n## 1. Earnings: \n\n\nThe Kaggle Survey does not track job satisfaction, work hours and bad bosses. So we will take the salaries as a proxy. While money cannot buy us everything, we will use the Ted Turner philosophy. \n\n> Life is a game. Money is a way to keep score. \n\nSady, this means that we will have to <span style=\"background:coral; font-weight:medium; color:white\">leave out roughly half the respondents<\/span> who did not mention their salaries. \n\nThe median US income is about USD 70,000 per year for a college graduate. Keeping this in mind, we have grouped the earnings into five categories\n\n* Up to USD 10,000 (significantly lower than the median)\n* USD 10,000 to USD 50,000 (slightly lower than the median)\n* USD 50,000 to USD 80,000 (around the median)\n* USD 80,000 to USD 150,000 (slighly more than the median)\n* USD 150,000 and more (significantly more than the median)\n\n\n## 2. Importance of attributes:\n\nYou would have heard that if you need to learn data science, you ***have*** to learn Python. While any new learning is great, we will need to know - \n* How ubiquitous is Python? \n* What is the competitive advantage one would get in the Data Science \/ Machine Learning world? \n\nIf find the number of respondents using Python **(81%)**. That's a lot of people. \n\nAnd if branch the usage across various Earnings brackets, we find that the number is steady. So it might not give me a competitive edge. \n\n\n![Explainer.png](attachment:Explainer.png)\n\n\n\n### To quantify this, we use two measures.\n\n### Incidence: \nThe fraction of people across all earnings buckets having the skill. \n\n![img%2001.png](attachment:img%2001.png)\n\n### Index: \nIf we keep the incidence as the base, we can calculate the importance index. \n\n![Index%20Formula.png](attachment:Index%20Formula.png)\n\nThe graph then looks something like this. \n\n![img%2003.png](attachment:img%2003.png)\n\nNow, if we read the two together,\n\n-  You will arguably learn Python to do well in Data Science.\n- *But just knowing Python will not give you a competitive edge.*\n\n\nOn the other hand an index like this gives you an attribute with a competitive edge in earnings. \n\n![img%2004.png](attachment:img%2004.png)\n\nWheras, an index like this perhaps indicates that the attribute is on the wane in the world of data science and may not help you your earnings potential. \n\n![img%2005.png](attachment:img%2005.png)\n\n\nTo reduce the effects of a small base, <span style=\"background:coral; font-weight:medium; color:white\">we have considered only those attributes that have at least 10% incidence for binary questions and at least 5% incidence for MCQ questions.<\/span>\n\nLet's do this.","2a77d5a6":"Not all roles in the Data Science world are equal. The pay almost mirrors the [Bloom's taxonomy](https:\/\/cft.vanderbilt.edu\/guides-sub-pages\/blooms-taxonomy\/#:~:text=Familiarly%20known%20as%20Bloom's%20Taxonomy,Analysis%2C%20Synthesis%2C%20and%20Evaluation.) for teaching. \n\n![Blooms-Taxonomy-v2.jpg](attachment:Blooms-Taxonomy-v2.jpg)\n\n* Data Analysis shows promise but cannot compete with the salaries at the higher end. \n* However, Data Analyst positions *might* be a stepping stone to get into Data Science. \n* Research does not pay too much. Data Science isn't and exact science after all!!\n\n#### \ud83d\udccc *To earn money in Data Science you need to get your hands dirty*\n","e6f57170":"# Toolbox\n\n* What are the skills needed to succeed in the field?\n* What is the basic requirement? \n* And what are the game changing tools?","eddd1053":"* The media consumption trends follow the knowledge sharing trends.\n* As Data Scientists get busy, they prefer one way communication like newsletters and articles versus community learning.\n\n","b4f3b3b2":"# Parting words\n\nData Science is a topsy turvy ocean. What seems like a continuing trend might change the next minute. We need to learn continuosly to stay on top. If you are new to Data Science, I hope this helps you figure out where to start. If you are an experienced professional, this might help you answer - What next?\n\nThe Kaggle Survey is a wonderful source of information about the Data Science \/ Machine Learning Community. This analysis helped strenghten some of my beliefs while also highlighting quite a few new things. \n\nI hope my analysis does justice to the data and you liked reading it as much as I did creating it. \n\n### **Thanks for reading and hope you have a safe and successful 2021!!**\n","00924b2c":"![The%20hierarchy.jpg](attachment:The%20hierarchy.jpg)\n\n* Data science is truly expanding. Even hitherto indispensable tools like Excel are declining. \n* SAS and SPSS still find value at the top end. It shows the prevalance of SAS in large companies. \n\n#### \ud83d\udccc In the Data Science world, spreadsheets are for kids. ","5daba7cc":"\nThe New Year is almost upon us and if you have not already enrolled for one of the various data science programs, it is very likely that you will feel pressured to change your life in 2021 by enrolling into one. Let us explore the Kaggle ML survey from 2020 and find out what are the attributes that pay in the Data Science \/ Machine Learning world. My hope is that this helps set gives a realistic expectations on what can be achieved and where to concentrate your efforts in order to achieve those targets. We will try to answer - \n\n* Where to work?\n* What to do?\n* What your toolbox should look like?\n* Specific ML skills to acquire?\n* Where to acquire those skills?\n* How to learn?\n\n","c2165338":"In this part we look at the Algorithms that set you apart. \n\n![ML%20Meme.jpeg](attachment:ML%20Meme.jpeg)\n","fa8cf9d4":"# Where to work?","561e6690":"**There is no substitute for experience.**\n\n![Compounding%20v2.jpg](attachment:Compounding%20v2.jpg)\n\nWhile courses and knowledge might bring short term bursts, to make it big, one needs to persevere. \n\n`As with every other profession, it takes time to succeed` \n\n#### \ud83d\udccc *It is a marathon, not a sprint. Pace yourself and temper your expectations*","e7cde4a2":"There is a <span style=\"background:lawngreen; font-weight:medium; color:white\">direct relationship<\/span> between the company size (by number of employees) and earnings. This might be perhaps because\n\n* A large company can afford to pay you more. \n* It can arguably afford the data science \/ machine learning infrastructure. \n* It can invest in continuous learning\n* It exposes you to a varied group of team mates to help you learn. \n \n\n#### \ud83d\udccc**Move to a large company to grow.**\n\n"}}