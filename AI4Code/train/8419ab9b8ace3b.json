{"cell_type":{"6a88c6fe":"code","9198fc0c":"code","cd5b8f8f":"code","9e830fc6":"code","d2f0d865":"code","c6ce7e73":"code","d412efa8":"code","690fff3a":"code","391acb95":"code","ff71e4d7":"code","202ea624":"code","1d042622":"code","55e190cd":"code","e2d53f6c":"code","aefdbf2e":"code","b45e6900":"code","7286bf7b":"code","186d8b75":"code","62227e97":"code","914b2109":"code","1e203a15":"code","459e1683":"code","927ab811":"code","753ab6cd":"code","81042326":"code","c6a16e22":"code","06758774":"code","d5ca1cc8":"code","55a885a8":"code","0a386238":"code","bc56c6f2":"code","eb0171d6":"code","a4b9e770":"code","63bbdfbc":"code","c327fc78":"code","b4812277":"code","18b2675f":"code","f739b999":"code","d8d7062e":"code","15938f63":"code","a155bf44":"code","42c1819e":"code","2c0159bb":"markdown","0c971df5":"markdown","0e561cca":"markdown","22a1a058":"markdown","2494dfa6":"markdown","883df87d":"markdown","eb15507c":"markdown","fcf9167a":"markdown","7825eead":"markdown","cba11c90":"markdown","a38144c0":"markdown","2079b00e":"markdown","55386845":"markdown","d756a30a":"markdown","212e7e31":"markdown","09b74e80":"markdown","69f47202":"markdown","4fd05db7":"markdown","fbead9ea":"markdown"},"source":{"6a88c6fe":"import numpy as np # Data processing\nimport pandas as pd # Data analysis\nimport seaborn as sns # Data visualization\nimport matplotlib.pyplot as plt # Data visualization\n%matplotlib inline","9198fc0c":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\") # Open data files\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\") \nconnect = [ df_train , df_test ]\nplt.style.use('fivethirtyeight') # Change the style of the plots","cd5b8f8f":"df_train.head(10)","9e830fc6":"df_train.info()\nprint('---------------')\ndf_test.info()","d2f0d865":"lived = df_train.query(\"Survived == 1\")\ndead = df_train.query(\"Survived == 0\")","c6ce7e73":"corr = df_train.corr()\nsns.heatmap(corr)","d412efa8":"df_train.describe()","690fff3a":"df_train.describe(include=['O'])","391acb95":"df_train.groupby('Survived')['Sex'].describe()","ff71e4d7":"g = sns.FacetGrid(df_train,col='Survived')\ng.map(plt.hist,'Age')","202ea624":"g = sns.FacetGrid(df_train,col='Sex',row='Survived')\ng.map(plt.hist,'Age')","1d042622":"g = sns.FacetGrid(df_train,row = 'Embarked')\ng.map(plt.hist ,'Survived')","55e190cd":"for dataset in connect:\n  dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nlived = df_train.query(\"Survived == 1\")\ndead = df_train.query(\"Survived == 0\")\n\nprint(lived.groupby('FamilySize')['FamilySize'].describe())\nprint(\"-\"*70)\nprint(dead.groupby('FamilySize')['FamilySize'].describe())","e2d53f6c":"g = sns.FacetGrid(df_train,row='Survived')\ng.map(plt.hist,'FamilySize')","aefdbf2e":"df_train['IsAlone'] = 0 \ndf_train.loc[ df_train['FamilySize'] == 1 , 'IsAlone' ] = 1\ndf_test['IsAlone'] = 0 \ndf_test.loc[ df_test['FamilySize'] == 1 , 'IsAlone' ] = 1\n\nlived = df_train.query(\"Survived == 1\")\ndead = df_train.query(\"Survived == 0\")","b45e6900":"print(lived.groupby('IsAlone')['Survived'].describe())\nprint('-'*50)\nprint(dead.groupby('IsAlone')['Survived'].describe())","7286bf7b":"g = sns.FacetGrid(data=df_train,row='Survived')\ng.map(plt.hist,'IsAlone')","186d8b75":"MostRepeat = dataset['Embarked'].dropna().mode()[0]\nprint(MostRepeat)","62227e97":"for dataset in connect:\n  dataset['Embarked'] = dataset['Embarked'].fillna(MostRepeat)","914b2109":"for dataset in connect:\n  dataset['Embarked'] = dataset['Embarked'].map({'S':0,'C':1,'Q':2})","1e203a15":"for dataset in connect:\n  dataset['Sex'] = dataset['Sex'].map({'male':0,'female':1})","459e1683":"guess_ages = np.zeros((2,3))\nguess_ages","927ab811":"for dataset in connect:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_guess = guess_df.median()\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","753ab6cd":"df_test['Fare'].fillna(df_test['Fare'].dropna().median(), inplace=True)","81042326":"y_train = df_train['Survived'].values\nx_train = df_train.drop(['PassengerId','Name','Ticket','Cabin','Survived'],axis=1)\nx_test = df_test.drop(['Name','Ticket','Cabin'],axis=1)","c6a16e22":"from sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression , SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","06758774":"IDs = x_test.pop('PassengerId')","d5ca1cc8":"RandomForestClassifier = RandomForestClassifier(random_state =33)\nRandomForestClassifier.fit(x_train,y_train)","55a885a8":"y_pred = RandomForestClassifier.predict(x_train)","0a386238":"print(\"RandomForestClassifier score: \" , np.round(RandomForestClassifier.score(x_train,y_train)*100))\nprint(classification_report(y_train,y_pred))\nprint(confusion_matrix(y_train,y_pred))","bc56c6f2":"Logistic_Regression = LogisticRegression(penalty='l1',solver='liblinear',random_state=33)","eb0171d6":"Logistic_Regression.fit(x_train,y_train)","a4b9e770":"y_pred = Logistic_Regression.predict(x_train)","63bbdfbc":"print(\"LogisticRegression score: \" , np.round(Logistic_Regression.score(x_train,y_train)*100))\nprint(classification_report(y_train,y_pred))\nprint(confusion_matrix(y_train,y_pred))","c327fc78":"SGDC = SGDClassifier(random_state=33)\nSGDC.fit(x_train,y_train)","b4812277":"y_pred = SGDC.predict(x_train)","18b2675f":"print(\"SGDClassifier score: \" , np.round(Logistic_Regression.score(x_train,y_train)*100))\nprint(classification_report(y_train,y_pred))\nprint(confusion_matrix(y_train,y_pred))","f739b999":"DecisionTree = DecisionTreeClassifier(random_state=33)\nDecisionTree.fit(x_train,y_train)","d8d7062e":"y_pred = DecisionTree.predict(x_train)","15938f63":"print(\"DecisionTreeClassifier score: \" , np.round(Logistic_Regression.score(x_train,y_train)*100))\nprint(classification_report(y_train,y_pred))\nprint(confusion_matrix(y_train,y_pred))","a155bf44":"y_pred = RandomForestClassifier.predict(x_test)","42c1819e":"solution = pd.DataFrame({'PassengerId':IDs,'Survived':y_pred})\nsolution = solution.to_csv('.\/Submission.csv',index=False)","2c0159bb":"Here we saw the correlation between the features , In my view I see that the effective data is :\n1. Age\n2. SibSp\n3. Parch\n4. Sex","0c971df5":"there we saw that most of our passengers was males and came from embarked S .\n\n > when we look to the following desribtion we discover most of the deaths were in males  we saw that in this dataset about 550 were dead , 468 of them were males .\n\n> In the other hand when we look on who is survived we found that females who is the most survived there , 342 people in this dataset were survived , 233 of them were females","0e561cca":"In the end of this notebook , please if you saw any mistakes in my code leave a comment and tell me what is wrong and why .\n\nOr if you take some new knowlage also leave a comment and tell us what you learnt from this notebook.\n\nThanks for your time at reading this notebook, \n\nall love ,\n\nGoodbye .","22a1a058":"First we will fill the nulls in embarked by give the nulls the value of most repeated embarked .\n","2494dfa6":"# Data processing :\n\nNow after we took a look at the data and saw which features were effective in this disaster , if we remember in the beginning we found some of missing data in our dataset (training \/ testing ) , now we want to process this data by filling the nulls or delete this nulls features if we can't fill it.\n\nalso we want to convert the categorical value to numeric values.","883df87d":"As we see first I sperate the training dataset into two groups one for who survived and the other one for who dead , that just to help as in data analysis .","eb15507c":"As we saw in the describtion above the mean of age is 29 years and at least everyone have 1 SibSp and 1 Parch .","fcf9167a":"\n# Data analysis :\n\nIn the few code cells below we will discover the following :\n\n1. In age there is about 120  missing values in training and about 100 missing values in testing\n2. In Embarked there is 3 missing values\n3. In Cabin there is more than 600 missing values in training and about 300 missing data in testing\n\n\n\n> Don't think a lot about what we are going to do with these missing data especially in cabin .\n\nwe will also see which data is more effective than other data .\n\n","7825eead":"Now we're going to create a new feature called **IsAlone** , it's return 1 if the **FamilySize** equal to 1 or 0 if the **FamilySize** equal to more than 1 .\n\nI'm thinking now you understand why we added 1 before .\n\n> In the few cells below we will find most of the people dead were alone ","cba11c90":"the following graph just to be sure about the info that we said in the beginnings which says \" Most of our passengers came from embarked S \" .","a38144c0":"# Model training :\n\nNow after we analysis our data then make some processing at it , to make it ready for training .\n\nwe will use several algorithms .","2079b00e":"Now after we use some of ML algorithms we found the scores :\n \n 1. RandomForestClassifier score:  98.0\n 2. LogisticRegression score:  81.0\n 3. SGDClassifier score:  81.0\n 4. DecisionTreeClassifier score:  81.0\n\n As we see the best one was RandomForestCalssifier , so we are going to use it to predict **x_test** .","55386845":"In the next two cells we will convert the embarked and sex from a categorical values to numeirc values .\n","d756a30a":"Print the first 10 lines","212e7e31":"In the following graph, we will see that people who were older than 50 years had less hope to rescue .\n\nAslo most of deaths were between 20 years and 25 years and it's start going down after 30 years .\n\nbabies between 1 year to 5 years have more hope to rescure also who is between 25 years and 35 years have hope to .","09b74e80":"The next function for filling the nulls in age .\nit's from [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","69f47202":"> the following graph shows as that is feamles have more hope than males to rescure , we will see the number of deaths in males is bigger than females , that makes as to discover in the titanic rescue operation, they were focused on females and babies but not the same focus with males, which makes the death numbers are higher in males","4fd05db7":"# Introduction :\n\n*NOTE : This notebook have some codes from [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)*\n\n## Our straegy :\n\n1. Read the data and see what is missing\n2. Analyzing the data by spreate this training dataset into two groups \n3. Try to clean our data and convert the categorical data to numeric data by filling the missing data and create new features using feature engineering \n4. Use several ML algorithms to predict the test data\n\n## Our problem :\n\nWe have two dataset (training\/testing) and we want to use the traning data to create the best ML model that can predict the correct labels for the testing data .\n\n## Goals :\n\n1. Analyzing the data \n2. Create new features to develop the model's preformance \n3. completing the missing data\n4. create the best ML model by using the best algorthim","fbead9ea":"Instead of looking at SibSp and Parch alone we are going we put them in the same feature under the name of FamilySize and that what we called features engineering.\n\n > I want you to think why I add 1 to?\n\n If you look at the graphs after the description below, you will find that families with more than 5 people have less hope than other families in rescue operations, because the large family were going to take more time than other families were which less than 5 people.\n"}}