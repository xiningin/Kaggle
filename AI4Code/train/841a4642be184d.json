{"cell_type":{"075cbdab":"code","1914e2b2":"code","a652a3c0":"code","245ebad1":"code","e649011f":"code","ba9326a4":"code","337d6fce":"code","0ed744a6":"code","1d242027":"code","48dd0fb4":"code","bd2fe512":"code","c2f27ba6":"code","38675dcf":"code","25a3ee92":"code","4efa2e59":"code","7368e8d3":"code","dfe270bf":"code","89f9f768":"code","e9879c24":"code","44737d09":"code","6d4bc592":"code","ad6727e0":"code","0c6a4134":"code","01ca6079":"code","7a22ddb8":"code","7c98f889":"code","9cb93650":"code","18a34881":"code","13293792":"code","2e7f7e18":"code","f5559446":"code","fa340be4":"code","68bd4691":"code","5d707241":"code","592f01f3":"code","7de33028":"code","ac1905aa":"code","73a3e04d":"code","c78e6cf1":"code","fe7b8abe":"code","44a38fc6":"code","b2a1014d":"code","a9f8b253":"code","db4c9af3":"code","80aa318d":"code","015af026":"code","621290c7":"code","45d0d8e6":"code","118837f1":"code","a02d7358":"code","2bd004d1":"code","0a146a33":"code","0dbfbcad":"code","82776b16":"code","51b8586f":"markdown","d3d35257":"markdown","ef5e638d":"markdown","d2df17e9":"markdown","d420c53f":"markdown","283a6b64":"markdown","e77e61b3":"markdown","5c6bf53e":"markdown","22494467":"markdown","96ec4914":"markdown","5fa47e22":"markdown","270e9d20":"markdown","c3f3e8fd":"markdown","34bb4755":"markdown","0773e714":"markdown","17baf8f0":"markdown","f1a19107":"markdown","86564cb8":"markdown","d039b013":"markdown","e59ea1e5":"markdown","24d3da09":"markdown","16a257be":"markdown","e5d7a2a7":"markdown","456f81b2":"markdown"},"source":{"075cbdab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings;warnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1914e2b2":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","a652a3c0":"df.shape","245ebad1":"df.head()","e649011f":"df.count()","ba9326a4":"df.dtypes","337d6fce":"for col in df.columns.to_list():\n    print(df[col].value_counts())\n    print()","0ed744a6":"# No phone service\ndf[df.PhoneService=='No'].MultipleLines.value_counts()","1d242027":"# No internet service\nfor col in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']:\n    print(df[df.InternetService=='No'][col].value_counts())\n    print","48dd0fb4":"df.Churn.value_counts()","bd2fe512":"df.columns.to_list()","c2f27ba6":"num_feature = ['tenure','MonthlyCharges'] #TotalCharges is processed separately since the data is not stored in float type\ncat_feature = ['gender',\n 'SeniorCitizen',\n 'Partner',\n 'Dependents',\n 'PhoneService',\n 'MultipleLines',\n 'InternetService',\n 'OnlineSecurity',\n 'OnlineBackup',\n 'DeviceProtection',\n 'TechSupport',\n 'StreamingTV',\n 'StreamingMovies',\n 'Contract',\n 'PaperlessBilling',\n 'PaymentMethod',\n    'tenure_bin']","38675dcf":"for col in num_feature:\n    sns.violinplot(x='Churn',y=col,data=df)\n    plt.show()","25a3ee92":"df[num_feature].describe()","4efa2e59":"df['tenure_bin'] = pd.cut(df.tenure,pd.IntervalIndex.from_tuples([(-1,6),(6,12),(12,18),(18,24),(24,30),(30,36),\n                                                                 (36,42),(42,48),(48,54),(54,60),(60,66),(66,72)]))","7368e8d3":"for col in cat_feature:\n    x,y = 'Churn', col\n\n    (df\n    .groupby(x)[y]\n    .value_counts(normalize=True)\n    .mul(100)\n    .rename('percent')\n    .reset_index()\n    .pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))\n\n    plt.show()","dfe270bf":"# Compare the MonthlyCharges between InternetService\nsns.boxplot(x='InternetService',y='MonthlyCharges',data=df)","89f9f768":"df[pd.to_numeric(df['TotalCharges'], errors='coerce').isna()][['TotalCharges','Churn']]","e9879c24":"# Cast TotalCharges as float\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')","44737d09":"# Remove rows with missing TotalCharges\ndf = df[df.TotalCharges.notna()]","6d4bc592":"df.count()","ad6727e0":"sns.violinplot(x='Churn',y='TotalCharges',data=df)\nplt.show()","0c6a4134":"df.TotalCharges.describe()","01ca6079":"df['Single'] = (df.Dependents + df.Partner == 0)*1","7a22ddb8":"def feat_engg(df):\n    df['Churn'] = (df.Churn=='Yes')*1\n    df['Female'] = (df.gender == 'Female')*1\n    df['Partner'] = (df.Partner=='Yes')*1\n    df['Dependents'] = (df.Dependents=='Yes')*1\n    df['PhoneService'] = (df.PhoneService=='Yes')*1\n    df['MultipleLines'] = (df.MultipleLines=='Yes')*1\n    df = pd.concat([df, pd.get_dummies(df.InternetService, prefix='InternetService').drop('InternetService_No',axis=1)], axis=1)\n    df['OnlineSecurity'] = (df.OnlineSecurity=='Yes')*1\n    df['OnlineBackup'] = (df.OnlineBackup=='Yes')*1\n    df['DeviceProtection'] = (df.DeviceProtection=='Yes')*1\n    df['TechSupport'] = (df.TechSupport=='Yes')*1\n    df['StreamingTV'] = (df.StreamingTV=='Yes')*1\n    df['StreamingMovies'] = (df.StreamingMovies=='Yes')*1\n    df = pd.concat([df, pd.get_dummies(df.Contract, prefix='Contract', drop_first=True)], axis=1)\n    df['PaperlessBilling'] = (df.PaperlessBilling=='Yes')*1\n    df = pd.concat([df, pd.get_dummies(df.PaymentMethod, prefix='PaymentMethod', drop_first=True)], axis=1)\n    df = pd.concat([df, pd.get_dummies(df.tenure_bin, prefix='tenure_bin')], axis=1)\n    df['Sum_of_internet_services'] = df.OnlineSecurity + df.OnlineBackup + df.DeviceProtection + df.StreamingTV + df.StreamingMovies + df.TechSupport\n    df['Single'] = (df.Dependents + df.Partner == 0)*1\n    # Drop unnecessary columns\n    df = df.drop(['customerID','gender','InternetService','Contract','PaymentMethod','tenure_bin'],axis=1)\n    return df\n    ","7c98f889":"engg_df = feat_engg(df)","9cb93650":"engg_df.head()","18a34881":"engg_df.shape","13293792":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(engg_df, test_size=0.30, random_state=42)","2e7f7e18":"train.Churn.value_counts()","f5559446":"test.Churn.value_counts()","fa340be4":"# Sanity check to ensure base rate between train and test are similar\nprint(len(train[train.Churn==1])\/len(train))\nprint(len(test[test.Churn==1])\/len(test))","68bd4691":"# Feature correlation with target\ntrain.corr()['Churn'].sort_values(ascending=False)","5d707241":"# Draw correlation heatmap\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=False)","592f01f3":"# Remove highly correlated features\nremoved_feat = ['Partner', 'tenure']","7de33028":"for df in [train,test]:\n    df = df.drop(removed_feat,axis=1)","ac1905aa":"X = train.drop('Churn',axis=1)\ny = train.Churn","73a3e04d":"roc_auc={}\npr_auc={}","c78e6cf1":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=42)\nscores = cross_validate(rfc,X,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision'))\nroc_auc['RandomForest'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['RandomForest'] = np.mean(np.array(scores['test_average_precision']))\n","fe7b8abe":"# SVC\nfrom sklearn.svm import SVC\n\nsvc = SVC(random_state=42)\nscores = cross_validate(svc,X,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision'))\nroc_auc['SVC'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['SVC'] = np.mean(np.array(scores['test_average_precision']))\n","44a38fc6":"# Xgbclassifier\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state=42)\nscores = cross_validate(xgb,X.values,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision')) #somehow model will turn Nan score due to naming convention in tenure_bin. hence, remove the column names\nroc_auc['XGBClassifier'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['XGBClassifier'] = np.mean(np.array(scores['test_average_precision']))","b2a1014d":"# LightGBM\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(random_state=42)\nscores = cross_validate(lgbm,X.values,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision')) #somehow model will turn Nan score due to naming convention in tenure_bin. hence, remove the column names\nroc_auc['LGBMClassifier'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['LGBMClassifier'] = np.mean(np.array(scores['test_average_precision']))","a9f8b253":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state=42)\nscores = cross_validate(lr,X,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision'))\nroc_auc['LogisticRegression'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['LogisticRegression'] = np.mean(np.array(scores['test_average_precision']))","db4c9af3":"# KNeighborClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknc = KNeighborsClassifier()\nscores = cross_validate(knc,X,y,cv=5, n_jobs=-1, scoring=('roc_auc', 'average_precision'))\nroc_auc['KNeighborsClassifier'] = np.mean(np.array(scores['test_roc_auc']))\npr_auc['KNeighborsClassifier'] = np.mean(np.array(scores['test_average_precision']))","80aa318d":"pd.DataFrame.from_dict(roc_auc,orient='index', columns=['ROC-AUC']).sort_values(by='ROC-AUC', ascending=False)","015af026":"pd.DataFrame.from_dict(pr_auc,orient='index', columns=['PR-AUC']).sort_values(by='PR-AUC', ascending=False)","621290c7":"from scipy.stats import loguniform\n\nparams = {\n    'penalty':['l1','l2','elasticnet'],\n    'C': loguniform(1e-3, 1000),\n    'solver':['newton-cg', 'lbfgs', 'liblinear']\n}","45d0d8e6":"model = RandomizedSearchCV(lr, params, random_state=42, n_iter=300, scoring='roc_auc', n_jobs=-1, cv=5, refit=True)","118837f1":"# This process might take some time due to large number of iterations\nsearch = model.fit(X, y)","a02d7358":"print(search.best_params_)\nprint(search.best_score_)","2bd004d1":"best_model = search.best_estimator_","0a146a33":"# Feature importance\nlr_coef = pd.DataFrame(best_model.coef_[0], index=X.columns.to_list(), columns=['coef'])\nlr_coef.sort_values('coef', ascending=False)","0dbfbcad":"pred = best_model.predict(test.drop('Churn',axis=1))\nprint('accuracy: ',accuracy_score(test.Churn.values,pred))\nprint('confusion_matrix: ',plot_confusion_matrix(best_model,test.drop('Churn',axis=1),test.Churn.values))","82776b16":"testy = test.Churn.values\n# predict probabilities\npred_proba = model.predict_proba(test.drop('Churn',axis=1))\n# keep probabilities for the positive outcome only\npred_proba = pred_proba[:, 1]\n# generate a no skill prediction (majority class)\nns_probs = [0 for _ in range(len(testy))]\n# calculate scores\nns_auc = roc_auc_score(testy, ns_probs)\nmodel_auc = roc_auc_score(testy, pred_proba)\n# summarize scores\nprint('No Skill: ROC AUC=%.3f' % (ns_auc))\nprint('Model: ROC AUC=%.3f' % (model_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\nmodel_fpr, model_tpr, _ = roc_curve(testy, pred_proba)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(model_fpr, model_tpr, marker='.', label='Model')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\nplt.title('ROC Curve on hold-out test set')\n# show the plot\nplt.show()","51b8586f":"## Loading Data","d3d35257":"**Insights**:\n1. `Tenure` of churned customers is skewed to short tenure duration and relatively shorter to non-churn customers, whereas the tenure of non-churn customers is more uniformly distributed. This makes sense since customer will be early to churn if they feel the product is not right fit for them and they will churn in their early customer lifecycle.\n2. Interestingly, `MonthlyCharges` of churn customers are relatively higher than non-churn customers'. It means the churn customers use \"more\" services hence they are charged more. We should analyze later the differences on used services between churn and non-churn customers.","ef5e638d":"**Key statistics**:\n1. Single\/no-partner customer dominates the churn customer set, reversely where customers with `Partner` dominates the non-churn customer set.\n2. Customers that used `Fiber optic` as `InternetService` dominates (~70%) the churn customers, whereas it is not the case in non-churn customers i.e. more uniformly distributed; ~35% Fiber optic in non-churn customers.\n3. ~80% of churn customers did not use `OnlineSecurity` on their InternetService. Additionaly, ~65% churn customers did not use `OnlineBackup`, ~63% did not use `DeviceProtection`\n4. ~80% of churn customers did not use `TechSupport`.\n5. ~85% of churn customers used `Month-to-month` contract.\n6. ~60% of churn customers used `Electronic check` as `PaymentMethod`, whereas it is not the case in non-churn customers i.e. more uniformly distributed; ~25% Electronic check in non-churn customers\n7. ~42% of churn customers have tenure less than 6 months.\n\n**Insights and Recommendations**:\n1. There is a correlation between **security services** and **churn**. The statistics show that most churn customers did not use security\/protection services (`OnlineSecurity`, `OnlineBackup` and `DeviceProtection`). It needs to be investigated why they did not use the security services; is it due their lack of awareness of security services, or lack of trust etc. Afterwards, we can launch campaign that increase awareness\/trust on security services that might lead to decrease on the churn rate.\n2. Majority of churn customers did not use `TechSupport`, which also might correlate #1 insight. It might probable that customers are only aware with security services or able to activate the security services once they talk to TechSupport. The onboarding process of security services might need to be relooked whether it is easy enough for the user to subscribe the security services without talking to TechSupport. *Alternatively*, we should look the problem in **wider** perspectives that some customers might be frustrated with the **general services**, not only security services. Instead of talking to TechSupport, they decided to churn instead. TechSupport might need to be made more accessible to the customers in order to reduce the churn rate.\n3. Majority of churn customers used `Month-to-month` contract, which make them easier to terminate the contract at any time. We need to beware of the quality of our services since `One year` and `Two year` might also want to terminate the services once the contract ends and we need to retent them in our platform. Those who signed up with `Month-to-month` might have already intention to do trial with the services without committing long-term contract.","d2df17e9":"Data looks good for no phone service and no internet service.","d420c53f":"It turns out **Logistic Regression** is the most performing model to perform the classification which achieves highest ROC-AUC and PR-AUC score.\nNext, we will tune the hyperparameter for Logistic Regression","283a6b64":"Assuming `tenure` is in month unit, we will bin tenure into 6-month bucket. The first two bucket might be important since tenure is lesser than 1 year contract.\n`MonthlyCharges` and `TotalCharges` are not binned since there is no significant gain in term of corellation to target label(Churn) out of the binned features","e77e61b3":"## Data Sanity Check","5c6bf53e":"## Predict Hold-out Test Set","22494467":"Connecting insights on `MonthlyCharges` with service selection, based on the data that we have, we learnt that actually churn customers use less number of services since most of them did not use security services. However, there is still one possible explanation that why churn customers ended up paying more due to their `InternetService` selection, whereas most of churn customers selected `Fiber optic`. Let's validate our hypothesis that `MonthlyCharges` differences came from InternetService selection.","96ec4914":"We have found the best hyperparameter for the model and turns out there is no significant gain compared to non-finetuned model. We will use this model to predict the hold-out test set.","5fa47e22":"## Split Training and Test Set","270e9d20":"## Plot Features Correlation","c3f3e8fd":"We have ~80% accuracy classifier.","34bb4755":"The ROC-AUC on test set has only decreased slightly from training set (0.85 to 0.84), hence our model does not overfit the training data.","0773e714":"## Model Selection","17baf8f0":"`TotalCharges` is stored as string. There are 11 rows stored TotalCharges that could not be casted as float and turned out they have missing values. I decided to remove those 11 rows since it is a small number and all of them are non-churn customers (majority class).","f1a19107":"## Data Cleansing","86564cb8":"## Hyperparemeter finetuning","d039b013":"The following features are decided to be removed:\n1. `Partner`: High correlation with `Single` and lesser correlated with `Churn`.\n2. `tenure`: Could be represented by `tenure_bin`\n\nRest of highly correlated features are not removed and even expected:\n1. `tenure` is highly correlated with `TotalCharges`. The longer the customer is using the service, the bigger amount of accumulated charges.\n","e59ea1e5":"**Outline**\n1. **Loading Data**\n2. **Data Sanity Check**: ensure consistent values across features\n3. **EDA, Insights & Recommendations**: find patterns and provide actionable insights out of the data.\n4. **Data Cleansing**: cast `TotalCharges` to float while dropping few rows with missing values.\n5. **Feature Engineering** : convert categorical variables to dummy variables\n6. **Split Training and Test Set**: provide hold-out test set to test model performance.\n7. **Plot Features Correlation**: remove highly correlated features if any\n8. **Model Selection**: select the best algorithm\n9. **Hyperparameter finetuning**: use RandomizedSearchCV to find best model hyperparameter\n10. **Predict Hold-out Test Set**","24d3da09":"It is confirmed that `MonthlyCharges` differences came from `InternetService` selection since `Fiber optic` price is significantly higher compared to `DSL` and `No internet service`. This explains why churn customers ended up paying more `MonthlyCharges`. Then, it is logical to assess the quality and the pricing of of `Fiber optic` since it is our premium service, whether we give sufficient quality for the price that they pay, enough awareness and easy onboarding for security services etc.","16a257be":"## Feature Engineering","e5d7a2a7":"Base rate is similar.","456f81b2":"## EDA, Insights & Recommendations"}}