{"cell_type":{"7a973c3b":"code","363dd3d5":"code","f4f02b62":"code","9b83e3de":"code","60058f6e":"code","c322f83f":"code","8623d57c":"code","99ec4318":"code","ab6a0746":"code","e44834ac":"code","cc6be1ae":"code","8ffc8b10":"code","a2c411ce":"code","f279b6c7":"code","a7dbcc2f":"markdown","86831983":"markdown","6f94724a":"markdown","ec855a5f":"markdown","350c6eee":"markdown","8478016a":"markdown","9a984fe7":"markdown","6025081d":"markdown","6fcf93db":"markdown","6eb351f5":"markdown","c036f213":"markdown","ec5f5c64":"markdown","cd45d6f2":"markdown"},"source":{"7a973c3b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.model_selection import cross_val_score","363dd3d5":"train=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","f4f02b62":"print(train.shape)\nprint(test.shape)","9b83e3de":"train.head()","60058f6e":"#check labels\ntrain.label.unique()","c322f83f":"y_train=train[\"label\"]\ntrain.drop(\"label\",axis=1,inplace=True)\nsns.countplot(y_train)","8623d57c":"#check missing values\ntrain.isnull().values.any()","99ec4318":"#plot first 20 digits\nfig,axes=plt.subplots(4,5,figsize=(6,4),subplot_kw={\"xticks\":[],\"yticks\":[]})\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(train.values[i,:].reshape(28,28))\nplt.show()","ab6a0746":"%%time\nX=train.values\ny=y_train.values\npca_line=PCA().fit(X)\nplt.figure(figsize=[20,5])\nplt.plot(np.cumsum(pca_line.explained_variance_ratio_))\nplt.xlabel(\"number of components after dimension reduction\")\nplt.ylabel(\"cumulative explained variance ratio\")\nplt.show()","e44834ac":"score=[]\nfor i in range(1,101,10):\n    X_dr=PCA(i).fit_transform(X)\n    once=cross_val_score(RFC(n_estimators=20,random_state=0),X_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=(10,5))\nplt.plot(range(1,101,10),score)\nplt.show()","cc6be1ae":"score=[]\nfor i in range(20,30):\n    X_dr=PCA(i).fit_transform(X)\n    once=cross_val_score(RFC(n_estimators=20,random_state=0),X_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=(10,5))\nplt.plot(range(20,30),score)\nplt.show()","8ffc8b10":"#PCA+KNN\nscore=[]\nfor i in range(10):\n    X_dr=PCA(30).fit_transform(X)\n    once=cross_val_score(KNN(i+1),X_dr,y,cv=5).mean()\n    score.append(once)\nplt.figure(figsize=(10,5))\nplt.plot(range(10),score)\nplt.show()","a2c411ce":"pca=PCA(n_components=30)\npca.fit(X)\ntrain_dr=pca.transform(X)\ntest_dr=pca.transform(test)\nclf=KNN(3)\nclf.fit(train_dr,y)\nresults=clf.predict(test_dr)\nresults=pd.Series(results,name=\"Label\")","f279b6c7":"#accuracy 0.97\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"knn_digit_recognizer.csv\",index=False)","a7dbcc2f":"* From the above plot,we can notice that RandomForestClassifier preforms great when the n_components of PCA ranges from 20 to 100.\n* We should not ignore the turning point which is around 20 to 30.Thus we narrow the n_components again to find the best one.","86831983":"## 1.1 Load data","6f94724a":"# 1. Data preparation","ec855a5f":"* We can notice that when K is 3, the accuracy for cross_val_score beyond 0.97.","350c6eee":"* Use cumulative explained variance ratio to check the range of components we should use for dimension reduction.","8478016a":"Summary:\n* PCA performs very well on this dataset,we can use few features to get a very high accuracy,even though I just used very basic models.\n* Cumulative explained variance ratio is very useful and important,we can use it to decide the most suitable n_components for PCA.\n* I just used very basic models this time, and I think with more complex models the accuracy can be improved.","9a984fe7":"# 4. KNN","6025081d":"# 2. Visulization","6fcf93db":"* It is so great that we can use just 30 features but get almost 0.94 accuracy,which means that the dimensionality reduction is successful.I think if we try to use grid search to adjust parameters for RandomForestClassifier,we can get a better score.But now I want to change another model,KNN.","6eb351f5":"## 1.2 Check missing values","c036f213":"# 3. PCA","ec5f5c64":" **If you think this notebook is helpful,some upvotes would be very much appreciated.**","cd45d6f2":"* It is obvious that the trend increases dramatically when the number of components range from 0 to 100 and the trend flattens after the number of components beyond 100.\n* Therefore, we need to draw a similar plot to narrow the range of components from 1 to 100 in order to find the most suitable component with the highest cumulative explained variance ratio."}}