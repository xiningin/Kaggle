{"cell_type":{"0d58a492":"code","de4f0c84":"code","64c3ffca":"code","82560437":"code","f224804e":"code","b2f061c2":"code","b61c7e41":"code","7d01abab":"code","708285a4":"code","09734fdd":"code","9240c633":"code","114a1d6e":"code","80aaa319":"code","1739247c":"code","3286133c":"code","4e1c5f00":"code","9a62f957":"code","0bf0380e":"code","d0603775":"code","43c77dee":"code","717e5409":"code","a5b075ec":"code","2ad84bb7":"code","a33ea3e8":"code","d0f88bbd":"code","eb9bb440":"code","dd622e7c":"code","cc1950ff":"code","248954bb":"code","fcb2f088":"code","abd96613":"code","42842188":"code","bdcb3a2e":"code","faec3d34":"code","4efce7b1":"code","1179c1e8":"code","bea4c2b9":"code","1ee9cd00":"code","eb31a855":"code","4fb8b838":"code","11bb0e59":"code","9acf6f83":"code","f4575c8e":"code","e40aaa40":"code","1d2f81ad":"code","5afb75da":"code","3df264c1":"code","68382c16":"code","28397c72":"code","3f358261":"code","5f393605":"code","26b53972":"code","0c976209":"code","ad045809":"code","d0567b39":"code","9ce93573":"code","d811c610":"code","3ff01c37":"code","649a5cc4":"code","08f0fdd3":"code","464005c9":"code","bf88e8a0":"code","6fb31c3d":"code","d638cc86":"code","12bce0f1":"code","4a3bf086":"code","83a25494":"code","5cf7b969":"code","7a84ecd5":"code","c2217240":"code","e7d4db06":"code","7b7edf66":"code","fe41c91e":"code","3c54a1a8":"code","183a6ad0":"code","70aba350":"code","e72c51fd":"code","38e44139":"code","353b80c8":"code","f5af0a93":"code","59d79fcf":"code","280ff168":"code","5d898e8c":"code","09c1c444":"code","64401c29":"code","b68c4e58":"code","02c7e8f4":"code","eaea6edb":"code","63fd38df":"code","f8263012":"code","97f5a2f4":"code","4a72f5f7":"code","174fd028":"code","8cd0e4df":"code","9b1f708c":"code","fec0d432":"code","4269a60f":"code","a0931c27":"code","26328cd4":"code","c5b6313a":"code","76d766a1":"code","580d79aa":"code","7d9994e2":"code","66f71a4d":"code","9d155d85":"code","918eaca1":"code","882bcae1":"code","d48d36e9":"code","43b9c19f":"code","e535ca8e":"code","1c1c1e79":"markdown","6eebb706":"markdown","c7fd78eb":"markdown","021e2af8":"markdown","6ec4b1c5":"markdown","6750285e":"markdown","3ce34f2b":"markdown","3a9850c5":"markdown","4255ec27":"markdown","a3e34a08":"markdown","137fe7b2":"markdown","68e62009":"markdown","f1b57a3b":"markdown","f4371fc2":"markdown","15f4fe64":"markdown","338f04de":"markdown","6a681177":"markdown","a669b2c4":"markdown","d23ca654":"markdown","c259a834":"markdown","2340977c":"markdown","f6ffacb9":"markdown","8e619e5b":"markdown","e2ee000b":"markdown","0cde193b":"markdown","94e89a5f":"markdown","f1be8a0e":"markdown","cc90fe7f":"markdown","f6a29e2d":"markdown","cf0d5315":"markdown","8f6964e0":"markdown","5dd977eb":"markdown","1cbb1649":"markdown","c2c0ba06":"markdown","91f9e179":"markdown","3a336eb4":"markdown","6363fa61":"markdown","785056b0":"markdown","3f52ba50":"markdown","16c43e65":"markdown","a5ed81b0":"markdown","43b7430f":"markdown","a06447e0":"markdown","27dd5987":"markdown","02e91982":"markdown","c96951b1":"markdown","1ad8ef20":"markdown","091e99a1":"markdown","8f9511bb":"markdown","085aeb73":"markdown","1ab1b5c3":"markdown","25b7c0d7":"markdown","18993599":"markdown","be79adb9":"markdown","7f273844":"markdown","6e39480b":"markdown","5ec923d7":"markdown","1b63da16":"markdown","3f4a98a2":"markdown","4da35efa":"markdown","dc7859d7":"markdown","f1d877bd":"markdown","684ba458":"markdown","9abfa464":"markdown","50a92e2d":"markdown","be2933fb":"markdown","1e771c90":"markdown","d6538f7f":"markdown","02373532":"markdown","9d43e62d":"markdown","1af14caf":"markdown","5aff40fa":"markdown","6772adbc":"markdown","0594dc00":"markdown","a9d38dd0":"markdown","608f5f3a":"markdown","92e113a9":"markdown","f54ffc20":"markdown","df840b09":"markdown","494f4508":"markdown","7f2f79ef":"markdown","a71f6a91":"markdown","63ee5531":"markdown","c795288e":"markdown","fe5d70b5":"markdown","e38a22c0":"markdown","831bdc64":"markdown","5d80230d":"markdown","d96c7d1e":"markdown","ab8a6860":"markdown","4f04668f":"markdown","07cbee4f":"markdown","aa789c49":"markdown","1d19ee3e":"markdown","49a33347":"markdown","3d904b86":"markdown","d8b118b3":"markdown","d6858600":"markdown","15eaf015":"markdown","112ea519":"markdown","5de5bf88":"markdown","33cc890d":"markdown","7dbd6d79":"markdown","85a1b826":"markdown","076a75fa":"markdown","7161087f":"markdown","99fb2521":"markdown","5bea2185":"markdown"},"source":{"0d58a492":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","de4f0c84":"#from google.colab import files\ntestpath = 'test.csv'\ntrainpath = 'train.csv'\ntest_df = pd.read_csv('..\/input\/loan-prediction\/test.csv')\ntrain_df = pd.read_csv('..\/input\/loan-prediction\/train.csv')","64c3ffca":"test_df.head()","82560437":"train_df.head()","f224804e":"test_df.columns","b2f061c2":"train_df.columns","b61c7e41":"train_df.shape,test_df.shape","7d01abab":"train_df.info()","708285a4":"train_df.nunique()","09734fdd":"train_info = train_df.describe(include='all').transpose()\ntest_info = test_df.describe(include='all').transpose()","9240c633":"train_info['DataType']=train_df.dtypes\ntrain_info['NullCount'] = train_df.isnull().sum()\ntest_info['DataType']=test_df.dtypes\ntest_info['NullCount'] = test_df.isnull().sum()","114a1d6e":"train_df.isnull().sum()","80aaa319":"test_info","1739247c":"train_info","3286133c":"train_df.duplicated().sum()","4e1c5f00":"test_df.duplicated().sum()","9a62f957":"CategoricalColsList=[ 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term','Loan_Status']\nContinuousColsList = ['ApplicantIncome','CoapplicantIncome','LoanAmount']","0bf0380e":"def PlotBarCharts(inpData, colsToPlot):\n    %matplotlib inline   \n    import matplotlib.pyplot as plt   \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(40,6))\n    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])","d0603775":"PlotBarCharts(inpData=train_df, colsToPlot=CategoricalColsList)","43c77dee":"PlotBarCharts(inpData=test_df, colsToPlot=[ 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term'])","717e5409":"def label_function(val):\n    return f'{val \/ 100 * len(train_df):.0f}\\n{val:.0f}%'\nN = 50\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(20, 10))\ntrain_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\ntrain_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\ntrain_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'orange'], ax=ax3)\ntrain_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\nax1.set_ylabel('Gender', size=15)\nax2.set_ylabel('Married', size=15)\nax3.set_ylabel('Dependents', size=15)\nax4.set_ylabel('Education', size=15)\nplt.tight_layout()\nfig, (ax1, ax2, ax3,ax4) = plt.subplots(ncols=4, figsize=(20, 10))\ntrain_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\ntrain_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\ntrain_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'yellow'], ax=ax3)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\nax1.set_ylabel('Self_Employed', size=15)\nax2.set_ylabel('Credit_History', size=15)\nax3.set_ylabel('Property_Area', size=15)\nax4.set_ylabel('Loan_Status', size=15)\nplt.tight_layout()\nplt.show()\n","a5b075ec":"N = 50\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(25, 10))\ntest_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\ntest_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\ntest_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'orange'], ax=ax3)\ntest_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax4)\nax1.set_ylabel('Gender', size=15)\nax2.set_ylabel('Married', size=15)\nax3.set_ylabel('Dependents', size=15)\nax4.set_ylabel('Education', size=15)\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 8))\ntest_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax1)\ntest_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], ax=ax2)\ntest_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'yellow'], ax=ax3)\nax1.set_ylabel('Self_Employed', size=15)\nax2.set_ylabel('Credit_History', size=15)\nax3.set_ylabel('Property_Area', size=15)\nplt.show()\n","2ad84bb7":"def PlotContinousCharts(inpData, colsToPlot):\n    %matplotlib inline   \n    plt.figure(figsize=(30, 50))\n    for i, col in enumerate(colsToPlot):\n      plt.subplot(9, 3, i*3+1)\n      plt.subplots_adjust(hspace =.25, wspace=.1)   \n      plt.grid(True)\n      plt.title('HistPlot : '+col)\n      sns.histplot(inpData[col], label=col, color = \"blue\")\n      plt.subplot(9, 3, i*3+2) \n      plt.title('BoxPlot : '+col)\n      sns.boxplot(inpData[col])\n      plt.subplot(9, 3, i*3+3) \n      plt.title('KDEPlot : '+col)\n      sns.kdeplot(inpData[col],shade=True)","a33ea3e8":"PlotContinousCharts(inpData=train_df, colsToPlot=ContinuousColsList)","d0f88bbd":"PlotContinousCharts(inpData=test_df, colsToPlot=ContinuousColsList)","eb9bb440":"cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term']\nnr_rows = 2\nnr_cols = 4\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):          \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(train_df[cols[i]], hue=train_df[\"Loan_Status\"], ax=ax,palette=['#347C17',\"#FF2400\"])\n        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n        ax.legend(title=\"Loan Status\", loc='best')    \n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \nplt.tight_layout()     ","dd622e7c":"fig, (ax1, ax2,ax3,ax4) = plt.subplots(ncols=4, figsize=(40, 15))\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax1)\ntrain_df.groupby('Gender').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax1)\nax1.set_ylabel('Gender', size=15)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax2)\ntrain_df.groupby('Married').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax2)\nax2.set_ylabel('Married', size=15)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax3)\ntrain_df.groupby('Dependents').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'yellow'], radius=0.7,startangle=90,ax=ax3)\nax3.set_ylabel('Dependents', size=15)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax4)\ntrain_df.groupby('Education').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax4)\nax4.set_ylabel('Education', size=15)\n\nfig, (ax1, ax2,ax3) = plt.subplots(ncols=3, figsize=(20, 7))\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax1)\ntrain_df.groupby('Self_Employed').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax1)\nax1.set_ylabel('Self_Employed', size=15)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax2)\ntrain_df.groupby('Credit_History').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green'], radius=0.7,startangle=90,ax=ax2)\nax2.set_ylabel('Credit_History', size=15)\ntrain_df.groupby('Loan_Status').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['blue', 'orange'],ax=ax3)\ntrain_df.groupby('Property_Area').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12},colors=['red', 'green', 'violet', 'yellow'], radius=0.7,startangle=90,ax=ax3)\nax3.set_ylabel('Property_Area', size=15)\n\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","cc1950ff":"labels = ['vegetable', 'fruit']\nsizes = [300, 200]\nlabels_vegefruit = ['potato', 'tomato', 'onion', 'apple',\n                    'banana', 'cherry', 'durian']\nsizes_vegefruit = [170, 70, 60, 70, 60, 50, 20]\ncolors = ['#FFB600', '#09A0DA']\ncolors_vegefruit = ['#FFCE53', '#FFDA7E', '#FFE9B2', '#30B7EA',\n                    '#56C7F2','#7FD6F7', '#B3E7FB']\n \nbigger = plt.pie(sizes, labels=labels, colors=colors,\n                 startangle=90, frame=True)\nsmaller = plt.pie(sizes_vegefruit, labels=labels_vegefruit,\n                  colors=colors_vegefruit, radius=0.7,\n                  startangle=90, labeldistance=0.7)\ncentre_circle = plt.Circle((0, 0), 0.4, color='white', linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n        \nplt.axis('equal')\nplt.tight_layout()\n\nplt.show()","248954bb":"sns.set(font_scale=1)\ncontinous_var = ContinuousColsList\nplt.figure(figsize=(30, 50))\nfor i, col in enumerate(continous_var):\n    plt.subplot(9, 3, i*3+1)\n    plt.subplots_adjust(hspace =.25, wspace=.1)\n    plt.grid(True)\n    plt.title('Bivariate Analysis with Density Plot : '+col,fontsize=12, fontweight='bold')\n    ax=sns.kdeplot(train_df.loc[train_df[\"Loan_Status\"]=='N', col], label=\"N\", color = \"red\", fill=True)\n    ax=sns.kdeplot(train_df.loc[train_df[\"Loan_Status\"]=='Y', col], label=\"Y\",  color = \"green\", fill=True)\n    ax.legend(title=\"Loan Status\", loc='best') \n    plt.subplot(9, 3, i*3+2) \n    ax1=sns.boxplot(y = col, data = train_df, x=\"Loan_Status\", palette = [\"red\", \"green\"])\n    plt.title('Bivariate Analysis with Box Plot : '+col,fontsize=12, fontweight='bold')\n    plt.subplot(9, 3, i*3+3) \n    ax2=sns.histplot(train_df.loc[train_df[\"Loan_Status\"]=='N', col], label=\"N\", color = \"red\")\n    ax2=sns.histplot(train_df.loc[train_df[\"Loan_Status\"]=='Y', col], label=\"Y\",  color = \"green\")\n    ax2.legend(title=\"Loan Status\", loc='best') \n    plt.title('Bivariate Analysis with Histogram Plot : '+col,fontsize=12, fontweight='bold')","fcb2f088":"sns.pairplot(train_df,hue='Loan_Status')","abd96613":"corr = train_df.corr()\nsns.set(font_scale=1)\nsns.clustermap(corr, cmap='BuGn', vmax=.3, center=0,square=False, linewidths=.5,annot=True, cbar_kws={\"shrink\": .5},annot_kws={\"size\": 10})","42842188":"train_imp = train_df\ntrain_imp.drop('Loan_ID',  axis='columns', inplace=True)","bdcb3a2e":"plt.figure(figsize=(15,6))\nsns.heatmap(train_imp.isna().transpose(),cmap=\"BuGn\",cbar_kws={'label': 'Missing Data'})\nplt.title(\" Missing Values in the given training data\")\nsns.set(font_scale=1)","faec3d34":"cat_null =[ 'Gender', 'Married', 'Dependents',  'Self_Employed', 'Credit_History', 'Loan_Amount_Term']\ncon_null = ['ApplicantIncome','CoapplicantIncome','LoanAmount']","4efce7b1":"from sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n# Run the imputer with a simple Random Forest estimator\nimp = IterativeImputer(RandomForestRegressor(n_estimators=5), max_iter=5, random_state=1)\nto_train = con_null\n#perform filling\ntrain_imp[to_train] = pd.DataFrame(imp.fit_transform(train_imp[to_train]), columns=to_train)","1179c1e8":"# Imputer object using the mean strategy and \n# missing_values type for imputation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import  RandomForestClassifier\ntrain_imp[cat_null] = train_imp[cat_null].apply(lambda series: pd.Series(LabelEncoder().fit_transform(series[series.notnull()]),index=series[series.notnull()].index))\nimp_cat = IterativeImputer(estimator=RandomForestClassifier(),initial_strategy='most_frequent',max_iter=10, random_state=0)\ntrain_imp[cat_null] = imp_cat.fit_transform(train_imp[cat_null])","bea4c2b9":"plt.figure(figsize=(15,6))\nsns.heatmap(train_imp.isna().transpose(),cmap=\"YlGnBu\",cbar_kws={'label': 'Missing Data'})\nplt.title(\"After Handling Missing Values in the given training data\")\nsns.set(font_scale=1)","1ee9cd00":"for x in ['ApplicantIncome','CoapplicantIncome','LoanAmount']:\n    q75,q25 = np.percentile(train_imp.loc[:,x],[75,25])\n    intr_qr = q75-q25\n    max = q75+(1.5*intr_qr)\n    min = q25-(1.5*intr_qr)\n    train_imp.loc[train_imp[x] < min,x] = min\n    train_imp.loc[train_imp[x] > max,x] = max","eb31a855":"cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area','Loan_Amount_Term']\nnr_rows = 2\nnr_cols = 4\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):          \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(train_imp[cols[i]], hue=train_imp[\"Loan_Status\"], ax=ax,palette=['#347C17',\"#FF2400\"])\n        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n        ax.legend(title=\"Loan Status\", loc='best')    \n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \nplt.tight_layout()     ","4fb8b838":"a = 4  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\nfig = plt.figure(figsize=(25,20))\nfor i in ContinuousColsList:\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.histplot(x=train_imp[i],hue=train_imp['Loan_Status'],palette = [\"green\", \"red\"])\n    c = c + 1\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.boxplot(y = i, data = train_imp, x='Loan_Status', palette = [\"green\", \"red\"])\n    c = c + 1\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.kdeplot(data = train_imp, x = i, hue = 'Loan_Status',fill=True,palette = [\"green\", \"red\"])\n    c = c + 1\nplt.show()","11bb0e59":"train_imp.head()","9acf6f83":"train_enc = train_imp\n# Treating the binary nominal variables first\ntrain_enc['Loan_Status'].replace({'Y':1, 'N':0}, inplace=True)\n# Treating all the nominal variables at once using dummy variables\ntrain_enc=pd.get_dummies(train_enc)\ntrain_enc.head()","f4575c8e":"train_enc.columns","e40aaa40":"inp_cat = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']\ninp_con = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\nout_cat = ['Loan_Status']","1d2f81ad":"train_enc.head()","5afb75da":"train_features = train_enc\ny = train_features['Loan_Status']\nX = train_features.drop('Loan_Status',axis=1)","3df264c1":"# ANOVA feature selection for numeric input and categorical output\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nbestfeatures = SelectKBest(score_func=f_classif, k=14)\nfit = bestfeatures.fit(X, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \ndf_fclassif = pd.concat([dfcolumns,dfscores],axis=1)\ndf_fclassif.columns = ['Features','fclass_Score']  ## naming the dataframe columns\n#print(featureScores.nlargest(10,'Score'))  ## print 10 best features\nplt.barh(df_fclassif['Features'],df_fclassif['fclass_Score'],color='green')\nplt.ylabel('Features')\nplt.xlabel('Feature Score')\nplt.title('Feature Selection using KBest,F_Classif',fontsize=14, fontweight='bold')\nplt.show()","68382c16":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nbestfeatures = SelectKBest(score_func=chi2, k=14)\nfit = bestfeatures.fit(X, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \ndf_chi = pd.concat([dfcolumns,dfscores],axis=1)\ndf_chi.columns = ['Features','chi_Score']  ## naming the dataframe columns\n#print(featureScores.nlargest(10,'Score'))  ## print 10 best features\nplt.barh(df_chi['Features'],df_chi['chi_Score'],color='violet')\nplt.ylabel('Features')\nplt.xlabel('Feature Score')\nplt.title('Feature Selection using KBest,Chi_Score',fontsize=14, fontweight='bold')\nplt.show()","28397c72":"df_KBest=df_chi\ndf_KBest['F_classif_Score'] = df_fclassif['fclass_Score']\ndf_KBest.plot(x='Features',y=['chi_Score','F_classif_Score'],kind='barh')\nplt.xlabel('Feature Score')\nplt.title('Feature Selection using Chi_Score,F_classif',fontsize=14, fontweight='bold')\nplt.show()","3f358261":"from sklearn.feature_selection import SelectKBest, mutual_info_classif\nselector = SelectKBest(mutual_info_classif, k=14)\nselector.fit_transform(X, y)\ndfscores = pd.DataFrame(selector.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \ndf_mc_score = pd.concat([dfcolumns,dfscores],axis=1)\ndf_mc_score.columns = ['Features','mc_Score']  ## naming the dataframe columns\n#plt.barh(X.columns, model.feature_importances_,color='blue')\nplt.barh(df_mc_score['Features'],df_mc_score['mc_Score'],color='orange')\nplt.xlabel('Feature Score')\nplt.ylabel('Features')\nplt.title('Feature Selection using KBest,Mutual_Info_Classifier',fontsize=14, fontweight='bold')\nplt.show()","5f393605":"# Feature Importance with Extra Trees Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n# load data\n# feature extraction\nmodel = ExtraTreesClassifier(n_estimators=14)\nmodel.fit(X, y)\n# Normalizing the individual importances\n#feature_importance_normalized = np.std([model.feature_importances_ for tree in model.estimators_],axis = 0)\n# Plotting a Bar Graph to compare the models\ndfscores = pd.DataFrame(model.feature_importances_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \ndf_extra_score = pd.concat([dfcolumns,dfscores],axis=1)\ndf_extra_score.columns = ['Features','etree_imp']  ## naming the dataframe columns\n#plt.barh(X.columns, model.feature_importances_,color='blue')\nplt.barh(df_extra_score['Features'],df_extra_score['etree_imp'],color='blue')\nplt.xlabel('Feature Importance')\nplt.ylabel('Features')\nplt.title('Feature Importances using Extra Tree Classifier',fontsize=14, fontweight='bold')\nplt.show()","26b53972":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(X, y)\n# Plotting a Bar Graph to compare the models\ndfscores = pd.DataFrame(fit.ranking_)\ndfsupport = pd.DataFrame(fit.support_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \ndf_log_score = pd.concat([dfcolumns,dfscores,dfsupport],axis=1)\n#print(df_log_score.head())\ndf_log_score.columns = ['Features','rfe_rank','Support']  ## naming the dataframe columns\nplt.barh(df_log_score['Features'],df_log_score['rfe_rank'],color='teal')\nplt.xlabel('Feature Ranking')\nplt.ylabel('Features')\nplt.title('Feature Ranking using Recursive Feature Elimination',fontsize=14, fontweight='bold')\nplt.show()\nplt.barh(df_log_score['Features'],df_log_score['Support'],color='brown')\nplt.xlabel('Feature Support')\nplt.ylabel('Features')\nplt.title('Feature Support using Recursive Feature Elimination',fontsize=14, fontweight='bold')\nplt.show()","0c976209":"train_corr = train_enc.corr()['Loan_Status'].sort_values(ascending=True).head(14)\ntop_corr_features = train_corr.index\ntrain_corr.plot(kind='barh')\nplt.xlabel('Corelation Index')\nplt.ylabel('Features')\nplt.title('Feature Selection using Correlation Method',fontsize=14, fontweight='bold')\nplt.show()","ad045809":"plt.figure(figsize=(18,8))\nmatrix = np.triu(train_enc.corr())\nsns.heatmap(train_enc.corr(), annot=True, mask=matrix,cbar_kws= {'orientation': 'horizontal'} , vmin=-1, vmax=1, center= 0, cmap=\"YlGnBu\")\n#sns.heatmap(train_imp.isna().transpose(),cmap=\"YlGnBu\",cbar_kws={'label': 'Missing Data'})\nplt.title(\"Correlation of Features\")\nsns.set(font_scale=1)","d0567b39":"PlotBarCharts(inpData=train_enc, colsToPlot=['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban','Loan_Status'])","9ce93573":"PlotContinousCharts(inpData=train_enc, colsToPlot=ContinuousColsList)","d811c610":"cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Education_Graduate', 'Education_Not Graduate',\n       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban','Loan_Status']\nnr_rows = 3\nnr_cols = 4\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4.5,nr_rows*4))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):          \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(train_enc[cols[i]], hue=train_enc[\"Loan_Status\"], ax=ax,palette=['#FF2400',\"#347C17\"])\n        ax.set_title(cols[i], fontsize=12, fontweight='bold')\n        ax.legend(title=\"Loan Status\", loc='best')    \n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))        \nplt.tight_layout()   ","3ff01c37":"a = 4  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\nfig = plt.figure(figsize=(25,20))\nfor i in ContinuousColsList:\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.histplot(x=train_enc[i],hue=train_enc['Loan_Status'],palette = [\"red\", \"green\"])\n    c = c + 1\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.boxplot(y = i, data = train_enc, x='Loan_Status', palette = [\"red\", \"green\"])\n    c = c + 1\n    plt.subplot(a, b, c)\n    plt.xlabel(i)\n    sns.kdeplot(data = train_enc, x = i, hue = 'Loan_Status',fill=True,palette = [\"red\", \"green\"])\n    c = c + 1\nplt.show()","649a5cc4":"sns.pairplot(train_enc,hue='Loan_Status')","08f0fdd3":"corr = train_enc.corr()\nsns.set(font_scale=1)\nsns.clustermap(corr, cmap='Greens', vmax=.8, center=0,square=False, linewidths=.5,annot=True, cbar_kws={\"shrink\": .5},annot_kws={\"size\": 10},figsize=(17, 10))","464005c9":"#sklearn\nimport time\nfrom sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,learning_curve,ShuffleSplit\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve,f1_score,log_loss,brier_score_loss,fbeta_score\nfrom sklearn import svm,model_selection, tree, linear_model, naive_bayes, ensemble,gaussian_process","bf88e8a0":"print(train_enc.columns)","6fb31c3d":"# Separate Target Variable and Predictor Variables\nTargetVariable='Loan_Status'\nPredictors_All=['Gender', 'Married', 'Dependents', 'Self_Employed', 'ApplicantIncome','CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History','Education_Graduate', 'Education_Not Graduate',\n       'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']","d638cc86":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(n_estimators=75, random_state=7),\n    ensemble.BaggingClassifier(n_estimators=75, random_state=7),\n    ensemble.ExtraTreesClassifier(n_estimators=75, random_state=7),\n    ensemble.GradientBoostingClassifier(n_estimators=75, random_state=7),\n    ensemble.RandomForestClassifier(n_estimators=75, random_state=7),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    naive_bayes.MultinomialNB(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),   \n    ]","12bce0f1":"def Models(Xtrain,ytrain,xtest,ytest):\n  MLA_columns = []\n  MLA_compare = pd.DataFrame(columns = MLA_columns)\n  kfold = model_selection.KFold(n_splits=10, random_state=None)\n  rfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n  row_index = 0\n  for alg in MLA:    \n      predicted = alg.fit(X_train, y_train).predict(X_test)\n      fp, tp, th = roc_curve(y_test, predicted)\n      MLA_name = alg.__class__.__name__\n      MLA_compare.loc[row_index,'Model Name'] = MLA_name\n      MLA_compare.loc[row_index, 'Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n      MLA_compare.loc[row_index, 'Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n      MLA_compare.loc[row_index, 'Precision'] = round(precision_score(y_test, predicted),4)\n      MLA_compare.loc[row_index, 'Recall'] = round(recall_score(y_test, predicted),4)\n      MLA_compare.loc[row_index, 'AUC'] = auc(fp, tp)\n      MLA_compare.loc[row_index, 'f1_macro'] = round(f1_score(y_test, predicted, average = \"macro\"),4)\n      MLA_compare.loc[row_index, 'f1_micro'] = round(f1_score(y_test, predicted, average = \"micro\"),4)\n      MLA_compare.loc[row_index, 'f1_weighted'] = round(f1_score(y_test, predicted, average = \"weighted\"),4)\n      MLA_compare.loc[row_index, 'f_beta'] = round(fbeta_score(y_test, predicted, beta=4),4)\n      MLA_compare.loc[row_index, 'Logloss'] = round(log_loss(y_test, predicted),4)\n      MLA_compare.loc[row_index, 'Brier Score Loss'] = round(brier_score_loss(y_test, predicted),4)\n      row_index+=1\n  return MLA_compare","4a3bf086":"def PlotRocCurve(X_train,y_train,X_test,y_test):\n  index = 0\n  fig, ax = plt.subplots(1, figsize=(15, 8))\n  for alg in MLA:\n      predicted = alg.fit(X_train, y_train).predict(X_test)\n      fp, tp, th = roc_curve(y_test, predicted)\n      roc_auc_mla = auc(fp, tp)\n      MLA_name = alg.__class__.__name__\n      plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla)) \n      index=index+1\n\n  plt.title('ROC Curve comparison',fontsize=14, fontweight='bold')\n  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n  plt.plot([0,1],[0,1],'b--')\n  plt.xlim([0,1])\n  plt.grid(False)\n  plt.ylim([0,1])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')    \n  plt.show()","83a25494":"def PlotConfusionMatrix(X_test, y_test):\n  import sklearn\n  index = 0\n  for alg in MLA:\n      MLA_name = alg.__class__.__name__\n      sklearn.metrics.plot_confusion_matrix(alg, X_test, y_test, display_labels=['N', 'Y'],cmap=plt.cm.Blues)\n      plt.grid(False)\n      plt.title('Confusion Matrix : '+MLA_name,fontsize=14, fontweight='bold')\n      index+=1  \n  plt.show()","5cf7b969":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(25, 7))\n\n    axes[0].set_title('Learning Curve of the Model : '+title,fontsize=14, fontweight='bold')\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"darkgreen\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, '*-', color=\"darkred\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-',color='orange')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1,color='darkorange')\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the Model : \"+title,fontsize=14, fontweight='bold')\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-',color='darkblue')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,color='b')\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the Model : \"+title,fontsize=14, fontweight='bold')\n    return plt","7a84ecd5":"def plotperfcurves(X,y):\n  index = 0\n  for alg in MLA:\n      MLA_name = alg.__class__.__name__\n      estimator = alg\n      title = MLA_name\n      plot_learning_curve(estimator, title, X, y,  ylim=(0.4, 1.05),cv=5, n_jobs=4)\n      plt.show()","c2217240":"X = train_enc.drop(columns=['Loan_Status'])\ny = train_enc['Loan_Status']","e7d4db06":"# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","7b7edf66":"plotperfcurves(X,y)","fe41c91e":"All_Features = Models(X_train,y_train,X_test,y_test)\nAll_Features","3c54a1a8":"All_Features.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","183a6ad0":"All_Features.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","70aba350":"PlotRocCurve(X_train,y_train,X_test,y_test)","e72c51fd":"train= X_train\ntrain['Loan_Status']=y_train\ntest=X_test\ntest['Loan_Status']=y_test","38e44139":"X_train_new = train[['Gender', 'Married', 'Credit_History','Education_Graduate','Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\ny_train_new= y_train\nX_test_new = X_test[['Gender', 'Married', 'Credit_History','Education_Graduate', 'Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\ny_test_new =y_test","353b80c8":"y_red = train_features['Loan_Status']\nX_red = train_features[['Gender', 'Married', 'Credit_History','Education_Graduate','Property_Area_Rural', 'Property_Area_Semiurban','Property_Area_Urban']]\nX_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.20)","f5af0a93":"Reduced_Features = Models(X_train_new,y_train_new,X_test_new,y_test_new)\nReduced_Features","59d79fcf":"PlotRocCurve(X_train_red,y_train_red,X_test_red,y_test_red)","280ff168":"Reduced_Features.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","5d898e8c":"Reduced_Features.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","09c1c444":"plotperfcurves(X_red,y_red)","64401c29":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(X, y)\nX_train_os, X_test_os, y_train_os, y_test_os = train_test_split(x_smote, y_smote, test_size=0.20)","b68c4e58":"All_OverSampling = Models(X_train_os,y_train_os,X_test_os,y_test_os)\nAll_OverSampling","02c7e8f4":"PlotRocCurve(X_train_os,y_train_os,X_test_os,y_test_os)","eaea6edb":"All_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","63fd38df":"All_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","f8263012":"plotperfcurves(x_smote,y_smote)","97f5a2f4":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\n# fit predictor and target variable\nx_smote_red, y_smote_red = smote.fit_resample(X_red, y_red)\nX_train_red_os, X_test_red_os, y_train_red_os, y_test_red_os = train_test_split(x_smote_red, y_smote_red, test_size=0.20)","4a72f5f7":"Red_OverSampling = Models(X_train_red_os,y_train_red_os,X_test_red_os,y_test_red_os)\nRed_OverSampling","174fd028":"Red_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","8cd0e4df":"Red_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","9b1f708c":"plotperfcurves(x_smote_red,y_smote_red)","fec0d432":"PlotRocCurve(X_train_red_os,y_train_red_os,X_test_red_os,y_test_red_os)","4269a60f":"Features = [ 'Married', 'ApplicantIncome', 'LoanAmount','Credit_History','Education_Not Graduate','Property_Area_Rural', 'Property_Area_Semiurban']\nX_train_fs= X_train[Features]\ny_train_fs= y_train\nX_test_fs = X_test[Features]\ny_test_fs =y_test","a0931c27":"train_enc.Loan_Status.value_counts()","26328cd4":"y_fs = y\nX_fs = X[Features]\nX_train_red_fs, X_test_red_fs, y_train_red_fs, y_test_red_fs = train_test_split(X_fs, y_fs, test_size=0.20)","c5b6313a":"plotperfcurves(X_fs,y_fs)","76d766a1":"PlotRocCurve(X_train_red_fs,y_train_red_fs,X_test_fs,y_test_fs)","580d79aa":"Reduced_Fs = Models(X_train_red_fs,y_train_fs,X_test_fs,y_test_fs)\nReduced_Fs","7d9994e2":"Reduced_Fs.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","66f71a4d":"Reduced_Fs.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","9d155d85":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\n# fit predictor and target variable\nx_smote_red_fs, y_smote_red_fs = smote.fit_resample(X_fs, y_fs)\nX_train_red_fs_os, X_test_red_fs_os, y_train_red_fs_os, y_test_red_fs_os = train_test_split(x_smote_red_fs, y_smote_red_fs, test_size=0.20)","918eaca1":"Red_fs_OverSampling = Models(X_train_red_fs_os,y_train_red_fs_os,X_test_red_fs_os,y_test_red_fs_os)\nRed_fs_OverSampling","882bcae1":"Red_fs_OverSampling.plot(x=\"Model Name\",y=[\"f1_macro\",\"f1_micro\",\"f1_weighted\",\"f_beta\"],kind=\"bar\",figsize=(30, 7))\nplt.xticks(rotation=90)\nplt.title('Model Comparision with different f-scores',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","d48d36e9":"Red_fs_OverSampling.plot(x=\"Model Name\",y=[\"Logloss\"],kind=\"barh\",figsize=(10, 5))\nplt.xticks(rotation=360)\nplt.title('Log Loss for Different Models',fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","43b9c19f":"plotperfcurves(x_smote_red_fs,y_smote_red_fs)","e535ca8e":"PlotRocCurve(X_train_red_fs_os,y_train_red_fs_os,X_test_red_fs_os,y_test_red_fs_os)","1c1c1e79":"### 3.3.2 Correlation Plot with Clustermap","6eebb706":"## 2.2 Handling Outliers","c7fd78eb":"### **Output 5** - Reduced Features - Using Feature Selection\n","021e2af8":"## 2.3 Categorical variable with Target Value after handling outliers and missing data","6ec4b1c5":"# **3. Exploratory Data Analysis**","6750285e":"# **4. Creation & Evaluation of classifcation models**","3ce34f2b":"### Univariate Selection","3a9850c5":"#### Log Loss Chart","4255ec27":"# Problem Statement Definition","a3e34a08":"We can spot a categorical variable in the data by looking at the unique values in them. Typically a categorical variable contains less than 20 Unique values AND there is repetition of values, which means the data can be grouped by those unique values.\n\nBased on the Basic Data Exploration above, we have spotted 7 categorical predictors and 1 target variable in the data\n\n   ##### **Categorical Predictors:** 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area',Loan_Amount_Term\n   ##### **Target Variable** 'Loan_Status'\n\nWe use bar charts to see how the data is distributed for these categorical columns.","137fe7b2":"## 2.5 Encoding","68e62009":"### Feature Importance using Extra Tree Classifier\n","f1b57a3b":"Based on the problem statement you can understand that we need to create a **Supervised ML classification** model, as the target variable is categorical.","f4371fc2":"### Feature Ranking using Recursive Feature Elimination","15f4fe64":"## 3.3 Multivariate Analysis","338f04de":"### 3.2 Splitting to training and test data","6a681177":"### **Output 3** - With All Features - Oversampling","a669b2c4":"#### ROC Comparision Curve","d23ca654":"#### Classification Model Report","c259a834":"#### Define Classification Algorithms for Prediction","2340977c":"#### Log Loss Chart","f6ffacb9":"#### Log Loss Chart","8e619e5b":"## 2.1 Handling Missing values","e2ee000b":"### 2.1.3 For Categorical Data","0cde193b":"#  **1. Data Collection**","94e89a5f":"### 3.1. Preparing Data for Classification","f1be8a0e":"#### Importing Classification Model Libraries and Metrics","cc90fe7f":"####  Learning Curve, Scalability and Performance Plots","f6a29e2d":"### **Output 6** - Reduced Features using Feature Selection with Oversampling","cf0d5315":"### 3.1.2 Plots for Continuous Variables","8f6964e0":"#### F Score Bar Charts","5dd977eb":"#### **KBest with Mutual Information** \n\nWe can select features according to the k highest scores. Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","1cbb1649":"#### **KBest with f_classif** \n\nWe can select features according to the k highest scores. Along with ANOVA F-value Score between label\/feature.","c2c0ba06":"#### Define ROC Curve Plotting","91f9e179":"### 2.5.1 Encoding Categorical Variables","3a336eb4":"The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other\"","6363fa61":"#### Classification Model Report","785056b0":"This step is performed to guage the overall data. The volume of data, the types of columns present in the data. Initial assessment of the data should be done to identify which columns are Quantitative, Categorical or Qualitative.\n\nThis step helps to start the column rejection process. You must look at each column carefully and ask, does this column affect the values of the Target variable? For example in this case study, you will ask, does this column affect the loan status? If the answer is a clear \"No\", then remove the column immediately from the data, otherwise keep the column for further analysis.\n\nThere are some of the below commands which are used for Basic data exploration in Python\n\n     head(),tail() : This helps to see a few sample rows of the data\n     shape : This helps us to identify how many rows and columns present in dataset\n     info(),dtypes : This provides the summarized information of the data\n     describe() : This provides the descriptive statistical details of the data\n     nunique(): This helps us to identify if a column is categorical or continuous\n     isnull(): This helps us to identify how many are null values in a column\n     duplicated() : This helps us to identify if we have any duplicate rows present in data set.","3f52ba50":"### Create a Predictive model for Loan Status\n\n","16c43e65":"#### Pie Chart","a5ed81b0":"#### F Score Bar Charts","43b7430f":"### **Output 4** - With Reduced Features Oversampling","a06447e0":"#### F Score Bar Chart","27dd5987":"#### Learning Curve, Scalability and Performance Plots","02e91982":"#### Learning Curve, Scalability and Performance Plots","c96951b1":"### 2.1.4 Result : Heat Map after Handling Missing Values","1ad8ef20":"####  Learning Curve, Scalability and Performance Plots","091e99a1":"# **2. Data Preparation**","8f9511bb":"### **Output 1** - All Features considered in model building","085aeb73":"### 3.2.2 For Continuous Variable with Target Variable","1ab1b5c3":"## Define the Type of Machine Learning Problem","25b7c0d7":"#### Classification Model Report","18993599":"#### Define Plotting Confusion Matrix","be79adb9":"#### Define Dataframe for Different Performance Values for Machine Learning Models","7f273844":"## 1.3 Bivariate Analysis ","6e39480b":"#### **KBest with chi2** \n\nWe can select features according to the k highest scores. Chi-squared stats of non-negative features.","5ec923d7":"#### ROC Comparision Curve","1b63da16":"#### F Score Bar Charts","3f4a98a2":"## 1.1  Basic Understanding of Data","4da35efa":"### 1.4.2 Clustermap","dc7859d7":"#### Log Loss Chart","f1d877bd":"#### ROC Comparision Curve","684ba458":"#### Definining Predictors and Output","9abfa464":"##### Categorical variables: Bar plot\n##### Continuous variables: Histogram, Density & Boxplots","50a92e2d":"### **Output 2** - With Reduced Features considered in model building","be2933fb":"**chi2, f_classif, mutual_info_classif**\nThe methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.","1e771c90":"### 1.3.2 For Continuous Variable with Target Variable","d6538f7f":"## 3.2 Bivariate Analysis\n\n","02373532":"#### Classification Model Report","9d43e62d":"#### Defining Variables","1af14caf":"### 1.4.1 Pair Plot","5aff40fa":"#### ROC Curve Comparision","6772adbc":"### 1.2.2 For Continuous Variables\n\n**Continuous Variable Predictors** - 'ApplicantIncome','CoapplicantIncome','LoanAmount'","0594dc00":"#### ROC Comparision Curve","a9d38dd0":"## 3.1 Univariate Analysis","608f5f3a":"\nRecursive Feature Elimination, or RFE for short, is a popular feature selection algorithm.\n\nRFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable.","92e113a9":"####  Log Loss Bar Chart","f54ffc20":"### 1.2.1 For Categorical variables","df840b09":"## 2.4 Continuous variable with Target Value after handling outliers and missing data","494f4508":"### 3.1.1 Bar Chart for Categorical Variables","7f2f79ef":"#### Log Loss Chart","a71f6a91":"#### Classification Report","63ee5531":"#### F Score Bar Charts","c795288e":"### 3.2.1 For Categorical Variables with Target Variable","fe5d70b5":"### Feature Selection using Correlation Value","e38a22c0":"#### F Score Bar Charts","831bdc64":"## 2.6 Feature Selection\n","5d80230d":"### 2.1.1 Plotting Missing Values","d96c7d1e":"## 1.2 Univariate Analysis","ab8a6860":"#### Learning Curve, Scalability and Performance Plots","4f04668f":"#### Plotting Performance Curves","07cbee4f":"#### ROC Comparision Curve","aa789c49":"Outliers are extreme values in the data which are far away from most of the values. You can see them as the tails in the histogram.\n\nOutlier must be treated one column at a time. As the treatment will be slightly different for each column.Outliers bias the training of machine learning models. As the algorithm tries to fit the extreme value, it goes away from majority of the data.\n\nThere are below two options to treat outliers in the data.\n\n    Option-1: Delete the outlier Records. Only if there are just few rows lost.\n    Option-2: Impute the outlier values with a logical business value                                         \nBelow we are finding out the **Inter Quartile Range Method** outliers by looking at the histogram.","1d19ee3e":"#### 2.2.1 For Continuous Variables","49a33347":"Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it\u2019s classification result. In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n\nTo perform feature selection using the above forest structure, during the construction of the forest, for each feature, the normalized total reduction in the mathematical criteria used in the decision of feature of split (Gini Index if the Gini Index is used in the construction of the forest) is computed. This value is called the Gini Importance of the feature. To perform feature selection, each feature is ordered in descending order according to the Gini Importance of each feature and the user selects the top k features ","3d904b86":"#### Classification Model Report","d8b118b3":"#### Define Plotting of Learning Curves","d6858600":"List of steps performed on predictor variables before data can be used for machine learning\n1.   Converting each Ordinal Categorical columns to numeric\n2.   Converting Binary nominal Categorical columns to numeric using 1\/0 mapping\n3.  Converting all other nominal categorical columns to numeric using pd.get_dummies()\n\n\n","15eaf015":"**Target Variable** : Loan_Status                                               \n**Predictors** : Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,  Credit_History,Property_Area,Loan_Status","112ea519":"### 3.3.1 Pair Plot","5de5bf88":"\nNow its time to finally choose the best columns(Features) which are correlated to the Target variable. This can be done directly by measuring the correlation values or ANOVA\/Chi-Square tests.\n\nHowever, we have visualized the relation between the Target variable and each of the predictors to get a better sense of data in Bivariate Analysis.\n\nIn machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n\n\n\n> simplification of models to make them easier to interpret by researchers\/users\n\n\n> shorter training times\n\n\n> enhanced generalization by reducing overfitting","33cc890d":"### BinaryClass ROC Curve ","7dbd6d79":"From the above plots, it is proved that we have removed outliers by using IQR method.","85a1b826":"#### Learning Curve, Scalability and Performance Plots","076a75fa":"#### **Result of Basic Exploration of Data**\n#### Based on the basic exploration above, you can now create a simple report of the data, noting down your observations regarding each column. Hence, creating a initial roadmap for further analysis.\n\nThe selected columns in this are considered for further study and then a final list will be created\n\n1.   Gender -->  Nomianl Categorical Variable\n2.   Married --> Boolean Categorical Variable\n3.   Dependents --> Ordinal Categorical Variable\n4.   Education -->  Nominal Categorical Variable\n5.   Self_Employed --> Nominal Categorical Variable\n6.\t ApplicantIncome --> Continuous Variable \n7.\tCoapplicantIncome --> Continuous Variable\n8.\tLoanAmount --> Continuous Variable\n9.\tLoan_Amount_Term --> Continuous Variable \n10. Credit_History --> Categorical Boolean Variable \n11. Property_Area --> Categorical Nominal Variable","7161087f":"### 1.3.1 For Categorical Variables with Target Variable","99fb2521":"## 1.4 Multivariate Analysis","5bea2185":"### 2.1.2 For Continuous Data"}}