{"cell_type":{"990f2aa4":"code","65132b50":"code","65da0822":"code","32eec3ac":"code","5f002092":"code","ad08299f":"code","414526ae":"code","dc826d49":"code","363b3122":"code","f5136ead":"code","3aa17d7d":"code","aa5387f6":"code","2b65a8b1":"code","2f7d43ba":"code","b84b1fd0":"code","bfc6fed8":"code","e007f5aa":"code","0e5f7f6a":"code","c6d27e19":"code","66b520a0":"code","5937d223":"code","b566bbc2":"code","24830316":"code","746ffdcb":"code","458ad9b1":"code","c33bcbcf":"code","345de213":"code","b75efa15":"code","6f9d901f":"code","9cf0aa2b":"code","e4aecc3e":"code","93e6f70e":"code","29806e5a":"code","7298b05c":"code","ed1c394d":"code","e592a76c":"code","7f36a08d":"markdown","4aa54876":"markdown","8c96902e":"markdown","9d891868":"markdown","df7400ce":"markdown","1d9e4b18":"markdown","80b5aadb":"markdown","14c06593":"markdown","bd5f2699":"markdown","e9adae2c":"markdown","89f90543":"markdown","fc42a2c5":"markdown","b3cb7747":"markdown","ea305b8c":"markdown","e17947c8":"markdown","06b0ac91":"markdown","373b1f3c":"markdown","c3d7ec24":"markdown","98f1c4f2":"markdown","b46a81b6":"markdown","48d43a17":"markdown","245bc4c2":"markdown","88b6cbc5":"markdown","b61c270e":"markdown","702deeb1":"markdown","4a43ced6":"markdown","e7505cdc":"markdown","e459c987":"markdown","0aa348f0":"markdown","c7ea953a":"markdown"},"source":{"990f2aa4":"import os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport zlib\nimport itertools\nimport sklearn\nimport itertools\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport csv\nfrom tqdm import tqdm\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix\nimport keras\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import models, layers, optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.utils import class_weight\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom keras.models import Model\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom keras.applications.mobilenet import MobileNet\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom tensorflow.python.keras.preprocessing.image import load_img,ImageDataGenerator,img_to_array\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","65132b50":"imageSize=224\ntrain_dir = \"..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/\"\ntest_dir =  \"..\/input\/kermany2018\/oct2017\/OCT2017 \/test\/\"\nval_dir= '..\/input\/kermany2018\/OCT2017 \/val\/'\n# ['DME', 'CNV', 'NORMAL', '.DS_Store', 'DRUSEN']\nfrom tqdm import tqdm\ndef get_data(folder):\n    \"\"\"\n    Load the data and labels from the given folder.\n    \"\"\"\n    X = [] #input images\n    y = [] #output images\n    for folderName in os.listdir(folder):\n        if not folderName.startswith('.'):\n            if folderName in ['NORMAL']:\n                label = 0\n            elif folderName in ['CNV']:\n                label = 1\n            elif folderName in ['DME']:\n                label = 2\n            elif folderName in ['DRUSEN']:\n                label = 3\n            else:\n                label = 4\n            for image_filename in tqdm(os.listdir(folder + folderName)):\n                img_file = cv2.imread(folder + folderName + '\/' + image_filename)\n                if img_file is not None:\n                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n                    img_arr = np.asarray(img_file)\n                    X.append(img_arr)\n                    y.append(label)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X,y\nX_test, y_test= get_data(test_dir)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.2) \n# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny_trainHot = to_categorical(y_train, num_classes = 4)\ny_testHot = to_categorical(y_test, num_classes = 4)","65da0822":"image = cv2.imread('..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/CNV\/CNV-13823-3.jpeg',0)\nplt.subplot(121)\nplt.imshow(image, 'bone')\n_= plt.xlabel( 'CNV',fontsize=15)\nplt.subplot(122)\n_= plt.hist(image.ravel(), bins = 256, color = 'orange', )\n_= plt.title( 'Intensity Value' ,fontsize=15 )\n_= plt.ylabel( 'Count',fontsize=15)\n_= plt.xlabel( 'CNV',fontsize=15)\nplt.tight_layout()\nplt.show()\n\nimage = cv2.imread('..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/DME\/DME-30521-4.jpeg',0)\nplt.subplot(121)\nplt.imshow(image, 'bone')\n_= plt.xlabel( 'DME',fontsize=15)\nplt.subplot(122)\n_= plt.hist(image.ravel(), bins = 256, color = 'blue', )\n_= plt.title( 'Intensity Value' ,fontsize=15 )\n_= plt.ylabel( 'Count',fontsize=15)\n_= plt.xlabel( 'DME',fontsize=15)\nplt.tight_layout()\nplt.show()\n\nimage = cv2.imread('..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/DRUSEN\/DRUSEN-95633-4.jpeg',0)\nplt.subplot(121)\nplt.imshow(image, 'bone')\n_= plt.xlabel( 'DRUSEN',fontsize=15)\nplt.subplot(122)\n_= plt.hist(image.ravel(), bins = 256, color = 'green', )\n_= plt.title( 'Intensity Value' ,fontsize=15 )\n_= plt.ylabel( 'Count',fontsize=15)\n_= plt.xlabel( 'DRUSEN',fontsize=15)\nplt.tight_layout()\nplt.show()\n\nimage = cv2.imread('..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/NORMAL\/NORMAL-469935-33.jpeg',0)\nplt.subplot(121)\nplt.imshow(image, 'bone')\n_= plt.xlabel( 'NORMAL',fontsize=15)\nplt.subplot(122)\n_= plt.hist(image.ravel(), bins = 256, color = 'red', )\n_= plt.title( 'Intensity Value' ,fontsize=15 )\n_= plt.ylabel( 'Count',fontsize=15)\n_= plt.xlabel( 'NORMAL',fontsize=15)\nplt.tight_layout()\nplt.show()","32eec3ac":"# Plot each type of image in the dataset\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 10))\n\ndrusen = random.choice(os.listdir(train_dir + \"DRUSEN\"))\ndrusen_image = load_img(train_dir + \"DRUSEN\/\" + drusen)\nax[0].imshow(drusen_image)\nax[0].set_title(\"DRUSEN\")\nax[0].axis(\"Off\")\n\ndme = random.choice(os.listdir(train_dir + \"DME\"))\ndme_image = load_img(train_dir + \"DME\/\" + dme)\nax[1].imshow(dme_image)\nax[1].set_title(\"DME\")\nax[1].axis(\"Off\")\n\ncnv = random.choice(os.listdir(train_dir + \"CNV\"))\ncnv_image = load_img(train_dir + \"CNV\/\" + cnv)\nax[2].imshow(cnv_image)\nax[2].set_title(\"CNV\")\nax[2].axis(\"Off\")\n\nnormal = random.choice(os.listdir(train_dir + \"NORMAL\"))\nnormal_image = load_img(train_dir + \"NORMAL\/\" + normal)\nax[3].imshow(normal_image)\nax[3].set_title(\"NORMAL\")\nax[3].axis(\"Off\")\n\nplt.show()","5f002092":"num_classes = 4\nimg_rows, img_cols = 224, 224\nbatch_size = 16\n\nimport os \nTrainPath=\"..\/input\/kermany2018\/OCT2017 \/train\/\"\nTestPath=\"..\/input\/kermany2018\/OCT2017 \/test\/\"\nValidationPath=\"..\/input\/kermany2018\/OCT2017 \/val\/\"\n\nprint(TrainPath)\nprint(TestPath)\nprint(ValidationPath)","ad08299f":"myList = os.listdir(TrainPath)\nprint(\"Total Number of Classes Detected :\",len(myList))\nnoOfclasses= len(myList)","414526ae":"numofSamples=[]\nfor x in range(0,noOfclasses):\n  numofSamples.append(len(np.where(y_train==x)[0]))","dc826d49":"print(numofSamples)","363b3122":"train_datagen_shear = ImageDataGenerator(\n    shear_range=0.8,\n    )\n\n#Zoom\ntrain_datagen_zoom = ImageDataGenerator(\n    zoom_range=0.5,\n    horizontal_flip=True)\n\n#Flipping the Image \ntrain_datagen_flip = ImageDataGenerator(\n    horizontal_flip=True,\n    vertical_flip=True)","f5136ead":"def image_show(x):\n    for i in range(3):\n        # define subplot\n        plt.subplot(330 + 1 + i)\n        # generate batch of images\n        batch = x.next()\n        # convert to unsigned integers for viewing\n        image = batch[0].astype('uint8')\n        # plot raw pixel data\n        plt.imshow(image)\n        # show the figure\n    plt.show()","3aa17d7d":"image=load_img('..\/input\/kermany2018\/OCT2017 \/train\/DME\/DME-1072015-1.jpeg')\nf=img_to_array(image)\nsample=np.expand_dims(f,axis=0)\nprint('Image size',sample.shape)","aa5387f6":"print(\"Shear image\")\ntrain_generator_shear=train_datagen_shear.flow(sample,batch_size=1)\nshears=image_show(train_generator_shear)","2b65a8b1":"print(\"Zoom image\")\ntrain_generator_zoom=train_datagen_zoom.flow(sample,batch_size=1)\nshears=image_show(train_generator_zoom)","2f7d43ba":"print(\"FLIP image\")\ntrain_generator_flip=train_datagen_flip.flow(sample,batch_size=1)\nshears=image_show(train_generator_flip)","b84b1fd0":"trainClass=os.listdir(TrainPath)\nfor i in range(len(trainClass)):\n    imageInfile=[]\n    imageInfile=os.listdir(os.path.join(TrainPath,trainClass[i]))\n    print( f\"the numbers of images in {trainClass[i]} class : \" , len(imageInfile))\n    plt.figure(figsize=(20,20))\n    for j in range(5):\n        \n        plt.subplot(1,5,j+1)\n        image=plt.imread(os.path.join(os.path.join(TrainPath,trainClass[i]),imageInfile[j]))\n        plt.title(trainClass[i])\n        plt.imshow(image,cmap='bone')\n    print(image.shape)\n    plt.show()","bfc6fed8":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimageDelegate=ImageDataGenerator(\n    samplewise_center=True,\n    samplewise_std_normalization=True,\n    \n    )","e007f5aa":"ImageSize=224\ntrainGenerator=imageDelegate.flow_from_directory(\n    TrainPath,\n    batch_size=100,\n    target_size=(ImageSize,ImageSize)  \n)\n\ntestGenerator =imageDelegate.flow_from_directory(\n     TestPath,\n     batch_size= 10,\n     target_size=(ImageSize, ImageSize)\n)\n\nvalidationGenerator =imageDelegate.flow_from_directory(\n     ValidationPath,\n     batch_size=4,\n     target_size=(ImageSize, ImageSize)\n)","0e5f7f6a":"Labels={0:\"BME\",1:\"CNV\",2:\"DRUSEN\",3:\"NORMAL\"}\ndef GetLabel(key):\n    return Labels[key]\n\n\nprint(GetLabel(0))\nprint(GetLabel(1))\nprint(GetLabel(2))\nprint(GetLabel(3))","c6d27e19":"plt.figure(figsize=(20,20))\nfor i  in range(20):\n    plt.subplot(4,5,i+1)\n    plt.imshow(trainGenerator.__getitem__(0)[0][i])\n    plt.title(GetLabel(np.argmax(trainGenerator.__getitem__(0)[1][i])))","66b520a0":"trainGenerator.__getitem__(0)[0][1].shape\nimg_height=224\nimg_width=224\nbatch_size=3\ntrainGenerator.__getitem__(0)[0][1].shape","5937d223":"map_characters = {0: 'Normal', 1: 'CNV', 2: 'DME', 3: 'DRUSEN'}\ndict_characters=map_characters\nimport seaborn as sns\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\nplt.title('No of Images for each Class')","b566bbc2":"from keras.layers import Input","24830316":"# Defining ResNet model\ndef resnet_model():\n    img_in = Input(trainGenerator.__getitem__(0)[0][1].shape)              #input of model \n    model = ResNet50(include_top= False , # remove  the 3 fully-connected layers at the top of the network\n                weights='imagenet',      # pre train weight \n                input_tensor= img_in, \n                input_shape= trainGenerator.__getitem__(0)[0][1].shape,\n                pooling ='avg') \n    x = model.output  \n    predictions = Dense(4, activation=\"softmax\", name=\"predictions\")(x)    # fuly connected layer for predict class \n    model = Model(inputs=img_in, outputs=predictions)\n    return model","746ffdcb":"model = resnet_model()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n                           metrics = ['accuracy'])\nmodel.summary()","458ad9b1":"from tensorflow.keras.utils import plot_model\nimport matplotlib.image as mpimg\nplot_model(model, show_shapes=True, show_layer_names=True)","c33bcbcf":"history = model.fit_generator(\n    trainGenerator,\n    steps_per_epoch = (83484\/100),\n    epochs = 10,\n    validation_data = validationGenerator,\n    validation_steps = (32\/16),\n    verbose = 1)","345de213":"from keras.models import load_model\nmodel.save(\"ResNet50.h5\")","b75efa15":"test_steps_per_epoch = np.math.ceil(testGenerator.samples \/ testGenerator.batch_size)\n\npredictions = model.predict_generator(testGenerator, steps = test_steps_per_epoch)\n\npredicted_classes = np.argmax(predictions, axis=1)","6f9d901f":"true_classes = testGenerator.classes\nclass_labels = list(testGenerator.class_indices.keys())","9cf0aa2b":"report = sklearn.metrics.classification_report(true_classes, predicted_classes, target_names = class_labels)\nprint(report) ","e4aecc3e":"import sklearn.metrics\ncm = sklearn.metrics.confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize=(8,8))\nsns.heatmap(cm, fmt='.0f', annot=True, linewidths=0.2, linecolor='purple')\nplt.xlabel('predicted value')\nplt.ylabel('Truth value')\nplt.show()","93e6f70e":"FP = cm.sum(axis=0) - np.diag(cm)  \nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\nFP = FP.astype(float)\nFN = FN.astype(float)\nTP = TP.astype(float)\nTN = TN.astype(float)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP\/(TP+FN)\nprint(\"Sensitivity\/TPR = {}\".format(TPR))\n\n# Specificity or true negative rate\nTNR = TN\/(TN+FP) \nprint(\"Specificity\/TNR = {}\".format(TNR))\n\n# Precision or positive predictive value\nPPV = TP\/(TP+FP)\nprint(\"Precision\/PPV = {}\".format(PPV))\n\n# Negative predictive value\nNPV = TN\/(TN+FN)\nprint(\"Negative Predict Value = {}\".format(NPV))\n\n# Fall out or false positive rate\nFPR = FP\/(FP+TN)\nprint(\"False Positive Rate = {}\".format(FPR))\n\n# False negative rate\nFNR = FN\/(TP+FN)\nprint(\"False Negative Rate = {}\".format(FNR))\n\n# False discovery rate\nFDR = FP\/(TP+FP)\nprint(\"False discovery rate = {}\".format(FDR))\n\n# Overall accuracy\nACC = (TP+TN)\/(TP+FP+FN+TN)\nprint(\"Overall Accuracy = {}\".format(ACC))","29806e5a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nprint(\"- the Accuracy and Loss for ResNet50 Model With 10 Epochs\")\nplt.figure(figsize=(40,20))\n# summarize history for accuracy\nplt.subplot(5,5,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\n\n\n\n# summarize history for loss\nplt.subplot(5,5,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()","7298b05c":"model.evaluate(testGenerator)\nscore = model.evaluate(testGenerator, verbose=2)\nprint('\\nResNet - precision:', score[1], '\\n')","ed1c394d":"fig = plt.figure(figsize=(10,5))\n_ = plt.hist(predictions, bins=25)\nplt.xlabel(\"Prediction Probability\")","e592a76c":"sklearn.metrics.cohen_kappa_score(true_classes, predicted_classes, labels=None, weights=None, sample_weight=None)","7f36a08d":"* **Choroidal neovascular membranes (CNVM)** are new, damaging blood vessels that grow beneath the retina. They break through the barrier between the choroid and the retina. When they leak or bleed in the retina they cause vision loss.","4aa54876":"* We can also ZOOM our image in order to have a better resolution to the area of our interest","8c96902e":"We tried to gain more accuracy of the model using a ResNet50 and a VGG19. In addition, to do that we add the data augmentation, in order to avoid overfitting during the model training.","9d891868":"**Balanced evaluation**","df7400ce":"> In order to understand better the dataset analyzed, here we sum up the characteristic of our images","1d9e4b18":"* **DRUSEN**: Lumps of deposits under the RPE.","80b5aadb":"> *ResNet50 is a variant of ResNet model which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer. \nThe network has an image input size of 224-by-224, so that's why we have resize the images with this input shape (224, 224, 3)*","14c06593":"* Or we can simply FLIP the image","bd5f2699":"Inception v3 is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, dropouts, and fully connected layers. Batchnorm is used extensively throughout the model and applied to activation inputs. Loss is computed via Softmax. Inception work with Factorizing Convolutions to reduce the number of connections and parameters to learn. This will increase the speed and gives a good performance. Factorization Into Asymmetric Convolutions is also used in Inception which also helps to help to reduce the learning parameter. One 3\u00d71 convolution followed by one 1\u00d73 convolution replaces one 3\u00d73. In one 3 x 3 has a total of 9 parameter whereas, 3 x 1 + 1 x 3 has a total of 6 parameters so it will reduce by 33%. This method is less likely to overfit the model as you go mode deeper in the training.  With 42 layers deep it is  much more efficient than that of VGGNet.\n\n![inceptionv3.png](attachment:d577503d-8354-4df8-8b33-6889fd7aac2d.png)   ![inceptionv3 architecture.png](attachment:17de347c-db63-4f8a-bbc2-f8a1161997e7.png)","e9adae2c":"The main base element of ResNet is the residual block. As we go deeper into the network with a large number of layers, computation becomes more complex. These layers put on top of each other and every layer try to learn some underlying mapping of the desired function and instead of having these blocks, we try and fit a residual mapping. In ResNet, stacks all these blocks together very deeply. Another thing with this very deep architecture is that it is enabling up to 150 layers deep of this, and then what we do is we stack all these layers periodically. We also double the number of filters and downsample spatially using stride two. In the end, only fully connected layer 1000 to output classes. In ResNet, it uses Batch Normalization after every conv layer. The learning rate is 0.1 and is divided by 10 as validation error becomes constant. Moreover, batch-size is 256 and weight decay is 1e-5. The important part is there is no dropout is used in ResNet.\n\n![resnet1.jpeg](attachment:77ef5b23-5962-4004-af1d-5877f2bd5ad8.jpeg)   ![resnet architecture.jpeg](attachment:5d64233d-c768-4f86-aeeb-40504a0237e5.jpeg)","89f90543":"> So now we have the unbalanced classes and we are going to text our DNN with these numbers of samples for each class.","fc42a2c5":"> A confusion matrix is used to express the performance of a classification model on validation data.","b3cb7747":"**2. Exploring the data**","ea305b8c":"![](https:\/\/www.researchgate.net\/publication\/338603223\/figure\/fig1\/AS:847598516711425@1579094642237\/ResNet-50-architecture-26-shown-with-the-residual-units-the-size-of-the-filters-and.png)","e17947c8":"# Detect Retina Damage From OCT Images","06b0ac91":"*After explore dataset we can observe that these images are not in the same size and same scale. Hence, the dataset is not balance. Let's try to use the Data Augmentation to read, resize, rescale and balance dataset(but we not do this now)*","373b1f3c":"VGG16 is 16 layer architecture with a pair of convolution layers, poolings layer and at the end fully connected layer. VGG network consists of much deeper networks and with much smaller filters. Right now it had models with 16 to 19 layers variant of VGGNet. These models kept very small filters with 3 x 3 conv all the way, which is basically the smallest conv filter size that is looking at a little bit of the neighbouring pixels. And they just kept this very simple structure of 3 x 3 convs with the periodic pooling all the way through the network. VGG used small filters because of fewer parameters, it has smaller filters with more depth instead of having large filters. This architecture leads to obtain the same effective receptive field as if having one 7 x 7 convolutional layers. VGG architecture has the 16 total number of convolutional and fully connected layers and then 19 for VGG 19.\n\n![vggnet.png](attachment:880d7697-54a8-4896-9da9-528fc0efb1ec.png)   ![architecture vggnet.jpeg](attachment:0698f995-6c69-47e9-af2f-36018d809b9b.jpeg)","c3d7ec24":"* **Diabetic macular edema (DME)**: is a serious eye condition that affects people with diabetes (type 1 or type 2).","98f1c4f2":"**DATA AUGMENTATION**","b46a81b6":"* Shear Image : Shear tool is used to shift one part of an image, a layer, a selection or a path to a direction and the other part to the opposite direction.","48d43a17":"**Outline of the notebook**\n\n**1. Explanation of the dataset and the alternative nets used**","245bc4c2":"**Training set inbalance data**","88b6cbc5":"*So now let's observe how our images are shown by the dataset taking on example some images from each class*","b61c270e":"**Detect Retina Damage From OCT Images**\n\n\nThe main application of this transfer learning algorithm is the diagnosis of retinal OCT images. OCT imaging is now a standard of care for guiding the diagnosis and treatment of some of the leading causes of blindness worldwide: age-related macular degeneration (AMD) and diabetic macular edema. Almost 170 million individuals suffer from AMD worldwide, and each year, more than 200,000 people develop choroidal neovascularization, a severe blinding form of advanced AMD and nearly 750,000 individuals aged 40 or older suffer from diabetic macular edema (Varma et al., 2014), a vision-threatening form of diabetic retinopathy that involves the accumulation of fluid in the central retina (Varma et al., 2014).Optical coherence tomography is a non-contact, high-resolution, in vivo imaging modality. It produces cross-sectional tomographic images just like ultrasound. Decreased OCT image quality can be attributable to cataracts which block light, patient motion artifact, or any other media opacity. With OCT, your ophthalmologist can see each of the retina\u2019s distinctive layers. This allows your ophthalmologist to map and measure their thickness. These measurements help with diagnosis. They also provide treatment guidance for glaucoma and diseases of the retina. OCT provides a clear cross-sectional representation of the retinal pathology allowing visualization of individual retinal layers, which is impossible with clinical examination by the human eye or by color fundus photography. The advantage of transfer learning is that it allows one to train deep learning models using relatively small datasets. One advantage of training with the smaller dataset is that it would allow you to train and evaluate your model in a single run of a single kernel -- thereby promoting your work in an easily reproducible manner.\n\n\n![](https:\/\/eyeguru.org\/wp-content\/uploads\/2016\/06\/image15-768x378.png)\n\n*A retinal detachment is usually diagnosed clinically and with exam, but shallow macular detachments are sometimes hard to appreciate early on. If any doubt, a retinal OCT can demonstrate a detachment easily.*\n\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (NORMAL,CNV,DME,DRUSEN). There are 84,495 X-Ray images (JPEG) and 4 categories (NORMAL,CNV,DME,DRUSEN).\n\nImages are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n\nThis notebook uses data from http:\/\/www.cell.com\/cell\/fulltext\/S0092-8674(18)30154-5 and consists of 83484, 964, 32 images in the training, testing and validating sets respectively.\n\nThe original paper uses both VGG16 Convolutional Network and INCEPTION V3 Convolutional Network to do transfer learning and obtaining an accuracy rate of approximately 95%. \n","702deeb1":"> Let's visualize the segmentation of the images of our dataset. The min\/max pixel values are already scaled between 0 and 1","4a43ced6":"*So now let's see different kind of image size we can perform*","e7505cdc":"> As you can see we have extracted the Image Size in order to decide with which image size we are going to work during this notebook","e459c987":"**RESNET 50**","0aa348f0":"> The mispredictions are due to the unbalanced dataset used to train the model.","c7ea953a":"*So now we have resize with these characteristic the images:*"}}