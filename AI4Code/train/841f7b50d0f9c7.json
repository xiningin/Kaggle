{"cell_type":{"24344306":"code","b7cbf029":"code","1ab390b9":"code","923a6569":"code","63acebb0":"code","f60fdfd3":"code","9099ccea":"code","270d330d":"code","e0f64b12":"code","a0c069de":"code","3377813f":"code","7170d38e":"code","ddbbfdef":"code","28ce3fcb":"code","b050d266":"code","3b6ffe24":"code","3a5255f8":"code","5682ee26":"markdown","75ada5bb":"markdown","c05792aa":"markdown","5aca5211":"markdown","5beb077f":"markdown","afe9eb95":"markdown","a520adae":"markdown"},"source":{"24344306":"import os\nimport pandas as pd\nimport numpy as np\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\nimport seaborn as sns\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ninit_notebook_mode(connected=True) #do not miss this line\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b7cbf029":"questions = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/questions_only.csv', low_memory=False)","1ab390b9":"questions.head()","923a6569":"# Preparing a corpus for analysis and checking all entries\ncol = questions.columns[1:]\ncorpus=[]\na=[]\nfor q in col:\n        a=questions[q][0]\n        corpus.append(a)\n        \ncorpus","63acebb0":"TEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","f60fdfd3":"# removing common words and tokenizing\nstoplist = stopwords.words('english') + list(punctuation) + list(\"([)]?\") + [\")?\"]\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n\ndictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'kaggle_survey.dict'))  # store the dictionary, for future reference","9099ccea":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'kaggle_survey.mm'), corpus)  # store to disk, for later use","270d330d":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model","e0f64b12":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","a0c069de":"total_topics = 5","3377813f":"lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi","7170d38e":"#Show first n important word in the topics:\nlda.show_topics(total_topics,5)","ddbbfdef":"data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n#data_lda","28ce3fcb":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","b050d266":"df_lda","3b6ffe24":"g=sns.clustermap(df_lda.corr(), center=0, standard_scale=1, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(15, 15))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","3a5255f8":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","5682ee26":"## Kaggle ML & DS Survey\n\n## We always need to compute the similarity in meaning between texts.\n\nSearch engines need to model the relevance of a document to a query, beyond the overlap in words between the two. For instance, question-and-answer sites such as Quora or Stackoverflow need to determine whether a question has already been asked before.\n\nIn legal matters, text similarity task allow to mitigate risks on a new contract, based on the assumption that if a new contract is similar to a existent one that has been proved to be resilient, the risk of this new contract being the cause of financial loss is minimised. Here is the principle of Case Law principle. Automatic linking of related documents ensures that identical situations are treated similarly in every case. Text similarity foster fairness and equality. Precedence retrieval of legal documents is an information retrieval task to retrieve prior case documents that are related to a given case document.\n\nIn customer services, AI system should be able to understand semantically similar queries from users and provide a uniform response. The emphasis on semantic similarity aims to create a system that recognizes language and word patterns to craft responses that are similar to how a human conversation works. For example, if the user asks \u201cWhat has happened to my delivery?\u201d or \u201cWhat is wrong with my shipping?\u201d, the user will expect the same response.\n\n![](https:\/\/websitedesign9100.blogspot.com\/2019\/11\/blog-post.html)","75ada5bb":"# Creating a transformation","c05792aa":"**From now on, tfidf is treated as a read-only object that can be used to apply a transformation to a whole corpus:**","5aca5211":"The transformations are standard Python objects, typically initialized by means of a training corpus:\n\nDifferent transformations may require different initialization parameters; in case of TfIdf, the \u201ctraining\u201d consists simply of going through the supplied corpus once and computing document frequencies of all its features.\nTraining other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involved and,\nconsequently, takes much more time.","5beb077f":"# LDA:\n\nLatent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA\u2019s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).","afe9eb95":"## Note\nTransformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and\/or runtime exceptions.","a520adae":"In the previous cells, I created a corpus of documents represented as a stream of vectors. To continue, lets use that corpus, with the help of Gensim."}}