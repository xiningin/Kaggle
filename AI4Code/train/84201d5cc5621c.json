{"cell_type":{"abf2f1af":"code","f266c1d7":"code","2467d5ec":"code","647ebae8":"code","1490da34":"code","440c8303":"code","a0f1547e":"code","0f8acb02":"code","cc1c0bcc":"code","b8abcf38":"code","23f6342f":"code","cfb26a2c":"code","73a0ad69":"code","998a17b0":"code","bf3febae":"code","1d2f81d0":"code","97a28bcd":"code","f5c4b8eb":"code","966316ee":"code","b403f01f":"markdown","43aeec42":"markdown","b9696a99":"markdown","09bcd59f":"markdown","4f508685":"markdown","a0ab393c":"markdown","3b96df8f":"markdown","d106b964":"markdown"},"source":{"abf2f1af":"!pip3 install transformers==4.11.2","f266c1d7":"import os\nimport os.path as osp\n\nimport pandas as pd","2467d5ec":"INPUT_PATH = '..\/input\/chaii-hindi-and-tamil-question-answering\/'","647ebae8":"train = pd.read_csv(osp.join(INPUT_PATH, 'train.csv'))\ntest = pd.read_csv(osp.join(INPUT_PATH, 'test.csv'))\nsub = pd.read_csv(osp.join(INPUT_PATH, 'sample_submission.csv'))","1490da34":"train[train.language == 'tamil'].head()","440c8303":"train[train.language == 'hindi'].head()","a0f1547e":"tamil_context = train[train.language == 'tamil']['context'].str.cat(sep='\\n')\nhindi_context = train[train.language == 'hindi']['context'].str.cat(sep='\\n')","0f8acb02":"print(\n    '\\nlength of tamil characters : ', len(set(tamil_context)),\n    '\\nlength of hindi characters : ', len(set(hindi_context)),\n    '\\nlength of hindi & tamil characters : ', len(set(tamil_context) & set(hindi_context)),\n    '\\nlength of only tamil characters : ', len(set(tamil_context) - set(hindi_context)),\n    '\\nlength of only hindi characters : ', len(set(hindi_context) - set(tamil_context))\n)","cc1c0bcc":"with open(\"tamil.txt\", \"w\") as f:\n    print(tamil_context, file=f)\n\nwith open(\"hindi.txt\", \"w\") as f:\n    print(hindi_context, file=f)","b8abcf38":"from tokenizers import BertWordPieceTokenizer\n\nos.makedirs('vocab', exist_ok=True)\n\ndef train_tokenizer(language: str):\n    print(f'>>> Training {language}...')\n    tokenizer = BertWordPieceTokenizer(\n        clean_text=True,\n        handle_chinese_chars=True,\n        strip_accents=False, # Must be False if cased model\n        lowercase=False,\n        wordpieces_prefix=\"##\"\n    )\n\n    tokenizer.train(\n        files=[f'{language}.txt'],\n        limit_alphabet=6000,\n        min_frequency=5,\n        show_progress=True,\n        vocab_size=30000\n    )\n\n    tokenizer.save(f'vocab\/{language}', True)","23f6342f":"train_tokenizer('tamil')\ntrain_tokenizer('hindi')","cfb26a2c":"import json\n\ndef save_vocab(language : str):\n    vocab_path = f'vocab\/{language}'\n\n    vocab_txt_path = f'vocab\/{language}.txt'\n\n    f = open(vocab_txt_path, 'w' ,encoding='utf-8')\n\n    with open(vocab_path) as json_file:\n        json_data = json.load(json_file)\n\n        for item in json_data[\"model\"][\"vocab\"].keys():\n            f.write(item + '\\n')\n    \n        f.close()\n    \n    print(f'{language} token Example:\\n')\n    for i,j in list(json_data['model']['vocab'].items())[3000:3020]:\n        print(f'{i} => {j}')\n    print('\\n')","73a0ad69":"save_vocab(\"tamil\")\nsave_vocab(\"hindi\")","998a17b0":"from transformers import BertTokenizer\n\ndef test_tokenizer(\n    language : str, \n    sample : int = 2\n):\n    \"\"\"\n    sample : sample count of testset\n    \"\"\"\n    print(f'{language} testing...\\n')\n    vocab_txt_path = f\"vocab\/{language}.txt\"\n\n    tokenizer = BertTokenizer(vocab_file=vocab_txt_path, do_lower_case=False)\n\n    for i in range(sample):\n        test_str = train[train.language == language].iloc[i+100]['context']\n        test_str = test_str[:test_str.find('\\n') + 1]\n\n        print(f'{i+1}th  Test Sentence: ',test_str)\n\n        encoded_str = tokenizer.encode(test_str,add_special_tokens=False)\n        print(f'{i+1}th Sentence Encoding: ',encoded_str)\n\n        decoded_str = tokenizer.decode(encoded_str)\n        print(f'{i+1}th  Sentence Decoding: ',decoded_str, '\\n')\n    \n    print()\n    return tokenizer","bf3febae":"tamil_tokenizer = test_tokenizer('tamil')\nhindi_tokenizer = test_tokenizer('hindi')","1d2f81d0":"os.makedirs('tamil_checkpoint', exist_ok=True)\ntamil_tokenizer.save_pretrained('tamil_checkpoint')","97a28bcd":"os.makedirs('hindi_checkpoint', exist_ok=True)\nhindi_tokenizer.save_pretrained('hindi_checkpoint')","f5c4b8eb":"from transformers import BertTokenizer\n\ndef test_tokenizer_from_pretrained(\n    language : str, \n    sample : int = 2\n):\n    \"\"\"\n    sample : sample count of testset\n    \"\"\"\n    print(f'{language} testing...\\n')\n    vocab_txt_path = f\"vocab\/{language}.txt\"\n\n    tokenizer = BertTokenizer.from_pretrained(f'.\/{language}_checkpoint')\n\n    for i in range(sample):\n        test_str = train[train.language == language].iloc[i+100]['context']\n        test_str = test_str[:test_str.find('\\n') + 1]\n\n        print(f'{i+1}th  Test Sentence: ',test_str)\n\n        encoded_str = tokenizer.encode(test_str,add_special_tokens=False)\n        print(f'{i+1}th Sentence Encoding: ',encoded_str)\n\n        decoded_str = tokenizer.decode(encoded_str)\n        print(f'{i+1}th  Sentence Decoding: ',decoded_str, '\\n')\n    \n    print()","966316ee":"test_tokenizer_from_pretrained('tamil')\ntest_tokenizer_from_pretrained('hindi')","b403f01f":"# Test BertTokenizer","43aeec42":"# Train BertWordPieceTokenizer","b9696a99":"# Save context depending on language","09bcd59f":"# Save Checkpoints ","4f508685":"We wants to solve open-domain QA task.\n\nMy process is as follows:\n#### 0. [Orientation](https:\/\/www.kaggle.com\/adldotori\/notebook-to-read-before-start-nlp-step-0\/)\n#### 1. [Tokenization](https:\/\/www.kaggle.com\/adldotori\/tokenizing-hindi-and-tamil-language-nlp-step-1)\n   * ver 1 : init (2021\/10\/03)\n   * ver 2 : change transformer version (2021\/10\/03)\n   * ver 3 : update description (2021\/10\/05)\n\n#### 2. [Demo](https:\/\/www.kaggle.com\/adldotori\/demo-training-nlp-step-2\/)\n#### 3. Research QA Model\n#### 4. Training\n#### 5. Inference","a0ab393c":"We will use this tokenizer in the same way as above. Now let's do QA training using this tokenizer on the next notebook.","3b96df8f":"Since only 700 characters out of a total of 1400 characters overlap, two languages are separated during train tokenizers.","d106b964":"# RESULT"}}