{"cell_type":{"e8a069b2":"code","f7250993":"code","449cab20":"code","40e8fd83":"code","826e2b65":"code","204254f4":"code","5f32b36e":"code","513b1996":"code","62d86c0f":"code","19f9f82e":"code","4d965381":"code","ab8b967c":"code","4cbcc9c2":"code","cb62a8ae":"code","dfdcf194":"code","a78383fa":"code","0227834a":"code","57dceb99":"code","2c220c6a":"code","9b531953":"code","779bbae1":"code","fbe8b5e0":"code","600ab0c0":"code","872c771a":"code","5a2ba392":"code","172db33f":"code","a59ecdac":"code","b62ec025":"code","0692ca44":"code","b3a59602":"code","843b6909":"code","c2a29784":"code","75b4cd12":"code","e377022f":"code","05845200":"code","7b64169b":"code","0098c6c8":"code","cc5ab975":"code","af0393df":"code","36676d2e":"code","e7701884":"code","ba90c9e1":"code","7aacbfed":"code","b7f7fe69":"code","9c897b8c":"code","d71e27b5":"code","e04e8369":"code","bfd52218":"code","f77c886b":"code","0375886e":"code","fa67a455":"code","df90191a":"code","ec344e1c":"code","17f9f208":"code","34520ad1":"code","e9b2334b":"code","8c0a6385":"code","fb869dd0":"code","344626ec":"code","c579443e":"code","4de81b78":"code","f93b7900":"code","d00715a3":"code","a799d224":"code","9575e1fe":"code","4c1fe6c0":"code","2b4bcde6":"code","86147651":"code","2d3f5209":"markdown","dfbd4fd0":"markdown","ece75fb5":"markdown","9fc14da6":"markdown","82da3b55":"markdown","0cb2448e":"markdown","ba5fd9ac":"markdown","749a110c":"markdown","7f4eecbc":"markdown","840b46d4":"markdown","0c5ec45a":"markdown","6640e96f":"markdown","526963d6":"markdown","02734c25":"markdown","0585fc05":"markdown","d3de668b":"markdown"},"source":{"e8a069b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","f7250993":"# Load Data\nheart_data = pd.read_csv('..\/input\/heart.csv')","449cab20":"heart_data.head(10)","40e8fd83":"heart_data.describe()","826e2b65":"heart_data.isnull().any()","204254f4":"# No Null Values in any column","5f32b36e":"all_columns = heart_data.columns.values.tolist()\nnum_columns = ['age','trestbps','chol','thalach','oldpeak']\ncat_columns = [clm_name for clm_name in all_columns if clm_name not in num_columns]\nprint('Columns with continuous data : {} Count = {}\\nColumns with catagorical data : {} Count = {}'.format(num_columns,len(num_columns),cat_columns,len(cat_columns)))","513b1996":"heart_data[heart_data.duplicated() == True]","62d86c0f":"heart_data.drop_duplicates(inplace = True)\nheart_data[heart_data.duplicated() == True]","19f9f82e":"# Removed the duplicate row","4d965381":"# Sex distribution \n\nmale_count = heart_data.sex.value_counts().tolist()[0]\nfemale_count = heart_data.sex.value_counts().tolist()[1]\nprint('Male :',male_count)\nprint('Female :',female_count)","ab8b967c":"fig, (ax1,ax2) = plt.subplots(1,2,figsize = (12,5),constrained_layout=True)\nplt.subplots_adjust(wspace = 0.5)\n\nax1.bar(heart_data.sex.unique(),heart_data.sex.value_counts(),color = ['blue','red'],width = 0.8)\nax1.set_xticks(heart_data.sex.unique())\nax1.set_xticklabels(('Male','Female'))\n\nax2.pie((male_count,female_count), labels = ('Male','Female'), autopct='%1.1f%%', shadow=True, startangle=90, explode=[0,0.3])\n\nplt.show()","4cbcc9c2":"# Population Distribution with age and sex\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,5),constrained_layout=True)\nbin_x = range(25,80,2)\n\nax1.hist(heart_data.age.tolist(),bins=bin_x,rwidth=0.9)\nax1.set_xticks(range(25,80,2))\nax1.set_xlabel('Age',fontsize=15)\nax1.set_ylabel('Population Count',fontsize=15)\nax1.set_title('Total population distribution',fontsize=20)\n\nax2.hist(heart_data[heart_data['sex']==1].age.tolist(),label = 'Male',bins=bin_x,rwidth=0.9)\nax2.hist(heart_data[heart_data['sex']==0].age.tolist(),label = 'Female',bins=bin_x,rwidth=0.5)\nax2.legend()\nax2.set_xticks(range(25,80,2))\nax2.set_xlabel('Age',fontsize=15)\nax2.set_ylabel('Population Count',fontsize=15)\nax2.set_title('Male vs female',fontsize=20)\n\nplt.show()","cb62a8ae":"# Population distribution for heart disease\n\nx = heart_data.groupby(['age','target']).agg({'sex':'count'})\ny = heart_data.groupby(['age']).agg({'sex':'count'})\nz = (x.div(y, level='age') * 100)\nq= 100 - z\n\nfig, axes = plt.subplots(2,2, figsize = (20,12))\nplt.subplots_adjust(hspace = 0.5)\n\naxes[0,0].hist(heart_data[heart_data['target']==1].age.tolist(),bins=bin_x,rwidth=0.8)\naxes[0,0].set_xticks(range(25,80,2))\naxes[0,0].set_xlabel('Age Range',fontsize=15)\naxes[0,0].set_ylabel('Population Count',fontsize=15)\naxes[0,0].set_title('People suffering from heart disease',fontsize=20)\n\naxes[0,1].hist(heart_data[heart_data['target']==0].age.tolist(),bins=bin_x,rwidth=0.8)\naxes[0,1].set_xticks(range(25,80,2))\naxes[0,1].set_xlabel('Age Range',fontsize=15)\naxes[0,1].set_ylabel('Population Count',fontsize=15)\naxes[0,1].set_title('People not suffering from heart disease',fontsize=20)\n\naxes[1,0].scatter(z.xs(1,level=1).reset_index().age,z.xs(1,level=1).reset_index().sex,s=(x.xs(1,level=1).sex)*30,edgecolors = 'r',c = 'yellow')\naxes[1,0].plot(z.xs(1,level=1).reset_index().age,z.xs(1,level=1).reset_index().sex)\naxes[1,0].set_xticks(range(25,80,2))\naxes[1,0].set_yticks(range(0,110,5))\naxes[1,0].set_xlabel('Age',fontsize=15)\naxes[1,0].set_ylabel('%',fontsize=15)\naxes[1,0].set_title('% of people with heart disease by age',fontsize=20)\n\naxes[1,1].scatter(z.xs(1,level=1).reset_index().age,q.xs(1,level=1).reset_index().sex,s=(x.xs(0,level=1).sex)*30,edgecolors = 'r',c = 'yellow')\naxes[1,1].plot(z.xs(1,level=1).reset_index().age,q.xs(1,level=1).reset_index().sex)\naxes[1,1].set_xticks(range(25,80,2))\naxes[1,1].set_yticks(range(0,110,5))\naxes[1,1].set_xlabel('Age',fontsize=15)\naxes[1,1].set_ylabel('%',fontsize=15)\naxes[1,1].set_title('% of people with no heart disease by age',fontsize=20)\n\nplt.show()","dfdcf194":"# Looking at other features and how they are distributed.\n# Scatter plot for continuous data\n# Pie plot for catagorical data","a78383fa":"fig, axes = plt.subplots(6,2, figsize = (20,40))\nplt.subplots_adjust(hspace = 0.5)\n\naxes[0,0].scatter(heart_data[heart_data['target']==0][['age','thalach']].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','thalach']].sort_values(by = ['age']).thalach, c = 'g',label = 'target=0')\naxes[0,0].scatter(heart_data[heart_data['target']==1][['age','thalach']].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','thalach']].sort_values(by = ['age']).thalach, c = 'r',label = 'target=1')\naxes[0,0].set_title('thalach distribution',fontsize=20)\naxes[0,0].set_xticks(range(25,80,2))\naxes[0,0].set_xlabel('Age',fontsize=15)\naxes[0,0].set_ylabel('thalach',fontsize=15)\naxes[0,0].axhline(np.mean(heart_data['thalach']),xmin=0,xmax=1,linewidth=1, color='black',linestyle = '--')\naxes[0,0].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\naxes[0,0].legend()\n\naxes[0,1].scatter(heart_data[heart_data['target']==0][['age','trestbps']].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','trestbps']].sort_values(by = ['age']).trestbps, c = 'g',label = 'target=0')\naxes[0,1].scatter(heart_data[heart_data['target']==1][['age','trestbps']].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','trestbps']].sort_values(by = ['age']).trestbps, c = 'r',label = 'target=1')\naxes[0,1].set_title('trestbps distribution',fontsize=20)\naxes[0,1].set_xticks(range(25,80,2))\naxes[0,1].set_xlabel('Age',fontsize=15)\naxes[0,1].set_ylabel('trestbps',fontsize=15)\naxes[0,1].axhline(np.mean(heart_data['trestbps']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[0,1].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\n# heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age'])\naxes[1,0].scatter(heart_data[heart_data['target']==0][['age','chol',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','chol',]].sort_values(by = ['age']).chol,c = 'g',label = 'target=0')\naxes[1,0].scatter(heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','chol',]].sort_values(by = ['age']).chol,c = 'r',label = 'target=1')\naxes[1,0].set_title('chol distribution',fontsize=20)\naxes[1,0].set_xticks(range(25,80,2))\naxes[1,0].set_xlabel('Age',fontsize=15)\naxes[1,0].set_ylabel('chol',fontsize=15)\naxes[1,0].axhline(np.mean(heart_data['chol']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[1,0].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\naxes[1,1].scatter(heart_data[heart_data['target']==0][['age','oldpeak',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==0][['age','oldpeak',]].sort_values(by = ['age']).oldpeak,c = 'g',label = 'target=0')\naxes[1,1].scatter(heart_data[heart_data['target']==1][['age','oldpeak',]].sort_values(by = ['age']).age,heart_data[heart_data['target']==1][['age','oldpeak',]].sort_values(by = ['age']).oldpeak,c = 'r',label = 'target=1')\naxes[1,1].set_title('oldpeak distribution',fontsize=20)\naxes[1,1].set_xticks(range(25,80,2))\naxes[1,1].set_xlabel('Age',fontsize=15)\naxes[1,1].set_ylabel('oldpeak',fontsize=15)\naxes[1,1].axhline(np.mean(heart_data['oldpeak']),xmin=0,xmax=1,linewidth=1, color='r',linestyle = '--')\naxes[1,1].axvline(np.mean(heart_data['age']),ymin=0,ymax=1,linewidth=1, color='b',linestyle = '--')\n\nfbs_count = heart_data['fbs'].value_counts()\nlabels = [('fbs = '+ str(x)) for x in fbs_count.index]\naxes[2,0].pie(fbs_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[2,0].axis('equal')\naxes[2,0].set_title('fbs share',fontsize=15)\n\nrestecg_count = heart_data['restecg'].value_counts()\nlabels = [('restecg = '+ str(x)) for x in restecg_count.index]\naxes[2,1].pie(restecg_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45,explode = [0,0,0.5])\naxes[2,1].axis('equal')\naxes[2,1].set_title('restecg share',fontsize=15)\n\nexang_count = heart_data['exang'].value_counts()\nlabels = [('exang = '+ str(x)) for x in exang_count.index]\naxes[3,0].pie(exang_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[3,0].axis('equal')\naxes[3,0].set_title('exang share',fontsize=15)\n\nslope_count = heart_data['slope'].value_counts()\nlabels = [('slope = '+ str(x)) for x in slope_count.index]\naxes[3,1].pie(slope_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[3,1].axis('equal')\naxes[3,1].set_title('slope share',fontsize=15)\n\nca_count = heart_data['ca'].value_counts()\nlabels = [('ca = '+ str(x)) for x in ca_count.index]\naxes[4,0].pie(ca_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[4,0].axis('equal')\naxes[4,0].set_title('ca share',fontsize=15)\n\nthal_count = heart_data['thal'].value_counts()\nlabels = [('thal = '+ str(x)) for x in thal_count.index]\naxes[4,1].pie(thal_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[4,1].axis('equal')\naxes[4,1].set_title('thal share',fontsize=15)\n\ncp_count = heart_data['cp'].value_counts()\nlabels = [('cp = '+ str(x)) for x in cp_count.index]\naxes[5,0].pie(cp_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[5,0].axis('equal')\naxes[5,0].set_title('CP share',fontsize=15)\n\ntarget_count = heart_data['target'].value_counts()\nlabels = [('target = '+ str(x)) for x in target_count.index]\naxes[5,1].pie(target_count,labels = labels,autopct='%1.1f%%',shadow=True, startangle=45)\naxes[5,1].axis('equal')\naxes[5,1].set_title('target share',fontsize=15)\n\nplt.show()","0227834a":"#  Lets look at the correlation matrix and plot it using Pandas Style and Matplotlib\nheart_data.corr().round(decimals =2).style.background_gradient(cmap = 'Oranges')","57dceb99":"names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']\ncorrelations = heart_data.corr()\n# plot correlation matrix\nfig, ax = plt.subplots(1,1, figsize = (10,8),constrained_layout=True)\n\ncax = ax.matshow(correlations, vmin=-1, vmax=1,cmap = 'afmhot')\nfig.colorbar(cax)\nticks = np.arange(0,14,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nfor i in range(len(names)):\n    for j in range(len(names)):\n        text = ax.text(j, i, heart_data.corr().as_matrix(columns= None)[i, j].round(decimals =2),\n                       ha=\"center\", va=\"center\", color=\"black\")\nplt.show()","2c220c6a":"# Corelation with target\n\nx = heart_data.corr()\npd.DataFrame(x['target']).sort_values(by='target',ascending = False).style.background_gradient(cmap = 'Greens')","9b531953":"# Importing stuff\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler,RobustScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","779bbae1":"# We have already saved all the continous columns\n\nprint('Columns with continous data = ',num_columns)\n\nimport scipy.stats as stats\n\nfig, axes = plt.subplots(2,2, figsize = (15,12))\nplt.subplots_adjust(hspace = 0.2)\n\nh= np.sort(heart_data.thalach)\nfit = stats.norm.pdf(h, np.mean(h), np.std(h)) \naxes[0,0].plot(h,fit,'--')\naxes[0,0].hist(h,density=True) \naxes[0,0].set_title(\"thalach\")\naxes[0,0].set_ylabel('Density')\n\nh2= np.sort(heart_data.trestbps)\nfit2 = stats.norm.pdf(h2, np.mean(h2), np.std(h2)) \naxes[0,1].plot(h2,fit2,'--')\naxes[0,1].hist(h2,density=True) \naxes[0,1].set_title(\"trestbps\")\naxes[0,1].set_ylabel('Density')\n\nh3= np.sort(heart_data.chol)\nfit3 = stats.norm.pdf(h3, np.mean(h3), np.std(h3)) \naxes[1,0].plot(h3,fit3,'--')\naxes[1,0].hist(h3,density=True) \naxes[1,0].set_title(\"chol\")\naxes[1,0].set_ylabel('Density')\n\nh4= np.sort(heart_data.oldpeak)\nfit4 = stats.norm.pdf(h4, np.mean(h4), np.std(h4)) \naxes[1,1].plot(h4,fit4,'--')\naxes[1,1].hist(h4,density=True) \naxes[1,1].set_title(\"oldpeak\")\naxes[1,1].set_ylabel('Density')\n\nplt.show()\n\nprint(r\"Scaling them using MinMax Scaler\")","fbe8b5e0":"mm = MinMaxScaler()\n\nnum_data = heart_data[num_columns]\nnum_data_tf = mm.fit_transform(num_data)\nnum_data_tf","600ab0c0":"ohe = OneHotEncoder()\n\ncat_columns.remove('target')\nprint(cat_columns)\ncat_data = heart_data[cat_columns]\ncat_data_tf = ohe.fit_transform(cat_data).toarray()\n\nheart_data_tf = np.hstack([num_data_tf,cat_data_tf])\nheart_data_tf","872c771a":"# Anomoly detection with DBscan (eps value was set using trial and error)\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=2.05)\npred = dbscan.fit_predict(heart_data_tf)\ndbanom = heart_data[pred == -1]\n\ncolumns_continous_data =  ['trestbps', 'chol', 'thalach', 'oldpeak']\n\nfig, axes = plt.subplots(2,2, figsize = (10,8))\n\np =0\nfor i in range(0,2):\n    for j in range(0,2):\n        axes[i,j].scatter(heart_data.index,heart_data[columns_continous_data[p]])\n        axes[i,j].scatter(dbanom.index,dbanom[columns_continous_data[p]],c='r')\n        axes[i,j].set_title('Anomalies with DBSCAN'+\" | plot = \"+columns_continous_data[p])\n        p +=1\n        \nplt.show()","5a2ba392":"#  Anomoly detection with EE\nfrom sklearn.covariance import EllipticEnvelope\nee = EllipticEnvelope(contamination=.03)\nee.fit_predict(heart_data_tf)\neeanom = heart_data[ee.predict(heart_data_tf) == -1]\n\nfig, axes = plt.subplots(2,2, figsize = (10,8))\n\np =0\nfor i in range(0,2):\n    for j in range(0,2):\n        axes[i,j].scatter(heart_data.index,heart_data[columns_continous_data[p]])\n        axes[i,j].scatter(eeanom.index,eeanom[columns_continous_data[p]],c='r')\n        axes[i,j].set_title('Anomalies with DBSCAN'+\" | plot = \"+columns_continous_data[p])\n        p +=1\n\nplt.show()","172db33f":"#Checking for similar Anomolies in both above methods and removing them from dataset\ndf = pd.DataFrame\ndf= dbanom.index.intersection(eeanom.index)\nheart_data_tf_df = pd.DataFrame(heart_data_tf)\nheart_data_tf_df.drop(df,inplace = True)\nheart_data.drop(df,inplace = True)","a59ecdac":"heart_data.info()","b62ec025":"heart_data_tf_df.head()","0692ca44":"heart_data.head()","b3a59602":"# Set X as feature data and Y as target data for Unscaled Data. set X_tf as feature data for scaled data.\nX = heart_data.drop(['target'],axis =1)\nY = heart_data.target\n\nX_tf = heart_data_tf_df","843b6909":"from sklearn import feature_selection\nchi2, pval = feature_selection.chi2(X,Y)\nprint(chi2)","c2a29784":"print(X.columns)","75b4cd12":"dep = pd.DataFrame(chi2)\ndep.columns = ['Dependency']\ndep.index = X.columns\nprint(\"\"\"Looks like \"fbs\" has lowest effect on target and \"thalach\" has highest\"\"\")\ndep.sort_values('Dependency', ascending = False).style.background_gradient(cmap = 'terrain')","e377022f":"# Since the number of columns are not much and all features looks important, we can go ahead with same number of columns. ","05845200":"Y.value_counts()","7b64169b":"# Target is not much imbalanced and there is no need to balance it, but will use oversampling, SMOTE method to balance data.","0098c6c8":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nX_reshaped, Y_reshaped = SMOTE().fit_sample(X, Y)","cc5ab975":"import collections\ncollections.Counter(Y_reshaped)\n# Balanced the classes, but will not use it in predictions","af0393df":"print(\"Split the data into train and test with unscaled data:\")\ntrainX,testX,trainY,testY = train_test_split(X,Y,test_size = 0.3,random_state = None)\nprint(\"trainX,testX,trainY,testY\")\nprint(\"Split the data into train and test with un scaled data:\")\ntrainX_tf,testX_tf,trainY_tf,testY_tf = train_test_split(X_tf,Y,test_size = 0.3,random_state = None)\nprint(\"trainX_tf,testX_tf,trainY_tf,testY_tf\")","36676d2e":"rf = RandomForestClassifier()","e7701884":"#Using grid search to get best params for Randomforest\nparams = {\n    'n_estimators':[10,50,100,150,200,250],\n    'random_state': [10,5,15,20,50]\n         }\ngs = GridSearchCV(rf, param_grid=params, cv=5, n_jobs=-1)\ngs.fit(trainX,trainY)","ba90c9e1":"n_est = []\nrnd_sta = []\nscore = []\nrand_state_list =  [5,10,15,20,50]\nfor x in range(len(gs.cv_results_['params'])):\n    n_est.append(gs.cv_results_['params'][x]['n_estimators'])\n    rnd_sta.append(gs.cv_results_['params'][x]['random_state'])\n    score.append(gs.cv_results_['mean_test_score'][x])\n\ngrid_frame = pd.DataFrame()\ngrid_frame['n_est'] = n_est\ngrid_frame['rnd_sta'] = rnd_sta\ngrid_frame['score'] = score\n\ngrid_frame[grid_frame['rnd_sta'] == 10]\n\nplt.figure(figsize=(10,6))\n\nfor value in rand_state_list:\n    plt.plot(grid_frame[grid_frame['rnd_sta'] == value].n_est,grid_frame[grid_frame['rnd_sta'] == value].score,'-o',label = 'random_state = value')\n\nplt.title('Mean Score with different params')\nplt.grid(True)\nplt.legend()\nplt.show()","7aacbfed":"# Grid Search Score with test Data\nprint(\"Grid search score with random forest classifier = \",gs.score(testX,testY)*100)","b7f7fe69":"#Best Params\ngs.best_params_","9c897b8c":"# Creating the Confusion matrix\npred = gs.predict(testX)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_pred=pred, y_true=testY)","d71e27b5":"# Using grid search to get best params for Randomforest . Now with scaled data (StandardScaler)\nparams = {\n    'n_estimators':[10,50,100,150,200,250,300],\n    'random_state': [10,5,15,20,50]\n         }\ngs = GridSearchCV(rf, param_grid=params, cv=10, n_jobs=-1)\ngs.fit(trainX_tf,trainY_tf)","e04e8369":"n_est = []\nrnd_sta = []\nscore = []\nfor x in range(len(gs.cv_results_['params'])):\n    n_est.append(gs.cv_results_['params'][x]['n_estimators'])\n    rnd_sta.append(gs.cv_results_['params'][x]['random_state'])\n    score.append(gs.cv_results_['mean_test_score'][x])\n\ngrid_frame = pd.DataFrame()\ngrid_frame['n_est'] = n_est\ngrid_frame['rnd_sta'] = rnd_sta\ngrid_frame['score'] = score\n\ngrid_frame[grid_frame['rnd_sta'] == 10]\n\nplt.figure(figsize=(10,6))\n\nfor value in rand_state_list:\n    plt.plot(grid_frame[grid_frame['rnd_sta'] == value].n_est,grid_frame[grid_frame['rnd_sta'] == value].score,'-o',label = 'random_state = value')\n\nplt.title('Mean Score with different params')\nplt.grid(True)\nplt.legend()\nplt.show()","bfd52218":"# Grid Search Score wih scaled test data \nprint(\"Grid search score with random forest classifier (Scaled Data)= \",gs.score(testX_tf,testY_tf)*100)","f77c886b":"#Best Params (We save best params for future use)\nprint(gs.best_params_)\nn_est = gs.best_params_['n_estimators']\nrnd_st = gs.best_params_['random_state']","0375886e":"# Creating the Confusion matrix\npred = gs.predict(testX_tf)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_pred=pred, y_true=testY_tf)","fa67a455":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier,RandomForestClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","df90191a":"models = [SVC(kernel='linear',C =100),\n          SGDClassifier(max_iter=1000,tol=0.003),\n          DecisionTreeClassifier(),\n          ExtraTreeClassifier(),\n          AdaBoostClassifier(), \n          BaggingClassifier(), \n          GradientBoostingClassifier(),\n          RandomForestClassifier(n_estimators=n_est,random_state=rnd_st),\n          GaussianNB(),\n          KNeighborsClassifier(), \n          LogisticRegression(max_iter=1000,solver='lbfgs')]\n\nmodelnames = ['SVC',\n              'SGDClassifier',\n              'DecisionTreeClassifier',\n              'ExtraTreeClassifier',\n              'AdaBoostClassifier', \n              'BaggingClassifier', \n              'GradientBoostingClassifier',\n              'RandomForestClassifier',\n              'GaussianNB',\n              'KNeighborsClassifier', \n              'LogisticRegression']","ec344e1c":"# Train and test with unscaled model\nscores_unscaled = []\nfor index,model in enumerate(models):\n    try:\n        model.fit(trainX,trainY)\n        print(modelnames[index],\"Accuracy =\",round(model.score(testX,testY)*100,2),\"%\")\n        scores_unscaled.append(round(model.score(testX,testY)*100,2))\n    except:\n        print(\"Skipped\",modelnames[index])","17f9f208":"# plt.bar(range(len(modelnames)),scores_unscaled, color = ['blue','red'])\nplt.plot(range(len(modelnames)),scores_unscaled, '-o')\n\nplt.xticks(range(0,11,1),labels = modelnames, rotation = 90)\nplt.grid(visible=True)\n\nplt.show()","34520ad1":"# Train and test with scaled model\nscores_unscaled = []\nfor index,model in enumerate(models):\n    try:\n        model.fit(trainX_tf,trainY_tf)\n        print(modelnames[index],\"Accuracy =\",round(model.score(testX_tf,testY_tf)*100,2),\"%\")\n        scores_unscaled.append(round(model.score(testX_tf,testY_tf)*100,2))\n    except:\n        print(\"Skipped\",modelnames[index])","e9b2334b":"plt.plot(range(len(modelnames)),scores_unscaled, '-o')\n\nplt.xticks(range(0,11,1),labels = modelnames, rotation = 90)\nplt.grid(visible=True)\n\nplt.show()","8c0a6385":"#  base_estimator=DecisionTreeClassifier\nab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=1000)\nab.fit(trainX,trainY)\nprint('AdaBoost Accuracy with Decision Tree = ',(ab.score(testX,testY)*100))\n\nab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=1000)\nab.fit(trainX_tf,trainY_tf)\nprint('AdaBoost Accuracy with Decision Tree (Scaled Data)= ',(ab.score(testX_tf,testY_tf)*100))","fb869dd0":"#  base_estimator=RandomForest\nab = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=1000,random_state=10),n_estimators=1000)\nab.fit(trainX,trainY)\nprint('AdaBoost Accuracy with Random Forest = ',(ab.score(testX,testY)*100))\n\nab = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=1000,random_state=10),n_estimators=1000)\nab.fit(trainX_tf,trainY_tf)\nprint('AdaBoost Accuracy with Random Forest (Scaled Data)= ',(ab.score(testX_tf,testY_tf)*100))","344626ec":"#  base_estimator=LogisticRegression\nab = AdaBoostClassifier(base_estimator=LogisticRegression(max_iter=1000,solver = 'lbfgs'),n_estimators=1000)\nab.fit(trainX,trainY)\nprint('AdaBoost Accuracy with Logistic Reg = ',(ab.score(testX,testY)*100))\n\nab = AdaBoostClassifier(base_estimator=LogisticRegression(max_iter=1000,solver = 'lbfgs'),n_estimators=1000)\nab.fit(trainX_tf,trainY_tf)\nprint('AdaBoost Accuracy with Logistic Reg (Scaled Data)= ',(ab.score(testX_tf,testY_tf)*100))","c579443e":"#  base_estimator=SVC\nab = AdaBoostClassifier(algorithm='SAMME',base_estimator=SVC(kernel='linear',C = 1000, gamma=1),n_estimators=1000)\nab.fit(trainX,trainY)\nprint('AdaBoost Accuracy with SVC = ',(ab.score(testX,testY)*100))\n\nab = AdaBoostClassifier(algorithm='SAMME',base_estimator=SVC(kernel='linear',C = 1000, gamma=1),n_estimators=1000)\nab.fit(trainX_tf,trainY_tf)\nprint('AdaBoost Accuracy with SVC = (Scaled Data)',(ab.score(testX_tf,testY_tf)*100))","4de81b78":"estimators = [ \n    ('RandomForestClassifier',RandomForestClassifier(n_estimators=n_est, random_state=rnd_st)),\n    ('SVC(kernel= rfb',SVC(kernel='rbf', probability=True,gamma=1)),\n    ('SVC(kernel= linear',SVC(kernel='linear',C = 100, gamma=1, probability=True)),\n    ('KNeighborsClassifier',KNeighborsClassifier()),\n    ('AdaBoostClassifier LR',AdaBoostClassifier(base_estimator=LogisticRegression(solver = 'lbfgs'),n_estimators=1000)),\n    ('LogisticRegression',LogisticRegression(max_iter= 10000,solver = 'lbfgs')),\n    ('GaussianNB',GaussianNB())\n]","f93b7900":"vc = VotingClassifier(estimators=estimators, voting='hard')","d00715a3":"vc.fit(trainX,trainY)\nprint('Voting Classifier accuracy = ',(vc.score(testX,testY)*100))","a799d224":"# Checking who contribute what % in voting","9575e1fe":"weights = []\nfor est,name in zip(vc.estimators_,vc.estimators):\n    score = est.score(testX,testY)\n    print (name[0], score*100)\n    weights.append((100\/(10-(score*10))))\n# Converting n saving Score in weights to be used later\nprint('Weights = ', weights)","4c1fe6c0":"# Adjusting weights and recalculating accuracy\nvc = VotingClassifier(estimators=estimators, voting='soft', weights=weights)\nvc.fit(trainX,trainY)\nprint('Voting Classifier accuracy = ',(vc.score(testX,testY)*100))","2b4bcde6":"vc.fit(trainX_tf,trainY_tf)\nprint('Voting Classifier accuracy = ',(vc.score(testX_tf,testY_tf)*100))","86147651":"weights = []\nfor est,name in zip(vc.estimators_,vc.estimators):\n    score = est.score(testX_tf,testY_tf)\n    print (name[0], score*100)\n    weights.append((100\/(10-(score*10))))\n# Converting n saving Score in weights to be used later\nprint('Weights = ', weights)","2d3f5209":"# Lets work on AdaBoost and try to use other base estimators and check if we can improve score further.","dfbd4fd0":"# Anomoly detection with combination of two methods. Just to be sure we are not removing more relevant data.","ece75fb5":"# Lets use Voting classifier to calculate a robust score. (Unscaled Data)","9fc14da6":"# Voting classifier to calculate a robust score. (Scaled Data)","82da3b55":"# Now Using other Methods including Random Forest, with Unscalled and scaled data.","0cb2448e":"# Checking data for null values","ba5fd9ac":"# Analysis :\n> * **Data has lot more entries for Male compare to Female**\n> * **Majority of people suffering from heart disease lies between age 40 to 65**\n> * **Proability of getting heart disease starts reduce significiently after age of 60**\n> * **People from age 37 to 59 has highest chance of getting heart disease by volume**","749a110c":"# One Hot encoding all catagorical columns","7f4eecbc":"# Chi-Square for Non-negative feature & class, feature selection method to check dependency among feature & target","840b46d4":"# First Look at Data","0c5ec45a":"# Check for target imbalance","6640e96f":"# Analyzing data ","526963d6":"# Using Random Forest Classifier with unscalled data and Grid Search CV","02734c25":"# Pre-Processing\n* Scaling the Data before doing anomoly detection\n* As anomoly detection methods works better with scaled data, but there is no compulsory need to do so.\n* Scale only continious data","0585fc05":"# Column details\n\n* age : age in years\n* sex : (1 = male; 0 = female)\n* cp : chest pain type\n* trestbps : resting blood pressure (in mm Hg on admission to the hospital)\n* chol : serum cholestoral in mg\/dl\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg : resting electrocardiographic results\n* thalach : maximum heart rate achieved\n* exang : exercise induced angina (1 = yes; 0 = no)\n* oldpeak : ST depression induced by exercise relative to rest\n* slope : the slope of the peak exercise ST segment\n* ca : number of major vessels (0-3) colored by flourosopy\n* thal : 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target : 1 or 0","d3de668b":"# Now checking for duplicates and removing them (if any)"}}