{"cell_type":{"fbe0d5b1":"code","08d7a87c":"code","67a732b9":"code","f3e50518":"code","a3290cb9":"code","83bfff51":"code","035143e9":"code","505c9e0f":"code","491dca94":"code","8a1775d2":"code","b0bbfbd8":"code","acb3c847":"markdown","f6587b42":"markdown","470721a5":"markdown","17d0395e":"markdown","6b32a404":"markdown","1638ec30":"markdown"},"source":{"fbe0d5b1":"print('What tables do I have?')\nimport pandas as pd\nfrom bq_helper import BigQueryHelper\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"google_analytics_sample\")\nbq_assistant.list_tables()[:5]","08d7a87c":"QUERY = \"\"\"\n    SELECT \n        *  -- Warning, be careful when doing SELECT ALL\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20160801` \n    LIMIT 10\n\"\"\"\n\ndf = bq_assistant.query_to_pandas(QUERY)\ndf.head()","67a732b9":"QUERY = \"\"\"\n  SELECT \n      fullVisitorId, \n      TIMESTAMP_SECONDS(visitStartTime) AS visitStartTime,\n      TIMESTAMP_ADD(TIMESTAMP_SECONDS(visitStartTime), INTERVAL hits.time MILLISECOND) AS hitTime,\n      hits.page.pagePath,\n      hits.type\n  FROM \n      `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, \n      unnest(hits) as hits\n  WHERE fullVisitorId = '0509972280802528263' AND\n      _TABLE_SUFFIX BETWEEN '20170801' AND '20170801'\n  ORDER BY hitTime \n  LIMIT 50\n  \"\"\"\ndf = bq_assistant.query_to_pandas(QUERY)\ndf","f3e50518":"QUERY = \"\"\"  \n  SELECT \n      fullVisitorId, \n      TIMESTAMP_SECONDS(visitStartTime) AS visitStartTime,\n      TIMESTAMP_ADD(TIMESTAMP_SECONDS(visitStartTime), INTERVAL hits.time MILLISECOND) AS hitTime,\n      hits.page.pagePath,\n      hits.type\n  FROM \n      `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, \n      unnest(hits) as hits\n  WHERE _TABLE_SUFFIX BETWEEN '20170801' AND '20170801'\n  ORDER BY fullVisitorId, hitTime \n  LIMIT 500\n  \"\"\"\ndf = bq_assistant.query_to_pandas(QUERY)\ndf","a3290cb9":"QUERY = \"\"\"\n  WITH user_hit AS\n  (SELECT \n      fullVisitorId, \n      TIMESTAMP_SECONDS(visitStartTime) AS visitStartTime,\n      TIMESTAMP_ADD(TIMESTAMP_SECONDS(visitStartTime), INTERVAL hits.time MILLISECOND) AS hitTime,\n      hits.page.pagePath,\n      REGEXP_EXTRACT(hits.page.pagePath,r\"(?:\\\/[^\/?&#]*){0,1}\") AS pathCategory\n  FROM \n      `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, \n      unnest(hits) as hits\n  WHERE _TABLE_SUFFIX BETWEEN '20170801' AND '20170801' \n      AND hits.type=\"PAGE\"\n  ORDER BY hitTime)\n  \n  SELECT pathCategory, COUNT(*) AS count \n  FROM user_hit\n  GROUP BY 1;\n  \"\"\"\ndf = bq_assistant.query_to_pandas(QUERY)\ndf","83bfff51":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import pyplot\nimport seaborn as sns\n\nplt.figure();\ndf = df.set_index('pathCategory')\ndf['count'].plot(kind='bar');","035143e9":"query = \"\"\"\n    SELECT\n        fullVisitorId,\n        EXTRACT(DATE FROM TIMESTAMP_SECONDS(visitStartTime)) AS date,\n        SUM(totals.totalTransactionRevenue)\/1e6 AS revenue\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n    WHERE\n        _TABLE_SUFFIX BETWEEN '20170701' AND '20170731'\n    GROUP BY\n        fullVisitorId,\n        date\n    HAVING revenue > 0\n\"\"\"\ndf = bq_assistant.query_to_pandas(query)\ndf.head(10)","505c9e0f":"#https:\/\/lifetimes.readthedocs.io\/en\/latest\/index.html","491dca94":"!pip install lifetimes","8a1775d2":"from lifetimes.utils import summary_data_from_transaction_data\nfrom lifetimes import BetaGeoFitter\n\neffective_date = \"2017-07-31\"\nsummary = summary_data_from_transaction_data(df, 'fullVisitorId', 'date', \n                                             observation_period_end=effective_date)\nsummary['evaluate_date'] = effective_date\nbgf = BetaGeoFitter(penalizer_coef=0.0) ## this is one of the models we can use :)\nbgf.fit(summary['frequency'], summary['recency'], summary['T'])\nt = 7 #predict purchases in 7 days\nsummary['predicted_purchases'] = bgf.predict(t, summary['frequency'], summary['recency'], summary['T'])\n\nfrom lifetimes.plotting import plot_period_transactions\nplot_period_transactions(bgf)\nsummary[summary['frequency']>1].head()","b0bbfbd8":"from lifetimes.plotting import plot_history_alive\n\nid = '2158257269735455737'\ndays_since_birth = 60\nsp_trans = df.loc[df['fullVisitorId'] == id]\nplot_history_alive(bgf, days_since_birth, sp_trans, 'date')","acb3c847":"# Getting insights from GA data #\n\n## What you will build ##\n\nIn this tutorial, you'll learn how to get online user journey from Bigquery, and will run LTV analysis on a single user to classify users into buckets.\n\nWe are going to use a public bigquery dataset, specifically  [google_analytics_sample](https:\/\/bigquery.cloud.google.com\/dataset\/bigquery-public-data:google_analytics_sample). \n\n## What you will learn ##\n\n* How to use python on ipython notebook to fetch GA data\n\n* How to plot out aggregated GA data for dashboards\/charts\n\n* How to train a LTV model and calculate LTV for a single user \n\n## Setup ##\n\n* There is no prerequites for running this notebook, as a small sample of dates will be extracted for demonstrating purposes\n* If you want to do serious analysis, would recommend datalab\/colab \n","f6587b42":"## Step 2.2 Get user journey for many users\n\n* To get user journey, for each user, we need user_id to identify the user, and then a series of activities in chronicle order.","470721a5":"## ADVANCED SECTION IF WE HAVE TIME - Step 4. Calculate LTV ##\n\nLTV is based on frequency, recency, transaction value.\n\n* Prepare transaction data for each user: id, date, transaction value\n* Use lifelines package to get frequency, recency, transaction value for each user.\n* Use lifelines package to calculate LTV","17d0395e":"## Step 3. Aggregate to analyze ##\n\nWhich kind of page was viewed the most?\n\n* Use simple regex to get page category\n* Aggregate & plot page view distribution","6b32a404":"1. ## Step 2. Get user journey for one user ##\n\n* To get user journey, for each user, we need user_id to identify the user, and then a series of activities in chronicle order.","1638ec30":"## Step 1. Fetch data from Bigquery ##\n\nYou can also run these queries in [BigQuery](https:\/\/bigquery.cloud.google.com\/table\/bigquery-public-data:google_analytics_sample.ga_sessions_20170801).\nHere we examine the data structure for Bigquery to understand each columns.\n"}}