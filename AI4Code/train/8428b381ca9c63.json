{"cell_type":{"e2052e97":"code","a8a8a86c":"code","80d8944a":"code","036040db":"code","fbf0353e":"code","9f067e09":"code","cda692ca":"code","5d736ee3":"code","e6f787fa":"code","17315beb":"code","eba3351a":"code","d63833a5":"code","a6b488f4":"code","db58da82":"code","70907cb9":"code","00ffed00":"code","18094be1":"code","41e64b7a":"code","06c55978":"code","21d8196f":"code","0a684d32":"code","c8f957f6":"code","a45a7687":"code","651673ce":"code","4dd8577f":"code","235e0e12":"code","df3a9e8e":"code","9ddf1f19":"code","40a77e8a":"code","24a88e14":"code","c984be46":"code","84fed904":"code","745ef7b5":"code","d32f0151":"code","170e23e5":"code","63ab93ce":"code","da37b1a7":"code","aeaf01a8":"code","da1885a0":"code","60845b76":"code","2a768337":"code","ad71c9a7":"code","3657fe67":"code","7d3bc80a":"code","a08a5fea":"code","512e2152":"code","2cf864b2":"code","6aff3aa0":"code","1c5fc8e7":"code","dccb3651":"code","3952ba63":"code","56383bbb":"code","312ae927":"code","a927e350":"code","26ab8d20":"code","e4fd3a18":"code","11d04ef0":"code","0934ed06":"code","5e53fb6c":"code","600d5669":"code","00c310e4":"code","7f512582":"code","03e4b0c9":"code","5dcdf558":"code","e91dafd2":"code","4ab891c1":"code","a4fd7440":"code","9e16889f":"code","017078a9":"code","d3aedb61":"code","5a3a62ff":"code","97911523":"code","af6496eb":"code","e1f93498":"code","ed6cb279":"code","7eec2631":"code","3fc94dba":"code","b5ee8be7":"code","a0d60c81":"code","2566b068":"code","a170f8f9":"code","a3e29bf3":"code","caa67cc2":"code","1ef8c7c5":"code","868ddfe3":"code","50194d6b":"markdown","e7caca30":"markdown","d2ea5b00":"markdown","4d8f9030":"markdown","c030f816":"markdown","9273ebd9":"markdown","962c46a4":"markdown","b0a880da":"markdown","41e4af00":"markdown","20bf124f":"markdown","f7f4cc0c":"markdown","d9276d6c":"markdown","5871d75c":"markdown"},"source":{"e2052e97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8a8a86c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport pickle\nimport os\nimport string\nimport warnings\nimport nltk\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tqdm import tqdm\nfrom textblob import TextBlob\n\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom nltk import tokenize,WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import wordnet","80d8944a":"# sqlite3 will create connection between file and database \nimport sqlite3\ncon=sqlite3.connect('database.sqlite')","036040db":"# loading csv file \ndf=pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')","fbf0353e":"# lets have a look on data\ndf.info()","9f067e09":"# I prefer random data thats why going for sample() instead of head()\ndf.sample(2)","cda692ca":"# checking for null values\ndf.isnull().sum()","5d736ee3":"# Duplicated records\ndf.duplicated().sum()\n# here I can see no duplicates but let me show you something....","e6f787fa":"df[(df['Score']!=3) & (df['UserId']=='AR5J8UI46CURR')]\n","17315beb":"df_final=df.drop_duplicates(subset=['UserId','ProfileName','Score','Time','Summary','Text'],keep='first')\n#dropping duplicates with mulitple same features","eba3351a":"# Information\ndf_final.info()","d63833a5":"# Its common that HelpfulnessNumerator cannt be greater than HelpfulnessDenominator \n# we are going to drop these two rows\ndf_final[df_final['HelpfulnessNumerator']>df_final['HelpfulnessDenominator']]","a6b488f4":"df_final=df_final[df_final['HelpfulnessNumerator']<=df_final['HelpfulnessDenominator']]","db58da82":"# Distribution of Score\n\n\nplt.figure(figsize=(10,10))\n\nax = sns.countplot(x=df_final[\"Score\"],  data=df_final, order = df_final[\"Score\"].value_counts().index )\nfor p, label in zip(ax.patches, df_final[\"Score\"].value_counts()):\n#     print(p)\n#     print(label)\n    ax.annotate(label, (p.get_x()+0.25, p.get_height()+0.5))","70907cb9":"df_products = df_final.groupby('ProductId').filter(lambda x: len(x) >= 500)\ndf_product_groups = df_products.groupby('ProductId')","00ffed00":"print(len(df_products))\nprint(len(df_product_groups))","18094be1":"plt.figure(figsize=(20,20))\nsns.countplot(y=\"ProductId\",  hue=\"Score\", data=df_products);","41e64b7a":"#ProductId with respective Scores\ndf_product_groups.count()['Score'].sort_values(ascending=False)\n\n# B007JFMH8M  has highest score ","06c55978":"# Time column is not in a readable format.\n\ndf_final['Time'] = pd.to_datetime(df_final['Time'], unit='s')","21d8196f":"# sorting the dataset on the basis of Time\ndf_final.sort_values(by='Time')","0a684d32":"from matplotlib import ticker\n\nnumUsers = len(df_final['UserId'].unique())\nnumProducts = len(df_final['ProductId'].unique())\n\nfig, axes = plt.subplots(1, 1, figsize=(8, 4))\naxes.xaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:,.0f}\"))\nsns.barplot(data=pd.DataFrame({\n    'data': ['reviews', 'users', 'products'],\n    'count': [len(df_final), numUsers, numProducts],\n}), x='count', y='data', ax=axes)","c8f957f6":"df_final['Score']=df_final['Score'].apply(lambda x: 'positive' if x>3 else 'negative')","a45a7687":"# unique values with respective counts\ndf_final['Score'].value_counts()    ","651673ce":"# using predefined list of stopwords given by NLTK\nstopwords1 = list(stopwords.words('english'))\nprint('Number of Stopwords:: ', len(stopwords1))\nprint('Not is present in Stopwords:: ','not' in stopwords1)","4dd8577f":"punctuation","235e0e12":"stopwords1 = list(stopwords.words('english'))+list(punctuation) # combining stopwords along wit punctiation marks","df3a9e8e":"print(len(stopwords1),stopwords1,sep='\\n\\n')","9ddf1f19":"'the' in stopwords1","40a77e8a":"ps=PorterStemmer()\nlemmatizer = WordNetLemmatizer()","24a88e14":"nltk.pos_tag(['Work','Delhi'])\n\n# here N means Noun","c984be46":"# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    \n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\n# # defining lemmatizer which will take POS tags into consideration before performing lemmatization\ndef lemmatize_sentence(sentence):   \n    \n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n    \n#     print(list(nltk_tagged))\n    \n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n\n    \n    lemmatized_sentence = []       \n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n#             print('IN CASE OF NONE: ', lemmatized_sentence)\n        \n        else:        \n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n#             print('IN CASE OF ELSE : ',lemmatized_sentence)\n\n    \n    return \" \".join(lemmatized_sentence)\n\nprint(lemmatize_sentence(\" i have been working on my skills \")) #I be love it","84fed904":"# # Removing Stopwords and punctuatons\ndef clean_text(a):\n    return  ' '.join([i.lower() for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n  \n    \n    # Refraning the data having less than 2 tokens\n    if len(ls)>2:\n        val= ' '.join(ls)\n        return val\n    else :\n        return None\n \n    \n# # Lemmatization by only considering Verb for the clean data\ndef clean_text_lemma(a):\n    val= ' '.join([lemmatizer.lemmatize(i.lower(),pos = 'v') for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n    return val\n\n\n\n# Stemming on clean data\ndef clean_text_stem(a):    \n    val= ' '.join([ps.stem(i.lower()) for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n    return val","745ef7b5":"def clean_text(a):\n    return  [i.lower() for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1]","d32f0151":"# # performing tokenization -> stopword removal -> normalization  -> allowing sentences having more than 3 tokens\ndf_final['clean_txt'] = df_final['Text'].apply(clean_text)\n\n# # # Dropping data having NONE into it\n# # data.dropna(inplace = True)\n# # len(data)","170e23e5":"df_final.clean_txt[2]","63ab93ce":"# # Lemmatization by only considering Verb for the clean data\ndf_final['clean_lemma'] = df_final['clean_txt'].apply (lambda x:' '.join(x)).apply(clean_text_lemma)\n\n# # Lemmatization by considering respective POS tags\ndf_final['clean_lemma_pos'] = df_final['clean_txt'].apply (lambda x:' '.join(x)).apply(lemmatize_sentence)\n\n# # Stemming on clean data\ndf_final['clean_stem_txt'] = df_final['clean_txt'].apply (lambda x:' '.join(x)).apply(clean_text_stem)","da37b1a7":"df_final.sample(2)","aeaf01a8":"df_final.Summary","da1885a0":"vect = CountVectorizer()","60845b76":"df_final['Summary'].fillna('NUll',inplace=True)","2a768337":"df_final['Summary'].isna().sum()","ad71c9a7":"vect.fit_transform(df_final['Summary'])","3657fe67":"print(vect.get_feature_names())","7d3bc80a":"df_final.drop(['Id','Time','ProfileName'],axis=1,inplace=True)","a08a5fea":"df_final.head(2)","512e2152":"df_final['Text'].iloc[10]","2cf864b2":"df_final['clean_txt'].iloc[10]","6aff3aa0":"df_final['clean_lemma'].iloc[10]","1c5fc8e7":"df_final['clean_lemma_pos'].iloc[10]","dccb3651":"df_final['clean_stem_txt'].iloc[10]","3952ba63":"# Calculating Sentiment Score\n\n## Polarity Score is float which lies in the range of [-1,1] \n\n# **It represents Whether person is happy or sad with the service**\n\n# ## Subjectivity is also a float which lies in the range of [0,1]\n\n# 0--- personal\n# 1---factual\n# **Subjective sentences generally refer Whether the comment is factual info or personal**","56383bbb":"# Using Textblob.Sentiment to calculate the sentiment score w.r.t to every cusotmer review\n\ndf_final['sentiment'] = df_final['clean_lemma_pos'].apply(lambda reveiw: TextBlob(reveiw).sentiment)\n","312ae927":"'''\nload the data -> word tokenize --- > remove stop words\/punctuation  ---> lemmetize\/ stemming\/ normalization each document ---> call sentiment\n'''","a927e350":"# Creating new column having Polarity\ndf_final['Polarity'] = df_final['sentiment'].apply(lambda x: round(x[0],2)) # Whether person is happy or sad with the service\n\n# Creating new column having Subjectivity\ndf_final['Subjectivity'] = df_final['sentiment'].apply(lambda x: round(x[1],2)) # Whether the comment is factual info or personal\n\n# dropping the sentiment column from the datafrarme\ndel df_final['sentiment']","26ab8d20":"pd.set_option('display.width',1000)","e4fd3a18":"TextBlob(\"rep great adobe website n't place get need rep show go account thanks\").sentiment","11d04ef0":"df_final['clean_lemma_pos'].iloc[3]","0934ed06":"df_final['Text'].iloc[3]","5e53fb6c":"# Bucketing the Polarity scores into Medium, lower and Higher buckets using Lambda expresisno\ndf_final['Polarity_buckets'] = df_final['Polarity'].apply(lambda x: 'lower' if x<=-0.1 else 'middle' if (x>=0.1 and x<=0.4) else 'higher')","600d5669":"df_final['Polarity_buckets'].value_counts()","00c310e4":"df_final['Polarity_buckets'].value_counts(normalize = True)*100","7f512582":"df_final['Score']=df_final['Score'].apply(lambda x:1 if x=='positive' else 0)","03e4b0c9":"# Creating Training and Testng Data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df_final['clean_lemma_pos'],\n                                                    df_final['Score'],\n                                                    test_size = 0.2,\n                                                    random_state = 37)\n","5dcdf558":"print('length of training data',len(x_train),'length of y_training data',len(y_train),'length of X_training data',len(x_test))","e91dafd2":"tf_idf = TfidfVectorizer(min_df = 0.01)","4ab891c1":"# Transforming the X_train to transfrmed tfidf sparse matrix --> BOW\n\ntf_idf.fit_transform(x_train) # output is sparse matrix\n","a4fd7440":"# Converting Sparse matrics into array representaion form\ntf_idf_x_train = tf_idf.fit_transform(x_train).toarray() # to have sparse matrix converted in form of array\n\n\ntf_idf_x_test = tf_idf.transform(x_test).toarray()","9e16889f":"display( tf_idf_x_train, tf_idf_x_train.shape)","017078a9":"tf_idf_x_train[0]","d3aedb61":"max(tf_idf_x_train[0])","5a3a62ff":"np.argmax(tf_idf_x_train[0])","97911523":"tf_idf_x_train[0,195:199]","af6496eb":"# lenght of tfidf vocalbulary\/ features\nlen(tf_idf.vocabulary_)","e1f93498":"# tfidf vocalbulary\/ features\ntf_idf.vocabulary_","ed6cb279":"for key, value in tf_idf.vocabulary_.items():\n    if value == 482:\n        print(key)","7eec2631":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","3fc94dba":"rf_clf = RandomForestClassifier()\n\nrf_clf.fit(tf_idf_x_train,  y_train)\n\nrf_pred = rf_clf.predict(tf_idf_x_test)\n\nprint('Random Forest Accuracy Score:  ',np.round(accuracy_score(y_test,rf_pred)*100,0))","b5ee8be7":"# Creating Wordcloud","a0d60c81":"# Importing libraries for creating wordcloud\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image # The pillow library is a package that enables image reading\nimport urllib # This is used for opening URLs\nimport requests # Allows to Send requests","2566b068":"# Creating Data with their respective Part of Speechs\nsentence = ' '.join(df_final.clean_lemma_pos[0:500].values)\n\nblob = TextBlob(sentence)\n\ntg = blob.tags\n\nprint(sentence,tg,sep='\\n\\n')","a170f8f9":"# Filtering Data , where specific tags like Names, Numbers etc. are not rrequired in wordcloud\nt1 = []\nfor ind,val in enumerate(tg):\n    if tg[ind][1] != 'CD' and tg[ind][1] != 'FW' and tg[ind][1] != 'NNP' and tg[ind][1] != 'PRP' and tg[ind][1] != 'DT':\n        t1.append(tg[ind][0])\n#         print(val)\n\n\nt1 = ' '.join(t1)\nt1","a3e29bf3":"#Simple Word CLoud\n\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=150, \n                      background_color=\"white\",\n#                       color_func=lambda *args, **kwargs:(150,100,100),\n                      stopwords= STOPWORDS).generate(t1)\n\n\nplt.figure(figsize=(10,8))\n\nplt.imshow(wordcloud) # Command to show the image\nplt.axis(\"off\") # to Turnoff the axis\nplt.show()","caa67cc2":"# Example to validate the words appearing in the wordcloud\nblob.word_counts['problem']","1ef8c7c5":"# Mask will change the wordcloud into any shape of your choice\nmask = np.array(Image.open(requests.get('http:\/\/www.clker.com\/cliparts\/O\/i\/x\/Y\/q\/P\/yellow-house-hi.png', stream=True).raw))\n# mask = np.array(Image.open(requests.get('https:\/\/ak.picdn.net\/shutterstock\/videos\/208984\/thumb\/1.jpg', stream=True).raw))\n\n# This function takes in your text and your mask and generates a wordcloud. \ndef generate_wordcloud(words, mask):\n    word_cloud = WordCloud(width = 512, \n                           height = 512, \n                           background_color='white', \n                           stopwords=STOPWORDS, \n                           max_words = 230,\n                           mask=mask).generate(words)\n    \n    plt.figure(figsize=(10,8),facecolor = 'white')\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n\ngenerate_wordcloud(t1, mask)","868ddfe3":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test,rf_pred)\n\nfrom sklearn.metrics import  plot_roc_curve\n\nplot_roc_curve(rf_clf, tf_idf_x_test, y_test) ","50194d6b":"We can see that Customer with Profile Name 'Geetha Krishnan' had given same score at same time with same summary and Text\nits duplicacy of data , so we need to drop it not only with Score but also with same Time, Summary","e7caca30":" clean data\n \n lemmatization and other techniques\n \n   sentiment score\n   \n classification model \/ Regression Model","d2ea5b00":"Lets Import","4d8f9030":"**# Calculating Sentiment Score\n\nPolarity Score is float which lies in the range of [-1,1] \n\nIt represents Whether person is happy or sad with the service**\n\nSubjectivity is also a float which lies in the range of [0,1]\nSubjective sentences generally refer Whether the comment is factual info or personal****","c030f816":" Tokenization\n \n Remove stopwords, Punctiuation marks\n \n Normalization - lowercasing\n \n Spelling correction, singualrize\n \n lemmatization or Stemmting\n\n Textblob.sentiment - Subjective Score, Polarity Score\n\n BOW - TfIDF , CountVectorizer - numerical form\n \n ML\/DL  model inorder to make classification\/ regression model","9273ebd9":"# Score Evaluation\nif score is 1,2 then we can assume that its negative score and if its 4,5 then its positive\nscore=3 is neither positive nor negative so we will not consider it","962c46a4":"Here been----be\n     working----- work\n     skills ------ skill","b0a880da":"# **Amazon Fine Food Reviews Analysis**\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n\nNumber of reviews: 568,454\n\nNumber of users: 256,059\n\nNumber of products: 74,258\n\nTimespan: Oct 1999 - Oct 2012\n\nNumber of Attributes\/Columns in data: 10\n    \n    \n    \n# **Attribute Information**:\n\nId\nProductId - unique identifier for the product\n\nUserId - unqiue identifier for the user\n\nProfileName\n\nHelpfulnessNumerator - number of users who found the review helpful\n\nHelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n\nScore - rating between 1 and 5\n\nTime - timestamp for the review\n\nSummary - brief summary of the review\n\nText - text of the review","41e4af00":"There are 11 groups of ProductID which has reviews of more than or equal to 500, means 11 products leads to the maximum sale of the Products","20bf124f":" 1)  Count vectorizer - give only count --> tfidf transformer ---> tf *idf\n\n 2)  TfidfVectorizer - tf , idf","f7f4cc0c":"# Text Preprocessing\n\nRemove Stopwords--not advisely always but its a good method as 'not' is also present in stopwords which sometimes changes the meaning of the sentence  eg..This pizza is not costly but delicious.\n\nLowercase-- in order to remove duplicacy eg ..... Pizza and pizza we will convert all in lower case so that BOW(Bag of Words) does not have duplicacy\n\nStemming---works by cutting suffix or prefix from the word.\n           taking root\/stem of the word eg.....tasty\/tastes\/taseful we will consider taste here i.e. root word\n           we will use two methods 1) Porter Stemmer\n                                   2) Snow ball Stemmer\n                                 \nlemmatization--aims to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma eg---running will give only 'run'\n\n","d9276d6c":"A **POS tag** (or part-of-speech tag) is a special label assigned to each token (word) in a text corpus to indicate the part of speech and often also other grammatical categories such as tense, number (plural\/singular), case etc. POS tags are used in corpus searches and in text analysis tools and algorithms.","5871d75c":"![image.png](attachment:image.png)"}}