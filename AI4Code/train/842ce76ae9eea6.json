{"cell_type":{"4bb85bd7":"code","b6292e2d":"code","43eaa460":"code","a8e5d639":"code","2ecff7c6":"code","9fea837f":"code","cdfec094":"code","9833154f":"code","4c3b5f69":"code","b7b0e2b5":"code","39477edb":"code","76612fc3":"code","fe625e13":"code","b9dc20c2":"code","e2a3ac66":"code","f5a45f10":"code","17cf6b9b":"code","abc99c11":"code","b2aa6ce5":"code","fae70ccf":"code","70b736ca":"code","46f514f2":"code","9e42368a":"code","42c12ae0":"code","31667ebd":"code","6b706e2c":"code","b8753628":"code","cb8f8fc6":"code","61151835":"code","113bb82c":"code","2dcb43ff":"code","2365a5cc":"code","119701f5":"code","fe843ecb":"code","2ec6d01d":"code","016ab14c":"code","dfcddd4f":"code","9f9c6055":"code","0a357a45":"code","7343c98c":"code","48e7e5b0":"code","381e5069":"code","f42af6ab":"code","a4bbfa67":"code","6fc5e973":"code","b592b8c1":"code","b78f58fc":"code","64c6e2e4":"code","ce13c339":"code","b8c1937c":"code","f40ead1f":"code","f36103c1":"code","d3e2fa3b":"code","d89b3014":"code","bb3b3dbf":"code","3e771c68":"code","069a085a":"code","13752e06":"code","7aae8bfa":"code","ba46f8ca":"code","64a40c21":"code","7d612574":"code","685b60cd":"code","b56bde2b":"code","3b23fd1e":"code","9214d56d":"code","c3b6976d":"code","90bc4fef":"code","b1d43506":"code","2d9fee72":"code","c68da645":"code","40dabbda":"code","f3146435":"code","b1af2f95":"code","6376aef9":"code","272311f4":"code","44d0e5cb":"code","171147c5":"code","60924f8a":"code","4e39bc14":"code","31e50a97":"code","0ae58ca0":"code","15994080":"code","5aa26168":"code","c0f1ee0b":"code","85b18f72":"code","42b61c84":"code","f9fefb0f":"code","a9a7753a":"code","2a91fdd7":"code","9a1b4faf":"code","d36af147":"code","a717b891":"code","1777bb4a":"code","84515e86":"code","d5857056":"code","4ed6cdc8":"code","1a9eaab5":"code","0586fa62":"code","10784ebf":"code","db89227c":"code","f4bd4163":"code","d9d5a81e":"code","27d87982":"code","40cf81c4":"code","afec462a":"code","bb5a666d":"code","0e076360":"code","d5bda4cf":"code","e45148b0":"code","46a5aea8":"code","830812cb":"code","75bd109d":"code","b30044f4":"code","75fb222b":"code","c95ce4f3":"code","a8449585":"code","3892b5af":"markdown","792696ea":"markdown","45509c7d":"markdown","152e803e":"markdown","9ee05cd1":"markdown","e4144d89":"markdown","0d2152ef":"markdown","ceb3976c":"markdown","df7629fb":"markdown","d33c3a5b":"markdown","8e1e9036":"markdown","f6c031ee":"markdown","0b17e1ee":"markdown","37692a4f":"markdown","46a6d0fd":"markdown","acbc4a39":"markdown","028d2cdf":"markdown","5c5ebb76":"markdown","2497552e":"markdown","2c04f85c":"markdown","39621951":"markdown","ef39c88b":"markdown","25b51c84":"markdown","d55f897f":"markdown","87856d43":"markdown","9060341b":"markdown","9920b34d":"markdown","518ddef0":"markdown","2227c6a9":"markdown","692b9bc8":"markdown","8fc240f0":"markdown","b10e8165":"markdown","78841daa":"markdown","2566f4e8":"markdown","e54b6869":"markdown","172c2716":"markdown","5de0d5e0":"markdown","85155312":"markdown","b91b20da":"markdown","f36ca286":"markdown","a00ef43c":"markdown","4146eeb7":"markdown","36d32c38":"markdown","33563004":"markdown","0aabdb69":"markdown","2734e898":"markdown","690409aa":"markdown","dcb7aeb5":"markdown","e51b7509":"markdown","35ae4984":"markdown","69e886e8":"markdown","0d6c0539":"markdown","a66a556e":"markdown","4bcf5cf9":"markdown","7bdab7c7":"markdown","69a06e31":"markdown","b00799c8":"markdown","e6479341":"markdown","49806418":"markdown","de2a2e5a":"markdown","f89d0a2f":"markdown","4d0d8b83":"markdown","7065d1fe":"markdown","c761e416":"markdown","b5da17c4":"markdown","50bf7d5d":"markdown","b4e2c96b":"markdown","20006e9a":"markdown","118b0f1e":"markdown","68e84dce":"markdown","e8e7c9c7":"markdown","cefb4323":"markdown","a466087b":"markdown","a02893f5":"markdown","28b07bbe":"markdown","e86e9d0e":"markdown"},"source":{"4bb85bd7":"import numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, LassoCV,LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, cross_val_predict,train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingRegressor,RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import norm, skew \n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","b6292e2d":"house_prices_train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_prices_test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsampl_sub=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","43eaa460":"house_prices_train.head()","a8e5d639":"house_prices_test.head()","2ecff7c6":"house_prices_train.shape","9fea837f":"house_prices_test.shape","cdfec094":"sampl_sub.shape","9833154f":"house_prices_train.info()","4c3b5f69":"house_prices_train.describe()","b7b0e2b5":"#check the column type \nhouse_prices_train.dtypes","39477edb":"#change the column type to it's appropriate type if possible\nhouse_prices_train=house_prices_train.apply(pd.to_numeric, errors='ignore')\n","76612fc3":"#check the numbers of uniques values in all columns \nhouse_prices_train.nunique(axis=0)","fe625e13":"#ckeck if there is a null in the train dataframe\n\nprint(\"is there missing samples in training set:\",house_prices_train.isnull().values.any())\n\n# get all the columns with a missing values\nhouse_prices_train.columns[house_prices_train.isnull().any()]","b9dc20c2":"#ckeck if there is a null in the test dataframe\n\n\nprint(\"is there missing samples in test set:\",house_prices_test.isnull().values.any())\n\n# get all the columns with a missing values\nhouse_prices_train.columns[house_prices_train.isnull().any()]","e2a3ac66":"# Check the statistical measures of the predicted target (SalePrice)\nhouse_prices_train['SalePrice'].describe()","f5a45f10":"#check SalePrice distribuation\nplt.figure(figsize=(10,5));\nplt.xlabel('xlabel', fontsize=16);\nplt.rc('xtick', labelsize=14); \nplt.rc('ytick', labelsize=14); \n\n\nsns.distplot(house_prices_train['SalePrice']);\nprint(\"Skewness: %f\" % house_prices_train['SalePrice'].skew())","17cf6b9b":"# YrSold has the lowest correlation with SalePrice\nsns.boxplot(x=\"YrSold\",y=\"SalePrice\",data=house_prices_train);\n","abc99c11":"# MiscVal has the second lowest correlation with SalePrice\nsns.lmplot (x=\"MiscVal\",y=\"SalePrice\",data=house_prices_train);\n","b2aa6ce5":"# observe the distributation for each column\n#get the z score for each column and plot it\nnumerical_cols = house_prices_train.dtypes[house_prices_train.dtypes !=\"object\"].index\nfor i in numerical_cols:\n    ax = house_prices_train[i].plot(x='ZScore', y='FreqDist', kind='kde', figsize=(10, 6),title=i)\n    plt.show()\n    print (\"\\nZ-score for\",i,\":\",\"\\n\", stats.zscore(house_prices_train[i]))\n    print(\"mean=\",house_prices_train[i].mean())\n    print(\"skew=\",house_prices_train[i].skew())","fae70ccf":"# Correlation of all numeric features in the training dataset\ncor_train=house_prices_train.corr()\nplt.figure(figsize=(29,19))\nsns.heatmap(cor_train,annot=True,cmap=\"BrBG\",square=True, annot_kws={'size': 8})","70b736ca":"#box plot to determine the outliers in all the features\n#get only numerical columns\nstand_df_houses=house_prices_train._get_numeric_data()\n\n#scale all columns\nstand_df_houses=StandardScaler().fit_transform(stand_df_houses)\n\n#plot to see the outliers\nfig = plt.figure(figsize=(10,10))\nax = fig.gca()\n\nsns.boxplot(data=stand_df_houses, orient='h', fliersize=2, linewidth=3, notch=True,\n                 saturation=0.5, ax=ax)\n\nax.set_title(' Outliers in All features \\n')\nplt.show()","46f514f2":"# Plotting Correlation of all numeric features in the training dataset\n\n# Plot Outline\nrows = 10\ncols = 4\nsorted_cols = cor_train.nlargest(len(cor_train), 'SalePrice')['SalePrice'].index #to sort columns from highest correlation with SalePrice\n#Number_numerical = house_prices_train.dtypes[house_prices_train.dtypes !=\"object\"].index\nnumerical_cols= len(sorted_cols)\nfig, axs = plt.subplots(rows, cols, figsize=(cols*3,rows*3))\n# Loop Definition\nfor r in range(0,rows):\n    for c in range(0,cols):\n        # Numerical Columns Condition\n        i = r*cols+c\n        if i < numerical_cols:\n            sns.regplot(house_prices_train[sorted_cols[i]], house_prices_train[\"SalePrice\"], ax = axs[r][c])\n            # Correlation Definition\n            correlation = house_prices_train[sorted_cols[i]].corr(house_prices_train[\"SalePrice\"])\n            corr_abs = round(abs(correlation),2)\n            \n            \n            \n            # Adjusting the plot appearance\n            title = \"r = \" + \"{}\".format(corr_abs)\n            axs[r][c].set_title(title,fontsize=10)\nplt.tight_layout()\nplt.show()","9e42368a":"# Top Numerical Correlations with SalePrice\n\n# get the only columns that have higher correlation than the threshold \n#select strong positive using threshold more than.4\n\nthreshold=0.4 \ncor_train=house_prices_train.corr()\nhigh_corre = cor_train.index[abs(cor_train[\"SalePrice\"])>threshold]\n\n#to sort columns from highest correlation with SalePrice\nsorted_cols = cor_train.nlargest(len(high_corre),\n'SalePrice')['SalePrice'].index \n\nplt.figure(figsize=(15,13))\nsns.set(font_scale=1.5)\n\n#plot heatmap with only the top features\nnr_corr_matrix = sns.heatmap(house_prices_train[sorted_cols].corr(),\nannot=True,cmap=\"BrBG\",square=True, annot_kws={'size':14})\n","42c12ae0":"# Overallquality column has the highest correlation with the SalePrice\nplt.subplot(1,1,1)\n\nsns.barplot(house_prices_train.OverallQual,house_prices_train.SalePrice)\nsns.lmplot (x=\"OverallQual\",y=\"SalePrice\",data=house_prices_train);\n","31667ebd":"# GrLivArea column has the second high correlation with SalePrice\nsns.lmplot (x=\"GrLivArea\",y=\"SalePrice\",data=house_prices_train)\nplt.show()","6b706e2c":"# GarageCars column has the third correlation with SalePrice\nsns.lmplot (x=\"GarageCars\",y=\"SalePrice\",data=house_prices_train)\nsns.barplot(house_prices_train.GarageCars,house_prices_train.SalePrice)\nplt.show()","b8753628":"# check the precentage of the nan values in all columns before cleaning\nhouse_prices_train.isnull().sum()","cb8f8fc6":"#function to replace np.nan with string'None' \n#takes columns list name and the datafrme name\ndef replace_none(column_name_list,dataframe):\n    \n    for i in column_name_list:\n        dataframe[i].replace(np.nan,'None',inplace=True)\n    return dataframe\n\n\n\n#name of column will we change\nlist_column=['Alley','MiscFeature','PoolQC','BsmtQual','BsmtCond','BsmtExposure',\n             'BsmtFinType1','BsmtFinType2','GarageType','GarageFinish',\n              'GarageQual','GarageCond','PoolQC','Fence','FireplaceQu']\n    \nreplace_none(list_column,house_prices_train)\n\n#check on one of the column\nhouse_prices_train.BsmtFinType1.unique()","61151835":"house_prices_train.shape","113bb82c":"#numerical columns index to replace each missing data with mode\/most_frequent\n\ncolumn_index_train=[3,25,26,6,74,72,30,31,32,33,35,42,58,60,63,64,72,73,57]\n","2dcb43ff":"from sklearn.impute import SimpleImputer\n#replace np.nan in all columns with most frequent value\nimr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nimr = imr.fit(house_prices_train.iloc[:,column_index_train])\n\nhouse_prices_train.iloc[:,column_index_train] = imr.transform(house_prices_train.iloc[:,column_index_train])","2365a5cc":"#see the relation between both years\nhouse_prices_train[['GarageYrBlt','YearBuilt']].head()","119701f5":"#replace each nan values in garage year with the year of the building in the same row\n\nhouse_prices_train['GarageYrBlt'] = house_prices_train.apply(\nlambda row: row['YearBuilt'] if np.isnan(row['GarageYrBlt']) else row['GarageYrBlt'],axis=1)\n","fe843ecb":"# check the precentage of the nan values in all columns after cleaning\n\nhouses_missing_value = house_prices_train.isnull().sum().sort_values(ascending=False)\n\nhouses_missing_percent = (house_prices_train.isnull().sum()\/house_prices_train.isnull().count()).sort_values(ascending=False)\n\nmissing = pd.concat([houses_missing_value, houses_missing_percent], axis=1, keys=['Value', 'Percent'])\n\nmissing.head(6)","2ec6d01d":"#drop ID column,because we can't use it on model\nhouse_prices_train.drop(\"Id\", axis = 1, inplace = True)","016ab14c":"plt.figure(figsize=(8,5))\nsns.boxplot(x=house_prices_train['OverallQual'], y=house_prices_train['SalePrice'])\n","dfcddd4f":"fig, ax = plt.subplots()\nax.scatter(x =house_prices_train['GrLivArea'], y = house_prices_train['SalePrice'],c='darkred')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9f9c6055":"#Deleting outliers from GrLivArea\nhouse_prices_train = house_prices_train.drop(house_prices_train[(house_prices_train['GrLivArea']>4000) & (house_prices_train['SalePrice']<300000)].index).reset_index(drop=True)\n","0a357a45":"plt.figure(figsize=(8,5))\nsns.boxplot(x=house_prices_train['GarageCars'], y=house_prices_train['SalePrice']);\n\n","7343c98c":"#call the function replace_none to replace Nan categorical to more appropiate value\nreplace_none(list_column,house_prices_test)\n","48e7e5b0":"#replace each nan values in garage year with the year of the building in the same row\n\nhouse_prices_test['GarageYrBlt'] = house_prices_test.apply(\nlambda row: row['YearBuilt'] if np.isnan(row['GarageYrBlt']) else row['GarageYrBlt'],axis=1)","381e5069":"# change 'None' category string to more meaningful string\nhouse_prices_test['MasVnrType'].replace('None','no masonry vnr',inplace=True)\n","f42af6ab":"#numerical columns index to replace each missing data with mode\/most_frequent\n\n\ncolumn_index=[2,3,9,23,24,25,26,34,36,37,38,47,48,53,55,60,61,62,77,78]\n\nfrom sklearn.impute import SimpleImputer\n\nimr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nimr = imr.fit(house_prices_test.iloc[:,column_index])\n\nhouse_prices_test.iloc[:,column_index] = imr.transform(house_prices_test.iloc[:,column_index])\n\n","a4bbfa67":"# check the precentage of the nan values in all columns after cleaning\n\n\nhouses_missing_value_test = house_prices_test.isnull().sum().sort_values(ascending=False)\n\nhouses_missing_percent_test = (house_prices_test.isnull().sum()\/house_prices_test.isnull().count()).sort_values(ascending=False)\n\nmissing_T = pd.concat([houses_missing_value_test, houses_missing_percent_test], axis=1, keys=['Value', 'Percent'])\n\nmissing_T.head(10)","6fc5e973":"#drop ID column, because we can't use it on models\nhouse_prices_test.drop(\"Id\", axis = 1, inplace = True)","b592b8c1":"\nhouse_prices_test['MSSubClass'] = house_prices_train['MSSubClass'].apply(str)\n\nhouse_prices_test['YrSold'] = house_prices_train['YrSold'].astype(str)\n\nhouse_prices_test['MoSold'] = house_prices_train['MoSold'].astype(str)","b78f58fc":"\nhouse_prices_test['MSSubClass'] = house_prices_test['MSSubClass'].apply(str)\n\nhouse_prices_test['YrSold'] = house_prices_test['YrSold'].astype(str)\n\nhouse_prices_test['MoSold'] = house_prices_test['MoSold'].astype(str)","64c6e2e4":"house_prices_train=pd.get_dummies(house_prices_train, columns=['MSSubClass','MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition','YrSold','MoSold'], drop_first=True)\n\n","ce13c339":"house_prices_test=pd.get_dummies(house_prices_test, columns=['MSSubClass','MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition','YrSold','MoSold'], drop_first=True)\n\n\n","b8c1937c":"\n# Get missing columns in the training test\nmissing_cols = set( house_prices_train.columns ) - set( house_prices_test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    house_prices_test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nhouse_prices_test = house_prices_test[house_prices_train.columns]","f40ead1f":"#drop the salePrice from test dataframe\nhouse_prices_test.drop(['SalePrice'],inplace=True,axis=1)\ntest_X_=house_prices_test","f36103c1":"# select the features as X\nX=house_prices_train.drop([\"SalePrice\"],axis=1).copy()\n\n#select the target as y\ny=house_prices_train[\"SalePrice\"].copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n   X, y, test_size=0.20, random_state=2) \n\n","d3e2fa3b":"#get the correlation for training \ncor_train_corr=house_prices_train.corr()\n","d89b3014":"\n#we put threshold to determine which features should be selected \n\n#select strong positive using threshold more than 0.4\nthreshold_P=0.4\nhigh_positive_corre=cor_train_corr['SalePrice']\n\n#compare correlation with threshold\npositives=(high_positive_corre[high_positive_corre>threshold_P])\npositive_corre=pd.DataFrame(positives)\npositive_corre.columns=positive_corre.columns.rename(\"positives\")\n\n\n#select strong negative using threshold less than 0.4\nthreshold_N=-0.4\nhigh_negative_corre=cor_train_corr['SalePrice']\n\n#compare correlation with threshold\nnegatives=high_negative_corre[high_negative_corre<threshold_N]\nnegative_corre=pd.DataFrame(negatives)\nnegative_corre.columns=negative_corre.columns.rename(\"negative\")\n","bb3b3dbf":"positive_corre.sort_values(by='SalePrice',ascending=False)","3e771c68":" negative_corre.sort_values(by='SalePrice',ascending=False)","069a085a":"feature_select=['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath','YearBuilt','YearRemodAdd','MasVnrArea','Fireplaces','Foundation_PConc',\n'ExterQual_Gd','BsmtFinType1_GLQ','Neighborhood_NridgHt','SaleType_New','SaleCondition_Partial','FireplaceQu_Gd',\n'GarageType_Attchd','MasVnrType_Stone','Neighborhood_NoRidge','KitchenQual_Gd','Exterior2nd_VinylSd',\n'Exterior1st_VinylSd','BsmtExposure_Gd','ExterQual_TA','KitchenQual_TA','BsmtQual_TA','GarageFinish_Unf']\n","13752e06":"X_FS=house_prices_train[feature_select]\n    \ny_FS=house_prices_train[\"SalePrice\"]\n\nX_train_FS, X_test_FS, y_train_FS, y_test_FS = train_test_split(\n  X_FS, y_FS, test_size=0.20, random_state=2) \n\n\ntest_X_FS=house_prices_test[feature_select]\n","7aae8bfa":"# standard_scalar = StandardScaler()\n\n# #standarlize X train\n# X_train_ss = standard_scalar.fit_transform(X_train)\n# X_train_ss = pd.DataFrame(X_train_ss, columns=X.columns)\n\n# # #standarlize X test\n# X_test_ss = standard_scalar.transform(X_test)\n# X_test_ss = pd.DataFrame(X_test_ss, columns=X.columns)\n\n# #standarlize the X to get the prediction  it\n# test_X_ss = standard_scalar.transform(test_X_)\n# test_X_ss = pd.DataFrame(X_test_ss, columns=X.columns)\n","ba46f8ca":"# range_lasso=np.arange(0.01,100,0.01)\n\n\n# model_lasso = LassoCV(alphas =range_lasso)\n\n# #train the lasso model\n# model_lasso.fit(X_train_ss,y_train)\n\n# # get the train score\n# model_lasso.score(X_train_ss,y_train)","64a40c21":"# model_lasso.score(X_test_ss,y_test)","7d612574":"# # Perform 2-fold cross validation on lasso model\n# scores = cross_val_score(model_lasso,X_train_ss,y_train, cv=5)\n# print(\"Cross-validated training scores for lassoCV model:\", scores.mean())\n","685b60cd":"# #get the prediction of X from the test dataframe \n# pred_lasso=model_lasso.predict(test_X_)\n# pred_lasso","b56bde2b":"# #save the prediction to submit and get the score of RMSE\n\n# y_dataframe=pd.DataFrame(pred_lasso,columns=['SalePrice_pred_lasso'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_lasso.csv',index=False)\n","3b23fd1e":"# from sklearn.linear_model import RidgeCV,LassoCV\n\n# range_Ridge=np.arange(0.01,100,0.01)\n# ridgecv = RidgeCV(alphas=range_Ridge)\n# # train ridgecv\n# ridgecv.fit(X_train, y_train)\n\n# #get the best alpha\n# print('The best Alpha:',ridgecv.alpha_)","9214d56d":"# #get the train score\n# ridgecv.score(X_train, y_train)","c3b6976d":"# #get the test score\n# ridgecv.score(X_test, y_test)","90bc4fef":"# #get the predict of the test dataframe\n# pred_Ridge=ridgecv.predict(test_X_)\n# pred_Ridge","b1d43506":"# Basline_ridge=1 - pred_Ridge.mean()\n# print('Baseline score for the test data dataframe:',Basline_ridge)","2d9fee72":"\n# # Perform 2-fold cross validation on RidgeCV model\n# scores = cross_val_score(ridgecv,X_train,y_train, cv=5)\n# print(\"Cross-validated training scores for RidgeCV model:\", scores.mean())\n","c68da645":"# #save the prediction to submit and get the score of RMSE\n\n# y_dataframe=pd.DataFrame(pred_Ridge,columns=['SalePrice_pred_ridge'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_ridge.csv',index=False)\n\n","40dabbda":"# range_Ridge=np.arange(0.01,100,0.01)\n# ridgecv_s = RidgeCV(alphas=range_Ridge)\n\n# ridgecv_s.fit(X_train_ss, y_train)\n\n# #get the best alpha\n# print('The best Alpha:',ridgecv_s.alpha_)","f3146435":"# ridgecv_s.score(X_train_ss, y_train)","b1af2f95":"# ridgecv_s.score(X_test_ss,y_test)","6376aef9":"# pred_Ridge_s=ridgecv_s.predict(test_X_)\n# pred_Ridge_s","272311f4":"\n# # Perform 2-fold cross validation on lasso model\n# scores = cross_val_score(ridgecv_s,X_train_ss,y_train, cv=5)\n# print(\"Cross-validated training scores for RidgeCV model:\", scores.mean())\n\n","44d0e5cb":"# #save the prediction to submit and get the score of RMSE\n\n# y_dataframe=pd.DataFrame(pred_Ridge_s,columns=['SalePrice_pred_ridge_s'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_ridge_s.csv',index=False)\n\n","171147c5":"# param_reg = { 'max_features': [0.3,.4,.5, 0.6,.7,.8, 1],\n#         'n_estimators': [50,100, 150, 200], \n#          'base_estimator__max_depth': [3, 5,10,15, 20]}","60924f8a":"# from sklearn.tree import DecisionTreeRegressor\n\n# model_reg=BaggingRegressor(base_estimator=DecisionTreeRegressor(),)\n\n# model_GSsearch = GridSearchCV(model_reg,param_reg, cv=9)\n# model_GSsearch.fit(X_train, y_train)\n","4e39bc14":"# model_GSsearch.best_params_","31e50a97":"# model_GSsearch.score(X_train, y_train)","0ae58ca0":"# model_GSsearch.score(X_test, y_test)\n","15994080":"# pred_bagging=model_GSsearch.predict(test_X_)\n# pred_bagging","5aa26168":"# scores_bagg = cross_val_score(model_GSsearch,X_train, y_train, cv=6)\n# print(\"Mean of Cross-validated scores:\",scores_bagg.mean())\n\n","c0f1ee0b":"#save the prediction to submit and get the score of RMSE\n\n# y_dataframe=pd.DataFrame(pred_bagging,columns=['SalePrice_pred_bagging'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_bagging.csv',index=False)\n","85b18f72":"\n# model_reg_s=BaggingRegressor(base_estimator=DecisionTreeRegressor(),)\n\n# model_GSsearch_s = GridSearchCV(model_reg_s,param_reg, cv=9)\n# model_GSsearch_s.fit(X_train_ss, y_train)","42b61c84":"model_GSsearch_s.best_params_","f9fefb0f":"# model_GSsearch_s.score(X_train_ss, y_train)","a9a7753a":"# model_GSsearch_s.score(X_test_ss, y_test)\n","2a91fdd7":"# pred_bagging_s=model_GSsearch_s.predict(test_X_ss)\n# pred_bagging_s","9a1b4faf":"# scores_bagg = cross_val_score(model_GSsearch_s,X_train_ss, y_train, cv=6)\n# print(\"Mean of Cross-validated scores:\",scores_bagg.mean())\n","d36af147":"#save the prediction to submit and get the score of RMSE\n\n# y_dataframe=pd.DataFrame(pred_bagging_s,columns=['SalePrice_pred_bagging_s'])\n# y_dataframe = y_dataframe[:-1]\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_bagging_s.csv',index=False)\n","a717b891":"# param_Rforest = { 'max_depth': [ 2, 3, 4, 5, 6,9,13,15,20],\n#           'max_features':[.1,.2,.3,.4,.6,.7,.8,.9,1],\n#           'n_estimators': [20,50,130,150],\n#           'min_samples_leaf': [1, 2, 3, 4]\n#           }","1777bb4a":"# model_Rforest = RandomForestRegressor(random_state=1)\n# Rforest_GSsearch = GridSearchCV(model_Rforest ,param_Rforest, cv=6, verbose=1, n_jobs=-1 )\n\n# Rforest_GSsearch.fit(X_train, y_train)\n\n# train_score_RF=Rforest_GSsearch.score(X_train, y_train)\n\n# test_score_RF=Rforest_GSsearch.score(X_test, y_test)\n \n\n# print(\"train score for Random Forest model:\",train_score_RF)\n# print(\"test score for Random Forest model:\",test_score_RF)\n","84515e86":"# y_pred_forest=Rforest_GSsearch.predict(test_X_)\n# y_pred_forest\n","d5857056":"\n# y_dataframe=pd.DataFrame(y_pred_forest,columns=['SalePrice_pred'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_forest.csv',index=False)\n","4ed6cdc8":"# model_Rforest_s = RandomForestRegressor(random_state=1)\n\n# Rforest_GSsearch_s = GridSearchCV(model_Rforest_s ,param_Rforest, cv=6, n_jobs=-1 )\n\n# Rforest_GSsearch_s.fit(X_train_ss, y_train)\n\n# train_score_s=Rforest_GSsearch_s.score(X_train_ss, y_train)\n\n# test_score_s=Rforest_GSsearch_s.score(X_test_ss, y_test)\n \n\n# print(\"train score for Random Forest Scaled :\",train_score_s)\n# print(\"test score  score for Random Forest Scaled :\",test_score_s)\n","1a9eaab5":"# y_pred_forest_s=Rforest_GSsearch_s.predict(test_X_)\n# y_pred_forest_s\n","0586fa62":"\n# y_dataframe=pd.DataFrame(y_pred_forest_s,columns=['SalePrice_pred_forest'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_forest_scale.csv',index=False)\n","10784ebf":"# param_DTress = { 'max_depth': [1, 2, 3, 4, 5, 6,9,13,15],\n#           'max_features':[.1,.2,.3,.4,.6,.7,.8],\n#           'max_leaf_nodes': [5, 6, 7, 8, 9, 10],\n#           'min_samples_leaf': [1, 2, 3, 4]\n#           }","db89227c":"\n# model_DTree = DecisionTreeRegressor( random_state=1)\n\n# DTress_GSsearch = GridSearchCV(model_DTree,param_DTress, cv=6, n_jobs=-1 )\n\n# DTress_GSsearch.fit(X_train, y_train)\n\n# train_scores=DTress_GSsearch.score(X_train, y_train)\n\n\n# test_scores=DTress_GSsearch.score(X_test, y_test)\n\n# print('Train score for Decision Tree without standarize: ',train_scores)\n\n# print('Test score for Decision Tree without standarize: ',test_scores)\n","f4bd4163":"# scores_DTress = cross_val_score(DTress_GSsearch,X_test, y_test, cv=5)\n# print(\"cross validation:\",scores_DTress.mean())\n","d9d5a81e":"# y_pred_tres=DTress_GSsearch.predict(test_X_)\n# y_pred_tres","27d87982":"# y_dataframe=pd.DataFrame(y_pred_tres,columns=['SalePrice_pred'])\n\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_DT.csv',index=False)\n","40cf81c4":"\n# model_DTree_s = DecisionTreeRegressor( random_state=1)\n\n# DTress_GSsearch_s = GridSearchCV(model_DTree_s,param_DTress, cv=6, n_jobs=-1 )\n\n# DTress_GSsearch_s.fit(X_train_ss, y_train)\n\n# #get the train score\n# train_scores_DT=DTress_GSsearch_s.score(X_train_ss, y_train)\n\n# #get the test score\n# test_scores_DT=DTress_GSsearch_s.score(X_test_ss, y_test)\n\n# print('Train score for Decision Tree with standarize: ',train_scores_DT)\n\n# print('Test score for Decision Tree with standarize: ',test_scores_DT)\n\n","afec462a":"# #cross validation for training the data\n# scores_DTress = cross_val_score(DTress_GSsearch,X_train_ss, y_train, cv=5)\n# print(\"cross validation for training Decision Tree:\",scores_DTress.mean())\n","bb5a666d":"# #predict the test X \n# y_pred_tres_s=DTress_GSsearch_s.predict(test_X_)\n# y_pred_tres_s","0e076360":"# #add the prediction to submission file\n\n# y_dataframe=pd.DataFrame(y_pred_tres_s,columns=['SalePrice_pred'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_DT_S.csv',index=False)\n","d5bda4cf":"# models_avg = (pred_Ridge+pred_bagging) \/ 2\n# models_avg","e45148b0":"# y_dataframe=pd.DataFrame(models_avg,columns=['average_pred'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_average.csv',index=False)\n\n","46a5aea8":"# models_avg_2 = (pred_Ridge+y_pred_tres) \/ 2\n\n# y_dataframe=pd.DataFrame(models_avg_2,columns=['average_pred'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_average_2.csv',index=False)\n\n\n","830812cb":"param_Rforest_fs = { 'max_depth': [ 2, 3, 4, 5, 6,9,13,15,20],\n          'max_features':[.1,.2,.3,.4,.6,.7,.8,.9,1],\n          'n_estimators': [20,50,130,150],\n          'min_samples_leaf': [1, 2, 3, 4]\n          }","75bd109d":"# model_Rforest_selection = RandomForestRegressor(random_state=2)\n\n# Rforest_GSsearch_selection = GridSearchCV(model_Rforest_selection ,\n# param_Rforest_fs,cv=6, n_jobs=-1 )\n\n# Rforest_GSsearch_selection.fit(X_train_FS, y_train_FS)\n\n# train_score_RF_selection=Rforest_GSsearch_selection.score(X_train_FS, y_train_FS)\n\n# test_score_RF_selection=Rforest_GSsearch_selection.score(X_test_FS, y_test_FS)\n \n# print(\"train score for Random Forest model with selection features:\",\n#       train_score_RF_selection)\n# print(\"test score for Random Forest model  with selection features:\",\n#       test_score_RF_selection)\n\n","b30044f4":"# y_pred_forest_FS=Rforest_GSsearch_selection.predict(test_X_FS)\n\n# y_dataframe=pd.DataFrame(y_pred_forest_FS,columns=['RandomF_pred'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_RForesr_FS.csv',index=False)\n","75fb222b":"param_reg_fs = { 'max_features': [0.3,.4,.5, 0.6,.7,.8, 1],\n        'n_estimators': [50,100, 150, 200], \n         'base_estimator__max_depth': [3, 5,10,15, 20]}","c95ce4f3":"# model_reg_fs=BaggingRegressor(base_estimator=DecisionTreeRegressor(),)\n\n# model_GSsearch_fs = GridSearchCV(model_reg_fs,param_reg_fs, cv=9)\n# model_GSsearch_fs.fit(X_train_FS, y_train_FS)\n\n# train_score_reg_selection=model_GSsearch_fs.score(X_train_FS, y_train_FS)\n\n# # test_score_reg_selection=model_GSsearch_fs.score(X_test_FS, y_test_FS)\n \n# print(\"train score for Bagging model with selection features:\",\n#       train_score_reg_selection)\n# print(\"test score for Bagging model  with selection features:\",\n#       test_score_reg_selection)\n\n","a8449585":"# pred_bagging_fs=model_GSsearch_fs.predict(test_X_FS)\n\n\n\n# y_dataframe=pd.DataFrame(pred_bagging_fs,columns=['RandomF_pred'])\n# dataset=pd.concat([sampl_sub['Id'],y_dataframe],axis=1)\n# dataset.columns=['Id','SalePrice']\n# dataset.to_csv('sample_submission_bagg_FS.csv',index=False)\n\n","3892b5af":"We can see here that some data in GrLivArea are bigger than the rest, but they have lower SalePrice. That is unsual behavior and we need to drop them","792696ea":"\nOverallQual outliers looks normal","45509c7d":"_______________________________","152e803e":"<b><I>The score of RMSE for RidgeCV model: 0.42697<\/b><\/I>","9ee05cd1":"<b><I>The Score of RMSE for Decision Tree Regressor model is: 0.25538<\/b><\/I>","e4144d89":"# 6. Machine Learning","0d2152ef":"# Introduction","ceb3976c":"### 6.3.1 Without Scalar Features","df7629fb":"For the model, we will try both models with standard scalar and without it to see the differences in performance","d33c3a5b":"Category with garage car size 4 got a less sales prices than the other categories. further investigation will be done in the next section","8e1e9036":"# The Final Result","f6c031ee":"### 6.5.2 With Scalar Features","0b17e1ee":"Note: you can try all the models, but for fast commit I make it most of them in comments","37692a4f":"<b><I>The average score of the two models: Ridge and Bagging:0.13164<\/b><\/I>","46a6d0fd":"The GrLiveArea is the size of the living area in square feet. the bigger the living area the higher sales prices\n\n","acbc4a39":"## 6.6 Models with Feature Selection","028d2cdf":"From the above boxplot, it is noticed there is almost zero correlation between Year of sales and sales Price, so it shouldn't be considered in our model","5c5ebb76":"# 5. Feature selection","2497552e":" From observation we can see that most of the garages are built in the same years of the houses\n\nso we will replace the nan values in the garage year built to the value of the house year built in the same row","2c04f85c":"From the plot we notice most columns have a positive skew and z score above the mean\n\n","39621951":"Each OverallQual repreasent Rate of the material and finish of the house. From the plot above, If the house gets a high rate the price will increase","ef39c88b":"<b><I>The RMSE score for Random forest model with standarize is: 1.15475<\/b><\/I>","25b51c84":"## 6.3 Bagging Regressor\n","d55f897f":"<b><I>The score of Bagging Regressor with standarize is: 0.558<\/b><\/I>","87856d43":"There are some columns are regonized as numerical columns, but after  observing them, we noticed these columns should be categorical rather than numerical, so we will change them\n\n\n","9060341b":"### 6.5.1 Without Scalar Features","9920b34d":"After observation the data in GarageCars, we notice unsual behavior on category 4.\n\nNote: deleting this category has caused the RMSE to rise than before,so we decided to keep it\n","518ddef0":"### Random Forest","2227c6a9":"<b><I>The score of Decision Tree model with standarize is: 1.07622<\/b><\/I>","692b9bc8":"### Baggin regressor with Feature Selection","8fc240f0":"### 3.3 Testing Dataframe","b10e8165":"## 6.5 Decision Tree Regressor","78841daa":"_________________________________","2566f4e8":"## 2.2 Statistics","e54b6869":"##### line of code to help get the column index for both train and test dataframe\ncolumn_index=np.argmax(np.array(DataframeName.columns=='columnName'))\n","172c2716":"After testing five models:<b> LassCV,RidgeCV,Bagging Regressor,Random Forest Regressor, and Decision Tree<\/b>, with both Standarization and feature selection.Using score,RMSE and cross validation metrics.<br> \n\nThe first best RMSE score is form the average of two models<b> Ridge and Bagging regressor<\/b> with a score<b> 0.131<\/b>. The secound best model give us RMSE score with<b> 0.148 <\/b>is the Random Forest Tree, without using standarization or a feature selection. \n\nBoth models have more ability to react and make a good prediction to new data it will get. In a simple word these models can generalize a new data","5de0d5e0":"The Score of RMSE for LassoCV model is: 6.90683","85155312":"<b><I>The RMSE score for Random forest model is: 0.148<\/b><\/I>","b91b20da":"##### Line of code to help get the column index for both train and test dataframe\ncolumn_index=np.argmax(np.array(DataframeName.columns=='columnName'))\n","f36ca286":"From the above scatter plot, it is noticed there is almost zero correlation between Value of miscellaneous feature and sales Price, so it shouldn't be considered in our model","a00ef43c":"## 6.4 Random forest","4146eeb7":"### 3.2 Cleaning Outliers From Top Features","36d32c38":"### 6.3.2 With Scalar Features","33563004":" From observation we can see that most of the garages are built in the same years of the houses\n\nso we will replace the nan values in the garage year built to the value of the house year built in the same row","0aabdb69":"There are some columns are regonized as numerical columns, but after  observing them, we noticed these columns should be categorical rather than numerical, so we will change them\n\n","2734e898":"### 6.4.2 With Scalar Features","690409aa":"<b><I>The RMSE score with Feature Selection: 0.15689<\/b><\/I>","dcb7aeb5":"Some columns in both dataframes contain categorical values. For some of these categories there is a category called NA (it doesn't mean nulls value) so we will change it through the function below\n\nWe will define function to replace all the np.nan (which are another type of categoricals but enter it as Na) from specific columns, to more meaningful value or string\n","e51b7509":"To get the most possible minimum RMSE score we leave the rest of the outliers as they are,because from practicing we found dropping any more outliers can cause the RMSE to rise","35ae4984":"## 2.1 Data Overview","69e886e8":"# 2. Exploratory Data Analysis","0d6c0539":"# 1. Import Training and Testing Dataset","a66a556e":"Compare the categoricals between the train and test dataframes. If we find a categorical column in one dataframe but not in the other, we will add the missing column with value of zero","4bcf5cf9":"The SalePrices is Positive skew this is mean there are outliers greater than the mean.","7bdab7c7":"<b><I>The score of RMSE for RidgeCV model with standarize is: 6.64239<\/b><\/I>","69a06e31":"## 6.2 Ridge","b00799c8":"### 6.2.1 Without Scalar Features","e6479341":"<b><I>The RMSE score with Feature Selection: 0.15814<\/b><\/I>","49806418":"Since we can't delete all the outliers, we will check the top features to see the unusual data behavior, and drop the rows of these data ","de2a2e5a":"# 3. Data Cleaning","f89d0a2f":"### 6.4.1 Without Scalar Features","4d0d8b83":"### calculate the average of more than one model prediction","7065d1fe":"# Conclusion and recommendations\n\nBy studying the effects of features on the houses sales prices, We investigated, cleaned and visualized 80 features in both training and testing datasets. \nAfter that we used the dummies to convert categorical variables into indicator 0 or 1. \nThen we used Correlation for feature selection to find the strong positive and also the strong negative features ,and tried to use them with models in order to give us the best RMSE score. \nThe Best RMSE score we got was 0.131 came from the average of two models ;Ridge and Bagging regressor models, by training all the features without scaling.\nThis research will help both the sellers and the buyers to ensure they know the correct house price through the prediction in more intelligent way.\nBased on the conclusion we recommend:\n- Modeling the data on more advanced models to get the possible minimum RMSE score\n- Nowadays, a lot of houses are designed with more intelligent security, so it will be interesting to collect this kind of new feature to the other features we already have","c761e416":"### 6.2.2 With Scalar Features","b5da17c4":"convert categorical variables to \"one-hot encoded\" using .get_dummies","50bf7d5d":"##### From the plot above we can see there are a lot of outliers, but we can't drop all of them because it will affect the results of the models","b4e2c96b":"## 4.1 find categorial features","20006e9a":"<b><I>The score of the Bagging Model is: 0.14994<\/b><\/I>","118b0f1e":"# 4. Dummies","68e84dce":"<b><I>The average score of the two models: Ridge and Decision Tree:0.14914<\/b><\/I>","e8e7c9c7":"The notebook is to create an accurate prediction model for house pricing  based on historical data that has +80 features.\n\nThe features are classified to numerical and categorical data. Our task is to study the correlation between each feature, clean the data by replacing nulls, dropping unnecessary features, and converting data type for some features. After cleaning part, we will standarize the distribution of feature values to have scaled model. Then we will select features of our model and apply different regression approaches to decide what is the most accurate model.\n\nThe consequential steps to have our model are as the following:\n 1. Import libraries and datasets\n 2. Exploratory Data Analysis<br>\n      2.1 Data Overview<br>\n      2.2 Statistics<br>\n      2.3 Correlation analysis<br>\n 3. Data Cleaning<br>\n      3.1 Training dataframe cleaning<br>\n      3.2 Finding the outliers<br>\n      3.3 Teasting dataframe cleaning\n 4. Get Dummies<br>\n      4.1 find categorial features\n 5. Select Features\n 6. Apply different Prediction models<br>\n    6.1 LassoCV model with standarize<br>\n    6.2 RidgeCV model<br>\n        6.2.1 RidgeCV modelwithout standarize<br>\n        6.2.2 RidgeCV model with standarize<br>\n    6.3 Bagging regresser<br>\n        6.3.1 Bagging regrosser without standarize<br>\n        6.3.2 Bagging regrosser with standarize<br>\n    6.4 Random forest regresser<br>\n        6.4.1 Random forest regresser without standarize<br>\n        6.4.2 Random forest regresser with standarize<br>\n    6.5 Decision Tree regressor with standarize<br>\n        6.5.1 Decision Tree regressor with standarize<br>\n        6.5.2 Decision Tree regressor without standarize<br>\n    6.6 models with Feature selection<br>  \n 7. Sumbission ","cefb4323":"## 2.3 Correlation Analysis","a466087b":"### 3.1 Training Dataframe","a02893f5":"## 6.1 Lasso","28b07bbe":"# Problem Statment\n\nBuild a model to solve a problem of predicting house prices with giving the minimum RMSE score prediction on prices based on diffrent features. This program will help the house sellers and also the buyers to get the model predict the located house price based on it current features\n","e86e9d0e":"### 6.1.1 With Scalar Features"}}