{"cell_type":{"fb7e1305":"code","0f910748":"code","333297a4":"code","5b982f95":"code","639271d4":"code","4dc1ca5f":"code","acc9feca":"code","79ebb6d7":"code","b7ea508a":"code","f93056ce":"code","b3097fc7":"code","6b88cd8a":"code","985df4da":"code","f55e4b77":"code","ac13e938":"code","6b924a67":"code","84b1bed1":"code","e571bf6f":"code","42557d48":"code","470dbb01":"code","aa67a8c9":"code","4e7ba7e8":"code","ef4009c6":"code","78943d4e":"code","9de83792":"code","34516c39":"code","18f36ff3":"code","1b9a33e9":"code","e02713ee":"code","ce977023":"code","109443e8":"code","fc8157af":"code","8da0a14c":"code","8d67f5a0":"code","71ca9e36":"code","38f04ab7":"code","fe7ebae2":"code","a65ff3fe":"code","3340edda":"code","be50d133":"code","75e6c923":"code","b6633c8a":"code","7957da37":"code","8605df3f":"code","66be9926":"code","98165ba9":"code","296f363e":"code","9998ad60":"code","f10efabd":"code","ba8de37e":"code","645c382d":"code","25400113":"code","96797e7f":"code","ae3f6844":"code","8ac43e83":"code","924c7290":"code","70b28b95":"code","65c0fc7b":"code","d8a34588":"code","dc12055b":"code","565dda4d":"code","df4f4a7d":"code","9fd533ca":"code","d9468719":"code","c083e092":"code","a441d1d2":"code","ddb52b54":"code","7b759b93":"code","f6e83f8c":"code","7bf981d6":"code","e58fb88c":"code","b1d576f4":"code","d0de8f9a":"code","72af9bd1":"code","5b26c590":"code","a1195b63":"code","e8f740ca":"code","4dd80e0f":"code","35ce9b1a":"code","13a83afc":"code","d517c336":"code","624497bf":"code","0bd2b0bc":"code","4d8c7f2a":"code","5ea9769b":"code","4b9c1f1c":"code","4a916e8c":"code","8b5e7ae8":"code","564a2456":"code","1b486a07":"code","4bba9354":"code","535d60c2":"code","873608e3":"code","d6be7d90":"code","f71edbcb":"code","86d23e2b":"code","89fb573b":"code","8b806a27":"code","955de4ea":"code","c9398995":"code","4b3e32fb":"code","b321882d":"code","dc31d089":"markdown","3ff13b26":"markdown","ee4803b7":"markdown","e686d5cb":"markdown","b65d636e":"markdown","4d31f647":"markdown","bc1d9be2":"markdown","5b08c885":"markdown","d23f8436":"markdown","4350af60":"markdown","3be58d66":"markdown","5e8a5f26":"markdown","af56901f":"markdown","30abd7de":"markdown","5c6cf1f2":"markdown","66d38055":"markdown","c7e7b4a3":"markdown","959c8a8b":"markdown","c7895ab1":"markdown","b2e30eac":"markdown","65e3f6cf":"markdown","ca1309e1":"markdown","91e9e206":"markdown","7eab1687":"markdown","416ca2d3":"markdown","65f6fad4":"markdown","6cbd20e0":"markdown","8bf4871a":"markdown","b355cee6":"markdown","73485927":"markdown","cfc81cf6":"markdown","bb78c4cf":"markdown","11b7f640":"markdown","307b2fb8":"markdown","545ea3a4":"markdown","29497e12":"markdown","be8c9822":"markdown","aadc1406":"markdown","22ff8445":"markdown","8f596093":"markdown","7c5c2835":"markdown","47c3da97":"markdown","fe2789b5":"markdown","18678d3c":"markdown","070478f2":"markdown","6f83426a":"markdown","d97cb3e8":"markdown","47847809":"markdown","186aa624":"markdown","77ee15ae":"markdown","7a6d6263":"markdown","e23d63b0":"markdown","3e196861":"markdown","09fa6116":"markdown","27013aad":"markdown","487c86f5":"markdown","2f84684f":"markdown","ccb5cd39":"markdown","b5b609cc":"markdown","6ff4af6a":"markdown"},"source":{"fb7e1305":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-deep')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f910748":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\noriginal = df.copy()","333297a4":"print('Data has', df.shape[0], 'rows and', df.shape[1], 'columns')","5b982f95":"df.info()","639271d4":"df.head()","4dc1ca5f":"df.describe()","acc9feca":"import missingno as msna\nmsna.matrix(df)\nplt.show()","79ebb6d7":"df['Outcome'].value_counts(normalize = True)","b7ea508a":"plt.rcParams['figure.figsize'] = (18, 7)\n\ndef univariate_plot(x):\n    plt.subplot(121)\n    sns.distplot(x, color = 'seagreen')\n    plt.title('Probability Distribution Function', fontsize = 15)\n    plt.ylabel('Probability')\n    \n    n = len(x)\n    a = np.sort(x)\n    b = np.arange(1, 1 + n) \/ n\n    plt.subplot(122)\n    plt.plot(a, b, color = 'seagreen', marker = '.', linestyle = 'none')\n    mean_x = np.mean(x)\n    plt.axvline(mean_x, label = 'Mean', color = 'k')\n    skew = '               Skew : ' + str(round(x.skew(), 2))\n    plt.annotate(skew, xy = (mean_x, 0.5), fontsize = 16)\n    plt.legend()\n    plt.title('Empirical Cumulative Distribution Function', fontsize = 15)","f93056ce":"univariate_plot(df['Age'])","b3097fc7":"univariate_plot(df['BMI'])","6b88cd8a":"univariate_plot(df['Pregnancies'])","985df4da":"univariate_plot(df['Glucose'])","f55e4b77":"univariate_plot(df['BloodPressure'])","ac13e938":"univariate_plot(df['SkinThickness'])","6b924a67":"univariate_plot(df['Insulin'])","84b1bed1":"univariate_plot(df['DiabetesPedigreeFunction'])","e571bf6f":"df.loc[df['Pregnancies'] > 10, :]","42557d48":"df.loc[df['BMI'] == 0]","470dbb01":"df.loc[df['Glucose'] == 0]","aa67a8c9":"df.loc[df['BloodPressure'] == 0]","4e7ba7e8":"df.loc[df['SkinThickness'] == 0]","ef4009c6":"df.loc[df['Insulin'] == 0]","78943d4e":"drop_index = df.loc[(df['BMI'] == 0) & (df['BloodPressure'] == 0) & (df['SkinThickness'] == 0) & (df['Insulin'] == 0), :].index\ndf.drop(drop_index, axis = 0, inplace = True)","9de83792":"for i in df.columns.tolist():\n    print(i, '-', len(df.loc[df[i] == 0, :]))","34516c39":"df.sample(20)","18f36ff3":"df['Outcome'] = df['Outcome'].astype('str')","1b9a33e9":"plt.rcParams['figure.figsize'] = (17, 6)\n\ndef plot_box(x):\n    plt.subplot(121)\n    sns.boxplot(y = x, x = 'Outcome', data = df)\n    plt.title(x, fontsize = 16)\n    \n    plt.subplot(122)\n    sns.violinplot(y = x, x = 'Outcome', data = df)\n    plt.title(x, fontsize = 16)","e02713ee":"plot_box('Age')","ce977023":"plot_box('Pregnancies')","109443e8":"plot_box('Insulin')","fc8157af":"plot_box('BMI')","8da0a14c":"plot_box('BloodPressure')","8d67f5a0":"plot_box('SkinThickness')","71ca9e36":"plot_box('DiabetesPedigreeFunction')","38f04ab7":"plot_box('Glucose')","fe7ebae2":"df['BMI'] = np.where(df['BMI'] == 0, np.nan, df['BMI'])\ndf['Glucose'] = np.where(df['Glucose'] == 0, np.nan, df['Glucose'])\ndf['BloodPressure'] = np.where(df['BloodPressure'] == 0, np.nan, df['BloodPressure'])\ndf['SkinThickness'] = np.where(df['SkinThickness'] == 0, np.nan, df['SkinThickness'])","a65ff3fe":"df['BMI'].fillna(27, inplace = True)\ndf['Glucose'].fillna(df['Glucose'].mean(), inplace = True)\ndf['BloodPressure'].fillna(df['BloodPressure'].mean(), inplace = True)\ndf['SkinThickness'].fillna(df['SkinThickness'].mean(), inplace = True)","3340edda":"df.describe()","be50d133":"df.loc[df['SkinThickness'] > 60]","75e6c923":"df.drop(df.loc[df['SkinThickness'] > 60].index, axis = 0, inplace = True)","b6633c8a":"df.loc[df['BMI'] > 55]","7957da37":"df.drop(df.loc[df['BMI'] > 55].index, axis = 0, inplace = True)","8605df3f":"df.describe()","66be9926":"for i in df.select_dtypes(['int64', 'float64']).columns.tolist():\n    print(i, ':', df[i].skew())","98165ba9":"def skew_visual():    \n    plt.rcParams['figure.figsize'] = (20, 8)\n\n    plt.subplot(241)\n    sns.distplot(df['Pregnancies'], color = 'k')\n    plt.title('PDF - Pregnancies')\n\n    plt.subplot(242)\n    sns.distplot(df['Glucose'], color = 'k')\n    plt.title('PDF - Glucose')\n\n    plt.subplot(243)\n    sns.distplot(df['BloodPressure'], color = 'k')\n    plt.title('PDF - BloodPressure')\n\n    plt.subplot(244)\n    sns.distplot(df['Insulin'], color = 'k')\n    plt.title('PDF - Insulin')\n\n    plt.subplot(245)\n    sns.distplot(df['SkinThickness'], color = 'k')\n    plt.title('PDF - SkinThickness')\n\n    plt.subplot(246)\n    sns.distplot(df['Age'], color = 'k')\n    plt.title('PDF - Age')\n\n    plt.subplot(247)\n    sns.distplot(df['DiabetesPedigreeFunction'], color = 'k')\n    plt.title('PDF - DiabetesPedigreeFunction')\n\n    plt.subplot(248)\n    sns.distplot(df['BMI'], color = 'k')\n    plt.title('PDF - BMI')\n    plt.tight_layout()","296f363e":"skew_visual()","9998ad60":"for i in ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']:\n    print(i, ':', np.sqrt(df[i]).skew())","f10efabd":"for i in ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']:\n    print(i, ':', np.log1p(df[i]).skew())","ba8de37e":"pd.DataFrame({'Feature' : ['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction'],\n             'Actual' : [df[i].skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Squared' : [np.sqrt(df[i]).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Cubed' : [(df[i] ** (1\/3)).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]],\n             'Logged' : [np.log1p(df[i]).skew() for i in df[['Pregnancies', 'Insulin', 'Age', 'DiabetesPedigreeFunction']]]})","645c382d":"df_v1 = df.copy()","25400113":"df['Pregnancies_trans'] = np.sqrt(df['Pregnancies'])\ndf['Insulin_trans'] = np.log1p(df['Insulin'])\ndf['Age_trans'] = np.log1p(df['Age'])\ndf['DiabetesPedigreeFunction_trans'] = df['DiabetesPedigreeFunction'] ** (1\/3)","96797e7f":"def skew_visual_trans():    \n    plt.rcParams['figure.figsize'] = (20, 8)\n\n    plt.subplot(241)\n    sns.distplot(df['Pregnancies_trans'], color = 'k')\n    plt.title('PDF - Pregnancies')\n\n    plt.subplot(242)\n    sns.distplot(df['Glucose'], color = 'k')\n    plt.title('PDF - Glucose')\n\n    plt.subplot(243)\n    sns.distplot(df['BloodPressure'], color = 'k')\n    plt.title('PDF - BloodPressure')\n\n    plt.subplot(244)\n    sns.distplot(df['Insulin_trans'], color = 'k')\n    plt.title('PDF - Insulin')\n\n    plt.subplot(245)\n    sns.distplot(df['SkinThickness'], color = 'k')\n    plt.title('PDF - SkinThickness')\n\n    plt.subplot(246)\n    sns.distplot(df['Age_trans'], color = 'k')\n    plt.title('PDF - Age')\n\n    plt.subplot(247)\n    sns.distplot(df['DiabetesPedigreeFunction_trans'], color = 'k')\n    plt.title('PDF - DiabetesPedigreeFunction')\n\n    plt.subplot(248)\n    sns.distplot(df['BMI'], color = 'k')\n    plt.title('PDF - BMI')\n    plt.tight_layout()","ae3f6844":"skew_visual_trans()","8ac43e83":"df['Pregnancies_bin'] = np.where(df['Pregnancies'] == 0, 0,\n                                np.where((df['Pregnancies'] > 0) & (df['Pregnancies'] <= 5), 1,\n                                        np.where((df['Pregnancies'] > 5) & (df['Pregnancies'] <= 10), 2, 3)))","924c7290":"df['Insulin_bin'] = np.where(df['Insulin'] == 0, 0,\n                                np.where((df['Insulin'] > 0) & (df['Insulin'] <= 50), 1,\n                                        np.where((df['Insulin'] > 50) & (df['Insulin'] <= 200), 2, 3)))","70b28b95":"bins = np.arange(0, 100, 10)\nnames = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ndf['Age_bin'] = pd.cut(df['Age'], bins = bins, labels = names)","65c0fc7b":"df.sample(10)","d8a34588":"df_v2 = df.copy()","dc12055b":"df = df[['Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction_trans', 'Age_bin', 'Insulin_bin', 'Pregnancies_bin', 'Outcome']]","565dda4d":"df.sample(10)","df4f4a7d":"df['Outcome'] = df['Outcome'].astype('int64')","9fd533ca":"df_scaled_mms = df.copy()","d9468719":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()","c083e092":"cols = df_scaled_mms.columns.tolist()\n\ndf_scaled_mms = pd.DataFrame(mms.fit_transform(df), columns = cols)","a441d1d2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","ddb52b54":"df_scaled_sc = pd.DataFrame(sc.fit_transform(df), columns = cols)","7b759b93":"df_scaled_mms.describe()","f6e83f8c":"df_scaled_sc.describe()","7bf981d6":"plt.rcParams['figure.figsize'] = (10, 8)\n\nsns.heatmap(df.corr() * 100, annot = True, cmap = 'coolwarm')\nplt.title('Correlation - Before Scaling', fontsize = 16)\nplt.show()","e58fb88c":"sns.heatmap(df_scaled_mms.corr() * 100, annot = True, cmap = 'plasma')\nplt.title('Correlation - Normalized Data', fontsize = 16)\nplt.show()","b1d576f4":"sns.heatmap(df_scaled_sc.corr() * 100, annot = True, cmap = 'Set1')\nplt.title('Correlation - Standardized Data', fontsize = 16)\nplt.show()","d0de8f9a":"df_scaled_mms.head()","72af9bd1":"from sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score","5b26c590":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","a1195b63":"X = df_scaled_mms.drop(columns = ['Insulin_bin', 'Outcome'])\nY = df_scaled_mms['Outcome']","e8f740ca":"train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.25, stratify = Y, random_state = 42)","4dd80e0f":"print('Train_X - ', train_x.shape)\nprint('Test_X - ', test_x.shape)\nprint('Train_Y - ', train_y.shape)\nprint('Test_Y - ', test_y.shape)","35ce9b1a":"def model_build(x):\n    model = x\n    model.fit(train_x, train_y)\n    pred = model.predict(test_x)\n    print('Accuracy :', accuracy_score(test_y, pred))\n    print('F1 :', f1_score(test_y, pred))\n    kfold = KFold(n_splits = 5, random_state = 56)\n    cv_res = cross_val_score(x, train_x, train_y, cv = kfold, scoring = 'accuracy')\n    print('CV Accuracy Score - 5 Splits :', cv_res.mean())\n    print('Precision :', precision_score(test_y, pred))\n    print('Recall :', recall_score(test_y, pred))","13a83afc":"model_build(LogisticRegression())","d517c336":"model_build(DecisionTreeClassifier())","624497bf":"model_build(RandomForestClassifier())","0bd2b0bc":"model_build(KNeighborsClassifier())","4d8c7f2a":"model_build(SVC())","5ea9769b":"model_build(GaussianNB())","4b9c1f1c":"rf = RandomForestClassifier()\nrf.fit(train_x, train_y)\npred = rf.predict(test_x)","4a916e8c":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","8b5e7ae8":"print(rf.feature_importances_)","564a2456":"print(train_x.columns)","1b486a07":"from sklearn.ensemble import VotingClassifier","4bba9354":"estimators = [('DT', DecisionTreeClassifier()), ('RF', RandomForestClassifier())]\nvc = VotingClassifier(estimators)\nvc.fit(train_x, train_y)\npred = vc.predict(test_x)","535d60c2":"print('Accuracy :', accuracy_score(test_y, pred))\nprint('Precision :', precision_score(test_y, pred))\nprint('Recall :', recall_score(test_y, pred))","873608e3":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","d6be7d90":"depth = []\nestimators = []\nrecall = []\n\nfor i in np.arange(2, 10):\n    for j in np.arange(100, 300, 10):\n        rf = RandomForestClassifier(max_depth = i, n_estimators = j, n_jobs = -1)\n        rf.fit(train_x, train_y)\n        pred = rf.predict(test_x)\n        r = recall_score(test_y, pred)\n        depth.append(i)\n        estimators.append(j)\n        recall.append(r)","f71edbcb":"res = pd.DataFrame({'Depth' : depth,\n                   'Estimators' : estimators,\n                   'Recall Score' : recall})\nres.sort_values(by = 'Recall Score', ascending = False).head()","86d23e2b":"rf = RandomForestClassifier(max_depth = 9, n_estimators = 120, n_jobs = -1)\nrf.fit(train_x, train_y)\npred = rf.predict(test_x)","89fb573b":"pd.DataFrame(confusion_matrix(test_y, pred), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","8b806a27":"prob = rf.predict_proba(test_x)[:, 1]\n\nprobabilities = pd.DataFrame({'Probability' : prob,\n             'P(0.4)' : '',\n             'P(0.5)' : '',\n             'P(0.6)' : ''})","955de4ea":"probabilities['P(0.4)'] = np.where(probabilities['Probability'] > 0.4, 1, 0)\nprobabilities['P(0.5)'] = np.where(probabilities['Probability'] > 0.5, 1, 0)\nprobabilities['P(0.6)'] = np.where(probabilities['Probability'] > 0.6, 1, 0)","c9398995":"probabilities.head()","4b3e32fb":"pd.DataFrame(confusion_matrix(test_y, probabilities['P(0.4)']), index = [\"Actual 0's\", \"Actual 1's\"], columns = [\"Predicted 0's\", \"Predicted 1's\"]).style.background_gradient(cmap = 'Set2')","b321882d":"recall_score(test_y, probabilities['P(0.4)'])","dc31d089":"### Bivariate Analysis","3ff13b26":"When we slice the dataset with BMI values equal to zero, we also see zero values in neighboring features as well. These observations must be erratic.","ee4803b7":"Now the question arises, why did we chose the recall score as the evaluation metric?\n\nAs per the problem statement, the model should classify the diabetic people rightly. The impact would be more if diabetic people are classified as normal than the vice versa.\n\nAs FP has more influence, let's check the recall score of the model. Recall is the measure of the total 1's correctly classified out of the actual 1's.\n\nLooking at the problem statement, our model should have high recall score.","e686d5cb":"# A Beginner's approach to Diabetes Classification","b65d636e":"I guess selecting 0.4 as the threshold would work out. Let's plot the confusion matrix for the same and check the recall score.","4d31f647":"The maximum number of pregnancies is 17. Let's check the dataset with observations having more than 10 pregnancies. ","bc1d9be2":"We'll drop the entries which have abnormal entries. We'll have a threshold for these features and drop the observations which exceed them. Although dropping entries should not be the first option, I'm doing it as there are not many observations which we would lose by doing so.","5b08c885":"As we have checked the distribution of features, let's check the dataset to get answers for our questions we have framed by looking at the summary statistics.","d23f8436":"This is a simple function which gets a feature as an input argument and plot the PDF and ECDF of the feature.\n\nIn addition to this, we'll also use the function to calculate the skewness of the feature.\n\n* Skewness: Distortion present in the data. If the feature is not normally distributed (Gaussian), it may skew either towards the left or right. Most models would perform better with normally distributed data, so let's check how the features are distributed.","4350af60":"65% of the people in the dataset are normal while the rest are diabetic.\n\nIs this an imbalanced dataset? I would hesitate to say so, we still have 35% of the class of interest. So let's go ahead.","3be58d66":"The summary statistics is the best measure of getting a quick insight about the accuracy of the data. The above abnormalities could be either erratic or rare occurences. We'll check the dataset further during our EDA process.","5e8a5f26":"There are some points to note here.\n\n* The minimum value of all the features is 0.\n* The maximum is at 1.\n* The mean is in the same range across the features.","af56901f":"Here it is...\n\nWe have a dataset on health attributes of several patients and we need to classify whether the patient has diabetes or not.\n\nPeople do talk about being independant and dependant. What are they?\n\nSimple!\n\n* **Independant Features** : These are the predictor variables using which you predict the outcome.\n* **Dependant Feature** : This is the target variable which we are going to predict.\n\nOkay, what independant features do we have for now?\n* Pregnancies\n* Glucose\n* Blood Pressure\n* Skin Thickess\n* Insulin\n* BMI\n* Diabetes Pedigree Function\n* Age\n\nBased on these attributes, we will predict the target variable.\n* Outcome","30abd7de":"Do you see something abnormal? Just look at the minimum value of each feature.\n\n* Pregnancies : 0\n    * This is acceptable as this dataset has the mix of genders.\n* Glucose\n    * Can Glucose level be zero in any case? Something to ponder over.\n* Blood Pressure\n    * Even patients with low blood pressure would never have BP as 0.\n* Skin Thickness\n    * Can skin thickness be zero? I do not think so, but we'll note this for further investigation.\n* Insulin\n    * Insulin could be very low in fasting conditions. But zero? Okay, hold it for now!\n* BMI\n    * Zero BMI? Impossible!","5c6cf1f2":"As we've tried with various models, let's take the model with the good recall score and check the confusion matrix for the predictions.","66d38055":"What do we observe?\n\n* 105 normal people are identified as normal (TP)\n* 39 diabetic people are classified as diabetic (TN)\n* 19 normal people are identified as diabetic (FN)\n* 27 diabetic people are classified as normal (FP)","c7e7b4a3":"From the summary statistics, we have observed that the range of values in every feature differ. Let's apply the feature scaling techniques to avoid this.\n\nThere are different scaling techniques.\n* Normalization - This would scale the values between a minimum and maximum value. In most cases, it would be between 0 and 1.\n* Standardization - This technique is aimed at making the standard deviation of the feature to zero.\n\nI'm not including the mathematical expressions for the above techniques. We can do this manually but let's go with the predefined methods available in Scikit - Learn's preprocessing module.","959c8a8b":"It is evident from the above figure that the dataset does not have any missing values.\n\n*Oh! You noticed that from the df.info() method itself? Cool! You've got an eagle's eye*","c7895ab1":"### Target Variable","b2e30eac":"From the heatmaps, we did not observe any difference in correlation among the dataframes. So let's go ahead with the normalized dataframe and include only the features which most correlate with the target variable.","65e3f6cf":"## Exploratory Data Analysis","ca1309e1":"The age of people who had more than 10 pregnancies seem to be either middle staged or old. Let's keep this feature as such.","91e9e206":"This did not turn fruitful as the combination of the models is putting the recall score down. We can try any combination using the Voting Classifier.","7eab1687":"Hello Viewers!\n\nThanks for taking a moment to stop by and looking at my Kernel. This notebook is aimed at drafting a simple approach to solve the diabetes classification problem.\n\nWithout talking much, let's get into the solution.","416ca2d3":"Then we'll split the dataset into train and test datasets.","65f6fad4":"We'll write a function which takes a model name as an argument to fit, predict and evaluate the model.","6cbd20e0":"Confused by the terms?\n\nThese are the statistical methods to check the distribution of the data.\n\n* **Probability Distribution Function**:\nProbability of getting a value, if it is randomly chosen from the data.\n\n* **Empirical Cumulative Distribution Function**:\nProbability of getting less than a value, if it is randomly chosen from the data.\n\nWe'll check the plots to further understand this definitions.\n\nShall we practise writing simple functions as part of our analysis?","8bf4871a":"## But, Where's the data?","b355cee6":"## Univariate","73485927":"### PDF and ECDF - Predictor Variables","cfc81cf6":"Now, let's try to find the optimal parameters to be given to the Random Forest Classifier by trying out with different combinations of values.","bb78c4cf":"### Feature Scaling","11b7f640":"As we have just completed the bivariate analysis, the common thing we have noticed that there were many outliers present in the features. We'll treat them in the subsequent sections.","307b2fb8":"The Age feature is right skewed as you see the tail is long in the right end. How to cross verify this? The skewness factor is 1.13, which is positive. The positive skewness means that the feature is right skewed.\n\nFrom the ECDF graph, it is observed the probability of getting an observation with age less than the mean age is ~60%.","545ea3a4":"Let us adjust the thresholds by using the predict_proba method. \n\nWe'll use three thresholds - 0.4, 0.5 and 0.6. The model would classify as 1, if the user defined threshold is exceeded. We'll compare the recall scores for different thresholds and conclude on the best one.","29497e12":"Let's run some simple scripts to check the shape, info and summary statistics of the dataset.","be8c9822":"Let's check the summary statistics of the dataset again to verify whether the changes are applied.","aadc1406":"We've dropped the observations which have zero values in all the four features. ","22ff8445":"## Importing Libraries and Dataset","8f596093":"Let's segregate the independant and dependant features.","7c5c2835":"As we have converted them to null entries, let's impute them with fillna() method.","47c3da97":"Let's first substitute the zero values in the features with np.NaN so that we can go ahead and simply impute the entries as the next step.","fe2789b5":"We can further improve the model by doing feature engineering(which requires the subject matter expertise) and bring in various ensemble techniques which are not covered in this kernel. \n\nHope you enjoyed reading this notebook. Kindly upvote and leave a comment if you like my work. Please let me know your suggestions in the comments section. Thanks!","18678d3c":"Let's see a voting classifier would help us get high recall score than 59, which the Random Forest baseline model has scored.","070478f2":"The transformations did not give the expected result, as we see some distortions. We'll try binning the values in those features to get rid of it.","6f83426a":"From the above dataframe, max_depth of 9 and 120 estimators are giving a recall score of 59. I do not see any improvement. Let's try some other techniques.","d97cb3e8":"As we have imported the required libraries, let us grab the dataset. I'm storing a copy of it, as we may require it in the future.","47847809":"Let's check the composition of the target varible. \n\nHere 0 corresponds to being normal and 1 to diabetic. We'll use the normalize parameter to tell the value_counts() method that we need the percentage rather than the actual numbers.","186aa624":"## Feature Transformation","77ee15ae":"Let's save a copy of the dataset as we are going to transform the features. Let's create new features with the transformations applied and we can drop the redundant features later. We'll plot the distribution of the features with the new features to check h","7a6d6263":"From the above plot, it is obvious that there are four features which are skewed. Let's check different transformations to see whether they can be fixed.","e23d63b0":"Let's check the correlation matrix for each of the dataframes to check the correlation between the features.","3e196861":"We have did some changes and they definitely would have influenced the skewness of the feature. We'll use the skew() method to check the skewness. Let's also write a function to plot the distribution of the features.","09fa6116":"We'll import the required libraries for model building and the evaluation of the model.","27013aad":"## Model Building","487c86f5":"The standardized dataset has the unit standard deviation (1). So we have applied two techniques and stored in different dataframes.","2f84684f":"## Check Point - Quick Inspection of the Data","ccb5cd39":"We've converted the target variable to string type as this would help us in plotting doing the bivariate analysis.","b5b609cc":"Looking at the confusion matrix, the model is now classifying the actual 1's better than before. The recall score has improved from the earlier approaches.","6ff4af6a":"Don't we think it is better to have a dataframe with the feature names and the skewness factor for each of them. We can compare the skewness by appyling different transformations."}}