{"cell_type":{"01e9f1e2":"code","272ac17d":"code","1da3842b":"code","be99c93d":"code","2acdfd18":"code","3210a439":"code","62113f99":"code","802e84fe":"code","8e500163":"code","d8c656eb":"code","703f3cd4":"code","e41403ae":"code","870b82ae":"code","9c75735d":"code","5e58aa45":"code","6fa05b25":"code","80ee220b":"code","1cbf2ab0":"code","8f31179f":"code","fee5d740":"code","ec1dce61":"code","7f3af809":"code","85342daa":"code","73eefd2b":"markdown","13ee132d":"markdown","e3adfb41":"markdown","9ed6b858":"markdown","046f9bbf":"markdown","c6e8a0ff":"markdown","e09d7e6a":"markdown","a2a824c6":"markdown","90005269":"markdown","88f0f888":"markdown","cfade32e":"markdown","e29fef57":"markdown","76b8223b":"markdown","5f1182f0":"markdown","30a88c73":"markdown","bfa7883c":"markdown","d56016aa":"markdown","6f436f92":"markdown","c8aebf58":"markdown"},"source":{"01e9f1e2":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","272ac17d":"df=pd.read_csv(\"..\/input\/BlackFriday.csv\")\ndf.head()","1da3842b":"print(\"Number of unique users:\",len(df.User_ID.unique()))\nprint(\"Number of unique products:\",len(df.Product_ID.unique()))\nprint(\"Number of unique occupations recorded:\",len(df.Occupation.unique()))","be99c93d":"df['Product_Category_2'] = df['Product_Category_2'].fillna(0)\ndf['Product_Category_3'] = df['Product_Category_3'].fillna(0)","2acdfd18":"user_profiles=df[['User_ID','Gender','Age','Occupation','City_Category','Stay_In_Current_City_Years','Marital_Status']].drop_duplicates()\nuser_profiles.head()","3210a439":"import matplotlib.pyplot as plt\n\ncolumns=['Gender','Age','City_Category','Stay_In_Current_City_Years','Marital_Status']\nfig = plt.figure(figsize=(30, 20))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, (len(columns)**2)+1):\n    ax=fig.add_subplot(len(columns), len(columns), i)\n    ax=sns.countplot(user_profiles[columns[int((i-1)\/len(columns))]],hue=columns[((i-1)%len(columns))],data=user_profiles)\n    ax=plt.legend(loc='best')\n\nplt.show()","62113f99":"columns=['Age','Gender','Occupation','City_Category','Stay_In_Current_City_Years','Marital_Status','Product_Category_1','Product_Category_2','Product_Category_3']\nfig = plt.figure(figsize=(30, 20))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, (len(columns))+1):\n    temp=df[['User_ID',columns[i-1],'Purchase']].groupby(['User_ID',columns[i-1]]).agg('sum').reset_index().sort_values('Purchase',ascending=False)\n    temp=temp.sort_values(columns[i-1]) \n    ax=fig.add_subplot(3, 3, i)\n    ax=sns.boxplot(x=columns[i-1],y=np.log(temp.Purchase),data=temp)","802e84fe":"columns=['Age','Gender','Occupation','City_Category','Stay_In_Current_City_Years','Marital_Status']\nfig = plt.figure(figsize=(20, 10))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, (len(columns))+1):\n    label=[]\n    temp=df[['User_ID',columns[i-1],'Purchase']].groupby(['User_ID',columns[i-1]]).agg('sum').reset_index().sort_values('Purchase',ascending=False)\n    ax=fig.add_subplot(2, 3, i)\n    for j in list(df[columns[i-1]].unique()):\n        hist_data=temp[temp[columns[i-1]]==j]\n        ax=sns.distplot(np.log(hist_data.Purchase),hist=False,label=j)\n        label.append(j)","8e500163":"def product_popularity_barplot(Category):    \n    by_cat=df[['Product_Category_1',Category,'User_ID']].groupby(['Product_Category_1',Category]).agg('count').reset_index()\n    by_cat=by_cat.pivot(index='Product_Category_1',columns=Category)['User_ID'].reset_index()\n\n    for i in by_cat.columns[1:]:\n        by_cat[i] = by_cat[i]\/df[df[Category]==i].count()[0]\n\n    by_cat=by_cat.melt('Product_Category_1', var_name=Category, value_name='Val')\n\n    columns=by_cat[Category].unique().tolist()\n    fig,ax=plt.subplots(1,len(columns),sharey=True)\n    fig.set_figheight(5)\n    fig.set_figwidth(8*len(columns))\n    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n    for i in range(0, (len(columns))):\n        temp=by_cat[by_cat[Category]==columns[i]]\n        title=str(Category)+':'+str(columns[i])  \n        sns.barplot(x='Product_Category_1',y='Val',data=temp,ax=ax[i])\n        ax[i].set_title(title,fontsize=20)","d8c656eb":"product_popularity_barplot('Gender')\nproduct_popularity_barplot('Age')\nproduct_popularity_barplot('City_Category')\nproduct_popularity_barplot('Stay_In_Current_City_Years')\nproduct_popularity_barplot('Marital_Status')","703f3cd4":"purchase_by_prod_id=df[['Product_ID','Purchase']].groupby('Product_ID').agg('sum').reset_index().sort_values('Purchase',ascending=False).head(10)\nprint(purchase_by_prod_id.head(10))","e41403ae":"purchase_by_user=df[['User_ID','Purchase']].groupby('User_ID').agg('sum').reset_index().sort_values('Purchase',ascending=False).head(10)\ntemp=df[df['User_ID'].isin(list(purchase_by_user['User_ID']))][['User_ID','Gender','Age','Occupation','City_Category',\n                                                          'Stay_In_Current_City_Years','Marital_Status']].drop_duplicates()\ntemp.merge(purchase_by_user,how='left').sort_values('Purchase',ascending=False)","870b82ae":"purchase_by_user=df[['User_ID','Purchase']].groupby('User_ID').agg('sum').reset_index().sort_values('Purchase',ascending=False)\nsns.distplot(purchase_by_user['Purchase'])\nplt.show()","9c75735d":"lst=[]\nfor item in df['User_ID'].unique():\n    lst2=list(set(df[df['User_ID']==item]['Product_ID']))\n    if len(lst2)>0:\n        lst.append(lst2)","5e58aa45":"from mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\nte=TransactionEncoder()\nte_data=te.fit(lst).transform(lst)\ndf_x=pd.DataFrame(te_data,columns=te.columns_)\nprint(df_x.head())","6fa05b25":"#get the frequent items (support >= 0.03)\nfrequent_items=apriori(df_x,use_colnames=True,min_support=0.03)","80ee220b":"frequent_items.head()","1cbf2ab0":"rules=association_rules(frequent_items,metric='lift',min_threshold=1)\nrules.antecedents=rules.antecedents.apply(lambda x: next(iter(x)))\nrules.consequents=rules.consequents.apply(lambda x: next(iter(x)))\nrules=rules.sort_values('lift',ascending=False)","8f31179f":"import plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\ninit_notebook_mode(connected=True)\n\nimport networkx as nx\n\nnx_data=rules[rules.lift>=3]\nGA=nx.from_pandas_edgelist(nx_data,source='antecedents',target='consequents',edge_attr='lift')\npos=nx.kamada_kawai_layout(GA,weight='lift')\n# pos = nx.nx_agraph.graphviz_layout(GA)\n# pos = nx.nx_agraph.graphviz_layout(GA, prog='dot')\n\nedge_trace = go.Scatter(\n    x=[],\n    y=[],\n    line=dict(width=0.5,color='#888'),\n    hoverinfo='none',\n    mode='lines')\n\nfor edge in GA.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_trace['x'] += tuple([x0, x1, None])\n    edge_trace['y'] += tuple([y0, y1, None])\n\nnode_trace = go.Scatter(\n    x=[],\n    y=[],\n    text=[],\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale='YlGnBu',\n        reversescale=True,\n        color=[],\n        size=10,\n        colorbar=dict(\n            thickness=15,\n            title='Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line=dict(width=2)))\n\nfor node in GA.nodes():\n    x, y = pos[node]\n    node_trace['x'] += tuple([x])\n    node_trace['y'] += tuple([y])\n\nfor node,adjacencies in enumerate(GA.adjacency()):\n    node_trace['marker']['color']+=tuple([len(adjacencies[1])])\n    node_info = str(adjacencies[0])+' - # of connections: '+str(len(adjacencies[1]))\n    node_trace['text']+=tuple([node_info])\n\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Network graph',\n                titlefont=dict(size=16),\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    text=\"Python code: <a href='https:\/\/plot.ly\/ipython-notebooks\/network-graphs\/'> https:\/\/plot.ly\/ipython-notebooks\/network-graphs\/<\/a>\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002 ) ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n\niplot(fig, filename='networkx')","fee5d740":"sns.heatmap(df.corr(),annot=True)","ec1dce61":"#Convert Product Category 2 and 3 into integers\ndf['Product_Category_2']=df['Product_Category_2'].astype(int)\ndf['Product_Category_3']=df['Product_Category_3'].astype(int)\n\n#remove Product ID and User ID\ndata=df.drop(['Product_ID','User_ID'],axis=1)\n\n#label categorical variables\ndata['Gender']=data['Gender'].map( {'M': 0, 'F': 1} ).astype(int)\ndata['City_Category']=data['City_Category'].map( {'A': 0, 'B': 1, 'C':2} ).astype(int)\ndata['Age']=data['Age'].map( {'0-17': 0, '18-25': 1, '26-35': 2,'36-45':3,'46-50':4,\n                         '51-55':5,'55+':6} ).astype(int)\ndata['Stay_In_Current_City_Years']=data['Stay_In_Current_City_Years'].map( {'0': 0, '1': 1, '2': 2,'3':3,'4+':4}).astype(int)\n\n#Get an array of feature variables X and target variable y\nX=data.drop(['Purchase'],axis=1).values\ny=data['Purchase'].values\n\n#Select features to keep based on percentile of the highest scores\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_regression\nSelector_f = SelectPercentile(f_regression, percentile=25)\nSelector_f.fit(X,y)\n\n#get the scores of all the features\nname_score=list(zip(data.drop(['Purchase'],axis=1).columns.tolist(),Selector_f.scores_))\nname_score_df=pd.DataFrame(data=name_score,columns=['Feat_names','F_scores'])\nname_score_df.sort_values('F_scores',ascending=False)","7f3af809":"data=df.copy()\ndata=data[['City_Category','Product_Category_1', \n       'Product_Category_3','Purchase']]\n\n#One-Hot Encoding\ndata=pd.get_dummies(data=data,columns=['City_Category','Product_Category_1','Product_Category_3'])\n\n#Avoid dummy variable trap by removing one category of each categorical feature after encoding but before training\ndata.drop(['City_Category_A','Product_Category_1_1','Product_Category_3_0'],axis=1,inplace=True)\n\nX=data.drop(['Purchase'],axis=1).values\ny=data['Purchase'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)\n\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,y_train)\n\ny_pred=regressor.predict(X_test)\nprint(\"Prediction\\n\",y_pred)\nprint(\"Actual\\n\",y_test)\n\nprint(\"R_squared Score:\",regressor.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(y_test,y_pred)\nprint(\"MAE:\",mae)\n\nfrom sklearn.metrics import mean_squared_error\nprint(\"RMSE:\",mean_squared_error(y_test,y_pred)**0.5)","85342daa":"data=df.copy()\ndata.drop(['User_ID','Product_ID'],axis=1,inplace=True)\n\ndata['Gender']=data['Gender'].map( {'M': 0, 'F': 1} ).astype(int)\n\ndata['Age']=data['Age'].map( {'0-17': 0, '18-25': 1, '26-35': 2,'36-45':3,'46-50':4,\n                         '51-55':5,'55+':6} ).astype(int)\n\ndata['City_Category']=data['City_Category'].map( {'A': 0, 'B': 1, 'C':2} ).astype(int)\n\ndata['Stay_In_Current_City_Years']=data['Stay_In_Current_City_Years'].map( {'0': 0, '1': 1, '2': 2,'3':3,'4+':4}).astype(int)\n\nX=data.drop(['Gender'],axis=1).values\ny=data['Gender'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=20,criterion='entropy',random_state=0)\nclassifier.fit(X_train,y_train)\n\ny_pred=classifier.predict(X_test)\nprint(\"Prediction:\",y_pred)\nprint(\"Actual:\",y_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix\\n\", cm)\n\nfrom sklearn.metrics import precision_score\nprint(\"Precision Score\\n\",precision_score(y_test,y_pred,average=None))\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred))","73eefd2b":"## Data Exploration","13ee132d":"### Missing Values","e3adfb41":"**Transforming the transaction data into one-hot encoded data**","9ed6b858":"## Classification: Predicting Gender (Random Forest Classifier)","046f9bbf":"### Distplot: Distribution of Purchase Amount Across Different Categories","c6e8a0ff":"**Association Rules Metrics **\n* support = how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.\n* confidence (A -> B) = support (A -> B)\/support(A). It is the probability of seeing the consequent in a transaction given that it also contains antecedent. The confidence of 1 (maximal) for a rule A->B means that the consequent and antecedent always occur together.\n* lift (A -> B) = confidence (A -> B)\/support(B). To measure how much more often the antecedent and consequent of a rule A->B occur together than we would expect if they were statistically independent. If A and B are independent, the lift score will be exactly 1.\n* leverage (A -> B = support (A -> B) - support(A) x support (B). The difference between the observed frequency of A and B appearing together and the frequency that would be expected if A and B were independent. Leverage value of 0 indicates independence.\n* conviction (A -> B) = (1-support(B))\/(1-confidence(A -> B)). High conviction value means that the consequent is highly depending on the antecedent. If items are independent, the conviction is 1.","e09d7e6a":"### Product Popularity by Category","a2a824c6":"**Generate a list of products purchased by each User_ID**","90005269":"### Distribution of Purchase Amount (Overall)","88f0f888":"### Boxplot: Distribution of Purchase Amount per User Across Different Categories\nOverall, those who spent more are:\n* People under Age Group 26-35\n* Males\n* from City B\n\nProducts that people spend more money on:\n* Product Category 1, 5, and 8","cfade32e":"### Top 10 most purchased items","e29fef57":"## Purchase Amount Prediction - Linear Regression","76b8223b":"### Feature Selection based on F-values","5f1182f0":"### User Profiles: Count Plots Across Categorical Variables\n* 72% of the users are Males\n* 35% are from Age group: 26-35\n* 53% are from City C\n* 35% have stayed in their current city for only 1 year.\n* 58% are still single","30a88c73":"### Heatmap","bfa7883c":"## Association Rules - Apriori Algorithm","d56016aa":"### Users who purchased the most and their profiles","6f436f92":"### Select top 3 features with the highest F-scores for Linear Regression","c8aebf58":"### All User Profiles\nProfiles of all 5891 unique users"}}