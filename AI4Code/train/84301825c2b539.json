{"cell_type":{"b83fa736":"code","3df4d8e4":"code","049484a5":"code","719c0c0c":"code","905c2874":"code","f6671c87":"code","732d0164":"code","080ea514":"code","a34e271a":"code","9213a86b":"code","af4b4eee":"code","fc05c170":"code","6b6da859":"code","df27ba8f":"code","5e8e8a9f":"code","c992c1fe":"code","d385bd97":"code","7d0123dc":"code","a2a2d683":"code","768c2cc8":"code","83a1d3e1":"code","c119fb42":"code","4da021b0":"code","debff156":"code","0575aeb3":"code","247690df":"code","89aaac9e":"code","6bfe06aa":"code","e5e5d9ca":"code","b0f027a9":"code","3baec7f4":"code","3cd09673":"markdown","b66359c8":"markdown","74a6984c":"markdown","62966fcb":"markdown","9a2ef97c":"markdown","1adbc12d":"markdown","262d644d":"markdown","7827c6dd":"markdown","3b1a92fe":"markdown","132b8a1d":"markdown","d9367b62":"markdown","e294e3b7":"markdown","f4efcf83":"markdown","bb2f56e3":"markdown","b925dd44":"markdown","f64a86bd":"markdown","7f2599e8":"markdown","bd1d58d6":"markdown","62e912ea":"markdown","ae693d88":"markdown","b086e889":"markdown","fd37bf70":"markdown","9682827d":"markdown","d2f2c1fa":"markdown","daf25bd1":"markdown","e8c23147":"markdown","dcc6a19a":"markdown","05d67e11":"markdown","5ddd3852":"markdown","ef7c8632":"markdown","b76672ee":"markdown","40070b77":"markdown","b5e409fd":"markdown","a38be84d":"markdown","49845dc8":"markdown","817c5f09":"markdown","f9abe201":"markdown","9daca7d9":"markdown"},"source":{"b83fa736":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# plotting\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# data encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.pipeline import Pipeline as imb_pipeline\n\n# classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","3df4d8e4":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv',usecols = lambda column : column not in \n[\"customerID\"])\ndf.head()","049484a5":"df.info()","719c0c0c":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])","905c2874":"empty_values = []\nfor i in range(len(df['TotalCharges'])):\n    if df['TotalCharges'].iloc[i] == ' ':\n        empty_values.append(i)\nprint(\"There are empty indexes found:\", end=' ')\nprint(empty_values)\nfor i in range(len(empty_values)):\n    print(df.iloc[empty_values[i]])","f6671c87":"df[\"TotalCharges\"] =  df[\"TotalCharges\"].replace(r' ', '0')","732d0164":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])","080ea514":"df.info()","a34e271a":"for i in df.columns:\n    if i not in ['tenure','MonthlyCharges','TotalCharges']:\n        print(i,'column has',len(pd.unique(df[i])),'unique values or rather:')\n        print(pd.unique(df[i]))","9213a86b":"plt.figure(figsize=(10,6))\n\nplt.title(\"Churn chart\")\n\nsns.countplot(df['Churn'])","af4b4eee":"churn_yes = df[df.Churn == \"Yes\"].shape[0]\nchurn_no = df[df.Churn == \"No\"].shape[0]\n\nchurn_yes_percent = round((churn_yes \/ (churn_yes + churn_no) * 100),2)\nchurn_no_percent = round((churn_no \/ (churn_yes + churn_no) * 100 ),2)\n\nprint('There are',churn_yes_percent,'percent of customers that will churn and',churn_no_percent,'percent of customers that will not churn')","fc05c170":"categorial_columns = [cname for cname in df.columns if cname not in ['tenure','MonthlyCharges','TotalCharges','Churn']]\n\nprint(\"Our categorial columns:\", categorial_columns)","6b6da859":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Churn', axis=1)\ny = df['Churn']\n\n# Creating train and test subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# One Hot Encoding\nX_train = pd.get_dummies(X_train_full)\nX_valid = pd.get_dummies(X_valid_full)\n\n# For y-values we will use LabelEncoder\n\nlabel_enc  = LabelEncoder()\ny_train = label_enc.fit_transform(y_train)\ny_valid = label_enc.fit_transform(y_valid)","df27ba8f":"X_train.head()","5e8e8a9f":"from sklearn.preprocessing import RobustScaler\n\n# I use RobustScaler because it's quite robust to outliers\n\nrob_scaler = RobustScaler()\n\ncolumns_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n\nX_train[columns_to_scale] = rob_scaler.fit_transform(X_train[columns_to_scale])\nX_valid[columns_to_scale] = rob_scaler.fit_transform(X_valid[columns_to_scale])","c992c1fe":"X_valid.head()","d385bd97":"# Use GridSearchCV to find the best parameters.\n# from sklearn.model_selection import GridSearchCV\n\n# Logistic Regression \nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train, y_train)","7d0123dc":"predictions = log_reg.predict(X_valid)","a2a2d683":"# Calculate accuracy\nACC = accuracy_score(y_valid, predictions)","768c2cc8":"print(ACC)","83a1d3e1":"log_reg_cf = confusion_matrix(y_valid, predictions)\n\nfig, axes = plt.subplots(1, 1, figsize=(12, 6))\n\nsns.heatmap(log_reg_cf, annot=True, cmap=plt.cm.Pastel1)\nplt.title(\"Logistic Regression Confusion Matrix\", fontsize=14)\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\n\nplt.show()","c119fb42":"from imblearn.under_sampling import NearMiss\n\nundersample_pipeline = make_pipeline(NearMiss(sampling_strategy='majority'), log_reg)\nundersample_model = undersample_pipeline.fit(X_train, y_train)\nundersample_predictions = undersample_model.predict(X_valid)","4da021b0":"# Calculate accuracy\nACC = accuracy_score(y_valid, undersample_predictions)","debff156":"print(ACC)","0575aeb3":"log_reg_cf = confusion_matrix(y_valid, undersample_predictions)\n\nfig, axes = plt.subplots(1, 1, figsize=(12, 6))\n\nsns.heatmap(log_reg_cf, annot=True, cmap=plt.cm.Pastel1)\nplt.title(\"Logistic Regression Confusion Matrix\", fontsize=14)\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\n\nplt.show()","247690df":"from imblearn.over_sampling import SMOTE\n\n# I use other solver and increase numer of iterations because our dataset will become larger\noversample_pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), LogisticRegression(solver = 'saga', max_iter=10000))\noversample_model = oversample_pipeline.fit(X_train, y_train)\noversample_predictions = oversample_model.predict(X_valid)","89aaac9e":"# Calculate accuracy\nACC = accuracy_score(y_valid, oversample_predictions)","6bfe06aa":"print(ACC)","e5e5d9ca":"log_reg_cf = confusion_matrix(y_valid, oversample_predictions)\n\nfig, axes = plt.subplots(1, 1, figsize=(12, 6))\n\nsns.heatmap(log_reg_cf, annot=True, cmap=plt.cm.Pastel1)\nplt.title(\"Logistic Regression Confusion Matrix\", fontsize=14)\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\n\nplt.show()","b0f027a9":"# I use Grid Search to find best parameters for our model\nfrom sklearn.model_selection import GridSearchCV\n\n#Creating pipeline with data augmentation and subsequent regression\npipeline = imb_pipeline(\n                    [('nearmiss', SMOTE(sampling_strategy='minority')),\n                     ('logreg', LogisticRegression(solver = 'saga', max_iter=10000))\n                     \n])\n\nparameters = {}\nparameters['logreg__penalty'] = ['l1', 'l2']\nparameters['logreg__C'] = [i for i in range(80,420,40)]\n\nCV = GridSearchCV(pipeline, parameters, scoring = 'accuracy', n_jobs= 1)\nCV.fit(X_train, y_train)   \n\nprint('Best parameter combination for linear regression is:', CV.best_params_)","3baec7f4":"oversample_pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), LogisticRegression(solver = 'saga', penalty = 'l1', C=80, max_iter=10000))\noversample_model = oversample_pipeline.fit(X_train, y_train)\noversample_predictions = oversample_model.predict(X_valid)\n\nprint('Accuracy on validation set: %s' % (accuracy_score(y_valid, oversample_predictions)))","3cd09673":"# ****Hi everyone!****","b66359c8":"But that's not all - let's try to oversmaple our data (that is, we will artificially increase the number of customers who are going to leave)!","74a6984c":"The conclusion suggests itself: it is necessary to somehow **\"normalize\" the data** so that the model does not retrain on the prevailing data or does not fail to learn on the data that are in the minority. To do this, you can use the methods of **artificial data normalization**, which will be described below, but first, we will create a test sample (which we will normalize) and a validation sample:","62966fcb":"Wow! We got something in between the first and second options - we can say that it is \"in the neutral zone\" - according to the predictions of clients who are going to leave, it is better than the first algorithm, but worse than the second, and vice versa with clients who are going to stay.","9a2ef97c":"As we can see, there's no empty cells - that's amazing! We don`t need to think how to fill the gaps. **But there are a couple of nuances**  - almost all columns are in the \"object\" format, which is inconvenient for processing. Especially the column \"TotalCharges\", which alone contains numerical characteristics, while others are categorical. We are going to fix it:","1adbc12d":"Let's see how many unique values each categorial column contains:","262d644d":"At first, we need to convert \"TotalCharges\" to float:","7827c6dd":"For example, you decided to choose third model - we want to get acceptable results on average, let's try to improve it:","3b1a92fe":"Wow! ****Our classes are very disbalanced.**** In numbers:","132b8a1d":"Let's create train and validation datasets; we should make One Hot Encoding after splitting, not before, because our model must at the testing stage work with \"raw data\" that it sees for the first time; if you process the entire dataset, then data leakage may occur during splitting","d9367b62":"Well, looks like we achieved good accuracy - 81%! You can adjust the parameters and improve the result by yourself, but I that's all for the moment. Thank you for watching! I would be glad to receive feedback and interesting suggestions! Also, I'm ready to listen to criticism and different opinions. See you later!","e294e3b7":"So:","f4efcf83":"# 4. Oversampling data","bb2f56e3":"# 2. Base model","b925dd44":"This can be done with [SMOTE](https:\/\/www.geeksforgeeks.org\/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python\/) technique:","f64a86bd":"Well... Looks like accuracy has decreased markedly. But this is not a reason to be upset - let's take a look at confusion matrix:","7f2599e8":"Now our data looks like this:","bd1d58d6":"\u0422ow everything is exactly the opposite: our model has small False Positive value (68 values) and very big False Negative velue (440 value) - it means that our model is very bad at detecting 'non-churn' customers and quite good at detecting 'churn' customers. \n\nHere a little philosophical question already arises - which is more profitable, poorly recognizing clients who are going to leave, or spamming a bunch of clients who are definitely not going to leave? I would love to participate in the discussion :)","62e912ea":"Oops! Looks like we have empty values - lets count them:","ae693d88":"New attempt to change data format:","b086e889":"This is my first public notebook in which I will analyse [Telco Dataset](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn). Based on the available data, it is necessary to predict the behavior of the customers - whether they will stay with the operator or leave.","fd37bf70":"**Interesting fact:** for all rows with an empty value in the \"TotalCharges\" cell, the \"tenure\" cell has a value of zero, which means that these are *new* users, and we can replace the empty value with zero:","9682827d":"Good! Now we are going to transform all categorial values using [One Hot Encoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html):","d2f2c1fa":"Note: to get acquainted with the confusion matrix, I recommend [this article](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)","daf25bd1":"*Contacts:*\ntelegramm - @univanxx, instagram - @univanxx","e8c23147":"# 5. Deep look into oversampled model","dcc6a19a":"Now let's take a look at target feature - \"Churn\"","05d67e11":"References: special thanks to Janio Martinez and his [notebook](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)!","5ddd3852":"Wow! Our algotithm is pretty accurate - let's take a look at confusion matrix:","ef7c8632":"Let's take a look at our data:","b76672ee":"As we can see, our model has big False Positive value (160 values) and (relatively) small False Negative value (94 value) - it means that our model is good at detecting 'non-churn' customers and bad at detecting 'churn' customers. This is quite to be expected, since the dataset is dominated by rows with information about customers who are not going to leave. That is, **our model was retrained on the original data**. Let's try to fix it.","40070b77":"For starters, you can shrink the original dataset by reducing the number of rows with a predominant target variable. This can be done with [NearMiss technique](https:\/\/imbalanced-learn.org\/stable\/generated\/imblearn.under_sampling.NearMiss.html):","b5e409fd":"# 3. Undersampling data","a38be84d":"# 1. Primary analysis","49845dc8":"Our data is ready to implement basic algorithm - let's use linear regression:","817c5f09":"# 0. Libraries!","f9abe201":"Yay! We will return to proccesing later.","9daca7d9":"Amazing! Now we normalize the data, more precisely, columns \"tenure\", \"MonthlyCharges\" and \"TotalCharges\" so that the model can quickly establish dependencies between the data:"}}