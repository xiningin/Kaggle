{"cell_type":{"05fad557":"code","e32d73c8":"code","5968ab38":"code","aac9a6e6":"code","856291df":"code","5f1c9340":"code","450cc42f":"code","255dee85":"code","75e651d4":"code","5d9585c1":"code","c66d3222":"code","ebdc2eb8":"code","67c40c65":"code","185078c6":"code","c52c6f8f":"code","a2c37e89":"code","be01641b":"code","df43dab3":"code","291e2043":"code","2c76a816":"code","4e056b85":"code","5f23b2fe":"code","3b715156":"code","4ef78b94":"code","33f935c5":"code","96975b3f":"code","255f57b0":"code","0783a815":"code","04e3a8e5":"code","7303bf42":"code","1980ca8b":"code","86b9ae83":"code","ab720ca6":"code","d7057a55":"code","f96d5963":"code","95524867":"code","e2765826":"code","b748f98a":"code","60d90e0d":"code","76fb6484":"code","49d98220":"markdown","4f8bacd0":"markdown","5d51368f":"markdown","885e7f37":"markdown","47ef5c44":"markdown","ea322c3b":"markdown","e4efb871":"markdown","41ed4cfd":"markdown","179f47a3":"markdown","05a8bf4c":"markdown","03007f10":"markdown","49051057":"markdown","40eae5d9":"markdown","cb4bea23":"markdown","84647816":"markdown","192dfd02":"markdown","d493d3fd":"markdown","6dead3e7":"markdown","5792f8b5":"markdown","e9eeff85":"markdown","62c92970":"markdown","07324f78":"markdown","e66005d2":"markdown"},"source":{"05fad557":"# import necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew","e32d73c8":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","5968ab38":"df_train.head(5)","aac9a6e6":"df_test.head(5)","856291df":"train_ID = df_train['Id']\ntest_ID = df_test['Id']\n\ndf_train.drop(\"Id\", axis=1, inplace=True)\ndf_test.drop(\"Id\", axis=1, inplace=True)","5f1c9340":"df_train['SalePrice'].describe()","450cc42f":"sns.distplot(df_train['SalePrice'])","255dee85":"print(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","75e651d4":"data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000))","5d9585c1":"# deleting out liars\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\n# plot again\ndata = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000))","c66d3222":"sns.distplot(df_train['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint(\"\\n mu = {:.3f} and sigma = {:.3f}\\n\".format(mu, sigma))\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","ebdc2eb8":"df_train['SalePrice'] = np.log(df_train['SalePrice'])\nsns.distplot(df_train['SalePrice'], fit=norm)\n\n# get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint(\"\\n mu = {:.3f} and sigma = {:.3f}\\n\".format(mu, sigma))\n\n# Plot the distribution\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","67c40c65":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\ndata = pd.concat((df_train, df_test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Data size: {}\".format(data.shape))","185078c6":"total = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['total', 'percent'])\nmissing_data.head(35)","c52c6f8f":"corrmat = df_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","a2c37e89":"data = data.drop((missing_data[missing_data['total']>20]).index, 1)","be01641b":"data = data.drop(['Utilities'], axis=1)\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\nfor col in ('SaleType', 'Exterior2nd', 'Exterior1st', 'KitchenQual', 'Electrical', 'MSZoning'):\n    data[col] = data[col].fillna(data[col].mode()[0])\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","df43dab3":"data.isnull().sum().max()","291e2043":"data['MSSubClass'] = data['MSSubClass'].apply(str)\ncols = ['OverallCond', 'YrSold', 'MoSold']\nfor col in cols:\n    data[col] = data[col].astype(str)","2c76a816":"data.columns","4e056b85":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor c in cols:\n    if c not in data.columns: continue\n    lbl = LabelEncoder()\n    lbl.fit(list(data[c].values))\n    data[c] = lbl.transform(list(data[c].values))\n\nprint('Shape data: {}'.format(data.shape))","5f23b2fe":"# Adding total sqfootage feature\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","3b715156":"numeric_features = data.dtypes[data.dtypes != \"object\"].index\n\nskewed_features = data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' : skewed_features})\nskewness.head(10)","4ef78b94":"skewness = skewness[abs(skewness)>0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlamb = 0.15\nfor feature in skewed_features:\n    data[feature] = boxcox1p(data[feature], lamb)","33f935c5":"data = pd.get_dummies(data)\nprint(data.shape)","96975b3f":"train = data[:ntrain]\ntest = data[ntrain:]\nprint(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))","255f57b0":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","0783a815":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse","04e3a8e5":"# LASSO\nlasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=42))\n                      \n# Elastic Net Regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, random_state=42))\n\n# Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel=\"polynomial\", degree=2, coef0=2.5)\n\n# Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n\n# XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =42, nthread = -1)\n\n# LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","7303bf42":"print(\"\\nLasso score: {:.4f}\\n\".format(rmsle_cv(lasso).mean()))\nprint(\"\\nElasticNet score: {:.4f}\\n\".format(rmsle_cv(ENet).mean()))\nprint(\"\\nKRR score: {:.4f}\\n\".format(rmsle_cv(KRR).mean()))\nprint(\"\\nGBoost score: {:.4f}\\n\".format(rmsle_cv(GBoost).mean()))\nprint(\"\\nXGB score: {:.4f}\\n\".format(rmsle_cv(model_xgb).mean()))\nprint(\"\\nLGB score: {:.4f}\\n\".format(rmsle_cv(model_lgb).mean()))","1980ca8b":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","86b9ae83":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    \n    def fit(self, X, Y):\n        self.models_ = [clone(x) for x in self.models]\n        for model in self.models_:\n            model.fit(X, Y)\n        \n        return self\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)","ab720ca6":"averaged_model = AveragingModels(models=(ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_model)\nprint(\"Averaged base models score: {:.4f}\".format(score.mean()))","d7057a55":"# Make submission for this approach -> scored 0.12216\naveraged_model.fit(train.values, y_train)\npred = np.expm1(averaged_model.predict(test.values))\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = pred\nsub.to_csv('\/kaggle\/working\/house_price_submission.csv', index=False)","f96d5963":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    \n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=2)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def predict(self, X):\n        meta_features = np.column_stack([np.column_stack([model.predict(X) for model in base_model]).mean(axis=1) for base_model in self.base_models_])\n        return self.meta_model_.predict(meta_features)","95524867":"stacked_averaged_model = StackingAveragedModels(base_models=(ENet, GBoost, KRR), meta_model=lasso)\nscore = rmsle_cv(stacked_averaged_model)\nprint(\"Stacking Averaged Model score: {:.4f}\".format(score.mean()))","e2765826":"# Make submission for this approach -> scored 0.12142\nstacked_averaged_model.fit(train.values, y_train)\npred = np.expm1(stacked_averaged_model.predict(test.values))\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = pred\nsub.to_csv('\/kaggle\/working\/house_price_prediction_stacked_model.csv', index=False)","b748f98a":"stacked_pred = np.expm1(stacked_averaged_model.predict(test.values))\nmodel_xgb.fit(train.values, y_train)\nxgb_pred = np.expm1(model_xgb.predict(test.values))\nmodel_lgb.fit(train.values, y_train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))","60d90e0d":"ensemble = stacked_pred*0.65 + xgb_pred*0.05 + lgb_pred*0.3","76fb6484":"# scored 0.11984\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('\/kaggle\/working\/submisison.csv', index=False)","49d98220":"### Analysing 'SalePrice'","4f8bacd0":"### Modelling","5d51368f":"#### Missing Data","885e7f37":"### More feature engineering","47ef5c44":"#### Score","ea322c3b":"#### Transforming some numerical variables that are really categorical","e4efb871":"### Averaging base models score","41ed4cfd":"### Features engineering","179f47a3":"#### Label Encoding some categorical variables","05a8bf4c":"### Base Models","03007f10":"#### Correlation Matrix","49051057":"### Submisison","40eae5d9":"## Stacking models","cb4bea23":"### Stacking averaged Models","84647816":"#### Base Model Scores","192dfd02":"### Log-transformation of the target value","d493d3fd":"#### Import Libraries","6dead3e7":"### Out Liars","5792f8b5":"### Averaging base models","e9eeff85":"### Ensemble Prediction","62c92970":"#### Dealing with missing data","07324f78":"#### Adding one more important feature","e66005d2":"#### Define a cross validation strategy"}}