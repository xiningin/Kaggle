{"cell_type":{"ec6ca900":"code","890ced41":"code","bc45cc9f":"code","82316ffe":"code","168e018e":"code","83441b77":"code","2aed0480":"code","03b917d3":"code","b581398b":"code","707420ed":"code","f602d81e":"code","3d78e5db":"code","4f13a9cd":"code","d3f9b36f":"markdown","0dc18ce5":"markdown","08017b68":"markdown","9044d217":"markdown"},"source":{"ec6ca900":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","890ced41":"df = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","bc45cc9f":"df.info()","82316ffe":"df.head()","168e018e":"#Making abnormal ones 1 and normal ones 0\ndf[\"class\"] = [1 if i == \"Abnormal\" else 0 for i in df[\"class\"]]","83441b77":"x = df.drop([\"class\"], axis=1)\ny = df[\"class\"]","2aed0480":"# normalization process\nx = (x - x.min()) \/ (x.max() - x.min())","03b917d3":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 42)","b581398b":"# knn for neighbor value 3\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","707420ed":"#lets find what is best neighbor value\n\nimport matplotlib.pyplot as plt\nscore_list = []\nfor i in range(1,100):\n    knn2 = KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\n    \nplt.plot(range(1,100), score_list)\nplt.xlabel(\"neighbor value\")\nplt.ylabel(\"score\")\nplt.show()","f602d81e":"import numpy as np\nnp.argmax(score_list)","3d78e5db":"#lets check\nknn = KNeighborsClassifier(n_neighbors=38)\nknn.fit(x_train, y_train)\nknn.score(x_test, y_test)","4f13a9cd":"score_list[37]","d3f9b36f":"It is arount 40. Lets find exact number.","0dc18ce5":"This data set is pretty neat :)","08017b68":"In this notebook I will use knn classifier to predict whether a patient is abnormal or normal.","9044d217":"So best neighbor value is 38 since when you look carefuly into for loop we start with 1 then add them to score_list. "}}