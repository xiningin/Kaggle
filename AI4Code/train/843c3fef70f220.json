{"cell_type":{"b7b8e2e2":"code","4669eef0":"code","4287fa6d":"code","131cf2d6":"code","1243ff7c":"code","856b3181":"code","dbf07bf7":"code","dac5b0d9":"code","d2628209":"code","c9314b3f":"code","39622818":"code","d0709c1f":"code","d55f9a29":"code","d67ea3a6":"code","aeca398b":"code","97d2fd7c":"code","98a9142b":"code","68ba4ac4":"code","a48ce2f4":"code","ec7f0952":"code","c112a679":"code","7f589e7c":"code","3e2cca05":"code","5524a030":"code","6c38d734":"code","1b4649e1":"code","7befbf3c":"code","86d68b92":"code","1765fcf1":"code","4bf4bc3f":"code","11021f1b":"code","90c78f4e":"code","16b2f69a":"code","d6029e05":"code","e07198b4":"code","c8c48e62":"code","ba418971":"code","1fdbd8e7":"code","5025fb21":"code","8694315e":"code","bfdaada0":"code","8ae147c9":"code","2cb3e40c":"code","3a849ae9":"code","f3734d2b":"code","25362632":"code","d2e0e0b6":"code","e344ea23":"code","e03ea36c":"code","691abce6":"code","f5a5d97a":"code","e8093008":"code","ec2eca85":"code","c63591ab":"code","a7f48028":"code","dd9b0cab":"code","b50faf21":"code","b8ed470b":"code","f0ecbeae":"code","dbf9ccc6":"code","957fd4d2":"code","c385e8be":"code","f06f9349":"code","5800f3e3":"code","31c23a45":"code","a8cb09e8":"code","6aedccff":"code","90849102":"markdown","b89b2acb":"markdown","f233d534":"markdown","1ee2d70a":"markdown","3effb6f6":"markdown","ef884361":"markdown","adba566b":"markdown","227cc675":"markdown","750ef8eb":"markdown","206016bb":"markdown","926b25b9":"markdown","1023de1e":"markdown","6d13a8a4":"markdown","3635857e":"markdown","2fb6fe1e":"markdown","dc800c59":"markdown","6d42ae17":"markdown","f65eed3a":"markdown","e0663e67":"markdown","fdc71956":"markdown","317e47ae":"markdown","f0076ee9":"markdown","71cbadb8":"markdown","bdac3dc8":"markdown","c09740e1":"markdown","b2637885":"markdown","f5704a90":"markdown","56b1920d":"markdown","5527845e":"markdown","81baa018":"markdown","54abc144":"markdown","7843b7bf":"markdown","e38f4bfb":"markdown","9bf98f56":"markdown","d1b0c28e":"markdown","4c66cf7a":"markdown","135a260d":"markdown","ec3ce7a8":"markdown","6df86dd1":"markdown","a682147a":"markdown","5424e7a9":"markdown","108d076b":"markdown","f6f409c8":"markdown","52efd8ad":"markdown","c4d123ce":"markdown","5f8d429d":"markdown","5f05adc6":"markdown","1762b067":"markdown","c8eee8ef":"markdown","cad64b70":"markdown","cd02ee62":"markdown","412bd132":"markdown","fe62a407":"markdown","a2df10f3":"markdown","f6a8d951":"markdown","967e1d0d":"markdown","c3e3e78e":"markdown","76184cba":"markdown","063080a0":"markdown","d5d30cfd":"markdown","69ffae3d":"markdown","8e5e7e6f":"markdown","35eee197":"markdown","2a2c0b0a":"markdown"},"source":{"b7b8e2e2":"# Imports\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra, we might not use it very much as we will do it from scratch\nimport plotly.express as px #just to design beautiful graphs","4669eef0":"import warnings\n\n# settings\nwarnings.filterwarnings('ignore')","4287fa6d":"sales_train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsub = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","131cf2d6":"def predict(x_i, beta):\n    \"\"\"assumes that the first element of each x_i is 1\"\"\"\n    return np.dot(x_i, beta)","1243ff7c":"def error(x_i, y_i, beta):\n    return y_i - predict(x_i, beta)\n\ndef squared_error(x_i, y_i, beta):\n    return error(x_i, y_i, beta) ** 2","856b3181":"def squared_error_gradient(x_i, y_i, beta):\n    \"\"\"The gradient (with respect to beta)\n    correspond to the ith squared error term\"\"\"\n    return [-2 * x_ij * error(x_i, y_i, beta)\n           for x_ij in x_i]","dbf07bf7":"def estimate_beta(x, y):\n    beta_initial = [random.random() for x in x[0]]\n    return minimize_stochastic(squared_error,\n                              squared_error_gradient,\n                              x, y,\n                              beta_initial,\n                              0.01)","dac5b0d9":"def in_random_order(data):\n    \"\"\"generator that returns the elements of data in random order\"\"\"\n#     print(\"[x for x in enumerate(data)]\", [x for x in enumerate(data)])\n    indexes = [i for i, _ in enumerate(data)] # create a list of indexes\n    random.shuffle(indexes)                   # suffle them\n    for i in indexes:\n        yield data[i]","d2628209":"thetas_value = []\n\ndef minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=2\/97592458):\n    print(\"My minimize stochstic function parameters: \")\n    print(target_fn)\n    print(gradient_fn)\n#     print(\"x: \",x)\n#     print(\"y: \", y)\n    data = zip(x,y)\n    print(\"theta_0: \", theta_0)\n    print([(x_i, y_i) for x_i, y_i in zip(x, y)])\n    print(\"data: \", data)\n    print(\"--------------------\")\n    theta = theta_0                           # the initial guess\n    alpha = alpha_0                           # initial step size\n    min_theta, min_value = None, float(\"inf\") # the minimum so far\n    iterations_with_no_improvement = 0\n    \n    # if we ever go 100 iterations with no improvement, stop\n    while iterations_with_no_improvement <100:\n        data = zip(x,y)\n        value = sum([target_fn(x_i, y_i, theta) for x_i, y_i in data])\n        thetas_value.append({\"beta0\": theta[0], \"beta1\": theta[1], \"beta2\": theta[2], \"cost\": value[0]})\n        if value < min_value:\n            # if we've found a new minimum, remeber it\n            # and go back to the original step size\n            min_theta, min_value = theta, value\n            iterations_with_no_improvement = 0\n            alpha = alpha_0\n        else:\n            # otherwise we're not improving, so try shrinking the step_size\n            iterations_with_no_improvement += 1\n            alpha *= 0.9\n        # and take a gradient step for each of the data points\n        for x_i, y_i in in_random_order(list(zip(x,y))):\n            gradient_i = gradient_fn(x_i, y_i, theta)\n            theta = np.subtract(theta, [alpha * x[0] for x in gradient_i])\n            \n    return min_theta\n","c9314b3f":"# ! pip install numdifftools","39622818":"import random\nrandom.seed(0)\nprint([random.random() for x in [1, 49, 4, 2500]])\n","d0709c1f":"Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\n                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],\n                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],\n                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],\n                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        \n                }\n\ndf = pd.DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price']) \n\ndf['Ones'] = 1\n# print(df)\nx = df[['Ones', 'Interest_Rate', 'Unemployment_Rate']].values\ny = df[['Stock_Index_Price']].values\n\nbeta = estimate_beta(x, y)","d55f9a29":"beta","d67ea3a6":"sales_train.head()","aeca398b":"sales_train['date'] = pd.to_datetime(sales_train['date'])","97d2fd7c":"sales_train.describe()","98a9142b":"items.head()","68ba4ac4":"item_categories.head()","a48ce2f4":"# extracting sub categories from item_category_name\n\n## splitting the category by '-'\nitem_categories['categories'] = item_categories['item_category_name'].str.split('-')\n\n## Extracting the first element from split\nitem_categories['type'] = item_categories['categories'].apply(lambda x: x[0].strip())\n\n## Extracting the second element if there is a second element, else return first element\nitem_categories['sub_type'] = item_categories['categories'].apply(lambda x: x[1].strip() if len(x) >1 else x[0].strip())\n\n","ec7f0952":"item_categories['categories'].apply(lambda x: x[1].strip() if len(x) >1 else x[0].strip())","c112a679":"# Dropping unnecessary columns\n\nitem_categories.drop(['item_category_name', 'categories'], inplace=True, axis=1)","7f589e7c":"# plotting\nfig = px.bar(item_categories.groupby('type')['sub_type'].count(), title = \"Number of sub categories in Each Category\")\nfig.show()","3e2cca05":"shops.head()","5524a030":"print(shops.iloc[[0,57]])\nprint(\"--\")\nprint(shops.iloc[[1,58]])\nprint(\"--\")\nprint(shops.iloc[[10,11]])","6c38d734":"shops.loc[shops['shop_id']==0, 'shop_id'] = 57\nshops.loc[shops['shop_id']==1, 'shop_id'] = 58\nshops.loc[shops['shop_id']==10, 'shop_id'] = 11","1b4649e1":"shops = shops.drop_duplicates(subset='shop_id')","7befbf3c":"! pip install google_trans_new","86d68b92":"from google_trans_new import google_translator  \ntranslator = google_translator()    \nshops['shop_name_en'] = shops['shop_name'].apply(translator.translate,lang_tgt='en')","1765fcf1":"shops['shop_name_en'].head()","4bf4bc3f":"shops.shop_name_en = shops.shop_name_en.replace(r'^! ', '', regex=True)","11021f1b":"shops['city'] = shops['shop_name_en'].str.split(' ').apply(lambda x: x[0])","90c78f4e":"shops['city'].head()","16b2f69a":"from geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"kaggle\")","d6029e05":"shops.head()","e07198b4":"import requests\nimport json\n\nyandex_apikey = '1e6bf15c-981a-47ec-a263-7bb089b9dec6'\n\n\ndef get_gps_location(address):\n    try:\n        response = requests.get(\n            \"https:\/\/geocode-maps.yandex.ru\/1.x\/\",\n            params=dict(format=\"json\", apikey=yandex_apikey, geocode=address),\n        )\n        data = response.json()[\"response\"]\n        coordinates = data[\"GeoObjectCollection\"]['featureMember'][0]['GeoObject'][\"Point\"][\"pos\"].split(\" \")\n        coordinates = [float(x) for x in coordinates]\n        return coordinates\n    except:\n        return None, None\n    ","c8c48e62":"shops[['lon','lat']] = shops.apply(lambda row: get_gps_location(row.shop_name),\n                                    axis=\"columns\", result_type=\"expand\")","ba418971":"shops.head()","1fdbd8e7":"# Merging the dataframes\n\n## merging sales_train with items\ndf = pd.merge(sales_train, items, how='left', on='item_id')","5025fb21":"## merging df with item_categories\ndf = pd.merge(df, item_categories, how='left', on='item_category_id')","8694315e":"#merging df and shops\ndf = pd.merge(df, shops, how='left', on='shop_id')","bfdaada0":"data = df.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_price':'mean', 'item_cnt_day':'sum'}).reset_index()\ndata.head()","8ae147c9":"# merging sales_train with item\ndata = pd.merge(data, items, how='left', on='item_id')\n\n##merging df and item_categories\ndata = pd.merge(data, item_categories, how = 'left', on='item_category_id')\n\n## merging df and shops\ndata = pd.merge(data,shops, how='left', on='shop_id')","2cb3e40c":"data.tail()","3a849ae9":"# month\ndata['month'] = data['date_block_num'].apply(lambda month: (month+1)%12)","f3734d2b":"# dummies\ndata = pd.concat([data, pd.get_dummies(data['shop_id'], \n                                       drop_first=True, \n                                       prefix='shop_')], axis=1)\n\ndata = pd.concat([data, pd.get_dummies(data['type'],\n                                      drop_first=True,\n                                      prefix='type')], axis=1)\n\ndata = pd.concat([data, pd.get_dummies(data['sub_type'],\n                                      drop_first=True,\n                                      prefix='sub_type',\n                                      )], axis=1)\n","25362632":"store_types = ['\u041c\u0422\u0420\u0426', 'TPK','\u0422\u0426','\u0422\u041a','\u0422\u0420\u0426']\n\ndef get_store_type(x):\n    if type(x) != float:\n        words = x.split(' ')\n        return next((store_types.index(s) for x in words for s in store_types if x in s),None)\n    else: return 5 # online\n\ndata['shop__type'] = data['shop_name'].apply(get_store_type)","d2e0e0b6":"# Getting the names of the feature columns\n\n# Collecting shop feature names\nshop_columns = [col for col in data.columns if 'shop__' in col]\n\n## Collecting type feature names\ntype_columns = [col for col in data.columns if 'type_' in col]\n\n## Collecting sub_type feature names\nsub_type_columns = [col for col in data.columns if 'sub_type_' in col]\n\n## collecting geo features\ngeo_columns = ['lat', 'lon']","e344ea23":"# setting the features and target variable\nfeatures = ['month', 'shop_id', 'item_id', 'item_price'] + geo_columns + shop_columns + type_columns + sub_type_columns \n# features = ['month', 'shop_id', 'item_id', 'item_price'] + geo_columns + type_columns + sub_type_columns \ntarget = ['item_cnt_day']","e03ea36c":"data[features].head()","691abce6":"test = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","f5a5d97a":"# adding data_block_num with our target month\ntest['date_block_num'] = 34\ntest['month'] = 11","e8093008":"item_price = data[['item_id', 'item_price']].groupby('item_id')['item_price'].mean().reset_index()","ec2eca85":"item_price.head()","c63591ab":"# merging the test data with dataframes\n\n##merging sales_train with item\ntest = pd.merge(test, item_price, how='left', on='item_id')\n\n## merging items with test\ntest = pd.merge(test, items, how='left', on='item_id')\n\n##merging df and item_categories\ntest = pd.merge(test, item_categories, how='left', on='item_category_id')\n\n## merging test and shops\ntest = pd.merge(test, shops, how='left', on='shop_id')","a7f48028":"store_types = ['\u041c\u0422\u0420\u0426', 'TPK','\u0422\u0426','\u0422\u041a','\u0422\u0420\u0426']\n\ndef get_store_type(x):\n    if type(x) != float:\n        words = x.split(' ')\n        return next((store_types.index(s) for x in words for s in store_types if x in s),None)\n    else: return 5 # online\n\ntest['shop__type'] = test['shop_name'].apply(get_store_type)","dd9b0cab":"test.head()","b50faf21":"# Dummifying the categorical columns\n\n# Creating dummies on shop_ids\ntest = pd.concat([test, pd.get_dummies(test['shop_id'], drop_first=True, prefix='shop_')], axis=1)\n\n# Creating dummies on types\ntest = pd.concat([test, pd.get_dummies(test['type'], drop_first=True, prefix='type_')], axis = 1)\n\n# Creating dummies on sub_types\ntest = pd.concat([test, pd.get_dummies(test['sub_type'], drop_first=True, prefix='sub_type')], axis =1)","b8ed470b":"# getting the names of the features columns\n\n# Collectin shop names features\ntest_shop_columns = [col for col in test.columns if 'shop__' in col]\n\n# Collecting item types features\ntest_type_columns = [col for col in test.columns if 'item_' in col]\n\n# Collecting sub_type feature names \ntest_sub_type_columns = [col for col in test.columns if 'sub_type_' in col]\n\n# Geo?\ntest_geo_columns = ['lat', 'lon']","f0ecbeae":"# setting the feature and target variable\n\ntest_features = ['month', 'item_id', 'shop_id', 'item_price'] + test_shop_columns + test_type_columns + test_sub_type_columns + test_geo_columns\n\n","dbf9ccc6":"print(f\"Number of predictors in train data: {len(features)}\")\nprint(f\"number of predictors in test data: {len(test_features)}\")\ncommon_features = list(set(features) & set(test_features))\nprint(f\"Common features: {len(common_features)}\")","957fd4d2":"# Preparing data for modeling\n\n## Setting feature and target variables\nX_train = data[common_features].fillna(value=0)\ny_train = data[target].fillna(value=0)","c385e8be":"X_train.shape","f06f9349":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\n\nlr= LinearRegression()\n\ntscv = TimeSeriesSplit(n_splits=5)\ni = 1\nscore = []\nfor tr_index, val_index in tscv.split(X_train):\n    print(tr_index, val_index)\n    X_tr, X_val = X_train.iloc[tr_index,:], X_train.iloc[val_index,:]\n    y_tr, y_val = y_train.iloc[tr_index,:], y_train.iloc[val_index,:]\n    ## fitting on training data\n    lr.fit(X_tr, y_tr)\n    lr.score(X_val, y_val)\n    score.append([i, lr.score(X_val, y_val)])\n    i += 1","5800f3e3":"test = test.fillna(0)","31c23a45":"test['preds'] = lr.predict(test[common_features])","a8cb09e8":"preds = test[['ID', 'preds']]\n\npreds.columns = ['ID', 'item_cnt_month']\npreds.head()","6aedccff":"# saving the submission dataframe\npreds.to_csv('my_submission.csv', index=False)","90849102":"From here, we're ready to find the optimal beta using stochastic gradient descent:","b89b2acb":"$$y_i = \\beta x_i + \\alpha + \\epsilon_i$$","f233d534":"Okay, now we are all set to apply the linear regression model we did to the test set.","1ee2d70a":"Perfect, now our test dataframe looks ready to be dummified.","3effb6f6":"We are adapting the dataframe to the required submission dataframe.","ef884361":"This means our model looks like:\n\n$$Stock\\_Index\\_Price = 80.2 + 550.52 \\ interest\\_rates -26.18 \\ Unemployment\\_rate $$","adba566b":"# 4. Example: Linear Regression from scratch","227cc675":"Now we create a feature for the type of store.","750ef8eb":"# 5. Explenatory data analysis","206016bb":"# 3. Data","926b25b9":"Now that we have trained our linear regression model (it now has the right weight), we are going to use them on the test\/submission set, provided separately.\n\nWe first need to create the futures that will be shared with the data we got for training. That is t say we need to derive what are the types and subtypes of our every items, the locations based on the shop_id...","1023de1e":"This time we won't import lots of libraries as we we do a linear regression from scratch","6d13a8a4":"There are other assumptions that are required for this model to make sense: \n\n1. the first is that the columns of x are _linearly independant_ - that there is no way to write one as weighted sum of the others. If this assumption fails, it's impossible to estimate `beta`. To see this in an extreme case, imagine we had an extra field `double_price`. Then any augmentation of `price` coefficient and reduction in `double_price` coefficient would leave the model unchanged. \n\n2. The second important assumption is that the columns of `x` are all uncorrelated with the errors $\\epsilon$. Otherwise our $\\beta$ estimates would always be wrong.","3635857e":"However we probably don't have only one feature about how often that visiting guy of our website visited it for our model. Probably I know what he already has bought, how much it was ... that leads us to **multiple regression**:","2fb6fe1e":"As a result to deal with that proble we can use ","dc800c59":"Now we need to group by month, shop_id, item_id to get total sales","6d42ae17":"Yet, some features in training data aren't shared by test data. Let's take only the features taht are in both.","f65eed3a":"A linear regression works when we have a linear relationship between variables. That's why we use correlation to measure the strength of the relationship. But knowing that such an relation exist isn't enough. We will want to  to understand the nature of the relationship. This is is where we'll ue simple linear regresion.","e0663e67":"`item_categories` is more interesting as we will be able to create subcategories from there.","fdc71956":"Probably the best thing to do would be to detect that with some NLP","317e47ae":"Should you translate these shop names you would realise something interesting.","f0076ee9":"# Shops","71cbadb8":"Indeed, one can see that we have first the category before - and then a subcategory.","bdac3dc8":"Unfortunately I wasn't able to use my own linear regressor as it seems we have too much data, more than one million rows. ","c09740e1":"Let's make month like date_block_num","b2637885":"All right, submission scored 2.81936 on the leader board. That's an improvement but that's not better than the submission setting everything to 0.5. In order to improve could try to do a linear regression model with a kfold validation technique for each of the item_id, shop_id.","f5704a90":"Then we **dummify** to have categorical encoding of the different shops, types and subtybes of items?","56b1920d":"So I got an error message: `Your notebook tried to allocate more memory than is available. It has restarted.`\nAs a result I will load a normal linear regression model from sklearn.","5527845e":"If you know calculus you can compute:","81baa018":"And we'll want to take a gradient step from each data point. This apporach leaves the possibility that we might curcle around near a minimum for ever, so whenever we stop getting improvements we'll decrease the step size and eventually quit:","54abc144":"Now, let's take a look at the data we were provided.","7843b7bf":"The task is to predict sales for each item in a given store for the next month. So I will represent monthly sales per item per store.","e38f4bfb":"# 2. Libraries","9bf98f56":"We still need to create `minimize_stochastic`. The predictive errors are additive, which means that the predictive error on the whole data set is simply the sum of the predictive errors for each data point. So we can instead apply a technique called stochastic gradient descent, which computes the gradient (and takes a step) for only one point at a time. It cycles over our data to iterate through our data in random order.","d1b0c28e":"Here you really need to look at the whole dataframe and you would notice that some rows seem duplicated.","4c66cf7a":"`sales_train` is the record of all sales of an `item_id` by `shop_id` and `date`.","135a260d":"# Linear Regression using common features","ec3ce7a8":"Let's have a look at the items name now. Nothing much of interest here.","6df86dd1":"Hi ! This is my second attempt and I still love Kaggle, so I will try to do the best to show you interesting findings that may help you win this competition ;)\n\nLast time I went straight to a time series predictive model and it wasn't very successful (score of >3000). This time I will use a more simple approach: linear regression. And because I am feeling funky right now and that I want teach you how it works, we are going to do it from scratch :)","a682147a":"### baseline model using linear regression","5424e7a9":"## Fitting the model\n\nAs in the simple linear model, we'll chose beta to minimize the sum of squared errors. Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent. We'll start by creating an error function to minimize. For stochastic gradient descent, we'll just want the squarred error corresponding to a single prediction.","108d076b":"We split data into training set and test set in everyday machine learning analyses, and oftentimes we use scikit-learn\u2019s random splitting function.\n\n```\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n```","f6f409c8":"where $y_i$ is the spendings of customer $i$ and $x_i$ is the number of friends user $i$ has, and $\\epsilon_i$ is the (hopefully small) error term representing the fact that there mu sbe other factors.\n\nAssuming we have such an $\\alpha$ and $\\beta$ then we make prediction simply with:\n\n```\ndef predict(alpha, beta, x_i):\n    return beta * x_i + alpha\n```\n\nHow do we chose $\\alpha$ and $\\beta$? Well, any choice gives us a predicted output for each input `x_i`. Since we know the actual output `y_i` we can compute the error for each pair. If add the square of this result on each pair we can chose $\\alpha$ and $\\beta$ that make the sum of the squarred errors as small as possible.","52efd8ad":"This time we will take much more attention to the data, and focus lesss on the time perspective of it.","c4d123ce":"We have to transform the date to one readable by our softwares.","5f8d429d":"$$y_i = \\alpha +\\beta_1x_{i1} + ... + \\beta_kx_{ik} + \\epsilon_i$$","5f05adc6":"- `sales_train.csv` - the training set. Daily historical data from January 2013 to October 2015. It has 1034 unique values `item_id` that were sold at a given `shop_id` at `date_block_num` time.\n- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015. 214k `item_id` sales to predict at `shop_id` for next `date_block_num` time.\n- `sample_submission.csv` - a sample submission file in the correct format.\n- items.csv - supplemental information about the items\/products. 22170\nunique values, with the categories.\n- item_categories.csv  - supplemental information about the items categories. 84\nunique values.\n- shops.csv- supplemental information about the shops. The name and sometimes the categories: TPK|\u0422\u0426|\u0422\u041a|\u0422\u0420\u0426|\u041c\u0422\u0420\u0426|\u0422\u0426","1762b067":"![image.png](attachment:a3fdeddf-a3c9-46c8-872e-aaee0890b177.png)","c8eee8ef":"We can now plot the number of store in each city","cad64b70":"Now let's gather the column names that will be used as features.","cd02ee62":"## Submission","412bd132":"In this particular case, our independant variable `x` will be a list of vectors, each of which looks like this\n\n```\n[1,   # constant \n 49,  # how often he visited before buying\n 4,   # how many products he looked at \n 250] # mean price of the products\n```","fe62a407":"# \ud83d\udcb0Predict Future Sales Competiton\ud83d\udcb0: time series prediction with Linear Regression","a2df10f3":"Let's remove the !","f6a8d951":"However because we are dealing with time series here, we can't since observations in our time series datasets are not independent [as Alexander Guschin pointed out in data his data splitting strategies lecture in week 2](https:\/\/www.coursera.org\/learn\/competitive-data-science\/lecture\/0jtAV\/data-splitting-strategies).","967e1d0d":"Let's take the example of an ecommerce website which have a certain number of people visiting it each day. Let's assume that I am convinced that the more often people visit the website causes them to have spend more.\n\nThis is where we can use a linear model to describe this relationship. In particular there are constants $\\alpha$ an $\\beta$ such that:","c3e3e78e":"And now we merge again to get all the data (categories, localisation ...) we left.","76184cba":"# Conclusion","063080a0":"All this cities start with the name of the city the shop is localised","d5d30cfd":"Let's get the GPS location:","69ffae3d":"# 1. Competition Outline\nIn this competition we work with time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.\n\nWe are asked to predict total sales for every product and store in the next month.\n\nBut what is time series prediction?\n\n> Time series forecasting is the use of a model to predict future values based on previously observed values.","8e5e7e6f":"Some categories have even more details on the right:","35eee197":"In multiple regression the vector of parameters is usually called $\\beta$. We'll want this to include the canstant term as well, which we can achieve by adding a column of ones to our data:\n\n```\nbeta = [alpha, beta_1, ..., beta_k]\n```\n\nand:\n\n```\nx_i = [1, x_i1, ..., x_ik]\n```\n\nThen our model is just:","2a2c0b0a":"### Data Processing"}}