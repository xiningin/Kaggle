{"cell_type":{"d29c0067":"code","a8f33636":"code","d207d7b4":"code","ff81cd53":"code","ab823b9f":"code","0259c70f":"code","d6cd9cde":"code","df673180":"code","0b79780e":"code","319d3972":"code","dfa7099e":"code","3ee5e235":"code","bcec263a":"code","7d2972e6":"code","930bd941":"code","105255ee":"code","fb8a3e21":"code","92663b16":"code","28b082e6":"code","582dc554":"code","2587d718":"code","2ba2e477":"code","ca4c6a2a":"code","0f260629":"code","8eb86706":"code","849d1378":"code","2b7552df":"code","5e050f01":"code","4b7bc6f6":"code","73e232a3":"code","319a81cf":"markdown"},"source":{"d29c0067":"import os\nimport tqdm\nimport matplotlib.pyplot as plt\nfrom keras import preprocessing, layers, models, optimizers\nimport numpy as np","a8f33636":"#FAST_RUN = True # controls whether to run kernel fast\nFAST_RUN = False","d207d7b4":"!ls ..\/input\/test_set\/test_set","ff81cd53":"!ls ..\/input\/test_set\/test_set\/cats | wc -l\n!ls ..\/input\/test_set\/test_set\/dogs | wc -l","ab823b9f":"# there are about 2000 samples in the test set","0259c70f":"!ls ..\/input\/training_set\/training_set\/","d6cd9cde":"!ls ..\/input\/training_set\/training_set\/dogs | wc -l\n!ls ..\/input\/training_set\/training_set\/cats | wc -l","df673180":"# there are about 8000 samples in the trianing set","0b79780e":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\n                       input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","319d3972":"model.summary()","dfa7099e":"path_cats = []\ntrain_path_cats = '..\/input\/training_set\/training_set\/cats'\nfor path in os.listdir(train_path_cats):\n    if '.jpg' in path:\n        path_cats.append(os.path.join(train_path_cats, path))\npath_dogs = []\ntrain_path_dogs = '..\/input\/training_set\/training_set\/dogs'\nfor path in os.listdir(train_path_dogs):\n    if '.jpg' in path:\n        path_dogs.append(os.path.join(train_path_dogs, path))\nlen(path_dogs), len(path_cats)","3ee5e235":"# load training set\ntraining_set = np.zeros((6000, 150, 150, 3), dtype='float32')\nfor i in range(6000):\n    if i < 3000:\n        path = path_dogs[i]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        training_set[i] = preprocessing.image.img_to_array(img)\n    else:\n        path = path_cats[i - 3000]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        training_set[i] = preprocessing.image.img_to_array(img)","bcec263a":"training_set.shape","7d2972e6":"# load validation set\nvalidation_set = np.zeros((2000, 150, 150, 3), dtype='float32')\nfor i in range(2000):\n    if i < 1000:\n        path = path_dogs[i + 3000]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        validation_set[i] = preprocessing.image.img_to_array(img)\n    else:\n        path = path_cats[i + 2000]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        validation_set[i] = preprocessing.image.img_to_array(img)","930bd941":"validation_set.shape","105255ee":"# make target tensor\ntrain_labels = np.zeros((3000,))\ntrain_labels = np.concatenate((train_labels, np.ones((3000,))))\nvalidation_labels = np.zeros((1000,))\nvalidation_labels = np.concatenate((validation_labels, np.ones((1000,))))","fb8a3e21":"train_datagen = preprocessing.image.ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow(\n    training_set,\n    train_labels,\n    batch_size=32)\nvalidation_generator = train_datagen.flow(\n    validation_set,\n    validation_labels,\n    batch_size=32)","92663b16":"# when augmenting data, you need to specify the step_per_epoch\n# usually, (num_samples \/ batch_size) * 2.5","28b082e6":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    epochs= 3 if FAST_RUN else 30,\n    validation_steps=50,\n    validation_data=validation_generator,\n)","582dc554":"model.save('cats_and_dogs_small_1.h5')","2587d718":"# plot error curves\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","2ba2e477":"# we're overfitting when train and validation diverge","ca4c6a2a":"# Demo data augmentation\ndatagen = preprocessing.image.ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n)","0f260629":"# visualize data augmentations\nplt.clf()\nfnames = [os.path.join(train_path_cats, fname) for fname in os.listdir(train_path_cats)]\nimg_path = fnames[3]\n\nimg = preprocessing.image.load_img(img_path, target_size=(150, 150))\nx = preprocessing.image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\nplt.figure(figsize=(20,20))\n\ni = 0\nfor batch in datagen.flow(x, batch_size=1):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(preprocessing.image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","8eb86706":"# Add a dropout layer to fight overfitting as well\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\n                       input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))                       # DROPOUT\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","849d1378":" # Use data augmentation\ntrain_datagen = preprocessing.image.ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n)\ntrain_generator = train_datagen.flow(\n    training_set,\n    train_labels,\n    batch_size=32)\n\n# do not augment validation data\ntest_datagen = preprocessing.image.ImageDataGenerator(rescale=1.\/255)\nvalidation_generator = test_datagen.flow(\n    validation_set,\n    validation_labels,\n    batch_size=32)","2b7552df":"# train\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    epochs= 3 if FAST_RUN else 200, # use more epochs if you are not limited by 1 hour limit\n    validation_data=validation_generator,\n    validation_steps=50)","5e050f01":"model.save('cats_and_dogs_small_2.h5')","4b7bc6f6":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","73e232a3":"# our accuracy keeps going up\n# you are overfitting when validation accuracy goes down again\n\n# our loss keeps going down\n# you are overfitting when validation loss increases again","319a81cf":"# Convolutional Neural Network trained from scratch | Keras\nThis kernel shows how to train a CNN from scratch. Keras has a directory-centric api but kaggle kernels cannot write too many files. We read in the images and rescale the values to between 0 and 1. You then make an ImageDataGenerator that preprocesses the data. If you overfit augment the data and add dropout.\n\nAdapted from Deep Learning with Python section 5.2"}}