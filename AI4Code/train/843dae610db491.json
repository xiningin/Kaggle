{"cell_type":{"89089b26":"code","af8130d9":"code","e6312107":"code","27b66575":"code","42c0b3d3":"code","a40a6129":"code","d4890def":"code","fd8898c9":"code","e5211407":"code","4787e9bd":"code","e0ddefcb":"code","9a4524ff":"code","df784859":"code","6ff6ffe8":"code","dc7ec05a":"code","3674c060":"code","ba991d35":"code","0a80f329":"code","d695839e":"code","69e9ff98":"code","1b44d193":"code","6d2a9cf2":"code","6aa56965":"code","3252dac9":"code","71475d2e":"code","16c4e4c1":"code","3312a3be":"code","54281b0c":"code","50d6c341":"code","6cedcb78":"code","e1f7e6d8":"code","8c0468dd":"code","77398c8f":"code","30219656":"code","2b2db633":"code","02f00fd6":"code","9c71e118":"code","c7026d4d":"code","3466cb85":"code","008a021d":"code","c72ce62f":"code","c05986ca":"code","36eba3ba":"code","623609a8":"code","48f25fd4":"code","d303ac50":"code","6f00942a":"code","ede42c4d":"code","ebcbb739":"code","024d7d13":"code","8138824a":"code","87e3e063":"markdown","3a91c88f":"markdown","21e881c7":"markdown","a3f3366c":"markdown","c18ec4e7":"markdown","e5eb03c1":"markdown","c1225090":"markdown","6bc65417":"markdown","d760358a":"markdown","7ec3dd6b":"markdown","75a30e6e":"markdown","f41b543e":"markdown","450d4c0f":"markdown","1739ea77":"markdown","995731ca":"markdown","660fcb4e":"markdown","72dcc67b":"markdown","daaa6074":"markdown","eb9c823e":"markdown","187b3fd0":"markdown","c0163a9d":"markdown","1aa0f2d4":"markdown","8f792465":"markdown","163cae46":"markdown","db8920fd":"markdown","9d915a81":"markdown","975d9f6d":"markdown","c165671c":"markdown","eb7094e8":"markdown","549e2e0c":"markdown","4fac46c2":"markdown","8b9b05d1":"markdown","49778789":"markdown","586ded2a":"markdown","6a588469":"markdown","4f2a1652":"markdown","9c9e9a2d":"markdown","45fe1b84":"markdown","dfa67a40":"markdown","230eb29c":"markdown","f6629d75":"markdown","7e895840":"markdown","226f3150":"markdown","39d2f13c":"markdown","b4d34cd6":"markdown","9cbe8cf1":"markdown","66bf4cb3":"markdown","817066a4":"markdown","79359c9b":"markdown","03bc5d50":"markdown","b8cd0648":"markdown","49bd8423":"markdown","7d79fc17":"markdown","2ffe6268":"markdown","2c995113":"markdown","cd42785e":"markdown","54c22334":"markdown","d3fe6c92":"markdown","3fb7b5c3":"markdown","838a0d6f":"markdown","9748e3c1":"markdown","01bd64e3":"markdown","7e6362c3":"markdown","e112fcc8":"markdown"},"source":{"89089b26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af8130d9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport lightgbm as lgbm\n\n# preprocessing\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import recall_score,roc_auc_score,accuracy_score,roc_curve\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA","e6312107":"#Data Ingestion \ndata_train=pd.read_csv('..\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_training_set.csv',error_bad_lines=False) \ndata_test=pd.read_csv('..\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_test_set.csv',error_bad_lines=False)","27b66575":"data_train.head()","42c0b3d3":"data_test.head()","a40a6129":"print(\"Number of positive classes = \", sum(data_train['class'] == 'pos'))\nprint(\"Number of negative classes = \", sum(data_train['class'] == 'neg'))","d4890def":"data_train = data_train.rename(columns = {'class' : 'Flag'})\ndata_train['Flag'] = data_train.Flag.map({'neg':0, 'pos':1})\ndata_train = data_train.replace(['na'],np.nan)\ndata_train.head()","fd8898c9":"data_test = data_test.rename(columns = {'class' : 'Flag'})\ndata_test['Flag'] = data_test.Flag.map({'neg':0, 'pos':1})\ndata_test = data_test.replace(['na'],np.nan)\ndata_test.head()","e5211407":"missing_percent_threshold = 0.50\ntotal_num_data = len(data_train.index)\nmissing_data_count = pd.DataFrame(data_train.isnull().sum().sort_values(ascending=False), columns=['Number'])\nmissing_data_percent = pd.DataFrame(data_train.isnull().sum().sort_values(ascending=False)\/total_num_data, columns=['Percent'])\nmissing_data = pd.concat([missing_data_count, missing_data_percent], axis=1)\nmissing_data","4787e9bd":"missing_data_percent.plot.bar(figsize=(50,10))\nplt.show()","e0ddefcb":"missing_column_headers = missing_data[missing_data['Percent'] > missing_percent_threshold].index\nprint(missing_column_headers)  #are the missing data header with more than 50%","9a4524ff":"data_train = data_train.drop(columns=missing_column_headers)\nprint(\"Training data-set shape after dropping features is \", data_train.shape)\ndata_test = data_test.drop(columns=missing_column_headers)\nprint(\"Test data-set shape after dropping features is \", data_test.shape)\nprint(data_train.describe())","df784859":"y_train = data_train.loc[:, 'Flag']\nx_train = data_train.drop('Flag', axis=1)\ny_test = data_test.loc[:, 'Flag']\nx_test = data_test.drop('Flag', axis=1)","6ff6ffe8":"impute_median = SimpleImputer(strategy='median')\nimpute_median.fit(x_train.values)\nx_train = impute_median.transform(x_train.values)\nx_test = impute_median.transform(x_test.values)","dc7ec05a":"scaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nscaler.fit(x_test)\nx_test_scaled = scaler.transform(x_test)","3674c060":"x_train_scaled_head=x_train_scaled[0:1000,3]\nx_test_scaled_head=x_test_scaled[0:1000,3]\nx_train_head=x_train[0:1000,3]\nx_test_head=x_test[0:1000,3]\n\nfig = plt.figure(figsize = (8, 8))\nfig.add_subplot(1,2,1)\nplt.plot(x_train_head,label='train')\nplt.plot(x_test_head,label='test')\nplt.ylabel('Original unit')\nfig.add_subplot(1,2,2)\nplt.plot(x_train_scaled_head,label='scaled_train')\nplt.plot(x_test_scaled_head,label='scaled_test')\nplt.ylabel('Scaled unit')\nplt.show()","ba991d35":"Count = pd.value_counts(y_train, sort = True).sort_index()\nCount.plot(kind = 'bar')\nplt.title(\"Class count\")\nplt.xlabel(\"Flag\")\nplt.ylabel(\"Frequency\")","0a80f329":"sm = SMOTE()\nx_train_new, y_train_new = sm.fit_sample(x_train, y_train)\nx_train_scaled_new, y_train_scaled_new = sm.fit_sample(x_train_scaled, y_train)","d695839e":"pca = PCA().fit(x_train_scaled_new)\nplt.rcParams[\"figure.figsize\"] = (12,6)\nfig, ax = plt.subplots()\nxi = np.arange(1, 163, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\nplt.axhline(y=0.70, color='r', linestyle='-')\nplt.text(0.5, 0.73, '70% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.90, color='r', linestyle='-')\nplt.text(0.5, 0.85, '90% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","69e9ff98":"n_comp=[0.70,0.75,0.80,0.90]","1b44d193":"pca = PCA(n_components=n_comp[0])\npca.fit(x_train_scaled_new)\nx_train_new_0 = pca.transform(x_train_scaled_new)\nx_test_0 = pca.transform(x_test_scaled)\nprint(\"Number of features after PCA = \", x_test_0.shape[1])\ncorrmat_pca = pd.DataFrame(x_train_new_0).corr()\nsn.heatmap(corrmat_pca, vmax=.8, square=True);\nplt.show()\nx_train_final_70 = x_train_new_0\ny_train_final_70 = y_train_scaled_new\nx_test_final_70= x_test_0\ny_test_final_70 = y_test","6d2a9cf2":"pca = PCA(n_components=n_comp[1])\npca.fit(x_train_scaled_new)\nx_train_new_1 = pca.transform(x_train_scaled_new)\nx_test_1 = pca.transform(x_test_scaled)\nprint(\"Number of features after PCA = \", x_test_1.shape[1])\ncorrmat_pca = pd.DataFrame(x_train_new_1).corr()\nsn.heatmap(corrmat_pca, vmax=.8, square=True);\nplt.show()\nx_train_final_75 = x_train_new_1\ny_train_final_75 = y_train_scaled_new\nx_test_final_75 = x_test_1\ny_test_final_75 = y_test","6aa56965":"pca = PCA(n_components=n_comp[2])\npca.fit(x_train_scaled_new)\nx_train_new_2 = pca.transform(x_train_scaled_new)\nx_test_2 = pca.transform(x_test_scaled)\nprint(\"Number of features after PCA = \", x_test_2.shape[1])\ncorrmat_pca = pd.DataFrame(x_train_new_1).corr()\nsn.heatmap(corrmat_pca, vmax=.8, square=True);\nplt.show()\nx_train_final_80 = x_train_new_2\ny_train_final_80 = y_train_scaled_new\nx_test_final_80 = x_test_2\ny_test_final_80 = y_test","3252dac9":"pca = PCA(n_components=n_comp[3])\npca.fit(x_train_scaled_new)\nx_train_new_3 = pca.transform(x_train_scaled_new)\nx_test_3 = pca.transform(x_test_scaled)\nprint(\"Number of features after PCA = \", x_test_3.shape[1])\ncorrmat_pca = pd.DataFrame(x_train_new_1).corr()\nsn.heatmap(corrmat_pca, vmax=.8, square=True);\nplt.show()\nx_train_final_90 = x_train_new_3\ny_train_final_90 = y_train_scaled_new\nx_test_final_90 = x_test_3\ny_test_final_90 = y_test","71475d2e":"def confusionmatrix(y_test,y_predict,x='name of model'):\n    cm=metrics.confusion_matrix(y_test,y_predict)\n    # recall=print(round(recall_score(y_test, y_predict, average='macro')*100,2))\n    plt.figure(figsize=(10,7))\n    sn.heatmap(cm,annot=True,cbar=False, fmt='g')\n    cm1 = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(x)\n    TC= 10*cm1.FP + 500*cm1.FN   \n    return [ plt.show(),print(cm1),print(TC)]","16c4e4c1":"from IPython.display import Image\nImage(\"..\/input\/image-req\/Capture.JPG\")","3312a3be":"c_parameter_range = [0.0001,0.001,0.01,0.1,1,10,100]\n\nlogistic_acc_table_70 = pd.DataFrame(columns = ['C_parameter','Recall'])\nlogistic_acc_table_70['C_parameter'] = c_parameter_range\n\n    \np=0\n\nfor c_param in c_parameter_range:\n    lr = LogisticRegression(C = c_param)\n    lr.fit(x_train_final_70, y_train_final_70.values.ravel())\n    y_pred = lr.predict(x_test_final_70)\n    logistic_acc_table_70.iloc[p,1] = recall_score(y_test_final_70,y_pred)\n    p+=1\n    \n","54281b0c":"x=logistic_acc_table_70.isin([max(logistic_acc_table_70.Recall)])\nseriesObj = x.any()\ncolumnNames = list(seriesObj[seriesObj == True].index)\nfor col in columnNames:\n        rows = list(x[col][x[col] == True].index)\nindex = np.array(rows)\nmax_c_param=logistic_acc_table_70.C_parameter[index[0]]\nprint(max(logistic_acc_table_70.Recall))  \n","50d6c341":"logreg = LogisticRegression(C=max_c_param)\nlogreg.fit(x_train_final_70, y_train_final_70)","6cedcb78":"# visualization using confusion matrix\nconfusionmatrix(y_test_final_70,logreg.predict(x_test_final_70),x='Logistic regression_70')","e1f7e6d8":"logistic_acc_table_75 = pd.DataFrame(columns = ['C_parameter','Recall'])\nlogistic_acc_table_75['C_parameter'] = c_parameter_range\n\n\n    \nj=0\n\nfor c_param in c_parameter_range:\n    lr = LogisticRegression(C = c_param)\n    lr.fit(x_train_final_75, y_train_final_75.values.ravel())\n    y_pred = lr.predict(x_test_final_75)\n    logistic_acc_table_75.iloc[j,1] = recall_score(y_test_final_75,y_pred)\n    j+=1\n\n","8c0468dd":"x=logistic_acc_table_75.isin([max(logistic_acc_table_75.Recall)])\nseriesObj = x.any()\ncolumnNames = list(seriesObj[seriesObj == True].index)\nfor col in columnNames:\n        rows = list(x[col][x[col] == True].index)\nindex = np.array(rows)\nmax_c_param=logistic_acc_table_75.C_parameter[index[0]]\n\n\nprint(max(logistic_acc_table_75.Recall)) ","77398c8f":"logreg = LogisticRegression(C=max_c_param)\nlogreg.fit(x_train_final_75, y_train_final_75)","30219656":"# visualization using confusion matrix\nconfusionmatrix(y_test_final_75,logreg.predict(x_test_final_75),x='Logistic regression_75')","2b2db633":"logistic_acc_table_80 = pd.DataFrame(columns = ['C_parameter','Recall'])\nlogistic_acc_table_80['C_parameter'] = c_parameter_range\n\n    \nk=0\n\nfor c_param in c_parameter_range:\n    lr = LogisticRegression(C = c_param)\n    lr.fit(x_train_final_80, y_train_final_80.values.ravel())\n    y_pred = lr.predict(x_test_final_80)\n    logistic_acc_table_80.iloc[k,1] = recall_score(y_test_final_80,y_pred)\n    k+=1","02f00fd6":"x=logistic_acc_table_80.isin([max(logistic_acc_table_80.Recall)])\nseriesObj = x.any()\ncolumnNames = list(seriesObj[seriesObj == True].index)\nfor col in columnNames:\n        rows = list(x[col][x[col] == True].index)\nindex = np.array(rows)\nmax_c_param=logistic_acc_table_80.C_parameter[index[0]]\n\n\n\nprint(max(logistic_acc_table_80.Recall)) ","9c71e118":"logreg = LogisticRegression(C=max_c_param)\nlogreg.fit(x_train_final_80, y_train_final_80)","c7026d4d":"# visualization using confusion matrix\nconfusionmatrix(y_test_final_80,logreg.predict(x_test_final_80),x='Logistic regression_80')","3466cb85":"logistic_acc_table_90 = pd.DataFrame(columns = ['C_parameter','Recall'])\nlogistic_acc_table_90['C_parameter'] = c_parameter_range\n\n    \nm=0\n\nfor c_param in c_parameter_range:\n    lr = LogisticRegression(C = c_param)\n    lr.fit(x_train_final_90, y_train_final_90.values.ravel())\n    y_pred = lr.predict(x_test_final_90)\n    logistic_acc_table_90.iloc[m,1] = recall_score(y_test_final_90,y_pred)\n    m+=1","008a021d":"x=logistic_acc_table_90.isin([max(logistic_acc_table_90.Recall)])\nseriesObj = x.any()\ncolumnNames = list(seriesObj[seriesObj == True].index)\nfor col in columnNames:\n        rows = list(x[col][x[col] == True].index)\nindex = np.array(rows)\nmax_c_param=logistic_acc_table_90.C_parameter[index[0]]\n\nprint(max(logistic_acc_table_90.Recall))  ","c72ce62f":"logreg = LogisticRegression(C=max_c_param)\nlogreg.fit(x_train_final_90, y_train_final_90)","c05986ca":"# visualization using confusion matrix\nconfusionmatrix(y_test_final_90,logreg.predict(x_test_final_90),x='Logistic regression_90')","36eba3ba":"clf = SVC(C = 0.03593813663804628,kernel = 'rbf',gamma = 0.1778279410038923)\nclf.fit(x_train_final_75, y_train_final_75)\ny_pred = clf.predict(x_test_final_75)\nRecall = recall_score(y_test_final_75,y_pred)\nconfusionmatrix(y_test_final_75,clf.predict(x_test_final_75),x='SUPPORT VECTOR MACHINE')","623609a8":"def confusionmatrix_knn(y_test,y_predict):\n    cm=metrics.confusion_matrix(y_test,y_predict)\n    cm1 = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n    TC= 10*cm1.FP + 500*cm1.FN   \n    return [TC]","48f25fd4":"total_cost_knn= np.empty((10, 1))\n\nfor i in range(0,10):\n      knn =KNeighborsClassifier(n_neighbors=i+1)\n      knn.fit(x_train_final_75, y_train_final_75)\n      total_cost_knn[i,:]=confusionmatrix_knn(y_test_final_75,knn.predict(x_test_final_75))\n      \n      \nplt.figure()        \nl = range(1,11)\nfor j in range(len(l)):     \n    plt.plot( l, total_cost_knn)\n    plt.xlabel('Values of n_neighbors')\n    plt.ylabel('Total cost')\n    plt.title('Variation of total cost with different n values in knn method')","d303ac50":"minElement = np.amin(total_cost_knn)    \nresult = np.where(total_cost_knn == np.amin(total_cost_knn))\nmin_cost_index=result[0]+1","6f00942a":"#visualization using confusion matrix for maximum accuracy as it comes at n_neighbors=9\nknn =KNeighborsClassifier(n_neighbors=min_cost_index[0])\nknn.fit(x_train_final_75, y_train_final_75)\nconfusionmatrix(y_test_final_75,knn.predict(x_test_final_75),x='KNN CLASSIFIER')","ede42c4d":"def confusionmatrix_RANDOM(y_test,y_predict):\n    cm=metrics.confusion_matrix(y_test,y_predict)\n    cm1 = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n    TC= 10*cm1.FP + 500*cm1.FN   \n    return [TC,print(cm1),print(TC)]","ebcbb739":"total_cost_RANDOM= []\n\n\n\nfor m in range(100,200):\n    random_forest=RandomForestClassifier(n_estimators= m,random_state=1)\n    random_forest.fit(x_train_final_75, y_train_final_75)\n    total_cost=confusionmatrix_RANDOM(y_test_final_75,random_forest.predict(x_test_final_75))\n    total_cost_RANDOM.append(total_cost)\n\n\nplt.figure()        \nl = range(100,200)\nfor j in range(len(l)):     \n    plt.plot( l, total_cost_RANDOM)\n    plt.xlabel('Values of n_estimators')\n    plt.ylabel('Total cost')\n    plt.title('Variation of total cost with different n values in random forest')","024d7d13":"def Extract(lst): \n    return [item[0] for item in lst]\n\ntotal_cost_min=Extract(total_cost_RANDOM)\n\nnew_list = []\nfor item in total_cost_min:\n    new_list.append(float(item))\ntotal_cost_RANDOM=np.asarray(new_list)\ntotal_cost_RANDOM = pd.DataFrame(total_cost_RANDOM)\ntotal_cost_RANDOM.insert(0, \"index\", range(100,200), True) \n\nx=total_cost_RANDOM.isin([min(total_cost_RANDOM[0])])\nseriesObj = x.any()\ncolumnNames = list(seriesObj[seriesObj == True].index)\nfor col in columnNames:\n        rows = list(x[col][x[col] == True].index)\nindex = np.array(rows)\nindex_value=total_cost_RANDOM.index[index[0]]\nindex_value_random=int(total_cost_RANDOM.loc[index_value]['index'])","8138824a":"#visualization using confusion matrix for maximum accuracy as it comes at n=80\nrandom_forest=RandomForestClassifier(n_estimators= index_value_random,random_state=1)\nrandom_forest.fit(x_train_final_75, y_train_final_75)\nconfusionmatrix(y_test_final_75,random_forest.predict(x_test_final_75),x='Random Forest')\n","87e3e063":"# Visualizing missing data in each column\n","3a91c88f":"# TEST AND TRAIN VALUES FOR 70% VARIANCE","21e881c7":"# **Support vector machine**","a3f3366c":"Visualize the result","c18ec4e7":"Find the value of n_estimators for representing minimum cost","e5eb03c1":"## let's choose the c value corrsponding to maximum recall","c1225090":"Let's fit the RANDOM FOREST model for different n_estimators","6bc65417":"# Standardize the training and test data-set","d760358a":"# Apply logistic regression on data having 70% feature after PCA","7ec3dd6b":"# Visualizing imbalance in the data set\n","75a30e6e":"# Extracting missing data column more than 50%\n","f41b543e":"**Let's fit the model**","450d4c0f":"## **CONFUSION MATRIX FUNCTION\n\nLet's define a function for confusion matrix","1739ea77":"Visualize confusion matrix and cost","995731ca":"# Fill missing data in training and test data-set\n# Imputing the NA values in our data with the median value to get the central tendency or behaviour of trucks.","660fcb4e":"Let's fit the model","72dcc67b":"# Drop the features with high amount of missing data in both train and test data-set","daaa6074":"# **Random Forest**","eb9c823e":"Let's fit the model","187b3fd0":"# **> EXPLORATORY DATA ANALYSIS AND DATA CLEANING**","c0163a9d":"# TEST AND TRAIN VALUES FOR 90% VARIANCE\n","1aa0f2d4":"Visualize","8f792465":"Let's first define a new function of confusion matrix for this :","163cae46":"## let's choose the c value corrsponding to maximum recall ","db8920fd":"Total cost comes out to be 27960$","9d915a81":"Find the value of n_neighbor for representing minimum cost","975d9f6d":"# Apply logistic regression on data having 75% feature after PCA","c165671c":"Visualize the confusion matrix for corresponding n_neighbors value","eb7094e8":"Total cost comes out to be 21930$","549e2e0c":"# Now comes the part of application of model :","4fac46c2":"# Extract features and labels from the training and test data-set","8b9b05d1":"**Let's continue with data of 75 % PCA Variance**","49778789":"# TEST AND TRAIN VALUES FOR 80% VARIANCE","586ded2a":"# Exploratory Data Analysis :\n# Compute the percentage of missing data for each attribute in the training data set","6a588469":"# 50 different combinations of C and Gamma values were used to evaluate the C and gamma parameters were chosen using the logarithmic scale between total cost came to be minimum when C=0.03593813663804628 and gamma = 0.1778279410038923 with recall score of 0.9786666666666667","4f2a1652":"# Apply logistic regression on data having 90% feature after PCA","9c9e9a2d":"## Here we need to create a model which reduces the misclassification based on Cost 1 (10) and Cost 2 (500) and since Cost 2s multiplication factor is higher than Cost 1s i am going with recall as the performance metric here which basically gives the measure of how many positives cases did we catch(TP\/TP+FN). \n## Higher recall means lesser FNs which in turn reduces our total cost.\n","45fe1b84":"## Logistic Regression","dfa67a40":"Let's fit the KNN model for different n_neighbors","230eb29c":"Visualize","f6629d75":"# TEST AND TRAIN VALUES FOR 75% VARIANCE","7e895840":"Let's fit the model","226f3150":"# Visualize the scaled data to ensure that the data look similar after scaling ","39d2f13c":"Total cost comes out to be 13250$","b4d34cd6":"# Replace class labels with integer values (neg = 0, pos = 1) in training and test data-set, also map the na values to NULL","9cbe8cf1":"# **KNN CLASSIFIER**","66bf4cb3":"Visualize the result","817066a4":"let's choose the c value corrsponding to maximum recall","79359c9b":"# Apply logistic regression on data having 80% feature after PCA","03bc5d50":"**## Principal Component Analysis**","b8cd0648":"# **Let's Import all required libraries :**","49bd8423":"Total cost comes out to be 25380$","7d79fc17":"## *This is highly imbalanced data set. We will have to do some sampling technique here before modelling.\n\n## *SMOTE (Synthetic Minority Oversampling Technique) for Handling Imbalanced Datasets","2ffe6268":"Total cost comes out to be 21940$","2c995113":"Let's first define a new function of confusion matrix for this :","cd42785e":"Total cost comes out to be 21040$","54c22334":"Total cost comes out to be 22530$","d3fe6c92":"*** From above it's evident that lowest cost comes out at PCA with 70 % of variance**\n","3fb7b5c3":"# Print number of positive classes and number of negative classes in the training data-set","838a0d6f":"*** From here we can see it is highly imbalanced data set**","9748e3c1":"**This is very interesting data which is highly imbalanced and task is to minimize the cost when there is lot of difference in it. This notebook contain Exploratory data analysis , data cleaning , data visualization ,Principal component Analysis and fitting of different model which involves :**\n1. Logistic regression\n2. Support vector machine\n3. KNN Classification\n4. Random forest\n\n**where ,Logistic regression is fitted for different training data set corresponding to different PCA (PRINCIPAL COMPONENT ANALYSIS) having different level of variance.\nMain motive of this notebook is minimize the misclassification to reduce the total cost of maintenance.**\n\n**Best result we obtained is with support vector machine i.e, Total cost = 13520$**","01bd64e3":"## let's choose the c value corrsponding to maximum recall","7e6362c3":"# Principal Component Analysis\n","e112fcc8":"**Now import data**"}}