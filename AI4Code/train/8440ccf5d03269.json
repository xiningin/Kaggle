{"cell_type":{"e086462e":"code","7342e41e":"code","33b4f62c":"code","307f72fd":"code","f4043340":"code","8d077d53":"code","33ee242c":"code","ceb8b380":"code","c33c0508":"code","29edf8f2":"code","6d537f98":"code","157abf02":"code","24856f84":"code","2572f2fd":"code","f6da7092":"code","20e9b19e":"code","614156c9":"code","e6e9fee5":"code","c0e4b51b":"code","59fe1dec":"code","dd974f13":"code","255930f3":"code","945b82d3":"code","18994826":"code","75f566d2":"code","3468577d":"code","fabaf85e":"code","b661ca22":"code","e3186b01":"code","468a6fcf":"code","d4d246ed":"code","3ee688ff":"code","862d4401":"code","584e40a0":"code","5977c97a":"code","4b875ef4":"code","b2bfdf81":"code","f3c76a16":"code","e7f34936":"code","a03e072e":"code","06f74b38":"code","65eaa92f":"code","8674e5ae":"code","6692dfb9":"code","0895d6ca":"code","43387ab1":"code","9e8a5fc1":"code","ff7c159a":"code","7de80de1":"code","b93ca1b1":"code","69c5a25b":"code","e8fd3d7d":"code","5fab95ba":"code","e32bf181":"code","b5835e9e":"code","579033ef":"code","9ed88b08":"code","8bf03283":"code","6aeec5ee":"code","657b5078":"code","6a41c4f4":"code","cc767099":"code","d27ac68b":"code","be6f3bec":"code","54685a3e":"code","df480fb6":"code","700d9d7e":"code","9ad1e9a5":"code","8231d783":"code","2a0f972c":"code","4dad7b9a":"code","8f2a9d8c":"code","f1eff7ea":"code","6767ccb9":"code","8413b392":"code","925673e6":"code","2d750bcc":"code","eea895e0":"code","79365f24":"code","2ca41bb1":"code","2617b3cd":"code","076a0d7d":"code","8f1c8676":"code","b76adc3b":"code","26ddb5b1":"markdown","e2a6ed3f":"markdown","f71512f2":"markdown","1ab91cea":"markdown","976c6a02":"markdown","21b0847e":"markdown","0fc3bcf0":"markdown","d5c37e87":"markdown","20523362":"markdown","b04b8b2c":"markdown","330d0e75":"markdown","76e2ea93":"markdown","61842a7f":"markdown","9a0d4461":"markdown","51880ea9":"markdown","1937482d":"markdown","a5ea9b0d":"markdown","4aad2032":"markdown","3b42fc24":"markdown","b371f414":"markdown","097d6196":"markdown","2567023f":"markdown","aa19aa6f":"markdown","e8cad670":"markdown","d2d94fe8":"markdown","d84d5df6":"markdown","2830a466":"markdown","eef0f6cf":"markdown","1557b19b":"markdown","9e7ebfc1":"markdown","0a1b8b82":"markdown","0431687a":"markdown","4232b517":"markdown","46bd26b3":"markdown","3c2d4e72":"markdown","bf61b427":"markdown","3dd85376":"markdown","fde79e7c":"markdown","a5171e29":"markdown","968c24f9":"markdown","e5ec88c1":"markdown","6530e7b0":"markdown","8e1a3c14":"markdown","394ed5f1":"markdown","0e7c4859":"markdown","67bb1e31":"markdown","38b92ef4":"markdown","a970eaae":"markdown","df222f05":"markdown","43a17a5f":"markdown","1551be75":"markdown","4ba9bf80":"markdown"},"source":{"e086462e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7342e41e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.stats import normaltest, norm\nimport scipy as sp\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')","33b4f62c":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Perceptron, LogisticRegression\nfrom xgboost import XGBRegressor, XGBRFClassifier\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, RocCurveDisplay,confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, classification_report, accuracy_score, f1_score\nfrom sklearn.metrics import recall_score, plot_confusion_matrix, precision_score, plot_precision_recall_curve, classification_report\n    \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","307f72fd":"sns.set()\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nprint(\"Setup Complete\")","f4043340":"file = '\/kaggle\/input\/smart-grid\/Data_for_UCI_named.csv'","8d077d53":"uci_data = pd.read_csv(file)","33ee242c":"uci_data.tail()","ceb8b380":"uci_data.info()","c33c0508":"uci_data.describe()","29edf8f2":"stabf_count = uci_data.stabf.value_counts()#\nstabf_count","6d537f98":"print('stabf target have {}% for unstable and {}% for stable.'.format(round(100*(stabf_count[0]\/stabf_count.sum())),\n                                                                  round(100*(stabf_count[1]\/stabf_count.sum()))))","157abf02":"plt.figure(dpi=100)\nsns.countplot(uci_data.stabf)\nplt.title(r'Stabf categorical count.')\nplt.show()","24856f84":"# we plot also stab\nfig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nplt.tight_layout()\n\nsns.barplot(x='stabf', y='stab', data=uci_data, ax=axes[0])\nsns.boxplot(x='stabf', y='stab', data=uci_data, ax=axes[1])\naxes[0].set_title('Stab Bar')\naxes[1].set_title('Stabf Box')\nplt.show()","2572f2fd":"fig, axis = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nplt.tight_layout()\n\nsns.boxplot(uci_data.stab, ax = axis[0])\nsns.distplot(uci_data['stab'], fit=norm, kde=False,  ax = axis[1], norm_hist=True)\n\naxis[0].set_title('Stab box')\naxis[1].set_title('Stab distplot')\nplt.show()","f6da7092":"#test if stab is normal\n# the null hypothesis is data come from normal distribution if p > alpha the null hypo. cannot rejected .\nnormaltest(uci_data.stab)","20e9b19e":"cols = list(set(uci_data.columns) - set(['stab', 'stabf']))","614156c9":"cols = sorted(cols)","e6e9fee5":"def distplot_multi(data):\n    \"\"\" plot multi distplot\"\"\"\n        \n    from scipy.stats import norm\n    cols = []\n        \n    #Feature that is int64 or float64 type \n    for i in data.columns:\n        if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n        \n    gp = plt.figure(figsize=(20,20))\n    gp.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i in range(1, len(cols)+1):\n        ax = gp.add_subplot(3,4,i)\n        sns.distplot(data[cols[i-1]], fit=norm, kde=False)\n        ax.set_title('{} max. likelihood gaussian'.format(cols[i-1]))","c0e4b51b":"def boxplot_multi(data):\n        \n    \"\"\" plot multi box plot\n        hue for plotting categorical data\n    \"\"\"\n    \n    cols = []\n    for i in data.columns:\n        if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n            cols.append(i)\n    \n    gp = plt.figure(figsize=(20,20))\n    gp.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i in range(1, len(cols)+1):\n        ax = gp.add_subplot(3, 4, i)\n        sns.boxplot(x = cols[i-1], data=data)\n        ax.set_title('Boxplot for {}'.format(cols[i-1]))","59fe1dec":"def correlation_plot(data, vrs= 'stab', vsr='stabf'):\n    \n    \"\"\"\n    This function plot only a variable that are correlated with a target  \n        \n        data: array m_observation x n_feature\n        vrs:  target feature (n_observation, )\n        cols: interested features\n    \"\"\"\n    cols = data.columns # we take all feature\n                \n    feat = list(set(cols) - set([vrs, vsr]))\n    \n    fig = plt.figure(figsize=(20, 20))\n    fig.subplots_adjust(wspace = 0.3, hspace = 0.3)\n    \n    for i in range(1,len(feat)+1):\n        \n        ax = fig.add_subplot(2, 2, i)     \n        sns.scatterplot(x=data[feat[i-1]], y=data[vrs], data=data, hue=vsr, ax=ax)   \n        ax.set_xlabel(feat[i-1])\n        ax.set_ylabel(vrs)\n        ax.set_title('Plotting data {0} vs {1}'.format(vrs, feat[i-1]))\n        ax.legend(loc='best')","dd974f13":"distplot_multi(uci_data[cols])","255930f3":"#we check if p1 comes from normal distribution\nnormaltest(uci_data.p1)","945b82d3":"boxplot_multi(uci_data[cols])","18994826":"uci_data.corr(method='spearman')","75f566d2":"plt.figure(dpi=100, figsize=(15,5))\nsns.heatmap(uci_data.corr(method='spearman'), robust=True, annot=True)\nplt.show()","3468577d":"correlation_plot(uci_data[['tau1', 'tau2', 'tau3', 'tau4', 'stab', 'stabf']])","fabaf85e":"#for gamma\ncorrelation_plot(uci_data[['g1', 'g2', 'g3', 'g4', 'stab', 'stabf']])","b661ca22":"correlation_plot(uci_data[['p2', 'p3', 'p4', 'p1', 'stabf']], vrs='p1')","e3186b01":"uci_data['stabf'] = uci_data['stabf'].astype('category')\nuci_data['stabf'].cat.categories = [0, 1] # 0 for stable, 1 for unstable\nuci_data['stabf'] = uci_data['stabf'].astype('int')","468a6fcf":"uci_data.tail()","d4d246ed":"#we  define data, reg_target, clas_target\ndata = uci_data.drop(columns=['p1', 'stab', 'stabf'])","3ee688ff":"reg_target = uci_data['stab'] #target for regression\nclas_target = uci_data['stabf']#target fot classification","862d4401":"# regression\nrtrain, rtest, rytrain, rytest = train_test_split(data, reg_target, test_size=0.2, random_state=42)","584e40a0":"print('Regression: xtrain: {}, xtest: {}, ytrain: {}, ytest: {}'.format(rtrain.shape, rtest.shape,\n                                                                        rytrain.shape, rytest.shape))","5977c97a":"# classification\nctrain, ctest, cytrain, cytest = train_test_split(data, clas_target, stratify=clas_target, test_size=0.2,\n                                                 random_state=42)","4b875ef4":"print('Classification: xtrain: {}, xtest: {}, ytrain: {}, ytest: {}'.format(ctrain.shape, ctest.shape,\n                                                                        cytrain.shape, cytest.shape))","b2bfdf81":"class toolsGrid:\n    \"\"\"\n        This class contains all function for classification and regresssion\n    \"\"\"\n    \n    def __init__(self, xtrain=None, ytrain=None):\n        \n        self.xtrain = xtrain # train data\n        self.ytrain = ytrain # train target data\n        \n        # list of different learner for regression\n        self.reg_model = {'LinearRegression': LinearRegression(), \n                'KNeighborsRegression': KNeighborsRegressor(),\n                'RandomForestRegression': RandomForestRegressor(),\n                'GradientBoostingRegression': GradientBoostingRegressor(),\n                'XGBoostRegression': XGBRegressor(),\n                'AdaboostRegression': AdaBoostRegressor(),\n                'ExtraTreesRegressor': ExtraTreesRegressor()}\n        \n        # list of different learner for classification\n        self.clas_model = {'LogisticRegression': LogisticRegression(), \n                'KNeighborsClassifier': KNeighborsClassifier(),\n                'RandomForestClassifier': RandomForestClassifier(),\n                'GradientBoostingClassifier': GradientBoostingClassifier(),\n                'XGBoostClassifier': XGBRFClassifier(),\n                'AdaboostClassifier': AdaBoostClassifier(),\n                'ExtraTreesClassifier': ExtraTreesClassifier(),\n                'Perceptron': Perceptron()}\n        \n    \n    def split_data(self, label= True):\n        \"\"\"\n        This function splits data to train set and target set\n        \n        data: matrix feature n_observation x n_feature dimension\n        \n        return Xtrain, Xvalid, Ytrain, Yvalid\n        \"\"\"\n        if label:\n            Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(self.xtrain, self.ytrain, random_state=42,\n                                                          test_size=0.2, shuffle=True, stratify=self.ytrain)\n        else:\n            Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(self.xtrain, self.ytrain, random_state=42,\n                                                          test_size=0.2, shuffle=True)\n        \n        return Xtrain, Xvalid, Ytrain, Yvalid\n    \n    \n    def regression_learner_selection(self):\n\n        \"\"\"\n            This function compute differents score measure like cross validation,\n            r2, root mean squared error and mean absolute error.\n            reg_model: dictionary type containing different model algorithm.     \n        \"\"\" \n    \n        result = {}\n        \n        #x, _, y, _ = self.split_data() # take only xtrain and ytrain\n    \n        # we take each regression model\n        for cm in list(self.reg_model.items()):\n        \n            name = cm[0] #name of learner\n            model = cm[1] # learner\n        \n            cvs = cross_val_score(model, self.xtrain, self.ytrain, cv=10).mean() #mean of cv score\n            ypred = cross_val_predict(model, self.xtrain, self.ytrain, cv=10) #prediction cv\n            r2 = r2_score(self.ytrain, ypred)\n            mse = mean_squared_error(self.ytrain, ypred)\n            mae = mean_absolute_error(self.ytrain, ypred)\n            rmse = np.sqrt(mse)\n        \n            result[name] = {'cross_val_score': cvs, 'rmse': rmse, 'mae': mae, 'r2': r2}\n        \n            print('{} model done !!!'.format(name))\n            \n        return pd.DataFrame(result)\n            \n    \n    def classification_learner_selection(self):\n\n        \"\"\"\n            This function compute differents score measure like cross validation,\n            auc, accuracy, recall, precision and f1.\n            reg_model: dictionary type containing different model algorithm.     \n        \"\"\" \n    \n        result = {}\n        matrix = []\n        \n        #\n    \n        # we take each classification model\n        for cm in list(self.clas_model.items()):\n        \n            name = cm[0] #name of learner\n            model = cm[1] # learner\n        \n            cvs = cross_val_score(model, self.xtrain, self.ytrain, cv=10).mean() #mean of cv score\n            ypred = cross_val_predict(model, self.xtrain, self.ytrain, cv=10) #prediction cv\n            auc = roc_auc_score(self.ytrain, ypred)\n            acc = accuracy_score(self.ytrain, ypred)\n            recall = recall_score(self.ytrain, ypred)\n            precision = precision_score(self.ytrain, ypred)\n            f1 = f1_score(self.ytrain, ypred)\n        \n            result[name] = {'cross_val_score': cvs, 'auc': auc, 'acc': acc, 'precision': precision,\n                           'recall':recall, 'f1': f1}\n        \n            print('{} model done !!!'.format(name))\n            \n        return pd.DataFrame(result)\n    \n    \n    def confusion_matrix(self):\n        \"\"\"\n            plot confusion matrix\n        \"\"\"\n        xtrain, xvalid, ytrain, yvalid = self.split_data() # take only xtrain and ytrain\n        \n        \n        feat = list(self.clas_model.keys()) # we take all learner\n        \n        fig = plt.figure(figsize=(20, 20))\n        fig.subplots_adjust(wspace = 0.4, hspace = 0.4)\n    \n        for i in range(1,len(feat)+1):\n        \n            ax = fig.add_subplot(2, 4, i)   \n            learner = self.clas_model[feat[i-1]]\n            \n            plot_confusion_matrix(learner.fit(xtrain, ytrain), xvalid, yvalid,\n                                  labels=[0,1], ax=ax)  \n           \n            ax.set_title('{} Conf. Matrix'.format(feat[i-1]))\n            plt.grid(False) \n            \n    \n    def roc_auc(self):\n        \"\"\"\n            plot roc_auc\n        \"\"\"\n        xtrain, xvalid, ytrain, yvalid = self.split_data() # take only xtrain and ytrain\n        \n        feat = list(self.clas_model.keys()) # we take all learner\n        \n        fig = plt.figure(figsize=(20, 20))\n        fig.subplots_adjust(wspace = 0.4, hspace = 0.4)\n    \n        for i in range(1,len(feat)+1):\n        \n            ax = fig.add_subplot(2, 4, i)   \n            learner = self.clas_model[feat[i-1]]\n            \n            plot_roc_curve(learner.fit(xtrain, ytrain), xvalid, yvalid, ax=ax)  \n           \n            ax.set_title('{} ROC'.format(feat[i-1]))\n            plt.grid(False) \n            \n    def precision_recall(self):\n        \"\"\"\n            plot precision recall\n        \"\"\"\n        xtrain, xvalid, ytrain, yvalid = self.split_data() # take only xtrain and ytrain\n        \n        feat = list(self.clas_model.keys()) # we take all learner\n        \n        fig = plt.figure(figsize=(20, 20))\n        fig.subplots_adjust(wspace = 0.4, hspace = 0.4)\n    \n        for i in range(1,len(feat)+1):\n        \n            ax = fig.add_subplot(2, 4, i)   \n            learner = self.clas_model[feat[i-1]]\n            \n            plot_precision_recall_curve(learner.fit(xtrain, ytrain), xvalid, yvalid, ax=ax)  \n           \n            ax.set_title('{} PR'.format(feat[i-1]))\n            plt.grid(False) \n        \n            \n            ","f3c76a16":"electrical = toolsGrid(xtrain=ctrain, ytrain=cytrain)","e7f34936":"result = electrical.classification_learner_selection()","a03e072e":"result","06f74b38":"electrical.confusion_matrix()","65eaa92f":"electrical.roc_auc()","8674e5ae":"electrical.precision_recall()","6692dfb9":"model = toolsGrid(xtrain=rtrain, ytrain=rytrain)","0895d6ca":"model.regression_learner_selection()","43387ab1":"#create neural network function\n\ndef neural_network(output_activation=None, data=None, n=None):\n    \n    \"\"\"\n        neural network function\n        output_activation is for the last dense\n        n number of output unit\n        \n    \"\"\"\n    inputs = keras.Input(shape=(data.shape[1], ), name = 'Electrical_Grid') #input for data\n    \n    # first hidden dense 100 neurons with dropout layers\n    x = layers.Dense(units=100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.02),\n                    name='dense_1')(inputs)\n    x = layers.Dropout(0.2)(x)\n    \n    # second hidden dense 100 neurons with dropout layers\n    x = layers.Dense(units=100, activation='relu', kernel_regularizer=keras.regularizers.l2(0.02),\n                    name='dense_2')(x)\n    x = layers.Dropout(0.2)(x)\n    \n    #output dense\n    outputs = layers.Dense(units=n, activation=output_activation, name='prediction')(x) \n    \n    model = keras.Model(inputs=inputs, outputs=outputs) # create model\n    \n    return model","9e8a5fc1":"#we start\nmodel_classifier = neural_network(output_activation='sigmoid', data=ctrain, n=1)","ff7c159a":"model_classifier.summary()","7de80de1":"#we can see my model\nkeras.utils.plot_model(model_classifier, \"multi_input_and_output_model.png\", show_shapes=True)","b93ca1b1":"model_classifier.compile(loss='binary_crossentropy', metrics=['AUC','accuracy','Precision', 'Recall'])","69c5a25b":"# Load the TensorBoard notebook extension\n%load_ext tensorboard","e8fd3d7d":"tensorboard_callback = keras.callbacks.TensorBoard( log_dir=\"\/logs\/classification\",\n    histogram_freq=1,  # How often to log histogram visualizations\n    embeddings_freq=0,  # How often to log embedding visualizations\n    update_freq=\"epoch\",\n)  # How often to write logs (default: once per epoch)\n\nhistory_classifier = model_classifier.fit(ctrain, cytrain, epochs=200, callbacks=[tensorboard_callback], verbose=0, batch_size=128,\n                     validation_split=0.2)","5fab95ba":"hist_class = pd.DataFrame(history_classifier.history).rolling(window=16).mean()","e32bf181":"lister = list(history_classifier.history.keys())","b5835e9e":"fig = plt.figure(figsize=(20,20))\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\n\nfor i in range(5):\n    ax = fig.add_subplot(2,3,i+1)\n    ax.plot(hist_class[lister[i]])\n    ax.plot(hist_class[lister[i+5]])\n    ax.set_title(lister[i] + ' vs '+ lister[i+5])\n    ax.set_xlabel('epoch')\n    ax.legend([lister[i], lister[i+5]], loc='upper left')","579033ef":"regression_model = neural_network(output_activation='linear', data=rtrain, n=1)","9ed88b08":"def coef_r2(y_true, y_pred):\n    ## we campute a coef. determination\n    \n    y_true = tf.cast(y_true, dtype=tf.float64)\n    y_pred = tf.cast(y_true, dtype=tf.float64)\n    ss_res = keras.backend.sum(keras.backend.square(y_true - y_pred))\n    ss_tot = keras.backend.sum(keras.backend.square(y_true - keras.backend.mean(y_true)))\n    \n    return (1 - ss_res\/(ss_tot))","8bf03283":"regression_model.compile(loss='mse', metrics=[coef_r2, 'mse','mae'])","6aeec5ee":"board_callback = keras.callbacks.TensorBoard( log_dir=\"logs\",\n    histogram_freq=1,  # How often to log histogram visualizations\n    embeddings_freq=1,  # How often to log embedding visualizations\n    update_freq=\"epoch\",\n)  # How often to write logs (default: once per epoch)\n\nhistory = regression_model.fit(rtrain, rytrain, epochs=200, callbacks=[board_callback], verbose=0, batch_size=128,\n                     validation_split=0.2)","657b5078":"story = pd.DataFrame(history.history)","6a41c4f4":"st = list(story.keys())","cc767099":"ig = plt.figure(figsize=(20,20))\nig.subplots_adjust(hspace=0.2, wspace=0.2)\n\nfor i in range(4):\n    ax = ig.add_subplot(2,2,i+1)\n    ax.plot(story[st[i]])\n    ax.plot(story[st[i+3]])\n    ax.set_title(st[i] + ' vs '+ st[i+4])\n    ax.set_xlabel('epoch')\n    ax.legend([st[i], st[i+4]], loc='upper left')","d27ac68b":"tree = toolsGrid(xtrain=ctrain, ytrain=cytrain)","be6f3bec":"#we split our data\nX_train, X_valid, Y_train, Y_valid = tree.split_data()","54685a3e":"print('X_train: {}, X_valid: {}'.format(X_train.shape, X_valid.shape))","df480fb6":"extratree = ExtraTreesClassifier(n_estimators=2000, criterion='entropy', random_state=42, n_jobs=-1)","700d9d7e":"extratree.fit(X_train, Y_train)","9ad1e9a5":"Y_pred = extratree.predict(X_valid)","8231d783":"#we compute auc, acc, precision, recal and f1\nprint(classification_report(Y_valid, Y_pred)) # 0 for stable and 1 for unstable","2a0f972c":"#AUC\nprint('AUC for ExtraTreeClassifier: {}'.format(roc_auc_score(Y_valid, Y_pred)))","4dad7b9a":"xgboost = toolsGrid(xtrain=rtrain, ytrain=rytrain)","8f2a9d8c":"#we split data\nx_train, x_valid, y_train, y_valid = xgboost.split_data(label=False)","f1eff7ea":"print('x_train: {}, x_valid: {}'.format(x_train.shape, x_valid.shape))","6767ccb9":"xgb = XGBRegressor(n_estimators=1000,learning_rate=0.100000012, importance_type='entropy', random_state=42)","8413b392":"xgb.fit(x_train, y_train)","925673e6":"y_pred = xgb.predict(x_valid)","2d750bcc":"print('r2: {}, mae: {}, rmse: {}'.format(r2_score(y_valid, y_pred), mean_absolute_error(y_valid, y_pred),\n                                        np.sqrt(mean_squared_error(y_valid, y_pred))) )","eea895e0":"model_classifier.evaluate(ctest, cytest)","79365f24":"proba_class = model_classifier.predict(ctest)","2ca41bb1":"rypred = xgb.predict(rtest)","2617b3cd":"print('r2: {}, mae: {}, rmse: {}'.format(r2_score(rytest, rypred), mean_absolute_error(rytest, rypred),\n                                        np.sqrt(mean_squared_error(rytest, rypred))))","076a0d7d":"prediction = pd.DataFrame()","8f1c8676":"# Classification\nprediction['stabf'] = cytest.values\nprediction['nn_proba'] = proba_class\nprediction['stab'] = rytest.values\nprediction['xgboost'] = rypred","b76adc3b":"prediction.head(10) #classification","26ddb5b1":"p1 is more or less collinear with p2, p3, p4 but opposite. p2, p3, and p4 are not correlated. (p1,p2,p3,p4) are not correlated with stab. but (tau1,tau2, tau3,tau4, g1, g2,g3,g4) are low correlated. This low correlation is due to the characteristic equation from article above.\n\n**we learn**\n\n- The 11 predictives feature are not correlated two by two. This means that reaction time of producer tau1 is not depend on 10 others predictive feature.\n\n- only p1 is correlated with p2,p3,p4 because there is producer and 2,3,4 are comsumer.. \n","e2a6ed3f":"**Comment**\n\n- For cross_val_score: ExtraTreesClassifier wins.\n\n- For auc score: ExtraTreesClassifier wins.\n\n- For  acc score: ExtraTreesClassifier wins.\n\n- For  precision score: random forest classifier wins.\n\n- For recall score : ExtraTreesClassifier wins.\n\n- For f1 score: ExtraTreesClassifier wins.\n\nExtraTreesClassifier wins all score except precision score for cross validation","f71512f2":"<a id = 'nn'><\/a>","1ab91cea":"The result for regression and classification is well. For our work, we have finally 3 good models for stability of decentral smart grid control concept. we summarize all our result in dataframe.","976c6a02":"# Prediction and evaluation","21b0847e":"## Regression","0fc3bcf0":"**END**\n\n**Be free to download, share and comment**","d5c37e87":"Neural network with only few layers  give us a good result. We have 3 good candidates for our problem.","20523362":"When stab is negative, star system is stable. When stab is positive star-system is unstable. stable have two outliers.","b04b8b2c":"This is a reason that p1 is not a predictive feature.","330d0e75":"Neural network for classification give a good result like ExtraTreeClassifier.","76e2ea93":"# Modelling\n\nWe have 3 good learners. Which learner for prediction? We make validation model for ExtraTreeClassifier and XGBoostRegression. ","61842a7f":"**Comment**\n\n- For FN: ExtraTreesClassifier produces 92 and randomForestClassifier produces 93.\n- for FP: ExtraTreesClassifier produces 33 and randomForestClassifier produces 50.\n---------------------------------------------------------------------------------\n**Total**: ExtraTreesClassifier produces 125 labels not true and randomForest 143 labels not true.\n\nExtraTreesClassifier wins","9a0d4461":"## Regression","51880ea9":"<a id = 'regress'><\/a>","1937482d":"<a id = 'explore'><\/a>","a5ea9b0d":"# Electrical Grid Stability\n\n**Abstract:** The local stability analysis of the 4-node star system (electricity producer is in the center) implementing Decentral Smart Grid Control concept.\n\n**Associated Tasks:** Classification, Regression\n\n## What is a Smart Grid?\n\n**A Smart Grid** is an electrical grid that includes a variety of operating and energy metrics, including smart meters, smart devices, renewable energy resources, and energy efficient resources.\nThe important aspect of a smart grid are the electronic power conditioning and control of the production and distribution of electricity.\n\n### The feature of the smart grid \n\nThe feature of the smart grid are:\n\n1. Reliability\n2. Flexibility in network topology\n3. Efficiency \n4. Sustainability\n5. Market-enabling\n\n\n<center>\n    <img  width=\"750\"  src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/84\/Staying_big_or_getting_smaller.jpg\">\n<\/center>\n<center> Characteristics of a smart grid (right) versus the traditional system (left). Wikipedia <\/center>\n\n\nSmart grid concept suggest to collect consumer demand data, centrally evaluate them given current supply and send price information back to customer for them to decide about usage. Due to the restrictions regarding IT information such that Cyber security, privacy protection and large required investments it remains unclear how such central can guarantee overall stability. That's why, theses articles [Decentral Smart Grid control](https:\/\/iopscience.iop.org\/article\/10.1088\/1367-2630\/17\/1\/015002) and [taming Instabilities in Power Grid Networks by Decentralized Control](https:\/\/www.researchgate.net\/publication\/280911891_Taming_Instabilities_in_Power_Grid_Networks_by_Decentralized_Control) propose a Decentral Smart Grid Control, the price is directly linked to the local grid frequency at each customer.\n\n\nThe objective in this work, it is to make classification and regression to find a local stability of the 4- node star system implementing decentral smart grid control concept.\n\nTo do so, we are talking about:\n\n1.[Load data and package](#load)\n- [Attribute information](#attribute)\n\n2.[Initial exploration](#explore)\n\n3.[Correlation between feature](#corr)\n\n4.[Feature engineering](#eng)\n\n5.[Preparing for modelling](#prepa)\n\n6.[The best models](#best)\n- [Classification](#class1)\n- [Regression](#regres1)\n\n- [Neural Network vs Best Models](#nn)\n\n7.[Modelling](#model)\n- [Classification](#class2)\n- [Regression](#regres2)\n\n8.[Preparing our prediction and evaluate](#prediction)\n- [Classification](#class3)\n- [Regression](#regres3)\n\n**Reference**:\n\n- [Smart grid](https:\/\/en.wikipedia.org\/wiki\/Smart_grid)\n\n- [UCI Electrical Grid Stability Simulated Data Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Electrical+Grid+Stability+Simulated+Data+)\n\n","4aad2032":"<a id = 'eng'><\/a>","3b42fc24":"# 4. Feature engineering\n\nWe transform (stable, unstable) to (0,1)","b371f414":"<a id ='best'><\/a>","097d6196":"The maximun likelihood gaussian of p1 fits well a histogram distribution. May be due to p1 = p2 + p3 + p4.\nWe are checking below if p1 is really gaussian or normal.","2567023f":"**Decision**\n\nThe best model for classification is ExtraTreesClassifier (congrats)","aa19aa6f":"<a id='attribute'><\/a>","e8cad670":"## 6.1 Classification\n\nwe seek the  best learner.","d2d94fe8":"<a id = 'class1'><\/a>","d84d5df6":"## Neural Network with Keras TensorFlow\n\nWe are making comparison between ExtraTreeClassifier and nn for classification. and between XGBoostRegression for regression\n\n### NN vs ExtraTreeClassifier for classification","2830a466":"<a id='load'><\/a>","eef0f6cf":"**Function tools**","1557b19b":"**Comment** \n\nExtraTreesClassifier, randomForestClassifier and GradientBoostClassifier have a same AP score 99%","9e7ebfc1":"stab observation is good. No outlier detected.","0a1b8b82":"All feature does not present outlier. That means all observation are good.","0431687a":"## 1.1 Attribute information\n\nWe have 11 predictive attributes (tau1,tau2,tau3,tau4,p2,p3,p4,g1,g2,g3,g4), 1 non-predictive (p1), 2 goals (stab, stabf).\n\n1. tau[x]: reaction time of participant (real from the range [0.5,10]s). Tau1 - the value for electricity producer.\n\n2. p[x]: nominal power consumed(negative)\/produced(positive)(real). For consumers from the range [-0.5,-2]s^-2; p1 = abs(p2 + p3 + p4)\n\n3. g[x]: coefficient (gamma) proportional to price elasticity (real from the range [0.05,1]s^-1). g1 - the value for electricity producer.\n\n4.  stab: the maximal real part of the characteristic equation root (if positive - the system is linearly unstable else is linearly stable)(real)\n\n5. stabf: the stability label of the system (categorical: stable\/unstable)","4232b517":"## Classification","46bd26b3":"<a id='model' a><\/a>","3c2d4e72":"<a id = 'prepa'><\/a>","bf61b427":"<a id='regress2' a><\/a>","3dd85376":"## Classification","fde79e7c":"<a id=class2><\/a>","a5171e29":"XGBoostRegression is the best model for regression.","968c24f9":"# 1. Load data and package","e5ec88c1":"<a id = 'regres1'><\/a>","6530e7b0":"**Decision for prediction**\n\n- For classification, we take **neural network or EXtraTreeClassifier** \n\n- For regression we take **XGBoostRegression** ","8e1a3c14":"# 2. Initial exploration\n\nwe describe and visualize a data","394ed5f1":"<a id = 'prediction'><\/a>","0e7c4859":"# 6. The best models\n\nIn this place, we are going to create a class denoted toolGrid. It contains all tools we need for classification and regression.","67bb1e31":"All graph are same because tau[i], i = {1,2,3,4} have same correlation coef. with stab.\n\n**We learn**\n-  two curves are similar if only if there have same correlation coef.","38b92ef4":"### NN vs XGBoostRegression ","a970eaae":"**Comment**\n\nAUC: ExtraTreesClassifier and randomForestClassifier have same score 98%","df222f05":"<a id='class3'><\/a>","43a17a5f":"# 3. Correlation between feature","1551be75":"# 5. Preparing for modelling\n\nWe divide our data by 4 subdata train, test, train_target, test_target for classification and regression.","4ba9bf80":"## 6.2 Regression"}}