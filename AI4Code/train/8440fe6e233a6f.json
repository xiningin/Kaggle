{"cell_type":{"82846c58":"code","5c12d101":"code","6a75b847":"code","874710ec":"code","ebe6771d":"code","642cbd5f":"code","7d3a6010":"code","36c743e9":"code","ba6b6206":"code","de6880f9":"code","d4f0571f":"code","2f85614d":"code","6a5033aa":"code","f82450db":"code","81e8596f":"code","aae13817":"code","d07c0796":"code","b44c9754":"code","be568e85":"code","b4f18a1c":"code","1f394ffa":"code","725a78ae":"code","d506747c":"code","472ace37":"code","8ed523e1":"markdown","a53de573":"markdown","d8506946":"markdown","76c3fb40":"markdown","739d6b3e":"markdown","c32c7a7d":"markdown","5cefd909":"markdown"},"source":{"82846c58":"import numpy as np\n\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt","5c12d101":"dataset = load_iris()\nX = dataset.data\ny = dataset.target\nprint(f'Shape of input : {X.shape}')\nprint(f'Example of input : {X[0,:]}')\nprint(f'Shape of output : {y.shape}')\nprint(f'Example of output : {y[0]}')","6a75b847":"def encode_with_one_hot_encoding(categorical_value):\n    number_of_classes = len(np.unique(categorical_value))\n    return np.eye(number_of_classes)[categorical_value]","874710ec":"Y = encode_with_one_hot_encoding(y)\nprint(f'Shape of output : {Y.shape}')\nprint(f'Example of output : {Y[0]}')","ebe6771d":"def extend_input_with_bias(network_input):\n    bias_extension = np.ones(network_input.shape[1]).reshape(1,-1)\n    network_input = np.vstack([bias_extension, network_input])\n    return network_input","642cbd5f":"support_x = X[:,0].reshape(-1,1)\nprint(support_x)\ntmp_x = extend_input_with_bias(support_x)\nprint(tmp_x)","7d3a6010":"X = X.T\nY = Y.T\nprint(f'Shape of input : {X.shape}')\nprint(f'Example of input : {X[:,0]}')\nprint(f'Shape of output : {Y.shape}')\nprint(f'Example of output : {Y[:,0]}')","36c743e9":"def describe_data(data):\n    return f'number of features is {data.shape[0]} and number of datapoint is {data.shape[1]}'\n\nprint(f'For input {describe_data(X)}.')\nprint(f'For output {describe_data(Y)}.')","ba6b6206":"def create_network(input_size, output_size, hidden_sizes):\n    network = []\n    layer_sizes = hidden_sizes\n    layer_sizes.append(output_size)\n    for neuron_count in layer_sizes:\n        layer = np.random.rand(input_size+1, neuron_count)*2-1\n        input_size = neuron_count\n        network.append(layer)\n    return network\n\ndef describe_layer(layer):\n    return f'there is {layer.shape[1]} neurons with {layer.shape[0]} inputs each'","de6880f9":"network = create_network(X.shape[0], Y.shape[0], [7, 6])\nfor idx, layer in enumerate(network):\n    print(f'In layer {idx} {describe_layer(layer)}')","d4f0571f":"def unipolar_activation(u):\n    return 1\/(1 + np.exp(-u))\n\ndef unipolar_derivative(u):\n    a = unipolar_activation(u)\n    return a * (1.0 - a)\n\ndemo_x = np.linspace(-5, 5, 100)\ndemo_y = unipolar_activation(demo_x)\ndemo_derivative = unipolar_derivative(demo_x)\nplt.plot(demo_x, demo_y, label='Unipolar')\nplt.plot(demo_x, demo_derivative, label='Derivative')\nplt.legend()\nplt.show()","2f85614d":"def feed_forward(network_input, network):\n    layer_input = network_input\n    responses = []\n    \n    for weights in network:\n        layer_input = extend_input_with_bias(layer_input)\n        response = unipolar_activation(weights.T @ layer_input)\n        layer_input = response\n        responses.append(response)\n    \n    return responses\n\ndef predict(network_input, network):\n    return feed_forward(network_input, network)[-1]\n\ndef calculate_mse(predicted, expected):\n    return np.sum((predicted-expected)**2)\/len(predicted)","6a5033aa":"Y_predicted = predict(X, network)\nprint(f'MSE = {calculate_mse(Y_predicted, Y)}')","f82450db":"responses = feed_forward(X, network)\nfor idx, response in enumerate(responses):\n    print(f'For response of {idx} layer {describe_data(response)}')","81e8596f":"print(np.argmax(Y_predicted, axis=0))","aae13817":"def backpropagate(network, responses, expected_output_layer_response):\n    gradients = []\n    error = responses[-1] - expected_output_layer_response\n    for weights, response in zip(reversed(network), reversed(responses)):\n        gradient = error + unipolar_derivative(response)\n        gradients.append(gradient)\n        error = weights @ gradient\n        error = error[1:,:]\n    return list(reversed(gradients))","d07c0796":"gradients = backpropagate(network, responses, Y)\nfor idx, gradient in enumerate(gradients):\n    print(f'For gradient of {idx} layer {describe_data(gradient)}')","b44c9754":"def clculate_weights_changes(network, network_input, network_responses, gradients, learning_factor):\n    layer_inputs = [network_input] + network_responses[:-1]\n    weights_changes = []\n    for weights, layer_input, gradient in zip(network, layer_inputs, gradients):\n        layer_input = extend_input_with_bias(layer_input)\n        change = layer_input.dot(gradient.T)*learning_factor\n        weights_changes.append(change)\n    return weights_changes","be568e85":"changes = clculate_weights_changes(network, X, responses, gradients, 0.3)\nfor idx, change in enumerate(changes):\n    print(f'For change of {idx} layer {describe_layer(change)}')","b4f18a1c":"def adjust_weights(network, changes):\n    new_network = []\n    for weights, change in zip(network, changes):\n        new_weights = weights - change\n        new_network.append(new_weights)\n    return new_network","1f394ffa":"network = adjust_weights(network, changes)\nfor idx, layer in enumerate(network):\n    print(f'In layer {idx} {describe_layer(layer)}')\nY_predicted = predict(X, network)\nprint(f'MSE = {calculate_mse(Y_predicted, Y)}')","725a78ae":"def train_network(network, network_input, expected_output, learning_factor, epochs):\n    mse_history = []\n    for _ in range(epochs):\n        responses = feed_forward(network_input, network)\n        mse_history.append(calculate_mse(responses[-1], expected_output))\n        gradients = backpropagate(network, responses, Y)\n        changes = clculate_weights_changes(network, network_input, responses, gradients, learning_factor)\n        network = adjust_weights(network, changes)\n    mse_history.append(calculate_mse(responses[-1], expected_output))\n    return network, np.asarray(mse_history)","d506747c":"network = create_network(X.shape[0], Y.shape[0], [7, 6])\nnetwork, mse_history = train_network(network, X, Y, 0.3, 10)\nplt.plot(mse_history)\nplt.show()","472ace37":"network = create_network(X.shape[0], Y.shape[0], [7, 6])\nnetwork, mse_history = train_network(network, X, Y, 0.001, 20)\nplt.plot(mse_history)\nplt.show()","8ed523e1":"### Creating neural network and implementing forward propagation","a53de573":"### Implementing backpropagation\nFirst we must compute and backpropagate gradient then we must calculate value by which weights must change.","d8506946":"Let us encode output with one hot encoding","76c3fb40":"# Backpropagation from scratch\nIn this notebook I will show you how to write a backpropagation algorithm from scratch using only numpy.\nOnly additional libraries are\n - sklearn.datasets : for convinient access to simple dataset,\n - matplotlib.pyplot : for visualisation purposes.\n \n Videotutorial created alongside this notebook is available at https:\/\/open.lbry.com\/@regule:9\/backprop-numpy:6?r=FTtRqPTErA36CEGvHu1nU4PJifq1YV2W","739d6b3e":"Now I will transpose input and output so that it is consistent with how I have it in an old book :(.","c32c7a7d":"### Data loading and preparation\nWhe must start with loading data (in this case from prepared library) and preprocessing it so that it will fit network","5cefd909":"We must add a 1 to every input vector to be able to include bias in weights."}}