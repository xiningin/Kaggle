{"cell_type":{"72946a16":"code","e18345df":"code","5e96bfeb":"code","43904b80":"code","25a3f432":"code","44f9dcd8":"code","eda4af5d":"code","f9ffebe7":"code","2846c826":"code","a9fed790":"code","0ea72374":"code","5c9936ee":"code","66f0041d":"code","a300488a":"code","ba6a4995":"code","191eb7ef":"code","1be8f4bd":"code","591c23d9":"code","bafd369d":"code","5268bddc":"code","45e39da8":"code","bb7e7791":"code","3c52d3a2":"code","4a9a5944":"code","735e1a92":"code","4b195019":"code","1fc76073":"code","80747c01":"code","b8f2c705":"code","85dc3947":"code","2fd4ae6a":"code","cdae5ad6":"code","bc61eff2":"code","28366ac7":"code","9950ce43":"code","3a9729cf":"code","c6139ea9":"code","bff6a687":"code","766f8bfe":"code","e3e4a93f":"code","296b7127":"code","7b9c6f31":"code","ffc07c9a":"code","20ee04ad":"code","9727e5db":"code","a0cc645f":"code","9569d112":"code","c6486cc0":"code","83574b12":"code","81c58408":"code","15c0527a":"code","63dc0fd2":"code","84acc94b":"code","834b0ccc":"code","9c441b3b":"code","86e8b957":"code","489aaa33":"code","661c4ad3":"code","14e7cb1e":"code","8164ee13":"code","bdbc1c0a":"code","1831eb21":"code","68a36a2a":"code","a9fb63da":"code","60d546a7":"code","16992cc5":"code","1383897c":"code","25d64189":"code","29f2fd89":"code","0a343bf6":"code","fc823d89":"code","4fe1d74b":"code","c5a2ab7e":"code","8dcba51a":"code","24720421":"code","5b02b1be":"code","2ac2fa68":"code","78f0a6bd":"code","8bcd65e0":"code","6e992b62":"code","496d234e":"code","12a7391d":"code","e5639072":"code","eed03874":"code","1f6f9ba8":"markdown","d5d669ca":"markdown","2f494dc9":"markdown","608181cb":"markdown","c7f4491e":"markdown","bcf6f264":"markdown","0b775f6f":"markdown","31a63127":"markdown","d8f3b386":"markdown","2d8a9554":"markdown","bab1e653":"markdown","cbd0218b":"markdown","b21bf3f3":"markdown","f244bea0":"markdown","5f7e6cd4":"markdown","43196d7a":"markdown","898e0233":"markdown","83163a1e":"markdown","a6b83a35":"markdown","d1b4626f":"markdown","549d6925":"markdown","3bd0e34c":"markdown","6878794c":"markdown","60fd04fa":"markdown","808c9ed5":"markdown","79497611":"markdown","c23fdbba":"markdown","d52f2c8f":"markdown","b9ef2b67":"markdown","86093252":"markdown","ac2d760d":"markdown","53dcde15":"markdown","6459e958":"markdown","5339e63f":"markdown","3e512783":"markdown","90ee61ae":"markdown","f372928c":"markdown","f0ad03ea":"markdown","fa0bd2d3":"markdown","3fee4a9b":"markdown","25be5f19":"markdown","8ded58ef":"markdown","5cd56713":"markdown","8d0deb4a":"markdown","b868d64d":"markdown","17e027a3":"markdown","83344b01":"markdown","e2ee5bd9":"markdown","83124516":"markdown","85ecbef4":"markdown","72c87884":"markdown","254b4db7":"markdown"},"source":{"72946a16":"!pip install --upgrade scikit-learn==1.0 --quiet","e18345df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # viz\nimport matplotlib.pyplot as plt # viz\nfrom scipy import stats\nimport json\nfrom typing import List, Tuple\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, roc_auc_score, precision_recall_fscore_support\nfrom sklearn import metrics, linear_model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings('ignore')","5e96bfeb":"train_df = pd.read_csv('\/kaggle\/input\/beth-dataset\/labelled_training_data.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/beth-dataset\/labelled_testing_data.csv')\nvalidation_df = pd.read_csv('\/kaggle\/input\/beth-dataset\/labelled_validation_data.csv')","43904b80":"assert train_df.columns.all() == test_df.columns.all() == validation_df.columns.all()","25a3f432":"train_df.dtypes","44f9dcd8":"train_df.head()","eda4af5d":"train_df.describe(include=['object', 'float', 'int'])","f9ffebe7":"train_df.evil.value_counts().plot(kind='bar', title='Label Frequency for evil label in Train Dataset')","2846c826":"train_df.sus.value_counts().plot(kind='bar', title='Label Frequency for sus label in Train Dataset')","a9fed790":"test_df.evil.value_counts().plot(kind='bar', title='Label Frequency for evil label in Test Dataset')","0ea72374":"test_df.sus.value_counts().plot(kind='bar', title='Label Frequency for sus label in Test Dataset')","5c9936ee":"validation_df.evil.value_counts().plot(kind='bar', title='Label Frequency for evil label in Validation Dataset')","66f0041d":"validation_df.sus.value_counts().plot(kind='bar', title='Label Frequency for sus label in Validation Dataset')","a300488a":"train_df.groupby(['sus', 'evil'])[['timestamp']].count()","ba6a4995":"train_df.groupby(['sus', 'evil'])[['timestamp']].count().plot(kind='bar')","191eb7ef":"test_df.groupby(['sus', 'evil'])[['timestamp']].count()","1be8f4bd":"test_df.loc[(test_df['sus'] == 1) & (test_df['evil'] == 1)].shape[0]","591c23d9":"test_df.groupby(['sus', 'evil'])[['timestamp']].count().plot(kind='bar')","bafd369d":"validation_df.groupby(['sus', 'evil'])[['timestamp']].count()","5268bddc":"validation_df.groupby(['sus', 'evil'])[['timestamp']].count().plot(kind='bar')","45e39da8":"def dataset_to_corr_heatmap(dataframe, title, ax):\n    corr = dataframe.corr()\n    sns.heatmap(corr, ax = ax, annot=True, cmap=\"YlGnBu\")\n    ax.set_title(f'Correlation Plot for {title}')","bb7e7791":"fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize = (15,20))\nfig.tight_layout(pad=10.0)\ndatasets = [train_df, test_df, validation_df]\ndataset_names = ['train', 'test', 'validation']\naxs = [ax1, ax2, ax3]\n\nfor dataset, name, ax in zip(datasets, dataset_names, axs):\n    dataset_to_corr_heatmap(dataset, name, ax)","3c52d3a2":"datasets = [train_df, test_df, validation_df]\n\nentropy_values = []\nfor dataset in datasets:\n    dataset_entropy_values = []\n    for col in dataset.columns:\n        if col == 'timestamp':\n            pass\n        else:\n            counts = dataset[col].value_counts()\n            col_entropy = stats.entropy(counts)\n            dataset_entropy_values.append(col_entropy)\n            \n    entropy_values.append(dataset_entropy_values)\n\nplt.boxplot(entropy_values)\nplt.title('Boxplot of Entropy Values')\nplt.ylabel(\"entropy values\")\nplt.xticks([0,1,2,3],labels=['','train', 'test', 'validate'])\nplt.show()","4a9a5944":"datasets = [train_df, test_df, validation_df]\n\nvariation_values = []\nfor dataset in datasets:\n    dataset_variation_values = []\n    for col in dataset.columns:\n        if col == 'timestamp':\n            pass\n        else:\n            counts = dataset[col].value_counts()\n            col_variation = stats.variation(counts)\n            dataset_variation_values.append(col_variation)\n            \n    variation_values.append(dataset_variation_values)\n\nplt.boxplot(variation_values)\nplt.title('Boxplot of Variation Values')\nplt.ylabel(\"Variation values\")\nplt.xticks([0,1,2,3],labels=['','train', 'test', 'validate'])\nplt.show()","735e1a92":"train_df.loc[:, ['eventId', 'eventName']].head(10)","4b195019":"train_df.loc[:, ['processName', 'hostName', 'args']].head(10)","1fc76073":"def column_uniques(df, col):\n    print(f'{col} - Uniques:\\n\\n{df[col].unique()} \\n\\nNo. Uniques: {df[col].nunique()}')","80747c01":"column_uniques(train_df, 'processName')","b8f2c705":"column_uniques(train_df, 'hostName')","85dc3947":"column_uniques(train_df, 'args')","2fd4ae6a":"sample = train_df['args'].sample(n=15, random_state=1)\nsample","cdae5ad6":"sample_df = pd.DataFrame(sample)\nsample_df","bc61eff2":"sample_df.iloc[0]","28366ac7":"sample1 = sample_df.iloc[0]\nsample1 = sample1.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", '\"')\nsample1 = sample1[0]\nsample1","9950ce43":"sample1 = json.dumps(sample1)\ntest1 = json.loads(sample1)","3a9729cf":"test1","c6139ea9":"def strip_string(input_str):\n    \"\"\"\n    Takes an input string and replaces specific\n    puncutation marks with nothing\n    \n    Args:\n        input_str: The string to be processed\n    \n    Returns:\n        The processed string\n    \"\"\"\n    assert isinstance(input_str, str)\n    return input_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", '\"')","bff6a687":"sample_df['stripped_args'] = sample_df['args'].apply(strip_string)","766f8bfe":"for i in sample_df['stripped_args']:\n    print(i)\n    print('\\n')","e3e4a93f":"sample_df['args'].iloc[2]","296b7127":"test2 = sample_df['args'].iloc[2]","7b9c6f31":"split_test2 = test2.split('},')\nsplit_test2","ffc07c9a":"strings = [string.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"'\", \"\").replace(\"}\", \"\").lstrip(\" \") for string in split_test2]\nstrings","20ee04ad":"list_of_lists = [item.split(',') for item in strings]\nlist_of_lists","9727e5db":"output = []\nfor lst in list_of_lists:\n    for key_value in lst:\n        key, value = key_value.split(': ', 1)\n        if not output or key in output[-1]:\n            output.append({})\n        output[-1][key] = value","a0cc645f":"output","9569d112":"json_output = json.dumps(output)","c6486cc0":"interim_df = pd.json_normalize(json.loads(json_output))\ninterim_df","83574b12":"interim_df.unstack()","81c58408":"interim_df.unstack().to_frame()","15c0527a":"interim_df.unstack().to_frame().T","63dc0fd2":"interim_df.unstack().to_frame().T.sort_index(1,1)","84acc94b":"final_df = interim_df.unstack().to_frame().T.sort_index(1,1)\nfinal_df.columns = final_df.columns.map('{0[0]}_{0[1]}'.format)\nfinal_df","834b0ccc":"def process_args_row(row):\n    \"\"\"\n    Takes an single value from the 'args' column\n    and returns a processed dataframe row\n    \n    Args:\n        row: A single 'args' value\/row\n        \n    Returns:\n        final_df: The processed dataframe row\n    \"\"\"\n    \n    row = row.split('},')\n    row = [string.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"'\", \"\").replace(\"}\", \"\").lstrip(\" \") for string in row]\n    row = [item.split(',') for item in row]\n    \n    processed_row = []\n    for lst in row:\n        for key_value in lst:\n            key, value = key_value.split(': ', 1)\n            if not processed_row or key in processed_row[-1]:\n                processed_row.append({})\n            processed_row[-1][key] = value\n    \n    json_row = json.dumps(processed_row)\n    row_df = pd.json_normalize(json.loads(json_row))\n    \n    final_df = row_df.unstack().to_frame().T.sort_index(1,1)\n    final_df.columns = final_df.columns.map('{0[0]}_{0[1]}'.format)\n    \n    return final_df","9c441b3b":"data = sample_df['args'].tolist()","86e8b957":"processed_dataframes = []\n\nfor row in data:\n    ret = process_args_row(row)\n    processed_dataframes.append(ret)","489aaa33":"processed = pd.concat(processed_dataframes).reset_index(drop=True)\nprocessed.columns = processed.columns.str.lstrip()\nprocessed","661c4ad3":"sample_df = sample_df.reset_index(drop=True)\nmerged_sample = pd.concat([sample_df, processed], axis=1)\nmerged_sample","14e7cb1e":"# Taken from here - https:\/\/github.com\/jinxmirror13\/BETH_Dataset_Analysis\ntrain_df[\"processId\"] = train_df[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS\/not OS\ntrain_df[\"parentProcessId\"] = train_df[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS\/not OS\ntrain_df[\"userId\"] = train_df[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS\/not OS\ntrain_df[\"mountNamespace\"] = train_df[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt\/ (all non-OS users) \/elsewhere\ntrain_df[\"eventId\"] = train_df[\"eventId\"]  # Keep eventId values (requires knowing max value)\ntrain_df[\"returnValue\"] = train_df[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2))  # Map to success\/success with value\/error","8164ee13":"train_df.head(5)","bdbc1c0a":"train = train_df[[\"processId\", \"parentProcessId\", \"userId\", \"mountNamespace\", \"eventId\", \"argsNum\", \"returnValue\"]]\ntrain_labels = train_df['sus']","1831eb21":"train.head(5)","68a36a2a":"train_labels","a9fb63da":"assert len(train_labels) == train.shape[0]","60d546a7":"def process_args_dataframe(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Processes the `args` column within the dataset\n    \"\"\"\n    \n    processed_dataframes = []\n    data = df[column_name].tolist()\n    \n    # Debug counter\n    counter = 0\n    \n    for row in data:\n        if row == '[]': # If there are no args\n            pass\n        else:\n            try:\n                ret = process_args_row(row)\n                processed_dataframes.append(ret)\n            except:\n                print(f'Error Encounter: Row {counter} - {row}')\n\n            counter+=1\n        \n    processed = pd.concat(processed_dataframes).reset_index(drop=True)\n    processed.columns = processed.columns.str.lstrip()\n    \n    df = pd.concat([df, processed], axis=1)\n    \n    return df\n\ndef prepare_dataset(df: pd.DataFrame, process_args=False) -> pd.DataFrame:\n    \"\"\"\n    Prepare the dataset by completing the standard feature engineering tasks\n    \"\"\"\n    \n    df[\"processId\"] = train_df[\"processId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS\/not OS\n    df[\"parentProcessId\"] = train_df[\"parentProcessId\"].map(lambda x: 0 if x in [0, 1, 2] else 1)  # Map to OS\/not OS\n    df[\"userId\"] = train_df[\"userId\"].map(lambda x: 0 if x < 1000 else 1)  # Map to OS\/not OS\n    df[\"mountNamespace\"] = train_df[\"mountNamespace\"].map(lambda x: 0 if x == 4026531840 else 1)  # Map to mount access to mnt\/ (all non-OS users) \/elsewhere\n    df[\"eventId\"] = train_df[\"eventId\"]  # Keep eventId values (requires knowing max value)\n    df[\"returnValue\"] = train_df[\"returnValue\"].map(lambda x: 0 if x == 0 else (1 if x > 0 else 2))  # Map to success\/success with value\/error\n    \n    if process_args is True:\n        df = process_args_dataframe(df, 'args')\n        \n    features = df[[\"processId\", \"parentProcessId\", \"userId\", \"mountNamespace\", \"eventId\", \"argsNum\", \"returnValue\"]]\n    labels = df['sus']\n        \n    return features, labels","16992cc5":"train_no_args_feats, train_no_args_labels = prepare_dataset(train_df)","1383897c":"train_no_args_feats.head()","25d64189":"train_no_args_labels.head()","29f2fd89":"# Uncomment to see similar output as below\n#train_with_args_feats, train_with_args_labels = prepare_dataset(train_df, process_args=True)","0a343bf6":"train_df_feats, train_df_labels = prepare_dataset(train_df)\ntest_df_feats, test_df_labels = prepare_dataset(test_df)\nval_df_feats, val_df_labels = prepare_dataset(validation_df)","fc823d89":"def metric_printer(y_true, y_pred):\n    \n    y_true[y_true == 1] = -1\n    y_true[y_true == 0] = 1\n    \n    metric_tuple = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", pos_label = -1)\n    print(f'Precision:\\t{metric_tuple[0]}')\n    print(f'Recall:\\t\\t{metric_tuple[1]:.3f}')\n    print(f'F1-Score:\\t{metric_tuple[2]:.3f}')\n\ndef output_roc_plot(y, pred):\n    fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n    roc_auc = metrics.auc(fpr, tpr)\n    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Isolation Forest')\n    display.plot()\n    plt.show()","4fe1d74b":"clf = IsolationForest(contamination=0.1, random_state=0).fit(train_df_feats)","c5a2ab7e":"y_pred= clf.predict(val_df_feats)\ny_probas = clf.score_samples(val_df_feats)\nmetric_printer(val_df_labels, y_pred)","8dcba51a":"y_pred= clf.predict(test_df_feats)\ny_probas = clf.score_samples(test_df_feats)\nmetric_printer(test_df_labels, y_pred)","24720421":"train_non_outliers = train_df_feats[train_df_labels==0]\nclf = linear_model.SGDOneClassSVM(random_state=0).fit(train_non_outliers)","5b02b1be":"y_preds = clf.predict(val_df_feats)\nmetric_printer(val_df_labels, y_preds)","2ac2fa68":"y_preds = clf.predict(test_df_feats)\nmetric_printer(test_df_labels, y_preds)","78f0a6bd":"train_data = pd.DataFrame(train_df[[\"processId\", \"parentProcessId\", \"userId\", \"mountNamespace\", \"eventId\", \"argsNum\", \"returnValue\"]])\ntrain_labels = pd.DataFrame(train_df[[\"sus\"]])","8bcd65e0":"data_tensor = torch.as_tensor(train_data.values, dtype=torch.int32)\nlabel_tensor = torch.as_tensor(train_labels.values, dtype=torch.int32)","6e992b62":"train_dataset = TensorDataset(data_tensor, label_tensor)","496d234e":"train_data_ldr = DataLoader(train_dataset, batch_size=64, shuffle=True)","12a7391d":"for i, (x, y) in enumerate(train_data_ldr):\n    if i == 1:\n        break\n    else:\n        print(f'Index: {i} \\n Data Tensor: {x} \\n Label Tensor: {y}')","e5639072":"def prepare_tensor_dataset(df: pd.DataFrame, feature_cols: List, label_col: str) -> Tuple[TensorDataset, DataLoader]:\n    \"\"\"\n    Converts an inpurt Pandas DataFrame to a Tensor Dataset and Data Loader.\n    \"\"\"\n    if all([col in df.columns for item in feature_cols]) and label_col in df.columns:\n        \n        labels = pd.DataFrame(df[[label_col]])\n        features = pd.DataFrame(df[feature_cols])\n\n        data_tensor = torch.as_tensor(features.values, dtype=torch.int32)\n        label_tensor = torch.as_tensor(train_labels.values, dtype=torch.int32)\n        \n        tensor_dataset = TensorDataset(data_tensor, label_tensor)\n        \n        data_ldr = train_data_ldr = DataLoader(train_dataset, batch_size=64, shuffle=True)\n        \n        return tensor_dataset, data_ldr\n    else:\n        raise ValueError('Unable to find all columns')\n    ","eed03874":"label_col = 'sus'\nfeat_cols = [\"processId\", \"parentProcessId\", \"userId\", \"mountNamespace\", \"eventId\", \"argsNum\", \"returnValue\"]\n\ntrain_tensor_dataset, train_data_ldr = prepare_tensor_dataset(train_df, feat_cols, label_col)","1f6f9ba8":"### Stage 2: Convert from Pandas Dataframe to Torch Tensor\n\nThis is done by using `.values` which returns a numpy array version of the pandas dataframe and then conversion into a PyTorch Tensor.\n\n*Note: The dtype here is really important with other datasets. In this case, we have no float values so the choice of a 32bit integer for the dtype is OK. Typically, this is not the case and using a dtype of int32 would probably cause the data to be changed and precision lowered*","d5d669ca":"### Stage 3: Combine both `data_tensor` and `label_tensor` together to make a PyTorch `TensorDataset`","2f494dc9":"Looks like we have a pretty big problem here. Most of the records within our sample are actually multiple dictionaries\/dictonary like objects with the same keys. It might be worth changing tactics and seeing if what we can do with the original field.\n\n## Lets take a more complicated example\n\nThe 3rd row in the sample is a bit more complicated. We have four sets of values which have the same keys (`name`, `type` and `value`) and a good variation of values. ","608181cb":"## Check Unique Values for all three of the fields above","c7f4491e":"This is starting to look promising! The next stage is to break each of element in each list into it's `key:value` pairs and then turn into an actual dictionary. \n\n### Stage Four: Breaking each element into key value pairs and convert to dictionary","bcf6f264":"## Check all of the columns are the same across all three dataframes","0b775f6f":"### Stage 4: Create a PyTorch `DataLoader` using the `TensorDataset`\n*Note: The batch size here has been chosen pretty arbitaryily.*","31a63127":"### Stage Two: Clean up the string by replacing punctuation and stripping blank space\n\n*Note: I have taken a similar approach to the string cleaning as the example above. The key difference here is that I am using a list comprehension to process all of the strings at once rather than just individual strings*","d8f3b386":"Looks like we may have cracked it!\n\n## Merge above back into Sample","2d8a9554":"This looks like a potential option if we can get each of the fields into a JSON compatible state. Let's see how it works on the rest of the sample","bab1e653":"## Are any events labelled both`sus` and `evil` in each dataset?\n\n### `train` dataset","cbd0218b":"We are so close now. The last thing to do now is to sot of the column names. For this we can use `map` as shown below\n\n### Stage 7: Pulling it all together and Tidy up Column Names","b21bf3f3":"# Initial Explore\n## Load Data","f244bea0":"### Stage One: Isolate each individual dictionary-like object","5f7e6cd4":"*Note: The snippet below is to double-check the results from the cell above.*","43196d7a":"### Variation","898e0233":"### Stage 5: Make sure the `DataLoader` works properly","83163a1e":"This column looks very useful but also very messy. Let's create a small sub-sample and investigate futher\n\n# Down the Rabbit Hole - `args` column\n\nLet's create a small sample of 15 random rows","a6b83a35":"### Stage 5: Convert the List of Dictionaries into a Pandas Dataframe using json_normalize\n\nThis stage has two steps - The first is to dump the list of dictionaries to a JSON object and then use `pd.json_normalize` and `json.loads` to load it into a DataFrame.","d1b4626f":"## Prepare the train dataset with processing `args`","549d6925":"## Exploring the Non-Numeric Cols\n\nLet's use the `train_df` for this exploration\n\nFrom looking at the annex of the paper related to this dataset, the `eventName` columns maps directly to the `eventId` column as shown in the cell below, becuase of this, it'll be ignored in the analysis. (Look at line 2 (idx 1) and line 8 (idx 7))","3bd0e34c":"**NOTE:** From looking at these plots, it looks fairly imbalanced across the board but this is expected with a dataset like this. From reading the paper, it's kinda' the whole point! The test dataset is the only dataset that contains `evil` labelled events. This means that anomoly detection approaches will probably the best place to start. Something like an Auto-Encoder or One-Class SVM.","6878794c":"## What do these models show us?\n\nIrrespective of the actual scores outputted, it looks like the validation set is significantly different from the train and test sets. This will make it a bit wierd for model development. Typically, you want to ensure that both the test and validation datasets are drawn from the same distrubition or you will be tweaking hyper-parameters and making model choices using the validation set which have no chance of actually performing as expected on the test set.\n\n# Let's get ready to train some Deep Learning Approaches\n\n## Tensor Dataloader\n\nBefore we can get to training some deep learning approaches, the first thing we need to do is get our data into a PyTorch DataLoader. The below code is not best practice but is a quick\/easy way of getting the data into an PyTorch Dataloader that can be iterated over.\n\n### Stage 1: Subset the data into two dataframes - One for features and one for labels","60fd04fa":"We are getting there. As you can see from the above output, we now have each of the dictionary-like objects cleaned and each one is an item within a list minus all of the extra punctuation. The next stage is to break up each string to give us `key:value` pairs. For example, for the first string in the list above `name: dirfd, type: int, value: -100`, we want to get `['name: dirfd', 'type: int', 'value: -100']`\n\n### Stage Three: Split each string into it's `key:value` pairs","808c9ed5":"This looks exactly how we want it. All we need to do now is create a function that pull all of the stages above together and see how it shapes up on the whole dataset (very slow I bet)\n\n# `args` Processing Function + Test on Sample","79497611":"# Prepare Train, Test and Validate Datasets without processing `args`\n\nAs the dataset is pre-split for us, each dataset just needs to be processed using the helper functions above.","c23fdbba":"This column contains the process names and could be processed further to create binary features. For example, a feature called `amazon or not` could be one or `systemd or not`. Something to explore later.","d52f2c8f":"### `test` Dataset","b9ef2b67":"This columns looks fairly useless for model training. As the dataset authors mention in the paper, this field is probs only useful for linking activity together in the dataset.","86093252":"Looks like we now have a good-to-go dataset with the extra fields added. Let's create some helper functions to prepare the datasets and let's start model training.","ac2d760d":"Let's test out the function above. First, we will convert the `sample_df['args']` column to a list so we can iterate through it easily. We will then loop over each item in the list and add each processed dataframe to another list called `processed_dataframes`. Once complete, we will then concatanate the dataframes together to see what the output looks like!","53dcde15":"### `validation` dataset","6459e958":"#### Step 3: Transpose the DataFrame (Flip it so it's horizontal instead of vertical)","5339e63f":"## One Class Support Vector Machine\n\nDue to the scale of this dataset, a Stohcastic Gradient Descent (SGD) version of One Class SVM is used. This is a fairly new addition to Sklearn and speeds up the training on bigger datasets by MILES.","3e512783":"### Stage 6: Pulling it all together into a helper function","90ee61ae":"## What does this show?\n\n* All three of the datasets have a heavy correlation between `userid` and the associated labels which feature in the dataset (`sus` and\/or `evil`).\n* `processid` and `threadid` are highly correlated and seem to have similar correlation vaules across all three datasets. This means that they are representing pretty much the same thing and one of them could be dropped.\n* The correlation plots all look significantly different. This probably means its a hard problem!","f372928c":"## Prepare the train dataset without processing `args`","f0ad03ea":"## Experimenting with ways to compare the datasets\n\n### Entropy","fa0bd2d3":"From visual inspection of the output of `.head()` and `.describe()`, there are several interesting things can be spotted.\n\n* The `timestamp` field is a bit funky and does not conform to the typical unix or ISO formats. From reading the paper, this is actually the time since last reboot. This fact makes the values for`std`, `min`, `25%`, `50%`, `75%` and `max` make much more sense.\n* The `mountNamepsace` field looks like the vast majority of the values are the same and could warrant further investigation to see if it could be removed.\n* The `processName` field has 36 unique values but the vast majority of them (approximately 60%) are related to the `ps` process.\n* The `hostName` field has 8 unique values with a smaller, but still significant amount of them being`ubuntu`. This probably warrants a bit of a deeper look to see what this means for the other `hostname` values and if the `ps` process events are linked to this in anyway.\n* The `eventName` field has 32 unique values with again a significant amount of them being `close`. This too probably warrants a bit more investigation to see what the distrubiton is across different values.\n* The `stackAddresses` field is typically blank. The value of these values is not initial clear but could be used to determine stack size maybe.\n* The `argsNum` and `args` fields seem coupled but it's currently unclear what these actually mean. The `args` field in particular seems to typically contain a `list` of values which could be extracted.\n* The `sus` and `evil` fields are essentially the labels.\n    >Logs marked suspicious indicate\nunusual activity or outliers in the data distribution, such as\nan external userId with a systemd process3\n, infrequent\ndaemon process calls (e.g., \u201cacpid\u201d or \u201caccounts-daemon\u201d),\nor calls to close processes that we did not observe as being\nstarted. Evil\nindicates a malicious external presence not\ninherent to the system, such as a bash execution call to list\nthe computer\u2019s memory information, remove other users\u2019\nssh access, or un-tar an added file.\n\n# EDA\n## What are the `evil` and `sus` counts across the `train`, `test` and `validation`?\n\n### `train` Dataset","3fee4a9b":"As we can see from this sample, there seems to be a pattern here. All of the values start with `[{` and end with `}]` which suggests it could be a dictionary within a list or maybe even json. Let's try to process a row and see if we can load it as json.","25be5f19":"#### Step 4: Sort the Indexes so each set of values is next to each other","8ded58ef":"#### Step 2: Turn into a DataFrame","5cd56713":"# BETH Dataset EDA, Preperation and Model Training\n\n>The BETH dataset currently represents 8,004,918 events collected over 23 honeypots, running for about five noncontiguous hours on a major cloud provider. For benchmarking and discussion, we selected the initial subset of the process logs. This subset was further divided into training, validation, and testing sets with a rough 60\/20\/20 split based on host, quantity of logs generated, and the activity logged\u2014only the test set includes an attack\n>\n>The dataset is composed of two sensor logs: kernel-level process calls and network traffic. The initial benchmark subset only includes process logs. Each process call consists of 14 raw features and 2 hand-crafted labels.\n\nThis notebook is focused solely on the the process call data.","8d0deb4a":"### `test` dataset","b868d64d":"It looks like the `args` processing fuction above does not properly account for all of the different possible fields within the `args` field and fails to process certain rows when the key `value` has a value of which is a list (as shown in the snippet below). If you want to see this output yourself, uncomment the function above. I might revisit the processing once I have trained some models but at least the above processing function get's us at least half the way there. \n```\nError Encounter: Row 16 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 51 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 86 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 124 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 160 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 196 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\nError Encounter: Row 230 - [{'name': 'pathname', 'type': 'const char*', 'value': '\/bin\/run-parts'}, {'name': 'argv', 'type': 'const char*const*', 'value': ['run-parts', '--report', '\/etc\/cron.hourly']}]\n---- snip ---\n```","17e027a3":"#  Let's Train some models\n\n## Helper Functions","83344b01":"This is starting to look much better but we still have a major problem. Each row within the dataset contains the `args` field and as we can see from the output above, we have generated 4 rows worth of data for just one row. The next stage is to use some pandas magic to turn this dataframe into a single row.\n\n### Stage 6: Convert `iterim_df` into a single row\n\n*Note: The 2\/3 line version of this can be found in a couple of cells time, I have broken it out to make it eaiser to understand how each steps works for folks that are unfamiliar with this sort of processing*\n\n#### Step 1: Unstack\n\nI like to think of unstack as like a whole dataframe groupby where you are grouping stuff by its column.","e2ee5bd9":"## Isolation Forest","83124516":"## What is the correlation of features across each dataset?","85ecbef4":"### `validation` dataset","72c87884":"## Visual Inspection \/ Summary","254b4db7":"# Process Training Dataset \n\nFrom reading the paper's annex and the code from the linked github, there are several easy processing steps that we can take. The below code has been directly copied from the github repo."}}