{"cell_type":{"83362a3c":"code","ef787063":"code","96515386":"code","0d50dc2f":"code","4e5c5406":"code","2633b81d":"code","57903d27":"code","b3073931":"code","0702cb8b":"code","b59ea936":"code","1c03c87b":"code","18067ea1":"code","55788c43":"code","3d957643":"code","a84a9510":"code","3cefb0e9":"code","41eb9bb1":"code","3512439b":"code","a9b8df84":"code","142de4db":"code","1355c562":"code","48e280b0":"code","2d775682":"code","610c031e":"code","03d5b7c6":"code","ce18d69b":"code","34c8eb7a":"code","26953c94":"code","a23601ec":"code","bd87b5a9":"code","4946b079":"code","aaaf4580":"code","c8b86de3":"code","5f6355fd":"code","26f02211":"code","a97ab8ab":"code","89ee0763":"code","863a7b94":"code","6ed3b591":"code","502b3439":"code","fa7c487c":"code","0c85ab72":"code","58c8f4f9":"code","3dd1782a":"code","e371d086":"code","5c35f8db":"code","729e1448":"code","d7b17d38":"code","dc1d88c9":"code","2ea97802":"code","42beacef":"code","e35329e0":"code","e9046aad":"code","16ed5445":"code","1677bd57":"markdown","45c06e4a":"markdown","372065e4":"markdown","7832cf05":"markdown","58aab396":"markdown","4c198b0f":"markdown","881dfd71":"markdown","de68956c":"markdown","a3681c35":"markdown","a98640b4":"markdown","cbabaf38":"markdown","364e93ce":"markdown","876a3630":"markdown","915fe16a":"markdown","767fb06c":"markdown","9f74d0fa":"markdown","40f0a0e3":"markdown","fbb5c3be":"markdown","d6b60667":"markdown","a4b36315":"markdown","ca028cab":"markdown","e1745e04":"markdown","2b55c9c9":"markdown","2ea23625":"markdown","51dc2d55":"markdown","362d698b":"markdown","97b3e194":"markdown","bd870fdf":"markdown","871a992e":"markdown","7a0a7f4a":"markdown","ff8ae708":"markdown","5fa5e5ad":"markdown","6ce50b30":"markdown","48499122":"markdown","41726d1f":"markdown","99ced89d":"markdown","790ef01a":"markdown","fb9d5192":"markdown","1367ce34":"markdown","81d9bcb9":"markdown","4c713e47":"markdown","a6f87993":"markdown","a83e6fc3":"markdown","4df6ac06":"markdown","8d2dca6d":"markdown"},"source":{"83362a3c":"pip install pmdarima","ef787063":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_rows\", 20)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import clear_output","96515386":"train = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\")\nfeatures = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\nsample = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\nstores  = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')","0d50dc2f":"print(test.Date.nunique())","4e5c5406":"train.info()","2633b81d":"features.info()","57903d27":"stores.info()","b3073931":"## Creating calendar function\ndef cal (start, end):\n    df_cal= pd.DataFrame({\"date\": pd.date_range(start, end)})\n    df_cal[\"year\"] = df_cal.date.dt.year\n    df_cal[\"month\"]= df_cal.date.dt.month\n    df_cal['year_month'] = pd.to_datetime(df_cal['date']).dt.to_period('M')\n    df_cal[\"week_num\"]= df_cal.date.dt.week\n    #df_cal[\"week_day\"]= df_cal.date.dt.weekday\n    #df_cal[\"week_day_name\"]= df_cal.date.dt.weekday_name\n    #df_cal[\"day_of_year\"]= df_cal.date.dt.dayofyear\n    #df_cal['date'] = df_cal['date'].dt.strftime('%Y-%m-%d')\n    return df_cal","0702cb8b":"pd.to_datetime(train.Date[0], format = '%Y-%m-%d').strftime('%A')","b59ea936":"# Assigning frequency \ntrain.Date = pd.to_datetime(train.Date, format = '%Y-%m-%d')\ntrain.Date.freq = 'W-FRI'","1c03c87b":"# Genearint calandar by use of calender function with start and end date\n# of our train set\ntrain_d = train.Date.unique()\ntrain_d.sort(axis = 0)\ntrain_cal = cal(train_d[0], train_d[-1])\n\n# merge calendar to train set\ntrain_d = pd.merge(train, train_cal, left_on = 'Date', right_on = 'date', how = 'left')\ntrain_d = train_d.drop('date', axis = 1)\n#train_d.head(1)\n\n# Assigning frequency to features set\nfeatures.Date = pd.to_datetime(features.Date, format = '%Y-%m-%d')\nfeatures.Date.freq = 'W-FRI'\n\ntfd = pd.merge (train_d[[i for i in train_d.columns if i not in ['IsHoliday']]], features, left_on = ['Date', 'Store'], right_on = ['Date', 'Store'], how = 'left')\ntfd = pd.merge(tfd,stores, how = 'left',  left_on= 'Store', right_on = 'Store')","18067ea1":"tfd.info()","55788c43":"ov_sales = tfd.groupby('Date').Weekly_Sales.sum()\nov_sales.plot(figsize = (20,3))\nplt.title ('Walmart Weekly_Sales Overall')\nplt.grid(axis = 'y', which = 'both');","3d957643":"from statsmodels.tsa.seasonal import seasonal_decompose as sd\ny = tfd.groupby('Date').Weekly_Sales.sum()\ny.index.freg = 'M-FRI'\ndec = sd(y, model='additive')\nf,ax = plt.subplots(4, figsize =(20,8))\nax[0].plot(dec.observed)\nax[1].plot(dec.trend)\nax[2].plot(dec.seasonal)\nax[3].plot(dec.resid)","a84a9510":"fig = plt.figure(figsize = (20,15))\nfig.suptitle('Walmart Weekly_Sales per every store\\n(Name is the store No) ',y = 0.93,  fontsize = 16)\nfor s in list(range(1,46)):\n    df_un0=tfd[tfd.Store == s].groupby('Date').Weekly_Sales.sum()\n    ax = fig.add_subplot(9,5,s)\n    #ax.plot(df_un0[:-len_pred])\n    ax.plot(df_un0)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_title(s)\nplt.show()","3cefb0e9":"store_num = 1\ns1 = tfd[tfd.Store == store_num][['Date', 'Dept', 'Weekly_Sales']]\nfig = plt.figure(figsize = (20,30))\nfig.suptitle(f'Walmart Weekly_Sales per {len(s1.Dept.unique())} departments in store No {store_num}\\n(Name is department No) ',y = 0.93,  fontsize = 16)\nfor dep, num_in_plot in  zip(s1.Dept.unique(), list(range(1,len(s1.Dept.unique())))):\n    df_un0=s1[s1.Dept == dep]\n    df_un0.set_index('Date')\n    df_un0 = df_un0[['Weekly_Sales']]\n    ax = fig.add_subplot(16,5,num_in_plot)\n    #ax.plot(df_un0[:-len_pred])\n    ax.plot(df_un0)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_title(dep)\nplt.show()","41eb9bb1":"# encode the 'Isholiday' feature\ntfd['hol_num'] = tfd['IsHoliday'].apply(lambda x: 0 if x is False else 1 if x is True else x)\ntfd['Type_num'] = tfd.Type.astype('category').cat.codes","3512439b":"tfd.info()","a9b8df84":"import seaborn as sns\ntfd_corr = tfd.corr()\nmask = np.triu(np.ones_like(tfd_corr, dtype=np.bool))\n\nplt.subplots(figsize=(20, 11))\n#cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(tfd_corr, mask=mask,vmax=.3, center=0, square=True,cmap=\"Reds\",annot = True,fmt='.2f', linewidths=.6, cbar_kws={\"shrink\": .5});","142de4db":"# Weekly_Sales vs Store\nfig= plt.figure(figsize = (20,3))\ndf_g = tfd.groupby('Store')['Weekly_Sales'].mean().reset_index()\nax = sns.barplot(y = df_g.Weekly_Sales, x = df_g.Store)\nplt.title('Weekly_Sales (MEAN) vs Store');","1355c562":"# Weekly_Sales vs Department\nfig= plt.figure(figsize = (20,3))\ndf_g = tfd.groupby('Dept')['Weekly_Sales'].mean().reset_index()\nax = sns.barplot(y = df_g.Weekly_Sales, x = df_g.Dept)\nplt.title('Weekly_Sales (MEAN) vs Department');","48e280b0":"# Weekly_Sales vs Type\nfig= plt.figure(figsize = (10,3))\ndf_g = tfd.groupby('Type')['Weekly_Sales'].mean().reset_index()\nax = sns.barplot(y = df_g.Weekly_Sales, x = df_g.Type)\nplt.title('Weekly_Sales (MEAN) vs Type');","2d775682":"fig= plt.figure(figsize = (5,3))\ndf_g = tfd.groupby('IsHoliday')['Weekly_Sales'].mean().reset_index()\nax = sns.barplot(y = df_g.Weekly_Sales, x = df_g.IsHoliday)\nplt.title('Weekly_Sales (MEAN) vs IsHoliday');","610c031e":"# Weekly_Sales vs Size\nfig= plt.figure(figsize = (20,3))\ndf_g = tfd.groupby('Size')['Weekly_Sales'].mean().reset_index()\nax = sns.barplot(y = df_g.Weekly_Sales, x = df_g.Size)\nplt.xticks (rotation = 45)\nplt.title('Weekly_Sales (MEAN) vs Store Size');","03d5b7c6":"fig, ax= plt.subplots(1,2, figsize = (20,5))\nsns.scatterplot(y = tfd.Weekly_Sales, x = tfd.CPI, ax = ax[0])\nsns.scatterplot(y = tfd.Weekly_Sales, x = tfd.Unemployment, ax = ax[1]);\n\nprint('\\n\\n Weekly Sales Vs Cpi & Unemployment rate\\n')","ce18d69b":"df_week = tfd[tfd.year<2012].groupby(['year','week_num'])['Weekly_Sales'].mean().reset_index()\nax = sns.catplot(data = df_week, y = 'Weekly_Sales', x = 'week_num', hue = 'year', kind = 'bar', height=5, aspect = 3)\nplt.xticks (rotation = 45)\nplt.ylim(12500)\nplt.title('Weekly_Sales (MEAN) vs Week number');","34c8eb7a":"df_month = tfd[tfd.year<2012].groupby(['year','month'])['Weekly_Sales'].mean().reset_index()\nax = sns.catplot(data = df_month, y = 'Weekly_Sales', x = 'month', hue = 'year', kind = 'bar', height=4, aspect = 1.5)\nplt.xticks (rotation = 45)\nplt.ylim(12500)\nplt.title('Weekly_Sales (MEAN) vs Month');","26953c94":"fig, ax = plt.subplots(2,3, figsize = (20,7))\nfig.suptitle('Walmart Weekly_Sales vs MarkDowns',y = 0.93,  fontsize = 16)\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.MarkDown1, ax = ax[0,0])\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.MarkDown2, ax = ax[0,1])\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.MarkDown3, ax = ax[0,2])\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.MarkDown4, ax = ax[1,0])\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.MarkDown5, ax = ax[1,1])\nplt.show()","a23601ec":"fig, ax = plt.subplots(ncols = 2, figsize = (20,3))\nfig.suptitle('Walmart Weekly_Sales vs Temperature and Fuel_price',y = 0.99,  fontsize = 16)\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.Temperature, ax = ax[0])\nsns.scatterplot(y = tfd.Weekly_Sales, x =tfd.Fuel_Price, ax = ax[1]);","bd87b5a9":"tfd_fil =tfd.drop(['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5', \n          'Unemployment', 'CPI', 'IsHoliday', 'Type', 'Temperature', 'Fuel_Price','year_month'], axis = 1)\ntfd_fil = tfd_fil[['Date', 'Store', 'Dept','year','month','week_num','hol_num','Size','Type_num','Weekly_Sales']]\n#final version of our train dataset\ntfd_fil.head(5)","4946b079":"print(test.Date.nunique())","aaaf4580":"from sklearn.model_selection import train_test_split\n\nthreshold_date = pd.to_datetime(tfd_fil.Date.unique()[-test.Date.nunique()]).strftime('%Y-%m-%d')\n\nall_train = tfd_fil[tfd_fil.Date<threshold_date]\nall_test = tfd_fil[tfd_fil.Date>=threshold_date]\n\nX_train = all_train.drop(['Date','Weekly_Sales'], axis = 1)\nX_test = all_test.drop(['Date','Weekly_Sales'], axis = 1)\n\ny_train = all_train['Weekly_Sales']\ny_test = all_test['Weekly_Sales']\n\n\n#Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train_sc = mms.transform(X_train)\nX_test_sc = mms.transform(X_test)\n","c8b86de3":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import  r2_score\n\nclf = RandomForestRegressor(n_estimators=100, max_samples = 0.4)\nclf.fit(X_train_sc, y_train)\nclf_pred = clf.predict(X_test_sc)\n\nprint('Results with NO differentiation')\nprint('Weekly_Sales_Mean_test:' ,round(np.mean(y_test) ))\nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, clf_pred))))\nprint('R2 Score:', r2_score(y_test, clf_pred))","5f6355fd":"rf_pr = pd.DataFrame({'Weekly_Sales':y_test.values, 'Weekly_Sales_rf_pred': clf_pred}, index = y_test.index)\nrf_pred_df = pd.merge(rf_pr, tfd_fil[['Date', 'Store', 'Dept']], left_index = True, right_index = True, how = 'left')","26f02211":"rf_pred_df.head(2)","a97ab8ab":"pred_g = rf_pred_df.groupby('Date')['Weekly_Sales','Weekly_Sales_rf_pred'].sum()\nact_g = tfd_fil.groupby('Date')['Weekly_Sales'].sum()\n\nax = act_g[act_g.index>'2011-01-01'].plot(figsize = (20,5), legend = True)\npred_g['Weekly_Sales_rf_pred'].plot(legend = True)\n\n\nthreshold_date = tfd_fil[tfd_fil.Date>'2012-01-01']['Date'].iloc[0].strftime('%Y-%m-%d')\nend_date =tfd_fil[tfd_fil.Date>'2012-01-01']['Date'].iloc[-1].strftime('%Y-%m-%d')\n\nplt.title(f'Actual Weekly_Sales VS Predictions\\n between {threshold_date} and {end_date}');","89ee0763":"#tfd_fil_pred_start\n#tfd_fil\nst = 12\ns1 = tfd_fil[tfd_fil.Store == st]\nfig = plt.figure(figsize = (22,35))\nfig.suptitle(f'Walmart Weekly_Sales VS Predictions per {len(s1.Dept.unique())} departments\\nfor the {st} store\\n(Name is department No) ',y = 0.93,  fontsize = 16)\n\nfor dep, num_in_plot in  zip(s1.Dept.unique(), list(range(1,len(s1.Dept.unique())))):\n    \n    #actual\n    act =tfd_fil[(tfd_fil.Store ==st)&(tfd_fil.Dept == dep)]\n    act.set_index('Date')\n    \n    #pred \n    \n    pred = rf_pred_df[(rf_pred_df.Store ==st )&(rf_pred_df.Dept == dep)]\n    pred.set_index('Date')\n    \n    ax = fig.add_subplot(19,4,num_in_plot)\n    #ax.plot(df_un0[:-len_pred])\n    \n    ax.plot(act['Weekly_Sales'])\n    ax.plot (pred['Weekly_Sales_rf_pred'])\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_title(dep)\n    #handles, labels = ax.get_legend_handles_labels()\nfig.legend(labels = ('Actual','RF_predicted'), loc=9) #bbox_to_anchor=(1, 0.5))#, bbox_to_anchor=(0.5, 0., 0.5, 0.5))\nplt.show()","863a7b94":"print('img source: https:\/\/www.youtube.com\/watch?v=UNmqTiOnRfg')","6ed3b591":"df_rnn = tfd_fil[['Date', 'Store', 'Dept', 'Weekly_Sales']]\ndf_rnn['Date'] = pd.to_datetime(df_rnn['Date'], format = '%Y-%m-%d')\ndf_rnn['Date'].freq = 'W-FRI'\ndf_rnn['Store_Dept_code'] = df_rnn.apply(lambda x: str(x['Store']) + '-' + str(x['Dept']), axis = 1)\ndf_rnn.head(2)","502b3439":"df_rnn_piv = df_rnn.pivot_table(index = 'Date', columns = 'Store_Dept_code', values = 'Weekly_Sales')\ndf_rnn_piv =df_rnn_piv.fillna(0)\ndf_rnn_piv.head(2)","fa7c487c":"threshold_date = pd.to_datetime(tfd_fil.Date.unique()[-test.Date.nunique()]).strftime('%Y-%m-%d')\n#threshold_date = df_rnn[df_rnn.Date>'2012-01-01']['Date'].iloc[0].strftime('%Y-%m-%d')\nthreshold_date","0c85ab72":"train_rnn = df_rnn_piv[df_rnn_piv.index<threshold_date]\ntest_rnn = df_rnn_piv[df_rnn_piv.index>=threshold_date]\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit(train_rnn)\nX_train_sc = mms.transform(train_rnn)\nX_test_sc = mms.transform(test_rnn)","58c8f4f9":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nlength  = 48\nbatch_size = 10 #Number of timeseries samples in each batch\ngenerator = TimeseriesGenerator(X_train_sc, X_train_sc, length=length, batch_size=batch_size)","3dd1782a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\nfrom keras import layers\n\n# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(LSTM(64,activation= 'tanh',return_sequences=True, input_shape=(length,X_train_sc.shape[1])))\nmodel.add(LSTM(128, activation='tanh', return_sequences = False))\nmodel.add(Dense(X_train_sc.shape[1])) \n\nmodel.compile(optimizer='adam', loss='mse')","e371d086":"validation_generator = TimeseriesGenerator(X_test_sc,X_test_sc, length=38, batch_size=batch_size)\n\nmodel.fit(generator,epochs=10,validation_data=validation_generator, verbose=2)\n\nlosses = pd.DataFrame(model.history.history)\nlosses.plot()","5c35f8db":"n_features = X_train_sc.shape[1]\ntest_predictions = []\n\nfirst_eval_batch = X_train_sc[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test_rnn)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n    clear_output()\n    print(len(test_predictions))","729e1448":"inv_test_pred = mms.inverse_transform(test_predictions)\npred_df = pd.DataFrame (data = inv_test_pred, columns =test_rnn.columns, index = test_rnn.index )","d7b17d38":"n = 4\ntest_rnn.iloc[:,n].plot()\npred_df.iloc[:,n].plot()","dc1d88c9":"print('No of target features: ', len(df_rnn_piv.columns))","2ea97802":"import timeit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom tensorflow.keras.layers import Dense,LSTM\nfrom keras import layers\n\n#Main Timer\nstart_time_main = timeit.default_timer()\n\n#Creating main prediction df\n    # identifying the test index once\ntest_index = df_rnn_piv[df_rnn_piv.index>=threshold_date].index\nmain_rnn_pred_df = pd.DataFrame(index = test_index)\n\n#threshold date\nthreshold_date = pd.to_datetime(tfd_fil.Date.unique()[-test.Date.nunique()]).strftime('%Y-%m-%d')\nthreshold_date\n\n#Sampling\nsample_size = 150\nwhole_range = df_rnn_piv.shape[1]\n\nsamp = 0\nend = sample_size\nstart = 0\n\nif (round(whole_range\/sample_size)*sample_size)<whole_range:\n    extra = 1\nelse:\n    extra = 0\n    \nsamples = round(whole_range\/sample_size)+extra\nwhile samp<samples:\n    start_time = timeit.default_timer()\n    print(f\"Working on Sample: [{start}:{end}]\")\n    #print('start:',start,'end:',end )\n \n    # Train Test split\n    train_rnn = df_rnn_piv[df_rnn_piv.index<threshold_date].iloc[:,start:end]\n    test_rnn = df_rnn_piv[df_rnn_piv.index>=threshold_date].iloc[:,start:end]\n    \n    # Sampling for next round\n    samp = samp+1\n    start= start+sample_size\n    end = end+sample_size\n\n\n    # Scaling\n    mms = MinMaxScaler()\n    mms.fit(train_rnn)\n    X_train_sc = mms.transform(train_rnn)\n    X_test_sc = mms.transform(test_rnn)\n    \n    #ML\n        #Generating the table\n    length  = 48\n    batch_size = 20 #Number of timeseries samples in each batch\n    generator = TimeseriesGenerator(X_train_sc, X_train_sc, length=length, batch_size=batch_size)\n    \n        # define model\n    model = Sequential()\n    model.add(LSTM(128,activation= 'tanh',return_sequences=True, input_shape=(length,X_train_sc.shape[1])))\n    model.add(LSTM(64, activation='tanh', return_sequences = False))\n    model.add(Dense(X_train_sc.shape[1]))\n    model.compile(optimizer='adam', loss='mse')\n    \n        # Model Training\n    validation_generator = TimeseriesGenerator(X_test_sc,X_test_sc, length=38, batch_size=batch_size)\n    model.fit(generator,epochs=160,\n                        validation_data=validation_generator, verbose=2)#,callbacks=[early_stop])\n        \n        # Making predictions\n    n_features = X_train_sc.shape[1]\n    test_predictions = []\n    first_eval_batch = X_train_sc[:length]\n    current_batch = first_eval_batch.reshape((1, length, n_features))\n\n    for i in range(len(test_rnn)):\n    \n        # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n        current_pred = model.predict(current_batch)[0]\n        # store prediction\n        test_predictions.append(current_pred) \n        # update batch to now include prediction and drop first value\n        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n\n        # Creating prediction df\n    inv_test_pred = mms.inverse_transform(test_predictions)\n    pred_df = pd.DataFrame (data = inv_test_pred, columns =test_rnn.columns, index = test_rnn.index )\n    \n        # Storing predictions into main prediction df\n    main_rnn_pred_df = pd.merge(main_rnn_pred_df,pred_df, right_index=True, left_index=True)\n    \n    stop_time = timeit.default_timer()\n    execution_time = stop_time - start_time\n    clear_output()\n    print('Samples Done:', samp )\n    print('Samples left:', samples- (samp) )\n    print(\"Sample Prediction time is \"+str(round(execution_time))+\" seconds\")\n\nstop_time_main = timeit.default_timer()\nexecution_time_main = stop_time_main - start_time_main\nprint (\"Forecasting is completed\")\nprint(\"Whole process took \"+str(round(execution_time_main))+\" seconds\")","42beacef":"# Store the results\nrnn_pred_trans = main_rnn_pred_df.stack().reset_index()\nrnn_pred_trans.columns = ['Date', 'Store_Dept_code', 'Weekly_Sales_Rnn']\ndf_with_pred = pd.merge(df_rnn,rnn_pred_trans, right_on = ['Date', 'Store_Dept_code'], left_on=['Date', 'Store_Dept_code'], how = 'left')","e35329e0":"df_with_pred[~df_with_pred.Weekly_Sales_Rnn.isnull()].Store_Dept_code.nunique()","e9046aad":"# Check for RMSE and R2 scores\n\nfrom sklearn import metrics\nfrom sklearn.metrics import  r2_score\nactual = df_with_pred[~df_with_pred.Weekly_Sales_Rnn.isnull()].Weekly_Sales\npredicted = df_with_pred[~df_with_pred.Weekly_Sales_Rnn.isnull()].Weekly_Sales_Rnn\n\n\nprint('Results of RNN Model')\nprint('Weekly_Sales_Mean_test:' ,round(np.mean(actual) ))\nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(actual, predicted))))\nprint('R2 Score:', r2_score(actual, predicted))","16ed5445":"#tfd_fil_pred_start\n#tfd_fil\nst = 12\ns1 = tfd_fil[tfd_fil.Store == st]\nfig = plt.figure(figsize = (22,35))\nfig.suptitle(f'Walmart Weekly_Sales VS Predictions per {len(s1.Dept.unique())} departments\\nfor the {st} store\\n(Name is department No) ',y = 0.93,  fontsize = 16)\n\nfor dep, num_in_plot in  zip(s1.Dept.unique(), list(range(1,len(s1.Dept.unique())))):\n    \n    #actual\n    df_temp = df_with_pred[~df_with_pred.Weekly_Sales_Rnn.isnull()]\n    df_temp =df_temp[(df_temp.Store ==st)&(df_temp.Dept == dep)]\n    df_temp.set_index('Date', inplace = True)\n    \n    ax = fig.add_subplot(19,4,num_in_plot)\n    \n    ax.plot(df_temp['Weekly_Sales'])\n    ax.plot(df_temp['Weekly_Sales_Rnn'])\n\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_title(dep)\nfig.legend(labels = ('Actual','Predicted_RNN'), loc=9) #bbox_to_anchor=(1, 0.5))#, bbox_to_anchor=(0.5, 0., 0.5, 0.5))\nplt.show()","1677bd57":"The next step would be to transform our data frame into the special format required for Recurrent Neural Network and has a few main paramaters to specify:\n1. length - this is the amount of input data that we will provide to our model in order to make predictions. For example if we have list of 5 values and these are [1,2,3,4,5] and we specify the length == 3, then algorithm will take three last values from the list ([3,4,5]).  This parameter is mostly specified in correspondence with  seasonal component. As we have seen lastly, we have a year seasonality and our data is weekly. So the formula is 4(weeks in a month) * 12 = 48.\n2. batch_size -stands for number of training example used in one iteration. Normally as it was said in studies, lower the batch_size more accurately model fits to the data, however at the same time it spends much more time per 1 epoch. So after several trials I came up with optimal value, which is 10-15.\nTensorflow provides with the function that allows us easily to generate the our data into write format for our model.  ","45c06e4a":"RNN stands for recurrent neural network and is the class of artificial neural network (ANN). The main principal of this network is that it takes the current input, then it returns the output. Afterwards the returned output, becomes an input and then from this input algorithm returns the new output. In comparison with feed forward ANN, Recurrent Neural Network uses its internal memory in order to process the sequence of inputs. That is why the algorithm could be approached with such problems as speech and handwriting recognition and Time Series data as well. ","372065e4":"So the main purpose of this study is to build the Machine Learning (ML) model that will come up with most accurate forecast for every store in every department based on given dateset. Before coming to the ML stage, traditionally we will examine the data by going through the descriptive analysis and feature engineering as well.\n\nThere are two main models that will be concidered for making the predictions which are Random Forest and Recurrent Neural Network.","7832cf05":"The above scatter plot represents the the relation of CPI level and Unemployment rate to Sales Volume. As it could be noticed both graphs indicates flat results. Thus it will not give any great value to our model for future forecasts and hence we will drop these 2 features.","58aab396":"### Train test split","4c198b0f":"The Departments are also playing a crucial role for sales volume of Walmart.","881dfd71":"# Descriptive Analysis","de68956c":"Train and test data sets are ready to go.  So the train data is the same for both models, while target variable is different. For the second case we will use scaled difference feature.","a3681c35":"# Machine Learning ","a98640b4":"From the above plot it could be simply noticed that different store has different sales volumes. Of course, we would use it anyway as our purpose is to predict the sales volumes for every store for every department, however it will help to ML model in terms of predictions as well.","cbabaf38":"Store type seems to be an interesting feature. For example store type \"A\" has significantly higher sales amounts compare to its colleague categories.","364e93ce":"So, finally we have 39 weeks to predict, then we will split our data on train and test set, where the last one will be 39 weeks.","876a3630":"### Run RF ","915fe16a":"The results for different MarkDowns, Temperature and Fuel price as for Unemployment rate and CPI are quite flat. Thus we will not use these features for our predictions. \n\nSo finally we will drop all MarkDown columns, Unemployment, CPI, Temperature and Fuel price. We also will drop object columns such as 'IsHoliday' and 'Type' as for these we will use encoded version. ","767fb06c":"The problem of RNN is concatenated  with the memory space as that is quite short and thus it leads to vanishing gradient problem. And here LSTM comes.\nLSTM is the abbreviation for  Long Short Term Memory networks and appears as the modification of RNN. This 'upgrade' allows the networks easier to remember past data, which is quite crucial in our case of problem solving.\n\nAt this chapter we will not go too deep into the algorithms' theory as this is a long topic. Instead we will use on our problem solving.\n\nSo first, we will create special data frame for our RNN model. Initially, we will create a new feature with the codes, which is simply the combination of store and department number. Then we will transform our df to pivot table, where values are the Sales Volumes, index is Date and columns are codes. Finally, we will fill our NAN values with zeros. ","9f74d0fa":"Alright, predictions are ready and before checking the accuracy, lets plot actual Sales vs predicted ones for some random department.","40f0a0e3":"From above plot it could be noticed that there is a definite yearly seasonal pattern. For example for both 2010 and 2011 starting in mid of November and ending on end of December there are a sky rocket sales volumes, while in January, while all Christmas holidays are finished, customers tend to buy much less.  \n\nThe overall trend is not very obvious. Thus let's make time series decomposition and see if there is.  ","fbb5c3be":"Above are the box plots that represents the sales volume mean for every week number and month. The data is split by year, so blue columns are showing the data for 2010 and brown ones corresponding to 2011. These charts are the good illustration that there are a week number and month seasonal patterns as weeks as well as months from both years represents quite similar decline or growth. Thus we will use these features for our machine learning modelling.","d6b60667":"The correlation matrix does not provide us with much information. However we could see the the store size has a positive correlation to sales volumes, even though the it is not very strong, however is the strongest one. Thus we will use it afterwards for our ML. \n\nOther method of feature importance identification is simply comparing particular features with the sales volume. So we will plot some of these and see the results.","a4b36315":"### Train\/Test split\n\nThe threshold for train and test split will be the same date as we have used for RF.\n\nAt the same time, it is a must to scale our data for RNN, what we will perform as well.","ca028cab":"Second plot from the above represents the general trend of our TS data set.  And we could see that there is an overall growing trend, however that is not rapid. Despite the fact that sales pick during the Christmas time in 2010 was higher to compare to 2011,  still with each year it shows the slightly growing trend. \n\nHowever, the question is, does this trend and seasonality applies to all stores or these are individual? To answer on this question, lets create set of subplots with sales volumes for every store.","e1745e04":"## RNN model","2b55c9c9":"As it could be seen from the plot RF model is quite close to reality. \n\nLets plot the actual and predicted values for one store of all departments and see models' forecasts. ","2ea23625":"![random-forest.png](attachment:random-forest.png)","51dc2d55":"## Random Forest","362d698b":"Lets also generate the validation set, which our model will use to calculate the loss on each epoch.","97b3e194":"As the correlation matrix showed, the store size has the positive correlation to sales volume. The above plot indicates the same picture as with the size rise of the stores's space the sales volumes slightly goes up. Hence, our machine learning model could utilize this feature for forecast generation.  ","bd870fdf":"The answer is 'yes' for some bunch of stores and 'no' for others. It could be noticed that there is one store group that has a\u00a0common seasonal pattern and trend. On the other hand: TS data of some stores contain much more noise compared to others; some represents climbing growth trend, while others vice-versa; there are few stores with individual seasonal factor.\n\nSo why it is important to know the seasonal and trend components of our data set for different stores? \n\n1st scenario\nWe could use the\u00a0SARIMAX model, which is a handy and popular machine learning algorithm for TS predictions. This algorithm could be an alternative if we would make the overall sales predictions of Walmart, however, our purpose is to make forecasts for every store and every department. SARIMAX model implies only one target feature at a time. So, one way is to build a model and then loop it through each target feature. It could work in case if all target features would have identical seasonal and trend patterns. So with the power of the\u00a0pmdarima library, we could approach the\u00a0auto_arima function and specify p,d,q,P,D,Q parameters and loop the model all over the target features. However, in some stores, the picture is different from others. It means that we have to specify the individual parameters for particular stores and departments and use the\u00a0autoarima function multiple times, which is not handy in terms of time and computational resources. Thus will drop this scenario for this case straight at a time.\n\n2nd scenario\nVARIMAX algorithm is also an\u00a0interesting tool\u00a0in terms of predicting TS data with multiple target features at a time, which is our case. However, it shows the\u00a0best performance, while targets have a\u00a0particular level of causality. In our case, there are some stores that represent their own development of sales volume story. Thus we will skip this algorithm as well.\n\nAt the end of the day, two interesting models that we will apply are Random Forest (RF) and Recurrent Neural Networks (RNN) as both could work in terms of prediction of multiple targets of sequential data and consider individuality for every target (in our case for every department of every store). We will come back to these models in more detail later ahead.\n\nNow lets explore our features. First, we will plot the correlation matrix and check weather particular features are correlated to our target variable, however first we will encode our categorical variables.","871a992e":"Lets see what we have in the train set.","7a0a7f4a":"Overall the results of RNN is a bit worse then RF model, but before making the conclusion lets plot the predictions for one random store and its departments.","ff8ae708":"Random Forest is the algorithm coming from the family of the decision trees. Basically RF consists of bunch of decision-making trees, where the root node is chosen by the random state. Afterwards, while the model is ready, each decision tree outputs the result (label), which at the same time appears as the vote. Votes of each tree are equal. So finally, Random Forest, collects the results and assign the final label, based on the majority of votes. \n\nThis algorithm could be approached as for solving the classification and regression problems as well. Below is the easy representation of how the algorithm works.","5fa5e5ad":"To conclude, Random Forest model made a better job comapre to Recurrent Neural Networks and secondly it was much faster. Thus RF model is the winner in this competition.","6ce50b30":"Now lets plot the results.","48499122":"Train set is quite clean in terms of null values, which is great. Lets see what we have in 'features' set.","41726d1f":"Before we train our model, we have to specify the threshold date in order to split our data on train and test set. First, will will check how many dates we have to predict for our 'test' data set, then we will use this number as the threshold for our train test split.","99ced89d":"This plot shows us the loss for validation and training set.\n\nNow time to make the predictions.","790ef01a":"So our train data set is ready. Now we will implement the ML. \nThe Machine Learning chapter consists of particular blocks:\n1. First of all we will apply Random Forest model and see the results. The dependent features were already specified in the previous chapter so the only thing, which has to be covered. is to to make splits of our data on train and test sets. \n2.  Recurrent neural network (RNN). There we will apply RNN algorithm which is a class of artificial\u00a0neural networks and found its implementation when it comes to sequential data forecasts and quite common in use when it comes  to prediction of multiple target variables.","fb9d5192":"Alright, our data set is ready to explore. Lets start from general explorations of overall sales of Walmart of given data set. ","1367ce34":"There is a true believe that people tend to buy more on holidays. The above plot does not significantly indicates this fact with the given data set, however still proves the theory. Thus we will leave this feature as well for ML model consideration.","81d9bcb9":"In 'features' the picture is a bit different as most of features contain null values. We could consider the way of filling null values for specific features, however before, we will examine how these would be crucial in terms of future predictions. \n\nSo next we will work a bit on feature engineering by creating the 'date dummy' fields, which will help us to explore are there any patterns for variety of 'date' type features, such as year, month, week number and etc. At the same time we will merge our test, features and stores sets for making data explorations just more handy. ","4c713e47":"# Overview ","a6f87993":"It could be noticed that even though our model's predictions are having the right vector, however the results are quite plain. \nThere are two prior methods to solve this problem:\n1. add more No of epochs - in this case our model will fit to the training set better and then the results will be less plain as now. However due to the thing that we have totally 3331 features to predict it will take much time.\n2. To decrease the number of target features - instead of giving all features to one model at a time, we could split our data into samples and provide just some portion of it. Finally, we could loop this process, until all features are predicted.\nIn our case we will combine 1st and 2nd method and see the results.  ","a83e6fc3":"![Simple%20Rnn.JPG](attachment:Simple%20Rnn.JPG)","4df6ac06":"Now its time to build our model.","8d2dca6d":"First we will load all available datasets that we have and initial step libraries."}}