{"cell_type":{"53feb9aa":"code","4fbfe184":"code","81418777":"code","22ddf886":"code","51a07d64":"code","770435d1":"code","d30b9687":"code","c0e05160":"code","27d88216":"code","39af5686":"code","de464c2a":"code","7110002e":"code","efa51626":"code","9e644143":"code","e65b3f83":"code","acd285fe":"code","e2492f01":"code","08ce34e2":"code","87455ab9":"code","e31d4f5d":"code","fefa80e0":"code","63be3592":"code","ad2cdbf2":"code","7fb106c3":"code","dffe966e":"code","5a3a1393":"code","77106f8f":"code","bc03f94f":"code","e3d08797":"code","f4eb16fd":"code","4d0c4632":"code","facde5b5":"code","b875b98b":"code","78e59c79":"code","e5b468fd":"code","ef7ebd6e":"code","e994c7b9":"code","e8e9a9e3":"code","b9848252":"code","036a4632":"code","a78f6a7b":"code","677a5097":"code","6b35cf11":"code","1d5ae380":"code","d77d7b37":"code","665f3719":"code","8de01c4a":"code","d73b79fe":"code","134e4e3a":"code","0719d7e2":"code","8ba330d5":"code","631c697f":"code","6a92aa46":"code","550efc43":"code","a58a8b6a":"code","a5ce4523":"code","a19b8111":"code","cbd4c353":"code","fc792281":"code","dc19e0dd":"code","9587b0dc":"markdown","b8932ebc":"markdown","097bc7bb":"markdown","519f76c2":"markdown","33daefb5":"markdown","dcec44fe":"markdown","afd465cb":"markdown","69b29abe":"markdown"},"source":{"53feb9aa":"#Importing Libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nimport warnings\nimport itertools\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\nimport time\n\nwarnings.filterwarnings(\"ignore\")","4fbfe184":"#loading data\n\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","81418777":"#descriptive statistics\ndf.describe()","22ddf886":"#shape of data\ndf.shape","51a07d64":"#checking for null values\ndf.isnull().values.any()","770435d1":"#printing column names\ndf.columns","d30b9687":"#printing number of fraud entries vs number of no fraud entries\nprint('Number of Entries with class = No Frauds is {}'.format(df['Class'].value_counts()[0]))\nprint('Number of Entries with class = Frauds is {}'.format(df['Class'].value_counts()[1]))","c0e05160":"#printing percentage of fraud entries and no fraud entries\nprint('Percentage of Entries with class = No Frauds is {}'.format(round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset'))\nprint('Percentage of Entries with class = Frauds is {}'.format(round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset'))","27d88216":"#count plot of class column\nsns.countplot('Class', data=df, palette=\"Set3\")\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)')","39af5686":"#distribution plot of Time \nsns.distplot(df[\"Time\"], hist = False, color=\"y\")\nplt.title('Distribution of Transaction Time')\nplt.xlim([min(df[\"Time\"]), max(df[\"Time\"])])\nplt.show()","de464c2a":"#distribution plot of Amount \nsns.distplot(df[\"Amount\"], hist = False, color=\"b\")\nplt.title('Distribution of Transaction Amount')\nplt.xlim([min(df[\"Amount\"]), max(df[\"Amount\"])])\nplt.show()","7110002e":"#Scaling amount and time column and replace original column with scaled column\n\nrob_scaler = RobustScaler()\n\ndf['Amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))","efa51626":"#printing 5 rows after scaling to check if we have scaled perfectly \ndf.head()","9e644143":"#seperating original dataframe for testing purposes\nX = df.drop('Class', axis=1)\nY = df['Class']\n\nss = StratifiedKFold(n_splits = 5, random_state = None, shuffle = False)\n\nfor train_index, test_index in ss.split(X, Y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = Y.iloc[train_index], Y.iloc[test_index]\n    \n#Turning values into array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","e65b3f83":"#Before performing undersampling or oversampling let's check accuracy with default parameters\n#Our data is already scaled we should split our training and test sets now\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)","acd285fe":"#Turn the values into an array\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","e2492f01":"#Initialising classifiers\nsvc = SVC()\nknc = KNeighborsClassifier()\ndtc = DecisionTreeClassifier()\nlrc = LogisticRegression()\nrfc = RandomForestClassifier()\netc = ExtraTreesClassifier()\nxgb = XGBClassifier()","08ce34e2":"clfs = {'SVC' : svc,'KN' : knc, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'ETC': etc, \"XGB\": xgb}","87455ab9":"#function for classifier\ndef train_classifier(clf, train_X, train_y):\n    clf.fit(train_X, train_y)","e31d4f5d":"#function for predictions\ndef predict_labels(clf, test_X):\n    return (clf.predict(test_X))","fefa80e0":"#calling training and predict function for every classifier\npred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, X_train, y_train)\n    pred = predict_labels(v,X_test)\n    pred_scores.append((k, [accuracy_score(y_test,pred)]))","63be3592":"#printing testing accuracy for every classifier\nscore = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\nscore","ad2cdbf2":"#Random undersampling of data(Take only 492 Entries of non_fraud data)\n\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n#Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\n#printing first 5 rows after undersampling\nnew_df.head()","7fb106c3":"#printing shape of new dataframe\nnew_df.shape","dffe966e":"#printing number of fraud entries vs number of no fraud entries\nprint('Number of Entries with class = No Frauds is {}'.format(new_df['Class'].value_counts()[0]))\nprint('Number of Entries with class = Frauds is {}'.format(new_df['Class'].value_counts()[1]))","5a3a1393":"#Correlation with heat map\ncorr = new_df.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(23,17))\n\n#create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","77106f8f":"# -----> V17 removing outliers from fraud transactions\nv17_fraud = new_df['V17'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v17_fraud, 25), np.percentile(v17_fraud, 75)\nv17_iqr = q75 - q25\n\nv17_cut_off = v17_iqr * 1.5\nv17_lower, v17_upper = q25 - v17_cut_off, q75 + v17_cut_off\nprint('V17 Lower: {}'.format(v17_lower))\nprint('V17 Upper: {}'.format(v17_upper))\noutliers = [x for x in v17_fraud if x < v17_lower or x > v17_upper]\nprint('V17 outliers: {}'.format(outliers))\nprint('Feature V17 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V17'] > v17_upper) | (new_df['V17'] < v17_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n# -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","bc03f94f":"#Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","e3d08797":"#Our data is already scaled we should split our training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","f4eb16fd":"#Turn the values into an array\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","4d0c4632":"clf = {'SVC' : svc,'KN' : knc, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'ETC': etc, 'XGB':xgb}","facde5b5":"#function for training classifier\ndef train_classifier(clf, train_X, train_y):    \n    clf.fit(train_X, train_y)","b875b98b":"#function for predictions\ndef predict_labels(clf, features):\n    return (clf.predict(features))","78e59c79":"#confusion matrix for train data\ndef train_cf(clf, X_train, y_train):\n    predi = clf.predict(X_train)\n    conf_mat = confusion_matrix(y_train, predi)\n    class_label = [\"No fraud\", \"Fraud\"]\n    df = pd.DataFrame(conf_mat, index = class_label, columns = class_label)\n    sns.heatmap(df, annot = True, fmt=\"d\")\n    plt.title(\"Confusion Matrix for train data with {}\".format(clf))\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.show()","e5b468fd":"#confusion matrix for test data\ndef test_cf(clf, X_test, y_test):\n    predi = clf.predict(X_test)\n    conf_mat = confusion_matrix(y_test, predi)\n    class_label = [\"No fraud\", \"Fraud\"]\n    df = pd.DataFrame(conf_mat, index = class_label, columns = class_label)\n    sns.heatmap(df, annot = True, fmt=\"d\")\n    plt.title(\"Confusion Matrix for test data with {}\".format(clf))\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.show()","ef7ebd6e":"#function for calling training, ,testing and confusion matrics\npred_scores = []\nfor k,v in clf.items():\n    train_classifier(v, X_train, y_train)\n    pred = predict_labels(v,X_test)\n    pred_scores.append((k, [accuracy_score(y_test,pred)]))\n    train_cf(v, X_train, y_train)\n    test_cf(v, X_test, y_test)","e994c7b9":"#printing testing accuracy\nscore = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\nscore","e8e9a9e3":"#Use GridSearchCV to find the best parameters.\n\n#Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_\n\n#KNearestNeighbour\nknears_params = {\"n_neighbors\": list(range(1,50,2)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\nknears_neighbors = grid_knears.best_estimator_\n\n#Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\nsvc = grid_svc.best_estimator_\n\n#DecisionTree Classifier\ndepths = [1, 5, 10, 15]\nmin_samples_leaf = [2, 5, 10, 15]\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": depths, \n              \"min_samples_leaf\": min_samples_leaf}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\ntree_clf = grid_tree.best_estimator_\n\n#Random Forest Classifier\nbase_learners = [20, 40, 60]\ndepths = [1, 5, 10, 15]\n\nforest_params = {'n_estimators': base_learners, 'max_depth':depths}\ngrid_forest = GridSearchCV(RandomForestClassifier(max_features='sqrt'), forest_params)\ngrid_forest.fit(X_train, y_train)\n\nforest_clf = grid_forest.best_estimator_\n\n#GBDt\nbase_learners = [20, 40, 60]\ndepths = [1, 5, 10, 15]\n\ngbdt_params = {'n_estimators': base_learners, 'max_depth':depths}\ngrid_gbdt = GridSearchCV(XGBClassifier(booster = 'gbtree'), gbdt_params)\ngrid_gbdt.fit(X_train, y_train)\n\ngbdt_clf = grid_gbdt.best_estimator_\n","b9848252":"# Overfitting Case\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n\nforest_score = cross_val_score(forest_clf, X_train, y_train, cv=5)\nprint('Random Forest Classifier Cross Validation Score', round(forest_score.mean() * 100, 2).astype(str) + '%')\n\ngbdt_score = cross_val_score(gbdt_clf, X_train, y_train, cv=5)\nprint('gbdt Classifier Cross Validation Score', round(gbdt_score.mean() * 100, 2).astype(str) + '%')","036a4632":"#Plotting Learning Curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, estimator5, estimator6, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3,2, figsize=(20,18), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    #First Estimator (Logistic Regression)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    #Second Estimator (K Nearest Neighbour)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    #Third Estimator (Support Vector Classifier)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    #Fourth Estimator (Decision Tree Classifier)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    #Fifth Estimator (Random Forest Classifier)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator5, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax5.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax5.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax5.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax5.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax5.set_title(\"Random Forest Classifier Learning Curve\", fontsize=14)\n    ax5.set_xlabel('Training size (m)')\n    ax5.set_ylabel('Score')\n    ax5.grid(True)\n    ax5.legend(loc=\"best\")\n    \n    #Sixth Estimator (XGBoost)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator6, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax6.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax6.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax6.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax6.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax6.set_title(\"GBDT Learning Curve\", fontsize=14)\n    ax6.set_xlabel('Training size (m)')\n    ax6.set_ylabel('Score')\n    ax6.grid(True)\n    ax6.legend(loc=\"best\")\n\n    return plt","a78f6a7b":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, forest_clf, gbdt_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","677a5097":"#Create a DataFrame with all the scores and the classifiers names.\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\")\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method=\"decision_function\")\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\nforest_pred = cross_val_predict(forest_clf, X_train, y_train, cv=5)\ngbdt_pred = cross_val_predict(gbdt_clf, X_train, y_train, cv=5)","6b35cf11":"print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\nprint(\"Random Forest Classifier: \",roc_auc_score(y_train, tree_pred))\nprint(\"GBDT: \",roc_auc_score(y_train, gbdt_pred))","1d5ae380":"print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n#List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n#Classifier with optimal parameters\n#log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n#Implementing SMOTE Technique \n#Cross Validating the right way\n#Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in ss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","d77d7b37":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","665f3719":"y_score = best_est.decision_function(original_Xtest)","8de01c4a":"average_precision = average_precision_score(original_ytest, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","d73b79fe":"fig = plt.figure(figsize=(12,6))\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\nplt.step(recall, precision, color='r', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='#F59B00')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n    average_precision), fontsize=16)","134e4e3a":"#SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(ratio='minority', random_state=42)\n\n#This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","0719d7e2":"#We Improve the score by 2% points approximately Implement GridSearchCV and the other models.\n#Logistic Regression\nt0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample data took :{} sec\".format(t1 - t0))","8ba330d5":"#Logistic Regression fitted using SMOTE technique\ny_pred_log_reg = log_reg_sm.predict(X_test)\n\n#Other models fitted with UnderSampling\ny_pred_knear = knears_neighbors.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree_clf.predict(X_test)\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nfig, ax = plt.subplots(2, 2,figsize=(22,12))\n\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nplt.show()","631c697f":"print('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Decision Tree Classifier:')\nprint(classification_report(y_test, y_pred_tree))","6a92aa46":"#Final Score in the test set of logistic regression\n\n#Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n\n\n#Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n\nd = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=d)\n\n#Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n#Note how high is accuracy score it can be misleading! \nfinal_df","550efc43":"#Neural Network\nn_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","a58a8b6a":"undersample_model.summary()","a5ce4523":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","a19b8111":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=1)","cbd4c353":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","fc792281":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","dc19e0dd":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=1)","9587b0dc":"**Observation**\n\nNo null values\/no missing values","b8932ebc":"**Observation**\n\nMost of the transactions are non-fraud. That means we have highly Imbalanced dataset. We can still use this dataset to get good accuracy score but we should balance dataset.","097bc7bb":"### Keras || OverSampling (SMOTE):\n","519f76c2":"**Observation**\n\nDistribution plots are also highly skewed","33daefb5":"**Observation**\n\nV17, V14, V12 and V10 are having negative correlation score with class feature so there are chances of outliers in these features\n\nSo need to remove outliers from these features first\n\nOne way to remove outliers is to take interquartile range and multiply it with some threshold value. The higher this threshold is,the less outliers will detect","dcec44fe":"### Conclusion: \n\nOversampled technique gives better accuracy score compare to undersampled\n\nHowever we have not removed outliers ","afd465cb":"**Observation**\n\nRandom Forest, Decision Tree and XGBOOST seems to work well","69b29abe":"**Observation**\n\nSuccessfully under sampled dataframe"}}