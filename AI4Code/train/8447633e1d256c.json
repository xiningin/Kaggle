{"cell_type":{"7b127cb8":"code","d5b7106a":"code","354b8094":"code","ad24a4ce":"code","caf9f92f":"code","721929db":"code","ccfcd71c":"code","293a18b7":"code","3c7e237c":"code","e48c9ce9":"code","d0f16862":"code","b4aaf5a3":"code","fa9e0266":"code","51b771ca":"code","8c2ab572":"code","d82ea8c5":"code","74b36441":"code","cad8b552":"markdown","788c0fa2":"markdown","1541e456":"markdown","c59b0fe3":"markdown","70991fd8":"markdown","b973baf5":"markdown","99258e04":"markdown"},"source":{"7b127cb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nimport re\n\nfrom gensim.models import Word2Vec # Word2Vec module\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder, Normalizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d5b7106a":"#Loading dataset\nnews_data = pd.read_csv('\/kaggle\/input\/bbc-fulltext-and-category\/bbc-text.csv')\n\n# Basic info of the dataset\nprint(f\"Shape : {news_data.shape}, \\n\\nColumns: {news_data.columns}, \\n\\nCategories: {news_data.category.unique()}\")\n\n# print sample data\nnews_data.head().append(news_data.tail())","354b8094":"# Plot category data\nplt.figure(figsize=(10,6))\nsns.countplot(news_data.category)\nplt.show()","ad24a4ce":"class DataPreparation:\n    def __init__(self, data, column='text'):\n        self.df = data\n        self.column = column\n    \n    def preprocess(self):\n        self.tokenize()\n        self.remove_stopwords()\n        self.remove_non_words()\n        self.lemmatize_words()\n        \n        return self.df\n    \n    def tokenize(self):\n        self.df['clean_text'] = self.df[self.column].apply(nltk.word_tokenize)\n        print(\"Tokenization is done.\")\n    \n    def remove_stopwords(self):\n        stopword_set = set(nltk.corpus.stopwords.words('english'))\n        \n        rem_stopword = lambda words: [item for item in words if item not in stopword_set]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(rem_stopword)\n        print(\"Remove stopwords done.\")\n    \n    def remove_non_words(self):\n        \"\"\"\n            Remove all non alpha characters from the text data\n            :numbers: 0-9\n            :punctuation: All english punctuations\n            :special characters: All english special characters\n        \"\"\"\n        regpatrn = '[a-z]+'\n        rem_special_chars = lambda x: [item for item in x if re.match(regpatrn, item)]\n        self.df['clean_text'] = self.df['clean_text'].apply(rem_special_chars)\n        print(\"Removed non english characters is done.\")\n        \n    def lemmatize_words(self):\n        lemma = nltk.stem.wordnet.WordNetLemmatizer()\n        \n        on_word_lemma = lambda x: [lemma.lemmatize(w, pos='v') for w in x]\n        \n        self.df['clean_text'] = self.df['clean_text'].apply(on_word_lemma)\n        print(\"Lemmatization on the words.\")","caf9f92f":"# Preprocessing activities on the data\ndata_prep = DataPreparation(news_data)\n\ncleanse_df = data_prep.preprocess()","721929db":"def vectorize(vector, X_train, X_test):\n    vector_fit = vector.fit(X_train)\n    \n    X_train_vec = vector_fit.transform(X_train)\n    X_test_vec = vector_fit.transform(X_test)\n    \n    print(\"Vectorization is completed.\")\n    return X_train_vec, X_test_vec\n\ndef label_encoding(y_train):\n    \"\"\"\n        Encode the given list of class labels\n        :y_train_enc: returns list of encoded classes\n        :labels: actual class labels\n    \"\"\"\n    lbl_enc = LabelEncoder()\n    \n    y_train_enc = lbl_enc.fit_transform(y_train)\n    labels = lbl_enc.classes_\n    \n    return y_train_enc, labels","ccfcd71c":"# Encode the class labels\ny_enc_train, labels = label_encoding(news_data['category'])\n\n# Split from the loaded dataset\nX_train, X_valid, y_train, y_test = train_test_split(news_data['text'], y_enc_train, test_size=0.2, shuffle=True)\n","293a18b7":"# Bag of words (BOW) matrix\nbow_vector = CountVectorizer(ngram_range=(1, 1), analyzer='word', max_features=5000, max_df=2, min_df=1)\nbow_vector.fit(X_train) \n\n","3c7e237c":"pipe = Pipeline([('bow', bow_vector),\n                ('tfidf', TfidfTransformer())]).fit(X_train)","e48c9ce9":"train_tfidf = pipe.transform(X_train)\nvalid_tfidf = pipe.transform(X_valid)","d0f16862":"print(train_tfidf.shape, valid_tfidf.shape)","b4aaf5a3":"def lsa_reduction(X_train, X_test, n_comp=120):\n    svd = TruncatedSVD(n_components=n_comp)\n    normalizer = Normalizer()\n    \n    lsa_pipe = Pipeline([('svd', svd),\n                        ('normalize', normalizer)]).fit(X_train)\n    \n    train_reduced = lsa_pipe.transform(X_train)\n    test_reduced = lsa_pipe.transform(X_test)\n    return train_reduced, test_reduced\n    \ndef lsa_nmf_reduction(X_train, X_test, n_comp=120):\n    nmf = NMF(n_components=n_comp)\n    normalizer = Normalizer()\n    \n    lsa_pipe = Pipeline([('nmf', nmf),\n                        ('normalize', normalizer)]).fit(X_train)\n    \n    train_reduced = lsa_pipe.transform(X_train)\n    test_reduced = lsa_pipe.transform(X_test)\n    return train_reduced, test_reduced","fa9e0266":"xtrain_svd, xtest_svd = lsa_reduction(train_tfidf, valid_tfidf, 2000)\n\n# NMF dimensionality function is called only for Multinomial Naive Bayes\n# xtrain_svd, xtest_svd = lsa_nmf_reduction(train_tfidf, valid_tfidf)","51b771ca":"sgd = SGDClassifier(random_state=0,loss='log',alpha=0.01,penalty='elasticnet')\nlr = LogisticRegression(C=1.0)\nsvc = SVC(kernel='linear')\nnb = MultinomialNB()\n\n# One vs Restclassifier\norc_clf = OneVsRestClassifier(estimator=svc).fit(xtrain_svd, y_train)","8c2ab572":"print(orc_clf.get_params)\nprint(orc_clf.intercept_)","d82ea8c5":"# Predict the test data\ny_pred = orc_clf.predict(xtest_svd)","74b36441":"print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\tF1-Score: %1.3f\\n\" % (accuracy_score(y_test, y_pred),\n                                                                                     precision_score(y_test, y_pred, average='micro'),\n                                                                                     recall_score(y_test, y_pred, average='micro'),\n                                                                                     f1_score(y_test, y_pred, average='micro')))","cad8b552":"## 2. Exploring Data","788c0fa2":"## Feature Engineering","1541e456":"## Data Preparation","c59b0fe3":"# NLP Text Classification(Multiclass)\n\n## Dataset\n\nBBC News Article datasets are made available for non-commercial and research purposes only, and all data is provided in pre-processed matrix format. \n\n* Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n* Class Labels: 5 (business, entertainment, politics, sport, tech)\n\n## Approach using OneVsRestClassifier\n\nOne-vs-the-rest (OvR) multiclass\/multilabel strategy, its also known as one-vs-all. This approach consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.\n\n\n## Import Libraries","70991fd8":"## Building Model","b973baf5":"## Metrics","99258e04":"## 1. Loading Dataset"}}