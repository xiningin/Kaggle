{"cell_type":{"d85767f4":"code","80e66eca":"code","9a03c927":"code","4fba4cf3":"code","245750b7":"code","b3c444c7":"code","fdb46c76":"code","3d566eb4":"code","a8cceee2":"code","a06512a0":"code","a0085a9e":"code","cc0befab":"code","703f7de8":"code","c7e7df53":"code","3347a1a1":"code","3ce39a59":"code","10559cd1":"code","e1612b5f":"code","bc54e0c4":"code","76ee4a7d":"code","f83c7fb3":"code","51f2af02":"code","1e96c077":"code","849d412e":"code","7d6c3ea2":"code","9cccba29":"code","1720932f":"code","a11b5a28":"code","2d290186":"code","a31f8ade":"code","0c419920":"code","4cf0bdbd":"code","02384d9e":"code","6b41ec1c":"code","ba8aa316":"code","83e977d2":"markdown","f8a47814":"markdown","16242226":"markdown","b042da13":"markdown","f17ce803":"markdown","104dabd3":"markdown","74ef2303":"markdown","7147548e":"markdown","fbfa5cea":"markdown","cfaf22d2":"markdown","092f65e4":"markdown","70ef9a73":"markdown","a4ca457e":"markdown","40289a80":"markdown","6947812a":"markdown","f51f2df8":"markdown","fe50f395":"markdown","53ea015a":"markdown","3cdba8a5":"markdown","c2306149":"markdown","72f149f0":"markdown","0ad52840":"markdown","6527a1a8":"markdown"},"source":{"d85767f4":"!pip install -U sentence-transformers\n!pip3 install summa\n!pip install textstat","80e66eca":"import numpy as np\nimport pandas as pd\nimport spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport networkx as nx\nimport textstat\n\nfrom torchtext import vocab\nfrom summa import keywords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sentence_transformers import SentenceTransformer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom matplotlib.pyplot import figure","9a03c927":"train_data_csv_path = '..\/input\/commonlitreadabilityprize\/train.csv'\ntest_data_csv_path = '..\/input\/commonlitreadabilityprize\/test.csv'\nsample_submission_csv_path = '..\/input\/commonlitreadabilityprize\/sample_submission.csv'","4fba4cf3":"train_data = pd.read_csv(train_data_csv_path)\ntest_data = pd.read_csv(test_data_csv_path)\n\nprint(\"Length of training data: \",len(train_data))\nprint(\"Length of testing data: \",len(test_data))","245750b7":"train_data.head(10)","b3c444c7":"text = train_data['excerpt'].values\ntext[0]","fdb46c76":"easy_text = train_data[train_data['target'] >= -1]['excerpt']\nhard_text = train_data[train_data['target'] < -1]['excerpt']\n\neasy_score = train_data[train_data['target'] >= -1]['target']\nhard_score = train_data[train_data['target'] < -1]['target']\n\nprint(\"Easy text present in the data: \",len(easy_text))\nprint(\"Hard text present in the data: \",len(hard_text))","3d566eb4":"plt.rcParams[\"figure.figsize\"] = (15,10)\n\ndef get_length_dict(text,type_of_text):\n    mean_length = 0\n    length_dict = {}\n    for t in text:\n        str_length = len(word_tokenize(t))\n        if length_dict.get(str_length):\n            length_dict[str_length] += 1\n        else:\n            length_dict[str_length] = 1\n        mean_length += str_length \n    mean_length \/= len(text)\n    print(\"Mean word length of the {} data: {} \".format(type_of_text,mean_length))\n    length_dict = {k: v for k, v in sorted(length_dict.items(),reverse=True,key=lambda item: item[1])[:25]}\n    return length_dict\n\nlength_dict = get_length_dict(text,'Total')\nplt.bar(range(len(length_dict)), list(length_dict.values()), align='center')\nplt.xticks(range(len(length_dict)), list(length_dict.keys()))\nplt.show()","a8cceee2":"def create_title_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black',\n        stopwords = STOPWORDS).generate(str(text))\n    return wordcloud\n\nwordcloud = create_title_word_cloud(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","a06512a0":"def generate_top_k_n_grams(text,k,n):\n    word_vectorizer = CountVectorizer(ngram_range=n,stop_words='english')\n    sparse_matrix = word_vectorizer.fit_transform(text)\n    frequencies = sum(sparse_matrix).toarray()[0]\n    freq_df = pd.DataFrame({\n        'words': word_vectorizer.get_feature_names(),\n        'frequency': frequencies \n    })\n    freq_df_sorted = freq_df.sort_values('frequency',ascending=False)[:k] \n    return freq_df_sorted","a0085a9e":"freq_data = generate_top_k_n_grams(text,30,(1,1))\nsns.barplot(x = 'words', y = 'frequency',data = freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()","cc0befab":"freq_data = generate_top_k_n_grams(text,30,(2,2))\nsns.barplot(x = 'words', y = 'frequency',data = freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()","703f7de8":"freq_data = generate_top_k_n_grams(text,30,(3,3))\nsns.barplot(x = 'words', y = 'frequency',data = freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()","c7e7df53":"easy_length_dict = get_length_dict(easy_text,'Easy')\nhard_length_dict = get_length_dict(hard_text,'Hard')\n\nplt.subplot(1,2,1)\nplt.bar(range(len(easy_length_dict)), list(easy_length_dict.values()), align='center')\nplt.xticks(range(len(easy_length_dict)), list(easy_length_dict.keys()),rotation=45,fontsize=13)\nplt.title(\"Easy essays\")\n\nplt.subplot(1,2,2)\nplt.bar(range(len(hard_length_dict)), list(hard_length_dict.values()), align='center')\nplt.xticks(range(len(hard_length_dict)), list(hard_length_dict.keys()),rotation=45,fontsize=13)\nplt.title(\"Hard essays\")\nplt.show()","3347a1a1":"easy_freq_data = generate_top_k_n_grams(easy_text,30,(1,1))\nhard_freq_data = generate_top_k_n_grams(hard_text,30,(1,1))\n\nplt.subplot(1,2,1)\nsns.barplot(y = 'words', x = 'frequency',data = easy_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Easy text bigrams')\n\nplt.subplot(1,2,2)\nsns.barplot(y = 'words', x = 'frequency',data = hard_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=20,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Hard text unigrams')\nplt.show()","3ce39a59":"easy_freq_data = generate_top_k_n_grams(easy_text,30,(2,2))\nhard_freq_data = generate_top_k_n_grams(hard_text,30,(2,2))\n\nplt.subplot(1,2,1)\nsns.barplot(y = 'words', x = 'frequency',data = easy_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Easy text bigrams')\n\nplt.subplot(1,2,2)\nsns.barplot(y = 'words', x = 'frequency',data = hard_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=20,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Hard text bigrams')\nplt.show()\n","10559cd1":"easy_freq_data = generate_top_k_n_grams(easy_text,30,(3,3))\nhard_freq_data = generate_top_k_n_grams(hard_text,30,(3,3))\n\nplt.subplot(1,2,1)\nsns.barplot(y = 'words', x = 'frequency',data = easy_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=45,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Easy text trigrams')\n\nplt.subplot(1,2,2)\nsns.barplot(y = 'words', x = 'frequency',data = hard_freq_data,\n            palette = 'hls',\n    )\nplt.xticks(rotation=20,fontsize=13)\nplt.yticks(fontsize=13)\nplt.title('Hard text trigrams')\nplt.show()","e1612b5f":"easy_wordcloud = create_title_word_cloud(easy_text)\nhard_wordcloud = create_title_word_cloud(hard_text)\n\nplt.subplot(1,2,1)\nplt.imshow(easy_wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Easy text')\nplt.tight_layout(pad=0)\n\nplt.subplot(1,2,2)\nplt.imshow(hard_wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Hard text')\nplt.tight_layout(pad=0)\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=2.5, \n                    wspace=0.2, \n                    hspace=0.2)\nplt.show()","bc54e0c4":"model = SentenceTransformer('bert-base-nli-mean-tokens')\n\neasy_text_slice = easy_text[:100]\nhard_text_slice = hard_text[:100]\n\neasy_embeddings = []\nfor text in easy_text_slice:\n    easy_embeddings.append(model.encode(text))\n    \nhard_embeddings = []\nfor text in hard_text_slice:\n    hard_embeddings.append(model.encode(text))","76ee4a7d":"pca_2 = PCA(n_components = 2)\ntotal_text = easy_embeddings + hard_embeddings \npca_2.fit(total_text)\n\neasy_pca_output = pca_2.transform(easy_embeddings)\nhard_pca_output = pca_2.transform(hard_embeddings)\n\neasy_df = pd.DataFrame(easy_pca_output)\ntype_col = ['easy'] * len(easy_df)\neasy_df.insert(2, 'type_of_data', type_col)\n\nhard_df = pd.DataFrame(hard_pca_output)\ntype_col = ['hard'] * len(hard_df)\nhard_df.insert(2, 'type_of_data', type_col)\n\ntotal_df = easy_df.append(hard_df, ignore_index=True)\ntotal_df.head()\n\nfig = px.scatter(\n    total_df.iloc[: , :2], x=0, y=1, color=total_df['type_of_data'],\n    title='Plotting document embeddings in 2 dimensions'\n)\nfig.show()","f83c7fb3":"pca = PCA(n_components = 3)\ntotal_text = easy_embeddings + hard_embeddings \npca.fit(total_text)\n\neasy_pca_output = pca.transform(easy_embeddings)\nhard_pca_output = pca.transform(hard_embeddings)\n\neasy_df = pd.DataFrame(easy_pca_output)\ntype_col = ['easy'] * len(easy_df)\neasy_df.insert(3, 'type_of_data', type_col)\n\nhard_df = pd.DataFrame(hard_pca_output)\ntype_col = ['hard'] * len(hard_df)\nhard_df.insert(3, 'type_of_data', type_col)\n\ntotal_df = easy_df.append(hard_df, ignore_index=True)\ntotal_df.head()\n\n\nfig = px.scatter_3d(\n    total_df.iloc[: , :3], x=0, y=1, z=2, color=total_df['type_of_data'],\n    title='Plotting document embeddings in 3 dimensions'\n)\nfig.show()","51f2af02":"easy_keywords = []\nfor text in easy_text:\n    easy_keywords.append(keywords.keywords(text).split('\\n'))\n\nhard_keywords = []\nfor text in hard_text:\n    hard_keywords.append(keywords.keywords(text).split('\\n'))","1e96c077":"easy_keyword_set = list({e for l in easy_keywords for e in l})\nhard_keyword_set = list({e for l in hard_keywords for e in l})\n\nprint(len(easy_keyword_set),len(hard_keyword_set))","849d412e":"!cp -r  ..\/input\/glove6b\/glove.6B.300d.txt .\/","7d6c3ea2":"VECTOR_PATH = '.\/'\nVECTOR_NAME = 'glove.6B.300d.txt'\n\nembeddings = vocab.Vectors(VECTOR_NAME,VECTOR_PATH)\n\nsliced_easy_keyword_set = easy_keyword_set[:100]\nsliced_hard_keyword_set = hard_keyword_set[:100]\n\neasy_embeddings = []\nfor word in sliced_easy_keyword_set:\n    if not  all(x == 0 for x in embeddings[word].tolist()):\n        easy_embeddings.append(embeddings[word])\n\nhard_embeddings = []\nfor word in sliced_hard_keyword_set:\n    if not  all(x == 0 for x in embeddings[word].tolist()):\n        hard_embeddings.append(embeddings[word])\n\nprint(len(easy_embeddings),len(hard_embeddings))","9cccba29":"pca_2 = PCA(n_components = 2)\ntotal_text = easy_embeddings + hard_embeddings\ntotal_text_list = [x.numpy() for x in total_text]\npca_2.fit(total_text_list)\n\neasy_embeddings_list = [x.numpy() for x in easy_embeddings]\nhard_embeddings_list = [x.numpy() for x in hard_embeddings]\neasy_pca_output = pca_2.transform(easy_embeddings_list)\nhard_pca_output = pca_2.transform(hard_embeddings_list)\n\neasy_df = pd.DataFrame(easy_pca_output)\ntype_col = ['easy'] * len(easy_df)\neasy_df.insert(2, 'type_of_data', type_col)\n\nhard_df = pd.DataFrame(hard_pca_output)\ntype_col = ['hard'] * len(hard_df)\nhard_df.insert(2, 'type_of_data', type_col)\n\ntotal_df = easy_df.append(hard_df, ignore_index=True)\ntotal_df.head()\n\nfig = px.scatter(\n    total_df.iloc[: , :2], x=0, y=1, color=total_df['type_of_data'],\n    title='Plotting keyword embeddings in 2 dimensions'\n)\nfig.show()","1720932f":"pca_3 = PCA(n_components = 3)\ntotal_text = easy_embeddings + hard_embeddings\ntotal_text_list = [x.numpy() for x in total_text]\npca_3.fit(total_text_list)\n\neasy_embeddings_list = [x.numpy() for x in easy_embeddings]\nhard_embeddings_list = [x.numpy() for x in hard_embeddings]\neasy_pca_output = pca_3.transform(easy_embeddings_list)\nhard_pca_output = pca_3.transform(hard_embeddings_list)\n\neasy_df = pd.DataFrame(easy_pca_output)\ntype_col = ['easy'] * len(easy_df)\neasy_df.insert(3, 'type_of_data', type_col)\n\nhard_df = pd.DataFrame(hard_pca_output)\ntype_col = ['hard'] * len(hard_df)\nhard_df.insert(3, 'type_of_data', type_col)\n\ntotal_df = easy_df.append(hard_df, ignore_index=True)\ntotal_df.head()\n\nfig = px.scatter_3d(\n    total_df.iloc[: , :3], x=0, y=1,z=2, color=total_df['type_of_data'],\n    title='Plotting keyword embeddings in 3 dimensions'\n)\nfig.show()","a11b5a28":"easy_words_per_sentences = []\neasy_characters_per_sentences = []\neasy_characters_per_word = []\nfor text in easy_text:\n    words = word_tokenize(text)\n    sentences = sent_tokenize(text)\n    easy_words_per_sentences.append(len(words)\/len(sentences))\n    easy_characters_per_sentences.append(len(str(''.join(words)))\/len(sentences))\n    easy_characters_per_word.append(len(str(''.join(words)))\/len(words))\n    \n\nhard_words_per_sentences = []\nhard_characters_per_sentences = []\nhard_characters_per_word = []\nfor text in hard_text:\n    words = word_tokenize(text)\n    sentences = sent_tokenize(text)\n    hard_words_per_sentences.append(len(words)\/len(sentences))\n    hard_characters_per_sentences.append(len(str(''.join(words)))\/len(sentences))\n    hard_characters_per_word.append(len(str(''.join(words)))\/len(words))\n\nprint(\"Easy characters per words: \",sum(easy_characters_per_word)\/len(easy_characters_per_word))\nprint(\"Easy characters per sentences: \",sum(easy_characters_per_sentences)\/len(easy_characters_per_sentences))\nprint(\"Easy words per sentences: \",sum(easy_words_per_sentences)\/len(easy_words_per_sentences))\n\n\nprint(\"Hard characters per words: \",sum(hard_characters_per_word)\/len(hard_characters_per_word))\nprint(\"Hard characters per sentences: \",sum(hard_characters_per_sentences)\/len(hard_characters_per_sentences))\nprint(\"Hard words per sentences: \",sum(hard_words_per_sentences)\/len(hard_words_per_sentences))","2d290186":"plt.subplot(1,3,1)\nplt.title('Score v\/s Characters per word')\nplt.scatter(easy_characters_per_word,list(easy_score.values))\nplt.scatter(hard_characters_per_word,list(hard_score.values))\n\nplt.subplot(1,3,2)\nplt.title('Score v\/s Characters per sentences')\nplt.scatter(easy_characters_per_sentences ,list(easy_score.values))\nplt.scatter(hard_characters_per_sentences,list(hard_score.values))\n\n\nplt.subplot(1,3,3)\nplt.title('Score v\/s Words per sentences')\nplt.scatter(easy_words_per_sentences ,list(easy_score.values))\nplt.scatter(hard_words_per_sentences, list(hard_score.values))\n\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=1.2, \n                    wspace=0.4, \n                    hspace=0.4)\n\nplt.show()","a31f8ade":"plt.subplot(1,3,1)\nsns.kdeplot(data=easy_characters_per_word,color='blue')\nsns.kdeplot(data=hard_characters_per_word,color='red')\nplt.title('Characters per Word')\n\nplt.subplot(1,3,2)\nsns.kdeplot(data=easy_characters_per_sentences,color='blue')\nsns.kdeplot(data=hard_characters_per_sentences,color='red')\nplt.title('Characters per Sentences')\n\nplt.subplot(1,3,3)\nsns.kdeplot(data=easy_words_per_sentences,color='blue')\nsns.kdeplot(data=hard_words_per_sentences,color='red')\nplt.title('Words per Sentences')\n\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=1.2, \n                    wspace=0.4, \n                    hspace=0.4)\n\nplt.show()","0c419920":"nlp = spacy.load('en_core_web_sm')\n\neasy_dictionary = {'NOUN':[],'PROPN':[],'VERB':[],'ADJ':[],'ADP':[],'AUX':[],}\neasy_dictionary_per_sentence = {'NOUN':[],'PROPN':[],'VERB':[],'ADJ':[],'ADP':[],'AUX':[],}\nhard_dictionary = {'NOUN':[],'PROPN':[],'VERB':[],'ADJ':[],'ADP':[],'AUX':[],}\nhard_dictionary_per_sentence = {'NOUN':[],'PROPN':[],'VERB':[],'ADJ':[],'ADP':[],'AUX':[],}\n\nfor index,text in enumerate(easy_text):\n    doc = nlp(text)\n    for key,value in easy_dictionary.items():\n        easy_dictionary[key].append(0)\n    for ent in doc:\n        if easy_dictionary.get(ent.pos_):\n            easy_dictionary[ent.pos_][-1]+=1\n    for key,value in easy_dictionary_per_sentence.items():\n        easy_dictionary_per_sentence[key].append(easy_dictionary[key][index]\/len(sent_tokenize(text)))\n\nfor index,text in enumerate(hard_text):\n    doc = nlp(text)\n    for key,value in hard_dictionary.items():\n        hard_dictionary[key].append(0)\n    for ent in doc:\n        if hard_dictionary.get(ent.pos_):\n            hard_dictionary[ent.pos_][-1]+=1\n    for key,value in hard_dictionary_per_sentence.items():\n        hard_dictionary_per_sentence[key].append(hard_dictionary[key][index]\/len(sent_tokenize(text)))\n        ","4cf0bdbd":"plt.subplot(2,3,1)\nplt.scatter(easy_dictionary_per_sentence['NOUN'],easy_score,color=\"blue\")\nplt.scatter(hard_dictionary_per_sentence['NOUN'],hard_score,color=\"lightblue\")\nplt.title('Nouns per sentence vs score')\n\nplt.subplot(2,3,2)\nplt.scatter(easy_dictionary_per_sentence['PROPN'],easy_score,color=\"green\")\nplt.scatter(hard_dictionary_per_sentence['PROPN'],hard_score,color=\"lightgreen\")\nplt.title('Proper nouns per sentence vs score')\n\nplt.subplot(2,3,3)\nplt.scatter(easy_dictionary_per_sentence['VERB'],easy_score,color=\"red\")\nplt.scatter(hard_dictionary_per_sentence['VERB'],hard_score,color=\"pink\")\nplt.title('Verbs per sentence vs score')\n\nplt.subplot(2,3,4)\nplt.scatter(easy_dictionary_per_sentence['ADJ'],easy_score,color=\"orange\")\nplt.scatter(hard_dictionary_per_sentence['ADJ'],hard_score,color=\"yellow\")\nplt.title('Adjectives per sentence vs score')\n\nplt.subplot(2,3,5)\nplt.scatter(easy_dictionary_per_sentence['ADP'],easy_score,color=\"darkgrey\")\nplt.scatter(hard_dictionary_per_sentence['ADP'],hard_score,color=\"lightgrey\")\nplt.title('ADP per sentence vs score')\n\nplt.subplot(2,3,6)\nplt.scatter(easy_dictionary_per_sentence['AUX'],easy_score,color=\"brown\")\nplt.scatter(hard_dictionary_per_sentence['AUX'],hard_score,color=\"peru\")\nplt.title('Auxillary verbs per sentence vs score')\n\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=2.5, \n                    wspace=0.4, \n                    hspace=0.4)","02384d9e":"plt.subplot(2,3,1)\nplt.scatter(easy_dictionary['NOUN'],easy_score,color=\"blue\")\nplt.scatter(hard_dictionary['NOUN'],hard_score,color=\"lightblue\")\nplt.title('Nouns per data point vs score')\n\nplt.subplot(2,3,2)\nplt.scatter(easy_dictionary['PROPN'],easy_score,color=\"green\")\nplt.scatter(hard_dictionary['PROPN'],hard_score,color=\"lightgreen\")\nplt.title('Proper nouns per data point vs score')\n\nplt.subplot(2,3,3)\nplt.scatter(easy_dictionary['VERB'],easy_score,color=\"red\")\nplt.scatter(hard_dictionary['VERB'],hard_score,color=\"pink\")\nplt.title('Verbs per data point vs score')\n\nplt.subplot(2,3,4)\nplt.scatter(easy_dictionary['ADJ'],easy_score,color=\"orange\")\nplt.scatter(hard_dictionary['ADJ'],hard_score,color=\"yellow\")\nplt.title('Adjectives per data point vs score')\n\nplt.subplot(2,3,5)\nplt.scatter(easy_dictionary['ADP'],easy_score,color=\"darkgrey\")\nplt.scatter(hard_dictionary['ADP'],hard_score,color=\"lightgrey\")\nplt.title('ADP per data point vs score')\n\nplt.subplot(2,3,6)\nplt.scatter(easy_dictionary_per_sentence['AUX'],easy_score,color=\"brown\")\nplt.scatter(hard_dictionary_per_sentence['AUX'],hard_score,color=\"peru\")\nplt.title('Auxillary verbs per sentence vs score')\n\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=2.5, \n                    wspace=0.4, \n                    hspace=0.4)","6b41ec1c":"total_text = list(easy_text.values) + list(hard_text.values)\nscores = list(easy_score.values) + list(hard_score.values)\nscores_dict = {'flesch_reading_ease':[],'flesch_kincaid_grade':[],'gunning_fog':[],\n              'smog_index':[],'automated_readability_index':[],'coleman_liau_index':[],\n              'linsear_write_formula':[],'dale_chall_readability_score':[],\n              'given_scores':scores}\n\nfor text in total_text:\n    scores_dict['flesch_reading_ease'].append(textstat.flesch_reading_ease(text))\n    scores_dict['flesch_kincaid_grade'].append(textstat.flesch_kincaid_grade(text))\n    scores_dict['gunning_fog'].append(textstat.gunning_fog(text))\n    scores_dict['smog_index'].append(textstat.smog_index(text))\n    scores_dict['automated_readability_index'].append(textstat.automated_readability_index(text))\n    scores_dict['coleman_liau_index'].append(textstat.coleman_liau_index(text))\n    scores_dict['linsear_write_formula'].append(textstat.linsear_write_formula(text))\n    scores_dict['dale_chall_readability_score'].append(textstat.dale_chall_readability_score(text))\n\n# combination of all\n# textstat.text_standard(text, float_output=False)","ba8aa316":"scores_dataframe = pd.DataFrame(scores_dict)\n\ncorrMatrix = scores_dataframe.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","83e977d2":"As we can see from the above wordcloud,the words are common English words without much orthographic errors. ","f8a47814":"We see the first few datapoints ","16242226":"# **N-gram comparison for easy vs hard essays**\nWe now compare the most common n-grams for easy as well as hard essays","b042da13":"# **Mean length of easy vs hard essays**\nWe compare the length of the easy and the hard essays. ","f17ce803":"As we can see in our plot, most of our datapoints contain 190-200 words. ","104dabd3":"# **Visualizing Word Cloud**\nData cloud helps us to visualize most important keywords in the whole text corpora  ","74ef2303":"### *That's all for now. Please upvote it if you liked the notebook :)*","7147548e":"# **Additional readability scores**\nWe compute additional reasablitlity scores using [textstat](https:\/\/github.com\/shivam5992\/textstat) library. The scores used are:\n* Flesch reading ease\n* Flesch kincaid grade\n* Gunning Fog\n* Smog Index\n* Automated Readability Index\n* Coleman liau index\n* Linsear write formula\n* Dale chall readability score\n\nWe then calculate the correlation between the given score and the above calculated scores.","fbfa5cea":"# **Checking word and character average count**\nWe calculate the words per sentence, characters per sentence and characters per word for both easy and hard text. ","cfaf22d2":"We will first try to understand the dataset by looking at each of the datapoints. We are plotting the frequency of words in our data. ","092f65e4":"# **Loading the data**\nIn this section we load the dataset into python lists. \nThe dataset contains three files- train_csv, test_csv and submission_csv. In the train_csv and test_csv we have the following columns- \n* id - unique ID for excerpt\n* url_legal - URL of source - this is blank in the test set.\n* license - license of source material - this is blank in the test set.\n* excerpt - text to predict reading ease of\n* target - reading ease. This score ranges from -4 to +2. Greater the number, greater the ease of readability. \n* standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n","70ef9a73":"# **Visualizing n-gram keywords in 2D and 3D space**\nWe first find the keywords using summa library. Then, we use 300-dimensional glove vector embeddings for those keywords. Then using PCA, we plot them in 2D as well as 3D space. ","a4ca457e":"# **Plotting the document embeddings of essays**\nHere, we are using bert to get the document embeddings of the datapoints and then using PCA we are plotting it in 2-dimensional and 3-dimensional space. ","40289a80":"We can see from the above density plots that the density funtion for the hard essays in all the three grpahs are a little right shifted as compared to the easy essays. Right shift indicates that the words per sentence, characters per sentence and the characters per word are more in hard essays than in easy essays.","6947812a":"# **POS Tagging for easy vs hard essays**\nWe use spacy to extract different parts of speech and then plot their count vs socre. ","f51f2df8":"We will classify our data into easy and hard essays **for visualizaion purposes only**. Easy essays are the ones whose scores are greater than or equal to 0 and hard essays are the ones whose scores lesser than 0. ","fe50f395":"# **Plotting n-grams**\nWe will plot the common unigrams, bigrams as well as trigrams for the whole text corpora ","53ea015a":"# **Plotting word length frequency**\n","3cdba8a5":"From the above frequencies of the common words, we can see that usually in hard essays, the frequency of most common words is less than that in easy essays. We can thus say that the hard essays have a more diverse set of unigrmas, bigrams and trigrams as compared to the easy essays. ","c2306149":"# **Imports and installations**\nHere we will be importing the necessary packages as well as installing the new ones. ","72f149f0":"From the above correlation heatmap we can see that the correlation for given score is negative for all the additional scores except for flesch reading ease.","0ad52840":"# **Easy wordcloud vs hard wordcloud**\nWe will try to plot the wordclouds for easy essays and hard essays. ","6527a1a8":"As we can see, the length of the essays doesn't play a major difference in their readability scores. "}}