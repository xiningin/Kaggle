{"cell_type":{"b2c25f49":"code","ac8f9739":"code","07f0db04":"code","6bacd6b9":"code","a846d80e":"code","637c8b19":"code","0c27de44":"code","6d654131":"code","abf6fc58":"code","f6260212":"markdown","30403d0d":"markdown"},"source":{"b2c25f49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ac8f9739":"from sklearn import datasets\nimport matplotlib.pyplot as plt","07f0db04":"\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\ndigits.keys()\n","6bacd6b9":"print(digits['DESCR'])","a846d80e":"\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)","637c8b19":"# Display digit 1010\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()","0c27de44":"# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Print the accuracy\nprint(knn.score(X_test,y_test))","6d654131":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n","abf6fc58":"# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","f6260212":" It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data.","30403d0d":"k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! "}}