{"cell_type":{"031a2ac0":"code","dc831915":"code","48ba1a97":"code","0c9368da":"code","e99bd6f3":"code","357fbbf4":"code","44e3c467":"code","6498daf8":"code","4142e449":"code","1f7a77c7":"code","2fda70f2":"code","5f184774":"code","ad8c3d64":"code","7955d802":"code","c9d0dcdf":"code","6d4504e1":"code","a70d5ffb":"code","bb2b2453":"markdown","c39a9d2c":"markdown","1275a7d1":"markdown","27b6cbae":"markdown","8fdb8551":"markdown","e249a627":"markdown","41271056":"markdown","9ab8f337":"markdown","e7159368":"markdown","4fc701db":"markdown","59269182":"markdown"},"source":{"031a2ac0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n","dc831915":"test_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","48ba1a97":"train_df.info()","0c9368da":"# Numeros faltantes no dataframe\ntotal = train_df.isnull().sum().sort_values(ascending=False)\nporcentagem_1 = train_df.isnull().sum() \/ train_df.isnull().count()*100\nporcentagem_2 = (round(porcentagem_1, 1)).sort_values(ascending=False)\ndados_nulos = pd.concat([total, porcentagem_2], axis=1, keys=['Total','%'])\ndados_nulos.head()","e99bd6f3":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nmulheres = train_df[train_df['Sex']=='female']\nhomens = train_df[train_df['Sex']=='male']\nax = sns.distplot(mulheres[mulheres['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(mulheres[mulheres['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Feminino')\nax = sns.distplot(homens[homens['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(homens[homens['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Masculino')","357fbbf4":"sns.barplot(x='Sex', y='Survived', data=train_df)","44e3c467":"sns.barplot(x='Pclass', y='Survived', data=train_df)","6498daf8":"# salvar os \u00edndices dos datasets para recupera\u00e7\u00e3o posterior\ntrain_idx = train_df.shape[0]\ntest_idx = test_df.shape[0]\n\n# salvar PassengerId para submissao ao Kaggle\npassengerId = test_df['PassengerId']\n\n# extrair coluna 'Survived' e excluir ela do dataset treino\ntarget = train_df.Survived.copy()\ntrain_df.drop(['Survived'], axis=1, inplace=True)\n\n# concatenar treino e teste em um \u00fanico DataFrame\ndf_merged = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n\nprint(\"df_merged.shape: ({} x {})\".format(df_merged.shape[0], df_merged.shape[1]))","4142e449":"df_merged.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","1f7a77c7":"df_merged.head()","2fda70f2":"# age\nage_median = df_merged['Age'].median()\ndf_merged['Age'].fillna(age_median, inplace=True)\n\n# fare\nfare_median = df_merged['Fare'].median()\ndf_merged['Fare'].fillna(fare_median, inplace=True)\n\n# embarked\nembarked_top = df_merged['Embarked'].value_counts()[0]\ndf_merged['Embarked'].fillna(embarked_top, inplace=True)","5f184774":"# Os valores da coluna Sex devem ser numericos \ndef mudar_sexo(valor):\n    if valor == 'female':\n        return 1\n    else:\n        return 0\n\ndf_merged['Sex'] = df_merged['Sex'].map(mudar_sexo)","ad8c3d64":"embarked_dummies = pd.get_dummies(df_merged['Embarked'], prefix='Embarked')\ndf_merged = pd.concat([df_merged, embarked_dummies], axis=1)\ndf_merged.drop('Embarked', axis=1, inplace=True)\n\ndisplay(df_merged.head())","7955d802":"train = df_merged.iloc[:train_idx]\ntest = df_merged.iloc[train_idx:]","c9d0dcdf":"train.shape, test.shape","6d4504e1":"random_f = RandomForestClassifier()\nrandom_f.fit(train, target)","a70d5ffb":"# verificar a acur\u00e1cia do modelo\nrandom_forest = round(random_f.score(train, target) * 100, 2)\nprint(\"Acur\u00e1cia do modelo de Regress\u00e3o Log\u00edstica: {}\".format(random_forest))","bb2b2453":"### Selecionar as features\nNessa etapa retiramos as colunas com valores n\u00e3o relevantes como 'PassengerId', 'Name', 'Ticket', 'Cabin'","c39a9d2c":"# Defini\u00e7\u00e3o do problema\nO Titanic foi lan\u00e7\u00e3o ao mar em 1911, sua constru\u00e7\u00e3o levou cerca de 2 anaos e foram gastos 7,5 milh\u00f5es de dolares. Com 269 metros de comprimento, 28 metros de lagura e 53 metros de altura e capacidade de levar 2435 passageiros Em 15 de abril de 1912 matou 1502 pessoas de um total de 2224 apos colidir com um iceberg quando ia de Southampton - UK para New York","1275a7d1":"# Prepara\u00e7\u00e3o dos dados","27b6cbae":"# Obten\u00e7\u00e3o dos dados","8fdb8551":"### Valores Faltantes","e249a627":"# Explorando\/Analizando os dados\n\n* No dataframe 'train_df' temos 891 exmplos e 11 features mais o taget(survived), analizando nosso dataframe podemo observer que temos alguns campos nulos com 'Age', 'Cabin', 'Embarked' e vemos os tipos de dados contidos no dataframe 5 s\u00e3o 'Object' 5 s\u00e3o 'int64', 2 tipos de features s\u00e3o 'float64'\n\n* Analizando o DataFrame podemos selecinar quais features s\u00e3o relevantes para nosso projeto","41271056":"* ## Constru\u00e7\u00e3o do modelo\n## Random Forest","9ab8f337":"### Preparar as vari\u00e1veis para o modelo\n","e7159368":"### Recuperando os datasets de treino e teste","4fc701db":"# Importan as bibliotecas","59269182":"Se comparado entre homes e mulheres mulheres teriam maior probabilidade de sobrevivencia\n\nNos gr\u00e1ficos acima podemos ver que a probabilidade de homes sobreviver \u00e9 entre 18 e 30 anos e entre as mulheres a maior chance de sobreviver \u00e9 entre 14 e 40 anos\n\nPara os homens a probabilidade de sobreviver \u00e9 muito baixa entre 5 e 18 anos ao cotr\u00e1rio das mulheres"}}