{"cell_type":{"01dd05e3":"code","6157ea79":"code","f8cabafe":"code","d7ec76ca":"code","4dbac31a":"code","a6cc53bd":"code","372d8b14":"code","66657d97":"code","29ca24d9":"code","d74fccbc":"code","b7c29b12":"code","47b36b3a":"code","82d815dd":"code","51af50b2":"code","ce902838":"markdown","27e971ce":"markdown","978433d3":"markdown","d94071b2":"markdown"},"source":{"01dd05e3":"import pandas as pd\nimport torch\nimport gc\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","6157ea79":"!pip install git+https:\/\/github.com\/huggingface\/transformers\/","f8cabafe":"from transformers import (AutoModel,AutoModelForMaskedLM,AutoModelForSequenceClassification, AutoTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling,Trainer, TrainingArguments)","d7ec76ca":"!pip install deepspeed","4dbac31a":"import deepspeed","a6cc53bd":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntext  = '.'.join(train_data.excerpt.tolist() + test_data.excerpt.tolist())\n\nwith open('excerpt.txt','w') as f:\n    f.write(text)","372d8b14":"model_name = 'roberta-large'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('.\/clrp_roberta_large_ds')","66657d97":"os.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '9998'\nos.environ['RANK'] = \"0\"\nos.environ['LOCAL_RANK'] = \"0\"\nos.environ['WORLD_SIZE'] = \"1\"\n# os.environ['MAX_JOBS'] = \"4\"","29ca24d9":"deepspeed_config = {\n    \"fp16\": {\n        \"enabled\": False,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 0,\n        \"offload_optimizer\": {\n         \"device\": \"cpu\",\n         \"pin_memory\": True\n     },\n     \"offload_param\": {\n        \"device\": \"cpu\",\n        \"pin_memory\": True\n        },\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": True,\n        \"cpu_offload\": True\n    },\n\n    \"zero_allow_untested_optimizer\": True,\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\" : \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\"\n        }\n    },\n    \"sparse_attention\": {\n        \"mode\": \"fixed\",\n        \"block\": 16,\n        \"different_layout_per_head\": True,\n        \"num_local_blocks\": 4,\n        \"num_global_blocks\": 1,\n        \"attention\": \"bidirectional\",\n        \"horizontal_global_attention\": False,\n        \"num_different_global_patterns\": 4,\n        \"num_random_blocks\": 0,\n        \"local_window_blocks\": [4],\n        \"global_block_indices\": [0],\n        \"global_block_end_indices\": None,\n        \"num_sliding_window_blocks\": 3\n  },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": False,\n    \"train_micro_batch_size_per_gpu\" : 'auto',\n    \"gradient_clipping\": \"auto\",\n    \"prescale_gradients\" : False\n}\n","d74fccbc":"dataset = LineByLineTextDataset(\n    tokenizer = tokenizer,\n    file_path = \"excerpt.txt\",\n    block_size = 256,\n)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer = tokenizer,\n    file_path = \"excerpt.txt\",\n    block_size = 256)","b7c29b12":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer = tokenizer,mlm = True, mlm_probability = 0.15\n)\n\ntraining_args = TrainingArguments(\n    num_train_epochs = 5,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    evaluation_strategy = 'epoch',\n    save_total_limit = 1,\n    #     eval_steps = 66,\n    save_steps = 268,\n    metric_for_best_model = 'eval_loss',\n    greater_is_better = False,\n#     gradient_accumulation_steps = 1,\n    load_best_model_at_end = True,\n    prediction_loss_only = True,\n    report_to = \"none\",\n    output_dir = \".\/clrp_roberta_base_trainer\",\n    overwrite_output_dir = True,\n    ## DeepSpeed Args\n    max_grad_norm = 1.0,\n    local_rank = 0,\n    ## optimizer\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.999,\n    adam_epsilon = 1e-8,\n    weight_decay = 1e-7,\n    ## learning rate scheduler\n#     warmup_steps = 0,\n#     warmup_max_lr = 2e-5,\n    fp16 = False,\n    learning_rate = 3e-5,)\n#     deepspeed = deepspeed_config)\n\ntraining_args._setup_devices","47b36b3a":"trainer = Trainer(\n    model = model,\n    args=training_args,\n    data_collator = data_collator,\n    train_dataset = dataset,\n    eval_dataset = valid_dataset)","82d815dd":"trainer.train()","51af50b2":"trainer.save_model('.\/clrp_roberta_large_ds')","ce902838":"#### The main reason to use this library in this competetion is the memory efficiency that it provides.\n#### Even with single GPU we'll be able to train bigger and better SOTA models\n#### Use larger models and increased batch sizes without Out Of Memory errors","27e971ce":"## Finetuning : In Progress\n## Inference : In Progress","978433d3":"## What is DeepSpeed?\n### DeepSpeed is a deep learning optimization library. It's used for efficient, effective and easy distributed training.","d94071b2":"# In this Notebook we use DeepSpeed to pretrain RoBERTa"}}