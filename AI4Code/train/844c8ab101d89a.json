{"cell_type":{"9e224c2f":"code","1966adfe":"code","628d4511":"code","2d974817":"code","6238a69c":"code","fe51682f":"code","1380eb73":"code","9fe5b7cd":"code","ede1776a":"code","38294f4e":"code","a8c94f96":"code","5c659346":"code","c71124e0":"code","7154c69c":"code","b543ae2e":"code","2d4190c8":"code","b99bfb2c":"code","cef73d2d":"code","529ecff2":"code","c0c288d8":"code","c001e1a3":"code","a51afca7":"code","089d33f8":"code","b7166d29":"code","c8b8f71d":"code","976e43fb":"code","d65c67ba":"code","e356d3cb":"code","69e013c0":"code","c643104f":"code","bcb18130":"code","d009fc6f":"code","52e92da5":"code","abb0503c":"code","2bb6face":"code","b44a4dcf":"code","778604ba":"code","00c5935b":"code","6ce554f1":"markdown","9e31da74":"markdown","000a1f3d":"markdown","af6500f3":"markdown","f1bede8b":"markdown","735054fa":"markdown","980916f9":"markdown","981daf33":"markdown","8c76b9a0":"markdown","75cb5def":"markdown","6474a66f":"markdown","5a77c206":"markdown","c40eaf2d":"markdown","17a1fc7d":"markdown","3569e6cf":"markdown","a575e021":"markdown","2402925e":"markdown","49909a75":"markdown","34d28ccb":"markdown"},"source":{"9e224c2f":"import os\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1966adfe":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score","628d4511":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","2d974817":"# Load the train dataset\ntrain_df = pd.read_csv('..\/input\/fake-news\/train.csv')\n\n# Load the test dataset\ntest_df = pd.read_csv('..\/input\/fake-news\/test.csv')","6238a69c":"train_df.head(10)","fe51682f":"train_df.shape","1380eb73":"# Assign nan in place of blanks in the text column\n\ntrain_df['text'] = train_df['text'].str.strip()\ntrain_df['text'] = train_df['text'].replace(r'^\\s*$', np.nan, regex=True)","9fe5b7cd":"# Remove all rows where complaints column is nan\ntrain_df.dropna(subset=['text'], inplace=True)","ede1776a":"train_df.duplicated(subset=[\"text\"]).value_counts()","38294f4e":"dup = train_df[train_df.duplicated(subset=[\"text\"])]\ndup.head()","a8c94f96":"# print one duplicate entry\ntrain_df[train_df['text'] == dup.loc[480]['text']]","5c659346":"# drop duplicated data\ntrain_df = train_df.drop_duplicates(subset={\"text\"}, keep='first', inplace=False)\ntrain_df.shape","c71124e0":"# Checking for missing values in the dataset\ntrain_df.isnull().sum()","7154c69c":"# dropping the nan values\ntrain_df = train_df.fillna('')","b543ae2e":"stopword_list = stopwords.words('english')\nprint(stopword_list)","2d4190c8":"!pip install contractions","b99bfb2c":"import contractions\ndef decontracted(sentance):\n    expanded_words = []    \n    for word in sentance.split():\n      # using contractions.fix to expand the shotened words\n      expanded_words.append(contractions.fix(word))   \n\n    expanded_text = ' '.join(expanded_words)\n    return expanded_text","cef73d2d":"def sentance_clean(sentance):\n    # change sentence to lower case\n    sentance = sentance.lower()\n    # removing URL from sentence\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    # removing HTML tags\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    # removing contraction of words from sentence   # call decontracted funtion for it\n    sentance = decontracted(sentance)\n    # removing digits\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    # removing special character\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    return sentance","529ecff2":"# Use Stemming \nps = PorterStemmer()\n\n# Performing the preprocessing steps on all messages\ndef preprocess(document):\n    preprocessed_reviews = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(document):\n        # call sentance_clean function to clean text\n        sentance = sentance_clean(sentance)\n        # tokenize into words\n        words = word_tokenize(sentance)\n        # remove stop words\n        tokens = [ps.stem(word) for word in words if word not in stopword_list]\n\n        # join words to make sentence\n        sentance = \" \".join(tokens).strip()\n\n        preprocessed_reviews.append(sentance)\n        \n    return preprocessed_reviews","c0c288d8":"%%time\ncorpus = preprocess(train_df['text'])","c001e1a3":"print(\"Before preprocess\\n\", train_df['text'][1])\nprint(\"***\"*40)\nprint(\"After preprocess\\n\", corpus[1])","a51afca7":"train_df['text'] = corpus","089d33f8":"# Seperating the data and the label \nx = train_df['text'].values\ny = train_df['label'].values","b7166d29":"tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\nX = tfidf_vectorizer.fit_transform(x)","c8b8f71d":"# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","976e43fb":"print(\"Train size:\", X_train.shape)\nprint(\"Test size:\", X_test.shape)","d65c67ba":"def plot_confusion_matrix(y_actual, y_pred):\n    '''\n    This method plots confusion matrix.\n    '''\n    classes = ['Fake News', 'Real News']\n    tick_marks = np.arange(len(classes))\n\n    accuracy = accuracy_score(y_actual, y_pred)\n    print(\"Accuracy score:\", \"{:2.3}\".format(accuracy))\n\n    conf_matrix = confusion_matrix(y_actual, y_pred)\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(conf_matrix, cmap=plt.cm.Reds, alpha=0.3)\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n    \n    plt.tight_layout()\n    plt.xticks(tick_marks , classes, rotation=0)\n    plt.yticks(tick_marks , classes)\n    plt.xlabel('Predictions')\n    plt.ylabel('Actuals')\n    plt.title('Confusion Matrix', fontsize=12)\n    plt.show()","e356d3cb":"metrics_dict = {}","69e013c0":"#  logistic regression object\nmodel = LogisticRegression(C=5)\nmodel.fit(X_train, y_train)","c643104f":"# predict on train data\nX_train_predict = model.predict(X_train)\n# train_accuracy\ntrain_accuracy = accuracy_score(y_train, X_train_predict)\n\n# predict on test data\nX_test_predict = model.predict(X_test)\n# test_accuracy\ntest_accuracy = accuracy_score(y_test, X_test_predict)","bcb18130":"accuracy = accuracy_score(y_test, X_test_predict)\nLR_TF_TFIDF = {'Vectorizer': 'TF-IDF', 'Algorithm': 'Logistic_Regression_1', \n               'Train Accuracy':train_accuracy, 'Test Accuracy':test_accuracy}\nmetrics_dict['Logistic_Regression_TF_IDF'] = LR_TF_TFIDF","d009fc6f":"# plot confusion matrix on test\nplot_confusion_matrix(y_test, X_test_predict)","52e92da5":"#  logistic regression object\nnb_model = MultinomialNB(alpha=0.8)\nnb_model.fit(X_train, y_train)","abb0503c":"# predict on train data\nX_train_predict = nb_model.predict(X_train)\n# train_accuracy\ntrain_accuracy = accuracy_score(y_train, X_train_predict)\n\n# predict on test data\nX_test_predict = nb_model.predict(X_test)\n# test_accuracy\ntest_accuracy = accuracy_score(y_test, X_test_predict)","2bb6face":"NB_TF_TFIDF = {'Vectorizer': 'TF-IDF', 'Algorithm': 'Naive_Bayes_1', \n               'Train Accuracy':train_accuracy, 'Test Accuracy':test_accuracy}\nmetrics_dict['Naive_Bayes_TF_IDF'] = NB_TF_TFIDF","b44a4dcf":"# plot confusion matrix on test\nplot_confusion_matrix(y_test, X_test_predict)","778604ba":"def metrics_table(metrics_dict):\n    '''\n    Metrics evolution table.\n    '''   \n    df = []\n    for key, value in metrics_dict.items():\n        df.append(metrics_dict[key])\n    df = pd.DataFrame(df)\n    df.set_index('Algorithm', inplace=True)\n    return df","00c5935b":"metrics_table(metrics_dict)","6ce554f1":"## 5.Model","9e31da74":"## 2.Data Preparation","000a1f3d":"### 4.1 TF-IDF Vectorizer","af6500f3":"**Drop duplicated data**","f1bede8b":"### 2.1 Remove empty `text news`","735054fa":"## 4. Vectorization","980916f9":"## Business Problem:\n\n#### Problem Statemtent:\nBuild a system to identify unreliable news articles\n\n#### Dataset Description\n1. [Dataset is taken from Kaggle](https:\/\/www.kaggle.com\/c\/fake-news)\n\n2. `train.csv:` A full training dataset with the following attributes:\n    - **id:** unique id for a news article\n    - **title:** the title of a news article\n    - **author:** author of the news article\n    - **text:** the text of the article; could be incomplete\n    - **label:** a label that marks the article as potentially unreliable\n        - **1:** Fake news\n        - **0:** Real news\n\n3. `test.csv:` A testing training dataset with all the same attributes at train.csv without the label.","981daf33":"## 1.Load dataset","8c76b9a0":"### 2.3 Replacing the null values","75cb5def":"**`label` column is target.**\n\n- 1 = Fake News\n- 0 = Real News","6474a66f":"### Conclusion:\n- Logistic regression with TF-IDF model are giving good reault.","5a77c206":"## 5.1 Logistic Regression","c40eaf2d":"## 6. Evolution Metrics","17a1fc7d":"### 2.2 Checking duplicates\n\n`News text` might contain duplicate entries. So, we need to remove the duplicate entries so that we get unbiased data for Analysis.","3569e6cf":"## 5.2 Naive Bayes","a575e021":"## 3. Preprocessing Text\n\nWe will perform the below preprocessing tasks:\n- Convert everything to lowercase\n- Remove HTML tags\n- Remove URL from sentence\n- Contraction mapping\n- Eliminate punctuations and special characters\n- Remove stopwords\n- Remove short words","2402925e":"# Fake News Classifier\nBuild a system to identify unreliable news articles","49909a75":"## Importing required Libraries","34d28ccb":"As we can see that we have few duplicate entries for `text` column"}}