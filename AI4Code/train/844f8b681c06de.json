{"cell_type":{"538f0dbd":"code","dd37c661":"code","0cf5d79d":"code","17288834":"code","6f11523a":"code","eecd8ba2":"code","de21aaa2":"code","0d5fdcd1":"code","85eccd97":"code","e785dd59":"code","a2f05fa1":"code","091d3c6b":"markdown","6cab56c4":"markdown","6d7d6b4a":"markdown","e58fc8ee":"markdown","a20618c6":"markdown","f9362c80":"markdown","246efedd":"markdown","39d09bf7":"markdown","ac54d572":"markdown","93482ddb":"markdown","e38805ee":"markdown","c8fd9294":"markdown","fbcbcb28":"markdown"},"source":{"538f0dbd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numpy.linalg import inv\nimport statsmodels.api as sm\nimport pylab as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt","dd37c661":"x = np.arange(0,20)\n# generate random noise (and the true y)\ny = x + 10*np.random.random((1,20)) - 1","0cf5d79d":"plt.scatter(x,y)","17288834":"# calculate regression paramters\nA = np.array([ x, np.ones(20)])\nw = np.linalg.lstsq(A.T, y.T)[0] \n# plotting the line\nline = w[0]*x + w[1]\npl.plot(line, 'r-', y.T, 'o')\npl.title('Linear Regression')\npl.xlabel('x')\npl.ylabel('y')\npl.show()","6f11523a":"# coeff\nw","eecd8ba2":"X = x.reshape(-1, 1)\nY = y.reshape(20,1)\nfrom sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(X,Y)\nprint (linear_regression.coef_, linear_regression.intercept_)","de21aaa2":"\nw_fake = np.array([[3],[4.0746206 ]])\n# plotting the line\ny_pred = w_fake[0]*x + w_fake[1]\npl.plot(y_pred, 'r-', y.T, 'o')\npl.title('Linear Regression')\npl.xlabel('x')\npl.ylabel('y')\npl.show()","0d5fdcd1":"error = np.sum (y - y_pred)**2\nprint (error)","85eccd97":"listOfWeights = np.linspace(0.5, 3.0, num=100) # generating a selection of 100 possible parameters","e785dd59":"# collecting the error for each of the selection of paramenters\nerrorList = []\n\nfor weight in listOfWeights:\n    y_pred_temp = weight*x + w[1]\n    error_temp = np.sum (y - y_pred_temp)**2\n    errorList.append(error_temp)\n    \nerrorArray   = np.array(errorList)\n","a2f05fa1":"plt.scatter(listOfWeights, errorArray)","091d3c6b":"In this notebook I wanted to practice **why** the error function reflects a **convex** curve.\n\n\n## Ordinary Least Square Method\nLinear regression method approximates the relationship between X (feature) and Y(true target) to a line. OLS (Ordinary Least Square) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function. (Wikipedia)\n![](https:\/\/i.imgur.com\/306wvA1.png)","6cab56c4":"Ok. The coeff are the same. ","6d7d6b4a":"I can now print the plot","e58fc8ee":"I can now look at the relation between the parameters and the error. I will look only at one parameter of the two, and check if I will get a convex curve.","a20618c6":"In this notebook I wanted to practice how the following equation of the error function reflects a convex curve.\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f607606c60752eff21a8fd43c12ced695975b61e)","f9362c80":"by using the above \"bad\" parameters I will get a higher sum or the errors","246efedd":"Using Linear regression method I can plot the line that best fit the data, as in the following image.","39d09bf7":"One key point in Linear Regression method is that the relation of the unknown parameters and the error follow a convex curve which is differentiable so that we can find the global minimum by applying calculus.\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*fU8XFt-NCMZGAWND.)","ac54d572":"The graph shows the convex shape of the error function. Taking the partial derivative of the error function w.r.t one paramenter and set it to zero, it will find the optimal minimum of this function for that parameter. Visually the lowest point of the curve seems to match the value on the parameter found by the linear regression method.","93482ddb":"Let\u2019s say we have few inputs and outputs. And we plot these scatter points in 2D space, we will get something like the following image.","e38805ee":"I can see the best parameters for the line by printing w","c8fd9294":"Just to check that I did the math correctly. I test the same task with Scikit Learn library ","fbcbcb28":"Those coefficients are the best for this particular task and if I choose other coefficients I would get a worse result. A worse fit line. I can try w = 3 and 4.07"}}