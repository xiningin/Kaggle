{"cell_type":{"b3cd9477":"code","670fa40f":"code","8d665fef":"code","e469d13b":"code","7786fa00":"code","df4952d7":"code","910fb8ff":"code","2400d58b":"code","95e26023":"code","5a99346d":"code","026d98ba":"code","98ff10c9":"code","5918b5a0":"code","70615eac":"code","650607c4":"code","8b7049c7":"code","0a842c02":"code","6cb4b5be":"code","cad00e05":"code","56a4829c":"code","863b6429":"code","48606aa9":"code","f6516e01":"code","256d041d":"code","1886cc4a":"code","aa9e0953":"code","834b91df":"code","c979da96":"code","1a0b232f":"markdown","23519c3c":"markdown","bc65197b":"markdown","124131f8":"markdown","d37bf94b":"markdown","74ce3c55":"markdown","d8807a69":"markdown","bcb5a156":"markdown","28ce88dc":"markdown","a0ce5d18":"markdown","bba6a6be":"markdown","b2d93997":"markdown","4455fb20":"markdown","5cd1a074":"markdown","9a65c76a":"markdown","89a4d9a3":"markdown","54d47bff":"markdown","7c4c0ef1":"markdown","c0b68c9b":"markdown","4a2fb984":"markdown","ada56b08":"markdown","8eac3baa":"markdown","3d41c16a":"markdown"},"source":{"b3cd9477":"from IPython.display import HTML\nHTML('<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/PGzT3cTPah8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","670fa40f":"import numpy as np\nimport random\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nfrom timeit import timeit\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n\n#color\nfrom colorama import Fore, Back, Style\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.models as Models\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","8d665fef":"def seed_everything(seed): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","e469d13b":"ROOT = '..\/input\/lish-moa'\n\ntrain_features_df = pd.read_csv(f'{ROOT}\/train_features.csv')\nprint(f'Train data has {train_features_df.shape[0]} rows and {train_features_df.shape[1]} columnns and looks like this:')\n","7786fa00":"train_features_df.sample(10)","df4952d7":"# CHECK FOR DUPLICATES & DEAL WITH THEM\n# keep = False: All duplicates will be shown\ndupRows_df = train_features_df[train_features_df.duplicated(subset = ['sig_id', 'cp_type'], \n                                                            keep = False )]\ndupRows_df.head()\nprint(f'There are {dupRows_df.shape[0]} duplicates.')","910fb8ff":"test_features_df = pd.read_csv(f'{ROOT}\/test_features.csv')\nprint(f'Test data has {test_features_df.shape[0]} rows and {test_features_df.shape[1]} columnns, has no duplicates and looks like this:')\ntest_features_df.head()","2400d58b":"uniq_test = test_features_df['sig_id'].unique()\nuniq_train = train_features_df['sig_id'].unique()","95e26023":"def intersec_using_sets_intersection(uniq_test, uniq_train):\n    return bool(set(uniq_test) & set(uniq_train))","5a99346d":"def intersec_using_generator_expression(uniq_test, uniq_train):\n    return any(i in uniq_test for i in uniq_train)","026d98ba":"def time_sets():\n    return intersec_using_sets_intersection(uniq_test, uniq_train)\n\ndef time_generator():\n    return intersec_using_generator_expression(uniq_test, uniq_train)\n\nduration_sets = timeit(time_sets, number = 3)\nduration_generator = timeit(time_generator, number = 3)\n\nprint(f\"Using the set-intersection takes {duration_sets \/ 3:.5f} sec, while the generator expression takes {duration_generator \/ 3:.3f} sec. Using set-intersection is {duration_generator\/duration_sets:.0f} times faster!\" )\n","98ff10c9":"intersec_using_sets_intersection(uniq_test, uniq_train)","5918b5a0":"## CHECK SUBMISSION FORMAT\nsub_df = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\nprint(f\"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns.\")","70615eac":"sub_df.head()","650607c4":"def preprocess(df):\n    # make a copy to not change original df\n    _df = df.copy()\n    # transform string containing columns to numeric values\n    _df.loc[:, 'cp_type'] = _df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    _df.loc[:, 'cp_dose'] = _df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})    \n    \n    # delete sig_id column as we dont need it anymore\n    del _df['sig_id']\n       \n    return _df","8b7049c7":"# use top features from https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2\ntop_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]","0a842c02":"## use top features extracted by feature engineering above\nX_feature_cols = top_feats\nY_feature_cols = top_feats","6cb4b5be":"## GET TRAINING DATA AND TARGET VALUE\n\n# get target values\ntrain_targets_df = pd.read_csv(f\"{ROOT}\/train_targets_scored.csv\")\ndel train_targets_df['sig_id']\nY_train = train_targets_df.astype(float).values\n\n\n# get training & test data, apply preprocessing\nX_train = preprocess(train_features_df).astype(float).values\nX_test = preprocess(test_features_df).astype(float).values\n\n# extract feature columns\nX_feature_cols = preprocess(train_features_df).columns\nY_feature_cols = train_targets_df.columns","cad00e05":"######## CONFIG ########\n# be careful, the resulsts are VERY SEED-DEPENDEND!\nseed_everything(2020)\n\n\n### Basics for training:\nNFOLDS = 7\nEPOCHS = 65\nBATCH_SIZE = 32\n\n### Optimizers\n# choose \"ADAM\" or \"SGD\"\nOPTIMIZER = \"SGD\"\n\n### Learning Rate Scheduler\ndef get_lr_callback(batch_size = 64, plot = False):\n    \"\"\"Returns a lr_scheduler callback which is used for training.\n    Feel free to change the values below!\n    \"\"\"\n    lr_start   = 0.001\n    lr_max     = 0.001 * BATCH_SIZE # higher batch size --> higher lr\n    lr_min     = 0.00001\n    # 30% of all epochs are used for ramping up the LR and then declining starts\n    lr_ramp_ep = EPOCHS * 0.3\n    lr_sus_ep  = 0\n    lr_decay   = 0.9\n\n    def lr_scheduler(epoch):\n            if epoch < lr_ramp_ep:\n                lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n\n            elif epoch < lr_ramp_ep + lr_sus_ep:\n                lr = lr_max\n\n            else:\n                lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n            return lr\n    \n    if plot == False:\n        # get the Keras-required callback with our LR for training\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose = False)\n        return lr_callback \n    \n    else: \n        return lr_scheduler\n    \n# plot & check the LR-Scheulder for sanity-check\nlr_scheduler_plot = get_lr_callback(batch_size = 64, plot = True)\nrng = [i for i in range(EPOCHS)]\ny = [lr_scheduler_plot(x) for x in rng]\nplt.plot(rng, y)\nprint(f\"Learning rate schedule: {y[0]:.3f} to {max(y):.3f} to {y[-1]:.3f}\")\n\n\n# logging & saving\nLOGGING = True\n\n# defining custom callbacks\nclass LogPrintingCallback(tf.keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs = None):\n        print(\"Training started\")\n        self.val_loss = [] \n        #self.val_score = [] not used for now\n        \n    def on_epoch_end(self, epoch, logs = None):\n        self.val_loss.append(logs['val_loss']) \n        # self.val_score.append(logs['val_score']) # not used for now\n        # logging: reduce printing of log-messages\n        if epoch % int((EPOCHS\/10)) == 0 or epoch == (EPOCHS -1 ):\n            print(f\"The average val-loss for epoch {epoch} is {logs['val_loss']:.4f}\") \n            \n    def on_train_end(self, lowest_val_loss, logs = None):\n        # get index of best epoch\n        best_epoch = np.argmin(self.val_loss)\n        # get score in best epoch\n        lowest_loss = self.val_loss[best_epoch]\n        print(f\"Stop training, best model was found and saved in epoch {best_epoch + 1} with loss: {lowest_loss}.\"\n              f\" Final results in this fold (last epoch):\") \n        \n        \ndef get_checkpont_saver_callback(fold):\n    checkpt_saver = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold,\n        monitor = 'val_loss',\n        verbose = 0,\n        save_best_only = True,\n        save_weights_only = True,\n        mode = 'min',\n        save_freq = 'epoch')\n    \n    return checkpt_saver","56a4829c":"import tensorflow_addons as tfa\n\ndef get_model(num_inputs, optimizer = 'ADAM', lr = 0.01):\n    \"Creates and returns a model\"    \n    # instantiate optimizer\n    optimizer = tf.keras.optimizers.Adam(lr = lr) if optimizer == 'ADAM' else tf.keras.optimizers.SGD(lr = lr)\n    model = tf.keras.Sequential([\n    Layers.Input(num_inputs),\n    Layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(6144, activation = \"elu\")),\n    Layers.BatchNormalization(),\n    Layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(6144, activation = \"elu\")),\n    Layers.BatchNormalization(),\n    Layers.Dropout(0.4),    \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = \"elu\")),\n    Layers.BatchNormalization(),\n    Layers.Dropout(0.2),    \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = \"sigmoid\"))\n    ])\n        \n    model.compile(loss = 'binary_crossentropy', \n                  optimizer = optimizer, \n                  metrics=[\"accuracy\", \"AUC\"])\n    \n    return model","863b6429":"## Create neural Network\n# the number of input neurons equals the number of columns in train,\n# which is identically to the number of features\/columns in test\nneuralNet = get_model(num_inputs = len(X_feature_cols), optimizer = OPTIMIZER, lr = 0.5)\nneuralNet.summary()","48606aa9":"## Non-Stratified KFold-split (can be further enhanced with stratification!)\n\"\"\"K-Folds cross-validator.\nProvides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default).\nEach fold is then used once as a validation while the k - 1 remaining folds form the training set.\n\"\"\"\n\n# set all columns which we need to predict to zero to initialize\ntrain_preds_df = train_targets_df.copy()\ntrain_preds_df.loc[:, Y_feature_cols] = 0\nsub_df.loc[:, Y_feature_cols] = 0\n\nfold = 0\nfor n, (train_idx, val_idx) in enumerate(KFold(n_splits = NFOLDS, shuffle = True).split(X_train)):\n    fold += 1\n    print(f\"FOLD {fold}:\")\n    \n    # LR Scheduler\n    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                                  factor = 0.5,\n                                                                  patience = 2,\n                                                                  verbose = 1,\n                                                                  epsilon = 1e-4,\n                                                                  mode = 'min')\n    \n    callbacks = [reduce_lr_loss]\n    # callbacks = [get_lr_callback(BATCH_SIZE)] un-comment if you want to use the learning-rate scheduler\n    \n    # callbacks: logging & model saving with checkpoints each fold\n    if LOGGING == True:\n        callbacks +=  [get_checkpont_saver_callback(fold),                     \n                     LogPrintingCallback()]\n\n    # build and train model\n    model = get_model(num_inputs = len(X_feature_cols), optimizer = OPTIMIZER, lr = 0.09)\n    \n    history = model.fit(X_train[train_idx], \n              Y_train[train_idx],\n              batch_size = BATCH_SIZE, \n              epochs = EPOCHS, \n              validation_data = (X_train[val_idx], Y_train[val_idx]), \n              callbacks = [get_checkpont_saver_callback(fold),                     \n                     LogPrintingCallback()],\n              verbose = 0) \n    \n    # evaluate\n    print(\"Train:\", model.evaluate(X_train[train_idx], Y_train[train_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    print(\"Val:\", model.evaluate(X_train[val_idx], Y_train[val_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    \n    ## Load best model to make pred\n    model.load_weights('fold-%i.h5'%fold)\n    \n    # use model to predict unseen val-data (used later for OOF evaluation)\n    train_preds_df.loc[val_idx, Y_feature_cols] = model.predict(X_train[val_idx],\n                                         batch_size = BATCH_SIZE,\n                                         verbose = 0)\n    \n    # predict test_data (later used for submission) \n    # and average the predictions over all folds\n    print(\"Predicting Test...\")\n    sub_df.loc[:,Y_feature_cols] += model.predict(X_test, batch_size = BATCH_SIZE, verbose = 0) \/ NFOLDS","f6516e01":"## PLOT results\n# fetch results from history\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\n# create subplots\nplt.figure(figsize = (20,5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\n#plt.ylim(0.05, 0.4) \nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\n# limit y-values for beter zoom-scale\nplt.ylim(0.1 * np.mean(val_loss), 1.8 * np.mean(val_loss)) \nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","256d041d":"from sklearn.metrics import log_loss\n\nmetrics = []\nfor _target in train_targets_df.columns:\n    metrics.append(log_loss(train_targets_df.loc[:, _target], train_preds_df.loc[:, _target]))\nprint(f'OOF Metric: {np.mean(metrics)}')","1886cc4a":"sub_df.loc[test_features_df['cp_type']==1, Y_feature_cols] = 0","aa9e0953":"sub_df.head()","834b91df":"sub_df.describe().T","c979da96":"sub_df.to_csv('submission.csv', index = False)","1a0b232f":"### Submission","23519c3c":"Let's start seeding everything to make results somewhat reproducible. Anyway, in keras it is quite hard to get 100% reproducible results.","bc65197b":"In this section you can configure the following:\n* Features used for training\n* Basic training setup: BATCH_SIZE and EPOCHS,\n* Optimizers, Learning-Rate-Schedulers incl. Learning Rate start- & endpoint\n* Custom Logging Callback\n* Checkpoint-Saving Callback\n\nThe Learning-Rate scheduler below is inspired by Chris great [Melanoma-detection notebook](https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords).  \nFeel free to experiment with the scheduler and it's max\/min and decay values.\n\n### Ever wondered why lr_max is scaled by BATCH_SIZE and therefore bigger for larger batches?\nThe reason for this is the following: the larger the BATCH_SIZE, the more averaged & smoothened a step of gradient decent is and the bigger our confidence in the *direction* of the step is. As there is less \"randomness\" in a huge averaged batch (compared with for example Stochastic Gradient Decent (=SGD) with batch size = 1) and our confidence in the direction is higher, the learning rate can be bigger to advance fast to the optimum.","124131f8":"Note, that we do not get rid of the data from the ctl_vehicle just yet. We will try this in a later version of the notebook.","d37bf94b":"This is a rather large dataset with respect to the number of roughly ~ 900 columns\/features. As [headsortails](https:\/\/www.kaggle.com\/headsortails) describes perfectly: \n> From the data description we learn that features starting with \u201cg-\u201d encode gene expression data (there are 772 of those), and features starting with \u201cc-\u201d (100 in total) show cell viability data.\n> \n> In addition, we have 3 \u201ccp_\u201d features: cp_type incidates the sample treatment, while cp_time and cp_dose encode the duration and dosage of the treatment.\n> \n> The sig_id is the unique primary key of the sample.\n\n\nThis clearly screams for feature selection & dimensionality-reduction methodologies, which I will tackle in a later version of this notebook.","74ce3c55":"\nThe first big challenge is data wrangling: \nWe could see in the ```train_features_df``` a column called ```cp_type```, which indicates whether a real medical compound (cp_vehicle) or with a control perturbation (ctrl_vehicle) was used. Control perturbations have no MoAs, which is why we are not going to use them in training.\n\nTo preprocess the data with this knowledge, we are going to use a modified version of some nice code introduced [here](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2) using the [pandas df.map function](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.map.html): \n","d8807a69":"# Model & Loss\nIn this section we are going to define the loss & a first model.\nFirst we are taking care of the model evaluatiation metric and the loss. We are trying to minimize the following, which is mean log-loss. A nice (and SHORT! ;)) explanation about log-loss can be found [here](https:\/\/www.kaggle.com\/dansbecker\/what-is-log-loss).\n**(PLEASE CORRECT MY IF MY UNDERSTANDING IS NOT PRECISELY CORRECT)**\n\n![image.png](attachment:image.png)\n\n\n\n\n","bcb5a156":"# Update history:\n\n### This notebook is largely derived from my notebook here: https:\/\/www.kaggle.com\/chrisden\/6-82-quantile-reg-lr-schedulers-checkpointsNew and can easily be modified with CV-backed Random Grid Hyperparameter Search by using [this](https:\/\/www.kaggle.com\/chrisden\/6-82x-cv-backed-hyperp-gridsearch-quantile-reg).\nV20: LR Optimization  \nV19: Further improvements, logging, hyperparameters  \nV2-5: Corrections, Adoptions, introducing plotting & evaluation of results  \nV1 Initial commit \n","28ce88dc":"## Neural Network Model\nIn this section we build an initial neural Network.\n\nFor the architecture: It's good practice to use numbers of units following the schema 2^x, with x element of N (= resulting in 1, 2, 4, 8, 16, 32, 64, 128,..).  \nWe are going to use dropout for regularization and not a too broad and deep network, as the training data is very limited.\n\n### Normalization & Regularization\nWe are going to use weight-normalization (link to the paper, click [here](https:\/\/arxiv.org\/abs\/1602.07868)) from ```tensorflow_addons``` to support faster convergence. \nThe authors describe the method like this:\n> Weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.\n\nAdditionally we gain more robustness for the choosing of the hyperparameter learning rate. Cited from page 3:\n> Empirically, we find that the ability to grow the norm ||v|| makes optimization of neural networks\n> with weight normalization very robust to the value of the learning rate: If the learning rate is too\n> large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate\n> is reached.\n\n### Activation function\nAs the given task without the usage of images is not very compute-intensive (you don't need a GPU, CPU will do), we will change the activation-function from 'relu' to 'elu'.\nFor more info you can read [here.](https:\/\/mlfromscratch.com\/activation-functions-explained\/#elu), below you can find a short summary:\n\n**Pros**\n* Avoids the \"dead ReLu\" problem: ReLus provides activation-values & gradients of 0 for negative input values\n* Produces activations for negative inputs instead of letting them be zero when calculating the gradient.\n* Produces negative outputs, which helps the network nudge weights and biases in the right directions for negative inputs, too.\n\n**Cons**\n* Introduces longer computation time, because of the exponential operation included.\n* Does not avoid the exploding gradient problem.","a0ce5d18":"## Load all dependencies you need\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","bba6a6be":"## GPU or CPU?\nThis notebook can be run using GPU or CPU. Using GPU is roughly 5+ times faster than CPU.","b2d93997":"# Quick intro\n\n\n## Competition description\nThe Connectivity Map, a project within the Broad Institute of MIT and Harvard, together with the Laboratory for Innovation Science at Harvard (LISH), presents this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.\n\n### What is the Mechanism of Action (MoA) of a drug? And why is it important?\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\n### How do we determine the MoAs of a new drug?\n\nOne approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n\nIn this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells\u2019 responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.\n\nAs is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.\n \n### What should I expect the data format to be & what am I predicting?\nIn this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.\n\n**Two notes:**\n\nthe training data has an additional (optional) set of MoA labels that are not included in the test data and not used for scoring.\nthe re-run dataset has approximately 4x the number of examples seen in the Public test.\n**Files**\n```train_features.csv```  - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n\n``` train_targets_scored.csv```  - The binary MoA targets that are scored.  \n``` train_targets_nonscored.csv```  - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.  \n``` test_features.csv```  - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.  \n``` sample_submission.csv```  - A submission file in the correct format.  \n\nThe final submission-file should contain a header and have the following format:\n\n\n## Submission File\nYou must predict a probability of a positive target for each sig_id-<MoA> pair. The id used for the submission is created by concatenating the sig_id with the MoA target for which you are predicting. The file should have a header and be in the following format:\n``` \nsig_id,11-beta-hsd1_inhibitor,ace_inhibitor,...,wnt_inhibitor\nid_000644bb2,0.32,0.01,...,0.57\nid_000a6266a,0.88,0.27,...,0.42\netc...\n``` \n\n\n","4455fb20":"## Domain knowledge\n\n### Some domain knowledge can be gained from watching the following video and from additionally watching [here.](https:\/\/www.youtube.com\/watch?v=UMxsZdVrA7A&ab_channel=PharmaStudy)","5cd1a074":"### OOF Evaluation\nI found a great and easy way to evaluate the out-of-fold performance [here](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2) which I adopted in the following. Good values are below 0.016.","9a65c76a":"# Data Wrangling\nIn this section we are going to do all the Data-Wrangling and pre-processing. For this we are going to define some functions and transformations, which then are applied to the data.\nLet's start with a look into the sample submission file.\n","89a4d9a3":"Let's check if there are overlapping ```sig_ids``` in train and in test data. I can think of two (but of course there are many more) fast and clean ways on how to do this:\n1. Converting the unique values to two sets and check for the intersection, or\n2. using a generator expression performing iteration over the lists\nLet's use both of them and compare the speed!","54d47bff":"## <font color='blue'>Thanks a lot for reading, I hope you could gain as much insights from reading this as I got from writing it. If you liked it, an upvote is highly appreciated. If you are interested in more content like this, feel free to follow me! ;)<\/font>","7c4c0ef1":"This is already in a format which we can easily use for making our predictions. The DataFrame has 207 columns, resulting in 207 - 1 (sig_id) = 206 values to predict.","c0b68c9b":"Besides that: there is no intersection\/overlapping between train and test sid_ids as we can see here:","4a2fb984":"# Evaluation & submission\n\n### Check loss and accuracy\nOkay, we made it! Let's evaluate our model, check our stats (Out-Of-Fold log-loss) and submit it! Let's start with some plots.\nThose plots can tell us, whether our model training is working as expected, or if it strongly overfits.  \nThe below **EXAMPLE-IMAGE** is an example of a strongly overfitting model:\n![image.png](attachment:image.png)\nLong before we hit the 10th epoch, the validation loss is increasing again, while the training loss keeps decreasing. We can also clearly ovserve that there is no improvement in our accuracy anymore. \n\nWhat can we do against strongly overfitting models?\nWe could do the following:\n\n* Collect more training data or use augmentation to generate new data: Sadly I have no brilliant idea on how to do this for this specific Kaggle competition.\n* Reduce the network\u2019s size (width andor\/ dept) by removing layers or reducing the number of neurons in the hidden layers\n* Use regularization like LASSO (=Least Absolute Shrinkage and Selection Operator; aka L1 regularization) or Ridge (aka L2 regularization) which results in adding a cost-term to the loss function\n* Use higher dropout-rate in the Dropout-Layers, which will randomly remove more connections by setting them to zero and forcing the network to generalize better (=avoid relying on a limitied number of strong-influence neurons).\n\nBut now check our model's result and evaluate it:","ada56b08":"# <font color='blue'>CONFIG Section <\/font>","8eac3baa":"# First glimpse at the data","3d41c16a":"# Training\nWe we do not have an intersection between training and test data, we don't need to focus on creating leak-free folds to get a robust **cross-validation strategy**. But we should use some stratification for creating Please note, that we  don't  use any  proper stratification currently!\n\nDuring testing several LR-Schedulers & value-combinations, it turned out that the LR-Scheduler was always outperformed by simply using ReduceLROnPlateau, so I am going to use ReduceLROnPlateau by default, but you can easily change it with un-commenting a row in belows-code."}}