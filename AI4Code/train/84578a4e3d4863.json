{"cell_type":{"77c115b1":"code","6ed4119c":"code","0b57c463":"code","7bf3c0aa":"code","98d4be23":"code","83664588":"code","b0f2d5fb":"code","c0b1e97e":"code","7330926a":"code","3d15e91c":"code","31bf17be":"code","aef17571":"code","716a14d6":"code","cff58709":"code","99410474":"code","fd6ca37b":"code","3fd902cf":"code","0222c5ed":"code","f3cf9bf8":"code","7f7db7e9":"code","7912ac7a":"code","01191dd2":"code","178bce38":"code","5b3d568c":"code","164ca4ed":"code","92da3d83":"code","a7da79b6":"code","b10ddf0e":"code","8941938a":"code","889337bb":"code","b465c608":"code","b32bd489":"code","932156ac":"code","d8ea9cf6":"code","99d1610e":"code","901e591c":"code","aeab8d52":"code","a3ff90cb":"code","8125e228":"code","402994d4":"code","ea77a7a5":"markdown","21793e27":"markdown","63564d3e":"markdown","021e742b":"markdown","4c2b3cc0":"markdown","20d13733":"markdown","8518f578":"markdown","e09ad210":"markdown","ca304689":"markdown","29c34cbd":"markdown","4a2a7a0e":"markdown","d2778691":"markdown","90bbb24f":"markdown","782af11e":"markdown","c47e1e8a":"markdown","965ae58d":"markdown","0bf162b8":"markdown","4576b927":"markdown","1f3f1a17":"markdown"},"source":{"77c115b1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6ed4119c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv('..\/input\/retail-store-sales-transactions\/scanner_data.csv')\ndf.head(10)","0b57c463":"df.info()\n\n# the result shows that there's no null values in the dataset. ","7bf3c0aa":"# Date is object, so I convert the string into 'datetime' dtype.\n\ndf['DateTime']= pd.to_datetime(df['Date'])\ndf.info()","98d4be23":"df.nunique()\n\n# unique customer = 22625\n# unique date = 363","83664588":"df.describe()","b0f2d5fb":"group = df.groupby(['DateTime','Customer_ID','Transaction_ID']).Sales_Amount.sum().reset_index()\ngroup.sort_values(by=['Transaction_ID']).head(10)  \n\n# transation_id= 7, the total value is 3.65+8.21 = 11.86","c0b1e97e":"group.head()","7330926a":"print('Min:{}; Max:{}'.format(min(group.DateTime),max(group.DateTime)))","3d15e91c":"import datetime","31bf17be":"# create a hypotheticial snapshot_day data as if we're doing analysis recently \nsnapshot_day = max(group.DateTime) + datetime.timedelta(days=1)\nsnapshot_day ","aef17571":"# Aggregate data at customer level \n\ndatamart = group.groupby('Customer_ID').agg({\n    'DateTime': lambda x:(snapshot_day - x.max()).days,\n    'Transaction_ID' : 'count',\n    'Sales_Amount':'sum'\n})\n\ndatamart.rename(columns = {'DateTime':'Recency',\n                          'Transaction_ID':'Frequency',\n                          'Sales_Amount':'MonetaryValue'}, inplace=True)\n\ndatamart.head()","716a14d6":"# Create the label, for Recency, the smaller the better, so the rank goes from 4,3,2,1\n# meaning when the recency is very small, it will be rank earlier, in our case, giving higher score\n\nr_labels = range(4,0,-1)\n\n# Create a new dataset including quartiles values\ndatamart_quartile = datamart.copy(deep=True)\ndatamart_quartile['R']= pd.qcut(datamart['Recency'],4, labels = r_labels)\n\ndatamart_quartile.head()","cff58709":"#same rules for Frequency & MonetaryValue\nf_labels = range(1,3)\ndatamart_quartile['F']= pd.qcut(datamart['Frequency'],4, labels = f_labels, duplicates='drop')\n\n# got error : Bin edges must be unique: array([ 1.,  1.,  1.,  1.,  2.,  4., 99.]).\n# solved by adding parameter ( duplicates='drop' ) in pd.qcut\n# got error: You can drop duplicate edges by setting the 'duplicates' kwarg\n# solved by chaning labels from range(1,5) to lower level, tried range (1,4) doesn't work, range (1,3) works.\n\nm_labels = range(1,5)\ndatamart_quartile['M']= pd.qcut(datamart['MonetaryValue'],4, labels = m_labels)\n\ndatamart_quartile.head()","99410474":"datamart_quartile.describe(include='all')  \n# we can see that F only have 2 labels (1 & 2)","fd6ca37b":"def join_rfm(x):\n    return str(x['R'])+str(x['F'])+str(x['M'])\n\ndatamart_quartile['RFM_Segment'] = datamart_quartile.apply(join_rfm, axis=1)\ndatamart_quartile['RFM_Score'] = datamart_quartile[['R']+['F']+['M']].sum(axis=1)\n\ndatamart_quartile.head()","3fd902cf":"# Largest 15 RFM segments\n\ndatamart_quartile.groupby('RFM_Segment').size().sort_values(ascending=False).head(15)\n\n# We observe that most of the segments are 424 & 111","0222c5ed":"#Filter on RFM segments, select botton RFM segment 111\ndatamart_quartile[datamart_quartile['RFM_Segment']=='111'][:5]","f3cf9bf8":"#Summary (Look at Recency, Frequecny, MonetaryValue at RFM_score level)\n\ndatamart_quartile.groupby('RFM_Score').agg({\n    'Recency':'mean',\n    'Frequency':'mean',\n    'MonetaryValue':['mean','count']\n}).round(1) \n\n# by count we see the size of each RFM_Score\n","7f7db7e9":"# manual choose the segment criteria\n\ndef segment_me(df):\n    if df['RFM_Score'] >=9:\n        return 'Gold'\n    elif (df['RFM_Score']>=6) and (df['RFM_Score']<9):\n        return 'Silver'\n    else:\n        return 'Bronze'","7912ac7a":"# create a column called general_segment, label the customers by the criteria set above\ndatamart_quartile['General_Segment']=datamart_quartile.apply(segment_me,axis=1)\ndatamart_quartile.head()","01191dd2":"#Summary (Look at Recency, Frequecny, MonetaryValue at General_Segment level)\n\ndatamart_quartile.groupby('General_Segment').agg({\n    'Recency':'mean',\n    'Frequency':'mean',\n    'MonetaryValue':['mean','count']\n}).round(1).sort_values(('Recency','mean'),ascending=True)","178bce38":"# Before K-means clustering, we should check if it fits the 3 assumptions \n# The variablew we are observing is Recency, Frequency, and MonetaryValue \n\ndatamart.describe() \n# They share different mean and std which it is against the K means assumptions","5b3d568c":"# Visualize the variables (Recency, Frequency & MonetaryValue) to check the Skewness\nplt.subplot(3,1,1); sns.distplot(datamart['Recency'])\nplt.subplot(3,1,2); sns.distplot(datamart['Frequency'])\nplt.subplot(3,1,3); sns.distplot(datamart['MonetaryValue'])","164ca4ed":"# unskew by getting the logarithm of the values\ndatamart_log = np.log(datamart)","92da3d83":"# Visualize again to check its skewness\nplt.subplot(3,1,1); sns.distplot(datamart_log['Recency'])\nplt.subplot(3,1,2); sns.distplot(datamart_log['Frequency'])\nplt.subplot(3,1,3); sns.distplot(datamart_log['MonetaryValue'])\nplt.show()\n\n# MonetaryValue after log is less skewed than before","a7da79b6":"# normalize by applying sklearn \nfrom sklearn.preprocessing import StandardScaler","b10ddf0e":"scaler = StandardScaler()\nscaler.fit(datamart_log) ","8941938a":"# store the normalized data back to log data for clustering\ndatamart_normalized = scaler.transform(datamart_log)","889337bb":"#create dataframe with datamart_normalized\ndatamart_normalized = pd.DataFrame(data=datamart_normalized, \n                                    index=datamart.index, \n                                    columns=datamart.columns)\n\ndatamart_normalized.head()","b465c608":"from sklearn.cluster import KMeans","b32bd489":"# decide how many groups (K) with Elbow criterion method\n\n#fit Kmeans and calculate SSE for each K\nsse = {}\n\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k, random_state=1)\n    kmeans.fit(datamart_normalized)\n    sse[k] = kmeans.inertia_\n    \n    \n# plot sse \nsns.pointplot(x=list(sse.keys()), y = list(sse.values()))\nplt.title('The Elbow Method')\nplt.xlabel('k')\nplt.ylabel('SSE')\nplt.show()\n\n# 2 is the elbow point, thus we can try 2 or 3 clusters for K-means.","932156ac":"# Run K-means clustering when K=2 \n\nkmeans2 = KMeans(n_clusters=2, random_state=1)\nkmeans2.fit(datamart_normalized)\ncluster_labels_k2 = kmeans2.labels_\n\n#Create a dataframe and assign the label back\ndatamart_rfm_k2 = datamart.assign(Cluster = cluster_labels_k2)\ndatamart_rfm_k2","d8ea9cf6":"#Summary (Look at Recency, Frequecny, MonetaryValue at cluster level)\n\ndatamart_rfm_k2.groupby('Cluster').agg({\n    'Recency':'mean',\n    'Frequency':'mean',\n    'MonetaryValue':['mean','count']\n}).round(0).sort_values(('Recency','mean'),ascending=True)","99d1610e":"# same process for k=3 \n\n# Run K-means clustering when K=3\n\nkmeans3 = KMeans(n_clusters=3, random_state=1)\nkmeans3.fit(datamart_normalized)\ncluster_labels_k3 = kmeans3.labels_\n\n#Create a dataframe and assign the label back\ndatamart_rfm_k3 = datamart.assign(Cluster = cluster_labels_k3)\ndatamart_rfm_k3","901e591c":"#Summary (Look at Recency, Frequecny, MonetaryValue at cluster level)\n\ndatamart_rfm_k3.groupby('Cluster').agg({\n    'Recency':'mean',\n    'Frequency':'mean',\n    'MonetaryValue':['mean','count']\n}).round(0).sort_values(('Recency','mean'),ascending=True)","aeab8d52":"# Using K=3 as example \n\n# melt the data so RFM values and metric names are stored in 1 column each \ndatamart_k3_melt = pd.melt(datamart_rfm_k3.reset_index(),\n                          id_vars=['Customer_ID','Cluster'],\n                          value_vars=['Recency','Frequency','MonetaryValue'],\n                          var_name='Attribute',\n                          value_name='Value')\ndatamart_k3_melt.head()","a3ff90cb":"#visualize a snake plot\nplt.title('Snake plot of standardized variables')\nsns.lineplot(x='Attribute',y='Value',hue='Cluster',data=datamart_k3_melt)","8125e228":"cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean()\npopulation_avg = datamart.mean() \nrelative_imp= cluster_avg \/ population_avg -1  # compare with population avg\n\nrelative_imp.round(2)","402994d4":"# heatmap \nplt.figure(figsize=(10,4))\nplt.title('Relative Importance of Attributes')\nsns.heatmap(data=relative_imp, annot= True, fmt='.2f', cmap='RdYlGn')\nplt.show()","ea77a7a5":"<a id=\"section-two\"><\/a>\n### Step 2 - Transform the data\nTo prepare the data for RFM analysis, I need the total value per transaction. Therefore, I need to combine different SKUs in the same transactions, since currently the data is stored by Transaction_ID and SKU_Category, meaning same transaction may have multiple rows due to different SKUs.","21793e27":"<a id=\"section-five\"><\/a>\n### Reflections & Extra Resources\n\n- Elbow method is a good indicator for choosing the number of clusters, but it doesn't always work. Other alternatives includes: \n\n    - [Silhouette coefficient](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html)\n    \n    - [Gap Statistics](https:\/\/web.stanford.edu\/~hastie\/Papers\/gap.pdf) \n\n\n\n- Limitation for K-means: \n\n    - For high-dimensional data. You need [extra steps](https:\/\/medium.com\/swlh\/k-means-clustering-on-high-dimensional-data-d2151e1a4240) in order to apply K-means clustering\n    - Skewness & Normalize data [Interesting article](http:\/\/varianceexplained.org\/r\/kmeans-free-lunch\/) challenging the assumption for K-means\n    \n    \n- Alternatives for K-means: [Important Clustering Algorithms](https:\/\/towardsdatascience.com\/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)","63564d3e":"# Customer Segmentation using RMF Analysis and K-means clustering\n\nThis is part of my individual project for Data Analyst Program 22 at Hyper Island.\n\n### Topic: How E-commerce companies can utilize customer's behavioral data to improve CRM\n\n#### Focusing on customer segmentation using RMF Analysis and K-means clustering in Python\n- Dataset: The data provide detailed information about quantities, characteristics and values of goods sold as well as their prices. The anonymized dataset includes 64.682 transactions of 5.242 SKU's sold to 22.625 customers during 2016.\n- Dataset source: [Retail Store Sales Transactions (Scanner Data)](https:\/\/www.kaggle.com\/marian447\/retail-store-sales-transactions)\n- Resources [Customer Segmentation in Python](https:\/\/learn.datacamp.com\/courses\/customer-segmentation-in-python) course on DataCamp\n\n#### Tables of contents\n- [Step 1 -  Data Cleaning and Pre-processing](#section-one)\n- [Step 2 - Transform the data](#section-two)\n- [Step 3 - RFM Analysis](#section-three)\n- [Step 4 - K-means clustering](#section-four)\n- [Reflections & Extra Resources](#section-five)\n\nWritten by T.L Li (Hyper Island 2021-09)","021e742b":"<a id=\"section-four\"><\/a>\n### Step 4 - K-means clustering\nK-means clustering is one of the most popular and fastest unsupervised machine learning algorithms, aiming to classify similar customers into the same segment.\n\n\nThe logic behind K-means is to find the centroid for k numbers of clusters and allocate every data point to the nearest clusters. (In other words, to minimize the sum of the squares of the distances from each data point to their assigned centroids)","4c2b3cc0":"#### Load the dataset & Exploratory Data Analysis (EDA)","20d13733":"#### Add the score label for R,F,M\nScore based on linear quantile method","8518f578":"#### Manual grouping into named segments ","e09ad210":"<a id=\"section-three\"><\/a>\n### Step 3 - RFM Analysis\nRFM Analysis is a customer segmentation method that divides the customers into different groups based on the metrics of Recency, Frequency, and Monetary Value. By labeling the customers based on the RFM score, we can identify the different stages of the customer cycles.\n","ca304689":"<a id=\"section-one\"><\/a>\n### Step 1 - Data Cleaning and Pre-processing\n####  Import the commonly-used libraries","29c34cbd":"#### Find the relative importance of segment attributes","4a2a7a0e":"#### Normalize the data for K-means clustering","d2778691":"#### View the result with snake plot","90bbb24f":"#### pre-processing for K-means clustering (metrics Recency, Frequency & MonetaryValue) \n3 assumtions for K-means: Skewness(symmetric), Center(same average) and Scale(same variance)","782af11e":"#### RFM Analysis Results","c47e1e8a":"#### Unskew the data for K-means clustering","965ae58d":"#### Analyze average RFM values of each clustering solution (k=2  & k =3)","0bf162b8":"#### Calculate RFM analysis metrics (Recency, Frequency, Monetary Value)\n- Recency - days since last customer transaction \n- Frequency - number of transactions in the last 12 months\n- Monetary Value - total spend in the last 12 months","4576b927":"#### Running K-means \nDeciding how many clusters by using Elbow method.\n\nSSE = sum of squared errors. \n\nAfter running Kmeans with k clusters, we calcuate its SSE for each k clusters. \n\nBy finding the elbow point, where it changed the angle the most, it can be used as a reference to determine the numbers of clusters for K-means.\n","1f3f1a17":"#### Build RFM segment and RFM score"}}