{"cell_type":{"94c056c1":"code","fb9159d7":"code","d1b1f2e9":"code","b4767562":"code","db9aba83":"code","22b4653f":"code","677dcabf":"code","a13ed61e":"code","0f71c230":"code","d04f201d":"code","81eb51d8":"code","746bbe08":"code","200afbc1":"code","3cc1e63f":"code","7c840732":"code","ff2f55bf":"code","37118a04":"code","8d9f7665":"code","c0aa03af":"code","a78dbfd4":"code","a69d68b8":"code","c2a377a8":"code","8b91a5fa":"code","187dfefe":"code","1fc122ac":"code","df88fcd9":"code","b6c94ffe":"code","b40325ba":"code","5809c57a":"code","0c1672d3":"code","592ad3c0":"code","b09ce6e9":"code","0b4be585":"code","83388b42":"code","978dcb0c":"markdown","36937a3d":"markdown","f82622d3":"markdown","8539d2b8":"markdown","18f47b82":"markdown","23963b91":"markdown"},"source":{"94c056c1":"!pip install gensim --quiet","fb9159d7":"import gensim, gensim.downloader\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pickle\nimport requests\nimport os\nimport time\nimport matplotlib.pyplot as plt","d1b1f2e9":"# reading files\ncategories = list()\nlabels_all = list()\nscores = list()\nwith open('..\/input\/hyppr-images-mapping\/categories.txt', 'r') as file:\n    for line in file.readlines():\n        categories.append(line.lower().rstrip().split(','))\n        \nwith open('..\/input\/hyppr-images-mapping\/labels.txt', 'r') as file:\n    for line in file.readlines():\n        labels_all.append(line.rstrip(',\\n').split(','))\n\nwith open('..\/input\/hyppr-images-mapping\/scores.txt', 'r') as file:\n    for line in file.readlines():\n        scores.append(line.rstrip(',\\n').split(','))\n        \nwith open(\"..\/input\/hyppr-images-mapping\/all_ids.p\", \"rb\") as f:\n    all_ids = pickle.load(f)","b4767562":"def set_key(dictionary, key, value):\n    if key not in dictionary:\n         dictionary[key] = value\n    elif type(dictionary[key]) == list:\n         dictionary[key].append(value)\n    else:\n         dictionary[key] = [dictionary[key], value]","db9aba83":"labels_n = [\" \".join(my_list) for my_list in labels_all]\nlabels_n = [\" \".join(re.split(' |-', my_list)) for my_list in labels_n]\n\ncategories_n = [\" \".join(my_list) for my_list in categories]\ncats = [category[0] for category in categories]","22b4653f":"# get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(ngram_range = (1, 3))\n%time tfidf_vec.fit_transform(labels_n)\n\nword_weight_dict = dict(zip(tfidf_vec.get_feature_names(), tfidf_vec.idf_))\n\nlen(tfidf_vec.get_feature_names())","677dcabf":"lexvec = gensim.models.KeyedVectors.load_word2vec_format(\n                    '..\/input\/lexvec\/lexvec.commoncrawl.300d.W.pos.neg3.vectors', binary = False)","a13ed61e":"n = lexvec[0].shape[0]\n\nimages_embeddings_lv = []   # aggregating embeddings\ncount_not_working  = 0\n\nids_of_zero_vectors = [] # we need to delete zero vectors since we can't categorize it\n\nfor idimage in range(len(labels_all)):  # going on lists of words; splitting words then on subwords for aggregating subwords into one word\n    agg1 = np.zeros(n)\n    weights_sum = 0\n    for label in labels_all[idimage]:\n        agg = np.zeros(n)\n        subwords = re.split(' |--|-', label.replace('&', 'and'))\n        if len(subwords) == 1: # no <subword>-<subword>-<subword> words then\n            try:\n                agg = word_weight_dict[label] * lexvec[label]\n                weights_sum += word_weight_dict[label]\n            except KeyError as e:            \n                count_not_working += 1\n                \n        else:\n            common_weight = 0\n            for subw in subwords:    # subw for subword\n               \n                aggregate_mult = np.zeros(n)\n                if subw not in stopwords.words('english') and subw != '':  # there is some NULL strings\n                    try:\n                        aggregate_mult = word_weight_dict[subw]*lexvec[subw]\n                        common_weight += word_weight_dict[subw]\n                    except KeyError as e:\n                        count_not_working += 1\n                        \n                agg = np.add(agg, aggregate_mult)\n            if common_weight > 1e-6:\n                weights_sum += common_weight\n                agg = np.divide(agg, common_weight)\n            \n        agg1 = np.add(agg1, agg)\n    if weights_sum > 1e-6:\n        m = weights_sum\n    else:\n        m = len(labels_all[idimage])\n        ids_of_zero_vectors.append(idimage)\n        \n    images_embeddings_lv.append(np.divide(agg1, m) ) #exchange this with dictionary\n                                    \nimages_embeddings_lv = np.array(images_embeddings_lv)\nprint(count_not_working)","0f71c230":"with open(\"..\/input\/hyppr-images-mapping\/category_to_urls.p\", \"rb\") as f:\n    categories_urls = pickle.load(f)\nwith open(\"..\/input\/hyppr-images-mapping\/urls_imagelabels.p\", \"rb\") as f:\n    urls_imagelabels = pickle.load(f)","d04f201d":"n = lexvec[0].shape[0]","81eb51d8":"nodata = ['dance', 'entertainment', 'tech']  # we don't have data on cite for this categories so we can't consider data communicated with it\ncats = [cat for cat in cats if cat not in nodata]   # so we just throw it away","746bbe08":"cat_embeddings = dict()\nnumb_of_fails = 0\nno_url = 0\n\nfor category in cats:\n    emb_cat = np.zeros(n)\n    numb_of_images = 0\n    for image in categories_urls[category]:\n        try:\n            emb_temp = np.zeros(n)\n            distr = urls_imagelabels[image]  # try for this dictionary applying    ## there is spawning a KeyError\n            weights_sum = 0\n            numb_of_images += 1\n            for label in distr:\n                label = label[0].lower()\n                emb_temp = np.zeros(n)\n                subwords = re.split(' |--|-|, ', label)\n                \n                if len(subwords) == 1: # no <subword>-<subword>-<subword> words then\n                    try:\n                        emb_temp = word_weight_dict[label] * lexvec[label]\n                        weights_sum += word_weight_dict[label]\n                    except TypeError as e:\n                        print(e)\n                        numb_of_fails += 1\n                        \n                else:\n                    common_weight = 0\n                    for subw in subwords:    # subw for subword\n                        emb_mult = np.zeros(n)\n                        try:\n                            emb_mult = word_weight_dict[subw]*lexvec[subw]\n                            common_weight += word_weight_dict[subw]\n                        except:\n                            numb_of_fails += 1\n                        emb_temp = np.add(emb_temp, emb_mult)\n                        \n                    if common_weight > 1e-6:\n                        weights_sum += common_weight\n                        emb_temp = np.divide(emb_temp, common_weight)\n                    # there will be a lot of zero values.\n                    \n            emb_temp = np.divide(emb_temp, weights_sum)\n            emb_cat = np.add(emb_cat, emb_temp)\n        except KeyError as e:\n            no_url += 1\n    cat_embeddings[category] = np.divide(emb_cat, numb_of_images) \nprint('There is no such url: {}'.format(no_url))\nprint('Number of fails: {}'.format(numb_of_fails))","200afbc1":"with open(\".\/cat_embeddings.p\", \"wb\") as f:\n    pickle.dump(cat_embeddings, f)","3cc1e63f":"dict_embeddings = dict()\nfor i in range(len(all_ids)):\n    dict_embeddings[all_ids[i]] = images_embeddings_lv[i]","7c840732":"with open(\".\/dict_embeddings.p\", \"wb\") as f:\n    pickle.dump(dict_embeddings, f)","ff2f55bf":"def set_key(dictionary, key, value):\n    if key not in dictionary:\n         dictionary[key] = value\n    elif type(dictionary[key]) == list:\n         dictionary[key].append(value)\n    else:\n         dictionary[key] = [dictionary[key], value]","37118a04":"import pickle\nwith open(\"..\/input\/hyppr-images-mapping\/objid_postid.p\", \"rb\") as f:\n    objid_postid = pickle.load(f)\n# reverse:\npostid_objid = dict()\nfor objid, postid in objid_postid.items():\n    set_key(postid_objid, postid, objid)","8d9f7665":"with open('..\/input\/hyppr-images-mapping\/category_to_posts_vision.p', 'rb') as f:   # opening given model\n    category_to_posts = pickle.load(f)\ncategory_to_posts = dict(category_to_posts)\n\nposts_to_category = dict()\nfor cat, posts in category_to_posts.items():\n    for post in posts:\n        if post not in posts_to_category.keys():\n            set_key(posts_to_category, post, cat)\ncat_emb = list(cat_embeddings.values())","c0aa03af":"nodata = ['dance', 'entertainment', 'tech']  # we don't have data on cite for this categories so we can't consider data communicated with it\ncats = [cat[0] for cat in categories if cat[0] not in nodata]   # so we just throw it away\ncats.append('none')","a78dbfd4":"len(cats)","a69d68b8":"with open(\"..\/input\/hyppr-images-mapping\/urls_imagelabels.p\", \"rb\") as f:\n    urls_imagelabels = pickle.load(f)\nwith open(\"..\/input\/hyppr-images-mapping\/postid_objurls.p\", \"rb\") as f:\n    postid_objurls = pickle.load(f)\n\nall_image_urls = []\nall_image_labels = []\nfor imageUrl, labels in urls_imagelabels.items():\n    all_image_labels.append([label[0].lower() for label in labels if label[0] != []])\n    all_image_urls.append(imageUrl)","c2a377a8":"threshold = 0.5\nyDef,yNew = [], []\nno_post, no_emb, no_labels = 0, 0, 0\narguable_data = dict()\nsec_dict_values = dict()\ncos_similarities = dict()\nfor post, cat in posts_to_category.items():\n    no_data = False\n    if post in postid_objid.keys():        \n        objid = postid_objid[post]\n        if objid in dict_embeddings.keys():\n            maxId = cosine_similarity(cat_emb, [dict_embeddings[objid]]).argmax()\n            if max(cosine_similarity(cat_emb, [dict_embeddings[objid]])) < threshold:    # chanching threshold for better results\n                yNew.append('none')\n            else:\n                cat_res = cats[maxId]\n                yNew.append(cat_res)\n        else:\n            no_emb += 1\n            no_data = True\n    else:\n        no_post += 1\n        no_data = True\n        \n    if no_data is False:\n        if type(cat) is list:\n            yDef.append(cat[0])\n        else:\n            yDef.append(cat)\n            \n        objid = postid_objid[post]   \n        if yNew[-1] != yDef[-1]:   # creating distributions on cosine_similarities for arguable_data\n            url = postid_objurls[post]\n            if type(url) == list:\n                url = url[0]\n            set_key(cos_similarities, cat, cosine_similarity(cat_emb, [dict_embeddings[objid]]) )\n            set_key(arguable_data, cat, url)\n            set_key(sec_dict_values, cat, yNew[-1])\n\nprint('There is {} post missing'.format(no_post))\nprint('There is {} embeddings missing'.format(no_emb))\nprint('There is {} post to labels missing'.format(no_labels))","8b91a5fa":"with open(\"..\/input\/hyppr-images-mapping\/objid_mark_category.p\", \"rb\") as f:\n       objid_mark_category = pickle.load(f)\nwith open(\"..\/input\/hyppr-images-mapping\/yRight.p\", \"rb\") as f:\n       yRight = pickle.load(f)","187dfefe":"len(yDef) == len(yNew)","1fc122ac":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score","df88fcd9":"conf_matr = confusion_matrix(yRight, yNew, labels = cats)\n\ndf_cm = pd.DataFrame(conf_matr, index = [i for i in range(len(cats))],\n                  columns = [i for i in range(len(cats))])\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm,cmap=\"YlGnBu\",linewidths=1, annot=True, fmt = 'd')\nplt.savefig('word2vecQuality.png', bbox_inches = 'tight')","b6c94ffe":"thresholds = np.arange(0,1,0.1)\nf_1_scores_new, prec_scores, rec_scores = [], [], []\nfor thresh in thresholds:\n    threshold = thresh\n    yDef,yNew = [], []\n    for post, cat in posts_to_category.items():\n        no_data = False\n        if post in postid_objid.keys():        \n            objid = postid_objid[post]\n            if objid in dict_embeddings.keys():\n                maxId = cosine_similarity(cat_emb, [dict_embeddings[objid]]).argmax()\n                if max(cosine_similarity(cat_emb, [dict_embeddings[objid]])) < threshold:    # chanching threshold for better results\n                    yNew.append('none')\n                else:\n                    cat_res = cats[maxId]\n                    yNew.append(cat_res)\n            else:\n                no_data = True\n        else:\n            no_data = True\n\n        if no_data is False:\n            if type(cat) is list:\n                yDef.append(cat[0])\n            else:\n                yDef.append(cat)\n    prec_scores.append(precision_score(yRight,yNew, labels = cats, average = 'macro'))\n    rec_scores.append(recall_score(yRight,yNew, labels = cats, average = 'macro'))\n    f_1_scores_new.append(f1_score(yRight,yNew, labels = cats, average = 'macro'))","b40325ba":"fig, ax = plt.subplots(1, 3, sharey = True, figsize = (16, 7))\nax[0].plot(thresholds, prec_scores, 'yx-')\nax[0].set_xlabel('t')\nax[0].set_ylabel('precision')\nax[1].plot(thresholds, rec_scores, 'rx-')\nax[1].set_xlabel('t')\nax[1].set_ylabel('recall')\nax[2].plot(thresholds, f_1_scores_new, 'gx-')\nax[2].set_xlabel('t')\nax[2].set_ylabel('$F_1$-score')\nplt.savefig('w2vQual.png', bbox_inches='tight')","5809c57a":"threshold = 0.5\nyDef,yNew = [], []\nno_post, no_emb, no_labels = 0, 0, 0\narguable_data = dict()\nsec_dict_values = dict()\ncos_similarities = dict()\nfor post, cat in posts_to_category.items():\n    no_data = False\n    if post in postid_objid.keys():        \n        objid = postid_objid[post]\n        if objid in dict_embeddings.keys():\n            maxId = cosine_similarity(cat_emb, [dict_embeddings[objid]]).argmax()\n            if max(cosine_similarity(cat_emb, [dict_embeddings[objid]])) < threshold:    # chanching threshold for better results\n                yNew.append('none')\n            else:\n                cat_res = cats[maxId]\n                yNew.append(cat_res)\n        else:\n            no_emb += 1\n            no_data = True\n    else:\n        no_post += 1\n        no_data = True\n        \n    if no_data is False:\n        if type(cat) is list:\n            yDef.append(cat[0])\n        else:\n            yDef.append(cat)\n            \n        objid = postid_objid[post]   \n        if yNew[-1] != yDef[-1]:   # creating distributions on cosine_similarities for arguable_data\n            url = postid_objurls[post]\n            if type(url) == list:\n                url = url[0]\n            set_key(cos_similarities, cat, cosine_similarity(cat_emb, [dict_embeddings[objid]]) )\n            set_key(arguable_data, cat, url)\n            set_key(sec_dict_values, cat, yNew[-1])\n            \nconf_matr = confusion_matrix(yRight, yNew, labels = cats)\n\ndf_cm = pd.DataFrame(conf_matr, index = [i for i in range(len(cats))],\n                  columns = [i for i in range(len(cats))])\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm,cmap=\"YlGnBu\",linewidths=1, annot=True, fmt = 'd')\nplt.savefig('word2vecQuality.png', bbox_inches = 'tight')","0c1672d3":"f1_score(yRight,yNew, labels = cats, average = 'macro')","592ad3c0":"recall_score(yRight,yNew, labels = cats, average = 'macro')","b09ce6e9":"precision_score(yRight,yNew, labels = cats, average = 'macro')","0b4be585":"result = dict()\nthreshold = 0.3\nfor postid, objid in postid_objid.items():        \n    objid = postid_objid[post]\n    if objid in dict_embeddings.keys():\n        maxId = cosine_similarity(cat_emb, [dict_embeddings[objid]]).argmax()\n        if max(cosine_similarity(cat_emb, [dict_embeddings[objid]])) < threshold:    # chanching threshold for better results\n            set_key(result, postid, 'none')\n        else:\n            cat_res = cats[maxId]\n            set_key(result, postid, cat_res)\n    else:\n        no_emb += 1\n        \nprint('There is {} embeddings missing'.format(no_emb))","83388b42":"with open(\".\/postid_category.p\", \"wb\") as f:\n     pickle.dump(result, f)","978dcb0c":"# Final categorization of all posts:","36937a3d":"# Quality estimation","f82622d3":"# Processing the other data which does not occur \/ almost does not occur on the site","8539d2b8":"Not taking 'dance', 'entertainment', 'tech'.","18f47b82":"# LEXVEC model testing","23963b91":"## Aggregating categories embeddings:"}}