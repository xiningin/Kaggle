{"cell_type":{"8b919a1f":"code","58e2eda8":"code","a62cdeab":"code","a4a7cad7":"code","ff3b304b":"code","71b8f916":"code","43044438":"code","8754aa50":"code","29e2b332":"code","62e4b88c":"code","73fe0a88":"code","bff957d4":"code","df9795bf":"code","ae01f1fd":"code","fe909b1f":"code","c2dad503":"code","912c7805":"code","113a39f8":"code","baae21f3":"code","94abef60":"code","a77eae9a":"code","a323bef2":"code","1f79d745":"code","99511cd7":"code","4099c2c0":"code","22497079":"code","7253f4d6":"code","c649928d":"code","31130695":"code","1cf86718":"code","0a72a523":"code","79a32f82":"code","98d3f7d9":"code","3a5ceee7":"code","60d1c174":"code","e073e26c":"code","7d2e3849":"code","31c27ed4":"code","e86b6bd8":"code","9a2c9a3c":"code","e6f84406":"code","ba50a691":"code","1bd89634":"code","6986a1b0":"code","5757d69e":"code","fa689c99":"code","0dd657d1":"code","90a7bcc3":"code","3b7e91e4":"code","328457b5":"code","214559c0":"code","694d990c":"code","ec9f9d52":"code","d27026b9":"code","f5da9886":"code","576f79bc":"code","3a0de114":"code","2892a783":"code","d15d497b":"markdown","31447197":"markdown","69ba7668":"markdown","44d74f42":"markdown","5d0761c4":"markdown","6c8cb5b6":"markdown","98f32ed7":"markdown","59ade68c":"markdown","627fb307":"markdown","6be33ec3":"markdown","ec8efe45":"markdown","0f6b092c":"markdown","a0f4f5d1":"markdown","7a39ceb1":"markdown","b2e70636":"markdown","a891a02c":"markdown","0cf3873a":"markdown","fdab7feb":"markdown","7b7e1e8f":"markdown","38cf6fcc":"markdown","e5d65a76":"markdown"},"source":{"8b919a1f":"!pip install openpyxl","58e2eda8":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_excel('..\/input\/soil-detection-for-cotton-crop\/Detection of Soil for cotton crop.xlsx', engine='openpyxl')\ndata","a62cdeab":"data.describe()","a4a7cad7":"cols = data.columns[data.dtypes==object]\ncols","ff3b304b":"for col in cols:\n    print(col)\n    print()\n    print(data[col].value_counts())\n    print('******************************')\n    print()","71b8f916":"# \"Cotton Crop\" is our label data and \"Sample ID \" is unwanted\ntarget = data['Cotton Crop']\ndata.drop(['Sample ID','Cotton Crop'], axis=1, inplace=True)","43044438":"target.value_counts()","8754aa50":"df = data.drop(cols, axis=1)","29e2b332":"from sklearn.preprocessing import MinMaxScaler\n  \n# copy the data\nnorm_df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n\nnorm_df.head()","62e4b88c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV,RepeatedStratifiedKFold","73fe0a88":"data['Particle Width'].value_counts()","bff957d4":"data['Particle Width'][36]","df9795bf":"pw = data['Particle Width'].fillna(method='ffill')","ae01f1fd":"pw[36]","fe909b1f":"from sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\npw = pd.DataFrame(enc.fit_transform(pw))\npw.value_counts()","c2dad503":"ps = pd.DataFrame(enc.fit_transform(data['Particle Spacing']))\nps.value_counts()","912c7805":"target.value_counts()","113a39f8":"est =[]\nest.append(('LogisticRegression', Pipeline([('LogisticRegression', LogisticRegression())])))\nest.append(('SVC', Pipeline([('SVC', SVC())])))\nest.append(('GaussianNB', Pipeline([('GaussianNB', GaussianNB())])))\nest.append(('BernoulliNB', Pipeline([('BernoulliNB', BernoulliNB())])))\nest.append(('MultinomialNB', Pipeline([('MultinomialNB', MultinomialNB())])))\nest.append(('KNeighborsClassifier', Pipeline([('KNeighborsClassifier', KNeighborsClassifier())])))\nest.append(('DecisionTreeClassifier', Pipeline([('DecisionTreeClassifier', DecisionTreeClassifier())])))\n# est.append(('XGBClassifier', Pipeline([('XGBClassifier', XGBClassifier())])))\nest.append(('GradientBoostingClassifier', Pipeline([('GradientBoostingClassifier', GradientBoostingClassifier())])))\nest.append(('AdaBoostClassifier', Pipeline([('AdaBoostClassifier', AdaBoostClassifier())])))\nest.append(('RandomForestClassifier', Pipeline([('RandomForestClassifier', RandomForestClassifier())])))\nest.append(('ExtraTreesClassifier', Pipeline([('ExtraTreesClassifier', ExtraTreesClassifier())])))\nest.append(('BaggingClassifier', Pipeline([('BaggingClassifier', BaggingClassifier())])))","baae21f3":"# import sklearn\n# sorted(sklearn.metrics.SCORERS.keys())","94abef60":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nseed = 4\nsplits = 5\nrepeats = 3\n\nscore = 'f1_weighted'\nresults_dict = {}\ncol_names  = ['Particle Spacing', 'Particle Width', 'Cotton Crop']\nfor index, column in enumerate([ps, pw, target]):\n    \n    models_score ={}\n    for i in est:\n        rskf = RepeatedStratifiedKFold(n_splits=splits, n_repeats=repeats, random_state=seed)\n        results = cross_val_score(i[1], norm_df, np.array(column).reshape(-1,), cv=rskf, scoring=score)\n        models_score.update({i[0] : results.mean()})\n      \n    results_dict[col_names[index]] = models_score","a77eae9a":"results_dict","a323bef2":"iterable = results_dict['Particle Spacing']\nclassifier = sorted(iterable, key=lambda i:iterable[i], reverse=True)[0]\nprint(classifier, iterable[classifier])","1f79d745":"iterable = results_dict['Particle Width']\nclassifier = sorted(iterable, key=lambda i:iterable[i], reverse=True)[0]\nprint(classifier, iterable[classifier])","99511cd7":"iterable = results_dict['Cotton Crop']\nclassifier = sorted(iterable, key=lambda i:iterable[i], reverse=True)[0]\nprint(classifier, iterable[classifier])","4099c2c0":"# \"Particle Width\" model\npw_model = DecisionTreeClassifier().fit(norm_df, np.array(pw).reshape(-1,))\n# \"Particle Spacing\" model\nps_model = SVC().fit(norm_df, np.array(pw).reshape(-1,))\n# \"Cotton Crop\" model\ntarget_model = SVC().fit(norm_df, np.array(target).reshape(-1,))","22497079":"gs = pd.DataFrame(enc.fit_transform(data['Grain Surface']))\ngs.value_counts()","7253f4d6":"from sklearn.svm import LinearSVC","c649928d":"est =[]\nest.append(('LogisticRegression', Pipeline([('LogisticRegression', LogisticRegression(multi_class='multinomial'))])))\nest.append(('LinearSVC', Pipeline([('LinearSVC', LinearSVC())])))\nest.append(('MultinomialNB', Pipeline([('MultinomialNB', MultinomialNB())])))\nest.append(('DecisionTreeClassifier', Pipeline([('DecisionTreeClassifier', DecisionTreeClassifier(class_weight='balanced'))])))\nest.append(('RandomForestClassifier', Pipeline([('RandomForestClassifier', RandomForestClassifier(class_weight='balanced'))])))\nest.append(('ExtraTreesClassifier', Pipeline([('ExtraTreesClassifier', ExtraTreesClassifier(class_weight='balanced'))])))","31130695":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nseed = 4\nsplits = 5\nrepeats = 3\n\nscore = 'f1_weighted'\nmodels_score = {}\n\nfor i in est:\n    rskf = RepeatedStratifiedKFold(n_splits=splits, n_repeats=repeats, random_state=seed)\n    results = cross_val_score(i[1], norm_df, np.array(gs).reshape(-1,), cv=rskf, scoring=score)\n    models_score.update({i[0] : results.mean()})\n    \nmodels_score","1cf86718":"classifier = sorted(models_score, key=lambda i:models_score[i], reverse=True)[0]\nprint(classifier, models_score[classifier])","0a72a523":"# \"Grain Surface\" model\ngs_model = RandomForestClassifier().fit(norm_df, np.array(gs).reshape(-1,))","79a32f82":"import tensorflow \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ntensorflow.config.run_functions_eagerly(True)\n\nprint(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))","98d3f7d9":"img_size_1 = 10\nlatent_space_dim = 2","3a5ceee7":"# Encoder\nx = tensorflow.keras.layers.Input(shape=(img_size_1, 1, 1), name=\"encoder_input\")\n\nencoder_conv_layer1 = tensorflow.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"encoder_conv_1\")(x)\nencoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_1\")(encoder_conv_layer1)\nencoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_leakyrelu_1\")(encoder_norm_layer1)\n\nencoder_conv_layer2 = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_2\")(encoder_activ_layer1)\nencoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_2\")(encoder_conv_layer2)\nencoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_2\")(encoder_norm_layer2)\n\nencoder_conv_layer3 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_3\")(encoder_activ_layer2)\nencoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_3\")(encoder_conv_layer3)\nencoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_3\")(encoder_norm_layer3)\n\nencoder_conv_layer4 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=2, name=\"encoder_conv_4\")(encoder_activ_layer3)\nencoder_norm_layer4 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_4\")(encoder_conv_layer4)\nencoder_activ_layer4 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_4\")(encoder_norm_layer4)\n\nencoder_conv_layer5 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", strides=1, name=\"encoder_conv_5\")(encoder_activ_layer4)\nencoder_norm_layer5 = tensorflow.keras.layers.BatchNormalization(name=\"encoder_norm_5\")(encoder_conv_layer5)\nencoder_activ_layer5 = tensorflow.keras.layers.LeakyReLU(name=\"encoder_activ_layer_5\")(encoder_norm_layer5)\n\nshape_before_flatten = tensorflow.keras.backend.int_shape(encoder_activ_layer5)[1:]\nencoder_flatten = tensorflow.keras.layers.Flatten()(encoder_activ_layer5)\n\nencoder_mu = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_mu\")(encoder_flatten)\nencoder_log_variance = tensorflow.keras.layers.Dense(units=latent_space_dim, name=\"encoder_log_variance\")(encoder_flatten)\n\nencoder_mu_log_variance_model = tensorflow.keras.models.Model(x, (encoder_mu, encoder_log_variance), name=\"encoder_mu_log_variance_model\")\n\ndef sampling(mu_log_variance):\n    mu, log_variance = mu_log_variance\n    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(mu), mean=0.0, stddev=1.0)\n    random_sample = mu + tensorflow.keras.backend.exp(log_variance\/2) * epsilon\n    return random_sample\n\nencoder_output = tensorflow.keras.layers.Lambda(sampling, name=\"encoder_output\")([encoder_mu, encoder_log_variance])\n\nencoder = tensorflow.keras.models.Model(x, encoder_output, name=\"encoder_model\")\n\nencoder.summary()","60d1c174":"decoder_input = tensorflow.keras.layers.Input(shape=(latent_space_dim), name=\"decoder_input\")\n\ndecoder_dense_layer1 = tensorflow.keras.layers.Dense(units=640, name=\"decoder_dense_1\")(decoder_input)\ndecoder_reshape = tensorflow.keras.layers.Reshape(target_shape=(10,1,64))(decoder_dense_layer1)\n\ndecoder_conv_tran_layer1 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_1\")(decoder_reshape)\ndecoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_1\")(decoder_conv_tran_layer1)\ndecoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_1\")(decoder_norm_layer1)\n\ndecoder_conv_tran_layer2 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_2\")(decoder_activ_layer1)\ndecoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_2\")(decoder_conv_tran_layer2)\ndecoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_2\")(decoder_norm_layer2)\n\ndecoder_conv_tran_layer3 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding=\"same\", strides=1, name=\"decoder_conv_tran_3\")(decoder_activ_layer2)\ndecoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name=\"decoder_norm_3\")(decoder_conv_tran_layer3)\ndecoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name=\"decoder_leakyrelu_3\")(decoder_norm_layer3)\n\ndecoder_conv_tran_layer4 = tensorflow.keras.layers.Conv2DTranspose(filters=1, kernel_size=(2, 2), padding=\"same\", strides=1, name=\"decoder_conv_tran_4\")(decoder_activ_layer3)\ndecoder_output = tensorflow.keras.layers.LeakyReLU(name=\"decoder_output\")(decoder_conv_tran_layer4 )\ndecode = tensorflow.keras.layers.Reshape(target_shape=(10,1,1))\ndecoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name=\"decoder_model\")\n \ndecoder.summary()","e073e26c":"def loss_func(encoder_mu, encoder_log_variance):\n    def vae_reconstruction_loss(y_true, y_predict):\n        \n        reconstruction_loss_factor = 1000\n        reconstruction_loss = tensorflow.keras.backend.mean(tensorflow.keras.backend.square(y_true-y_predict), axis=[1, 2, 3])\n        return reconstruction_loss_factor * reconstruction_loss\n\n    def vae_kl_loss(encoder_mu, encoder_log_variance):\n        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n        return kl_loss\n\n    def vae_kl_loss_metric(y_true, y_predict):\n        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)\n        return kl_loss\n\n    def vae_loss(y_true, y_predict):\n        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)\n        kl_loss = vae_kl_loss(y_true, y_predict)\n\n        loss = reconstruction_loss + kl_loss\n        return loss\n\n    return vae_loss","7d2e3849":"vae_input = tensorflow.keras.layers.Input(shape=(img_size_1, 1, 1), name=\"VAE_input\")\nvae_encoder_output = encoder(vae_input)\nvae_decoder_output = decoder(vae_encoder_output)\nvae = tensorflow.keras.models.Model(vae_input, vae_decoder_output, name=\"VAE\")\nvae.summary()","31c27ed4":"vae.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.0005), loss=loss_func(encoder_mu, encoder_log_variance))","e86b6bd8":"norm_df.shape","9a2c9a3c":"norm_df.iloc[0].index.to_list()","e6f84406":"col_index  = {i:j for i, j in enumerate(norm_df.iloc[0].index.to_list())}\ncol_index","ba50a691":"reshaped_array = np.array(norm_df).reshape(-1, 10, 1, 1)\nreshaped_array.shape","1bd89634":"vae.fit(reshaped_array, reshaped_array, epochs=1000, batch_size=10, shuffle=True)","6986a1b0":"# Passing any random 2 numbers to decoder\nz_sample = np.array([[-1, 0.6]])\ndecoder.predict(z_sample)","5757d69e":"# Generating 4 rows of synthetic data\n\nscale = 1.0\nn = 2\ngrid_x = np.linspace(-scale, scale, n)\ngrid_y = np.linspace(-scale, scale, n)","fa689c99":"synthetic_data = []\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = decoder.predict(z_sample)\n        synthetic_data.append(x_decoded[0])","0dd657d1":"np.array(synthetic_data).reshape(-1, 10)","90a7bcc3":"pd.DataFrame(np.array(synthetic_data).reshape(-1, 10), columns=col_index.values())","3b7e91e4":"%%time\n\nscale = 1.0\nn = 100\ngrid_x = np.linspace(-scale, scale, n)\ngrid_y = np.linspace(-scale, scale, n)\n\nsynthetic_data = []\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = decoder.predict(z_sample)\n        synthetic_data.append(x_decoded[0])\n        \nsyn_data = np.array(synthetic_data).reshape(-1, 10)\nsynth_data = pd.DataFrame(syn_data, columns=col_index.values())","328457b5":"synth_data","214559c0":"# Particle Width\npw_predicted = pw_model.predict(synth_data)\n# Particle Spacing\nps_predicted = ps_model.predict(synth_data)\n# Cotton Crop\ntarget_predicted = target_model.predict(synth_data)\n# Grain Surface\ngs_predicted = gs_model.predict(synth_data)","694d990c":"pw_d = pd.DataFrame(pw_predicted, columns=['Particle Width'])\nps_d = pd.DataFrame(ps_predicted, columns=['Particle Spacing'])\ncc_d = pd.DataFrame(target_predicted, columns=['Cotton Crop'])\ngs_d = pd.DataFrame(gs_predicted, columns=['Grain Surface'])\n\ncat_col = pd.concat([pw_d, ps_d, cc_d, gs_d], axis=1)\ncat_col","ec9f9d52":"pw_d.value_counts()","d27026b9":"ps_d.value_counts()","f5da9886":"cc_d.value_counts()","576f79bc":"gs_d.value_counts()","3a0de114":"new_synth_data = pd.concat([synth_data, cat_col], axis=1)\nnew_synth_data","2892a783":"new_synth_data.to_csv('synthetic_scaled_data.csv')","d15d497b":"# Generating 100 * 100 synthetic data","31447197":"# VAE (Variational AutoEncoder) for generating synthetic data\n\n**************************************************\n\nVAE are a class of deep neural networks which comprises of encoder and decoder network and is used to generate synthetic data. It is architecturally similar to AutoEncoders.Variational autoencoders creats a latent distribution which is statistically similar to the original data distribution and hence it ensures that the new data follows the same distribution.\n\nFor more details, please read **[this](https:\/\/towardsdatascience.com\/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9)** and **[this](https:\/\/towardsdatascience.com\/understanding-variational-autoencoders-vaes-f70510919f73)** article","69ba7668":"# Reshaping norm_df before training","44d74f42":"# Classifying the new synthetic data into previous categorical columns","5d0761c4":"# INTRODUCTION\n*************************************\n\n* In this notebook, we will generate synthetic data for the [dataset](https:\/\/www.kaggle.com\/zohasohail\/soil-detection-for-cotton-crop) using VAE (Variational AutoEncoder). This dataset is very small having only 48 rows.\n\n* This notebook is in the continuation of [my previous notebook](https:\/\/www.kaggle.com\/shweta2407\/soil-best-for-cotton-crop-using-ml\/) on the same dataset in which I have done preprocessing and discussed about those methods.\n\n* In that notebook, I have used **Synthetic Minority Oversampling technique (SMOTE)** to overcome the problem of imbalanced dataset and also to generate data but it couldn't increase much data. The dataset has only 48 rows and SMOTE could only increase 10-12 rows more.\n\n* Hence we are using **Variational Auto Encoders (VAE)** which is a deep learning architecture and a generative model that will create a latent distribution out of the dataset from where we can **generate new datapoints of the same distribution**.\n\n* For more details on VAE, please read **[this](https:\/\/towardsdatascience.com\/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9)** and **[this](https:\/\/towardsdatascience.com\/understanding-variational-autoencoders-vaes-f70510919f73)** article\n\n* **Using VAE I will increase the size of the dataset from 48 to 10,000**.\n****************","6c8cb5b6":"# Conclusion\n\n************************************************\n* We firstly took all the numeric columns and scaled it\n\n* We separated the categorical columns and made classifiers (binary + multiclass) using the numeric data\n\n* Then we passed the numeric columns into VAE and generated scaled sythetic data.\n\n* We passed these data into the different classifiers to classify the synthetic data into the different categories as in the original data.\n\n* All the columns are there from the original data except \"Sample ID\" and \"Particle Attached\". We have deleted \"Particle Attached\" column because it contained only one value and hence it was not a contributing feature. I have explained it more in [my previous notebook](https:\/\/www.kaggle.com\/shweta2407\/soil-best-for-cotton-crop-using-ml\/)","98f32ed7":"# Decoder","59ade68c":"# Encoder","627fb307":"# Muticlass Classifier","6be33ec3":"# Compile","ec8efe45":"# Scaling ","0f6b092c":"# Categorical Columns","a0f4f5d1":"# Classifier for each categorical columns\n********************************************\nAmong the categorical columns,\n\n**Particle Attached** is of no use as there is one category \n\nFor **Particle Width** and **Particle Spacing**, we will make **binary classifier**\n\n**Grain Surface** requires a **Multiclass classifier**\n\n**Cotton Crop** is our target label which is of boolean type, hence a **binary classifier**\n\nAs our most of the categorical columns are **imbalanced**, hence we will use **Stratified** cross-validation to evaluate the model.","7a39ceb1":"# Testing","b2e70636":"#### Now as we want to generate synthetic data, we will not include the categorical columns. My idea is to work with only numerical columns and for each categorical column I am going to make a classifier.","a891a02c":"# Training","0cf3873a":"# Loss Function (Reparametrization Trick)","fdab7feb":"# Scope for Improvement\n\n******************************************\n\n* We can tweak the architecture of VAE to minimize the loss as the loss we obtained was 6.68 which is still high.\n\n* For the categorical columns, the multiclass classifier was not so good as its F1 score was around 0.57. There is room for improvement there","7b7e1e8f":"# Sorting the best classifier for each column","38cf6fcc":"# Combine Encoder and Decoder","e5d65a76":"# Binary Classifiers\n********************************"}}