{"cell_type":{"545b0aff":"code","3dcf3b9c":"code","f693260f":"code","1b24731c":"code","8c69b8ce":"code","98c6947e":"code","834e0a8d":"code","d430c38a":"code","3e67525d":"code","03a76cb8":"code","dec6f350":"code","38fe4982":"code","f8a5c11e":"code","59bdd79b":"code","9aa51919":"code","646e35a7":"code","94db974f":"code","b2a9f680":"markdown","2861e572":"markdown","e49c1eb9":"markdown","a382cf53":"markdown","2ed20719":"markdown","bacf08ac":"markdown","43258fe0":"markdown","c76fe906":"markdown","2793235d":"markdown","f2623102":"markdown","aadab87a":"markdown"},"source":{"545b0aff":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, recall_score, precision_score\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n%matplotlib inline","3dcf3b9c":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","f693260f":"df.head()","1b24731c":"# Checking NaN objects\ndf.isnull().sum()","8c69b8ce":"# heatmap reflecting correlations between features\n\ncorr_matrix = df.corr()\n\nsns.set(rc={'figure.figsize':(40, 40)})\nsns.heatmap(data=corr_matrix, annot=True)","98c6947e":"# distribution of classes graph\nsns.set(rc={'figure.figsize':(10, 10)})\ndf[['Class']].value_counts().plot(kind='bar').set(xticklabels=[0, 1])\nplt.ylabel('Frequency')\nplt.xlabel('Class')\nplt.title('Distribution of classes')","834e0a8d":"# droping 'Time' and 'Amount' from the whole train dataset\n\ndata_preproc = df.drop(['Time'], axis=1)\ndata_preproc = data_preproc.drop(['Amount'], axis=1)","d430c38a":"# Let's standardize features except target (\"Class\")\nstand_scl = StandardScaler()\nscaled_df = stand_scl.fit_transform(data_preproc.drop(['Class'], axis=1))","3e67525d":"# Let's create DataFrame containing standardized features and target\ncolumns_name = list(data_preproc)\ncolumns_name.remove('Class')\nscaled_df = pd.DataFrame(scaled_df, columns=columns_name)\nscaled_df['Class'] = df[['Class']]","03a76cb8":"scaled_df.head()","dec6f350":"X = scaled_df.drop(['Class'], axis=1)\ny = scaled_df.Class.ravel()","38fe4982":"# Splitting dataset on train and test samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","f8a5c11e":"#using SMOTE algorithm\nsmote = SMOTE()\n\n# fit X_train and y_train\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n\nprint('Distribution of classes before SMOTE:', Counter(y_train))\nprint('Distribution of classes before SMOTE:', Counter(y_train_smote))","59bdd79b":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train_smote, y_train_smote)","9aa51919":"y_pred = lr_clf.predict(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))","646e35a7":"list_of_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\nfor threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:    \n    \n\n    # Creating and fitting Logistic Regression Classifier on train data\n    lr_clf = LogisticRegression()\n    lr_clf.fit(X_train_smote, y_train_smote)\n\n    # Making a prediction on test data. Evaluating metrics (roc_auc_score, recall_score, precision_score)\n    y_pred_proba = lr_clf.predict_proba(X_test)\n    y_pred = np.where(y_pred_proba[:, 1] > threshold, 1, 0)\n    r_a_score = roc_auc_score(y_test, y_pred)\n    rec_score = recall_score(y_test, y_pred)\n\n    print('threshold:', threshold)\n    print()\n    print('Primary test sample')\n    print('roc_auc_score:', r_a_score)\n    print('recall_score:', rec_score)\n    print()\n    print()","94db974f":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train_smote, y_train_smote)\n\ny_pred_proba = lr_clf.predict_proba(X_test)\ny_pred = np.where(y_pred_proba[:, 1] > 0.2, 1, 0)\n\nprint('roc_auc_score:', roc_auc_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))","b2a9f680":"As we can see the best combination of roc_auc_score and recall_score reaches at the value of threshould 0.2.","2861e572":"As we can see there are no Nan objects","e49c1eb9":"# 1. Data exploration","a382cf53":"Let's augment data for the the minority class (Class == 1) by oversampling the dataset using Synthetic Minority Oversampling Technique (SMOTE).","2ed20719":"To solve the classification problem let's create and fit Logistic Regression Classifier","bacf08ac":"# 4. Logistic Regression","43258fe0":"We must maximize the recall score, because we have to detect as many frauds as possible.\nLet's try different thresholds of class detecting, and chose the best one.","c76fe906":"# 2. Data preprocessing","2793235d":"<font size=\"3.5\">Before the data preprocessing let's create some graphs:\n* heatmap of correlations between features;\n* istribution of classes graph.<\/font>\n","f2623102":"# 3. SMOTE","aadab87a":"<font size=\"3.5\">After studing the graphs can make the next inferences:\n* 'Time' and 'Amount' have noteble correlation with some other features. We can drop this features;\n* on the distribution of classes graph we can see totally class disbalance;\n* also we can standardize our data.<\/font>"}}