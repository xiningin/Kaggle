{"cell_type":{"5f3d8d4e":"code","b021459c":"code","9f851512":"code","8418e4ba":"code","69859b87":"code","7cd03ff4":"code","0257da6b":"code","1b0800d5":"code","416e5018":"code","072c8729":"code","a31c3640":"code","452be3a9":"code","2d8343a5":"code","70e157ec":"code","2f233181":"code","e752f5f0":"code","48559ca1":"code","37e7ac84":"code","abc0788f":"code","33d756cc":"code","a4ead108":"code","877474ee":"code","3b46f242":"code","508ad6b1":"code","a2416ccb":"code","277031e6":"code","4fd9abb5":"code","77f63417":"code","792995b2":"code","39cb0d9e":"code","655bea27":"code","dc376c55":"code","323fe9a8":"code","f7ee36a6":"code","1abb457b":"code","efce214e":"markdown","f2e35937":"markdown","11e49c2f":"markdown","eb6a2249":"markdown","dd862eea":"markdown","f8ea5f30":"markdown","1a22029b":"markdown","757f4a01":"markdown","f62b30f0":"markdown","b1e46a6f":"markdown","c475fbcd":"markdown","cda9091a":"markdown","fa4b35cc":"markdown","98aa91f8":"markdown"},"source":{"5f3d8d4e":"# environment init\n!pip install ..\/input\/panda-essential\/pretrained-models.pytorch-master\/pretrained-models.pytorch-master\/\n!pip install ..\/input\/landmark-additional-packages\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master\/\n!pip install ..\/input\/..\/input\/landmark-additional-packages\/timm-0.3.4-py3-none-any.whl\n\n!mkdir \/root\/.cache\/torch\n!mkdir \/root\/.cache\/torch\/hub\/\n!mkdir \/root\/.cache\/torch\/hub\/checkpoints\n!cp -r ..\/input\/landmark-additional-packages\/rwightman_gen-efficientnet-pytorch_master\/rwightman_gen-efficientnet-pytorch_master \/root\/.cache\/torch\/hub\/\n!cp -r ..\/input\/landmark-additional-packages\/tf_efficientnet_* \/root\/.cache\/torch\/hub\/checkpoints\n!cp -r ..\/input\/landmark-additional-packages\/se* \/root\/.cache\/torch\/hub\/checkpoints\n!cp -r ..\/input\/landmark-additional-packages\/resnet200d* \/root\/.cache\/torch\/hub\/checkpoints\n!cp -r ..\/input\/landmark-additional-packages\/dense* \/root\/.cache\/torch\/hub\/checkpoints","b021459c":"!ls \/root\/.cache\/torch\/hub\/checkpoints","9f851512":"!mkdir leaf\n!cp -r ..\/input\/leaf-search-7fa0ec2\/leaf-search\/* leaf\n!cp ..\/input\/leaf-search-7fa0ec2\/leaf-search\/notebook_infer.py .","8418e4ba":"import glob\nfrom subprocess import call\nimport numpy as np\nimport pandas as pd\nget_score = lambda x: float(x.split('\/')[-1][-10:-4])","69859b87":"cmd = ('python notebook_infer.py --model={} ' + \n        '--config={} ' +\n       '--df=..\/input\/cassava-leaf-disease-classification\/sample_submission.csv --output=test')","7cd03ff4":"model_1 = '..\/input\/leaf-0210-four-models\/0129_dense_dpp_cutmix_744_n1_sin_aug_cut_1_fn\/0129_dense_dpp_cutmix_744_n1_sin_aug_cut_1_fn'\nmodel_2 = '..\/input\/leaf-0129-eff-se50-three-model\/0129_eff_dpp_cutmix_eff_16_ext_v1_f0_n1_sin_aug_cut_1_744_beta6_f1\/0129_eff_dpp_cutmix_eff_16_ext_v1_f0_n1_sin_aug_cut_1_744_beta6_f1.yaml'\nmodel_3 = '..\/input\/leaf-0210-four-models\/0209_upload_eb7_eb7_fn\/0209_upload_eb7_eb7_f0.yaml'\nmodel_4 = '..\/input\/leaf-0210-four-models\/0209_upload_r200d_light_beta6_800_16_r200d_800_16_beta6_fn_16\/0209_upload_r200d_light_beta6_800_16_r200d_800_16_beta6_fn_16'\nmodel_5 = '..\/input\/leaf-0210-four-models\/0209_upload_se50_full_se50_744_sin_full_fn\/0209_upload_se50_full_se50_744_sin_full_fn'\nmodel_6 = '..\/input\/leaf-0129-eff-se50-three-model\/upload_clean_ext_no_ext_sin_clean_600_drop_snapmix_32_ext_v1_fn_n1_sin_aug_cut_0.25.yaml\/upload_clean_ext_no_ext_sin_clean_600_drop_snapmix_32_ext_v1_fn_n1_sin_aug_cut_0.25.yaml'","0257da6b":"sheep_models = []\nfor m in [model_1, model_2, model_3, model_4, model_5, model_6]:\n    preds = []\n    for f in range(5):\n        mdl = glob.glob(f'{m}\/f{f}\/*.pth')[0]\n        cfg = glob.glob(f'{m}\/f{f}\/*.json')[0]\n        call(cmd.format(mdl, cfg), shell=True)\n        preds.append(np.load('test.npy'))\n    pred = np.zeros_like(preds[0])\n    for x in preds:\n        pred += x\n    pred = pred \/ 5\n    sheep_models.append(pred)","1b0800d5":"sheep_models","416e5018":"import os\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport pandas as pd\nimport numpy as np\nimport time\nimport cv2\nimport PIL.Image\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nimport joblib\nimport lightgbm as lgb\nimport timm\nimport glob\ndevice = torch.device('cuda') ","072c8729":"class CLDDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        image = cv2.imread(row.filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image']\n        \n        image = image.astype(np.float32)\n        image = image.transpose(2,0,1)\n        if self.mode == 'test':\n            return torch.tensor(image).float()\n        else:\n            return torch.tensor(image).float(), torch.tensor(row.label).float()","a31c3640":"\n\nclass CustomTimmModel(nn.Module):\n    def __init__(self, backbone, out_dim, pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(backbone, pretrained=pretrained)\n        if 'efficientnet' in backbone or 'densenet' in backbone:\n            in_ch = self.model.classifier.in_features\n            self.model.classifier = nn.Identity() \n        elif 'resnext' in backbone or 'resnet' in backbone or 'resnest' in backbone:\n            in_ch = self.model.fc.in_features\n            self.model.fc = nn.Identity() \n        elif 'vit' in backbone:\n            in_ch = self.model.head.in_features\n            self.model.head = nn.Identity()\n        elif 'csp' in backbone:\n            in_ch = self.model.head.fc.in_features\n            self.model.head.fc = nn.Identity()\n            \n        self.myfc = nn.Linear(in_ch, out_dim)\n    def forward(self, x):\n        x = self.model(x)\n        x = self.myfc(x)\n        return x\n\n\n\nclass CLDResNext(nn.Module):\n\n    def __init__(self, backbone, out_dim, pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(backbone, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, out_dim)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\nclass CLDVit(nn.Module):\n    def __init__(self, backbone, out_dim, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(backbone, pretrained=pretrained)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, out_dim)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n\ndef inference_func(test_loader):\n    model.eval()\n    print('Predicting...')\n    LOGITS = []\n    PREDS = []\n    \n    with torch.no_grad():\n        for images in test_loader:\n            x = images.to(device)\n            logits = model(x)\n            LOGITS.append(logits.cpu())\n            PREDS += [torch.softmax(logits, 1).detach().cpu()]\n        PREDS = torch.cat(PREDS).cpu().numpy()\n        LOGITS = torch.cat(LOGITS).cpu().numpy()\n    return PREDS\n\n\n\ntest = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/sample_submission.csv')\ntest['filepath'] = test.image_id.apply(lambda x: os.path.join('..\/input\/cassava-leaf-disease-classification\/test_images', f'{x}'))\n\n","452be3a9":"sin_models = []","2d8343a5":"batch_size = 32\nimage_size = 384\nmodel_type = ['vit_base_patch16_384'] * 5\nmodel_path = ['..\/input\/cass-vit-base-384\/fold0.pth', \n              '..\/input\/cass-vit-base-384\/fold1.pth', \n              '..\/input\/cass-vit-base-384\/fold2.pth',\n              '..\/input\/cass-vit-base-384\/fold3.pth',\n              '..\/input\/cass-vit-base-384\/fold4.pth']\n\ntransforms_valid = albumentations.Compose([\n    albumentations.CenterCrop(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CLDVit(model_type[i], out_dim=5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","70e157ec":"sin_models.append(test_preds)","2f233181":"batch_size = 32\nimage_size = 512\nmodel_type = ['tf_efficientnet_b4_ns'] * 5\nmodel_path = ['..\/input\/b4ns-512\/fold0.pth', \n              '..\/input\/b4ns-512\/fold1.pth', \n              '..\/input\/b4ns-512\/fold2.pth',\n              '..\/input\/b4ns-512\/fold3.pth',\n              '..\/input\/b4ns-512\/fold4.pth']\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CustomTimmModel(model_type[i], out_dim=5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]\n","e752f5f0":"sin_models.append(test_preds)","48559ca1":"batch_size = 24\nimage_size = 700\nmodel_type = ['resnest50d_1s4x24d'] * 5\nmodel_path = ['..\/input\/resnest50-pl-cv898\/fold0.pth', \n              '..\/input\/resnest50-pl-cv898\/fold1.pth', \n              '..\/input\/resnest50-pl-cv898\/fold2.pth',\n              '..\/input\/resnest50-pl-cv898\/fold3.pth',\n              '..\/input\/resnest50-pl-cv898\/fold4.pth',]\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CustomTimmModel(model_type[i], out_dim=5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","37e7ac84":"sin_models.append(test_preds)","abc0788f":"batch_size = 32\nimage_size = 648\nmodel_type = ['cspdarknet53'] * 5\nmodel_path = ['..\/input\/cspdarknet53\/fold0.pth', \n              '..\/input\/cspdarknet53\/fold1.pth', \n              '..\/input\/cspdarknet53\/fold2.pth',\n              '..\/input\/cspdarknet53\/fold3.pth',\n              '..\/input\/cspdarknet53\/fold4.pth',]\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CustomTimmModel(model_type[i], out_dim=5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","33d756cc":"sin_models.append(test_preds)","a4ead108":"sin_models = [np.mean(i, axis=0) for i in sin_models]","877474ee":"class CassavaClassifier(nn.Module):\n    def __init__(self, model_arch, num_classes, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        ### EffNet\n        if model_arch == 'tf_efficientnet_b4_ns' or model_arch == 'tf_efficientnet_b5_ns':\n            num_features = self.model.classifier.in_features\n            self.model.classifier = nn.Linear(num_features, num_classes)\n           \n        ### Vit\n        if model_arch == 'vit_base_patch16_384':\n            num_features = self.model.head.in_features\n            self.model.head = nn.Linear(num_features, num_classes)\n\n        ### ResNet\n        if model_arch == 'resnext50d_32x4d' or model_arch == 'resnet200d_320' or model_arch == 'seresnet152d_320':\n            num_features = self.model.fc.in_features\n            self.model.fc = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","3b46f242":"chen_models = []","508ad6b1":"batch_size = 32\nimage_size = 384\nmodel_type = ['vit_base_patch16_384'] * 5\nmodel_path = ['..\/input\/cldcvitb16sin\/models\/vit_base_patch16_384_fold0_best.ckpt', \n              '..\/input\/cldcvitb16sin\/models\/vit_base_patch16_384_fold1_best.ckpt', \n              '..\/input\/cldcvitb16sin\/models\/vit_base_patch16_384_fold2_best.ckpt',\n              '..\/input\/cldcvitb16sin\/models\/vit_base_patch16_384_fold3_best.ckpt',\n              '..\/input\/cldcvitb16sin\/models\/vit_base_patch16_384_fold4_best.ckpt',]\n\ntransforms_valid = albumentations.Compose([\n    albumentations.CenterCrop(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CassavaClassifier(model_type[i], 5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","a2416ccb":"chen_models.append(test_preds)","277031e6":"batch_size = 32\nimage_size = 512\nmodel_type = ['resnet200d_320'] * 5\nmodel_path = ['..\/input\/cldcresnet200d-320sin\/models\/resnet200d_320_fold0_best.ckpt', \n              '..\/input\/cldcresnet200d-320sin\/models\/resnet200d_320_fold1_best.ckpt', \n              '..\/input\/cldcresnet200d-320sin\/models\/resnet200d_320_fold2_best.ckpt',\n              '..\/input\/cldcresnet200d-320sin\/models\/resnet200d_320_fold3_best.ckpt',\n              '..\/input\/cldcresnet200d-320sin\/models\/resnet200d_320_fold4_best.ckpt',]\n\ntransforms_valid = albumentations.Compose([\n    albumentations.CenterCrop(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CassavaClassifier(model_type[i], 5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","4fd9abb5":"chen_models.append(test_preds)","77f63417":"batch_size = 32\nimage_size = 512\nmodel_type = ['tf_efficientnet_b4_ns'] * 5\nmodel_path = ['..\/input\/cldcb4sin\/models\/tf_efficientnet_b4_ns_fold0_best.ckpt', \n              '..\/input\/cldcb4sin\/models\/tf_efficientnet_b4_ns_fold1_best.ckpt', \n              '..\/input\/cldcb4sin\/models\/tf_efficientnet_b4_ns_fold2_best.ckpt',\n              '..\/input\/cldcb4sin\/models\/tf_efficientnet_b4_ns_fold3_best.ckpt',\n              '..\/input\/cldcb4sin\/models\/tf_efficientnet_b4_ns_fold4_best.ckpt',]\n\ntransforms_valid = albumentations.Compose([\n    albumentations.CenterCrop(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\ntest_dataset = CLDDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=4)\n\ntest_preds = []\nfor i in range(len(model_type)):\n    model = CassavaClassifier(model_type[i], 5, pretrained=False)\n    model = model.to(device)\n    model.load_state_dict(torch.load(model_path[i], map_location='cuda:0'))\n    test_preds += [inference_func(test_loader)]","792995b2":"chen_models.append(test_preds)","39cb0d9e":"chen_models = [np.mean(i, axis=0) for i in chen_models]","655bea27":"sin_models.extend(sheep_models)","dc376c55":"sin_models.extend(chen_models)","323fe9a8":"preds_overall = np.hstack(np.array(sin_models).squeeze())","f7ee36a6":"preds = []\nfor model in glob.glob('..\/input\/13model-stacking\/*.pkl'):\n    gbm = joblib.load(model)\n    try:\n        preds += [gbm.predict(preds_overall)]\n    except:\n        preds += [gbm.predict(preds_overall.reshape(1,-1))]","1abb457b":"submission = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/sample_submission.csv')\nsubmission.label = np.argmax(np.mean(preds, axis=0), axis=1)\nsubmission.to_csv('submission.csv', index=False)","efce214e":"## Model list\n```python\n'..\/input\/leaf-0210-four-models\/0129_dense_dpp_cutmix_744_n1_sin_aug_cut_1_fn\/0129_dense_dpp_cutmix_744_n1_sin_aug_cut_1_fn'\n'..\/input\/leaf-0129-eff-se50-three-model\/0129_eff_dpp_cutmix_eff_16_ext_v1_f0_n1_sin_aug_cut_1_744_beta6_f\/0129_eff_dpp_cutmix_eff_16_ext_v1_f0_n1_sin_aug_cut_1_744_beta6_f1.yaml'\n'..\/input\/leaf-0210-four-models\/0209_upload_eb7_eb7_fn\/0209_upload_eb7_eb7_f0.yaml'\n'..\/input\/leaf-0210-four-models\/0209_upload_r200d_light_beta6_800_16_r200d_800_16_beta6_fn_16\/0209_upload_r200d_light_beta6_800_16_r200d_800_16_beta6_fn_16'\n'..\/input\/leaf-0210-four-models\/0209_upload_se50_full_se50_744_sin_full_fn\/0209_upload_se50_full_se50_744_sin_full_fn'\n'..\/input\/leaf-0129-eff-se50-three-model\/upload_clean_ext_no_ext_sin_clean_600_drop_snapmix_32_ext_v1_fn_n1_sin_aug_cut_0.25.yaml\/upload_clean_ext_no_ext_sin_clean_600_drop_snapmix_32_ext_v1_fn_n1_sin_aug_cut_0.25.yaml'\n\n```","f2e35937":"## tf_efficientnet_b4_ns","11e49c2f":"## resnet200d-320","eb6a2249":"# sin","dd862eea":"## Darknet","f8ea5f30":"## Stacking","1a22029b":"## Preds","757f4a01":"## Resnest50","f62b30f0":"## B4ns_512","b1e46a6f":"# Sheep","c475fbcd":"## Combine Preds","cda9091a":"# Holy Chen","fa4b35cc":"## Vit","98aa91f8":"## Vit"}}