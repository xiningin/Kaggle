{"cell_type":{"a45fd4ed":"code","d4c7bfb5":"code","c31d0c92":"code","779e9e2d":"code","a3bc12cf":"code","2e9918fe":"code","69b866da":"code","09cf7625":"code","6253f271":"code","b179440f":"code","ece2e073":"code","5de6479e":"code","ac4fa5c0":"code","47a25378":"code","95a173a5":"code","4f52e023":"code","32dfb572":"code","d11cb5c7":"code","6f9f2150":"code","856ab7af":"code","ace38cd0":"code","7e538839":"code","5fc15ebb":"code","7ef806e3":"code","792ad988":"code","95c8c09c":"code","f5608593":"code","f77c535a":"code","c9c642fd":"code","5eac4750":"code","b705280a":"code","79127ff4":"code","e74ba4f7":"code","3e984299":"code","eeb44efd":"code","2d9af0d9":"markdown","2cc86ce4":"markdown","7ff2667e":"markdown"},"source":{"a45fd4ed":"# notebook settings\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# increase plot resolution\n%config InlineBackend.figure_format = 'retina'","d4c7bfb5":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","c31d0c92":"from sklearn.decomposition import PCA","779e9e2d":"# Custom functions\ndef run_pca(data, meta, n_components=50, data_only=False):\n    \"\"\"Applies PCA to `data` and returns a tuple of principle components with metadata and just principle components.\"\"\"\n    pca = PCA(n_components=n_components)\n    data_pca = pca.fit_transform(data)\n    print(\"Explained variance ratio: {}\".format(np.sum(pca.explained_variance_ratio_)))\n    # create dataframe\n    data_pca = pd.DataFrame((data_pca),\n                             index=data.index,\n                             columns=[\"PCA_{}\".format(i) for i in range(n_components)])\n    # join with metadata\n    joined = meta.merge(data_pca, how='inner', left_index=True, right_index=True)\n    \n    if data_only:\n        return joined.drop(columns=meta.columns)\n    else:\n        return joined","a3bc12cf":"# metadata\nmeta = pd.read_csv('\/kaggle\/input\/ai4all-project\/data\/metatable_with_viral_status.csv', delimiter=',', index_col=0)\n# get viral calls data\nviral = pd.read_csv(\"\/kaggle\/input\/ai4all-project\/results\/viral_calls\/viruses_with_pval.csv\", delimiter=\",\", index_col=0)\n# samples to CZB_IDS\nsample_map = pd.read_csv(\"\/kaggle\/input\/ai4all-project\/data\/viral_calls\/ngs_samples.csv\", index_col=0)","2e9918fe":"# merge tables\nviral = viral.join(sample_map, how='inner')\n# remove rows with non-significant viral detection\nrows, cols = viral.shape\nviral = viral[viral['p_val']<=0.05]\nprint(\"Removed {} rows\".format(rows - viral.shape[0]))\n# now lets pivot\nviral_pivot = viral.pivot_table(values='nt_rpm', index='CZB_ID', columns='name', fill_value=0.)\nviral_pivot.head()","69b866da":"viral['name'].value_counts().head()","09cf7625":"sc2_meta = meta[meta['viral_status']=='SC2']\nsc2_viral = viral[viral['CZB_ID'].isin(sc2_meta.index)]\ncoinf = sc2_viral['CZB_ID'].value_counts()[sc2_viral['CZB_ID'].value_counts() > 1].index","6253f271":"coinf_vir = viral[viral['CZB_ID'].isin(coinf)]['name'].value_counts()\ncoinf_vir.head(10)\ncoinf_vir = coinf_vir[coinf_vir > 3].index","b179440f":"viral[(viral['CZB_ID'].isin(coinf)) & (viral['name'].isin(coinf_vir))].boxplot('nt_rpm', by='name', rot=45)","ece2e073":"all_viral_pca = run_pca(viral_pivot, meta)\nall_viral_pca.head()","5de6479e":"sns.scatterplot(x=\"PCA_0\", y=\"PCA_1\", hue='viral_status', data=all_viral_pca)","ac4fa5c0":"# is this outlier skewing our PCA?\n# lets remove and rerun\noutlier = all_viral_pca['PCA_1'].nlargest(1).index\n\nall_viral_pca = run_pca(viral_pivot.drop(index=outlier), meta.drop(index=outlier))","47a25378":"sns.scatterplot(x=\"PCA_0\", y=\"PCA_1\", hue='viral_status', data=all_viral_pca)","95a173a5":"# ok, thhe scales seem to be extreme so lets rescale each feature\nfrom sklearn.preprocessing import minmax_scale\nscaled_viral_pivot = pd.DataFrame(minmax_scale(viral_pivot),\n                                  index=viral_pivot.index,\n                                  columns=viral_pivot.columns)","4f52e023":"all_viral_pca = run_pca(scaled_viral_pivot.drop(index=outlier), meta.drop(index=outlier))","32dfb572":"sns.scatterplot(x=\"PCA_0\", y=\"PCA_1\", hue='viral_status', data=all_viral_pca)","d11cb5c7":"# ok lets try this with only SC2+ patients because the variance from some outliers is still skewing the results\nsc2_meta = meta[meta['viral_status']=='SC2']\nsc2_scaled_viral_pivot = scaled_viral_pivot.reindex(sc2_meta.index).dropna()","6f9f2150":"sc2_viral_pca = run_pca(sc2_scaled_viral_pivot, sc2_meta)","856ab7af":"sns.scatterplot(x=\"PCA_0\", y=\"PCA_1\", hue='SC2_rpm', data=sc2_viral_pca)","ace38cd0":"# again, is this outlier skewing our PCA?\n# lets remove and rerun\noutlier = sc2_viral_pca['PCA_0'].nlargest(1).index\n\nsc2_viral_pca = run_pca(sc2_scaled_viral_pivot.drop(index=outlier), sc2_meta.drop(index=outlier))","7e538839":"sns.scatterplot(x=\"PCA_0\", y=\"PCA_1\", hue='SC2_rpm', data=sc2_viral_pca)","5fc15ebb":"import torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split","7ef806e3":"# utility functions\ndef get_device():\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n    return device","792ad988":"class Autoencoder(nn.Module):\n    def __init__(self, n_features):\n        self.n_features = n_features\n        super(Autoencoder, self).__init__()\n \n        # encoder\n        self.enc1 = nn.Linear(in_features=self.n_features, out_features=128)\n        self.enc2 = nn.Linear(in_features=128, out_features=64)\n        self.enc3 = nn.Linear(in_features=64, out_features=16)\n        self.enc4 = nn.Linear(in_features=16, out_features=2)\n \n        # decoder\n        self.dec1 = nn.Linear(in_features=2, out_features=16)\n        self.dec2 = nn.Linear(in_features=16, out_features=64)\n        self.dec3 = nn.Linear(in_features=64, out_features=128)\n        self.dec4 = nn.Linear(in_features=128, out_features=self.n_features)\n \n    def encoder(self, x):\n        x = F.relu(self.enc1(x))\n        x = F.relu(self.enc2(x))\n        x = F.relu(self.enc3(x))\n        x = F.relu(self.enc4(x))\n        \n        return x\n    \n    def decoder(self, x):\n        x = F.relu(self.dec1(x))\n        x = F.relu(self.dec2(x))\n        x = F.relu(self.dec3(x))\n        x = F.relu(self.dec4(x))\n        \n        return x\n    \n    def get_encodings(self, x):\n        self.eval()\n        with torch.no_grad():\n            x = self.encoder(x)\n\n        return x\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n\n        return x","95c8c09c":"def train(net, trainloader, testloader, NUM_EPOCHS):\n    train_loss = []\n    test_loss = []\n    for epoch in range(NUM_EPOCHS):\n        running_loss = 0.0\n        net.train()\n        for data in trainloader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            outputs = net(data)\n            loss = criterion(outputs, data)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        loss = running_loss \/ len(trainloader)\n        train_loss.append(loss)\n        \n        # Test at the end of each epoch\n        test_loss_epoch = test(net, testloader)\n        test_loss.append(test_loss_epoch)\n        print('Epoch {} of {}, Train Loss: {:.3f}, Test Loss: {:.3f}'.format(\n            epoch+1, NUM_EPOCHS, loss, test_loss_epoch))\n\n    return train_loss, test_loss\n\ndef test(net, testloader):\n    running_loss = 0.0\n    net.eval()\n    with torch.no_grad():\n        for data in testloader:\n            data = data.to(device)\n            outputs = net(data)\n            loss = criterion(outputs, data)\n            running_loss += loss.item()\n    return running_loss \/ len(testloader)","f5608593":"device = get_device()\ndevice\n\n# hyperparameters\nNUM_EPOCHS = 100\nLEARNING_RATE = 1e-6\nBATCH_SIZE = 8","f77c535a":"# Convert pd dataframe to pytorch tensor\nX = torch.tensor(sc2_scaled_viral_pivot.to_numpy(), device=device).float()\n\n# train\/test split\nX_train, X_test, train_idx, test_idx = train_test_split(X, sc2_scaled_viral_pivot.index, test_size=0.15, random_state=42)\nX_train.shape, X_test.shape","c9c642fd":"# dataloaders\ntrainloader = DataLoader(\n    X_train, \n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\ntestloader = DataLoader(\n    X_test, \n    batch_size=BATCH_SIZE, \n    shuffle=True\n)","5eac4750":"# define network\nnet = Autoencoder(n_features=X_train.shape[1])\nprint(net)\n\n# define loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n\n# load the neural network onto the device\nnet.to(device)\n\n# train the network\ntrain_loss, test_loss = train(net, trainloader, testloader, NUM_EPOCHS)","b705280a":"plt.plot(train_loss, label='train')\nplt.plot(test_loss, label='test')\nplt.legend()\nplt.title('Training and Testing Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')","79127ff4":"train_enc = net.get_encodings(X_train)\ntest_enc = net.get_encodings(X_test)","e74ba4f7":"all_enc = pd.DataFrame(np.concatenate((train_enc, test_enc)),\n                       index=np.concatenate((train_idx, test_idx)),\n                       columns=['AE_{}'.format(i) for i in range(2)])\nall_enc['split'] = np.concatenate((np.repeat('train', len(train_idx)),\n                                   np.repeat('test', len(test_idx))))\nall_enc = all_enc.join(meta)","3e984299":"sns.scatterplot(x='AE_0', y='AE_1', hue='split', data=all_enc)","eeb44efd":"sns.scatterplot(x='AE_0', y='AE_1', hue='SC2_rpm', data=all_enc)","2d9af0d9":"## Visualize Latent Variables","2cc86ce4":"### Next, apply an autoencoder to the gene expression data","7ff2667e":"# Autoencoder in PyTorch\nPlease read this: https:\/\/debuggercafe.com\/implementing-deep-autoencoder-in-pytorch\/"}}