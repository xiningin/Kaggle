{"cell_type":{"129d139b":"code","d3e10b46":"code","237b9e89":"code","17b00e81":"code","fa21132c":"code","112e0e04":"code","a23c5630":"code","488042c3":"code","6dcf684b":"code","753fc8f6":"code","c1867a5b":"code","8076b88a":"code","de6578fe":"code","eafde80a":"code","47c6c1e4":"code","fe9bf68e":"code","8cb29584":"code","19327429":"code","273eb1f7":"code","a788db85":"code","3a53a667":"code","fc149d22":"code","57fe6c36":"code","10ceacbf":"code","57374f4a":"code","d44a291e":"code","b43a0402":"code","829f5231":"code","40b929cb":"code","0e9c5e5b":"code","39a756bb":"code","c4bff601":"code","f9f6d6b2":"code","95b3991c":"code","147c7ecd":"code","2f798446":"code","ce135564":"code","b849f211":"code","be978396":"markdown","13dd4918":"markdown","77851e97":"markdown","5c049e47":"markdown","184c14bb":"markdown","ebfcd2aa":"markdown","da3120b4":"markdown","51169c6f":"markdown","c0dba66f":"markdown","097568e8":"markdown","b3b9da09":"markdown","b819b244":"markdown","23e06029":"markdown","a350fc10":"markdown","8f78a18f":"markdown","ef745f02":"markdown","9d51923a":"markdown","feed7969":"markdown","1e67d3ce":"markdown","2c6be26c":"markdown","a1d473b5":"markdown","339dd7f4":"markdown","473d7239":"markdown"},"source":{"129d139b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3e10b46":"import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.tsa.api as smt\nimport scipy.stats as scs\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","237b9e89":"sale_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitem_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')","17b00e81":"sale_df.head()","fa21132c":"print('Original dtypes', sale_df.info())\nsale_df.date = sale_df.date.apply(lambda x: datetime.datetime.strptime(x,\"%d.%m.%Y\"))\nprint(sale_df.info())","112e0e04":"sale_agg = sale_df.groupby([\"date_block_num\", \"shop_id\",\"item_id\"])[\"item_cnt_day\",\"item_price\"].agg({'item_cnt_day' : 'sum', 'item_price':'mean'}) \nsale_agg","a23c5630":"item_df.head()","488042c3":"# number of items per cat \nx=item_df.groupby(['item_category_id']).count()\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n","6dcf684b":"# #plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","753fc8f6":"sales_pm = sale_df.groupby([\"date_block_num\"]).agg({'item_cnt_day':'sum'})\nplt.title('Total sales of the company')\nplt.xlabel('Month')\nplt.ylabel('Sales')\nplt.plot(sales_pm)","c1867a5b":"rolling_window = sales_pm.rolling(12)\nplt.figure(figsize= (16,6))\nplt.plot(rolling_window.mean(), label = \"Rolling mean\")\nplt.plot(rolling_window.std(), label = \"Rolling standard deviation\")\nplt.legend()","8076b88a":"result1 = sm.tsa.seasonal_decompose(sales_pm,model='additive', freq=12)\nfig1 = result1.plot()","de6578fe":"result2 = sm.tsa.seasonal_decompose(sales_pm, model ='multiplicative', freq=12)\nfig2 = result2.plot()","eafde80a":"def adf_test(timeseries):\n    result = adfuller(timeseries, autolag='AIC')\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    for key, value in result[4].items():\n        print('Critial Values:')\n        print(f'   {key}, {value}')","47c6c1e4":"adf_test(sales_pm)","fe9bf68e":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","8cb29584":"ts=sales_pm.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","19327429":"adf_test(new_ts)","273eb1f7":"def tsplot(y, lags = None, figsize = (10,8), style ='bmh', title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):\n        fig = plt.figure(figsize=figsize)\n        layout = (3,2)\n        ts_ax=plt.subplot2grid(layout, (0,0), colspan = 2)\n        acf_ax = plt.subplot2grid(layout, (1,0))\n        pacf_ax = plt.subplot2grid(layout,(1,1))\n        qq_ax = plt.subplot2grid(layout,(2,0))\n        pp_ax = plt.subplot2grid(layout,(2,1))\n        \n        y.plot(ax = ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags = lags, ax= acf_ax, alpha = 0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax,alpha = 0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')\n        scs.probplot(y, sparams =(y.mean(),y.std()), plot=pp_ax )\n        plt.tight_layout()\n    return","a788db85":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = 1000\nalpha = 0.6\nx = w = np.random.normal(size = n_samples)\nfor t in range(n_samples):\n    x[t] = alpha * x[t-1] + w[t]\n_ = tsplot(x, lags = 12,title = \"AR(1)  process\" )","3a53a667":"# Simulate an AR(2) process\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\n\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma= ma, nsample = n)\n_ = tsplot(ar2, lags=12, title = \"AR(2) process\")","fc149d22":"# Simulate an MA(1) process\nn = int(1000)\n# set the AR(p) alphas equal to 0\nalphas = np.array([0.])\nbetas = np.array([0.8])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\nma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \nlimit=12\n_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")","57fe6c36":"# Simulate MA(2) process with betas 0.6, 0.4\nn = int(1000)\nalphas = np.array([0.])\nbetas = np.array([0.6, 0.4])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n_ = tsplot(ma3, lags=12,title=\"MA(2) process\")","10ceacbf":"# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\nmax_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n\/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")","57374f4a":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","d44a291e":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            # mle = most likelihood estimate, nc = no constant\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","b43a0402":"ts = sale_df.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.index = pd.date_range(start = '2013-01-01', end = '2015-10-01', freq='MS')\nts = ts.reset_index()\nts.head()","829f5231":"from fbprophet import Prophet\n#prophet reqiures a pandas df at the below config \n# ( date column named as DS and the value column as Y)\nts.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(ts) #fit the model with your dataframe","40b929cb":"# predict for five months in the future and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 5, freq = 'MS')  \n# now lets make the forecasts\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","0e9c5e5b":"fig = model.plot(forecast)","39a756bb":"fig2 = model.plot_components(forecast)","c4bff601":"total_sales = sale_df.groupby([\"date_block_num\"])['item_cnt_day'].sum()\ndates = pd.date_range(start= '2013-01-01', end = '2015-10-01', freq = 'MS')\ntotal_sales.index = dates\ntotal_sales.head()","f9f6d6b2":"# get the unique combinations of item-store from the sales data at monthly level\nsale_pm_item = sale_df.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum()\n\n# arrange it conviniently to perform the hts \nsale_pm_item = sale_pm_item.unstack(level = -1).fillna(0)\nsale_pm_item = sale_pm_item.T\ndates = pd.date_range(start = '2013-01-01', end = '2015-10-01',freq = 'MS')\nsale_pm_item.index = dates\nsale_pm_item = sale_pm_item.reset_index()\nsale_pm_item.head()","95b3991c":"import time\nstart_time = time.time()\n\n# Bottoms up\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\n\nforecastsDict = {}\n\nfor node in range(len(sale_pm_item[0])):\n    nodeToForecast = pd.concat([sale_pm_item.iloc[:,0], sale_pm_item.iloc[:,node+1]], axis = 1)\n    # rename for prophet compatability\n    nodeToForecast.columns = [\"ds\", \"y\"]\n    growth = 'linear'\n    model = Prophet(growth, yearly_seasonality = True)\n    model.fit(nodeToForecast)\n    future = model.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = model.predict(future)\n    if (node== 10):\n        end_time=time.time()\n        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n        break\n    \n    ","147c7ecd":"sale_pm_shop = sale_df.groupby([\"date_block_num\", \"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nsale_pm_shop = sale_pm_shop.unstack(level = 1)\nsale_pm_shop = sale_pm_shop.fillna(0)\nsale_pm_shop.index = dates \nsale_pm_shop = sale_pm_shop.reset_index()\nsale_pm_shop.head()","2f798446":"start_time = time.time()\n\n# Bottoms up\n# Calculating the base forecasts using prophet\n# From HTSprophet package -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\n\nforecastsDict = {}\n\nfor node in range(len(sale_pm_shop[0])):\n    nodeToForecast = pd.concat([sale_pm_shop.iloc[:,0], sale_pm_shop.iloc[:,node+1]], axis = 1)\n    # rename for prophet compatability\n    nodeToForecast.columns = [\"ds\", \"y\"]\n    growth = 'linear'\n    model = Prophet(growth, yearly_seasonality = True)\n    model.fit(nodeToForecast)\n    future = model.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = model.predict(future)\n    ","ce135564":"for key in range(len(forecastsDict.keys())):\n    \n    f1 = np.array(forecastsDict[key].yhat)\n    f2 = f1[:,np.newaxis]\n    if key == 0:\n        predictions = f2.copy()\n    else:\n        predictions = np.concatenate((predictions, f2), axis = 1)","b849f211":"predictions[-1]","be978396":"Plot rolling mean and standard deviation with a window of 12( one year)","13dd4918":"**Aggregate to monthly sale for each shop item pair**","77851e97":"Let's use a systematic approach to finding the order of AR and MA processes.","5c049e47":"# AR(1) process -- has ACF tailing out and PACF cutting off at lag=1","184c14bb":"**Plot no of items in each category.**","ebfcd2aa":"# AR(2) process -- has ACF tailing out and PACF cutting off at lag=2","da3120b4":"**We've correctly identified the order of the simulated process as ARMA(2,2).**\nLets use it for the sales time-series.","51169c6f":"# **Plot total sales per month.**\n\nThe objective requires us to predict sales for the next month at a store-item combination.\n\nSales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n\nI've chosen to predict for the total sales per month for the entire company.\n\nFirst let's compute the total sales per month and plot that data.","c0dba66f":"Simply use best_mdl.predict() to predict the next values\n","097568e8":"# Prophet:\nRecently open-sourced by Facebook research. It's a very promising tool, that is often a very handy and quick solution to the frustrating flatline","b3b9da09":"From the statistics, we see that the data is stationary now. We also check for 12 lag differenced on the de-trended data to check its stationarity.","b819b244":"The ADF Statistic value should be less than all critical values and p-value should be within 5% for data to be stationary. This is not the case here. So we de-trend the data first to check whether data is stationary or not.","23e06029":"Convert date column to datetime dtype","a350fc10":"# **AR, MA and ARMA models:**\nTL: DR version of the models:\n\nMA - Next value in the series is a function of the average of the previous n number of values AR - The errors(difference in mean) of the next value is a function of the errors in the previous n number of values ARMA - a mixture of both.\n\nNow, How do we find out, if our time-series in AR process or MA process?\n\nLet's find out!","8f78a18f":"Read data","ef745f02":"Thus we find the order of our data is ()","9d51923a":"# MA(2) process -- has ACF cut off at lag=2","feed7969":"Count number of items in each category","1e67d3ce":"**Middle out:**\n* Let's predict for the store level","2c6be26c":"# Stationarity:\n\nStationarity refers to time-invariance of a series. (ie) Two points in a time series are related to each other by only how far apart they are, and not by the direction(forward\/backward)\n\nWhen a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary.\n\nThere are multiple tests that can be used to check stationarity.\n\nADF( Augmented Dicky Fuller Test)\nKPSS\nPP (Phillips-Perron test)\nLet's just perform the ADF which is the most commonly used one.","a1d473b5":"We achieved better stationarity of the data with only de-trending. Hence we will use that data itself. \nNow after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series. \nWe can easily get back the original series using the inverse transform function that we have defined above.","339dd7f4":"# Quick observations: \nThere is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".\n\nLet's check that with a quick decomposition into Trend, seasonality and residuals.","473d7239":"# MA(1) process -- has ACF cut off at lag=1"}}