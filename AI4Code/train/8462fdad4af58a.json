{"cell_type":{"2e4e2d0c":"code","88387b38":"code","4dc9de66":"code","46ff4c2a":"code","a236ba25":"code","087ffd66":"code","991458e6":"code","14d099e5":"code","769fd379":"code","3d2c5c61":"code","25d38a84":"code","00f94fc2":"code","a049929d":"code","d3688dda":"code","6e09d711":"code","4a60df9a":"code","c11a12fb":"code","0a7acb17":"code","55dc0717":"code","d3720f13":"code","89be1755":"code","9b44a234":"code","f65c9776":"code","b9f2311e":"code","89e721aa":"code","72ecf40c":"code","99640a87":"code","d9d17ce0":"code","751fbd82":"code","2356a509":"code","6f9f15af":"code","3cc0989a":"code","95dbe169":"code","01b29d13":"code","b6ff6278":"code","e5221960":"markdown","3c00a656":"markdown","3d050f9e":"markdown","78c71313":"markdown","9f7c0c73":"markdown","fe1f0fce":"markdown","b6b53af6":"markdown","9f0473fd":"markdown","ee5592d5":"markdown","65943ada":"markdown","6784c7cb":"markdown","06fdf9b6":"markdown","565a120d":"markdown","81f58953":"markdown","e645c759":"markdown","24019013":"markdown","2c60c7b3":"markdown","963749b7":"markdown","16687bfe":"markdown","d9082365":"markdown","104adcbf":"markdown","9a723ca8":"markdown","cd82c1f9":"markdown","27a70fa7":"markdown","322d11cd":"markdown","0999cd1e":"markdown","f2b87086":"markdown","48bf392e":"markdown","194ba297":"markdown","affd9164":"markdown","8e64ebac":"markdown","b9cdc71d":"markdown","9fba7051":"markdown","baaaa241":"markdown","332d9880":"markdown","4a793c39":"markdown","ab2160e8":"markdown","9bd2e06a":"markdown","f7442718":"markdown"},"source":{"2e4e2d0c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","88387b38":"df = pd.read_csv('..\/input\/avocado.csv', index_col=0)\n\ndf.head(5)","4dc9de66":"df = df.reset_index(drop=True)\ndf['type'] = df['type'].astype('category')\ndf['type'] = df['type'].cat.codes","46ff4c2a":"filter1=df.region!='TotalUS'\ndata1=df[filter1]\n\nsorted_average = data1.groupby([\"region\"])['Total Volume'].aggregate(np.mean).reset_index().sort_values('Total Volume')\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.xticks(rotation=90)\nax=sns.barplot(x='region',y='Total Volume', data=data1, palette='magma', order=sorted_average['region'])","a236ba25":"filter2=df['type']==1\ndata2=df[filter2]\n\nsorted_average = data2.groupby([\"region\"])['AveragePrice'].aggregate(np.mean).reset_index().sort_values('AveragePrice')\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.xticks(rotation=90)\nplt.title('Organic, Average Price')\nax=sns.barplot(x='region',y='AveragePrice', data=data2, palette='magma', order=sorted_average['region'])","087ffd66":"filter2=df['type']==0\ndata2=df[filter2]\n\n\nsorted_average = data2.groupby([\"region\"])['AveragePrice'].aggregate(np.mean).reset_index().sort_values('AveragePrice')\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.xticks(rotation=90)\nplt.title('Conventional, Average Price')\nax=sns.barplot(x='region',y='AveragePrice', data=data2, palette='magma', order=sorted_average['region'])","991458e6":"filter3=df['region']!='TotalUS'\ndata3=df[filter3]\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.title('Average Price per year')\ng = sns.barplot(x = 'year', y = 'AveragePrice', hue='type', data=data3)","14d099e5":"filter3=df['region']=='TotalUS'\ndata3=df[filter3]\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.title('Total Volume per year (TotalUS only)')\ng = sns.barplot(x = 'year', y = 'Total Volume', hue='type', data=data3, estimator=sum)","769fd379":"filter5=df['type']==0\ndata5=df[filter5]\n\ng = sns.lmplot(x='Total Volume',y='AveragePrice', data=data5, fit_reg=True, height=8, aspect=1.2)\nfig = g.fig\nfig.suptitle(\"Conventional: Volume vs. Average Price\")\nplt.show()","3d2c5c61":"filter5=df['type']==1\ndata5=df[filter5]\n\ng = sns.lmplot(x='Total Volume',y='AveragePrice', data=data5, fit_reg=True, height=8, aspect=1.2)\nfig = g.fig\nfig.suptitle(\"Organic: Volume vs. Average Price\")\nplt.show()","25d38a84":"sns.clustermap(df.corr(), center=0, cmap=\"vlag\", annot = True, linewidths=.75, figsize=(13, 13));","00f94fc2":"g = sns.PairGrid(df)\ng = g.map_diag(plt.hist)\ng = g.map_offdiag(plt.scatter);","a049929d":"df['fregion'] = df['region'].values\ndf = pd.get_dummies(df, columns=['fregion'])\ndf.head()","d3688dda":"drop_list = ['AveragePrice', 'Date', 'region']\n\nX = df.drop(drop_list, axis=1)\ny = df['AveragePrice'].values.ravel()\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclf = RandomForestRegressor(n_estimators= 100, random_state=42)\nclf.fit(Xtrain,ytrain)\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\n\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\nprint(f\"MSE {mean_squared_error(y_pred=clf.predict(Xtest), y_true=ytest)}\")","6e09d711":"predictions = clf.predict(Xtest)\n\n#print(predictions)\n# Calculate the absolute errors\nerrors = abs(predictions - ytest)\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplt.hist(errors, bins = 10, edgecolor = 'black');\n# Print out the mean absolute error (mae)\n#print(f\"Test: {mean_squared_error(ytest, predictions)} \")\nprint(f\"R2 score: {r2_score(ytest, predictions)}\")\nprint(f\"Mean absolute error:  {mean_absolute_error(ytest, predictions)} USD\")","4a60df9a":"fig, ax = plt.subplots(figsize=(12, 8))\ng = sns.regplot(x = predictions,y = ytest)","c11a12fb":"# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n\nfeature_list = list(X.columns)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","0a7acb17":"EconomicAnalysisRegion = []\n\nfor region in df['region']:\n    if region in ['California', 'LasVegas', 'LosAngeles', 'Portland', 'Sacramento', 'SanDiego', 'SanFrancisco', 'Seattle', 'Spokane']:\n        EconomicAnalysisRegion.append('Far West')\n    elif region in ['Chicago', 'CincinnatiDayton', 'Columbus', 'Detroit', 'GrandRapids', 'Indianapolis']:\n        EconomicAnalysisRegion.append('Great Lakes')\n    elif region in ['GreatLakes']:\n        EconomicAnalysisRegion.append('GreatLakes')\n    elif region in ['Albany', 'BaltimoreWashington', 'BuffaloRochester', 'HarrisburgScranton', 'HartfordSpringfield', 'NewYork', 'Philadelphia', 'Pittsburgh', 'Syracuse']:\n        EconomicAnalysisRegion.append('Mideast')\n    elif region in ['Midsouth']:\n        EconomicAnalysisRegion.append('Midsouth') \n    elif region in ['Boston', 'HartfordSpringfield']:\n        EconomicAnalysisRegion.append('New England') \n    elif region in ['Northeast']:\n        EconomicAnalysisRegion.append('Northeast')\n    elif region in ['NorthernNewEngland']:\n        EconomicAnalysisRegion.append('NorthernNewEngland')\n    elif region in ['Plains', 'StLouis']:\n        EconomicAnalysisRegion.append('Plains')\n    elif region in ['Boise', 'Denver']:\n        EconomicAnalysisRegion.append('Rocky Mountains')\n    elif region in ['SouthCarolina']:\n        EconomicAnalysisRegion.append('SouthCarolina')\n    elif region in ['SouthCentral']:\n        EconomicAnalysisRegion.append('SouthCentral')\n    elif region in ['Atlanta', 'Charlotte', 'Jacksonville', 'Louisville', 'MiamiFtLauderdale', 'Nashville', 'NewOrleansMobile', 'Orlando', 'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Southeast']:\n        EconomicAnalysisRegion.append('Southeast')\n    elif region in ['DallasFtWorth', 'Houston', 'PhoenixTucson', 'Tampa']:\n        EconomicAnalysisRegion.append('SouthWest')\n    elif region in ['TotalUS']:\n        EconomicAnalysisRegion.append('TotalUS')\n    elif region in ['West']:\n        EconomicAnalysisRegion.append('West')\n    elif region in ['WestTexNewMexico']:\n        EconomicAnalysisRegion.append('WestTexNewMexico')\n        \n\ndf['Economic Analysis Region'] = EconomicAnalysisRegion\n\ndf = pd.get_dummies(df, columns=['Economic Analysis Region'])","55dc0717":"df['Date'] = pd.to_datetime(df['Date'])\ndf['month'] = df['Date'].dt.month\n\nseasons = []\n\nfor month in df['month']:\n    if month in [1, 2, 12]:\n        seasons.append('winter')\n    elif month in [3, 4, 5]:\n        seasons.append('spring')\n    elif month in [6, 7, 8]:\n        seasons.append('summer')\n    elif month in [9, 10, 11]:\n        seasons.append('fall')\n                \ndf['season'] = seasons\ndf = pd.get_dummies(df, columns=['season'])","d3720f13":"df['Date'] = pd.to_datetime(df['Date'])\ndf['week'] = df['Date'].dt.week.shift(-2).ffill()","89be1755":"df.head(5)","9b44a234":"drop_list = ['AveragePrice', 'Date', '4046', '4225', '4770', 'Total Bags', 'Total Volume', 'region', 'Small Bags', 'Large Bags', 'XLarge Bags']\n\nX = df.drop(drop_list, axis=1)\ny = df['AveragePrice'].values.ravel()\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = RandomForestRegressor(n_estimators= 100, random_state=42)\n\nclf.fit(Xtrain,ytrain)\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\n\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\nprint(f\"MSE {mean_squared_error(y_pred=clf.predict(Xtest), y_true=ytest)}\")","f65c9776":"predictions = clf.predict(Xtest)\n\n#print(predictions)\n# Calculate the absolute errors\nerrors = abs(predictions - ytest)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nplt.hist(errors, bins = 10, edgecolor = 'black');\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error: USD', round(np.mean(errors), 3))","b9f2311e":"# Get numerical feature importances\nimportances = list(clf.feature_importances_)\n\nfeature_list = list(X.columns)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","89e721aa":"from fbprophet import Prophet","72ecf40c":"m = Prophet()\ndf = pd.read_csv('..\/input\/avocado.csv', index_col=0)\ndf['Date'] = pd.to_datetime(df['Date'])\n\nmask = (df['region'] == 'TotalUS') & (df['type'] == 'conventional')\ndf = df[mask]\n\n# Change column names as Prophet requires.\ndf.rename(columns={'Date': 'ds', 'AveragePrice': 'y'}, inplace=True)\nm.fit(df);","99640a87":"future = m.make_future_dataframe(periods=52,freq='w')\nfuture.tail(3)","d9d17ce0":"forecast = m.predict(future)\nm.plot(forecast, xlabel = 'Date', ylabel = 'Price');","751fbd82":"fig2 = m.plot_components(forecast)","2356a509":"cmp_df = df.join(forecast.set_index('ds'), on='ds')\ncmp_df = cmp_df[cmp_df['y'].notnull()]","6f9f15af":"print(f\"MSE {mean_squared_error(y_pred=cmp_df.yhat, y_true=cmp_df.y)}\")\nprint(f\"R2 score: {r2_score(cmp_df.y, cmp_df.yhat)}\")\nprint(f\"Mean absolute error:  {mean_absolute_error(cmp_df.y, cmp_df.yhat)} USD\")","3cc0989a":"errors = abs(cmp_df.yhat - cmp_df.y)\nplt.hist(errors, bins = 15, edgecolor = 'black');","95dbe169":"sns.set(rc={'figure.figsize':(12,10)})\ng = sns.regplot(x = cmp_df.yhat,y = cmp_df.y)","01b29d13":"m = Prophet()\ndf = pd.read_csv('..\/input\/avocado.csv', index_col=0)\ndf['Date'] = pd.to_datetime(df['Date'])\n\nmask = (df['region'] == 'Houston') & (df['type'] == 'organic')\ndf = df[mask]\n\ndf.rename(columns={'Date': 'ds', 'AveragePrice': 'y'}, inplace=True)\nm.fit(df);\nfuture = m.make_future_dataframe(periods=52,freq='w')\nforecast = m.predict(future)\nm.plot(forecast, xlabel = 'Date', ylabel = 'Price');","b6ff6278":"cmp_df = df.join(forecast.set_index('ds'), on='ds')\ncmp_df = cmp_df[cmp_df['y'].notnull()]\n\nprint(f\"MSE {mean_squared_error(y_pred=cmp_df.yhat, y_true=cmp_df.y)}\")\nprint(f\"R2 score: {r2_score(cmp_df.y, cmp_df.yhat)}\")\nprint(f\"Mean absolute error:  {mean_absolute_error(cmp_df.y, cmp_df.yhat)} USD\")","e5221960":"So Prophet's prediction is 0.067 cents off on average which is 41 cents or 37.9% better than the Random Forest using the numeric features. It seems Prophet is definitely better suited for the job. ","3c00a656":"Lets now try another approach, lets see how Prophet, an algorithm developed for time series predictions, does with this data. ","3d050f9e":"# Bring in the Prophet","78c71313":"Hmmm, USD 0.126 absolute error. Maybe this Prophet cannot always see the future. :-D\n\nI have ran Prophet on other regions using the conventional type and results were often as good as for TotalUS. I guess it doesn't do so well with the organic data since price variability in this set is larger than for the convetional type. (Standard deviation: Organic -> 0.363502 vs Conventional -> 0.263041)","9f7c0c73":"Lets plot the predictions vs reality. ","fe1f0fce":"Lets see how the predictions look. \n\nWow, feature engineering does make all the difference. the MSE dropped below 0.02 and the absolute error improved by 0.0136 cents. And pretty much all the  predictions fall between 0 and 20 cents. So quite good after all. And better than guessing, right?  ","b6b53af6":"Lets add a few new features and see if they can replace some of these numeric features.  We will add features that would be readily available for future predictions.\n\nSince there are significant differences in prices among different regions lets add a grouping using the Economic Analysis Regions from the Bureau of Economic Analysis (BEA) of the United States Department of Commerce. \n\nLets also add seasons, since produce is usually very season dependant and it might help the model. Lets also extract the week number from the date. And lets finally convert the date to numeric months. ","9f0473fd":"Well type is an important feature for the model. Again, we were expecting that. But, we also can see how Large Bags, 4046, 4225, Total Volume, etc., basically all the numeric features, carry a decent weight in the model's predictions. And this for me is a **PROBLEM** since this data is totally dependant on sales activity. We wouldn't have this kind of data if we would attempt to predict future prices. Therefore I think these features cannot be considered as part of the solution. So lets try something else. ","ee5592d5":"Lets convert date to datetime type and also type to a category so they can be useful later in the model. And reset the index to avoid any potential duplicates.","65943ada":"Lets see which features were more important for the model. \n\nIts seems the type feature remains as strong as ever. But week proved to be a game changer, and month and year scores well too. However the other new features are almost ignored. Type and dates are really the features that are important for the model to predict the prices. ","6784c7cb":"Lets predict prices for 52 weeks into the future. ","06fdf9b6":"Lets now try it with the Houston data and the organic variety and see if we get similar results.","565a120d":"Finally, lets do a pairing of all the numeric features in order to clearly visualize the above results. ","81f58953":"# Models","e645c759":"Lets train the new model dropping the features which are not useful and only considering the new features. \n\nThe result show a slight increase in accuracy compared to the previous training. So it seems the new features did some good. Lets see how the predictions fair out. ","24019013":"# Now lets see how the features correlate with each other.\n\nAll the numeric features correlate highly with each other and none with the label (AveragePrice). Type is moderately correlated to AveragePrice which was evident by the differences in prices between conventional and organic avocados which was displayed in the visualizations. ","2c60c7b3":"Since this is a time series algorithm, we cannot train all the data together, so we need to look at one region at a time. Lets start with TotalUS and also the conventional type. Doesnt make sense to mix the types since there are significant price differences between the two","963749b7":"There are obvious differences between regions\/cities in terms of volumes. Some like their guacamole more than others. ","16687bfe":"# Train the new model","d9082365":"90%+ of the predictions fall between 0 and 0.20 cents absolute difference. ","104adcbf":"## Visualizations","9a723ca8":"Obvious price differences between organic and conventional, which is no surprise since it costs more to grow organic and this \ntype of fruit caters to a premium segment of the market. Except for 2017, prices for conventional avocados have been quite stable on average. ","cd82c1f9":"# Lets train the data and score it. \n\nNice, the result makes it seems like this is the right model to use. ","27a70fa7":"Lets convert our region feature so we can use it in the model. We will keep the region column for later. ","322d11cd":"# Lets see how Volumes and Average Prices relate to each other for conventional and organic avocados. \n\nThere is a slight tendency for lower prices when volumes are significantly large, which is usually what happens in any market in terms of pricing.  ","0999cd1e":"Year after year the volumes are growing steadily for both kinds of hass avocado. ","f2b87086":"Lets plot the predictions vs the targets. ","48bf392e":"# New approach","194ba297":"## Disclaimer: this is my first notebook, so excuse my mistakes or incorrect assumptions. I have tried to apply everything I have been learning by looking at people\u00b4s kernels and reading ML theory.  So this first kernel is a mix of visualizations, feature engineering and model comparisson. \n\nSo lets start and try to predict avocados future prices.","affd9164":"# One more run for Prophet","8e64ebac":"Lets begin by loading the data.","b9cdc71d":"The story repeats itself with the conventional type, it is much cheaper for states closer to Mexico.","9fba7051":"Lets prepare the data so we can compare the predctions with the historical data. ","baaaa241":"Lets check out now which features are important for the model. ","332d9880":"# If you got this far, thank you for you patience and attention!!! \n\n# If you have comments or suggestions I will be happy to read them, thanks in advance. ","4a793c39":"Visualizations are always nice but lets get to the reason we are all here. Lets start with Random Forest which seems to be the more suitable algorithm for this job. ","ab2160e8":"Not everyones pays the same for organic avocados, southern states (close to Mexico) seem to benefit from better prices. This is not surprising taking into account Mexico represent 45% of all the avocado exports in the world. Ay caramba!","9bd2e06a":"And plot the predictions along with the historical data. ","f7442718":"And now lets evaluate the predictions. We have USD 0.1026 difference on average on the predictions given by the Random Forest. Not bad. And lets see how the predictions look in a histogram. Not too bad, most prediction fall between 0 and 20 cents. "}}