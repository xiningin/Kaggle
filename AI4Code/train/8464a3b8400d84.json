{"cell_type":{"0c004208":"code","9722027a":"code","dd34eb12":"code","6f694f80":"code","9ea788fc":"code","89d65a7c":"code","43b930b6":"code","1224242e":"code","e883a0fb":"code","d9ae106f":"code","12098503":"code","fbfca8a6":"markdown","ca40371c":"markdown","213ecf02":"markdown","8b80246c":"markdown","6739d71a":"markdown","c7fd637e":"markdown","4bd4aa1d":"markdown","1b5992f4":"markdown","d2d8cdc7":"markdown","ae567c54":"markdown","6bbdbc5a":"markdown","cfcc41ab":"markdown","f35961a3":"markdown","a165fe13":"markdown","e60000df":"markdown"},"source":{"0c004208":"****************# TP 1 de data mining\n# # \n# # Dans ce TP, nous vous proposons de commencer par cr\u00e9er des fonctions utiles pour lire un fichier csv, puis de manipuler certains classifieurs.\n# # \n# # Apr\u00e8s chaque question, on vous demande de cr\u00e9er un bloc de code afin d'y d\u00e9poser votre r\u00e9ponse. N'h\u00e9sitez pas \u00e0 commenter et\/ou \u00e0 cr\u00e9er un autre block de markdown pour r\u00e9pondre aux questions.\n# # \n# # ## 0.\n# # Incorporer la base de travail 'credit card fraud detection' \u00e0 ce notebook.\n# # ## 1.\n# # \n# # \n**# # Ouvrir le fichier *creditcard.csv*. Attention, ce fichier se trouve dans le dossier *creditcardfraud* qui lui-m\u00eame se trouve dans le dossier parent du dossier courant (ouf).\n# # \n# # Le fichier devra \u00eatre lu et enregistr\u00e9 dans deux variables, *X* et *Y*, de dimensions respectives *NxF* et *N* o\u00f9 *N* est le nombre d'exemples du jeu de donn\u00e9es et *F* le nombre d'attributs. *X* contiendra les features et *Y* la classe de sortie qui doit \u00eatre identifi\u00e9e au pr\u00e9alable (apprentissage supervis\u00e9). Toutes les valeurs de *X* doivent \u00eatre converties en type *float*.\n# # \n# # Affichez \u00e9galement le nombre d'exemples de la base, le nombre de features et la liste compl\u00e8te de toutes les classes.****","9722027a":"import pandas as pd \ndata=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head(3)\nX=data.drop('Class', axis=1)\nY=data.Class\ndata.head(3)","dd34eb12":"display(data.shape, data.Class.unique())","6f694f80":"df_c = data.select_dtypes(include=['int64']).copy()\nlabels = df_c['Class'].astype('category').cat.categories.tolist()\nreplace_map = {'Class' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}","9ea788fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y,train_size=0.7, random_state=5)\nprint(X_train.size,X_test.size,y_train.size,y_test.size )","89d65a7c":"from sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)","43b930b6":"knn.predict (X_test)[0:5]\nknn.score (X_test, y_test)","1224242e":"from sklearn import metrics\ny_pred = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","e883a0fb":"from sklearn import neighbors\nknn1 = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn1.fit(X_train, y_train)\nknn1.predict (X_test)[0:5]\nknn1.score (X_test, y_test)","d9ae106f":"\n\nfrom sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB(binarize=0.0)\nbnb.fit(X_train, y_train)\nBernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\nbnb.score(X_test,y_test)","12098503":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nGaussianNB(priors=None)\ngnb.score(X_test,y_test)","fbfca8a6":"## 10.\n# # \n# # Testez la methode gaussienne, comparez avec Bernoulli et concluez","ca40371c":"Les m\u00e9thodes de naive bayes sont:\n# # * .  **Gaussian Naive Bayes:** c'est un algorithme ayant une approche probabiliste d'apprentissage automatique utilis\u00e9 pour la classification bas\u00e9 le th\u00e9or\u00e8me de Bayes.\n# # \n# # * **Multinomial Naive Bayes**: impl\u00e9mente l'algorithme na\u00efve Bayes pour les donn\u00e9es distribu\u00e9es multinomiales utilis\u00e9es dans la classification des textes.\n# # * **Complement Naive Bayes**:il a \u00e9t\u00e9 con\u00e7u pour corriger les \u00absevere assumptions\u00bb formul\u00e9es par le classificateur Multinomial Naive Bayes. Il est adapt\u00e9 aux ensembles de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9s.\n# # * **Bernoulli Naive Bayes**: Le classificateur Bernoulli Naive Bayes suppose que toutes nos fonctionnalit\u00e9s sont binaires.\n# # \n# # \n# #","213ecf02":"## 8.\n# # \n# # Faites une batterie de test en modifiant chaque param\u00e8tre de knn et concluez sur le meilleur r\u00e9sultat.\n# # En particulier, posez-vous les questions suvantes :\n# # - quel est l'effet de choisir un nombre de voisins plus ou moins important ?\n# # - quel est l'effet d'avoir des attributs fortement corr\u00e9l\u00e9s ?\n# # - quel est l'effet de modifier la m\u00e9trique ?\n# # (n'h\u00e9sitez pas \u00e0 mettre en \u00e9vidence l'effet des param\u00e8tres dans des codes idoines)","8b80246c":"## 7.\n# # \n# # Souvent, une seule m\u00e9trique n'est pas suffisante pour conclure. Des fonctions de mesure de la pr\u00e9cision et du rappel dans chaque classe sont disponibles dans *sklearn*. Utilisez les. D\u00e9terminez l'origine des differences de pr\u00e9cison et de rappel entre les deux classes et donnez votre conclusion.","6739d71a":"Pourquoi ne peut-on toujours pas affirmer ques les donn\u00e9es ainsi transform\u00e9es sont quantitatives ? Donner des pistes de r\u00e9flexion afin de d\u00e9terminer une meilleure pratique afin de transformer des donn\u00e9es quantitatives en des donn\u00e9es num\u00e9riques.\n# # indice : dans certains cas, on transformera une donn\u00e9e qualitative en plusieurs donn\u00e9es quantitatives.","c7fd637e":"n_neighbors: les voisins les plus proches d'un nouveau point de donn\u00e9es.","4bd4aa1d":"## 5.\n# # Utilisez la m\u00e9thode des plus proches voisins disponible dans sklearn. Utilisez la avec un seul voisin pour le moment et expliquez chacun des param\u00e8tres de la m\u00e9thode.\n# #","1b5992f4":"## 6.\n# # sklearn dispose de fonction permettant de calculer la pr\u00e9cision. Utilisez l\u00e0 ici. Le r\u00e9sultat obtenu vous semble-t-il bon ?","d2d8cdc7":"## 9.\n# # \n# # La biblioth\u00e8que sklearn propose plusieurs m\u00e9thodes de *naive bayes*. Listez les, expliquez chacune d'entre elles (il y en a 4) et testez Bernoulli. Concluez.","ae567c54":"Notre mod\u00e8le a une pr\u00e9cision d'environ 99,80%, je la consid\u00e8re comme une bonne pr\u00e9cision","6bbdbc5a":"## 2.\n# # Affichez le nombre d'exemples de la base, le nombre de features et la liste compl\u00e8te de toutes les classes (sur une m\u00eame ligne).\n# # \n# #","cfcc41ab":"****************# TP 1 de data mining\n# # \n# # Dans ce TP, nous vous proposons de commencer par cr\u00e9er des fonctions utiles pour lire un fichier csv, puis de manipuler certains classifieurs.\n# # \n# # Apr\u00e8s chaque question, on vous demande de cr\u00e9er un bloc de code afin d'y d\u00e9poser votre r\u00e9ponse. N'h\u00e9sitez pas \u00e0 commenter et\/ou \u00e0 cr\u00e9er un autre block de markdown pour r\u00e9pondre aux questions.\n# # \n# # ## 0.\n# # Incorporer la base de travail 'credit card fraud detection' \u00e0 ce notebook.\n# # ## 1.\n# # \n# # \n**# # Ouvrir le fichier *creditcard.csv*. Attention, ce fichier se trouve dans le dossier *creditcardfraud* qui lui-m\u00eame se trouve dans le dossier parent du dossier courant (ouf).\n# # \n# # Le fichier devra \u00eatre lu et enregistr\u00e9 dans deux variables, *X* et *Y*, de dimensions respectives *NxF* et *N* o\u00f9 *N* est le nombre d'exemples du jeu de donn\u00e9es et *F* le nombre d'attributs. *X* contiendra les features et *Y* la classe de sortie qui doit \u00eatre identifi\u00e9e au pr\u00e9alable (apprentissage supervis\u00e9). Toutes les valeurs de *X* doivent \u00eatre converties en type *float*.\n# # \n# # Affichez \u00e9galement le nombre d'exemples de la base, le nombre de features et la liste compl\u00e8te de toutes les classes.****","f35961a3":"## 3.\n# # \n# # La plupart des algorithmes ne travaillent pas avec des donn\u00e9es qualitatives mais avec des donn\u00e9es quantitatives. Cr\u00e9ez donc un algorithme capable de remplacer **AUTOMATIQUEMENT** chaque classe par un num\u00e9ro et remplacer les cha\u00eenes de caract\u00e8res de *Y* par ces num\u00e9ros puis affichez de nouveau la liste de toutes les classes.\n# #","a165fe13":"## 4.\n# # \n# # Maintenant que vous disposez de votre *X* et de votre *Y*, divisez les en *X_train*, *X_test*, *Y_train*, *Y_test* avec un ration train\/test de 70\/30. Attention, il faut que ce ratio soit respect\u00e9 pour chacune des classes et non pas pour l'ensemble du jeu de donn\u00e9es. Aussi, il faut *X* et *Y* soient d\u00e9coup\u00e9s de la m\u00eame fa\u00e7on (obvious). Il existe une m\u00e9thode de la biblioth\u00e8que *sklearn* qui permet de faire cela en une seule ligne. Par souci de tra\u00e7abilit\u00e9, utilisez l'argument qui permet de g\u00e9rer la graine de la partie al\u00e9atoire et fixez la \u00e0 5. Affichez \u00e9galement la taille de *X_train*, *X_test*, *Y_train* et *Y_test*.","e60000df":"## 11. Pandas\n# # Suivre sur kaggle le tutoriel sur Pandas.\n# # \n# #"}}