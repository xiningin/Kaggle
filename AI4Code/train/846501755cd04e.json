{"cell_type":{"670b094e":"code","48b116a2":"code","1d24f4c7":"code","561cd42d":"code","16d85dfe":"code","e8824602":"code","61499ed0":"code","5fc79cc9":"code","c259c5aa":"code","1ac0bdea":"code","2119dc4d":"code","ef660b3d":"code","c9c7b095":"code","c694bd2e":"code","2104a490":"code","83a9534a":"code","6218f35e":"code","e2a774c4":"code","4aa3f0b5":"code","1f37aefe":"code","d714a675":"markdown","19d29934":"markdown","d9c8b9f8":"markdown","3caa2cfe":"markdown","89170fc3":"markdown","3ee7be53":"markdown","2cf5eb8b":"markdown","5bee1389":"markdown"},"source":{"670b094e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48b116a2":"df=pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","1d24f4c7":"#General info about the data set attributes\ndf.describe()","561cd42d":"#Dropping unwanted columns\ndf=df.drop(['EmployeeNumber','EmployeeCount','StandardHours'], axis=1)\ndf.isna().sum() #Check if there is any 'NaN' values","16d85dfe":"df.isnull().values.any() #Check if there is missing values","e8824602":"#Creating correleation matrix showing connection rates between attributes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(30, 30))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\",annot_kws={\"size\":15})","61499ed0":"#Defining the categorial columns\ncategorial_col = df.select_dtypes(include=\"object\")\ncategorial_col.head()","5fc79cc9":"#Converting categorial attributions to numbers.\nfrom sklearn.preprocessing import LabelEncoder\nlr = LabelEncoder()\n\nfor i in categorial_col:\n    df[i]=lr.fit_transform(df[i])\n    \ndf[categorial_col.columns].head()","c259c5aa":"#Explaining the connection between categorial data to the target feature\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(font_scale=1.1)\nplt.figure(figsize=(30, 30))\n\nfor i, column in enumerate(categorial_col, 1):\n    plt.subplot(3, 3, i)\n    g = sns.barplot(x=f\"{column}\", y='Attrition', data=df)\n    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n    plt.ylabel('Attrition Count')\n    plt.xlabel(f'{column}')","1ac0bdea":"#Converting attrition values to binary [No Attrition=0, Yes Attrition=1]\ndf['Attrition'] = df.Attrition.astype(\"category\").cat.codes\ndf.Attrition.value_counts()","2119dc4d":"#We split the data into test and train\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Attrition', axis=1)\ny = df.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nX_train.shape","ef660b3d":"#Applying Decision Tree Clasiffier\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train)","c9c7b095":"#Checking the accuracy score for train\ny_train_pred = dtc.predict(X_train)\ny_train_prob = dtc.predict_proba(X_train)[0,1]\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve,confusion_matrix, f1_score\naccuracy_score(y_train, y_train_pred)","c694bd2e":"#Since we have accuracy score of 1.0, we have the model on overfitting, hence, we do hypermeter tuning to avoid that and then scoring\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\n\ndtc = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(dtc, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")","2104a490":"#Making the model on the best params created by the model_selection\ndtc_best = DecisionTreeClassifier(**best_params)\ndtc_best.fit(X_train, y_train)\n#Accuracy score of train\ny_train_pred = dtc_best.predict(X_train)\ny_train_prob = dtc_best.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_DTC = dtc_best.predict(X_test)\ny_test_prob_DTC = dtc_best.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Decsision Tree Classifier: \",roc_auc_score(y_test, y_test_prob_DTC))\n#Plotting confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(dtc_best, X_test, y_test,normalize=\"true\", cmap=\"Blues\")\n\nDTCfpr, DTCtpr, DTCthresholds = roc_curve(y_test, y_test_prob_DTC)","83a9534a":"#Visualizing the tree in order to get insights\nfrom IPython.display import Image\nfrom io import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = list(df.columns)\nfeatures.remove(\"Attrition\")\ndot_data = StringIO()\nexport_graphviz(dtc_best, out_file=dot_data, feature_names=features, filled=True)\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","6218f35e":"#Applying Logisitic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nlog_reg_model = LogisticRegression(max_iter=1000, solver = \"newton-cg\")\nlog_reg_model.fit(X_train, y_train)\n#Accuracy score of train\ny_train_pred = log_reg_model.predict(X_train)\ny_train_prob = log_reg_model.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_LR = log_reg_model.predict(X_test)\ny_test_prob_LR = log_reg_model.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Logistic Regression: \",roc_auc_score(y_test, y_test_prob_LR))\nprint(classification_report(y_test, y_test_pred_LR))\n#Plotting confusion matrix\nplot_confusion_matrix(log_reg_model, X_test, y_test, normalize=\"true\", cmap=\"Blues\")\nLRfpr, LRtpr, LRthresholds = roc_curve(y_test, y_test_prob_LR)\n","e2a774c4":"#Applying Random Forest Classifier model\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=700,max_depth=10,n_jobs=-1,random_state=123)\nrfc.fit(X_train,y_train)\n#Accuracy score of train\ny_train_pred = rfc.predict(X_train)\ny_train_prob = rfc.predict_proba(X_train)[0,1]\nprint(\"Accuracy score for Train:\",accuracy_score(y_train, y_train_pred))\n#ROC_AUC score for visualizing the score\ny_test_pred_RFC = rfc.predict(X_test)\ny_test_prob_RFC = rfc.predict_proba(X_test)[:,1]\nprint(\"ROC_AUC score for Random Forest Classifier: \",roc_auc_score(y_test, y_test_prob_RFC))\n#Plotting confusion matrix\nplot_confusion_matrix(rfc, X_test, y_test, normalize=\"true\", cmap=\"Blues\")\nRFCfpr, RFCtpr, RFCthresholds = roc_curve(y_test, y_test_prob_RFC)","4aa3f0b5":"#Plotting importance of attributes that are responsible for attrition according to the RFC model\ndef plot_feature_importance(importance,names,model_type): \n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data) \n    \n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True) \n    \n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\nplot_feature_importance(rfc.feature_importances_,X_train.columns,'RANDOM FOREST')","1f37aefe":"plt.figure(dpi=150)\nplt.plot(DTCfpr, DTCtpr, color='orange', label='Decision Tree Classifier ROC')\nplt.plot(LRfpr, LRtpr, color='blue', label='Logistic Regression ROC')\nplt.plot(RFCfpr, RFCtpr, color='red', label='Random Forest Classifier ROC')\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')","d714a675":"Random Forest Classifier","19d29934":"**Pre-processing & Correlation check**","d9c8b9f8":"**File Handling**","3caa2cfe":"Logistic Regression","89170fc3":"**Train-Test split**","3ee7be53":"**Algorithms**\nDecision Tree Classifier","2cf5eb8b":"**We are sorry for the uncomfortable grid on the confusion matrix. Unknown issue that isn't shown on the final report**","5bee1389":"Results using ROC AUC score graph"}}