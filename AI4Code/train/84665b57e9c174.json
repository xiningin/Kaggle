{"cell_type":{"8fe1d254":"code","b14b6239":"code","d56069a7":"code","ffa34903":"code","d00b002e":"code","f3a203c3":"code","ae91e192":"code","b5ea0bb3":"code","fee69f25":"code","9b33b301":"code","757507ec":"code","6110313d":"code","5caddd32":"code","60e884f5":"code","a3fba1f6":"code","eb7472d8":"code","2ff60d20":"code","ff980215":"markdown","4c962696":"markdown","cedf5155":"markdown","7351b118":"markdown","619853aa":"markdown","9a60cd34":"markdown"},"source":{"8fe1d254":"!pip install datatable","b14b6239":"# Load libraries\nimport datatable as dt\nprint(dt.__version__)\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# to print all outputs of a cell\nfrom IPython.core.interactiveshell import InteractiveShell  \nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')","d56069a7":"## Data Table Reading\nstart = time.time()\ndata_dir = Path('..\/input\/tabular-playground-series-nov-2021\/')\ndt_train = dt.fread(data_dir \/ \"train.csv\")\nend = time.time()\nprint(end - start)","ffa34903":"# peek at the data\ndt_train.head(5)\n\n# number of rows and columns in training dataset\ndt_train.shape","d00b002e":"# Convert datatable Frame to pandas DataFrame.\ndf_train = dt_train.to_pandas()","f3a203c3":"from sklearn.model_selection import KFold\n\ndef create_kfolds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = KFold(n_splits=num_splits, shuffle=True, random_state=11)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['target'])):\n        data.loc[v_, 'kfold'] = f\n    return data","ae91e192":"df_train_10folds = create_kfolds(df_train.copy(), num_splits=10)\ndf_train_20folds = create_kfolds(df_train.copy(), num_splits=20)\ndf_train_30folds = create_kfolds(df_train.copy(), num_splits=30)\ndf_train_40folds = create_kfolds(df_train.copy(), num_splits=40)\ndf_train_50folds = create_kfolds(df_train.copy(), num_splits=50)","b5ea0bb3":"df_train_10folds['kfold'].value_counts()\ndf_train_20folds['kfold'].value_counts()\ndf_train_30folds['kfold'].value_counts()\ndf_train_40folds['kfold'].value_counts()\ndf_train_50folds['kfold'].value_counts()","fee69f25":"df_train_10folds.to_csv('train_10folds.csv', index=False)\ndf_train_20folds.to_csv('train_20folds.csv', index=False)\ndf_train_30folds.to_csv('train_30folds.csv', index=False)\ndf_train_40folds.to_csv('train_40folds.csv', index=False)\ndf_train_50folds.to_csv('train_50folds.csv', index=False)","9b33b301":"from sklearn.model_selection import StratifiedKFold\n\n# function copied from https:\/\/www.kaggle.com\/kishalmandal\/fold-is-power\/notebook?scriptVersionId=78574712&cellId=5\ndef create_stratified_kfolds(data, num_splits):\n    data[\"stratifiedkfold\"] = -1\n    kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=11)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['target'])):\n        data.loc[v_, 'stratifiedkfold'] = f\n    return data","757507ec":"df_train_stratified_10folds = create_stratified_kfolds(df_train.copy(), num_splits=10)\ndf_train_stratified_20folds = create_stratified_kfolds(df_train.copy(), num_splits=20)\ndf_train_stratified_30folds = create_stratified_kfolds(df_train.copy(), num_splits=30)\ndf_train_stratified_40folds = create_stratified_kfolds(df_train.copy(), num_splits=40)\ndf_train_stratified_50folds = create_stratified_kfolds(df_train.copy(), num_splits=50)","6110313d":"df_train_stratified_10folds['stratifiedkfold'].value_counts()\ndf_train_stratified_20folds['stratifiedkfold'].value_counts()\ndf_train_stratified_30folds['stratifiedkfold'].value_counts()\ndf_train_stratified_40folds['stratifiedkfold'].value_counts()\ndf_train_stratified_50folds['stratifiedkfold'].value_counts()","5caddd32":"df_train_stratified_10folds.to_csv('train_stratified_10folds.csv', index=False)\ndf_train_stratified_20folds.to_csv('train_stratified_20folds.csv', index=False)\ndf_train_stratified_30folds.to_csv('train_stratified_30folds.csv', index=False)\ndf_train_stratified_40folds.to_csv('train_stratified_40folds.csv', index=False)\ndf_train_stratified_50folds.to_csv('train_stratified_50folds.csv', index=False)","60e884f5":"from sklearn.model_selection import ShuffleSplit\n\ndef create_folds_shuffle(data, num_splits):\n    data[\"shufflesplit\"] = -1\n    kf = ShuffleSplit(n_splits=num_splits, random_state=11)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['target'])):\n        data.loc[v_, 'shufflesplit'] = f\n    return data","a3fba1f6":"df_train_shuffle_10folds = create_folds_shuffle(df_train.copy(), num_splits=10)\ndf_train_shuffle_20folds = create_folds_shuffle(df_train.copy(), num_splits=20)\ndf_train_shuffle_30folds = create_folds_shuffle(df_train.copy(), num_splits=30)\ndf_train_shuffle_40folds = create_folds_shuffle(df_train.copy(), num_splits=40)\ndf_train_shuffle_50folds = create_folds_shuffle(df_train.copy(), num_splits=50)","eb7472d8":"df_train_shuffle_10folds['shufflesplit'].value_counts()\ndf_train_shuffle_20folds['shufflesplit'].value_counts()\ndf_train_shuffle_30folds['shufflesplit'].value_counts()\ndf_train_shuffle_40folds['shufflesplit'].value_counts()\ndf_train_shuffle_50folds['shufflesplit'].value_counts()","2ff60d20":"df_train_shuffle_10folds.to_csv('train_shuffle_10folds.csv', index=False)\ndf_train_shuffle_20folds.to_csv('train_shuffle_20folds.csv', index=False)\ndf_train_shuffle_30folds.to_csv('train_shuffle_30folds.csv', index=False)\ndf_train_shuffle_40folds.to_csv('train_shuffle_40folds.csv', index=False)\ndf_train_shuffle_50folds.to_csv('train_shuffle_50folds.csv', index=False)","ff980215":"# Resampling using StratifiedKFold Cross Validation\n\nThe splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value.","4c962696":"# Thank you to Kishal Mandal\n\nI got the idea for this step\/notebook from Kishal Mandal's [notebook](https:\/\/www.kaggle.com\/kishalmandal\/fold-is-power\/notebook). Please have a look at it.\n\n# Problem Statement\n\nIn this competition, we predict whether or not an email is spam.\n\nIt is a binary (2-class) classification problem. The number of observations for each class is balanced. There are 600,000 observations in the training dataset with 101 input variables (id and f0 to f99) and 1 output variable (target). We do not have any missing values.\n\n<u>Goal of the Competition<\/u>: The Goal of the Tabular Playground Series November 2021 is to predict whether or not an email is spam.\n\n<u>Goal of this Notebook<\/u>: In this notebook, we are going to prepare training dataset using the various resampling techniques such as KFold, StratifiedKFold and Repeated Random Splits.\n\nWe are going to cover the following steps:\n1. Load Data\n2. Resampling using KFold Cross Validation\n3. Resampling using StratifiedKFold\n4. Resampling using Repeated Random Splits\n5. Resampling using Leave One Out Cross Validation\n6. References\n\nLet's get started\n\n# Load Data","cedf5155":"# Resampling using KFold Cross Validation\n\nCross validation is an approach that we can use to estimate the performance of an algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The algorithm is trained on k\u22121 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross validation we end up with k different performance scores that we can summarize using a mean and a standard deviation.\n\nThe result is a more reliable estimate of the performance of the algorithm on new data. It is more accurate because the algorithm is trained and evaluated multiple times on different data. The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data.","7351b118":"# Resampling using Repeated Random Splits\n\nAnother variation on k-fold cross validation is to create a random split of the data like the train\/test split, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation. This has the speed of using a train\/test split and the reduction in variance in the estimated performance of k-fold cross validation. We can also repeat the process many more times as needed to improve the accuracy. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation.","619853aa":"# References\n\n1. Thank you to Jason Brownlee for [Machine Learning Mastery](https:\/\/machinelearningmastery.com\/).\n2. Thank you to Kishal Mandal for his [notebook](https:\/\/www.kaggle.com\/kishalmandal\/fold-is-power\/notebook).","9a60cd34":"# Resampling using Leave One Out Cross Validation\n\nIn Leave One Out Cross Validation, we configure cross validation so that the size of the fold is 1 (k is set to the number of observations in our dataset). The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of our model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross validation.\n\nWe have not used Leave One Out Cross Validation because it would have been computationally very expensive."}}