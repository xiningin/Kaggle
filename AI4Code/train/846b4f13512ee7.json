{"cell_type":{"b42e4cf3":"code","731191d7":"code","5e806940":"code","a1c5cab2":"code","aa6073e0":"code","07a4461b":"code","d7ff3479":"code","ec0b1555":"code","3446f297":"code","567dee00":"code","2fcc04f4":"code","792116dd":"code","15c336e0":"code","3dd7226d":"code","ed0aec55":"code","cc86b003":"code","fd7e0159":"code","302ec5a9":"code","90573c4e":"code","752d7115":"code","69dd9121":"code","0d1f4f86":"code","a7adf4cf":"code","0a55ae9a":"code","2f85df2f":"code","a131fa2a":"code","9b861c1e":"code","fe7c5a48":"code","41cbc413":"code","3204b8d9":"code","2d91ca12":"code","b9c49353":"code","c42a6121":"code","318adc21":"code","80ef0c24":"code","aa2aee8c":"code","87a11d23":"code","715d6c22":"code","5e2300c0":"code","7d556c4b":"code","c4cad3c3":"code","35a63498":"code","fc082d48":"code","302aa7a7":"code","fb6e9980":"code","91cf5393":"code","64f01deb":"code","064fb59d":"code","931a655e":"code","e33f6a1a":"code","bf218e0c":"code","70917310":"code","f92af50f":"code","8e9a701c":"code","1d987941":"code","a08a06cf":"code","62198dbe":"code","ac1f6c00":"code","45b0c6d8":"code","00a76f71":"code","b136febd":"code","61ebd7b0":"code","2996e82e":"code","a5d43ff2":"code","4bf0fbb4":"code","c8cac1e8":"code","d80ab14a":"code","aea7dd6d":"code","93614736":"code","3e63b04d":"code","b90c3f07":"code","925bb6cb":"code","163073f9":"code","ba2f13d3":"code","b8d91b7c":"code","0acc2036":"code","266401a1":"code","f2766a44":"code","1428b81b":"code","8a54b24a":"code","b47164b9":"code","ee164961":"code","7552d0a6":"code","98896875":"code","c57a32b5":"code","f8d25cf4":"code","8551f008":"code","533d66c6":"code","78390ca4":"markdown","bfbe808b":"markdown","02cc20bf":"markdown","90430465":"markdown","588b4894":"markdown","eb6d05ec":"markdown","f4dd9016":"markdown","286ead9a":"markdown","972bb947":"markdown","37699831":"markdown","b7b8af8d":"markdown","8aad8950":"markdown","aaf920d4":"markdown","16aa8cb7":"markdown","ae5fe5b2":"markdown","8e627250":"markdown","3308503f":"markdown","6b8bc414":"markdown","4766886c":"markdown","9b9688b5":"markdown","d850effe":"markdown","733bb31a":"markdown","fdf5ad89":"markdown","7f733893":"markdown","caf69efd":"markdown","ca3a6b08":"markdown","70e7a09f":"markdown","c6f4d44c":"markdown","833e0e0c":"markdown","f03fe605":"markdown","47f713a5":"markdown","9c4d50f1":"markdown","cf876bca":"markdown","a1e23776":"markdown","4926c0c8":"markdown","e9dea892":"markdown","7fee54de":"markdown","0ae9b13c":"markdown","c3973579":"markdown","dc5c3dfa":"markdown","715623fa":"markdown","ca4ec713":"markdown","362811bb":"markdown","04aa9f67":"markdown","24cd2c68":"markdown","e4f83425":"markdown","13f50391":"markdown","8f1d6324":"markdown","dd68df56":"markdown","8fa0899a":"markdown","b33a0c20":"markdown","ba0594d3":"markdown","ba65e856":"markdown","7ca066c5":"markdown","29497b08":"markdown","7de5c6c3":"markdown","498e0abf":"markdown","2be4403a":"markdown","a4dc6f40":"markdown","e6c7fb84":"markdown","310b2ce8":"markdown","b1e9841f":"markdown","c859174c":"markdown","82977efd":"markdown","a0aced7c":"markdown"},"source":{"b42e4cf3":"\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import log_loss,roc_auc_score,precision_score,f1_score,recall_score,roc_curve,auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score,fbeta_score,matthews_corrcoef\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom imblearn.metrics import geometric_mean_score\nimport warnings\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC \nimport xgboost as xgb\nfrom vecstack import stacking\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","731191d7":"dt = pd.read_csv('\/kaggle\/input\/heart-statlog-cleveland-hungary-final\/heart_statlog_cleveland_hungary_final.csv')","5e806940":"dt.head()","a1c5cab2":"# renaming features to proper name\ndt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope','target']","aa6073e0":"# converting features to categorical features \n\ndt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\ndt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n\n\n\ndt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\ndt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\n\ndt['st_slope'][dt['st_slope'] == 0] = 'normal'\ndt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\ndt['st_slope'][dt['st_slope'] == 2] = 'flat'\ndt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n\ndt[\"sex\"] = dt.sex.apply(lambda  x:'male' if x==1 else 'female')","07a4461b":"# checking the top 5 entries of dataset after feature encoding\ndt.head()","d7ff3479":"## Checking missing entries in the dataset columnwise\ndt.isna().sum()","ec0b1555":"# first checking the shape of the dataset\ndt.shape","3446f297":"# summary statistics of numerical columns\ndt.describe(include =[np.number])","567dee00":"# summary statistics of categorical columns\ndt.describe(include =[np.object])","2fcc04f4":"# Plotting attrition of employees\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=(14,6))\n\nax1 = dt['target'].value_counts().plot.pie( x=\"Heart disease\" ,y ='no.of patients', \n                   autopct = \"%1.0f%%\",labels=[\"Heart Disease\",\"Normal\"], startangle = 60,ax=ax1);\nax1.set(title = 'Percentage of Heart disease patients in Dataset')\n\nax2 = dt[\"target\"].value_counts().plot(kind=\"barh\" ,ax =ax2)\nfor i,j in enumerate(dt[\"target\"].value_counts().values):\n    ax2.text(.5,i,j,fontsize=12)\nax2.set(title = 'No. of Heart disease patients in Dataset')\nplt.show()","792116dd":"plt.figure(figsize=(18,12))\nplt.subplot(221)\ndt[\"sex\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = sns.color_palette(\"prism\",5),startangle = 60,labels=[\"Male\",\"Female\"],\nwedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.1,.1],shadow =True)\nplt.title(\"Distribution of Gender\")\nplt.subplot(222)\nax= sns.distplot(dt['age'], rug=True)\nplt.title(\"Age wise distribution\")\nplt.show()","15c336e0":"attr_1=dt[dt['target']==1]\nattr_0=dt[dt['target']==0]\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_0['age'])\nplt.title('AGE DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_0['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold' )\nplt.show()\n\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_1['age'])\nplt.title('AGE DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_1['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold' )\nplt.show()","3dd7226d":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","ed0aec55":"#Exploring the Heart Disease patients based on Chest Pain Type\nplot_criteria= ['chest_pain_type', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","cc86b003":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[2].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[2].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","fd7e0159":"#Exploring the Heart Disease patients based on REST ECG\nplot_criteria= ['rest_ecg', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","302ec5a9":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","90573c4e":"#Exploring the Heart Disease patients based on ST Slope\nplot_criteria= ['st_slope', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","752d7115":"sns.pairplot(dt, hue = 'target', vars = ['age', 'resting_blood_pressure', 'cholesterol'] )","69dd9121":"sns.scatterplot(x = 'resting_blood_pressure', y = 'cholesterol', hue = 'target', data = dt)","0d1f4f86":"sns.scatterplot(x = 'resting_blood_pressure', y = 'age', hue = 'target', data = dt)","a7adf4cf":"# filtering numeric features as age , resting bp, cholestrol and max heart rate achieved has outliers as per EDA\n\ndt_numeric = dt[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved']]","0a55ae9a":"dt_numeric.head()","2f85df2f":"# calculating zscore of numeric columns in the dataset\nz = np.abs(stats.zscore(dt_numeric))\nprint(z)","a131fa2a":"# Defining threshold for filtering outliers \nthreshold = 3\nprint(np.where(z > 3))","9b861c1e":"#filtering outliers retaining only those data points which are below threshhold\ndt = dt[(z < 3).all(axis=1)]","fe7c5a48":"# checking shape of dataset after outlier removal\ndt.shape","41cbc413":"## encoding categorical variables\ndt = pd.get_dummies(dt, drop_first=True)\n\ndt.head()\n","3204b8d9":"# checking the shape of dataset\ndt.shape","2d91ca12":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target'],axis=1)\ny = dt['target']","b9c49353":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","c42a6121":"print('------------Training Set------------------')\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint('------------Test Set------------------')\nprint(X_test.shape)\nprint(y_test.shape)","318adc21":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","80ef0c24":"X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","aa2aee8c":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\n# function initializing baseline machine learning models\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR_L2'   , LogisticRegression(penalty='l2')))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN7'  , KNeighborsClassifier(7)))\n    basedModels.append(('KNN5'  , KNeighborsClassifier(5)))\n    basedModels.append(('KNN9'  , KNeighborsClassifier(9)))\n    basedModels.append(('KNN11'  , KNeighborsClassifier(11)))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM Linear'  , SVC(kernel='linear',gamma='auto',probability=True)))\n    basedModels.append(('SVM RBF'  , SVC(kernel='rbf',gamma='auto',probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier(n_estimators=100,max_features='sqrt')))\n    basedModels.append(('RF_Ent100'   , RandomForestClassifier(criterion='entropy',n_estimators=100)))\n    basedModels.append(('RF_Gini100'   , RandomForestClassifier(criterion='gini',n_estimators=100)))\n    basedModels.append(('ET100'   , ExtraTreesClassifier(n_estimators= 100)))\n    basedModels.append(('ET500'   , ExtraTreesClassifier(n_estimators= 500)))\n    basedModels.append(('MLP', MLPClassifier()))\n    basedModels.append(('SGD3000', SGDClassifier(max_iter=1000, tol=1e-4)))\n    basedModels.append(('XGB_2000', xgb.XGBClassifier(n_estimators= 2000)))\n    basedModels.append(('XGB_500', xgb.XGBClassifier(n_estimators= 500)))\n    basedModels.append(('XGB_100', xgb.XGBClassifier(n_estimators= 100)))\n    basedModels.append(('XGB_1000', xgb.XGBClassifier(n_estimators= 1000)))\n    basedModels.append(('ET1000'   , ExtraTreesClassifier(n_estimators= 1000)))\n    \n    return basedModels\n\n# function for performing 10-fold cross validation of all the baseline models\ndef BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n    seed = 7\n    results = []\n    names = []\n    for name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n         \n        \n    return results,msg","87a11d23":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","715d6c22":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","5e2300c0":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","7d556c4b":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","c4cad3c3":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","35a63498":"xgb = xgb.XGBClassifier(n_estimators= 100)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","fc082d48":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","302aa7a7":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","fb6e9980":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","91cf5393":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","64f01deb":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","064fb59d":"import xgboost as xgboost\n# selecting list of top performing models to be used in stacked ensemble method\nmodels = [\n    RandomForestClassifier(criterion='entropy',n_estimators=100),\n    MLPClassifier(),\n    RandomForestClassifier(criterion='gini',n_estimators=100),\n    KNeighborsClassifier(9),\n    ExtraTreesClassifier(n_estimators= 500),\n    ExtraTreesClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 1000),\n    xgboost.XGBClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 500),\n    xgboost.XGBClassifier(n_estimators= 2000),\n    xgboost.XGBClassifier(),\n    SGDClassifier(max_iter=1000, tol=1e-4),\n    \n    SVC(kernel='linear',gamma='auto',probability=True),\n    AdaBoostClassifier(),\n    DecisionTreeClassifier(),\n    LinearDiscriminantAnalysis(),\n    GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),\n    ExtraTreesClassifier(n_estimators= 1000),\n]","931a655e":"S_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=accuracy_score, \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","e33f6a1a":"# initializing generalizer model i.e., MLP classifier in our case\nmodel = MLPClassifier()\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))","bf218e0c":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN\/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","70917310":"data = {'Random Forest': y_pred_rfe, \n                'MLP': y_pred_mlp, \n                'KNN': y_pred_knn, \n                'EXtra tree classifier': y_pred_et500,\n                'XGB': y_pred_xgb, \n                'SVC': y_pred_svc, \n                'SGD': y_pred_sgd,\n                'Adaboost': y_pred_ada, \n                'CART': y_pred_decc, \n                'GBM': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN\/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results\n","f92af50f":"def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])\n    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,\n            label=\"%s (area=%.3f)\"%(label,roc_auc_score(y_true, y_proba[:,1])))\n\nf, ax = plt.subplots(figsize=(12,8))\n\nroc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Classifier ',l='-')\nroc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nroc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nroc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\n\nax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--', \n        )    \nax.legend(loc=\"lower right\")    \nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Receiver Operator Characteristic curves')\nsns.despine()","8e9a701c":"def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    precision, recall, _ = precision_recall_curve(y_test,\n                                                  y_proba[:,1])\n    average_precision = average_precision_score(y_test, y_proba[:,1],\n                                                     average=\"micro\")\n    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),\n            linestyle=l, linewidth=lw)\n\nf, ax = plt.subplots(figsize=(14,10))\nprecision_recall_plot(y_test,model.predict_proba(S_test),label='Stacking classifier ',l='-')\nprecision_recall_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nprecision_recall_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nprecision_recall_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.legend(loc=\"lower left\")\nax.grid(True)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Precision-recall curves')\nsns.despine()","1d987941":"num_feats=11\n\ndef cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","a08a06cf":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","62198dbe":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","ac1f6c00":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", solver='lbfgs'), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","45b0c6d8":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, criterion='entropy'), max_features=num_feats)\nembeded_rf_selector.fit(X, y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","00a76f71":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)\n\nembeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","b136febd":"# put all selection together\nfeature_name = X.columns\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","61ebd7b0":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target','resting_blood_pressure'],axis=1)\ny = dt['target']","2996e82e":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","a5d43ff2":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","4bf0fbb4":"X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","c8cac1e8":"\nmodels = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","d80ab14a":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","aea7dd6d":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","93614736":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","3e63b04d":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","b90c3f07":"xgb = xgb.XGBClassifier(n_estimators= 100)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","925bb6cb":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","163073f9":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","ba2f13d3":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","b8d91b7c":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","0acc2036":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","266401a1":"import xgboost as xgboost\n# selecting list of top performing models to be used in stacked ensemble method\nmodels = [\n    RandomForestClassifier(criterion='entropy',n_estimators=100),\n    MLPClassifier(),\n    RandomForestClassifier(criterion='gini',n_estimators=100),\n    KNeighborsClassifier(9),\n    ExtraTreesClassifier(n_estimators= 500),\n    ExtraTreesClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 1000),\n    xgboost.XGBClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 500),\n    xgboost.XGBClassifier(n_estimators= 2000),\n    xgboost.XGBClassifier(),\n    SGDClassifier(max_iter=1000, tol=1e-4),\n    \n    SVC(kernel='linear',gamma='auto',probability=True),\n    AdaBoostClassifier(),\n    DecisionTreeClassifier(),\n    LinearDiscriminantAnalysis(),\n    GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),\n    ExtraTreesClassifier(n_estimators= 1000),\n]","f2766a44":"S_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=accuracy_score, \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","1428b81b":"# initializing generalizer model i.e., MLP classifier in our case\nmodel = MLPClassifier()\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))","8a54b24a":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN\/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier2',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","b47164b9":"data = {'Random Forest2': y_pred_rfe, \n                'MLP2': y_pred_mlp, \n                'KNN2': y_pred_knn, \n                'EXtra tree classifier2': y_pred_et500,\n                'XGB2': y_pred_xgb, \n                'SVC2': y_pred_svc, \n                'SGD2': y_pred_sgd,\n                'Adaboost2': y_pred_ada, \n                'CART2': y_pred_decc, \n                'GBM2': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN\/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results","ee164961":"def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])\n    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,\n            label=\"%s (area=%.3f)\"%(label,roc_auc_score(y_true, y_proba[:,1])))\n\nf, ax = plt.subplots(figsize=(12,8))\n\nroc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Classifier ',l='-')\nroc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nroc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nroc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\n\nax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--', \n        )    \nax.legend(loc=\"lower right\")    \nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Receiver Operator Characteristic curves')\nsns.despine()","7552d0a6":"def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    precision, recall, _ = precision_recall_curve(y_test,\n                                                  y_proba[:,1])\n    average_precision = average_precision_score(y_test, y_proba[:,1],\n                                                     average=\"micro\")\n    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),\n            linestyle=l, linewidth=lw)\n\nf, ax = plt.subplots(figsize=(14,10))\nprecision_recall_plot(y_test,model.predict_proba(S_test),label='Stacking classifier ',l='-')\nprecision_recall_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nprecision_recall_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nprecision_recall_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.legend(loc=\"lower left\")\nax.grid(True)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Precision-recall curves')\nsns.despine()","98896875":"import shap \nexplainer = shap.TreeExplainer(rf_ent)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","c57a32b5":"shap.summary_plot(shap_values[1], X_test)","f8d25cf4":"features = [c for c in X_test.columns]\n\nfrom pdpbox import pdp, get_dataset, info_plots\n\npdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='st_slope_upsloping')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'st_slope_upsloping')\n\nplt.show()","8551f008":"pdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='st_slope_flat')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'st_slope_flat')\n\nplt.show()","533d66c6":"pdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='exercise_induced_angina')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'exercise_induced_angina')\n\nplt.show()","78390ca4":"## 6. Outlier Detection & Removal <a id='data-out'><\/a>","bfbe808b":"As we can see from above dataset entries some of the features should be nominal and to be encoded as their category type. In the next step we will be encoding features to their respective category as per the dataset description.","02cc20bf":"### ROC AUC Curve","90430465":"from these points it is diffciult to say which points are outliers so we will now define threshold","588b4894":"### Distribution of Heart disease (target variable)","eb6d05ec":"So, there are total 1190 records and 11 features with 1 target variable. Lets check the summary of numerical and categorical features.","f4dd9016":"As we can see from above plot, in this dataset males percentage is way too higher than females where as average age of patients is around 55.","286ead9a":"### F1 Score\n\n F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n\n**F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)**","972bb947":"Don\u2019t be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[30][2] have a Z-score higher than 3. There are total 17 data points which are outliers.","37699831":"### Distribution of Rest ECG","b7b8af8d":"### SHAP Summary Plot","8aad8950":"## 4. Data Cleaning & Preprocessing <a id='data-prep'><\/a>\n In this step we will first change the name of columns as some of the columns have weird naming pattern and then we will encode the features into categorical variables","aaf920d4":"Great !! all the 17 data points which are outliers are now removed.\n\nNow before splitting dataset into train and test we first encode categorical variables as dummy variables and segregate feature and target variable.","16aa8cb7":"## 7. Train Test Split <a id='data-train'><\/a>","ae5fe5b2":"### Multi Layer Perceptron","8e627250":"Now, as per performance of different baseline models on cross validation accuracy we will be selecting best performing models for level 0 of **stacked ensemble** so that their ensemble will produce higher performance in comparison to individual machine learning model.","3308503f":"### XGBoost (n_estimators=100)","6b8bc414":"![](https:\/\/i.ibb.co\/d43FVfJ\/Sensitivity-and-specificity-svg.png)\n\n### Mathew Correlation coefficient (MCC)\n\nThe Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.\n\n![](https:\/\/i.ibb.co\/mH6MmG4\/mcc.jpg)","4766886c":"### Detecting outlier using z-score\n![](https:\/\/i.ibb.co\/bgRRWrp\/outlier.jpg)\n![](https:\/\/i.ibb.co\/jDHP7Sj\/Z-score-formula.jpg)","9b9688b5":"### Random Forest Classifier (criterion = 'entropy')","d850effe":"![](https:\/\/i.ibb.co\/R3V4tWC\/heart1.png)\nAn electrocardiogram records the electrical signals in your heart. It's a common test used to detect heart problems and monitor the heart's status in many situations. Electrocardiograms \u2014 also called ECGs or EKGs. but ECG has limits. It measures heart rate and rhythm\u2014but it doesn\u2019t necessarily show blockages in the arteries.Thats why in this dataset around 52% heart disease patients have normal ECG","733bb31a":"### feature normalization\nIn this step we will normalize all the numeric feature in the range of 0 to 1","fdf5ad89":"![](https:\/\/i.ibb.co\/jw2P8Nr\/ST-segment-depression-upsloping-downsloping-horizontal.png)\n\nThe ST segment \/heart rate slope (ST\/HR slope), has been proposed as a more accurate ECG criterion for diagnosing significant coronary artery disease (CAD) in most of the research papers. \n\nAs we can see from above plot upsloping is positive sign as 74% of the normal patients have upslope where as 72.97% heart patients have flat sloping.","7f733893":"### Extra Tree Classifier (n_estimators=500)","caf69efd":"## 12. Model Evaluation  <a id='model-eval'><\/a>\n\n In this step we will first define which evaluation metrics we will use to evaluate our model. The most important evaluation metric for this problem domain is **sensitivity, specificity, Precision, F1-measure, Geometric mean and mathew correlation coefficient and finally ROC AUC curve**\n \n### Sensitivity vs Specificity","ca3a6b08":"The dataset is balanced having 629 heart disease patients and 561 normal patients","70e7a09f":"As we can see highest average area under the curve (AUC) of 0.950 is attained by Extra Tree Classifier","c6f4d44c":"### Distribution of Numerical features","833e0e0c":"From the above plot we can see outliers clearly as for some of the patients cholestrol is 0 whereas for one patient both cholestrol and resting bp is 0 which is may be due to missing entries we will filter these ouliers later","f03fe605":"### SHAP Feature Importance\nThe global mean(|Tree SHAP|) method applied to the heart disease prediction model. The x-axis is essentially the average magnitude change in model output when a feature is \u201chidden\u201d from the model (for this model the output has log-odds units). \u201chidden\u201d means integrating the variable out of the model. Since the impact of hiding a feature changes depending on what other features are also hidden, Shapley values are used to enforce consistency and accuracy.","47f713a5":"1.  ## 10. Model Selection <a id='selct-model'><\/a>","9c4d50f1":"## Comparison with other Models","cf876bca":"## 11. Stacked Ensemble (2 Level) <a id='stack-ensemble'><\/a>\n\nStacking, also called Super Learning or Stacked Regression, is a class of algorithms that involves training a second-level **\u201cmetalearner\u201d** to find the optimal combination of the base learners. Unlike bagging and boosting, the goal in stacking is to ensemble strong, diverse sets of learners together.\nReference: [H2O](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/stacked-ensembles.html)\n\n### Stacking Algorithm\n\n**Step 1. Set up the ensemble.**<br>\n a. Specify a list of L base algorithms (with a specific set of model parameters)..<br>\n b. Specify a metalearning algorithm..<br>\n \n**Step 2. Train the ensemble.**<br>\n a. Train each of the L base algorithms on the training set..<br>\n b. Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the L algorithms..<br>\n c. The N cross-validated predicted values from each of the L algorithms can be combined to form a new N x L matrix. This matrix, along wtih the original response vector, is called the \u201clevel-one\u201d data. (N = number of rows in the training set.).<br>\n d. Train the metalearning algorithm on the level-one data. The \u201censemble model\u201d consists of the L base learning models and the metalearning model, which can then be used to generate predictions on a test set.<br>\n \n**Step 3. Predict on new data.**<br>\n a. To generate ensemble predictions, first generate predictions from the base learners.<br>\n b. Feed those predictions into the metalearner to generate the ensemble prediction.<br>\n\n![](https:\/\/i.ibb.co\/3cRfkxK\/stacked-ensemble.png)","a1e23776":"### decision Tree Classifier (CART)","4926c0c8":"The higher the SHAP value of a feature, the higher is the log odds of heart disease in this heart disease prediction model. Every patient in the dataset is run through the model and a dot is created for each feature attribution value, so one patient gets one dot on each feature\u2019s line. Dot\u2019s are colored by the feature\u2019s value for that patient and pile up vertically to show density. In above plot we see that **st_slope_upsloping** is the most important risk factor for heart disease patients. The lower values of **st_slope_upsloping** leads to heart disease, whereas its higher values decreases the chances of heart disease. Higher values of **exercise_induced_angina** increases the risk of heart disease whereas its lower values decreases the chances of heart disease.","e9dea892":"As we can see from above plot more patients accounts for heart disease in comparison to females whereas mean age for heart disease patients is around 58 to 60 years","7fee54de":"- As we can see from above plot it is evident that higher values of st_slope_upsloping decreases the chances of heart disease which we also find through summary plot.","0ae9b13c":"## 8. Cross Validation <a id='cross-val'><\/a>\n\nIn this step, we will build different baseline models and perform 10-fold cross validation to filter top performing baseline models to be used in level 0 of stacked ensemble method.","c3973579":"From the above plot it is clear that as the age increases chances of heart disease increases","dc5c3dfa":"## 9. Model building <a id='data-model'><\/a>","715623fa":"## 13. Model Interpretation  <a id='model-inter'><\/a>","ca4ec713":"Next we will see how st_slope_upsloping, st_slope_flat and excercise_induced_angina which are top three contributing features affects model prediction by visualizing them using [**Partial Dependence Plots**](https:\/\/www.kaggle.com\/dansbecker\/partial-plots) ","362811bb":"### Support Vector Classifier (kernel='linear')","04aa9f67":"As we can see from above plot **76%** of the chest pain type of the heart disease patients have asymptomatic chest pain. \n\nAsymptomatic heart attacks medically known as **silent myocardial infarction (SMI)** annually accounts for around 45-50% of morbidities due to cardiac ailments and even premature deaths in India. The incidences among middle aged people experiencing SMI is twice likely to develop in males than females. The symptoms of SMI being very mild in comparison to an actual heart attack; it is described as a silent killer. Unlike the symptoms in a normal heart attack which includes extreme chest pain, stabbing pain in the arms, neck & jaw, sudden shortness of breath, sweating and dizziness, the symptoms of SMI are very brief and hence confused with regular discomfort and most often ignored.\n\n[reference](https:\/\/www.maxhealthcare.in\/blogs\/cardiology\/rise-cases-asymptomatic-heart-attacks-amongst-middle-aged-people)","24cd2c68":"### gradient boosting machine ","e4f83425":"So, there are no missing entries in the dataset thats great. Next we will move towards exploring the dataset by performing detailed EDA","13f50391":"- After seeing the above two plots, it is clear now higher values of st_slope_flat and excercise_induced_angina leads to heart disease which is also evident through SHAP summary plots.","8f1d6324":"### K nearest neighbour (n=9)","dd68df56":"### Adaboost Classifier","8fa0899a":"As we can see from above description resting_blood_pressure and cholestrol have some outliers as they have minimum value of 0 whereas cholestrol has outlier on upper side also having maximum value of 603.","b33a0c20":"## 3. Loading Dataset<a id='data-load'><\/a>","ba0594d3":"### Checking Gender & Agewise Distribution","ba65e856":"### Stochastic Gradient Descent","7ca066c5":"As we can see features are encoded sucessfully to their respective categories. Next we will be checking if there is any missing entry or not ?","29497b08":"## 14. Conclusion  <a id='data-conc'><\/a>\n\n- As we have seen, stacked ensemble of power machine learning algorithms resulted in higher performance than any individual machine learning model.\n- We have also interpreted second best performing algo i.e., random forest algorithm\n- The top 5 most contribution features are **st_slope_upsloping, st_slope_flat, excercise_induced_angina, sex** and **cholesterol **\n- Further we have also deep dive into top 3 features to know how their values affecting the final model prediction and find that ***higher value of st_slope_upsloping reduces the risk of heart disease while lower values increases the chances of heart disease where as in case of st_slope_flat and excercise_induced_angina their higher values are riskier for heart disease while lower values are safe***","7de5c6c3":"## 1. Dataset description<a id='data-desc'><\/a>\n\nThis dataset consists of 11 features and a target variable. It has 6 nominal variables and 5 numeric variables. The detailed description of all the features are as follows:\n\n**1. Age:** Patients Age in years (Numeric)<br>\n**2. Sex:** Gender of patient (Male - 1, Female - 0) (Nominal)<br>\n**3. Chest Pain Type:** Type of chest pain experienced by patient categorized into 1 typical, 2 typical angina, 3 non-        anginal pain, 4 asymptomatic (Nominal)<br>\n**4. resting bp s:** Level of blood pressure at resting mode in mm\/HG (Numerical)<br>\n**5. cholestrol:** Serum cholestrol in mg\/dl (Numeric)<br>\n**6. fasting blood sugar:** Blood sugar levels on fasting > 120 mg\/dl represents as 1 in case of true and 0 as false (Nominal)<br>\n**7. resting ecg:** Result of electrocardiogram while at rest are represented in 3 distinct values 0 : Normal 1: Abnormality in ST-T wave 2: Left ventricular hypertrophy (Nominal)<br>\n**8. max heart rate:** Maximum heart rate achieved (Numeric)<br>\n**9. exercise angina:** Angina induced by exercise 0 depicting NO 1 depicting Yes (Nominal)<br>\n**10. oldpeak:** Exercise induced ST-depression in comparison with the state of rest (Numeric)<br>\n**11. ST slope:** ST segment measured in terms of slope during peak exercise 0: Normal 1: Upsloping 2: Flat 3: Downsloping (Nominal)<br>\n\n#### Target variable\n**12. target:** It is the target variable which we have to predict 1 means patient is suffering from heart risk and 0 means patient is normal.\n","498e0abf":"## Precision Recall curve","2be4403a":"### Distribution of Chest Pain Type","a4dc6f40":"### Log Loss\nLogarithmic loss  measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of our machine learning models is to minimize this value. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high log loss.\n\nThe graph below shows the range of possible log loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n\n![](https:\/\/i.ibb.co\/6BdDczW\/log-loss.jpg)","e6c7fb84":"## 5. Exploratory Data Analysis (EDA) <a id='data-eda'><\/a>","310b2ce8":"As we have seen Randomforest is second best performing model in every performance measure. So we will be plotting SHAP feature importance plot using random forest to know which features are actually contributing in model's prediction.","b1e9841f":"### Findings\n- AS we can see from above results, Stacked Ensemble Classifier is best performer as it has highest test accuracy of 0.906, sensitivity of 0.943 and specificity of 0.866 and highest f1-score of 0.9133 and lowest Log Loss of 3.23 and highest ROC value of 0.904\n- Random Forest & Extra Tree classifier are second best having same performance measure in every aspect\n- XGboost has third best sensitivity level","c859174c":"![](https:\/\/i.ibb.co\/6mBRYRq\/heart-p.jpg)\n\nHeart Disease (including Coronary Heart Disease, Hypertension, and Stroke) remains the No. 1\ncause of death in the US.The Heart Disease and Stroke Statistics\u20142019 Update from the **American Heart Association** indicates that:\n* 116.4 million, or 46% of US adults are estimated to have hypertension. These are findings related to the new 2017 Hypertension Clinical Practice Guidelines.\n* On average, someone dies of CVD every 38 seconds. About 2,303 deaths from CVD each day, based on 2016 data.\n* On average, someone dies of a stroke every 3.70 minutes. About 389.4 deaths from stroke each day, based on 2016 data.\n\nIn this notebook i will try to unleash useful insights using this heart disease datasets and by building stacked ensemble model by combining the power of best performing machine learning algorithms.\n\nThis notebook is divided into 13 major steps which are as follows:\n\n1. [Data description](#data-desc)\n2. [Importing Libraries & setting up environment](#imp-lib)\n3. [Loading dataset](#data-load)\n4. [Data Cleaning & Preprocessing](#data-prep)\n5. [Exploratory Data Analysis](#data-eda)\n6. [OUtlier Detection & Removal](#data-out)\n7. [Training & Test Split](#data-train)\n8. [Cross Validation](#cross-val)\n9. [Model Building](#data-model)\n10. [Model Selection](#selct-model)\n11. [Stacked Ensemble](#stack-ensemble)<br>\n12. [Model evaluation & comparison](#model-eval)<br>\n13. [Model Interpretation](#model-inter)\n14. [Conclusion](#data-conc)\n\n","82977efd":"## 2. Importing Libraries<a id='imp-lib'><\/a>","a0aced7c":"Lets see some of the sample entries of dataset"}}