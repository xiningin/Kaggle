{"cell_type":{"be542f19":"code","a8f3bdc2":"code","6a3166fa":"code","aa946750":"code","0b04df83":"code","52159a49":"code","c42eb657":"code","a92b53aa":"code","357b0653":"code","e497516d":"code","fd7d92c0":"code","71b670a9":"code","f1811795":"code","f9a05ed6":"code","9e3a90a7":"code","29bc448e":"code","04190a86":"code","7c7b8711":"code","0f4fa391":"code","f0222d71":"code","d4a0c9e8":"code","398f38fe":"code","76a1a9b2":"code","c846dd99":"code","a947c7c2":"code","b788f70d":"code","0d7d7c15":"code","a3927463":"code","0d8f2904":"code","2381ce85":"code","73a009f0":"code","c7f7611e":"code","948ac14d":"code","90ba601a":"code","e04032c7":"code","df9a5596":"code","7402e563":"code","c8ff5bef":"code","ec813004":"code","372cd931":"code","4a1967af":"code","5a5fc0ff":"code","1d264d41":"markdown","c438b547":"markdown","c0d663d8":"markdown","32173161":"markdown","8c80b6c8":"markdown","bf1c1747":"markdown","721ba06b":"markdown","7159bc8c":"markdown","051ec0b9":"markdown","ee4114e0":"markdown","e35627e8":"markdown","b070f2de":"markdown","b5f10407":"markdown","026445cb":"markdown","ff00488d":"markdown","b12e827c":"markdown","4779e3e0":"markdown","c7ea9f8b":"markdown","d380e43d":"markdown","37444300":"markdown","86d77b94":"markdown","8bb57550":"markdown","e64c8d6d":"markdown","4e1b8097":"markdown"},"source":{"be542f19":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, PolynomialFeatures, PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, cross_validate, GridSearchCV, cross_val_predict\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn import set_config\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom yellowbrick.model_selection import ValidationCurve\nimport shap","a8f3bdc2":"plt.rcParams['figure.figsize'] = (12, 8)\nset_config(display='diagram')\n\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\nshap.initjs()","6a3166fa":"full_df = pd.read_csv('..\/input\/wine-quality\/winequalityN.csv')\n\nfull_df.head()","aa946750":"full_df['quality'].value_counts()","0b04df83":"def impute_quality_group(quality):\n    if quality <= 5:\n        return 0 # low\n    if quality > 5 and quality < 7:\n        return 1 # average\n    if quality >= 7:\n        return 2 # high\n\nfull_df['quality_group'] = full_df['quality'].apply(impute_quality_group)","52159a49":"full_df['quality_group'].value_counts()","c42eb657":"for feature in ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'pH', 'sulphates']:\n    full_df[feature] = full_df.groupby(['type'])[feature].transform(lambda x: x.fillna(x.median()))","a92b53aa":"def impute_sweetness(residual_sugar):\n    if residual_sugar < 1:\n        return 0\n    if residual_sugar >= 1 and residual_sugar < 9:\n        return 1\n    if residual_sugar >= 9 and residual_sugar < 18:\n        return 2\n    if residual_sugar >= 18 and residual_sugar < 50:\n        return 3\n    if residual_sugar >= 50 and residual_sugar < 120:\n        return 4\n    if residual_sugar >= 120:\n        return 5\n\nfull_df['sweetness'] = full_df['residual sugar'].apply(impute_sweetness)","357b0653":"full_df['fixed_acidity_red_wine'] = (full_df['type'] == 'red') * full_df['fixed acidity']\nfull_df['fixed_acidity_white_wine'] = (full_df['type'] == 'white') * full_df['fixed acidity']\n\nfull_df['molecular_sulfur_dioxid'] = full_df['free sulfur dioxide'] \/ (1 + 10 ** (full_df['pH'] - 1.8))\nfull_df['free_total_so2_rate'] = full_df['free sulfur dioxide'] \/ full_df['total sulfur dioxide']\nfull_df['bound_sulfur_dioxid'] = full_df['total sulfur dioxide'] - full_df['free sulfur dioxide']\nfull_df['sugar_acidity_ratio'] = full_df['residual sugar'] \/ full_df['fixed acidity']\n\nalcohol_labels = ['low', 'medium', 'high']\nalcohol_bins = [0, 9.5, 11.5, 20]\nfull_df['alcohol_groups'] = pd.cut(full_df['alcohol'], bins=alcohol_bins, labels=alcohol_labels) \n\npH_labels = ['high', 'mod high', 'medium', 'low']\npH_bins = [2.5, 3.2, 3.3, 3.4, 4.1]\nfull_df['pH_groups'] = pd.cut(full_df['pH'], bins=pH_bins, labels=pH_labels) ","e497516d":"model_features = [\n    'type',\n    'alcohol',\n    'fixed acidity',\n    'volatile acidity',\n    'citric acid',\n    'pH',\n    'residual sugar',\n    'free sulfur dioxide',\n    'chlorides',\n    'density',\n    'sulphates',\n    'bound_sulfur_dioxid',\n    'molecular_sulfur_dioxid',\n    'sugar_acidity_ratio'\n]\n\nX = full_df[model_features]\ny = full_df['quality_group']\n\nstratified_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)\n\nfor train_idx, test_idx in stratified_splitter.split(X, y):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]","fd7d92c0":"def get_feature_transformer():\n    oneplus_transformer = FunctionTransformer(func=lambda x: 1 + x, inverse_func=lambda x: 1 - x)\n    boxcox_transformer = PowerTransformer(method='box-cox', standardize=False)\n\n    numerical_transformer = Pipeline([\n        ('positive_transforming', oneplus_transformer),\n        ('boxcox_transforming', boxcox_transformer),\n    ])\n\n    return ColumnTransformer([\n            ('feature_transforming', numerical_transformer, [\n                'fixed acidity', 'chlorides', 'citric acid', 'volatile acidity', \n                'sulphates', 'alcohol', 'residual sugar', 'free sulfur dioxide', \n                'sulphates', 'pH', 'sugar_acidity_ratio'\n            ]),\n            ('wine_type_onehot', OneHotEncoder(), ['type']),\n        ],\n        remainder='passthrough'\n    )","71b670a9":"def plot_confusion_matrix_by_predictions(y_true, y_predicted, *, labels=None,\n                          sample_weight=None, normalize=None,\n                          display_labels=None, include_values=True,\n                          xticks_rotation='horizontal',\n                          values_format=None,\n                          cmap='viridis', ax=None):\n    \n    cm = confusion_matrix(y_true, y_predicted, sample_weight=sample_weight,\n                          labels=labels, normalize=normalize)\n\n    if display_labels is None:\n        if labels is None:\n            display_labels = unique_labels(y_true, y_predicted)\n        else:\n            display_labels = labels\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=display_labels)\n\n    return disp.plot(include_values=include_values,\n                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,\n                     values_format=values_format)\n","f1811795":"def score_classification_model(model, X_train, y_train):\n    \n    cv_scores = cross_validate(\n        model, X_train, y_train, \n        scoring=['f1_weighted', 'balanced_accuracy'],\n        cv=5,\n        n_jobs=-1, verbose=0\n    )\n\n    cv_y_predicted = cross_val_predict(\n        model, X_train, y_train,\n        cv=5,\n        n_jobs=-1\n    )\n\n    cv_f1_weighted, f1_weighted_std = cv_scores['test_f1_weighted'].mean(), cv_scores['test_f1_weighted'].std()\n    cv_balanced_accuracy, balanced_accuracy_std = cv_scores['test_balanced_accuracy'].mean(), cv_scores['test_balanced_accuracy'].std()\n\n    model.fit(X_train, y_train)\n\n    y_train_predicted = model.predict(X_train)\n\n    train_f1_weighted = f1_score(y_train, y_train_predicted, average='weighted')\n    train_balanced_accuracy = balanced_accuracy_score(y_train, y_train_predicted)\n\n    print('[Train] F1 Weighted: %.4f' % (train_f1_weighted))\n    print('[Train] Balanced Accuracy: %.4f' % (train_balanced_accuracy))\n    print('Train Set Report:')\n    print(classification_report(y_train, y_train_predicted, digits=3))\n\n    print('[CV] F1 Weighted: %.4f (%.4f)' % (cv_f1_weighted, f1_weighted_std))\n    print('[CV] Balanced Accuracy: %.4f (%.4f)' % (cv_balanced_accuracy, balanced_accuracy_std))\n    print('CV Report:')\n    print(classification_report(y_train, cv_y_predicted, digits=3))\n    \n    # display confusion matrixes\n\n    _, (ax0, ax1) = plt.subplots(1, 2)\n\n    ax0.set_title('Train Confusion Matrix')\n    plot_confusion_matrix(\n        model, X_train, y_train,\n        cmap=plt.cm.Blues,\n        normalize='true',\n        ax=ax0,\n    )\n\n    ax1.set_title('CV Confusion Matrix')\n    plot_confusion_matrix_by_predictions(\n        y_train, cv_y_predicted,\n        cmap=plt.cm.Blues,\n        normalize='true',\n        ax=ax1,\n    )\n\n    return y_train_predicted, cv_y_predicted","f9a05ed6":"# sklearn's pipeline API is limited at this point and doesn't provide a way to get columns of transformed X array\n# This snippet will cover our back \n\ndef get_columns_from_transformer(column_transformer, input_colums):    \n    col_name = []\n\n    for transformer_in_columns in column_transformer.transformers_[:-1]: #the last transformer is ColumnTransformer's 'remainder'\n        raw_col_name = transformer_in_columns[2]\n        if isinstance(transformer_in_columns[1],Pipeline): \n            transformer = transformer_in_columns[1].steps[-1][1]\n        else:\n            transformer = transformer_in_columns[1]\n        try:\n            names = transformer.get_feature_names(raw_col_name)\n        except AttributeError: # if no 'get_feature_names' function, use raw column name\n            names = raw_col_name\n        if isinstance(names,np.ndarray): # eg.\n            col_name += names.tolist()\n        elif isinstance(names,list):\n            col_name += names    \n        elif isinstance(names,str):\n            col_name.append(names)\n\n    [_, _, reminder_columns] = column_transformer.transformers_[-1]\n\n    for col_idx in reminder_columns:\n        col_name.append(input_colums[col_idx])\n\n    return col_name","9e3a90a7":"logistic_regression = LogisticRegression(\n    solver='liblinear',\n    penalty='l1',\n    C=0.9,\n    max_iter=500,\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n    n_jobs=-1,\n)\n\nlogistic_regression_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('scaling', StandardScaler()),\n    ('quality_classification', logistic_regression),\n])\n\nlogistic_regression_pipeline\n\n## [Tr] F1 Weighted: 0.5732, Balanced Accuracy: 0.5920\n## [CV] F1 Weighted: 0.5688 (0.0135), Balanced Accuracy: 0.5866 (0.0099)\n# solver='liblinear',\n# penalty='l1',\n# C=0.9,\n# max_iter=500,\n# class_weight='balanced'","29bc448e":"score_classification_model(logistic_regression_pipeline, X_train, y_train);","04190a86":"parameters = {\n    'quality_classification__penalty': ['l2', 'l1', 'elasticnet', 'none'], # 'l1', 'elasticnet', 'none'\n    'quality_classification__C': [1.0, 0.95, 0.9, 0.8], # 1.0\n    'quality_classification__tol': [1e-4],\n    'quality_classification__class_weight': ['balanced'],\n    'quality_classification__solver': ['lbfgs', 'liblinear', 'sag', 'saga'], # lbfgs\n    'quality_classification__max_iter': [500],\n    'quality_classification__l1_ratio': [1.0, 0.0, 0.3, 0.4, 0.5],\n}\n\nparam_searcher = GridSearchCV(\n   estimator=logistic_regression_pipeline,\n   scoring='balanced_accuracy',\n   param_grid=parameters,\n   cv=5,\n   n_jobs=-1, \n   verbose=3\n)\n\n#param_searcher.fit(X_train, y_train)\n#param_searcher.best_params_, param_searcher.best_score_","7c7b8711":"logistic_classifier = LogisticRegression(\n    penalty='l2',\n    solver='newton-cg',\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n    n_jobs=-1,\n)\n\npolynomial_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('polynomial_features', PolynomialFeatures()),\n    ('scaling', StandardScaler()),\n    ('quality_classification', logistic_classifier),\n])\n\npolynomial_pipeline","0f4fa391":"score_classification_model(polynomial_pipeline, X_train, y_train);","f0222d71":"parameters = [\n    {\n        'quality_classification__solver': ['newton-cg'], # lbfgs, liblinear, 'lbfgs', 'sag', 'saga',\n        'quality_classification__penalty': ['l2', 'l1', 'elasticnet', 'none'], # 'l1', 'elasticnet', 'none'\n        'quality_classification__C': [1.0], # 1.0\n        'quality_classification__l1_ratio': [1.0, 0.9],\n        'quality_classification__max_iter': [100, 200],\n        'quality_classification__class_weight': ['balanced'],\n        'polynomial_features__degree': [2],\n    },\n]\n\nparam_searcher = GridSearchCV(\n   estimator=polynomial_pipeline,\n   scoring='balanced_accuracy',\n   param_grid=parameters,\n   cv=5,\n   n_jobs=-1, \n   verbose=3\n)\n\n#param_searcher.fit(X_train, y_train)\n#param_searcher.best_params_, param_searcher.best_score_","d4a0c9e8":"from sklearn.svm import LinearSVC, SVC\n\nlsvm_classifier = LinearSVC(\n    C=0.01,\n    max_iter=1000,\n    loss='squared_hinge',\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n)\n\nlsvm_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('scaling', StandardScaler()),\n    ('quality_classification', lsvm_classifier),\n])\n\npsvm_classifier = SVC(\n    kernel='poly',\n    degree=4,\n    coef0=1,\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n)\n\npsvm_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('scaling', StandardScaler()),\n    ('quality_classification', psvm_classifier),\n])\n\nksvm_classifier = SVC(\n    kernel='rbf',\n    C=5,\n    gamma=0.01,\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n)\n\nksvm_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('scaling', StandardScaler()),\n    ('quality_classification', ksvm_classifier),\n])","398f38fe":"score_classification_model(lsvm_pipeline, X_train, y_train);\n\n# [CV] F1 Weighted: 0.5552 (0.0132)\n# [CV] Balanced Accuracy: 0.5846 (0.0094)","76a1a9b2":"score_classification_model(psvm_pipeline, X_train, y_train);\n\n## [Train] F1 Weighted: 0.5476, Balanced Accuracy: 0.5654\n## [CV] F1 Weighted: 0.5225 (0.0097), Balanced Accuracy: 0.5395 (0.0134)\n# kernel='poly',\n# degree=2,\n# class_weight='balanced'\n\n## [Train] F1 Weighted: 0.5979, Balanced Accuracy: 0.6337\n## [CV] F1 Weighted: 0.5807 (0.0036), Balanced Accuracy: 0.6147 (0.0090)\n# kernel='poly',\n# degree=2,\n# coef0=1,\n# class_weight='balanced'\n\n## [Train] F1 Weighted: 0.6323, Balanced Accuracy: 0.6724\n## [CV] F1 Weighted: 0.5865 (0.0068), Balanced Accuracy: 0.6264 (0.0127)\n# kernel='poly',\n# degree=3,\n# coef0=1,\n# class_weight='balanced'\n\n## [Train] F1 Weighted: 0.6874, Balanced Accuracy: 0.7242\n## [CV] F1 Weighted: 0.6072 (0.0154), Balanced Accuracy: 0.6388 (0.0167)\n# kernel='poly',\n# degree=4,\n# coef0=1,\n# class_weight='balanced'","c846dd99":"score_classification_model(ksvm_pipeline, X_train, y_train);\n\n## [Train] F1 Weighted: 0.7151, [CV] Balanced Accuracy: 0.7483\n## [CV] F1 Weighted: 0.6048 (0.0102), [CV] Balanced Accuracy: 0.6400 (0.0124)\n# kernel='rbf',\n# C=10,\n# class_weight='balanced'\n\n## [Train] F1 Weighted: 0.6772, [CV] Balanced Accuracy: 0.7152\n## [CV] F1 Weighted: 0.5971 (0.0140), [CV] Balanced Accuracy: 0.6353 (0.0190)\n# kernel='rbf',\n# C=5,\n# class_weight='balanced'\n\n## [Train] F1 Weighted: 0.5954, [CV] Balanced Accuracy: 0.6341\n## [CV] F1 Weighted: 0.5784 (0.0041), [CV] Balanced Accuracy: 0.6191 (0.0039)\n# kernel='rbf',\n# C=5,\n# gamma=0.01,\n# class_weight='balanced'","a947c7c2":"parameters = {\n    'quality_classification__C': [0.01, 0.1, 1],\n}\n\nparam_searcher = GridSearchCV(\n   estimator=lsvm_pipeline,\n   scoring='balanced_accuracy',\n   param_grid=parameters,\n   cv=5,\n   n_jobs=-1, \n   verbose=3\n)\n\n# param_searcher.fit(X_train, y_train)\n# param_searcher.best_params_, param_searcher.best_score_","b788f70d":"parameters = {\n    'quality_classification__C': [20, 60, 70, 80, 90],\n    'quality_classification__gamma': ['scale', 'auto', 0.01, 0.1, 1, 5, 10],\n}\n\nparam_searcher = GridSearchCV(\n   estimator=ksvm_pipeline,\n   scoring='balanced_accuracy',\n   param_grid=parameters,\n   cv=5,\n   n_jobs=-1, \n   verbose=3\n)\n\n#param_searcher.fit(X_train, y_train)\n#param_searcher.best_params_, param_searcher.best_score_","0d7d7c15":"from sklearn.tree import DecisionTreeClassifier\n\ntree_classifier = DecisionTreeClassifier(\n    max_depth=12,\n    max_leaf_nodes=65,\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n)\n\ntree_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('quality_classification', tree_classifier),\n])\n\n## [CV] F1 Weighted: 0.5421 (0.0147), Balanced Accuracy: 0.5787 (0.0163)\n# max_leaf_nodes=25","a3927463":"score_classification_model(tree_pipeline, X_train, y_train);","0d8f2904":"parameters = {\n    'quality_classification__max_depth': np.arange(1, 15),\n    'quality_classification__max_leaf_nodes': np.arange(1, 80, 5),\n}\n\nparam_searcher = GridSearchCV(\n   estimator=tree_pipeline,\n   scoring='balanced_accuracy',\n   param_grid=parameters,\n   cv=5,\n   n_jobs=-1, \n   verbose=3\n)\n\nparam_searcher.fit(X_train, y_train)\nparam_searcher.best_params_, param_searcher.best_score_","2381ce85":"rf_classifier = RandomForestClassifier(\n    criterion='entropy',\n    n_estimators=200,\n    max_depth=6,\n    max_leaf_nodes=10,\n    max_features='sqrt',\n    class_weight='balanced',\n    random_state=RANDOM_SEED,\n    n_jobs=-1,\n)\n\nrf_pipeline = Pipeline([\n    ('feature_processing', get_feature_transformer()),\n    ('quality_classification', rf_classifier),\n])\n\nrf_pipeline\n\n## F1 Weighted: 0.6949 (0.0167), Balanced Accuracy: 0.6948 (0.0211)\n# criterion='entropy',\n# n_estimators=179,\n# min_samples_split=5,\n# min_samples_leaf=4,\n# max_features='sqrt',\n# class_weight='balanced'\n\n## F1 Weighted: 0.4946 (0.0078), Balanced Accuracy: 0.5912 (0.0106)\n# criterion='entropy',\n# n_estimators=200,\n# max_depth=6,\n# max_leaf_nodes=10,\n# max_features='sqrt',\n# class_weight='balanced'","73a009f0":"y_train_pred, y_cv_pred = score_classification_model(rf_pipeline, X_train, y_train);","c7f7611e":"X_train_features = get_columns_from_transformer(rf_pipeline.named_steps['feature_processing'], list(X_train.columns))","948ac14d":"features_importance = sorted(zip(rf_pipeline.named_steps['quality_classification'].feature_importances_, X_train_features), reverse=True)\npd.DataFrame(features_importance, columns=['importance', 'feature'])","90ba601a":"X_train_transformed = rf_pipeline.named_steps['feature_processing'].fit_transform(X_train)\n\nrf_explainer = shap.TreeExplainer(rf_classifier)\nrf_explanation = rf_explainer.shap_values(X_train_transformed)","e04032c7":"shap.summary_plot(rf_explanation, X_train_transformed, X_train_features)","df9a5596":"def score_model_generalization(model, X_test, y_test):\n    y_test_predicted = model.predict(X_test)\n\n    test_f1_weighted = f1_score(y_test, y_test_predicted, average='weighted')\n    test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_predicted)\n\n    print('[Test] F1 Weighted: %.4f' % (test_f1_weighted))\n    print('[Test] Balanced Accuracy: %.4f' % (test_balanced_accuracy))\n    print('Test Set Report:')\n    print(classification_report(y_test, y_test_predicted, digits=3))\n\n    plot_confusion_matrix_by_predictions(\n        y_test, y_test_predicted,\n        cmap=plt.cm.Greens,\n        normalize='true',\n    )","7402e563":"score_model_generalization(logistic_regression_pipeline, X_test, y_test)","c8ff5bef":"score_model_generalization(polynomial_pipeline, X_test, y_test)","ec813004":"score_model_generalization(lsvm_pipeline, X_test, y_test)","372cd931":"score_model_generalization(psvm_pipeline, X_test, y_test)","4a1967af":"score_model_generalization(ksvm_pipeline, X_test, y_test)","5a5fc0ff":"score_model_generalization(rf_pipeline, X_test, y_test)","1d264d41":"The most obvious classification objective for this training set is **multiclass wine quality classification**. \n\nThe dataset is **higly imbalanced**. We have only 5 samples of exellent wines and 30 samples of the lowest quality wines. If we take into account test set split and cross-validation folds, we may have only a couple of examples during training. This means that there may be a problem of applying SMOTE and similar synthetical methods to balance datasets as they would require more samples to create clusters for samplings from.\n\nOther possible objectives are:\n- multiclass quality classification with only 3 classes: low, medium, high quality wines\n- binary quality classification: good or bad quality wine\n- binary wine type classification: red or white wine (which would also suffer from imbalance, but could be fixed by synthetic resampling)","c438b547":"## DecisionTree","c0d663d8":"## SVC","32173161":"## LogisticRegression","8c80b6c8":"# Data Processing","bf1c1747":"## SVM","721ba06b":"### Hypertuning","7159bc8c":"## Polynomial Regression","051ec0b9":"We will stick with **multiclass quality classification** and 3 classes: low, medium, high quality wines:","ee4114e0":"# Model Inspection \ud83d\udd0e","e35627e8":"# Feature Engineering","b070f2de":"# Classification Objective \ud83c\udfaf","b5f10407":"### Hyperparam Tuning","026445cb":"# Summary \ud83d\udcab\n\nWine Quality database is a good example of datasets you may face in the real life. It's **imbalanced** and **quality classes** are hard to separate. It pushed us to rethinking the our classification objectives and assuming what we could potentialy sqeeze from it.\n\nWe have trained several models from a simple Logistic Regression and SVM to RandomForest and measured their performance with **balanced accuracy** metric.\n\nTurned out, the polinomial **SVC model** performs best for us:\n- CV: 63.88% (-+1.67%)\n- Test: 61.73% \n\nOur goal was to get the higher balanced accuracy while keeping a score difference between train and CV scores small.\n\nRandomForest and XGBoost are easily overfit and show around 70% of balanced accuracy on the CV and test datasets (while almost 100% on training sets). However, we don't believe these model would generalize when even if they showed good results on the current test set (which include only 1300 observations (20% of the overall dataset)).\n\n**Another approach to improve the accuracy** is to train two separate models for red and white wines. Meanwhile, the fact that quality classes are hardly separable makes us think it would be little improvement.","ff00488d":"## Logistic Regression","b12e827c":"# Modelling \ud83e\uddea","4779e3e0":"Feature Engineering:\n- `total sulfur dioxide` - doesn't improve models in a raw view\n- `free_total_so2_rate` - brings 0 improvements\n- `sweetness` - degrades performance of all models\n- `alcohol_groups` - degrades performance of all models\n- `pH_groups` - degrades performance of all models\n- `sugar_acidity_ratio` improves score's std but degrades CV scores","c7ea9f8b":"## Polynomial Regression","d380e43d":"### Hyperparam Tuning","37444300":"# Generalization","86d77b94":"### Hypertuning","8bb57550":"## Random Forest","e64c8d6d":"## RandomForest","4e1b8097":"# Wine Quality Analysis\n\n<img style=\"margin-left:0\" src=\"https:\/\/thumbor.forbes.com\/thumbor\/fit-in\/1200x0\/filters%3Aformat%28jpg%29\/https%3A%2F%2Fspecials-images.forbesimg.com%2Fdam%2Fimageserve%2F1133888244%2F0x0.jpg%3Ffit%3Dscale\" width=\"600px\" \/>\n\nThis notebook analyse a database of **red** and **white** variants of the Portuguese \"Vinho Verde\" wine based on wine **physicochemical test results** and quality scores that experts assign to each wine sample.\n\n- EDA Part of the Analysis: https:\/\/www.kaggle.com\/glushko\/wine-quality-domain-driven-eda-part-i\n- Feel free to upvote this notebook if you find it helpful \ud83d\udcab"}}