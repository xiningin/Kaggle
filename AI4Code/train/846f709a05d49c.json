{"cell_type":{"e8bfdf52":"code","d780057d":"code","440775b2":"code","896b8672":"code","53a9d6bd":"code","7fbce96c":"code","5a5e0577":"code","188ffe71":"code","f484c7d6":"code","1132a9fc":"code","466fabdf":"code","9e62dfe4":"code","97f84ec3":"code","c18f6a67":"code","e2f01f3d":"code","9403c8d4":"code","ff5bed44":"code","980d3e24":"code","c5e24f9d":"code","11114697":"code","088b923f":"code","c1a77ac6":"code","5be132bb":"code","5355ea78":"code","1bf29f93":"code","751022af":"code","38bf6f7d":"code","b41055d9":"code","fa00c25e":"code","2930e167":"code","6a99c802":"code","8d8c1475":"code","1bf2030c":"code","4b2ac00a":"code","250ee915":"code","5cf32700":"code","27d5457f":"code","ac5cd104":"code","110ad3ac":"code","f94b9c3e":"code","2250dd5f":"code","abb4f8b8":"code","f869f548":"code","0bf0eff1":"code","a32f4f1a":"code","b4851909":"code","990dc67a":"code","e6352297":"code","8ff2b6f0":"code","e2d85385":"code","9e2317f1":"code","bbb11724":"code","2dfab154":"code","5db31b59":"code","ddc21fe2":"code","f6278eb5":"code","869521d4":"code","83961351":"code","7f173fe6":"code","cb5ff515":"code","8d6a7186":"code","3c7b19d8":"code","efca25b7":"code","da8f6cee":"code","e7fc8541":"code","a9425ba4":"code","b4311007":"code","cdac1057":"code","0d3f8b17":"code","6921e2df":"code","f35ddb7f":"code","1f16c666":"code","a32b40a4":"code","892bfc7c":"code","6ba4f8a0":"code","2267d810":"code","a0b8976d":"code","4eb303d9":"code","a23b6a94":"code","5729f2f4":"code","b5bf5663":"code","2d057c2a":"code","ba09aaae":"code","f23b3189":"code","cda6c6e8":"code","f25f140f":"code","3530531d":"code","0d044588":"code","db02d3a9":"code","9804281f":"code","f7012f01":"code","461b8b3c":"code","1802ca92":"code","dfa46a3f":"code","11f9f75e":"code","811735d0":"code","c1ef5fb3":"code","b9b5c4a0":"code","0182ae7b":"code","55cd2355":"code","a66fb98f":"code","30406f2f":"code","8e391f27":"code","d56ff66c":"code","2e4ef979":"code","f0949d6b":"code","986171a9":"code","b0e054bc":"code","71b029f2":"code","958f2157":"code","142da285":"code","91e50162":"code","327d7912":"code","756c6628":"code","f26f64f0":"code","6492f296":"code","7bb52a6c":"code","d3fc728a":"code","774584dc":"code","da715c0e":"code","5893fec6":"markdown","888e0e73":"markdown","bef4c298":"markdown","1e8c6f38":"markdown","cd4de342":"markdown","23454175":"markdown","ee4f6a70":"markdown","c339df70":"markdown","b60c36a0":"markdown","aa38d71a":"markdown","88a63a0e":"markdown","a6b5b6c4":"markdown","d892c651":"markdown","bc22b1bc":"markdown","23a9223f":"markdown","73ce9bdb":"markdown","8909848f":"markdown","c8733a48":"markdown","3c2352ea":"markdown","105087f9":"markdown","7706dfbb":"markdown","55529fb0":"markdown","3b271dde":"markdown","ea59ff7f":"markdown","ef23d9b8":"markdown","515f5d11":"markdown","95b708c3":"markdown"},"source":{"e8bfdf52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d780057d":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","440775b2":"train.head(5)","896b8672":"train.shape","53a9d6bd":"#the train dataset is reasonably balanced in that there are a roughly similar proportion of 1 and 0 in target column\ntrain.target.sum() \/ train.shape[0]","7fbce96c":"#standard practice is to concat train and test as test may contain text never seen before \ndf = pd.concat([train,test],axis=0)","5a5e0577":"#settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(use_idf=True,\n                                ngram_range=(1,5),\n                                #token_pattern='[a-zA-Z]{3}',#only alpha data >= 3 in length\n                                stop_words='english',\n                                min_df=3,\n                                max_features=5000,\n                                binary=False # try a separate one with this set to true\n                                )\n\n#fit transform\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(df['text'])","188ffe71":"#tfidf_vectorizer_vectors","f484c7d6":"\ndf['hashtags'] = df['text'].str.findall(r'#.*?(?=\\s|$)').apply(lambda x: ','.join(map(str, x)))\ntrain['hashtags'] = train['text'].str.findall(r'#.*?(?=\\s|$)').apply(lambda x: ','.join(map(str, x))) \ntest['hashtags'] = test['text'].str.findall(r'#.*?(?=\\s|$)').apply(lambda x: ','.join(map(str, x))) \n","1132a9fc":"#length of tweet\ntrain['len'] = train['text'].str.len()\ntest['len'] = test['text'].str.len()","466fabdf":"#number of mentions @\ntrain['no_ats_spec_char'] = train['text'].str.count('@')\ntest['no_ats_spec_char'] = test['text'].str.count('@')","9e62dfe4":"#number of hashtags # present \ntrain['no_hashtags_spec_char'] = train['text'].str.count('#')\ntest['no_hashtags_spec_char'] = test['text'].str.count('#')","97f84ec3":"#special characters total count\npat = '[(:\/,#%\\=@)]'\ntrain['no_total_spec_char'] = train['text'].str.count(pat)\ntest['no_total_spec_char'] = test['text'].str.count(pat)","c18f6a67":"#position of mention @ in text\ntrain['pos_at_char'] = train['text'].str.find('@') \ntest['pos_at_char'] = test['text'].str.find('@') ","e2f01f3d":"#position of hashtag # in text\ntrain['pos_hashtag_char'] = train['text'].str.find('#') \ntest['pos_hashtag_char'] = test['text'].str.find('#') ","9403c8d4":"#number of words\ntrain['no_words'] = train['text'].str.count(' ').add(1).value_counts(sort=False)\ntest['no_words'] = test['text'].str.count(' ').add(1).value_counts(sort=False)","ff5bed44":"#average length of words\ntrain['avg_len'] = train['len'] \/ train['no_words']\ntest['avg_len'] = test['len'] \/ test['no_words']","980d3e24":"#categorise keyword data\n#nb we should ensure that we apply same categories to train and test \ntrain['keyword'] = train['keyword'].replace(np.nan, 'None')\ntest['keyword'] = test['keyword'].replace(np.nan, 'None')\nkeyword_unique = pd.concat([train['keyword'],test['keyword']],axis=0).unique()\ntrain['keyword'] = pd.Categorical(train['keyword'],categories=keyword_unique)\ntest['keyword'] = pd.Categorical(test['keyword'],categories=keyword_unique)\n","c5e24f9d":"keyword_unique","11114697":"#categorise location data\n#nb we should ensure that we apply same categories to train and test \ntrain['location'] = train['location'].replace(np.nan, 'None')\ntest['location'] = test['location'].replace(np.nan, 'None')\nlocation_unique = pd.concat([train['location'],test['location']],axis=0).unique()\ntrain['location'] = pd.Categorical(train['location'],categories=location_unique)\ntest['location'] = pd.Categorical(test['location'],categories=location_unique)\n","088b923f":"#tweet contains numeric data\npat = '[01232456789]'\ntrain['contains_number'] = np.where(train['text'].str.count(pat)>=1,1,0)\ntest['contains_number'] = np.where(test['text'].str.count(pat)>=1,1,0)","c1a77ac6":"df.head()","5be132bb":"#spit into train validation datasets\nX_train, X_val, y_train, y_val = train_test_split(train,\n                                                  train['target'],\n                                                 random_state = 888,\n                                                 test_size = 0.2,\n                                                 shuffle = True)","5355ea78":"#apply the tfidf to datasets\ntrain_set = tfidf_vectorizer.transform(X_train['text'])\nval_set = tfidf_vectorizer.transform(X_val['text'])\ntest_set = tfidf_vectorizer.transform(test['text'])","1bf29f93":"#set the parameters of the model\nparams = { \"objective\" : \"binary\", # binary classification is the type of business case we are running\n        \"metric\" :\"F1\", #F1 score is 2 * (TP) \/ (TP + FP) is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 2000,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n    \n    \n}","751022af":"#Run the model\nm1_lgb = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1#, \n#                             learning_rate=0.05, \n#                             max_depth=20, \n#                             num_leaves=50, \n#                             n_estimators=1000, \n#                             max_bin=200\n                           ) #params,  verbose_eval = 50)","38bf6f7d":"m1_lgb.fit(train_set, y_train) ","b41055d9":"#plot feature importance\nfeature_imp = pd.DataFrame({'Value':m1_lgb.feature_importances_,'Feature':tfidf_vectorizer.get_feature_names()})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","fa00c25e":"#generate predictions use predict_proba for ensemble models later on\npred_test = m1_lgb.predict_proba(test_set)[:,1]\npred_val = m1_lgb.predict_proba(val_set)[:,1]","2930e167":"#pred_val","6a99c802":"#test.head()","8d8c1475":"#f1 score, between 0 and 1, the higher the better\nf1_score(X_val['target'],np.where(pred_val>=0.5,1,0))","1bf2030c":"submission_m1 = test[['id']].copy()\nsubmission_m1['target'] =  np.where(pred_test>= 0.5,1,0)","4b2ac00a":"submission_m1.head()","250ee915":"submission_m1.shape","5cf32700":"#save submission\nsubmission_m1.to_csv('.\/submission_m1.csv',index=False)","27d5457f":"#settings that you use for count vectorizer will go here\ntfidf_vectorizer_hashtags=TfidfVectorizer(use_idf=True,\n                                ngram_range=(1,1),\n                                #token_pattern='[a-zA-Z]{3}',#only alpha data >= 3 in length\n                                stop_words='english',\n                                min_df=3,\n                                max_features=500,\n                                binary=False # try a separate one with this set to true\n                                )","ac5cd104":"#fit transform\ntfidf_vectorizer_vectors_hashtags=tfidf_vectorizer_hashtags.fit_transform(df['hashtags'])","110ad3ac":"train_set_hashtags = tfidf_vectorizer_hashtags.transform(X_train['hashtags'])\nval_set_hashtags = tfidf_vectorizer_hashtags.transform(X_val['hashtags'])\ntest_set_hashtags = tfidf_vectorizer_hashtags.transform(test['hashtags'])","f94b9c3e":"#Run the model\nm1_lgb_hashtags = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1#, \n#                             learning_rate=0.05, \n#                             max_depth=20, \n#                             num_leaves=50, \n#                             n_estimators=1000, \n#                             max_bin=200\n                                    ) #params,  verbose_eval = 50)","2250dd5f":"m1_lgb_hashtags.fit(train_set_hashtags, y_train) ","abb4f8b8":"len(tfidf_vectorizer_hashtags.get_feature_names())","f869f548":"#tfidf_vectorizer_hashtags.get_feature_names()","0bf0eff1":"m1_lgb_hashtags.feature_importances_.shape","a32f4f1a":"#plot feature importance\nfeature_imp_hashtags = pd.DataFrame({'Value':m1_lgb_hashtags.feature_importances_,'Feature':tfidf_vectorizer_hashtags.get_feature_names()})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_hashtags.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","b4851909":"pred_test_hashtags = m1_lgb_hashtags.predict_proba(test_set_hashtags)[:,1]\npred_val_hashtags = m1_lgb_hashtags.predict_proba(val_set_hashtags)[:,1]","990dc67a":"f1_score(X_val['target'],np.where(pred_val_hashtags>=0.5,1,0))","e6352297":"submission_m2 = test[['id']].copy()\nsubmission_m2['target'] =  np.where(pred_test_hashtags>= 0.5,1,0)","8ff2b6f0":"submission_m2.to_csv('.\/submission_m2.csv',index=False)","e2d85385":"df.head()","9e2317f1":"import nltk\nimport string\nfrom nltk.stem.porter import PorterStemmer\n\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item))\n    return stems","bbb11724":"tfidf_stem = TfidfVectorizer(tokenizer=tokenize, stop_words='english')","2dfab154":"#fit transform\ntfidf_vectorizer_vectors_stem=tfidf_stem.fit_transform(df['text'])","5db31b59":"train_set_stem = tfidf_stem.transform(X_train['text'])\nval_set_stem = tfidf_stem.transform(X_val['text'])\ntest_set_stem = tfidf_stem.transform(test['text'])","ddc21fe2":"#Run the model\nm1_lgb_stem = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1, \n                            learning_rate=0.05, \n                            max_depth=20, \n                            num_leaves=50, \n                            n_estimators=1000, \n                            max_bin=200) #params,  verbose_eval = 50)","f6278eb5":"m1_lgb_stem.fit(train_set_stem, y_train) ","869521d4":"#plot feature importance\nfeature_imp_stem = pd.DataFrame({'Value':m1_lgb_stem.feature_importances_,'Feature':tfidf_stem.get_feature_names()})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_stem.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","83961351":"pred_test_stem = m1_lgb_stem.predict_proba(test_set_stem)[:,1]\npred_val_stem = m1_lgb_stem.predict_proba(val_set_stem)[:,1]\n","7f173fe6":"f1_score(X_val['target'],np.where(pred_val_stem>=0.5,1,0))","cb5ff515":"submission_m3 = test[['id']].copy()\nsubmission_m3['target'] =  np.where(pred_test_stem>= 0.5,1,0)","8d6a7186":"submission_m3.to_csv('.\/submission_m3.csv',index=False)","3c7b19d8":"df.columns","efca25b7":"train_set_other = X_train[['len','no_ats_spec_char', 'no_hashtags_spec_char', 'no_total_spec_char','pos_at_char', 'pos_hashtag_char', 'contains_number','avg_len','keyword','location']]\nval_set_other = X_val[['len','no_ats_spec_char', 'no_hashtags_spec_char', 'no_total_spec_char','pos_at_char', 'pos_hashtag_char', 'contains_number','avg_len','keyword','location']]\ntest_set_other = test[['len','no_ats_spec_char', 'no_hashtags_spec_char', 'no_total_spec_char','pos_at_char', 'pos_hashtag_char', 'contains_number','avg_len','keyword','location']]","da8f6cee":"#set the parameters\nparams = { \"objective\" : \"binary\", # binary classification is the type of business case we are running\n        \"metric\" :\"F1\", #F1 score is 2 * (TP) \/ (TP + FP) is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 500,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n    \n    \n}","e7fc8541":"\n#Run the model\nm5_lgb = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1#, \n                            #learning_rate=0.05, \n                            #max_depth=20, \n                            #num_leaves=50, \n                            #n_estimators=1000, \n                            #max_bin=200\n                           ) #params,  verbose_eval = 50)\n","a9425ba4":"m5_lgb","b4311007":"m5_lgb.fit(train_set_other, y_train) ","cdac1057":"\n#plot feature importance\nfeature_imp = pd.DataFrame({'Value':m5_lgb.feature_importances_,'Feature':train_set_other.columns})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()\n","0d3f8b17":"pred_test_other = m5_lgb.predict_proba(test_set_other)[:,1]\npred_val_other = m5_lgb.predict_proba(val_set_other)[:,1]","6921e2df":"f1_score(X_val['target'],np.where(pred_val_other>=0.5,1,0))","f35ddb7f":"\nsubmission_m5 = test[['id']].copy()\nsubmission_m5['target'] =  np.where(pred_test_other>= 0.5,1,0)","1f16c666":"submission_m5.to_csv('.\/submission_m5.csv',index=False)","a32b40a4":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n","892bfc7c":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","6ba4f8a0":"corpus=create_corpus(df)","2267d810":"corpus[:5]","a0b8976d":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","4eb303d9":"embedding_dict","a23b6a94":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","5729f2f4":"#file we train on:\ntweet_pad","b5bf5663":"\n\nword_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\n\n","2d057c2a":"word_index","ba09aaae":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","f23b3189":"train_glove=tweet_pad[:train.shape[0]]\ntest_glove=tweet_pad[train.shape[0]:]","cda6c6e8":"X_train_glove,X_val_glove,y_train_glove,y_val_glove=train_test_split(train_glove,train['target'].values,test_size=0.20)\nprint('Shape of train',X_train_glove.shape)\nprint(\"Shape of Validation \",X_val_glove.shape)","f25f140f":"\n\n#Run the model\nm6_lgb = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1, \n                            learning_rate=0.05, \n                            max_depth=20, \n                            num_leaves=50, \n                            n_estimators=1000, \n                            max_bin=200) #params,  verbose_eval = 50)","3530531d":"m6_lgb.fit(X_train_glove, y_train_glove) ","0d044588":"pred_test_glove = m6_lgb.predict_proba(test_glove)[:,1]\npred_val_glove = m6_lgb.predict_proba(X_val_glove)[:,1]","db02d3a9":"f1_score(X_val['target'],np.where(pred_val_glove>=0.5,1,0))","9804281f":"\nsubmission_m6 = test[['id']].copy()\nsubmission_m6['target'] =  np.where(pred_test_glove>= 0.5,1,0)","f7012f01":"\nsubmission_m6.to_csv('.\/submission_m6.csv',index=False)","461b8b3c":"#settings that you use for count vectorizer will go here\ntfidf_vectorizer_binary=TfidfVectorizer(use_idf=True,\n                                ngram_range=(1,5),\n                                token_pattern='[a-zA-Z]{2,20}',#only alpha data >= 2 in length\n                                #stop_words='english',\n                                min_df=3,\n                                max_df=0.70,\n                                max_features=5000,\n                                binary=True # try a separate one with this set to true\n                                )\n\n#fit transform\ntfidf_vectorizer_vectors_binary=tfidf_vectorizer_binary.fit_transform(df['text'])","1802ca92":"train_set_binary = tfidf_vectorizer_binary.transform(X_train['text'])\nval_set_binary = tfidf_vectorizer_binary.transform(X_val['text'])\ntest_set_binary = tfidf_vectorizer_binary.transform(test['text'])","dfa46a3f":"#Run the model\nm7_lgb_binary = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1, \n                            learning_rate=0.05, \n                            max_depth=20, \n                            num_leaves=50, \n                            n_estimators=1000, \n                            max_bin=200) #params,  verbose_eval = 50)\n","11f9f75e":"m7_lgb_binary.fit(train_set_binary, y_train) ","811735d0":"#plot feature importance\nfeature_imp_stem = pd.DataFrame({'Value':m7_lgb_binary.feature_importances_,'Feature':tfidf_vectorizer_binary.get_feature_names()})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_stem.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","c1ef5fb3":"pred_test_binary = m7_lgb_binary.predict_proba(test_set_binary)[:,1]\npred_val_binary = m7_lgb_binary.predict_proba(val_set_binary)[:,1]","b9b5c4a0":"f1_score(X_val['target'],np.where(pred_val_binary>=0.5,1,0))","0182ae7b":"\nsubmission_m7 = test[['id']].copy()\nsubmission_m7['target'] =  np.where(pred_test_binary>= 0.5,1,0)","55cd2355":"submission_m7.to_csv('.\/submission_m7.csv',index=False)","a66fb98f":"texts = []\nfor tweet in X_train['text']: \n    texts.append(tweet.split())","30406f2f":"texts1 = []\nfor tweet in texts:\n    texts1.append([x for x in tweet if x not in stopwords.words()])","8e391f27":"texts1[:2]","d56ff66c":"from gensim.corpora import Dictionary\n# you can use any corpus, this is just illustratory\n\ndictionary = Dictionary(texts1)\ncorpus = [dictionary.doc2bow(text) for text in texts1]\n\nimport numpy\nnumpy.random.seed(1) # setting random seed to get the same results each time.\n\nfrom gensim.models import ldamodel\nmodel = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=20, minimum_probability=1e-8)\nmodel.show_topics()","2e4ef979":"# we can now get the LDA topic distributions for these\n# get the disaster text words\nbow_keyword_unique = model.id2word.doc2bow(keyword_unique)\nlda_bow_disaster = model[bow_keyword_unique]","f0949d6b":"X_train.head()","986171a9":"from gensim.matutils import hellinger","b0e054bc":"X_train_lda = []\nfor tweet in texts1:\n    bow_tweet = model.id2word.doc2bow(tweet) \n    lda_bow_tweet = model[bow_tweet]\n    X_train_lda.append(hellinger(lda_bow_disaster, lda_bow_tweet))\n","71b029f2":"\nX_train_lda","958f2157":"#Run the model\nm8_lgb_lda = lgb.LGBMClassifier(objective='binary', \n                            metric='F1',\n                            verbose=-1, \n                            learning_rate=0.05, \n                            max_depth=20, \n                            num_leaves=50, \n                            n_estimators=1000, \n                            max_bin=200) #params,  verbose_eval = 50)\n\n","142da285":"m8_lgb_lda.fit(pd.DataFrame(X_train_lda,columns=['lda']), y_train)","91e50162":"texts_val = []\nfor tweet in X_val['text']: \n    texts_val.append(tweet.split())\n\n\ntexts_val1 = []\nfor tweet in texts_val:\n    texts_val1.append([x for x in tweet if x not in stopwords.words()])\n\nX_val_lda = []\nfor tweet in texts_val1:\n    bow_tweet = model.id2word.doc2bow(tweet) \n    lda_bow_tweet = model[bow_tweet]\n    X_val_lda.append(hellinger(lda_bow_disaster, lda_bow_tweet))","327d7912":"texts_test = []\nfor tweet in test['text']: \n    texts_test.append(tweet.split())\n\n\ntexts_test1 = []\nfor tweet in texts_test:\n    texts_test1.append([x for x in tweet if x not in stopwords.words()])\n\nX_test_lda = []\nfor tweet in texts_test1:\n    bow_tweet = model.id2word.doc2bow(tweet) \n    lda_bow_tweet = model[bow_tweet]\n    X_test_lda.append(hellinger(lda_bow_disaster, lda_bow_tweet))","756c6628":"pred_test_lda = m8_lgb_lda.predict_proba(pd.DataFrame(X_test_lda,columns=['lda']))[:,1]\npred_val_lda = m8_lgb_lda.predict_proba(pd.DataFrame(X_val_lda,columns=['lda']))[:,1]","f26f64f0":"f1_score(X_val['target'],np.where(pred_val_lda>=0.5,1,0))","6492f296":"submission_ens1 = test[['id']].copy()","7bb52a6c":"submission_ens1['target'] =  (pred_test * 0.225 + \n                              pred_test_hashtags * 0.0 + \n                              pred_test_stem * 0.225 + \n                              pred_test_other * 0.15 + \n                              pred_test_glove * 0.09 + \n                              pred_test_binary * 0.3 +\n                              pred_test_lda * 0.01) ","d3fc728a":"submission_ens1['target'] =  np.where(submission_ens1['target'] >= 0.5,1,0)","774584dc":"submission_ens1.to_csv('.\/submission_ens1.csv',index=False)","da715c0e":"submission_ens1.head()","5893fec6":"### Real or Not Disaster Tweets","888e0e73":"Extract the tokens from the text column, use term frequency - inverse document frequency (tfidf) to add more weight to rare terms and reduce weight on common terms across both the individual tweet and across the whole corpus.","bef4c298":"### 4. Train Basic TFIDF Model","1e8c6f38":"https:\/\/www.kaggle.comreal-or-not-disaster-tweets","cd4de342":"### 11. Ensemble Models","23454175":"### 9. Binary Token Model","ee4f6a70":"#### Weighted Ensemble","c339df70":"#Extract the hashtags as a separate column of data","b60c36a0":"### 2.3 Other Feature Engineering","aa38d71a":"#### 4.3 Feature Importance","88a63a0e":"#### 4.2 Train model","a6b5b6c4":"#### 4.6 Submission","d892c651":"#### 5.1 Train tfidf on hashtag data","bc22b1bc":"### 1. Load Data","23a9223f":"#### 4.4 Predictions","73ce9bdb":"### Settings","8909848f":"### 2.2 Hashtags","c8733a48":"#### 4.5 Score","3c2352ea":"### 2. Generate Text Features","105087f9":"### 3. Train Validation Split","7706dfbb":"### 7. Other Data Model","55529fb0":"### 8. Glove Model","3b271dde":"### 6. Stemming Model","ea59ff7f":"### 10. LDA Model","ef23d9b8":"#### 4.1 Apply tfidf to datasets","515f5d11":"### 2.1 TF-IDF matrix","95b708c3":"### 5. Hashtags Model"}}