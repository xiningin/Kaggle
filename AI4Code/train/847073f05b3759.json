{"cell_type":{"8c142176":"code","bc5d1343":"code","a82ab758":"code","3a6634b2":"code","0909f968":"code","d1fc617d":"code","5fcf0f4a":"code","a2462674":"code","7e53c289":"code","b5c4fb88":"code","89a51fd1":"code","4b66f408":"code","c6901630":"code","3527060f":"code","70323a1e":"code","1b99465c":"code","eab23136":"code","87aadd91":"code","eb0de295":"code","142adbdb":"code","3c12fd96":"code","7debfb2d":"code","94551ea1":"code","2bbebbbb":"code","fb5468cf":"code","7e0a8f54":"code","5df83712":"code","e2b8fca2":"code","e184eeb0":"code","2cd26f1c":"code","0e5c744e":"code","27f0f121":"code","188aca48":"code","36f8e46e":"code","e63c140d":"code","9253ae3b":"code","aa6f9971":"code","6abc6b3f":"code","37c4edc0":"code","0c6f44a5":"code","037cd810":"code","0f9cb014":"code","be5111cd":"code","70795447":"code","dd4a7c54":"code","e89f3f7f":"code","20476e35":"markdown","76eaf250":"markdown","4f2b05ce":"markdown","d1f7cde3":"markdown","116a9d9f":"markdown","c9f15f87":"markdown","feeeaab3":"markdown","2bd7f583":"markdown","5edc46bd":"markdown","6addb526":"markdown","ad598920":"markdown","8e9b4cfd":"markdown","9d302a80":"markdown","c4175c2f":"markdown","85b66ba1":"markdown","58fbe3b6":"markdown","fc283463":"markdown","e8d952dd":"markdown","8bcda3c1":"markdown"},"source":{"8c142176":"from fastai.core import *\nimport transformers; transformers.__version__","bc5d1343":"KAGGLE_WORKING = Path(\"\/kaggle\/working\")","a82ab758":"path = Path(\"..\/input\/tweet-sentiment-extraction\/\")\ntrain_df = pd.read_csv(path\/'train.csv')\ntest_df = pd.read_csv(path\/'test.csv')\ntrain_df = train_df.dropna().reset_index(drop=True)","3a6634b2":"# max approx sequence length\n(max(len(o.split()) for o in array(train_df['text'])), \nmax(len(o.split()) for o in array(test_df['text'])))","0909f968":"train_df.sentiment.value_counts()","d1fc617d":"# an example for SQUAD json data format\nsquad_sample = {\n    \"version\": \"v2.0\",\n    \"data\": [\n        {\n            \"title\": \"Beyonc\\u00e9\",\n            \"paragraphs\": [\n                {\n                    \"qas\": [\n                        {\n                            \"question\": \"When did Beyonce start becoming popular?\",\n                            \"id\": \"56be85543aeaaa14008c9063\",\n                            \"answers\": [\n                                {\n                                    \"text\": \"in the late 1990s\",\n                                    \"answer_start\": 269\n                                }\n                            ],\n                            \"is_impossible\": False\n                        }\n                    ],\n                    \"context\": \"Beyonc\\u00e9 Giselle Knowles-Carter (\/bi\\u02d0\\u02c8j\\u0252nse\\u026a\/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \\\"Crazy in Love\\\" and \\\"Baby Boy\\\".\"\n                }\n            ]\n        }\n    ]\n}","5fcf0f4a":"def get_answer_start(context, answer):\n    len_a = len(answer)\n    for i, _ in enumerate(context):\n        if context[i:i+len_a] == answer: return i\n    raise Exception(\"No overlapping segment found\")","a2462674":"def generate_qas_dict(text_id, context, answer, question):\n    qas_dict = {}\n    qas_dict['question'] = question\n    qas_dict['id'] = text_id\n    qas_dict['is_impossible'] = False\n    \n    if answer is None: \n        qas_dict['answers'] = []\n    else: \n        answer_start = get_answer_start(context, answer)\n        qas_dict['answers'] = [{\"text\":answer, \"answer_start\":answer_start}]\n    return qas_dict","7e53c289":"def create_squad_from_df(df):\n    data_dicts = []\n    for _, row in df.iterrows():\n        text_id = row['textID']\n        context = row['text']\n        answer =  row['selected_text'] if 'selected_text' in row else None\n        question = row['sentiment']\n\n        qas_dict = generate_qas_dict(text_id, context, answer, question)\n        data_dict = {\"paragraphs\" : [{\"qas\" : [qas_dict], \"context\":context}]}\n        data_dict['title'] = text_id\n        data_dicts.append(data_dict)\n\n    return {\"version\": \"v2.0\", \"data\": data_dicts}","b5c4fb88":"# train_no_neutral_df = train_df[train_df.sentiment != 'neutral'].reset_index(drop=True)\n# test_no_neutral_df = test_df[test_df.sentiment != 'neutral'].reset_index(drop=True)\n# train_no_neutral_df.shape, test_no_neutral_df.shape","89a51fd1":"from sklearn.model_selection import KFold","4b66f408":"# create 5 fold trn-val splits with only positive\/negative tweets\nos.makedirs(\"squad_data\", exist_ok=True)\nkfold = KFold(5, shuffle=True, random_state=42)\nfold_idxs = list(kfold.split(train_df))\n\nfor i, (trn_idx, val_idx) in enumerate(fold_idxs):\n    _trn_fold_df = train_df.iloc[trn_idx]\n    _val_fold_df = train_df.iloc[val_idx]\n    train_squad_data = create_squad_from_df(_trn_fold_df)\n    valid_squad_data = create_squad_from_df(_val_fold_df)\n    with open(f\"squad_data\/train_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(train_squad_data))\n    with open(f\"squad_data\/valid_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(valid_squad_data))","c6901630":"# create for test \ntest_squad_data =  create_squad_from_df(test_df)\nwith open(\"squad_data\/test_squad_data.json\", \"w\") as f: f.write(json.dumps(test_squad_data))","3527060f":"_train_dict = json.loads(open(KAGGLE_WORKING\/'squad_data\/train_squad_data_0.json').read())","70323a1e":"sample_idx = np.random.choice(range(len(_train_dict['data'])))\n_train_dict['data'][sample_idx]","1b99465c":"textid = _train_dict['data'][sample_idx]['paragraphs'][0]['qas'][0]['id']","eab23136":"train_df[train_df.textID == textid]","87aadd91":"from fastai.text import *\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForQuestionAnswering\nfrom transformers.data.processors.squad import (SquadResult, SquadV1Processor, SquadV2Processor,\n                                                SquadExample, squad_convert_examples_to_features)","eb0de295":"PRETRAINED_TYPE = 'roberta-base'\n# PRETRAINED_TYPE = 'distilbert-base-uncased'","142adbdb":"processor = SquadV2Processor()\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TYPE, do_lower_case=True)\nos.makedirs(KAGGLE_WORKING\/f'{PRETRAINED_TYPE}-tokenizer', exist_ok=True)\ntokenizer.save_pretrained(KAGGLE_WORKING\/f'{PRETRAINED_TYPE}-tokenizer')","3c12fd96":"max_seq_length = 128\nmax_query_length = 10\n\ndef get_dataset(examples, is_training):\n    return squad_convert_examples_to_features(\n        examples=examples,\n        tokenizer=tokenizer,\n        doc_stride=200,\n        max_seq_length=max_seq_length,\n        max_query_length=10,\n        is_training=is_training,\n        return_dataset=\"pt\",\n        threads=defaults.cpus,\n    )","7debfb2d":"class SQUAD_Dataset(Dataset):\n    def __init__(self, dataset_tensors, examples, features, is_training=True):\n        self.dataset_tensors = dataset_tensors\n        self.examples = examples\n        self.features = features\n        self.is_training = is_training\n        \n        \n    def __getitem__(self, idx):\n        'fastai requires (xb, yb) to return'\n        'AutoModel handles loss computation in forward hence yb will be None'\n        input_ids = self.dataset_tensors[0][idx]\n        attention_mask = self.dataset_tensors[1][idx]\n        token_type_ids = self.dataset_tensors[2][idx]\n        xb = (input_ids, attention_mask, token_type_ids)\n        if self.is_training: \n            start_positions = self.dataset_tensors[3][idx]\n            end_positions = self.dataset_tensors[4][idx]\n        yb = [start_positions, end_positions]\n        return xb, yb\n    \n    def __len__(self): return len(self.dataset_tensors[0])","94551ea1":"def get_fold_ds(foldnum):\n    data_dir = \"\/kaggle\/working\/squad_data\"\n    train_filename = f\"train_squad_data_{foldnum}.json\"\n    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n    test_filename = \"test_squad_data.json\"\n    \n    # tokenize\n    train_examples = processor.get_train_examples(data_dir, train_filename)\n    valid_examples = processor.get_train_examples(data_dir, valid_filename)\n    test_examples = processor.get_dev_examples(data_dir, test_filename)\n\n    # create tensor dataset\n    train_features, train_dataset = get_dataset(train_examples, True)\n    valid_features, valid_dataset = get_dataset(valid_examples, True)\n    test_features, test_dataset = get_dataset(test_examples, False)\n    \n    # create pytorch dataset\n    train_ds = SQUAD_Dataset(train_dataset.tensors, train_examples, train_features)\n    valid_ds = SQUAD_Dataset(valid_dataset.tensors, valid_examples, valid_features)\n    test_ds = SQUAD_Dataset(test_dataset.tensors, test_examples, test_features, False)\n    \n    return train_ds, valid_ds, test_ds    ","2bbebbbb":"#export\nclass TSEDataAugmentor():\n\n    def __init__(self, tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map): \n\n        self.tokenizer = tokenizer \n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        \n        # initial answer start and end positions\n        self.ans_start_pos, self.ans_end_pos = start_position.item(), end_position.item()\n                \n        # initial context start and end positions\n        self.token_to_orig_map = token_to_orig_map\n        self.context_start_pos, self.context_end_pos = min(token_to_orig_map), max(token_to_orig_map)\n\n        \n    \n    # left and right indexes excluding answer tokens and eos token\n    @property\n    def left_idxs(self): return np.arange(self.context_start_pos, self.ans_start_pos)\n    \n    @property\n    def right_idxs(self): return np.arange(self.ans_end_pos+1, self.context_end_pos+1)\n    \n    @property\n    def left_right_idxs(self): return np.concatenate([self.left_idxs, self.right_idxs])\n    \n    @property\n    def rand_left_idx(self): return np.random.choice(self.left_idxs) if self.left_idxs.size > 0 else None\n    \n    @property\n    def rand_right_idx(self): return np.random.choice(self.right_idxs) if self.right_idxs.size > 0 else None\n        \n    \n    \n    def right_truncate(self, right_idx):\n        \"\"\"\n        Truncate context from random right index to beginning, answer pos doesn't change\n        Note: token_type_ids NotImplemented\n        \"\"\"\n        if not right_idx: raise Exception(\"Right index can't be None\")\n        \n        # clone for debugging\n        new_input_ids = self.input_ids.clone()\n        nopad_input_ids = new_input_ids[self.attention_mask.bool()]\n        \n        # truncate from right idx to beginning - add eos_token_id to end\n        truncated = torch.cat([nopad_input_ids[:right_idx+1], tensor([self.tokenizer.eos_token_id])])\n        \n        # pad new context until size are equal\n        # replace original input context with new\n        n_pad = len(nopad_input_ids) - len(truncated)\n        new_context = F.pad(truncated, (0,n_pad), value=self.tokenizer.pad_token_id)\n        new_input_ids[:self.context_end_pos+2] = new_context\n        \n        \n        # find new attention mask, update new context end position (exclude eos token)\n        # Note: context start doesn't change since we don't manipulate question\n        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1 \n        self.context_end_pos = new_context_end_pos\n        \n        # update input_ids and attention_masks\n        self.input_ids = new_input_ids\n        self.attention_mask = new_attention_mask\n        \n        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n\n    def random_right_truncate(self):\n        right_idx = self.rand_right_idx\n        if right_idx: self.right_truncate(right_idx)\n    \n    \n    def left_truncate(self, left_idx):\n        \"\"\"\n        Truncate context from random left index to end, answer pos changes too\n        Note: token_type_ids NotImplemented\n        \"\"\"\n        \n        if not left_idx: raise Exception(\"Left index can't be None\")\n        \n        # clone for debugging\n        new_input_ids = self.input_ids.clone()\n        \n        # pad new context until size are equal\n        # replace original input context with new\n\n        n_pad = len(new_input_ids[self.context_start_pos:]) - len(new_input_ids[left_idx:])\n        \n        new_context = F.pad(new_input_ids[left_idx:], (0,n_pad), value=self.tokenizer.pad_token_id)\n        \n        new_input_ids[self.context_start_pos:] = new_context\n        \n                \n        # find new attention mask, update new context end position (exclude eos token)\n        # Note: context start doesn't change since we don't manipulate question\n        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1\n        self.context_end_pos = new_context_end_pos\n        \n        # find new answer start and end positions\n        # update new answer start and end positions\n        ans_shift = left_idx - self.context_start_pos\n        self.ans_start_pos, self.ans_end_pos = self.ans_start_pos-ans_shift, self.ans_end_pos-ans_shift\n        \n        \n        # update input_ids and attention_masks\n        self.input_ids = new_input_ids\n        self.attention_mask = new_attention_mask\n        \n        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n        \n    def random_left_truncate(self):\n        left_idx = self.rand_left_idx\n        if left_idx: self.left_truncate(left_idx)\n        \n        \n    def replace_with_mask(self, idxs_to_mask):\n        \"\"\"\n        Replace given input ids with tokenizer.mask_token_id\n        \"\"\"\n        # clone for debugging\n        new_input_ids = self.input_ids.clone()\n        new_input_ids[idxs_to_mask] = tensor([tokenizer.mask_token_id]*len(idxs_to_mask))\n        self.input_ids = new_input_ids\n\n        \n    def random_replace_with_mask(self, mask_p=0.2):\n        \"\"\"\n        mask_p: Proportion of tokens to replace with mask token id\n        \"\"\"\n        idxs_to_mask = np.random.choice(self.left_right_idxs, int(len(self.left_right_idxs)*mask_p))\n        if idxs_to_mask.size > 0: self.replace_with_mask(idxs_to_mask)\n        \n                ","fb5468cf":"do_tfms = {}\ndo_tfms[\"random_left_truncate\"] = {\"p\":0.3}\ndo_tfms[\"random_right_truncate\"] = {\"p\":0.3}\ndo_tfms[\"random_replace_with_mask\"] = {\"p\":0.3, \"mask_p\":0.2}\ndo_tfms","7e0a8f54":"#export\nclass SQUAD_Dataset(Dataset):\n    def __init__(self, tokenizer, dataset_tensors, examples, features, is_training=True, do_tfms=None):\n        self.dataset_tensors = dataset_tensors\n        self.examples = examples\n        self.features = features\n        self.is_training = is_training\n        self.tokenizer = tokenizer\n        self.do_tfms = do_tfms\n                \n        \n    def __getitem__(self, idx):\n        'fastai requires (xb, yb) to return'\n        \n        input_ids = self.dataset_tensors[0][idx]\n        attention_mask = self.dataset_tensors[1][idx]\n        token_type_ids = self.dataset_tensors[2][idx]\n        if self.is_training: \n            start_position = self.dataset_tensors[3][idx]\n            end_position = self.dataset_tensors[4][idx]\n            \n            if self.do_tfms:\n                token_to_orig_map = self.features[idx].token_to_orig_map\n                \n                augmentor = TSEDataAugmentor(self.tokenizer,\n                                             input_ids,\n                                             attention_mask,\n                                             start_position, end_position,\n                                             token_to_orig_map)\n\n                if np.random.uniform() < self.do_tfms[\"random_left_truncate\"][\"p\"]:\n                    augmentor.random_left_truncate()\n                if np.random.uniform() < self.do_tfms[\"random_right_truncate\"][\"p\"]:\n                    augmentor.random_right_truncate()\n                if np.random.uniform() < self.do_tfms[\"random_replace_with_mask\"][\"p\"]:\n                    augmentor.random_replace_with_mask(self.do_tfms[\"random_replace_with_mask\"][\"mask_p\"])\n\n                input_ids = augmentor.input_ids\n                attention_mask = augmentor.attention_mask\n                start_position, end_position = tensor(augmentor.ans_start_pos), tensor(augmentor.ans_end_pos)\n                \n            \n        xb = (input_ids, attention_mask, token_type_ids)\n        if self.is_training: yb = (start_position, end_position)\n        else: yb = 0\n        \n        return xb, yb\n    \n    def __len__(self): return len(self.dataset_tensors[0])","5df83712":"#export\ndef get_fold_ds(foldnum, tokenizer=tokenizer):\n    data_dir = \"\/kaggle\/working\/squad_data\"\n    train_filename = f\"train_squad_data_{foldnum}.json\"\n    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n    test_filename = \"test_squad_data.json\"\n    \n    # tokenize\n    train_examples = processor.get_train_examples(data_dir, train_filename)\n    valid_examples = processor.get_train_examples(data_dir, valid_filename)\n    test_examples = processor.get_dev_examples(data_dir, test_filename)\n\n    # features and tensors\n    train_features, train_dataset = get_dataset(train_examples, True)\n    valid_features, valid_dataset = get_dataset(valid_examples, True)\n    test_features, test_dataset = get_dataset(test_examples, False)\n    train_dataset_tensors = train_dataset.tensors\n    valid_dataset_tensors = valid_dataset.tensors\n    test_dataset_tensors = test_dataset.tensors\n    \n    # create pytorch dataset\n    do_tfms = {}\n    do_tfms[\"random_left_truncate\"] = {\"p\":0.3}\n    do_tfms[\"random_right_truncate\"] = {\"p\":0.3}\n    do_tfms[\"random_replace_with_mask\"] = {\"p\":0.3, \"mask_p\":0.3}\n\n    train_ds = SQUAD_Dataset(tokenizer, train_dataset_tensors, train_examples, train_features, True, do_tfms)\n    valid_ds = SQUAD_Dataset(tokenizer, valid_dataset_tensors, valid_examples, valid_features, True)\n    test_ds = SQUAD_Dataset(tokenizer, test_dataset_tensors, test_examples, test_features, False)\n    \n    return train_ds, valid_ds, test_ds    ","e2b8fca2":"from transformers import AutoModelForPreTraining, RobertaModel, BertModel","e184eeb0":"# MODEL_TYPE = 'distilbert'\nMODEL_TYPE = 'roberta'","2cd26f1c":"# train_ds, valid_ds, test_ds  = get_fold_ds(0)\n\n# model = AutoModel.from_pretrained(PRETRAINED_TYPE)\n\n# data = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs=32)\n\n# xb,yb = data.one_batch()","0e5c744e":"class QAHead(Module): \n    def __init__(self, p=0.5):    \n        self.d0 = nn.Dropout(p)\n        self.l0 = nn.Linear(768, 2)\n#         self.d1 = nn.Dropout(p)\n#         self.l1 = nn.Linear(256, 2)        \n    def forward(self, x):\n        return self.l0(self.d0(x))\n    \nclass TSEModel(Module):\n    def __init__(self, model): \n        self.sequence_model = model\n        self.head = QAHead()\n        \n    def forward(self, *xargs):\n        inp = {}\n        inp[\"input_ids\"] = xargs[0]\n        inp[\"attention_mask\"] = xargs[1]\n        inp[\"token_type_ids\"] = xargs[2]\n        if MODEL_TYPE in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]: del inp[\"token_type_ids\"]\n    \n        sequence_output, _ = self.sequence_model(**inp)\n        start_logits, end_logits = self.head(sequence_output).split(1, dim=-1)\n        return (start_logits.squeeze(-1), end_logits.squeeze(-1))","27f0f121":"class CELoss(Module):\n    \"single backward by concatenating both start and logits with correct targets\"\n    def __init__(self): self.loss_fn = nn.CrossEntropyLoss()\n    def forward(self, inputs, start_targets, end_targets):\n        start_logits, end_logits = inputs\n        \n        logits = torch.cat([start_logits, end_logits]).contiguous()\n        \n        targets = torch.cat([start_targets, end_targets]).contiguous()\n        \n        return self.loss_fn(logits, targets)","188aca48":"# tse_model = TSEModel(model)\n\n# out = tse_model(*xb)\n\n# loss_func = CELoss()\n\n# loss = loss_func(out, *yb); loss","36f8e46e":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","e63c140d":"def get_best_start_end_idxs(_start_logits, _end_logits):\n    best_logit = -1000\n    best_idxs = None\n    for start_idx, start_logit in enumerate(_start_logits):\n        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n            logit_sum = (start_logit + end_logit).item()\n            if logit_sum > best_logit:\n                best_logit = logit_sum\n                best_idxs = (start_idx, start_idx+end_idx)\n    return best_idxs","9253ae3b":"def get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map):\n    \n    start_offset_id = token_to_orig_map[start_idx] \n    end_offset_id = token_to_orig_map[end_idx]\n    \n    \n    return \"\".join([ct for ct, char_offs in zip(context_text, char_to_word_offset) if \n                                     (char_offs >= start_offset_id) & (char_offs <= end_offset_id)])","aa6f9971":"def model_split_func(m): \n    n = len(m.sequence_model.encoder.layer) - 5\n    return (m.sequence_model.embeddings, m.sequence_model.encoder.layer[n], m.head)","6abc6b3f":"def model_split_func(m): \n    \"4 layer groups\"\n    n = len(m.sequence_model.encoder.layer)\/\/2\n    return (m.sequence_model.embeddings, m.sequence_model.encoder.layer[:n], m.sequence_model.encoder.layer[n:], m.head)","37c4edc0":"class JaccardScore(Callback):\n    \"Stores predictions and targets to perform calculations on epoch end.\"\n    def __init__(self, valid_ds): \n        self.valid_ds = valid_ds\n        self.token_to_orig_map = [o.token_to_orig_map for o in valid_ds.features]\n        self.context_text = [o.context_text for o in valid_ds.examples]\n        self.answer_text = [o.answer_text for o in valid_ds.examples]\n        self.char_to_word_offset = [o.char_to_word_offset for o in valid_ds.examples]\n\n        self.offset_shift = min(self.token_to_orig_map[0].keys())\n        \n        \n    def on_epoch_begin(self, **kwargs):\n        self.jaccard_scores = []  \n        self.valid_ds_idx = 0\n        \n        \n    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n        \n#         import pdb;pdb.set_trace()\n        \n        input_ids = last_input[0]\n        attention_masks = last_input[1].bool()\n        token_type_ids = last_input[2].bool()\n\n        start_logits, end_logits = last_output\n        \n        # mask select only context part\n        for i in range(len(input_ids)):\n            \n            if MODEL_TYPE == \"roberta\": \n                \n                _input_ids = input_ids[i].masked_select(attention_masks[i])\n                _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n                start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n\n#             else:\n#                 _input_ids = input_ids[i].masked_select(token_type_ids[i])\n#                 _start_logits = start_logits[i].masked_select(token_type_ids[i])\n#                 _end_logits = end_logits[i].masked_select(token_type_ids[i])\n#                 _offset_shift = sum(~token_type_ids[i][attention_masks[i]])\n#                 start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n#                 start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n            \n            context_text = self.context_text[self.valid_ds_idx]\n            char_to_word_offset = self.char_to_word_offset[self.valid_ds_idx]\n            token_to_orig_map = self.token_to_orig_map[self.valid_ds_idx]\n            \n            _answer =  get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map)\n            _answer_text = self.answer_text[self.valid_ds_idx]\n            \n            score = jaccard(_answer, _answer_text)\n            self.jaccard_scores.append(score)\n\n            self.valid_ds_idx += 1\n            \n    def on_epoch_end(self, last_metrics, **kwargs):        \n        res = np.mean(self.jaccard_scores)\n        return add_metrics(last_metrics, res)","0c6f44a5":"from fastai.callbacks import *\n\ndef new_on_train_begin(self, **kwargs:Any)->None:\n    \"Initializes the best value.\"\n    if not hasattr(self, 'best'):\n        self.best = float('inf') if self.operator == np.less else -float('inf')\n\nSaveModelCallback.on_train_begin = new_on_train_begin","037cd810":"model = AutoModel.from_pretrained(PRETRAINED_TYPE)\nos.makedirs(f\"{PRETRAINED_TYPE}-config\", exist_ok=True)\nmodel.config.save_pretrained(f\"{PRETRAINED_TYPE}-config\")\ndel model; gc.collect()","0f9cb014":"def run_fold(foldnum):\n    n_epochs = 5\n    wd = 0.002 # true weight decay\n    \n    # DATA\n    train_ds, valid_ds, test_ds = get_fold_ds(foldnum, tokenizer)\n    data = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs=64)\n    \n    # LEARNER\n    model = AutoModel.from_pretrained(PRETRAINED_TYPE)\n    tse_model = TSEModel(model)\n    learner = Learner(data, tse_model, loss_func=CELoss(), metrics=[JaccardScore(valid_ds)], model_dir=f\"models_fold_{foldnum}\")\n    learner.split(model_split_func)\n    learner.to_fp16() \n    \n    # CALLBACKS\n    early_stop_cb = EarlyStoppingCallback(learner, monitor='jaccard_score',mode='max',patience=2)\n    save_model_cb = SaveModelCallback(learner,every='improvement',monitor='jaccard_score',name=f'{MODEL_TYPE}-qa-finetune')\n    csv_logger_cb = CSVLogger(learner, f\"training_logs_{foldnum}\", True)\n\n\n    ### Train\n    # We can find the maximun learning rate to start training using `lr_finder()`. `1e-2` seems like a good choice from the plot.\n\n    lr = 1e-2\n    \n    # Last Param Group\n    learner.freeze_to(3);\n    learner.fit_one_cycle(1, lr, pct_start=0.4, div_factor=50,\n                          wd=wd, callbacks=[early_stop_cb, save_model_cb, csv_logger_cb])\n\n    # Last 2 Param Groups\n    learner.freeze_to(2)\n    learner.fit_one_cycle(n_epochs, slice(lr\/100, lr\/10), pct_start=0.4, div_factor=50, \n                          wd=wd, callbacks=[early_stop_cb, save_model_cb, csv_logger_cb])\n\n    # All Param Groups\n#     data = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs=64) # decrease bs to fit GPU MEM\n#     learner.data = data\n#     learner.to_fp16()\n    learner.freeze_to(1) # exclude embeddings layer\n    learner.fit_one_cycle(n_epochs, slice(lr\/1000, lr\/100), pct_start=0.4, div_factor=50, \n                          wd=wd, callbacks=[early_stop_cb, save_model_cb, csv_logger_cb])\n    \n    # don't save opt state\n    learner.save(f'{MODEL_TYPE}-qa-finetune', with_opt=False)\n    del learner; gc.collect()","be5111cd":"for foldnum in range(5): run_fold(foldnum)","70795447":"# learner.load('bert-large-cased-qa-step1')\n# learner.model.eval();","dd4a7c54":"# from tqdm import tqdm\n# test_answers = []\n# with torch.no_grad():\n#     for xb,yb in tqdm(learner.data.test_dl):\n#         output = learner.model(*xb)\n#         input_ids = xb[0]\n#         attention_masks = xb[1].bool()\n#         token_type_ids = xb[2].bool()\n\n#         start_logits, end_logits = output\n\n#         batch_answers = []\n#         # mask select only context part\n#         for i in range(len(input_ids)):\n#             _input_ids = input_ids[i].masked_select(token_type_ids[i])\n#             _start_logits = start_logits[i].masked_select(token_type_ids[i])\n#             _end_logits = end_logits[i].masked_select(token_type_ids[i])\n#             best_idxs = get_best_start_end_idxs(_start_logits, _end_logits)\n#             _answer = tokenizer.decode(_input_ids[best_idxs[0]:best_idxs[1]+1])\n#             batch_answers.append(_answer)\n            \n#         test_answers += batch_answers","e89f3f7f":"# test_df['selected_text'] = test_answers\n\n# # keep neutral as it is\n# test_df['selected_text'] = test_df.apply(lambda o: o['text'] if o['sentiment'] == 'neutral' else o['selected_text'], 1)\n\n# subdf = test_df[['textID', 'selected_text']]\n\n# subdf.head()\n\n# ## this shouldn't be necessary since evaluation code in Kaggle is fixed\n# # def f(selected): return \" \".join(set(selected.lower().split()))\n# # subdf.selected_text = subdf.selected_text.map(f)\n\n# subdf.to_csv(\"submission.csv\", index=False)","20476e35":"### Dataset","76eaf250":"### Check SQUAD json","4f2b05ce":"### Run 5 folds","d1f7cde3":"TODO: \n\n- No wd to LayerNorm and Biases. \n- Explore better optimizations methods for better jaccard score.","116a9d9f":"For each epoch we will calculate `JaccardScore` on validation set","c9f15f87":"### Submit","feeeaab3":"## Huggingface meets Fastai \n\n![](https:\/\/huggingface.co\/landing\/assets\/transformers-docs\/huggingface_logo.svg)\n![](https:\/\/docs.fast.ai\/images\/company_logo.png)","2bd7f583":"### fin","5edc46bd":"### SQUAD Q\/A Data Prep\n\nHere we are creating a SQUAD format dataset as we will leverage data prep utilities from `transformers` library. We could use SQUAD V1 or V2 for preparing data, but this dataset doesn't require `is_impossible` as it doesn't have any adversarial questions. Questions are coming from sentiment of the tweets; being either `positive` or `negative`. The idea has been taken from other kernels, so thanks!","6addb526":"### Create KFold Validation\n\nFor training we will be using only positive and negative tweets, as neutral tweets have a score of `~0.97` when submitted as it is. Also, positive and negative tweets are balanced so I am here using a vanilla KFold cross validation approach. Trained models for all folds can be used for further ensembling if needed.","ad598920":"### Predict","8e9b4cfd":"### Data\n\nSince we are training a BERT model we will use bert tokenizer for `bert-base-cased`. You can use `Auto*` classes from `transformers` library to load and train with any model available. For demonstration I am training with `foldnum=0`.","9d302a80":"We will choose start and end indexes for predictions such that sum of logits is maximum while satisfying `start_idx <= end_idx`","c4175c2f":"### DataAugmentor & Dataset","85b66ba1":"### Model\n\nHere we have `ModelWrapper` to make model from `transformers` to work with `fastai` 's `Learner` class. Also, loss is computed within the model, for this we will use a `DummyLoss` to work with `Learner.fit()`","58fbe3b6":"Here we define parameter group split points for `gradual unfreezing`. Idea is coming from [ULMFIT paper](https:\/\/arxiv.org\/pdf\/1801.06146.pdf).","fc283463":"This notebook shows training Bert model from `transformers` library  with `fastai` library interface. By doing so we get to use goodies like `lr_finder()`, `gradual_unfreezing`, `Callbacks`, `to_fp16()` and other customizations if necessary.\n\nSince finetuning requires loading pretrained models from the internet this notebook can't be submitted directly but required models or output of this notebook can be saved as a Kaggle dataset for further submission.","e8d952dd":"Here we are initialazing model, splitting parameter groups and putting model callback for mixed precision training. `bs=128` is a good choice for the GPU memory we have at hand.","8bcda3c1":"Hope this is helpful to someone :)"}}