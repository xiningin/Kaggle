{"cell_type":{"000ea0af":"code","85565a56":"code","d7037eac":"code","bb93a080":"code","c8263c2d":"code","823d8fbd":"code","cfbc9400":"code","44c4d8ae":"code","8eaa0c07":"code","7bfa1cba":"code","f6212d0c":"code","18c85572":"code","12005ce8":"code","271e8d78":"code","46c1b7fd":"code","17f5bf81":"code","acdd42a8":"code","f2176b7a":"code","9517498e":"code","81bc5ec8":"code","771fc821":"code","46c400b9":"code","367cd2e8":"code","e594daef":"code","980ecf26":"markdown","4e785a8e":"markdown","514a91d0":"markdown","314d4a78":"markdown","d89d66c9":"markdown","013100d4":"markdown","b6d1f7d5":"markdown","79e481e3":"markdown","2efed11a":"markdown","60848489":"markdown","f978c856":"markdown","bb59678b":"markdown","053bdc1f":"markdown","d75524f0":"markdown","b5a1fbe4":"markdown"},"source":{"000ea0af":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nimport lightgbm as lgb\nfrom numpy import sort\nfrom sklearn.ensemble import RandomForestClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\nplt.style.use('seaborn')\nsns.set(font_scale=1)\npd.set_option('display.max_columns', 500)","85565a56":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"Train: \",train.shape)\nprint(\"Test: \", test.shape)","d7037eac":"train.head()","bb93a080":"data = [go.Bar(\n            x = train[\"target\"].value_counts().index.values,\n            y = train[\"target\"].value_counts().values,\n            text='Distribution of target variable'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","c8263c2d":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","823d8fbd":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","cfbc9400":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","44c4d8ae":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","8eaa0c07":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","7bfa1cba":"target = train.pop('target')\ntrain_ids = train.pop('ID_code')\ntest_ids = test.pop('ID_code')\nlen_train = len(train)","f6212d0c":"y = target.values\nX = train\n#Train & Validation\nfrom sklearn.model_selection import train_test_split\n# create training and testing vars\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","18c85572":"rfc_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist())","12005ce8":"del data, fig\ngc.collect()","271e8d78":"#Merge test and train\nmerged = pd.concat([train, test])\n#Saving the list of original features in a new list `original_features`.\noriginal_features = merged.columns\nmerged.shape","46c1b7fd":"%%time\nidx = features = merged.columns.values[0:200]\nfor df in [merged]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","17f5bf81":"#Getting the list of names of the added features.\nnew_features = set(merged.columns) - set(original_features)","acdd42a8":"print(\"Total number of features: \",merged.shape[1])","f2176b7a":"train = merged.iloc[:len_train]\nX = train\ntrain.head()","9517498e":"test = merged.iloc[len_train:]\ntest.head()","81bc5ec8":"del merged\ngc.collect()","771fc821":"params_tuned = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","46c400b9":"%%time\nX_test = test.values.astype(float)\ny_pred_lgb = np.zeros(len(X_test))\nfeatures = [c for c in X.columns]\noof = np.zeros(len(train))\nfold_n = 5\ncv = fold_n\n    \nfolds = StratifiedKFold(n_splits=fold_n, random_state=10, shuffle=True)\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())    \n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params_tuned,train_data,num_boost_round=100000,\n                valid_sets = [train_data, valid_data],verbose_eval=1000,early_stopping_rounds = 3000)\n    oof[valid_index] = lgb_model.predict(X.iloc[valid_index][features], num_iteration=lgb_model.best_iteration)\n    \n    feature_importance_df = pd.DataFrame()\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n    fold_importance_df[\"fold\"] = fold_n + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\/cv\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))","367cd2e8":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","e594daef":"submission_lgb = pd.DataFrame({\n        \"ID_code\": test_ids,\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)","980ecf26":"Let's check the distribution of the mean values per columns in the train and test set.","4e785a8e":"Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target.","514a91d0":"Let's show the distribution of standard deviation of values per row for train and test datasets.","314d4a78":"## Load Data","d89d66c9":"We have a class imbalance problem with more records for class '0' than class '1' .... I tried SMOTE over sampling before but didn't seem to help...","013100d4":"## LGBM with new features","b6d1f7d5":"More EDA visualizations see Gabriel Preda's kernel (https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction)","79e481e3":"Let's check the distribution of the standard deviation of values per columns in the train and test datasets.","2efed11a":"## Submission File","60848489":"### Feature Importance","f978c856":"### Permutation Importance","bb59678b":"### Distribution of Mean & Std","053bdc1f":"## Computing new features","d75524f0":"The values towards the top are the most important features, and those towards the bottom matter least.\n\nThe first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n\nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck\/chance.\n\nIn our case, the top 10 most important feature are var_81, var_53, var_139, var_179, var_174, var_40, var_26, var_13, var_24 and var_109. But, Still all the features seems to have value importance close to zero.","b5a1fbe4":"## Credit:\nAlexey Pronin's kernel (https:\/\/www.kaggle.com\/graf10a\/logistic-regression-with-new-features-feather) \n\nGabriel Preda's kernel (https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction)\n\nMohanRaj's kernel (https:\/\/www.kaggle.com\/mytymohan\/sct-prediction-eda-smote-lgbm)\n\nJeremyLane's kernel (https:\/\/www.kaggle.com\/lane203j\/methods-and-common-mistakes-for-evaluating-models)"}}