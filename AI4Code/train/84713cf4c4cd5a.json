{"cell_type":{"df053051":"code","8c1f4ace":"code","5499e795":"code","50381a66":"code","711ececc":"code","23f889a5":"code","a47f30ed":"code","676b502d":"code","b81aac47":"code","35afe2a8":"code","7fe8b43c":"code","c6437fdb":"code","7b02e08d":"code","110efdff":"code","15c7b95b":"code","36ed22a7":"code","602d7aee":"code","85bb2e50":"code","88ec7bec":"code","ff36892b":"code","1ad909de":"code","d70fb41c":"code","dc162dc5":"code","ec0d7e98":"code","868daebe":"markdown","ad9eaee1":"markdown","19bd115e":"markdown","6eb417fb":"markdown","ff35b4a1":"markdown","0f51d1f9":"markdown","a6e7e858":"markdown","7416c06f":"markdown","cd24919e":"markdown","5e60d535":"markdown","a84f5386":"markdown","5cbec21a":"markdown","6ef431e3":"markdown"},"source":{"df053051":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c1f4ace":"pd.set_option('max_columns',None)","5499e795":"train_data =  pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')","50381a66":"def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n    ints = df.select_dtypes(include=['int']).columns.tolist()\n    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n    return df\n\ndef optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n    floats = df.select_dtypes(include=['float']).columns.tolist()\n    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n    return df","711ececc":"train_data = optimize_ints(train_data)","23f889a5":"X = train_data.drop('Cover_Type',axis=1).set_index('Id')\ny = train_data.Cover_Type","a47f30ed":"# remove constant features\nto_remove = []\nsummary = X.describe()\nfor c in X.columns:\n    if summary.loc['std',c]==0:\n        to_remove.append(c)\n        \nprint(to_remove)\n\nX=X.drop(to_remove,axis=1)","676b502d":"# too few data for a class\nto_remove = []\nfor c in y.unique():\n    indices = list(y[y==c].index)\n    if len(indices) <= 1:\n        to_remove += indices\n        \nprint(to_remove)\n\nX=X.drop(to_remove,axis=0)\ny=y.drop(to_remove,axis=0)","b81aac47":"from sklearn.preprocessing import LabelEncoder\ny = LabelEncoder().fit_transform(y)","35afe2a8":"X.info(memory_usage='deep')","7fe8b43c":"X.min(axis=0).min()","c6437fdb":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\nxgb.__version__","7b02e08d":"params = {\"max_depth\": 6,\n          'subsample': 0.1,\n          \"colsample_bytree\": 0.25,\n          'learning_rate': 0.05,\n          'min_child_weight': 0\n         }\nparams['verbosity'] = 2\nparams['tree_method'] = 'gpu_hist'\nparams['predictor'] = 'gpu_predictor'\nparams['sampling_method'] = 'gradient_based'\nparams['n_jobs'] = -1\nparams['random_state']=42\nparams['n_estimators'] = 5000","110efdff":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,shuffle=True,random_state=42,stratify=y)","15c7b95b":"from sklearn.metrics import accuracy_score","36ed22a7":"%%time\nclf = XGBClassifier(**params, use_label_encoder=False).fit(X_train,y_train, eval_metric=['merror','mlogloss'],\n                                                           eval_set=[(X_train,y_train),(X_test,y_test)],verbose=1000)","602d7aee":"%%time\nprint(F'Train accuracy: {accuracy_score(y_train,clf.predict(X_train))}')\nprint(F'Test accuracy: {accuracy_score(y_test,clf.predict(X_test))}')","85bb2e50":"sparse_cols = [c for c in X.columns if c.startswith('Soil_Type') or c.startswith('Wilderness_Area')]\nX[sparse_cols].mean(axis=0)","88ec7bec":"MISSING = -32768","ff36892b":"Xs = X.copy()\nXs[sparse_cols] = Xs[sparse_cols].astype(np.int16) # expand to signed 16-bit integer\nXs[sparse_cols] = Xs[sparse_cols].replace(0,MISSING) # substitute 0's by -32768's\nXs[sparse_cols] = Xs[sparse_cols].astype(pd.SparseDtype(np.int16,MISSING)) # convert columns to sparse arrays","1ad909de":"Xs.info(memory_usage='deep')","d70fb41c":"Xs_train,Xs_test,y_train,y_test = train_test_split(Xs,y,test_size=0.25,shuffle=True,random_state=42,stratify=y)","dc162dc5":"%%time\nclf = XGBClassifier(**params, use_label_encoder=False, missing=MISSING).fit(Xs_train,y_train, eval_metric=['merror','mlogloss'],\n                                                           eval_set=[(Xs_train,y_train),(Xs_test,y_test)],verbose=1000)","ec0d7e98":"%%time\nprint(F'Train accuracy: {accuracy_score(y_train,clf.predict(Xs_train))}')\nprint(F'Test accuracy: {accuracy_score(y_test,clf.predict(Xs_test))}')","868daebe":"... and quality has not degraded.","ad9eaee1":"# Sparse representation of dataframe","19bd115e":"Let us first investigate how sparse the columns really are. ","6eb417fb":"We note in particular memory usage of this baseline dataframe.","ff35b4a1":"The whole dataframe consists of integers. We now check the smallest integer in the dataframe. The reason is we need to find an unused integer to represent a missing value, as will be clear later.","0f51d1f9":"Now we continue to set up XGBoost to run our experiment to get baseline timing and quality (accuracy). Nothing groundbreaking here, just splitting the data into training and testing and using some possibly non-optimal hyperparameters to illustrate the approach.","a6e7e858":"# Setting up data and baseline experiment","7416c06f":"# Introduction\n\nIn this notebook, I attempt to exploit the sparity of the binary features for soil type and wilderness area. This dataset is a synthetic version of the well-known [forest covertype dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/covertype). In the original dataset, soil type features and wilderness area features are one-hot encoded, i.e., they are one-hot encoded binary features from categorical variables `Soil_Type` and `Wilderness_Area`. In the synthetic version, one-hotness has been lost. Nevertheless, they are still relatively sparse. Can we still exploit the sparsity to benefit classifier training, especially for tree-based classifiers? ","cd24919e":"As expected, soil type columns are sparse; wilderness area columns, not so much.","5e60d535":"It looks like we get more than 25% speed up in training...","a84f5386":"The reason why we go through this apparently trivial exercise of replacing \\\\(0\\\\)'s by a missing token is that, as explained in the [XGBoost paper](https:\/\/arxiv.org\/abs\/1603.02754), there is a sparsity-aware algorithm for splitting samples at a node that learns a \"default direction\" for samples with missing values, and it only needs to examine the samples that have a value. Long story short, we expect a speed up in training.","5cbec21a":"Now we choose \\\\(-32768\\\\) to be our token for \"missing value\". This value is not used in the training set as we have checked earlier (and also not used in the test set, something we've checked behind the scene). This would necessitate expanding the data width for the binary columns from `int8` to `int16`. The additional space needed for this would be more than compensated for when we finally convert to sparse matrix representation.\n\nNow you may wonder why, we don't have missing values in the dataframe. That's correct, but we are going to consider the \\\\(0\\\\)'s in the binary columns as \"missing\".  After all, \\\\(0\\\\) and \\\\(1\\\\) are just names we call the two different states.","6ef431e3":"Everything looks good. Memory usage is actually reduced by almost 50%.\n\nNow repeat the experiment with this sparse dataframe."}}