{"cell_type":{"6e4ebb2b":"code","5e1fd698":"code","14f81c4c":"code","5a9d787a":"code","61cc3b58":"code","122f8585":"code","6c12eb99":"code","24df0aa9":"code","ffce2775":"code","95a7bc41":"code","091a6d0a":"code","c1bcfca8":"code","e51b48b9":"code","44c62498":"code","3a823ccc":"code","d881a9c6":"code","111f7a90":"code","3ae49882":"code","467f601e":"code","9925f30b":"code","b1cbf5b6":"code","1865ec34":"code","453535af":"code","fce1a9c9":"code","fe0abfbc":"code","4a071e57":"code","cb90d532":"code","5a619288":"code","83ee9156":"markdown","c7d79406":"markdown","8c330608":"markdown","52710820":"markdown","9b322fb7":"markdown","4ac20450":"markdown"},"source":{"6e4ebb2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e1fd698":"import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport nltk\nnlp = spacy.load('en_core_web_sm')","14f81c4c":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","5a9d787a":"train_df.isna().sum()","61cc3b58":"keywords_dis = train_df[train_df['target'] == 1].keyword.value_counts().sort_values(ascending=False)\nkeywords_dis","122f8585":"keywords_nondis = train_df[train_df['target'] == 0].keyword.value_counts().sort_values(ascending=False)\nkeywords_nondis ","6c12eb99":"train_df.keyword.value_counts().sort_values(ascending=False)","24df0aa9":"sns.barplot(y = keywords_dis.index[:40], x = keywords_dis[:40])\nsns.set(rc={'figure.figsize':(20,10)})","ffce2775":"sns.barplot(y = keywords_nondis.index[:40], x = keywords_nondis[:40])\nsns.set(rc={'figure.figsize':(20,10)})","95a7bc41":"train_df['text'] = train_df['text'].str.lower()","091a6d0a":"train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\ntrain_df['text'].head()","c1bcfca8":"#Importing stopwords from nltk library\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n# Function to remove the stopwords\ndef stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n# Applying the stopwords to 'text_punct' and store into 'text_stop'\ntrain_df[\"text\"] = train_df[\"text\"].apply(stopwords)","e51b48b9":"# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in train_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntrain_df[\"text\"] = train_df[\"text\"].apply(freqwords)","44c62498":"# Function for url's\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\n#Passing the function to 'text_rare'\ntrain_df['text'] = train_df['text'].apply(remove_urls)","3a823ccc":"from bs4 import BeautifulSoup\n#Function for removing html\ndef html(text):\n    return BeautifulSoup(text, \"lxml\").text\n# Passing the function to 'text_rare'\ntrain_df['text'] = train_df['text'].apply(html)","d881a9c6":"train_df['lemmatized'] = train_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","111f7a90":"tfidvectorizer = TfidfVectorizer(stop_words= 'english', max_df=0.8, ngram_range=(1,6))\ntrain_vectors = tfidvectorizer.fit_transform(train_df['lemmatized'])","3ae49882":"test_df['text'] = test_df['text'].str.lower()\n\ntest_df['text'] = test_df['text'].str.replace('[^\\w\\s]','')\n\ntest_df[\"text\"] = test_df[\"text\"].apply(stopwords)\n\n# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in test_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntest_df[\"text\"] = test_df[\"text\"].apply(freqwords)\ntest_df[\"text\"].head()\n\ntest_df['text'] = test_df['text'].apply(remove_urls)\n\ntest_df['text'] = test_df['text'].apply(html)\n\n\n\ntest_df['lemmatized'] = test_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","467f601e":"train_df.tail()","9925f30b":"test_vectors = tfidvectorizer.transform(test_df['lemmatized'])","b1cbf5b6":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB(alpha=0.1)","1865ec34":"cross_val = model_selection.cross_val_score(model, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\ncross_val","453535af":"model.fit(train_vectors, train_df['target'])","fce1a9c9":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","fe0abfbc":"submission['target'] = model.predict(test_vectors)","4a071e57":"submission.head()","cb90d532":"submission.to_csv('submission.csv', index=False)","5a619288":"test_df.head()","83ee9156":"## most used keywords are different for disaster and non disaster samples which will help algorithm to differentiate","c7d79406":"## despite many missing values, location can be useful for future versions. Tweets from the same location aboout the same keywords can be used","8c330608":"## now do same for test data","52710820":"## preprocess the text data","9b322fb7":"##  barplot for all keywords was too big and words couldn't be differentiated so I limited only top 40","4ac20450":"## First let's make some eda"}}