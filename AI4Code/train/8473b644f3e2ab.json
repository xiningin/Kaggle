{"cell_type":{"3180ab89":"code","438bc3a1":"code","6b91493d":"code","5908fbe8":"code","97c4f7b5":"code","9e25f70b":"code","7aebf414":"code","cfe566e7":"code","c5386d44":"code","0bd8263c":"code","ccb4b304":"code","939236e2":"code","7e5378f9":"code","a35fc9a3":"code","5f4490ff":"code","873c8e09":"code","dc265930":"code","bb26fe8c":"code","42569f36":"code","41487b0a":"code","7e126e24":"code","e29a3a40":"code","d84acb45":"code","b0efdd1b":"code","58d69538":"code","b8cee6fb":"code","9b9a6f67":"code","9cc8ea12":"code","e6f317fa":"code","db9a4056":"code","938c409b":"code","1f590762":"code","cce409a7":"code","9ab2be81":"code","d1c2120b":"code","eb7e4d52":"code","ad2577d8":"code","bd24451a":"code","8d228015":"code","d8bc63c9":"code","2474d0d4":"code","8966cdc8":"code","ff51c83a":"code","4c2cabb2":"code","28f99d97":"code","fa331fe6":"code","cf788647":"code","afbadd2e":"code","a69b3e7c":"code","d639caf0":"code","cf80511c":"code","196b4377":"code","e4cf01d8":"code","87c0ba2e":"code","681f27bf":"code","6751453e":"code","fa51d06d":"code","742c3108":"code","586b345f":"code","fcf3dd7d":"code","ab29a299":"code","1b21152e":"markdown","675fcfcf":"markdown","d7501c91":"markdown","29f883ac":"markdown","9c480425":"markdown","0c473319":"markdown","1091d7dd":"markdown","6c033d4d":"markdown","56e534d4":"markdown","56b0b1df":"markdown","2d7f4548":"markdown","90a73a6c":"markdown","74fe253a":"markdown","68706d36":"markdown","412db697":"markdown","39994dea":"markdown","2f1f8149":"markdown","6a5de65d":"markdown","0e55e964":"markdown","69fa415d":"markdown","67f17cc4":"markdown","a0065395":"markdown","a8ac9011":"markdown","1860bdc4":"markdown","d012d229":"markdown","48194d1c":"markdown","1a6750a3":"markdown","4f206b39":"markdown","60c9a8e3":"markdown","078081f2":"markdown","01acc1a6":"markdown","c97a4cbe":"markdown","628498b8":"markdown","733888c2":"markdown","dd8ddcff":"markdown","fedbadca":"markdown","649ffff1":"markdown","37e720be":"markdown","1153dd1d":"markdown","d9204370":"markdown","b3279328":"markdown","b64ffd37":"markdown","44c9d952":"markdown","feb998c8":"markdown","6beb4122":"markdown","1e0f0b38":"markdown","1b1a536d":"markdown","1358f3a2":"markdown","c9d776fc":"markdown","57bab36a":"markdown","4e6383ee":"markdown","f1c8aa12":"markdown","c67927c1":"markdown","17e29a73":"markdown","4847709a":"markdown","7c208111":"markdown","e29ca1db":"markdown","ddc42078":"markdown","1d4aebbf":"markdown"},"source":{"3180ab89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","438bc3a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n","6b91493d":"df= pd.read_csv(\"\/kaggle\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\")","5908fbe8":"df.describe()\ndf.info()","97c4f7b5":"df.head(10)","9e25f70b":"df.size","7aebf414":"df.dropna()\ndf.size","cfe566e7":"### Checking for null values\nprint('Null Values -', df.isnull().values.any())","c5386d44":"df['isFraud'].value_counts()\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"isFraud\", data=df)\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))","0bd8263c":"import math\na= (8213\/(6354407+8213))*100\nprint ('The percentage of Fraud transactions is ' \"%.2f\" % a, '%' )","ccb4b304":"print(df.type.value_counts())\nf, ax = plt.subplots(1, 1, figsize=(8, 8))\ndf.type.value_counts().plot(kind='bar', title=\"Transaction type\", ax=ax, figsize=(8,8))\nplt.ticklabel_format(style='plain', axis='y')\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))\nplt.show()","939236e2":"payment_df = df[df[\"type\"] == 'PAYMENT']\npayment_df.head(5)","7e5378f9":"transfer_df = df[df[\"type\"] == 'TRANSFER']\ntransfer_df.head(5)","a35fc9a3":"cashout_df = df[df[\"type\"] == 'CASH_OUT']\ncashout_df.head(5)","5f4490ff":"cashin_df = df[df[\"type\"] == 'CASH_IN']\ncashin_df.head(5)","873c8e09":"debit_df = df[df[\"type\"] == 'DEBIT']\ndebit_df.head(5)","dc265930":"f_df = df[(df['isFraud'] == 1)] \nf_df    \n","bb26fe8c":"#Fraud and non fraud distribution\nfraud = df[df[\"isFraud\"] == 1]\nvalid = df[df[\"isFraud\"] == 0]","42569f36":"ax = df.groupby(['isFlaggedFraud','isFraud']).size().plot(kind='bar',figsize=(8, 6), color='green')\nax.set_title(\" Comparison between Fraud and Flagged Fraud transactions\")\nax.set_xlabel(\"(isFlaggedFraud, isFraud)\")\nax.set_ylabel(\"Count of transaction\")\nplt.ticklabel_format(style='plain', axis='y')\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))","41487b0a":"ax = df.groupby(['type', 'isFraud']).size().plot(kind='bar',figsize=(8, 6), color='purple')\nax.set_title(\" Transaction which are the actual fraud per transaction type\")\nax.set_xlabel(\"(Type, isFraud)\")\nax.set_ylabel(\"Count of transaction\")\nplt.ticklabel_format(style='plain', axis='y')\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))\n","7e126e24":"ax=sns.countplot('type', data=f_df)\nplt.title('Fraud Distribution', fontsize=14)\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))\n\nplt.show()","e29a3a40":"ax = df.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar',figsize=(8, 6), color='pink')\nax.set_title(\"Transaction which are flagged as fraud per transaction type\")\nax.set_xlabel(\"(Type, isFlaggedFraud)\")\nax.set_ylabel(\"Count of transaction\")\nfor p in ax.patches:\n    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))","d84acb45":"print('All Transactions ',df['nameOrig'].size)\nprint('Unique Transactions ',df['nameOrig'].unique().size)\nprint('Transactions from existing accounts ',df['nameOrig'].size-df['nameOrig'].unique().size)\n","b0efdd1b":"print('All Transactions ',df['nameDest'].size)\nprint('Unique Transactions ',df['nameDest'].unique().size)\nprint('Transactions from existing accounts ',df['nameDest'].size-df['nameOrig'].unique().size)\n","58d69538":"print('\\nAre there any merchants among originator accounts for CASH_IN \\\ntransactions? {}'.format(\\\n(df.loc[df.type == 'CASH_IN'].nameOrig.str.contains('M')).any()))","b8cee6fb":"print('\\nAre there any merchants among destination accounts for CASH_OUT \\\ntransactions? {}'.format(\\\n(df.loc[df.type == 'CASH_OUT'].nameDest.str.contains('M')).any()))","9b9a6f67":"print('\\nAre there merchants among any originator accounts? {}'.format(\\\n      df.nameOrig.str.contains('M').any())) \n\nprint('\\nAre there any transactions having merchants among destination accounts\\\n other than the PAYMENT type? {}'.format(\\\n(df.loc[df.nameDest.str.contains('M')].type != 'PAYMENT').any()))","9cc8ea12":"# Eliminate columns shown to be irrelevant for analysis in the EDA\ndf = df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)\n","e6f317fa":"df.head()","db9a4056":"df_new = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]\n\nrandomState = 5\nnp.random.seed(randomState)\ndf_new","938c409b":"wrong_orig_bal = sum(df[\"oldbalanceOrg\"] - df[\"amount\"] != df[\"newbalanceOrig\"])\nwrong_dest_bal = sum(df[\"newbalanceDest\"] + df[\"amount\"] != df[\"newbalanceDest\"])\nprint(\"Percentage of observations with balance errors in the account giving money: \", 100*round(wrong_orig_bal\/len(df),2))\nprint(\"Percentage of observations with balance errors in the account receiving money: \", 100*round(wrong_dest_bal\/len(df),2))","1f590762":"# flatten the subsetted dataframe of floats into an array of floats\nrelevant_cols = df[[\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\"]].values.flatten()\n# number of observations with negative numbers\nnum_neg_amt = sum(n < 0 for n in relevant_cols)\n# number of observations where the amount given is greater than the amount that is in the sender's account\nnum_amt_oldgiver = sum(df[\"amount\"] > df[\"oldbalanceOrg\"]) \n# number of observations where the amount received is greater than the amount that is in the receiver's account\nnum_amt_newreceiver = sum(df[\"amount\"] > df[\"newbalanceDest\"]) \n\nprint(\"number of observations with negative numbers: \", num_neg_amt)\nprint(\"number of observations where the amount given is greater than the amount that is in the sender's account: \"\n      , num_amt_oldgiver)\nprint(\"number of observations where the amount received is greater than the amount that is in the receiver's account: \"\n      , num_amt_newreceiver)","cce409a7":"df_new[\"errorbalanceOrg\"] = df_new.newbalanceOrig + df_new.amount - df_new.oldbalanceOrg\ndf_new[\"errorbalanceDest\"] = df_new.oldbalanceDest + df_new.amount - df_new.newbalanceDest","9ab2be81":"# Subsetting data into observations with fraud and valid transactions:\nfraud = df_new[df_new[\"isFraud\"] == 1]\nvalid = df_new[df_new[\"isFraud\"] == 0]","d1c2120b":"print(\"Proportion of fraudulent transactions with errorBalanceDest > 0: \", len(fraud[fraud.errorbalanceDest > 0])\/len(fraud))\nprint(\"Proportion of valid transactions with errorBalanceDest > 0: \", len(valid[valid.errorbalanceDest > 0])\/len(valid))\nprint(\"Proportion of fraudulent transactions with errorBalanceOrg > 0: \", len(fraud[fraud.errorbalanceOrg > 0])\/len(fraud))\nprint(\"Proportion of valid transactions with errorBalanceOrg > 0: \", len(valid[valid.errorbalanceOrg > 0])\/len(valid))","eb7e4d52":"# Time patterns\n\nbins = 50\n\nvalid.hist(column=\"step\",color=\"green\",bins=bins)\nplt.xlabel(\"1 hour time step\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of valid transactions over time\")\n\nfraud.hist(column =\"step\",color=\"red\",bins=bins)\nplt.xlabel(\"1 hour time step\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of fraud transactions over time\")\n\nplt.tight_layout()\nplt.show()\n","ad2577d8":"num_days = 7\nnum_hours = 24\nfraud_days = fraud.step % num_days\nfraud_hours = fraud.step % num_hours\nvalid_days = valid.step % num_days\nvalid_hours = valid.step % num_hours\n\n# plotting scatterplot of the days of the week, identifying the fraudulent transactions (red) from the valid transactions (green) \nplt.subplot(1, 2, 1)\nfraud_days.hist(bins=num_days,color=\"red\")\nplt.title('Fraud transactions by Day')\nplt.xlabel('Day of the Week')\nplt.ylabel(\"# of transactions\")\n\nplt.subplot(1,2,2)\nvalid_days.hist(bins=num_days,color=\"green\")\nplt.title('Valid transactions by Day')\nplt.xlabel('Day of the Week')\nplt.ylabel(\"# of transactions\")\n\nplt.tight_layout()\nplt.show()","bd24451a":"# plotting scatterplot of the hours of days, identifying the fraudulent transactions (red) from the valid transactions (green) \nplt.subplot(1, 2, 1)\nfraud_hours.hist(bins=num_hours, color=\"red\")\nplt.title('Fraud transactions by Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel(\"# of transactions\")\n\n\nplt.subplot(1, 2, 2)\nvalid_hours.hist(bins=num_hours, color=\"green\")\nplt.title('Valid transactions by Hour')\nplt.xlabel('Hour of the Day')\nplt.ylabel(\"# of transactions\")\n\nplt.tight_layout()\nplt.show()","8d228015":"dataset = df_new.copy()\n\n\n# adding feature HourOfDay to Dataset1 \ndataset[\"HourOfDay\"] = np.nan # initializing feature column\ndataset.HourOfDay = df_new.step % 24\n\n\nprint(\"Head of dataset1: \\n\", pd.DataFrame.head(dataset))","d8bc63c9":"print('CASE AMOUNT STATISTICS')\nprint('\\nNON-FRAUD CASE AMOUNT STATS')\nprint(valid['amount'].describe())\nprint('\\nFRAUD CASE AMOUNT STATS')\nprint(fraud['amount'].describe())","2474d0d4":"dataset.head()","8966cdc8":"dataset = pd.get_dummies(dataset,prefix=['type'])","ff51c83a":"dataset.head()","4c2cabb2":"from random import seed,sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nseed(21)\n\nX = dataset.drop(\"isFraud\",1)\nY = dataset.isFraud\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n\n# Normalizing data so that all variables follow the same scale (0 to 1)\nscaler = StandardScaler()\n\n# Fit only to the training data\nscaler.fit(X_train)\n\n# Now apply the transformations to the data:\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n","28f99d97":"print(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)","fa331fe6":"print('fruad ',fraud.shape)\nprint('Valid', valid.shape)\n","cf788647":"from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\nfrom sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\nfrom xgboost import XGBClassifier # XGBoost algorithm\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_curve, auc, precision_score","afbadd2e":"# Train model\nparametersRF = {'n_estimators':15,'class_weight': \"balanced\",'n_jobs':-1,'random_state':42}\nRF = RandomForestClassifier(**parametersRF)\nfitted_vals = RF.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsRF = RF.predict(X_test)\n \n     \n# Evaluating model\nCM_RF = confusion_matrix(y_test,predictionsRF)\nCR_RF = classification_report(y_test,predictionsRF)\nfprRF, recallRF, thresholdsRF = roc_curve(y_test, predictionsRF)\nAUC_RF = auc(fprRF, recallRF)\n\nresultsRF = {\"Confusion Matrix\":CM_RF,\"Classification Report\":CR_RF,\"Area Under Curve\":AUC_RF}\n\n# showing results from Random Forest\n\nfor measure in resultsRF:\n    print(measure,\": \\n\",resultsRF[measure])","a69b3e7c":"# Train model\nDT = DecisionTreeClassifier()\nfitted_vals = DT.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsDT = DT.predict(X_test)\n \n     \n# Evaluating model\nCM_DT = confusion_matrix(y_test,predictionsDT)\nCR_DT = classification_report(y_test,predictionsDT)\nfprDT, recallDT, thresholdsDT = roc_curve(y_test, predictionsDT)\nAUC_DT = auc(fprDT, recallDT)\n\nresultsDT = {\"Confusion Matrix\":CM_DT,\"Classification Report\":CR_DT,\"Area Under Curve\":AUC_DT}\n\n# showing results from Random Forest\n\nfor measure in resultsDT:\n    print(measure,\": \\n\",resultsDT[measure])","d639caf0":"# Train model\nparametersXGB = {'max_depth':3,'class_weight': \"balanced\",'n_jobs':-1,'random_state':42,'learning_rate':0.1}\nXGB = XGBClassifier(**parametersXGB)\n    \n    \nfitted_vals = XGB.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsXGB = XGB.predict(X_test)\n \n     \n# Evaluating model\nCM_XGB = confusion_matrix(y_test,predictionsXGB)\nCR_XGB = classification_report(y_test,predictionsXGB)\nfprXGB, recallXGB, thresholds_XGB = roc_curve(y_test, predictionsXGB)\nAUC_XGB = auc(fprXGB, recallXGB)\nresultsXGB = {\"Confusion Matrix\":CM_XGB,\"Classification Report\":CR_XGB,\"Area Under Curve\":AUC_XGB}\n# showing results from Extreme Gradient Boosting\nfor measure in resultsXGB:\n    print(measure,\": \\n\",resultsXGB[measure],\"\\n\")","cf80511c":"print(\"Note: scores in the same vertical level as 0 are scores for valid transactions. \\n \\\n      Scores in the same vertical level as 1 are scores for fraudulent transactions. \\n\")\nprint(\"Classification Report of Random Forest: \\n\", CR_RF)\nprint(\"Classification Report of Decision trees: \\n\", CR_DT)\nprint(\"Classification Report of XGB trees: \\n\", CR_XGB)\n","196b4377":"\n# check version number\nimport imblearn\nprint(imblearn.__version__)\n# check version number\nimport imblearn\nprint(imblearn.__version__)","e4cf01d8":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler \nrus = RandomUnderSampler(random_state=42)\nX_resampled, Y_resampled = rus.fit_resample(X, Y)\nprint(\"Resampled shape of X: \", X_resampled.shape)\nprint(\"Resampled shape of Y: \", Y_resampled.shape)\nvalue_counts = Counter(Y_resampled)\nprint(value_counts)\ntrain_X, test_X, train_Y, test_Y = train_test_split(X_resampled, Y_resampled, test_size= 0.3, random_state= 42)                              \n                              ","87c0ba2e":"# Train model\nparametersRF = {'n_estimators':15,'n_jobs':-1,'random_state':42}\nRF = RandomForestClassifier(**parametersRF)\nfitted_vals = RF.fit(train_X, train_Y)\n \n# Predict on testing set\npredictionsRF = RF.predict(test_X)\n \n     \n# Evaluating model\nCM_RF = confusion_matrix(test_Y,predictionsRF)\nCR_RF = classification_report(test_Y,predictionsRF)\nfprRF, recallRF, thresholdsRF = roc_curve(test_Y, predictionsRF)\nAUC_RF = auc(fprRF, recallRF)\n\nresultsRF = {\"Confusion Matrix\":CM_RF,\"Classification Report\":CR_RF,\"Area Under Curve\":AUC_RF}\n\n# showing results from Random Forest\n\nfor measure in resultsRF:\n    print(measure,\": \\n\",resultsRF[measure])","681f27bf":"# Train model\nDT = DecisionTreeClassifier()\nfitted_vals = DT.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsDT = DT.predict(X_test)\n \n     \n# Evaluating model\nCM_DT = confusion_matrix(y_test,predictionsDT)\nCR_DT = classification_report(y_test,predictionsDT)\nfprDT, recallDT, thresholdsDT = roc_curve(y_test, predictionsDT)\nAUC_DT = auc(fprDT, recallDT)\n\nresultsDT = {\"Confusion Matrix\":CM_DT,\"Classification Report\":CR_DT,\"Area Under Curve\":AUC_DT}\n\n# showing results from Random Forest\n\nfor measure in resultsDT:\n    print(measure,\": \\n\",resultsDT[measure])","6751453e":"# Train model\nparametersXGB = {'max_depth':3,'class_weight': \"balanced\",'n_jobs':-1,'random_state':42,'learning_rate':0.1}\nXGB = XGBClassifier(**parametersXGB)\n    \n    \nfitted_vals = XGB.fit(X_train, y_train)\n \n# Predict on testing set\npredictionsXGB = XGB.predict(X_test)\n \n     \n# Evaluating model\nCM_XGB = confusion_matrix(y_test,predictionsXGB)\nCR_XGB = classification_report(y_test,predictionsXGB)\nfprXGB, recallXGB, thresholds_XGB = roc_curve(y_test, predictionsXGB)\nAUC_XGB = auc(fprXGB, recallXGB)\nresultsXGB = {\"Confusion Matrix\":CM_XGB,\"Classification Report\":CR_XGB,\"Area Under Curve\":AUC_XGB}\n# showing results from Extreme Gradient Boosting\nfor measure in resultsXGB:\n    print(measure,\": \\n\",resultsXGB[measure],\"\\n\")","fa51d06d":"print(\"Note: scores in the same vertical level as 0 are scores for valid transactions. \\n \\\n      Scores in the same vertical level as 1 are scores for fraudulent transactions. \\n\")\nprint(\"Classification Report of Random Forest: \\n\", CR_RF)\nprint(\"Classification Report of Decision trees: \\n\", CR_DT)\nprint(\"Classification Report of XGB trees: \\n\", CR_XGB)\n","742c3108":"print(\"Number of valid transactions labelled as fraudulent by Random Forest: \\n\", CM_RF[0,1])\nprint(\"Number of valid transactions labelled as fraudulent by XGB trees: \\n\", CM_XGB[0,1])\nprint(\"Number of valid transactions labelled as fraudulent by Decision Tree: \\n\", CM_DT[0,1])","586b345f":"print(\"Number of fraud transactions labelled as valid by Random Forest: \\n\", CM_RF[1,0])\nprint(\"Number of fraud transactions labelled as valid by XGB trees: \\n\", CM_XGB[1,0])\nprint(\"Number of fraud transactions labelled as valid by Decision Tree: \\n\", CM_DT[1,0])","fcf3dd7d":"print(\"\\nAUC of Random Forest: \\n\", AUC_RF)\nprint(\"\\nAUC of Decision Tree: \\n\", AUC_DT)\nprint(\"\\nAUC of XGB trees: \\n\", AUC_XGB)","ab29a299":"ncols = len(X.columns)\nx = np.arange(ncols)\n\n# getting importances of features\nimportances = RF.feature_importances_\n\n# getting the indices of the most important feature to least important\nsort_ind = np.argsort(importances)[::-1]\nplt.figure(figsize=(18,7))\nplt.bar(x, importances[sort_ind])\nplt.xticks(x,tuple(X.columns.values[sort_ind]))\nplt.title(\"Important Features: Greatest to Least\")\nplt.show()","1b21152e":"1. The dataset is huge with over million data points, and the ratio of fraud to valid data is heavily skewed towads valid data\n2. Feature engineering and creation of two new features namely 'errorbalance' and 'HourofDay' yielded fruitful results.\n3. Random Forest Classifier is the best model in the given situation as it is fairly accurate in predicting both fraud and valid data, and has the heigest AUC .\n4. Working with such a large dataset is always a challenge and this helped me learn a lot .\n\nThank You!","675fcfcf":" \nWhat does the Area under Curve tells us ?\nArea Under Curve is a plot of the true positive rates (in our case, the proportion of valid transactions labelled as valid) against the false positive rate (in our case, the proportion of fraudulent transactions labelled as valid). The curve is also known as the Receiver Operating Characteristic Curve or ROC.\n\nThe ideal AUC is then 1 (all transactions predicted as valid are actually valid).\n\nFrom the above results:\n#### Random Forest \nhas the Area under Curve closest to 1, so I can confidently say that Random forest is the right model in this scenario.","d7501c91":"So, only 16 out of 6 million transactions were flagged by the system. It is safe to say the system uses an unreasonable parameter to detect fraud transactions","29f883ac":"#### Payment ","9c480425":"#### Cash Out","0c473319":"#### Transfer ","1091d7dd":"From the above graph we can see that the transfers that are Flagged Fraud are from TRANSFER(16) transaction type.\nSo out of 4097 Fraud Transfers only 16 were flagged by the system.","6c033d4d":"### Fraud transactions and Transaction types ","56e534d4":"### Model 3: XGBoost Classifier ","56b0b1df":"We will perfrom one-hot-encoding on the 'type' column which is a categorical feature\n\nOne-Hot encoding involves creating indicator variables for each category in a categorical variable.\n\nIf an observation is part of a particular category (e.g. the transaction type is CASH_OUT), the indicator variable associated with the category would be 1. If it isn't part of a particular category, then the indicator variable associated with that category would be 0.","2d7f4548":"### Model 1: Random Forest","90a73a6c":"From the above results, we can conclude that :\n1. There is erronous results in the new and old balance accounts for both sender and receiver\n2. Some of this erronous results are due to fraudulent transactions\n3. We cannot get rid of this features as well so we will let them be and add a new feature called 'errorbalance'","74fe253a":"### Model 2: Decision Tree ","68706d36":"## Important Features ","412db697":"### Model 1: Random Forest: Balanced Data","39994dea":"# Fraud Detection System\n#### Synthetic datasets generated by the PaySim mobile money simulator","2f1f8149":"### Datasets with respect to Transaction types ","6a5de65d":"## Conclusion ","0e55e964":"The distribution of fraud and valid dont show much of differnce to work with so I won't add this as a feature ","69fa415d":"### Fraud Dataset ","67f17cc4":"## Importing the data","a0065395":"Decision trees are an effective model for binary classification tasks, although by default, they are not effective at imbalanced classification.","a8ac9011":"As most of the transactions has errors in showing the account balances before and after transaction , we calculate the error","1860bdc4":"It seems that during fraudulent transactions, the amount moved is capped at 10 million currency units.\n\nWhereas for valid transactions, the amount moved is capped at about 92.4 million currency units.\n\nOnly valid transaction involved amounts larger than 10,000,000, however these transactions make up less than 0.01% of the relevant data.\n\nWhen the amounts moved is less than 10,000,000 there doesn't seem to be a large difference fraudulent and valid transactions.\n\nI leave the variable amount as it is and will not be creating a feature out of it.","d012d229":"## Comparing the models: Unbalanced data","48194d1c":"Conclusion: We do not get any benificial information from the nameOrig or nameDest, so we'll be dropping these columns","1a6750a3":"Number of transactions that is safe - 6354407 \n    ","4f206b39":"Conclusions:\n1. Most of the transactions where errorbalanceDest > 0 has high chance of being fraud\n2. Valid transactions are most likely to have errorbalanceOrig > 0 ","60c9a8e3":"### Taking note of the balances before and after transactions ","078081f2":"### Taking note of the Step feature","01acc1a6":"## Comparing the models: Balanced data ","c97a4cbe":"Number of transactions that are Fraud but not Flagged by the system - 8197\n   ","628498b8":"## Feature Engineering and Data Cleaning ","733888c2":"## Model Selection ","dd8ddcff":"## What does the columns mean\n\nstep - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n\ntype - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n\namount -\namount of the transaction in local currency.\n\nnameOrig - customer who started the transaction\n\noldbalanceOrg - initial balance before the transaction\n\nnewbalanceOrig - new balance after the transaction\n\nnameDest - customer who is the recipient of the transaction\n\noldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n\nnewbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n\nisFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n\nisFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.","fedbadca":"Merchant accounts are not included in the  fraudulent Transactions. Merchant accounts were only identified in 'Payment' type transactions","649ffff1":"Conclusion: Although isFraud is always set when isFlaggedFraud is set, since isFlaggedFraud is set just 16 times in a seemingly meaningless way, we can treat this feature as insignificant and discard it in the dataset without loosing information.","37e720be":"From the exploratory data analysis (EDA), we know that fraud only occurs in 'TRANSFER's and 'CASH_OUT's. So we assemble only the corresponding data in X for analysis.","1153dd1d":"From the above graph we can see that the Fraudulent transafers are from CASH_OUT(4116) and TRANSFER(4097) , transaction types","d9204370":"## Exploratory Data Analysis (EDA) ","b3279328":" Number of transactions that are Fraud and are Flagged bt the system - 16","b64ffd37":"#### Debit ","44c9d952":"### Distribution of the Transaction Type Column ","feb998c8":"From the graphs above, there is strong evidence to suggest that from hour 0 to hour 9, valid transactions very rarely occur. On the other hand, fraudulent transactions still occur at similar rates to any hour of the day outside of hours 0 to 9.\n\nSo I will add a new feature hour_of_day which is just the [(step column) %24]","6beb4122":"Things to note:\n1. A lot of Valid transactions occur during 0 to 60 hours and then again 120 to 400 hrs\n2. The fraud transactions dont change much throughout the time frame","1e0f0b38":"### Taking note of amount column ","1b1a536d":"### Dealing with Account Ids 'nameOrig' and 'nameDest'","1358f3a2":"### Handling Categorical Variables ","c9d776fc":"The data is extremely skewed, so we will test the models on unbalanced datd first and then perform Resampling technique to make it a balanced dataset","57bab36a":"#### Cash In","4e6383ee":"errorBalanceOrg is the most important feature by far for classifying transactions followed by oldBalanceOrg and newBalanceOrig.","f1c8aa12":"### Model 2: Decision Tree Classifier: Balanced Data ","c67927c1":"So, no null values ","17e29a73":"####  Converting the step feature from hours into days","4847709a":"From the results , Decision Tree and Random Forest has the better results but since the data is heavily skewed we need to handle the datasets to yield better results. ","7c208111":"###  XGBoost Classifier: Balanced Data","e29ca1db":"## Handling Imbalanced Data\n\n\nWe will use undersampling when we have huge data and undersampling the majority call won't effect the data. ","ddc42078":"Interestingly, even though XGBoost predicts no valid data as fraudulent but has a huge error in predicting the fraud transactions which is our aim here.\n\nSo we will prefer Random Forest from this metric","1d4aebbf":"###  Relation Between the Fraud Transactions and the transactions Flagged by the system"}}