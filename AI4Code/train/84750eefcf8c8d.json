{"cell_type":{"60a18454":"code","eb4b8e96":"code","22d0f926":"code","3444eaf0":"code","7af3c902":"code","76d7eb15":"code","329aa8e3":"code","5360b661":"code","ed3e78da":"code","d168006d":"code","81aebad3":"code","dfc3ea0a":"code","570b45bc":"code","b38f3a3f":"code","80dde3a1":"code","da2599a0":"code","32518c34":"code","faff001a":"code","83360712":"code","712c9d4d":"code","14f1c4fa":"code","646dfa41":"code","9331ab0c":"code","20760ac9":"code","a0c3ffaf":"code","0323f605":"code","301f0da2":"code","ed5513cb":"code","74d3bb24":"code","30cbfb08":"code","5d36a1aa":"code","556e0af8":"code","6adf0932":"code","bc68e066":"code","ca75ab93":"code","f78d3177":"code","5cdd1ac5":"code","3ae35b89":"code","7754f8b8":"code","2277546a":"code","2b98ac8f":"code","4e338a96":"code","c3718db0":"code","1dd8b407":"code","5b1cf962":"code","034d0903":"code","f05b1ac2":"code","2a2125f5":"code","352c637c":"code","491b878b":"code","bd96079b":"code","72a67890":"markdown","5d873323":"markdown","94ef0049":"markdown","44b124eb":"markdown","7ecbac3d":"markdown","bed1a1df":"markdown","d04eaf59":"markdown","5b5c12db":"markdown","d2c560d9":"markdown","bb5c91c5":"markdown","2a7311ca":"markdown","68bcabb5":"markdown","22af1cb8":"markdown","e32d0141":"markdown","9c459b90":"markdown","6a579e64":"markdown","1f5e3cc5":"markdown","3be593d3":"markdown","e685f17e":"markdown"},"source":{"60a18454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb4b8e96":"studInfo=pd.read_csv(\"\/kaggle\/input\/open-university-learning-analytics-dataset\/anonymiseddata\/studentInfo.csv\")\nassessments=pd.read_csv(\"\/kaggle\/input\/open-university-learning-analytics-dataset\/anonymiseddata\/assessments.csv\")\nstudAss=pd.read_csv(\"\/kaggle\/input\/open-university-learning-analytics-dataset\/anonymiseddata\/studentAssessment.csv\")\nstudVle=pd.read_csv(\"\/kaggle\/input\/open-university-learning-analytics-dataset\/anonymiseddata\/studentVle.csv\")\nvle=pd.read_csv(\"\/kaggle\/input\/open-university-learning-analytics-dataset\/anonymiseddata\/vle.csv\")","22d0f926":"exams=assessments[assessments[\"assessment_type\"]==\"Exam\"]\nothers=assessments[assessments[\"assessment_type\"]!=\"Exam\"]\namounts=others.groupby([\"code_module\",\"code_presentation\"]).count()[\"id_assessment\"] \namounts=amounts.reset_index()\namounts.head()\n#Here we have the total amount of assessments by module","3444eaf0":"#Function to determine whether a student passed a given assessment\ndef pass_fail(grade):\n    if grade>=40:\n        return True\n    else:\n        return False\n#Creating the stud_ass dataframe to join infos about the assessment weights and their respective grades\nstud_ass=pd.merge(studAss,others,how=\"inner\",on=[\"id_assessment\"])\nstud_ass[\"pass\"]=stud_ass[\"score\"].apply(pass_fail)\nstud_ass[\"weighted_grade\"]=stud_ass[\"score\"]*stud_ass[\"weight\"]\/100","7af3c902":"#Final assessment average per student per module\navg_grade=stud_ass.groupby([\"id_student\",\"code_module\",\"code_presentation\"]).sum()[\"weighted_grade\"].reset_index()\navg_grade.head()","76d7eb15":"#Pass rate per student per module\npass_rate=pd.merge((stud_ass[stud_ass[\"pass\"]==True].groupby([\"id_student\",\"code_module\",\"code_presentation\"]).count()[\"pass\"]).reset_index(),amounts,how=\"left\",on=[\"code_module\",\"code_presentation\"])\npass_rate[\"pass_rate\"]=pass_rate[\"pass\"]\/pass_rate[\"id_assessment\"]\npass_rate.drop([\"pass\",\"id_assessment\"], axis=1,inplace=True)\npass_rate.head()","329aa8e3":"#Final exam scores\nstud_exams=pd.merge(studAss,exams,how=\"inner\",on=[\"id_assessment\"])\nstud_exams[\"exam_score\"]=stud_exams[\"score\"]\nstud_exams.drop([\"id_assessment\",\"date_submitted\",\"is_banked\", \"score\",\"assessment_type\",\"date\",\"weight\"],axis=1,inplace=True)\nstud_exams.head()","5360b661":"vle","ed3e78da":"vle[~vle[\"week_from\"].isna()]\n#Only 1121 from the 6364 entries have the reference week for the materials (the week in which they would be used in course.)\n#With this in mind, the construction of a metric to track study commitment becomes impractical","d168006d":"studVle.head()","81aebad3":"#Here we can track the average time after the start of the course the student took to use the materials\n#and the average amount of clicks per material\navg_per_site=studVle.groupby([\"id_student\",\"id_site\",\"code_module\",\"code_presentation\"]).mean().reset_index()\navg_per_site.head()","dfc3ea0a":"#General average per student per module\navg_per_student=avg_per_site.groupby([\"id_student\",\"code_module\",\"code_presentation\"]).mean()[[\"date\",\"sum_click\"]].reset_index()\navg_per_student.head()","570b45bc":"#Removing the cases where the student has withdrawn their registration to the module\nstudInfo=studInfo[studInfo[\"final_result\"]!=\"Withdrawn\"]\nstudInfo=studInfo[[\"code_module\",\"code_presentation\",\"id_student\",\"num_of_prev_attempts\",\"final_result\"]]\nstudInfo.head()","b38f3a3f":"df_1=pd.merge(avg_grade,pass_rate,how=\"inner\",on=[\"id_student\",\"code_module\",\"code_presentation\"])\nassessment_info=pd.merge(df_1, stud_exams, how=\"inner\", on=[\"id_student\",\"code_module\",\"code_presentation\"])\nassessment_info.head()","80dde3a1":"df_2=pd.merge(studInfo,assessment_info,how=\"inner\",on=[\"id_student\",\"code_module\",\"code_presentation\"])\nfinal_df=pd.merge(df_2,avg_per_student,how=\"inner\", on=[\"id_student\",\"code_module\",\"code_presentation\"])\nfinal_df.drop([\"id_student\",\"code_module\",\"code_presentation\"],axis=1,inplace=True)\nfinal_df.head()\n#The final dataframe only has information relevant to the problem","da2599a0":"final_df.describe()","32518c34":"final_df.info()","faff001a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(8,6))\nsns.heatmap(final_df.corr(),annot=True)","83360712":"plt.figure(figsize=(8,6))\nsns.countplot(data=final_df, x=\"final_result\")","712c9d4d":"sns.pairplot(final_df)","14f1c4fa":"final_df[final_df[\"sum_click\"]>10]","646dfa41":"final_df[final_df[\"num_of_prev_attempts\"]>4]","9331ab0c":"final_df=final_df[final_df[\"sum_click\"]<=10]\nfinal_df=final_df[final_df[\"num_of_prev_attempts\"]<=4]\nfinal_df.head()","20760ac9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.callbacks import EarlyStopping","a0c3ffaf":"X=final_df.drop(\"final_result\", axis=1)\ny=final_df[\"final_result\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","0323f605":"#1 contains both, 2 just pass_rate e 3 just weighted_grade\nX1_test=X_test\nX1_train=X_train\nX2_test=X_test.drop(\"weighted_grade\",axis=1)\nX2_train=X_train.drop(\"weighted_grade\",axis=1)\nX3_test=X_test.drop(\"pass_rate\",axis=1)\nX3_train=X_train.drop(\"pass_rate\",axis=1)","301f0da2":"scaler1=MinMaxScaler()\nscaler2=MinMaxScaler()\nscaler3=MinMaxScaler()","ed5513cb":"X1_train=scaler1.fit_transform(X1_train)\nX1_test=scaler1.transform(X1_test)\nX2_train=scaler2.fit_transform(X2_train)\nX2_test=scaler2.transform(X2_test)\nX3_train=scaler3.fit_transform(X3_train)\nX3_test=scaler3.transform(X3_test)","74d3bb24":"lr1=LogisticRegression(max_iter=10000)\nlr1.fit(X1_train,y_train)\nresult_lr1=lr1.predict(X1_test)\nprint(confusion_matrix(y_test,result_lr1))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lr1))","30cbfb08":"lr2=LogisticRegression(max_iter=10000)\nlr2.fit(X2_train,y_train)\nresult_lr2=lr2.predict(X2_test)\nprint(confusion_matrix(y_test,result_lr2))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lr2))","5d36a1aa":"lr3=LogisticRegression(max_iter=10000)\nlr3.fit(X3_train,y_train)\nresult_lr3=lr3.predict(X3_test)\nprint(confusion_matrix(y_test,result_lr3))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lr3))","556e0af8":"lda1=LinearDiscriminantAnalysis()\nlda1.fit_transform(X1_train,y_train)\nresult_lda1=lda1.predict(X1_test)\nprint(confusion_matrix(y_test,result_lda1))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lda1))","6adf0932":"lda2=LinearDiscriminantAnalysis()\nlda2.fit_transform(X2_train,y_train)\nresult_lda2=lda2.predict(X2_test)\nprint(confusion_matrix(y_test,result_lda2))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lda2))","bc68e066":"lda3=LinearDiscriminantAnalysis()\nlda3.fit_transform(X3_train,y_train)\nresult_lda3=lda3.predict(X3_test)\nprint(confusion_matrix(y_test,result_lda3))\nprint(\"\\n\")\nprint(classification_report(y_test,result_lda3))","ca75ab93":"rf1=RandomForestClassifier(n_estimators=300)\nrf1.fit(X1_train,y_train)\nresult_rf1=rf1.predict(X1_test)\nprint(confusion_matrix(y_test,result_rf1))\nprint(\"\\n\")\nprint(classification_report(y_test,result_rf1))","f78d3177":"rf2=RandomForestClassifier(n_estimators=300)\nrf2.fit(X2_train,y_train)\nresult_rf2=rf2.predict(X2_test)\nprint(confusion_matrix(y_test,result_rf2))\nprint(\"\\n\")\nprint(classification_report(y_test,result_rf2))","5cdd1ac5":"rf3=RandomForestClassifier(n_estimators=300)\nrf3.fit(X3_train,y_train)\nresult_rf3=rf3.predict(X3_test)\nprint(confusion_matrix(y_test,result_rf3))\nprint(\"\\n\")\nprint(classification_report(y_test,result_rf3))","3ae35b89":"model1=Sequential()\n\nmodel1.add(Dense(6, activation=\"relu\"))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(3, activation=\"relu\"))\nmodel1.add(Dense(1, activation=\"sigmoid\"))\n\nmodel1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","7754f8b8":"model2=Sequential()\n\nmodel2.add(Dense(5, activation=\"relu\"))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(3, activation=\"relu\"))\nmodel2.add(Dense(1, activation=\"sigmoid\"))\n\nmodel2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","2277546a":"model3=Sequential()\n\nmodel3.add(Dense(5, activation=\"relu\"))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(3, activation=\"relu\"))\nmodel3.add(Dense(1, activation=\"sigmoid\"))\n\nmodel3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","2b98ac8f":"#For the neural network training, the outputs needed to be codified, and in order to avoid the ordinalization\n#of the classes I chose to classify the distintion cases toghether with thw pass cases\ndef categories(cat):\n    if cat==\"Fail\":\n        return 0\n    if cat==\"Pass\":\n        return 1\n    if cat==\"Distinction\":\n        return 1\n    \ny_test=list(map(categories,y_test))\ny_train=list(map(categories,y_train))","4e338a96":"y_train=np.asarray(y_train)\ny_test=np.asarray(y_test)\nearly_stop=EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=25)","c3718db0":"model1.fit(x=X1_train, y=y_train, epochs=2000, validation_data=(X1_test,y_test),callbacks=[early_stop])","1dd8b407":"losses=pd.DataFrame(model1.history.history)\nlosses.plot()","5b1cf962":"predictions=model1.predict_classes(X1_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","034d0903":"model2.fit(x=X2_train, y=y_train, epochs=2000, validation_data=(X2_test,y_test),callbacks=[early_stop])","f05b1ac2":"losses=pd.DataFrame(model2.history.history)\nlosses.plot()","2a2125f5":"predictions=model2.predict_classes(X2_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","352c637c":"model3.fit(x=X3_train, y=y_train, epochs=2000, validation_data=(X3_test,y_test),callbacks=[early_stop])","491b878b":"losses=pd.DataFrame(model3.history.history)\nlosses.plot()","bd96079b":"predictions=model3.predict_classes(X3_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","72a67890":"# The first step towards the solution is identifying which data sources will be relevant to the problem.\n\nHere we have a schema (from https:\/\/analyse.kmi.open.ac.uk\/open_dataset) to illustrate the data structure of the set.\n\n![schema.PNG](attachment:schema.PNG)\n\nAs you can see, there are many different types of data involved, but since we want to make predictions about student performance it would be interesting to have:\n\n* A measurement of the students' commitment to the course throughout the period\n\n* A measurement of their performance during the period\n\n* Their final grades, as they are a huge part of the final grade composition\n\nGoing to the indicated website, we can see that this info is contained under the following tables:\n\n- studentInfo\n- studentAssessment\n- assessments\n- studentVle\n- vle\n\nThese tables will be our data sources for the model building process","5d873323":"# We have sucessfully developed models for the prediction of the student performance, but how can we choose one?\n\n* First of all, the models in which both *weighted_grade* and *pass_rate* were included overall performed better than their omitted counterparts, suggesting that our hypotheses was wrong\n\n* The neural network classifiers had difficulties predicting the cases of failure, but overall had a better performance tha the other models, probably due to the removal of one class.\n\n* The other models could be used in headhunting programs, developed to select students who are very likely to graduate with distinction and offering them scholarships, jobs, etc.\n\n* Altough I created a lot of features I wonder how many other features could be created for this problem and how would they improve model performance. If you are curious too, fork this notebook and give it a try too!\n\nThanks for reading my kernel and keep learning!","94ef0049":"# Model 3: Random Forest","44b124eb":"# Assessments\n\nThe performance in each assessement is a good indicator of the students' knowledge of the course and, as it composes the grade for the final evaluation, it's interesting to make it a feature in the final model. But, as there are many different courses, each with a different structure, it's unfeasible to create a feature for each assessment. In order to include the assessments, we will build 2 features: One of them is the final grade given by the score and the weight of each assessment. The other is a pass rate, created on the premise that a student must get at least 40% score on an assessment to pass it, calculating the percentage of assessments the student sucessfully passed. We also will split final exams from the other assessments, given their status and participation in the final evaluation is different from the other assessments.","7ecbac3d":"# Part 1: Feature Engineering\n\nHere we will discuss how we used the given data in order to create features that made sense in order to build the model","bed1a1df":"# Hi there!\n\nThis is my first complete project, developed from beggining to end with all steps usually followed throughout the development of a machine learning and data science project. I originally developed this as part of an interview process and I'll be adapting the notebook I created for that to this environment. If you enjoy this work or if it was useful to you please upvote! Thanks!","d04eaf59":"# Model 1: Logistic Regression","5b5c12db":"# Part 3: Modeling\n\nFor the modeling step we will use the following techniques and models:\n\n* Cross validation paired with classification reports and confusion matrices to evaluate model performance\n* Logistic Regression\n* Linear Discriminant Analysis\n* Random Forest\n* Neural Network Classifier","d2c560d9":"# With a \"Pass\" count much higher than the other labels, we must pay attention to the performance metrics fot the models and analyse the least represented cases more closely","bb5c91c5":"# The fact that the goal feature is categorical makes it not possible for us to include it in a correlation matrix, but we can see a tendency of correlation between the grading features (*weighted_grade, pass_rate* and *exam_score*)","2a7311ca":"# StudentInfo\n\nThe studentInfo table contains various info about the students, but the relevant ones for this analysis are:\n\n* The amount of times the student has already tried to finish the module\n* The students' final result\n\nThe last one is our interest variable as we build our prediction model","68bcabb5":"# Compiling all relevant tables","22af1cb8":"# Model 4: Neural Network Classifier","e32d0141":"# On the pairplot we can detect two outliers: One with an average click number way above average and another one with a sole occurrence of an amount of previous attempts. In order to keep our data as consistent as possible, these cases will be removed","9c459b90":"# Part 2: EDA\n\nWe start the exploratory data analysis by checking the dataframe integrity","6a579e64":"# Model 2: LDA","1f5e3cc5":"# VLE\n\nThe datasets referring to the VLE (Virtual Learning Environment) contain the interaction feed of the students with the content available for reference throughout the duration of the period. From this data we can infer how in touch a student was with their subjects, whether they studied it on a solid basis and how they used the content.","3be593d3":"# Given the high correlation between *weighted_grade* and *pass_rate* the models will be fit to 3 types of inputs: One with both features and one with each one of them removed from the dataset","e685f17e":"The project started by deciding which problem we were going to solve. You could choose between making a clustering model in order to profile the students or make a forecasting model to predict whether a student would pass or fail their modules.\n\nI chose the prediction modeling problem and the data given was the same as given here. You can also go to https:\/\/analyse.kmi.open.ac.uk\/open_dataset to find a more complete description of the data used here."}}