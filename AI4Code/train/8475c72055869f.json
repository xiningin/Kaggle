{"cell_type":{"4183888d":"code","c6a8c8e4":"code","5fbb66b7":"code","ddef2040":"code","7bca0ab2":"code","ffa60061":"code","5c5ec0ad":"code","990233f3":"code","6fbeb882":"code","772b447e":"code","e422390f":"code","08308049":"code","7b8d55c7":"code","f6e403ed":"code","8c20f553":"code","5ebcfc68":"code","7964dacc":"code","e5816491":"code","3bf39236":"code","208623cc":"code","a9fa68fc":"code","9d937f29":"code","3daad7b9":"code","a5c0888b":"code","17668d0b":"code","4aad0fe4":"code","cf5cfcd9":"code","a46bc568":"code","6c92e28c":"code","b57f6945":"code","fa27ff6b":"code","f75cdbc8":"code","60606f57":"code","c8eb059c":"code","01d78825":"code","0bd39c61":"code","b5076c0a":"code","44dd99f4":"code","4f068d85":"code","efbe8c59":"code","4ed39e86":"code","90e02f5c":"code","8922b2e2":"code","eacb2ae7":"code","15b41a55":"code","d949b9ab":"code","2a8777ee":"code","9ccb0c6f":"markdown","37ef8a8f":"markdown","742a6a37":"markdown","e641a922":"markdown","44301997":"markdown","f3889e64":"markdown","6e751d16":"markdown","a87d80c5":"markdown","442b4bfe":"markdown","9e7b6ea6":"markdown","e76a054e":"markdown","e4557e35":"markdown","46d9a192":"markdown","21516683":"markdown","e038ab16":"markdown","cadd6a73":"markdown","6ea4d245":"markdown","3e430c63":"markdown","7fcb24dc":"markdown","6e7c1908":"markdown","cda640f2":"markdown","c975ce60":"markdown","81bf9e7f":"markdown","e7592e58":"markdown"},"source":{"4183888d":"!pip install simple-colors","c6a8c8e4":"import numpy as np\nimport pandas as pd\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5fbb66b7":"#Setting up options\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","ddef2040":"# Load the data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","7bca0ab2":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print(\"Number of rows:\", colored(df.shape[0], 'green', attrs=['bold']))\n    print(\"Number of columns:\", colored(df.shape[1], 'green', attrs=['bold']))\n    \n    # missing values\n    print('\\n*******************')\n    print(cyan('Missing value checking', 'bold'))\n    print('*******************\\n')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green'))\n        print('*******************')\n    else:\n        print(colored('Missing value detected!', 'green', attrs=['bold']))\n        print(\"\\nTotal number of missing values:\", colored(sum(df.isna().sum()), 'green', attrs=['bold']))\n        \n        print('\\n*******************')\n        print(cyan('Missing values of features', 'bold'))\n        print('*******************\\n')\n        display(df.isna().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('Purples', axis = None))\n        print('\\n*******************')\n        print(cyan('Percentage of missing values of features', 'bold'))\n        print('*******************\\n')\n        display(round((df.isnull().sum() \/ (len(df.index)) * 100) , 3).sort_values(ascending = False).to_frame().rename({0:'%'}, axis = 1).T.style.background_gradient('PuBuGn', axis = None))\n\n        \n    # applying describe() method for categorical features\n    cat_feats = [col for col in df.columns if 'int' in str(df[col].dtype) and col not in ('id', 'target')]\n    print('\\n*******************')\n    print(cyan('Categorical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total categorical (binary) features:\", colored(len(cat_feats), 'green', attrs=['bold']))\n    display(df.describe())\n        \n        \n    # describe() for numerical features\n    cont_feats = [col for col in df.columns if 'float' in str(df[col].dtype) and col not in ('id', 'target')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['id', 'claim'], sort = False)]\n    display(df.describe())\n    \n    # Checking for duplicated rows -if any-\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates!', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found!', 'green', attrs=['bold']))\n        print('*******************')\n        display(df[df.duplicated()])\n\n    print('\\n*******************')\n    print(cyan('Preview of the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","ffa60061":"data_desc(train)","5c5ec0ad":"data_desc(test)","990233f3":"plt.figure(figsize=(10, 7))\nax = sns.countplot(y=train[\"target\"], palette='muted', zorder=3, linewidth=5, orient='h', saturation=1, alpha=1)\nax.set_title('Distribution of Target', fontname = 'Times New Roman', fontsize = 30, color = '#8c49e7', x = 0.5, y = 1.05)\nbackground_color = \"#8c49e7\"\nsns.set_palette(['#ffd514']*120)\n\nfor a in ax.patches:\n    value = f'Amount and percentage of values: {a.get_width():,.0f} | {(a.get_width()\/train.shape[0]):,.3%}'\n    x = a.get_x() + a.get_width() \/ 2 - 230000\n    y = a.get_y() + a.get_height() \/ 2 \n    ax.text(x, y, value, ha='left', va='center', fontsize=18, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=0.7))\n\n\n# ax.margins(-0.12, -0.12)\nax.grid(axis=\"x\")\n\nsns.despine(right=True)\nsns.despine(offset=15, trim=True)","6fbeb882":"categorical_features =[]\nnumerical_features =[]\n\nfor col in train.columns:\n    if train[col].dtype == 'int64' and col not in ('id', 'target'):\n        categorical_features.append(col)\n    elif train[col].dtype != 'int64' and col not in ('id', 'target'):\n        numerical_features.append(col)\nprint('Catagoric features: ', categorical_features)\nprint()\nprint('Numerical features: ', numerical_features)","772b447e":"# Cardinality check\n\nprint(colored(\"In Train Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(train[col].nunique(), col))\n\nprint()\nprint(colored(\"In Test Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(test[col].nunique(), col))","e422390f":"def cardinality(data):\n    for k in categorical_features:\n        print(f'{k}\\n{(np.round((data[k].value_counts() \/ len(data[k]))*100,3))}\\n')","08308049":"cardinality(train)","7b8d55c7":"cardinality(test)","f6e403ed":"fig = go.Figure([go.Bar(x = train[categorical_features].nunique().index, y = train[categorical_features].nunique().values, marker_color='rgb(100, 14, 175)')])\n#fig.show()\n\nfig.update_traces(marker_line_color='rgb(120, 15, 155)', marker_line_width=1, opacity=0.7)\n\nfig.update_layout(\n    title=\"<b>Number of unique values of categorical features<b>\",\n    width=1600,\n    height=900,\n    \n    xaxis = dict(showline=True,\n    title = '<b>Categorical Variables<b>',\n    tickangle = -30,\n    tickfont = dict(family='Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    ),\n\n    yaxis = dict(showline=True,\n    ticks = \"outside\", tickwidth=2, tickcolor='red', ticklen=7.5,\n    title = '<b># of unique values<b>',\n    tickfont = dict(family = 'Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    title_standoff = 5,\n    ),\n    bargap = 0.50, # gap between bars of adjacent location coordinates.   \n)","8c20f553":"def count_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 80))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x = feature, palette='rocket_r', data=data, hue=None)\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        for p in ax.patches:\n            height = p.get_height()\n            value = f'{p.get_height():,.0f} | {(p.get_height()\/data[feature].shape[0]):,.3%}'\n            ax.text(p.get_x()+p.get_width()\/2., height+5000, value, ha=\"center\", fontsize = 12, fontweight = 'bold')     \n        i += 1\n    \n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'darkorange')\n    plt.show()    ","5ebcfc68":"count_plot(train, categorical_features, 'Categorical features of train dataset', hue=None)","7964dacc":"count_plot(test, categorical_features, 'Categorical features of train dataset', hue=None)","e5816491":"def count_plot_testtrain(data1, data2, features, titleText):\n  \n    L = len(features)\n    nrow= int(np.ceil(L\/4))\n    ncol= 5\n    remove_last= (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 80))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#61057c', data=data1, label='train')         \n        ax = sns.countplot(x=feature, color='#b7f035', data=data2, label='test')\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        ax = ax.legend(loc = \"best\", fontsize = 12)\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'indigo')\n    plt.show()","3bf39236":"count_plot_testtrain(train, test, categorical_features, titleText = 'Categorical features of train & test datasets')","208623cc":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","a9fa68fc":"correlation_matrix(train, categorical_features)","9d937f29":"correlation_matrix(test, categorical_features)","3daad7b9":"def box_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        v0 = sns.color_palette(palette = \"pastel\").as_hex()[2]\n        ax = sns.boxplot(x = data[feature], color=v0, saturation=.75)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Values', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    plt.show()","a5c0888b":"box_plot(train, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","17668d0b":"box_plot(test, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","4aad0fe4":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    \n    plt.show()","cf5cfcd9":"train_frac = train.sample(frac = 0.01).reset_index(drop = True)\n\nkde_plot(train_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","a46bc568":"test_frac = test.sample(frac = 0.01).reset_index(drop = True)\n\nkde_plot(test_frac, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","6c92e28c":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","b57f6945":"correlation_matrix(train, numerical_features)","fa27ff6b":"correlation_matrix(test, numerical_features)","f75cdbc8":"def hist_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.histplot(data[feature], edgecolor=\"black\", color=\"darkseagreen\", alpha=0.7)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Frequency', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","60606f57":"hist_plot(train_frac, numerical_features, titleText = 'Histogram of Numerical Features of Train Dataset', hue = None)","c8eb059c":"hist_plot(test_frac, numerical_features, titleText = 'Histogram of Numerical Features of Test Dataset', hue = None)","01d78825":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 150))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n        \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Sample Quantile', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","0bd39c61":"qqplot(train_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","b5076c0a":"qqplot(test_frac, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","44dd99f4":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","4f068d85":"normality_check(train)","efbe8c59":"normality_check(test)","4ed39e86":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","90e02f5c":"# Detect all Outliers \noutliers = detect_outliers(train['target'])\nprint(\"Total Outliers count for claim : \", len(outliers[0]))\n\nprint(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\nprint(\"Shape after removing outliers : \",train.shape)","8922b2e2":"train_iqr = pd.DataFrame()\ntrain_iqr.reindex(columns=[*train_iqr.columns.tolist(), \"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"], fill_value = 0)","eacb2ae7":"from scipy.stats import iqr\n\ndata = []\n\nk = 0\ncolumns = [\"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"]\n\nfor i in numerical_features:\n\n    q1 = train[i].quantile(0.25)\n    q3 = train[i].quantile(0.75)\n    \n    iqr = (q3 - q1)\n    lob_1 = q1 - (iqr * 1.5)\n    uob_1 = q3 + (iqr * 1.5)\n    lob_3 = q1 - (iqr * 3)\n    uob_3 = q3 + (iqr * 3)\n    \n    number_uob_1 = f'{round(sum(train[numerical_features[k]] > uob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_1 = f'{round(sum(train[numerical_features[k]] < lob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_uob_3 = f'{round(sum(train[numerical_features[k]] > uob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_3 = f'{round(sum(train[numerical_features[k]] < lob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n\n    values = [number_lob_3, number_lob_1, number_uob_1, number_uob_3]\n    zipped = zip(columns, values)\n    a_dictionary = dict(zipped)\n    print(a_dictionary)\n    data.append(a_dictionary)\n    \n    k = k + 1","15b41a55":"train_iqr = train_iqr.append(data, True)\ntrain_iqr.set_axis([numerical_features], axis='index')","d949b9ab":"def colour(value):\n\n    if float(value.strip('%')) > 10:\n      color = 'red'\n    elif float(value.strip('%')) > 5:\n        color = 'darkorange'   \n    else:\n      color = 'green'\n\n    return 'color: %s' % color\n\n# train_iqr = train_iqr.set_axis([numerical_features], axis='index')\ntrain_iqr = train_iqr.style.applymap(colour)","2a8777ee":"train_iqr","9ccb0c6f":"<a id=\"numerical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Numerical Variables<\/p>","37ef8a8f":"<a id=\"hist_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.4. Histogram Plot of Numerical Variables<\/p>","742a6a37":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.3. Normality Check and Outlier Detection<\/p>","e641a922":"* **The logic in the KDE plots is also executed in the histogram plots.**","44301997":"* **Since KDE plots are processed in a long time, plots were created on 1% of the data sets. Supporting the box chart, it can be seen from this chart that there are various outliers.**","f3889e64":"* **It is very obvious that some features contain significant amount of outlier value in both data sets. This has to be handled.**","6e751d16":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\n* **All numerical and categorical variables will be explored in this section.**","a87d80c5":"<a id=\"mild_extreme_outlier\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.3.1. Mild and Extreme Outlier Detection<\/p>","442b4bfe":"<a id=\"no_cat_features\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.1. Number of Categorical Variables<\/p>","9e7b6ea6":"<a id=\"corr_categorical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.2. Correlation Matrix of Categorical Variables<\/p>","e76a054e":"<a id=\"corr_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.3. Correlation Matrix of Numerical Variables<\/p>","e4557e35":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\n* **Loading packages and importing some helpful libraries.**","46d9a192":"* **Obviously, since the target variable is 0-1 (binary), there is no outlier value for this variable. There are many outliers for other features, but no direct data dropping is done in order not to lose an enormous number of rows.** ","21516683":"* **Since there is a large amount of data, it may make sense to randomly take some of the data and create the plots.**","e038ab16":"<a id=\"qq_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.5. Q-Q Plot of Numerical Variables<\/p>","cadd6a73":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\n* **First of all, some setting up options were made. It is aimed to show all rows and columns in order to improve the general view of data sets. Next, I will load the train and test data sets and display train and test data sets as well.**","6ea4d245":"* **There is no significant correlation between categorical variables in both train and test dataset. No correlation between variables is even greater than 0.01. Additionally, the relationships between the variables are similar in both data sets.**","3e430c63":"<a id=\"box_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.1. Box Plot of Numerical Variables<\/p>","7fcb24dc":"* **There is no significant correlation between numerical variables in both train and test dataset.**","6e7c1908":"<a id=\"kde_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.2. KDE Plot of Numerical Variables<\/p>","cda640f2":"* **This dataframe about how to manage outlier values during the feature engineering section while developing the model will be very helpful.** ","c975ce60":"<a id=\"categorical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Categorical Variables<\/p>","81bf9e7f":"## <p style=\"background-color:#3a2c57; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Categorical Variables](#categorical_variables)\n        * 2.1.1. [Number of Categorical Variables](#no_cat_features)\n        * 2.1.2. [Correlation Matrix of Categorical Variables](#corr_categorical_variables)\n    * 2.2. [Numerical Variables](#numerical_variables)\n        * 2.2.1. [Box Plot of Numerical Variables](#box_numerical_variables)\n        * 2.2.2. [KDE Plot of Numerical Variables](#kde_numerical_variables)\n        * 2.2.3. [Correlation Matrix of Numerical Variables](#corr_numerical_variables)\n        * 2.2.4. [Histogram Plot of Numerical Variables](#hist_numerical_variables)\n        * 2.2.5. [Q-Q Plot of Numerical Variables](#qq_numerical_variables)\n    * 2.3. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n       * 2.3.1. [Mild and Extreme Outlier Detection](#mild_extreme_outlier)","e7592e58":"* **The Q-Q plot with clues to the normal distribution also shows tremendously that the data is not normally distributed.**"}}