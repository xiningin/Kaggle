{"cell_type":{"d6f5bea4":"code","063ebc77":"code","3e8e5234":"code","d1783404":"code","4ebfed86":"code","aabeee93":"code","eb2e8367":"code","0b8694fa":"code","345dab2d":"code","ba5461af":"code","2e2e8141":"code","269e467d":"code","8a88261f":"code","fcc857a9":"code","5bbe272f":"code","e84165da":"code","45db9e83":"code","3fb1369b":"code","af5545f8":"code","90951615":"code","7d177440":"code","1d896c16":"code","872565d5":"code","396700d0":"code","e367e3c0":"code","2642c541":"code","4582eeb3":"code","cbbc1776":"code","13929a1b":"code","ab361773":"code","3d77a7fc":"code","1a4ee6ce":"code","7fabf500":"code","2723b500":"code","acfc6c43":"code","a5f38ee6":"code","b5aac42d":"code","e1d1372d":"code","9cc6d19d":"code","7cb2d896":"code","295e2f47":"code","a9b4de71":"code","4568d214":"code","1a0fa5d0":"code","bf1fa748":"code","22f22e61":"code","b2e0b413":"code","c0e6c1fc":"code","288e0290":"code","f6c62a8b":"code","c4b0a983":"code","3c4f0fb9":"code","954753c4":"code","e6defea5":"markdown","fa7aac2a":"markdown","b26cf942":"markdown","31585936":"markdown","9374f4c9":"markdown","00a2e7f0":"markdown","a39294e8":"markdown","fded17cc":"markdown","360ae0f4":"markdown","e18926c8":"markdown"},"source":{"d6f5bea4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\nimport gc\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\nfrom keras.layers import Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler","063ebc77":"# load data\ntrain = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\")\ncalendar = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nsell_prices = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")","3e8e5234":"train.shape,calendar.shape,sell_prices.shape","d1783404":"train.info()","4ebfed86":"calendar.info()","aabeee93":"train.head()","eb2e8367":"train.drop(['item_id','dept_id','cat_id','store_id','state_id'],1,inplace=True)","0b8694fa":"calendar.head()","345dab2d":"sell_prices.head()","ba5461af":"train.isnull().sum().sort_values(ascending = False)","2e2e8141":"calendar.isnull().sum().sort_values(ascending = False)","269e467d":"#Downcast in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","8a88261f":"train = downcast(train)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","fcc857a9":"for i in range(1942,1970):\n    col = \"d_\"+ str(i)\n    train[col] = 0","5bbe272f":"train_new = train.T\ntrain_new.shape","e84165da":"train_new = train_new[1:]\ntrain_new","45db9e83":"train_new.shape","3fb1369b":"sc = MinMaxScaler(feature_range = (0, 1))\ntrain_new = sc.fit_transform(train_new)","af5545f8":"len(train_new)","90951615":"# training data, from 0 to 1913\nX = []\nlookup = 14\nfor i in range(0,1899): \n    X.append(train_new[i:i+lookup])","7d177440":"j=0\ny=[]\nfor i in range(lookup,1913):    \n    y.append(train_new[i][0:30490])\nprint(len(y))","1d896c16":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","872565d5":"X_train = np.array(X_train, dtype = 'float16')\ny_train = np.array(y_train, dtype = 'float16')","396700d0":"X_train.shape,y_train.shape","e367e3c0":"from keras.layers import LSTM\nfrom keras.layers import GRU\n\nmodel = Sequential()\n\nmodel.add(GRU(64,input_shape=(np.array(X_train).shape[1], np.array(X_train).shape[2]),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(64,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(64,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(30490))\n","2642c541":"model.compile(\n  loss='mse',\n  metrics=[tf.keras.metrics.MeanSquaredError()],\n  optimizer=tf.keras.optimizers.Adam(0.001)\n)\nmodel.summary()","4582eeb3":"history = model.fit(X_train, y_train,\nepochs=20,\nbatch_size=10,\nvalidation_split=0.2)","cbbc1776":"acc = history.history['mean_squared_error']\nval_acc = history.history['val_mean_squared_error']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","13929a1b":"from sklearn.metrics import mean_squared_error\n\nX_test_1 = np.array(X_test[0:14])\ny_test_1 = np.array(y_test[0:14])\n\ny_test_pred = np.array(model.predict(X_test_1))\n\ny_test_pred = sc.inverse_transform(y_test_pred)\ny_test_1 = sc.inverse_transform(y_test_1)\n\nmean_squared_error(y_test_1,y_test_pred)","ab361773":"# validation data, from d_1914 - d_1941\nval = []\nlookup = 14\nfor i in range(1913,1941):  \n    val.append(train_new[i:i+lookup])","3d77a7fc":"np.array(val).shape","1a4ee6ce":"np.array(y_val).shape","7fabf500":"val1 = np.array(val[:14])\nval1.shape","2723b500":"val2 = np.array(val[14:])\nval2.shape","acfc6c43":"y_val1_pred = model.predict(val1)\nnp.array(y_val1_pred).shape","a5f38ee6":"y_val2_pred = model.predict(val2)\nnp.array(y_val2_pred).shape","b5aac42d":"y_val_pred = np.concatenate((y_val1_pred,y_val2_pred))\nnp.array(y_val_pred).shape","e1d1372d":"y_val_pred = sc.inverse_transform(y_val_pred)","9cc6d19d":"test =y_val_pred.reshape(28,1,30490)\nnp.array(test).shape","7cb2d896":"test1 = test[0:14]\nnp.array(test1).shape","295e2f47":"test2 = test[14:]\nnp.array(test2).shape","a9b4de71":"test_pred1 = model.predict(test1)","4568d214":"test_pred2 = model.predict(test2)","1a0fa5d0":"test_pred = np.concatenate((test_pred1,test_pred2))\nnp.array(test_pred).shape","bf1fa748":"test_pred = sc.inverse_transform(test_pred)","22f22e61":"sub = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\nsub.head()","b2e0b413":"sub_ids = sub.id\nsub.drop('id',1,inplace=True)","c0e6c1fc":"sub.shape","288e0290":"sub[0:30490] = test_pred[0:30490].T","f6c62a8b":"sub[30490:60980] = test_pred[0:30490].T","c4b0a983":"sub.insert(loc=0, column='id', value=sub_ids)","3c4f0fb9":"sub.head()","954753c4":"sub.to_csv('submission.csv',index=False)","e6defea5":"Lots of zeros above shows particular item was either not sold on that particular day or was not in stock","fa7aac2a":"# Prediction on X_test","b26cf942":"# Let's take a sneak peek of the data","31585936":"# Introduction\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin.\n\n# Task in hand\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the test set.\n\nThe rows  [d_1914 - d_1941] form the validation set.\n\nRemaining rows form the training set.\n\n    This notebook covers Modelling only, to check EDA, check https:\/\/www.kaggle.com\/jagdmir\/m5-forecasting-part-one-eda.","9374f4c9":"# Submission","00a2e7f0":"# Predict for test time frame","a39294e8":"# Predict for validation time frame","fded17cc":"# Check Null Values","360ae0f4":"# Load Data","e18926c8":"# Memory Usage Reduction\n\nWe need to melt the dataset in order to proceed further. but before we do that, we need to reduce the memory usage. if we dont reduce memory usage, we may get memory usage errors."}}