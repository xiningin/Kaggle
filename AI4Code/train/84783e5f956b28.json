{"cell_type":{"7a177cd7":"code","b6fddc45":"code","812d4817":"code","2dd00c8d":"code","c5eb8efa":"code","b42d3164":"code","ba991a5b":"code","032974c9":"code","c4d995bd":"code","7254d123":"code","e5cf841a":"code","1f0631c9":"code","04b9c86e":"code","f54faa94":"code","be421e7a":"code","cc1e1011":"code","ba012f2f":"code","c7af214d":"code","b09a36e3":"code","cb7c5e2a":"code","71f308b1":"code","917d2510":"code","dbc68ff3":"code","4948ad5a":"code","34421a50":"code","a1260594":"code","37a9df48":"code","d3eac8d4":"code","b7c4d510":"code","c3c9c252":"code","41391f2a":"code","f11b670f":"code","44a97f67":"code","d1ce7e48":"code","730314ca":"code","dd7b0aab":"code","eca09afa":"code","f3d6f379":"code","70fcea37":"markdown","7debd41c":"markdown","59cdc0cc":"markdown","10a41bab":"markdown","98551449":"markdown","3bda601f":"markdown","35b1dea9":"markdown","79fea3ca":"markdown","b29b6763":"markdown","c86a8585":"markdown","8c4b9911":"markdown","37fc3a3c":"markdown","acc0d30c":"markdown","568dbec9":"markdown","1d020d62":"markdown","c2143a78":"markdown","71b025f2":"markdown","394e5d42":"markdown","67b7e7c0":"markdown","fc160d3f":"markdown","9c46887f":"markdown","bd9c852b":"markdown","6c59c821":"markdown","c49287a7":"markdown","345ab157":"markdown","33eb9494":"markdown","faaae5a8":"markdown","d77c845c":"markdown"},"source":{"7a177cd7":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nimport os","b6fddc45":"image_dataset_path = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset'\ncaption_dataset_path = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_text\/Flickr8k.token.txt'","812d4817":"from IPython.display import Image\nImage('..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset\/1000268201_693b08cb0e.jpg')","2dd00c8d":"import tensorflow as tf\n\n# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","c5eb8efa":"# load the caption file & read it\ndef load_caption_file(path):\n    \n    # dictionary to store captions\n    captions_dict = {}\n    \n    # iterate through the file\n    for caption in open(path):\n    \n        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n        tokens = caption.split()\n        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n        caption_text = ' '.join(caption_text)\n        \n        # save it in the captions dictionary\n        if caption_id not in captions_dict:\n            captions_dict[caption_id] = caption_text\n        \n    return captions_dict\n\n# call the function\ncaptions_dict = load_caption_file(caption_dataset_path)","b42d3164":"# clean the captions\nimport string\n\n# dictionary to store the cleaned captions\nnew_captions_dict = {}\n\n# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\ntable = str.maketrans('', '', string.punctuation)\n\n# loop through the dictionary\nfor caption_id, caption_text in captions_dict.items():\n    # tokenize the caption_text\n    caption_text = caption_text.split()\n    # convert it into lower case\n    caption_text = [token.lower() for token in caption_text]\n    # remove punctuation from each token\n    caption_text = [token.translate(table) for token in caption_text]\n    # remove all the single letter tokens like 'a', 's'\n    caption_text = [token for token in caption_text if len(token)>1]\n    # store the cleaned captions\n    new_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'\n    ","ba991a5b":"# delete unwanted \ndel captions_dict","032974c9":"print('\"' + list(new_captions_dict.keys())[0] + '\"' + ' : ' + new_captions_dict[list(new_captions_dict.keys())[0]])","c4d995bd":"len(new_captions_dict)","7254d123":"caption_images_list = []\n\nimage_index = list(new_captions_dict.keys())\n\ncaption_images_list = [ image.split('.')[0] for image in os.listdir(image_dataset_path) if image.split('.')[0] in image_index ]","e5cf841a":"caption_images_list[0]","1f0631c9":"len(caption_images_list)","04b9c86e":"train_validate_images = caption_images_list[0:8081]  ","f54faa94":"test_images = caption_images_list[8081:8091]\ntest_images","be421e7a":"# extract features from each photo in the directory\ndef extract_features(directory, image_keys):\n    # load the model\n    model = InceptionV3()\n    \n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    \n    # summarize\n    print(model.summary())\n    \n    # extract features from each photo\n    features = dict()\n    \n    for name in image_keys:\n        \n        # load an image from file\n        filename = directory + '\/' + name + '.jpg'\n        \n        # load the image and convert it into target size of 299*299\n        image = load_img(filename, target_size=(299, 299))\n        \n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        \n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        \n        # prepare the image for the inceptionV3 model\n        image = preprocess_input(image)\n        \n        # get features\n        feature = model.predict(image, verbose=0)\n        \n        # get image id\n        image_id = name.split('.')[0]\n        \n        # store feature\n        features[image_id] = feature\n        \n#         print('>%s' % name)\n        \n\n    return features","cc1e1011":"# extracting image features for train_validate_images\ntrain_validate_features = extract_features(image_dataset_path, train_validate_images)","ba012f2f":"print(\"{} : {}\".format(list(train_validate_features.keys())[0], train_validate_features[list(train_validate_features.keys())[0]] ))","c7af214d":"len(train_validate_features)","b09a36e3":"from pickle import dump\ndump(train_validate_features, open('.\/train_validate_features.pkl', 'wb'))","cb7c5e2a":"from pickle import load\ntrain_validate_features = load(open('..\/input\/pickle-file\/train_validate_features.pkl', 'rb'))","71f308b1":"# load libraries\nimport numpy as np\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense, Dropout, LSTM, Embedding, RepeatVector, TimeDistributed, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical, plot_model\nfrom keras.layers.merge import add, concatenate\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","917d2510":"# make a dictionary of image with caption for train_validate_images\ntrain_validate_image_caption = {}\n\nfor image, caption in new_captions_dict.items():\n    \n    # check whether the image is available in both train_validate_images list and train_validate_features dictionary\n    if image in train_validate_images and image in list(train_validate_features.keys()):\n        \n         train_validate_image_caption.update({image : caption})\n\nlen(train_validate_image_caption)","dbc68ff3":"list(train_validate_image_caption.values())[2]","4948ad5a":"from IPython.display import Image\nImage(image_dataset_path+'\/'+list(train_validate_image_caption.keys())[2]+'.jpg')","34421a50":"# initialise tokenizer\ntokenizer = Tokenizer()\n\n# create word count dictionary on the captions list\ntokenizer.fit_on_texts(list(train_validate_image_caption.values()))\n\n# how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0 \nvocab_len = len(tokenizer.word_index) + 1\n\n# store the length of the maximum sentence\nmax_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n\nprint(\"vocab_len \", vocab_len)\nprint(\"max_len \", max_len)\n\ndef prepare_data(image_keys):\n    \n    # x1 will store the image feature, x2 will store one sequence and y will store the next sequence\n    x1, x2, y = [], [], []\n\n    # iterate through all the images \n    for image in image_keys:\n\n        # store the caption of that image\n        caption = train_validate_image_caption[image]\n\n        # split the image into tokens\n        caption = caption.split()\n\n        # generate integer sequences of the\n        seq = tokenizer.texts_to_sequences([caption])[0]\n\n        length = len(seq)\n\n        for i in range(1, length):\n\n            x2_seq, y_seq = seq[:i] , seq[i]  \n\n            # pad the sequences\n            x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]\n\n\n            # encode the output sequence                \n            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n\n            x1.append( train_validate_features[image][0] )\n\n            x2.append(x2_seq)\n\n            y.append(y_seq)\n               \n    return np.array(x1), np.array(x2), np.array(y)","a1260594":"train_x1, train_x2, train_y = prepare_data( train_validate_images[0:7081] )\nvalidate_x1, validate_x2, validate_y = prepare_data( train_validate_images[7081:8081] )","37a9df48":"len(train_x1)","d3eac8d4":"embedding_size = 300\nlstm_units = 256\n\nwith strategy.scope():\n\n    # feature extractor model\n    image_input = Input(shape=(2048,))\n    image_model_1 = Dense(embedding_size, activation='relu')(image_input)\n    image_model = RepeatVector(max_len)(image_model_1)\n\n    # sequence model\n    caption_input = Input(shape=(max_len,))\n    caption_model_1 = Embedding(vocab_len, embedding_size, mask_zero=True)(caption_input)\n    caption_model_2 = LSTM(lstm_units, return_sequences=True)(caption_model_1)\n    caption_model = TimeDistributed(Dense(embedding_size))(caption_model_2)\n\n    # decoder model\n\n    merged = add([image_model, caption_model])\n    decoder1 = Bidirectional(LSTM(lstm_units, return_sequences=False))(merged)\n\n    outputs = Dense(vocab_len, activation='softmax')(decoder1)\n\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[image_input, caption_input], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# summarize model\nprint(model.summary())","b7c4d510":"plot_model(model, to_file='model.png', show_shapes=True)","c3c9c252":"# define checkpoint callback\nfilepath = '.\/new_model-ep{epoch:02d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n\ncallbacks = [ ModelCheckpoint(filepath= filepath, save_best_only=True, monitor='val_loss', mode='min') ]\n","41391f2a":"print(\"shape of train_x1 \", train_x1.shape)\nprint(\"shape of train_x2 \", train_x2.shape)\nprint(\"shape of train_y \", train_y.shape)\nprint()\nprint(\"shape of validate_x1 \", validate_x1.shape)\nprint(\"shape of validate_x2 \", validate_x2.shape)\nprint(\"shape of validate_y \", validate_y.shape)","f11b670f":"# fit model\n# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\n# Define training epochs\nEPOCHS = 20\n\nhistory = model.fit([train_x1, train_x2],  \n                    train_y,              \n                    verbose = 1,            \n                    epochs = EPOCHS,\n                    batch_size = BATCH_SIZE,\n                    callbacks = callbacks, \n                    validation_data=([validate_x1, validate_x2], validate_y)) ","44a97f67":"# plot training loss and validation loss\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","d1ce7e48":"# saving the model with last parameter \nmodel.save('.\/fork_latest_model.h5')","730314ca":"# extract features from each photo in the directory\ndef extract_feat(filename):\n    # load the model\n    model = InceptionV3()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # load the photo\n    image = load_img(filename, target_size=(224, 224))\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    # reshape data for the model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the image for the VGG model\n    image = preprocess_input(image)\n    # get features\n    feature = model.predict(image, verbose=0)\n    return feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizr):\n    for word, index in tokenizr.word_index.items():\n        if index == integer:\n            return word\n    return None\n ","dd7b0aab":"# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","eca09afa":"# load the model\nmodl = load_model('.\/new_model-ep01-loss2.671-val_loss4.697.h5')\n\n# generate description\ntokenizr = Tokenizer()\ntokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in train_validate_images])\nmax_length = max_len\n\nfor count in range(10):\n\n    photo = extract_feat('{}.jpg'.format(image_dataset_path+'\/'+train_validate_images[count]))  \n\n    # generate description\n    description = generate_desc(modl, tokenizr, photo, max_length)\n    print(\"Predicted caption -> \", description)\n    print()\n    print(\"Actual caption -> \", new_captions_dict[train_validate_images[count]])\n    print('*********************************************************************')\n    print()","f3d6f379":"# load the model\nmodl = load_model('.\/new_model-ep01-loss2.671-val_loss4.697.h5')\n\n# generate description\ntokenizr = Tokenizer()\ntokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in test_images])\nmax_length = 30\n\nfor count in range(10):\n\n    photo = extract_feat('{}.jpg'.format(image_dataset_path+'\/'+test_images[count]))  \n\n    # generate description\n    description = generate_desc(modl, tokenizr, photo, max_length)\n    print(\"Predicted caption -> \", description)\n    print()\n    print(\"Actual caption -> \", new_captions_dict[test_images[count]])\n    print('*********************************************************************')\n    print()\n","70fcea37":"## Model architecture","7debd41c":"# Make training, validation and test data","59cdc0cc":"## Plot the model architecture","10a41bab":"### taking 7081 images for training, 1000 for validation and rest 10 for testing","98551449":"But still model has performed very bad, lets tune model to perform better","3bda601f":"## Evaluating model on training images using the latest model","35b1dea9":"## Reducing Overfitting","79fea3ca":"**extract_features** function extracts the important features out of the images passed using VGG16 model.","b29b6763":"## Preparing the input data","c86a8585":"# Evaluate the model  - Part 1","8c4b9911":"# Data Preparation","37fc3a3c":"We can see that right from epoch 6, validation loss is increasing while training loss is decreasing which means that the model is overfitting. \n\nTo overcome this overfitting, following techniques are used:-\n\n1) Regularization \n\n2) Cross Validation\n    \n3) Generalisation   - Adding more data\n\n4) Data Augmentation  - modifying the present data ","acc0d30c":"## Train the model using training data and validation data","568dbec9":"### Make sure feature data and target data share the same first dimension","1d020d62":"# Introduction","c2143a78":"## Make a list of only those images who has caption","71b025f2":"This Notebook shows how to make a deep learning Model for generating caption for any given image. The model will have three parts:\n\n**Image Feature Extractor** - 16 layer VGG model which is pretrained on ImageNet dataset. We will remove the outer 2 dense layer from the architecture and will use the feature extractor part of VGG to preprocess our image dataset.\n\n**Caption Processor** - This model will have one embedding layer with LSTM to process our captions text.\n\n**Image_Caption_Combination** - We will prepare the input dictionary to feed to the final model by mapping the image and text.\n\n**Final Model** - This is the final model in which we input the image features extracted by the \"Image Feature Extractor\" and text features extracted by the \"Caption Processor\" and train it to generate captions for any given image.\n\nWe will go step by step in training the model from worst to best. \n","394e5d42":"\n\n1) Convert the captions into lowercase\n\n2) Tokenize the captions into different tokens\n\n3) Remove all the punctuations from the tokens\n\n4) add \"start_index\" and \"end_index\" as pointers to tell the model start of the caption and end of the caption","67b7e7c0":"# Image Feature Extractor","fc160d3f":"### make sure the correct caption is mapped with the correct image","9c46887f":"Each caption will be split into words. The model will be provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. This is how the model will be trained. So we will have two features, x1 (image) , x2 (text_sequence) and one target variable, y (generated_word).\n\nImage,\t\t   text sequence, \t\t\t\t\t\tgenerated_word\n\nphoto\t         startseq, \t\t\t\t\t\t\t\t\tlittle\n\nphoto\t       startseq, little,\t\t\t\t\t\t\tgirl\n\nphoto\t       startseq, little, girl, \t\t\t\t\trunning\n\nphoto\t       startseq, little, girl, running, \t\t\tin\n\nphoto\t       startseq, little, girl, running, in, \t\tfield\n\nphoto\t       startseq, little, girl, running, in, field, endseq","bd9c852b":"## Evaluating model on test images using the latest model","6c59c821":"# Final Model","c49287a7":"## ALERT: Following piece of code takes time ","345ab157":"# Preprocess the captions","33eb9494":"# Configuring TPU","faaae5a8":"# Training Model - Part 1 ","d77c845c":"# Caption Processor"}}