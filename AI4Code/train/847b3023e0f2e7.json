{"cell_type":{"fb5c35d1":"code","3a13a6a8":"code","be71dea4":"code","c6fc5313":"code","36de5410":"code","23b01c91":"code","a3664560":"code","e73b36dc":"code","ec2dbea0":"code","807d8f3b":"code","24d79081":"code","220185ee":"code","73282db8":"code","4e96c0b7":"code","92891886":"code","d7edf9eb":"code","a1479a0c":"code","eb5fed7a":"code","1a47bc46":"code","8dab7bec":"code","0fa4008b":"code","77dae092":"code","0629d52b":"code","d31b9e28":"code","a57e4da1":"code","970988b9":"code","953f0a96":"code","bab436ea":"code","98a46ef3":"code","df3008c7":"code","40f702f3":"code","40e71c74":"code","9b38ee2e":"code","61df105c":"code","225b0946":"code","b6b5981b":"code","c10b263b":"code","ba2c7c55":"code","3f55e8d8":"code","5a73ed68":"code","0b6c223c":"code","449dc117":"code","878f3120":"code","57350ced":"code","6725bdd2":"code","ca81c5a2":"code","ff5be3b2":"code","51676b61":"code","c30326c5":"code","708c80ca":"code","0f7cbf5d":"code","dbfe82e2":"code","e0a544a2":"code","f42bb2b8":"code","6a2a654b":"code","06974273":"code","2429093d":"code","b1168109":"code","3f6e8f3a":"code","7693e970":"code","4a07aaaa":"code","38192e08":"code","6a454538":"code","8ce592a5":"code","f3a3fefb":"code","02922ed9":"code","0c20be39":"code","aa3f434a":"code","2984832e":"code","ebb5c8cc":"code","8e1454c4":"code","366a0ae9":"code","82fb9506":"code","45e3d1b1":"markdown","870f14bf":"markdown","d25da504":"markdown","49130d66":"markdown","cd7cd5d7":"markdown","7775388f":"markdown","643ae111":"markdown","82555f21":"markdown","94701d9a":"markdown","ea27cd76":"markdown","8eb0fc1e":"markdown","3f5384d1":"markdown","ce860a09":"markdown","903a03b1":"markdown","a94af788":"markdown","7b842390":"markdown","248ed00a":"markdown","598cc70c":"markdown","ff3a97bd":"markdown","2c6c57bc":"markdown","fe3afc87":"markdown","c2b4b5fb":"markdown","1db2aea2":"markdown","a06b9d59":"markdown","2e516002":"markdown","2c7d521f":"markdown","9bf9aa15":"markdown","c94d659c":"markdown"},"source":{"fb5c35d1":"#loading dataset\nimport pandas as pd\nimport numpy as np\n\n#visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# data splitting\nfrom sklearn.model_selection import train_test_split\n\n# data modeling\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\n# All Model`s\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB","3a13a6a8":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","be71dea4":"data","c6fc5313":"data.info()","36de5410":"data.isnull().sum()","23b01c91":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndata.describe()","a3664560":"data.target.value_counts()","e73b36dc":"total_cnt = data['target'].count()\nplt.figure(figsize=(8,6))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='target',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","ec2dbea0":"categorical_val = []\ncontinous_val = []\nfor column in data.columns:\n    if len(data[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","807d8f3b":"categorical_val","24d79081":"plt.figure(figsize=(10,6))\ndataframe_categorical = data.loc[:,['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']]\nsns.countplot(x=\"variable\", hue=\"value\",data= pd.melt(dataframe_categorical));","220185ee":"plt.figure(figsize=(15, 15))\nprint (\"sex=[1 = male, 0 = female]\")\nprint (\"chest pain type =[0=Typical angina, 1=Atypical angina, 2=Non-anginal pain, 3=Asymptomatic]\")\nprint (\"Fasting Blood Sugar=[1 = true, 0 = false]\")\nprint (\"Resting Electrocardiographic=[0 = Normal, 1 = Non-Normal, 2 = Risk]\")\nprint (\"Exercise induced Angina =[1 = yes; 0 = no]\")\nprint (\"Peak Exercise=[0= better heart rate with excercise 1=typical healthy heart, 2=signs of unhealthy heart]\")\nprint (\"=[0=,1=,2=,3=,4=]\")\nprint (\"=[0=,1=,2=,3=]\")\nprint (\"=[0=,1=]\")\nfor i, column in enumerate(categorical_val, 1):\n    plt.subplot(3, 3, i)\n    data[data[\"target\"] == 0][column].hist(bins=35, color='Red', label='Have Heart Disease = YES', alpha=.8)\n    data[data[\"target\"] == 1][column].hist(bins=35, color='Black', label='Have Heart Disease = NO', alpha=.5)\n    plt.legend()\n    plt.xlabel(column)","73282db8":"total_cnt = data['target'].count()\nplt.figure(figsize=(12,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='sex',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","4e96c0b7":"total_cnt = data['target'].count()\nplt.figure(figsize=(20,10))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='cp',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","92891886":"total_cnt = data['target'].count()\nplt.figure(figsize=(12,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='fbs',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","d7edf9eb":"total_cnt = data['target'].count()\nplt.figure(figsize=(16,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='restecg',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","a1479a0c":"total_cnt = data['target'].count()\nplt.figure(figsize=(14,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='exang',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","eb5fed7a":"total_cnt = data['target'].count()\nplt.figure(figsize=(16,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='slope',hue='target', palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","1a47bc46":"total_cnt = data['target'].count()\nplt.figure(figsize=(20,10))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x= 'ca', hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","8dab7bec":"total_cnt = data['target'].count()\nplt.figure(figsize=(16,8))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x= 'thal',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","0fa4008b":"total_cnt = data['target'].count()\nplt.figure(figsize=(8,6))\nsns.set(font_scale = 1)\nsns.set_style(\"white\")\nax = sns.countplot(data=data, x='target',hue='target',palette='tab20')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 2, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=12)\nplt.legend(labels=None)\nsns.despine()","77dae092":"continous_val","0629d52b":"plt.figure(figsize=(15, 15))\n\nfor i, column in enumerate(continous_val, 1):\n    plt.subplot(3, 2, i)\n    data[data[\"target\"] == 0][column].hist(bins=35, color='Red', label='Have Heart Disease = YES', alpha=.8)\n    data[data[\"target\"] == 1][column].hist(bins=35, color='Black', label='Have Heart Disease = NO', alpha=.8)\n    plt.legend()\n    plt.xlabel(column)","d31b9e28":"# Create another figure\nplt.figure(figsize=(9, 7))\n\n# Scatter with postivie examples\nplt.scatter(data.age[data.target==1],\n            data.thalach[data.target==1],\n            c=\"red\")\n\n# Scatter with negative examples\nplt.scatter(data.age[data.target==0],\n            data.thalach[data.target==0],\n            c=\"black\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","a57e4da1":"# Create another figure\nplt.figure(figsize=(9, 7))\n\n# Scatter with postivie examples\nplt.scatter(data.age[data.target==1],\n            data.chol[data.target==1],\n            c=\"red\")\n\n# Scatter with negative examples\nplt.scatter(data.age[data.target==0],\n            data.chol[data.target==0],\n            c=\"black\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Cholestrol\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Cholestrol\")\nplt.legend([\"Disease\", \"No Disease\"]);","970988b9":"# Create another figure\nplt.figure(figsize=(9, 7))\n\n# Scatter with postivie examples\nplt.scatter(data.age[data.target==1],\n            data.trestbps[data.target==1],\n            c=\"red\")\n\n# Scatter with negative examples\nplt.scatter(data.age[data.target==0],\n            data.trestbps[data.target==0],\n            c=\"black\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Blood Pressure\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","953f0a96":"# Create another figure\nplt.figure(figsize=(9, 7))\n\n# Scatter with postivie examples\nplt.scatter(data.age[data.target==1],\n            data.oldpeak[data.target==1],\n            c=\"red\")\n\n# Scatter with negative examples\nplt.scatter(data.age[data.target==0],\n            data.oldpeak[data.target==0],\n            c=\"black\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Physical Condition\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","bab436ea":"data.corr()","98a46ef3":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(corr, center=0, square=True, linewidths=.5, fmt=\".2f\", annot=True, cbar_kws={\"shrink\": .5});\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","df3008c7":"categorical_val.remove('target')\ndataset = pd.get_dummies(data, columns = categorical_val)","40f702f3":"dataset","40e71c74":"print(data.columns)\nprint(dataset.columns)","9b38ee2e":"from sklearn.preprocessing import StandardScaler\n\ns_sc = StandardScaler()\ncol_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale])","61df105c":"dataset","225b0946":"from sklearn.model_selection import train_test_split\n\nX = dataset.drop('target', axis=1)\ny = dataset.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=120)","b6b5981b":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","c10b263b":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\n\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","ba2c7c55":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(lr_clf, X_train, y_train)  \nplt.show()\n","3f55e8d8":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(lr_clf, X_test, y_test) \n\nplt.show()","5a73ed68":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = lr_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_lr_clf,true_positive_rate_lr_clf,threshold_lr_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","0b6c223c":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_lr_clf,true_positive_rate_lr_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","449dc117":"test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","878f3120":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(n_jobs=-1, p=1)\nknn_clf.fit(X_train, y_train)\n\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=False)","57350ced":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(knn_clf, X_train, y_train)  \nplt.show()","6725bdd2":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(knn_clf, X_test, y_test) \n\nplt.show()","ca81c5a2":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = knn_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn_clf,true_positive_rate_knn_clf,threshold_knn_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","ff5be3b2":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn_clf,true_positive_rate_knn_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","51676b61":"test_score = accuracy_score(y_test, knn_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","c30326c5":"from sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=0.1, C=1.0,probability=True, max_iter=-1)\nsvm_clf.fit(X_train, y_train)\n\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","708c80ca":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(svm_clf, X_train, y_train)  \nplt.show()","0f7cbf5d":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(svm_clf, X_test, y_test) \nplt.show()","dbfe82e2":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = svm_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_svm_clf,true_positive_rate_svm_clf,threshold_svm_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","e0a544a2":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_svm_clf,true_positive_rate_svm_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","f42bb2b8":"test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","6a2a654b":"from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(random_state=120, min_impurity_decrease=0.01)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","06974273":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(tree_clf, X_train, y_train)  \nplt.show()","2429093d":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(tree_clf, X_test, y_test) \nplt.show()","b1168109":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = tree_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_tree_clf,true_positive_rate_tree_clf,threshold_tree_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","3f6e8f3a":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_tree_clf,true_positive_rate_tree_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","7693e970":"test_score = accuracy_score(y_test, tree_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","4a07aaaa":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=120, random_state=120)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","38192e08":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(rf_clf, X_train, y_train)  \nplt.show()","6a454538":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(rf_clf, X_test, y_test) \nplt.show()","8ce592a5":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = rf_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_rf_clf,true_positive_rate_rf_clf,threshold_rf_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","f3a3fefb":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_rf_clf,true_positive_rate_rf_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","02922ed9":"test_score = accuracy_score(y_test, rf_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rf_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","0c20be39":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier(n_jobs=-1,n_estimators=12, random_state=42)\nxgb_clf.fit(X_train, y_train)\n\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","aa3f434a":"from sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(xgb_clf, X_train, y_train)  \nplt.show()","2984832e":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(xgb_clf, X_test, y_test) \nplt.show()","ebb5c8cc":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n#Get predicted probabilites from the model\ny_probabilities = xgb_clf.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_xgb_clf,true_positive_rate_xgb_clf,threshold_xgb_clf = roc_curve(y_test,y_probabilities)\n#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","8e1454c4":"from sklearn.metrics import roc_curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_xgb_clf,true_positive_rate_xgb_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","366a0ae9":"test_score = accuracy_score(y_test, xgb_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","82fb9506":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\n\nplt.plot(false_positive_rate_lr_clf,true_positive_rate_lr_clf,label='Logistic Regression')\nplt.plot(false_positive_rate_knn_clf,true_positive_rate_knn_clf,label='k-Nearest Neighbor')\nplt.plot(false_positive_rate_svm_clf,true_positive_rate_svm_clf,label='Support Vector Machine')\nplt.plot(false_positive_rate_tree_clf,true_positive_rate_tree_clf,label='Decision Tree Classifier')\nplt.plot(false_positive_rate_rf_clf,true_positive_rate_rf_clf,label='Random Forest Classifier')\nplt.plot(false_positive_rate_xgb_clf,true_positive_rate_xgb_clf,label='XGBoost Classifier')\n\nplt.plot([0,1],ls='--',)\nplt.plot([0,0],[1,0],c='.1')\nplt.plot([1,1],c='.1')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","45e3d1b1":"# Confusion Matrix","870f14bf":"# Roc Curve ","d25da504":"# Confusion Matrix","49130d66":"## **Model prepration**","cd7cd5d7":"# Confusion Matrix","7775388f":"## 1. Logistic Regression","643ae111":"# Data Transform into StandardScaler","82555f21":"### **Missing Value Detection**","94701d9a":"# Confusion Matrix","ea27cd76":"## 4. Decision Tree Classifier ","8eb0fc1e":"1. `age` - age in years\n2. `sex` - (1 = male; 0 = female)\n3. `cp` - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. `trestbps` - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. `chol` - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. `fbs` - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. `restecg` - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. `thalach` - maximum heart rate achieved\n9. `exang` - exercise induced angina (1 = yes; 0 = no)\n10. `oldpeak` - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. `slope` - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. `ca` - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. `thal` - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. `target` - have disease or not (1=yes, 0=no) (= the predicted attribute)","3f5384d1":"## 2. K-nearest neighbors","ce860a09":"# Roc Curve","903a03b1":"# Roc Curve ","a94af788":"# **Comparing ROC Curve of k-Nearest Neighbors, Logistic Regression and Decision Tree**","7b842390":"# Roc Curve","248ed00a":"# Confusion Matrix","598cc70c":"- `fbs` and `chol` are the lowest correlated with the target variable.\n- All other variables have a significant correlation with the target variable.","ff3a97bd":"# Visualization ","2c6c57bc":"# Data Processing\n\nAfter exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.\nFirst, I'll use the `get_dummies` method to create dummy columns for categorical variables.","fe3afc87":"## 6. XGBoost Classifer","c2b4b5fb":"### **Missing Value Detection**","1db2aea2":"## 5. Random Forest","a06b9d59":"# Roc Curve","2e516002":"## 3. Support Vector machine","2c7d521f":"# **HEART DISEASE ANALYSIS** ","9bf9aa15":"# Roc Curve","c94d659c":"# Confusion Matrix"}}