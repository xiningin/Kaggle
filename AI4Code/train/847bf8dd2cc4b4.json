{"cell_type":{"62921d47":"code","6a15d771":"code","13845ee5":"code","917ffe0d":"code","7a6d5229":"code","97ebea9f":"markdown"},"source":{"62921d47":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","6a15d771":"kidney = pd.read_csv('https:\/\/assets.datacamp.com\/production\/repositories\/943\/datasets\/82c231cd41f92325cf33b78aaa360824e6b599b9\/chronic_kidney_disease.csv',header=None)","13845ee5":"kidney=kidney.sort_values([0,1])\nkidney=kidney.reset_index()  #loose leaking info \nkidney=kidney.drop('index',axis=1)\nkidney[24].replace({'notckd':'0','ckd':'1'}, inplace=True) #replace 24 kidney disease with numeric value\nkidney[24]=kidney[24].astype('int32')\nkidney.replace('?',0,inplace=True)  #fill blanco's\nkidney","917ffe0d":"\nkidney.info()","7a6d5229":"def clustertechniques2(dtrain,label,indexv):\n    print('#encodings',dtrain.shape)\n    cols=[ci for ci in dtrain.columns if ci not in [indexv,'index',label]]\n    dtest=dtrain[dtrain[label].isnull()==True][[indexv,label]]\n    print(dtest)\n\n    if True:\n        # One Hot Encode target mean()\n        cols=[ci for ci in dtrain.columns if ci not in [indexv,'index',label]]\n        coltype=dtrain.dtypes\n        for ci in cols:\n            if (coltype[ci]==\"object\"):\n                codes=dtrain[[ci,label]].groupby(ci).mean().sort_values(label)\n                codesdict=codes[label].to_dict()\n                dtrain[ci]=dtrain[ci].map(codesdict)\n                #if len(dtest)>0:  not necessary\n                #    dtest[ci]=dtest[ci].map(codesdict)\n    elif False:\n        for i in cols:\n            dtrain[i].fillna(\"Missing\", inplace=True)\n            dummies=pd.get_dummies(dtrain[i], prefix=i)\n            dtrain=pd.concat([dtrain,dummies],axis=1)\n            dtrain.drop([i],axis=1,inplace=True)\n    print('encodings  after shape',dtrain.shape)\n    #split data or use splitted data\n    X_train=dtrain[dtrain[label].isnull()==False].drop([indexv,label],axis=1).fillna(0)\n    Y_train=dtrain[dtrain[label].isnull()==False][label]\n    X_test=dtrain[dtrain[label].isnull()==True].drop([indexv,label],axis=1).fillna(0)\n    Y_test=np.random.random((X_test.shape[0],1))\n    if len(X_test)==0:\n        from sklearn.model_selection import train_test_split\n        X_train,X_test,Y_train,Y_test = train_test_split(dtrain.drop(label,axis=1).fillna(0),dtrain[label],test_size=0.250,random_state=0)\n    lenxtr=len(X_train)\n    print('splitting data train test X-y',X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)\n    \n    \n    import matplotlib.pyplot as plt    \n    #COHEN STATS\n    splitval=Y_train.mean()\n    print('Cohen splitting on',splitval)\n    group1, group2 = X_train[Y_train<splitval], X_train[Y_train>splitval]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) \/ (n1 + n2)\n    d = diff \/ np.sqrt(pooled_var)\n    #GRAPH\n    features=[ci for ci in dtrain.columns if ci in d.index]\n    d.reindex(d.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));plt.show()\n    print('Features with the 30 largest effect sizes')\n    significant_features = [f for f in features if np.abs(d.loc[f]) > 0.125]\n    print('Significant features %d: %s' % (len(significant_features), significant_features))        \n    X_train=X_train[significant_features]\n    X_test=X_test[significant_features]\n\n\n\n    from sklearn import preprocessing\n    scale = preprocessing.MinMaxScaler().fit(X_train)\n    X_train = scale.transform(X_train)\n    X_test = scale.transform(X_test)\n\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE #limit number of records to 100000\n\n    clusters = [#\n                #\n                PCA(n_components=0.7,random_state=0,whiten=True),       \n                TruncatedSVD(n_components=5, n_iter=7, random_state=42),\n                UMAP(n_neighbors=5,n_components=10, min_dist=0.3,metric='minkowski'),\n                Dummy(1),\n                #FastICA(n_components=7,random_state=0),\n                #NMF(n_components=10,random_state=0),            \n                TSNE(n_components=2,random_state=0)\n                ] \n    clunaam=['PCA','tSVD','UMAP','raw','tSNE']#,'ICA','tSVD','nmf','UMAP','tSNE']\n    \n    \n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC, LinearSVC,NuSVC\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n    from sklearn.calibration import CalibratedClassifierCV\n    from sklearn.neural_network import MLPClassifier,MLPRegressor\n    from sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,SGDClassifier,LogisticRegression\n    import xgboost as xgb\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n    \n    classifiers = [#LinearDiscriminantAnalysis(n_components=2),\n                   #NeighborhoodComponentsAnalysis(n_components=2,random_state=42),\n                   CalibratedClassifierCV(LogisticRegression( solver=\"lbfgs\",max_iter=500,n_jobs=-1), method='isotonic', cv=5),\n                   #PassiveAggressiveClassifier(max_iter=50, tol=1e-3,n_jobs=-1),    \n                   CalibratedClassifierCV(KNeighborsClassifier(n_neighbors=5,n_jobs=-1), method='isotonic', cv=5),\n                   CalibratedClassifierCV(RandomForestClassifier(n_estimators=100, random_state=42,n_jobs=-1, oob_score=True), method='isotonic', cv=5),\n                   CalibratedClassifierCV(ExtraTreesClassifier(n_estimators=10, max_depth=50, min_samples_split=5, min_samples_leaf=1, random_state=None, min_impurity_decrease=1e-7), method='isotonic', cv=5),\n                   #SVC(gamma='auto'),                   \n                   MLPClassifier(alpha=0.510,activation='logistic'),\n                   #SGDClassifier(average=True,max_iter=100),\n                   #xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11),\n                   #SVC(kernel=\"rbf\", C=0.025, probability=True),\n                   #NuSVC(probability=True),\n                   CalibratedClassifierCV(DecisionTreeClassifier(), method='isotonic', cv=5),\n                   #AdaBoostClassifier(),\n                   #GradientBoostingClassifier(),\n                   #GaussianNB(),\n                   LinearDiscriminantAnalysis(),\n                   QuadraticDiscriminantAnalysis()\n                  ]\n    clanaam= ['Logi','KNN','rFor','Xtr','MLP','Decis','LinDis','QuadDis']#['Logi','KNN','rFor','SVC','MLP','SGD']\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    \n    results=[]\n\n\n    #cluster data\n    for clu in clusters:\n        clunm=clunaam[clusters.index(clu)] #find naam\n        X_total_clu = clu.fit_transform(np.concatenate( (X_train,X_test),axis=0))\n        #embed cluster with raw data\n        X_total_clu=np.concatenate((X_total_clu,np.concatenate( (X_train,X_test),axis=0)),axis=1)\n\n        print(X_total_clu.shape)\n        plt.scatter(X_total_clu[:lenxtr,0],X_total_clu[:lenxtr,1],c=Y_train.values,cmap='prism')\n        plt.title(clu)\n        plt.show()\n        \n        #classifiy \n        for cla in classifiers:\n            import datetime\n            start = datetime.datetime.now()\n            clanm=clanaam[classifiers.index(cla)] #find naam\n            \n            print('    ',cla)\n            #cla.fit(X_total_clu,np.concatenate( (Y_train,Y_test)) )\n            cla.fit(X_total_clu[:lenxtr],Y_train )\n            \n            #predict\n            #trainpredi=cla.predict(X_total_clu[:lenxtr])\n\n            #embed prediction with data    \n            if classifiers.index(cla) in [0,1,2,3,4,5,6,7,8,9,10,11,12,13]:            \n                totpredi=cla.predict_proba(X_total_clu)\n                clus2=TruncatedSVD(n_components=5, n_iter=7, random_state=42).fit_transform(np.concatenate( (np.concatenate( (X_train,X_test),axis=0),totpredi),axis=1)  )\n                X_total_clu=np.concatenate( (np.concatenate( (X_train,X_test),axis=0),clus2),axis=1)\n                \n            else:\n                totpredi=cla.predict(X_total_clu)\n                X_total_clu=np.concatenate( (X_total_clu,totpredi.reshape(-1,1)),axis=1)\n            #X_total_clu = clu.fit_transform(X_total_clu)\n            \n            cla.fit(X_total_clu[:lenxtr],Y_train )\n            \n            trainpredi=cla.predict(X_total_clu[:lenxtr])            \n            print(classification_report(trainpredi,Y_train))            \n            testpredi=cla.predict(X_total_clu[lenxtr:])  \n            if classifiers.index(cla) in [0,2,3,4,5,7,8,9,10,11,12,13]:\n                trainprediprob=cla.predict_proba(X_total_clu[:lenxtr])\n                testprediprob=cla.predict_proba(X_total_clu[lenxtr:]) \n                plt.scatter(x=testprediprob[:,1], y=testpredi, marker='.', alpha=0.3)\n                plt.show()            \n            #testpredi=converging(pd.DataFrame(X_train),pd.DataFrame(X_test),Y_train,pd.DataFrame(testpredi),Y_test,clu,cla) #PCA(n_components=10,random_state=0,whiten=True),MLPClassifier(alpha=0.510,activation='logistic'))\n            \n            if len(dtest)==0:\n                test_score=cla.score(X_total_clu[lenxtr:],Y_test)\n                accscore=accuracy_score(testpredi,Y_test)\n                \n                train_score=cla.score(X_total_clu[:lenxtr],Y_train)\n\n                li = [clunm,clanm,train_score,accscore]\n                results.append(li)\n                verhoud=len(Y_test)\/len(Y_train)\n                print(np.round( confusion_matrix(testpredi,Y_test)*verhoud \/ ( confusion_matrix(trainpredi,Y_train)+1 ) *100,0) )\n\n                plt.title(clanm+'test accuracy versus unknown:'+np.str(test_score)+' '+np.str(accscore)+' and test confusionmatrix')\n                plt.scatter(x=Y_test, y=testpredi, marker='.', alpha=1)\n                plt.scatter(x=[np.mean(Y_test)], y=[np.mean(testpredi)], marker='o', color='red')\n                plt.xlabel('Real test'); plt.ylabel('Pred. test')\n                plt.show()\n\n\n            else:\n#                testpredlabel=le.inverse_transform(testpredi)  #use if you labellezid the classes \n                testpredlabel=testpredi\n                print(confusion_matrix(trainpredi,Y_train))\n                submit = pd.DataFrame({indexv: dtest[indexv],label: testpredlabel})\n                submit[label]=submit[label].astype('int')\n\n                filenaam='subm_'+clunm+'_'+clanm+'.csv'\n                submit.to_csv(path_or_buf =filenaam, index=False)\n                \n            print(clanm,'0 classifier time',datetime.datetime.now()-start)\n            \n    if len(dtest)==0:       \n        print(pd.DataFrame(results).sort_values(3))\n        submit=[]\n    return submit\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass Dummy( ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self._feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def fit_transform( self, X, y = None ):\n        return X \nclustertechniques2(kidney.reset_index(),24,'index') #total[len(train):].fillna(0)","97ebea9f":"# Kidney disease case\n\nWe will use that [Chronic Kidney Disease](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Chronic_Kidney_Disease\/#)  from **UC Irvine Machine Learning Repository** to predict Chronic Kidney Disease.\n\nproblem: ID is leaking info, since data are sorted..\nand it's nearly impossible to find a classifier below the 99%"}}