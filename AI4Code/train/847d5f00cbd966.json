{"cell_type":{"1d3d43a3":"code","b6ebbd47":"code","d0ecd1ff":"code","cde40744":"code","9410e4f3":"code","1beb3c1f":"code","f541da6d":"code","5268c7bf":"code","3a3ce98d":"code","9954bf7b":"code","d5245e6f":"code","447e6fe3":"code","257a857d":"code","2e571df0":"code","fcfa8fec":"code","6a90631b":"code","0abb126a":"code","797f8ddb":"code","483911f6":"code","90df70aa":"code","168cf388":"code","2d9d6daa":"code","1994514d":"code","1b76aa49":"code","66743fd8":"code","2fab25ec":"code","3a6bbf85":"code","45b445cc":"code","78926699":"code","f4694242":"code","be39b600":"code","1fd259a4":"markdown","03ddf221":"markdown","3d3f6eaf":"markdown","57b76e18":"markdown","a4a1e6e6":"markdown","50832da6":"markdown","a5702e02":"markdown","0429c38e":"markdown","f5224159":"markdown","598289d3":"markdown","b9269106":"markdown","e2d84fe1":"markdown","89adfafa":"markdown","f7b4a3a3":"markdown","eeb0e797":"markdown","6705d751":"markdown","763c9c7e":"markdown","3e5f7bd1":"markdown","da9730f4":"markdown","fc777af7":"markdown","3508df8d":"markdown","b8ccad1a":"markdown","34c5f76f":"markdown","6b81a9ee":"markdown","69d62688":"markdown"},"source":{"1d3d43a3":"# Imports and preparing the dataset\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport folium # for the map\n\n# Setting the default style of the plots\nsns.set_style('whitegrid')\nsns.set_palette('Set2')\n\n# My custom color palette\nmy_palette = [\"#7A92FF\", \"#FF7AEF\", \"#B77AFF\", \"#A9FF7A\", \"#FFB27A\", \"#FF7A7A\",\n             \"#7AFEFF\", \"#D57AFF\", \"#FFDF7A\", \"#D3FF7A\"]\n\n# Importing the 3 datasets\ndata_2015 = pd.read_csv(\"..\/input\/world-happiness\/2015.csv\")\ndata_2016 = pd.read_csv(\"..\/input\/world-happiness\/2016.csv\")\ndata_2017 = pd.read_csv(\"..\/input\/world-happiness\/2017.csv\")\n\n# First we need to prepare the data for merging the tables together (to form only 1 table)\n# Tables have different columns, so first we will keep only the columns we need\ndata_2015 = data_2015[['Country', 'Happiness Rank', 'Happiness Score', 'Economy (GDP per Capita)', 'Family',\n                       'Health (Life Expectancy)', 'Freedom', 'Generosity', 'Trust (Government Corruption)', \n                       'Dystopia Residual']]\ndata_2016 = data_2016[['Country', 'Happiness Rank', 'Happiness Score', 'Economy (GDP per Capita)', 'Family',\n                       'Health (Life Expectancy)', 'Freedom', 'Generosity', 'Trust (Government Corruption)', \n                       'Dystopia Residual']]\ndata_2017 = data_2017[['Country', 'Happiness.Rank', 'Happiness.Score', 'Economy..GDP.per.Capita.', 'Family',\n                       'Health..Life.Expectancy.', 'Freedom', 'Generosity', 'Trust..Government.Corruption.', \n                       'Dystopia.Residual']]\n\n# Tables do not have the same column names, so we need to fix that\nnew_names = ['Country', 'Happiness Rank', 'Happiness Score', 'Economy (GDP per Capita)', 'Family',\n                       'Health (Life Expectancy)', 'Freedom', 'Generosity', 'Trust (Government Corruption)', \n                       'Dystopia Residual']\n\ndata_2015.columns = new_names\ndata_2016.columns = new_names\ndata_2017.columns = new_names\n\n# Add a new column containing the year of the survey\ndata_2015['Year'] = 2015\ndata_2016['Year'] = 2016\ndata_2017['Year'] = 2017\n\n# Merge the data together\ndata = pd.concat([data_2015, data_2016, data_2017], axis=0)\ndata.head(3)","b6ebbd47":"# New data\ndata_2018 = pd.read_csv(\"..\/input\/world-happiness\/2018.csv\")\ndata_2019 = pd.read_csv(\"..\/input\/world-happiness\/2019.csv\")\n\n# Concatenate data\ndata_2018['Year'] = 2018\ndata_2019['Year'] = 2019\n\nnew_data = pd.concat([data_2018, data_2019], axis=0)\n\n# Switching overall rank column with country\/ region\ncolumns_titles = ['Country or region', 'Overall rank', 'Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Year']\nnew_data = new_data.reindex(columns=columns_titles)\n\n# Renaming old data columns:\nold_data = data[['Country', 'Happiness Rank', 'Happiness Score','Economy (GDP per Capita)', 'Family', \n                 'Health (Life Expectancy)', 'Freedom', 'Generosity', 'Trust (Government Corruption)', 'Year']]\nold_data.columns = ['Country or region', 'Overall rank', 'Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption', 'Year']\n\n# Finally, concatenating all data\ndata = pd.concat([old_data, new_data], axis=0)\n\ndata.head(3)","d0ecd1ff":"data[data['Perceptions of corruption'].isna()]","cde40744":"data.dropna(axis = 0, inplace = True)","9410e4f3":"# Double check to see if there are any missing values left\nplt.figure(figsize = (16,6))\nsns.heatmap(data = data.isna(), cmap = 'Blues')\n\nplt.xticks(fontsize = 13.5);","1beb3c1f":"data.shape\n\n# 10 columns, 781 rows","f541da6d":"data.groupby(by='Year')['Score'].describe()","5268c7bf":"# First we group the data by year and average the factors\ngrouped = data.groupby(by = 'Year')[['Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption']].mean().reset_index()\n\n# Now we reconstruct the df by using melt() function\ngrouped = pd.melt(frame = grouped, id_vars='Year', value_vars=['Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption'], var_name='Factor', value_name='Avg_value')\n\ngrouped.head()","3a3ce98d":"plt.figure(figsize = (16, 9))\n\nax = sns.barplot(x = grouped[grouped['Factor'] != 'Score']['Factor'], y = grouped['Avg_value'], \n            palette = my_palette[1:], hue = grouped['Year'])\n\nplt.title(\"Difference in Factors - Then and Now - \", fontsize = 25)\nplt.xlabel(\"Factor\", fontsize = 20)\nplt.ylabel(\"Average Score\", fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.legend(fontsize = 15)\n\nax.set_xticklabels(['Money','Family', 'Health', 'Freedom', 'Generosity', 'Trust']);","9954bf7b":"# Average top 5 most happy countries\ncountry_score_avg = data[data['Year']==2019].groupby(by = ['Country or region'])['Score'].mean().reset_index()\ntable = country_score_avg.sort_values(by = 'Score', ascending = False).head(10)\n\ntable","d5245e6f":"plt.figure(figsize = (16, 9))\nsns.barplot(y = table['Country or region'], x = table['Score'], palette = my_palette)\n\nplt.title(\"Top 10 Happiest Countries in 2019\", fontsize = 25)\nplt.xlabel(\"Happiness Score\", fontsize = 20)\nplt.ylabel(\"Country\", fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15);","447e6fe3":"# Average top 5 most \"not that happy\" countries\ntable2 = country_score_avg.sort_values(by = 'Score', ascending = True).head(10)\n\ntable2","257a857d":"plt.figure(figsize = (16, 9))\nsns.barplot(y = table2['Country or region'], x = table2['Score'], palette = my_palette)\n\nplt.title(\"Top 10 Least Happy Countries in 2019\", fontsize = 25)\nplt.xlabel(\"Happiness Score\", fontsize = 20)\nplt.ylabel(\"Country\", fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15);","2e571df0":"# Checking the distribution for Happiness Score\nplt.figure(figsize = (16, 9))\n\nsns.distplot(a = country_score_avg['Score'], bins = 20, kde = True, color = \"#A9FF7A\")\nplt.xlabel('Happiness Score', fontsize = 20)\nplt.title('Distribution of Average Happiness Score - 2019 -', fontsize = 25)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xlim((1.5, 8.9));","fcfa8fec":"## Creating the grouped table\ncountry_factors_avg = data[data['Year'] == 2019].groupby(by = ['Country or region'])[['GDP per capita',\n       'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption']].mean().reset_index()\n\nplt.figure(figsize = (16, 9))\n\nsns.kdeplot(data = country_factors_avg['GDP per capita'], color = \"#B77AFF\", shade = True)\nsns.kdeplot(data = country_factors_avg['Social support'], color = \"#FD7AFF\", shade = True)\nsns.kdeplot(data = country_factors_avg['Healthy life expectancy'], color = \"#FFB27A\", shade = True)\nsns.kdeplot(data = country_factors_avg['Freedom to make life choices'], color = \"#A9FF7A\", shade = True)\nsns.kdeplot(data = country_factors_avg['Generosity'], color = \"#7AFFD4\", shade = True)\nsns.kdeplot(data = country_factors_avg['Perceptions of corruption'], color = \"#FF7A7A\", shade = True)\n\nplt.xlabel('Factors Score', fontsize = 20)\nplt.title('Distribution of Average Factors Score - 2019 -', fontsize = 25)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xlim((-0.5, 2.3))\nplt.legend(fontsize = 15);","6a90631b":"# Calculating the Pearson Correlation\n\nc1 = scipy.stats.pearsonr(data['Score'], data['GDP per capita'])\nc2 = scipy.stats.pearsonr(data['Score'], data['Social support'])\nc3 = scipy.stats.pearsonr(data['Score'], data['Healthy life expectancy'])\nc4 = scipy.stats.pearsonr(data['Score'], data['Freedom to make life choices'])\nc5 = scipy.stats.pearsonr(data['Score'], data['Generosity'])\nc6 = scipy.stats.pearsonr(data['Score'], data['Perceptions of corruption'])\n\nprint('Happiness Score + GDP: pearson = ', round(c1[0],2), '   pvalue = ', round(c1[1],4))\nprint('Happiness Score + Family: pearson = ', round(c2[0],2), '   pvalue = ', round(c2[1],4))\nprint('Happiness Score + Health: pearson = ', round(c3[0],2), '   pvalue = ', round(c3[1],4))\nprint('Happiness Score + Freedom: pearson = ', round(c4[0],2), '   pvalue = ', round(c4[1],4))\nprint('Happiness Score + Generosity: pearson = ', round(c5[0],2), '   pvalue = ', round(c5[1],4))\nprint('Happiness Score + Trust: pearson = ', round(c6[0],2), '   pvalue = ', round(c6[1],4))","0abb126a":"# Computing the Correlation Matrix\n\ncorr = data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.title('What influences our happiness?', fontsize = 25)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15);","797f8ddb":"# import os\n# print(list(os.listdir(\"..\/input\")))","483911f6":"#json file with the world map\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\ncountry_geo = gpd.read_file('..\/input\/worldcountries\/world-countries.json')\n\n#import another CSV file that contains country codes\ncountry_codes = pd.read_csv('..\/input\/iso-country-codes-global\/wikipedia-iso-country-codes.csv')\ncountry_codes.rename(columns = {'English short name lower case' : 'Country or region'}, inplace = True)\n\n#Merge the 2 files together to create the data to display on the map\ndata_to_plot = pd.merge(left= country_codes[['Alpha-3 code', 'Country or region']], \n                        right= country_score_avg[['Score', 'Country or region']], \n                        how='inner', on = ['Country or region'])\ndata_to_plot.drop(labels = 'Country or region', axis = 1, inplace = True)\n\ndata_to_plot.head(2)","90df70aa":"#Creating the map using Folium Package\nmy_map = folium.Map(location=[10, 6], zoom_start=1.49)\n\nmy_map.choropleth(geo_data=country_geo, data=data_to_plot, \n                  name='choropleth',\n                  columns=['Alpha-3 code', 'Score'],\n                  key_on='feature.id',\n                  fill_color='BuPu', fill_opacity=0.5, line_opacity=0.2,\n                  nan_fill_color='white',\n                  legend_name='Average Happiness Indicator')\n\nmy_map.save('data_to_plot.html')\n\nfrom IPython.display import HTML\nHTML('<iframe src=data_to_plot.html width=850 height=500><\/iframe>')","168cf388":"# Importing the libraries\nfrom sklearn.model_selection import train_test_split # for data validation\n\n# Models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, LassoLars\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\n# Metrics and Grid Search\nfrom sklearn import model_selection, metrics\nfrom sklearn.model_selection import GridSearchCV","2d9d6daa":"# Creating the table\ndata_model = data.groupby(by= 'Country or region')['Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption'].mean().reset_index()\n\n# Creating the dependent and independent variables\ny = data_model['Score']\nX = data_model[['GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption']]\n\n# Splitting the data to avoid under\/overfitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","1994514d":"# Creating a predefined function to test the models\ndef modelfit(model):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    mae = metrics.mean_absolute_error(y_test, preds)\n    print('MAE:', round(mae,4))","1b76aa49":"# Linear Regression\n\nlm = LinearRegression(n_jobs = 10000)\nmodelfit(lm)","66743fd8":"# Random Forest Regressor\n\nrf = RandomForestRegressor(n_jobs = 1000)\nmodelfit(rf)","2fab25ec":"# XGBoost\nxg = XGBRegressor(learning_rate=0.1, n_estimators=5000)\nmodelfit(xg)","3a6bbf85":"# Decision Tree\ndt = DecisionTreeRegressor()\nmodelfit(dt)","45b445cc":"# Bayesian Linear Model\nbr = BayesianRidge(n_iter=1000, tol = 0.5)\nmodelfit(br)","78926699":"# Lasso Lars\nls = LassoLars()\nmodelfit(ls)","f4694242":"final_model = BayesianRidge(n_iter = 10, tol = 0.1, alpha_2 = 0.1)\nfinal_model.fit(X_train, y_train)","be39b600":"# How important is each variable into predicting the overall Happiness Score?\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(estimator=final_model, random_state=1)\nperm.fit(X_test, y_test)\n\neli5.show_weights(estimator= perm, feature_names = X_test.columns.tolist())","1fd259a4":"Now, let's see the intensity of the correlation between the happiness score and its 7 main influencer factors:\n\n## VI. What Influences Happiness?","03ddf221":"## I. Imports:","3d3f6eaf":"After all that jazz, I couldn't help myself.\n\nWhat if we tried to predict the happiness score of a country using the other factors available in the analysis?","57b76e18":"![Annotation%202020-02-19%20215951.png](https:\/\/i.imgur.com\/BRX6Zur.png)\n\nSo.......\n\n1. **The Nordics** and the **West** is the happiest. + **Australia** (kangaroos just MUST be a factor)\n2. **East Europe** and the majority of Asia is in the middle.\n3. **The South** is the least happiest.","a4a1e6e6":"# 4. Final Thoughts\n\nThis report is amazing. Very helpful for many industries, as it assesses the overall mood of a nation, as well as gives a glimpse into how it is evolving in time.\n\nIt also points out to what makes us happy. What we value the most as beings. What do we want in order to feel contempt and happy with our lives.\n\n*And this report gives just that answer: money and healthy relationships... in exactly that order* \ud83d\ude05\n\nIf you guys have any ideas on how to improve this, do not hold yourselves.","50832da6":"We're done with the preprocessing part.\n\n#### Let's get to business!","a5702e02":"# 1. Imports, Data Preprocessing, Missing Values?","0429c38e":"![](https:\/\/i.imgur.com\/E4eJdxu.png)\n\nSo... how **happy** are we? \n\nAre we **happier** than last year? \n\nHow happy are some of us **other than others**? \n\nWhat **dictates** our happiness?\n\nIt's time to find out. \ud83d\ude0e","f5224159":"## II. Summary Statistics","598289d3":"### Update: 2018 and 2019 data dropped in \ud83d\ude01","b9269106":"### Missing values\n\nThere is only one missing value in the data, so we will just drop it.","e2d84fe1":"## II. Preparing...","89adfafa":"## III. Models probation:","f7b4a3a3":"# 2. Let's familiarize with the numbers\n\n## I. Shape of Data","eeb0e797":"## IV. Which are the least happy people in 2019?","6705d751":"<div class=\"alert alert-block alert-info\">\n<p><p>\n<p>If you liked this, don't be shy, upvote! \ud83d\ude01<p>\n<b>Cheers!<b>\n<p><p>\n<\/div>","763c9c7e":"# 3. A predictive model, because why not ?","3e5f7bd1":"Let's look closer at the **top 10 nations** from top to bottom and from bottom to top... how do they look like?\n## III. Which are the happiest people in 2019?","da9730f4":"## IV. How important are the variables?","fc777af7":"Linear Regression and the Bayesian Ridge were the models that performed the best (they had the smallest mae out of all)\n\nAlso did some **parameter tuning**, but the MAE score didn't change.\n\nSo, we have a winner: Congrats to Bayesian Ridge (if you found a better model, please don't keep it to yourself \ud83d\ude01)","3508df8d":"* The distribution of happiness is quite **platykurtic**, evenly spread between a score of ~3 and 7.5.\n\n## Distribution for the other factors","b8ccad1a":"It seems that Happiness is influenced the **most by GDP (money moneyyy\ud83d\udcb0)** (very strong correlation) and **Health**. There is also medium positive correlation between Happiness, Freedom and Health. \n\n## VII. Globe Map 2019","34c5f76f":"* Nothing surprising here either. Countries in **war zones** or with **poor sanitation systems**, diseases or very poor infrastructure are the least happy people out of all.\n\nLet's give them a hand! \ud83e\udd1d\n\n## V. Distribution of Smiles","6b81a9ee":"What actually influences our general well-being?\n\n* Looks like **money** is of the highest importance.\n* Following up next is **social support**, meaning the relationships in a family and the closest group of friends. Human interaction.\n* I would like to point out **freedom** as well. Freedom to act. To talk. But careful not to overstep others tho.\n* The last one is **generosity**, but who likes to share anyway?","69d62688":" * Well, looks like we are **slowly but shurely** becoming less and less happy.\n * 2019 was **better** than 2018, but still 2015 is the happiest year in our data.\n \n## Factors difference between 2015 and 2019\n \n We first need to create a dataframe with the next columns:\n * `Factor` - our 7 factors\n * `Year` - the years between 2015 and 2019\n * `Avg_value` - average value of the factor for the year"}}