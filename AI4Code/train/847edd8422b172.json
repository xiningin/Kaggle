{"cell_type":{"7c4b16dc":"code","664af9ff":"code","ebcb13ef":"code","5fb5d117":"code","c1758f71":"code","6717ce25":"code","ea8935d8":"code","a22b11e5":"code","8e593ff5":"code","226d88e7":"code","3d48e2ff":"code","fda5830d":"code","40c747fd":"code","7a015fa5":"code","c2bde202":"code","d95d61e5":"code","19747d6e":"code","e2a87211":"code","4aac2155":"code","425b56ae":"code","7392abf0":"code","93ca7e10":"code","baa4d82d":"markdown","7fa6c569":"markdown","28db20d7":"markdown","946f04e1":"markdown","fb7cc5f9":"markdown","633000ef":"markdown","9a02a052":"markdown","574f731c":"markdown","835c9ae0":"markdown","d1674412":"markdown","7d902704":"markdown","490be07a":"markdown","fb0088e6":"markdown","8bcccf9d":"markdown","c870ef44":"markdown","c27a4281":"markdown","c72e6f91":"markdown","5a1b94d1":"markdown"},"source":{"7c4b16dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport chainer as ch\nfrom chainer import datasets\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import training\nfrom chainer.training import extensions\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","664af9ff":"mfile='..\/input\/mushrooms.csv'\ndata_array=np.genfromtxt(mfile,delimiter=',',dtype=str,skip_header=True)\ndf=pd.read_csv('..\/input\/mushrooms.csv')\ndf.head()\n","ebcb13ef":"df.shape","5fb5d117":"df.describe()","c1758f71":"df.describe().iloc[1][1:].sum()   # summing along the \"unique\" row . all columns except the first one which corresponds to the class of the sample.","6717ce25":"l=[x for x in df.columns[1:]]   # make a list of all the column names , all except the class column\nprint(l)\ndata_df=pd.get_dummies(df[l],drop_first=False)\ndata_df.shape","ea8935d8":"data_df.describe()","a22b11e5":"from sklearn.preprocessing import LabelEncoder\nclass_le=LabelEncoder()\ny=class_le.fit_transform(df['class'].values)\ny=y.reshape(y.shape[0],1)\nassert(y.size==data_df.shape[0])              # I do this because I intend to create tuples of (data_df,y), so both the structures need to be of equal length\nprint(y)","8e593ff5":"TRAIN_SIZE_PERC=0.7\n\ntuple_dataset=datasets.TupleDataset(data_df.values.astype(np.float32),y)\ntrain,test=datasets.split_dataset_random(tuple_dataset,first_size=int(TRAIN_SIZE_PERC*len(tuple_dataset)))  # 70% of the data is used for training\nprint(len(train))\nprint(len(test))","226d88e7":"BATCH_SIZE=120\ntrain_iter=ch.iterators.SerialIterator(train,BATCH_SIZE)\ntest_iter=ch.iterators.SerialIterator(test,BATCH_SIZE,repeat=False,shuffle=False)","3d48e2ff":"import chainer\nfrom chainer import initializers\nimport chainer.functions as F\n\nclass CustomLinearLayer(chainer.Link):\n    \n    def __init__(self,n_in,n_out):\n        super(CustomLinearLayer,self).__init__()\n        with self.init_scope():\n            self.W=chainer.Parameter(\n                                        initializers.HeNormal(),      # He-initialization\n                                        (n_out,n_in)                # W matrix is (n_out X n_in)\n                                        \n                                    )\n            self.b=chainer.Parameter(\n                                        initializers.Zero(),        # initialized to zero\n                                        (n_out,)                   # bias is of shape (n_out,)\n                                    )\n            \n    #forward propogation implementation:\n    def forward(self,x):\n        return F.linear(x,self.W,self.b)\n    \n","fda5830d":"import chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nclass CustomMultiLayerPerceptron(chainer.Chain):\n    \n    def __init__(self,n_in,n_hidden,n_out):\n        super(CustomMultiLayerPerceptron,self).__init__()\n        with self.init_scope():\n            self.layer1 = CustomLinearLayer(n_in,n_hidden)                                     # input layer\n            self.layer2 = CustomLinearLayer(n_hidden,n_hidden)                                 # hidden layer\n            self.layer3 = CustomLinearLayer(n_hidden,n_hidden)                                 # hidden layer\n            self.layer4 = CustomLinearLayer(n_hidden,n_hidden)                                 # hidden layer\n            self.layer5 = CustomLinearLayer(n_hidden,n_hidden)                                 # hidden layer\n            self.layer6 = CustomLinearLayer(n_hidden,n_out)                                    # output layer\n        \n        #forward propagation\n    def forward(self,*args):\n        x=args[0]\n        h1=F.relu(self.layer1(x))        # implements the  CustomLinearLayer link's forward propogation on x. i.e. h1=relu(x.W_1+b_1)\n        h2=F.relu(self.layer2(h1))       # h2= relu( h1.W_2 + b_2)\n        h3=F.relu(self.layer3(h2))       # h3= relu( h2.W_3 + b_3)\n        h4=F.relu(self.layer4(h3))       # h4= relu( h3.W_4 + b_4)\n        h5=F.relu(self.layer5(h4))       # h5= relu( h4.W_5 + b_5)\n        #h6=F.sigmoid(self.layer6(h5))    # h6= sigmoid( h5.W_6 + b_6)\n        h6=self.layer6(h5)    # h6=  h5.W_6 + b_6\n        #print(h6)\n        return h6\n    ","40c747fd":"from chainer.functions.evaluation import accuracy\nfrom chainer.functions.loss import softmax_cross_entropy\nfrom chainer import link\nfrom chainer import reporter\n\nclass CustomClassifier(link.Chain):\n    def __init__(self,\n                    predictor,                                            #predictor network that this classifier wraps\n                    lossfun=softmax_cross_entropy.softmax_cross_entropy,  #the lossfunction it uses        \n                    accfun=accuracy.accuracy,                              #the performance metric used\n                    label_key=-1                                          #the location of the label in the input minibatch. (defaulted to the rightmost column)\n                ):\n        super(CustomClassifier,self).__init__()\n        self.lossfun = lossfun\n        self.accfun  = accfun\n        self.y       = None                                               # the prediction from the last minibatch  y_hat\n        self.loss    = None                                               #loss value for the last minibatch\n        self.accuracy= None                                               #accuracy for the last minibatch\n        self.label_key=label_key                                         # the location of the label in the input minibatch\n        with self.init_scope():                                          # creates an initialization scope. See documentation for details.\n            self.predictor = predictor\n    \n    \n    def forward(self,*args,**kwargs):\n        \"\"\"\n            Computes loss value for an input \/label pair\n            Computes accuracy \n            \n            Args:\n                args  : Input minibatch  \n                kwargs: Input minibatch\n            \n        \"\"\"\n        self.y = None\n        self.loss = None\n        self.accuracy = None\n        \n        t=args[self.label_key]                                              #ground truth for the minibatch\n    \n        self.y = self.predictor(*args)                                 #get the output from the predictor\n        self.loss=self.lossfun(self.y,t)                               #calculate the loss for this minibatch\n        reporter.report({'loss':self.loss},self)\n        self.accuracy = self.accfun(self.y,t)                          #the performance metric\n        reporter.report({'accuracy':self.accuracy},self)\n        \n        return self.loss","7a015fa5":"model=CustomClassifier(CustomMultiLayerPerceptron(n_in=data_df.shape[1],n_hidden=data_df.shape[1]*3,n_out=1),\n                       lossfun=F.sigmoid_cross_entropy,\n                       accfun=F.binary_accuracy\n                      )","c2bde202":"optimizer=ch.optimizers.SGD(lr=0.001).setup(model)","d95d61e5":"updater=training.StandardUpdater(iterator=train_iter,optimizer=optimizer,device=-1) # set up the updater using \n                                                                          #the iterator and the optimizer","19747d6e":"PERIOD=50                           \ntrainer=training.Trainer(updater,(PERIOD,'epoch'),out='result')","e2a87211":"trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\ntrainer.extend(extensions.dump_graph('main\/loss'))\ntrainer.extend(extensions.snapshot(), trigger=(20, 'epoch'))\ntrainer.extend(extensions.LogReport())\n\nif extensions.PlotReport.available():\n    trainer.extend(\n        extensions.PlotReport(['main\/loss', 'validation\/main\/loss'],\n                              'epoch', file_name='loss.png'))\n    trainer.extend(\n        extensions.PlotReport(\n            ['main\/accuracy', 'validation\/main\/accuracy'],\n            'epoch', file_name='accuracy.png'))\n\n    \ntrainer.extend(extensions.PrintReport(\n    ['epoch', 'main\/loss', 'validation\/main\/loss',\n     'main\/accuracy', 'validation\/main\/accuracy', 'elapsed_time']))\ntrainer.run()","4aac2155":"from chainer import serializers\nserializers.save_hdf5('mushroom_model_01.hdf5',model)","425b56ae":"#Load the trained model\nnmod=CustomClassifier(CustomMultiLayerPerceptron(n_in=data_df.shape[1],n_hidden=data_df.shape[1]*3,n_out=1),\n                       lossfun=F.sigmoid_cross_entropy,\n                       accfun=F.binary_accuracy\n                      )\nserializers.load_hdf5('mushroom_model_01.hdf5',nmod)\n","7392abf0":"#Check on test data\nxtest=[x[0] for x in test]  # extract the test features\nytest=[x[1] for x in test]  # get the test labels\nprobs=nmod.predictor(np.array(xtest)).data # see Variable class. predictor(x) does the forward prop returning a Variable object h6. The .data is a member of the class\n\n\npreds=np.where(probs>0,1,0)             # predictions thresholded at 0\n","93ca7e10":"from sklearn.metrics import roc_curve,auc,roc_auc_score\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfalse_pos_rates,true_pos_rates,thresholds=roc_curve(ytest,probs)                       # we need to plot the roc . the y-axis are all true positives and the x-axis are all false positives.\nprint(false_pos_rates)                                                                 # we use the probabilities here while plotting the roc and NOT the predictions.\n\nroc_auc=auc(false_pos_rates,true_pos_rates)\nprint(roc_auc)\n\nroc_auc_sc=roc_auc_score(ytest,probs)\nprint(roc_auc_sc)\n\n#plotting the roc \ntrace=go.Scatter(\n    x=false_pos_rates,\n    y=true_pos_rates,\n    mode='lines',\n    name='ROC',\n    \n)\nrandomGuess=go.Line(\n    x=[0,1],\n    y=[0,1],\n    line=dict(dash='dash'),\n    name='random guess'\n)\n\nlayout=go.Layout(\n    annotations=[\n        dict(\n        x=0.2,\n        y=0.6,\n        text='AUC: \\n '+str(roc_auc_sc),\n        showarrow=False\n        )\n    ]\n)\ndata=[trace,randomGuess]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig)","baa4d82d":"Ok so now we have a predictor network. i.e one that does this ${x \\implies h_{1}\\implies h_{2}\\implies h_{3} \\implies \\hat{y } }$   where ${\\hat{y}}$  is the predicted output.   \nBut we want our network to calculate a loss and also output a metric that shows how well our network is performing at the task of classifying the mushrooms as either edible or deadly.  \nWe create a new Chain, a CustomClassifier which is our implementation of Chainer's [Classifier](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.links.Classifier.html#chainer.links.Classifier). \n\nThis CustomClassifier will wrap the predictor network chain . It will then compute the loss and metric(accuracy) based on a given input\/truth pair\n\n\n","7fa6c569":"Next some performance [metrics](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)","28db20d7":"**Inference**  \nFirst load the saved model and check the model on the test data","946f04e1":"This seems like a pretty decent classifier. ","fb7cc5f9":"Now we perform LabelEncoding on the class labels","633000ef":"Ok now to create the model   \nWe pass the Predictor chain to the Classifier  along with a loss function and performance metric  \nThe predictor has 3 hidden layers as defined in the class, each parameterized to have 234 nodes in each of the hidden layers and a single node in the output layer  \n\n","9a02a052":"Expecting a resulting 117-dimensional feature space at the end of ohe","574f731c":"Now to the next step  \n\n**[The Iterator](https:\/\/docs.chainer.org\/en\/stable\/reference\/iterators.html#module-chainer.iterators)**  \nThe iterator is used to implement strategies to create mini-batches by iterating over the datasets. I will be using the [SerialIterator](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator). and a batch size of 120 .  \nFor the training iterator, we set the shuffle and repeat parameters to True  while for the testing iterator we set them to False (This is to basically prevent overfitting of the training data . The  variance between the mini batches aka the sampling variance is reduced each time there's a shuffle at the beginning of the epoch , [further logic answered on stackoverflow](https:\/\/datascience.stackexchange.com\/questions\/24511\/why-should-the-data-be-shuffled-for-machine-learning-tasks))\n","835c9ae0":"\n\nNext to convert the data to TupleDataset and split into training and testing sets using [datasets.split_dataset_random](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.datasets.split_dataset_random.html#chainer.datasets.split_dataset_random).\nThis function creates two instances of [SubDataset](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.datasets.SubDataset.html#chainer.datasets.SubDataset). These instances do not share any examples, and they together cover all examples of the original dataset.","d1674412":"**Save the model.**","7d902704":"**The Model**  \nThis is where we create a neural network .\nThere is an easy way to create the neural network using Chainer's [Sequential](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.Sequential.html#chainer.Sequential) class.  \n\nBut I will be building from more basic classes  \n\nChainer uses [Link](https:\/\/docs.chainer.org\/en\/stable\/guides\/links.html) as it's building block of neural network. You can use this class to define your own implementation of a neural network.   \nAs an example consider we want to make our own implementation of a Chainers connected  [Linear](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.links.Linear.html#chainer.links.Linear)  where each layer implements its forward propogation as ${y_{i}=Wx_{i}+b}$ but instead of initializing the weights as Linears default Normal distribution, ours will be a [He-initialization](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.initializers.HeNormal.html#chainer.initializers.HeNormal)  this is how we go about doing this.  \n\n\n\n\n\n\n","490be07a":"Once the updater is made, time to create the [**Trainer**](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.Trainer.html#chainer.training.Trainer)  \nThe trainer represents a training loop  and consists of two parts. The Updater which actually updates the parameters and the [Extension](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.Extension.html#chainer.training.Extension) for arbitary functionalities viz producing test scores.  \n\nEach iteration of this loop does :  \n* Update of the parameters by the Updater. This includes loading of the mini-batch using the training iterator,  the backward and forward propagations and the update of the parameters.   \n* Call any extensions attached to the trainer (We havent attached any yet. But we will once our trainer is built)\n\n\nThe Trainer takes as arguments:  \nThe updater  \nA [Trigger](https:\/\/docs.chainer.org\/en\/stable\/reference\/training.html#triggers) which is a callable that decides when to process some specific event within the training loop.   \n\nIn our example we will be doing 50 Sweeps across the entire training set. i.e 50 epochs which is specified  here as a tuple  (period,unit) where period is the length of an interval and the unit of measurement is  'epoch'. see this [link](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.triggers.IntervalTrigger.html#chainer.training.triggers.IntervalTrigger)  \n","fb0088e6":" The first column is the label. The rest are nominal features. We can use LabelEncoder for the converting label values to integers ${y \\in \\{0,1\\}}$ depending on whether ${label=p}$ or ${label=e}$.  \n However the same can't be said for the 22 nominal features. Applying the LabelEncoder on these features will convert the nominal values to integers , so if ${k=1}$ and ${n=2}$ for the feature gill color, it implies ${k}$ which represents the color black  is less than ${n}$ which stands for  the color brown i.e black is less than brown which is obviously an incorrect assumption since there is no ordering amongst the colors . Such an approach will affect the learning of our algorithm.  \n Here I will be using the One-Hot-Encoding (ohe) which will end up creating dummy features for each new value that the original feature takes. So if the stalk_shape feature takes 2 values (e,t) then the ohe of this feature will produce two more \"dummy\" features stalk_shape_e and stalk_shape_t , so for a sample where ${stalk\\_shape=e \\implies stalk\\_shape\\_e=1}$ and ${stalk\\_shape\\_t=0 }$ for this sample .  \n For a dataframe we will be using the *get_dummies()*  method implemented in  pandas to get ohe done.","8bcccf9d":"**[Extensions](https:\/\/docs.chainer.org\/en\/stable\/guides\/extensions.html)**  \nWe have the option to attach any Extensions to the Trainer using the extend() method of the trainer.  \nIn addition we can write our own Extensions as simple functions and attach them to the trainer.   \nIn the example we will be using [PlotReport](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.extensions.PlotReport.html#chainer.training.extensions.PlotReport) and [PrintReport](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.extensions.PrintReport.html#chainer.training.extensions.PrintReport) to plot our loss and accuracy.\n","c870ef44":"This above Link can be linked together to form a  [Chain](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.Chain.html#chainer.Chain).( i.e a neural network architecture ).  Here are some of Chainer's [links](https:\/\/docs.chainer.org\/en\/stable\/reference\/links.html#module-chainer.links).  \n\nLet's create a CustomMultiLayerPerceptron Chain  using the above CustomLinearLayer Link .  \nOur architecture will have 1 input layer, 3 hidden layers and an output layer.    \nThe hidden layers will have a [relu activation](https:\/\/docs.chainer.org\/en\/stable\/reference\/functions.html#activation-functions) function while the output layer will have a sigmoid.","c27a4281":"[Chainer reference](https:\/\/docs.chainer.org\/en\/stable\/glance.html)    \nChainer has a \"[Define-by-run](https:\/\/docs.chainer.org\/en\/stable\/guides\/define_by_run.html)\" philosophy which lets us build flexible neural architectures.   \nThis means that the network can be defined dynamically using the forward computation. \n\nChainer has a *trainer* which is used to set up the neural network  and data for training. The trainer has a structure that's hierarchical and looks like this  \n![](http:\/\/gdurl.com\/Hu7m)  \n\nEach of the components is fed information from the components within it.   \n**Therefore in order to set up the trainer we start with the inner most components first and move outwards.** (with the extensions as an exception which are added after the trainer is made.)  \n\nSo we  are going to follow this particular order so as to set up a trainer:  \n\n1. format the dataset  \n2. configure the iterator to step through the dataset for training and validation\n3. define the neural network to include in the model\n4. pick an optimizer and setup the model to use it\n5. create an updater using the optimizer. The updater is called after the training batches \n6. Set up the trainer using the updater.\n7. The extensions are added to the trainer once the latter is built. an e.g. of an extension to the trainier is the [Evaluator](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.extensions.Evaluator.html#chainer.training.extensions.Evaluator) which provides test scores.   \n\nThe following set of images   describes how we go about building a trainer.(steps 1->7)\n![](http:\/\/gdurl.com\/cYDc)\n\n\n\n\n\nOk so we are now going to move through these steps one at a time.\n\n\n**[Datasets](https:\/\/docs.chainer.org\/en\/stable\/reference\/datasets.html)**   \nFirst lets transform the dataset into a type of [Chainers General Datasets](https:\/\/docs.chainer.org\/en\/stable\/reference\/datasets.html#module-chainer.dataset) called the [TupleDataset](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.datasets.TupleDataset.html#chainer.datasets.TupleDataset)  \nThe TupleDataset basically does what the zip() function does i.e it takes in two equal length datasets d1=[1,2,3,4] and d2=[5,6,7,8] and returns a tuple dataset (1,5), (2,6), (3,7), (4,8)  \nWe are now going to load the raw mushrooms csv file into a Chainer TupleDataset","c72e6f91":"At this point we have the dataset , the training iterator  , the model and now the optimizer.  \nNow to the next step which is setting up the updater.  \n\n**[Updater](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.Updater.html#chainer.training.Updater)**  \nThe updater uses the minibatch from the training iterator and  does the forward and backward processing of the model.  Based on the optimizer we chose this class updates the  weights and biases of the network.   More specifically \nIf the training is to be done on a GPU , then set the device parameter to the number of the GPU device (usually device=0) while for a CPU use device=-1  \nThe Updater is responsible for   \n* getting the minibatch from  the dataset via the iterator.  \n* run the forward and backward process of the Chain by using the Optimizer (which in turn calls its own [update()](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.Optimizer.html#chainer.Optimizer.update) method to do these tasks).  \n* Update parameters according to their [UpdateRule](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.UpdateRule.html#chainer.UpdateRule). This too is handled up the Optimizer.update() method.   \nThe UpdateRule is a class that is implements how to update a parameter variable using the gradient of the loss function  \nIf you wish to write your own implementation of the update rule, then this can be done by overriding the [Updater.update()](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.Updater.html#chainer.training.Updater.update) method \n\nWe will be using the [StandardUpdater](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.training.updaters.StandardUpdater.html)\n","5a1b94d1":"**[Optmizer](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.Optimizer.html#chainer.Optimizer)**  \nThis is the class responsible for all numerical optimizers and  provides basic features for all optimization methods.   \nWe will use the [SGD](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.optimizers.SGD.html#chainer.optimizers.SGD) optimizer provided in Chainer which represents the vanilla Stochastic Gradient Descent Algorithm.  \n\nThe Optimizer optimizes parameters (weights and biases) of a target link (our model from the previous step).   \nThe target link which is our model is registered via the setup() method of the Optimizer, and then the parameter update is taken care of by the [update()](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.Optimizer.html#chainer.Optimizer.update) method which does the update based on a given loss function which in our case is the sigmoid cross entropy function  \n\nWe will be using Chainers [SGD](https:\/\/docs.chainer.org\/en\/stable\/reference\/generated\/chainer.optimizers.SGD.html#chainer.optimizers.SGD) which is its implementation of the vanilla Stochastic Gradient Descent"}}