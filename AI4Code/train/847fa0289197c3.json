{"cell_type":{"c326c06b":"code","28014166":"code","497868b1":"code","6290fd9e":"code","91f2e6d5":"code","7d6ad1e0":"code","97d07c0d":"code","58e2aefe":"code","69977cbc":"code","2c14081f":"code","b5ae5578":"code","2616236a":"code","7459e457":"code","56c25e29":"code","5c4e7780":"code","56451b05":"code","0c5846cc":"code","2ca8a887":"code","78b715b2":"code","449f3466":"code","6f2260cc":"code","a4624917":"code","beff776c":"code","106e627c":"code","40a87b5e":"code","06f4ac52":"code","8c10a8c0":"code","9aed0e9d":"code","2c41e347":"code","02381e12":"code","241f676c":"code","a65048e9":"code","6e340e04":"code","d4625cec":"code","b8b5cbe5":"code","147d4343":"code","d4da6a21":"code","9498db94":"code","825364b0":"code","0e1de4d6":"code","e3d687f8":"code","c35a21a1":"code","b20fe073":"markdown","641fd5d5":"markdown","d6dd3498":"markdown","11735db5":"markdown","b7cf9338":"markdown","3d146e2e":"markdown"},"source":{"c326c06b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_column',100)\nfrom scipy.stats import pearsonr\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28014166":"original_data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\noriginal_data.head()","497868b1":"original_data.isnull().sum()","6290fd9e":"y = original_data.diagnosis\ndataset = original_data.drop(['id','Unnamed: 32','diagnosis'],axis =1)\ndataset.head()","91f2e6d5":"print('Observations for Malignant and Benign is \\n{}'.format(y.value_counts()))\n_ = sns.countplot(y).set(title = 'Number of Labels')","7d6ad1e0":"dataset.describe()","97d07c0d":"data_normalize = (dataset - dataset.mean())\/dataset.std()","58e2aefe":"data = pd.concat([y , data_normalize.iloc[:,0:10]] , axis = 1)\ndata.head()","69977cbc":"data = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\ndata.head()","2c14081f":"plt.figure(figsize = (15,8))\nsns.violinplot(x = 'features',y = 'value',hue = 'diagnosis' , split=True ,inner='quart' , data = data)\nplt.xticks(rotation = 45)","b5ae5578":"data = pd.concat([y , data_normalize.iloc[:,10:20]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.violinplot(x = 'features',y = 'value',hue = 'diagnosis' , split=True ,inner='quart' , data = data)\nplt.xticks(rotation = 45)","2616236a":"data = pd.concat([y , data_normalize.iloc[:,20:]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.violinplot(x = 'features',y = 'value',hue = 'diagnosis' , split=True , inner = 'quart',data = data)\nplt.xticks(rotation = 45)","7459e457":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata = pd.concat([y , data_normalize.iloc[:,0:10]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.swarmplot(x = 'features',y = 'value',hue = 'diagnosis', data = data)\nplt.xticks(rotation = 45)","56c25e29":"data = pd.concat([y , data_normalize.iloc[:,10:20]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.swarmplot(x = 'features',y = 'value',hue = 'diagnosis', data = data)\nplt.xticks(rotation = 45)","5c4e7780":"data = pd.concat([y , data_normalize.iloc[:,20:]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.swarmplot(x = 'features',y = 'value',hue = 'diagnosis', data = data)\nplt.xticks(rotation = 45)","56451b05":"data = pd.concat([y , data_normalize.iloc[:,0:10]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.boxplot(x = 'features',y='value',data = data,hue = 'diagnosis')\nplt.xticks(rotation = 45)","0c5846cc":"data = pd.concat([y , data_normalize.iloc[:,10:20]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.boxplot(x = 'features',y='value',data = data,hue = 'diagnosis')\nplt.xticks(rotation = 45)","2ca8a887":"data = pd.concat([y , data_normalize.iloc[:,20:]] , axis = 1)\ndata = pd.melt(data, id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize = (15,8))\nsns.boxplot(x = 'features',y='value',data = data,hue = 'diagnosis')\nplt.xticks(rotation = 45)","78b715b2":"sns.jointplot(x = data_normalize.concavity_mean , y = data_normalize['concave points_mean'] , kind = 'regg').annotate(pearsonr)","449f3466":"sns.set(style = 'white')\ndata = data_normalize.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(data,diag_sharey=False)\ng.map_lower(sns.kdeplot,cmap='Blues_d')\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw=3)","6f2260cc":"f,ax = plt.subplots(figsize = (18,18))\nsns.heatmap(data_normalize.corr() , annot=True, linewidths=.5, fmt= '.1f',ax=ax)","a4624917":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean',\n              'radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst',\n              'concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\n\ndata = data_normalize.drop(drop_list1 , axis = 1)\ndata.head()","beff776c":"f,ax = plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr() , annot=True, linewidths=.5, fmt= '.1f',ax=ax)","106e627c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,accuracy_score,confusion_matrix\n\nx_train,x_test,y_train,y_test = train_test_split(data.values,y.values,test_size = 0.2,random_state = 42)\nclf_1 = RandomForestClassifier()\nclf_1.fit(x_train,y_train)\ny_pred = clf_1.predict(x_test)\naccuracy_score(y_pred,y_test)","40a87b5e":"sns.heatmap(confusion_matrix(y_pred,y_test) , annot = True , fmt = '.1f')","06f4ac52":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# We don't know how to select K, the simple way is to try different values of K \n\ndef validation_error(x_train,x_test,y_train,y_test):\n    m = 0\n    for k in range(3,10):\n        select_feature = SelectKBest(chi2 , k=k)\n        select_feature.fit(x_train,y_train)\n        x_train2 = select_feature.transform(x_train)\n        x_test2 = select_feature.transform(x_test)\n\n        clf_2 = RandomForestClassifier()\n        clf_2.fit(x_train2,y_train)\n\n        y_pred = clf_2.predict(x_test2)\n        acc = accuracy_score(y_pred,y_test)\n        if acc > m:\n            m = acc\n            model = clf_2\n            pred = y_pred\n            scores = select_feature.scores_\n    return (m,model,pred,scores)","8c10a8c0":"x_train,x_test,y_train,y_test = train_test_split(dataset.values,y.values,test_size = 0.2,random_state = 42)\n(acc,model,y_pred,scores) = validation_error(x_train,x_test,y_train,y_test)\nprint(scores,dataset.columns)\ncm = confusion_matrix(y_pred,y_test)\nprint('Accuracy Score is {}'.format(acc))\nsns.heatmap(cm,annot=True)","9aed0e9d":"from sklearn.feature_selection import RFE\nclf_3 = RandomForestClassifier()\nrfe = RFE(estimator = clf_3,n_features_to_select=5,step=1)\nrfe = rfe.fit(x_train,y_train)","2c41e347":"dataset.columns[rfe.support_]","02381e12":"from sklearn.feature_selection import RFECV\nclf_4 = RandomForestClassifier()\nrfecv = RFECV(estimator=clf_4,step=1,cv=5,scoring='accuracy')\nx_train,x_test,y_train,y_test = train_test_split(data.values,y.values,test_size = 0.2,random_state = 42)\nrfecv = rfecv.fit(x_train,y_train)","241f676c":"print('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', data.columns[rfecv.support_])","a65048e9":"rfecv.grid_scores_","6e340e04":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,8))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","d4625cec":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf_5.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), data.columns[indices],rotation=45)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","b8b5cbe5":"x_train, x_test, y_train, y_test = train_test_split(dataset,y,test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())\/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())\/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(np.cumsum(pca.explained_variance_ratio_), linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","147d4343":"X_reduced = pca.fit_transform(data)\n\nclf_rf_5.fit(x_train_N,y_train)\ny_pred = clf_rf_5.predict(x_test_N)\naccuracy_score(y_pred,y_test)","d4da6a21":"from boruta import BorutaPy","9498db94":"x_train,x_test,y_train,y_test = train_test_split(data_normalize,y,test_size=0.2,random_state=42)\nclf_6 = RandomForestClassifier(class_weight='balanced',max_depth=6)\nboruta_selector = BorutaPy(clf_6,n_estimators='auto',verbose=2)\nboruta_selector.fit(dataset.values,y.values)","825364b0":"print('Number of Selected Features',boruta_selector.n_features_)","0e1de4d6":"feature_df = pd.DataFrame(dataset.columns.tolist(),columns = ['features'])\nfeature_df['rank'] = boruta_selector.ranking_\nfeature_df = feature_df.sort_values('rank').reset_index(drop = True)\nfeature_df.head(boruta_selector.n_features_)","e3d687f8":"data_boruta = data_normalize[data_normalize.columns[boruta_selector.support_]]\ndata_boruta.head()","c35a21a1":"x_train,x_test,y_train,y_test = train_test_split(data_boruta,y,test_size=0.2,random_state=42)\nclf_6.fit(x_train,y_train)\ny_pred = clf_6.predict(x_test)\naccuracy_score(y_test,y_pred)","b20fe073":"Feature Selection using correlation matrix, the idea is we have to choose all unrelated features so the information loss is minimum. \nOkay, so radius_mean, perimeter_mean and area_mean are correlated so we drop any two of them, but looking at above plots we can choose smartly, the feature which shows more difference with our label is the best to choose. So by swarm plot I select **area_mean**. \nCompactness_mean, concavity_mean, concave points_mean.I choose concavity_mean. Apart from these, **radius_se**, perimeter_se and area_se are correlated and I only use **area_se**. radius_worst, perimeter_worst and area_worst are correlated so I use **area_worst**. Compactness_worst, concavity_worst and concave points_worst so I use **concavity_worst**. Compactness_se, concavity_se and concave points_se so I use **concavity_se**. texture_mean and texture_worst are correlated and I use **texture_mean**. area_worst and area_mean are correlated, I use **area_mean**.","641fd5d5":"# Visualisation\n\nFirst Ten Features","d6dd3498":"#### Reducing the dimensions to 5 would'nt lose much of the variance","11735db5":"Well, we choose our features but did we choose correctly ? Lets use random forest and find accuracy according to chosen features.","b7cf9338":"These are all set of features we have to look, we don't have any idea about these features which one is important and which one is not","3d146e2e":"Clearly we can observe difference in the values of different categories"}}