{"cell_type":{"c22c895b":"code","83ad01d9":"code","05212c8b":"code","4a6b1a16":"code","948642a7":"code","baebc323":"code","2df91832":"code","6eb05462":"code","bfad4a40":"code","ab66cf71":"code","4a650f0c":"code","11ee9ab9":"code","d89c9646":"code","388b3869":"code","9f0de227":"code","8c1a4f67":"code","ff29f383":"code","1cbf264a":"code","b52eecf6":"code","81ced439":"code","9c10c67f":"code","046c7c1f":"code","b2c42da4":"code","aa2c0f9b":"code","07cf28bf":"code","04339aca":"code","35630283":"code","484dd6d1":"code","928d92c9":"code","29c5e6c2":"code","52732133":"code","9b3d7e5d":"code","414af0dd":"code","0d790718":"code","d959dc3f":"code","7f08c3be":"code","3228aff2":"code","865354b5":"code","ae7edf38":"code","06bd499a":"code","01808b4e":"code","1cee2915":"markdown","9b3beade":"markdown","3622a3c1":"markdown","6bdc33cd":"markdown","5d6daf60":"markdown","9dcad0b8":"markdown","189bc922":"markdown","938638a9":"markdown","51dee0d5":"markdown","38009a4e":"markdown","7c55106b":"markdown","6c657c93":"markdown","3cce1402":"markdown","9aa8ffdd":"markdown","c93ede66":"markdown"},"source":{"c22c895b":"#import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","83ad01d9":"#import the dataset\ndf = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","05212c8b":"df.head()","4a6b1a16":"df.describe()","948642a7":"df.info()","baebc323":"df.isnull().sum()","2df91832":"sns.set(style = 'whitegrid' ,font = 'Rubik', font_scale = 1.1, rc={'axes.facecolor':'#f8f0e3', 'figure.facecolor':'#f8f0e3', \"axes.spines.right\": False, \"axes.spines.top\": False})\nsns.countplot(x='output', data=df, palette=[\"#003F7D\", \"#FD7702\"])\n","6eb05462":"df['output'].value_counts()","bfad4a40":"#Converting sex column into string representation of male-female to help in visualizations\n#Splitting the age column into ranges for neat and clean visualizations\ndf['range'] = pd.cut(df.age, [0, 10, 20, 30, 40, 50, 60 ,70, 80])\ndf['M\/F'] = df['sex'].apply(lambda x : 'male' if x == 1 else 'female')\ndf.head()","ab66cf71":"sns.set(style = 'whitegrid' ,font = 'Rubik', font_scale = 1.1, rc={'axes.facecolor':'#f8f0e3', 'figure.facecolor':'#f8f0e3', \"axes.spines.right\": False, \"axes.spines.top\": False})\nsns.countplot(x='M\/F', data=df, palette=[\"#003F7D\", \"#FD7702\"])","4a650f0c":"df['M\/F'].value_counts()","11ee9ab9":"#Age vs output\n\nplt.figure(figsize=(12, 9))\nsns.countplot(x='range', data = df, hue='output', palette=[\"#003F7D\", \"#FD7702\"])","d89c9646":"#Sex vs Ouput\nsns.countplot(x='M\/F', data = df, hue = 'output', palette=[\"#003F7D\", \"#FD7702\"])","388b3869":"df.groupby(['M\/F', 'output'])['output'].count()","9f0de227":"#age vs chestpain\n#age vs resting_blood_pressure\n#age vs cholestrol_in_mg\/dl\n#age vs fasting_blood_sugar\n#age vs resting_electrocardiographic\n#age vs maximum_heart_rate_acheived","8c1a4f67":"cat_attribute = ['cp','fbs', 'restecg']\nnum_attribute = ['trtbps', 'chol', 'thalachh']\n\nfor i in cat_attribute:\n    plt.figure(figsize=(12, 9))\n    sns.countplot(x='range', data = df, hue=i, palette=[\"#FD7702\",\"#003F7D\", '#FF0000', '#9400D4'])","ff29f383":"for i in num_attribute:\n    plt.figure(figsize=(12, 9))\n    sns.stripplot(x='range', y = i, data = df, palette=[\"#FD7702\",\"#003F7D\"])","1cbf264a":"df.groupby('range')['trtbps', 'chol', 'thalachh'].count()","b52eecf6":"#output vs age\n#output vs chestpain\n#output vs resting_blood_pressure\n#output vs cholestrol_in_mg\n#output vs fasting_blood_sugar\n#output vs resting_electrocardigraphic\n#output vs maximum_heart_rate_acheived\n#output vs previous_peak\n#output vs number_od_major_vessels","81ced439":"cat_attr = ['cp', 'fbs', 'restecg', 'caa']\nnum_attr = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\n\nfor i in cat_attr:\n    plt.figure(figsize=(9, 6))\n    sns.countplot(x=i, data = df, hue='output', palette=[\"#003F7D\", \"#FD7702\"])","9c10c67f":"for i in num_attr:\n    plt.figure(figsize=(9, 6))\n    sns.stripplot(x='output', y = i, data = df, palette=[\"#003F7D\", \"#FD7702\"])","046c7c1f":"#Distribution of numeric features with the target variable.\nparameters = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nfor i in parameters:\n    sns.displot(x = i, data=df, hue = 'output', kind = 'kde', palette=[\"#003F7D\", \"#FD7702\"])","b2c42da4":"#Correlation of features with respect to target variable\ncorr_matrix = df.corr()\ncorr_matrix['output'].sort_values(ascending=False)","aa2c0f9b":"#heatmap\nmatrix = np.triu(corr_matrix)\nplt.figure(figsize=(15, 11))\nsns.heatmap(corr_matrix, annot=True, mask=matrix, cmap = 'BuPu')","07cf28bf":"features = ['age', 'sex', 'cp', 'trtbps', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output']\nsns.pairplot(df[features], hue = 'output', height = 2, aspect=0.7)","04339aca":"df.head()","35630283":"#dropping the range and M\/F columns added earlier\ndf = df.drop(['range', 'M\/F'], axis = 1)\ndf.head()","484dd6d1":"#seperating the dependent variable from the independent variable\nX = df.drop('output', axis = 1)\ny = df['output'].values","928d92c9":"#one hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(drop='first'), [1, 2, 5, 6, 8, 10, 11, 12])], \n                       remainder='passthrough')\n\nX = ct.fit_transform(X)","29c5e6c2":"#spliting the dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","52732133":"#applying feature scaling to the data\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","9b3d7e5d":"#logistic regression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\n\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred_log = log_reg.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_log))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_log))\nprint('\\n')\nprint(classification_report(y_test, y_pred_log))\n","414af0dd":"#support vector classification\nfrom sklearn.svm import SVC\nsvc_clf = SVC()\nsvc_clf.fit(X_train, y_train)\ny_pred_svc = svc_clf.predict(X_test)\n\n\nprint(accuracy_score(y_test, y_pred_svc))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_svc))\nprint('\\n')\nprint(classification_report(y_test, y_pred_svc))","0d790718":"#random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc_clf = RandomForestClassifier()\nrfc_clf.fit(X_train, y_train)\ny_pred_rfc = rfc_clf.predict(X_test)\n\n\nprint(accuracy_score(y_test, y_pred_rfc))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_rfc))\nprint('\\n')\nprint(classification_report(y_test, y_pred_rfc))","d959dc3f":"#xgboost\nimport xgboost as xgb\nxgb_clf = xgb.XGBClassifier()\nxgb_clf.fit(X_train, y_train, eval_metric='error')\ny_pred_xgb = xgb_clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_xgb))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_xgb))\nprint('\\n')\nprint(classification_report(y_test, y_pred_xgb))","7f08c3be":"#hyperparaeter tuning of logistic regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid_log = {'penalty' : ['l2'], 'C' : [10, 1, 0.1, 0.01, 0.001], 'solver' : ['lbfgs', 'newton-cg']}\ngrid_log = GridSearchCV(LogisticRegression(), param_grid_log, scoring='accuracy', refit=True, verbose = 2)\ngrid_log.fit(X_train, y_train)\n\nlog_final_model = grid_log.best_estimator_\ny_pred_log_tune = log_final_model.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_log_tune))\n","3228aff2":"#hyperparameter tuning of SVC\nfrom sklearn.model_selection import GridSearchCV\nparam_grid_svc = {'C': [10, 20, 30, 40, 50, 60, 70], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\ngrid_svc = GridSearchCV(SVC(), param_grid_svc, refit=True, scoring = 'accuracy', cv = 3, verbose=2)\ngrid_svc.fit(X_train, y_train)\n\nsvc_final_model = grid_svc.best_estimator_\ny_pred_svc_tune = svc_final_model.predict(X_test)\n\naccuracy_score(y_test, y_pred_svc_tune)","865354b5":"#hyperparameter tuning of Random Forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid_rf = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid_rf, n_iter = 100, cv = 3, verbose=2, \n                               random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\nbest_random = rf_random.best_estimator_\n\n\ny_pred_rfc_tune = best_random.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_rfc_tune))","ae7edf38":"results = pd.DataFrame(columns =['accuracy_score', 'F1 score', 'accuracy_score_after_tuning'], index = ['Logistic Regression', 'Support Vector Classification', 'Random Forest Classifier', 'Xgboost'])","06bd499a":"results.loc['Logistic Regression'] = [accuracy_score(y_test, y_pred_log), round(f1_score(y_test, y_pred_log), 2), accuracy_score(y_test, y_pred_log_tune)]\nresults.loc['Support Vector Classification'] = [accuracy_score(y_test, y_pred_svc), round(f1_score(y_test, y_pred_svc), 2), accuracy_score(y_test, y_pred_svc_tune)]\nresults.loc['Random Forest Classifier'] = [accuracy_score(y_test, y_pred_rfc), round(f1_score(y_test, y_pred_rfc), 2), accuracy_score(y_test, y_pred_rfc_tune)]\nresults.loc['Xgboost'] = [accuracy_score(y_test, y_pred_xgb), round(f1_score(y_test, y_pred_xgb), 2), np.nan ]","01808b4e":"results","1cee2915":"##### Thanks for reading!\n##### Your feedbacks and comments would be appreciated.\n##### All the best analyzing and modeling data \ud83d\udc4d","9b3beade":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#f8f0e3;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\nCardiovascular diseases (CVDs) are the leading cause of death globally. An estimated 17.9 million people died from CVDs in 2019, \nrepresenting 32% of all global deaths. Of these deaths, 85% were due to heart attack and stroke.<br>\nWell it is important to identify risk of heart attack in patients, and treat them with proper medicine to prolong their life.\n<\/p>\n<\/div>","3622a3c1":"As we can see the target variable is not unevenly distributed, so we can use accuracy as the metric for model performance.","6bdc33cd":"#### Well it seems Logistic Regression performs the best in classifying and generalizing with the target variable.","5d6daf60":"People in the age range of 50-60 have higher chances of heart attack.","9dcad0b8":"## Symptoms of Heart Attack:\n\n* Severe pressure, fullness, squeezing, pain, or discomfort in the center of the chest that lasts for more than a few minutes\n* Pain or discomfort that spreads to the shoulders, neck, arms, or jaw\n* Chest pain that gets worse\n* Chest pain that doesn't get better with rest\n* Chest pain that happens along with any of these symptoms:\n    1. Sweating, cool, clammy skin, or paleness\n    2.Shortness of breath\n    3.Nausea or vomiting\n    4.Dizziness or fainting\n    5.Unexplained weakness or fatigue\n    6.Rapid or irregular pulse\n    ","189bc922":"As we can see from the above bar chart, females have more chances of heart attack than compared to male","938638a9":"As we can see from the above bar chart, people with the age of 50-60 have high chest pain(cp),fasting blood sugar(fbs) and resting electrocardiographic results(restecg).","51dee0d5":"# EDA","38009a4e":"# Model Building","7c55106b":"* People having chest pain(cp) type 2 : atypical angina have high chances of heart attack.\n* People with blood sugar less than 120 mg\/dl have chances of heart attack.\n* People with resting electrocardiographic results of value 1 : having ST-T wave abnormality have high chances of heart attack.\n* People with caa type 0 have high chances of heart attack.","6c657c93":"We will use the heart attack dataset containing following parameters, to predict the chances of heart attack in patients:\n\n* Age : Age of the patient\n* Sex : Sex of the patient\n* exang: exercise induced angina (1 = yes; 0 = no)\n* ca: number of major vessels (0-3)\n* cp : Chest Pain type chest pain type\n    * Value 1: typical angina\n    * Value 2: atypical angina\n    * Value 3: non-anginal pain\n    * Value 4: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* rest_ecg : resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n* target : 0= less chance of heart attack 1= more chance of heart attack","3cce1402":"# <center> <span style=\"font-size:35px;\"> Heart Attack <\/span> <\/center>\nHeart Attack (myocardial infarction) happens when one or more areas of the heart muscle don't get enough oxygen. This happens when blood flow to the heart muscle is blocked.","9aa8ffdd":"Age group of 50-60 has more instances of people having high resting blood pressure(trtbps), cholestrol(chol) and heart rate(thalachh).                                           ","c93ede66":"The dataset is unevenly distributed in terms of male-female ratio."}}