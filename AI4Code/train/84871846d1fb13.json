{"cell_type":{"fd32de33":"code","b0dd8724":"code","69f8b8d1":"code","d321ee00":"code","76c1ee6b":"code","283ce3c3":"code","93c829ae":"code","917035d4":"code","3d1c2a0e":"code","9ff1929e":"code","53785072":"code","4f91440f":"code","38116523":"code","2ca2aea6":"code","552d8ac3":"code","9e89a1c6":"code","c937a495":"code","a484a8ab":"code","e2cfc645":"code","e12eb2c1":"code","4d763d5f":"code","9f273202":"code","870df5ef":"code","f5c0ca2f":"markdown","112ec0db":"markdown","6d0c818f":"markdown","649cdbdc":"markdown","751d499e":"markdown","424eb0b2":"markdown","f6c8a047":"markdown","d302f1f3":"markdown","2796ffb7":"markdown","878d7017":"markdown","4079e5d9":"markdown","53e3fa88":"markdown","572e75b0":"markdown","a809d401":"markdown","ad568398":"markdown","51308b56":"markdown","dfab20da":"markdown","8457377d":"markdown","9601e249":"markdown","7eaa9acb":"markdown","9a7071a4":"markdown"},"source":{"fd32de33":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nreviews_original = pd.read_csv('\/kaggle\/input\/wine-reviews\/winemag-data-130k-v2.csv')\nreviews_original.rename(columns={'Unnamed: 0': 'WineReviewId'}, inplace = True)\n\nreviews_original.head(10)","b0dd8724":"def ExamineNullValues(df):\n    global null_ratio\n    df_isna = df.isna()\n    null_ratio = pd.DataFrame()\n    null_ratio['CountNullValues'] = df_isna.sum()\n    null_ratio.reset_index(inplace=True)\n    null_ratio['TotalValues'] = len(df)\n    null_ratio['PercentNullValues'] = null_ratio['CountNullValues']\/null_ratio['TotalValues']\n    null_ratio.sort_values(by=['PercentNullValues'], inplace=True)\n    return null_ratio;\n\nExamineNullValues(reviews_original)","69f8b8d1":"reviews = reviews_original.drop(['region_2'], axis = 1)\nreviews.head(5)","d321ee00":"reviews['designation'].nunique() # ~38,000 distinct values\ndesignation = pd.DataFrame(reviews.groupby(['designation'])['WineReviewId'].agg('count'))\ndesignation['Prevalence'] = designation['WineReviewId']\/designation.sum()[0]\ndesignation.sort_values(['Prevalence'], ascending = False, inplace = True)\ndisplay(designation)\n\nreviews.fillna(value = {'designation': 'Undefined'}, inplace = True)","76c1ee6b":"ExamineNullValues(reviews)","283ce3c3":"name_to_twitter = reviews.groupby(['taster_name'])['taster_twitter_handle'].agg('nunique')\ntwitter_to_name = reviews.groupby(['taster_twitter_handle'])['taster_name'].agg('nunique')\n\n\"\"\"\nEach name has only one twitter handle, but one twitter handle has two names. The twitter handle with two names does not have\nany rows where the twitter handle is populated and the name is not, so the twitter handle is not useful here.\n\"\"\"\ntwitter_check = reviews[(reviews['taster_twitter_handle'] == '@worldwineguys') & \\\n                        (reviews['taster_name'].isnull())]\n\n\"\"\"\nSince the twitter handle appears to be redundant, and has so many nulls, we'll remove that column. But first we'll check\nand see if there are rows where twitter handle is populated and name is not.\n\"\"\"\ntwitter_name_count = reviews.loc[reviews['taster_twitter_handle'].notnull() & \\\n                                 reviews['taster_name'].isnull()]\nprint('Total rows with twitter handle but no name: {}'.format(len(twitter_name_count)))","93c829ae":"reviews.drop(['taster_twitter_handle'], axis = 1, inplace = True)\nreviews['taster_name'].fillna('Undefined',inplace = True)","917035d4":"reviews.dropna(subset=['variety'],inplace=True)","3d1c2a0e":"array = ['country','province']\nfor i in range(0,len(array)):\n    col = array[i]\n    wine_loc = reviews.groupby(['winery'])[col] \\\n                          .value_counts(normalize=True) \\\n                          .unstack() \\\n                          .fillna(0) \\\n                          .reset_index()\n    wine_loc = wine_loc.melt(id_vars = 'winery', var_name = col, value_name = 'percent')\n    wine_loc = wine_loc.sort_values('percent', ascending = False).groupby('winery', as_index=False).first()\n    wine_loc.drop(wine_loc[wine_loc.percent <= 0.5].index, inplace=True)\n    mapping = dict(wine_loc.drop(['percent'], axis = 1).values)\n    reviews[col].fillna(reviews['winery'].map(mapping), inplace = True)\n\nwine_loc.sample(n = 10)","9ff1929e":"reviews.fillna(value = {'province': 'Undefined', 'country': 'Undefined'}, inplace = True)","53785072":"ExamineNullValues(reviews)","4f91440f":"missing_reg = reviews.loc[reviews['region_1'].isnull()]\nmissing_reg = missing_reg.isna().sum()\nprint(missing_reg)","38116523":"reg_impute = reviews[['country','province','winery','region_1']] \\\n            .dropna(subset=['region_1'])\nreg_impute_cts = reg_impute.groupby(['country','province','winery'])['region_1'] \\\n                      .value_counts(normalize=True) \\\n                      .unstack() \\\n                      .fillna(0) \\\n                      .reset_index()\nreg_impute_cts = reg_impute_cts.melt(id_vars = ['winery','country','province'], \\\n                                     var_name = 'region_1', \\\n                                     value_name = 'percent')\nreg_impute_cts = reg_impute_cts.sort_values('percent', ascending = False) \\\n                               .groupby(['winery','country','province'], as_index=False) \\\n                               .first()\n\n# Create a key to easily join back to the original dataset\nreg_impute_cts['key'] = reg_impute_cts['winery'] + '|' + reg_impute_cts['country'] + '|' + reg_impute_cts['province']\n\nreg_impute_cts.head(10)","2ca2aea6":"mapping = dict(reg_impute_cts[['key','region_1']].values)\nreviews['key'] = reviews['winery'] + '|' + reviews['country'] + '|' + reviews['province']\nreviews['region_1'].fillna(reviews['key'].map(mapping), inplace = True)\nreviews.drop(['key'], axis = 1, inplace = True)\nreviews['region_1'].fillna('Undefined', inplace = True)","552d8ac3":"ExamineNullValues(reviews)","9e89a1c6":"reviews[['points','price']].corr(method = 'pearson')","c937a495":"price_impute = reviews[['points','price']].dropna()\nX = price_impute['points']\nX = np.array(X.values.tolist()).reshape(-1,1)\nY = price_impute['price']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)\nmod = LinearRegression()\nmod.fit(X_train,Y_train)\nY_pred = mod.predict(X_test)","a484a8ab":"Y_test_arr = np.array(Y_test)\nY_pred_arr = np.array(Y_pred)\nY_test_arr_usingMean = np.array([Y.mean()]*len(Y_test))\ndataset = pd.DataFrame({'Y_test': Y_test, 'Y_pred': list(Y_pred), 'Y_test_usingMean': list(Y_test_arr_usingMean)}, columns=['Y_test', 'Y_pred', 'Y_test_usingMean'])\nplt.scatter(dataset['Y_test'], dataset['Y_pred'], color='blue',linewidth=3)\nplt.scatter(dataset['Y_test'], dataset['Y_test_usingMean'], color='red',linewidth=3)","e2cfc645":"dataset_mod = dataset.loc[dataset['Y_test'] <= 600]\nplt.scatter(dataset_mod['Y_test'], dataset_mod['Y_pred'], color='blue',linewidth=3)\nplt.scatter(dataset_mod['Y_test'], dataset_mod['Y_test_usingMean'], color='red',linewidth=3)","e12eb2c1":"dataset_mod = dataset.loc[dataset['Y_test'] <= 100]\nplt.scatter(dataset_mod['Y_test'], dataset_mod['Y_pred'], color='blue',linewidth=3)\nm, b = np.polyfit(dataset_mod['Y_test'], dataset_mod['Y_pred'], 1)\nplt.plot(dataset_mod['Y_test'], m*dataset_mod['Y_test']+b, color = 'black',linewidth = 3)\nplt.scatter(dataset_mod['Y_test'], dataset_mod['Y_test_usingMean'], color='red',linewidth=3)\nplt.legend(['LinRegLine','ModelPred','Mean'])","4d763d5f":"# Compute metrics to evaluate model using our predicted value\nMSE = mean_squared_error(Y_test,Y_pred)\nRMSE = math.sqrt(mean_squared_error(Y_test,Y_pred))\nr2 = r2_score(Y_test,Y_pred)\n\n# Compute metrics to evaluate model using mean instead of predicted value\nMSE_usingMean = mean_squared_error(Y_test,Y_test_arr_usingMean)\nRMSE_usingMean = math.sqrt(mean_squared_error(Y_test,Y_test_arr_usingMean))\nr2_usingMean = r2_score(Y_test,Y_test_arr_usingMean)\n\noutput = pd.DataFrame(data = [[MSE,RMSE,round(r2,4)],[MSE_usingMean,RMSE_usingMean,round(r2_usingMean,4)]], \\\n                      columns = ['MeanSquareError','RootMeanSquareError','R-Squared'], \\\n                      index = ['UsingPrediction','UsingMean'] \\\n                     )\ndisplay(output)","9f273202":"X = reviews['points']\nX = np.array(X.values.tolist()).reshape(-1,1)\nY_pred = mod.predict(X)\nreviews['pred_price'] = Y_pred\nreviews['price'].fillna(reviews['pred_price'], inplace = True)\nreviews.drop(['pred_price'], axis = 1, inplace = True)\ndisplay(reviews)","870df5ef":"reviews.isna().sum()","f5c0ca2f":"Looking at the 'country' and 'province' null ratios, we only have 63 rows where those values are missing. I believe we can use the 'winery' attribute to try to impute 'country' and 'province', so I examine the relationship between 'winery', 'province', and 'country' where the data exists. I decide to use the mode values from any winery where each combination of province and country occurs more than 50% of the time.","112ec0db":"I define a dictionary mapping using the information from the dataframe above and join to fill null values based on the shared 'key' I created. I'll fill any remaining null 'region_1' values with 'Undefined'.","6d0c818f":"It's a little hard to tell, so I'll focus on wines less than $600:","649cdbdc":"Unfortunately we could only fill in half the missing values, so we'll set the remaining null values to 'Undefined'. Depending on compute time, this process may not have been worth it for the amount of information gained.","751d499e":"The column with the next highest amount of null values (29%) is 'designation'. It appears to contain valuable information that is not represented elsewhere in the dataset.\n\nAs shown in the dataframe below, the mode value only occurs 2% of the time. Since there isn't a clear value that appears substantially more than any other, I'll fill in null values for 'designation' with 'Undefined'.","424eb0b2":"# Demonstrating Data Preprocessing\nThis notebook shows data preprocessing for the Wine Reviews dataset. Once the data has been preprocessed, it is ready for further exploratory analysis or building a machine learning model.\n\nBelow is an outline of my process:\n\n1. Import libraries and read in the dataset.\n2. Build a function I can repeatedly call to assess the ratio of null values in each column.\n3. Work through each column that has null values and determine how best to handle them.\n\n### I use a few different methods to deal with null values:\n* Building a linear regression model to predict missing numerical values\n* Finding relationships between variables to create a dictionary mapping (example: using the mode for categorical variables to fill in nulls)\n* Removing redundant columns with a high percentage of null values\n* Where applicable, filling in 'Undefined' if we don't want to lose data and can't reasonably impute the values","f6c8a047":"From the above dataframe, I see 'region_2' has the most null values of any attribute, at 61%. Looking at the wine reviews dataset, it appears I have enough information in 'region_1' that would make 'region_2' somewhat redundant. I don't want to impute 61% of the values, so I will remove the column altogether:","d302f1f3":"I can see that all records with a missing 'region_1' value have populated 'winery', 'country', and 'province' values:","2796ffb7":"It looks better, but let's see how the model performs for wines under $100.","878d7017":"So I'll use 'country', 'province', and 'winery' to impute the missing 'region_1' values.","4079e5d9":"It appears that the linear regression model is going to be more accurate than just using the mean to impute the missing price values. I'll compute the MSE, RMSE, and R-squared values to compare the actual price values to predicted, and actual price values to the mean.","53e3fa88":"I only have one attribute left to work on, which is 'price':","572e75b0":"I want to see what our true values look like compared to our predictions and the mean value. I'll create a quick scatterplot to see if I'm on the right track with the model.","a809d401":"I'll rerun the function to examine nulls and see what we have left:","ad568398":"I've made the decision to use a linear regression model to predict the missing price values.\n\nI don't want to delete 8,996 rows, and I think using the mean or median would be quick but perhaps inaccurate given the wide range of wine prices in the dataset.\n\nOnce I've built the model and predicted the missing price values,  I'll compare the output to what would have happened if I had just used the mean or median value to see if I made the right decision.\n\nBelow, I see the correlation between 'points' and 'price' might be strong enough to create some reasonable predictions.","51308b56":"Next, I notice the 'variety' column has only 1 row that is null. I'll go ahead and drop that row since our total dataset size is so large (~130,000).","dfab20da":"Based on this, I'll remove 'taster_twitter_handle' since there are so many nulls and it doesn't appear to give us information we don't already have with 'taster_name'. I'll then fill 'taster_name' nulls with 'Undefined'.","8457377d":"Finally, I confirm we have no more missing values in our dataset, and the data is ready to use.","9601e249":"My model predicted prices that are more accurate than just using the mean value of all prices in the dataset, and it didn't take a lot of compute time to build. I'll fill in the null values now using the predicted values:","7eaa9acb":"Next up is 'taster_twitter_handle' with 24% null values. I notice that 'taster_name' may contain similar information, as we saw in the 'region_1', 'region_2' scenario. Examining that further:","9a7071a4":"Let's see what column is next for filling in null values:"}}