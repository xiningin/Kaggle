{"cell_type":{"ab5534fb":"code","b379498a":"code","c4c9f777":"code","94f8dcba":"code","746dadd1":"code","a1f3ff6d":"code","f415d887":"code","513b6421":"code","52fd1161":"code","bf4adec5":"code","817a6fb3":"code","d0f1b4ed":"code","c5dcc099":"code","e3c0890a":"code","49fe30dc":"code","6e258184":"markdown","f41dc0aa":"markdown","0998052f":"markdown","1966c7f3":"markdown","9c70c971":"markdown","ac80cc0a":"markdown","ec039fbf":"markdown"},"source":{"ab5534fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b379498a":"data = pd.read_csv(\"..\/input\/c_0000.csv\",sep=\",\")","c4c9f777":"data.head()","94f8dcba":"# We filter \"id\" columns because id has no effect on relations.\ndata = data.loc[:,data.columns != 'id']","746dadd1":"data.head()","a1f3ff6d":"data2 = data.iloc[:,[0,3]]","f415d887":"data2.head()\n","513b6421":"plt.scatter(data2.x,data2.vx,color=\"green\")\nplt.xlabel(\"X positon of stars\")\nplt.ylabel(\"Velocity in X axis of stars\")\nplt.show()","52fd1161":"from sklearn.cluster import  KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data2)\n    wcss.append(kmeans.inertia_) # inertia means that find to value of wcss\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","bf4adec5":"# we can take elbow as 4\nkmean2 = KMeans(n_clusters=4)\nclusters = kmean2.fit_predict(data2)\n\ndata2[\"label\"] = clusters\n\nplt.scatter(data2.x[data2.label == 0], data2.vx[data2.label == 0], color=\"red\")\nplt.scatter(data2.x[data2.label == 1], data2.vx[data2.label == 1], color=\"blue\")\nplt.scatter(data2.x[data2.label == 2], data2.vx[data2.label == 2], color=\"green\")\nplt.scatter(data2.x[data2.label == 3], data2.vx[data2.label == 3], color=\"purple\")\n\nplt.scatter(kmean2.cluster_centers_[:,0],kmean2.cluster_centers_[:,1], color=\"orange\") # scentroidler\n\nplt.show()","817a6fb3":"# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans3 = KMeans(n_clusters=i)\n    kmeans3.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","d0f1b4ed":"data3 = data2.iloc[:,data2.columns != 'label'].head(1000)","c5dcc099":"# dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data3, method=\"ward\") # scipy is an algorithm of hiyerarchal clusturing\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n","e3c0890a":"\n# HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters=4, affinity=\"euclidean\",linkage=\"ward\")\ncluster = hiyerartical_cluster.fit_predict(data3)\n\ndata3[\"label\"] = cluster","49fe30dc":"plt.scatter(data3.x[data3.label == 0], data3.vx[data3.label == 0], color=\"red\")\nplt.scatter(data3.x[data3.label == 1], data3.vx[data3.label == 1], color=\"blue\")\nplt.scatter(data3.x[data3.label == 2], data3.vx[data3.label == 2], color=\"green\")\nplt.scatter(data3.x[data3.label == 3], data3.vx[data3.label == 3], color=\"purple\")\n","6e258184":"**1. EDA**","f41dc0aa":" **4. Hierarchical Clustering**","0998052f":"**5. Conclusion**\n\nThere is a great difference between K-Means and Hierarchical Clustering methods as you see in graphics.\n","1966c7f3":"**2. Visaualization**","9c70c971":"It looks that 4 has an elbow point.","ac80cc0a":"**3. K-Means**","ec039fbf":"**EXPLANATION**\n\nIn this kernel, I used 2 different clustering methods on Star Cluster dataset. They are K-Means and Hierarchical Clustering .\n\n**CONTENTS**\n\n**1. EDA**\n\nI am looking data in general and delete columns like \"id\".\n\n**2. Visualization**\n\n**3. K-Means**\n\n**4. Hierarchical Clustering**\n\n**5. Conclusion**"}}