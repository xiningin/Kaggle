{"cell_type":{"e9e0c127":"code","d3be6c6c":"code","cd1db35e":"code","632590eb":"code","ba5a39e2":"code","278e8623":"code","e0cb6f82":"code","1c77d112":"code","1b6cb753":"code","70056dfc":"code","7f2e9162":"code","49b1bc3b":"code","4e7b98e5":"code","a1a29b4d":"code","428d604b":"code","87cbcd84":"code","ba836ed9":"code","544b1637":"code","25921f6b":"code","43cbae54":"code","d0e6cfef":"code","d6a4d2cf":"code","546cc93f":"code","cd8c6fc6":"code","648441cb":"code","6db9ddf2":"code","e8dbfd9d":"code","4d642603":"code","25778b59":"code","9d9a69a8":"code","4ee9654a":"code","c503675d":"code","5e997a47":"code","40c44d68":"code","06712961":"code","3b86c8ea":"code","34469229":"code","428b714a":"code","5210e794":"code","ae30d507":"code","2671bd10":"code","4c98692d":"code","2925d1ae":"code","19fd7d93":"code","402ad182":"code","d768845e":"code","bc647ac1":"code","809f69ad":"code","507ab09c":"code","05b13dfa":"code","f1bd99f1":"code","6327f034":"code","20af8fab":"code","8b3cadf5":"code","62707cc1":"code","9aa07c65":"code","72511052":"markdown","a37f4982":"markdown","9b272d2b":"markdown","5c472cb9":"markdown","1a3338b3":"markdown","109e0391":"markdown","12132f06":"markdown","fb1be2e0":"markdown","b765fe42":"markdown","c4049cd8":"markdown","b849c69f":"markdown","adb24827":"markdown","7b3d47bf":"markdown","f6606849":"markdown","b434390c":"markdown","8a97389d":"markdown","58fd6206":"markdown","0a23b7e3":"markdown","38a8e139":"markdown","fa51c227":"markdown"},"source":{"e9e0c127":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3be6c6c":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","cd1db35e":"train.head()","632590eb":"n = len(train)\ntrain.shape","ba5a39e2":"train.dtypes.value_counts()","278e8623":"train.isna().sum() \/ train.shape[0] * 100","e0cb6f82":"plt.figure(figsize=(10, 5))\nsns.heatmap(train.isna())","1c77d112":"train.info()","1b6cb753":"train.describe(include='all')","70056dfc":"(train.Survived.value_counts() \/ train.shape[0] * 100).plot.bar(title='Target distribution')","7f2e9162":"quantitative_col = [ 'Pclass', 'Age', 'SibSp',\n       'Parch', 'Fare']\n\nfor col in quantitative_col :\n    plt.figure(figsize=(10, 5))\n    sns.distplot(train[col])","49b1bc3b":"train.select_dtypes('object').columns","4e7b98e5":"qualitative_col = ['Sex', 'Ticket', 'Cabin', 'Embarked']\n\nfor col in qualitative_col :\n    print(f'{col :-<50} {train[col].unique()}')","a1a29b4d":"train.drop('PassengerId', axis = 1, inplace = True)","428d604b":"# color palette from seaborn\ncm = sns.light_palette(\"green\", as_cmap=True)\n \n# Visualizing the DataFrame with set precision\ntrain.corr().style.background_gradient(cmap=cm).set_precision(2)","87cbcd84":"survived_people = train[train.Survived == 1]\nunsurvived_people = train[train.Survived == 0]","ba836ed9":"for col in quantitative_col :\n    plt.figure()\n    sns.distplot(survived_people[col], label='survived')\n    sns.distplot(unsurvived_people[col], label='unsurvived')\n    plt.legend()","544b1637":"for col in ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'] :\n    pd.crosstab(train['Survived'], train[col]).plot.bar()","25921f6b":"data = [train, test]\n\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'\n\naxes = sns.factorplot('relatives','Survived', \n                      data=train, aspect = 2.5, )","43cbae54":"# concatenate train and test set for the pre-processing\ndf = train.append(test).reset_index(drop=True)","d0e6cfef":"for i in range(len(df)):\n    if not(pd.isnull(df['Cabin'].iloc[i])):\n        df['Cabin'].iloc[i]=df['Cabin'].iloc[i][0] \n    else :\n        df['Cabin'].iloc[i]='No'","d6a4d2cf":"# add familly size column\ndf['Fsize'] = df['Parch'] + df['SibSp'] + 1","546cc93f":"df['travelled_alone'] = 'No'\ndf.loc[df.Fsize == 1, 'travelled_alone'] = 'Yes'","cd8c6fc6":"df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Capt', 'Col', 'Rev', 'Don', 'Countess', 'Jonkheer', 'Dona', 'Sir', 'Dr', 'Major', 'Dr'], 'Rare')\ndf['Title'] = df['Title'].replace(['Mlle', 'Mme', 'Ms'], 'Miss')\ndf['Title'] = df['Title'].replace(['Lady'], 'Mrs')","648441cb":"df.Embarked.fillna(train.Embarked.mode()[0], inplace = True)","6db9ddf2":"mean = train[\"Age\"].mean()\nstd = train[\"Age\"].std()\n\nis_null = df[\"Age\"].isnull().sum()\n# compute random numbers between the mean, std and is_null\nrand_age = np.random.randint(mean - std, mean + std, size = is_null)\n# fill NaN values in Age column with random values generated\nage_slice = df[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\n\ndf[\"Age\"] = age_slice\ndf[\"Age\"] = df[\"Age\"].astype(int)","e8dbfd9d":"df['Fare'].fillna(train['Fare'].mean(), inplace = True)","4d642603":"plt.figure(figsize=(12,6))\nplt.scatter(train.Fare, train.Survived)","25778b59":"train.Fare.max()","9d9a69a8":"test.Fare.max()","4ee9654a":"train[train.Fare>300].Fare.count() # there is just three elements between 300 and 5** so we will remove it","c503675d":"test[test.Fare>300].Fare.count()","5e997a47":"df.columns","40c44d68":"features = [\"Sex\", \"Pclass\",\"travelled_alone\", \"Cabin\", \"Embarked\", \"Title\"]","06712961":"df=pd.get_dummies(df,columns=features,drop_first=True)","3b86c8ea":"df.head(2)","34469229":"df.drop(['Name', 'Ticket'], axis = 1, inplace = True)","428b714a":"train = df[:n ] # the three outliers\ntest = df[n:]","5210e794":"train.isna().sum() \/ train.shape[0] * 100","ae30d507":"test.isna().sum() \/ test.shape[0] * 100","2671bd10":"X = train.drop(['Survived','PassengerId'], axis = 1)\ny = train.Survived\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","4c98692d":"def evaluation(model):\n    \n    model.fit(x_train, y_train)\n    ypred = model.predict(x_test)\n    \n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))\n    \n    N, train_score, val_score = learning_curve(model, x_train, y_train,\n                                              cv=4, scoring='f1',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()\n    \n    ","2925d1ae":"model = RandomForestClassifier(random_state=0)","19fd7d93":"evaluation(model)","402ad182":"preprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False), SelectKBest(f_classif, k=10))","d768845e":"RandomForest = make_pipeline(preprocessor, RandomForestClassifier(random_state=0))\nSVM = make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\nKNN = make_pipeline(preprocessor, StandardScaler(), KNeighborsClassifier())","bc647ac1":"dict_of_models = {'RandomForest': RandomForest,\n                  'SVM': SVM,\n                  'KNN': KNN\n                 }","809f69ad":"for name, model in dict_of_models.items():\n    print(name)\n    evaluation(model)","507ab09c":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","05b13dfa":"SVM","f1bd99f1":"hyper_params = {'svc__gamma':[1e-3, 1e-4, 0.0005],\n                'svc__C':[1, 10, 100, 1000, 3000], \n               'pipeline__polynomialfeatures__degree':[2, 3],\n               'pipeline__selectkbest__k': range(45, 60)}","6327f034":"grid = RandomizedSearchCV(SVM, hyper_params, scoring='recall', cv=4,\n                          n_iter=40)\n\ngrid.fit(x_train, y_train)\nprint(grid.best_params_)","20af8fab":"evaluation(grid.best_estimator_)","8b3cadf5":"submit = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","62707cc1":"submit['Survived']=grid.predict(test.drop(['Survived', 'PassengerId'], axis = 1)).astype('int')\nsubmit.to_csv('submission.csv',index=False)","9aa07c65":"submit.head(2)","72511052":"## **Objective** : Predict if a passenger can survive on the titanic or not.","a37f4982":" ## Qualitative variables :","9b272d2b":"## Imputation :","5c472cb9":"# 1. Exploratory Data Analysis\n- Identify the target :\nSurvived\n- Number of lines and columns :\n(891, 12)\n- Type of variables :\nQuantitative variables : 7\nQualitative : 5\n- Identification of missing values :\n    - a lot of Nan variables in Cabin column = 70 % -->  missing values can be a significant information in that case\n    - many Nan variables in Age column = 20 % \n","1a3338b3":"- Embarked and Sex columns can easily be encoded\n- for Ticket, cabin and name columns we can extract other meaningful ones = Feature engineering ","109e0391":"## Encoding :","12132f06":"- The target is not perfectly distributed on Survived and unSurvived peaple, so using F-score as a metric is a good option\n- Pclass, SibSp and Parch can be encoded as they contain just some diffirent values \n- Age and Fare may be normalized","fb1be2e0":"- Pclass and Fare are highly correlated with our target and between each other, also SibSp and Parch have significant correlation with it and also between each other\n- The younger you are the more likely to survive\n- More people in class 3 died\n- females have more chance to survive\n- Alone People have more chance to dy and if you travel with 1 to 3 people you have more chance to survive\n- People who travel from C have more chance to not survive","b765fe42":"# 2. Pre-Processing :","c4049cd8":"    There are probably other outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. \n  \n  Outliers removal is note always safe. \n  \n  Ps : I tried with removing these three samples (Fare column) and the result become worse!  So I will keep it.","b849c69f":"# 3. Modeling and Evaluation :","adb24827":" ## Relationship between target and variables :","7b3d47bf":"## Train test split :","f6606849":" ## Quantitative variables :","b434390c":"## Import Libraries :","8a97389d":"### If you find this notebook useful, please don't forget to upvote it!","58fd6206":"## Feature Engineering :","0a23b7e3":"let's see what hapens with SibSp and Parch :","38a8e139":" ## Target visualization :","fa51c227":"## Read Data :"}}