{"cell_type":{"be5a4603":"code","983d177d":"code","c4c962c5":"code","3e9a6bab":"code","dffcea2e":"code","177c26a4":"code","11d106c8":"code","8e2b9ef6":"code","da6cdc87":"code","fce6cd0f":"code","51e90d89":"code","35674215":"code","6c8f1339":"markdown","6931e72d":"markdown","a3510600":"markdown","ab894f33":"markdown","9a0d42e5":"markdown","71ad8b30":"markdown","6ffd7fe7":"markdown","0103ffc7":"markdown","80690379":"markdown","23802cc2":"markdown","ee90e467":"markdown","52e3f45b":"markdown","bb12ff2d":"markdown","9726faf7":"markdown","141a1f99":"markdown","b7f3cec4":"markdown"},"source":{"be5a4603":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #plottting\nimport seaborn as sns # more plotting\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","983d177d":"# Load in only the training data as this is what we will be exploring \ntrain_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")","c4c962c5":"# Print the features and the first 5 rows of the dataset\nprint(train_data.columns.values)\ntrain_data.head(5)","3e9a6bab":"# Print the length of the data\nprint(\"# of training Rows = \", len(train_data))\n\n# Check for NaNs in the data\nprint(\"NaNs in each training Feature\")\ndfNull = train_data.isnull().sum().to_frame('nulls')\nprint(dfNull.loc[dfNull['nulls'] > 0]) # Print only features that have Null values","dffcea2e":"# Drop the features mentioned above\ntrain_data.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], inplace=True)\n\n# Checking if any features take all the same value in which case they could be removed (in this case none are removed)\nfor i in train_data.keys():\n    if len(pd.unique(train_data[i])) < 2:\n        print(i)","177c26a4":"# Plot a heatmap of the correlations for a quick check on the numerical features\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train_data.corr(), annot = False, cmap='viridis', square=True)","11d106c8":"# Make histogram of SalePrice using seaborn\nsns.set(color_codes=True)\nfig, ax = plt.subplots(1,1, figsize=(15,6))\nsns.distplot(train_data['SalePrice']\/1000.,ax=ax, bins=40, rug=True) # Plot price in $1000s\nax.set(xlabel='SalePrice ($1000s)')","8e2b9ef6":"# Find the 10 largest correlations to SalePrice\nbigCorr = train_data.corr().nlargest(10, 'SalePrice')['SalePrice']\nprint(bigCorr)\n\n# Find the 10 largest anti-correlations to SalePrice\nbigAnti = train_data.corr().nsmallest(10, 'SalePrice')['SalePrice']\nprint(bigAnti)","da6cdc87":"sns.pairplot(train_data[bigCorr.index])","fce6cd0f":"train_data.hist(figsize=(16,20), bins=20)","51e90d89":"# Make a list of the continuous and categorical features to plot\ncontinuous_keys = ['SalePrice', 'LotArea']\ncategorical_keys = ['OverallCond', 'OverallQual']\n\n# Iterate through each feature and plot\nfor i in continuous_keys:\n    sns.catplot(x = 'ExterQual',y=i, data = train_data)\nfor i in categorical_keys:\n    sns.catplot(x = 'ExterQual',y=i, kind='bar', data = train_data)","35674215":"# Find the 10 largest correlations to SalePrice\nbigCorr = train_data.corr().nlargest(10, 'LotFrontage')['LotFrontage']\nprint(bigCorr)\n\n# Find the 10 largest anti-correlations to SalePrice\nbigAnti = train_data.corr().nsmallest(10, 'LotFrontage')['LotFrontage']\nprint(bigAnti)","6c8f1339":"## 3.1. Feature Correlations\n\nI always think it is a good idea to plot a quick heatmap at the start in order to get an overview of some obvious correlations. A heatmap will tell you a large amount of information about the variables in only a couple of lines of code! Perfect! The heatmap will however not show correlations of text-based categories because they must be converted to numbers first. We can plot a heatmap as shown below.","6931e72d":"# 3. Feature Distributions and Correlations\n\nNow that we have done some preliminary data cleaning (i.e. removing the columns that we don't need) we can start exploring the dataset to see what distributions and correlations between features exist. There is no 'perfect' way of exploring data so my advice is to get stuck in and see what you find.","a3510600":"### 3.2.2. SalePrice Correlations\n\nNext we can list the strongest (anti-)correlations to SalePrice from the heatmap above. It is definitely important to know which features are closely (anti-)correlated to the SalePrice as these will be the features that help our prediction to be as accurate as possible.","ab894f33":"Now that is A LOT of plots! \n\nI won't (and can't!) detail every bit of information that we can get from the above plots, but it is worth having a look at the output above to get some more intuition about the dataset. On the diagonal, for example, we can see histograms of the features. This tells us that some of the features approximate normal distributions (e.g. TotRmsAbvGrd) whereas others are more uniform (e.g. FullBath).\n\nThere are also some obvious outliers in the data that may need to be removed when we are fitting our models. This is good to know and be prepared for.\n\nAs the 10 strongest anti-correlations are much weaker than the 10 strongest correlations, we will not make all the plots for the anti-correlations but this could be done using one line of code as above.\n\n\n## 3.3. Feature Distributions\n\n### 3.3.1. Numerical Features\nWe can get a quick overview of the distributions of all numerical features by plotting histograms for them as shown below.","9a0d42e5":"## 2.2. Removing Features With Too Many Missing Values\n\nIn general, when we have missing values in our data there are two ways to handle them. The first is to fill in the missing values by using correlations between other features to predict the 'correct' value. This works well when only a few values are missing and the values are reasonably predictable. However, when the number of missing values is too large it is difficult to fill them in a meaningful way. As such, it can be much more helpful to drop those features instead of trying to train a model on them.\n\nWe will now check the number of missing values for each feature in our dataset and remove the ones that we think are not easily fillable (i.e. they have too many missing values).","71ad8b30":"From the histogram we can see that the distribution deviates from normal with positive skewness. There are also some outliers in terms of SalePrice. We may want to exclude these in the training data to improve our model. In fact, there are very few houses worth more than $500,000.","6ffd7fe7":"Now the above heatmap shows correlations between variables by the colour of the square. With the color scheme (called a colour map) that I have used, yellow shows strong positive correlations whereas purple shows strong negative correlations. Blue shows that there is a minimal, if any, correlation between two variables.\n\nNote: the yellow diagonal line through the middle is where the correlation between the same feature is measured (i.e. the correlation between SalePrice and SalePrice is obviously 1).\n\nUsing this colour scheme, we can see that there are some strong correlations. These include but are definitely not limited to:\n* GarageCars and GarageArea (which is makes sense as the bigger the garage the more cars can fit in it)\n* GarageYrBlt and YearBuilt (which again makes sense as garages are likely built at the same time as the house)\n* GrLivArea and TotRmsAbvGrd\n\nI have not listed all the strong (anti-) correlations for the sake of space but you can always refer back to this heatmap to check how two variables may be correlated.\n\n## 3.2. Exploring SalePrice\n\n### 3.2.1. SalePrice Distribution\n\nBefore we go any further, it is worthwhile taking a look at the feature 'SalePrice'. This is after all the quantity that we are aiming to predict. We can start by plotting a histogram of the SalePrice data as outlined below.","0103ffc7":"The output of the above cell provides us with the paths to the data files that we wish to import. They are saved as *.csv* files and so all we have to do is use the *read_csv* option in the *Pandas* module. The following code saves the data as a *Pandas Dataframe* (you can imagine it like a table of data) with the variable name *train_data*.","80690379":"Firstly, from the output we know that there are 1460 training rows. This would be considered as quite a small amount of rows for a more general problem, but as a learning competition this will be sufficient.\n\nSecondly, we can tell from the above list that there are some features that contain NaN values. The ones with really high numbers of NaNs (> 600) should be removed as it will be very difficult to fill in the missing values to train a model on. This means that we should drop the following features before doing anymore data exploration:\n* Alley\n* FireplaceQu\n* PoolQC\n* Fence\n* MiscFeature\n\nWe have kept the feature 'LotFrontage' in the training data here even though it has a non-negligible number of missing values (259). Before training a model, we will have to fill in these values but that is for the next notebook. For now, we just care about understanding the data.","23802cc2":"## 3.4. LotFrontage\n\nAs the LotFrontage feature had a lot of missing values, it is worth checking what it is most correlated to. This will help us decide if it is possible to fill the missing values by using other known features about the property. We can check the correlations of LotFrontage in the same way we did for SalePrice.","ee90e467":"From the output, we can see that there are quite a few features that are strongly correlated with LotFrontage. This should help us fill in the missing values and so for now, we will not drop the LotFrontage feature.\n\nOkay that's all for now as an initial look at the data for this competition. If you are planning on making predictions with this dataset I would recommend repeating this analysis for most of the features just to understand how they are related to each other. Finally, if you've got this far **please give this an upvote, it really helps!**.\n","52e3f45b":"It is always a good idea to think about the most correlated features and see if they are logical. As an example, it makes sense that SalePrice is closely correlated to the GrLivArea as bigger houses are more expensive. We can repeat this for each feature to sense check that the data is behaving as we would expect. \n\nNow that we know what the strongest correlated features to SalePrice are, it is worthwhile exploring their distributions. We can do this with one line of code as below.","bb12ff2d":"Whilst we won't really discuss the distributions of most of the features here. This is another good way to summarise the data and may be helpful if you encounter problems with features further down the line.\n\n### 3.3.2. Text-Based Categorical Features\nWe can also now look at the distributions of the text based categorical features. As an example, we plot the ExterQual feature against a few continuous and categorical features. Again, we don't do this for all of the possible combinations for brevity.\n\n","9726faf7":"From the above output, it is clear that there are many features in the data that describe the houses. It is important not to panic at this stage because whilst there are a lot of features, we can take it one step at a time to learn about them. You may have also noticed that one of the features is 'SalePrice' (i.e what we would like to predict in the competition). The SalePrice is contained in the training set that we have imported because before we make any predictions, we have to be able to build a model based on already known SalePrices.\n\nFrom looking at the table output, it is clear that the data is not all the same type. The four types of data we have are:\n* Numerical and continuous data (e.g. LotFrontage) \n* Numerical and categorical data (e.g. MSSubClass)\n* Text-Based and categorical data (e.g. LotShape)\n* Missing data (e.g. NaNs in PoolQC)\n\nThis is fairly typical of real world data in the sense that it is not perfect and requires cleaning and preparing before we can use it to make predictions. For now, we won't get into too much feature engineering (the name given to preparing data for models) but we will do a small amount of data cleaning to make the data more manageable.","141a1f99":"# 2. Initial Data Cleaning and Exploration\n\nNow that we have a new data set, it is important to get a quick idea of the features available to us. It is then often helpful to clean the data (this means that we make the data more manageable by removing unneccessary features and filling in missing values if possible). This section does some initial data cleaning that makes it easier for us to interpret the data later on.\n\n## 2.1. Seeing What Data We Have\n\nFirst, let's take a look at the available features by printing the column values. We can also show the first 5 rows of the *Dataframe* using *.head(5)* as shown below.","b7f3cec4":"# House Price Competition - Comprehensive Data Exploration\n\nThis notebook is designed to explore the dataset provided for the House Price Competition. If you use this notebook or find it helpful, **please give it an upvote!**\n\nMy others notebooks on various topics can be found on Kaggle [here](https:\/\/www.kaggle.com\/thomaswoolley\/notebooks) and on GitHub [here](https:\/\/github.com\/Woolley12345\/kaggle-competitions).\n\n\n### Competition Overview\n\nThe aim of this competition is to predict the sales price of various houses given a number of features. This makes this a regression problem (predicting a continuous variable e.g. price) rather than a classification problem (predicting distinct classes\/groups e.g. survived or not). In order to approach this competition (or any data science problem for that matter), it is essential to first understand the data that we are working with. The aim of this notebook is to develop that understanding of the data. A follow-up notebook will focus on the predictions of house sale prices.\n\n### Notebook Structure\nThis notebook is structured into three main sections as follows:\n\n1. Load Modules and Data\n2. Initial Data Cleaning and Exploration\n3. Feature Distributions and Correlations\n\n\n\n# 1. Load Modules and Data\n\nAs the first step, we have to load in the data that we want to use. It is also important to load in some modules. Modules help us to do additional things with Python such as plotting, mathematics and data processing. Some commonly used modules are imported below to help us explore the data we have been given. For more information on the modules that are used here, check out the documentation at the following links:\n\n* [Numpy](https:\/\/numpy.org)\n* [Pandas](https:\/\/pandas.pydata.org)\n* [MatplotLib](https:\/\/matplotlib.org)\n* [Seaborn](https:\/\/seaborn.pydata.org)"}}