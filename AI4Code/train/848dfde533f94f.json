{"cell_type":{"f8dbb583":"code","940da78b":"code","6b0d23c2":"code","4d053ea8":"code","a0baec55":"code","775b1631":"code","7f6ad9ef":"code","cd2b33cc":"code","ffd64a05":"code","a2741e37":"code","41805c5d":"code","f5dd0ff4":"code","b93b9709":"code","329c1629":"code","419ec09e":"code","7ebd24d1":"code","fb8fb671":"code","c0c10695":"code","e8844714":"code","8c980b84":"code","ee3ba3b6":"code","4e074ae0":"code","f61818ec":"code","415ea58e":"code","db5f80fe":"code","aa46b3eb":"code","7a9badfb":"code","d3bae17c":"code","ba0dd9e9":"code","51537a1c":"code","739f146c":"code","e3b9c8a6":"code","aba96657":"code","ac04bc40":"code","86ebd463":"code","dd429f26":"code","026ccabc":"code","91bad38f":"markdown","8defb62c":"markdown","37a4953b":"markdown","8c779901":"markdown","64c3c5a1":"markdown","86fbd640":"markdown","adb7aefa":"markdown","4cdb4a78":"markdown","53a971a4":"markdown","3e0c5e03":"markdown","2cadd2ed":"markdown","82a0d380":"markdown","d3f38631":"markdown","991b5c5e":"markdown","b7d81a3e":"markdown","e033bfa4":"markdown","4acdb0ac":"markdown","6bd81202":"markdown","5ca975d2":"markdown","88b408b0":"markdown"},"source":{"f8dbb583":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","940da78b":"#importing modules\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","6b0d23c2":"#Reading dataset\ndf = pd.read_csv('\/kaggle\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')","4d053ea8":"df.info()","a0baec55":"df.head()","775b1631":"#Renaming columns inorder to avoid confusion\ndf.rename({'PAY_0':'PAY_1','default.payment.next.month':'DEFAULT'},axis = 1,inplace = True)\ndf.head()","7f6ad9ef":"#Since ID is the least inportant in this data.So removing ID column\ndf.drop(['ID'],axis = 1,inplace = True)\ndf.head()","cd2b33cc":"#Let's check the unique values of columns\nprint({'Sex':df['SEX'].unique()},{'Education':df['EDUCATION'].unique()},{'Marriage':df['MARRIAGE'].unique()},sep='\\n')","ffd64a05":"#Let's see the number of rows of missing values\nlen(df[(df['EDUCATION'] == 0) | (df['EDUCATION'] == 5) | (df['EDUCATION']== 6) | (df['MARRIAGE'] == 0) ])","a2741e37":"print('Percentage of Missing Values is {} %'.format(round(399\/len(df) * 100,2)))","41805c5d":"#Although imputer techniques can be used but removing the rows won't effect the outcome much\ndf_no_missing = df[(df['EDUCATION'] != 0) & (df['EDUCATION'] != 5) & (df['EDUCATION'] != 6) & (df['MARRIAGE'] != 0) ]\nlen(df_no_missing)","f5dd0ff4":"#Let's check the length of rows having default as 0 and as 1\nprint('No of people defaulted is {0}'.format(len(df[df['DEFAULT'] == 1])))\n\nprint('No of people not defaulted is {0}'.format(len(df[df['DEFAULT'] == 0])))","b93b9709":"#importing library\nfrom sklearn.utils import resample","329c1629":"#Let's take 1000 samples from each category of default \n\n#First splitting the dataset into default and not default dataset\ndf_no_default = df_no_missing[df_no_missing['DEFAULT'] == 0]\ndf_default = df_no_missing[df_no_missing['DEFAULT'] == 1]\n\n#Now downsizing the dataset\ndf_no_default_downsampled = resample(df_no_default,\n                                    replace = False,\n                                    n_samples = 1000,\n                                    random_state = 42)\n\ndf_default_downsampled  = resample(df_default,\n                      replace = False,\n                      n_samples = 1000,\n                      random_state = 42)\n\ndf_downsampled = pd.concat([df_no_default_downsampled,df_default_downsampled],axis = 0)\nlen(df_downsampled)","419ec09e":"#Splitting dataset into dependent(X) and independent(y) dataset\n\nX = df_downsampled.drop(['DEFAULT'],axis = 1).copy()\ny = df_downsampled['DEFAULT'].copy()","7ebd24d1":"X_encoded = pd.get_dummies(X,columns = ['SEX','EDUCATION','MARRIAGE','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'])\n\nX_encoded.head()","fb8fb671":"#importing library\nfrom sklearn.model_selection import train_test_split","c0c10695":"X_train,X_test,y_train,y_test = train_test_split(X_encoded,y,random_state = 42)","e8844714":"#importing library\nfrom sklearn.preprocessing import StandardScaler","8c980b84":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)","ee3ba3b6":"#importing library\nfrom sklearn.svm import SVC","4e074ae0":"#First making a model without any hyperparameter tuning\nsvc = SVC(random_state=42)\nsvc.fit(X_train_scaled,y_train)","f61818ec":"#importing library\nfrom sklearn.metrics import plot_confusion_matrix","415ea58e":"#Confusion matrix for training set\n\nplot_confusion_matrix(svc,X_train_scaled,y_train,display_labels = ['Did not Default','Default'])","db5f80fe":"#Confusion Matrix for test set\n\nX_test_scaled = scaler.transform(X_test)\nplot_confusion_matrix(svc,X_test_scaled,y_test,display_labels = ['Did not Default','Default'])","aa46b3eb":"from sklearn.model_selection import GridSearchCV","7a9badfb":"param_grid = [\n    {'C':[0.5,1,10,100,1000],\n     'gamma':[1,0.1,0.001,0.0001],\n     'kernel':['rbf']}]\n\nsvc_optimised = GridSearchCV(\n    SVC(),\n    param_grid,\n    cv = 4,\n    scoring = 'accuracy',\n    verbose = 0)\n\nsvc_optimised.fit(X_train_scaled,y_train)\nprint(svc_optimised.best_params_)\n","d3bae17c":"svc_final = SVC(random_state=42,C = 1,gamma=0.001)\nsvc_final.fit(X_train_scaled,y_train)","ba0dd9e9":"#Confusion matrix for training set\n\nplot_confusion_matrix(svc_final,X_train_scaled,y_train,display_labels = ['Did not Default','Default'])","51537a1c":"#Confusion Matrix for test set\n\nplot_confusion_matrix(svc_final,X_test_scaled,y_test,display_labels = ['Did not Default','Default'])","739f146c":"#importing library\nfrom sklearn.decomposition import PCA","e3b9c8a6":"pca = PCA()\n\nX_train_pca = pca.fit_transform(X_train_scaled)\n\nper_var = np.round(pca.explained_variance_ratio_*100,decimals = 1)\nlabels = [str(x) for x in range(1,len(per_var) + 1)]\n\nplt.bar(x = range(1,len(per_var) +1),height = per_var)\nplt.tick_params(\n    axis = 'x',\n    which = 'both',\n    bottom = False,\n    top = False,\n    labelbottom = False)\n\nplt.ylabel('Percentage of Explained Varaince')\nplt.xlabel('Principle Component')\nplt.title('Scree Plot')\nplt.show()","aba96657":"train_pc1 = X_train_pca[:,0]\ntrain_pc2 = X_train_pca[:,1]\n\npca_trained_scaled = scaler.fit_transform(np.column_stack((train_pc1,train_pc2)))\n\nparam_grid_pca = [\n    {'C':[0.5,1,10,100,1000],\n    'gamma':[1,0.1,0.01,0.001,0.001],\n    'kernel':['rbf']}]\n\noptimal_params_pca = GridSearchCV(\n    SVC(),\n    param_grid_pca,\n    cv = 4,\n    scoring = 'accuracy',\n    verbose = 0 )\n\noptimal_params_pca.fit(pca_trained_scaled,y_train)\nprint(optimal_params_pca.best_params_)","ac04bc40":"svc_pca = SVC(random_state=42,C = 1000,gamma = 0.1)\nsvc_pca.fit(pca_trained_scaled,y_train)","86ebd463":"X_test_pca = pca.transform(X_train_scaled)\n\ntest_pc1 = X_test_pca[:,0]\ntest_pc2 = X_test_pca[:,1]\n\n\nx_min = test_pc1.min() - 1\nx_max = test_pc1.max() + 1\n\ny_min = test_pc2.min() - 1\ny_max = test_pc2.max() + 1\n\nxx,yy = np.meshgrid(np.arange(start = x_min,stop = x_max,step = 0.1),\n                   np.arange(start = y_min,stop = y_max,step = 0.1 ))\n\n\nZ = svc_pca.predict(np.column_stack((xx.ravel(),yy.ravel())))\nZ = Z.reshape(xx.shape)","dd429f26":"import matplotlib.colors as colors","026ccabc":"fig,ax = plt.subplots(figsize = (10,10))\n\nax.contourf(xx,yy,Z,alpha = 0.1 )\n\ncmap = colors.ListedColormap(['#e41a1c','#4daf4a'])\n\nscatter = ax.scatter(test_pc1,test_pc2,c = y_train,cmap = cmap,s = 100,edgecolors = 'k',alpha = 0.7)\n\nlegend = ax.legend(scatter.legend_elements()[0],\n                  scatter.legend_elements()[1],\n                  loc = 'upper right')\n\nlegend.get_texts()[0].set_text('Not Default')\nlegend.get_texts()[1].set_text('Default')\n\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.set_title('Classifer Visualisation using transformed featured by PCA')\nplt.show()","91bad38f":"## Train - Test Split","8defb62c":"## Inference: Here we see that by using GridSearch the False Positive Rates have been decreased but False negatives have increased since accuracy is used as evaluation metric.<br\/>\n\n### Precision is increased while Recall is decreased\n","37a4953b":"# Visualising the prediction using PCA","8c779901":"# Plot of PCA","64c3c5a1":"# Training SVM","86fbd640":"Here we can see that ```Education``` column has values other than 1,2,3,4 i.e 5,6,0.<br\/>\nAlthough as mentioned in dataset 5,6 are unknown,we will also assume 0 as unknown too.<br\/>\nSimilarly ```Marriage``` has also 0.So we take it as unkown too\n","adb7aefa":"# Optimization of Parameters using Cross Validation and GridSearch ","4cdb4a78":"## Downsizing of Data","53a971a4":"*Since Sklearn only uses numerical values we need to one-hot encode the categorical variables in dataset* <BR\/>\n*Categorical variables are - ```SEX```, ```EDUCATION``` , ```MARRIAGE```, ```PAYS```*","3e0c5e03":"## One-Hot Encoding","2cadd2ed":"# Confusion Matrix","82a0d380":"## Info about data\n\nID: ID of each client\n\nLIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n\nSEX: Gender (1=male, 2=female)\n\nEDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n\nMARRIAGE: Marital status (1=married, 2=single, 3=others)\n\nAGE: Age in years\n\nPAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, \u2026 8=payment \ndelay for eight months, 9=payment delay for nine months and above)\n\nPAY_2: Repayment status in August, 2005 (scale same as above)\n\nPAY_3: Repayment status in July, 2005 (scale same as above)\n\nPAY_4: Repayment status in June, 2005 (scale same as above)\n\nPAY_5: Repayment status in May, 2005 (scale same as above)\n\nPAY_6: Repayment status in April, 2005 (scale same as above)\n\nBILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n\nBILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n\nBILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n\nBILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n\nBILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n\nBILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n\nPAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n\nPAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n\n\nPAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n\nPAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n\nPAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n\nPAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n\ndefault.payment.next.month: Default payment (1=yes, 0=no)","d3f38631":"# Training using Optimised Parameter","991b5c5e":"## Thank you","b7d81a3e":"## Feature Scaling","e033bfa4":"## Dependent and Independent dataset","4acdb0ac":"*SVM kernels such as rbf can be sometimes sensitive to datas that are large to datas that are small.*<br\/>\n*So normalising the data* ","6bd81202":"#### Here we can see that Number of people ``` NOT DEFAULTED``` is almost 4x times the Number of people  ```DEFAULTED``` . So the dataset is skewed\n#### Also SVM performs well in small number of dataset \n#### Hence it is better to *Downsize* the Samples and perform the training\n","5ca975d2":"*Normally there is a rule of thumb is that if the percentage of missing values is less than 5% of total dataset then it can be removed*","88b408b0":"# Data Preprocessing"}}