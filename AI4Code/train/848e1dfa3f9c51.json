{"cell_type":{"b2bca834":"code","5287fc7e":"code","38415d52":"code","bbb6e672":"code","3243aa3c":"code","3bc37cc9":"code","323a5180":"code","c28fb5f7":"code","5d46c946":"code","6f13736b":"code","fa80b02c":"markdown","8c429a2c":"markdown","46805a72":"markdown","9634c7d8":"markdown","33316182":"markdown","39cae7b7":"markdown","c6accc5b":"markdown","52242e3f":"markdown"},"source":{"b2bca834":"import glob\nimport numpy\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n","5287fc7e":"TRAIN_PATH = '..\/input\/deepfake-extraction\/'\ntrain_fns = sorted(glob.glob(TRAIN_PATH + '*.csv'))\nprint('There are {} samples in the train set.'.format(len(train_fns)))\n","38415d52":"# imdb data already was split with 50 : 50, we cant change that\n# top_words mean frequency ranking\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)","bbb6e672":"print('review for training: {}'.format(len(X_train)))\nprint('review for testing: {}'.format(len(X_test)))\nnum_classes = max(y_train) +1 \nprint('category : {}'.format(num_classes))\n","3243aa3c":"print(X_train[0])\nprint(y_train[0])\n","3bc37cc9":"word_to_index = imdb.get_word_index()\nindex_to_word = {}\nfor key, value in word_to_index.items():\n    index_to_word[value+3] = key","323a5180":"# we need to plus +3 than what we want\nprint('frequency ranking first : {}'.format(index_to_word[4]))\n","c28fb5f7":"# truncate and pad input sequences\nmax_review_length = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","5d46c946":"# The first layer is the Embedded layer that uses 32 length vectors to represent each word\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n","6f13736b":"model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","fa80b02c":"<a id=\"four\"><\/a>\n# 4. Training and Evaluation","8c429a2c":"<a id=\"two\"><\/a>\n\n# 2. Prepare Data","46805a72":"<a id=\"three\"><\/a>\n# 3. Modeling","9634c7d8":"* **numbers** in X_train means ranking of word frequency\n* **lable 1** in y_train means positive","33316182":"## Truncate and Pad\n* **Truncate** : cut the length\n* **Pad** : put meaningless length\n* Because we need to put all the same length input sequences for modeling","39cae7b7":"<a id=\"one\"><\/a>\n# 1. Import libraries","c6accc5b":"We can check what number means in X_train","52242e3f":"# Sequence Classification with LSTM\n<hr>\n\nLSTM (Long term Short Term memory)\n* **RNN (Recurrent Neural Network)** has problem about long term memory, **LSTM** has improvement about that\n* Our **LSTM** is focusing on classification some movie's review is good or not, when **LSTM** gets people's review\n* We will use Keras module\n![161.webp](attachment:161.webp)\n\n<hr>\n\n\nHow to use this notebook :\n\nThere is only minimum explanation\n\nThis notebook could be helpful for who want to see how code works right away\n\nPlease upvote if it was helpful ! \n<hr>\n\n## Content\n1. [Import libraries](#one)\n2. [Prepare Data](#two)\n3. [Modeling](#three)\n4. [Training and Evaluation](#four)"}}