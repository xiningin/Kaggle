{"cell_type":{"1765e4c8":"code","a70119ea":"code","f1bbb662":"code","012e318c":"code","5f5e728d":"code","ae2bc945":"code","3757319c":"code","c55569ad":"code","ac926b8a":"code","7cb9bed3":"code","3dcea16b":"code","8cce33d3":"code","7adfae3d":"code","23e5d632":"code","971d427f":"code","2f4f5601":"code","4dc9d7a9":"code","824244a3":"code","3dd125d5":"code","9de4dff4":"code","3d3f6298":"code","2c1aa073":"code","e8358cae":"code","078cbb1e":"code","95fb10b8":"code","1f52318e":"code","bd45eb6e":"code","0dee7acd":"code","bcabc1c5":"code","8ec1f18f":"code","4fe5f456":"code","74b18b9b":"code","0b232d0e":"code","92023954":"code","e8c193ad":"code","112c419e":"code","5ce4d85e":"code","a9793a30":"code","7913e363":"code","b735b985":"code","8a188409":"code","90fb7720":"code","cedc583f":"code","c5bfdb65":"code","0acd7070":"code","60bd437d":"code","b51b1ebd":"code","8bfb3668":"code","41c74841":"code","1ccea004":"code","fd9ba356":"code","223e83cd":"code","7623483e":"code","85d274e5":"code","8bc0af7d":"code","58b7eb86":"code","8c3f3f9a":"code","66935935":"code","7feb0c82":"code","6eb57c60":"code","c2e5308f":"code","4643f1ef":"code","cb13899d":"code","439355b8":"code","58222e37":"code","34f1405e":"code","4dd27f3f":"code","5dad7d83":"code","2c0f93e8":"code","2e5c2c38":"code","36c7c09f":"code","dbeab1d1":"code","9dd2a821":"code","3bcdb449":"code","cc70fd06":"code","bf91d316":"code","fe0319eb":"code","274bb569":"code","e62b4545":"code","c949e741":"code","1d7725f7":"code","088b1db3":"code","fb1fbc38":"code","6b72e5ce":"code","5691e685":"code","bde0090f":"code","24355100":"code","22dbb8a9":"code","20d5da05":"code","728b8254":"code","cd968343":"code","85c5104b":"code","3eacd3b7":"code","02bb8710":"code","78e80b6b":"code","bdb917b4":"code","91eb803a":"code","d7af2c8f":"code","049171b4":"code","96c065eb":"code","9adfb774":"code","7e10dc80":"code","4810e01c":"code","31ddbd8d":"code","61b0c03d":"code","d5a2edbc":"code","0f1ae23b":"code","2fe8e7af":"code","c32a5043":"code","db12e247":"code","70bc482f":"code","81e66e58":"code","44cc251b":"code","73d1d8d3":"code","8088368c":"code","831c2247":"code","ee4ef0a3":"code","a797a758":"code","a4c4d95f":"code","6fd1addc":"code","91e05040":"code","1f1faaca":"code","d755b450":"code","137d30a1":"code","5b671824":"code","1df46340":"code","9691b735":"code","017d2751":"code","ae68ea91":"code","bdf6dc4b":"code","6fa5ecc9":"markdown","10e3440f":"markdown","fae46d4a":"markdown","99ff2bf2":"markdown","9c4ff0dd":"markdown","1e036b66":"markdown","d7ed1b90":"markdown","05b6f804":"markdown","4d1f762b":"markdown","d093911d":"markdown","38b80fb2":"markdown","26ed3a69":"markdown","241ff3b3":"markdown","eb10900d":"markdown","66d65dd2":"markdown","64888684":"markdown","52113b31":"markdown","5013067f":"markdown","f1b8a36f":"markdown","be98bcd9":"markdown","af767ed0":"markdown","07f69cfc":"markdown","81231aae":"markdown","58800083":"markdown","d2becd08":"markdown","ed9ccff8":"markdown","7870ce00":"markdown","0b5929b0":"markdown","6a723935":"markdown","f28fd2f8":"markdown","c4a945b1":"markdown","d991f870":"markdown","251fd935":"markdown","9ad21c60":"markdown","9ca37306":"markdown","372c50a8":"markdown","143305d7":"markdown","a5c5b088":"markdown","6dc5ee6c":"markdown","a838c40b":"markdown","a1f85023":"markdown","d07d1afd":"markdown","85361d8f":"markdown","f1f9a83a":"markdown","3d104e0b":"markdown","ec0fe4d0":"markdown","527fac28":"markdown","e6a418ae":"markdown","9a4774af":"markdown","b2fcb128":"markdown","e6c87e24":"markdown","643bd820":"markdown","f3292366":"markdown"},"source":{"1765e4c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a70119ea":"#!pip install pyforest\n\n# 1-Import Libraies\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nfrom sklearn.compose import make_column_transformer\n\n#Scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n\n\n#Importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n#Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n#!pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored\n\nimport ipywidgets\nfrom ipywidgets import interact","f1bbb662":"# To view summary information about the column\n\ndef first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","012e318c":"# Function for determining the number and percentages of missing values\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","5f5e728d":"df = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")","ae2bc945":"# df = pd.read_csv(\"Churn_Modelling.csv\")","3757319c":"df.head()","c55569ad":"df.tail()","ac926b8a":"df.sample(5)","7cb9bed3":"df.info()","3dcea16b":"df.shape","8cce33d3":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset.\")","7adfae3d":"df.duplicated().sum()","23e5d632":"print(\"There is\", df.duplicated().sum(), \"duplicated observations in the dataset.\")","971d427f":"df.isna().sum().any()","2f4f5601":"missing(df)","4dc9d7a9":"df.nunique()","824244a3":"# to find how many unique values numerical features have\n\nfor col in df.select_dtypes(include=[np.number]).columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","3dd125d5":"# to find how many unique values object features have\n\nfor col in df.select_dtypes(include=\"object\").columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","9de4dff4":"df.describe().T","3d3f6298":"df.describe(include=object).T","2c1aa073":"drop_columns = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n\ndf.drop(drop_columns, axis = 1, inplace = True)","e8358cae":"df.shape","078cbb1e":"first_looking(\"Exited\")","95fb10b8":"df[\"Exited\"].value_counts()\n\nsns.countplot(x = df[\"Exited\"], data = df)\nfor index,value in enumerate(df[\"Exited\"].value_counts()):\n     plt.text(index, value, f\"{value}\", ha=\"center\", va=\"bottom\", fontsize = 13)\n        \n","1f52318e":"print(df[\"Exited\"].value_counts())\ndf[\"Exited\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(10,10));","bd45eb6e":"y = df['Exited']\nprint(f'Percentage of Exited-1: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} observations for Exited-1)\\nPercentage of Exited-0: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} observations for Exited-0)')","0dee7acd":"df['Exited'].describe()","bcabc1c5":"df[df['Exited']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","8ec1f18f":"df[df['Exited']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","4fe5f456":"numerical= df.drop(['Exited'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","74b18b9b":"df[numerical].head().T","0b232d0e":"df[numerical].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","92023954":"df[numerical].iplot(kind='histogram', subplots=True,bins=50)","e8c193ad":"for i in numerical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","112c419e":"sns.pairplot(df, hue=\"Exited\", corner=True);","5ce4d85e":"plt.figure(figsize=(12, 8))\nsns.heatmap (df.corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1)\nplt.xticks(rotation=45);","a9793a30":"df.corr()[\"Exited\"].sort_values().drop(\"Exited\").iplot(kind = \"barh\");","7913e363":"df_temp = df.corr()\n\ncount = \"Done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is NO multicollinearity problem\") \n\nprint(\"\\033[1mThe number of strong corelated features:\\033[0m\", count) ","b735b985":"px.histogram(df, x=df.CreditScore, color=\"Exited\", marginal=\"box\", hover_data=df.columns)","8a188409":"px.scatter(df, x = \"CreditScore\", y = \"Age\", color = \"Exited\")","90fb7720":"px.histogram(df, x=df.Tenure, color=\"Exited\", marginal=\"box\", hover_data=df.columns)","cedc583f":"px.scatter(df, x = \"Tenure\", y = \"Age\", color = \"Exited\")","c5bfdb65":"px.histogram(df, x=df.Balance, color=\"Exited\", marginal=\"box\", hover_data=df.columns)","0acd7070":"px.scatter(df, x = \"Balance\", y = \"Age\", color = \"Exited\")","60bd437d":"pd.crosstab(df[\"NumOfProducts\"], df[\"Exited\"])","b51b1ebd":"px.histogram(df, x=df.NumOfProducts, color=\"Exited\", marginal=\"box\", hover_data=df.columns)","8bfb3668":"px.scatter(df, x = \"NumOfProducts\", y = \"Age\", color = \"Exited\")","41c74841":"pd.crosstab(df[\"HasCrCard\"], df[\"Exited\"])","1ccea004":"px.histogram(df, x=df.HasCrCard, color=\"Exited\", hover_data=df.columns)","fd9ba356":"pd.crosstab(df[\"IsActiveMember\"], df[\"Exited\"])","223e83cd":"px.histogram(df, x=df.IsActiveMember, color=\"Exited\", hover_data=df.columns)","7623483e":"px.histogram(df, x=df.EstimatedSalary, color=\"Exited\", hover_data=df.columns)","85d274e5":"px.scatter(df, x = \"EstimatedSalary\", y = \"Age\", color = \"Exited\")","8bc0af7d":"df[categorical].head().T","58b7eb86":"df[categorical].describe()","8c3f3f9a":"df[categorical].iplot(kind='histogram', subplots=True,bins=50)","66935935":"first_looking(\"Geography\")","7feb0c82":"px.histogram(df, x=df.Geography, color=\"Exited\")","6eb57c60":"pd.crosstab(df[\"Geography\"], df[\"Exited\"])","c2e5308f":"sns.swarmplot(y=\"Age\", x=\"Geography\", hue=\"Exited\", data=df);","4643f1ef":"first_looking(\"Gender\")","cb13899d":"px.histogram(df, x=df.Gender, color=\"Exited\")","439355b8":"pd.crosstab(df[\"Gender\"], df[\"Exited\"])","58222e37":"sns.swarmplot(y=\"Age\", x=\"Gender\", hue=\"Exited\", data=df);","34f1405e":"df = pd.get_dummies(df)","4dd27f3f":"df.head(1)","5dad7d83":"df1 = df.copy()","2c0f93e8":"X = df1.drop('Exited', axis = 1)\ny = df1['Exited']","2e5c2c38":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.1, random_state = 99)","36c7c09f":"scaler = MinMaxScaler()","dbeab1d1":"X.head()","9dd2a821":"y","3bcdb449":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","cc70fd06":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, SGD, Adamax, RMSprop,Adadelta\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV","bf91d316":"X_train.shape","fe0319eb":"X_test.shape","274bb569":"model = Sequential()\n\nmodel.add(Dense(32, activation = \"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(16, activation = \"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nopt = Adam(lr = 0.001)\nmodel.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"Recall\"])","e62b4545":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)","c949e741":"model.fit(x = X_train, y = y_train, validation_split = 0.1, batch_size = 64, epochs = 500, verbose=1,\n          callbacks = [early_stop])","1d7725f7":"model.summary()","088b1db3":"loss_df = pd.DataFrame(model.history.history)\nloss_df.head()","fb1fbc38":"loss_df = pd.DataFrame(model.history.history)\nloss_df.tail()","6b72e5ce":"loss_df.plot();","5691e685":"model.evaluate(X_test, y_test, verbose=0)","bde0090f":"loss, recall = model.evaluate(X_test, y_test, verbose=0)\nprint(\"loss : \", loss)\nprint(\"recall : \", recall)","24355100":"y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","22dbb8a9":"y_pred_proba = model.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Keras Model')\nplt.show()","20d5da05":"roc_auc_score(y_test, y_pred_proba)","728b8254":"average_precision_score(y_test, y_pred_proba)","cd968343":"model_weighted = Sequential()\n\nmodel_weighted.add(Dense(32, activation = \"relu\"))\nmodel_weighted.add(Dropout(0.25))\nmodel_weighted.add(Dense(16, activation = \"relu\"))\nmodel_weighted.add(Dropout(0.25))\nmodel_weighted.add(Dense(1, activation = \"sigmoid\"))\n                                             \nopt = Adam(lr = 0.001)\nmodel_weighted.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"Recall\"])","85c5104b":"px.histogram(df, x=df.Exited)","3eacd3b7":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","02bb8710":"class_weight = {0: class_weights[0], 1:class_weights[1]}","78e80b6b":"class_weight","bdb917b4":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)","91eb803a":"model_weighted.fit(x = X_train, y = y_train, validation_split = 0.1, batch_size = 64, epochs = 500, verbose=1,\n          callbacks = [early_stop], class_weight=class_weight)","d7af2c8f":"model_weighted.summary()","049171b4":"loss_df = pd.DataFrame(model_weighted.history.history)\nloss_df.head()","96c065eb":"loss_df = pd.DataFrame(model_weighted.history.history)\nloss_df.tail()","9adfb774":"loss_df.plot();","7e10dc80":"model_weighted.evaluate(X_test, y_test, verbose=0)","4810e01c":"loss, recall = model_weighted.evaluate(X_test, y_test, verbose=0)\nprint(\"loss : \", loss)\nprint(\"recall : \", recall)","31ddbd8d":"y_pred = (model_weighted.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","61b0c03d":"y_pred_proba = model_weighted.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Keras Model_Weighted')\nplt.show()","d5a2edbc":"roc_auc_score(y_test, y_pred_proba)","0f1ae23b":"average_precision_score(y_test, y_pred_proba)","2fe8e7af":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 32, activation = 'relu'))\n    classifier.add(Dropout(0.25))\n    classifier.add(Dense(units = 16, activation = 'relu'))\n    classifier.add(Dropout(0.25))\n    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\"Recall\"])\n    return classifier","c32a5043":"early_stop = EarlyStopping(monitor = \"loss\", mode = \"auto\", verbose = 1, patience = 20)","db12e247":"class_weight = {0: class_weights[0], 1:class_weights[1]}","70bc482f":"class_weight","81e66e58":"classifier = KerasClassifier(build_fn = build_classifier, epochs = 200)\nparameters = {'batch_size': [32, 64],\n              'optimizer': ['adam', 'rmsprop', \"SGD\", \"adagrad\", \"adadelta\"]}\ngrid_model = GridSearchCV(estimator = classifier,\n                          param_grid = parameters,\n                          scoring = 'recall',\n                          cv = 10,\n                          n_jobs = -1,\n                          verbose = 1)\ngrid_model.fit(X_train, y_train, callbacks = [early_stop], class_weight = class_weight)","44cc251b":"grid_model.best_score_","73d1d8d3":"grid_model.best_params_","8088368c":"y_test_pred = (grid_model.predict(X_test) > 0.5).astype(\"int32\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))","831c2247":"y_pred_proba = grid_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Grid Model')\nplt.show()","ee4ef0a3":"roc_auc_score(y_test, y_pred_proba)","a797a758":"average_precision_score(y_test, y_pred_proba)","a4c4d95f":"import pickle\npickle.dump(scaler, open(\"scaler_exited\", 'wb'))","6fd1addc":"final_model = Sequential()\n\nfinal_model.add(Dense(32, activation = \"relu\"))\nfinal_model.add(Dropout(0.25))\nfinal_model.add(Dense(16, activation = \"relu\"))\nfinal_model.add(Dropout(0.25))\nfinal_model.add(Dense(1, activation = \"sigmoid\"))\n                                           \nopt = Adam(lr = 0.001)\n\nfinal_model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"Recall\"])\n\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)\n\nclass_weight = {0: 0.62787777, 1: 2.45499182}\n\nfinal_model.fit(x=X_train,\n                y=y_train,\n                validation_data=(X_test, y_test),\n                callbacks=[early_stop],\n                batch_size=64,\n                epochs=500,\n                verbose=1,\n                class_weight=class_weight)","91e05040":"loss_df = pd.DataFrame(final_model.history.history)\nloss_df.plot()","1f1faaca":"y_pred = (final_model.predict(X_test) > 0.5).astype(\"int32\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","d755b450":"model.save('model_exited.h5')","137d30a1":"from tensorflow.keras.models import load_model","5b671824":"model_exited = load_model('model_exited.h5')\nscaler_exited = pickle.load(open(\"scaler_exited\", \"rb\"))","1df46340":"first_ten_customer = df.drop('Exited', axis = 1).iloc[0:10, :]\nfirst_ten_customer","9691b735":"first_ten_customer = scaler_exited.transform(first_ten_customer)\nfirst_ten_customer","017d2751":"(model_exited.predict(first_ten_customer) > 0.5).astype(\"int32\")","ae68ea91":"# rm -rf .nv\/","bdf6dc4b":"df[\"Exited\"].iloc[0:10]","6fa5ecc9":"***Based on the examinations made above,***\n\n- There is no multicollinearity problem among the features.\n- We have weak level correlation between the numerical features and the target column.\n- Also there is weak level correlation between the columns.\n- Target variable demonstrates a slight negative correlation with the variables of \"creditscore\", \"tenure\", \"numberofproducts\" 'hascrcard' and 'isactivemember', \n- Target variable demonstrates slight positive correlation with the variables of 'age', 'balance' and 'estimatedsalary\".\n- Age has more influence on the decision to leave the bank than the other columns.\n- It is noteworthy that those who left the bank are in the 45-65 age group. \n- The increase in the number of products negatively affects the decision to continue with the bank.\n- Another remarkable situation is the concentration of leaving the bank in the group where the number of products is 3 and 4. ","10e3440f":"### Numerical Columns","fae46d4a":"***Train | Test Split, Scalling***","99ff2bf2":"***HasCrCard and Exited***","9c4ff0dd":"- All the values in \"RowNumber\" and \"CustomerId\" are unique. The values contained in these columns need to be dropped as they will not help the prediction.\n- In addition, the \"Surname\" column also should be dropped as it does not contain information to help prediction. ","1e036b66":"## Modelling & Model Performance","d7ed1b90":"***Before constructing our model, we should convert categorical features into dummies.***","05b6f804":"## GridSearchCV","4d1f762b":"# Churn Prediction For Bank Customer","d093911d":"### Creating Model","38b80fb2":"#### Evaluate","26ed3a69":"***IsActiveMember and Exited***","241ff3b3":"- As I mentioned above, \"RowNumber\", \"CustomerId\" and \"Surname\" columns will not help the prediction.\n- \"Tenure\", \"NumOfProducts\", \"Geography\" columns have multi-class type values.\n- \"HasCrCard\", \"IsActiveMember\", \"Exited\", \"Gender\" columns have binary type values.","eb10900d":"***Below shown how to calculate the weights of the classes.***","66d65dd2":"### ROC (Receiver Operating Curve) and AUC (Area Under Curve) for keras_model_weighted","64888684":"***NumOfProducts and Exited***","52113b31":"***The features in the given dataset are:***\n\n- Rownumber: Row Numbers from 1 to 10000\n- Customerid: A unique ID that identifies each customer.\n- Surname: The customer\u2019s surname.\n- Creditscore: A credit score is a number between 300\u2013850 that depicts a consumer's creditworthiness.\n- Geography: The country from which the customer belongs to.\n- Gender: The customer\u2019s gender: Male, Female\n- Age: The customer\u2019s current age, in years, at the time of being customer.\n- Tenure: The number of years for which the customer has been with the bank.\n- Balance: Bank balance of the customer.\n- Numofproducts: the number of bank products the customer is utilising.\n- Hascrcard: The number of credit cards given to the customer by the bank.\n- Isactivemember: Binary Flag for indicating if the client is active or not with the bank before the moment where the client exits the company (recorded in the variable \"exited\")\n- Exited: Binary flag 1 if the customer closed account with bank and 0 if the customer is retained.","5013067f":"## Exploratory Data Analysis and Visualization","f1b8a36f":"***Gender and Exited***","be98bcd9":"## Conclusion","af767ed0":"### with class_weigth","07f69cfc":"- We have an imbalanced data based on the target variable.\n- 20.37 % of the customers didn't continue with the bank and left (Exited-1).\n- 2037 customer left the bank.\n- 79.63 % of the customers continue with the bank and didn't leave (Exited-0).\n- 7963 customer didn't leave.","81231aae":"***Geography and Exited***","58800083":"***Balance and Exited***","d2becd08":"- The aim should be to accurately predict those who leave the bank and develop measures to minimize these livings. \n- We have an imbalanced data. It is therefore necessary to predict as much as possible to the Exited 1 class truely. \n- To predict Class Exited 1 data as accurately as possible, we need to use the class_weight parameter.\n- Below shown how to find the weights of the classes in the target column. ","ed9ccff8":"1. Implement basic steps to see how is your data looks like\n2. Check for missing values\n3. Drop the features that not suitable for modelling\n4. Implement basic visualization steps such as histogram, countplot, heatmap\n5. Convert categorical variables to dummy variables","7870ce00":"#### Evaluate","0b5929b0":"#### Evaluate","6a723935":"***EstimatedSalary and Exited***","f28fd2f8":"***CreditScore and Exited***","c4a945b1":"***According to the basic examinations on the dataset;***\n\n- We have a classification problem.\n- We are going to make classification on the target variable \"Exited\".\n- And we will build a model to get the best classification on the \"Exited\" column.\n- Because of that we are going to look at the balance of \"Exited\" column.\n- \"RowNumber\", \"CustomerId\" and \"Surname\" columns will not help the prediction. So we will drop them.","d991f870":"## Preprocessing of Data","251fd935":"***Tenure and Exited***","9ad21c60":"### The Examination of Target Variable","9ca37306":"### Import related libraries","372c50a8":"- As mentioned above, we can clearly see that leaving the bank is concentrated in the 45-65 age group.","143305d7":"### without class_weigth","a5c5b088":"### ROC (Receiver Operating Curve) and AUC (Area Under Curve) for grid_model","6dc5ee6c":"### Prediction","a838c40b":"## Ingest Data","a1f85023":"- https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n- https:\/\/keras.rstudio.com\/reference\/fit.html","d07d1afd":"### Categorical Columns","85361d8f":"## Final Model and Model Deployment","f1f9a83a":"## Improt Libraries","3d104e0b":"We have a dataset in which there are details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","ec0fe4d0":"### ROC (Receiver Operating Curve) and AUC (Area Under Curve) for keras_model","527fac28":"- We have an imbalanced data based on the target variable as seen above.\n- 20.37 % of the customers didn't continue with the bank and left (Exited-1).\n- 2037 customer left the bank.\n- 79.63 % of the customers continue with the bank and didn't leave (Exited-0).\n- 7963 customer didn't leave.","e6a418ae":"- There is no missing value in the dataset.","9a4774af":"## User Defined Functions","b2fcb128":"### Loading Model and Scaler","e6c87e24":"***Let's go on with the examination of numerical and categorical columns.***","643bd820":"- The dataset has 14 columns and 10000 observations.\n\n- 11 columns contain numerical data and 3 columns contain categorical values. \n\n- There seems to be no missing value. ","f3292366":"In the above study, we;\n- Implemented basic steps to see how data looks like,\n- Checked for missing values,\n- Dropped the features that not suitable for modelling,\n- Implemented basic visualization steps such as histogram, countplot, heatmap,\n- Converted categorical variables to dummy variables,\n- Created models with Keras,\n- Chose final model and deployed it,\n- And finally made prediction with final model."}}