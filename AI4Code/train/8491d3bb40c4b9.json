{"cell_type":{"0d9ea4f2":"code","4ba67d38":"code","ee752f6b":"code","67e3bd2f":"code","79f82004":"code","5a265425":"code","6f9be9d1":"code","f86ceb04":"code","5dd626b5":"code","f1f8c506":"code","8e68338d":"code","13f9a019":"code","30d8a2a2":"code","efa67a08":"code","b7037457":"code","8deb050b":"code","90c3b60d":"code","69b71990":"code","0eb85054":"markdown","b60932cf":"markdown","8ae3a6ae":"markdown","eb4803df":"markdown","11ab4e7c":"markdown","95f38fa6":"markdown","75c6a472":"markdown","85cbba22":"markdown","d08d8a08":"markdown","327593d3":"markdown","8b4df221":"markdown","4bdd427d":"markdown","9f9007a5":"markdown","861cb0ed":"markdown","fa3ef989":"markdown","e751f180":"markdown"},"source":{"0d9ea4f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dense, Input, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer, RegexpTokenizer\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport os\nimport string\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ba67d38":"tweets = pd.read_csv('..\/input\/usa-presidential-debate-fake-accounts-on-twitter\/U.S_Presidential_Debate_Twitter_data.csv', parse_dates=['date'])\ntweets.head()","ee752f6b":"tweets = tweets[tweets['tag']!='Missing Value']\ntweets.shape","67e3bd2f":"def processtext(text):\n    text = text.lower()\n    text = re.sub('http:\/\/.*|t.co.*|http','',text)\n    text = re.sub('&lt;\/?.*?&gt;',' &lt;&gt; ',text)\n    text=re.sub('(\\\\W)+',' ',text)\n    text = re.sub('[.;:!\\'?,\\\"()\\[\\]]', '', text)\n    \n    return text\ntweets['text']=tweets['text'].apply(lambda x: processtext(x))\nstop = set(stopwords.words('english'))\ntweets['text_no_stop']= tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))","79f82004":"def lemmamytext(corpus):\n    lemma = WordNetLemmatizer()\n    return [' '.join([lemma.lemmatize(word) for word in x.split()])for x in corpus]\ntweets['text_no_stop']=lemmamytext(tweets['text_no_stop'])\ntk = TweetTokenizer()\ntweets['tokenized']=tweets['text_no_stop'].apply(tk.tokenize)","5a265425":"tweets['tokenized_tags']=tweets['tokenized'].apply(nltk.pos_tag)\ntweets['tokenized_tags_chunked']=tweets['tokenized_tags'].apply(nltk.ne_chunk)","6f9be9d1":"fake_tweets = tweets[tweets['tag']=='FAKE_PROFILE']\nreal_tweets = tweets[tweets['tag'] == 'REAL_PROFILE']","f86ceb04":"plt.style.use('fivethirtyeight')\nwords = fake_tweets['tokenized']\nallwords = []\nfor word in words:\n    allwords +=word\nfig, ax = plt.subplots(figsize = (30,15))\n\nfake_fd_common_50 = nltk.FreqDist(allwords).most_common(50)\nfake_fd_common_50 = pd.Series(dict(fake_fd_common_50))\nsns.barplot(x = fake_fd_common_50.index,y = fake_fd_common_50.values)\nplt.title('50 Most Common Words in Fake Tweets')\nplt.xticks(rotation =90)\nplt.show()","5dd626b5":"from wordcloud import WordCloud, ImageColorGenerator\nfig, ax = plt.subplots(figsize = (30,30))\nwc = WordCloud(width = 2000, height = 2000).generate(' '.join(fake_tweets['text_no_stop']))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","f1f8c506":"\nwords2 = real_tweets['tokenized']\nallwords2 = []\nfor word in words2:\n    allwords2 +=word\nfig, ax = plt.subplots(figsize = (30,15))\n\nreal_fd_common_50 = nltk.FreqDist(allwords2).most_common(50)\nreal_fd_common_50 = pd.Series(dict(real_fd_common_50))\nsns.barplot(x = real_fd_common_50.index,y = real_fd_common_50.values)\nplt.title('50 Most Common Words in Real Tweets')\nplt.xticks(rotation =90)\nplt.show()","8e68338d":"from wordcloud import WordCloud, ImageColorGenerator\nfig, ax = plt.subplots(figsize = (30,30))\nwc = WordCloud(width = 2000, height = 2000).generate(' '.join(real_tweets['text_no_stop']))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","13f9a019":"X = tweets[['tokenized']]\ntarget = tweets['tag']\nle = LabelEncoder()\ny = le.fit_transform(target)\n#y= np.asarray(y)\n#y = np.expand_dims(y, -1)\nfrom sklearn.model_selection import train_test_split\n","30d8a2a2":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .3,random_state = 123)\n","efa67a08":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\ncv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1), tokenizer = tk.tokenize)\ntextcts = cv.fit_transform(tweets['text_no_stop'])\nX_traincv,X_testcv,y_traincv,y_testcv = train_test_split(textcts,y,test_size = .3,random_state = 123)\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.metrics import accuracy_score\nclf = MultinomialNB()\nclf.fit(X_traincv,y_traincv)\nclfpreds = clf.predict(X_testcv)\nacc_score = accuracy_score(y_testcv,clfpreds)\nprint(acc_score)","b7037457":"def impt_features(clf, vec, n = 15):\n    feat_names = vec.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feat_names))\n    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n    for (coef_1, fn_1), (coef_2, fn_2) in top:\n        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\nimpt_features(clf,cv,30)","8deb050b":"tf = TfidfVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1), tokenizer = tk.tokenize)\ntexttf = tf.fit_transform(tweets['text_no_stop'])\nX_traintf,X_testtf,y_traintf,y_testtf = train_test_split(texttf,y,test_size = .3,random_state = 123)\n\nclftf = MultinomialNB()\nclftf.fit(X_traintf,y_traintf)\nclfpredstf = clftf.predict(X_testtf)\nacc_scoretf = accuracy_score(y_testtf,clfpredstf)\nprint(acc_scoretf)","90c3b60d":"impt_features(clftf,tf,30)","69b71990":"postag1 = tweets[tweets['type']=='tweet'].reset_index()['tokenized_tags_chunked'][227]\nprint(postag1)","0eb85054":"In contrast with the fake tweets, it is notable that real tweets do not have the same focus on big themes such as globalist cabals or the prospect of the USA turning into a dystopian nightmare with no personal freedoms. Real tweets are more concerned with the **process** in order to prove their conspiracy theories - look at the presence of 'glue', 'lens' and 'cheating' in the word cloud.\n\nPerhaps then, fake tweets have the purpose to change people's minds through big ideas, which would then be manifested in the real tweets as a fear of certain mundane observations.","b60932cf":"In order to work with these tweets, we need to do the following:\n\n**1. Discard Missing Values**\nMissing values in tags will affect our ability to classify the text. As the number of tweets without missing values is still rather substantial, we do not have to 'fill in' null values.\n\n**2. Replace Regular Expressions**\nCertain aspects of tweets, such as special characters, should be removed in order to ease text processing. The text is also lowercased for ease of processing.\n\n**3. Remove Stopwords**\nStopwords are words such as 'I', 'to', 'of'. As there will be a plot of the most common words used, in order to gain meaningful conclusions about the data, these stopwords should be removed.\n\n**4. Lemmatize Text**\nThe text has to be lemmatized (e.g. 'best' and 'better' will be replaced by 'good') for words with the same root to be unified.\n\n**5. Tokenize Text**\nFinally, we tokenize the text for it to be understandable by code.","8ae3a6ae":"The Count Vectorizer counts the number of instances of each word in a text. One example is given below (credits to Educative.io).","eb4803df":"While 80.2% and 81.6% seem like good accuracies, it must also be acknowledged that the dataset is largely skewed towards real tweets. This means that the model could probably get away with predicting that every tweet is real, and ending up with a still-high accuracy. \n\nFurthermore, very 'vanilla' models were used, and hyperparameters were not tuned in this notebook. Thus there is definitely room for the model to improve, and perhaps more representative results could be found with a less lopsided dataset.\n\nAs an aside, the NLTK library also allows for part-of-speech tagging of words. This can be seen in the example below.","11ab4e7c":"In this case, NN stands for noun, JJ is an adjective, VB is a verb, and DT is a determiner. While the quick tagging wasn't 100% perfect - 'answer' is a verb rather than an adjective, it still is rather accurate.\n\nThis is the tweet in question, for reference.","95f38fa6":"![](https:\/\/www.educative.io\/api\/edpresso\/shot\/5197621598617600\/image\/6596233398321152)","75c6a472":"Before we go into the ML proper, we will look at the most common words used by fake and real tweets in order to see if the eye test can yield any results. We thus split the tweets into real and fake tweets, and plot the 50 most common words for each.","85cbba22":"Besides just a bar graph, word clouds can offer good visualisation.","d08d8a08":"While I might come back to this notebook in the future and do additional sub-projects, for now, that concludes this notebook. \n\nI hope you liked it!","327593d3":"The Tfidf in Tfidf Vectorizer stands for Term Frequency - Inverse Document Frequency. It is given by the product of the term frequency and the logarithmic ratio of the total number of documents divided by the number of documents with the term in question present. \n\nThe Tfidf has additional advantages over the Count Vectorizer as it normalizes the data and also accounts for the document frequency.","8b4df221":"Using the Count Vectorizer, we get a rather good accuracy of 80.2%. However, this accuracy comes with a caveat which will be explained below.","4bdd427d":"From this wordcloud, we are able to see that fake tweets play to **larger**, **more existential** themes about democracy, freedoms and control. Of course, there are the \"wired Joe\" theories that suggest that Joe Biden was wearing a wire, and the 'Chris' within the word cloud refers to Chris Wallace.","9f9007a5":"# Naive Bayes\n\nIn the ML segment of this notebook, we will compare two vectorizers and their ability to weigh certain terms within the document. \n\nRetweets were kept rather than discarded, as they still represent a way for the tweet to get further reach (and in fact, a sizeable amount of entries in this dataset were retweets) despite retweets causing significant distortion in the frequency of terms.\n\nThe two vectorizers used are the Count Vectorizer and the Tfidf Vectorizer. The general workflow begins with label encoding oftarget features, splitting the data into train and test sets, vectorizing tokenised text data, and then fitting a Naive Bayes predictor to make predictions. The accuracy score will then be calculated. Naive Bayes classifiers work assuming that one feature is independent of another feature and contribute independently to the probability that the account posting the tweet is real or fake.","861cb0ed":"# US Presidential Election Fake Tweets Detection\n\nIt's almost been a year since the Presidential Debates in 2020 between Donald Trump and Joe Biden, and Twitter activity reached peaks as people shared their [instinctive live reactions](https:\/\/www.vogue.com\/article\/best-twitter-reactions-final-presidential-debate) to things that happened on-air.\n\nWhile the level of polarization and position of the debate in the race means that voters' minds were largely made up in a bitter election campaign, it was [unlikely](https:\/\/fivethirtyeight.com\/features\/is-tonights-presidential-debate-too-late-to-shake-up-the-race\/) that the debate may have had a sizeable impact on changing voters' minds. In contrast, [misinformation](https:\/\/www.usatoday.com\/story\/news\/politics\/elections\/2020\/11\/17\/2020-presidential-election-misinformation-predictable-experts\/6322926002\/), possibly through fake tweets, could have had such an impact.\n\nWithin this notebook, some text processing will be done, and two vectorizers will be compared for their accuracy in distinguishing real and fake tweets from each other.","fa3ef989":"As seen, there is a slight improvement when the Tfidf Vectorizer is used. The weighting of features is also slightly different.","e751f180":"![image.png](attachment:0fcbb941-f5cd-4a54-bcb0-e61da44ad288.png)"}}