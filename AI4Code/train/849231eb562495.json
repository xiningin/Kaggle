{"cell_type":{"9b21f2f6":"code","2b1ce3b4":"code","c249836a":"code","fb051384":"code","9bbe3349":"code","40438a5c":"code","8d1a98c6":"code","185c191e":"code","9110604a":"code","3719c1e7":"code","1f7401ea":"code","31ccaecd":"code","9969de8b":"code","270e3d12":"code","037cfec1":"code","80ea0fde":"markdown","9480394e":"markdown","cba15495":"markdown","d56c9401":"markdown","ea2ca865":"markdown","0565029b":"markdown","d31aec7f":"markdown","bcbe84ae":"markdown","cc7693f7":"markdown"},"source":{"9b21f2f6":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings, gc\nwarnings.filterwarnings(\"ignore\")\n\n# SKLearn Classification Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# SKLearn Generic\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, validation_curve, KFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Tabulation\nfrom tabulate import tabulate\n","2b1ce3b4":"url = '..\/input\/all-datasets-for-practicing-ml\/Class\/Class_Abalone.csv'\ndata = pd.read_csv(url, header='infer')","c249836a":"# Total Records\nprint(\"Total Records: \", data.shape[0])","fb051384":"#Check for null\/missing values\nprint(\"Is there missing data? - \", data.empty)","9bbe3349":"# Records per sex\ndata.groupby(\"Sex\").size()","40438a5c":"#Stat Summary\ndata.describe().transpose()","8d1a98c6":"# Instantiating Label Encoder\nencoder = LabelEncoder()\n\n#Encoding Sex column\ndata['Sex'] = encoder.fit_transform(data['Sex'])\n\n\n#Feature & Target Selection\ncolumns = data.columns\ntarget = ['Sex']\nfeatures = columns[1:]\n\nX = data[features]\ny = data[target]\n\n\n# Dataset Split \n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0, shuffle=True) \n\n\n#Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.transform(X_val)","185c191e":"# -- Building Model List --\nmodels = []\n\nmodels.append(('GaussianNB', GaussianNB()))       # Gaussian Naive Bayes\nmodels.append(('RandomForest', RandomForestClassifier(verbose=0, random_state=1, max_features=8))) #Random Forest\nmodels.append(('KNN',KNeighborsClassifier()))  #KNN\nmodels.append(('DecisionTree',DecisionTreeClassifier(max_features=8, random_state=1)))  #Decision Tree\nmodels.append(('LinearDiscriminant',LinearDiscriminantAnalysis()))  #Linear Discriminant Analysis\n","9110604a":"# Voting Classifier\nvc = VotingClassifier(estimators=[('gnb', GaussianNB()),('rf', RandomForestClassifier(verbose=0, random_state=1, max_features=8)),\n                                  ('knn', KNeighborsClassifier()), ('dt', DecisionTreeClassifier(max_features=8, random_state=1)), \n                                  ('lda', LinearDiscriminantAnalysis())], voting='hard')\n\n#Fit\nvc.fit(X_train, y_train)\n\n# Appending the Voting Classifier to Model List\nmodels.append(('VotingClassifier',vc)) \n\nfor label, model in models:\n    kfold = KFold(n_splits=10, random_state=None, shuffle=False)\n    cross_val = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    print(\"Accuracy Score: %0.2f -- [%s]\" % (cross_val.mean(), label))","3719c1e7":"# Train & Predict (top 5 records)\ngnb_pred = GaussianNB().fit(X_train, y_train).predict(X_val[:5,])\nrf_pred = RandomForestClassifier(verbose=0, random_state=1, max_features=8).fit(X_train, y_train).predict(X_val[:5,])\nknn_pred = KNeighborsClassifier().fit(X_train, y_train).predict(X_val[:5,])\ndt_pred = DecisionTreeClassifier(max_features=8, random_state=1).fit(X_train, y_train).predict(X_val[:5,])\nlda_pred = LinearDiscriminantAnalysis().fit(X_train, y_train).predict(X_val[:5,])\nvc_pred = vc.predict(X_val[:5,])\n","1f7401ea":"# Tabulating the classifier predictions of first 5 records \ntab_pred = []\n\nfor idx,(a,b,c,d,e,f) in enumerate(zip(gnb_pred,rf_pred,knn_pred,dt_pred,lda_pred,vc_pred)):\n    tab_pred.append([idx+1,a,b,c,d,e,f])\n\nprint(\"Classifier Predictions: \\n\",tabulate(tab_pred, headers=[\"Record\",\"GaussianNB\",\"RandomF\",\"KNN\",\"DTree\",\"LDA\",\"Voting\"], tablefmt='pretty'))","31ccaecd":"# -- Building Model List --  \nmodels = []\n\nmodels.append(('GaussianNB', GaussianNB()))       # Gaussian Naive Bayes\nmodels.append(('RandomForest', RandomForestClassifier(verbose=0, random_state=1, max_features=8))) #Random Forest\nmodels.append(('KNN',KNeighborsClassifier()))  #KNN\nmodels.append(('DecisionTree',DecisionTreeClassifier(max_features=8, random_state=1)))  #Decision Tree\nmodels.append(('LinearDiscriminant',LinearDiscriminantAnalysis()))  #Linear Discriminant Analysis","9969de8b":"# Voting Classifier\nvc = VotingClassifier(estimators=[('gnb', GaussianNB()),('rf', RandomForestClassifier(verbose=0, random_state=1, max_features=8)),\n                                  ('knn', KNeighborsClassifier()), ('dt', DecisionTreeClassifier(max_features=8, random_state=1)), \n                                  ('lda', LinearDiscriminantAnalysis())], voting='soft')\n\n#Fit\nvc.fit(X_train, y_train)\n\n# Appending the Voting Classifier to Model List\nmodels.append(('VotingClassifier',vc)) \n\nfor label, model in models:\n    kfold = KFold(n_splits=10, random_state=None, shuffle=False)\n    cross_val = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    print(\"Accuracy Score: %0.2f -- [%s]\" % (cross_val.mean(), label))","270e3d12":"# Train & Predict (top 5 records)\ngnb_pred = GaussianNB().fit(X_train, y_train).predict(X_val[:5,])\nrf_pred = RandomForestClassifier(verbose=0, random_state=1, max_features=8).fit(X_train, y_train).predict(X_val[:5,])\nknn_pred = KNeighborsClassifier().fit(X_train, y_train).predict(X_val[:5,])\ndt_pred = DecisionTreeClassifier(max_features=8, random_state=1).fit(X_train, y_train).predict(X_val[:5,])\nlda_pred = LinearDiscriminantAnalysis().fit(X_train, y_train).predict(X_val[:5,])\nvc_pred = vc.predict(X_val[:5,])","037cfec1":"# Tabulating the classifier predictions of first 5 records \ntab_pred = []\n\nfor idx,(a,b,c,d,e,f) in enumerate(zip(gnb_pred,rf_pred,knn_pred,dt_pred,lda_pred,vc_pred)):\n    tab_pred.append([idx+1,a,b,c,d,e,f])\n\nprint(\"Classifier Predictions: \\n\",tabulate(tab_pred, headers=[\"Record\",\"GaussianNB\",\"RandomF\",\"KNN\",\"DTree\",\"LDA\",\"Voting\"], tablefmt='pretty'))","80ea0fde":"### In the table above, we can view the Voting Classifier (hard voting) in action. The voting classifier is selecting a class that has majority.","9480394e":"# Libraries","cba15495":"## Weighted Average Probabilities (Soft Voting)\n\nIn contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.\n\nSpecific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.\n\nNote: In this example, we're going to let the classifier give weights to the classes.","d56c9401":"## Majority Class Label (Majority\/Hard Voting)\n\nIn majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order","ea2ca865":"# Voting Classifier","0565029b":"# Data Prep, Feature Engineering, Data Split & Feature Scaling","d31aec7f":"# Data","bcbe84ae":"# Voting Classifier\n\n\nIn this notebook, we're going to learn about SKLearn's **Voting Classifier** with the help of [Abalone Classification](https:\/\/archive.ics.uci.edu\/ml\/datasets\/abalone) Dataset.\n\nSo, what is Voting Classifier anyways?\n\nAs per SK-Learn, The idea behind the **VotingClassifier** is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n\nWe'll perform both Hard & Soft Voting on couple of SK-Learn's classifiers & observe the accuracy\n\nFollowing are the SK-Learn Classifiers on which we're going to implement the Voting Classifier\n\n* GaussianNB\n* Random Forest\n* KNN\n* Decision Tree\n* Linear Discriminant Analysis\n\n\nAnd as always, I will keep the notebook fairely organized & well commented for easy reading. Please do consider to UPVOTE ","cc7693f7":"I hope, this notebook was helpful in understanding the workings of Voting Classifier."}}