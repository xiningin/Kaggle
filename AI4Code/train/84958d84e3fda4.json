{"cell_type":{"56b668a7":"code","dcaeb6af":"code","ecbc8797":"code","736ef528":"code","d17193af":"code","5144b3f1":"code","eb30ecb9":"code","cc0bdd3e":"code","bbd5924b":"code","f175c88c":"code","1933071a":"code","11e5b559":"code","923dfb40":"code","a731ec8c":"code","e89ee5e4":"code","93597235":"code","bfe76b00":"code","2d1e0112":"code","b33ab235":"code","d6f3c1da":"code","0089e6fd":"code","4e11e7ee":"code","705fef86":"code","a4b0071e":"code","8c68936f":"code","ad662a8f":"code","89287e9c":"code","7140e03f":"code","26464a6b":"code","9e8874d3":"code","fcac9d1e":"code","01c2c1fc":"code","7be93bfe":"code","63b1da14":"code","9b826bd2":"code","48f69e2b":"code","4c50943c":"code","ce75477d":"code","54e76b2f":"code","e173a8c4":"code","86967f05":"code","008f6189":"code","cfa2d713":"code","0285665d":"code","4b840e0a":"code","46f84fe4":"code","7df43648":"code","d8bb7bff":"code","d56e4267":"code","b848347c":"code","a01bb886":"code","ada1f8c9":"code","41194a59":"code","58cfe8e0":"markdown","5835972d":"markdown","dd318042":"markdown","47da0340":"markdown","bf9a6cd7":"markdown","96893827":"markdown","d6613d70":"markdown","e9ff7505":"markdown","fe4f9c70":"markdown","8785375c":"markdown","828d59bb":"markdown","bfa8914a":"markdown","e3b76a2c":"markdown","53c6db68":"markdown","8fb663e0":"markdown","d0871df9":"markdown","bab61ebd":"markdown","2516f623":"markdown","23cfa10d":"markdown","67d0ef1b":"markdown","fe01bfbf":"markdown","557d2065":"markdown","c66fc84b":"markdown","a2ec2e2a":"markdown","39cb3280":"markdown","45cf2141":"markdown","fa3400e0":"markdown","9ed41e12":"markdown","c520ccfd":"markdown","6c312685":"markdown","c0a48a90":"markdown"},"source":{"56b668a7":"#Import Essential Libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","dcaeb6af":"#Load Data\ndata = pd.read_excel('\/kaggle\/input\/mortality-risk-clinincal-data-of-covid19-patients\/Mortality_incidence_sociodemographic_and_clinical_data_in_COVID19_patients.xlsx')\ndata.head()","ecbc8797":"#Display the name of each column for reference\ndata.columns.sort_values()","736ef528":"#Analyse Data - Overview Shape\ndata.shape","d17193af":"#Display the type of each column and null (if present)\ndata.info()","5144b3f1":"#Is there any null?\ndata.isna().sum().any()","eb30ecb9":"#Age describe\ndata['Age.1'].describe()","cc0bdd3e":"#Describe age grouped\ndata.Age.describe()","bbd5924b":"#Plot distribution by sevetiry class\nplt.figure(figsize=(18,5))\nplt.title('Deaths by Severity distribution')\nsns.histplot(x='Severity', hue='Death', multiple=\"dodge\", data=data)","f175c88c":"#Plot distribution of ages\nplt.figure(figsize=(18,5))\nplt.title('Age distribution')\nsns.distplot(a=data['Age.1'], bins=8, kde_kws={\"color\":\"red\"})","1933071a":"#Plot boxplot with detailed info of age\nplt.figure(figsize=(18,5))\nplt.title('Age distribution')\nsns.boxplot(x=data['Age.1'])","11e5b559":"#Plot scatter with detailed info of age vs survived\nplt.figure(figsize=(18,5))\nplt.title('Age distribution')\nsns.scatterplot(x=data.index, y=data['Age.1'], hue=data.Death)","923dfb40":"grouped = data.Death.value_counts(normalize=True)*100\ngrouped.plot.bar(title='Deaths in general')","a731ec8c":"print('Percentage of people that survived: ',(data.Death.value_counts(normalize=True)*100)[0])\nprint('Percentage of people that not survived: ',(data.Death.value_counts(normalize=True)*100)[1])","e89ee5e4":"# How many times each age appears\ndata['Age.1'].value_counts()[:10]","93597235":"# Plot the percentage of deaths by each severity class\ngrouped = data.groupby('Severity').sum()['Death'] \/ data.groupby('Severity').count()['Death']\ngrouped.plot(title = '`%` of Deaths by sevetiry')","bfe76b00":"# Plot the percentage of deaths by Maps Less than 70\ngrouped = data.groupby('MAP < 70').sum()['Death'] \/ data.groupby('MAP < 70').count()['Death']\ngrouped.plot.bar(title = '`%` of deaths by MAP Less than 70')","2d1e0112":"# Plot the percentage of deaths by Age\ngroupedAge = data.groupby('Age.1').sum()['Death'] \/ data.groupby('Age.1').count()['Death']\ngroupedAge.plot.bar(title = '`%` of deaths by Age', figsize=(25,5))","b33ab235":"# Plot the percentage of deaths by BUN > 30\t\ngrouped = data.groupby('Age').sum()['Death'] \/ data.groupby('Age').count()['Death']\ngrouped.plot.bar(title='Porcentage of Death by age category')","d6f3c1da":"grouped","0089e6fd":"#show only column names with more than 2 unique values, discarding all boolean ones. \ncols = ((data.dtypes != 'object') & (data.nunique() > 2))\ncols = cols.drop(['LOS', 'Severity', 'Age.1', 'AgeScore','CrtnScore']) #drop items that show categories, scores, etc.\nnormFeatures = list(cols[cols].index)\nprint(normFeatures)","4e11e7ee":"#Apply StandardScaler() in a couple of columns\nfrom sklearn.preprocessing import StandardScaler\n\n\nnormalizedData = data.copy()\n\nfor name in normFeatures:\n    normalizedData[name] = (normalizedData[name] - normalizedData[name].mean()) \/ normalizedData[name].std()\n\nnormalizedData[normFeatures]","705fef86":"#Display all features after normalization\nnormalizedData[normFeatures].hist(figsize=(25, 20))\nplt.show()","a4b0071e":"#Retrieve the data with the original name\ndata = normalizedData","8c68936f":"#Change \"<\" for Less and \"<\" for Grater to allow the use of the XGBoost Classifier\ndata.columns = data.columns.str.replace(r\"[<]\", \"Less\")\ndata.columns = data.columns.str.replace(r\"[>]\", \"Greater\")","ad662a8f":"#Which columns do we have now?\ndata.columns","89287e9c":"#Group (cut) severity by classes - Apply Binning\nlabels = [0,1,2,3]\ndata['Severity_class'] = pd.cut(data['Severity'], bins=4, labels=labels, right=False)\ndata.head()","7140e03f":"#How many records do we have in each class\ndata['Severity_class'].value_counts()","26464a6b":"#The severity class may be a Dtype-Category. So we transform it in a int to use below.\n#Severity_class to int\ndata['Severity_class'] = data['Severity_class'].astype('int64')\ndata['Severity_class'][:5]","9e8874d3":"# Calculate the correlation between variables and Deaths\ncorrelation_order = data.corr()[['Death']].sort_values(by=['Death'], ascending = False).drop(['Death'])\ncorrelation_order.head(25)","fcac9d1e":"# Calculate the correlation between variables and Severity_class\ncorrelation_order_Severity = data.corr()[['Severity_class']].sort_values(by=['Severity_class'], ascending = False).drop(['Severity_class'])\ncorrelation_order_Severity[:25]","01c2c1fc":"# Mutual Information\nfrom sklearn.feature_selection import mutual_info_regression\nimport numpy as np\n\nXforMI = data.copy()\nyForMI = XforMI.pop('Severity_class')\n\n# Label encoding for categoricals\nfor colname in XforMI.select_dtypes(\"object\"):\n    XforMI[colname], _ = XforMI[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = XforMI.dtypes == int\n\n\ndef make_mi_scores(X, y, discrete_features):\n    # mutualX = data.copy()\n    # for colname in mutualX.select_dtypes([\"object\", \"category\"]):\n    #     mutualX[colname], _ = mutualX[colname].factorize()\n    # # All discrete features should now have integer dtypes\n    # discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nmi_scores = make_mi_scores(XforMI, yForMI, discrete_features)\nmi_scores.head(15)\n","7be93bfe":"# plot MI Scores\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores.head(20)) ","63b1da14":"#Choose the more correlated features to predict Severity Class\nfeatures = list(correlation_order_Severity.head(25).index)  #select the N features more correlatad\nfeatures.remove('Age.1') #drop columns with same meaning from the original one hot encoder\nfeatures.remove('Death') #drop Death to predict in the end\nfeatures.remove('Severity') #drop Severity because we grouped this in classes 'Severity_class'\n# features.remove('BUN')\n# features.remove('Procalcitonin')\n\nfeatures","9b826bd2":"#Correlation heat Map\n\n#Add the Sevetiry_class to see the correlation with each variable\nfeaturesForCorrMap = features.copy()\nfeaturesForCorrMap.append('Severity_class') \n\nfig,ax=plt.subplots(1,1,figsize=(20,15))\nsns.heatmap(data[featuresForCorrMap].corr(),annot=True,label='spearman Correlation Heat Map')","48f69e2b":"#Train and Test Spliting\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = data[features]\ny = data['Severity_class']\n\n# Divis\u00e3o da base de dados entre treinamento e teste (20% para testar e 80% para treinar)\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.2,\n                                                    random_state = 0)\n","4c50943c":"y.unique()","ce75477d":"#Pipeline With some Models to test\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n#import model algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nrandomState = 12\nnum_class = len(y.unique())\n\n# Define the model to use:\nmodel=[]\nmodel.append(('randomForestClassifier', RandomForestClassifier(n_estimators = 100, random_state=randomState)))\nmodel.append(('SVC', SVC(kernel='rbf')))\nmodel.append(('NaiveBayes', GaussianNB()))\nmodel.append(('XGClassifier', XGBClassifier(learning_rate=0.05, objective='multi:softmax', n_estimators=100, use_label_encoder=False, num_class=num_class)))\n\nscores=[]\n\n# Apply CrossValidation for each (accuracy for classification)XGBClassifier\nfor name, models in model:\n    pipeline = Pipeline(steps=[('model', models)])\n\n    score = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy', n_jobs=1, verbose=False)\n    scores.append((name, np.mean(score)))\n    \nscores","54e76b2f":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n# Predict using the best models\nXGClass_model = XGBClassifier(learning_rate=0.01, n_estimators=1500, max_depth=4, \n                              early_stopping_rounds=7, \n                              eval_set=[(X_test, y_test)],\n                              num_class=num_class,\n                              verbose=False)\n\nXGClass_model.fit(X_train, y_train)\n\n# Proccess the predictions for the training of the model\npredictions_XGClass = XGClass_model.predict(X_test)\n\n\n# Model Validation using accuracy score\nprint('Accuracy for the XGClassifier model was: \\n', accuracy_score(y_test, predictions_XGClass))\n","e173a8c4":"# Confusion Matrix to visualize errors\nfrom sklearn.metrics import confusion_matrix\n\nconfusion = confusion_matrix(y_test, predictions_XGClass)\nconfusion","86967f05":"#Choose the more correlated features to predict Severity Class\nfeaturesForDeath = list(correlation_order.head(28).index)  #select the N features more correlatad\nfeaturesForDeath.remove('Age.1') #drop columns with same meaning from the original one hot encoder\nfeaturesForDeath.remove('Severity') #drop because we will use the just the Severity Class\nfeaturesForDeath.remove('LOS') #drop because we will not have this information when patiente arrive\nfeaturesForDeath.remove('LOS_Y') #drop because we will use the just the Severity Class\n\nfeaturesForDeath","008f6189":"#Train and Test Spliting for Deaths\n\nX_death = data[featuresForDeath]\ny_death = data['Death']\n\n# Divis\u00e3o da base de dados entre treinamento e teste (20% para testar e 80% para treinar)\nX_trainDeath, X_testDeath, y_trainDeath, y_testDeath = train_test_split(X_death,\n                                                                        y_death,\n                                                                        test_size = 0.2,\n                                                                        random_state = 0)\ny_death.unique()","cfa2d713":"#Pipeline With some Models to test\n\nrandomState = 12\nnum_class = len(y_death.unique())\n\n# Define the model to use:\nmodel=[]\nmodel.append(('randomForestClassifier', RandomForestClassifier(n_estimators = 100, random_state=randomState)))\nmodel.append(('SVC', SVC(kernel='rbf')))\nmodel.append(('NaiveBayes', GaussianNB()))\nmodel.append(('LogisticRegression', LogisticRegression(n_jobs=50)))\nmodel.append(('XGBClassifier', XGBClassifier(learning_rate=0.05, n_estimators=500, use_label_encoder=False)))\n\nscores=[]\n\n# Apply CrossValidation for each (accuracy for classification)XGBClassifier\nfor name, models in model:\n    pipeline = Pipeline(steps=[('model', models)])\n\n    score = cross_val_score(pipeline, X_death, y_death, cv=5, scoring='accuracy', n_jobs=1, verbose=False)\n    scores.append((name, np.mean(score)))\n    \nscores","0285665d":"#Random Forest Test\nrandomForest = RandomForestClassifier(n_estimators = 100,  max_depth=50, random_state=randomState)\nrandomForest.fit(X_trainDeath, y_trainDeath)\n\n# Proccess the predictions for the training of the model\npredictions_RandomForest = randomForest.predict(X_testDeath)\n\n# Create the probability to calculate the AUC\nrandomForest_pred_probability = randomForest.predict_proba(X_testDeath)[::,1]\n\n# Model Validation using accuracy score\nprint('Accuracy was: \\n', accuracy_score(y_testDeath, predictions_RandomForest))\nprint('AUC Score was: \\n', roc_auc_score(y_testDeath, randomForest_pred_probability))\n\n# Plot the AUC Curve\nfpr, tpr, _ = roc_curve(y_testDeath,  randomForest_pred_probability)\nauc = roc_auc_score(y_testDeath, randomForest_pred_probability)\nplt.plot(fpr,tpr,label=\"Random Forest, auc=\" + str(auc))\nplt.legend(loc=4)\nplt.show()","4b840e0a":"#Load Data\nnewData = pd.read_excel('\/kaggle\/input\/mortality-risk-clinincal-data-of-covid19-patients\/Mortality_incidence_sociodemographic_and_clinical_data_in_COVID19_patients.xlsx')\n\nactualDeath = newData.pop('Death').head(100)\n\n#drop Severity and LOS Related Columns\nnewData = newData.drop(columns=['Severity', 'LOS', 'LOS_Y'])\nnewData = newData.head(100)\nnewData.shape","46f84fe4":"#Apply the same normalization\n\nnormalizedNewData = newData.copy()\n\nfor name in normFeatures:\n    normalizedNewData[name] = (normalizedNewData[name] - normalizedNewData[name].mean()) \/ normalizedNewData[name].std()\n\nnormalizedNewData[normFeatures]","7df43648":"# Apply the same column name cleaning\nnewData = normalizedNewData\n\n#Change \"<\" for Less and \"<\" for Grater to allow the use of the XGBoost Classifier\nnewData.columns = newData.columns.str.replace(r\"[<]\", \"Less\")\nnewData.columns = newData.columns.str.replace(r\"[>]\", \"Greater\")","d8bb7bff":"newDataSeverityPred = newData[features]","d56e4267":"# Proccess the predictions for the selected features\nactualPredictions = XGClass_model.predict(newDataSeverityPred)","b848347c":"#Get the predictions as a column to predict the Deaths\nnewData['Severity_class'] = actualPredictions\nnewData.head()","a01bb886":"#Predict the Deaths\nnewData = newData[featuresForDeath]\n\n# Proccess the predictions for the training of the model\nactualPredictions_RandomForest = randomForest.predict(newData)\n\n# Create the probability to calculate the AUC\nactualRandomForest_pred_probability = randomForest.predict_proba(newData)[::,1]\n\n# Model Validation using accuracy score\nprint('Accuracy was: \\n', accuracy_score(actualDeath, actualPredictions_RandomForest))\nprint('AUC Score was: \\n', roc_auc_score(actualDeath, actualRandomForest_pred_probability))\n\n# Plot the AUC Curve\nfpr, tpr, _ = roc_curve(actualDeath,  actualRandomForest_pred_probability)\nauc = roc_auc_score(actualDeath, actualRandomForest_pred_probability)\nplt.plot(fpr,tpr,label=\"RandomForest, auc=\" + str(auc))\nplt.legend(loc=4)\nplt.show()","ada1f8c9":"# Confusion Matrix to visualize errors\nconfusion = confusion_matrix(actualDeath, actualPredictions_RandomForest)\nconfusion","41194a59":"# Save results to disk\n# Export dataset with the predicted data\n\noutput = pd.DataFrame({'Id': newData.index, 'Severity_class': actualPredictions,'Death': actualPredictions_RandomForest})\noutput\n","58cfe8e0":"The \u201cAge\u201d feature was divided into 4 categories, and most occurrences are between 0-60 years old, with 1854 entries.","5835972d":"#### Apply same data processing","dd318042":"## Apply Normalization","47da0340":"### New Feature Creation\n\nAs the probability of death is well defined in relation to \u201cSeverity\u201d, we can apply a binning method to cluster these classes into 4 groups. Class 3 contains the 4 highest values for original \u201cSeverity\u201d values, with a high probability of death. So, we were able to improve the predictive capacity of the model that can focus between 4 classes, instead of 12 as we had originally.\n\nWe then created the feature \u201cSeverity_class\u201d.\n","bf9a6cd7":"The great majority of people survive, with an index of 75,63%.","96893827":"# Second step\n\nThe second step is to use a new model to predict whether the patient will die or survive, based on the original data and the new class we have grouped (Severity_class). So, after predicting Severity_class, we can use each patient's measures and predict whether they will survive.","d6613d70":"# Third step\nThe third step is to use a new dataset, without the death and severity data, so we can predict:\n-\tFirst: which will be the severity of the disease in the patient considering all the variables measured as soon as he arrives at the hospital.\n-\tSecond: what is the patient's chance of surviving.\n\n----\n\nFor this we will use the first 100 rows from the original dataset, as we don't have any new data. Afterwards, we will withdraw from this new set the dependent variables and use only what we will be able to measure when the patient arrives.\n","e9ff7505":"## Final result\nWith the first 100 lines, we predicted the Severity_Class and then whether the patient would die or survive, using the same previously trained models, obtaining an accuracy of 88% in predicting deaths and an AUC score of 0.92.","fe4f9c70":"## EDA - Visualization and Analyses","8785375c":"### Correlations","828d59bb":"Most promising models are XGClassifier and SVC.","bfa8914a":"## Models","e3b76a2c":"### Random Forest for Deaths","53c6db68":"The MAP feature seems to have a good correlation with deaths. Just 20% of the patients that were positive on the grouped feature \u201cMAP < 70\u201d could survive.","8fb663e0":"This Scatterplot suggests a trend of older people being more propense to death in this context.","d0871df9":"When the class \u201cSeverity\u201d increase in value, the chance of death rise significantly. 80% of patients with severity above 7 die. ","bab61ebd":"#### Load new data\nJust simulation new unseen data, using the first 100 rows opf the original dataset just to simulate.","2516f623":"### XGClassifier","23cfa10d":"The 25 features with the highest direct correlation with deaths and the 25 variables with the highest direct correlation with Severity_Class. In this way, we can have insights of which variables to use to have a better model.","67d0ef1b":"The plot shows that as greater the value for Severity is, greater is the proportion of deaths.","fe01bfbf":"### Predict Deaths","557d2065":"### Chosen Model\nAfter applying the models with best accuracy in Cross_Validation, we obtained the best score with XGClassifier, reaching 96% correct answers for predicting Severity_class. We then choose the XGClassifier model as final.","c66fc84b":"## Models for Death","a2ec2e2a":"We chose the 25 best variables, just droping:\nAge.1 - as we already have Age Score and another age variable grouped,\nDeath - as we won't have this information in the patient's entry, and this is what we will predict in a second moment,\nSeverity - since this will be our target at first.","39cb3280":"We discovered that the dataset has no null data, and has a total of 4711 records and 85 features. Some variables are groups of other related variables, such as those with \u201cscore\u201d in the name. We also take into account that the variable \u201cLOS\u201d and \u201cDeath\u201d will not be known when new patient just arrive, so we will not use them to avoid Data Leakage. ","45cf2141":"In the category over 80 years old, the percentage of deaths reaches 44%, against 12% between 0 and 60 years old.","fa3400e0":"### Model chosen for Death\nAfter applying the models with greater accuracy in Cross_Validation, we obtained the best score with Random Forest, reaching AUC of 0.84 using Sevetiry_class.","9ed41e12":"## Deeper Features Analysis","c520ccfd":"## Load Data","6c312685":"### Predict Severity Class","c0a48a90":"The plot above tells us that we have just a few outliers considering the age and that the greater amount is between 54 and 76 years."}}