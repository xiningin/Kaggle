{"cell_type":{"c1ce4353":"code","29db632d":"code","01cfd486":"code","33f8c03b":"code","63eb6a73":"code","d38882d0":"code","4355f1b4":"code","2d81d68c":"code","0769a802":"code","dc0658e8":"code","b13176fa":"code","f2869cfa":"code","6bf3d2de":"code","464faf10":"code","77b6570c":"code","4b1bdb13":"code","ba35f265":"code","69c0c7e4":"markdown"},"source":{"c1ce4353":"from google.colab import drive\n\ndrive.mount('\/content\/drive\/')","29db632d":"!cp \/content\/drive\/MyDrive\/x5_lab2\/mipt-x5-lab-2.zip .\n!unzip -q mipt-x5-lab-2.zip","01cfd486":"import os\nimport shutil\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport time\n\nfrom IPython.display import clear_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","33f8c03b":"print('\u0414\u0430\u043d\u043d\u044b\u0445 \u0432 train: ', len(os.listdir('\/content\/mipt-x5-lab-2\/train')))\nprint('\u0414\u0430\u043d\u043d\u044b\u0445 \u0432 val: ', len(os.listdir('\/content\/mipt-x5-lab-2\/valid')))\nprint('\u0414\u0430\u043d\u043d\u044b\u0445 \u0432 test: ', len(os.listdir('\/content\/mipt-x5-lab-2\/test')))","63eb6a73":"class ClassificationDataset(Dataset):\n    def __init__(self, path_to_data, path_to_labels, stage, input_size=224):\n        self.path_to_data = path_to_data\n\n        if stage == 'train' or stage == 'valid':\n            self.labels = pd.read_csv(path_to_labels)\n        else:\n            self.labels = None\n\n        # \u0432 \u0442\u0430\u043a\u043e\u043c \u0436\u0435 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043f\u0443\u0442\u0438 \u0434\u043e \u0444\u0430\u0439\u043b\u043e\u0432\n        if self.labels is not None:\n            self.file_names = [path_to_data + file_nm for file_nm in self.labels['id']]\n        else:\n            self.file_names = [\n                path_to_data + file_nm for file_nm in sorted(\n                    os.listdir(self.path_to_data), key = lambda x: int(x[2:-4])\n                )\n            ]\n\n        # \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0442\u0440\u0430\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438\n        self.input_size = input_size\n        \n        if stage == 'train':\n            self._preprocess = transforms.Compose([\n                transforms.Resize((input_size, input_size)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n        else:\n            self._preprocess = transforms.Compose([\n                transforms.Resize((input_size, input_size)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def _load_file(self, path):\n        with open(path, 'rb') as input_file:\n            with Image.open(input_file) as img:\n                return img.convert('RGB')\n\n    def __getitem__(self, idx):\n        image = self._load_file(self.file_names[idx])\n        image = self._preprocess(image)\n\n        if self.labels is not None:\n            return image, self.labels.iloc[idx]['target_people']\n        else:\n            return image","d38882d0":"train_dataset = ClassificationDataset('\/content\/mipt-x5-lab-2\/train\/', '\/content\/mipt-x5-lab-2\/train.csv', 'train')\nval_dataset = ClassificationDataset('\/content\/mipt-x5-lab-2\/valid\/', '\/content\/mipt-x5-lab-2\/valid.csv', 'valid')\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","4355f1b4":"def plot_learning_curves(history, ylim=(2, 4)):\n    '''\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u044b\u0432\u043e\u0434\u0430 \u043b\u043e\u0441\u0441\u0430 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.\n\n    :param history: (dict)\n        accuracy \u0438 loss \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n    '''\n    sns.set_style(style='whitegrid')\n    fig = plt.figure(figsize=(20, 7))\n\n    plt.subplot(1,2,1)\n    plt.title('\u041b\u043e\u0441\u0441', fontsize=15)\n    plt.plot(history['loss']['train'], label='train')\n    plt.plot(history['loss']['val'], label='val')\n    plt.ylabel('\u041b\u043e\u0441\u0441', fontsize=15)\n    plt.xlabel('\u042d\u043f\u043e\u0445\u0430', fontsize=15)\n    plt.legend()\n    plt.ylim(ylim)\n\n    plt.subplot(1,2,2)\n    plt.title('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c', fontsize=15)\n    plt.plot(history['auc']['train'], label='train')\n    plt.plot(history['auc']['val'], label='val')\n    plt.ylabel('\u041c\u0435\u0442\u0440\u0438\u043a\u0430', fontsize=15)\n    plt.xlabel('\u042d\u043f\u043e\u0445\u0430', fontsize=15)\n    plt.legend()\n    plt.show()","2d81d68c":"def eval_epoch(model, batch_gen, is_train=False):\n    '''\n    \u041e\u0434\u043d\u0430 \u044d\u043f\u043e\u0445\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\/\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n    '''\n\n    # \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f loss \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train(is_train)\n\n    # \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n    preds = []\n    labels = []\n    for X_batch, y_batch in tqdm(batch_gen):\n\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        logits = model(X_batch)\n        \n        loss = criterion(logits.reshape(-1), y_batch.float().reshape(-1).to(device))\n\n        # \u0435\u0441\u043b\u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c -- \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u0435\u0442\u0438\n        if is_train:\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        epoch_loss += np.sum(loss.detach().cpu().numpy())\n        preds += logits.detach().cpu().numpy().tolist()\n        labels += y_batch.detach().cpu().numpy().tolist()\n    \n    epoch_loss \/= len(batch_gen)\n    epoch_auc = roc_auc_score(labels, preds)\n\n    return epoch_loss, epoch_auc","0769a802":"def train(\n    model, \n    criterion,\n    optimizer, \n    scheduler,\n    sch_type,\n    train_batch_gen,\n    val_batch_gen,\n    num_epochs=50,\n    ylim=(2, 4)\n):\n    '''\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u044b\u0432\u043e\u0434\u0430 \u043b\u043e\u0441\u0441\u0430 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.\n\n    :param model: \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\n    :param criterion: \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\n    :param optimizer: \u043c\u0435\u0442\u043e\u0434 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\n    :param train_batch_gen: \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0431\u0430\u0442\u0447\u0435\u0439 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    :param val_batch_gen: \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0431\u0430\u0442\u0447\u0435\u0439 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n    :param num_epochs: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445\n\n    :return: \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\n    :return: (dict) accuracy \u0438 loss \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 (\"\u0438\u0441\u0442\u043e\u0440\u0438\u044f\" \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f)\n    '''\n\n    history = defaultdict(lambda: defaultdict(list))\n    \n    best_val_loss = np.infty\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n\n        train_loss, train_auc = eval_epoch(model, train_batch_gen, True)\n        history['loss']['train'].append(train_loss)\n        history['auc']['train'].append(train_auc)\n\n        val_loss, val_auc = eval_epoch(model, val_batch_gen, False)\n        history['loss']['val'].append(val_loss)\n        history['auc']['val'].append(val_auc)\n        \n        clear_output()\n\n        # \u041f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0438\n        print(\"Epoch {} of {} took {:.3f}s\".format(\n            epoch + 1, num_epochs, time.time() - start_time))\n        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n        print(\"  training auc: \\t\\t\\t{:.2f} %\".format(train_auc * 100))\n        print(\"  validation auc: \\t\\t\\t{:.2f} %\".format(val_auc * 100))\n\n        if sch_type == 'plateao':\n            scheduler.step(val_loss)\n        else:\n            scheduler.step()\n\n        plot_learning_curves(history, ylim)\n        \n    return model, history","dc0658e8":"class ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = torchvision.models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Sequential(\n            nn.Linear(2048, 1000),\n            nn.Dropout(),\n            nn.ReLU(),\n            nn.Linear(1000, 1)\n        )\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return x","b13176fa":"net = ResNet50()","f2869cfa":"device = torch.device('cuda')\n\nnet = ResNet50()\nnet = net.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(net.parameters(), lr=3e-04)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3)\n\nnet, _ = train(\n    net, criterion, optimizer, scheduler, 'plateao',\n    train_loader, val_loader, \n    num_epochs=3, ylim=(0, 1.5)\n)","6bf3d2de":"def get_predicts(model, batch_gen):\n\n    model.train(False)\n\n    preds = []\n    for X_batch in tqdm(batch_gen):\n\n        X_batch = X_batch.to(device)\n\n        logits = model(X_batch)\n        \n        y_pred = logits.detach().cpu().numpy().reshape(-1)\n\n        preds += y_pred.tolist()\n\n    return preds","464faf10":"test_dataset = ClassificationDataset('\/content\/mipt-x5-lab-2\/test\/', None, 'test')\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\npreds = get_predicts(net, test_loader)","77b6570c":"subm = pd.read_csv('\/content\/mipt-x5-lab-2\/sample_submission.csv')\nsubm.head()","4b1bdb13":"subm['target_people'] = 1 \/ (1 + np.exp(-np.array(preds)))","ba35f265":"subm.to_csv('answer.csv', index=False)","69c0c7e4":"## \u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f"}}