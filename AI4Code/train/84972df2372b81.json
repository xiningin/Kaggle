{"cell_type":{"03cbb96f":"code","7dcd7cb0":"code","7bb0575b":"code","92b918a4":"code","eb37bc38":"code","58855211":"code","d8520650":"code","d672270e":"code","e885c211":"code","48a7d5a0":"code","3be153ab":"code","61c1ed4b":"code","6b38a739":"code","a190c61d":"code","b5ebee92":"code","df05d7a3":"code","deb2bf8c":"code","abf6df58":"code","2620b44e":"code","85d51865":"code","0c6a8f98":"code","c2f98f4f":"code","ed120857":"code","943c1149":"code","f7f5865a":"code","8b1ac14f":"code","8d504e7b":"code","85663b1f":"code","474d36fc":"code","5442aa9d":"code","3775b5f6":"code","c95ddd0b":"code","6134b472":"code","17130349":"code","9db3b07d":"code","e435f895":"code","2cc44dc9":"code","10e1bf99":"code","df1f774b":"code","6fe3cfab":"code","38f54da0":"code","542680ed":"code","57de8a3b":"code","09ce178b":"code","4f8ccd8c":"code","d16307ab":"code","06d64012":"code","717e7622":"code","9f8e8aad":"code","39831ea3":"code","83388fff":"code","923d9044":"code","834edaec":"markdown","ed8f1d7f":"markdown","b6166844":"markdown","090f966f":"markdown","e6598fec":"markdown","32c1c1c0":"markdown","8d6b69d4":"markdown","ab5c2643":"markdown","0624e2bd":"markdown","fd58bfa2":"markdown","642e144f":"markdown","65bece1f":"markdown","c35a7fce":"markdown","3517d7c6":"markdown","ed44c293":"markdown","2f1c112e":"markdown","c4d43642":"markdown","06e6f8de":"markdown","d94fd3c3":"markdown","67ee2af5":"markdown","6baceea9":"markdown","2734f934":"markdown","c8ea9bec":"markdown","d209c62b":"markdown","775b6d39":"markdown","54b47ffd":"markdown","c964217a":"markdown","392c5c25":"markdown","b33ef013":"markdown","20142930":"markdown","1b994c26":"markdown","a22ac326":"markdown","fe0070d2":"markdown","cb833dd0":"markdown","d73e8c18":"markdown","7bb55df2":"markdown","e4bbc0c3":"markdown","8fa3ce20":"markdown","71bdaac4":"markdown","84f2bc3b":"markdown","f66fa1b0":"markdown","b4db486d":"markdown","4d328cd3":"markdown","e50b118d":"markdown","04dd1462":"markdown","fb1eb0f1":"markdown","f5298146":"markdown","97e218e9":"markdown","b7692a68":"markdown","70052850":"markdown","b80f96fe":"markdown","7c85a7fc":"markdown","c332e1c3":"markdown","8c126494":"markdown","087df322":"markdown"},"source":{"03cbb96f":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nimport re\nimport warnings \nwarnings.filterwarnings('ignore')\n\n#plotting libraries!\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom geopandas import GeoDataFrame\n%matplotlib inline\n\n\n#PLOTLY\nimport plotly\nimport plotly.plotly as py\nimport plotly.offline as offline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\nfrom collections import defaultdict\nfrom plotly import tools\nfrom plotly.graph_objs import Scatter, Figure, Layout\ncf.set_config_file(offline=True)\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport squarify\n\nprint(os.listdir('..\/input'))","7dcd7cb0":"twitter_data = pd.read_csv('..\/input\/auspol2019.csv',parse_dates=['created_at','user_created_at'])\ngeo_data = pd.read_csv('..\/input\/location_geocode.csv')","7bb0575b":"twitter_data.head()","92b918a4":"geo_data.head()","eb37bc38":"twitter_data.shape","58855211":"geo_data.shape","d8520650":"#merging two data frames based on user location\ntwitter_data = twitter_data.merge(geo_data, how='inner', left_on='user_location', right_on='name')","d672270e":"twitter_data.head()","e885c211":"twitter_data = twitter_data.drop('name',axis =1)","48a7d5a0":"#lets check for null values\ntwitter_data.isnull().mean()*100","3be153ab":"print(f\" Data Available since {twitter_data.created_at.min()}\")\nprint(f\" Data Available upto {twitter_data.created_at.max()}\")","61c1ed4b":"#lets check latest and oldest twitter members in the dataframe\nprint(f\" Data Available since {twitter_data.user_created_at.min()}\")\nprint(f\" Data Available upto {twitter_data.user_created_at.max()}\")","6b38a739":"print('The oldest user in the data was',twitter_data.loc[twitter_data['user_created_at'] == '2006-03-21 21:04:12', 'user_name'].values)","a190c61d":"print('The newest user in the data was',twitter_data.loc[twitter_data['user_created_at'] == '2019-05-19 10:49:59', 'user_name'].values)","b5ebee92":"#lets explore created_at column\ntwitter_data['created_at'] =  pd.to_datetime(twitter_data['created_at'])\ncnt_srs = twitter_data['created_at'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=12)\nplt.title(\"Number of tweets according to dates\")\nplt.show()","df05d7a3":"#lets explore user_created_at column\ncount_  = twitter_data['user_created_at'].dt.date.value_counts()\ncount_ = count_[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(count_.index, count_.values, alpha=0.8)\nplt.title('Most accounts created according to date')\nplt.xticks(rotation='vertical')\nplt.ylabel('Number of accounts', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.show()","deb2bf8c":"#lets derive some columns from date colums\ntwitter_data['tweeted_day_of_week'] = twitter_data['created_at'].dt.weekday_name\ntwitter_data['created_day_of_week'] = twitter_data['user_created_at'].dt.weekday_name","abf6df58":"cnt_ = twitter_data['tweeted_day_of_week'].value_counts()\ncnt_ = cnt_.sort_index() \nfig = {\n  \"data\": [\n    {\n      \"values\": cnt_.values,\n      \"labels\": cnt_.index,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Number of tweets per day\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Percentage of tweets per days of the week\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n             \"text\": \"Percentage of Tweets according to days of the week\",\n                \"x\": 0.50,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)\ncnt_","2620b44e":"\nx = 0.\ny = 0.\nwidth = 50.\nheight = 50.\ntype_list = list(twitter_data['tweeted_day_of_week'].unique())\nvalues = [len(twitter_data[twitter_data['tweeted_day_of_week'] == i]) for i in type_list]\n\nnormed = squarify.normalize_sizes(values, width, height)\nrects = squarify.squarify(normed, x, y, width, height)\n\ncolor_brewer = ['#2D3142','#4F5D75','#BFC0C0','#F2D7EE','#EF8354','#839788','#EEE0CB']\nshapes = []\nannotations = []\ncounter = 0\n\nfor r in rects:\n    shapes.append( \n        dict(\n            type = 'rect', \n            x0 = r['x'], \n            y0 = r['y'], \n            x1 = r['x']+r['dx'], \n            y1 = r['y']+r['dy'],\n            line = dict( width = 2 ),\n            fillcolor = color_brewer[counter]\n        ) \n    )\n    annotations.append(\n        dict(\n            x = r['x']+(r['dx']\/2),\n            y = r['y']+(r['dy']\/2),\n            text = \"{}-{}\".format(type_list[counter], values[counter]),\n            showarrow = False\n        )\n    )\n    counter = counter + 1\n    if counter >= len(color_brewer):\n        counter = 0\n\n# For hover text\ntrace0 = go.Scatter(\n    x = [ r['x']+(r['dx']\/2) for r in rects ], \n    y = [ r['y']+(r['dy']\/2) for r in rects ],\n    text = [ str(v) for v in values ], \n    mode = 'text',\n)\n        \nlayout = dict(\n    height=700, \n    width=700,\n    xaxis=dict(showgrid=False,zeroline=False),\n    yaxis=dict(showgrid=False,zeroline=False),\n    shapes=shapes,\n    annotations=annotations,\n    hovermode='closest',\n    font=dict(color=\"#FFFFFF\")\n)\n\n# With hovertext\nfigure = dict(data=[trace0], layout=layout)\niplot(figure, filename='squarify-treemap')","85d51865":"cnt_ = twitter_data['created_day_of_week'].value_counts()\ncnt_ = cnt_.sort_index() \nfig = {\n  \"data\": [\n    {\n      \"values\": cnt_.values,\n      \"labels\": cnt_.index,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Number of tweets per day\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Percentage of created accounts per day\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n             \"text\": \"Percentage of accounts created according to days of the week\",\n                \"x\": 0.50,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)\ncnt_","0c6a8f98":"x = 0.\ny = 0.\nwidth = 50.\nheight = 50.\ntype_list = list(twitter_data['created_day_of_week'].unique())\nvalues = [len(twitter_data[twitter_data['created_day_of_week'] == i]) for i in type_list]\n\nnormed = squarify.normalize_sizes(values, width, height)\nrects = squarify.squarify(normed, x, y, width, height)\n\ncolor_brewer = ['#99B2DD','#F9DEC9','#3A405A','#494949','#FF5D73','#7C7A7A']\nshapes = []\nannotations = []\ncounter = 0\n\nfor r in rects:\n    shapes.append( \n        dict(\n            type = 'rect', \n            x0 = r['x'], \n            y0 = r['y'], \n            x1 = r['x']+r['dx'], \n            y1 = r['y']+r['dy'],\n            line = dict( width = 2 ),\n            fillcolor = color_brewer[counter]\n        ) \n    )\n    annotations.append(\n        dict(\n            x = r['x']+(r['dx']\/2),\n            y = r['y']+(r['dy']\/2),\n            text = \"{}-{}\".format(type_list[counter], values[counter]),\n            showarrow = False\n        )\n    )\n    counter = counter + 1\n    if counter >= len(color_brewer):\n        counter = 0\n\n# For hover text\ntrace0 = go.Scatter(\n    x = [ r['x']+(r['dx']\/2) for r in rects ], \n    y = [ r['y']+(r['dy']\/2) for r in rects ],\n    text = [ str(v) for v in values ], \n    mode = 'text',\n)\n        \nlayout = dict(\n    height=700, \n    width=700,\n    xaxis=dict(showgrid=False,zeroline=False),\n    yaxis=dict(showgrid=False,zeroline=False),\n    shapes=shapes,\n    annotations=annotations,\n    hovermode='closest',\n    font=dict(color=\"#FFFFFF\")\n)\n\n# With hovertext\nfigure = dict(data=[trace0], layout=layout)\niplot(figure, filename='squarify-tree')","c2f98f4f":"#lets extract the hours from the created_at and user_created_at column\ntwitter_data['created_at_hour'] = twitter_data['created_at'].dt.hour\ntwitter_data['user_created_at_hour'] = twitter_data['user_created_at'].dt.hour","ed120857":"cnt_ = twitter_data['created_at_hour'].value_counts()\ncnt_ = cnt_.sort_index() \ntrace1 = go.Scatter(\n                    x = cnt_.index,\n                    y = cnt_.values,\n                    mode = \"lines\",\n                    name = \"citations\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)')\n                    )\n\ndata = [trace1]\nlayout = dict(title = 'Number of tweets per hour',\n              xaxis= dict(title= 'Tweets per hour',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","943c1149":"cnt_ = twitter_data['user_created_at_hour'].value_counts()\ncnt_ = cnt_.sort_index() \ntrace1 = go.Scatter(\n                    x = cnt_.index,\n                    y = cnt_.values,\n                    mode = \"lines\",\n                    name = \"citations\",\n                    marker = dict(color = 'rgba(210, 113, 25, 0.8)')\n                    )\n\ndata = [trace1]\nlayout = dict(title = 'Number of Accounts Created per hour ',\n              xaxis= dict(title= 'Accounts per hour',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","f7f5865a":"#most favourite and retweeted tweet\nprint(f\" Maximum number of retweets {twitter_data.retweet_count.max()}\")\nprint(f\" Maximum number of favorites {twitter_data.favorite_count.max()}\")","8b1ac14f":"#lets see the tweet which has the maximum retweet count\ntwitter_data.loc[twitter_data['retweet_count']==6622.0,'full_text'].values","8d504e7b":"twitter_data.loc[twitter_data['favorite_count']==15559.0,['full_text','user_name','user_description']].values","85663b1f":"#most number of occurances of a person\ntwitter_data.user_name.value_counts()[:5,]","474d36fc":"#wordcloud\n\nwordcloud__ = WordCloud(\n                          background_color='white',\n                          stopwords=set(STOPWORDS),\n                          max_words=250,\n                          max_font_size=40, \n                          random_state=1705\n                         ).generate(str(twitter_data['user_screen_name'].dropna()))\ndef cloud_plot(wordcloud):\n    fig = plt.figure(1, figsize=(20,15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\ncloud_plot(wordcloud__)","5442aa9d":"#wordcloud\nwordcloud_ = WordCloud(\n                          background_color='black',\n                          stopwords=set(STOPWORDS),\n                          max_words=250,\n                          max_font_size=40, \n                          random_state=1705\n                         ).generate(str(twitter_data['user_description'].dropna()))\ndef cloud_plot(wordcloud):\n    fig = plt.figure(1, figsize=(20,15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\ncloud_plot(wordcloud_)","3775b5f6":"twitter_data['sentiment'] = twitter_data['full_text'].map(lambda text: TextBlob(text).sentiment.polarity)","c95ddd0b":"print(\"5 random tweets with highest positive sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==1, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","6134b472":"print(\"5 random tweets with highest nagative sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==-1, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","17130349":"print(\"5 random tweets with neutral sentiment polarity: \\n\")\ncL = twitter_data.loc[twitter_data.sentiment==0, ['full_text']].sample(5).values\nfor c in cL:\n    print(c[0])\n    print()","9db3b07d":"trace1 = go.Histogram(\n    x = twitter_data['sentiment'],\n    opacity=0.75,\n    name = \"Sentiment\",\n    marker=dict(color='rgba(122, 75, 196, 0.6)'))\n\ndata = [trace1]\nlayout = go.Layout(barmode='overlay',\n                   title='Histogram plot of sentiment',\n                   xaxis=dict(title='Sentiment'),\n                   yaxis=dict( title='Count'),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","e435f895":"cut = pd.cut(\n    twitter_data['sentiment'],\n    [-np.inf, -.01, .01, np.inf],\n    labels=['negative', 'neutral', 'positive']\n)\ntwitter_data['polarity'] = cut.values\ntwitter_data[['polarity','sentiment']][:20]","2cc44dc9":"twitter_data['polarity'].value_counts()","10e1bf99":"data = [go.Scatterpolar(\n  r = [twitter_data['polarity'].value_counts()[0],twitter_data['polarity'].value_counts()[1],twitter_data['polarity'].value_counts()[2]],\n  theta = list(twitter_data['polarity'].unique()),\n  fill = 'toself'\n)]\n\nlayout = go.Layout(\n  polar = dict(\n    radialaxis = dict(\n      visible = True,\n      range = [0, 60000]\n    )\n  ),\n  showlegend = False,\n  title ='Radar chart of polarities'\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename = \"Single Pokemon stats\")","df1f774b":"twitter_data['count_sent']=twitter_data[\"full_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ntwitter_data['count_word']=twitter_data[\"full_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ntwitter_data['count_unique_word']=twitter_data[\"full_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ntwitter_data['count_letters']=twitter_data[\"full_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ntwitter_data[\"count_punctuations\"] =twitter_data[\"full_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ntwitter_data[\"count_words_upper\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ntwitter_data[\"count_words_title\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ntwitter_data[\"count_stopwords\"] = twitter_data[\"full_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ntwitter_data[\"mean_word_len\"] = twitter_data[\"full_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","6fe3cfab":"twitter_data.describe().T","38f54da0":"sample_df = twitter_data[['count_sent','count_word','count_unique_word','count_letters','count_punctuations','count_words_upper','count_words_title','count_stopwords','mean_word_len' ]]\nsns.pairplot(sample_df,palette=\"husl\")\ndel sample_df","542680ed":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nfreq_dict = defaultdict(int)\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots.html')\n","57de8a3b":"freq_dict = defaultdict(int)\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\niplot(fig, filename='word-plots')","09ce178b":"freq_dict = defaultdict(int)\nplotly.tools.set_credentials_file(username='Ratan2513', api_key='atZYQqpeRmUlL5jaST4E')\nfor sent in twitter_data[\"full_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\nfig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams\", \n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig['layout'].update(height=1200, width=1500, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","4f8ccd8c":"cnt_ = twitter_data['user_location'].value_counts()\ncnt_.reset_index()\ncnt_ = cnt_[:20,]\ntrace1 = go.Bar(\n                x = cnt_.index,\n                y = cnt_.values,\n                name = \"Number of tweets on Australia polls by state.\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets on Australia polls by state.')\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","d16307ab":"data = [go.Scattermapbox(\n            lat= twitter_data['lat'] ,\n            lon= twitter_data['long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'orange',\n                opacity = .8,\n            ),\n          )]\nlayout = go.Layout(\n    title = go.layout.Title(\n        text = 'Tweets on Australia polls by state'\n    ),\n    geo = go.layout.Geo(\n        scope = 'world',\n        projection = go.layout.geo.Projection(type = 'albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(255, 255, 255)'),\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'd3-cloropleth')","06d64012":"trace1 = go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'negative','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'negative','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'black',\n                opacity = .5,\n            ),\n          )\ntrace2= go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'neutral','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'neutral','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'blue',\n                opacity = .3,\n            ),\n          )\ntrace3= go.Scattermapbox(\n            lat= twitter_data.loc[twitter_data['polarity'] == 'positive','lat'] ,\n            lon= twitter_data.loc[twitter_data['polarity'] == 'positive','long'],\n            mode='markers',\n            marker=dict(\n                size= 4,\n                color = 'gold',\n                opacity = .2,\n            ),\n          )\n\n\ndata = [trace1,trace2,trace3]\nlayout = go.Layout(\n    title = go.layout.Title(\n        text = 'Tweets on Australia polls according to polarity by state '\n    ),\n    geo = go.layout.Geo(\n        scope = 'world',\n        projection = go.layout.geo.Projection(type = 'albers usa'),\n        showlakes = True,\n        lakecolor = 'rgb(200, 125, 255)'),\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'd3-cloropleth-ma')","717e7622":"vectorizer_ = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ntweets_vectorized = vectorizer_.fit_transform(twitter_data['full_text'])","9f8e8aad":"lda_ = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\ntweets_lda = lda_.fit_transform(tweets_vectorized)","39831ea3":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","83388fff":"print(\"Tweets LDA Model:\")\nselected_topics(lda_, vectorizer_)","923d9044":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_, tweets_vectorized, vectorizer_, mds='tsne')\ndash","834edaec":"## Treemap of number of accounts created by days of the week.","ed8f1d7f":"## Checking for nulls","b6166844":"Most members tweeted around 10-11AM.","090f966f":"## Plotting number of accounts created per hour","e6598fec":"## Checking the head of the dataframes","32c1c1c0":"## Plotting the number of accounts created by days of the week.","8d6b69d4":"**Thanks for reading this.Upvote if you like the kernel. I welcome suggestions to improve this kernel further.**","ab5c2643":"## Importing libraries","0624e2bd":"#### Table of contents:\n1. [Importing libraries](#import)\n2. [Imporing dataframes](#data)\n3. [Checking the head and shape of the dataframes](#heads)\n4. [Merging the two dataframes](#merge)\n5. [Checking for null values](#nulls)\n6. [Getting the oldest and newest members in the dataframe](#old)\n7. [Plotting number of tweets according to dates](#no)\n8. [Plotting dates on which most accounts are created](#most)\n9. [Getting the name of the day in the week](#week)<br>\n      a. [Plotting the number of tweets by days of the week](#plot)<br>\n      b. [Treemap of number of tweets by days of the week](#treemap1)<br>\n      c. [Plotting the number of accounts created by days of the wee.](#plot2)<br>\n      d. [Treemap of number of accounts created by days of the week](#treemap2)<br>\n10. [Extracting hours from the date columns](#hour)<br>\n      a. [Plotting number of tweets per hour](#hours1)<br>\n      b. [Plotting number of accounts created per hour](#hours2)<br>\n11. [Checking most retweeted and favourite tweet](#re)\n12. [Wordclouds](#cloud)<br>\n      a. [Wordcloud of user screen name](#cloud1)<br>\n      b. [Wordcloud of user description](#cloud2)<br>\n13. [Getting the sentiment of the tweets](#sentiment)\n14. [Checking different tweets with different ploarities](#printing)\n15. [Histogram plot of sentiment](#histse)\n16. [Radar chart of polarities](#radar)\n17. [Extracting features from tweets](#ext)<br>\n      a. [Pairplot of text based features](#pair)\n18. [Ngrams Visualisations of tweets](#n)\n19. [Plotting number of tweets by state(Top 20)](#tweetsbystate)\n20. [Visualizing number of tweets by state](#state)\n21. [Tweets on Australia polls according to polarity by state](#bysate)\n22. [Topic modeling](#model)<br>\n       a. [Count Vectorizers for the data](#count)<br>\n       b. [Applying Latent Dirichlet Allocation models](#lda)<br>\n       c. [Visualizing LDA results of tweets with pyLDAvis](#py)<br>\n    ","fd58bfa2":"## Importing the dataframes","642e144f":"## Checking the shapes of the dataframes","65bece1f":"### Plotting the number of tweets by days of the week.","c35a7fce":"**user description has null values in it.**","3517d7c6":"The most retweeted and favorite tweet was same and was tweeted by Sara A. Carter.","ed44c293":"## Treemap of number of tweets by days of the week","2f1c112e":"## Printing keywords","c4d43642":"## Applying Latent Dirichlet Allocation models","06e6f8de":"## Number of tweets according to Dates","d94fd3c3":"## Word cloud of user description","67ee2af5":"## Extracting features from tweets\n1. count of sentences\n2. count of words\n3. count of unique words\n4. count of letters\n5. count of punctuations\n6. count of uppercase words\/letters\n7. count of stop words\n8. Avg length of each word","6baceea9":"## Tweets on Australia polls according to polarity by state ","2734f934":"## Mapping sentiment to polarities(positive,neutral,negative)","c8ea9bec":"There are 11153 rows and 3 columns","d209c62b":"## Checking most retweeted and favourite tweet","775b6d39":"Most of the accounts created at 2AM.","54b47ffd":"## Histogram plot of sentiment","c964217a":"## Checking the head of the dataframe","392c5c25":"The more tweets was on 2019-05-18.","b33ef013":"## Dates on which the most accounts was created (Top10)","20142930":"## Visualizing LDA results of tweets with pyLDAvis","1b994c26":"## Visualizing number of tweets by state","a22ac326":"There are 183379 rows and 11 features ","fe0070d2":"# Exploratory Data Analysis of tweets on Australian Election","cb833dd0":"## Topic modeling\n**Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.**","d73e8c18":"## Getting the sentiment of the tweets","7bb55df2":"## Merging the two dataframes on user_location","e4bbc0c3":"We can see most of the tweets was on saturdays and sundays.","8fa3ce20":"## Checking different tweets with different ploarities","71bdaac4":"We can see the most accounts was created on 2011-09-14.","84f2bc3b":"### Getting the minimum and maximum dates in dataframe","f66fa1b0":"## Extracting hours from the date columns","b4db486d":"most of the tweets are with zero polarity.","4d328cd3":"## Radar chart of polarities","e50b118d":"### Getting the oldest and newest twitter member in the dataframe","04dd1462":"## Pairplot of text based features","fb1eb0f1":"## Count Vectorizers for the data","f5298146":"### Getting the oldest member name in the data.","97e218e9":"## Plotting number of tweets by state(Top 20)","b7692a68":"## Plotting number of tweets per hour","70052850":"### Getting the newest member name in the data","b80f96fe":"## Ngrams Visualisations of tweets","7c85a7fc":"## Wordcloud of user screen name","c332e1c3":"Only 10 days data was available.","8c126494":"### Getting the name of the day from the dates.","087df322":"The dataset comprises of over 180,000 tweets on australian elections between the time 10.05.2019 and 20.05.2019.The dataset columns are \n* created_at: Date and time of tweet creation\n* id: Unique ID of the tweet\n* full_text: Full tweet text\n* retweet_count: Number of retweets\n* favorite_count: Number of likes\n* user_id: User ID of tweet creator\n* user_name: Username of tweet creator\n* user_screen_name: Screen name of tweet creator\n* user_description: Description on tweet creator's profile\n* user_location: Location given on tweet creator's profile\n* user_location: Location given on tweet creator's profile <br>\n\nAnd loacation_geocode.csv contains latitude and longitude of the user."}}