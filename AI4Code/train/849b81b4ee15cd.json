{"cell_type":{"71d5f2c3":"code","5b1edf3b":"code","9bf80b4c":"code","fcb72623":"code","01fa4551":"code","8cdececc":"code","d469cd72":"code","6283c838":"code","d3cab181":"code","fc0b9927":"code","f0288293":"code","b3a6d958":"code","e0ce3b00":"markdown","4501c694":"markdown","e84eda70":"markdown","a28268fc":"markdown","2cbe0b16":"markdown","7b72d7c6":"markdown","4f2c1286":"markdown","ae5504eb":"markdown","dc2df063":"markdown","85af26c2":"markdown","6255ae3a":"markdown","18b8da2b":"markdown","8689e902":"markdown","3626cc09":"markdown","0b9c0d1f":"markdown"},"source":{"71d5f2c3":"import os\nimport math as m\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read train \/ test data.\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\n","5b1edf3b":"from sklearn.preprocessing import LabelEncoder\nimport dateutil.easter as easter\nimport datetime\nimport itertools\nimport operator\n\n# A - Add year\/month\/day column\ntrain['date'] = pd.to_datetime(train['date'], format='%Y-%m-%d')\ntest['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n\ntrain['year'] = train['date'].apply(lambda x: x.year)\ntrain['month'] = train['date'].apply(lambda x: x.month)\ntrain['day'] = train['date'].apply(lambda x: x.day)\n\ntest['year'] = test['date'].apply(lambda x: x.year)\ntest['month'] = test['date'].apply(lambda x: x.month)\ntest['day'] = test['date'].apply(lambda x: x.day)\n\n# B - Add weekday column\ntrain['weekday'] = train['date'].apply(lambda x: x.weekday())\ntest['weekday'] = test['date'].apply(lambda x: x.weekday())\nweekday_label = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# C - Add GDP per capita column\nGDP_PC = pd.read_csv('..\/input\/gdp-per-capita-finland-norway-sweden-201519\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')\nGDP_PC_dict = GDP_PC.unstack().to_dict()\ntrain['GDP_PC'] = train.set_index(['country', 'year']).index.map(GDP_PC_dict)\ntest['GDP_PC'] = test.set_index(['country', 'year']).index.map(GDP_PC_dict)\n\n# D - Add snow depth column\nsnow = pd.read_csv('..\/input\/finland-norway-and-sweden-weather-data-20152019\/nordics_weather.csv')\nsnow['date'] = pd.to_datetime(snow['date'], format='%m\/%d\/%Y')\nsnow['year'] = snow['date'].apply(lambda x: x.year)\nsnow['month'] = snow['date'].apply(lambda x: x.month)\nsnow['day'] = snow['date'].apply(lambda x: x.day)\nsnow = snow[['country', 'year', 'month', 'day', 'snow_depth']]\n\nsnow_map = snow.set_index(['country', 'year', 'month', 'day']).to_dict()['snow_depth']\n\ntrain['snow_depth'] = train.set_index(['country', 'year', 'month', 'day']).index.map(snow_map)\ntest['snow_depth'] = test.set_index(['country', 'year', 'month', 'day']).index.map(snow_map)\n\n# E - Holiday (Samuel Cortinhas)\ntrain['holiday'] = 0\nholiday = pd.read_csv('..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519\/holidays.csv')\nholiday['date'] = pd.to_datetime(holiday['date'], format='%Y-%m-%d')\n\n## Add Divine Mercy Sunday\nholiday_dms = holiday[holiday['event']=='Easter Sunday']\nholiday_dms['event'] = 'Divine Mercy Sunday'\nholiday_dms['date'] = holiday_dms['date'] + datetime.timedelta(days=7)\nholiday = pd.concat([holiday, holiday_dms], axis=0)\n\n## add year, month, day\nholiday['year'] = holiday['date'].apply(lambda x: x.year)\nholiday['month'] = holiday['date'].apply(lambda x: x.month)\nholiday['day'] = holiday['date'].apply(lambda x: x.day)\n\nfor df in [train, test]:\n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n# F - Fourier (Samuel Cortinhas)\nfor df in [train, test]:\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 2):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * m.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * m.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n# drop temporary one hot encoding\ntrain = train.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)\ntest = test.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)","9bf80b4c":"series = train.groupby(['year', 'date']).num_sold.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year].index, series[year].values)\n    fig.show()\n    ax.set_title(f'Sales trend in {year}', fontsize=8)\n    ax.set_xlabel('Date', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","fcb72623":"series = train.groupby(['year', 'product', 'month']).num_sold.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year]['Kaggle Hat'].index,\n            series[year]['Kaggle Hat'].values,\n            label='Kaggle Hat')\n    ax.plot(series[year]['Kaggle Mug'].index,\n            series[year]['Kaggle Mug'].values,\n            label='Kaggle Mug')\n    ax.plot(series[year]['Kaggle Sticker'].index,\n            series[year]['Kaggle Sticker'].values,\n            label='Kaggle Sticker')\n    ax.set_title(f'Sales trend in {year}', fontsize=8)\n    ax.set_xlabel('Month', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","01fa4551":"plt.plot(GDP_PC.index, GDP_PC['Finland'].values, label='Finland')\nplt.plot(GDP_PC.index, GDP_PC['Norway'].values, label='Norway')\nplt.plot(GDP_PC.index, GDP_PC['Sweden'].values, label='Sweden')\nplt.xticks([2015, 2016, 2017, 2018, 2019])\nplt.xlabel('year')\nplt.ylabel('GDP per capita')\nplt.legend()\nplt.show()","8cdececc":"series = snow.groupby(['year', 'country', 'month']).snow_depth.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year]['Norway'].index, series[year]['Norway'].values, label='Norway')\n    ax.plot(series[year]['Sweden'].index, series[year]['Sweden'].values, label='Sweden')\n    ax.plot(series[year]['Finland'].index, series[year]['Finland'].values, label='Finland')\n    # ax.set_title(f'Sales trend in {year}', fontsize=8, loc='right')\n    ax.set_xlabel('Day', fontsize=8)\n    ax.set_ylabel('Snow depth', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","d469cd72":"for year in [2015, 2016, 2017, 2018]:\n    month = 4\n    print(fin_holiday[(fin_holiday['year']==year) & (fin_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    print('----')\n    print(swe_holiday[(swe_holiday['year']==year) & (swe_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    print('----')\n    print(nor_holiday[(nor_holiday['year']==year) & (nor_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    \n    series = train.groupby(['year', 'country', 'month', 'date']).num_sold.sum()\n    \n    fig = plt.figure(figsize=(20, 5))\n    for i, country in enumerate(['Finland', 'Sweden', 'Norway']):\n        ax = fig.add_subplot(1, 3, i+1)\n        ax.plot(series[year][country][month].index, series[year][country][month].values)\n        fig.show()\n        ax.set_title(f'Sales trend in {country}', fontsize=12)\n        ax.set_xlabel('Date', fontsize=8)\n        ax.set_ylabel('Sales', fontsize=8)\n        plt.xticks(fontsize=6)\n        plt.yticks(fontsize=8)\n        plt.legend(fontsize=8)\n    plt.show()","6283c838":"train_test_concat = pd.concat([train, test])\n\nfor c in ['country', 'store', 'product']:\n    # Determine how to transform the data based on the training data.\n    le = LabelEncoder()\n\n    # Transform\n    train_test_concat[c] = le.fit_transform(train_test_concat[c].fillna('NA'))\n\ntrain = train_test_concat[train_test_concat['row_id'].isin(train['row_id'])]\ntest = train_test_concat[train_test_concat['row_id'].isin(test['row_id'])]\n\n# Delete unuse columns.\n# Separate training data and target variables.\nresponse_label = 'num_sold'\ntrain_y = train[response_label]\ndrop_label = ['row_id', 'date', response_label]\ntrain_x = train.drop([response_label, *drop_label], axis=1)\ntest_x = test.drop([*drop_label], axis=1)","d3cab181":"import lightgbm as lgb\n# import optuna.integration.lightgbm as lgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nfrom rgf.sklearn import RGFRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\n# Define sMAPE func.\ndef smape(true, preds):\n    return 1\/len(true) * np.sum(2 * np.abs(preds-true) \/ (np.abs(true) + np.abs(preds)) * 100)\n\n\nlgbm_params = {\n            'task': 'train',\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'force_col_wise': True,\n            'metric': 'mape'\n}\n\nkeras_params = {'epochs': 400,\n                'batch_size': 512}\n\net_params = {'n_estimators': 100,\n             'max_features': 0.5,\n             'max_depth': 18,\n             'min_samples_leaf': 4,\n             'n_jobs': -1}\n\nrf_params = {'n_estimators': 125,\n             'max_features': 0.2,\n             'max_depth': 25,\n             'min_samples_leaf': 4,\n             'n_jobs': -1}\n\nrgf_params = {'algorithm': 'RGF_Sib',\n              'loss': 'Log'}\n\n\ndef build_fn():\n    model_nn = Sequential()\n    model_nn.add(Dense(64, input_dim=train_x.shape[1]))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Activation('selu'))\n    model_nn.add(Dense(128))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Dropout(0.2))\n    model_nn.add(Activation('selu'))\n    model_nn.add(Dense(64, activation='selu'))\n    model_nn.add(Dense(1))\n\n    model_nn.compile(loss='mean_absolute_percentage_error',\n                     optimizer='adam',\n                     metrics=['MeanAbsolutePercentageError'])\n\n    return model_nn\n\n\nkeras_reg = KerasRegressor(build_fn=build_fn, verbose=0, **keras_params)\nkeras_reg._estimator_type = 'regressor'\n\nestimators = [\n    ('lgb', lgb.LGBMRegressor(**lgbm_params)),\n    ('nn', keras_reg),\n    ('rgf', RGFRegressor(**rgf_params)),\n    ('et', ExtraTreesRegressor(**et_params)),\n    ('rf', RandomForestRegressor(**rf_params)),\n    # ('lr', LinearRegression()),\n    ('knn', KNeighborsRegressor())\n]\n\nmodel_stack = StackingRegressor(estimators=estimators,\n                                final_estimator=LinearRegression())\nmodel_stack.fit(train_x, train_y)\npred = model_stack.predict(test_x)\n\n# Cross validation.\n# Split train data into 4 folds.\n#score_folds = []\n#kf = KFold(n_splits=4, shuffle=True, random_state=71)\n#for tr_idx, va_idx in kf.split(train_x):\n#    # Separate training data and validation data.\n#    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n#    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n#\n#    # Convert feature values and objective variables to lightgbm data structure.\n#    lgb_train = lgb.Dataset(tr_x, tr_y)\n#    lgb_eval = lgb.Dataset(va_x, va_y)\n#\n#    #  Hyper param setting.\n#    params = {\n#        'task': 'train',\n#        'boosting_type': 'gbdt',\n#        'objective': 'regression',\n#        'force_col_wise': True,\n#        'metric': 'mape'\n#    }\n#    \n#    num_round = 8000\n#\n#    # Train\n#    model = lgb.train(params,\n#                      train_set=lgb_train,\n#                      num_boost_round=num_round,\n#                      early_stopping_rounds=1000,\n#                      verbose_eval=100,\n#                      valid_sets=lgb_eval)\n#\n#    # Score confirmation with validation data.\n#    va_pred = model.predict(va_x)\n#    mape = smape(va_y, va_pred)\n#    score_folds.append(mape)\n#    best_params_folds.append(model.params)\n#\n#print(score_folds)\n#pred = model.predict(test_x)\ntest['num_sold'] = pred\n\nplt.plot(train['date'], train_y)\nplt.plot(test['date'], pred)\nplt.show()","fc0b9927":"# Make submission file\nsubmission = pd.DataFrame({'row_id': test['row_id'],\n                           'num_sold': pred.astype('int')})\nsubmission.to_csv('submission.csv', index=False)","f0288293":"series = train.groupby(['year', 'date']).num_sold.sum()\nseries_test = test.groupby(['date']).num_sold.sum()\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year].index, series[year].values, label='train')\n    ax.plot(series[year].index, series_test.values, label='test')\n    fig.show()\n    ax.set_title(f'Sales trend in {year}', fontsize=12)\n    ax.set_xlabel('Date', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=4)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()","b3a6d958":"series = train.groupby(['year', 'country', 'product', 'date']).num_sold.sum()\nseries_test = test.groupby(['country', 'product', 'date']).num_sold.sum()\nfig = plt.figure(figsize=(30, 45))\ncountry_label = {0: 'Finland', 1: 'Norway', 2: 'Sweden'}\nproduct_label = {0: 'Kaggle Mug', 1: 'Kaggle Hat', 2: 'Kaggle Speaker'}\nfor k, year in enumerate([2015, 2017, 2018]):\n    for j, country in enumerate([0, 1, 2]):\n        for i, product in enumerate([0, 1, 2]):\n            ax = fig.add_subplot(9, 3, i+1+j*3+k*9)\n            ax.plot(series[year][country][product].index, series[year][country][product].values, label='train')\n            ax.plot(series[year][country][product].index, series_test[country][product].values, label='test')\n            ax.set_title(f'Sales trend in {product_label[product]}, {country_label[country]}', fontsize=12)\n            ax.set_xlabel('Date', fontsize=8)\n            ax.set_ylabel('Sales', fontsize=8)\n            plt.xticks(fontsize=10)\n            plt.yticks(fontsize=8)\n            plt.legend(fontsize=8)\nplt.show()","e0ce3b00":"- Cyclical movements are influenced by the day of the week.\n- Sales peak at the end of December.\n- There is also a peak in April.\n\n**need to add weekday column**\n\n## Total sales by products.","4501c694":"# Visualization\n## Total sales by date","e84eda70":"- Weekly trend looks good. They have enough feature.\n- Annual offset trend might be related in GDP.\n\n**I don't have any idea to be added external dataset..**","a28268fc":"# Feature Engineering","2cbe0b16":"- I think prediction data has enough feature of previous year.\n- But, I feel like it's missing the peak of April.\n- Covid-19 spread from 2019, but we may need to dig a little deeper into the impact of this.\n- For example, the number of infected people and monthly GDP.","7b72d7c6":"# Modeling","4f2c1286":"- GDP must be related to sales, and data in 2019 is important to predict.\n- I think the impact of Covid-19 will also show up in GDP, but I don't know with annual data through 2019.\n\n## Snow depth","ae5504eb":"# Output","dc2df063":"- **Divine Mercy Sunday** is as effective as Easter sunday.\n\n**We need to add Divine Mercy Sunday column**","85af26c2":"- Prediction data has enough feature of previous years.\n- But, end-of-year sales has some amount of fluctuation each year.\n\n**Can we add some feature that related to end-of-year sales?**","6255ae3a":"# Load Data","18b8da2b":"# Re-Visualization\n## Comparison of each year sales and predicted sales","8689e902":"# Pre-processing","3626cc09":"- This feature might be related to Hat sales, but, really?\n\n## Holiday in April","0b9c0d1f":"- Only Hat has peak in April.\n\n**Might need to add some kind of feature, but I'm not sure...**\n\n-> is it effected Divine Mercy Sunday?\n\n## GDP per capita"}}