{"cell_type":{"548cd2a1":"code","dbf48f49":"code","339d0518":"code","743e3159":"code","3513e713":"code","444d00d2":"code","3c24e4aa":"code","e58884ba":"code","9f4d4f03":"code","2d19159b":"code","8af8498d":"code","4aa19c64":"code","3361087d":"code","81ed36e7":"code","96a97e32":"code","c96b1c89":"code","c9abdb43":"code","887a6963":"code","9f8725f1":"code","e9f3cb2e":"code","a91c17d4":"code","241d467f":"code","ee8a324d":"code","ddd4b1c7":"code","23ddbd3a":"code","9c06edf5":"code","d39c1e20":"code","60b5f4e8":"code","f51d4ee8":"code","8d08600d":"code","afc91876":"code","4ffede8e":"code","debe72b7":"code","a5537dc5":"code","ba8b0610":"code","4d129497":"code","19b55820":"code","395506b0":"code","1854a7c6":"code","a99571aa":"code","11775d20":"code","6aaacf92":"code","3fb2de80":"code","e4b0f898":"code","da95a2a8":"code","a3bf0ab0":"code","f3427d6d":"code","4b08db9d":"code","529c5a1e":"code","5736a940":"code","553f6365":"code","aca6ea3f":"code","26816153":"code","e92a57f3":"code","2ccc0ab2":"code","ba66e89b":"code","37946070":"code","4285b783":"code","aee53641":"code","e8e10a2d":"code","44024a38":"code","229d79d5":"code","4976ce63":"code","c7830c42":"code","4d72d807":"code","2fda4d6d":"code","3f24a233":"code","b52f5109":"code","c6625e71":"code","3be503ac":"code","42ec861f":"code","2f7801bc":"code","8f40898f":"code","0fc39a67":"code","bb2150fd":"code","a11985d1":"code","28e37eea":"code","885410c1":"code","a0240ca5":"code","a5b643d2":"code","a1ec85f9":"code","6d9fa65d":"code","4b9f849f":"code","e6504eb0":"code","505e52bd":"markdown","adf0904f":"markdown","8b32a864":"markdown","32c5d377":"markdown","4979da3f":"markdown","059e2afb":"markdown","627987ed":"markdown","96747ed9":"markdown","3526e6eb":"markdown","757e9e5c":"markdown","7dc05212":"markdown","6d40057a":"markdown","c7d01802":"markdown"},"source":{"548cd2a1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport scikitplot as skplt\n\nfrom keras import callbacks\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 10)\n\nplt.rc('figure', figsize=(10, 7))\n\nnum_epoch = 5","dbf48f49":"data = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')\ndata.drop(columns=\"Unnamed: 0\", axis=1, inplace=True)\ndata","339d0518":"department_list = data['Department Name'].dropna().unique()\ndepartment_list = [x.lower() for x in department_list]\ndepartment_list","743e3159":"class_list = data['Class Name'].dropna().unique()\nclass_list = [x.lower() for x in class_list]\nclass_list","3513e713":"department_and_class = np.concatenate((department_list, class_list, ['dress', 'petite', 'petit', 'skirt', 'shirt', 'jacket', 'intimate', 'blouse', 'coat', 'sweater']), axis=0)\ndepartment_and_class","444d00d2":"review_data = data[['Review Text','Recommended IND']]\nreview_data","3c24e4aa":"review_data.isnull().sum().sort_values()","e58884ba":"review_data.dropna(axis=0,inplace=True)","9f4d4f03":"review_data","2d19159b":"#import for test train split and vect\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf(data):\n    tfidf_vectorizer =TfidfVectorizer(min_df=3,  max_features=None, \n             analyzer='word', use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer","8af8498d":"from sklearn.decomposition import  TruncatedSVD\nimport matplotlib\nimport matplotlib.patches as mpatches\n\n\ndef plot_LSA(test_data, test_labels):\n        #reduce into 2 dimensions using svd \n        lsa = TruncatedSVD(n_components=2)\n        #fits to the train data\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue','blue']\n        if plt:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            red_patch = mpatches.Patch(color='orange', label='Recommended IND = 0')\n            blue_patch = mpatches.Patch(color='blue', label='Recommended IND = 1')\n            plt.legend(handles=[red_patch, blue_patch], prop={'size': 12})","4aa19c64":"X = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)\n\n# \u9650\u5236\u6700\u957f\u957f\u5ea6\u4e3a70\uff0c\u8fc7\u957f\u622a\u65ad\uff0c\u8fc7\u77ed\u5c31\u5728\u540e\u65b9\uff08post\uff09\u8865\u9f50\nmax_length = 70\n\nsequences = tokenizer.texts_to_sequences(X)\nfeatures = pad_sequences(sequences, maxlen=max_length, padding='post')","3361087d":"from string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n# if you don't have stopwords and have some error, please use the download code bollow!\n# nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n### Text Normalizing function. Part of the following function was taken from this link. \ndef clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    text = [w for w in text if not w in stop_words]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n#     text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","81ed36e7":"review_data['Review Text'] = review_data['Review Text'].map(lambda x: clean_text(x))","96a97e32":"review_data","c96b1c89":"from keras.utils import to_categorical\n\nX = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)","c9abdb43":"sequences = tokenizer.texts_to_sequences(review_data['Review Text'])\nnp.max([len(x) for x in sequences])","887a6963":"# \u9650\u5236\u6700\u957f\u957f\u5ea6\u4e3a70\uff0c\u8fc7\u957f\u622a\u65ad\uff0c\u8fc7\u77ed\u5c31\u5728\u540e\u65b9\uff08post\uff09\u8865\u9f50\nmax_length = 70\npadded_features = pad_sequences(sequences, maxlen=max_length, padding='post')","9f8725f1":"plot_LSA(padded_features, y)\nplt.show()","e9f3cb2e":"from scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc(n_classes, y_test, y_score, title, class_name_list):\n    # Plot linewidth.\n    lw = 2\n\n    y_test = sentiment_test[1]\n    y_score = test_score\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure(1)\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='ROC curve of class {0} (area = {1:0.2f})'\n                 ''.format(class_name_list[i], roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","a91c17d4":"def train_test_split(features, labels, **kwargs):\n    \n    # concatenate the features and labels array\n    dataset = np.c_[features, labels]\n\n    # shuffle the dataset\n    np.random.shuffle(dataset)\n\n    # split the dataset into features, labels\n    features, labels = dataset[:, 0:max_length], dataset[:, max_length:]\n\n    # get the split size for training dataset\n    split_index = int(kwargs['train_size'] * len(features))\n\n    # split the dataset into training\/validation dataset\n    train_features, validation_features = features[:split_index], features[split_index:]\n    train_labels, validation_labels = labels[:split_index], labels[split_index:]\n\n    # get the split size for validation dataset\n    split_index = int(kwargs['validation_size'] * len(validation_features))\n\n    # split the validation dataset into validation\/testing dataset\n    validation_features, test_features = validation_features[:split_index], validation_features[split_index:]\n    validation_labels, test_labels = validation_labels[:split_index], validation_labels[split_index:]\n\n    # return the partitioned dataset\n    return [train_features, train_labels], [validation_features, validation_labels], [test_features, test_labels]","241d467f":"labels = np.array(review_data['Recommended IND'], np.int)\nlabels = to_categorical(labels)\ntrain_dataset, validation_dataset, test_dataset = train_test_split(features=padded_features, labels=labels,\n                                                                   train_size=0.80, validation_size=0.50)","ee8a324d":"print('Dataset size : {}'.format(padded_features.shape))\nprint('Train dataset size : {}'.format(train_dataset[0].shape))\nprint('Validation dataset size : {}'.format(validation_dataset[0].shape))\nprint('Test dataset size : {}'.format(test_dataset[0].shape))","ddd4b1c7":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(train_dataset[0], train_dataset[1], epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(validation_dataset[0], validation_dataset[1]))\n\nscore = model.evaluate(test_dataset[0], test_dataset[1], verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","23ddbd3a":"test_score = model.predict(test_dataset[0])\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_dataset[1], axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_dataset[1], axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_dataset[1], axis=1), test_predictions, average='micro')  ","9c06edf5":"skplt.metrics.plot_roc(np.argmax(test_dataset[1], axis=1), model.predict_proba(test_dataset[0]),\n                      title='ROC Curves - two layers Bi-LSTM') ","d39c1e20":"from sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(review_data['Review Text'], review_data['Recommended IND'], test_size=0.2, random_state=666)\nX_test, X_val, y_test, y_val = model_selection.train_test_split(X_val, y_val, test_size=0.5, random_state=888)","60b5f4e8":"print(len(X_train))\nprint(len(X_val))\nprint(len(X_test))","f51d4ee8":"train_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_features = pad_sequences(train_sequences, maxlen=max_length, padding='post')","8d08600d":"# \u9996\u5148\u662f\u7b80\u5355\u7684RandomUnderSample\uff0c\u5c31\u662f\u51cf\u5c11\u6b63\u7c7b\u6837\u672c\u7684\u6570\u91cf\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n\nfig = plt.figure(figsize=(10,7)) \n\n\nrus = RandomUnderSampler()\n\nprint(train_features.shape, y_train.shape)\ntrain_X_rus, train_y_rus = rus.fit_sample(train_features, y_train)\nprint(train_X_rus.shape, train_y_rus.shape)\n\nprint('Random under-sampling')\n\nplot_LSA(train_X_rus, train_y_rus)\nplt.show()","afc91876":"train_labels = to_categorical(train_y_rus)\ntrain_labels[0]\nprint(train_labels.shape)","4ffede8e":"val_sequences = tokenizer.texts_to_sequences(X_val)\nval_features = pad_sequences(val_sequences, maxlen=max_length, padding='post')\nval_labels = to_categorical(y_val)\nprint(val_features.shape, val_labels.shape)","debe72b7":"test_sequences = tokenizer.texts_to_sequences(X_test)\ntest_features = pad_sequences(test_sequences, maxlen=max_length, padding='post')\ntest_labels = to_categorical(y_test)\nprint(test_features.shape, test_labels.shape)","a5537dc5":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(train_X_rus, train_labels, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels))","ba8b0610":"score = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","4d129497":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro') ","19b55820":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - RandomUnderSampler') ","395506b0":"from imblearn.over_sampling import RandomOverSampler\nfig = plt.figure(figsize=(10,7)) \n\nros = RandomOverSampler()\ntrain_X_ros, train_y_ros = ros.fit_sample(train_features, y_train)\n\n\nprint('Random over-sampling')\n\n\nplot_LSA(train_X_ros, train_y_ros)\nplt.show()","1854a7c6":"train_labels = to_categorical(train_y_ros)\ntrain_labels[0]\nprint(train_labels.shape)","a99571aa":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(train_X_ros, train_labels, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels))","11775d20":"score = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","6aaacf92":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro') ","3fb2de80":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - RandomOverSampler') ","e4b0f898":"from imblearn.over_sampling import BorderlineSMOTE \n\nbds = BorderlineSMOTE(random_state=66)\ntrain_X_bds, train_y_bds = bds.fit_sample(train_features, y_train)\n\nprint('BorderlineSMOTE')\n\n\nplot_LSA(train_X_bds, train_y_bds)\nplt.show()","da95a2a8":"train_labels = to_categorical(train_y_bds)\ntrain_labels[0]\nprint(train_labels.shape)","a3bf0ab0":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(train_X_bds, train_labels, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels))","f3427d6d":"score = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","4b08db9d":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro') ","529c5a1e":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - RandomOverSampler') ","5736a940":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nclass_weight = {0: 10, 1: 1}\n\nmodel.fit(train_dataset[0], train_dataset[1], epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(validation_dataset[0], validation_dataset[1]), class_weight=class_weight)\n\nscore = model.evaluate(test_dataset[0], test_dataset[1], verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","553f6365":"test_score = model.predict(test_dataset[0])\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_dataset[1], axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_dataset[1], axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_dataset[1], axis=1), test_predictions, average='micro')  \n# \u53ef\u4ee5\u770b\u51fa\u53ea\u662f\u6539\u4e86\u6743\u91cd\uff0c\u8d1f\u6837\u672c\u7684recall\u4f1a\u660e\u663e\u589e\u9ad8\uff0c\u4f46\u662f\u5bf9\u5e94\u8d1f\u6837\u672c\u7684pre\u4e5f\u4e0b\u964d\u4e86\u3002\u3002\u6b63\u7c7b\u6837\u672cpre\u9ad8\u4e86\u4e00\u70b9\u70b9\uff0c\u4f46\u662frecall\u4e0b\u964d\u3002\u3002\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u626f\u4e00\u4e0b\u8fd9\u4e2a\u5728\u8bc4\u4ef7\u7b5b\u9009\u91cc\u9762\u5f88\u6709\u7528\n# \u56e0\u4e3a\u5f80\u5f80\u7528\u6237\u548c\u5546\u5bb6\u6bd4\u8f83\u5173\u5fc3\u5dee\u8bc4","aca6ea3f":"skplt.metrics.plot_roc(np.argmax(test_dataset[1], axis=1), model.predict_proba(test_dataset[0]),\n                      title='ROC Curves - two layers Bi-LSTM') ","26816153":"print(train_features.shape)\nprint(y_train.shape)","e92a57f3":"y_train.value_counts()","2ccc0ab2":"X_over, X_cat, y_over, y_cat = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=888)\nX_over, X_under, y_over, y_under = model_selection.train_test_split(X_over, y_over, test_size=0.5, random_state=888)","ba66e89b":"print(X_under.shape, X_over.shape, y_under.shape, y_over.shape, X_cat.shape, y_cat.shape)","37946070":"ros_sequences = tokenizer.texts_to_sequences(X_over)\nros_features = pad_sequences(ros_sequences, maxlen=max_length, padding='post')\ntrain_X_ros, train_y_ros = ros.fit_sample(ros_features, y_over)","4285b783":"print(train_X_ros.shape, train_y_ros.shape)\ntrain_y_ros.value_counts()","aee53641":"rus_sequences = tokenizer.texts_to_sequences(X_under)\nrus_features = pad_sequences(rus_sequences, maxlen=max_length, padding='post')\ntrain_X_rus, train_y_rus = rus.fit_sample(rus_features, y_under)","e8e10a2d":"print(train_X_rus.shape, train_y_rus.shape)\ntrain_y_rus.value_counts()","44024a38":"print(X_cat.shape, y_cat.shape)\ny_cat.value_counts()","229d79d5":"y_cat.value_counts()","4976ce63":"cat_0_idx = y_cat[y_cat == 0]\ncat_0_idx = list(cat_0_idx.keys())\ncat_1_idx = y_cat[y_cat == 1]\ncat_1_idx = list(cat_1_idx.keys())","c7830c42":"X_cat_0 = X_cat[cat_0_idx]\nX_cat_1 = X_cat[cat_1_idx]","4d72d807":"count_0 = len(cat_0_idx)","2fda4d6d":"import random\nnew_X_0 = []\nfor idx in cat_0_idx:\n    cur = X_cat_0[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) \/\/ 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_0.append(cur)\nnew_X_0.extend(list(X_cat_0.values))\n\nnew_X_1 = []\nfor idx in cat_1_idx:\n    cur = X_cat_1[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) \/\/ 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_1.append(cur)\nnew_X_1 = random.sample(new_X_1, count_0)\nnew_X_1.extend(random.sample(list(X_cat_1.values), count_0))","3f24a233":"print(len(new_X_0))\nprint(len(new_X_1))","b52f5109":"len(new_X_0 + new_X_1)","c6625e71":"X_cat = pd.Series(new_X_0 + new_X_1)\ny_cat = pd.Series([0] * count_0 * 2 + [1] * count_0 * 2)","3be503ac":"cat_sequences = tokenizer.texts_to_sequences(X_cat)\ncat_features = pad_sequences(cat_sequences, maxlen=max_length, padding='post')","42ec861f":"cat_features.shape","2f7801bc":"cat_features","8f40898f":"train_X_rus","0fc39a67":"train_X_ros","bb2150fd":"features_all = np.concatenate((cat_features, train_X_rus, train_X_ros))","a11985d1":"features_all.shape","28e37eea":"y_all = y_cat.append(train_y_rus).append(train_y_ros)","885410c1":"labels_all = to_categorical(y_all)\nlabels_all[0]\nprint(labels_all.shape)","a0240ca5":"model = Sequential()\n\nclass_weight = {0: 10, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), class_weight=class_weight, shuffle=True)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","a5b643d2":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro') \n\n# test_score = model.predict(test_features)\n# test_predictions = np.argmax(test_score, axis=1)\n\n# class_names = ['(0) Not recommended class', '(1) Recommended class']\n# report = classification_report(np.argmax(test_features, axis=1), test_predictions, target_names=class_names)\n# matrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n#                                         index=class_names, columns=class_names)\n# print(matrix)\n# print(report)\n# f1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')  ","a1ec85f9":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","6d9fa65d":"model = Sequential()\n\nclass_weight = {0: 10, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=20, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), class_weight=class_weight, shuffle=True)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","4b9f849f":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')","e6504eb0":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper - 20 epoches') ","505e52bd":"\u5f31\u667a\u7b97\u6cd51\uff1a0.4undersampling\uff0c0.4oversamplling\uff0c0.2\u662f\u53d6\u968f\u673a\u53d6\u53e5\u5b50\u7684\u524d\u534a\u53e5\u6216\u8005\u540e\u534a\u53e5\u751f\u6210\u65b0\u7684\u6570\u636e\u3002\u8fd9\u6837\u7684\u7406\u7531\u662f\uff0c\u6709\u90e8\u5206\u7528\u6237\u4f1a\u5728\u8bc4\u8bba\u4e00\u5f00\u59cb\u6216\u8005\u6700\u540e\u9762\u5f3a\u70c8\u8868\u8fbe\u81ea\u5df1\u7684\u60c5\u611f\u3002\u6240\u4ee5\u4f7f\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u505a\u6570\u636e\u589e\u5f3a\u6709\u5229\u4e8e\u5206\u7c7b\u3002","adf0904f":"# Review Text Feature Transformation","8b32a864":"# Basic Visualization","32c5d377":"\u8fd9\u91cc\u662f\u5148\u8c03\u5305\u5c1d\u8bd5\u51e0\u79cd\u5e38\u7528nlp\u7684resample\u65b9\u5f0f\uff0c\u6765\u770b\u770b\u6548\u679c","4979da3f":"# Recommended IND Classification","059e2afb":"\u867d\u7136\u597d\u8bc4\u7684recall\u548c\u5dee\u8bc4\u7684prec\u90fd\u4e0b\u964d\u4e86\uff0c\u597d\u8bc4\u7684prec\u548c\u5dee\u8bc4\u7684recall\u90fd\u5f88\u9ad8\uff0c\u4f46\u662f\u6211\u89c9\u5f97\u662f\u5408\u7406\u7684\uff1a\u5982\u679c\u8981\u505a\u8bc4\u8bba\u7cbe\u9009\uff0c\u6bd4\u5982\u628a\u597d\u8bc4\u653e\u5728\u524d\u9762\uff0c\u90a3\u4e48\u597d\u8bc4prec\u9ad8\u662f\u5408\u7406\u7684\uff0c\u8bf4\u660e\u7ed9\u7528\u6237\u770b\u7684\u8bc4\u8bba\u57fa\u672c\u90fd\u662f\u597d\u8bc4\uff1b\u5982\u679c\u5e97\u5bb6\u60f3\u770b\u8206\u60c5\u5206\u6790\uff0c\u5c31\u662f\u60f3\u770b\u81ea\u5df1\u5e97\u94fa\u7684\u5dee\u8bc4\uff0c\u90a3\u4e48\u5dee\u8bc4recall\u9ad8\u662f\u5408\u7406\u7684\uff0c\u8bf4\u660e\u6b64\u65f6\u7ed9\u5e97\u5bb6\u770b\u7684\u57fa\u672c\u90fd\u662f\u5dee\u8bc4\u3002\n\u540e\u671f\u4e0d\u77e5\u9053\u591a\u8bad\u7ec3\u591a\u51e0\u6b21\uff0c\u8fd8\u6709\u8c03\u6574\u4e09\u79cdresample\u65b9\u5f0f\u7684\u6bd4\u91cd\uff0c\u4e0d\u77e5\u9053\u6548\u679c\u5982\u4f55\u3002","627987ed":"Tokenizer\u662f\u4e00\u4e2a\u7528\u4e8e\u5411\u91cf\u5316\u6587\u672c\uff0c\u6216\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5e8f\u5217\uff08\u5373\u5355\u8bcd\u5728\u5b57\u5178\u4e2d\u7684\u4e0b\u6807\u6784\u6210\u7684\u5217\u8868\uff0c\u4ece1\u7b97\u8d77\uff09\u7684\u7c7b\u3002\n\nword_index: \u5b57\u5178\uff0c\u5c06\u5355\u8bcd\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5c04\u4e3a\u5b83\u4eec\u7684\u6392\u540d\u6216\u8005\u7d22\u5f15\u3002\u4ec5\u5728\u8c03\u7528fit_on_texts\u4e4b\u540e\u8bbe\u7f6e\u3002\n\ntexts_to_sequences(texts)\n\ntexts\uff1a\u5f85\u8f6c\u4e3a\u5e8f\u5217\u7684\u6587\u672c\u5217\u8868\n\n\u8fd4\u56de\u503c\uff1a\u5e8f\u5217\u7684\u5217\u8868\uff0c\u5217\u8868\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u5bf9\u5e94\u4e8e\u4e00\u6bb5\u8f93\u5165\u6587\u672c\n\npad_sequences \u5c06\u591a\u4e2a\u5e8f\u5217\u622a\u65ad\u6216\u8865\u9f50\u4e3a\u76f8\u540c\u957f\u5ea6\u3002\n\n\u8be5\u51fd\u6570\u5c06\u4e00\u4e2a num_samples \u7684\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4e00\u4e2a 2D Numpy \u77e9\u9635\uff0c\u5176\u5c3a\u5bf8\u4e3a (num_samples, num_timesteps)\u3002 num_timesteps \u8981\u4e48\u662f\u7ed9\u5b9a\u7684 maxlen \u53c2\u6570\uff0c\u8981\u4e48\u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6\u3002\n\n\u6bd4 num_timesteps \u77ed\u7684\u5e8f\u5217\u5c06\u5728\u672b\u7aef\u4ee5 value \u503c\u8865\u9f50\u3002\n\n\u6bd4 num_timesteps \u957f\u7684\u5e8f\u5217\u5c06\u4f1a\u88ab\u622a\u65ad\u4ee5\u6ee1\u8db3\u6240\u9700\u8981\u7684\u957f\u5ea6\u3002\u8865\u9f50\u6216\u622a\u65ad\u53d1\u751f\u7684\u4f4d\u7f6e\u5206\u522b\u7531\u53c2\u6570 pading \u548c truncating \u51b3\u5b9a\u3002\n\n\u5411\u524d\u8865\u9f50\u4e3a\u9ed8\u8ba4\u64cd\u4f5c\u3002","96747ed9":"Bidirectional(LSTM(256), merge_mode='sum')\n\nmerge_mode:\nmerge_mode: \u524d\u5411\u548c\u540e\u5411 RNN \u7684\u8f93\u51fa\u7684\u7ed3\u5408\u6a21\u5f0f\u3002 \u4e3a {'sum', 'mul', 'concat', 'ave', None} \u5176\u4e2d\u4e4b\u4e00\u3002 \u5982\u679c\u662f None\uff0c\u8f93\u51fa\u4e0d\u4f1a\u88ab\u7ed3\u5408\uff0c\u800c\u662f\u4f5c\u4e3a\u4e00\u4e2a\u5217\u8868\u88ab\u8fd4\u56de\u3002\n\nIt also allows you to specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer. The options are:\n\n\u2018sum\u2018: The outputs are added together.\n\n\u2018mul\u2018: The outputs are multiplied together.\n\n\u2018concat\u2018: The outputs are concatenated together (the default), providing double the number of outputs to the next layer.\n\n\u2018ave\u2018: The average of the outputs is taken.\n\nThe default mode is to concatenate, and this is the method often used in studies of bidirectional LSTMs","3526e6eb":"# Data Structure","757e9e5c":"# Reference\n1. https:\/\/medium.com\/@sabber\/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n2. https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer\n3. https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/","7dc05212":"\u63a5\u4e0b\u6765\u662f\u4ec0\u4e48resample\u90fd\u4e0d\u505a\u4f46\u662f\u6539\u4e86class weight\u7684baseline","6d40057a":"\u5176\u5b9e\u4e0b\u9762\u8fd9\u4e2aBorderlineSMOTE\u5e94\u8be5\u8868\u73b0\u4f1a\u5f88\u597d\uff0c\u4f46\u662f\u8fd9\u91cc\u5f88\u5dee\uff0c\u662f\u56e0\u4e3a\u5b83\u8981\u57fa\u4e8e\u7279\u5f81\u5316\u4e4b\u540e\uff08\u6bd4\u5982tfidf\uff0c\u6df1\u5ea6\u5b66\u4e60\u7684embedding\u7b49\u4e4b\u540e\uff09\u624d\u505a\u7684\uff0c\u8fd9\u91cc\u76f4\u63a5\u7528\u4e86\u8bcd\u8868\u7684index\uff0c\u5426\u5219\u76f4\u63a5\u9884\u8bad\u7ec3\u4e00\u4e2aembedding\u6bd4\u8f83\u9ebb\u70e6\uff0c\u6216\u8005\u5957\u7528\u4e00\u4e9bembedding\u7684\u6a21\u578b\u6bd4\u5982glove\uff0c\u592a\u5927\u4e86\uff0c\u8981\u5f88\u4e45\u3002\u6240\u4ee5\u76f4\u63a5\u7528\u8bcd\u8868\u76f8\u5f53\u4e8e\u662f\u7528\u8bcd\u7684index\u505a\u7279\u5f81\uff0c\u6548\u679c\u4f1a\u5dee\u3002\u770b\u770b\u6709\u6ca1\u6709\u65f6\u95f4\u6539\uff0c\u6ca1\u65f6\u95f4\u6539\u7684\u8bdd\uff0c\u4e0d\u7528\u8fd9\u4e2a\u5e93\u7684\u4f8b\u5b50\uff0c\u76f4\u63a5\u5220\u6389","c7d01802":"\u7559\u6548\u679c\u6700\u597d\u7684\u6ca1\u505aresample\u7684\u4f5c\u5bf9\u6bd4\u7684baseline\uff0c\u4e0d\u641e\u90a3\u4e48\u591a\u4e86"}}