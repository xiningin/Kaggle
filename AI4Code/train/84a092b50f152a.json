{"cell_type":{"2342c781":"code","520cda3d":"code","db826f73":"code","9d330ab1":"code","47ff125b":"code","0ded3d32":"code","b2733a50":"code","c2dcb234":"code","99dfd455":"code","3cbd7f88":"code","bbeffe60":"code","8e341a12":"code","f1fe5480":"code","46223b0a":"code","0e319da4":"code","09f97cfc":"code","da3a0921":"code","1675a65c":"code","ebe34e3f":"code","2b8e4e80":"code","b4e1febb":"code","0eafb469":"code","1ecd76b7":"code","be988662":"code","f2dbe27f":"code","3fc226fc":"code","ba581340":"code","34dc0aa1":"code","38eb329c":"code","1d92fa11":"markdown","5bc6f9e5":"markdown","6ecc3840":"markdown","f53ce31c":"markdown","20a1be0b":"markdown","5b8ea2c0":"markdown","919ab853":"markdown","5e4fb9e9":"markdown","78defc21":"markdown","f360133a":"markdown","20b24f38":"markdown","6ea1194b":"markdown","6e8dad03":"markdown"},"source":{"2342c781":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","520cda3d":"from nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize","db826f73":"stop_words = stopwords.words('english')\ndf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',encoding=\"latin_1\")\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',encoding=\"latin_1\")\ndata1=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","9d330ab1":"texts = df['text']\ntexts_test = df_test['text']\ny = df.pop('target')","47ff125b":"def pre_process_data(text):\n    lemm = WordNetLemmatizer()\n    text  = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n    tokenized = word_tokenize(text)\n    text = [lemm.lemmatize(i.lower()) for i in tokenized if not(i.lower() in stop_words) and i.isalpha()]\n    text = [i.replace('http','') for i in text]\n    text = [i.replace('co','') for i in text]\n    text = [i.replace('amp','') for i in text]\n    return ' '.join(text)","0ded3d32":"import re\ntexts_test = [pre_process_data(i) for i in texts_test]\ntexts = [pre_process_data(i) for i in texts]","b2733a50":"labels   = data1['target'].values.tolist()","c2dcb234":"import keras_preprocessing\nfrom keras_preprocessing.text import one_hot\nfrom keras_preprocessing.sequence import pad_sequences","99dfd455":"# integer encode the documents\nvocab_size = 10000\nencoded_docs = [one_hot(d, vocab_size) for d in texts ]\n","3cbd7f88":"max_length = 1000\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","bbeffe60":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential","8e341a12":"from tensorflow.keras import layers","f1fe5480":"# define the model\nmodel = tf.keras.Sequential()\nmodel.add(layers.Embedding(vocab_size, 10, input_length=max_length))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1, activation='sigmoid'))","46223b0a":"#fitting with ndarray\nlabels=np.array(labels) \n#data=np.array(labels,dtype=float)","0e319da4":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nmodel.summary()\n# fit the model\nbatch_size = 32\nepochs = 50\nhistory = model.fit(padded_docs, labels,batch_size=batch_size,epochs=epochs)\nhistory","09f97cfc":"plt.figure(0)\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['loss'], label='Value Loss')\nplt.title('Training accuracy & Value Loss')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","da3a0921":"model.save(\"Natural Language Processing with Disaster Tweets.h5\")","1675a65c":"#from keras.models import load_model\n#model = load_model(.\/Natural Language Processing with Disaster Tweets.h5')\nmodel = tf.keras.models.load_model(\".\/Natural Language Processing with Disaster Tweets.h5\")","ebe34e3f":"texts_test","2b8e4e80":"# integer encode the documents\nvocab_size = 10000\nencoded_docs_test = [one_hot(d, vocab_size) for d in texts_test ]","b4e1febb":"max_length = 1000\npadded_docs = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')","0eafb469":"padded_docs","1ecd76b7":"pred = model.predict_classes(padded_docs)","be988662":"pred.shape","f2dbe27f":"sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nmy_submission = pd.DataFrame()\nmy_submission['id'] = sample['id']\nmy_submission['target'] = pred\n\nmy_submission.to_csv('Submission.csv', index=False)\n","3fc226fc":"from wordcloud import WordCloud, ImageColorGenerator\nfrom os import path, getcwd\nfrom PIL import Image","ba581340":"data=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","34dc0aa1":"#Creating the text variable for positve reviews\nneg=data.loc[data['target']==0].reset_index(drop=True)\nneg.head()\n\n# Adding Text to a Variable\ntext=neg['text'][5]\n# Creating the Word Cloud\nwordcloud = WordCloud().generate(text)\n# Plotting the Word Cloud\nplt.figure(figsize = (20,20))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","38eb329c":"#Creating the text variable for positve reviews\npos=data.loc[data['target']==1].reset_index(drop=True)\npos.head()\n\n# Adding Text to a Variable\ntext=pos['text'][5]\n# Creating the Word Cloud\nwordcloud = WordCloud().generate(text)\n# Plotting the Word Cloud\nplt.figure(figsize = (20,20))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","1d92fa11":"# Load model","5bc6f9e5":"# Data Preprocesing","6ecc3840":"# Compile the model","f53ce31c":"# My submission","20a1be0b":"# Plotting Graphs for accuracy","5b8ea2c0":"# Import packages","919ab853":"# Define the model","5e4fb9e9":"# keras embeddings technique","78defc21":"# Importing WordCloud","f360133a":"# Import packages","20b24f38":"# Testinng Data","6ea1194b":"# Loading data","6e8dad03":"# Saveing model"}}