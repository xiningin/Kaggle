{"cell_type":{"a74646e5":"code","8c7e0467":"code","14fb64cc":"code","a4b687f4":"code","a4299ad1":"code","a62d17d0":"code","0795f00b":"code","568c6619":"code","b5505a43":"code","4912270e":"code","aadcc64e":"code","d624a22a":"code","388aaf39":"code","05e2e458":"code","fc1d93c1":"code","a1de827e":"code","a8bcc673":"code","cdf31280":"code","6778a7c1":"code","c87c523e":"code","163488dd":"code","e19bdb8d":"code","2e852140":"code","59055874":"code","f45f9a99":"code","10ac6243":"code","0d57e8bf":"markdown","bd4445cf":"markdown","5df53808":"markdown","8767612c":"markdown","c1fdeafa":"markdown","50129c77":"markdown","d1709a70":"markdown","7a10180d":"markdown","9d473813":"markdown","e990a4c3":"markdown","25c3ba56":"markdown"},"source":{"a74646e5":"%%capture\n!pip install --upgrade wandb","8c7e0467":"import wandb\nfrom wandb.keras import WandbCallback\nwandb.login()","14fb64cc":"wandb.init(entity=\"authors\", project=\"kaggle_license\")","a4b687f4":"%%capture\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nimport os\nimport cv2","a4299ad1":"# Check the version of tf\nprint(tf.__version__)","a62d17d0":"df = pd.read_json(\"..\/input\/vehicle-number-plate-detection\/Indian_Number_plates.json\", lines=True)\ndf.head()","0795f00b":"df.columns","568c6619":"%%capture\n# Make a directory where the downloaded cars are kept\n%mkdir Cars\n\ndataset = dict()\ndataset[\"image_name\"] = list()\ndataset[\"top_x\"] = list()\ndataset[\"top_y\"] = list()\ndataset[\"bottom_x\"] = list()\ndataset[\"bottom_y\"] = list()\n\ncounter = 0\nfor index, row in df.iterrows():\n    path = tf.keras.utils.get_file('\/kaggle\/working\/Cars\/car{}.jpg'.format(counter),\n                            row[\"content\"])\n    \n    dataset[\"image_name\"].append(path)\n    \n    data_points = row[\"annotation\"]\n    \n    dataset[\"top_x\"].append(data_points[0][\"points\"][0][\"x\"])\n    dataset[\"top_y\"].append(data_points[0][\"points\"][0][\"y\"])\n    dataset[\"bottom_x\"].append(data_points[0][\"points\"][1][\"x\"])\n    dataset[\"bottom_y\"].append(data_points[0][\"points\"][1][\"y\"])\n    \n    counter += 1","b5505a43":"df_store = pd.DataFrame(dataset)\ndf_store.head()","4912270e":"def load_img(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_image(img, channels=3, dtype=tf.float32)\n    img = tf.image.resize(img, (224,224))\n    if len(img.shape) == 4:\n        img = tf.squeeze(img,0)\n    img = tf.expand_dims(img, axis=0)\n    return img","aadcc64e":"# For traininig\nimgs = []\nlabels = []\nfor index, row in df_store[:int(0.9*len(df_store))].iterrows():\n    imgs.append(load_img(row['image_name']))\n    labels.append(tf.constant([[row['top_x'],row['top_y'],row['bottom_x'],row['bottom_y']]]))\ntrain_images = tf.concat(imgs,axis=0)\ntrain_labels = tf.concat(labels,axis=0)\n\n# For testing\nimgs = []\nlabels = []\nfor index, row in df_store[int(0.9*len(df_store)):].iterrows():\n    imgs.append(load_img(row['image_name']))\n    labels.append(tf.constant([[row['top_x'],row['top_y'],row['bottom_x'],row['bottom_y']]]))\ntest_images = tf.concat(imgs,axis=0)\ntest_labels = tf.concat(labels,axis=0)","d624a22a":"print('train_images: {}'.format(train_images.shape))\nprint('train_labels: {}'.format(train_labels.shape))\nprint('test_images: {}'.format(test_images.shape))\nprint('test_labels: {}'.format(test_labels.shape))","388aaf39":"def show_img_bbox(img, label):\n    img = img.numpy()\n    y_hat = label.numpy()*224\n    xt, yt = int(y_hat[0]), int(y_hat[1])\n    xb, yb = int(y_hat[2]), int(y_hat[3])\n    image = cv2.rectangle(img, (xt, yt), (xb, yb), (0, 0, 255), 3)\n    plt.imshow(image)\n    plt.show()\n\n# Use the function\nshow_img_bbox(train_images[13], train_labels[13])","05e2e458":"train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=64).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(buffer_size=64).batch(32)","fc1d93c1":"for i,l in train_ds.take(1):\n    show_img_bbox(i[0], l[0])\n    show_img_bbox(i[2], l[2])","a1de827e":"tf.keras.backend.clear_session()\n\ni = tf.keras.layers.Input(shape=(224, 224, 3))\nx = tf.keras.layers.Conv2D(64, (5,5), activation='relu')(i)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(128, (5,5), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(256, (7,7), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(512, (7,7), activation='relu')(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(256,activation='relu')(x)\nx = tf.keras.layers.Dense(128,activation='relu')(x)\nx = tf.keras.layers.Dense(64,activation='relu')(x)\no = tf.keras.layers.Dense(4,activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=[i], outputs=[o])\n\nmodel.summary()","a8bcc673":"model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","cdf31280":"history = model.fit(train_ds,\n                    validation_data=test_ds,\n                    callbacks=[callback,\n                               WandbCallback()\n                              ],\n                    epochs=50)","6778a7c1":"model.evaluate(test_ds)","c87c523e":"for i,l in test_ds.take(1):\n    pred = tf.constant(model.predict(i))\n    for j in range(10):\n        show_img_bbox(i[j], pred[j])\n        show_img_bbox(i[j], l[j])","163488dd":"def wandb_img(temp_img, temp_pred, temp_label):\n    img = wandb.Image(\n        temp_img,\n        boxes={\n            \"predictions\": {\n                \"box_data\": [{\n                    \"position\": {\n                        \"minX\": float(temp_pred[0]),\n                        \"maxX\": float(temp_pred[2]),\n                        \"minY\": float(temp_pred[1]),\n                        \"maxY\": float(temp_pred[3]),\n                    },\n                    \"class_id\": 1,\n                },]\n            },\n            \"ground_truth\": {\n                \"box_data\": [{\n                    \"position\": {\n                        \"minX\": float(temp_label[0]),\n                        \"maxX\": float(temp_label[2]),\n                        \"minY\": float(temp_label[1]),\n                        \"maxY\": float(temp_label[3]),\n                    },\n                    \"class_id\": 1,\n                },]\n            }\n        }\n    )\n    return img","e19bdb8d":"for i,l in test_ds.take(1):\n    b_box = []\n    pred = tf.constant(model.predict(i))\n    for j in range(20):\n        temp_img = i[j].numpy()\n        temp_label = l[j].numpy()\n        temp_pred = pred[j].numpy()\n        \n        img = wandb_img(temp_img, temp_pred, temp_label)\n        b_box.append(img)\n    wandb.log({\"B_Box_change\":b_box})","2e852140":"df_store.head()","59055874":"def change_name(string):\n    name = string.split('\/')[-1]\n    name = '..\/input\/license\/Cars\/'+name\n    return name\ndf_store['image_name'] = df_store['image_name'].apply(change_name)\ndf_store.head()","f45f9a99":"df_store.to_csv('\/kaggle\/working\/license.csv')","10ac6243":"wandb.finish()","0d57e8bf":"## Image Utils\nThe load_img function is a util function that helps in reading, decoding and resing the image.","bd4445cf":"## Visualize the prediction","5df53808":"We use the `tf.keras.utils.get_file` method to download the images. We also create a dictionary called `dataset` that has the following keys:\n1. image_name: The path where the image is downloaded; str\n2. top_x: The % value of the topx bbox point; float\n3. top_y: The % value of the topy bbox point; float\n4. bottom_x: The % value of the bottomx bbox point; float\n5. bottom_y: The % value of the bottomy bbox point; float","8767612c":"# Object Detection\nIn this kernel, we will dive into the problem of object detection. Here we will provide images of cars and try to put a bounding box around the license plates. This is a very simple task, as there is only one kind of object (license plate). The idea is to use a CNN and then project the flattened layer to 4 output values. These 4 values will be the co-ordinates of the license plates.\n\nThis version of the code will have [wandb](https:\/\/wandb.ai) integration along with it. This is a tool for ML and DL developers to quiclky log their necessary metrics and to reproduce and convey results.\n\n## Imports\nThe following packages are used:\n1. numpy\n2. pandas\n3. tensorflow\n4. matplotlib\n5. os\n6. cv2\n7. wandb","c1fdeafa":"The columns of the dataframe are:\n1. content: The urls to the vehicle images\n2. annotation: The license plate co-ordinates\n3. extras: NaN","50129c77":"A util function to show the bbox and the image.","d1709a70":"## Store the dataframe","7a10180d":"Let's visually check the datasets formation","9d473813":"## Model\nI have tried keeping this as simple as possible with leraning from scratch","e990a4c3":"This is the code which is used to load the images in memory. The training images and testing images are transformed in tensors and then are used to build `tf.data.Dataset`","25c3ba56":"## Data\nThe data is provided in the form of a json. The important fields in the json are `content` and the `annotation`. The `content` field consists of the urls to the images. The `annotation` field consists of all the metadata of the image along with the co-ordinates of the bounding boxes."}}