{"cell_type":{"5e84dfe2":"code","ccc30d6e":"code","3d9b669c":"code","d7bb95cf":"code","e8df9e1a":"code","1b3bd86f":"code","0a8e6f51":"code","2c68d224":"code","bd9d953a":"code","d9e2568d":"code","f06dd0e7":"code","7a519b78":"code","f8ccac6f":"code","72a826bd":"code","161e293a":"markdown","0857a711":"markdown","28c90849":"markdown","4ea75ab3":"markdown","e358b023":"markdown","8739b05d":"markdown","2b5bfe3f":"markdown","5a05e114":"markdown","625dfd06":"markdown","9dc3e20b":"markdown","b5a53336":"markdown","7174ac33":"markdown","018a8627":"markdown","1f31586b":"markdown","5c8d7a3a":"markdown"},"source":{"5e84dfe2":"%%capture\n!apt-get install libav-tools -y\n\n%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport torchvision.models as models\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\n\nsns.set(style='whitegrid')\nplt.style.use('seaborn-darkgrid')\n#seaborn-ticks\nfrom fastai.callbacks import *\nfrom sklearn.metrics import roc_curve, auc\nfrom fastai.vision import *\nfrom glob import glob","ccc30d6e":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\nprint('Train Size: ', df_train.shape)\nprint('Test Size: ', df_test.shape)","3d9b669c":"Class = df_train.Class.value_counts()\ncolours = [\"#aaaaaa\", \"#aaaaaa\", \"#aaaaaa\",\"#aaaaaa\",\"#aaaaaa\",\"#d11111\",\"#aaaaaa\",\"#aaaaaa\",\"#aaaaaa\",\"#d11111\"]\nf, ax = plt.subplots(figsize=(18,5)) \nax = sns.countplot(x='Class', data=df_train, palette=colours)\nplt.title('Class Distribution');","d7bb95cf":"i = random.choice(df_train.index)\ny, sr = librosa.load('..\/input\/train\/Train\/' + str(df_train.ID[i]) + '.wav')\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y,sr);\nprint('Class: ', df_train.Class[i])\nprint('Sampling Rate: ',sr,'Hz')\nprint('Duration: ',len(y)\/sr)\nprint('Number of samples: ', len(y))\nipd.Audio(data=y, rate=sr)","e8df9e1a":"y, sr = librosa.load('..\/input\/train\/Train\/68.wav')\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y,sr);\nplt.title('Dog_bark')\nipd.Audio(data=y, rate=sr)","1b3bd86f":"plt.figure(figsize=(12,5))\nnoise = np.random.randn(len(y))\ndata_noise = y + 0.05 * noise\nlibrosa.display.waveplot(data_noise,sr);\nplt.title('Dog_bark with background noise')\nipd.Audio(data=data_noise, rate=sr)","0a8e6f51":"plt.figure(figsize=(12,5))\ny_fast = librosa.effects.time_stretch(y, 0.5)\nlibrosa.display.waveplot(y_fast,sr);\nplt.title('Dog_bark')\nipd.Audio(data=y_fast, rate=sr)","2c68d224":"plt.figure(figsize=(12,5))\ny_third = librosa.effects.pitch_shift(y, sr, n_steps=20)\nlibrosa.display.waveplot(y_third,sr)\nplt.title('Dog_bark')\nipd.Audio(data=y_third, rate=sr)","bd9d953a":"# Loading audio\ndef load_sound_files(parent_dir, file_paths):\n    raw_sounds = []\n    for fp in file_paths:\n        X,sr = librosa.load(parent_dir + str(fp))\n        raw_sounds.append(X)\n    return raw_sounds\n\n#plot waveplots\ndef plot_waves(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,12), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(2,5,i)\n        librosa.display.waveplot(np.array(f), sr)\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Waveplot',x=0.5, y=0.95,fontsize=18)\n    plt.show()\n\n#plot fourier transform\ndef plot_fft(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,12), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(2,5,i)\n        X = scipy.fft(f)\n        X_mag = np.absolute(X)\n        f = np.linspace(0, sr, len(X_mag))\n        plt.title(n.title())\n        plt.plot(f, X_mag)\n        plt.xlabel('Frequency')\n        i += 1\n    plt.suptitle('Fourier Transform',x=0.5, y=0.95,fontsize=18)\n    plt.show()\n    \n    \n#plot Mel-scale-power Spectrogram\ndef plot_mel_specgram(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,10), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(2,5,i)\n        S = librosa.feature.melspectrogram(y=f, sr=sr,n_fft=2048, hop_length=512)\n        librosa.display.specshow(librosa.power_to_db(S, ref=np.max), x_axis='time', y_axis='mel')\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Mel-scaled power spectrogram',x=0.5, y=0.95,fontsize=18)\n    plt.show()\n    \n#plot Mfccs\ndef plot_mfccs(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,10), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(2,5,i)\n        S = librosa.feature.mfcc(y=f, sr=sr, dct_type=2)\n        librosa.display.specshow(S)\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Mel frequency cepstral coefficients ',x=0.5, y=0.95,fontsize=18)\n    plt.show()\n    \n#plot Spectrogram Constrast    \ndef plot_chroma(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,10), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(2,5,i)\n        S = np.abs(librosa.stft(f))\n        chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n        librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Chromagram of a short time fourier transform ',x=0.5, y=0.95,fontsize=18)\n    plt.show()\n    \n#plot ROC_AUC\ndef cal_auc_and_plot(learn):\n    preds, y = learn.get_preds()\n    probs = np.exp(preds[:,1])\n    fpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.01])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    return roc_auc\n    \nsound_file_paths = ['77.wav','88.wav','6.wav','71.wav','2.wav','17.wav','59.wav','104.wav','3.wav','1.wav']\nsound_names = list(np.unique(df_train.Class))\nparent_dir = \"..\/input\/train\/Train\/\"\n\nraw_sounds = load_sound_files(parent_dir, sound_file_paths)\n","d9e2568d":"plot_waves(sound_names, raw_sounds)","f06dd0e7":"plot_fft(sound_names,raw_sounds)","7a519b78":"plot_mel_specgram(sound_names, raw_sounds)","f8ccac6f":"plot_mfccs(sound_names, raw_sounds)","72a826bd":"plot_chroma(sound_names, raw_sounds)","161e293a":"#   UrbanSound Classification \n![](http:\/\/www.100urban.nl\/media\/wysiwyg\/slide-5.jpg)\n\n\n## Description :\n>UrbanSound dataset contains 8,732 labelled sound clips (4 seconds each) from ten classes: air conditioner, car horn, children playing, dog bark, drilling, engine idling, gunshot, jackhammer, siren, and street music.\n\n## Class Name\n*  Air Conditon\n*  Car horn\n*  Children playing\n*  Dog bark\n*  Drilling\n*  Engine Idling\n*  Gunshot\n*  Jackhammer\n*  Siren\n*  Street Music\n","0857a711":"# Time Stretching\n","28c90849":"> ## **Check if we have a balance dataset**\n> We can see from the distribution of the classes that we have a balance dataset, with only two classes under-represented: car horn and gunshot","4ea75ab3":"# Mel Frequency Cepstral Coefficients\n* The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.\n","e358b023":"# Time Domain\n","8739b05d":"# **Feature Extraction**\n> Feature extraction is used to convert the acoustic signal into a sequence of acoustic feature vectors that carry a good representation of input speech signal. The idea is to reduce the ten thousands of data points in each file into a much smaller set of features of fixed number.\n\n* **Mel-scaled power spectrogram**- The Mel Scale is used to provide greater resolution for more informative ( lower) frequencies\n* **Mel-frequency cepstral coefficients (MFCCs)**- The coefficients that collectively make up the short-term power spectrum of a sound\n* **Chroma_stft** - Projects into bins representing the 12  distinct semitones (or chroma) of the musical octave\n","2b5bfe3f":"## There are 2 commony ways to represent sound:\n\n![](https:\/\/data.crazyengineers.com\/old-attachments\/1\/1452-time_vs_freq1.gif)\n     \n> *  **Time domain**:  each sample represents the variation in air pressure.\n> *  **Frequency domain:**  at each time stamp we indicate the amplitude for each frequency.","5a05e114":"# Chroma short time fourier transform\n* Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.","625dfd06":"# Fourier Transform\n* The Fourier transform is a mathematical technique that allows an MR signal to be decomposed into a sum of sine waves of different frequencies, phases, and amplitudes.\n![](http:\/\/mriquestions.com\/uploads\/3\/4\/5\/7\/34572113\/3311485_orig.gif)","9dc3e20b":"# Frequency Domain\n> It transforms our time-domain signal into the frequency domain\n\n> We will use Fourier\u2019s Transform to convert our audio data to the frequency domain. This allows for a much more simple and compact representation of the data.","b5a53336":"# Background Noise","7174ac33":"# **Loading an audio file**\n*  loads and decodes the audio as a time series y, represented as a one-dimensional numpy  array. \n* The variable sr contains the sampling rate of y, that is, the number of samples per second of audio. \n* By default, all audio is mixed to mono and resampled to 22050 Hz at load time. There 22050 values stored for every second of audio\n\n#### This means that 4 secs. audio contains 88,200 samples that's a lot of information, and we need to reduce this to more manageable level.","018a8627":"# Audio Data Augmentation\n> Data augmentation is generally used for machine learning and deep learning in order to achieve a good performance after training generating a large amount of data\n* **Background noise** - Mix the sample with another recording\n* **Time Stretching** - Slowdown or speedup the audio\n* **Pitch Shifting** - Lower the pitch of the audio","1f31586b":"# Full tutorial\n> https:\/\/github.com\/gabbygab1233\/UrbanSound-Cassification-Using-Fastai\n\n\n# Reference\n*  [music-genre-classification-with-python](https:\/\/towardsdatascience.com\/music-genre-classification-with-python-c714d032f0d8?fbclid=IwAR24KYm_kffHIus5VlAwquqJvF9FFk10YjGDYGVGvC2eGkeKUxjRZ7N80D0)\n\n* [audio-voice-processing-deep-learning](https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/audio-voice-processing-deep-learning\/)\n\n*  [MUSIC\/AUDIO ANALYSIS IN PYTHON](https:\/\/cdn.cs50.net\/2016\/fall\/seminars\/automated_analysis_music\/automated_analysis_music.pdf)\n\n*  [Deep learning architectures for music audio classification](http:\/\/www.jordipons.me\/media\/UPC-2018.pdf)\n\n*  [Classifying Environmental Sounds withImage Networks](https:\/\/pdfs.semanticscholar.org\/09c2\/f123e5f329f4143becad27075d7f8d8ee4d2.pdf)\n\n*  [Librosa: Audio and Music Signal Analysis in Python](hhttp:\/\/conference.scipy.org\/proceedings\/scipy2015\/pdfs\/brian_mcfee.pdf)\n\n*  [Ai which classifier sounds code](https:\/\/hackernoon.com\/ai-which-classifies-sounds-code-python-6a07a2043810)\n\n*  [How to teach Neural Networks to detect everyday sounds:](https:\/\/www.skcript.com\/svr\/building-audio-classifier-nueral-network\/)\n\n*  [Pytorch:](https:\/\/pytorch.org\/)\n\n* [Fastai:](https:\/\/docs.fast.ai\/)\n\n*  [transfer-learning-using-the-fastai-library](https:\/\/towardsdatascience.com\/transfer-learning-using-the-fastai-library-d686b238213e)\n\n*  [deep-learning-handwritten-arabic-digits](https:\/\/towardsdatascience.com\/deep-learning-handwritten-arabic-digits-5c7abc3c0580)\n\n*  [ten-techniques-from-fast-ai](https:\/\/blog.floydhub.com\/ten-techniques-from-fast-ai\/)\n\n*  [Sounds augmentation:](https:\/\/www.kaggle.com\/huseinzol05\/sound-augmentation-librosa)\n\n*  [Machine Learning for music:](https:\/\/www.slideshare.net\/PetkoNikolov\/machine-learning-for-music?qid=7e5ad741-4e48-4648-8bcc-f8ee919ff297&v=&b=&from_search=34)\n\n*  [Single-word speech recognition with Convolutional Neural Networks on raw\nwaveforms](https:\/\/www.theseus.fi\/bitstream\/handle\/10024\/144982\/Jansson_Patrick.pdf?sequence=1&isAllowed=y)\n\n","5c8d7a3a":"# Pitch Shifting"}}