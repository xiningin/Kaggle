{"cell_type":{"4cf43a4b":"code","4a49bd6a":"code","060fd47c":"code","6733255b":"code","25b95f78":"code","a7ad118f":"code","b0394404":"code","631055f3":"code","e3a66d33":"code","64ba7e9f":"code","ce4c713c":"code","9bfa4b53":"code","11889fb3":"code","464e1f9f":"code","7394017d":"code","5c95025b":"code","fd1bb43a":"code","fe0affda":"code","a5df4290":"code","b54e3d7f":"code","898a0d17":"code","54417a24":"code","548ce78b":"code","529e6644":"code","5f968efe":"code","969b3305":"code","c9fded11":"code","f7b97971":"code","fc5ff0fd":"markdown","aea89889":"markdown","f0dda6b8":"markdown","bf932798":"markdown","a14656c2":"markdown","d066c5cb":"markdown"},"source":{"4cf43a4b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4a49bd6a":"from numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer","060fd47c":"import string\nfrom nltk.corpus import stopwords","6733255b":"import sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split","25b95f78":"!pip install simpletransformers","a7ad118f":"#read the filea\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","b0394404":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', '\u00fc', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in STOPWORDS])\n\n#maybe use later (or delete)\ndef preprocess(col):\n    clean = col.apply(text_process)\n    hashtags = col.apply(lambda x: [word.lower() for word in x.split() if word.startswith(\"#\")])\n    return clean.to_list(), hashtags.to_list()","631055f3":"#preprocess the text in train and test\ntrain['clean'] = train['text'].apply(text_process)\ntest['clean'] = test['text'].apply(text_process)","e3a66d33":"train.head()","64ba7e9f":"from sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\n#arguments: current model, targets of the val dataset (y_val) and predictions for the val dataset (predictions)\ndef precision_recall(mod, y_val, predictions):\n    \n    print(mod)\n    \n    #create a list of thresholds for probabilities: [1, 0.9999, 0.9998, ..., 0.002, 0.001]\n    #approxim. thresholds between 1 and 0.001 with the step of -0.0001\n    thresholds = np.append(np.arange(1,0.9, -0.0001), np.arange(0.9,0, -0.001))\n    \n    #create lists to save precision and recall for each further iteration\n    precision_scores, recall_scores = list(), list()\n    \n    #iterate over generated thresholds\n    for threshold in tqdm(thresholds):\n        #create list of rounded predictions: if prediction>=current threshold, set it to 1, otherwise to 0\n        prob_preds = np.where(predictions>=threshold, 1, 0)\n        #print(threshold)\n        #print(\"prob_preds: \\n\",prob_preds)\n        #print(\"y_val: \\n\", y_val)\n        \n        #check precision and recall for the new rounded predictions\n        temp_classification_report = classification_report(y_true=y_val, y_pred=prob_preds, output_dict=True)['1']\n        \n        #save precision and recall for the new predictions to the previously created lists\n        precision = round(temp_classification_report['precision'], 3)\n        precision_scores.append(precision)                      \n        recall_scores.append(round(temp_classification_report['recall'], 3))\n\n    #res is list of tuples (precision, recall, threshold) from resulted lists, where precision>0    \n    res = [item for item in zip(precision_scores, recall_scores, list(thresholds)) if item[0] != 0]\n    print(\"Last 10 tuples: \", res[-10:])\n    \n    #res_precision1 is list of tuples (precision, recall, threshold) where precision==1\n    res_precision1 = [i for i in zip(precision_scores, recall_scores, list(thresholds)) if i[0]==1]\n    print(\"Last 10 tuples for precision=1: \", res_precision1[-10:])\n    \n    try:\n        #take the last element of res_precision1, which contains min threshold for precision 1\n        print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(res_precision1[-1][0], res_precision1[-1][1], res_precision1[-1][2]))\n    except:\n        #if res_precision1 is empty\n        print(\"Model doesn't reach precision of 1.00\")\n    try:\n        #res_precision095 is list of (precision, recall, threshold) tuples for precision between 0.95 and 1\n        res_precision095 = [item for item in res if 0.95 < item[0] < 1]\n        print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(res_precision095[-1][0], res_precision095[-1][1], res_precision095[-1][2]))\n        print(\"F1: {:.3f}\".format(2*res_precision095[-1][0]*res_precision095[-1][1]\/(res_precision095[-1][0] + res_precision095[-1][1])))\n    except:\n        print(\"Model doesn't reach precision of 0.95\")\n    try:\n        #for precision between 0.9 and 1\n        res_precision09 = [item for item in l if 0.90 < item[0] < 1]\n        print(\"Comp. questions: max Prec. {:.3f} with Rec. {:.3f} at thresh. {:.6f}\".format(res_precision09[-1][0], res_precision09[-1][1], res_precision09[-1][2]))\n        print(\"F1: {:.3f}\".format(2*res_precision09[-1][0]*res_precision09[-1][1]\/(res_precision09[-1][0] + res_precision09[-1][1])))\n    except:print(\"Model doesn't reach precision of 0.90\")","ce4c713c":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\nmodel = ClassificationModel('distilbert', 'distilbert-base-uncased-finetuned-sst-2-english')","9bfa4b53":"model.args","11889fb3":"args = {'learning_rate': 4e-05,\n               'overwrite_output_dir': True, \n               'num_train_epochs': 10} ","464e1f9f":"X_train, X_val, y_train, y_val = train_test_split(train['clean'], train['target'], test_size=0.2, random_state=42)\ntrain_data = pd.DataFrame({'clean':X_train, 'target':y_train})\nval_data = pd.DataFrame({'clean':X_val, 'target':y_val})\nprint(train_data.head())","7394017d":"model = ClassificationModel('distilbert', 'distilbert-base-uncased-finetuned-sst-2-english', args=args)\nmodel.train_model(train_data)\nresult, outputs, wrong_predictions = model.eval_model(val_data, acc=sklearn.metrics.accuracy_score)\nprint(result['acc'])","5c95025b":"predictions, raw_outputs = model.predict(X_val.to_list())","fd1bb43a":"result","fe0affda":"outputs","a5df4290":"precision_recall(\"DistilBERT\", y_val, predictions)","b54e3d7f":"from sklearn.linear_model import LogisticRegression\n\n\nmodel = LogisticRegression()\npreprocessor = CountVectorizer()\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# evaluate the model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nX_train_vec = preprocessor.fit_transform(X_train.to_list())\nn_scores = cross_val_score(model, X_train_vec, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# Preprocessing of validation data, get predictions\npredictions = my_pipeline.predict_proba(X_val.to_list())\ny_pred = my_pipeline.predict(X_val.to_list())","898a0d17":"predictions","54417a24":"pred_prob = list(predictions[:,1])\nprecision_recall(\"Logistic Regression\", y_val.to_list(), pred_prob)","548ce78b":"# gradient boosting for classification in scikit-learn\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nmodel = GradientBoostingClassifier()\npreprocessor = CountVectorizer()\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# evaluate the model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nX_train_vec = preprocessor.fit_transform(X_train.to_list())\nn_scores = cross_val_score(model, X_train_vec, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# Preprocessing of validation data, get predictions\npredictions = my_pipeline.predict_proba(X_val.to_list())\ny_pred = my_pipeline.predict(X_val.to_list())\npred_prob = list(predictions[:,1])","529e6644":"precision_recall(\"Light GBM\", y_val, pred_prob)","5f968efe":"def ensemble_preparation(mod, threshold):\n    if mod == 'logistic':\n        model = LogisticRegression()\n    else:\n        model = GradientBoostingClassifier()\n        \n    preprocessor = CountVectorizer()\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('model', model)\n                                 ])\n\n    # Preprocessing of training data, fit model \n    my_pipeline.fit(X_train, y_train)\n\n    # Get predictions for test set\n    test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n    X_test = test['text'].to_list()\n    predictions = my_pipeline.predict_proba(X_test)\n    pred = predictions[:,1]\n    \n    #define predictions for a model based on the chosen threshold\n    prob_preds = np.where(pred>=threshold, 1, 0)\n    return prob_preds","969b3305":"#pick threshold for logistic regression for precision 0.95\npreds_logistic = ensemble_preparation(\"logistic\", 0.802)\n\n#pick threshold for LightGBM for precision 0.95\npreds_lightGBM = ensemble_preparation(\"lightGBM\", 0.644)","c9fded11":"#redefine predictions based on thresholds for both models\npredictions = list()\nfor x, y in zip(preds_logistic, preds_lightGBM):\n    \n    #if both models agree on the prediction, pick this prediction\n    if x==y:\n        predictions.append(x)\n        \n    #if one of the models predicted 1, pick this predictions\n    elif x==1 or y==1:\n        predictions.append(1)","f7b97971":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","fc5ff0fd":"# Submit results","aea89889":"# Try LightGBM","f0dda6b8":"# Try Logistic Regression","bf932798":"# Try Distilbert","a14656c2":"# Create an ensemble based on selected thresholds for each model","d066c5cb":"# Function to find better thresholds for predictions"}}