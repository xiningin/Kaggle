{"cell_type":{"a28af229":"code","828fccdb":"code","0888c57a":"code","84b2a142":"code","17216e76":"code","01b35845":"code","2ac71432":"code","81bd00cd":"code","bfd506ab":"code","2cd25e56":"code","5d2738a0":"code","0cee70f6":"code","661ea637":"code","f7044e64":"code","a228d2ae":"code","c66a15ba":"code","54169c55":"code","6ad42b57":"code","493620e5":"code","f7789d6e":"code","69dfe2c1":"code","67b681aa":"code","c00fc2f8":"code","1a8d6e6c":"code","03aff48d":"code","fb3fe847":"code","bbd4df13":"code","37b07ea8":"code","2c961df3":"code","3cdc7865":"code","f4e245fd":"code","4bc155da":"code","3fcb3972":"code","4eb5ffe6":"code","581815a4":"code","3fe59756":"code","a3720fa0":"code","a4418011":"code","94d1c4dc":"code","dc825cd2":"markdown","9b3c9045":"markdown","673c5d42":"markdown","ff293197":"markdown","91a10fb3":"markdown","029a29d2":"markdown","f49e4578":"markdown","749ee2bc":"markdown","a48333c2":"markdown","bd090903":"markdown","687e9356":"markdown","7b5da2e7":"markdown","168c9823":"markdown","91fd1418":"markdown","7052c0ee":"markdown","a829df6d":"markdown","6ba4c10b":"markdown","6a9d2f0d":"markdown","e62da828":"markdown","bf93b056":"markdown","28a5f040":"markdown","b8ea1625":"markdown","0d7e6031":"markdown","208532d6":"markdown","cdbfd6de":"markdown","fa596553":"markdown"},"source":{"a28af229":"from IPython.display import Image\nimport os\nImage(\"..\/input\/spacyimage\/Introduction.PNG\")","828fccdb":"# Importing the packages\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport string\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0888c57a":"# Reading the dataset\ndf = pd.read_csv('..\/input\/nlpdata\/nlp_data.csv')\nprint(df.head(2))","84b2a142":"# https:\/\/cmdlinetips.com\/2018\/11\/how-to-split-a-text-column-in-pandas\/\n# https:\/\/www.geeksforgeeks.org\/python-pandas-split-strings-into-two-list-columns-using-str-split\/?ref=rp\nsentiment_data = pd.read_csv(\"..\/input\/nlpdata\/nlp_data.csv\", names = [\"comments_text\"])\nprint(sentiment_data.head(2))\n\n#sentiment_data['comments'] = sentiment_data.comments_text.str.split('\\t',expand=True,)\nsentiment_data = sentiment_data[\"comments_text\"].str.split(\" \", n = 1, expand = True) \nsentiment_data.head(2)\n","17216e76":"# Getting the head of the dataset\nsentiment_data.head(2)","01b35845":"# Shape of the dataset\nsentiment_data.shape","2ac71432":"sentiment_data.dtypes","81bd00cd":"sentiment_data.rename(columns={0:'label',1:'comments'}, inplace=True)","bfd506ab":"# Value counts for Target column\nprint('Label: 0-Negative Sentiment,1-Positive Sentiment\\n')\nprint(sentiment_data['label'].value_counts())\n","2cd25e56":"# checking for null values\nsentiment_data.isnull().sum()","5d2738a0":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\ndoc = nlp(sentiment_data['comments'][0])\ndisplacy.render(doc, style=\"dep\" , jupyter=True)","0cee70f6":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][0])\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.dep_)","661ea637":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][0])\n\n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.lemma_)","f7044e64":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][2])\n \nsentences = list(doc.sents)\nlen(sentences)","a228d2ae":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][2])\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)","c66a15ba":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][0])\nentities=[(i, i.label_, i.label) for i in doc.ents]\nprint('Entities')\nprint(entities)\nprint('\\nDocument at Index 0')\nprint(doc)\nprint('\\nEntities Captured')\ndisplacy.render(doc, style = \"ent\",jupyter = True)","54169c55":"# Create an nlp object\ndoc = nlp(sentiment_data['comments'][0])\n\nfor token in doc:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","6ad42b57":"doc = nlp(sentiment_data['comments'][0])\n\nfor token1 in doc:\n    for token2 in doc:\n        print(token1.text, token2.text, token1.similarity(token2))","493620e5":"from IPython.display import YouTubeVideo\nYouTubeVideo('ZIiho_JfJNw')","f7789d6e":"# Creating our list of punctuations\npunc = string.punctuation\n\n# Creating our list of stopwords\nnlp = spacy.load('en_core_web_sm')\nstopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n# Loading parser\nparse = English()\n\n# Creating the tokenizer function\ndef tokenize(token):\n    # Creating the token object where all the tokenization functions are applied starting \n    # with parsing\n    token_obj = parse(token)\n    \n    # Lemmatizing each token and converting it into lowercase\n    token_obj = [i.lemma_.lower().strip() if i.lemma_!='-PRON-' else i.lower_ for i in token_obj]\n    \n    # Removing stop words and punctuations\n    token_obj = [i for i in token_obj if i not in stopwords and i not in punc]\n    \n    # Returning the token\n    return token_obj\n","69dfe2c1":"# Creating custom transformer using spaCy\nclass transformer(TransformerMixin):\n    def transform(self,X,**transform_params):\n        \n        # Cleaning the text\n        return [clean(i) for i in X]\n    # Fitting the transformer\n    \n    def fit(self,X,y=None,**fit_params):\n        return self\n    \n    # Predicting the transformer\n    def get_params(self,deep=True):\n        return{}\n    \n# Basic clean function\ndef clean(i):\n    \n    # Removing the spaces and converting all the text into lowercase\n    return i.strip().lower()","67b681aa":"# Creating Count Vectorizer\ncount_vector = CountVectorizer(tokenizer=tokenize,ngram_range=(1,2))","c00fc2f8":"# Creating TF-IDF Vectorizer\ntfidf_vect= TfidfVectorizer( tokenizer=tokenize,use_idf=True, smooth_idf=True, sublinear_tf=False)","1a8d6e6c":"vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',stop_words='english')\nvect.fit(sentiment_data['comments'])\nidf = vect._tfidf.idf_\nwordDict=dict(zip(vect.get_feature_names(), idf))\nprint(wordDict)","03aff48d":"from IPython.display import YouTubeVideo\nYouTubeVideo('vEmm9fZJuuM')","fb3fe847":"word1={k: v for k, v in wordDict.items() if v < 5}\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(' '.join(word1.keys()))\nplt.title('Review Comments')\n# plot the WordCloud image                        \nplt.imshow(wordcloud); \n","bbd4df13":"word1={k: v for k, v in wordDict.items() if v > 5 and v < 10}\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(' '.join(word1.keys()))\nplt.title('Review Comments')\n# plot the WordCloud image                        \nplt.imshow(wordcloud); ","37b07ea8":"# Importing train_test split\nfrom sklearn.model_selection import train_test_split\n\n# Assigning X and Y values\n\nx = sentiment_data['comments']# the feature we want to analyze\ny = sentiment_data['label'] # the labels\n\n# Splitting the values into train and test\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2,test_size = 0.25)","2c961df3":"# Importing Random Forest Classifier and fitting the model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(class_weight='balanced')\n\n# Importing RandomizedSearchCV and assigning the parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'criterion':['entropy','gini'],'max_depth':range(1,15,2)}\n\n# Fitting the RandomizedSearchCV model\nrfc = RandomizedSearchCV(rf,params)","3cdc7865":"# Creating the pipeline\npipe_rf = Pipeline([('clean',transformer()),\n                 ('vectorizer',tfidf_vect),\n                 ('model',rfc)])\n\n# Model generation\npipe_rf.fit(x_train,y_train)","f4e245fd":"# Importing the metrics\nfrom sklearn import metrics\n\n# Predicting with test data\npred = pipe_rf.predict(x_test)","4bc155da":"# Model accuracy,precision and recall\nprint('Accuracy',metrics.accuracy_score(y_test,pred))\nprint('Precision',metrics.precision_score(y_test,pred,average=None))\nprint('Recall',metrics.recall_score(y_test,pred,average=None))\nprint('F1-Score',metrics.f1_score(y_test,pred,average=None))","3fcb3972":"# Confusion matrix\nconf_matrix_rf = metrics.confusion_matrix(y_test,pred)\nprint(conf_matrix_rf)","4eb5ffe6":"# Plotting the confusion matrix\ncfm= conf_matrix_rf\nlbl1=[\"Predicted Negative\", \"Predicted Positive\"]\nlbl2=[\"Actual Negative\", \"Actual Poistive\"]\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nax.set_ylim([0,2])\nplt.show()","581815a4":"# Importing Support Vector Classifier algorithm\nfrom sklearn.svm import SVC\nsvc = SVC(class_weight='balanced')\n\nparams1 = {'kernel':['linear','rbf','poly','sigmoid'],'C': [0.01, 0.1, 1,10],'gamma': [0.01,0.1,1,10]}\n\n# Fitting the RandomizedSearchCV model\nsvcc = RandomizedSearchCV(svc,params1)","3fe59756":"# Creating the pipeline\npipe = Pipeline([('clean',transformer()),\n                 ('vectorizer',tfidf_vect),\n                 ('model',svcc)])\n\n# Model generation\npipe.fit(x_train,y_train)","a3720fa0":"# Predicting with test data\npred = pipe.predict(x_test)\n\n# Model accuracy,precision and recall\nprint('Accuracy',metrics.accuracy_score(y_test,pred))\nprint('Precision',metrics.precision_score(y_test,pred,average=None))\nprint('Recall',metrics.recall_score(y_test,pred,average=None))\nprint('F1-Score',metrics.f1_score(y_test,pred,average=None))","a4418011":"# Confusion matrix\nconf_matrix = metrics.confusion_matrix(y_test,pred)\nprint(conf_matrix)","94d1c4dc":"# Plotting the confusion matrix\nconf= conf_matrix\nlbl1=[\"Predicted Negative\", \"Predicted Positive\"]\nlbl2=[\"Actual Negative\", \"Actual Poistive\"]\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(conf, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\nax.set_ylim([0,2])\nplt.show()","dc825cd2":"* [Table of Contents](#Tableofcontents)\n<a id=\"EvaluatingNLPPipeline\"><\/a>\n\n## Evaluating NLP pipeline","9b3c9045":"* [Table of Contents](#Tableofcontents)\n<a id=\"CreateNLPPipelineforRandomForestClassifier\"><\/a>\n\n## Creating NLP Pipeline for RandomForestClassifier","673c5d42":"* [Table of Contents](#Tableofcontents)\n<a id=\"3.Lemmatization\"><\/a>\n\n### 3.Lemmatization\n\nLemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a lemma.\n","ff293197":"* [Table of Contents](#Tableofcontents)\n<a id=\"References\"><\/a>\n\n## References\n\n### Special Thanks to PavanSanagapati and Farsana.\n\n* https:\/\/github.com\/Apress\/building-an-enterprise-chatbot\/blob\/master\/Chapter%205%20NLP%20NLU%20NLG.py\n* https:\/\/medium.com\/@giacomo.veneri\/natural-language-processing-tutorial-with-sota-2020-python-packages-2a9817952903\n* https:\/\/www.kaggle.com\/pavansanagapati\/knowledge-graph-nlp-tutorial-bert-spacy-nltk\n* https:\/\/www.youtube.com\/watch?v=vEmm9fZJuuM\n* https:\/\/www.youtube.com\/watch?v=ZIiho_JfJNw","91a10fb3":"* [Table of Contents](#Tableofcontents)\n<a id=\"Introduction\"><\/a>\n\n## Introduction\n\n#### Project Name: Sentiment Classification\n\n#### Objective: Predict the sentiment of the given text.\n\nThis is a text classification task - sentiment classification. Every document (a line in the data file) is a sentence extracted from social media (blogs). Your goal is to classify the sentiment of each sentence into \"positive\" or \"negative\".The data contains 7086 sentences, already labeled with 1 (positive sentiment) or 0 (negative sentiment).","029a29d2":"* [Table of Contents](#Tableofcontents)\n<a id=\"CreatingTokenizer\"><\/a>\n\n## Creating Tokenizer","f49e4578":"* [Table of Contents](#Tableofcontents)\n<a id=\"EvaluatingNLPPipeline\"><\/a>\n\n## Evaluating NLP pipeline","749ee2bc":"* [Table of Contents](#Tableofcontents)\n<a id=\"TF-IDFVectorizer\"><\/a>\n\n## Creating TF-IDF Vectorizer","a48333c2":"* [Table of Contents](#Tableofcontents)\n<a id=\"WordCloudVisualization\"><\/a>\n\n## WordCloud Visualization","bd090903":"* [Table of Contents](#Tableofcontents)\n<a id=\"2.DependencyParsing\"><\/a>\n\n### 2.Dependency Parsing\n\nDependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the head of the sentence. \n\nDependency parsing helps you know what role a word plays in the text and how different words relate to each other. It\u2019s also used in shallow parsing and named entity recognition.","687e9356":"## NLP: Spacy\n### Topic: Sentiment Classification.\n### Objective : Main intension of developing this project is to predict the sentiment of the given text.","7b5da2e7":"* [Table of Contents](#Tableofcontents)\n<a id=\"7.Similarity\"><\/a>\n\n\n### 7.Similarity\n\nSimilarity is determined by comparing word vectors or \u201cword embeddings\u201d, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this:\n\nSpacy also provides inbuilt integration of dense, real valued vectors representing distributional similarity information.\n\nModels that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n","168c9823":"* [Table of Contents](#Tableofcontents)\n<a id=\"CustomTransformer\"><\/a>\n\n## Creating Custom Transformer for cleaning the dataset","91fd1418":"* [Table of Contents](#Tableofcontents)\n<a id=\"Splittingthedatasetintotrainandtest\"><\/a>\n\n## Splitting the dataset into train and test","7052c0ee":"* [Table of Contents](#Tableofcontents)\n<a id=\"ImportPackagesandReadingthedataset\"><\/a>\n\n\n## Import packages and reading the dataset","a829df6d":"* [Table of Contents](#Tableofcontents)\n<a id=\"Conclusion\"><\/a>\n\n## Conclusion\n\n### SupportVectorClassifier has predict with 99% accuracy for this dataset when compare to RandomForestClassifier","6ba4c10b":"* [Table of Contents](#Tableofcontents)\n<a id=\"6.EntityDetection\"><\/a>\n\n### 6.Entity Detection\n\nEntity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text, since you can quickly pick out important topics or indentify key sections of text.\n","6a9d2f0d":"* [Table of Contents](#Tableofcontents)\n<a id=\"SpacyReferenceLiveSessionVideoReference2\"><\/a>\n\n### Spacy Reference Live Session Video Reference 2","e62da828":"* [Table of Contents](#Tableofcontents)\n<a id=\"4.SentenceBoundaryDetection\"><\/a>\n\n### 4.Sentence Boundary Detection (SBD)\n\nSentence Boundary Detection is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You\u2019ll use these units when you\u2019re processing your text to perform tasks such as part of speech tagging and entity extraction.In spaCy, the sents property is used to extract sentences. \n","bf93b056":"* [Table of Contents](#Tableofcontents)\n<a id=\"CountVectorizer\"><\/a>\n\n## Creating Count Vectorizer","28a5f040":"* [Table of Contents](#Tableofcontents)\n<a id=\"ExploratoryDataAnalysis\"><\/a>\n\n## Exploratory Data Analysis","b8ea1625":"* [Table of Contents](#Tableofcontents)\n<a id=\"1.Displacy\"><\/a>\n\n### 1.Displacy \nUsing spaCy\u2019s built-in displaCy visualizer,The quickest way to visualize Doc is to use displacy.serve. This will spin up a simple web server and let you view the result straight from your browser. displaCy can either take a single Doc or a list of Doc objects as its first argument.\n","0d7e6031":"* [Table of Contents](#Tableofcontents)\n<a id=\"CreateNLPPipelineforSupportVectorClassifier\"><\/a>\n\n## Creating NLP Pipeline for SupportVectorClassifier","208532d6":"* [Table of Contents](#Tableofcontents)\n<a id=\"5.NamedEntityRecognition\"><\/a>\n\n### 5.Named Entity Recognition (NER)\n\nA named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn\u2019t always work perfectly and might need some tuning later, depending on your use case.\n","cdbfd6de":"* [Table of Contents](#Tableofcontents)\n<a id=\"SpacyReferenceLiveSessionVideoReference1\"><\/a>\n\n### Spacy Reference Live Session Video Reference 1","fa596553":"<a id=\"Tableofcontents\"><\/a>\n# Table of Contents\n\n* [Introduction](#Introduction)\n* [Import Packages and Reading the dataset](#ImportPackagesandReadingthedataset)\n* [Exploratory Data Analysis](#ExploratoryDataAnalysis)\n* [1.Displacy](#1.Displacy)\n* [2.Dependency Parsing](#2.DependencyParsing)\n* [3.Lemmatization](#3.Lemmatization)\n* [4.Sentence Boundary Detection](#4.SentenceBoundaryDetection)\n* [5.Named Entity Recognition](#5.NamedEntityRecognition)\n* [6.Entity Detection](#6.EntityDetection)\n* [7.Similarity](#7.Similarity)\n* [Spacy Reference Live Session Video Reference 1](#SpacyReferenceLiveSessionVideoReference1)\n* [Creating Tokenizer](#CreatingTokenizer)\n* [Custom Transformer](#CustomTransformer)\n* [Count Vectorizer](#CountVectorizer)\n* [TF-IDF Vectorizer](#TF-IDFVectorizer)\n* [Spacy Reference Live Session Video Reference 2](#SpacyReferenceLiveSessionVideoReference2)\n* [WordCloud Visualization](#WordCloudVisualization)\n* [Splitting the dataset into train and test](#Splittingthedatasetintotrainandtest)\n* [Create NLP Pipeline for RandomForestClassifier](#CreateNLPPipelineforRandomForestClassifier)\n* [Evaluating NLP Pipeline](#EvaluatingNLPPipeline)\n* [Create NLP Pipeline for SupportVectorClassifier](#CreateNLPPipelineforSupportVectorClassifier)\n* [Evaluating NLP Pipeline](#EvaluatingNLPPipeline)\n* [Conclusion](#Conclusion)\n* [References](#References)"}}