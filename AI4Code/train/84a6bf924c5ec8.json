{"cell_type":{"7f46e8ed":"code","f7035cf1":"code","7902e779":"code","798ec904":"code","b0180516":"code","0ff37740":"code","92eea927":"code","98bb2f71":"code","8b21352c":"markdown","c5813374":"markdown","032a0c68":"markdown","d3f34ddd":"markdown","4fe98fc7":"markdown","741582a9":"markdown","4c3de25e":"markdown","b12859a8":"markdown","7ee2bc92":"markdown","f134a712":"markdown"},"source":{"7f46e8ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import models \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# metrics \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve\n\n# graphs \nimport matplotlib.pyplot as plt \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f7035cf1":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\nprint ( 'distinct class values : ' , data['class'].unique())\n\ndata['class'] = [1 if each == 'Normal' else 0 for each in data['class'] ]\nx = data.loc[:,data.columns != 'class'] # veya data.drop(['class'], axis = 1) \ny = data.loc[:,'class']\n\nx.head()","7902e779":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 1)\n\nprint ('x_train shape: {} '.format(x_train.shape))\nprint ('y_train shape: {} '.format(y_train.shape))\nprint ('x_test shape: {} '.format(x_test.shape))\nprint ('y_test shape: {} '.format(y_test.shape))","798ec904":"\n# find  best k value \nknn_accuracy_list =[]\nfor k in  range (1,25):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(x_train, y_train)\n    knn_accuracy_list.append(knn.score(x_test, y_test))\n    \nprint ('best k is :{} , best acccuracy is :{} '.format(  knn_accuracy_list.index (np.max(knn_accuracy_list))+1, np.max(knn_accuracy_list)))\n\n#knn classifier \nknn = KNeighborsClassifier(n_neighbors = knn_accuracy_list.index (np.max(knn_accuracy_list))+1 )\nknn.fit(x_train, y_train)\ny_pred_knn = knn.predict(x_test)\nprint ('KNeighborsClassifier test accuracy ' , knn.score(x_test, y_test) )\n","b0180516":"rfc_accuracy_list =[]\nfor r in  range (1,25):\n    rfc = RandomForestClassifier(random_state = r)\n    rfc.fit(x_train, y_train)\n    rfc_accuracy_list.append(rfc.score(x_test, y_test))\n    \nprint ('best r is :{} , best acccuracy is :{} '.format(  rfc_accuracy_list.index (np.max(rfc_accuracy_list))+1, np.max(rfc_accuracy_list)))\n    \nrfc = RandomForestClassifier(random_state = rfc_accuracy_list.index (np.max(rfc_accuracy_list))+1)\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\nprint ('RandomForestClassifier test accuracy ' , rfc.score(x_test, y_test) )","0ff37740":"logreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_logreg = logreg.predict(x_test)\nprint ('LogisticRegression test accuracy ' , logreg.score(x_test, y_test) )","92eea927":"cm = confusion_matrix(y_test,y_pred_knn)\nprint('KNN Confusion matrix: \\n',cm)\nprint('KNN Classification report: \\n',classification_report(y_test,y_pred_knn))\n\ncm = confusion_matrix(y_test,y_pred_rfc)\nprint('RandomForestClassifier Confusion matrix: \\n',cm)\nprint('RandomForestClassifier Classification report: \\n',classification_report(y_test,y_pred_rfc))\n\ncm = confusion_matrix(y_test,y_pred_logreg)\nprint('LogisticRegression Confusion matrix: \\n',cm)\nprint('LogisticRegression Classification report: \\n',classification_report(y_test,y_pred_logreg))","98bb2f71":"\ny_pred_knn_prob = knn.predict_proba(x_test)[:,1]\ny_pred_rfc_prob = rfc.predict_proba(x_test)[:,1]\ny_pred_lr_prob = logreg.predict_proba(x_test)[:,1]\n\nfpr_knn, tpr_knn, thresholds = roc_curve(y_test, y_pred_knn_prob) \nfpr_rfc, tpr_rfc, thresholds = roc_curve(y_test, y_pred_rfc_prob) \nfpr_lr, tpr_lr, thresholds = roc_curve(y_test, y_pred_lr_prob) \n\nplt.figure (figsize=[13 ,8])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_knn, tpr_knn, label='KNN')\nplt.plot(fpr_rfc, tpr_rfc, label='Random Forest')\nplt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('ROC')\nplt.show()","8b21352c":"## <div id=\"5\">5.ROC Curve<div\/>\n    \nThe ROC curve is created by plotting the **true positive rate (TPR) ** against the ** false positive rate (FPR)** at various threshold settings. The** true-positive rate** is also known as **sensitivity, recall or probability of detection **in machine learning. ","c5813374":"## <div id=\"4\">4. Confusion matrixes<div\/>\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known<br><br>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Pred No** &nbsp;&nbsp;&nbsp;&nbsp;**Pred Yes**<br>\n**Actual No**  &nbsp;TN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FP <br>\n**Actual Yes** &nbsp;FN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP<br>\n\nThis is a list of rates that are often computed from a confusion matrix for a binary classifier: <br>\n\n**Accuracy:** Overall, how often is the classifier correct?<br>\n    * (TP+TN)\/total\n**Misclassification Rate: **Overall, how often is it wrong?<br>\n    * (FP+FN)\/total\n    * 1 - Accuracy\n    * also known as \"Error Rate\"\n**True Positive Rate: **When it's actually yes, how often does it predict yes?<br>\n    * TP\/actual yes\n    * also known as \"Sensitivity\" or \"Recall\"\n**False Positive Rate:** When it's actually no, how often does it predict yes?<br>\n    * FP\/actual no \n**True Negative Rate: **When it's actually no, how often does it predict no?<br>\n    * TN\/actual no\n    * 1 - False Positive Rate\n    * also known as \"Specificity\"\n**Precision: **When it predicts yes, how often is it correct?<br>\n    * TP\/predicted yes\n**Prevalence:** How often does the yes condition actually occur in our sample?<br>\n    * Actual yes\/total","032a0c68":"### <div id=\"3.2\">3.2. Random Forest Classifier<div\/>","d3f34ddd":"## <div id=\"2\">2. Train Test Split<div\/>","4fe98fc7":"## <div id=\"3\">3. Fit Classifiers<div\/>\n### <div id=\"3.1\">3.1.K Neighbors Classifier<div\/>","741582a9":"### <div id=\"3.3\">3.3. Logistic Regression<div\/>","4c3de25e":"## <div id=\"6\">6. Conclusion<div\/>\n\nAccording to accurancy and precision , for this data , below are classifiers from best to worst :\n\n1. Random Forest Classifier\n2. K Neighbors Classifier\n3. Logistic Regression","b12859a8":"<a id=\"0\"> <a\/>\n# Compare Classifiers <br>\nIn this program, we will read file \"column-2C-weka.csv\"  . This file has class property and 6 numeric features  . Class property has 2 values named \"Abnormal\" and \"Normal\" . We will fit 3 different classfiers for this file and compare 3 models and find which model is most suitable for the data <br>\n\n1. [Read Data](#1)<br>\n2. [Train Test Split](#2)<br>\n3. [Fit Classifiers](#3)<br>\n    3.1. [K Neighbors Classifier ](#3.1)<br>\n    3.2. [Random Forest Classifier](#3.2)<br>\n    3.3. [Logistic Regression ](#3.3)<br>\n4. [Confusion matrixes ](#4)<br>\n5. [ROC Curve](#5)<br>\n6. [Conclusion](#6)<br>","7ee2bc92":"## <div id=\"1\">1. Read Data <div\/>","f134a712":"KNeighborsClassifier test accuracy  **0.8817204301075269**<br>\nRandomForestClassifier test accuracy  **0.8924731182795699**<br>\nLogisticRegression test accuracy  **0.8602150537634409**<br>\n\nAccording to accuracy RandomForestClassifier > KNeighborsClassifier > LogisticRegression"}}