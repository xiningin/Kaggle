{"cell_type":{"bb82b661":"code","6b0789e3":"markdown","b92086fe":"markdown","0612fc06":"markdown","e6cf2fbc":"markdown","f0718a94":"markdown","f47b592d":"markdown","b422282b":"markdown"},"source":{"bb82b661":"#Label Encoder\n#for c in train.columns[train.dtypes == 'object']:\n#    X[c] = X[c].factorize()[0]\n\n#plt.plot(rf.feature_importances_)\n#plt.xticks(np.arange(X.shape[1]), X.columns.tolist(),rotation=90);","6b0789e3":"### Building intuition about the data\n- Getting knowledge domain\n    - Understand the column names and the problem\n- Checking if the data is intuitive\n    - Ex, age can't be 350\n    - Create feature named \"is_incorrect\" and mark incorrect data\n- Understanding how the data was generated\n    - To set up a proper validation","b92086fe":"### Explatory Data Analysis\n- What and why\n- Things to explore \n- Exploration and visualization tools\n- Dataset cleanin\n- Kaggle EDA\n\n#### What is EDA?\n- Understanding and building an intiution about the data\n- Generate hypothesis and find insights\n- Visualization -> Idea , Idea -> Visualization\n- Find magic features\n- Find data leaks: mistakes made by the organizers during data preparation","0612fc06":"### Exploring anonymized data\n- Guess the meaning and the types of the columns\n- Find relation between pairs\n- Find feature groups\n- Try to decode the features\n    - find scaling parameter, backscale and shift back to reach original data\n- Use df.dtypes, df.info(), x.value_counts(), x.isnull()","e6cf2fbc":"### Data Leakage\n- Unexpected information in the data that allows us to make unrealistically good predictions.\n- Leaks in time series\n    - Split should be done in time, check it first. If not by time, it mat be a leak. Features like \"prices_next_week\" will be the most important\n    - Look for the test set and create new features about future.\n- Unexpected Information\n    - Try to find meta information\n    - Information in IDs (may be hash of something)\n    - Row Order\n### Leaderboard Probing\n- Submit your result multiple times, each time change different part to understand ground truth.\n### Expedia Kaggle Competition\n- Which hotel group a user is going to book. Search results, clicks, books given.\n    - worked on spherical distance (Haversine Formula)","f0718a94":"### Dataset Cleaning and things to check\n#### Dataset Cleaning\n- Constant features\n    - remove if all same\n    - train.nunique(axis = 1) == 1\n- Duplicated features\n    - remove one of identical columns\n    - train.T.drop_duplicates()\n    - for categorical features:\n>         for f in categorical_features:\n>             train[f] = train[f].factorize()\n>         train.T.drop_duplicates()\n        \n#### Other things to check\n- Duplicated rows\n    - Understand and why\n- If exact row appears at both train and test sets, label manually\n- Check if data is shuffled\n    - If not shuffled, high chance to find data leakage. check and plot for rolling mean and mean for target.\n- Check null counts:\n    - row: df.isnull().sum(axis=1).head(15)\n    - column : df.isnull().sum(axis=0).head(15)\n- Check unique counts:\n    - df.nunique(dropna=False).sort_values(), drop column if it has only a unique value\n- Check similar columns to create new features\n- Get column types:\n    - num_cols = list(df.select_dtypes(exclude=['object']).columns)\n- New features:\n    - mod, diff, year, month, date\n- Sort correlation matrix","f47b592d":"### Visualization\n#### Explore Individual Features\n- plt.figure(figsize=(15,5))\n- Histograms\n    - plt.hist(x)\n    - use number of bins\n    - take log and rehistogram to see from different perspective\n- Plots\n    - plt.plot(x,'.')\n    - X axis: row index, Y axis: feature values\n- Scatter Plots\n    - plt.scatter(range(len(x)),x,c=y)\n- Statistics\n    - x.var(), x.mean()\n    - x.describe(), x.isnull(), x.value_counts()\n    \n#### Explore feature relation\n- Scatter Plots\n    - plt.scatter(x1,x2)\n- Correlation Plots\n    - pd.scatter_matrix(df)\n    - df.corr(),plt.matshow(..)\n- Plot (index vs feature statistics)\n    - df.mean().sort_values().plot(style='.')","b422282b":"### Validation Strategies\n- Holdout: sklearn.model_selection.ShuffleSplit\n    - ngroups = 1\n    - Data -> 0.8*Train + 0.2*Test\n- K-fold: sklearn.model_selection.KFold\n    - ngroups = k\n    - Data = (0.8*Train + 0.2*Test) + (0.6*Train + 0.2*Test + 0.2*Train) + (04*Train + 0.2*Test + 0.4*Train) + (0.2*Train + 0.2*Test + 0.6Train) + (0.2*Test + 0.8*Train):\n    - Final measure is the average\n- Leave-one-out: sklearn.model_selection.LeaveOneOut\n    - ngroups = len(train)\n    - Data = k-1 for train, 1 for test\n    - Use with small amount of data\n- Stratification:\n    - Useful for small, unbalanced datasets, multiclass classification\n### Data Splitting Strategies\n- Set up validation to mimic train\/test split. ***\n- Logic of feature generation depends on the data splitting strategy\n- If time series problem, don't random split.\n- Different splitting strategies depend on:\n    - generated features\n    - way the model will rely on that features\n    - some kind of target leak\n- Split Types:\n    - Random, rowwise\n    - Timewise(moving window validation)\n    - By id\n    - Combined\n### Validation Problems\n- Validation Stage\n    - Causes:\n        - Too little or diverse and inconsistent data\n    - Solution:\n        - Different Kfolds with differen number of folds and different random seeds.\n- Submission Stage\n    - Think LB as another validation split\n    - Causes:\n        - too little data on LB\n        - train and test are from different distributions   \n        - incorrect train\/test split\n    - Solution:\n        - Leaderboard probing\n        - Distribute data in the same way in train and test\n    - train: 80%man-20%woman, test: 20%man-80%woman -> try to mimic test set in validations.\n    - Expect LB shuffle because of randomness, little amount of data and different public\/private distributions"}}