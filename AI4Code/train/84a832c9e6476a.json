{"cell_type":{"18b1c4cf":"code","4ed7b109":"code","8857026c":"code","441e39d4":"code","04d0a98f":"code","b38ef94a":"code","f4c19278":"code","95a02b0c":"code","763e493b":"code","56963f9c":"code","c6c22891":"code","9852071f":"code","49c062f9":"code","0b5e0218":"code","55405d8a":"code","d789e9ce":"code","adb08272":"markdown","f6deb173":"markdown","47e5bdc1":"markdown","615f6a7d":"markdown","788181da":"markdown","e66e7419":"markdown","e9f3b43f":"markdown","3a865d5a":"markdown"},"source":{"18b1c4cf":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7","4ed7b109":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')","8857026c":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\nimport gc\npd.set_option('display.max_columns', None)\n\n# Visualizations\nfrom PIL import Image\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Machine Learning\n# Utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n# Deep Learning\nimport tensorflow as tf\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n# Metrics\nfrom sklearn.metrics import roc_auc_score\n# TPU Specific\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Select Accelerator\ndef auto_select_accelerator():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        TPU_DETECTED =True\n    except:\n        pass\n    \n    return TPU_DETECTED\n\nTPU_DETECTED = auto_select_accelerator()\n\nif TPU_DETECTED:\n    device = 'TPU'\nelif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","441e39d4":"csv_dir = '..\/input\/seti-breakthrough-listen'\ntrain_dir = '..\/input\/seti-breakthrough-listen\/train'\ntest_dir = '..\/input\/seti-breakthrough-listen\/test'\n\ntrain_file_path = os.path.join(csv_dir, 'train_labels.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","04d0a98f":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(sample_sub_file_path)","b38ef94a":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, name[0], f'{name}.npy')\n    return path","f4c19278":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))","95a02b0c":"(X_train, X_valid, y_train, y_valid) = train_test_split(train_df['image_path'],\n                                                        train_df['target'],\n                                                        test_size=0.2,\n                                                        stratify=train_df['target'],\n                                                        shuffle=True,\n                                                        random_state=RANDOM_SEED)","763e493b":"print(\"Available Vision Transformer Models: \")\ntimm.list_models(\"vit*\")","56963f9c":"params = {\n    'model': 'vit_base_patch32_384',\n    'im_size': 384,\n    'inp_channels': 1,\n    'device': device,\n    'lr': 1e-4,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 0,\n    'epochs': 10,\n    'out_features': 1,\n    'Balance_Dataset': False,\n}","c6c22891":"def get_train_transforms(IMG_SIZE):\n    '''\n    Return Augmented Image tensor for training dataset\n    '''\n    return albumentations.Compose(\n        [\n            albumentations.Resize(IMG_SIZE,IMG_SIZE),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=10, max_h_size=12, max_w_size=12,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms(IMG_SIZE):\n    '''\n    Return resized Tensor for Validation Dataset\n    '''\n    return albumentations.Compose(\n        [\n            albumentations.Resize(IMG_SIZE,IMG_SIZE),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms(IMG_SIZE, TTA):\n    '''\n    Returns resized tensor if TTA = 1 otherwise returns\n    Augmented Image tensor if TTA > 1. Values <1 not accepted.\n    '''\n    if TTA > 1:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(IMG_SIZE,IMG_SIZE),\n                albumentations.HorizontalFlip(p=0.5),\n                albumentations.VerticalFlip(p=0.5),\n                albumentations.Rotate(limit=180, p=0.7),\n                albumentations.RandomBrightness(limit=0.6, p=0.5),\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n                ),\n                ToTensorV2(p=1.0)\n            ]\n        )\n    else:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(IMG_SIZE,IMG_SIZE),\n                ToTensorV2(p=1.0)\n            ]\n        )","9852071f":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath)\n        image = image.astype(np.float32)\n        image = np.vstack(image).transpose((1, 0))\n            \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = self.targets[idx].reshape(-1,)\n        return image, label","49c062f9":"class AlienNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n                 inp_channels=params['inp_channels'], pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        if model_name.split('_')[0] == 'efficientnet':\n            n_features = self.model.classifier.in_features\n            self.model.conv_stem = nn.Conv2d(inp_channels, 40, kernel_size=(3, 3),\n                                             stride=(2, 2), padding=(1, 1), bias=False)\n            self.model.classifier = nn.Linear(n_features, out_features)\n        \n        elif model_name.split('_')[0] == 'nfnet':\n            n_features = self.model.head.fc.in_features\n            self.model.head.fc = nn.Linear(n_features, out_features)\n            \n        elif model_name.split('_')[0] == 'vit':\n            n_features = self.model.head.in_features\n            self.model.head = nn.Linear(n_features, out_features, bias=True)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \n    def roc_score(self, output, target):\n        try:\n            y_pred = torch.sigmoid(output).cpu()\n            y_pred = y_pred.detach().numpy()\n            target = target.cpu()\n            \n            return roc_auc_score(target, y_pred)\n        except:\n            return 0.5\n    \n    def train_one_epoch(self, train_loader, criterion, optimizer, params):\n        epoch_loss = 0.0\n        epoch_roc = 0.0\n        self.model.train()\n        \n        for i, (data, target) in enumerate(train_loader):\n            data = data.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            optimizer.zero_grad()\n            output = self.forward(data)\n            loss = criterion(output, target)\n            roc = self.roc_score(output, target)\n            epoch_loss += loss\n            epoch_roc += roc\n            loss.backward()\n            \n            if params['device'].type == 'xla':\n                xm.optimizer_step(optimizer)\n                if i % 20 == 0:\n                    xm.master_print(f\"\\tBATCH {i+1}\/{len(train_loader)} - LOSS: {loss}\")\n            else:\n                optimizer.step()\n                \n        return epoch_loss \/ len(train_loader), epoch_roc \/ len(train_loader)\n    \n    def validate_one_epoch(self, valid_loader, criterion, params):\n        valid_loss = 0.0\n        valid_roc = 0.0\n        self.model.eval()\n        \n        for data, target in valid_loader:\n            data = data.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            \n            with torch.no_grad():\n                output = self.model(data)\n                loss = criterion(output, target)\n                roc = self.roc_score(output, target)\n                valid_loss += loss\n                valid_roc += roc\n\n        return valid_loss \/ len(valid_loader), valid_roc \/ len(valid_loader)","0b5e0218":"def fit_tpu(model, params, criterion, optimizer, train_loader,\n            valid_loader=None):\n\n    valid_loss_min = np.Inf\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoch in range(1, params['epochs'] + 1):\n        gc.collect()\n        para_train_loader = pl.ParallelLoader(train_loader, [params['device']])\n\n        xm.master_print(f\"{'='*50}\")\n        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n        train_loss, train_acc = model.train_one_epoch(\n            para_train_loader.per_device_loader(params['device']),\n            criterion, optimizer, params\n        )\n        xm.master_print(\n            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ROC: {train_acc}\\n\"\n        )\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        gc.collect()\n\n        if valid_loader is not None:\n            gc.collect()\n            para_valid_loader = pl.ParallelLoader(valid_loader, [params['device']])\n            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n            valid_loss, valid_acc = model.validate_one_epoch(\n                para_valid_loader.per_device_loader(params['device']),\n                criterion, params\n            )\n            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ROC: {valid_acc}\\n\")\n            valid_losses.append(valid_loss)\n            valid_accs.append(valid_acc)\n            gc.collect()\n\n            if valid_loss <= valid_loss_min and epoch != 1:\n                xm.master_print(\n                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n                        valid_loss_min, valid_loss\n                    )\n                )\n            valid_loss_min = valid_loss\n\n    return {\n        \"train_loss\": train_losses,\n        \"valid_losses\": valid_losses,\n        \"train_acc\": train_accs,\n        \"valid_acc\": valid_accs,\n    }","55405d8a":"def _run():\n    train_dataset = SETIDataset(\n        images_filepaths=X_train.values,\n        targets=y_train.values,\n        transform=get_train_transforms(params['im_size'])\n    )\n\n    valid_dataset = SETIDataset(\n        images_filepaths=X_valid.values,\n        targets=y_valid.values,\n        transform=get_valid_transforms(params['im_size'])\n    )\n\n    tpu_train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas = xm.xrt_world_size(),\n        rank = xm.get_ordinal(),\n        shuffle = True)\n\n    tpu_valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas = xm.xrt_world_size(),\n        rank = xm.get_ordinal(),\n        shuffle = False)\n    \n    class_counts = y_train.value_counts().to_list()\n    num_samples = sum(class_counts)\n    labels = y_train.to_list()\n\n    class_weights = [num_samples\/class_counts[i] for i in range(len(class_counts))]\n    weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n    balanced_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n\n    if params['device'] == 'TPU':\n        train_loader = DataLoader(\n            train_dataset, batch_size=params['batch_size'],\n            sampler = tpu_train_sampler, num_workers=params['num_workers'],\n            pin_memory=True, drop_last=True\n        )\n\n        val_loader = DataLoader(\n            valid_dataset, batch_size=params['batch_size'],\n            sampler = tpu_valid_sampler, num_workers=params['num_workers'],\n            pin_memory=True\n        )\n\n    elif params['device'].type == 'cuda':\n        train_loader = DataLoader(\n            train_dataset, batch_size=params['batch_size'],\n            sampler = balanced_sampler, num_workers=params['num_workers'],\n            pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            valid_dataset, batch_size=params['batch_size'], shuffle=False,\n            num_workers=params['num_workers'], pin_memory=True\n        )\n    else:\n        train_loader = DataLoader(\n            train_dataset, batch_size=params['batch_size'], shuffle=True,\n            num_workers=params['num_workers'], pin_memory=True\n        )\n\n        val_loader = DataLoader(\n            valid_dataset, batch_size=params['batch_size'], shuffle=False,\n            num_workers=params['num_workers'], pin_memory=True\n        )\n    \n    params['device'] = xm.xla_device()\n    model = AlienNet()\n    model = model.to(params['device'])\n    criterion = nn.BCEWithLogitsLoss().to(params['device'])\n\n    lr = params['lr'] * xm.xrt_world_size()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n    start_time = datetime.now()\n    xm.master_print(f\"Start Time: {start_time}\")\n    \n    logs = fit_tpu(\n        model=model,\n        params=params,\n        criterion=criterion,\n        optimizer=optimizer,\n        train_loader=train_loader,\n        valid_loader=val_loader,\n    )\n\n    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n\n    xm.master_print(\"Saving Model\")\n    xm.save(model.state_dict(), f\"AlienNet_{params['model']}_{datetime.now().strftime('%Y%m%d-%H%M')}.pth\")","d789e9ce":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\n    a = _run()\n\n# _run()\nFLAGS = {}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")","adb08272":"# Model Params","f6deb173":"# Train","47e5bdc1":"# Read the Dataset","615f6a7d":"# Dataset","788181da":"![SETI](https:\/\/earthsky.org\/upl\/2020\/02\/Earth-transit-zone-Breakthrough-Listen.jpg)\n# About This Notebook\nThis is a try to demonstate the use of Vision Transformers on this Dataset by using TPUs.  \n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a\n\n# Problem Statement\n* The Breakthrough Listen team at the University of California, Berkeley, employs the world\u2019s most powerful telescopes to scan millions of stars for signs of technology.\n* It\u2019s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology.\n* Current methods use two filters to search through the haystack.\n    * First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn\u2019t coming from the direction of the target star.\n    * Second, the pipeline discards signals that don\u2019t change their frequency, because this means that they are probably nearby the telescope.\n* Use data science skills to help identify anomalous signals in scans of Breakthrough Listen targets.\n* Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals.\n\n# Why this competition?\nAs evident from the problem statement, this competition presents an interesting challenge straight out of a Sci-Fi movie stuff!  \nAlso (if successful) this model should be able to answer one of the biggest questions in science.\n\n# Expected Outcome\nGiven a numpy array of signal, we should be able to identify it as a positive class (signal from an alien lifeform) or negative class (signal from one of our devices).\n\n# Data Description\nData is stored in a numpy float16 format in training folder and the labes are mentioned in the `train_labels.csv` file where the first letter of the file name indicates the subfolder the `.npy` file is placed inside the train directory.  \nThe data consist of two-dimensional arrays {shape = (6, 273, 256)}, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more.\n\n# Grading Metric\nSubmissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target.\n\n# Problem Category\nFrom the data and objective its is evident that this is a **Classification Problem**. But we have an option for the approach starting with vanilla ML methods to Computer Vision to Anomaly detection etc.\n\n# Brief Introduction to Vision Transformers\nIt all started with [this paper](https:\/\/arxiv.org\/abs\/2010.11929) from Google Brain team in late 2020. And it literally says *AN IMAGE IS WORTH 16X16 WORDS*.  \n\nWhile the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. But this paper shows that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.  \n\nThe principal approach of Transformers is to pre-train on a huge dataset and then fine-tune on a task-specific dataset. Leveraging the efficiency and scalability of transformer based networks, it has become possible to train huge models. With the models and datasets growing, there is still no sign of saturating performance.  \n\nThe architecture of ViT is shown below:-  \n\n![ViT](https:\/\/amaarora.github.io\/images\/ViT.png)\n\nOn fine-tuning and application on a sample image we get the following result:-  \n\n![Vit Result](https:\/\/user-images.githubusercontent.com\/6073256\/101206904-2a338f00-36b3-11eb-8920-f617abab1604.png)\n\nAs you can observe clearly the architecture attends to image regions that are semantically relevant for classification; or loosely speaking the attention mask only focus on important areas in an image.\n\nSpecial thanks to [rwightman](https:\/\/github.com\/rwightman\/pytorch-image-models) for creating timm which makes implementing this SOTA method incredibly easy and contains all the pre-trained weights as well.\n\nSo without further ado, let's now start with some basic imports to take us through this:-\n\n# Imports","e66e7419":"# Model","e9f3b43f":"This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Pytorch has many SOTA Image models which you can try out using the guidelines in this notebook.\n\nI hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.\n\n**If you liked this notebook and use parts of it in you code, please show some support by upvoting this kernel. It keeps me inspired to come-up with such starter kernels and share it with the community.**\n\nThanks and happy kaggling!","3a865d5a":"# Image Augmentation"}}