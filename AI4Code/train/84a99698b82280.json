{"cell_type":{"3477fa38":"code","be2c4f96":"code","1d1e2b36":"code","e3f9c4e7":"code","a72f4f80":"code","c2083961":"code","72b000d8":"code","92daabdb":"code","0a7e9228":"code","7e141399":"code","eacc7c2e":"code","a0d2fd24":"code","4da87739":"code","33a1556d":"code","681666b5":"markdown","bdcdfd28":"markdown","27bcd244":"markdown","ed7a2703":"markdown","e9b8a014":"markdown","3486fd19":"markdown","a511ad7b":"markdown","8745a284":"markdown","bc46f6ef":"markdown"},"source":{"3477fa38":"import os\nimport pandas as pd\nimport numpy as np\nfrom statistics import mode, mean\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\npd.plotting.register_matplotlib_converters()\npd.options.mode.chained_assignment = None  # default='warn'\n\nfrom category_encoders import CountEncoder\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\")\n\nD = pd.read_csv('..\/input\/train.csv', index_col='Id')\nT = pd.read_csv('..\/input\/test.csv', index_col='Id')","be2c4f96":"print('Plots of GrLivArea, GarageArea, and OpenPorchSF, before outliers where removed.')\nfig, axs = plt.subplots(ncols=3, figsize=(13,4))\nsns.scatterplot(x=D.GrLivArea, y=D.SalePrice, ax=axs[0])\nsns.scatterplot(x=D.GarageArea, y=D.SalePrice, ax=axs[1])\nsns.scatterplot(x=D.OpenPorchSF, y=D.SalePrice, ax=axs[2])\n\n# Identify outliers...\noutliers = list(D[((D.GrLivArea > 4200) & (D.SalePrice < 500000))|\n                  ((D.GarageArea > 1200) & (D.SalePrice < 270000))|\n                  ((D.OpenPorchSF > 500) & (D.SalePrice < 60000))].index)\n\n# ... and non-outliers.\nregular = [k for k in D.index if k not in outliers]","1d1e2b36":"# Most ML models works better with normally distributed data.\nz = np.log(D.SalePrice)\nX = D.drop(['SalePrice'], axis=1)\n# We will clean as one dataset\nF = pd.concat([X, T])\n\ndel X, T  # To avoid confusion","e3f9c4e7":"### First type: honestly missing\ndef impute(index, column, function, condition=None):\n    df = F.loc[regular]\n    if condition is not None:\n        value = F.loc[index, condition]\n        df = df[df[condition]==value]\n    F.loc[index, column] = function(df[column])\n\nfor k in [1916, 2217, 2251, 2905]:\n    impute(k, 'MSZoning', mode, 'Neighborhood')\nfor k in [2041, 2186]:\n    impute(k, 'BsmtCond', mode, 'YearBuilt')\nfor k in [2127, 2577]:\n    impute(k, 'GarageFinish', mode, 'YearBuilt')\n    impute(k, 'GarageQual', mode, 'YearBuilt')\n    impute(k, 'GarageCond', mode, 'YearBuilt')\n\nimpute(2577, 'GarageCars', mode, 'YearBuilt')\nimpute(2577, 'GarageArea', mean, 'YearBuilt')\nimpute(2127, 'GarageCond', mode, 'YearBuilt')\nimpute(333, 'BsmtFinType2', mode, 'YearBuilt')\nimpute(2152, 'Exterior1st', mode, 'YearBuilt')\nimpute(1556, 'KitchenQual', mode, 'YearBuilt')\n\nF.BsmtExposure = F.BsmtExposure.fillna('No')\nF.Electrical = F.Electrical.fillna('SBrkr')\nF.Exterior2nd = F.Exterior2nd.fillna('Other')\nF.RoofMatl = F.RoofMatl.fillna('CompShg')\nF.Functional = F.Functional.fillna('Typ')\nF.SaleType = F.SaleType.fillna('WD')\n\nF.GarageYrBlt = F.GarageYrBlt.fillna(F.YearBuilt)\n\nF['LotFrontage'] = F.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\n### Second type: zero, NA, None, void\n\ncols = ['BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageQual' , 'GarageCond',\n        'PoolQC', 'Fence', 'MiscFeature']\nF[cols] = F[cols].fillna('NA')\n\ncols = ['BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtFullBath',\n       'BsmtHalfBath', 'MasVnrArea']\nF[cols] = F[cols].fillna(0)\n\ncols = ['BsmtFinType1', 'BsmtFinType2', 'GarageFinish']\nF[cols] = F[cols].fillna('Unf')\n\ncols = ['MasVnrType', 'GarageType', 'Alley']\nF[cols] = F[cols].fillna('None')\n\n","a72f4f80":"del F['Utilities']","c2083961":"# These distributions are also log-normal\nF.LotArea = np.log(F.LotArea)\nF.LotFrontage = np.log(F.LotFrontage)\n\n# Columns which are parts of already existing aggregated columns.\ndel F['BsmtUnfSF'], F['LowQualFinSF']\n\n# Computing new aggregated columns\nF['BsmtFinSF'] = F.BsmtFinSF1 + F.BsmtFinSF2\ndel F['BsmtFinSF1'], F['BsmtFinSF2']\n\ncols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\nF['PorchSF'] = sum([np.array(F[col]) for col in cols])\nfor col in cols:\n    del F[col]\n\ncols = ['FullBath', 'BsmtFullBath']\nF['TotalFullBaths'] = sum([np.array(F[col]) for col in cols])\ncols = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\nF['TotalBaths'] = sum([np.array(F[col]) for col in cols])\nfor col in cols:\n    del F[col]\n\n# Computing new indicator columns \nF['BigLot'] = [int(x) for x in F.LotArea > 10.5]\n\nF['Fireplace'] = [int(x) for x in F.Fireplaces != 0]\ndel F['Fireplaces']\n\nF['Pool'] = [int(x) for x in F.PoolArea > 0]\ndel F['PoolArea'], F['PoolQC']\n\n# Adjusitng YearRemodAdd for missing data\nindices = F[F.YearRemodAdd==1950].index\nF.loc[indices, 'YearRemodAdd'] = F.loc[indices, 'YearBuilt']\n\n# Adjusiting Condition1 and Condition 2\npostv = {'Artery': 0, 'Feedr': 0, 'Norm': 0, 'RRNn': 0, 'RRAn': 0, 'RRNe': 0,\n         'RRAe': 0, 'PosN': 1, 'PosA': 1}\nF['Positive'] = (np.array([postv[value] for value in F['Condition1']])+\n                    np.array([postv[value] for value in F['Condition2']]))\nnegtv = {'Artery': 2, 'Feedr': 1, 'Norm': 0, 'RRNn': 1, 'RRAn': 1, 'RRNe': 1,\n         'RRAe': 1, 'PosN': 0, 'PosA': 0}\nF['Negative'] = (np.array([negtv[value] for value in F['Condition1']])+\n                    np.array([negtv[value] for value in F['Condition2']]))\ndel F['Condition1'], F['Condition2']\n\n# Merging categories in some columns\ndctnry = {'Shed': 'Shed', 'Othr': 'Othr', 'Gar2': 'Othr', 'TenC': 'Othr',\n        'NA': 'NA'}\nF.MiscFeature = [dctnry[value] for value in F.MiscFeature]\n\ndctnry = {'WD': 'WD', 'CWD': 'WD', 'VWD': 'WD', 'New': 'New', 'COD': 'COD',\n          'Con': 'Con', 'ConLw': 'Con', 'ConLI': 'Con', 'ConLD': 'Con',\n          'Oth': 'Oth'}\nF.SaleType = [dctnry[value] for value in F.SaleType]\n\ndctnry = {'Normal': 'Normal', 'Partial': 'Partial', 'Abnorml': 'Abnorml',\n          'Family': 'Family', 'Alloca': 'Alloca', 'AdjLand': 'Alloca'}\nF.SaleCondition = [dctnry[value] for value in F.SaleCondition]\n\ndctnry = {'Gable': 'Gable', 'Hip': 'Hip', 'Flat': 'Other', 'Gambrel': 'Other', \n          'Mansard': 'Other', 'Shed': 'Other'}\nF.RoofStyle = [dctnry[value] for value in F.RoofStyle]\n\ndctnry = {'CompShg': 'CompShg', 'Tar&Grv': 'Tar&Grv', 'WdShngl': 'Tar&Grv', \n          'WdShake': 'Tar&Grv', 'Metal': 'Tar&Grv', 'Roll': 'Tar&Grv',\n          'Membran': 'Tar&Grv', 'ClyTile': 'Tar&Grv'}\nF.RoofMatl = [dctnry[value] for value in F.RoofMatl]\n\ndctnry = {'VinylSd': 'VinylSd', 'HdBoard': 'Plywood', 'MetalSd': 'MetalSd',\n          'Wd Sdng': 'Wd Sdng', 'Plywood': 'Plywood', \n          'CemntBd': 'CemntBd', 'CmentBd': 'CemntBd', 'CBlock': 'CemntBd',\n          'BrkFace': 'BrkFace', 'BrkComm': 'BrkFace', 'Brk Cmn': 'BrkFace',\n          'WdShing': 'WdShing', 'Wd Shng': 'WdShing',\n          'Stucco': 'Stucco', 'AsbShng': 'AsbShng', 'AsphShn': 'AsbShng',\n          'Stone': 'Other', 'ImStucc': 'Other', 'Other': 'Other'}\nF.Exterior1st = [dctnry[value] for value in F.Exterior1st]\nF.Exterior2nd = [dctnry[value] for value in F.Exterior2nd]\n\ndctnry = {20: '20', 30: '30', 40: '20', 45: '45', 50: '45',\n          60: '60', 70: '70', 75: '60', 80: '80', 85: '80',\n          90: '80', 120: '20', 150: '45', 160: '60',\n          180: '80', 190: '80'}\nF.MSSubClass = [dctnry[value] for value in F.MSSubClass]\n\ndctnry = {'NAmes': 'NAmes', 'CollgCr': 'CollgCr', 'OldTown': 'OldTown', \n          'Edwards': 'Edwards', 'Somerst': 'Somerst', 'Gilbert': 'Gilbert', \n          'NridgHt': 'NridgHt', 'Sawyer': 'Sawyer', 'NWAmes': 'NWAmes',\n          'SawyerW': 'Sawyer', 'BrkSide': 'BrkSide', 'Crawfor': 'Crawfor',\n          'Mitchel': 'Mitchel', 'NoRidge': 'NoRidge', 'Timber': 'Timber',\n          'IDOTRR': 'IDOTRR', 'ClearCr': 'ClearCr', 'StoneBr': 'StoneBr',\n          'SWISU': 'SWISU', 'Blmngtn': 'Blmngtn', 'MeadowV': 'MeadowV',\n          'BrDale': 'BrDale', 'Veenker': 'Timber', 'NPkVill': 'NPkVill',\n          'Blueste': 'NPkVill'}\nF.Neighborhood = [dctnry[value] for value in F.Neighborhood]\n\ndctnry = {'1Story': '1Story', '1.5Unf': '1.5Fin', '1.5Fin': '1.5Fin', \n          '2Story': '2Story', '2.5Unf': '2Story',\n          '2.5Fin': '2Story', 'SFoyer': 'SFoyer', 'SLvl': 'SFoyer'}\nF.HouseStyle = [dctnry[value] for value in F.HouseStyle]\n\ndctnry = {'1Fam': '1Fam', 'TwnhsE': 'TwnhsE', 'Duplex': 'Duplex',\n          'Twnhs': 'Twnhs', '2fmCon': 'Duplex'}\nF.BldgType = [dctnry[value] for value in F.BldgType]\n\ndctnry = {'FR3': 'FR2', 'FR2': 'FR2', 'CulDSac': 'CulDSac',\n          'Corner': 'Corner', 'Inside': 'Inside'}\nF.LotConfig = [dctnry[value] for value in F.LotConfig]\n\ndctnry = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:5, 8:5}\nF.BedroomAbvGr = [dctnry[value] for value in F.BedroomAbvGr]\n\ndctnry = {1:2, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10}\nF.OverallCond = [dctnry[value] for value in F.OverallCond]\nF.OverallQual = [dctnry[value] for value in F.OverallQual]","72b000d8":"# Columns for which 'NA' stands for a missing value. \n# Merging Poor and Fair to avoid overfitting.\ndctnry = {'NA': 0, 'Po': 0, 'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\ncols = ['HeatingQC', 'ExterQual', 'ExterCond', 'KitchenQual']\nfor col in cols:\n    F[col] = [dctnry[value] for value in F[col]]\n\n# Columns for which 'NA' stands for a missing feature. \n# Merging Poor and Fair to avoid overfitting.\ndctnry = {'NA': 0, 'Po': 1, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 3}\ncols = ['BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageQual', 'GarageCond']\nfor col in cols:\n    F[col] = [dctnry[value] for value in F[col]]\n\n# Ordinal categorical variables\ndctnry = {'Gtl': 0, 'Mod': 1, 'Sev': 2}\nF.LandSlope = [dctnry[value] for value in F.LandSlope]\n\ndctnry = {'Typ': 0, 'Min1': 1, 'Min2': 1, 'Mod': 2, 'Maj1': 3, 'Maj2': 3,\n          'Sev': 4, 'Sal': 5}\nF.Functional = [dctnry[value] for value in F.Functional]\n\ndctnry = {'N': 0, 'P': 1, 'Y': 2}\nF.PavedDrive = [dctnry[value] for value in F.PavedDrive]\n\ndctnry = {'Reg': 0, 'IR1': 1, 'IR2': 1, 'IR3': 1}\nF.LotShape = [dctnry[value] for value in F.LotShape]\n\ndctnry = {'Grvl': 1, 'Pave': 2, 'None': 0}\nF.Alley = [dctnry[value] for value in F.Alley]\n\n\n# The CountEncoder should only count in the training data.\nR = F.loc[regular]\ncols = ['Heating', 'CentralAir', 'Electrical', 'LandContour', 'RoofMatl',\n        'YrSold', 'MoSold']\nencoder = CountEncoder(cols=cols)\nencoder.fit(R[cols])\nF[cols] = encoder.transform(F[cols])\ndel R\n\ncols = ['BldgType', 'SaleCondition']\nencoder = LabelEncoder()\nfor col in cols:\n    F[col] = encoder.fit_transform(F[col])\n\ncols = ['Street', 'MasVnrType', 'MSZoning', 'MiscFeature', 'BsmtExposure',\n        'Foundation', 'GarageFinish', 'LotConfig', 'RoofStyle', 'GarageType',\n        'Neighborhood', 'MSSubClass', 'HouseStyle', 'Fence', 'SaleType']\nencoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\none_hot = pd.DataFrame(encoder.fit_transform(F[cols]))\none_hot.columns = encoder.get_feature_names()\none_hot.index = F.index\nF = pd.concat([F, one_hot], axis=1)\nF.drop(cols, axis=1, inplace=True)","92daabdb":"# Summing Condition and Quality for a few features\n\nF['BsmtCondQual'] = F.BsmtCond + F.BsmtQual\ndel F['BsmtCond'], F['BsmtQual']\n\nF['GarageCondQual'] = F.GarageCond + F.GarageQual\ndel F['GarageCond'], F['GarageQual']\n\nF['OverallCondQual'] = F.OverallCond + F.OverallQual\ndel F['OverallCond'], F['OverallQual']\n\n\n# Combining BsmtFinType1 and BsmtFinType2 to one joint one-hot encoding\nfor string in F.BsmtFinType1.unique():\n    F['BFT_' + string] = [int(x) for x in ((F.BsmtFinType1 == string)| \n                                           (F.BsmtFinType2 == string))]\ndel F['BsmtFinType1'], F['BsmtFinType2']\n\n\n# Combining Exterior1st and Exterior2nd to one joint one-hot enocding\nfor string in F.Exterior1st.unique():\n    F['Ext_' + string] = [int(x) for x in ((F.Exterior1st == string)| \n                                           (F.Exterior2nd == string))]\ndel F['Exterior1st'], F['Exterior2nd']","0a7e9228":"from catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import (ElasticNetCV, LassoCV, RidgeCV, \n                                  LinearRegression)\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n# Recovering the train data\nX = F.loc[regular]\ny = z.loc[regular]\n\nkf = KFold(10)\n\nalphas = [k\/100 for k in range(211, 230)]\nridge = make_pipeline(RobustScaler(), \n                      RidgeCV(alphas=alphas,\n                              cv=kf))\n\n\nalphas = [k\/1000 for k in range(1, 21)]\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7,\n                              alphas=alphas,\n                              cv=kf))\n\n\nalphas = [k\/1000 for k in range(1, 21)]\nratios = [k\/1000 for k in range(1, 21)]\nelast = make_pipeline(RobustScaler(),\n                      ElasticNetCV(max_iter=1e7,\n                                    alphas=alphas,\n                                    cv=kf,\n                                    l1_ratio=ratios))\n\n\nxgbst = XGBRegressor(learning_rate=.01,\n                      n_estimators=2500,\n                      max_depth=4,\n                      min_child_weight=0,\n                      subsample=.79,\n                      colsample_bytree=.15,\n                      nthread=-1,\n                      scale_pos_weight=2)\n\n\ngrbst = GradientBoostingRegressor(n_estimators=2000,\n                                  learning_rate=.02,\n                                  max_depth=4,\n                                  max_features='sqrt',\n                                  min_samples_leaf=18,\n                                  loss='huber')\n\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=41,\n                        epsilon=.028,\n                        gamma=.00013,\n                        tol=.0001))\n\n\n# UserWarning: \"timeout or by a memory leak.\"\n# ctbst = CatBoostRegressor(logging_level='Silent',\n#                           iterations=1300,\n#                           learning_rate=.03)\n\n\nlinrg = LinearRegression()\n\n\n# Stacking only the linear models and xgboost.\n# regressors = (ridge, lasso, elast, xgbst, linrg)\n# stack = StackingCVRegressor(regressors=regressors,\n#                             meta_regressor=xgbst,\n#                             use_features_in_secondary=True)","7e141399":"models = [ridge, lasso, elast, xgbst, grbst, svr, linrg]\nnames = ['ridge', 'lasso', 'elast', 'xgbst', 'grbst', 'svr  ', 'linrg']\n\nscores = []\nfor name, model in zip(names, models):\n    print(f'Cross validating: {name}.')\n    try:\n        cvr = cross_validate(model, X, y, cv=kf,\n                              scoring='neg_root_mean_squared_error',\n                              return_train_score=True, n_jobs=-1)\n    except ValueError:\n        cvr = cross_validate(model, X.values, y.values, cv=kf,\n                              scoring='neg_root_mean_squared_error',\n                              return_train_score=True, n_jobs=-1)\n    scores.append({'name': name, 'score': cvr['test_score'].mean(),\n                    'sdt': cvr['test_score'].std()})","eacc7c2e":"print(pd.DataFrame(scores))","a0d2fd24":"T = F.drop(regular + outliers)\n\npreds = {}\nfor name, model in zip(names, models):\n     print(f'Fitting {name}.')\n     model.fit(X, y)\n     preds.update({name: np.exp(model.predict(T))})","4da87739":"y_p = (.50*preds['ridge'] + \n       .20*preds['elast'] + \n       .20*preds['linrg'] + \n       .05*preds['lasso'] +\n       .05*preds['xgbst'])\n\nprint(y_p)","33a1556d":"output = pd.DataFrame({'Id': T.index,\n                       'SalePrice': y_p})\noutput.to_csv('submission.csv', index=False)","681666b5":"# Feature Enginering II","bdcdfd28":"The `Utilities` column has all entries equal except for one, and one missing entry. Such a column can only pick up noice.","27bcd244":"# Feature Engineering\n\nThere are a number of features which fits better either aggregated or as indicator columns. There are a few existing columns which are aggregates. Remeber to _replace_ rather than _addding_ columns, to avoid overfitting. \n\nThere is a large number of categorical columns with lopsided distributions. Categories with few entries may contribute (especiall if one-hot encoding is used) to overfitting. We will merge similar categories in many columns to avoid this problem.\n\nThe column `YearRemodAdd` has an intriguing property: no houses has a value smaller than `1950`, and many houses has exactly this value. The natural interpretation is that `1950` has been used as a placeholder for a missing value. We adjust by replacing each instance of `1950` by the `YearBuilt` value for the item.\n\nThe columns `Condition1` and `Condition2` are somewhat special. These columns encode approximity to railroads (negative), larger roads (negative), and parks (positive). If the house is in the approximity of _two_ entities, then there are values in both columns. We will create two calculated columns, one which approximates the total positive value, and one which approximates the negative value.","ed7a2703":"# Encoding\nMany of the categorical variables are ordinal. If you are unsure about the meaning of some abbreviations, then the meta-data is available in the documentation. Since the standard label encoder does not respect ordinal categorical variables, we will encode them manually.","e9b8a014":"# Imputation\n\nThere are two types of missing data. The first type is honestly missing data points. We impute either the mean or the most frequent of the values appearing in the training data, possibly after filtering on some characteristic feature. For example, the `MSZoning` column has 4 missing values. The value `RL` (Residenial Low Density) is the most common value in the training dataset, by quite a wide margin. However, three of the missing values belong to the `IDOTRR` neighborhood, where there is only `RM` (Residential Medium Density) and `C` (Commercial) zones. Accordingly, we impute with `RM`, the most common zoning in the `IDOTRR` neighborhood.\n\nThe columns `LotFrontage` will be imputed with the median value from the given neighborhood. ","3486fd19":"In general, the model will score better if trained without outliers. There are more datapoints which could be considered outliers than the ones we have discarded here. We used only visual inspection to identify the,","a511ad7b":"## Intro\n\nDo not underestimate how much of the work for a good score that lies in proper data cleaning, imputation, transformation, and encoding. In terms of machine learning models, if the data is handled properly, then standard linear models will score very well.\n\n## Setup","8745a284":"## Model\n\nA shoutout to [Alex Lekov](https:\/\/www.kaggle.com\/itslek\/stack-blend-lrs-xgb-lgb-house-prices-k-v17) from who I got some inspiration.","bc46f6ef":"Blending the best perfoming models. "}}