{"cell_type":{"852e775b":"code","1122d4c1":"code","15cd5c3e":"code","d855fdef":"code","af9467d6":"code","88829d7e":"code","3abeed86":"code","428101bc":"code","71120fac":"code","07dec639":"code","f456ce49":"code","51da8c7d":"code","e3379fe3":"code","b8c7bcf1":"code","bc53d076":"code","d1ef41fa":"code","cd120a23":"code","dd9370f8":"code","12a5ea6a":"code","54a2062c":"code","e3878b1b":"code","51ef7112":"code","7a4b06b3":"code","52cc209a":"code","d8ebe6e7":"code","6358d025":"code","186e77c7":"code","1d13fb28":"code","71b355e2":"code","dcd2ad76":"code","69c51cc7":"code","0006cc9f":"code","de0933bb":"code","ff41bbd2":"code","b2ae64bb":"code","c57b020f":"code","958dbd7b":"code","61d72331":"markdown","292bb3b3":"markdown","ffbbb4c4":"markdown","a7f4420b":"markdown","5c93013b":"markdown","42e778b1":"markdown","19552a5d":"markdown","8ca35457":"markdown","ab1dbf90":"markdown","eb827f26":"markdown","5911ce74":"markdown","ac05dda5":"markdown","964847d4":"markdown","368a8e56":"markdown","ac92192b":"markdown","d62b359f":"markdown","37c4e218":"markdown","fd268054":"markdown","fb11227d":"markdown","7ec374ad":"markdown","bed9f976":"markdown","d5e2c55f":"markdown","55910562":"markdown","0318f472":"markdown","84cafa83":"markdown","1ef426f1":"markdown","d6d331e6":"markdown"},"source":{"852e775b":"import seaborn as sns\nimport warnings","1122d4c1":"sns.set(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans- serif\", font_scale=1, color_codes=True)\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import f1_score \nfrom sklearn.utils import resample\nfrom xgboost import XGBClassifier\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn import tree\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np","15cd5c3e":"data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndata.head()","d855fdef":"data.shape","af9467d6":"data.info()","88829d7e":"print(data.isnull().sum())\n\nplt.figure(figsize=(15,6))\nsns.heatmap(data.isnull(),cmap=\"viridis\")","3abeed86":"data_1 = data.copy()","428101bc":"print(\"Unique values are : {}\".format(data_1[\"quality\"].unique()))\ndata_1[\"quality\"].value_counts()","71120fac":"for i,j in tqdm(enumerate(data_1[\"quality\"])):\n    if j > 6.5 :\n        data_1[\"quality\"].replace([j],[\"GOOD\"],inplace=True)\n    else:\n        data_1[\"quality\"].replace([j],[\"BAD\"],inplace=True)\n    \nprint(data_1[\"quality\"].unique())  \nprint(data_1[\"quality\"].value_counts())\ndata_1[\"quality\"].head()","07dec639":"plt.figure(figsize=(18,6))\nsns.countplot(data=data_1,x=data_1[\"quality\"])","f456ce49":"data_1.columns","51da8c7d":"sns.pairplot(data=data_1, hue=\"quality\", palette=\"husl\", markers=[\"o\", \"s\"])","e3379fe3":"data_1.corr()","b8c7bcf1":"# Compute the correlation matrix\ncorr = data_1.corr()\nplt.figure(figsize=(15, 18))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\nplt.tight_layout()\nplt.show()","bc53d076":"data_1.describe()","d1ef41fa":"a = data_1[\"quality\"].value_counts()\nplt.figure(figsize=(10,6))\nplt.pie(a,labels=a.index,startangle=90,autopct='%1.1f%%',explode = (0, 0.1))\nplt.show()","cd120a23":"def dist(a):\n    plt.figure(figsize=(16,4))\n    sns.distplot(data_1[a],kde=False,color=\"r\")\n    plt.title(\"Original\")\n    plt.show()\n    plt.figure(figsize=(16,4))\n    sns.distplot(data_1[a],kde=False,fit=norm,color=\"g\")\n    plt.title(\"Normalized\")\n    plt.show()\ncc = data_1.columns\nfor i,x in  enumerate(cc[0:len(cc)-1]):\n    y = i\n    dist(cc[y])","dd9370f8":"def Box(a):\n    plt.figure(figsize=(16,4))\n    sns.boxplot(x=data_1[a],hue=data_1[\"quality\"])\n    plt.title(\"with all qualities\")\n    plt.show()\n\n    plt.figure(figsize=(16,4))\n    sns.boxplot(x=a,y=\"quality\",data=data_1,linewidth=2.5,palette=\"Set1\")\n    plt.title(\"With Quality Distribution\")\n    plt.show()\n\n    plt.figure(figsize=(16,4))\n    sns.boxplot(x=a,y=\"quality\",data=data_1,linewidth=2.5,palette=\"Set2\")\n    sns.swarmplot(x=a,y=\"quality\",data=data_1,color=\"0.1\")\n    plt.title(\"Boxplot with Swarmplot\")\n    plt.show()\ncc = data_1.columns\nfor i,j in enumerate(cc[0:len(cc)-1]):\n    Box(cc[i])","12a5ea6a":"def facet(a):\n\n  facet = sns.FacetGrid(data_1,hue=\"quality\",aspect=4)\n  facet.map(sns.kdeplot,a,shade=True)\n  facet.add_legend()\n  plt.title(a)\n    \ncc = data_1.columns\nfor i,x in  enumerate(cc[0:len(cc)-1]):\n    y = i\n    facet(cc[y])","54a2062c":"plt.figure(figsize=(16,7))\nsns.scatterplot(data=data_1,x=\"alcohol\",y=\"sulphates\",hue=\"quality\",size=\"citric acid\",palette=\"Set2\")\nplt.show()","e3878b1b":"plt.figure(figsize=(16,7))\nsns.scatterplot(data=data_1,y=\"alcohol\",x=\"pH\",hue=\"quality\",size=\"volatile acidity\",palette=\"Set2\")\nplt.legend(loc=\"best\")\nplt.show()","51ef7112":"X = data_1.drop([\"quality\"],axis=1)\nY = data_1[\"quality\"]\nbestfeatures2 = ExtraTreesClassifier()\nfit2 = bestfeatures2.fit(X,Y)\nfeat_importances = pd.Series(fit2.feature_importances_, index=X.columns)\nplt.figure(figsize = (16,5))\nfeat_importances.plot(kind='barh')\nplt.title(\"Importance of features\")\nplt.show()\n","7a4b06b3":"col = data_1.columns\nDict = {}\nfor i in tqdm(col):\n    a = data_1[i].unique()\n    b = len(a)\n    Dict[i] = b\nprint(Dict)    \nprint(min(Dict.values()))    ","52cc209a":"## UPsampling\ndata_1_minority = data_1[data_1.quality == \"GOOD\"] \ndata_1_majority = data_1[data_1.quality == \"BAD\"] \n\n\nUpsample = resample(data_1_minority,replace=True,n_samples=1382,random_state=123)\nUpsample.quality.value_counts()\nresultant_data_UP = pd.concat([data_1_majority,Upsample],axis=0)\n\nsns.countplot(x=\"quality\",data=resultant_data_UP)\nplt.show()","d8ebe6e7":"## DOWNsampling\ndata_1_minority = data_1[data_1.quality == \"GOOD\"] \ndata_1_majority = data_1[data_1.quality == \"BAD\"] \n\n\ndownsample = resample(data_1_majority,replace=True,n_samples=217,random_state=123)\ndownsample.quality.value_counts()\nresultant_data_DOWN = pd.concat([data_1_minority,downsample],axis=0)\n\nsns.countplot(x=\"quality\",data=resultant_data_DOWN)\nplt.show()","6358d025":"def outlier(df,columns):\n    for i in tqdm(columns[0:len(columns)-1]):\n        quartile_1,quartile_3 = np.percentile(df[i],[25,75])\n        quartile_f,quartile_l = np.percentile(df[i],[1,99])\n        IQR = quartile_3-quartile_1\n        lower_bound = quartile_1 - (1.5*IQR)\n        upper_bound = quartile_3 + (1.5*IQR)\n        print(i,lower_bound,upper_bound,quartile_f,quartile_l)\n                \n        df[i].loc[df[i] < lower_bound] = quartile_f\n        df[i].loc[df[i] > upper_bound] = quartile_l\nnum_col = data_1.columns       \noutlier(data_1,num_col)","186e77c7":"data_1 = data_1.apply(LabelEncoder().fit_transform)\ndata_1.head()","1d13fb28":"X = data_1.drop([\"quality\"],axis=1)\nY = data_1[\"quality\"]\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\nprint(\"shapes of data :\\n{}\\n{}\\n{}\\n{}\".format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","71b355e2":"print(\"y_test :\\n{}\\ny_train :\\n{}\".format(y_test.value_counts(),y_train.value_counts()))","dcd2ad76":"parameters = {'max_depth':range(3,20),\"criterion\":['gini', 'entropy']}\nclf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4)\nclf.fit(X=X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) \n\naa = tree_model.predict(X_test)\nprint(accuracy_score(aa,y_test))\nprint(classification_report(aa,y_test))\nprint(confusion_matrix(aa,y_test))\nprint(\"roc auc score : \",roc_auc_score(aa,y_test))","69c51cc7":"X_train_new = np.array(X_train)\ny_train_new = np.array(y_train)\n\ndata_y, data_yhat = list(), list()\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=True)\n\n# enumerate splits\nfor train_ix, test_ix in tqdm(kfold.split(X_train_new,y_train_new)):\n    # get data\n    train_X, test_X = X_train_new[train_ix], X_train_new[test_ix]\n    train_y, test_y = y_train_new[train_ix], y_train_new[test_ix]\n    # fit model\n    model = DecisionTreeClassifier(criterion= 'gini', max_depth=4)\n    model.fit(train_X, train_y)\n    # make predictions\n    yhat = model.predict(test_X)\n    # store\n    data_y.extend(test_y)\n    data_yhat.extend(yhat)\n    # evaluate the model\n    acc = accuracy_score(data_y, data_yhat)\n    print('Accuracy of validation set(Unseen data): %.3f' % (acc))\n    import pickle\n    Pkl_filename = \"DTK_model.pkl\"\n    with open(Pkl_filename, 'wb') as file:\n      pickle.dump(model, file)\n\n\n# Load from file\nwith open(Pkl_filename, 'rb') as file:\n    pickle_model = pickle.load(file)\n    \npre =  pickle_model.predict(X_test)    \n\nprint(\"Report of X_test:\\nAccuracy : {}\\nroc_auc_score : {}\\nConfusion_matrix :\\n{}\\nClassification_report :\\n {}\".format(accuracy_score(pre,y_test),roc_auc_score(pre,y_test),confusion_matrix(pre,y_test),classification_report(pre,y_test)))\n    \n    ","0006cc9f":"params = {\"learning_rate\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\"max_depth\":[1,2,3,4,5,6,8,9,10],\"min_child_weight\":[1,2,3,4,5,6,7,8,9],\"gamma\":[0.0,0.1,0.2,0.3,0.4,0.5],\"colsample_bytree\":[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\"n_estimators\":[100,200,300,400,500]}\nclassifier = XGBClassifier()\nrandom_search = RandomizedSearchCV(classifier,param_distributions=params,n_iter=10,scoring=\"roc_auc\",n_jobs=-1,cv=5,verbose=3)\nrandom_search.fit(X_train,y_train)\nrandom_search.best_estimator_\n\nXGB = random_search.best_estimator_\nXGB_fit = XGB.fit(X_train,y_train)\nXGB_pred = XGB_fit.predict(X_test)\nXGB_pred_train = XGB_fit.predict(X_train)\nprint(\"Training set accuracy : {}\\nConfusion matrix :\\n {}\\nFull Report :\\n{}\\nroc_auc_score : {}\".format(accuracy_score(XGB_pred_train,y_train),confusion_matrix(XGB_pred_train,y_train),classification_report(XGB_pred_train,y_train),roc_auc_score(XGB_pred_train,y_train)))\nprint(\"Testing set accuracy : {}\\nConfusion matrix :\\n {}\\nFull Report :\\n{}\\nroc_auc_score : {}\".format(accuracy_score(XGB_pred,y_test),confusion_matrix(XGB_pred,y_test),classification_report(XGB_pred,y_test),roc_auc_score(XGB_pred,y_test)))\n","de0933bb":"parameters = {\"learning_rate\":[0.01,0.1,0.05],\"max_depth\":[2,3,4,5],\"min_child_weight\":[1,2,3,4],\"gamma\":[0.0,0.1,0.05],\"colsample_bytree\":[0.3,0.4,0.5],\"n_estimators\":[100,200,300]}\nxgb_model = XGBClassifier()\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=True)\n\nclf_g = GridSearchCV(xgb_model, parameters, n_jobs=5, cv=kfold,  scoring='roc_auc',verbose=2, refit=True)\nclf_g.fit(X_train, y_train)\n\n\nXGB_predg = clf_g.predict(X_test)\nXGB_pred_traing = clf_g.predict(X_train)\nprint(\"Training set accuracy : {}\\nConfusion matrix :\\n {}\\nFull Report :\\n{}\\nroc_auc_score : {}\".format(accuracy_score(XGB_pred_traing,y_train),confusion_matrix(XGB_pred_traing,y_train),classification_report(XGB_pred_traing,y_train),roc_auc_score(XGB_pred_traing,y_train)))\nprint(\"Testing set accuracy : {}\\nConfusion matrix :\\n {}\\nFull Report :\\n{}\\nroc_auc_score : {}\".format(accuracy_score(XGB_predg,y_test),confusion_matrix(XGB_predg,y_test),classification_report(XGB_predg,y_test),roc_auc_score(XGB_predg,y_test)))\n    \n    ","ff41bbd2":"Best_roc_Score  = [roc_auc_score(XGB_predg,y_test),roc_auc_score(XGB_pred,y_test),roc_auc_score(pre,y_test),roc_auc_score(aa,y_test)]\nBest_acuuracy_Score = [accuracy_score(XGB_predg,y_test),accuracy_score(XGB_pred,y_test),accuracy_score(pre,y_test),accuracy_score(aa,y_test)]\nmodel_name = [\"XGBOOST+Gs+Skfold\",\"XGBOOST+RSCV\",\"DecisionTre+Skfold\",\"DecisionTree+Gridsearch\"]\nprint(Best_roc_Score)\nprint(Best_acuuracy_Score)                                                                                                                                   ","b2ae64bb":"plt.figure(figsize=(15,6))\nsns.pointplot(x=model_name,y=Best_roc_Score,color=\"r\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Algorithm name\")\nplt.ylabel(\"Roc auc score\")\nplt.show()\n\nplt.figure(figsize=(15,6))\nsns.pointplot(x=model_name,y=Best_acuuracy_Score,color=\"g\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Algorithm name\")\nplt.ylabel(\"Accuracy score\")\nplt.show()","c57b020f":"Best_roc_Score  = [roc_auc_score(XGB_predg,y_test),roc_auc_score(XGB_pred,y_test),roc_auc_score(pre,y_test),roc_auc_score(aa,y_test)]\nModel_name = [clf_g,XGB_fit,pickle_model,tree_model]\np = max(Best_roc_Score)\nfor i,j in enumerate(Best_roc_Score):\n   if j == p :\n      print(\"The Best model is :\\n {} \".format(Model_name[i]))\n      print(\"Predictions with Best Model :\")\n      Best_predictions = Model_name[i].predict(X_test)\n      print(Best_predictions)\n    \n      print(\"roc_auc_score is :\",roc_auc_score(Best_predictions,y_test))\n      print(\"accuracy_score is : \",accuracy_score(Best_predictions,y_test))\n      print(\"Confusion_matrix is :\\n {}\".format(confusion_matrix(Best_predictions,y_test)))\n      print(\"classification_report is :\\n {}\".format(classification_report(Best_predictions,y_test)))\n      \n      ########### ROC curve   ######\n      fpr, tpr, thresholds = roc_curve(y_test,Best_predictions)\n      roc_auc = roc_auc_score(y_test,Best_predictions)\n      plt.figure(figsize  = (15,6))\n      plt.plot(fpr, tpr, label = 'Sensitivity = %0.3f' % roc_auc)\n      plt.plot([0, 1], [0, 1],'r--')\n      plt.xlabel('FALSE POSITIVE RATE')\n      plt.ylabel('TRUE POSITIVE RATE')\n      plt.title('ROC curve for test data')\n      plt.legend(loc=\"lower Right\")\n      plt.show()\n      \n      ########### ROC  AUC curve   ######\n      # generate a no skill prediction (majority class)\n      ns_probs = [0 for _ in range(len(y_test))]\n\n      # predict probabilities\n      lr_probs = Model_name[i].predict_proba(X_test)\n\n      # keep probabilities for the positive outcome only\n      lr_probs = lr_probs[:, 1]\n      # calculate scores\n      ns_auc = roc_auc_score(y_test, ns_probs)\n      lr_auc = roc_auc_score(y_test, lr_probs)\n\n      # summarize scores\n      print('No Skill: ROC AUC=%.3f' % (ns_auc))\n      print('XGBOOST: ROC AUC=%.3f' % (lr_auc))\n\n      # calculate roc curves\n      ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n      lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n      plt.figure(figsize = (15,6))\n      plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n      plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n      plt.xlabel('False Positive Rate')\n      plt.ylabel('True Positive Rate')\n      plt.legend()\n      plt.title(\"ROC_AUC curve for test data\")\n      plt.show()    \n      \n      # Precision Recall curve\n      lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n      lr_f1, lr_auc = f1_score(y_test, Best_predictions), auc(lr_recall, lr_precision)\n      no_skill = len(y_test[y_test==1]) \/ len(y_test)\n      # summarize scores\n      print('XGBOOST: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n      plt.figure(figsize=(15,6))\n      plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n      plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n      plt.xlabel('Recall')\n      plt.ylabel('Precision')\n      plt.legend()\n      plt.show()\n      ","958dbd7b":"Comparison_data = pd.DataFrame(y_test)\nComparison_data['Predicted quality'] = Best_predictions\nprint(Comparison_data.head())\nComparison_data.to_csv(\"Output_Comparison.csv\",index=False)","61d72331":"Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of a model. The data features that we use to train our machine learning models have a huge influence on the performance of a model.\n\nFeature importance gives us a score for each feature of the data, the higher the score more important or relevant is the feature towards our output variable.","292bb3b3":"![image.png](attachment:image.png)","ffbbb4c4":"# <a id='dgbs'>9.2.Decision Tree with Stratified K_fold <\/a>","a7f4420b":"# <a id='xgg'>9.4.XGBOOST+Stratified K_fold+GridSearchCV <\/a>","5c93013b":"# <a id='up'>5. UPsampling<\/a>","42e778b1":"# <a id='EDA'>2. Basic EDA<\/a>\n","19552a5d":"  **Here I am converting quality measurments into 0,1 Format.**\n* If value of quality is more than 6.5 Then quality of wine will be assuming as a Good quality wine.\n* If value of quality is less than 6.5 Then quality of wine will be assuming as a Bad quality wine.","8ca35457":"# **What is ROC and AUC Curve ?**","ab1dbf90":"# <a id='od'>7. Outliers Detector<\/a>","eb827f26":"**This neat and clean Heatmap says that there is No null values present in the given Dataset.**","5911ce74":"# <a id='xgr'>9.3.XGBOOST with RandomSearchCV<\/a>","ac05dda5":"* Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.\n* Larger values on the y-axis of the plot indicate higher true positives and lower false negatives.\n\nIf you are confused, remember, when we predict a binary outcome, it is either a correct prediction (true positive) or not (false positive). There is a tension between these options, the same with true negative and false negative.\n\nA skilful model will assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average. This is what we mean when we say that the model has skill. Generally, skilful models are represented by curves that bow up to the top left of the plot.\n\nA no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases. A model with no skill is represented at the point (0.5, 0.5). A model with no skill at each threshold is represented by a diagonal line from the bottom left of the plot to the top right and has an AUC of 0.5.\n\nA model with perfect skill is represented at a point (0,1). A model with perfect skill is represented by a line that travels from the bottom left of the plot to the top left and then across the top to the top right.\n\nAn operator may plot the ROC curve for the final model and choose a threshold that gives a desirable balance between the false positives and false negatives.","964847d4":"# <a id='viz'>3. Visualizaions<\/a>","368a8e56":"#  <a id='dgb'>9.1. Decision Tree with GridSearchCV<\/a>","ac92192b":"# <a id='down'>6. DOWNsampling<\/a>","d62b359f":"# **Please Do give an Upvote If you liked the notebook.**","37c4e218":"# <a id='mb'>9. Model Building<\/a>","fd268054":"**The ratio of target value is 86.4:13.6.\nSo, We can say that data is Highly Imbalanced. Which might leads to the problem of Overfitting.**","fb11227d":"# <a id='data'>1. Libraries<\/a>","7ec374ad":"# <a id='fi'>4. Feature Importance<\/a>","bed9f976":"![image.png](attachment:image.png)","d5e2c55f":"# <a id='le'>8. Label Encoding<\/a>","55910562":"# <a id='pwbb'>10. Predictions with best model <\/a>","0318f472":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#data\" role=\"tab\" aria-controls=\"profile\">Libraries<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>                                                                                             <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#EDA\" role=\"tab\" aria-controls=\"messages\">Basic EDA<span\nclass=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#viz\" role=\"tab\" aria-controls=\"profile\">Visualizations<span\nclass=\"badge badge-primary badge-pill\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fi\" role=\"tab\" aria-controls=\"profile\">Feature Importance<span\nclass=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#up\" role=\"tab\" aria-controls=\"profile\">UPsampling<span\nclass=\"badge badge-primary badge-pill\">5<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#down\" role=\"tab\" aria-controls=\"profile\">Downsampling<span\nclass=\"badge badge-primary badge-pill\">6<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#od\" role=\"tab\" aria-controls=\"profile\">Outliers Detector<span\nclass=\"badge badge-primary badge-pill\">7<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#le\" role=\"tab\" aria-controls=\"profile\">Label Encoding<span\nclass=\"badge badge-primary badge-pill\">8<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#mb\" role=\"tab\" aria-controls=\"profile\">Model Building<span\nclass=\"badge badge-primary badge-pill\">9<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#dgb\" role=\"tab\" aria-controls=\"profile\">Decision Tree with GridSearchCV<span\nclass=\"badge badge-primary badge-pill\">9.1<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#dgbs\" role=\"tab\" aria-controls=\"profile\">Decision Tree with Staratified K_fold<span\nclass=\"badge badge-primary badge-pill\">9.2<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#xgr\" role=\"tab\" aria-controls=\"profile\">XGBOOST + RandomSearchCV<span\nclass=\"badge badge-primary badge-pill\">9.3<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#xgg\" role=\"tab\" aria-controls=\"profile\">XGBOOST + Stratified K_Fold + GridSearchCV<span\nclass=\"badge badge-primary badge-pill\">9.4<\/span><\/a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#pwbb\" role=\"tab\" aria-controls=\"profile\">Predictions with best model<span\nclass=\"badge badge-primary badge-pill\">10<\/span><\/a>  ","84cafa83":"#                         **WINE QUALITY ANALYSIS**","1ef426f1":"![image.png](attachment:image.png)[](http:\/\/)","d6d331e6":"**Outliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.**"}}