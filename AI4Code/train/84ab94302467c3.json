{"cell_type":{"5305eb05":"code","70872c2b":"code","c23f5cba":"code","905237cd":"code","b9dc8111":"code","642978e1":"code","d78d154c":"code","69e8ea05":"code","7d051e0a":"code","b3d6c5dc":"code","56bfe3c7":"code","af04b36d":"code","0d6b4207":"code","5427a55b":"code","d64fcb3a":"code","e7a65b5e":"code","7d6c9593":"code","3cd76bb5":"code","8a66a697":"code","8793f145":"code","9fb20d0f":"code","f6627e05":"code","08630885":"code","ac8aec21":"code","110c39e6":"code","e921528e":"code","6ca163fa":"code","8f30073b":"code","1c6bf24c":"code","6429ea4e":"code","c4f55e15":"code","53444567":"code","4279a1fe":"code","e86d6edc":"code","4e106c83":"code","7d039b44":"code","15f9745c":"code","04adfb18":"code","926979dd":"code","260475f2":"code","78e5e273":"code","98aba070":"code","2c4cba9a":"code","e62c5520":"code","dd26fd67":"code","457a04e2":"code","f218dd0a":"code","25fba7e3":"code","53382715":"code","0123a67b":"code","9937cd1c":"code","d3ee4a21":"code","b75eaf2a":"code","93f7ae06":"code","e2157879":"code","19c35a7e":"markdown","e890d288":"markdown","66c44503":"markdown","414332af":"markdown","b10451cf":"markdown","519dd22d":"markdown","b0cb2366":"markdown","393f68cb":"markdown","4558e6d0":"markdown","1628fcfd":"markdown","39f57fbf":"markdown","82640cb4":"markdown","9a988ef5":"markdown","a0712e2c":"markdown","998d3c0c":"markdown","46333cf2":"markdown"},"source":{"5305eb05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70872c2b":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification","c23f5cba":"df = pd.read_csv(r\"..\/input\/winequalityred\/winequality-red.csv\")\ndf.head()","905237cd":"df.shape","b9dc8111":"df.columns","642978e1":"df.info()","d78d154c":"msno.matrix(df)  # just to visualize. no missing values.","69e8ea05":"df.describe()","7d051e0a":"sns.factorplot(data=df,kind='box')","b3d6c5dc":"fig,axes = plt.subplots(5,5)\ncolumns=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol', 'quality']\nfor i in range (5):\n    for j in range (5):\n        axes[i,j].hist(x=columns[i+j],data=df,edgecolor='#000000',linewidth=2,color='#ff4125')\n        axes[i,j].set_title('Variation of '+columns[i+j])\nfig=plt.gcf()\nfig.set_size_inches(18,18)\nfig.tight_layout()\n","56bfe3c7":"corr_mat=df.corr()\nmask=np.array(corr_mat)\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=corr_mat,annot=True,cbar=True,square=True)","af04b36d":"def plot(feature_x,target='quality'):\n    sns.factorplot(x=target,y=feature_x,data=df,kind='bar',size=5,aspect=1)\n    sns.factorplot(x=target,y=feature_x,data=df,kind='violin',size=5,aspect=1)\n    sns.factorplot(x=target,y=feature_x,data=df,kind='swarm',size=5,aspect=1)","0d6b4207":"# for fixed acidity.\nplot('fixed acidity','quality')","5427a55b":"# for alcohol.\nplot('alcohol','quality')","d64fcb3a":"lb = LabelEncoder()","e7a65b5e":"df['quality']=lb.fit_transform(df['quality'])","7d6c9593":"x_train,x_test,y_train,y_test = train_test_split(df.drop('quality',axis=1),df['quality'],test_size=0.25,random_state=42)","3cd76bb5":"models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n       DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\n\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd\n","8a66a697":"acc_frame=pd.DataFrame(d)\nacc_frame","8793f145":"sns.barplot(x='Modelling Algo', y='Accuracy',data=acc_frame)","9fb20d0f":"sns.factorplot(x='Modelling Algo',y='Accuracy',data=acc_frame,kind='point',size=4,aspect=3.5)","f6627e05":"def func(x_train,x_test,y_train,y_test,name_scaler):\n    models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\n    acc_sc=[]\n    for model in range(len(models)):\n        clf=models[model]\n        clf.fit(x_train,y_train)\n        pred=clf.predict(x_test)\n        acc_sc.append(accuracy_score(pred,y_test))\n        \n    acc_frame[name_scaler]=np.array(acc_sc)","08630885":"scalers= [MinMaxScaler(),StandardScaler()]\nnames=['Acc_Min_Max_Scaler','Acc_Standard_Scaler']\nfor scale in range(len(scalers)):\n    scaler=scalers[scale]\n    scaler.fit(df)\n    scaled_df=scaler.transform(df)\n    X=scaled_df[:,0:11]\n    Y=df['quality']\n    x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)\n    func(x_train,x_test,y_train,y_test,names[scale])","ac8aec21":"acc_frame","110c39e6":"# just to visualize the accuracies.\nsns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)","e921528e":"sns.barplot(y='Modelling Algo',x='Acc_Min_Max_Scaler',data=acc_frame)","6ca163fa":"sns.barplot(y='Modelling Algo',x='Acc_Standard_Scaler',data=acc_frame)","8f30073b":"# preparing the features by using a StandardScaler as it gave better resluts.\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(df)\nX=scaled_df[:,0:11]\nY=df['quality']\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","1c6bf24c":"params_dict={'C':[0.001,0.01,0.1,1,10,100,1000], 'penalty':['l1','l2']}\nmodel=GridSearchCV(estimator=LogisticRegression(),param_grid=params_dict,scoring='accuracy',cv=5)\nmodel.fit(x_train,y_train)","6429ea4e":"model.best_params_","c4f55e15":"model.best_score_","53444567":"pred=model.predict(x_test)\naccuracy_score(pred,y_test)","4279a1fe":"l=[i+1 for i in range(50)]\nparams_dict={'n_neighbors':l,'n_jobs':[-1]}\nclf_knn=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_knn.fit(x_train,y_train)","e86d6edc":"clf_knn.best_params_","4e106c83":"clf_knn.best_score_","7d039b44":"pred=clf_knn.predict(x_test)\naccuracy_score(pred,y_test)","15f9745c":"params_dict={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100],'kernel':['linear','rbf']}\nclf_svc=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_svc.fit(x_train,y_train)","04adfb18":"clf_svc.best_score_","926979dd":"clf_svc.best_params_","260475f2":"# now tuning finally around these values of C and gamma and the kernel for \n#further increasing the accuracy.\n\nparams_dict={'C':[0.90,0.92,0.96,0.98,1.0,1.2,1.5],'gamma':[0.90,0.92,0.96,0.98,1.0,1.2,1.5],'kernel':['linear','rbf']}\nclf_svm=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf_svm.fit(x_train,y_train)","78e5e273":"clf_svm.best_score_","98aba070":"clf_svm.best_params_","2c4cba9a":"pred=clf_svm.predict(x_test)\naccuracy_score(pred,y_test)","e62c5520":"params_dict={'n_estimators':[500],'max_features':['auto','sqrt','log2']}\nclf_rf= GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1),param_grid=params_dict,\n                     scoring='accuracy',cv=10)\nclf_rf.fit(x_train,y_train)","dd26fd67":"clf_rf.best_score_","457a04e2":"clf_rf.best_params_","f218dd0a":"pred=clf_rf.predict(x_test)\naccuracy_score(pred,y_test)","25fba7e3":"clf_gb=GridSearchCV(estimator=GradientBoostingClassifier(),cv=10,param_grid=\n                    dict({'n_estimators':[500]}))\nclf_gb.fit(x_train,y_train)","53382715":"clf_gb.best_score_","0123a67b":"clf_gb.best_params_","9937cd1c":"pred=clf_rf.predict(x_test)\naccuracy_score(pred,y_test)","d3ee4a21":"clf_dt=GridSearchCV(estimator=DecisionTreeClassifier(),\n                    scoring='accuracy',cv=10,param_grid=dict({'max_depth':[3]}))\nclf_dt.fit(x_train,y_train)","b75eaf2a":"clf_dt.best_score_","93f7ae06":"clf_dt.best_params_","e2157879":"pred=clf_dt.predict(x_test)\naccuracy_score(pred,y_test)\n","19c35a7e":"### NOW WE CAN VISUALIZE HOW QUALITY(ie Target) VARIES WITH DIFFERENT NUMERIC FEATURES.","e890d288":"## HENCE GIVEN PARAMETER TUNING USING GRID SEARCH, Random Forest and Gradient Boosting  ALGO GIVES APPROX THE SAME HIGHEST ACCURACY OF 70%","66c44503":"## **PARAMETER TUNING USING GridSearchCV.","414332af":"## Now we can move to Univariate Analysis","b10451cf":"## HENCE TILL NOW THE BEST ACCURACY IS GIVEN BY SVM WITH rbf KERNEL WITH C=10 and gamma=0.98 .","519dd22d":"## 3. SUPPORT VECTOR MACHINE (SVM)","b0cb2366":"### similarly for other variables.","393f68cb":"## NOTE THAT THIS IS WITHOUT FEATURE SCALING. NOW SINCE FEATURES HAVE DIFFERENT SCALES LET US TRY TO DO FEATURE SCALING AND SEE THE IMPACT.","4558e6d0":"## 4. **RANDOM FOREST**","1628fcfd":" ## 2.**KNN**","39f57fbf":" ## 1. **LOGISTIC REGRESSION**","82640cb4":"## 5.**GRADIENT BOOSTING**","9a988ef5":"### NOW THIS CLEARLY SHOWS THE ACCUARCIES OF DIFFERENT MODELLING ALGOS ON USING DIFFERENT SCALERS.\n\n1. Note that here the accuracies increase marginally on scaling.\n\n2. Also for this data, StandardScaling seems to give slightly better results than the MinMaxScaling.\n\n3. For some modelling algos there is a considerable increase in accuracies upon scaling the features like SVM, KNN wheras for others there isn't a considerable increase in accuracies upon scaling.","a0712e2c":"## INFERENCES FROM THE ABOVE HEAT MAP--\n\n1. The quality of wine is highly related to volatile acidity.\n2. Also the quality of wine is highly corelated to alcohol\n3. pH and citric acid\/ fixed acidity are highly inversely related as all of us know that acids have smaller pH values.\n4. Self Relation ie of a fetaure to itself is 1 as expected.\n5. Some other similar inferences can be drawn.","998d3c0c":"## 6. **DECISION TREE**","46333cf2":"# *** *An Upvote is a sign of Appreciation And Motivation. Please consider Upvoting if you find useful.* ***"}}