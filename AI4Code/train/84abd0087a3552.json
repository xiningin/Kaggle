{"cell_type":{"590a3352":"code","75c86424":"code","a78e29b6":"code","4db9770f":"code","acd90f23":"code","7e611773":"code","61eb20f8":"code","3d0bcf82":"code","357ec48e":"code","977ee73b":"code","5f6cd522":"code","fc1c09f3":"code","e54bfcda":"code","6c8be5f2":"code","d832ad0b":"code","43a319e9":"code","b550fa77":"code","ec823cb5":"code","68e421b2":"code","e834b6d3":"code","440ed1e1":"code","18c6aece":"code","49861844":"code","2b2ac50d":"code","f36a625d":"code","545cb52c":"code","4476ffbb":"code","edb73074":"code","83c3681d":"code","edd21215":"code","04f34393":"code","5e3ab97f":"code","d2ead6c3":"code","b24675e6":"code","e69994d1":"code","4209b054":"code","482cbd69":"code","63f5189c":"code","18efd568":"code","223959b6":"code","4792dd72":"markdown","26ef6423":"markdown","f589e6e2":"markdown","16929313":"markdown","2c616791":"markdown","1a7aa2c6":"markdown","9209f3ca":"markdown","ee29c3ab":"markdown","752c9322":"markdown","fa8b3da0":"markdown","92c32a3c":"markdown","cefd0a98":"markdown","2d7beab2":"markdown","442001ac":"markdown","4e482c73":"markdown","a2e6e0d1":"markdown","1a12b2db":"markdown","9c7347ac":"markdown","d291808c":"markdown","88b3f342":"markdown"},"source":{"590a3352":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75c86424":"from wordcloud  import WordCloud,STOPWORDS\nfrom nltk.corpus import stopwords \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk as nlt","a78e29b6":"test_data = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')","4db9770f":"train_data.head(10)","acd90f23":"test_data.head(10)","7e611773":"train_data['Premise_Length'] = train_data['premise'].apply(lambda x: len(x))\ntrain_data['Hypothesis_Length'] = train_data['hypothesis'].apply(lambda x: len(x))\ntest_data['Premise_Length'] = test_data['premise'].apply(lambda x: len(x))\ntest_data['Hypothesis_Length'] = test_data['hypothesis'].apply(lambda x: len(x))\n","61eb20f8":"#We will create a dictionary for our language colum so we can reparse our encoding if needed\nlangs = train_data['language'].value_counts().to_frame().index.to_list()\nlang_dic = {langs[i]:i for i in range(0,len(langs))}\ntrain_data['language'] = train_data['language'].replace(lang_dic)\ntest_data['language'] = test_data['language'].replace(lang_dic)","3d0bcf82":"#We will add a feature wich will represent the amount of similar tokens between  the premise and the hypothesis\ntokenizer = nlt.RegexpTokenizer(r\"\\w+\")\ndef similar_tokens(sir):\n    tokens_p = tokenizer.tokenize(sir[0])\n    tokens_h = tokenizer.tokenize(sir[1])\n    tokens_p = set(tokens_p)\n    tokens_h = set(tokens_h)\n    return len(tokens_p.intersection(tokens_h))\ndef dissimilar_tokens(sir):\n    tokens_p = tokenizer.tokenize(sir[0])\n    tokens_h = tokenizer.tokenize(sir[1])\n    tokens_p = set(tokens_p)\n    tokens_h = set(tokens_h)\n    total = len(tokens_p)+len(tokens_h)\n    return total - len(tokens_p.intersection(tokens_h))\ndef num_of_words(sir,unique = 0):\n    tokens_p = tokenizer.tokenize(sir)\n    if unique:\n        tokens_p = set(tokens_p)\n    return len(tokens_p)\ndef average_word_length(sir):\n    tokens_p = tokenizer.tokenize(sir)\n    lengths = [len(word) for word in tokens_p]\n    avg_len = np.array(lengths).sum()\/len(lengths)\n    return avg_len\n    \n\ntrain_data['Similar_Tokens#'] = train_data[['premise','hypothesis']].apply(similar_tokens,axis=1) \ntrain_data['Dissimilar_Tokens#'] = train_data[['premise','hypothesis']].apply(dissimilar_tokens,axis=1) \ntrain_data['premise_#_of_words'] = train_data['premise'].apply(num_of_words) \ntrain_data['premise_#_of_unique_words'] = train_data['premise'].apply(num_of_words,unique=1) \ntrain_data['hypothesis_#_of_words'] = train_data['hypothesis'].apply(num_of_words) \ntrain_data['hypothesis_#_of_unique_words'] = train_data['hypothesis'].apply(num_of_words,unique=1) \ntrain_data['hypothesis_avg_word_length'] = train_data['hypothesis'].apply(average_word_length)\ntrain_data['premise_avg_word_length'] = train_data['premise'].apply(average_word_length) \n\ntest_data['Similar_Tokens#'] = test_data[['premise','hypothesis']].apply(similar_tokens,axis=1) \ntest_data['Dissimilar_Tokens#'] = test_data[['premise','hypothesis']].apply(dissimilar_tokens,axis=1) \ntest_data['premise_#_of_words'] = test_data['premise'].apply(num_of_words) \ntest_data['premise_#_of_unique_words'] = test_data['premise'].apply(num_of_words,unique=1) \ntest_data['hypothesis_#_of_words'] = test_data['hypothesis'].apply(num_of_words) \ntest_data['hypothesis_#_of_unique_words'] = test_data['hypothesis'].apply(num_of_words,unique=1) \ntest_data['hypothesis_avg_word_length'] = test_data['hypothesis'].apply(average_word_length)\ntest_data['premise_avg_word_length'] = test_data['premise'].apply(average_word_length) ","357ec48e":"train_data","977ee73b":"#Also I Want to have the most common token in each label\ndef keep_track(my_dict,key):\n    if key in my_dict:\n        my_dict[key] += 1\n    else:\n        my_dict[key] = 1\n    return my_dict\n\n\n#the new insight we will try to extract\nmost_common_premise_hypothesis_token_0_label = []\nmost_common_premise_hypothesis_token_1_label = []\nmost_common_premise_hypothesis_token_2_label = []\nmost_common_premise_hypothesis =  [most_common_premise_hypothesis_token_0_label,most_common_premise_hypothesis_token_1_label,most_common_premise_hypothesis_token_2_label]\n\nfor label in range (0,3):\n    label_data = train_data[train_data['label']==label]\n    for i in range(0,15):\n        if i == 0:\n            stop_words = set(stopwords.words(str.lower(langs[i])))\n            stop_words.add('uh')\n        zero_label_dic = {}\n\n\n        language_labeld = label_data[label_data.language==i]\n\n        for sen in language_labeld['premise']:\n            tokens = tokenizer.tokenize(sen)\n            for token in tokens:\n                zero_label_dic = keep_track(zero_label_dic,token)\n        inverse = [(value, key) for key, value in zero_label_dic.items() if str.lower(key) not in stop_words]\n        most_common_premise_hypothesis[label].append(max(inverse)[1])\n        zero_label_dic={}\n        for sen in language_labeld['hypothesis']:\n            tokens = tokenizer.tokenize(sen)\n            for token in tokens:\n                zero_label_dic = keep_track(zero_label_dic,token)\n        inverse = [(value, key) for key, value in zero_label_dic.items()]\n        inverse.sort()\n        most_common_premise_hypothesis[label].append(max(inverse)[1])\n    \n#================================================================================================================\n","5f6cd522":"#Now That We Have Our Most Common Token For Each Label Lets Create A Boolean Feature That Tells Us Does The Sample Contain The Most Common Word Or Not\nhypothesis_status_column = []\npremise_status_column = []\n\nfor index in range(0,train_data.shape[0]):\n    lang = train_data.iloc[index,4]\n    lbl = train_data.iloc[index,5]\n    if train_data.iloc[index,1].find(most_common_premise_hypothesis[lbl][lang]) != -1:\n        premise_status_column.append(1)\n    else:\n        premise_status_column.append(0)\n    if train_data.iloc[index,2].find(most_common_premise_hypothesis[lbl][lang+1]) != -1:\n        hypothesis_status_column.append(1)\n    else:\n        hypothesis_status_column.append(0)\n\n#mct = most common token    \ntrain_data['premise_contains_mct'] = premise_status_column\ntrain_data['hypothesis_contains_mct'] = hypothesis_status_column\n\n\n\n","fc1c09f3":"train_data","e54bfcda":"train_data.describe()","6c8be5f2":"plt.figure(figsize=(20,11))\nsns.set_style('darkgrid')\nax =sns.countplot(train_data.language)\nax.set_xticklabels(labels=langs)\nax.set_title(\"Counts Of Different Languages In Our Data\")\nplt.show()","d832ad0b":"plt.figure(figsize=(20,11))\nax =sns.countplot(train_data.label)\nax.set_title(\"Counts Of Different Labels In Our Data\")\nplt.show()","43a319e9":"plt.figure(figsize=(20,11))\nax =sns.scatterplot(x=train_data['Premise_Length'],y=train_data['Hypothesis_Length'],size=train_data['label'],palette='Blues',hue=train_data['label'])\nax.set_title(\"The Spread Of The Premise And Hypothesis Lengths In Our Data Via Label\")\nplt.show()","b550fa77":"plt.figure(figsize=(20,11))\nax =sns.scatterplot(x=train_data['hypothesis_avg_word_length'],y=train_data['premise_avg_word_length'],size=train_data['label'],palette='Blues',hue=train_data['label'])\nax.set_title(\"The Spread Of The Premise And Hypothesis Average Word Lengths In Our Data Via Label\")\nplt.show()","ec823cb5":"plt.figure(figsize=(20,11))\nax =sns.jointplot(x=train_data['Similar_Tokens#'],y=train_data['label'],cmap='mako',height=12,kind='kde',n_levels=10)\nplt.show()","68e421b2":"plt.figure(figsize=(20,11))\nax =sns.scatterplot(x=train_data['Hypothesis_Length'],y=train_data['Dissimilar_Tokens#'],size=train_data['label'],palette='Blues',hue=train_data['label'])\nax.set_title(\"The Spread Of The Hypothesis Length And Dissimilar Tokens In Our Data Via Label\")\nplt.show()","e834b6d3":"plt.figure(figsize=(20,11))\nax =sns.scatterplot(x=train_data['hypothesis_#_of_words'],y=train_data['Dissimilar_Tokens#'],size=train_data['label'],palette='Blues',hue=train_data['label'])\nax.set_title(\"The Spread Of The Hypothesis Length And Dissimilar Tokens In Our Data Via Label\")\nplt.show()","440ed1e1":"fig = plt.figure(figsize=(20,11))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(train_data['Dissimilar_Tokens#'],train_data['hypothesis_#_of_words'],train_data['Hypothesis_Length'],s=10,cmap='coolwarm',c=train_data['label'])\nax.set_xlabel('Dissimilar_Tokens',fontsize=13)\nax.set_ylabel('hypothesis_#_of_words',fontsize=13)\nax.set_zlabel('Hypothesis_Length',fontsize=13)\nplt.show()","18c6aece":"train_data = train_data[train_data['premise_avg_word_length']<25]\ntrain_data = train_data[train_data['Premise_Length']<400]\ntrain_data = train_data[train_data['Hypothesis_Length']<150]\ntrain_data = train_data[train_data['Dissimilar_Tokens#']<60]\ntrain_data = train_data[train_data['hypothesis_#_of_words']<40]\ntrain_data = train_data[train_data['Dissimilar_Tokens#']<80]\n\n","49861844":"plt.figure(figsize=(20,11))\ncor = train_data.corr('pearson')\nax =sns.heatmap(cor,cmap=\"coolwarm\",annot=True)\nax.set_title(\"Correlations Of Different Features In Our Data\")\nplt.show()","2b2ac50d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as ascore\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom keras.layers import Dense\nfrom keras import Sequential","f36a625d":"target = train_data.pop('label')","545cb52c":"#Our features will be all feature with reasonable correlation\n\n\n\nselector = SelectKBest(chi2,k=6)\nselector.fit(train_data[train_data.columns[4:]],target)\nX = selector.transform(train_data[train_data.columns[4:]])\nselected_features = [train_data.columns[4:][i] for i in range(0,len(train_data.columns[4:])) if selector.get_support()[i] == True]\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,target)\n#train_data\nselected_features","4476ffbb":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = KNeighborsClassifier(n_neighbors = n)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(ascore(pred,test_y))\n    return results","edb73074":"n_list = [10,20,30,50,80,130,210,350,560]\nresult = optimal_n(train_x,test_x,train_y,test_y,n_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(n_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(n_list)\nax.set_title('KNN Accuracy Depending On Number Of Neighbors',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","83c3681d":"def optimal_e(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = RandomForestClassifier(max_leaf_nodes = n,random_state=42)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(ascore(pred,test_y))\n    return results","edd21215":"n_list = [2,3,5,8,13,21,35,56,91,147]\nresult = optimal_e(train_x,test_x,train_y,test_y,n_list)\nplt.figure(figsize=(20,11))\nax = sns.lineplot(x=np.arange(0,10),y=result)\n#n_list.insert(0,1)\nax.set_xticklabels(labels = n_list)\nax.set_title('RandomForest Accuracy Depending On Number Of Estimators',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()\n","04f34393":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = AdaBoostClassifier(n_estimators = n,random_state=42,learning_rate=0.05)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(ascore(pred,test_y))\n    return results","5e3ab97f":"ee_list = [2,3,5,8,13,21,35,56,91,147,300]\nresult = optimal_n(train_x,test_x,train_y,test_y,ee_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(ee_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(labels = ee_list)\nax.set_title('AdaBoost Accuracy Depending On Number Of Max Leaf Nodes',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","d2ead6c3":"def optimal_n(train_x,test_x,train_y,test_y,n_list):\n    results = []\n    for n in n_list:\n        model = DecisionTreeClassifier(max_leaf_nodes = n,random_state=42,criterion='entropy')\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        results.append(ascore(pred,test_y))\n    return results","b24675e6":"ee_list = [2,3,5,8,13,21,35,56,91,147,300]\nresult = optimal_n(train_x,test_x,train_y,test_y,ee_list)\nplt.figure(figsize=(20,11))\nax =sns.lineplot(x=np.arange(len(ee_list)),y=result)\nn_list.insert(0,1)\nax.set_xticklabels(labels = ee_list)\nax.set_title('Decision Tree Accuracy Depending On Number Of Max Leaf Nodes',fontsize=16)\nax.set_xlabel('N Value',fontsize=16)\nax.set_ylabel('Accuracy Score',fontsize=16)\nplt.show()","e69994d1":"model = Sequential()\nmodel.add(Dense(10,activation='sigmoid',input_dim=len(selected_features)))\nmodel.add(Dense(16,activation='tanh'))\nmodel.add(Dense(16,activation='sigmoid'))\nmodel.add(Dense(1,activation='tanh'))\nmodel.compile(optimizer='sgd',loss='categorical_crossentropy',metrics='accuracy')","4209b054":"model.fit(train_x,train_y,epochs=10,verbose=False)","482cbd69":"rf_model = RandomForestClassifier(n_estimators=96,random_state=42)\nADA_model = AdaBoostClassifier(n_estimators=96,random_state=42,learning_rate=0.3)\ndt_model = DecisionTreeClassifier(max_leaf_nodes = 21,random_state=42,criterion='entropy')\n\nX = train_data[selected_features].append(test_data[selected_features])\nrf_model.fit(train_data[selected_features],target)\nADA_model.fit(train_data[selected_features],target)\ndt_model.fit(train_data[selected_features],target)","63f5189c":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\npredictions = rf_model.predict(test_data[selected_features])*0.2 + dt_model.predict(test_data[selected_features])*0.5 + ADA_model.predict(test_data[selected_features])*0.3\npredictions = (np.round(predictions)).astype('int64')\npredictions\n\nsm = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')\nsm = sm['prediction'].to_list()\n\ncf_m = confusion_matrix(sm,predictions)\nplt.figure(figsize=(20,11))\nax = sns.heatmap(cf_m,annot=True,fmt='d')\n","18efd568":"result = pd.DataFrame({'id':test_data['id'].to_list(),'prediction':predictions})","223959b6":"result.to_csv('submission.csv',index=False)","4792dd72":"# So we can see that we have no particularly\u200b high correlation between any of our numeric data features and the target labels except from the Similar_Tokens featuere which holds a negative correlation ","26ef6423":"# So we see that even our neural network cant find an optimal fit for our classification. \n## the model i submit will unfortunately only have an accuracy of 50% at most.","f589e6e2":"## So our random forest model did a tiny bit better then the KNN algorithm but nothing worth the attention yet again we will try another model","16929313":"# AdaBoost Model","2c616791":"# After observing that we have not real strong numeric correlation between our features and our labels any regrresion type algorithms goes of the list and leaves us with the more 'obvious' classification algorithms and approaches ","1a7aa2c6":"## Removing Outliers","9209f3ca":"# ***Lets Start Our EDA And Feature Engineering***\n## *We Will Start With Feature Engineering And Data Preprocessing*\n## First i will add the following features:\n### 1)The lengths of the premise and the hypothesis\n### 2)The number of similar token between the premise and the hypothesis\n### 3)The number of dissimilar token between the premise and the hypothesis\n### 4)The number of words in each premise and the hypothesis\n### 5)Does the premise and the hypothesis contain the most common premise and hypothesis of the relevent language\n### 6)The average word length in each premise and the hypothesis\n### 7)The number of unique words in each premise and the hypothesis\n\n","ee29c3ab":"# *Model Selection And Evaluation*","752c9322":"# We can see that our label distribution is more or less the same across all our data samepls which means our model will have a smaller chance overfitting to a certain label (if all else is done properly)","fa8b3da0":"# Decision Tree Model","92c32a3c":"## Lets split our train data and try it on a couple of models and compare the accuracy of those models,if no accurate model will be assembeld then we will try to construct a neural network.","cefd0a98":"# RandomForest Model","2d7beab2":"# AdaBoost is still around 0.45 lets try another model","442001ac":"## Looks like our decision tree model falls in the same pit as all the other classifiers \n## Lets try and find out will a neural network do better ","4e482c73":"### We can see that our mean length of the Premise is 107 and the mean length of the Hypothesis is 54 also the mean amount of simillar tokens us 3.6 and the SD is 3.17 ~ from here we can derive that the similarity\u200bof the Premise and the  Hypothesis has a fairly normal distribution but we will confirm that in the following analysis steps","a2e6e0d1":"# EDA","1a12b2db":"## We see that our knn model picks at 0.435 lets try out a different model","9c7347ac":"# KNN Model","d291808c":"# Our data is clearly skewed because most of our sampels are based on the english language but we will assume that for our model its less relevent even thoe languages vary in a significant way from one another ","88b3f342":"## The extra challenge\u200b in this situation is that there are stepwords in 15 languages and its not clear in this stage will the new features show any useful trands that are worth investing time in and finding lists of stopwords for all the 15 languages . for now lets see if the noisy feature containing the stopwords will help us predict the label or not. "}}