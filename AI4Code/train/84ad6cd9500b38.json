{"cell_type":{"9ebf31b4":"code","e854ed37":"code","63ba313d":"code","e84f70a6":"code","82d6392f":"code","186b9a6a":"code","434ad824":"code","b69016c2":"code","6f483cf6":"code","b328cf0d":"code","c09d7480":"code","4dc43335":"code","4a88e00e":"code","37a70de6":"code","92bc26c1":"code","1c0c719e":"code","eb169479":"code","8d08d99a":"code","675906e9":"code","425cb2e0":"code","097dd093":"code","5eed1d5b":"code","dea196f1":"code","f49473b6":"code","278f0bed":"code","013f3c2e":"code","477aa33c":"code","d39057d7":"code","2a0c624b":"code","49ca45bb":"code","dc74bb0c":"code","e5d69b28":"code","cd78e880":"code","e98cbef6":"code","18237d04":"code","87d536b9":"code","d10b6767":"code","9ab56157":"code","424c215a":"code","c3f5539d":"code","30da208d":"code","4177baff":"code","3cafe32b":"code","1a2ea9ae":"code","30fa4a8b":"code","eabb632b":"code","1dfe2c09":"code","d53a38da":"code","f658993e":"markdown","03b68e50":"markdown"},"source":{"9ebf31b4":"!nvidia-smi","e854ed37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n","63ba313d":"import os\nimport gc\nimport glob\nimport torch\nfrom tqdm.autonotebook import tqdm\nfrom transformers import *\nfrom transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n","e84f70a6":"from sklearn.model_selection import KFold","82d6392f":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn import functional as F\nfrom torchvision.datasets import MNIST\nfrom torchvision import datasets, transforms\n","186b9a6a":"import logging","434ad824":"logging.info(\"Imports Complete\")","b69016c2":"ROOT_DIR =  \"..\/input\/tweet-sentiment-extraction\/\"","6f483cf6":"train = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\ntest = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\nsub = pd.read_csv(os.path.join(ROOT_DIR, \"sample_submission.csv\"))","b328cf0d":"train.head()","c09d7480":"#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n#model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased')","4dc43335":"!ls ..\/input\/bert-qa\/","4a88e00e":"tokenizer = BertTokenizer.from_pretrained('..\/input\/bert-qa\/tokenizer\/')\n#model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\nmodel = BertForQuestionAnswering.from_pretrained('..\/input\/bert-qa\/\/model\/')","37a70de6":"train.dropna(0, how=\"any\", inplace=True)\ntrain = train.reset_index()","92bc26c1":"train[\"sentiment_q\"]=train[\"sentiment\"].apply(lambda x: \"what text is \"+x+\"?\")\ntest[\"sentiment_q\"]=test[\"sentiment\"].apply(lambda x: \"what text is \"+x+\"?\")","1c0c719e":"train","eb169479":"def get_beg_end(full_str, sub_str):\n        start = full_str.index(sub_str)\n        end = start+len(sub_str)\n        return (start, end)\nvfunc = np.vectorize(get_beg_end)    ","8d08d99a":"train[\"start\"], train[\"end\"] = vfunc(train[\"text\"].values, train[\"selected_text\"].values)","675906e9":"\nclass TextDataset(torch.utils.data.Dataset):\n\n    def __init__(self, data, tokenizer,is_test=False, return_attention_masks=False, max_length=200):\n        self.df = data\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.return_attn_mask = return_attention_masks\n        self.max_len = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        #print(idx)\n        #new_list = [(a, b) for a,b in zip(self.df.loc[idx, \"sentiment_q\"], self.df.loc[idx, \"text\"])]\n        s, t = self.df.loc[idx, \"sentiment_q\"], self.df.loc[idx, \"text\"]\n        #print(s, t)\n        token_ids = self.tokenizer.encode_plus(s, t, add_special_tokens=True,\n                                                     return_attention_mask=self.return_attn_mask,\n                                                     max_length = self.max_len,\n                                                     pad_to_max_length=True\n                                                     )\n        input_ids = torch.tensor(token_ids[\"input_ids\"])\n        attn_mask = torch.tensor(token_ids[\"attention_mask\"])\n\n        #print(torch.tensor(token_ids[\"input_ids\"]).shape)\n        #print(token_ids)\n        if self.is_test:\n            return input_ids, attn_mask\n        return input_ids, attn_mask, self.df.loc[idx, \"start\"], self.df.loc[idx, \"end\"]\n","425cb2e0":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nkf.get_n_splits(train)","097dd093":"max_len = len(max(train[\"text\"].values, key=len))\nprint(max_len+len(\"What text is neutral?\"))","5eed1d5b":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncritation = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')","dea196f1":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\njac_v = np.vectorize(jaccard)","f49473b6":"device = torch.device(\"cuda\")","278f0bed":"model.to(device);","013f3c2e":"train_loss_hist = []\nval_loss_hist = []\ntrn_epoch_loss=[]\nval_epoch_loss =[]","477aa33c":"def train_fn(model, train_loader, optimizer):\n    print(\"training...\")\n    train_loss=0\n    for i, train_batch in tqdm(enumerate(train_loader), total=len(train_set)\/\/batch_size):\n        op = train_batch\n        input_ids = op[0].to(device)\n        attn_masks = op[1].to(device)\n        start_t = op[2].to(device)\n        end_t = op[3].to(device)\n\n        optimizer.zero_grad()\n\n        start_p, end_p = model(input_ids, attention_mask=attn_masks)\n\n        loss = (critation(start_p, start_t) + critation(end_p, end_t))\/2.0\n        train_loss_hist.append(loss.item()\/batch_size)\n        if i%300 ==0:\n            print(f\"[{i}, loss: {loss.item()\/batch_size}]\")\n        \n        loss.backward()\n        optimizer.step\n\n        torch.cuda.empty_cache()\n        gc.collect()\n    return np.mean(train_loss_hist)","d39057d7":"def evaluate(model, valid_loader):\n    val_s=[]\n    val_e=[]\n    with torch.no_grad():\n        for i, valid_batch in tqdm(enumerate(valid_loader), total = len(valid_set)\/\/batch_size):\n            op = valid_batch\n            input_ids = op[0].to(device)\n            attn_masks = op[1].to(device)\n            start_t = op[2].to(device)\n            end_t = op[3].to(device)\n\n            start_p, end_p = model(input_ids, attention_mask=attn_masks)\n            val_s.append(torch.argmax(start_p, 1).cpu())\n            val_e.append(torch.argmax(end_p, 1).cpu())\n\n            loss = (critation(start_p, start_t) + critation(end_p, end_t))\/2.0\n            val_loss_hist.append(loss.item()\/batch_size)\n            if i%100 ==0:\n                print(f\"[{i}, loss: {loss.item()}]\")\n\n        assert torch.cat(val_s).shape[0] == val_.shape[0]\n        val_s = torch.cat(val_s)\n        assert torch.cat(val_e).shape[0] == val_.shape[0]\n        val_e = torch.cat(val_e)\n\n        val_[\"predicted_text\"]=\"\"\n        for num in (range(val_.shape[0])):\n            val_.loc[num, \"predicted_text\"] = val_.loc[num, \"text\"][val_s[num]:val_e[num]]\n        scores = jac_v(val_[\"selected_text\"].values, val_[\"predicted_text\"].values)\n        print(f\"Metric :{scores.mean()}\")\n\n        torch.cuda.empty_cache()\n        gc.collect()\n        return np.mean(val_loss_hist)","2a0c624b":"batch_size = 16","49ca45bb":"test[\"text\"] = test[\"text\"].str.lower()","dc74bb0c":"test_set = TextDataset(test, tokenizer, is_test=True, return_attention_masks=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","e5d69b28":"for fold, (train_index, valid_index) in enumerate(kf.split(train)):\n    print(f\"FOLD: {fold}\")\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(\"TRAIN:\", train_index, \"TEST:\", valid_index)\n    #X_train, X_test = X[train_index], X[test_index]\n    #y_train, y_test = y[train_index], y[test_index]\n    train_, val_ = train.loc[train_index], train.loc[valid_index]\n\n    train_set = TextDataset(train_, tokenizer, is_test=False, return_attention_masks=True, max_length=150)\n    valid_set = TextDataset(val_, tokenizer, is_test=False, return_attention_masks=True, max_length=150)\n    \n\n    train_loader = DataLoader(train_set, batch_size=batch_size)\n    valid_loader = DataLoader(valid_set, batch_size=batch_size)\n    \n'''\n    num_epochs = 1\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch: {epoch}\")\n        model.train()\n        #TRAINING LOOP\n        \n        train_loss = train_fn(model, train_loader, optimizer)\n        print(f\"Training loss: {train_loss}\")\n        trn_epoch_loss.append(train_loss)\n        \n        torch.save(model.state_dict(), \"initial_bert_bkp.pth\")\n        \n        print(\"valid\")\n        # VALIDATION LOOP\n        model.eval()\n        val_loss = evaluate(model, valid_loader)\n        print(f\"valid loss: {val_loss}\")\n        val_epoch_loss.append(val_loss)\n\n        scheduler.step(val_loss)\n\n'''\n","cd78e880":"#torch.save(model.state_dict(), \"\/content\/drive\/My Drive\/kaggle\/twitter_select_text\/initial_bert.pth\")","e98cbef6":"model.eval()\nops_s = []\nops_e = []\nwith torch.no_grad():\n    for i, test_batch in tqdm(enumerate(test_loader), total = len(test_set)\/\/batch_size):\n        op = test_batch\n        input_ids = op[0].to(device)\n        attn_masks = op[1].to(device)\n        \n        start_p, end_p = model(input_ids, attention_mask=attn_masks)\n        ops_s.append(torch.argmax(start_p, 1).cpu())\n        ops_e.append(torch.argmax(end_p, 1).cpu())\n","18237d04":"assert torch.cat(ops_s).shape[0] == test.shape[0]\nops_s = torch.cat(ops_s)\nassert torch.cat(ops_e).shape[0] == test.shape[0]\nops_e = torch.cat(ops_e)","87d536b9":"for num in range(test.shape[0]):\n    test.loc[num, \"selected_text\"] = test.loc[num, \"text\"][ops_s[num]:ops_e[num]]\nprint()","d10b6767":"(ops_s>ops_e).sum()","9ab56157":"idx = test[test[\"selected_text\"]==\"\"].index","424c215a":"test.loc[idx]","c3f5539d":"neutral_idx = test[(test[\"sentiment\"]==\"neutral\") & (test[\"selected_text\"]==\"\")].index","30da208d":"test.loc[neutral_idx, \"selected_text\"] = test.loc[neutral_idx, \"text\"]","4177baff":"non_neutral_idx = test[(test[\"sentiment\"]!=\"neutral\") & (test[\"selected_text\"]==\"\")].index","3cafe32b":"test.loc[non_neutral_idx, \"selected_text\"] = test.loc[non_neutral_idx, \"text\"]","1a2ea9ae":"test.head()","30fa4a8b":"submit = test[[\"textID\", \"selected_text\"]]","eabb632b":"submit","1dfe2c09":"submit.to_csv(\"submission.csv\", index=False)","d53a38da":"reopen=pd.read_csv(\"submission.csv\")","f658993e":"Hope Test Data will also be in the same order ","03b68e50":"Can we reverse map? can we get relevent part in the answer from DistillBERT Qna model? Lets find out"}}