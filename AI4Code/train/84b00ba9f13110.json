{"cell_type":{"44ab13fc":"code","fec47283":"code","dc7af911":"code","a9f4dacb":"code","624061f2":"code","4e384d78":"code","702abafe":"code","5a6c31be":"code","a19c2496":"code","8630465f":"code","10c8225f":"code","20a889f8":"code","4ef0ed33":"code","5987323e":"code","62ef2751":"code","e72e519f":"code","fd5b46bc":"code","5350c103":"code","cb3759b3":"code","03cbe0e6":"code","e80cd947":"code","09f0cb90":"code","ab134dd1":"code","81bd02ae":"code","5a1ecc6d":"code","af345546":"code","49ccacf3":"code","a40dd6ce":"code","4c81f6e3":"code","09371b45":"code","e0f75b42":"code","31a20c36":"code","4cf0e20e":"code","a2cdfae0":"code","b16181a4":"code","76403c1c":"code","1a6fbde8":"code","93f8d247":"code","e426011d":"code","01d2ecec":"code","ed477212":"code","99125005":"code","bb6523c5":"code","8637ce32":"code","4a5aac5c":"code","a1f75df9":"code","8169eb95":"code","17161e4e":"code","3a06595e":"code","6d5b3558":"code","5a513dd3":"code","e83919a0":"code","8988a5ef":"code","0e4316c1":"code","cc78582a":"code","0fd76f24":"code","91ddc2ad":"code","fad0b744":"code","ef29850e":"code","55e94519":"code","7bc61148":"code","ca5451d1":"code","cf9987f7":"code","51f0a7e9":"code","dc94dc21":"code","5edc1741":"code","0ba91839":"code","15620ddc":"code","f49476f0":"code","99572684":"code","3281ecd9":"code","3f0f4928":"code","3c1c57cf":"code","022d529b":"code","b3f5154d":"code","56a32aa6":"markdown","b847d1f9":"markdown","a8086fa5":"markdown","3313b7ab":"markdown","b008b996":"markdown","884fcc3a":"markdown","3c942dda":"markdown","cf29b924":"markdown","ceea4161":"markdown","895691b0":"markdown","10ea36da":"markdown","5dd172cb":"markdown","c3e6baa7":"markdown","bb288ee4":"markdown","26b2295f":"markdown","b0f0daba":"markdown","70a2dbfd":"markdown","ab439bb1":"markdown","3b8dbe38":"markdown"},"source":{"44ab13fc":"import copy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom mpl_toolkits import mplot3d\nfrom scipy import stats\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nimport pandas_profiling as pp\n\n# models\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, SGDRegressor, RidgeCV\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nimport sklearn.model_selection\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom scipy.stats import pearsonr\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fec47283":"features = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'street', 'city', 'statezip', 'country']","dc7af911":"train0 = pd.read_csv('..\/input\/housedata\/data.csv')\ntrain0 = train0[features]\ntrain0.head(5)","a9f4dacb":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train0.columns.values.tolist()\nfor col in features:\n    if train0[col].dtype in numerics: continue\n    categorical_columns.append(col)\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train0.columns:\n        le = LabelEncoder()\n        le.fit(list(train0[col].astype(str).values))\n        train0[col] = le.transform(list(train0[col].astype(str).values))","624061f2":"train0['price'] = (train0['price']).astype(int)\ntrain0['floors'] = (train0['floors']).astype(int)\ntrain0['bedrooms'] = (train0['bedrooms']).astype(int)","4e384d78":"valid_part = 0.3\npd.set_option('max_columns',100)","702abafe":"train0 = train0.dropna()\ntrain0.head(5)","5a6c31be":"train0.info()","a19c2496":"train0.corr()","8630465f":"# Thanks to: https:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices\nmask = np.zeros_like(train0[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(train0[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});","10c8225f":"sns.distplot(train0['yr_built'])","20a889f8":"train0.describe(percentiles=[.01, .02, .03, .05, .1, .5, .9, .92, .93, .99])","4ef0ed33":"train0 = train0[(\n                (train0['price'] <= 1000000) & \n                (train0['price'] > 150000) & \n                (train0['bathrooms'] <= 6) & \n                (train0['condition'] > 2) & \n                (train0['sqft_living'] > 800) & \n                (train0['bedrooms'] >= 1) & \n                (train0['bedrooms'] <= 4.5) \n                )]","5987323e":"train0.info()","62ef2751":"pp.ProfileReport(train0)","e72e519f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    \n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplotting_3_chart(train0, 'price')","fd5b46bc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ny = np.array(train0.price)\nplt.subplot(131)\nplt.plot(range(len(y)),y,'.');plt.ylabel('price');plt.xlabel('index');\nplt.subplot(132)\nsns.boxplot(y=train0.price)","5350c103":"#Thanks to https:\/\/towardsdatascience.com\/an-easy-introduction-to-3d-plotting-with-matplotlib-801561999725\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection=\"3d\")\n\nz_points = train0['price']\nx_points = train0['condition']\ny_points = train0['yr_built']\nax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');\n\nax.set_xlabel('condition')\nax.set_ylabel('yr_built')\nax.set_zlabel('price')\n\nplt.show()","cb3759b3":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\nfig=plt.figure(figsize=(19,12.5))\nax=fig.add_subplot(2,2,2, projection=\"3d\")\nax.scatter(train0['view'],train0['condition'],train0['yr_built'],c=\"red\",alpha=.5)\nax.set(xlabel='\\nView',ylabel='\\nCondition',zlabel='\\nyr_built')\nax.set(ylim=[0,12])\nplt.show()","03cbe0e6":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\nfig=plt.figure(figsize=(19,12.5))\nax=fig.add_subplot(2,2,2, projection=\"3d\")\nax.scatter(train0['floors'],train0['bedrooms'],train0['sqft_living'],c=\"darkgreen\",alpha=.5)\nax.set(xlabel='\\nFloors',ylabel='\\nBedrooms',zlabel='\\nsqft Living')\nax.set(ylim=[0,12])\nplt.show()","e80cd947":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\ngrpby_bedrooms_df = train0[[\"price\", \"bedrooms\"]].groupby(by = \"bedrooms\", as_index = False)\ngrpby_bedrooms_df = grpby_bedrooms_df.mean().astype(int)\ngrpby_bedrooms_df.head()","09f0cb90":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\nax1.set(yscale = \"log\")\nsns.stripplot(x = \"bedrooms\", y = \"price\", data = train0, ax = ax1, jitter=True, palette=\"Blues_d\")\nsns.barplot(x = \"bedrooms\", y = \"price\", data = grpby_bedrooms_df, ax = ax2, palette=\"Blues_d\")\nplt.show()","ab134dd1":"# Clone data for FE \ntrain_fe = copy.deepcopy(train0)\ntarget_fe = train_fe['price']\ndel train_fe['price']","81bd02ae":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nX = train_fe\nz = target_fe","5a1ecc6d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","af345546":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","49ccacf3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","a40dd6ce":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","4c81f6e3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","09371b45":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","e0f75b42":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","31a20c36":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","4cf0e20e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Standardization for regression model\ntrain_fe = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","a2cdfae0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","b16181a4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","76403c1c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","1a6fbde8":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","93f8d247":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to: https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","e426011d":"feature_score.sort_values('mean', ascending=False)","01d2ecec":"# Thanks to: Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","ed477212":"feature_score.sort_values('total', ascending=False)","99125005":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ntarget_name = 'price'\ntrain_target0 = train0[target_name]\ntrain0 = train0.drop([target_name], axis=1)","bb6523c5":"# Synthesis test0 from train0\ntrain0, test0, train_target0, test_target0 = train_test_split(train0, train_target0, test_size=0.2, random_state=0)","8637ce32":"# For boosting model\ntrain0b = train0\ntrain_target0b = train_target0\n# Synthesis valid as test for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, train_target0b, test_size=valid_part, random_state=0)","4a5aac5c":"#For models from Sklearn\nscaler = StandardScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)","a1f75df9":"train0.head(3)","8169eb95":"len(train0)","17161e4e":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, train_target0, test_size=valid_part, random_state=0)","3a06595e":"train.head(3)","6d5b3558":"test.head(3)","5a513dd3":"train.info()","e83919a0":"test.info()","8988a5ef":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nacc_train_r2 = []\nacc_test_r2 = []\nacc_train_d = []\nacc_test_d = []\nacc_train_rmse = []\nacc_test_rmse = []","0e4316c1":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","cc78582a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_boosting_model(num,model,train,test,num_iteration=0):\n    # Calculation of accuracy of boosting model by different metrics\n    \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    if num_iteration > 0:\n        ytrain = model.predict(train, num_iteration = num_iteration)  \n        ytest = model.predict(test, num_iteration = num_iteration)\n    else:\n        ytrain = model.predict(train)  \n        ytest = model.predict(test)\n\n    print('target = ', targetb[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(targetb, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(targetb, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(targetb, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_testb[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_testb, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_testb, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_testb, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","0fd76f24":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_model(num,model,train,test):\n    # Calculation of accuracy of model from Sklearn by different metrics   \n  \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    ytrain = model.predict(train)  \n    ytest = model.predict(test)\n\n    print('target = ', target[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(target, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(target, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(target, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_test[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_test, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_test, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_test, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","91ddc2ad":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Random Forest\n\n#random_forest = GridSearchCV(estimator=RandomForestRegressor(), param_grid={'n_estimators': [100, 1000]}, cv=5)\nrandom_forest = RandomForestRegressor()\nrandom_forest.fit(train, target)\nacc_model(1,random_forest,train,test)","fad0b744":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nxgb_clf = xgb.XGBRegressor({'objective': 'reg:squarederror'}) \nparameters = {'n_estimators': [60, 100, 120, 140], \n              'learning_rate': [0.01, 0.1],\n              'max_depth': [5, 7],\n              'reg_lambda': [0.5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1).fit(trainb, targetb)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_boosting_model(2,xgb_reg,trainb,testb)","ef29850e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","55e94519":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': False,\n        'seed':0,        \n    }\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=10000,\n                   early_stopping_rounds=8000,verbose_eval=500, valid_sets=valid_set)","7bc61148":"acc_boosting_model(3,modelL,trainb,testb,modelL.best_iteration)","ca5451d1":"fig =  plt.figure(figsize = (5,5))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","cf9987f7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nmodels = pd.DataFrame({\n    'Model': ['Random Forest', 'XGB', 'LGBM'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })","51f0a7e9":"pd.options.display.float_format = '{:,.2f}'.format","dc94dc21":"print('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)","5edc1741":"print('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)","0ba91839":"print('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)","15620ddc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 3 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","f49476f0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 3 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","99572684":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 3 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","3281ecd9":"test0.info()","3f0f4928":"test0.head(3)","3c1c57cf":"#For models from Sklearn\ntestn = pd.DataFrame(scaler.transform(test0), columns = test0.columns)","022d529b":"# LGB Regression model for basic train\nlgb_predict = modelL.predict(test0)\nlgb_predict[:3]","b3f5154d":"# Thanks to: https:\/\/www.kaggle.com\/dnzcihan\/house-sales-prediction-and-eda\nfinal_df = test_target0.values\nfinal_df = pd.DataFrame(final_df,columns=['Real_price'])\nfinal_df['predicted_prices'] = lgb_predict.astype(int)\nfinal_df['difference'] = abs(final_df['Real_price'] - final_df['predicted_prices']).astype(int)\nfinal_df.head(20)","56a32aa6":"<a class=\"anchor\" id=\"4.2\"><\/a>\n### 4.2 XGB\n##### [Back to Table of Contents](#0.1)","b847d1f9":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","a8086fa5":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [EDA](#3)\n4. [FE: building the feature importance diagrams](#4)\n  -  [LGBM](#4.1)\n  -  [XGB](#4.2)\n  -  [Logistic Regression](#4.3)\n  -  [Linear Reagression](#4.4)\n5. [Comparison of the all feature importance diagrams](#5)\n6. [Dada for modeling](#6)\n7. [Preparing to modeling](#7)\n8. [Tuning models](#8)\n  -  [Random Forest](#8.1)\n  -  [XGB](#8.2)\n  -  [LGBM](#8.3)\n9. [Models comparison](#9)\n10. [Prediction](#10)","3313b7ab":"<a class=\"anchor\" id=\"6\"><\/a>\n## 6. Dada for modeling\n##### [Back to Table of Contents](#0.1)","b008b996":"## Acknowledgements\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\\\n   - https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\n   - https:\/\/www.kaggle.com\/fulrose\/copy-fix-house-price-prediction-1-2\n   - https:\/\/www.kaggle.com\/sid321axn\/house-price-prediction-gboosting-adaboost-etc\n   - https:\/\/www.kaggle.com\/dnzcihan\/house-sales-prediction-and-eda","884fcc3a":"<a class=\"anchor\" id=\"1\"><\/a>\n## 1. Import libraries \n##### [Back to Table of Contents](#0.1)","3c942dda":"<a class=\"anchor\" id=\"4.4\"><\/a>\n### 4.4 Linear Regression\n##### [Back to Table of Contents](#0.1)","cf29b924":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. Comparison of the all feature importance diagrams \n##### [Back to Table of Contents](#0.1)","ceea4161":"<a class=\"anchor\" id=\"8.2\"><\/a>\n### 8.2 XGB\n##### [Back to Table of Contents](#0.1)","895691b0":"<a class=\"anchor\" id=\"8\"><\/a>\n## 8. Tuning models\n##### [Back to Table of Contents](#0.1)","10ea36da":"<a class=\"anchor\" id=\"8.3\"><\/a>\n### 8.3 LGBM\n##### [Back to Table of Contents](#0.1)","5dd172cb":"<a class=\"anchor\" id=\"8.1\"><\/a>\n### 8.1 Random Forest\n##### [Back to Table of Contents](#0.1)","c3e6baa7":"<a class=\"anchor\" id=\"2\"><\/a>\n## 2. Download datasets\n##### [Back to Table of Contents](#0.1)","bb288ee4":"<a class=\"anchor\" id=\"10\"><\/a>\n### 10 Prediction\n##### [Back to Table of Contents](#0.1)","26b2295f":"<a class=\"anchor\" id=\"7\"><\/a>\n## 7. Preparing to modeling\n##### [Back to Table of Contents](#0.1)","b0f0daba":"<a class=\"anchor\" id=\"3\"><\/a>\n## 3. EDA\n##### [Back to Table of Contents](#0.1)","70a2dbfd":"<a class=\"anchor\" id=\"9\"><\/a>\n### 9 Models comparison\n##### [Back to Table of Contents](#0.1)","ab439bb1":"<a class=\"anchor\" id=\"4.1\"><\/a>\n### 4.1 LGBM ","3b8dbe38":"<a class=\"anchor\" id=\"4.3\"><\/a>\n### 4.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)"}}