{"cell_type":{"e795a45e":"code","2237ced2":"code","2c5fc241":"code","698f4163":"code","cd6bf9fa":"code","8bbe39c1":"code","861d0849":"code","175d5b7d":"code","3797a5ae":"code","c2cc095c":"code","b9302528":"code","cc5aeb7a":"code","26abd511":"code","fbfe5363":"code","6b32070f":"code","fb025ea7":"code","285a3e21":"code","b7133e83":"code","dc924a4c":"code","dd97a8a7":"markdown","62427393":"markdown","5f2e6de9":"markdown","8a3ee36b":"markdown","8fb9be40":"markdown","1448725a":"markdown","a5a9e97a":"markdown","5d254036":"markdown","53c8a970":"markdown","c0a602c4":"markdown","0048268b":"markdown","ef84f27e":"markdown","437cc8b0":"markdown"},"source":{"e795a45e":"!pip install -qU '..\/input\/libraries\/pytorch_lightning-1.5.0rc1-py3-none-any.whl'","2237ced2":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","2c5fc241":"# Helper libraries\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport collections\nimport multiprocessing\nfrom pathlib import Path\nimport warnings\nfrom argparse import Namespace\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection\nfrom collections import OrderedDict\n\n#Pytorch, transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n\n#Import pytorch lightning: \nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","698f4163":"train_df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntest_df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nexternal_mlqa = pd.read_csv('..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('..\/input\/mlqa-hindi-processed\/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad], ignore_index=True)\n\ndel external_mlqa, external_xquad\ngc.collect()","cd6bf9fa":"def optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value","8bbe39c1":"config = Namespace(\n    seed = 7,\n    \n    trainer = Namespace(\n        precision = 16,\n        accumulate_grad_batches = 2,\n        max_epochs = 5,\n        weights_summary='top',\n        num_sanity_val_steps = 0,\n        gpus = 1,\n    ),\n    \n    model = Namespace(\n        model_name_or_path = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\/\",\n        config_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\/\",\n        optimizer_type = 'AdamW',\n        learning_rate = 3e-5,\n        weight_decay = 1e-2,\n        epsilon = 1e-8,\n        max_grad_norm = 1.0,\n        decay_name = 'linear-warmup',\n        warmup_ratio = 0.1,\n    ),\n    \n    data = Namespace(\n        train_batch_size = 4,\n        eval_batch_size = 8,\n        tokenizer_name = \"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-base-squad2\/\",\n        max_seq_length = 384,\n        doc_stride = 128,\n        valid_split = 0.25,\n    ),\n)","861d0849":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        res = {\n            'input_ids': torch.tensor(feature['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(feature['attention_mask'], dtype=torch.long),\n        }\n        if self.mode == 'train':\n            res.update({\n                'start_position': torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position': torch.tensor(feature['end_position'], dtype=torch.long)\n            })\n        else:\n            res.update({\n                'id': feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            })\n            \n        return res","175d5b7d":"class DataModuleFit(pl.LightningDataModule):\n    def __init__(self, df=None, **kwargs):\n        super().__init__()\n        self.save_hyperparameters(ignore=['df'])\n        self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n        features = []\n        for i, offsets in enumerate(offset_mapping):\n            feature = {}\n\n            input_ids = tokenized_example[\"input_ids\"][i]\n            attention_mask = tokenized_example[\"attention_mask\"][i]\n\n            feature['input_ids'] = input_ids\n            feature['attention_mask'] = attention_mask\n            feature['offset_mapping'] = offsets\n\n            cls_index = input_ids.index(self._tokenizer.cls_token_id)\n            sequence_ids = tokenized_example.sequence_ids(i)\n\n            sample_index = sample_mapping[i]\n            answers = example[\"answers\"]\n\n            if len(answers[\"answer_start\"]) == 0:\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                token_start_index = 0\n                while sequence_ids[token_start_index] != 1:\n                    token_start_index += 1\n\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != 1:\n                    token_end_index -= 1\n\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    feature[\"start_position\"] = cls_index\n                    feature[\"end_position\"] = cls_index\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    feature[\"start_position\"] = token_start_index - 1\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    feature[\"end_position\"] = token_end_index + 1\n\n            features.append(feature)\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        self.df = self.df.sample(frac=1.)\n        train_split = int((1 - self.hparams.valid_split) * df.shape[0])\n        train_set = df.iloc[:train_split]\n        valid_set = df.iloc[train_split:]\n\n        train_features, valid_features = [[] for _ in range(2)]\n        for _, row in train_set.iterrows():\n            train_features += self._prepare_features(row)\n        for _, row in valid_set.iterrows():\n            valid_features += self._prepare_features(row)\n\n        self._train_features = train_features\n        self._valid_features = valid_features\n        \n    def setup(self, stage = None):\n        self._train_dset = DatasetRetriever(self._train_features)\n        self._valid_dset = DatasetRetriever(self._valid_features)\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self._train_dset,\n            batch_size=self.hparams.train_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._valid_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False,\n    )\n    \nclass DataModulePredict(DataModuleFit):\n    def __init__(self, df, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters(ignore=['df'])\n        self.df = df\n        \n    def _prepare_features(self, example):\n        example[\"question\"] = example[\"question\"].lstrip()\n\n        tokenized_example = self._tokenizer(\n            example[\"question\"],\n            example[\"context\"],\n            truncation=\"only_second\",\n            max_length=self.hparams.max_seq_length,\n            stride=self.hparams.doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        features = []\n        for i in range(len(tokenized_example[\"input_ids\"])):\n            feature = {}\n            feature[\"example_id\"] = example['id']\n            feature['context'] = example['context']\n            feature['question'] = example['question']\n            feature['input_ids'] = tokenized_example['input_ids'][i]\n            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n            cls_index = feature['input_ids'].index(self._tokenizer.cls_token_id)\n            feature['cls_index'] = cls_index\n            features.append(feature)\n\n        return features\n        \n    def prepare_data(self):\n        self._tokenizer = AutoTokenizer.from_pretrained(self.hparams.tokenizer_name)\n        \n        pred_features = []\n\n        for _, row in self.df.iterrows():\n            pred_features += self._prepare_features(row)\n\n        self.pred_features = pred_features\n        \n    def setup(self, stage = None):\n        self._pred_dset = DatasetRetriever(self.pred_features, mode='predict')\n    \n    def predict_dataloader(self):\n        return DataLoader(\n            self._pred_dset,\n            batch_size=self.hparams.eval_batch_size,\n            num_workers=optimal_num_of_loader_workers(),\n            pin_memory=True,\n            drop_last=False,\n            shuffle=False\n        )    ","3797a5ae":"class Model(pl.LightningModule):\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model_config = AutoConfig.from_pretrained(self.hparams.config_name)\n        self.model = AutoModel.from_pretrained(self.hparams.model_name_or_path, config=self.model_config)\n        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"The forward step performs the next step for the model while training.\"\"\"\n        sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]\n        qa_logits = self.qa_outputs(sequence_output)\n\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": self.hparams.weight_decay\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": 0.0\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.hparams.learning_rate,\n            eps = self.hparams.epsilon,\n            correct_bias=True\n        )\n\n        # Defining LR Scheduler\n        num_training_steps = len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n        num_warmup_steps = num_training_steps * self.hparams.warmup_ratio\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n        )\n\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n    \n    def _compute_loss(self, preds, labels):\n        start_preds, end_preds = preds\n        start_labels, end_labels = labels\n        start_loss = F.cross_entropy(start_preds, start_labels, ignore_index=-1)\n        end_loss = F.cross_entropy(end_preds, end_labels, ignore_index=-1)\n        total_loss = (start_loss + end_loss) \/ 2\n        return total_loss\n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = self._compute_loss((outputs_start, outputs_end), (targets_start, targets_end))\n        self.log('val_loss', loss, prog_bar=True)\n\n    def predict_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        pred_start, pred_end = self(input_ids, attention_mask=attention_mask)\n        return {\n            'pred_start': pred_start,\n            'pred_end': pred_end,\n        }","c2cc095c":"pl.seed_everything(config.seed)","b9302528":"df = pd.concat([train_df, external_train], ignore_index=True)\ndf['answers'] = df[['answer_start', 'answer_text']].apply(lambda x: {'answer_start': [x[0]], 'text': [x[1]]}, axis=1)\n\ntest_df['context'] = test_df['context'].apply(lambda x: ' '.join(x.split()))\ntest_df['question'] = test_df['question'].apply(lambda x: ' '.join(x.split()))\n\ndel train_df, external_train\ngc.collect()","cc5aeb7a":"# initiate callbacks\nlr_monitor = LearningRateMonitor(logging_interval='step')\nlogger = CSVLogger(save_dir='logs\/')\n# Checkpoint\nckpt = ModelCheckpoint(\n    monitor=f'val_loss',\n    save_top_k=1,\n    save_last=False,\n    save_weights_only=True,\n    dirpath='checkpoints',\n    filename='{epoch:02d}-{val_loss:.4f}',\n    verbose=False,\n    mode='min',\n)","26abd511":"model = Model(**vars(config.model))\ndm = DataModuleFit(df, **vars(config.data))\n\ntrainer = pl.Trainer(\n    logger=logger,\n    callbacks=[ckpt, lr_monitor],\n    **vars(config.trainer)\n)\ntrainer.fit(model, datamodule=dm)\n\ntorch.cuda.empty_cache()\ndel trainer, model, dm\ngc.collect()","fbfe5363":"dm = DataModulePredict(test_df, **vars(config.data))\n\nsub_pred_start = None\nsub_pred_end = None\npred_features = None\n\ntrainer = pl.Trainer(\n    enable_checkpointing=False,\n    **vars(config.trainer),\n)\nmodel = Model.load_from_checkpoint(ckpt.best_model_path, **vars(config.model))\npreds = trainer.predict(model, datamodule=dm)\n\nsub_pred_start = np.vstack([x['pred_start'] for x in preds])\nsub_pred_end = np.vstack([x['pred_end'] for x in preds])    \ngc.collect()","6b32070f":"def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = []\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"cls_index\"]\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        predictions.append((example[\"id\"], best_answer[\"text\"]))\n        \n    return predictions","fb025ea7":"processed_preds = postprocess_qa_predictions(test_df, dm.pred_features, (sub_pred_start, sub_pred_end))\nsub_df = pd.DataFrame(processed_preds, columns=['id', 'PredictionString'])\nsub_df = sub_df.merge(test_df, how='left', on='id')","285a3e21":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\ntamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\ntamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\ntamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\nhindi_ad = \"\u0908\"\nhindi_bc = \"\u0908.\u092a\u0942\"\n\n\ncleaned_preds = []\nfor _, (pred, context) in sub_df[[\"PredictionString\", \"context\"]].iterrows():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    if pred.endswith(\"...\"):\n            pred = pred[:-3]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n        \n    cleaned_preds.append(pred)\n\nsub_df[\"PredictionString\"] = cleaned_preds\nsub_df = sub_df[['id', 'PredictionString']]","b7133e83":"sub_df.to_csv('submission.csv', header=True, index=False)","dc924a4c":"sub_df.head()","dd97a8a7":"# Loading data","62427393":"# Dataset class","5f2e6de9":"# Defining the `LightningDataModule` with Pytorch Lightning\n","8a3ee36b":"# Introduction\n\n**THIS is adjusted fork of https:\/\/www.kaggle.com\/hoshi7\/chaii-pytorch-lightining-w-b**\n\nThe model building aspect was taken from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n\nOver the course of the month, I have learnt a lot about transformers and pytorch. During one such lesson, I stumbled across Pytorch-lightning and how it can create a general framework for the pytorch deep learning model that we are building. \nKeeping that in mind, I set across learning about how to structure regular pytorch code into lightning code. This is one such attempt at that, with WanDB to showcase the ML-OPS part of the training. ","8fb9be40":"# Post process the results\nCredits: https:\/\/www.kaggle.com\/kishalmandal\/5-folds-infer-combined-model-0-792\/","1448725a":"Also, over the course of a lot of learning, I realised that most of the tutorials that can be found easily on the web generally use a prepackaged model such as BertSequenceClassification, this made it difficult to figure out where the _forward_ step should go, or where the linear layers should have gone. Overcoming that obstacle was the main challenge of this notebook, along with finding a way to fit the model on Kaggle's less optimal memory provision. \n\nThus, this notebook highlights upon: \n1. Use of custom model (XLM Roberta) with pytorch lightning. \n2. Weights and Biases to showcase the ML-OPS\n\n\n","a5a9e97a":"## Necessary Functions\n\nThe index of necessary functions in order:\n- **optimal_num_of_loader_workers**: Find the optimal number of workers based on config. Code from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit","5d254036":"## Installing and Importing Libraries\n\nInstalling pytorch lightning and torch to a newer version as torch 1.7.0 has an error while training with lightning. ","53c8a970":"# Defining Configuration","c0a602c4":"# Train Model with Pytorch lightning","0048268b":"# Predict with Pytorch lightning","ef84f27e":"# Defining the `LightningModule` with Pytorch Lightning","437cc8b0":"![image.jpg](data:image\/png;base64,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)"}}