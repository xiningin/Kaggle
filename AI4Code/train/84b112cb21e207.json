{"cell_type":{"d99d752e":"code","4abcdc2e":"code","9bde3c1d":"code","ece59ac3":"code","c183e304":"code","a4475a4d":"code","3d3e26ac":"code","c0e0eb8d":"code","4dbfaa28":"code","f6decfaa":"code","cb7648f6":"code","1092b31e":"code","67a2526b":"code","11d9c09f":"code","3387b776":"code","faf2d5b1":"code","9b08e75a":"code","8338904c":"code","567d8ba2":"code","c2ac410f":"code","61e282d1":"code","79ccf473":"code","6594dd05":"code","471e770f":"code","d7b1a383":"code","e591059d":"code","863d8401":"code","943e9f7b":"code","7d284ce5":"code","f79aa9b4":"code","596d8bf7":"code","01e92b9d":"code","d39486b7":"code","27b25948":"code","040b7ac2":"code","b4497a53":"code","6a60f9be":"code","6769867a":"code","40bb6f16":"code","c52b1ca3":"code","1dd98a36":"code","7f1157af":"code","e3c4ebea":"code","24b99255":"code","4e1ceaba":"code","bf1c1620":"code","b23b09b7":"code","d2b7ec26":"code","312409f4":"code","4654ae36":"code","a362fe6f":"code","437dedd8":"code","af92b4d4":"code","704a892e":"code","4c15f021":"code","8fd74d15":"code","00a7b23c":"code","f175f6c8":"code","4592d3b8":"code","fbe1e008":"code","de360d38":"code","d8065a6e":"code","fd956e9f":"code","e82b8770":"code","17e08942":"code","e6327085":"code","07d7e30b":"code","ade9e137":"code","ca53c6c0":"code","ffe08fd6":"code","002ae14d":"code","936780a5":"code","3bc5d7a7":"code","82ed3233":"code","fd0e501b":"code","ad7fa4b1":"code","d2a05f5c":"code","38e027e6":"code","2b0ec133":"code","2bccb420":"code","6ed31b1a":"code","a9e47c3f":"code","f4213364":"code","900972a3":"markdown","27b4e95b":"markdown","93e50de9":"markdown","1c5f83ef":"markdown","66f497e1":"markdown","b8796ecf":"markdown","ee56e41d":"markdown","b943ca91":"markdown","4aab6589":"markdown","c9537e39":"markdown","0c74d07f":"markdown","8e71ef87":"markdown","aff767cc":"markdown","bc15fe10":"markdown","7d01f53f":"markdown","78a1d55c":"markdown","df6732cf":"markdown","2ea27708":"markdown","963fbe3e":"markdown","0637decb":"markdown","ad296ec1":"markdown","69025948":"markdown","72f686bb":"markdown","b0ce26fd":"markdown","e9bc4113":"markdown","52918938":"markdown","ffafec74":"markdown","e1ad58cf":"markdown","b775da83":"markdown","a8f10375":"markdown","97ecea69":"markdown","28433bfa":"markdown","9197948c":"markdown","f14711b5":"markdown","02c2ff69":"markdown","70721919":"markdown","af036d58":"markdown","cd182057":"markdown","3e7356fd":"markdown","01a70336":"markdown","c325413c":"markdown","fb9775c8":"markdown","97fe76ed":"markdown","ca0368fc":"markdown","e54e0e06":"markdown","aa18f778":"markdown","09866bb1":"markdown","16326b23":"markdown","a4439987":"markdown","1491726e":"markdown","6475afe4":"markdown","8827a367":"markdown","4d846cb9":"markdown","c6e3d6f4":"markdown","ddb1f797":"markdown","fd919fa1":"markdown","d12b6cf2":"markdown","13206211":"markdown","84737a54":"markdown","8e3f88b7":"markdown","9e3fd305":"markdown","46acb170":"markdown","d6f57cfd":"markdown","43a63042":"markdown","c4dcb5d7":"markdown","d1303fee":"markdown","36cedfba":"markdown","b96e2701":"markdown","e85260f6":"markdown","4a2d2392":"markdown","dde543d4":"markdown","4cbe0096":"markdown","6e961a8b":"markdown"},"source":{"d99d752e":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nfrom statistics import mode\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","4abcdc2e":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","9bde3c1d":"target = train.Survived","ece59ac3":"train.head()","c183e304":"print(f'Unique Values in Pclass :{train.Pclass.unique()}')","a4475a4d":"print(f'Unique Values in SibSp :{train.SibSp.unique()}')","3d3e26ac":"print(f'Unique Values in Embarked :{train.Embarked.unique()}')","c0e0eb8d":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train.Survived)\nplt.title('Number of passenger Survived');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train)\nplt.title('Number of passenger Survived');","4dbfaa28":"plt.style.use('seaborn')\nplt.figure(figsize=(10,5))\nsns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')\nplt.title('Null Values in Training Set');","f6decfaa":"plt.figure(figsize=(15,5))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nsns.countplot(train['Pclass'])\nplt.title('Count Plot for PClass');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)\nplt.title('Number of passenger Survived');","cb7648f6":"pclass1 = train[train.Pclass == 1]['Survived'].value_counts(normalize=True).values[0]*100\npclass2 = train[train.Pclass == 2]['Survived'].value_counts(normalize=True).values[1]*100\npclass3 = train[train.Pclass == 3]['Survived'].value_counts(normalize=True).values[1]*100\n\nprint(\"Lets look at some satistical data!\\n\")\nprint(\"Pclaas-1: {:.1f}% People Survived\".format(pclass1))\nprint(\"Pclaas-2: {:.1f}% People Survived\".format(pclass2))\nprint(\"Pclaas-3: {:.1f}% People Survived\".format(pclass3))","1092b31e":"train['Age'].plot(kind='hist')","67a2526b":"train['Age'].hist(bins=40)\nplt.title('Age Distribution');","11d9c09f":"# set plot size\nplt.figure(figsize=(15, 3))\n\n# plot a univariate distribution of Age observations \nsns.distplot(train[(train[\"Age\"] > 0)].Age, kde_kws={\"lw\": 3}, bins = 50)\n\n# set titles and labels\nplt.title('Distrubution of passengers age',fontsize= 14)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n# clean layout\nplt.tight_layout()","3387b776":"plt.figure(figsize=(15, 3))\n\n# Draw a box plot to show Age distributions with respect to survival status.\nsns.boxplot(y = 'Survived', x = 'Age', data = train,\n     palette=[\"#3f3e6fd1\", \"#85c6a9\"], fliersize = 0, orient = 'h')\n\n# Add a scatterplot for each category.\nsns.stripplot(y = 'Survived', x = 'Age', data = train,\n     linewidth = 0.6, palette=[\"#3f3e6fd1\", \"#85c6a9\"], orient = 'h')\n\nplt.yticks( np.arange(2), ['drowned', 'survived'])\nplt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)\nplt.ylabel('Passenger status after the tragedy')\nplt.tight_layout()","faf2d5b1":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['SibSp'])\nplt.title('Number of siblings\/spouses aboard');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"SibSp\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","9b08e75a":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['Embarked'])\nplt.title('Number of Port of embarkation');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Embarked\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","8338904c":"sns.heatmap(train.corr(), annot=True)\nplt.title('Corelation Matrix');","567d8ba2":"corr = train.corr()\nsns.heatmap(corr[((corr >= 0.3) | (corr <= -0.3)) & (corr != 1)], annot=True, linewidths=.5, fmt= '.2f')\nplt.title('Configured Corelation Matrix');","c2ac410f":"sns.catplot(x=\"Embarked\", y=\"Fare\", kind=\"violin\", inner=None,\n            data=train, height = 6, order = ['C', 'Q', 'S'])\nplt.title('Distribution of Fare by Embarked')\nplt.tight_layout()","61e282d1":"sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"swarm\", data=train, height = 6)\n\nplt.tight_layout()","79ccf473":"sns.catplot(x=\"Pclass\", y=\"Fare\",  hue = \"Survived\", kind=\"swarm\", data=train, \n                                    palette=[\"#3f3e6fd1\", \"#85c6a9\"], height = 6)\nplt.tight_layout()","6594dd05":"train['Fare'].nlargest(10).plot(kind='bar', title = '10 largest Fare', color = ['#C62D42', '#FE6F5E']);\nplt.xlabel('Index')\nplt.ylabel('Fare');","471e770f":"train['Age'].nlargest(10).plot(kind='bar', color = ['#5946B2','#9C51B6']);\nplt.title('10 largest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","d7b1a383":"train['Age'].nsmallest(10).plot(kind='bar', color = ['#A83731','#AF6E4D'])\nplt.title('10 smallest Ages')\nplt.xlabel('Index')\nplt.ylabel('Ages');","e591059d":"train.isnull().sum()","863d8401":"test.isnull().sum()","943e9f7b":"sns.heatmap(train.corr(), annot=True)","7d284ce5":"train.loc[train.Age.isnull(), 'Age'] = train.groupby(\"Pclass\").Age.transform('median')\n\n\n#Same thing for test set\ntest.loc[test.Age.isnull(), 'Age'] = test.groupby(\"Pclass\").Age.transform('median')","f79aa9b4":"train.Embarked.value_counts()","596d8bf7":"train['Embarked'] = train['Embarked'].fillna(mode(train['Embarked']))\n\n#Applying the same technique for test set\ntest['Embarked'] = test['Embarked'].fillna(mode(test['Embarked']))","01e92b9d":"train['Fare']  = train.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\ntest['Fare']  = test.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","d39486b7":"train.Cabin.value_counts()","27b25948":"train['Cabin'] = train['Cabin'].fillna('U')\ntest['Cabin'] = test['Cabin'].fillna('U')","040b7ac2":"train.Sex.unique()","b4497a53":"train['Sex'][train['Sex'] == 'male'] = 0\ntrain['Sex'][train['Sex'] == 'female'] = 1\n\ntest['Sex'][test['Sex'] == 'male'] = 0\ntest['Sex'][test['Sex'] == 'female'] = 1","6a60f9be":"train.Embarked.unique()","6769867a":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntrain = train.join(temp)\ntrain.drop(columns='Embarked', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntest = test.join(temp)\ntest.drop(columns='Embarked', inplace=True)","40bb6f16":"train.columns","c52b1ca3":"train.Cabin.tolist()[0:20]","1dd98a36":"train['Cabin'] = train['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntest['Cabin'] = test['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","7f1157af":"train.Cabin.unique()","e3c4ebea":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ntrain['Cabin'] = train['Cabin'].map(cabin_category)\ntest['Cabin'] = test['Cabin'].map(cabin_category)","24b99255":"train.Name","4e1ceaba":"train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ntest['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","bf1c1620":"train['Name'].unique().tolist()","b23b09b7":"train.rename(columns={'Name' : 'Title'}, inplace=True)\ntrain['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n                                      \ntest.rename(columns={'Name' : 'Title'}, inplace=True)\ntest['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","d2b7ec26":"train['Title'].value_counts(normalize = True) * 100","312409f4":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Title']]).toarray())\ntrain = train.join(temp)\ntrain.drop(columns='Title', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Title']]).toarray())\ntest = test.join(temp)\ntest.drop(columns='Title', inplace=True)","4654ae36":"train['familySize'] = train['SibSp'] + train['Parch'] + 1\ntest['familySize'] = test['SibSp'] + test['Parch'] + 1","a362fe6f":"fig = plt.figure(figsize = (15,4))\n\nax1 = fig.add_subplot(121)\nax = sns.countplot(train['familySize'], ax = ax1)\n\n# calculate passengers for each category\nlabels = (train['familySize'].value_counts())\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+6, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n    \nplt.title('Passengers distribution by family size')\nplt.ylabel('Number of passengers')\n\nax2 = fig.add_subplot(122)\nd = train.groupby('familySize')['Survived'].value_counts(normalize = True).unstack()\nd.plot(kind='bar', color=[\"#3f3e6fd1\", \"#85c6a9\"], stacked='True', ax = ax2)\nplt.title('Proportion of survived\/drowned passengers by family size (train data)')\nplt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\nplt.xticks(rotation = False)\n\nplt.tight_layout()","437dedd8":"# Drop redundant features\ntrain = train.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\ntest = test.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)","af92b4d4":"train.head()","704a892e":"columns = train.columns[2:]\nfrom sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(train.drop(columns=[\"PassengerId\",\"Survived\"]))\n\nnew_df = pd.DataFrame(X_train, columns=columns)","4c15f021":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(new_df)","8fd74d15":"plt.figure(figsize =(8, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c = target, cmap ='plasma')\n# labeling x and y axes\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component');","00a7b23c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'], test_size = 0.2, random_state=2)","f175f6c8":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))","4592d3b8":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=10000, C=50)\nlogreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test, y_test)))","fbe1e008":"print(logreg.intercept_)\nprint(logreg.coef_)","de360d38":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)","d8065a6e":"logreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train_scaled, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))","fd956e9f":"from sklearn.neighbors import KNeighborsClassifier\n\nknnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the training sets\nknnclf.fit(X_train, y_train)\ny_pred = knnclf.predict(X_test)","e82b8770":"from sklearn.metrics import accuracy_score\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","17e08942":"knnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the scaled training sets\nknnclf.fit(X_train_scaled, y_train)\ny_pred = knnclf.predict(X_test_scaled)","e6327085":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","07d7e30b":"from sklearn.svm import LinearSVC\n\nsvmclf = LinearSVC(C=50)\nsvmclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test, y_test)))","ade9e137":"svmclf = LinearSVC()\nsvmclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))","ca53c6c0":"from sklearn.svm import SVC\n\nsvcclf = SVC(gamma=0.1)\nsvcclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test, y_test)))","ffe08fd6":"svcclf = SVC(gamma=50)\nsvcclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))","002ae14d":"from sklearn.tree import DecisionTreeClassifier\n\ndtclf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))","936780a5":"from sklearn.ensemble import RandomForestClassifier\nrfclf = RandomForestClassifier(random_state=2)","3bc5d7a7":"# Set our parameter grid\nparam_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3, 5, 7]    \n}","82ed3233":"from sklearn.model_selection import GridSearchCV\n\nrandomForest_CV = GridSearchCV(estimator = rfclf, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)","fd0e501b":"randomForest_CV.best_params_","ad7fa4b1":"rf_clf = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 100)\n\nrf_clf.fit(X_train, y_train)","d2a05f5c":"predictions = rf_clf.predict(X_test)","38e027e6":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) * 100","2b0ec133":"#Linear Model\nprint(\"Linear Model R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"Linear Model R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))\nprint()\n\n#Logistic Regression\nprint(\"Logistic Regression R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"Logistic Regression R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))\nprint()\n\n#KNN Classifier\nprint(\"KNN Classifier Accuracy:\",accuracy_score(y_test, y_pred))\nprint()\n\n#SVM\nprint('SVM Accuracy on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('SVM Accuracy on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))\nprint()\n\n#Kerelize SVM\nprint('SVC Accuracy on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))\nprint()\n\n#Decision Tree\nprint('Accuracy of Decision Tree on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))\nprint()\n\n#Random Forest\nprint('Random Forest Accuracy:{:.3f}'.format(accuracy_score(y_test, predictions) * 100))","2bccb420":"scaler = MinMaxScaler()\n\ntrain_conv = scaler.fit_transform(train.drop(['Survived', 'PassengerId'], axis=1))\ntest_conv = scaler.transform(test.drop(['PassengerId'], axis = 1))","6ed31b1a":"svcclf = SVC(gamma=50)\nsvcclf.fit(train_conv, train['Survived'])","a9e47c3f":"test['Survived'] = svcclf.predict(test_conv)","f4213364":"test[['PassengerId', 'Survived']].to_csv('MySubmission1.csv', index = False)","900972a3":"Hehe!, null values spotted!","27b4e95b":"> **As maximum values in train set is S let's replace it with the null values**","93e50de9":"- The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n- The smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers.","1c5f83ef":"### What is in the name?\nEach passenger Name value contains the title of the passenger which we can extract and discover.\nTo create new variable \"Title\":\n\n- I am using method 'split' by comma to divide Name in two parts and save the second part\n- I am splitting saved part by dot and save first part of the result\n- To remove spaces around the title I am using 'split' method\n- To visualize, how many passengers hold each title, I chose countplot.","66f497e1":"# KNN Classifier\n\nK Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms.KNN algorithm used for both classification and regression problems.","b8796ecf":"# Table of content \n\n- EDA\n- Handle Missing Values\n- Feature Engineering\n- linear Regression\n- Logistic Regression\n- Scalling\n- KNN Classifier\n- Support Vector Machine(SVM)\n- Kernelize SVM\n- Decision Tree\n- Random Forest","ee56e41d":"> **Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class**","b943ca91":"**It's Time to look at the Age column!**","4aab6589":"> **Additionally, you can view the y-intercept and coefficients**","c9537e39":"# Support Vector Machine(SVM)","0c74d07f":"looking at some satistical data!","8e71ef87":"## PCA(Principle component analysis)\n\nlet\u2019s visualize our final dataset by implementing PCA and plot the graph","aff767cc":"Dateset is completely ready now!\n","bc15fe10":"**- Want to know how to analize dataset using pandas, matplotlib and seaborn?**\n\n**- Want to know how to solve classification problems!**\n\n**- Want to know how to train the best model?**\n\n**- Want to know how to use scaling?**\n\n**- Want to know how to improve your model accuracy?**\n\n## **This notebook will answer all of your questions!**","7d01f53f":"# Feature Engineering","78a1d55c":"**Most Important thing when plotting histograms : Arrange Number of Bins**","df6732cf":"<h1 style=\"color:green\"><center>Don't forget to upvote if you like it! It's free! :)","2ea27708":"# Logistic Regression\n\nAs our target variable is discrete value(i.e 0 and 1) logistic regression is more likely to fit well the model","963fbe3e":"Performed Well!","0637decb":"**Age by surviving status**\n\nDid age had a big influence on chances to survive?\nTo visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:","ad296ec1":"# Magic Weapon#2: **Support Vector Machine with RBF kernel**","69025948":"Below we set the hyperparameter grid of values with 4 lists of values:\n\n- 'criterion' : A function which measures the quality of a split.\n- 'n_estimators' : The number of trees of our random forest.\n- 'max_features' : The number of features to choose when looking for the best way of splitting.\n- 'max_depth' : the maximum depth of a decision tree.","72f686bb":"<h1 style='color:red'>Handle Missing Values","b0ce26fd":"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):","e9bc4113":"<h1 style='color:red'>EDA (Exploratory Data Analysis)","52918938":"# Plz Upvote!","ffafec74":"> We can get the alphabets by running regular expression","e1ad58cf":"**Fare vs Pclass**","b775da83":"one of the effectitve way to fill the null values is by finding correlation","a8f10375":"Hmmm... but we know from part 2 that Sibsp is the number of siblings \/ spouses aboard the Titanic, and Parch is the number of parents \/ children aboard the Titanic... So, what is another straightforward feature to engineer?\nYes, it is the size of each family aboard!\n","97ecea69":"**We will be dealling with null values later on.**","28433bfa":"Ourdataset contain some outliers and randomness but still let's use this to train the model.","9197948c":"> So many different values let's place missing values with U as \"Unknown\"","f14711b5":"# Submitting the solutions\n\nI am choosing SVC model for the instance, you can try submiting solution with different models","02c2ff69":"Variable Name | Description\n--------------|-------------\nSurvived      | Survived (1) or died (0)\nPclass        | Passenger's class\nName          | Passenger's name\nSex           | Passenger's sex\nAge           | Passenger's age\nSibSp         | Number of siblings\/spouses aboard\nParch         | Number of parents\/children aboard\nTicket        | Ticket number\nFare          | Fare\nCabin         | Cabin\nEmbarked      | Port of embarkation","70721919":"**Let's try on scaled data**","af036d58":"**Looks like single person Non-survived count is almost double than survived, while others have 50-50 % ratio**","cd182057":"**Wow!, Pclass is also a good feature to train our model.**","3e7356fd":"Let's print our solutions","01a70336":"**Look in to relationships among dataset**","c325413c":"# MinMaxScaler\n","fb9775c8":"> Sex is categorical data so we can replace male to 0 and femail to 1","97fe76ed":"**Let's look at Number of siblings\/spouses aboard**","ca0368fc":"**Can't say much!**","e54e0e06":"**Let's first vizualize null values on our training set on graph**","aa18f778":"**Configure the heatmap**","09866bb1":"**Fare vs Embarked**","16326b23":"\n\n# Magic Weapon#1: **Let's Scale our data and re-train the model**","a4439987":"# Magic Weapon #3: Hyperparameter Tuning","1491726e":"# Magic Weapon #4: All model Accuracy Score","6475afe4":"> Haha, much better!","8827a367":"Let's print our optimal hyperparameters set","4d846cb9":"**So the plot says we have more number of non-survived people and females are more likely to survived than male!. so, 'Sex' looks like a very strong explanatory variable, and it can be good choice for our model!**","c6e3d6f4":"**For absoulte beginners, do check the notebook**\n\n# [Beginners Notebook with EDA](https:\/\/www.kaggle.com\/harshkothari21\/beginners-notebook-90-accuracy-with-eda)","ddb1f797":"# Linear Regression\n\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.","fd919fa1":"# Kernelize SVM\n","d12b6cf2":"**Let's analysize Pclass**","13206211":"# Random Forest\n\nSecondly, I would like to introduce one of the most popular algorithms for classification (but also regression, etc), Random Forest! In a nutshell, Random Forest is an ensembling learning algorithm which combines decision trees in order to increase performance and avoid overfitting.","84737a54":"> Let's encode with OneHotEncoder technique","8e3f88b7":"**Let's look at target feature first**","9e3fd305":"Some statistical values of null values in dataset.","46acb170":"it's clear from the score that linear regression doesn't makes sence","d6f57cfd":"**Wohh that's lot's of title. So, let's bundle them**\n","43a63042":"**Now Looking at Port of embarkation**","c4dcb5d7":"**Let's try some other Techniques**","d1303fee":"Let's look at some maximum and minimum values of features!","36cedfba":"**Look Accuracy on Training data, lol**","b96e2701":"### Let's try on scaled data","e85260f6":"# Decision Tree","4a2d2392":"**Age column has non-uniform data and many outliers**\n\n**Outlier** : An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.","dde543d4":"> Also, corr(Fare, Pclass) is the highest correlation in absolute numbers for 'Fare', so we'll use Pclass again to impute the missing values!","4cbe0096":"### That increases the accuracy a lot!","6e961a8b":"Better! let's convert to numeric"}}