{"cell_type":{"7bb43799":"code","be76400d":"code","c563d59f":"code","012d3ea3":"code","dbde7b47":"code","35d09b8a":"code","16614e59":"code","a3e0e56e":"code","9c64a906":"code","181c8c07":"code","f64f90c8":"code","823e3d4e":"code","0b8322e4":"code","1d14993c":"code","50c75772":"code","430184b8":"code","e1c3231f":"code","c6063644":"code","cd37aecb":"code","15fec402":"code","62041966":"code","76cb9dd0":"code","d7bc4180":"markdown","7e868902":"markdown","193f9483":"markdown","8638ac9c":"markdown","eb7c45ed":"markdown","ebdddc8a":"markdown","591e5b11":"markdown","0ce6925b":"markdown","ca5a5bcb":"markdown","15273069":"markdown","39c4a22e":"markdown","269fc400":"markdown","694762b4":"markdown","757353b0":"markdown","f80a9167":"markdown","5e6aadf2":"markdown","db428956":"markdown"},"source":{"7bb43799":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be76400d":"from sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c563d59f":"data = pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/Country-data.csv')\ndata","012d3ea3":"data.info()","dbde7b47":"data.describe()","35d09b8a":"country_names = data['country']\ndata = data.drop(['country'], axis='columns')","16614e59":"correlation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True)","a3e0e56e":"# sns.pairplot(data)","9c64a906":"scikit_copy = data.copy()\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(scikit_copy)","181c8c07":"cost_values = []\nfor k in range(1, 15):\n    model = KMeans(n_clusters=k)\n    model.fit(scaled_data)\n    cost_values.append(model.inertia_)\n\nplt.plot(cost_values)","f64f90c8":"k = 3\nmodel = KMeans(n_clusters=k)\nclusters = model.fit_predict(scaled_data)","823e3d4e":"scikit_copy['country'] = country_names\nscikit_copy['cluster'] = clusters\nscikit_copy","0b8322e4":"scikit_copy['cluster'].value_counts()","1d14993c":"sns.pairplot(scikit_copy.drop(['country'], axis='columns'), hue='cluster')","50c75772":"print('Least developed countries:')\nscikit_copy[scikit_copy['cluster'] == 2]","430184b8":"print('Moderately developed countries:')\nscikit_copy[scikit_copy['cluster'] == 0]","e1c3231f":"print('Highly developed countries:')\nscikit_copy[scikit_copy['cluster'] == 1]","c6063644":"class CustomKMeans(BaseEstimator):\n    '''\n    Class implementing the K-Means Clustering algorithm. \n    Implements the scikit's BaseEstimator, which enables us to use it in conjunction\n    with other scikit's tools such as Pipeline\n    '''\n    def __init__(self, n_clusters=3, n_init=10, max_iter=200):\n        self.n_clusters = n_clusters\n        self.n_init = n_init\n        self.max_iter = max_iter\n        \n        \n    def fit(self, X):\n        '''\n        Performs the K-Means Clustering algorithm.\n        Args:\n            X - data to be clustered\n            n_clusters - number of clusters\n            n_init - number of interations to initialize and perform the k means algorithm\n        '''\n        self.cost_history = []\n        self.centroids_history = []\n    \n        for act_run in range(self.n_init):\n            self.centroids = self.get_initial_centroids(X, self.n_clusters)\n        \n            for i in range(self.max_iter):\n                # Assign each data point to the closest centroid.\n                # sample_assignments[i] corresponds to i-th row of X, the index of the centroid assigned to example i\n                sample_assignments = self.find_closest_centroids(X, self.centroids)\n       \n                old_centroids = self.centroids.copy()\n                # Compute the new centroids based on the newly assigned samples\n                self.update_centroids(X, sample_assignments, self.centroids)\n            \n                # if the centroids stayed the same, they won't change anymore, so we break the loop\n                if np.all(old_centroids == self.centroids):\n                    break\n    \n            self.cost_history.append(self.cost_function(X, sample_assignments, self.centroids))\n            self.centroids_history.append(self.centroids)\n            \n        self.centroids = self.find_best_params()\n        \n        \n    def predict(self, X):\n        '''Assigns the samples from the dataset to clusters'''\n        return self.find_closest_centroids(X, self.centroids)\n    \n    \n    def fit_predict(self, X):\n        self.fit(X)\n        return self.predict(X)\n\n            \n    def get_initial_centroids(self, X, n_clusters):\n        '''Chooses n_clusters random samples from the data as the initial centroids'''\n        random_indexes = np.random.choice(X.shape[0], n_clusters)\n        return X[random_indexes, :]\n\n\n    def find_closest_centroids(self, X, centroids):\n        '''Assign each sample to its closest centroid'''\n        sample_assignments = []\n        for sample in X:\n            distances = np.linalg.norm(sample - centroids, axis=1)\n            min_index = np.argmin(distances)\n            sample_assignments.append(min_index)\n        return np.array(sample_assignments)\n\n\n    def update_centroids(self, X, sample_assignments, centroids):\n        '''Computes new coordinates for each centroid based on the assigned samples'''\n        for k in range(centroids.shape[0]):\n            samples_assigned_to_centroid = (sample_assignments == k)\n            centroids[k,:] = np.mean(X[samples_assigned_to_centroid], axis=0)\n        \n        \n    def cost_function(self, X, sample_assignments, centroids):\n        '''Calculates the inertia of the model with given centroids'''\n        cost = 0\n        for i in range(X.shape[0]):\n            sample_centroid = sample_assignments[i]\n            cost += np.linalg.norm(X[i,:] - centroids[sample_centroid,:])\n        return cost \/ X.shape[0]\n\n\n    def find_best_params(self):\n        best_index = np.argmin(self.cost_history)\n        return self.centroids_history[best_index]","cd37aecb":"model = CustomKMeans(n_clusters=3)\n\ncustom_copy = data.copy()\nX_custom_scaled = scaler.transform(custom_copy)\n\ncustom_clusters = model.fit_predict(X_custom_scaled)","15fec402":"custom_copy['country'] = country_names\ncustom_copy['cluster'] = custom_clusters\ncustom_copy","62041966":"print(custom_copy['cluster'].value_counts())\nprint()\nprint('Centroid coordinates:')\nprint(model.centroids)","76cb9dd0":"print('Highly developed countries:')\ncustom_copy[custom_copy['cluster'] == 1]","d7bc4180":"The algorithm runs as follows:\n    1. as a starting point, initialize each centroid to a random sample's coordinates\n    2. repeat until max_iter is reached:\n        2a. assign each sample to the closest centroid\n        2b. calculate new centroids' positions based on the newly assigned points\n            (centroid position is calculated as a mean of coordinates of all the points assigned to it)\n            \nThe algorithm is fairly simple, but there is a risk of it not finding the best fit and instead falling into what's called a local minimum of the cost function (in this case inertia). We can prevent it by running the algorithm multiple times, each time with different random samples as initial centroids and then choose the centroids with the lowest cost function.\n\nFor simplicity, we will ommit error checking and focus on the algorithm implementation","7e868902":"We can see that the data definitely needs scaling. Also, it seems that there are a couple of potential outliers in the dataset","193f9483":"One thing we don't for clustering is the country names' column, therefore we will drop it","8638ac9c":"# Clustering\n\nWe're going to use the K-Means Clustering algorithm. Since we don't have to choose a specific number of clusters. we will choose their number using the elbow method.\n\nIn kmeans, we can define a cost function as the sum of distances between each point and the center of the cluster it's assigned to. This cost if often called *inertia*. We want this cost to be as low as possible, but at the same time we don't want to many clusters, because that wouldn't be very informative.\n\nThe elbow method means that we will plot the inertia as a funciton of number of clusters. For the first couple values, the cost should drastically decrease with each new cluster. But, at some point curve will begin to \"flatten\". This point is exactly the value we're looking for. \n\nThe plot we'll look a bit like bent arm and the point in question can remind the place where an elbow should be. Hence the name of this method.","eb7c45ed":"# K-Means Algorithm - My Own Implementation","ebdddc8a":"# Data preprocessing","591e5b11":"So, as we can see the samples were divided into clusters the same way as scikit's implementation.\n\nWe can also look at the column of the least developed countries for comparison","0ce6925b":"In this dataset our goal is to find the most under-developed countries. We can use that as an opportunity to dive into the technique known as clustering. For this purpose, we'll use the most popular algorithm called K-Means Clustering. \n\nWe'll first see how it works on the example of scikit learn's implementation and then we'll try to implement it ourselves.","ca5a5bcb":"Ok, so we can see that this cluster is the same as cluster of least developed countries from the scikit implementation.","15273069":"Let's try to visualize the clusters","39c4a22e":"Let's inspect the correlation of columns","269fc400":"We can see a few columns have strong correlations:\n - child_mort - total_fer \n - child_mort - life_expec\n - exports - imports\n - income - gdpp\n - life_expec - total_fer","694762b4":"# Data analysis\n\nLet's see how many null values are there?","757353b0":"So, from this plots we can see that:\n - cluster 0: \n     in most categories countries fall somewhere in between cluster 1 and cluster 2\n - cluster 1:\n     - lowest children mortality\n     - highest income\n     - highest life expectancy\n     - lowest total fertility\n     - highest GDPP\n - cluster 2:\n     - highest children mortality\n     - lowest income\n     - lowest life_expectancy\n     - highest total fertility\n     - lowest GDPP\n     \n     \nThis analysis leads me to believe that we can name these clusters as:\n    - cluster 2 - least developed countries\n    - cluster 0 - moderately developed countries\n    - cluster 1 - highly developed countries","f80a9167":"We can see that the curve begins somewhere around 3. So that will be our k.","5e6aadf2":"Let's see the countries on this list","db428956":"We can see that there are no null values in this set.\n\nLet's now inspect the statistical properties of the data"}}