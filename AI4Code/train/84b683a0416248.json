{"cell_type":{"e3d055d1":"code","4ed84882":"code","2caa0395":"code","32236937":"code","16e7e1b2":"code","f8c40e4c":"code","b7ff4203":"code","6d873f2b":"code","8b347e8b":"code","f643f6de":"code","6ccc80e6":"code","cf6608c9":"code","5aabcbb9":"code","d7e90ee6":"code","a00da37e":"code","f097fdda":"code","3c8480dd":"code","6bf2f599":"code","ae2e90ac":"code","e19c2446":"code","d11a53d8":"code","e47fb222":"code","5cc8c4ba":"code","2b6cebd7":"code","67ef193d":"code","7daf808e":"code","78c87257":"code","097a2ca9":"code","2367a82e":"markdown","c61a7b9d":"markdown","107eb576":"markdown","d81b9b98":"markdown","b0da5bca":"markdown","827c70b1":"markdown","d9cbb8eb":"markdown","d26da63f":"markdown","147c6ef9":"markdown","d0f4b26f":"markdown","fbe6dfe0":"markdown","52ba9310":"markdown","cd2c70c6":"markdown","07372ad4":"markdown","a0ab4b9d":"markdown","c6e34668":"markdown"},"source":{"e3d055d1":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# tf.config.experimental.set_memory_growth(gpus[0], True)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import BatchNormalization\n# from tensorflow.keras import backend as K\n\nimport random\nfrom tqdm import tqdm","4ed84882":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","2caa0395":"# Gdrive path\n# path_to_dataset = os.path.normpath(\"\/content\/drive\/MyDrive\/HWR_recognition\/dataset\/\")\n# path_to_train_dir = os.path.normpath(\"\/content\/drive\/MyDrive\/HWR_recognition\/dataset\/train\")\n# path_to_unicode = os.path.normpath('\/content\/unicode.txt')\n\n# Windows\n# path_to_dataset = os.path.normpath(\"C:\/Users\/prajw\/\/Downloads\/Compressed\/dataset-20210408T023653Z-001\/dataset\/\")\n# path_to_train_dir = os.path.normpath(\"C:\/Users\/prajw\/\/Downloads\/Compressed\/dataset-20210408T023653Z-001\/dataset\/train\")\n# path_to_test_dir = os.path.normpath(\"C:\/Users\/prajw\/\/Downloads\/Compressed\/dataset-20210408T023653Z-001\/dataset\/test\")\n# path_to_unicode = os.path.normpath('C:\/Users\/prajw\/Downloads\/unicode.txt')\n\n# Linux paths\n# path_to_dataset = os.path.normpath(\"\/home\/lawjarp\/Documents\/DeepLearning\/HWR_RCNN\/dataset-20210209T032633Z-001\/dataset\/\")\n# path_to_train_dir = os.path.normpath(\"\/home\/lawjarp\/Documents\/DeepLearning\/HWR_RCNN\/dataset-20210209T032633Z-001\/dataset\/train\")\n# path_to_unicode = os.path.normpath(\"\/home\/lawjarp\/Downloads\/unicode.txt\")\n\n#Kaggle paths\npath_to_dataset = os.path.normpath(\"..\/input\/kannada-handwritten-words\/dataset\")\npath_to_train_dir = os.path.normpath(\"..\/input\/kannada-handwritten-words\/dataset\/train\")\npath_to_test_dir = os.path.normpath(\"..\/input\/kannada-handwritten-words\/dataset\/test\")\npath_to_unicode = os.path.normpath(\"..\/input\/unicode\/unicode.txt\")","32236937":"#Find labels for each directory\nraw_labels_for_dir =[]\nunicode_file = open(path_to_unicode, 'r')\nfor line in unicode_file.readlines():\n  characters = line.split(\";\")[:-1]\n  characters =list(map(lambda x:chr(int(x[2:])), characters ))\n  label = ''.join(characters)\n  raw_labels_for_dir.append(label)\n\n#Find number of characters and unique labels to learn\nmax_length = max([len(label) for label in raw_labels_for_dir])\nprint(max_length)\nlabels_for_dir = [label.ljust(max_length) for label in raw_labels_for_dir]\nset_label_characters = set(char for label in labels_for_dir for char in label)\nset_label_characters = sorted(list(set_label_characters))\nprint(\"Max length: \",max_length)\nprint(\"Number of unique characters: \",len(set_label_characters))\nprint(\"Unique characters to learn: \", set_label_characters)\nprint(\"Labels for directory: \", labels_for_dir)\n","16e7e1b2":"from pathlib import Path\n\n# Get images and corresponding labels\nimages = []\nlabels = []\nnum_classes = 210\nfor class_num in tqdm(range(1, num_classes+1), desc = \"Train data\"):\n  path = Path(os.path.join(path_to_train_dir, str(class_num)))\n  # print(class_num, labels_for_dir[class_num-1])\n  for img in sorted(list(map(str, list(path.glob(\"*.jpg\"))))):\n    if(len(img)<10):\n      print(\"This class has less than 10 images to train\")\n    images.append(str(img))\n    labels.append(labels_for_dir[class_num-1])\n\n# Get images and corresponding labels for Test dataset\ntest_images = []\ntest_labels = []\nnum_classes = 210\nfor class_num in tqdm(range(1, num_classes+1), desc=\"Test data\"):\n  path = Path(os.path.join(path_to_test_dir, str(class_num)))\n  # print(class_num, labels_for_dir[class_num-1])\n  for img in sorted(list(map(str, list(path.glob(\"*.jpg\"))))):\n    test_images.append(str(img))\n    test_labels.append(labels_for_dir[class_num-1])\n","f8c40e4c":"#Test print\nprint(labels[:10])\nprint(images[:10])\n\n# Test datase\nprint(test_labels[:10])\nprint(test_images[:10])","b7ff4203":"# Here are the hyper parameters to tweak\nimg_height = 50\nimg_width = 100","6d873f2b":"# Mapping characters to integers\nchar_to_num = layers.experimental.preprocessing.StringLookup(\n    vocabulary=list(set_label_characters), num_oov_indices=0, mask_token=None\n)\n\n# Mapping integers back to original characters\nnum_to_char = layers.experimental.preprocessing.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\ndef split_data(images, labels, train_size=0.9, shuffle=True):\n    # 1. Get the total size of the dataset\n    size = len(images)\n    # 2. Make an indices array and shuffle it, if required\n    indices = np.arange(size)\n    if shuffle:\n        np.random.shuffle(indices)\n    # 3. Get the size of training samples\n    train_samples = int(size * train_size)\n    # 4. Split data into training and validation sets\n    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n    return x_train, x_valid, y_train, y_valid\n\n\n# Splitting data into training and validation sets\nx_train, x_valid, y_train, y_valid = split_data(np.asarray(images), np.asarray(labels))\n\n\ndef encode_single_sample(img_path, label):\n    # 1. Read image\n    img = tf.io.read_file(img_path)\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_jpeg(img, channels=1)\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n    # 6. Map the characters in label to numbers\n    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n    # 7. Return a dict as our model is expecting two inputs\n    return {\"image\": img, \"label\": label}","8b347e8b":"# Create validation and training data for training\nbatch_size = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = (\n    train_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\nvalidation_dataset = (\n    validation_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\n# Create test data post for accuracy measure post training.\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\ntest_dataset = (\n    test_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    .batch(1)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)","f643f6de":"#Plot and show the words and corresponding images\n_, ax = plt.subplots(4, 4, figsize=(10, 5))\nfor batch in train_dataset.take(1):\n  images = batch[\"image\"]\n  labels = batch[\"label\"]\n  for i in range(16):\n      img = (images[i] * 255).numpy().astype(\"uint8\")\n      label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n      print(label)\n      ax[i \/\/ 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n      ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","6ccc80e6":"class CTCLayer(layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n#         self.loss_fn = tf.nn.ctc_loss\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n#         loss = self.loss_fn(labels=y_true, logits=y_pred, logit_length=input_length, label_length=label_length)\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        \n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred","cf6608c9":"def build_model():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        128,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n\n    # Third conv block\n    x = layers.Conv2D(\n        256,\n        (3,3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv3\",\n    )(x)\n    # x = BatchNormalization()(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n\n\n    #  Fourth conv block\n    x = layers.Conv2D(\n        512,\n        (3,3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv4\",\n    )(x)\n    x = BatchNormalization()(x)\n\n\n    #5th Layer\n    x = layers.Conv2D(\n        512,\n        (3,3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv5\",\n    )(x)\n    x = BatchNormalization()(x)\n    x = layers.MaxPooling2D((1, 2), name=\"pool5\")(x)\n\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 8) * 512)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    x = layers.Dense(128, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    # Three layers of bidirectional LSTMs\n    # RNNs\n    x = layers.Bidirectional(layers.GRU(256, return_sequences=True, dropout=0.3))(x)\n    x = layers.Bidirectional(layers.GRU(256, return_sequences=True, dropout=0.3))(x)\n    x = layers.Bidirectional(layers.GRU(256, return_sequences=True, dropout=0.3))(x)\n    # x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.4))(x)\n\n\n    # Output layer\n#     x = layers.Dense(len(set_label_characters)+1, activation=\"softmax\", name=\"dense2\")(x)\n    x = layers.Dense(59, activation=\"softmax\", name=\"dense2\")(x)\n    \n\n    # Add CTC layer for calculating CTC loss at each step\n    # output = CTCLayer(K, name=\"ctc_loss\")(labels, x)\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n\n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], outputs=output, name=\"v2\"\n    )\n    # Optimizer\n    opt = keras.optimizers.Adam()\n    # Compile the model and return\n    model.compile(optimizer=opt)\n    return model\n\n","5aabcbb9":"# Init model\nnew_model = build_model()\nnew_model.summary()","d7e90ee6":"# run = wandb.init(entity=\"pesuhwr\", project=\"kannada_hwr\", job_type=\"inference\")\nmodel_name = \"pesuhwr\/kannada-hwr\/rcnn_model:\"\nmodel_at = run.use_artifact(model_name+'v1', type='model')\n#Download\nmodel_dir = model_at.download()","a00da37e":"#Load model\nloaded_model = tf.keras.models.load_model(model_dir)","f097fdda":"loaded_model.summary()","3c8480dd":"# initialize wandb with your project name and optionally with configutations.\nrun = wandb.init(entity=\"pesuhwr\", project='kannada-hwr',\n           config={\n              \"batch_size\": 32,\n              \"loss_function\": \"ctc_loss\",\n              \"architecture\": \"Recurrent Convolutional Neural Network\",\n              \"dataset\": \"custom\"\n           })\nconfig = wandb.config","6bf2f599":"model = loaded_model\nmodel = new_model","ae2e90ac":"epochs = 10\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    callbacks=[WandbCallback()],\n)","e19c2446":"training_model_artifact = wandb.Artifact(\"rcnn_model\", type=\"model\", description=\"\")","d11a53d8":"model.save('.\/saved_model\/model')","e47fb222":"training_model_artifact.add_dir('.\/saved_model\/model')\nrun.log_artifact(training_model_artifact)","5cc8c4ba":"model_g = model","2b6cebd7":"def decode_batch_predictions(pred, test = False):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n        :, :max_length\n    ]\n    if(test):\n      return results\n    # print(\"results: \", results)\n    # Iterate over the results and get back the text\n    output_text = []\n    for res in results:\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text","67ef193d":"prediction_model = keras.models.Model(\n    model_g.get_layer(name=\"image\").input, model_g.get_layer(name=\"dense2\").output\n)","7daf808e":"import pickle\n# Get the prediction model by extracting layers till the output layer\n\n\n# prediction_model = keras.models.Model(\n#     model_g.get_layer(name=\"image\").input, model_g.get_layer(name=\"dense2\").output\n# )\n# prediction_model = model_g\n# prediction_model = model\n\n# A utility function to decode the output of the network\n\n#  Let's check results on some validation samples\nfor batch in validation_dataset.take(1):\n    batch_images = batch[\"image\"]\n    batch_labels = batch[\"label\"]\n\n    preds = prediction_model.predict(batch_images)\n    # with open(os.path.join(path_to_dataset, 'label.pkl'), 'wb') as file:\n    #   pickle.dump(batch_labels, file)\n    # # print(batch_labels)\n\n\n    pred_texts = decode_batch_predictions(preds)\n\n    orig_texts = []\n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        orig_texts.append(label)\n\n    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n    for i in range(len(pred_texts)-16):\n        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n        img = img.T\n        title = f\"Prediction: {pred_texts[i]}\"\n        print(title)\n        ax[i \/\/ 4, i % 4].imshow(img, cmap=\"gray\")\n        # ax[i \/\/ 4, i % 4].set_title(title)\n        ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","78c87257":"count = 0\ncorrect = 0\nincorrect = []\nfor sample in tqdm(random.sample(list(test_dataset),500)):\n  count+=1\n  image = sample[\"image\"]\n  label = sample[\"label\"]\n  pred = prediction_model.predict(image)\n  pred_label = decode_batch_predictions(pred, test=True)\n  k = tf.math.equal(label, pred_label).numpy().all()\n  if(k):\n    correct+=1\n  else:\n    pred_texts = decode_batch_predictions(pred)\n    label_texts = decode_batch_predictions(label)\n    incorrect.append((pred_texts, label_texts))\n\nacc = (correct\/count)*100\n  \nprint(\"\\nAccuracy = \", str(acc), \"%\")","097a2ca9":"for i in incorrect:\n  print(i)","2367a82e":"## Import required libraries","c61a7b9d":"## Creating train, valid and test dataset","107eb576":"# RCNN for HWR (Kannada)\n\n* This architecture is a combination of a **CNN** and **RNN**.\n\n* The CNN extracts the features and the RNN uses the predicted features as time series data to identify the word.\n\n### Advantages\n* Need not train on every possible words\n* Learns to recognise words from the given dataset.\n\n### Disadvantage\n* Tricky to train\n* May lose accuracy sometimes of characters are messy in the picture.","d81b9b98":"## Prediction","b0da5bca":"## Model defintions","827c70b1":"## Training and optimization","d9cbb8eb":"## CTC Layer","d26da63f":"## Getting data ready","147c6ef9":"## Setup the project to track training","d0f4b26f":"## Saving a model and versioning it with wandb","fbe6dfe0":"## Plotting Data","52ba9310":"## Define path directories","cd2c70c6":"## WandB setup","07372ad4":"## Instantiate a model (SKIP[](http:\/\/) this if you are using a versioned model from wandb)","a0ab4b9d":"## Load model from WandB","c6e34668":"## Accuracy check"}}