{"cell_type":{"584c7f9b":"code","d3d3876d":"code","7efb8287":"code","521c6e15":"code","2f0dc600":"code","262cf43d":"code","77b03f5a":"code","cc572770":"code","a3fe4234":"markdown","7ce0eeee":"markdown","3782e287":"markdown","ace71b81":"markdown","7496a574":"markdown","85ad0055":"markdown","5f74cebd":"markdown","cdb8267c":"markdown"},"source":{"584c7f9b":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\nprint(tokenizer )","d3d3876d":"import nltk\nnltk.download(\"stopwords\")","7efb8287":"from string import punctuation\nfrom os import listdir\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n\t# split into tokens by white space\n\ttokens = doc.split()\n\t# remove punctuation from each token\n\ttable = str.maketrans('', '', punctuation)\n\ttokens = [w.translate(table) for w in tokens]\n\t# remove remaining tokens that are not alphabetic\n\ttokens = [word for word in tokens if word.isalpha()]\n\t# filter out stop words\n\tstop_words = set(stopwords.words('english'))\n\ttokens = [w for w in tokens if not w in stop_words]\n\t# filter out short tokens\n\ttokens = [word for word in tokens if len(word) > 1]\n\treturn tokens\n\n# load doc and add to vocab\ndef add_doc_to_vocab(filename, vocab):\n\t# load doc\n\tdoc = load_doc(filename)\n\t# clean doc\n\ttokens = clean_doc(doc)\n\t# update counts\n\tvocab.update(tokens)\n\n# load all docs in a directory\ndef process_docs(directory, vocab, is_trian):\n\t# walk through all files in the folder\n\tfor filename in listdir(directory):\n\t\t# skip any reviews in the test set\n\t\tif is_trian and filename.startswith('cv9'):\n\t\t\tcontinue\n\t\tif not is_trian and not filename.startswith('cv9'):\n\t\t\tcontinue\n\t\t# create the full path of the file to open\n\t\tpath = directory + '\/' + filename\n\t\t# add doc to vocab\n\t\tadd_doc_to_vocab(path, vocab)\n\n# define vocab\nvocab = Counter()\n# add all docs to vocab\nprocess_docs('..\/input\/txt-sentoken\/txt_sentoken\/neg', vocab, True)\nprocess_docs('..\/input\/txt-sentoken\/txt_sentoken\/pos', vocab, True)\n# print the size of the vocab\nprint(len(vocab))\n# print the top words in the vocab\nprint(vocab.most_common(50))","521c6e15":"# keep tokens with a min occurrence\nmin_occurane = 2\ntokens = [k for k,c in vocab.items() if c >= min_occurane]\nprint(len(tokens))\n# save list to file\ndef save_list(lines, filename):\n\t# convert lines to a single blob of text\n\tdata = '\\n'.join(lines)\n\t# open file\n\tfile = open(filename, 'w')\n\t# write text\n\tfile.write(data)\n\t# close file\n\tfile.close()\n \n# save tokens to a vocabulary file\nsave_list(tokens, 'vocab.txt')","2f0dc600":"#from keras.models import Sequential\n#Sequential?\n","262cf43d":"#from string import punctuation\n#from os import listdir\nfrom numpy import array\n#from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n","77b03f5a":"# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# turn a doc into clean tokens\ndef clean_doc(doc, vocab):\n\t# split into tokens by white space\n\ttokens = doc.split()\n\t# remove punctuation from each token\n\ttable = str.maketrans('', '', punctuation)\n\ttokens = [w.translate(table) for w in tokens]\n\t# filter out tokens not in vocab\n\ttokens = [w for w in tokens if w in vocab]\n\ttokens = ' '.join(tokens)\n\treturn tokens\n\n# load all docs in a directory\ndef process_docs(directory, vocab, is_trian):\n\tdocuments = list()\n\t# walk through all files in the folder\n\tfor filename in listdir(directory):\n\t\t# skip any reviews in the test set\n\t\tif is_trian and filename.startswith('cv9'):\n\t\t\tcontinue\n\t\tif not is_trian and not filename.startswith('cv9'):\n\t\t\tcontinue\n\t\t# create the full path of the file to open\n\t\tpath = directory + '\/' + filename\n\t\t# load the doc\n\t\tdoc = load_doc(path)\n\t\t# clean doc\n\t\ttokens = clean_doc(doc, vocab)\n\t\t# add to list\n\t\tdocuments.append(tokens)\n\treturn documents\n\n# load the vocabulary\nvocab_filename = 'vocab.txt'\nvocab = load_doc(vocab_filename)\nvocab = vocab.split()\nvocab = set(vocab)\n\n# load all training reviews\npositive_docs = process_docs('..\/input\/txt-sentoken\/txt_sentoken\/pos', vocab, True)\nnegative_docs = process_docs('..\/input\/txt-sentoken\/txt_sentoken\/neg', vocab, True)\ntrain_docs = negative_docs + positive_docs\n\n# create the tokenizer\ntokenizer = Tokenizer()\n# fit the tokenizer on the documents\ntokenizer.fit_on_texts(train_docs)\n\n# sequence encode\nencoded_docs = tokenizer.texts_to_sequences(train_docs)\n# pad sequences\nmax_length = max([len(s.split()) for s in train_docs])\nXtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# define training labels\nytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n\n# load all test reviews\npositive_docs = process_docs('..\/input\/txt-sentoken\/txt_sentoken\/pos', vocab, False)\nnegative_docs = process_docs('..\/input\/txt-sentoken\/txt_sentoken\/neg', vocab, False)\ntest_docs = negative_docs + positive_docs\n# sequence encode\nencoded_docs = tokenizer.texts_to_sequences(test_docs)\n# pad sequences\nXtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# define test labels\nytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n\n# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1","cc572770":"# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_length))\nmodel.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(Xtrain, ytrain, epochs=10, verbose=2)\n# evaluate\nloss, acc = model.evaluate(Xtest, ytest, verbose=0)\nprint('Test Accuracy: %f' % (acc*100))","a3fe4234":"____________________________________________________________________________________\n_____________________________________________________________________________________","7ce0eeee":"**Develop a Deep Learning Model to Automatically Classify Movie Reviews\nas Positive or Negative in Python with Keras**","3782e287":"> Referred from : https:\/\/machinelearningmastery.com\/develop-word-embedding-model-predicting-movie-review-sentiment\/","ace71b81":">  Padding means adding values before and after Tensor values\n\n>Refer:https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences\n\n> A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. A Sequential model is not appropriate when: Your model has multiple inputs or multiple outputs. Any of your layers has multiple inputs or multiple outputs.\n> you can check all about sequential by typing below code in code section\n\n\n","7496a574":"> more on Kera Squential : https:\/\/youtu.be\/VGCHcgmZu24\n\n> What do \u201ccompile\u201d, \u201cfit\u201d, and \u201cpredict\u201d do in Keras sequential models?\n> 1. First, we want to decide a model architecture, this is the number of hidden layers and activation functions, etc. (compile)\n> 1. Secondly, we will want to train our model to get all the paramters to the correct value to map our inputs to our outputs. (fit)\n> 1. Lastly, we will want to use this model to do some feed-forward passes to predict novel inputs. (predict)\n> Reffer: https:\/\/datascience.stackexchange.com\/questions\/46124\/what-do-compile-fit-and-predict-do-in-keras-sequential-models\n\n\n> Dense: https:\/\/keras.io\/api\/layers\/core_layers\/dense\/\n\n> Flatten: Once the pooled featured map is obtained, the next step is to flatten it. Flattening involves transforming the entire pooled feature map matrix into a single column which is then fed to the neural network for processing.\n> for flattening and embedding: https:\/\/medium.com\/analytics-vidhya\/understanding-embedding-layer-in-keras-bbe3ff1327ce\n\n> Embedding:Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0\u2019s and 1\u2019s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions. \n\n> overview:https:\/\/heartbeat.fritz.ai\/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed","85ad0055":"> Stopwords are the English words which does not add much meaning to a sentence.\nThey can safely be ignored without sacrificing the meaning of the sentence.\nFor example, the words like the, he, have etc.\n\n> Refer : https:\/\/www.tutorialspoint.com\/python_text_processing\/python_remove_stopwords.htm","5f74cebd":"> Punctutaions in strings: https:\/\/www.geeksforgeeks.org\/string-punctuation-in-python\/\n\n> listdir: This method returns a list containing the names of the entries in the directory given by path.https:\/\/www.tutorialspoint.com\/python\/os_listdir.htm\n\n> Couters: https:\/\/www.geeksforgeeks.org\/counters-in-python-set-1\/\n\n> maketrans: https:\/\/www.w3schools.com\/python\/ref_string_maketrans.asp\n\n","cdb8267c":"> Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\n> https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/what-is-tokenization-nlp\/\n\n> About Tokenizer\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer"}}