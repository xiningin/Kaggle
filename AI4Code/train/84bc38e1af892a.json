{"cell_type":{"1ce4b3eb":"code","e399e1a7":"code","8dcaeaa3":"code","44d665b7":"code","9027cc9d":"code","57e29bcc":"code","02eee421":"code","7eaf1f57":"code","61c4e2ef":"code","6536be42":"code","b9fc9325":"code","4bdecfb0":"code","74a5255d":"markdown","735aae53":"markdown","20e60089":"markdown","85eec94d":"markdown","88cc248c":"markdown","fcb1263b":"markdown","bfbb5869":"markdown","9561e607":"markdown","88aa5c70":"markdown","f443c43c":"markdown","bc007043":"markdown","f5313481":"markdown","ecebd9ea":"markdown","d0fe6896":"markdown","992b2d1f":"markdown","87f28cdd":"markdown","df48b4a6":"markdown","09b523f4":"markdown","ad963c30":"markdown","24e45277":"markdown","d3df4b73":"markdown","f3ce7026":"markdown","85f84336":"markdown","04cc83d4":"markdown","aeb5e376":"markdown","75344786":"markdown","4750eeab":"markdown","c746aafd":"markdown"},"source":{"1ce4b3eb":"import numpy as np\nimport torch\nimport os\nimport imageio\nimport requests\n\nfrom PIL import Image\nfrom io import BytesIO","e399e1a7":"url = \"https:\/\/raw.githubusercontent.com\/deep-learning-with-pytorch\/dlwpt-code\/master\/data\/p1ch2\/bobby.jpg\"\n\nresponse = requests.get(url)\n\nimg = Image.open(BytesIO(response.content))\nimg","8dcaeaa3":"img_arr = imageio.imread(response.content)\nimg_arr.shape","44d665b7":"img = torch.from_numpy(img_arr)\nout = img.permute(2, 0, 1)","9027cc9d":"img","57e29bcc":"out","02eee421":"batch_size = 3\nbatch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)","7eaf1f57":"data_dir = '..\/input\/'\nfilenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n\nfor i, filename in enumerate(filenames):\n    img_arr = imageio.imread(os.path.join(data_dir, filename))\n    img_t = torch.from_numpy(img_arr)\n    img_t = img_t.permute(2, 0, 1)     # Here we keep only the first three channels.\n    img_t = img_t[:3]                  # Sometimes images also have an alpha channel\n    batch[i] = img_t                   # indicating transparency, but our network only wants RGB input.\n    \nbatch","61c4e2ef":"batch.shape","6536be42":"batch = batch.float()\nbatch \/= 255.0\nbatch","b9fc9325":"n_channels = batch.shape[1]\nfor c in range(n_channels):\n    mean = torch.mean(batch[:, c])\n    std = torch.std(batch[:, c])\n    batch[:, c] = (batch[:, c] - mean) \/ std","4bdecfb0":"batch","74a5255d":"We mentioned colors earlier. There are several ways to encode colors into numbers. 1\nThe most common is RGB, where a color is defined by three numbers representing\nthe intensity of red, green, and blue. We can think of a color channel as a grayscale\nintensity map of only the color in question, similar to what you\u2019d see if you looked at\nthe scene in question using a pair of pure red sunglasses. Figure 4.1 shows a rainbow,\nwhere each of the RGB channels captures a certain portion of the spectrum (the fig-\nure is simplified, in that it elides things like the orange and yellow bands being repre-\nsented as a combination of red and green).","735aae53":"At this point, img is a NumPy array-like object with three dimensions: two spatial\ndimensions, width and height; and a third dimension corresponding to the red,\ngreen, and blue channels. Any library that outputs a NumPy array will suffice to obtain\na PyTorch tensor. The only thing to watch out for is the layout of the dimensions.\nPyTorch modules dealing with image data require tensors to be laid out as C \u00d7 H \u00d7 W :\nchannels, height, and width, respectively.","20e60089":"Images come in several different file formats, but luckily there are plenty of ways to\nload images in Python. Let\u2019s start by loading a PNG image using the imageio module\n(code\/p1ch4\/1_image_dog.ipynb).","85eec94d":"## Changing the layout","88cc248c":"This indicates that our batch will consist of three RGB images 256 pixels in height and\n256 pixels in width. Notice the type of the tensor: we\u2019re expecting each color to be rep-\nresented as an 8-bit integer, as in most photographic formats from standard consumer\ncameras. We can now load all PNG images from an input directory and store them in\nthe tensor:","fcb1263b":"![image.png](attachment:image.png)","bfbb5869":"As a slightly more efficient alternative to using stack to build up the tensor, we can pre-\nallocate a tensor of appropriate size and fill it with images loaded from a directory, like so:","9561e607":"We\u2019ve seen this previously, but note that this operation does not make a copy of the\ntensor data. Instead, out uses the same underlying storage as img and only plays with\nthe size and stride information at the tensor level. This is convenient because the\noperation is very cheap; but just as a heads-up: changing a pixel in img will lead to a\nchange in out .","88aa5c70":"**NOTE** Here, we normalize just a single batch of images because we do not\nknow yet how to operate on an entire dataset. In working with images, it is good\npractice to compute the mean and standard deviation on all the training data\nin advance and then subtract and divide by these fixed, precomputed quanti-\nties. We saw this in the preprocessing for the image classifier in section 2.1.4.","f443c43c":"**NOTE** We\u2019ll use imageio throughout the chapter because it handles different\ndata types with a uniform API. For many purposes, using TorchVision is a\ngreat default choice to deal with image and video data. We go with imageio\nhere for somewhat lighter exploration.","bc007043":"We can perform several other operations on inputs, such as geometric transforma-\ntions like rotations, scaling, and cropping. These may help with training or may be\nrequired to make an arbitrary input conform to the input requirements of a network,\nlike the size of the image. We will stumble on quite a few of these strategies in section\n12.6. For now, just remember that you have image-manipulation options available.","f5313481":"We mentioned earlier that neural networks usually work with floating-point tensors as\ntheir input. Neural networks exhibit the best training performance when the input\ndata ranges roughly from 0 to 1, or from -1 to 1 (this is an effect of how their building\nblocks are defined).","ecebd9ea":"The red band of the rainbow is brightest in the red channel of the image, while the\nblue channel has both the blue band of the rainbow and the sky as high-intensity.\nNote also that the white clouds are high-intensity in all three channels.","d0fe6896":"* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-dog-detection\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-gan-horse-zebra\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch3-tensors","992b2d1f":"# Working with images","87f28cdd":"## Adding color channels","df48b4a6":"An image is represented as a collection of scalars arranged in a regular grid with a\nheight and a width (in pixels). We might have a single scalar per grid point (the\npixel), which would be represented as a grayscale image; or multiple scalars per grid\npoint, which would typically represent different colors, as we saw in the previous chap-\nter, or different features like depth from a depth camera.","09b523f4":"Another possibility is to compute the mean and standard deviation of the input data\nand scale it so that the output has zero mean and unit standard deviation across each\nchannel:","ad963c30":"Note also that other deep learning frameworks use different layouts. For instance,\noriginally TensorFlow kept the channel dimension last, resulting in an H \u00d7 W \u00d7 C lay-\nout (it now supports multiple layouts). This strategy has pros and cons from a low-level\nperformance standpoint, but for our concerns, it doesn\u2019t make a difference as long as\nwe reshape our tensors properly.","24e45277":"## Loading an image file","d3df4b73":"The introduction of convolutional neural networks revolutionized computer vision\n(see http:\/\/mng.bz\/zjMa), and image-based systems have since acquired a whole new\nset of capabilities. Problems that required complex pipelines of highly tuned algorith-\nmic building blocks are now solvable at unprecedented levels of performance by train-\ning end-to-end networks using paired input-and-desired-output examples. In order to\nparticipate in this revolution, we need to be able to load an image from common\nimage formats and then transform the data into a tensor representation that has the\nvarious parts of the image arranged in the way PyTorch expects.","f3ce7026":"Scalars representing values at individual pixels are often encoded using 8-bit inte-\ngers, as in consumer cameras. In medical, scientific, and industrial applications, it is\nnot unusual to find higher numerical precision, such as 12-bit or 16-bit. This allows a\nwider range or increased sensitivity in cases where the pixel encodes information\nabout a physical property, like bone density, temperature, or depth.","85f84336":"All data from book **Deep Learning with PyTorch** https:\/\/pytorch.org\/deep-learning-with-pytorch","04cc83d4":"We can use the tensor\u2019s permute method with the old dimensions for each new dimen-\nsion to get to an appropriate layout. Given an input tensor H \u00d7 W \u00d7 C as obtained pre-\nviously, we get a proper layout by having channel 2 first and then channels 0 and 1:","aeb5e376":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","75344786":"## Normalizing the data","4750eeab":"So far, we have described a single image. Following the same strategy we\u2019ve used\nfor earlier data types, to create a dataset of multiple images to use as an input for our\nneural networks, we store the images in a batch along the first dimension to obtain an\nN \u00d7 C \u00d7 H \u00d7 W tensor.","c746aafd":"So a typical thing we\u2019ll want to do is cast a tensor to floating-point and normalize\nthe values of the pixels. Casting to floating-point is easy, but normalization is trickier,\nas it depends on what range of the input we decide should lie between 0 and 1 (or -1\nand 1). One possibility is to just divide the values of the pixels by 255 (the maximum\nrepresentable number in 8-bit unsigned):"}}