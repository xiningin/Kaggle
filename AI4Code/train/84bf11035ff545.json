{"cell_type":{"fb5c193e":"code","52a985e4":"code","109e04f3":"code","6e08c82b":"code","716f866e":"code","61d65770":"code","d8ec24f8":"code","cb7cd3bf":"code","1353465b":"code","4c4eb1bb":"code","0701cfad":"code","ff3bbeca":"code","2c56fc2c":"code","49161ee8":"code","c8dbe6f4":"code","ea938166":"code","02162a9b":"code","4c6dfecf":"code","feda255e":"code","788747cc":"code","549ac727":"code","72b2505e":"code","3b0b1f3b":"code","db0744cb":"code","d96e6f31":"code","7b759fd6":"code","20b4d89b":"code","0e3e450b":"code","4a4ef05d":"code","307b667c":"code","b96e106b":"code","67b1707a":"code","7811361b":"markdown","f54292a5":"markdown","f79ee3ee":"markdown","c8689305":"markdown","a5264fbb":"markdown","3ec28aa6":"markdown","9793d94c":"markdown","db39a268":"markdown","e666f067":"markdown","32097487":"markdown","24ab7a3d":"markdown","e74bfa56":"markdown","acb0410d":"markdown","10b45395":"markdown","4c64be1a":"markdown","0ed37cc0":"markdown","3457b37f":"markdown","be58f505":"markdown","b943b058":"markdown","e2d2d247":"markdown","45edfb7d":"markdown","1e37dc49":"markdown","8069e212":"markdown","154c28cf":"markdown","f2f28de5":"markdown","31e449de":"markdown","120cbef7":"markdown","9d40b2b5":"markdown","ae017a83":"markdown","adf36612":"markdown","0485e293":"markdown"},"source":{"fb5c193e":"import numpy as np\nimport pandas as pd\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\npd.set_option('display.max_columns', None)\nfrom pathlib import Path\ninput_path = Path('\/kaggle\/input\/tabular-playground-series-jan-2021\/')\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","52a985e4":"# Load the data as dataframes\ntrain = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ntest = pd.read_csv(input_path \/ 'test.csv', index_col='id')","109e04f3":"train.info()","6e08c82b":"test.info()","716f866e":"print(train.isna().any().any())\nprint(test.isna().any().any())","61d65770":"train.describe()","d8ec24f8":"test.describe()","cb7cd3bf":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nfig=plt.figure(figsize=(6,6))\nax = sns.distplot(train['target'], color=\"b\")\nax.set(xlabel=\"Target\", ylabel=\"Frequency\", title=\"Target Variable Distribution\")\nplt.show()","1353465b":"train.drop(train[train['target'] == 0].index, inplace = True)","4c4eb1bb":"train.boxplot(column = list(train.columns[0:14]), figsize= (15,10))","0701cfad":"test.boxplot(column = list(test.columns[0:14]), figsize= (15,10))","ff3bbeca":"# Create dataset with only numerical values\nnumericaldata = train.select_dtypes(exclude='object')\n\n# Create plot space\nfig = plt.figure(figsize=(10,15))\n\n# Create subplot for each loop\nfor i in range(len(numericaldata.columns)):\n    fig.add_subplot(6,4,i+1)\n    sns.distplot(numericaldata.iloc[:,i])\n    plt.xlabel(numericaldata.columns[i])\n\n# Display plots        \nplt.tight_layout()\nplt.show()","2c56fc2c":"# Loop through each feature\nfor column in train:\n    \n    # calculate skew for feature\n    sk = round(train[column].skew(), 2)\n    \n    # Print if skew is significant\n    if sk > 2 or sk < -2:\n        print(\"Skew for\", column, \"is\", sk)","49161ee8":"# Create plotting space\nplt.subplots(figsize=(16,12))\n\n# Calculate correlations\ncorr = train[train.columns[1:]].corr()\n\n# Plot the correlations\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True, annot=True)\n\nplt.yticks(rotation=0, fontsize = 15)\nplt.xticks(rotation=0, fontsize = 15)\nplt.tight_layout()","c8dbe6f4":"# Create test and train data\nX = train.loc[:, ((train.columns != 'target') & (train.columns != 'id'))] \ny = train.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state = 42)","ea938166":"# Create model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make predictions \ny_pred = lm.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lm, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","02162a9b":"# Create the parameter grid\nparams = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Create the base model\nlasso = Lasso(random_state=42)\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=lasso, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 5, verbose = 0)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)","4c6dfecf":"# Set best parameter to model\nlasso.set_params(random_state = 42, alpha = 0.001)\n\n# Fit the new model\nlasso.fit(X_train,y_train)\n\n# Make predictions \ny_pred = lasso.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lasso, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","feda255e":"# Create the parameter grid\nparams = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Create the base model\nridge = Ridge(random_state=42)\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=ridge, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 5, verbose = 0)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)","788747cc":"# Set best parameter to model\nridge.set_params(random_state = 42, alpha = 10)\n\n# Fit the new model\nridge.fit(X_train,y_train)\n\n# Make predictions \ny_pred = ridge.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(ridge, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","549ac727":"# Create the base model\nrf_model = RandomForestRegressor(random_state = 42, n_jobs = -1)\n\n# Fit the model\nrf_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = rf_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(rf_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","72b2505e":"\"\"\"\n# Create the parameter grid\nparams = {'max_depth': [5,10,15,20],\n         'max_features': ['auto', 'sqrt']}\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=rf_model, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 3, verbose = 1)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)\n\n\"\"\"","3b0b1f3b":"# Set best parameters to model\nrf_model.set_params(random_state = 42, max_depth=20, max_features = 'sqrt') \n\n# Fit the new model\nrf_model.fit(X_train,y_train)\n\n# Make predictions \ny_pred = rf_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(rf_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","db0744cb":"# Create the base model\nxgb_model = xgb.XGBRegressor(random_state=42)\n\n# Fit the model\nxgb_model.fit(X_train,y_train)\n\n# Make predictions\npreds = xgb_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(xgb_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","d96e6f31":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'colsample_bytree': [.1,.2,.3,.4,.5,.6,.7,.8,.9],\n          'n_estimators': [100],\n          'max_depth': [4,5,6],\n          'learning_rate': [0.05, 0.10, .015, 0.20, 0.25, 0.30]}\n\nxgb_model = gbm.XGBRegressor()\n\ngrid = GridSearchCV(estimator=xgb_model, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 4, verbose = 1)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best parameters found: \", grid.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n\"\"\"","7b759fd6":"# Set best parameters to model\nxgb_model.set_params(random_state=42, colsample_bytree=0.5, learning_rate= 0.2,max_depth=6, n_estimators=100) \n# Fit the new model\nxgb_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = xgb_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(xgb_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","20b4d89b":"# Create the base model\nlgbm_model = LGBMRegressor(random_state=42)\n\n# Fit the model\nlgbm_model.fit(X_train,y_train)\n\n# Make predictions\ny_pred = lgbm_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lgbm_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","0e3e450b":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'feature_fraction': [.1,.2,.3,.4,.5,.6,.7,.8,.9],\n         'max_depth': [2,4,6,8,10,12,14,16,18,20],\n         'max_bin': [5,10,15,20],\n         'n_estimators': [100,500,1000,2000,4000]}\n\ngrid = GridSearchCV(estimator=lgbm_reg, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 4, verbose = False)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best parameters found: \", grid.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n\"\"\"","4a4ef05d":"# Set the best parameters\nlgbm_model.set_params(random_state=42, feature_fraction=.4, max_depth=6, max_bin = 20, n_estimators=500) \n\n# Fit the new model\nlgbm_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = lgbm_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lgbm_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))\n","307b667c":"# Create the voting model \nclf_voting = VotingRegressor(\n    \n    estimators=[\n        ('XGBoost',xgb_model),\n        ('LGBoost',lgbm_model)],\n    \n    #Choose weights for each model\n    weights = [.5,.5]\n)","b96e106b":"# Train the model\nclf_voting.fit(X_train,y_train)\n\n#Make predictions\ny_pred = clf_voting.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(clf_voting, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","67b1707a":"# Create submission\nsubmission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\nsubmission['target'] = clf_voting.predict(test)\nsubmission.to_csv('Jan_Tab_Playground.csv')","7811361b":"# Linear Regression","f54292a5":"* The cross validated RMSE for the tuned LGBM model is 0.7034.","f79ee3ee":"Let's look at the remainder of the variables with boxplots.","c8689305":"# Introduction\n\nFor this competition, we will be predicting a continuous target variable with 14 continuous independent variables.\n\nSubmissions are evaluated on the Root-Mean-Squared-Error (RMSE).   Thus, the goal of this notebook is to obtain the lowest RMSE.\n\nNotebook Layout\n\n* Exploratory Data Analysis\n* Model Creation\n* Model Stacking\n* Predictions\n\nIf you have any feedback, please let me know!\n\n## Let's get started!","a5264fbb":"The target variable is bimodal and has one outlier valued at 0.  Let's remove it.  ","3ec28aa6":"* The variables are mostly bimodal and multimodal.\n* None of the variables seem skewed.  Let's check just in case.","9793d94c":"* The cross validated RMSE for the base random XGBoost is 0.7050.  Looks like we keep improving!  This is now our best model\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense.","db39a268":"# XGBoost","e666f067":"Neither of the datasets have missing values.","32097487":"# Light GBM","24ab7a3d":"* The test data has 200,000 observations and 14 columns.\n* The 15th column is absent because it is what we are trying to predict.","e74bfa56":"The training data has 300,000 observations and 15 columns.","acb0410d":"* The cross validated RMSE for the base LGBM m odel is 0.7034.\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense.","10b45395":"The cross validated RMSE for the lasso model is 0.7269.  This model performed roughly as well as the linear regression.","4c64be1a":"* The cross validated RMSE for the base random forest model is 0.7089.  This is our best model so far.\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense.","0ed37cc0":"* None of the variables were highly skewed.\n* Let's explore the relationships between variables.","3457b37f":"# Lasso","be58f505":"# Ridge Regression","b943b058":"* The cross validated RMSE for the base voting regressor is .7014, which is officially our lowest score. ","e2d2d247":"# Modeling\n\n* We cannot test our models on the given test dataset, because it doesn't contain the target variable.\n* Let's create a new train and test dataset out of the given train dataset.","45edfb7d":"The cross validated RMSE for the linear regression is 0.7265.\n\nThe minimum and maximum of the target variable are 3.70 and 10.27, respectively.  Range: 10.27 - 3.70 = 6.57.\n\nThe RMSE\/Range ratio is .7265\/6.57 = 0.111.  \n\nBecause RMSE is the standard deviation of the model's residuals, we want this to be as low as possible.\n\nThis is a good start.  Let's see if we can obtain a better score with different models.","1e37dc49":"# Voting","8069e212":"The cross validated RMSE for the ridge regression is 0.7265.  This model performed roughly as well as the linear and ridge regression models.","154c28cf":"The train and dest data look very similar.  Let's first look at the target variable.","f2f28de5":"The cross validated RMSE for the tuned random forest model is 0.7058.  This is now our best model!","31e449de":"After experimenting with different weights, it became clear that the voting model performs best without the random forest.","120cbef7":"# Random Forest","9d40b2b5":"The cross validated RMSE for the tuned XGBoost model is 0.7030.  We continue to lower the score!","ae017a83":"* Unfortunately, none of the predictor variables are highy correlated with the target variable.  \n* There is a cluster of variables that are correlated with each other.","adf36612":"* The boxplots confirm that the two datasets are similar.  The boxplots of each variable are roughly the same in both datasets.\n* Cont5 and Cont7 have similar outliers in both datasets.\n* Let's not remove any of them for now.  We should consider using cooks distance to remove influential outliers.\n* Let's plot the distributions of each variables.","0485e293":"# EDA"}}