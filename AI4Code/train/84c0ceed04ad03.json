{"cell_type":{"df7f0cc8":"code","a6c13b99":"code","67b432b4":"code","9c9f36d0":"code","f04bb999":"code","2763db5a":"code","75c83866":"code","b4304965":"code","04383814":"code","f35f1da6":"code","521dfeac":"code","c247d6b5":"code","c7ebdb22":"code","52ace401":"code","8925631d":"code","a15c2e9f":"code","25abfd26":"code","5cc5701d":"code","5727ccec":"code","c58f2b9d":"code","54a7921e":"code","2676b5da":"code","1df131c2":"code","aa7ba995":"code","789bbe0b":"code","d4a73289":"code","a5e57a3e":"code","06a7cfee":"code","10c8235a":"code","c6702841":"code","ebd46609":"code","a513a0bb":"code","27b3a502":"code","8afc9993":"code","20c40aed":"code","a1b21eda":"code","7101ffc8":"code","5fb421f2":"code","6f3556c8":"code","f3f1f30f":"code","03403a38":"code","6d3daddd":"code","69a6c645":"code","e2b03276":"code","c824a785":"code","898c1f51":"code","d098c8fa":"code","a8bfd347":"code","d6618f07":"code","0fe6b062":"code","8faa37cc":"code","b9e7b45e":"code","6faa8296":"code","6dba9141":"code","a493dad7":"code","bc14bf66":"code","517740d5":"code","fcd5c18e":"code","ce76e0c5":"code","7ae579b8":"code","0de927ad":"code","ac861187":"code","f1599b0d":"code","4e2a131c":"code","ade910fb":"code","4fa24407":"code","1115e0ee":"code","a0aab6b4":"code","548f8f16":"code","8f7e0d34":"code","dfb908f2":"code","a1768f5a":"code","38dd503a":"code","b04a8d59":"code","9804f0c4":"code","6c1bf54d":"code","3200c035":"code","cc7e1378":"code","404a538a":"code","0aabaa20":"code","50313a8c":"code","5dc7e1e0":"code","6611dd20":"code","8281d523":"code","c3c517c8":"markdown","a2eccd33":"markdown","0e497951":"markdown","86c4eeb0":"markdown","ba5f36fb":"markdown","417fba2a":"markdown","b66f2835":"markdown","dcc88094":"markdown","3c5b0cd1":"markdown","d4108bc3":"markdown","896bfcd5":"markdown","e9196ea6":"markdown"},"source":{"df7f0cc8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns',None)","a6c13b99":"import plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify","67b432b4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9c9f36d0":"prima=pd.read_csv(\"\/kaggle\/input\/prima-diabetes\/prima_diabetes.csv\")","f04bb999":"prima.shape","2763db5a":"prima.head()","75c83866":"list(prima.columns)","b4304965":"prima.describe()","04383814":"## Data Exploration\n#plt.figure(figsize=(12,5))\nprint(prima.corr()['Outcome'])\nsns.heatmap(prima.corr(),annot=True)","f35f1da6":"prima.isnull().sum()","521dfeac":"prima[prima['Glucose']==0]","c247d6b5":"prima[prima['BloodPressure']==0]","c7ebdb22":"prima[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = prima[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","52ace401":"prima.isnull().sum()","8925631d":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","a15c2e9f":"# Plotting \nmissing_plot(prima, 'Outcome')","25abfd26":"prima['Glucose'].median()","5cc5701d":"prima['Glucose'].mean()","5727ccec":"# patient who is suffering with dIABETES Will have more Glucose level.\n# patient who is not suffering with dIABETES Will have less Glucose level.","c58f2b9d":"prima.groupby('Outcome').agg({'Glucose':'median'}).reset_index()","54a7921e":"def median_target(var):   \n    temp = prima[prima[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","2676b5da":"median_target('Insulin')","1df131c2":"prima.loc[(prima['Outcome'] == 0 ) & (prima['Insulin'].isnull()), 'Insulin'] = 102.5\nprima.loc[(prima['Outcome'] == 1 ) & (prima['Insulin'].isnull()), 'Insulin'] = 169.5","aa7ba995":"median_target('Glucose')","789bbe0b":"prima.loc[(prima['Outcome'] == 0 ) & (prima['Glucose'].isnull()), 'Glucose'] = 107.0\nprima.loc[(prima['Outcome'] == 1 ) & (prima['Glucose'].isnull()), 'Glucose'] = 140.0","d4a73289":"median_target('SkinThickness')","a5e57a3e":"prima.loc[(prima['Outcome'] == 0 ) & (prima['SkinThickness'].isnull()), 'SkinThickness'] = 27.0\nprima.loc[(prima['Outcome'] == 1 ) & (prima['SkinThickness'].isnull()), 'SkinThickness'] = 32.0","06a7cfee":"median_target('BloodPressure')","10c8235a":"prima.loc[(prima['Outcome'] == 0 ) & (prima['BloodPressure'].isnull()), 'BloodPressure'] = 70.0\nprima.loc[(prima['Outcome'] == 1 ) & (prima['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","c6702841":"median_target('BMI')","ebd46609":"prima.loc[(prima['Outcome'] == 0 ) & (prima['BMI'].isnull()), 'BMI'] = 30.1\nprima.loc[(prima['Outcome'] == 1 ) & (prima['BMI'].isnull()), 'BMI'] = 34.3","a513a0bb":"missing_plot(prima, 'Outcome')","27b3a502":"plt.style.use('ggplot') # Using ggplot2 style visuals \nf, ax = plt.subplots(figsize=(11, 15))\nax.set_facecolor('#fafafa')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = prima, \n  orient = 'h', \n  palette = 'Set2')","8afc9993":"median_target('Glucose')","20c40aed":"prima.head()","a1b21eda":"sns.distplot(prima['Age'])","7101ffc8":"# Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nvar=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\nprima[var] = scaler.fit_transform(prima[var])\nprima.head()","5fb421f2":"labels = \"Diabetes\", \"Non Diabetes\"\nplt.title('Diabetes Status')\nplt.ylabel('Condition')\nprima['Outcome'].value_counts().plot.pie(explode = [0, 0.25], autopct = '%1.2f%%',\n                                                shadow = True, labels = labels)","6f3556c8":"prima.head()","f3f1f30f":"y=prima['Outcome']\nX=prima.drop('Outcome',axis=1)","03403a38":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","6d3daddd":"y_train","69a6c645":"X_train.shape","e2b03276":"X_test.shape","c824a785":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg=LogisticRegression()\nlogreg.fit(X_train,y_train)\nprint(logreg.intercept_)\nprint(logreg.coef_)\nprint(metrics.accuracy_score(y_train,logreg.predict(X_train)))","898c1f51":"print(logreg.intercept_)","d098c8fa":"print(logreg.coef_)","a8bfd347":"metrics.accuracy_score(y_train,logreg.predict(X_train))","d6618f07":"import statsmodels.api as sm\nmodel = sm.GLM(y_train,(sm.add_constant(X_train)),family=sm.families.Binomial()).fit()\nprint(model.summary())","0fe6b062":"from sklearn import metrics\ny_train_pred=model.predict(sm.add_constant(X_train))\ny_train_pred=round(y_train_pred).astype('int')\nmetrics.accuracy_score(y_train,y_train_pred)","8faa37cc":"X_test.head()","b9e7b45e":"y_test_pred=model.predict(sm.add_constant(X_test))","6faa8296":"PredictedOutcome=pd.DataFrame(y_test_pred,columns=['PredicbtedOutcome'])","6dba9141":"PredictedOutcome.head()","a493dad7":"Test=pd.concat([X_test,PredictedOutcome],axis=1)","bc14bf66":"Test.describe()","517740d5":"Test.head()","fcd5c18e":"Test['Outcome']=Test['PredicbtedOutcome'].apply(lambda x: 1 if x > 0.71 else 0)","ce76e0c5":"Test.head()","7ae579b8":"Test.shape","0de927ad":"sns.countplot(Test['Outcome'])","ac861187":"labels = \"Diabetes\", \"Non Diabetes\"\nplt.title('Diabetes Status')\nplt.ylabel('Condition')\nTest['Outcome'].value_counts().plot.pie(explode = [0, 0.25], autopct = '%1.2f%%',\n                                                shadow = True, labels = labels)","f1599b0d":"from sklearn import metrics\nmetrics.confusion_matrix(y_test,Test['Outcome'])","4e2a131c":"TN= 135\nTP=38\nFN=43\nFP=15","ade910fb":"Accuracy= (TP+TN)\/(TP+FN+FP+TN)  \nprint(Accuracy)","4fa24407":"Precision = TP \/ (TP+FP) \nprint(Precision)","1115e0ee":"Recall = TP\/(TP+FN) \nprint(Recall)","a0aab6b4":"Specificity = TN \/ (TN+FP)\nprint(Specificity)","548f8f16":"F1_Score = 2 * Precision * Recall \/ (Precision + Recall)\nprint(F1_Score)","8f7e0d34":"## False Positive Rate or Fall Out or Probability of False Alarm\nFPR= FP \/ (FP+ TN )\nprint(FPR)","dfb908f2":"# False Negative Rate or Miss Rate:\nFNR = FN\/(FN+TP)\nprint(FNR)","a1768f5a":"Prevalence=   (TP + FN) \/ (TP + TN + FP + FN) \nprint(Prevalence)","38dd503a":"FPR,TPR,thresholds=metrics.roc_curve(y_test,Test['Outcome'])","b04a8d59":"plt.plot(FPR,TPR)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"FPR  or (1- Specificity)\")\nplt.ylabel(\"TPR or Sensitivity\")\nplt.title(\"ROC - Receiver Operating Characteristics\")","9804f0c4":"AUC=metrics.accuracy_score(y_test,Test['Outcome'])\nprint(AUC)","6c1bf54d":"# ROC on Train data\nFPR,TPR,thresholds=metrics.roc_curve(y_train,y_train_pred)\nplt.plot(FPR,TPR)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel(\"FPR  or (1- Specificity)\")\nplt.ylabel(\"TPR or Sensitivity\")\nplt.title(\"ROC - Receiver Operating Characteristics\")","3200c035":"AUC=metrics.accuracy_score(y_train,y_train_pred)\nprint(AUC)","cc7e1378":"Test.head()","404a538a":"numbers = [float(x)\/10 for x in range(11) ]\nfor i in numbers:\n    Test[i]=Test.PredicbtedOutcome.map(lambda x: 1 if x > i else 0)\nTest.head()","0aabaa20":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(Test['Outcome'], Test[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","50313a8c":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()\n","5dc7e1e0":"Test['final_predicted'] = Test.PredicbtedOutcome.map( lambda x: 1 if x > 0.7 else 0)\nTest.head()","6611dd20":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_test, Test['final_predicted'])","8281d523":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_test, Test['final_predicted'])\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","c3c517c8":"#### Precision : Precision is a measure that tells us what proportion of customers that we classified as diabetes, \n     actually had diabetes. The predicted positives (Customers predicted as diabetes are TP and FP) and the customers \n     actually  diabetes are TP.\n\n    Precision = TP \/ (TP+FP) ","a2eccd33":"#### From the curve above, 0.7 is the optimum point to take it as a cutoff probability","0e497951":"###  Accuracy : (TP+TN)\/(TP+FN+FP+TN)","86c4eeb0":"<h5>Context<\/h5>\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic\nmeasurements included in the dataset. Several constraints were placed on the selection of these instances from a larger \ndatabase. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n<h5>Content<\/h5>\nThe datasets consists of several medical predictor variables and one target variable, Outcome. \nPredictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","ba5f36fb":"## Feature Information\n    Pregnancies -- Number of times pregnant\n    Glucose-- Plasma glucose concentration a 2 hours in an oral glucose tolerance test,\n    BloodPressure-- Diastolic blood pressure (mm Hg),\n    SkinThickness -- Triceps skin fold thickness (mm),\n    Insulin--2-Hour serum insulin (mu U\/ml),\n    BMI -- Body mass index (weight in kg\/(height in m)^2),\n    DiabetesPedigreeFunction--Diabetes pedigree function(a function which scores likelihood of diabetes based on family history)\n    Age-- Age (years),\n    Outcome -- Class variable (0 or 1) 268 of 768 are 1, the others are 0","417fba2a":"#### Recall or Sensitivity or Hit Rate or True Positive Rate:\n               Recall is a measure that tells us what proportion of customers that actually had churned and  was classified by the algorithm as churned. The actual positives (Customers who had churned are TP and FN) and the customers classified by the model as churn are TP. (Note: FN is included because the Person actually had a churned even though the model predicted otherwise).\n\n        Recall = TP\/(TP+FN) \n","b66f2835":"#### Hence we can see decided threshold based on precision and recall is absolutely correct!!!!","dcc88094":"###  A receiver operating characteristics (ROC) : \n    ROC graphs are two-dimensional graphs in which TPR is plotted on the Y axis and FPR is plotted on the\n    X axis. An ROC graph depicts relative tradeoffs between benefits (true positives) and costs (false positives).","3c5b0cd1":"# Prevalence:\n      Prevalence = Actual Positive \/ Total Population\n                  =   TP + FN \/ TP + TN + FP + FN \n","d4108bc3":"## Precision and Recall threshold","896bfcd5":"### F1 Score ","e9196ea6":"## Specificity or Selectivity or True Negative Rate:\n           Specificity is a measure that tells us what proportion of patient that did NOT have diabetes, were predicted by the           model as non-diabetes. The actual negatives (patient actually NOT churned are FP and TN) and the customers classified          by us not as not churned are TN. (Note: FP is included because the Person did NOT actually have churned even though            the model     predicted     as churned).\n\n    Specificity = TN \/ (TN+FP) \n"}}