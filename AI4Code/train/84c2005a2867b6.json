{"cell_type":{"d97a4eb1":"code","dee5e0b8":"code","8b6f5dde":"code","22b4fcba":"code","da02f7d2":"code","a8c922e7":"code","532977c3":"code","c7aeb2c7":"code","cc2f99f3":"code","fca25b76":"code","1a97f38b":"markdown","47c2c349":"markdown","7d87be88":"markdown"},"source":{"d97a4eb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dee5e0b8":"data=pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","8b6f5dde":"data.info() #to see our data","22b4fcba":"data.head() #first five row","da02f7d2":"#converting \"class\" labels string to integer\ndata[\"class\"]= [0 if i == \"Abnormal\" else 1 for i in data[\"class\"]]\n\nx=data.drop([\"class\"], axis=1) # all features except \"class\".\ny=data[\"class\"].values.reshape(-1,1)\n     \n","a8c922e7":"X=(x-np.min(x))\/(np.max(x)-np.min(x))","532977c3":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=2)\n\n","c7aeb2c7":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n","cc2f99f3":"#Accuracy\nprint(\"Accuracy for {} : {}\".format(3,knn.score(x_test,y_test)))","fca25b76":"#Finding best k value:\n\nscore_list=[]\nfor i in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","1a97f38b":"So we can see that the best k value is 11.","47c2c349":"What is KNN?\n\nKNN is a classification algorithm. \nWe have four steps for this algorithm.\n1-)Choose the K value\n2-)Find closest data points\n3-)Find out which class the nearest neighbor points belong to\n4-)Find out which class the point which we test belong to\n\n","7d87be88":"Normalization in KNN:\n\nThe goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. This scale can be between 0 and 1.\n\nsome values may dominate other values. So to prevent this situation, we use normalization.\n"}}