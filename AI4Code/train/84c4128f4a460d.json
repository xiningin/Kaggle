{"cell_type":{"609bb362":"code","2426160e":"code","1ff054bd":"code","08a91806":"code","e382533b":"code","46bb21cd":"code","b2fcb088":"code","4e1f96f6":"code","c616edd4":"code","6fa0f1f2":"code","43fc9e2b":"code","8062df66":"code","b568a12a":"code","477e717d":"code","1c77dbd5":"code","2e4a5723":"code","85efc421":"code","b4cf46fc":"code","4e1102de":"markdown","86e4b76d":"markdown","2607fdb9":"markdown","091ee791":"markdown","ffdab4cd":"markdown"},"source":{"609bb362":"import os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom pytz import timezone\nfrom dateutil import tz\nfrom datetime import datetime, timedelta\nimport geojson\nimport geopandas as gpd  \nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport osmnx as ox\n\n\nimport os \n# Disable warnings, set Matplotlib inline plotting and load Pandas package\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n#pd.options.display.mpl_style = 'default'\nfrom datetime import datetime\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nfrom dateutil import tz\nimport geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom geopandas.tools import sjoin\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.cm as cm\nimport folium","2426160e":"# def getDuplicateColumns(df):\n#     '''\n#     Get a list of duplicate columns.\n#     It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n#     :param df: Dataframe object\n#     :return: List of columns whose contents are duplicates.\n#     '''\n#     duplicateColumnNames = set()\n#     # Iterate over all the columns in dataframe\n#     for x in range(df.shape[1]):\n#         # Select column at xth index.\n#         col = df.iloc[:, x]\n#         # Iterate over all the columns in DataFrame from (x+1)th index till end\n#         for y in range(x + 1, df.shape[1]):\n#             # Select column at yth index.\n#             otherCol = df.iloc[:, y]\n#             # Check if two columns at x 7 y index are equal\n#             if col.equals(otherCol):\n#                 duplicateColumnNames.add(df.columns.values[y])\n \n#     return list(duplicateColumnNames)","1ff054bd":"def getDuplicateColumns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n    for t, v in groups.items():\n        dcols = frame[v].to_dict(orient=\"list\")\n\n        vs = list(dcols.values())\n        ks = list(dcols.keys())\n        lvs = len(vs)\n\n        for i in range(lvs):\n            vsi = vs[i]\n            for j in range(i+1, lvs):\n                vsj = vs[j]\n                if vsi == vsj: \n                    dups.append(ks[i])\n                    break\n\n    return dups","08a91806":"df_bruxelles = gpd.read_file('..\/input\/belgium-obu\/Anderlecht_streets.json')\nprint('Anderlecht total number of streets '+str(df_bruxelles.shape[0]))\n\n\npolygons = df_bruxelles\nm = folium.Map([50.85045, 4.34878], zoom_start=13, tiles='cartodbpositron')\nfolium.GeoJson(polygons).add_to(m)\nm","e382533b":"DF_5 = pd.read_csv('..\/input\/belgium-obu\/And_05min_0101_0103_2019.csv', header=None)\nDF_5.columns = ['datetime','street_id','count','vel']\nnRow_5, nCol_5 = DF_5.shape\n\nDF_15 = pd.read_csv('..\/input\/belgium-obu\/And_15min_0101_0103_2019.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('..\/input\/belgium-obu\/And_30min_0101_0103_2019.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\n\n# print(f'in Anderlecht 1 min there are {nRow_1} rows and {nCol_1} columns')\nprint(f'in Anderlecht 5 min there are {nRow_5} rows and {nCol_5} columns')\nprint(f'in Anderlecht 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in Anderlecht 30 min there are {nRow_30} rows and {nCol_30} columns')\n","46bb21cd":"table_5 = DF_5.pivot_table(index='datetime', columns='street_id')['count']\ntable_5 = table_5.fillna(0)\n\ntable_vel_5 = DF_5.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_5 = table_vel_5.fillna(0)\n\nprint(table_5.shape)\nprint('')\n\ntable_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\nprint(table_15.shape)\nprint('')\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\nprint(table_30.shape)\nprint('')\n\n","b2fcb088":"list_duplicates = getDuplicateColumns(table_vel_5)\nprint(len(list_duplicates))","4e1f96f6":"file_name = 'Flow_Anderlecht_street_5min'\nprint(file_name)\ntable_5 = table_5.reset_index().drop(list_duplicates, axis=1)\ntable_5.to_csv(file_name + '.csv',index=False)\nprint(table_5.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_5min'\nprint(file_name)\ntable_vel_5 = table_vel_5.reset_index().drop(list_duplicates, axis=1)\ntable_vel_5.to_csv(file_name + '.csv',index=False)\nprint(table_vel_5.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_Anderlecht_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index().drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index().drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_Anderlecht_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index().drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_Anderlecht_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index().drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n","c616edd4":"df_bruxelles = gpd.read_file('..\/input\/belgium-obu\/Bruxelles_streets.json')\nprint('Bruxelles total number of streets '+str(df_bruxelles.shape[0]))\n\n\npolygons = df_bruxelles\nm = folium.Map([50.85045, 4.34878], zoom_start=12, tiles='cartodbpositron')\nfolium.GeoJson(polygons).add_to(m)\nm","6fa0f1f2":"DF_5 = pd.read_csv('..\/input\/belgium-obu\/Bxl_05min_0101_0103_2019.csv', header=None)\nDF_5.columns = ['datetime','street_id','count','vel']\nnRow_5, nCol_5 = DF_5.shape\n\nDF_15 = pd.read_csv('..\/input\/belgium-obu\/Bxl_15min_0101_0103_2019.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('..\/input\/belgium-obu\/Bxl_30min_0101_0103_2019.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\n\n\nprint(f'in BXL 5 min there are {nRow_5} rows and {nCol_5} columns')\nprint(f'in BXL 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in BXL 30 min there are {nRow_30} rows and {nCol_30} columns')\n","43fc9e2b":"table_5 = DF_5.pivot_table(index='datetime', columns='street_id')['count']\ntable_5 = table_5.fillna(0)\n\ntable_vel_5 = DF_5.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_5 = table_vel_5.fillna(0)\n\nprint(table_5.shape)\nprint('')\n\n\ntable_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\nprint(table_15.shape)\nprint('')\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\nprint(table_30.shape)\nprint('')\n\n","8062df66":"list_duplicates = getDuplicateColumns(table_vel_5)\nprint(len(list_duplicates))","b568a12a":"file_name = 'Flow_BXL_street_5min'\nprint(file_name)\ntable_5 = table_5.reset_index().drop(list_duplicates, axis=1)\ntable_5.to_csv(file_name + '.csv',index=False)\nprint(table_5.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_5min'\nprint(file_name)\ntable_vel_5 = table_vel_5.reset_index().drop(list_duplicates, axis=1)\ntable_vel_5.to_csv(file_name + '.csv',index=False)\nprint(table_vel_5.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BXL_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index().drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index().drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BXL_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index().drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_BXL_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index().drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n","477e717d":"df_belgium = gpd.read_file('..\/input\/belgium-obu\/Belgium_streets.json')\nprint('Belgium total number of highways '+str(df_belgium.shape[0]))\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\nm","1c77dbd5":"DF_5 = pd.read_csv('..\/input\/belgium-obu\/Bel_05min_0101_0103_2019.csv', header=None)\nDF_5.columns = ['datetime','street_id','count','vel']\nnRow_5, nCol_5 = DF_5.shape\n\nDF_15 = pd.read_csv('..\/input\/belgium-obu\/Bel_15min_0101_0103_2019.csv', header=None)\nDF_15.columns = ['datetime','street_id','count','vel']\nnRow_15, nCol_15 = DF_15.shape\n\nDF_30 = pd.read_csv('..\/input\/belgium-obu\/Bel_30min_0101_0103_2019.csv', header=None)\nDF_30.columns = ['datetime','street_id','count','vel']\nnRow_30, nCol_30 = DF_30.shape\n\nDF_60 = pd.read_csv('..\/input\/belgium-obu\/Bel_60min_0101_0103_2019.csv', header=None)\nDF_60.columns = ['datetime','street_id','count','vel']\nnRow_60, nCol_60 = DF_60.shape\n\n# print(f'in BEL 5 min there are {nRow_5} rows and {nCol_5} columns')\nprint(f'in BEL 15 min there are {nRow_15} rows and {nCol_15} columns')\nprint(f'in BEL 30 min there are {nRow_30} rows and {nCol_30} columns')\nprint(f'in BEL 60 min there are {nRow_60} rows and {nCol_60} columns')","2e4a5723":"table_5 = DF_5.pivot_table(index='datetime', columns='street_id')['count']\ntable_5 = table_5.fillna(0)\n\ntable_vel_5 = DF_5.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_5 = table_vel_5.fillna(0)\n\ntable_15 = DF_15.pivot_table(index='datetime', columns='street_id')['count']\ntable_15 = table_15.fillna(0)\n\ntable_vel_15 = DF_15.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_15 = table_vel_15.fillna(0)\n\ntable_30 = DF_30.pivot_table(index='datetime', columns='street_id')['count']\ntable_30 = table_30.fillna(0)\n\ntable_vel_30 = DF_30.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_30 = table_vel_30.fillna(0)\n\ntable_60 = DF_60.pivot_table(index='datetime', columns='street_id')['count']\ntable_60 = table_60.fillna(0)\n\ntable_vel_60 = DF_60.pivot_table(index='datetime', columns='street_id')['vel']\ntable_vel_60 = table_vel_60.fillna(0)\n\nprint(table_60.shape)\nprint('')","85efc421":"list_duplicates = getDuplicateColumns(table_vel_5)\nprint(len(list_duplicates))","b4cf46fc":"file_name = 'Flow_BEL_street_5min'\nprint(file_name)\ntable_5 = table_5.reset_index().drop(list_duplicates, axis=1)\ntable_5.to_csv(file_name + '.csv',index=False)\nprint(table_5.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_5min'\nprint(file_name)\ntable_vel_5 = table_vel_5.reset_index().drop(list_duplicates, axis=1)\ntable_vel_5.to_csv(file_name + '.csv',index=False)\nprint(table_vel_5.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BEL_street_15min'\nprint(file_name)\ntable_15 = table_15.reset_index().drop(list_duplicates, axis=1)\ntable_15.to_csv(file_name + '.csv',index=False)\nprint(table_15.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_15min'\nprint(file_name)\ntable_vel_15 = table_vel_15.reset_index().drop(list_duplicates, axis=1)\ntable_vel_15.to_csv(file_name + '.csv',index=False)\nprint(table_vel_15.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BEL_street_30min'\nprint(file_name)\ntable_30 = table_30.reset_index().drop(list_duplicates, axis=1)\ntable_30.to_csv(file_name + '.csv',index=False)\nprint(table_30.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_30min'\nprint(file_name)\ntable_vel_30 = table_vel_30.reset_index().drop(list_duplicates, axis=1)\ntable_vel_30.to_csv(file_name + '.csv',index=False)\nprint(table_vel_30.shape)\nprint('')\nprint('')\n\n\nfile_name = 'Flow_BEL_street_60min'\nprint(file_name)\ntable_60 = table_60.reset_index().drop(list_duplicates, axis=1)\ntable_60.to_csv(file_name + '.csv',index=False)\nprint(table_60.shape)\nprint('')\n\nfile_name = 'Velocity_BEL_street_60min'\nprint(file_name)\ntable_vel_60 = table_vel_60.reset_index().drop(list_duplicates, axis=1)\ntable_vel_60.to_csv(file_name + '.csv',index=False)\nprint(table_vel_60.shape)","4e1102de":"## Genarl Import","86e4b76d":"## To start traffic Predictions check notebooks:\n\n#### * LSTM encoder decoder (Best) - https:\/\/www.kaggle.com\/giobbu\/lstm-encoder-decoder-tensorflow ;\n#### * simple LSTM (Worse) - https:\/\/www.kaggle.com\/giobbu\/simple-lstm-tensorflow ;\n#### * seasonal persistence model (Baseline) - https:\/\/www.kaggle.com\/giobbu\/seasonal-persistence-model.","2607fdb9":"# Anderlecht Streets","091ee791":"# Bruxelles Streets","ffdab4cd":"# Belgium Streets"}}