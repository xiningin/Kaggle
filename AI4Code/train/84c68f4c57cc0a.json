{"cell_type":{"787d5bf2":"code","6ebb180e":"code","5ded2a72":"code","d0f0c726":"code","a490762d":"code","6994bd2f":"code","5b28c516":"code","3436cbaa":"code","69ed7d2d":"code","5f0dbee4":"code","930dfa9d":"code","3b7e00f6":"code","b21b34b9":"code","2f826dc0":"code","a41f2c85":"code","0f07b5b3":"code","4b3a13a7":"code","2499e581":"code","b1e630e4":"code","196e3242":"markdown"},"source":{"787d5bf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ebb180e":"train_data = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv', index_col='Id')\ntrain_data.head()","5ded2a72":"from sklearn.model_selection import train_test_split\ntrain_data = train_data[train_data['Cover_Type'] != 5] # because we have only a single sample labeled 5\ny_train = train_data['Cover_Type']\nX_train = train_data.drop(['Cover_Type'], axis=1)\n\n# corrMatrix = X_train.corr(method='pearson', min_periods=1)\n# corrMatrix.style.background_gradient(axis=None)","d0f0c726":"cor_targ = train_data.corrwith(train_data[\"Cover_Type\"]).reset_index()\ncor_targ.columns =['feature', 'CorrelatioWithTarget']\ncor_targ.sort_values('CorrelatioWithTarget',ascending = False).T","a490762d":"X_train.drop(['Soil_Type15', 'Soil_Type7'], axis=1, inplace=True)","6994bd2f":"ax = plt.figure(figsize=(12, 6))\ncover_type= train_data['Cover_Type'].value_counts().sort_index()\nprint(cover_type)\nsn.barplot(x=cover_type.index, y=cover_type,palette=\"BuPu_r\")\nplt.show()","5b28c516":"plt.figure(figsize=(15,8))\nfeatures = train_data.columns.values[0:54]\nsn.histplot(train_data[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsn.histplot(test_data[features].mean(axis=1),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per row in the train and test data\")\nplt.legend()\nplt.show()","3436cbaa":"plt.figure(figsize=(15,8))\nfeatures = train_data.columns.values[0:54]\nsn.distplot(train_data[features].mean(axis=0),color=\"red\", kde=True,bins=120, label='train')\nsn.distplot(test_data[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per row in the train and test data\")\nplt.legend()\nplt.show()","69ed7d2d":"# num_cols = [\n#     \"Elevation\",\n#     \"Aspect\",\n#     \"Slope\",\n#     \"Horizontal_Distance_To_Hydrology\",\n#     \"Vertical_Distance_To_Hydrology\",\n#      \"Horizontal_Distance_To_Roadways\",\n#     \"Hillshade_9am\",\n#     \"Hillshade_Noon\",\n#     \"Hillshade_3pm\",\n#     \"Horizontal_Distance_To_Fire_Points\",\n# ]\n# i = 1\n# plt.figure()\n# fig, ax = plt.subplots(figsize=(18, 15))\n# for col in num_cols:\n#     plt.subplot(5,3,i)\n#     sn.histplot(train_data[col],color=\"yellow\", kde=True,bins=100, label='train')\n#     sn.histplot(test_data[col],color=\"Darkblue\", kde=True,bins=100, label='test')\n#     i += 1\n# plt.legend()\n# plt.title(\" numirical features Distribution in both train and test data\")  \n# plt.show()","5f0dbee4":"test_data.drop(['Soil_Type15', 'Soil_Type7'], axis=1, inplace=True)","930dfa9d":"label_encoder = LabelEncoder()\n# X_train = X_train.loc[train.Cover_Type != 5]\n\nX = X_train.astype(np.float32).to_numpy()\ny = label_encoder.fit_transform(y_train)\nfeat_names = X_train.columns\nnum_class = len(label_encoder.classes_)\nX_test = test_data.astype(np.float32).to_numpy()\ndel test_data\n\noof_proba = np.zeros((len(y), num_class), dtype=np.float32)\ntest_proba = np.zeros((len(X_test), num_class), dtype=np.float32)","3b7e00f6":"from sklearn.model_selection import StratifiedKFold\nscaler = MinMaxScaler() \n# lm = LogisticRegression(multi_class='ovr', solver='liblinear')\n# lm =  RandomForestClassifier(n_estimators=i,random_state=0)\nparams = {\n    'num_class': num_class,\n    'objective': 'multi:softprob',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'eval_metric': ['merror', 'mlogloss'],\n    'learning_rate': .09,\n    'max_depth': 0,\n    'subsample': .15,\n    'sampling_method': 'gradient_based',\n    'seed': 64,\n    'grow_policy': 'lossguide',\n    'max_leaves': 255,\n    'lambda': 100,\n}\nmodel = xgb.XGBClassifier(params=params, num_boost_round=600, early_stopping_rounds=30,\n        verbose_eval=100)\nmy_pipeline = Pipeline(steps=[('preprocessor', scaler),\n                ('model', model)\n                ])\nparam_grid = {\n    \"model__n_estimators\": [20, 50, 100, 200],\n    \"model__learning_rate\": [0.01, 0.1, 0.05],\n}\n\n\n\nfit_params = {\"model__early_stopping_rounds\": 10, \n              \"model__verbose\": False} \n\n# skf = StratifiedKFold(n_splits=2, random_state=0, shuffle=True)\nX_train, X_val, y_train, y_val = train_test_split(X, y, shuffle=True, test_size=0.33, stratify=y)\nevalset = [(X_val,y_val)]\nmodel.fit(X_train, y_train, eval_set=evalset, verbose=False)\n    \nyhat = model.predict(X_val)\nscore = accuracy_score(y_val, yhat)\nprint('Accuracy: %.3f' % score)\n\nresults = model.evals_result()\n# plot learning curves\nplt.plot(results['validation_0']['mlogloss'], label='validation')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n\n## Stratified K-fold - took too long to train so I omitted it.\n# for train_index, val_index in skf.split(X, y):\n# #     dval = xgboost.DMatrix(X[val_idx], label=y[val_idx])\n#     X_train, X_val = X[train_index], X[val_index]\n#     y_train, y_val = y[train_index], y[val_index]\n    \n#     evalset = [(X_val,y_val)]\n#     model.fit(X_train, y_train, eval_set=evalset, verbose=False)\n    \n#     yhat = model.predict(X_val)\n#     score = accuracy_score(y_val, yhat)\n#     print('Accuracy: %.3f' % score)\n#     # retrieve performance metrics\n#     results = model.evals_result()\n#     # plot learning curves\n#     plt.plot(results['validation_0']['mlogloss'], label='train')\n#     plt.plot(results['validation_1']['mlogloss'], label='test')\n#     # show the legend\n#     plt.legend()\n# # show the plot\n#     plt.show()\n    \n    \n\n# searchCV = GridSearchCV(my_pipeline, cv=5, param_grid=param_grid,scoring='accuracy')\n# searchCV.fit(X_train, y_train)  \n# cross_val_scores = cross_val_score(my_pipeline, X_train, y_train,\n#                               cv=5,\n#                               scoring='accuracy')\n# lm.fit(X_train, y_train)\n# y_pred = lm.predict(X_valid)\n# print(cross_val_scores)","b21b34b9":"y_pred = model.predict(X_test)","2f826dc0":"results","a41f2c85":"df_sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv', usecols=['Id'])\ndf_sub['Cover_type'] = pd.Series(label_encoder.inverse_transform(y_pred))\ndf_sub.to_csv('submission.csv', index=False)\n","0f07b5b3":"df_sub.tail()","4b3a13a7":"! kaggle competitions submit -c tabular-playground-series-dec-2021 -f .\/submission.csv","2499e581":"!kaggle competitions submit -c tabular-playground-series-dec-2021 -f .\/submission.csv","b1e630e4":"!ls -alh ~\/.kaggle","196e3242":"> EDA from https:\/\/www.kaggle.com\/mhslearner\/tps-dec-eda-resnet\/notebook"}}