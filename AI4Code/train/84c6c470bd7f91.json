{"cell_type":{"aeb0fe27":"code","43b44c32":"code","16a0c99d":"code","6457e8e5":"code","f3813243":"code","f52f01f4":"code","8bd764ab":"code","ddcb756d":"code","e44e08eb":"code","6bf8d7f2":"code","6c81e275":"code","bc558974":"code","392e8991":"code","8ad4fc7d":"code","9ddd1694":"code","062da481":"code","27ed69c6":"code","5d1710e4":"code","86daefd4":"code","b2410a2c":"code","bd69709f":"code","bc98c6ab":"code","63264e3b":"code","8dd6f42f":"code","a8d7d1f1":"code","fc1844f4":"code","ec942a60":"code","ec95018a":"code","59e69dd1":"code","1350842d":"code","8caf2cb5":"code","5b36471a":"code","6a148d32":"code","59941ff3":"code","9875542f":"code","502b4daa":"code","ffd5a593":"code","360afec1":"code","acc54bf7":"code","db5cc532":"code","cc1f0fd7":"code","c43dce61":"code","27df5ed0":"code","ffd202a9":"code","472ca72a":"code","d1cf5bdc":"code","fc9e58a4":"markdown","1134e7fb":"markdown","bdb313b5":"markdown","332a1737":"markdown","07ade2a5":"markdown","9a82c57a":"markdown","fcb83324":"markdown"},"source":{"aeb0fe27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43b44c32":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","16a0c99d":"train.head()","6457e8e5":"test.head()","f3813243":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show() ","f52f01f4":"corr = train.corr()\ncorr['SalePrice']","8bd764ab":"#plt.figure(figsize=(30,8))\n#sns.heatmap(train.corr(),cmap='coolwarm',annot = True)\n#plt.show()","ddcb756d":"\nvariables_most_corr = corr[corr['SalePrice']>0.3].index.tolist()\nvariables_most_corr","e44e08eb":"train = train[variables_most_corr]\n\nvariables_most_corr.remove('SalePrice')\n\ntest = test[variables_most_corr]","6bf8d7f2":"lenc = len(train.columns)\n\nfor c in train.columns:\n    sns.lmplot(x=c ,y='SalePrice',data=train)","6c81e275":"missing_values_columns = train.isnull().sum().sort_values(ascending=False)\nmissing_values_columns","bc558974":"#train = train.drop(columns=['LotFrontage'])\n#test = test.drop(columns=['LotFrontage'])","392e8991":"train.dtypes","8ad4fc7d":"# Get categorical features - None in this case\n\n# Get numerical features - All in this case","9ddd1694":"train.head()","062da481":"train.isnull().sum().sort_values(ascending=False)","27ed69c6":"test.isnull().sum().sort_values(ascending=False)","5d1710e4":"# Handle missing values - numerical values\nprint(\"NAs for numerical features in train : \" + str(train.isnull().values.sum()))\ntrain = train.fillna(train.mean()) ##\nprint(\"Remaining NAs for numerical features in train : \" + str(train.isnull().values.sum()))\nprint(train.shape)","86daefd4":"# Handle missing values - numerical values\nprint(\"NAs for numerical features in test : \" + str(test.isnull().values.sum()))\ntest = test.fillna(train.mean()) ##\nprint(\"Remaining NAs for numerical features in test : \" + str(test.isnull().values.sum()))\nprint(test.shape)","b2410a2c":"train.columns","bd69709f":"train.shape","bc98c6ab":"test.columns","63264e3b":"train.head()","8dd6f42f":"from sklearn.model_selection import train_test_split\n\nX = train.drop('SalePrice', axis=1)\nY = train['SalePrice']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=101)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a8d7d1f1":"y_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_X.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)\n","fc1844f4":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nlm_pred = lm.predict(X_test)","ec942a60":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, lm_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, lm_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lm_pred)))","ec95018a":"from sklearn import ensemble\n\ngbr = ensemble.GradientBoostingRegressor(loss='ls', learning_rate=0.01, max_depth=5, n_estimators = 1110)\ngbr.fit(X_train,y_train)\ngbr_pred = gbr.predict(X_test)\n\nfrom sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, gbr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, gbr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, gbr_pred)))","59e69dd1":"from sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor(random_state = 100)\ndtreg.fit(X_train, y_train)\ndtr_pred = dtreg.predict(X_test)","1350842d":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, dtr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))","8caf2cb5":"from sklearn.svm import SVR\nsvr = SVR(kernel = 'rbf')\nsvr.fit(X_train, y_train)\nsvr_pred = svr.predict(X_test)","5b36471a":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, svr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, svr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))","6a148d32":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 0)\nrfr.fit(X_train, y_train)\nrfr_pred= rfr.predict(X_test)","59941ff3":"print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, rfr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))","9875542f":"import lightgbm as lgb\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=12,\n                              learning_rate=0.02, n_estimators=1000,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 4, feature_fraction = 0.2319,\n                              feature_fraction_seed=10, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodel_lgb.fit(X_train,y_train)\nlgb_pred = model_lgb.predict(X_test)\n\nprint('MAE:', metrics.mean_absolute_error(y_test, lgb_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, lgb_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lgb_pred)))","502b4daa":"error_rate=np.array([metrics.mean_squared_error(y_test, lm_pred),\n                     metrics.mean_squared_error(y_test, gbr_pred),\n                     metrics.mean_squared_error(y_test, dtr_pred),\n                     metrics.mean_squared_error(y_test, svr_pred),\n                     metrics.mean_squared_error(y_test, rfr_pred)])\n\nplt.figure(figsize=(16,5))\nplt.plot(error_rate)","ffd5a593":"test","360afec1":"a = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntest_id = a['Id']\na = pd.DataFrame(test_id, columns=['Id'])","acc54bf7":"test = sc_X.fit_transform(test)","db5cc532":"test.shape","cc1f0fd7":"test_prediction_lgbm=gbr.predict(test)\ntest_prediction_lgbm= test_prediction_lgbm.reshape(-1,1)","c43dce61":"test_prediction_lgbm =sc_y.inverse_transform(test_prediction_lgbm)","27df5ed0":"test_prediction_lgbm = pd.DataFrame(test_prediction_lgbm, columns=['SalePrice'])\ntest_prediction_lgbm.head()","ffd202a9":"result = pd.concat([a,test_prediction_lgbm], axis=1)\n","472ca72a":"result.head()","d1cf5bdc":"result.to_csv('submission.csv',index=False)","fc9e58a4":"# Light GBM","1134e7fb":"# Decision Tree Regressor","bdb313b5":"# Gradient Boosting Regressor","332a1737":"# Support Vector Regressor","07ade2a5":"MAE: 0.23018546067505324\n\nMSE: 0.13491787118250295\n\nRMSE: 0.36731168124972957\n","9a82c57a":"# Simple Linear Regression","fcb83324":"# Random Forest Regressor"}}