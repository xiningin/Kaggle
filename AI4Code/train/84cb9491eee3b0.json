{"cell_type":{"5c06edb5":"code","0ad863ea":"code","5aff2a2f":"code","e80f3b07":"code","a44d2bc1":"code","27f47f1f":"code","05d87963":"code","12a095b1":"code","20a39326":"code","e10dd40d":"code","bf6828bc":"code","daf572b5":"code","db3fab66":"code","664b4c86":"code","ca7595af":"code","45f62d5e":"code","16a22f25":"code","239de0b1":"code","904952c4":"code","0af31f03":"code","56f26f60":"code","436b180d":"code","80a9b915":"code","6795b84d":"code","d056911a":"code","d5cbfaa3":"code","f6055af0":"markdown","01c224c2":"markdown","0cddba10":"markdown","1388b19c":"markdown","d21552fd":"markdown","19c166fe":"markdown","4f591dfc":"markdown"},"source":{"5c06edb5":"## Imports (code & data)\nimport re\nimport pandas as pd\nimport yake_helper_funcs as yhf\nfrom datetime import datetime, timedelta\nfrom math import sqrt, floor\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\nimport itertools\nfrom matplotlib import pyplot as plt\nimport removing_polite_posts as rpp\nfrom flashtext.keyword import KeywordProcessor\nimport string\nimport nltk\nimport math\n\nforum_posts = pd.read_csv(\"..\/input\/meta-kaggle\/ForumMessages.csv\")\n\n# read in pre-tuned vectors\nvectors = pd.read_csv(\"..\/input\/fine-tuning-word2vec-2-0\/kaggle_word2vec.model\", \n                      delim_whitespace=True,\n                      skiprows=[0], \n                      header=None\n                     )\n\n# set words as index rather than first column\nvectors.index = vectors[0]\nvectors.drop(0, axis=1, inplace=True)","0ad863ea":"## Utility functions\n\n# get vectors for each word in post\n# TODO: can we vectorize this?\ndef vectors_from_post(post):\n    all_words = [] \n\n    for words in post:\n        all_words.append(words) \n        \n    return(vectors[vectors.index.isin(all_words)])\n\n\n# create document embeddings from post\ndef doc_embed_from_post(post):\n    test_vectors = vectors_from_post(post)\n\n    return(test_vectors.mean())\n\n# explore our posts by cluster\ndef get_keyword_set_by_cluster(number):\n    cluster_index = list(clustering.labels_ == number)\n    return(list(itertools.compress(keyword_sets, cluster_index)))\n\n# get sample post info by #\ndef get_post_info_by_cluster(number, \n                             data,\n                             cluster):\n    return(data[cluster.labels_ == number])\n\n# remove HTML stuff\n# https:\/\/medium.com\/@jorlugaqui\/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\ndef remove_html_tags(text):\n    clean = re.compile('<.*?>')\n    return(re.sub(clean, '', text))\n\n# remove \"good\", \"nice\", \"thanks\", etc\ndef remove_thanks(text):\n    text = text.lower()\n    \n    text = re.sub(\"nice\", \"\", text)\n    text = re.sub(\"thank.*\\s\", \" \", text)\n    text = re.sub(\"good\",\"\", text)\n    text = re.sub(\"hi\", \"\", text)\n    text = re.sub(\"hello\", \"\", text)\n    \n    return(text)\n\ndef polite_post_index(forum_posts):\n    '''Pass in a list of fourm posts, get\n    back the indexes of short, polite ones.'''\n    \n    polite_indexes = []\n    \n    # create  custom stop word list to identify polite forum posts\n    stop_word_list = [\"no problem\", \"thanks\", \"thx\", \"thank\", \"great\",\n                      \"nice\", \"interesting\", \"awesome\", \"perfect\", \n                      \"amazing\", \"well done\", \"good job\"]\n\n    # create a KeywordProcess\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keywords_from_list(stop_word_list)\n\n    # test our keyword processor\n    for i,post in enumerate(forum_posts):\n        post = post.lower().translate(str.maketrans({a:None for a in string.punctuation}))\n        \n        if len(post) < 100:\n            keywords_found = keyword_processor.extract_keywords(post.lower(), span_info=True)\n            if keywords_found:\n                polite_indexes.append(i)\n\n    return(polite_indexes)","5aff2a2f":"## Hyperprameters\n\n# number of clusters currently based on the square root of the # of posts\ndays_of_posts = 1","e80f3b07":"# For sample posts, get forum title and topic title\n# based on queries from https:\/\/www.kaggle.com\/pavlofesenko\/strategies-to-earn-discussion-medals\ntopics = pd.read_csv('..\/input\/meta-kaggle\/\/ForumTopics.csv').rename(columns={'Title': 'TopicTitle'})\nforums = pd.read_csv('..\/input\/meta-kaggle\/Forums.csv').rename(columns={'Title': 'ForumTitle'})\n\ndf1 = pd.merge(forum_posts[['ForumTopicId', 'PostDate', 'Message']], topics[['Id', 'ForumId', 'TopicTitle']], left_on='ForumTopicId', right_on='Id')\ndf1 = df1.drop(['ForumTopicId', 'Id'], axis=1)\n\nforum_posts = pd.merge(df1, forums[['Id', 'ForumTitle']], left_on='ForumId', right_on='Id')\nforum_posts = forum_posts.drop(['ForumId', 'Id'], axis=1)\nforum_posts.head()","a44d2bc1":"# parse dates\nforum_posts['Date'] = pd.to_datetime(forum_posts.PostDate, format=\"%m\/%d\/%Y %H:%M:%S\")\n\n# posts from the last X days\nstart_time = datetime.now() + timedelta(days=-days_of_posts)  \n\n# forum posts from last week (remember to convert to str)\nsample_post_info = forum_posts.loc[forum_posts.Date > start_time]\nsample_posts = sample_post_info.Message.astype(str)\n\n# reindex from 0\nsample_posts.reset_index(drop=True)\nsample_post_info.reset_index(drop=True)\n\n# remove html tags\nsample_post_info.Message = sample_post_info.Message\\\n    .astype(str)\\\n    .apply(remove_html_tags)\nsample_posts = sample_posts.apply(remove_html_tags)\n\n# remove polite posts (make sure you remove HTML tags first)\npolite_posts = sample_posts.index[polite_post_index(sample_posts)]\n# posts aren't being dropped \nsample_posts = sample_posts.drop(polite_posts)\nsample_post_info = sample_post_info.drop(polite_posts)\n\n# number of posts\nnum_of_posts = sample_posts.shape[0]\n\n# Number of clusters is square root of the # of posts (rounded down)\nnumber_clusters = floor(sqrt(num_of_posts))","27f47f1f":"# extact keywords & tokenize\n#keywords = yhf.keywords_yake(sample_posts, )\nkeywords_tokenized = yhf.tokenizing_after_YAKE(sample_posts)\nkeyword_sets = [set(post) for post in keywords_tokenized]","05d87963":"# create empty array for document embeddings\ndoc_embeddings = np.zeros([num_of_posts, 300])\n\n# get document embeddings for posts\nfor i in range(num_of_posts):\n    embeddings = np.array(doc_embed_from_post(keyword_sets[i]))\n    if np.isnan(embeddings).any():\n        doc_embeddings[i,:] = np.zeros([1,300])\n    else:\n        doc_embeddings[i,:] = embeddings","12a095b1":"# the default k-means label assignment didn't work well\nclustering = SpectralClustering(n_clusters=number_clusters, \n                                assign_labels=\"discretize\",\n                                n_neighbors=number_clusters).fit(doc_embeddings)","20a39326":"# look at distrobution of cluster labels\npd.Series(clustering.labels_).value_counts()","e10dd40d":"for i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = clustering))\n    print(\"\\n\")\n    ","bf6828bc":"# for i in range(number_clusters):\n    \n#     print(f\"Cluster {i}:\\n\")\n#     print(get_keyword_set_by_cluster(i))\n#     print(\"\\n\")","daf572b5":"# count of posts\/cluster\ncluster_counts = pd.Series(clustering.labels_).value_counts()\n\n# get clusters bigger than expected\nmax_cluster_size = number_clusters * 2\nbig_clusters = cluster_counts[cluster_counts > max_cluster_size]","db3fab66":"# sub-cluster first (biggest) cluster\ncluster_label = big_clusters.index[0]\n\nsub_sample = sample_post_info[clustering.labels_ == cluster_label]\nsub_cluster_embeddings = doc_embeddings[clustering.labels_ == cluster_label]\n\nnumber_sub_clusters = floor(sqrt(sub_sample.shape[0]))\n\nsub_cluster = SpectralClustering(n_clusters=number_sub_clusters, \n                                 assign_labels=\"discretize\", \n                                 n_neighbors=number_sub_clusters).fit(sub_cluster_embeddings)","664b4c86":"# see how it looks\nfor i in range(number_sub_clusters):\n\n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, data = sub_sample, \n                                   cluster = sub_cluster))\n    print(\"\\n\")","ca7595af":"pd.Series(sub_cluster.labels_).value_counts()","45f62d5e":"from os import path\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","16a22f25":"# TODO why do I see thank you?\nposts_as_string = sample_post_info\\\n    .Message\\\n    .to_string(index=False)\n\n# shouldn't have to do this b\/c I removed polite posts earlier\nposts_as_string = remove_thanks(posts_as_string)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(posts_as_string)\n\n# Display the generated image:\n# the matplotlib way:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","239de0b1":"# next week: get saliency measure by day, \n# look at shift between sliency on day & in corpus as whole pick summary words","904952c4":"# TODO: \n# make sure to match preprocessing (lower cased)\n# for each cluster, find the nomralized saliencey measure \n# rank words based on difference in normalizd saliency in whole corpus\n\n# edge cases:\n# OOV words, add smoothing or set corpus freq. to 0\n# ","0af31f03":"frequency_table = pd.read_csv(\"..\/input\/kaggle-forum-term-frequency-unstemmed\/kaggle_lex_freq.csv\",\n                             error_bad_lines=False)","56f26f60":"def get_cluster_saliency_dict(cluster_number):\n    # create corpus from a cluster\n    text = get_post_info_by_cluster(cluster_number, data = sub_sample, cluster = sub_cluster)\\\n        .Message.astype(str).str.cat(sep=' ')\n\n    # tokenize\n    words = nltk.word_tokenize(text)\n\n    # Remove single-character tokens (mostly punctuation)\n    words = [word for word in words if len(word) > 1]\n\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n\n    # remove non-breaking space\n    words = [word for word in words if word != \"nbsp\"]\n\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n\n    # Calculate frequency distribution\n    fdist = nltk.FreqDist(words)\n\n    cluster_dict = dict() \n\n    # get saliency measures\n    for word, frequency in fdist.most_common():\n        saliency_measure_smoothed = math.log(frequency + 0.0001)\/(math.log(fdist.most_common(1)[0][1] + 0.0001))\n        cluster_dict[word] = saliency_measure_smoothed\n        \n    return(cluster_dict, fdist)","436b180d":"def get_surprising_words(cluster_number, frequency_table):\n    cluster_dict, fdist = get_cluster_saliency_dict(cluster_number)\n    \n    words = []\n    surprisal = []\n\n    for word, freq in fdist.most_common():\n        words.append(word)\n        surprisal_measure = cluster_dict[word] - frequency_table.saliency[frequency_table.word == word]\n        if surprisal_measure.empty:\n            surprisal.append(cluster_dict[word] - .0001)\n        else:\n            surprisal.append(surprisal_measure.values[0])\n\n    cluster_surprisal_measures = pd.DataFrame(list(zip(words, surprisal)), \n                                              columns =['Words', 'Surprisal']) \n\n    suprising_words = cluster_surprisal_measures.Words[cluster_surprisal_measures.Surprisal > 0]\n    \n    return(suprising_words)","80a9b915":"get_surprising_words(1, frequency_table)","6795b84d":"get_post_info_by_cluster(1, data = sub_sample, cluster = sub_cluster).Message","d056911a":"get_surprising_words(0, frequency_table)","d5cbfaa3":"get_post_info_by_cluster(0, data = sub_sample, cluster = sub_cluster).Message","f6055af0":"# Get word vectors for keywords in post","01c224c2":"# Visualization brain storming\n\nSlides on text visualizatoin: https:\/\/courses.cs.washington.edu\/courses\/cse512\/15sp\/lectures\/CSE512-Text.pdf\n\n* Bigram based method, reporting the two terms with the median freuquency\n* term saliency, normalize by freq of most common term log(tf_w) \/ log(tf_the) (and then some sort of regression?)\n* Termite-based model: Topics as columns, terms as rows and weight visualiation of term distinctivenes as KL divergence p(T|term)\/p(T|any_term)","0cddba10":"# Going forward\n\nBiggest problem: redundent clusters\n\nPossible solutions: \n\n* Remove very short posts\n* Don't include posts on kernels\n* Build filter for removing short \"thanks!\" type posts\n* Start w\/ sentiment analys & put all very high sentiment posts in a single bin","1388b19c":"# Clustering!","d21552fd":"# Refining clustering\n\nSteps:\n\n1. Drop empty clusters\n2. Identify large clusters (2 times more than expected)\n3. Recluster those clusters (# clusters = sqrt # posts)\n\n","19c166fe":"# Word clouds","4f591dfc":"# Preprocessing posts"}}