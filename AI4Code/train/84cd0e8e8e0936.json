{"cell_type":{"d88392fe":"code","f516c623":"code","9b37c56e":"code","503a216a":"code","624e4fb4":"code","310721de":"code","2d23dfc6":"code","adc5ff87":"code","dd517698":"code","f7e6a287":"code","9585e1d7":"code","2fa6ecac":"code","775d8917":"code","d4a7f1e8":"markdown","60efddc5":"markdown","e6445ecb":"markdown","1f44db4e":"markdown","9b84afa9":"markdown","e31c3392":"markdown","f9eed24f":"markdown","ab170b05":"markdown","4f564575":"markdown"},"source":{"d88392fe":"#importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","f516c623":"#importing dataset\nds = pd.read_csv('..\/input\/insurance\/insurance.csv')\nds.head()","9b37c56e":"#assigning dependent and independent variables\nx = ds.iloc[:,:-1].values\ny = ds.iloc[:,-1].values","503a216a":"#checking for missing values\nds.isnull().any()","624e4fb4":"#applying label encoding on column 1 and 4\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:,1]=le.fit_transform(x[:,1])\nx[:,4]=le.fit_transform(x[:,4])","310721de":"print(x)","2d23dfc6":"#applying lone hot encoding on column 5\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers = [('encoder',OneHotEncoder(),[5])],remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x)","adc5ff87":"#splitting into training and test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","dd517698":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(x_train,y_train)\nlr=LinearRegression()\nlr.fit(x_train,y_train)\nlra = lr.score(x_test,y_test)\nprint(lr.score(x_test,y_test))","f7e6a287":"from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree = 4)\nx_new = pf.fit_transform(x_train)\nplr=LinearRegression()\nplr.fit(x_new,y_train)\nlra2 = plr.score(pf.transform(x_test),y_test)\nprint(plr.score(pf.transform(x_test),y_test))","9585e1d7":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(x_train, y_train)\ndta = dt.score(x_test,y_test)\nprint(dt.score(x_test,y_test))","2fa6ecac":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state = 0)\nrf.fit(x_train, y_train)\nrfa = rf.score(x_test,y_test)\nprint(rf.score(x_test,y_test))","775d8917":"#comparing accuracy\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['LR', 'PR', 'DTR', 'RFR']\nstudents = [lra,lra2,dta,rfa]\nax.bar(langs,students)\nplt.show()","d4a7f1e8":"# ENCODING CATEGORICAL DATA","60efddc5":"**DECISON TREE REGRESSION**","e6445ecb":"APPLYING VARIOUS REGRESSION MODELS TO PREDICT THE COST OF TREATMENT OF PATIENTS.\n\n\n","1f44db4e":"**NO MISSING VALUES**","9b84afa9":"**RANDOM FOREST REGRESSION**","e31c3392":"# APPLYING VARIOUS MODELS","f9eed24f":"**LINEAR REGRESSION**","ab170b05":"**POLYNOMIAL REGRESSION**","4f564575":"**AS YOU CAN SEE THE BEST ACCURACY CAME FOR RANDOM FOREST REGRESSION THAT IS 87.6% AND HENCE IT'S THE BEST MODEL FOR OUR DATASET**"}}