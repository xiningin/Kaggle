{"cell_type":{"eca77923":"code","76e52b4c":"code","757748f6":"code","7d07b850":"code","a8e01a21":"code","03708ddc":"code","ec2b02ec":"code","21f003ca":"code","102201d5":"code","e015c967":"code","e6f734af":"code","13b8c8d7":"code","064177c5":"code","3a844262":"code","30786009":"code","deafd3c1":"code","b0551d38":"markdown","27cede17":"markdown","00defd99":"markdown","e1d2d8a6":"markdown","616a0404":"markdown","29df2e7d":"markdown","0a5ec6f6":"markdown","4937d1cd":"markdown","50c2a836":"markdown","d92575ff":"markdown","bdbfa8b5":"markdown","3e82501b":"markdown"},"source":{"eca77923":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport re\nimport warnings\n\nimport lightgbm as lgb\nfrom unidecode import unidecode\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom itertools import combinations\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 50\nplt.style.use('ggplot')\n\n%matplotlib inline","76e52b4c":"df_train = pd.read_csv('\/kaggle\/input\/kalapas\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/kalapas\/test.csv')\ndf_all = df_train.drop(['label'], 1).append(df_test)\ndf_all.info()","757748f6":"# Process date\/datetime fields\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\nDATETIME = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n\ndf_all[DATE + DATETIME + [\"Field_34\", \"ngaySinh\"]].sample(10)","7d07b850":"def correct_34_ngaysinh(s):\n    if s != s:\n        return np.nan\n    try:\n        s = int(s)\n    except ValueError:\n        s = s.split(\" \")[0]\n        \n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\ndef datetime_normalize(s):\n    if s != s:\n        return np.nan\n    \n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\":\n        s = s[:-1]\n        \n    date, time = s.split(\"T\")\n    datetime_obj = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n    return datetime_obj\n\ndef date_normalize(s):\n    if s != s:\n        return np.nan\n    \n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n        \n    return datetime_obj\n\ndef process_datetime_cols(df):\n    cat_cols = []\n    for col in DATETIME:\n        df[col] = df[col].apply(datetime_normalize)\n        \n    for col in DATE:\n        if col == \"Field_34\":\n            continue\n        df[col] = df[col].apply(date_normalize)\n\n    df[\"Field_34\"] = df[\"Field_34\"].apply(correct_34_ngaysinh)\n    df[\"ngaySinh\"] = df[\"ngaySinh\"].apply(correct_34_ngaysinh)\n\n    cat_cols += DATE + DATETIME\n    for col in DATE + DATETIME:\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_startDate'] = df[f'{cat}_startDate'].dt.strftime('%m-%Y')\n        df[f'{cat}_endDate'] = df[f'{cat}_endDate'].dt.strftime('%m-%Y')\n        \n        cat_cols.append(f'{cat}_startDate')\n        cat_cols.append(f'{cat}_endDate')\n    \n    for col in cat_cols:\n        df[col] = df[col].astype(\"category\")\n        \n    return df","a8e01a21":"def str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n    df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize).astype(\"category\")\n    df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize).astype(\"category\")\n\n    return df","03708ddc":"def job_category(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnv\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"lao \u0111\u1ed9ng\" in x\\\n        or \"th\u1ee3\" in x or \"co\u00f5ng nha\u00f5n tr\u1eed\u00f9c tie\u1ecfp ma\u1ef1y may co\u00f5ng nghie\u1ecdp\" in x or \"c.n\" in x or \"l\u0111\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x or \"gv\" in x or \"g\u00edao vi\u00ean\" in x:\n            return \"GV\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"c\u00e1n b\u1ed9\" in x or \"nv\" in x or \"cb\" in x or \"nh\u00f5n vi\u1eddn\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i\" in x or \"t\u00e0i x\u00ea\" in x:\n            return \"TX\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"ph\u00f3 ph\u00f2ng\" in x or \"hi\u1ec7u ph\u00f3\" in x:\n            return \"QL\"\n        elif \"undefined\" in x:\n            return \"missing\"\n        elif \"gi\u00e1m \u0111\u1ed1c\" in x or \"hi\u1ec7u tr\u01b0\u1edfng\" in x:\n            return \"G\u0110\"\n        elif \"ph\u1ee5c v\u1ee5\" in x:\n            return \"PV\"\n        elif \"chuy\u00ean vi\u00ean\" in x:\n            return  \"CV\"\n        elif \"b\u00e1c s\u0129\" in x or \"d\u01b0\u1ee3c s\u0129\" in x or \"y s\u0129\" in x or \"y s\u1ef9\" in x:\n            return \"BS\"\n        elif \"y t\u00e1\" in x:\n            return \"YT\"\n        elif \"h\u1ed9 sinh\" in x:\n            return \"HS\"\n        elif \"ch\u1ee7 t\u1ecbch\" in x:\n            return \"CT\"\n        elif \"b\u1ebfp\" in x:\n            return \"\u0110B\"\n        elif \"s\u01b0\" in x:\n            return \"KS\"\n        elif \"d\u01b0\u1ee1ng\" in x:\n            return \"\u0110D\"\n        elif \"k\u1ef9 thu\u1eadt\" in x or \"k\u0129 thu\u1eadt\" in x:\n            return \"KTV\"\n        elif \"di\u1ec5n vi\u00ean\" in x:\n            return \"DV\"\n        else:\n            return \"missing\"\n    else:\n        return x    \n    \ndef process_diaChi_maCv(df):\n    df[\"maCv\"] = df[\"maCv\"].apply(str_normalize).apply(job_category).astype(\"category\")\n    return df","ec2b02ec":"def combine_gender(s):\n    x, y = s\n    \n    if x != x and y != y:\n        return \"nan\"\n    \n    if x != x:\n        return y.lower()\n    \n    return x.lower()\n\ndef process_gender(df):\n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    return df","21f003ca":"def process_misc(df):        \n    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n    \n    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n        \n    for col in df.columns:\n        if df[col].dtype.name == \"object\":\n            df[col] = df[col].apply(str_normalize).astype(\"category\")\n            \n    return df\n        ","102201d5":"# drop some fields we do not need (homeTown is optionally)\nDROP = [\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \\\n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]]\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_gender(df)\n    df = process_location(df)\n    df = process_diaChi_maCv(df)\n    df = process_misc(df)\n    return df.drop(DROP, 1)","e015c967":"df_all_fe = transform(df_all.copy())\ndf_all_fe.info()","e6f734af":"df_fe = df_all_fe.copy()\ndf_fe.replace([np.inf, -np.inf], 999, inplace=True)\n\nfor col in df_fe.columns:\n    if df_fe[col].dtype.name == \"category\":\n        if df_fe[col].isnull().sum() > 0:\n            df_fe[col] = df_fe[col].cat.add_categories(f'missing_{col}')\n            df_fe[col].fillna(f'missing_{col}', inplace=True)\n    else:\n        df_fe[col].fillna(-1, inplace=True)\n\ny_label = df_train[\"label\"]\ntrain_fe = df_fe[df_fe[\"id\"] < df_train.shape[0]]\ntest_fe = df_fe[df_fe[\"id\"] >= df_train.shape[0]]\n\nprint(train_fe.shape)\nprint(test_fe.shape)","13b8c8d7":"features = [c for c in train_fe.columns if c not in ['id', 'label']]\n\nlen_train = len(train_fe)\ntrain_fe['label'] = 1\ntrain_fe = train_fe.append(test_fe).reset_index(drop = True)\ntrain_fe['label'] = train_fe['label'].fillna(0)","064177c5":"random_state = 42\nnp.random.seed(random_state)\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'verbose': 1,\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'feature_fraction': 0.7,\n    'min_data_in_leaf': 200,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 20,\n    'min_hessian': 0.01,\n    'feature_fraction_seed': 2,\n    'bagging_seed': 3,\n    \"seed\": random_state\n}","3a844262":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\noof = train_fe[['id', 'label']]\noof['predict'] = 0\nval_aucs = []","30786009":"for fold, (trn_idx, val_idx) in enumerate(skf.split(train_fe, train_fe['label'])):\n    X_train, y_train = train_fe.iloc[trn_idx][features], train_fe.iloc[trn_idx]['label']\n    X_valid, y_valid = train_fe.iloc[val_idx][features], train_fe.iloc[val_idx]['label']\n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    evals_result = {}\n    lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        7500,\n                        valid_sets=[val_data],\n                        early_stopping_rounds=100,\n                        verbose_eval=50,\n                        evals_result=evals_result)\n\n    p_valid = lgb_clf.predict(X_valid[features], num_iteration=lgb_clf.best_iteration)\n\n    oof['predict'][val_idx] = p_valid\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)","deafd3c1":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['label'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))","b0551d38":"### diaChi, maCv\nTo make the best use of these 2 fields, we should fit them into a much smaller range of categories. Temporarily, we just handle `maCv` and leave `diaChi` as a ordinary category type","27cede17":"M\u00ecnh s\u1eed d\u1ee5ng LigbGBM \u0111\u01a1n gi\u1ea3n","00defd99":"### xxxLocationId, xxxCountry, xxxState, xxxLongtitute, xxxLatitude\n\nThere are 0 values in these fields which seem to be irrelevant and kind of noisy, we should eliminate them.","e1d2d8a6":"- Ch\u1ecdn ra c\u00e1c `features` \u0111\u1ec3 train.\n- G\u00e1n l\u1ea1i nh\u00e3n cho t\u1eadp train v\u00e0 test, sau \u0111\u00f3 g\u1ed9p c\u1ea3 hai v\u00e0o l\u00e0m 1 \u0111\u1ec3 th\u1ef1c hi\u1ec7n cross-validation","616a0404":"## Modelling\nLet's fillout missing values and prepair training files","29df2e7d":"### other fields\n\nJust clean, map some noisy data (`Field_38`, `Field_13`, `Field_27`, `Field_28`), normalize ordinal data (`Field_62`, `Field_47`)","0a5ec6f6":"OK, v\u1eady l\u00e0 AUC l\u00e0 0.96 (cao), n\u00ean b\u00e0i n\u00e0y m\u00ecnh ngh\u0129 s\u1ebd c\u00f3 shakeup l\u1edbn :). C\u00e1c b\u1ea1n h\u00e3y c\u1ea9n th\u1eadn v\u1edbi solution c\u1ee7a m\u00ecnh.   \nBTW, C\u00e1m \u01a1n ban t\u1ed5 ch\u1ee9c \u0111\u00e3 c\u00f3 gi\u1ea3i th\u01b0\u1edfng cho public Leaderboard, n\u00f3 s\u1ebd gi\u00fap m\u1ecdi ng\u01b0\u1eddi c\u00f3 \u0111\u1ed9ng l\u1ef1c h\u01a1n trong competition n\u00e0y.  \nGLHF","4937d1cd":"KFold cross-validation ","50c2a836":"## Mannualy inspect each field and group them for further processing\nWe divide features into different groups:\n1. Date \/ Datetime, xxx_startDate, xxx_endDate\n2. xxxLocationId, xxxCountry, xxxState, xxxLongtitute, xxxLatitude\n3. diaChi, maCv\n4. gioiTinh, info_social_sex\n5. to be dropped (not used in training): fields that have too little meaning\/contribution (e.g. too many missing; just 1 value across all cells; duplicates)\n6. other","d92575ff":"### Date \/ Datetime, xxx_startDate, xxx_endDate\n\nWe can see `Field_34` and `ngaySinh` need to be corrected. With date & datetime kind of data, we can extract some new features like whether it is on weekends, which period of time in a day, how far from the present. From information extracted from `ngaySinh`, we can infer the age of the record owner and then bin them.\n\nFor the sake of simplicity, in this version we just do the normalization \/ cleaning process and not make any new features. ","bdbfa8b5":"### gioiTinh, info_social_sex\nCombine two columns to reduce missing cases","3e82501b":"# L\u1eddi n\u00f3i \u0111\u1ea7u \n\n\n**Adversarial Validation** l\u00e0 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p gi\u00fap ki\u1ec3m tra t\u00ednh gi\u1ed1ng nhau (similarity) c\u1ee7a distribution, t\u00ednh ch\u1ea5t (property) gi\u1eefa t\u1eadp train v\u00e0 t\u1eadp test. \n\nVi\u1ec7c th\u1ef1c hi\u1ec7n adversarial validation s\u1ebd gi\u00fap ta bi\u1ebft n\u00ean trust Local cross-validation hay Leaderboard v\u00e0 c\u00f3 k\u1ebf ho\u1ea1ch c\u1ee5 th\u1ec3 cho vi\u1ec7c validate model, tr\u00e1nh tr\u01b0\u1eddng h\u1ee3p b\u1ecb overfit hay shakeup \n\n\nC\u00e1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n g\u1ed3m: \n- Drop c\u00e1c c\u1ed9t th\u00f4ng tin kh\u00f4ng c\u1ea7n thi\u1ebft \u1edf t\u1eadp train v\u00e0 test (Ex: id). C\u1ed9t target\/label c\u1ee7a t\u1eadp train c\u0169ng \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf. \n- G\u00e1n m\u1ed9t label m\u1edbi cho to\u00e0n b\u1ed9 t\u1eadp train l\u00e0 '1', label cho t\u1eadp test l\u00e0 '0'.\n- D\u00f9ng m\u1ed9t model th\u00f4ng th\u01b0\u1eddng, kh\u00f4ng c\u1ea7n qu\u00e1 *c\u1ed3ng k\u1ec1nh* (Randomforest, SVM, Xgboost, LightGBM,...) \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u0111\u00e2u l\u00e0 sample c\u1ee7a t\u1eadp train (label=1) v\u00e0 \u0111\u00e2u l\u00e0 data c\u1ee7a t\u1eadp test (label=0). \u1ede \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n binary classification n\u00ean hay d\u00f9ng AUC \u0111\u1ec3 l\u00e0 metrics\n\n\nD\u1ef1a v\u00e0o gi\u00e1 tr\u1ecb AUC \u0111o \u0111\u01b0\u1ee3c \u1edf tr\u00ean, c\u00e1c tr\u01b0\u1eddng h\u1ee3p c\u00f3 th\u1ec3 x\u1ea3y ra: \n- AUC c\u00e0ng th\u1ea5p (~0.5): Ngh\u0129a l\u00e0 vi\u1ec7c ph\u00e2n bi\u1ec7t sample c\u1ee7a train v\u00e0 test l\u00e0 **kh\u00f3** \u0111\u1ed1i v\u1edbi model \u0111\u00f3 => Train v\u00e0 test **gi\u1ed1ng** nhau => L\u00fac n\u00e0y t\u1ec9 l\u1ec7 shake-up s\u1ebd th\u1ea5p\n- AUC c\u00e0ng cao (0.9): Ngh\u0129a l\u00e0 vi\u1ec7c ph\u00e2n bi\u1ec7t sample c\u1ee7a train v\u00e0 test l\u00e0 **d\u1ec5** \u0111\u1ed1i v\u1edbi model \u0111\u00f3 => Train v\u00e0 test **kh\u00e1c** nhau => T\u1ec9 l\u1ec7 shake-up cao. B\u1ea1n c\u1ea7n ch\u1ecdn c\u00e1ch chia validation sao cho n\u00f3 gi\u1ed1ng v\u1edbi test nh\u1ea5t.\nC\u00e1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t \u0111\u00f3 l\u00e0 ch\u1ea1y cross-validation v\u1edbi model th\u00f4ng th\u01b0\u1eddng tr\u00ean. V\u1edbi m\u1ed7i fold x\u00e1c \u0111\u1ecbnh xem sample n\u00e0o b\u1ecb miss-classified v\u00e0 \u0111\u01b0a n\u00f3 v\u00e0o t\u1eadp validation cho b\u00e0i to\u00e1n g\u1ed1c \n\n\nTrong b\u00e0i vi\u1ebft n\u00e0y, m\u00ecnh s\u1ebd th\u1ef1c hi\u1ec7n Adverarial Validation v\u1edbi dataset c\u1ee7a Kalapa v\u00e0 \u0111\u00e1nh gi\u00e1. \nB\u1eddi v\u00ec c\u00f3 m\u1ed9t s\u1ed1 b\u01b0\u1edbc c\u1ea7n clean v\u00e0 preprocessing data n\u00ean m\u00ecnh m\u01b0\u1ee3n t\u1ea1m ph\u1ea7n feature engineering \u1edf \u0111\u00e2y: \nhttps:\/\/www.kaggle.com\/tailongnguyen\/klp-creditscoringchallenge-tutorial?fbclid=IwAR2kLruTiAqSG_Z_4ezoc4RCtKukTI-0pr3eWQzs7dKjjLS5ACysXbuaNUA "}}