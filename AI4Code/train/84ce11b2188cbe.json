{"cell_type":{"3f695bdc":"code","31e12f13":"code","563d5b0b":"code","812ba486":"code","48cc09a9":"code","ffa40de9":"code","72650c7c":"code","e0de0ade":"code","82754940":"code","b8f63c08":"code","1edeb852":"code","8366927c":"code","80833825":"code","0593d7bc":"code","cb347011":"code","2676575d":"code","373d1f95":"code","8b5e9e0a":"code","8d12003c":"code","8fd4a050":"code","0afa5613":"code","9fafac39":"code","3062132a":"code","9a3aeb6e":"code","f2a315e0":"code","218620a0":"code","5a44795e":"code","8edbd98e":"code","6eb186b6":"code","aed23234":"code","09fb0c2a":"code","3d1dd2c7":"code","4f5becdd":"code","a840f63a":"code","bf7c8b5c":"code","90749377":"code","977f3666":"code","e4862b2b":"code","f1e4879a":"code","1ded64f1":"code","db1da4c6":"code","9f0155ed":"code","50b728c8":"code","ff3b6153":"code","926e44e5":"code","beb6e696":"code","ab91d1d5":"code","0d00e7fb":"code","635df122":"code","a2b21966":"code","e4d60708":"code","6a303713":"code","bcb02a8f":"code","cc074ad0":"code","a88b75df":"code","e9f6adca":"code","4d2a31eb":"code","c58b32c1":"code","c0bcf0de":"code","750861cd":"code","7606d028":"code","46e92e5a":"code","befb73e9":"code","1178306c":"code","3f3fad44":"code","482b7a6b":"code","7e0db611":"code","1d5816db":"code","ee967a72":"code","8e6f4325":"code","1bb0fc3e":"code","4cd66efa":"code","e5e1eaf3":"code","0fb18d11":"code","2b6d87dd":"code","3aea5870":"code","23f9be6e":"code","039863e5":"code","87af2dcb":"code","c56ecd68":"code","9f748442":"code","7710bfab":"code","8dbf64c9":"code","87d54267":"code","4ae9233f":"code","982a26b8":"code","12ed9f5b":"code","c085dbb7":"code","95811592":"code","3f8627f6":"markdown"},"source":{"3f695bdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","31e12f13":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nwarnings.filterwarnings(\"ignore\")","563d5b0b":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', parse_dates=['date'], \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","812ba486":"# Reading the display of Input data\nitem_categories.head()","48cc09a9":"items.head()","ffa40de9":"sales_train.head()","72650c7c":"shops.head()","e0de0ade":"test.head()","82754940":"#EDA","b8f63c08":"print('sales_train')\ndisplay(sales_train.describe(include='all').T)\n\nprint('test')\ndisplay(test.describe(include='all').T)\n\nprint('items')\ndisplay(items.describe(include='all').T)\n\nprint('item_categories')\ndisplay(item_categories.describe(include='all').T)\n\nprint('shops')\ndisplay(shops.describe(include='all').T)","1edeb852":"## Joining the data set\n\ntrain = sales_train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","8366927c":"print('Train rows: ', train.shape[0])\nprint('Train columns: ', train.shape[1])","80833825":"train.head().T","0593d7bc":"train.describe().T","cb347011":"## data leakage\n\ntest_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nlk_train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]","2676575d":"print('Data set size before leaking:', train.shape[0])\nprint('Data set size after leaking:', lk_train.shape[0])","373d1f95":"## Data cleaning(only take item price>0)\n\ntrain = train.query('item_price > 0')","8b5e9e0a":"# Select only useful features.\ntrain_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","8d12003c":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","8fd4a050":"## for each month we need to create the missing records for each shop and item, since we don't have data for them I'll replace them with 0","0afa5613":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","9fafac39":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","3062132a":"# New data set\n\ntrain_monthly.head().T","9a3aeb6e":"train_monthly.describe().T","f2a315e0":"# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x\/\/12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","218620a0":"#EDA","5a44795e":"# Grouping data for EDA.\ngp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()","8edbd98e":"# Sales in Year\n\nf, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_mean, ax=axes[0]).set_title(\"Monthly mean\")\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_sum, ax=axes[1]).set_title(\"Monthly sum\")\nplt.show()","6eb186b6":"##As we can see we have a trending increase of item sales count (mean) towards the ending of the year.","aed23234":"# Category wise sales\n\nf, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","09fb0c2a":"#Also only few of the categories seems to hold most of the sell count.","3d1dd2c7":"#Shops wise sales count\n\nf, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","4f5becdd":"#Outliers checking\n\nsns.jointplot(x=\"item_cnt\", y=\"item_price\", data=train_monthly, height=8)\nplt.show()","a840f63a":"sns.jointplot(x=\"item_cnt\", y=\"transactions\", data=train_monthly, height=8)\nplt.show()","bf7c8b5c":"# \"item_cnt\" distribution\n\nplt.subplots(figsize=(22, 8))\nsns.boxplot(train_monthly['item_cnt'])\nplt.show()","90749377":"# Removing outliers\n #\"item_cnt\" > 20 and < 0, \"item_price\" >= 400000 as outliers.\n\ntrain_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')\n    ","977f3666":"# Label creating(item_cnt)\n\ntrain_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)","e4862b2b":"train_monthly.describe().T","f1e4879a":"train_monthly['item_price_unit'] = train_monthly['item_price'] \/\/ train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","1ded64f1":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","db1da4c6":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","9f0155ed":"#Feature Engineering","50b728c8":"train_monthly['item_price_unit'] = train_monthly['item_price'] \/\/ train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","ff3b6153":"train_monthly.describe().T","926e44e5":"#Group based features\n\ngp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","beb6e696":"# Lag based features\n\nlag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)","ab91d1d5":"# Data set after feature Enginnering\n\ntrain_monthly.head().T","0d00e7fb":"train_monthly.describe().T","635df122":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] \/= len(lag_list) + 1","a2b21966":"# Train\/validation split\nAs we know the test set in on the future, so we should try to simulate the same distribution on our train\/validation split.\nOur train set will be the first 3~28 blocks, validation will be last 5 blocks (29~32) and test will be block 33.\nI'm leaving the first 3 months out because we use a 3 month window to generate features, so these first 3 month won't have really windowed useful features.","e4d60708":"train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]\/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]\/train_monthly.shape[0])*100)))","6a303713":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","bcb02a8f":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","cc074ad0":"X_train.dtypes","a88b75df":"# Integer features (used by catboost model).\nint_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","e9f6adca":"X_train.dtypes","4d2a31eb":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","c58b32c1":"#Build test set\nWe want to predict for \"date_block_num\" 34 so our test set will be block 33 and our predictions should reflect block 34 values. In other words we use block 33 because we want to forecast values for block 34.","c0bcf0de":"# Replacing missing values\n\nsets = [X_train, X_validation]\n\n# This was taking too long.\n# Replace missing values with the median of each item.\n# for dataset in sets:\n#     for item_id in dataset['item_id'].unique():\n#         for column in dataset.columns:\n#             item_median = dataset[(dataset['item_id'] == item_id)][column].median()\n#             dataset.loc[(dataset[column].isnull()) & (dataset['item_id'] == item_id), column] = item_median\n\n# Replace missing values with the median of each shop.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nX_test.fillna(X_test.mean(), inplace=True)\n","750861cd":"# Test data\n\nX_test.head().T","7606d028":"X_test.describe().T","46e92e5a":"## Linear Models","befb73e9":"# Use only part of features on linear Regression.\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]","1178306c":"# Normalize feature\n\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)","3f3fad44":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)","482b7a6b":"lr_train_pred = lr_model.predict(lr_train)\nlr_val_pred = lr_model.predict(lr_val)\nlr_test_pred = lr_model.predict(lr_test)","7e0db611":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))","1d5816db":"#Tree based models","ee967a72":"# Random Forest\n\n# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n     'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]\n","8e6f4325":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)","1bb0fc3e":"rf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)","4cd66efa":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, rf_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, rf_val_pred)))","e5e1eaf3":"# I'm dropping \"item_category_id\", we don't have it on test set and would be a little hard to create categories for items that exist only on test set.\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","0fb18d11":"cat_features = [0, 1, 7, 8]\n\ncatboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, Y_train,\n    cat_features=cat_features,\n    eval_set=(X_validation,Y_validation)\n)","2b6d87dd":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=Y_train, cat_features=cat_features)))), columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","3aea5870":"catboost_train_pred = catboost_model.predict(X_train)\ncatboost_val_pred = catboost_model.predict(X_validation)\ncatboost_test_pred = catboost_model.predict(X_test)","23f9be6e":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, catboost_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, catboost_val_pred)))","039863e5":"# XGBoost\n\n# Use only part of features on XGBoost.\nxgb_features = ['item_cnt', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt']\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]","87af2dcb":"xgb_model = XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)","c56ecd68":"# XGBoost feature importance\n\nplt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(xgb_model)\nplt.show()","9f748442":"xgb_train_pred = xgb_model.predict(xgb_train)\nxgb_val_pred = xgb_model.predict(xgb_val)\nxgb_test_pred = xgb_model.predict(xgb_test)","7710bfab":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, xgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)))","8dbf64c9":"# Ensemble Technique\n\n# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(catboost_val_pred, columns=['catboost'])\nfirst_level['xgbm'] = xgb_val_pred\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['linear_regression'] = lr_val_pred\nfirst_level['label'] = Y_validation.values\nfirst_level.head(20)","87d54267":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_test_pred, columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_test_pred\nfirst_level_test['random_forest'] = rf_test_pred\nfirst_level_test['linear_regression'] = lr_test_pred\nfirst_level_test.head()","4ae9233f":"meta_model = LinearRegression(n_jobs=-1)","982a26b8":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_validation)","12ed9f5b":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)","c085dbb7":"print('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_validation)))","95811592":"# Output dataframe\n\nprediction_df = pd.DataFrame(test['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission.csv', index=False)\nprediction_df.head(10)","3f8627f6":"Sales_train EDA"}}