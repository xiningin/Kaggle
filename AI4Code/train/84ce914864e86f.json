{"cell_type":{"aacd3fdf":"code","eecf8b2d":"code","03328db5":"code","d72aa270":"code","ec2a4ce6":"code","7db500de":"code","36f7c532":"code","f2d1e889":"code","776447c0":"code","a51d1dc8":"code","bc5ac975":"code","f262233f":"code","fd194346":"code","c1a32f94":"code","b7af0440":"code","c751fa42":"code","de80dadb":"code","26f57d77":"code","87332aea":"code","f8b52fcf":"code","592c01a3":"code","328b762c":"code","e7a1c80e":"code","4d891d12":"code","3593631a":"code","8d978baa":"code","a1f863ad":"code","981e9925":"code","87ba262d":"code","a9a1e293":"code","385cfdd3":"code","46f2dc4f":"code","f544e406":"markdown","35ea52af":"markdown","5df05f25":"markdown","5e742838":"markdown","5f9a3119":"markdown","240c673e":"markdown","df899db7":"markdown"},"source":{"aacd3fdf":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.metrics import RSquare","eecf8b2d":"data = pd.read_csv('..\/input\/finance-accounting-courses-udemy-13k-course\/udemy_output_All_Finance__Accounting_p1_p626.csv')","03328db5":"data","d72aa270":"data.info()","ec2a4ce6":"data = data.drop(['id', 'title', 'url'], axis=1)","7db500de":"data","36f7c532":"data['discount_price__currency'].unique()","f2d1e889":"data['price_detail__currency'].unique()","776447c0":"data = data.drop(['discount_price__currency', 'price_detail__currency'], axis=1)","a51d1dc8":"data","bc5ac975":"data.dtypes","f262233f":"data.isna().mean()","fd194346":"data = data.drop(['discount_price__price_string', 'price_detail__price_string'], axis=1)","c1a32f94":"data","b7af0440":"for column in ['discount_price__amount', 'price_detail__amount']:\n    data[column] = data[column].fillna(data[column].mean())","c751fa42":"print(\"Total missing values:\", data.isna().sum().sum())","de80dadb":"data['created_year'] = data['created'].apply(lambda x: np.int(x[0:4]))\ndata['created_month'] = data['created'].apply(lambda x: np.int(x[5:7]))\n\ndata['published_year'] = data['published_time'].apply(lambda x: np.int(x[0:4]))\ndata['published_month'] = data['published_time'].apply(lambda x: np.int(x[5:7]))\n\ndata = data.drop(['created', 'published_time'], axis=1)","26f57d77":"data['is_paid'] = data['is_paid'].astype(np.int)\ndata['is_wishlisted'] = data['is_wishlisted'].astype(np.int)","87332aea":"data","f8b52fcf":"y = data['rating'].copy()\nX = data.drop('rating', axis=1).copy()","592c01a3":"X","328b762c":"(data['avg_rating_recent'] == data['rating']).all()","e7a1c80e":"X = X.drop('avg_rating_recent', axis=1)","4d891d12":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","3593631a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=32)","8d978baa":"X.shape","a1f863ad":"inputs = tf.keras.Input(shape=(13,))\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='linear')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mse'\n)\n\n\nbatch_size = 32\nepochs = 100\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.12,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],\n    verbose=0\n)","981e9925":"fig = px.line(\n    history.history,\n    y=['loss', 'val_loss'],\n    labels={'x': \"epoch\", 'y': \"loss\"}\n)\n\nfig.show()","87ba262d":"model.evaluate(X_test, y_test)","a9a1e293":"y_pred = np.squeeze(model.predict(X_test))","385cfdd3":"rsquared = RSquare()\n\nrsquared.update_state(y_test, y_pred)","46f2dc4f":"print(\"R^2 Score:\", rsquared.result().numpy())","f544e406":"# Cleaning","35ea52af":"# Getting Started","5df05f25":"# Modeling\/Training","5e742838":"# Task for Today  \n\n***\n\n## Finance\/Accounting Course Rating Prediction  \n\nGiven *data about finance and accounting courses on Udemy*, let's try to predict the **rating** of a given course.  \n  \nWe will use a TensorFlow ANN to make our predictions.","5f9a3119":"# Feature Engineering\/Encoding\/Splitting\/Scaling","240c673e":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/h1AFMLZcDSA","df899db7":"# Results"}}