{"cell_type":{"b470cabd":"code","30496844":"code","90d318eb":"code","fb255eee":"code","efef167e":"code","8e22a68b":"code","38dc4fc8":"code","dff32042":"code","b583aabc":"code","738b5cbb":"code","d738b498":"code","5ff664a7":"code","a1a8ca04":"code","12783a50":"code","ada84f88":"code","d828e5df":"code","a758ef57":"code","74c8b9d9":"code","93441c0d":"code","8239a277":"code","3fde0356":"code","1c469e16":"code","87f653f3":"code","7aa56468":"code","ca096adf":"code","fb19823b":"code","f0188d4b":"code","2ca163ff":"code","b37b673f":"code","11aa190e":"code","eddd21d9":"code","559f4675":"code","9a1788a4":"code","bd18d382":"code","ae8aaf6f":"code","a04fbf14":"code","7af0d890":"code","251e46ff":"code","33930eb5":"code","6366abc2":"code","4ef46f15":"code","650a2403":"code","5db1d63f":"code","1fafbb8b":"code","a7ab8a8c":"code","dc18789c":"code","5af5ebf5":"code","72444ef4":"code","5a2b782a":"code","9f527f0b":"markdown","394c9020":"markdown","7e2bdae1":"markdown","f4628007":"markdown","b5c30bfc":"markdown","a005aea6":"markdown","ffe8af97":"markdown","bc5caa9a":"markdown","fd1b6786":"markdown","0e795634":"markdown","4b796dd0":"markdown","e8729347":"markdown","72c98be3":"markdown","56f24bc7":"markdown","245c1969":"markdown","b53423d9":"markdown","c3f34911":"markdown","9e24c713":"markdown","996f9aae":"markdown","7276e115":"markdown","359979e6":"markdown","e70fd8de":"markdown","686e2d50":"markdown","fc1d6acd":"markdown","81ce9d1a":"markdown","e18de5d2":"markdown","dd1f47f7":"markdown","a2b32f7e":"markdown","03a5d016":"markdown","ce4f1e73":"markdown","0ae9fd2b":"markdown","ecea14e2":"markdown","2cf4b188":"markdown","37103b1c":"markdown","ed150356":"markdown"},"source":{"b470cabd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30496844":"heart = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","90d318eb":"heart.head()","fb255eee":"heart.describe()","efef167e":"heart.shape","8e22a68b":"heart.isna().any()","38dc4fc8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport scipy ","dff32042":"fig, axs = plt.subplots(1,2, figsize=(10,5))\nsns.distplot(heart.age, color=\"blue\", ax=axs[0])\nsns.boxplot(heart.age, color=\"red\", ax=axs[1])\nImage(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/8c\/Standard_deviation_diagram.svg\/1920px-Standard_deviation_diagram.svg.png\", width=400 )","b583aabc":"fig, axs = plt.subplots(1,2, figsize=(20,5))\nsns.countplot(heart.sex, ax= axs[0])\nsns.countplot(heart.cp, ax= axs[1])","738b5cbb":"sns.countplot(heart.output)","d738b498":"fig, axs = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(heart.thalachh, color=\"blue\", ax=axs[0])\nsns.boxenplot(heart.thalachh, ax= axs[1])","5ff664a7":"scipy.stats.skew(heart.thalachh)","a1a8ca04":"scipy.stats.kurtosis(heart.thalachh)","12783a50":"heart.columns","ada84f88":"sns.pairplot(heart)","d828e5df":"numerical_features = ['age','trtbps','chol','thalachh','oldpeak']\nplt.figure(figsize=(20,5))\nsns.heatmap(heart[numerical_features].corr(), annot=True, vmin=-1, vmax=1)","a758ef57":"sns.pairplot(heart[numerical_features])","74c8b9d9":"plt.figure(figsize=(7,7))\nsns.regplot(x=heart.age, y=heart.thalachh, data=heart)","93441c0d":"fig, axs = plt.subplots(1,2, figsize=(10,5))\nsns.boxplot(data = heart['chol'], ax= axs[0])\nsns.distplot(heart.chol, color='red', ax = axs[1])","8239a277":"heart.drop(heart[heart['chol']>380].index, inplace=True)","3fde0356":"fig, axs = plt.subplots(1,2, figsize=(10,5))\nsns.boxplot(data = heart['chol'], ax= axs[0])\nsns.distplot(heart.chol, color='red', ax = axs[1])","1c469e16":"fig, axs = plt.subplots(1,2, figsize=(10,5))\nsns.regplot(heart.age, heart.chol, ax = axs[0])\nsns.regplot(heart.trtbps, heart.chol, color='red', ax = axs[1])","87f653f3":"plt.figure(figsize=(7,7))\nsns.scatterplot(heart.age, heart.chol, hue=heart.output)","7aa56468":"sns.barplot(x=heart.cp, y=heart.age)","ca096adf":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate","fb19823b":"X = heart.drop(['output'], axis=1)\ny = heart.output","f0188d4b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","2ca163ff":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.40, random_state=42)","b37b673f":"X_train.shape, X_val.shape, X_test.shape","11aa190e":"from sklearn.linear_model import LogisticRegression\nmodel =LogisticRegression(penalty='l2', max_iter=1000)\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_val)\n\n","eddd21d9":"from sklearn.metrics import accuracy_score\naccuracy_score(y_val, predicted)","559f4675":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_val, y_val)","9a1788a4":"model =LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_val)\nplot_confusion_matrix(model, X_val, y_val)","bd18d382":"from sklearn.tree import DecisionTreeClassifier\nmodel_tree = DecisionTreeClassifier()\nmodel_tree.fit(X_train, y_train)\npredict_tree = model_tree.predict(X_val)\npredict_tree_train = model_tree.predict(X_train)\naccuracy_score(y_val, predict_tree), accuracy_score(y_train, predict_tree_train)","ae8aaf6f":"for max_d in range(1,5):\n    for min_s in range(1,5):\n        model_tree = DecisionTreeClassifier(max_depth=max_d, min_samples_leaf= min_s)\n        model_tree.fit(X_train, y_train)\n        predict_tree = model_tree.predict(X_val)\n        predict_tree_train = model_tree.predict(X_train)\n        acc_dif =abs((accuracy_score(y_val, predict_tree) - accuracy_score(y_train, predict_tree_train)))\n        print(acc_dif, max_d, min_s)\n            ","a04fbf14":"model_tree = DecisionTreeClassifier(max_depth=2, min_samples_leaf= 3)\nmodel_tree.fit(X_train, y_train)\npredict_tree = model_tree.predict(X_val)\npredict_tree_train = model_tree.predict(X_train)\nplot_confusion_matrix(model_tree, X_val, y_val)\n","7af0d890":"cross_validate(model_tree, X_train, y_train, cv=3, scoring='r2',return_train_score=True)","251e46ff":"from sklearn.ensemble import RandomForestClassifier\nmodel_rand = RandomForestClassifier()\nmodel_rand.fit(X_train, y_train)\npredict_rand = model_rand.predict(X_val)\npredict_rand_train = model_rand.predict(X_train)\naccuracy_score(y_val, predict_rand), accuracy_score(y_train, predict_rand_train)","33930eb5":"for ns in [50, 100, 150, 200, 250, 300, 350, 400]:\n    model_rand = RandomForestClassifier(n_estimators=ns)\n    model_rand.fit(X_train, y_train)\n    predict_rand = model_rand.predict(X_val)\n    predict_rand_train = model_rand.predict(X_train)\n    acc_dif_rand =abs((accuracy_score(y_val, predict_rand) - accuracy_score(y_train, predict_rand_train)))\n    print(acc_dif_rand, ns)","6366abc2":"from sklearn.model_selection import RandomizedSearchCV\nmodel_r = RandomForestClassifier()\ndistribution = {'criterion': ['gini', 'entropy'],\n                'max_depth': [2, 3, 4],\n                'max_features': ['auto', 'sqrt'],\n                'min_samples_leaf': [4, 6, 8, 10],\n                'min_samples_split': [5, 7, 10],\n                'n_estimators':[50, 100, 150, 200] \n               }\nmodel_rand = RandomizedSearchCV(estimator=model_r, param_distributions=distribution  , cv=4)\nmodel_rand.fit(X_train, y_train)","4ef46f15":"model_rand.best_params_ , model_rand.best_score_","650a2403":"model_rand = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_leaf=8, min_samples_split=5, max_features='sqrt')\nmodel_rand.fit(X_train, y_train)\npredict_rand = model_rand.predict(X_val)\npredict_rand_train = model_rand.predict(X_train)\naccuracy_score(y_val, predict_rand), accuracy_score(y_train, predict_rand_train)","5db1d63f":"plot_confusion_matrix(model_rand, X_val, y_val)","1fafbb8b":"from sklearn.model_selection import GridSearchCV\nmodel_r_g = RandomForestClassifier()\ndist = {'criterion': ['gini', 'entropy'],\n                'max_depth': [2, 3, 4],\n                'max_features': ['auto', 'sqrt'],\n                'min_samples_leaf': [4, 6, 8, 10],\n                'min_samples_split': [5, 7, 10],\n                'n_estimators':[50, 100, 150, 200]}\nmodel_rand_g  = GridSearchCV(estimator=model_r_g, param_grid=dist, cv=4)\nmodel_rand_g.fit(X_train, y_train)","a7ab8a8c":"model_rand_g.best_params_, model_rand_g.best_score_","dc18789c":"predict_rand_g = model_rand_g.best_estimator_.predict(X_val)\npredict_rand_train_g = model_rand_g.best_estimator_.predict(X_train)\n\naccuracy_score(y_val, predict_rand_g), accuracy_score(y_train, predict_rand_train_g)","5af5ebf5":"predict_logestic_final = model.predict(X_test)\npredict_rand_final = model_rand.predict(X_test)\nprint( 'Logestic Regression ==>', accuracy_score(y_test, predict_logestic_final))\nprint( 'Random Forest Classifer ==>', accuracy_score(y_test, predict_rand_final))","72444ef4":"plot_confusion_matrix(model_rand, X_test, y_test)","5a2b782a":"from xgboost import XGBClassifier\nmodel_xg = XGBClassifier(n_estimators=1000,  learning_rate=0.05)\nmodel_xg.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_val, y_val)])\nxg = model_xg.predict(X_val)\nxg_train = model_xg.predict(X_train)\naccuracy_score(y_val, xg), accuracy_score(y_train, xg_train)","9f527f0b":"In this figure, we can notice that generally at higher ages and high chol, we are expected more 0 output than 1 output :)\n\nYou can find more interesting info from categorical data using barplots: ","394c9020":"# Model Developing","7e2bdae1":"We have almost twice \"1\" gender than \"0\" gender. Chest Pain type 1 which is \"typical angina\" is most common chest pain type followed by \"non-anginal pain\". ","f4628007":"We can even move forward and calculate skewness and kurtosis. For quick remind, skew and kurtosis are showing the shape of the distribution. skew negative means the data have tailed toward negative values.","b5c30bfc":"Lets have a general pairplot to see what interesting pairs we can found:","a005aea6":"Most of the features are kinda categorical variables which are turned to numbers. We will back to those variables later. Lets see how numerical variables are relating together with heatmap. ","ffe8af97":"We are going to separate train, test, and validation sets. We are not going to touch test set before making sure about the performance of our model. However, we are going to play with train and validation sets to tune the model.","bc5caa9a":"Well, this looks much better :). \n\nP.S. domain expert is required for advance outlier detection. In some cases, chol>400 maybe a reasonable number, not an outlier. ","fd1b6786":"2. Grid Search","0e795634":"As can be seen in the right figure (blue one), with increasing the age we expected to increase chol. You can find many more interesting \"general weak\" trends with scatterplots or regplots.\n\nLets move a bit to advance analysis and include the categorical parameters as well:","4b796dd0":"Now we have 2 model candidates: 1. Logestic Regression and 2. Random Forest Classifier \n\nHow they can perform in the test dataset?","e8729347":"Well! we have a perfect match in the training set but weak prediction in the test set. What happend? OVERFITTING :)\n\nmax_depth, min_samples_split, and min_samples_leaf, min_weight_fraction_leaf and min_impurity_decrease are the hyperparameters which can be used to avoid overfitting. First three are stopping parameters and last two are purning parameters.","72c98be3":"It does not change anything! We can try other ML algorithms:","56f24bc7":"We have relatively equal number of output cases. Hence, we are dealing with a balanced binary classification problem. ","245c1969":"The kurtosis of any univariate normal distribution is 3. Kurtosis < 3 ==> No significant outlier, kurtosis > 3 ==> there is a good chance for having outlier.\n\nNote: You can find many other usefull info from skew and kurtosis. #DYOR","b53423d9":"Well, we predicted 66 output correctly out of 85. We also have a wrong prediction in 19 outputs. Lets remove the regularization penalty to see what will happend:","c3f34911":"It seems there is some strong negative correlation btw age and maximum heart rate achieved which is expected. We can see this correlation in the scatter plot (below). There is also some weak positive correlations btw \"resting blood pressure\" and \"age\". Lets check these:","9e24c713":"# Lets Have A Few Univarent Analysis","996f9aae":"I will choose 150 n_estimators and to avoid overfitting I will add max_depth for each tree in random forest. You can play with hyperparameters and check cross validation score to avoid overfitting as much as possible. ","7276e115":"The main parameters used by a Random Forest Classifier are:\n\ncriterion = the function used to evaluate the quality of a split.\n\nmax_depth = maximum number of levels allowed in each tree.\n\nmax_features = maximum number of features considered when splitting a node.\n\nmin_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n\nmin_samples_split = minimum number of samples necessary in a node to cause node splitting.\n\nn_estimators = number of trees in the ensemble.","359979e6":"We have a few outliers based on box plot. As can be seen in the right figure, there are some points above the maximum (380) and one obvious outlier above 500. The distribution plot also shows that we have relatively normally distributed data if we ignore the outliers. ","e70fd8de":"Other Classification Methods?\n\nXGboost","686e2d50":"*What we are looking for?*\n1. correlations (positive, negative relation?, segnemtaion)\n2. regression\n3. constrains\n4. outliers","fc1d6acd":"# Bivarent Analysis","81ce9d1a":"As can be seen, we have slightly negative skew, which means the data have tail toward negative values.","e18de5d2":"About this dataset\n\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\n\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack\n\nn","dd1f47f7":"We have relatively normal distributed age with mean = 54.36 and std = 9.08. We do not have a significant outlier and every thing seems great to progress :)","a2b32f7e":"Although we got almost 0.72 accuracy in binary classification using decision tree, cross validation shows our result is not repeatable for other cross sections. Hence, we are going to test Random Forest Classifier.","03a5d016":"Is there any better method than this manual searching for optimal hyperparameters?\n\n1. Random Search ","ce4f1e73":"In the chol data, it seems we have an outlier. Lets check it:","0ae9fd2b":"It seems RandomForest works better for our dataset :)","ecea14e2":"We increased the accuracy a bit compared to decision tree but still we have OVERFITIING!","2cf4b188":"If we set max_depth to 2 and min_samples_leaf to 1 to 4, we can have minimum accuracy difference btw training and test sets.","37103b1c":"We have slightly left-skewed distribution in \"maximum heart rate achieved\". Best way to show how data are behaving in left tail is boxen plot. As shown in the boxen plot, we have more distrubuted data in the left side compared to the right side. Also, we may have some outliers in the far left and far right sections. In general, the un-normality in the \"maximum heart rate\" is not too much that needed to be normalized. Although, we can check the result of normalization later. ","ed150356":"*What we are looking for?*\n1. mean, variance, standard deviation, mode, medium \n2. distribution (normal, skew, kurtosis)\n3. \"possible\" outliers"}}