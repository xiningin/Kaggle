{"cell_type":{"d09f8f40":"code","586ba219":"code","8f26e1e9":"code","b76b5713":"code","1392b18f":"code","a433cce7":"code","e620a05f":"code","fc0f0f85":"code","cad5bed1":"code","a1e0887e":"code","d450e85b":"code","cb2e294d":"code","fbe5918d":"code","b5cdaa72":"code","64d46b4c":"code","66e6f20f":"code","6273f14c":"code","9259ca50":"code","04968f8a":"markdown","85085fae":"markdown","d3808b67":"markdown","6772b5c4":"markdown","f2553af5":"markdown","295ea638":"markdown","2da06c29":"markdown","d6477f61":"markdown","630bb2be":"markdown","85377d2f":"markdown","19c633c9":"markdown","05fbd441":"markdown","fd8f23a5":"markdown","e5fea852":"markdown","93c3b4d2":"markdown","81a90e21":"markdown","e9287933":"markdown","f76acaca":"markdown","f4634c7d":"markdown","f432cc9a":"markdown","077226a8":"markdown","22b8e187":"markdown","eb421999":"markdown"},"source":{"d09f8f40":"import os\nimport gc\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport math\nimport torch\nimport random\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom torch.utils.data import Dataset\nwarnings.filterwarnings(\"ignore\")\nNUM_WORKERS = 4","586ba219":"class Config:\n    seed = 42\n    verbose = 1\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    save_weights = True\n    k = 5\n    selected_folds = [0]\n    selected_model = 'rnn'\n    input_dim = 5\n    dense_dim = 512\n    logit_dim = 512\n    num_classes = 1\n    loss = \"L1Loss\"  \n    epochs =150\n    warmup_prop = 0\n    T_max=50 \n    T_0=50 \n    min_lr=1e-6\n    num_cycles=0.5\n    val_bs = 256\n    first_epoch_eval = 0","8f26e1e9":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \ndef worker_init_fn(worker_id):\n    np.random.seed(np.random.get_state()[1][0] + worker_id)","b76b5713":"import wandb\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","1392b18f":"DATA_PATH = \"\/kaggle\/input\/ventilator-pressure-prediction\/\"\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_test = pd.read_csv(DATA_PATH + 'test.csv')","a433cce7":"# code taken from https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n\nclass VentilatorDataset(Dataset):\n    def __init__(self, df):\n        if \"pressure\" not in df.columns:\n            df['pressure'] = 0\n\n        self.df = df.groupby('breath_id').agg(list).reset_index()\n        \n        self.prepare_data()\n                \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def prepare_data(self):\n        self.pressures = np.array(self.df['pressure'].values.tolist())\n        \n        rs = np.array(self.df['R'].values.tolist())\n        cs = np.array(self.df['C'].values.tolist())\n        u_ins = np.array(self.df['u_in'].values.tolist())\n        \n        self.u_outs = np.array(self.df['u_out'].values.tolist())\n        \n        self.inputs = np.concatenate([\n            rs[:, None], \n            cs[:, None], \n            u_ins[:, None], \n            np.cumsum(u_ins, 1)[:, None],\n            self.u_outs[:, None]\n        ], 1).transpose(0, 2, 1)\n\n    def __getitem__(self, idx):\n        data = {\n            \"input\": torch.tensor(self.inputs[idx], dtype=torch.float),\n            \"u_out\": torch.tensor(self.u_outs[idx], dtype=torch.float),\n            \"p\": torch.tensor(self.pressures[idx], dtype=torch.float),\n        }\n        \n        return data\n    ","e620a05f":"class RNNModel(nn.Module):\n    def __init__(\n        self,\n        dropout=0.2,\n        rnn_cell='lstm',\n        input_dim=5,\n        lstm_dim=256, \n        dense_dim=256,\n        logit_dim=256,\n        num_classes=1,\n    ):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, dense_dim \/\/ 2),\n            nn.ReLU(),\n            nn.Linear(dense_dim \/\/ 2, dense_dim\/\/2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dense_dim \/\/ 2, dense_dim),\n            nn.ReLU(),\n        )\n        self.cell=rnn_cell\n        self.lstm = nn.LSTM(dense_dim, lstm_dim, batch_first=True, bidirectional=True)\n        self.gru=nn.GRU(dense_dim, lstm_dim, batch_first=True, bidirectional=True)\n        self.logits = nn.Sequential(\n            nn.Linear(lstm_dim * 2, logit_dim),\n            nn.ReLU(),\n            nn.Linear(logit_dim, num_classes),\n        )\n\n    def forward(self, x):\n        features = self.mlp(x)\n        if self.cell=='lstm':\n            features, _ = self.lstm(features)\n        else:\n            features, _ = self.gru(features)\n        pred = self.logits(features)\n        return pred","fc0f0f85":"sweep_config = {\n    'method': 'random'\n}","cad5bed1":"parameters_dict = {\n    'optimizer': {\n        'values': ['adam','sgd','adamw']\n        },\n    'lstm_dim': {\n        'values': [128, 256, 512]\n        },\n    'dropout':{\n        'values': [0.2,0.3,0.4]\n    },\n    'rnn_cell':{\n        'values':['gru','lstm']\n    },\n    'scheduler':{\n        'values':['linear','cosine','CosineAnnealingLR','CosineAnnealingWarmRestarts']\n    }\n}\nsweep_config['parameters'] = parameters_dict","a1e0887e":"metric = {\n    'name': 'MAE',\n    'goal': 'minimize'   \n    }\nsweep_config['metric'] = metric","d450e85b":"parameters_dict.update({\n    'learning_rate': {\n        'distribution': 'uniform',\n        'min': 0,\n        'max': 0.01\n      },\n    'batch_size': {\n        # integers between 128 and 256\n        # with evenly-distributed logarithms  \n        'distribution': 'q_log_uniform',\n        'q': 1,\n        'min': math.log(128),\n        'max': math.log(256),\n      }\n    })\nsweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")","cb2e294d":"sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")","fbe5918d":"def compute_metric(df, preds):\n    y = np.array(df['pressure'].values.tolist())\n    w = 1 - np.array(df['u_out'].values.tolist())\n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    mae = w * np.abs(y - preds)\n    mae = mae.sum() \/ w.sum()\n    return mae\n\nclass VentilatorLoss(nn.Module):\n    def __call__(self, preds, y, u_out):\n        w = 1 - u_out\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae","b5cdaa72":"def fit(\n    model,\n    CFG,\n    train_dataset,\n    val_dataset,\n    optimizer,\n    scheduler,\n    batch_size,\n    lr,\n    verbose,\n    first_epoch_eval,\n    device,\n    loss_name=\"L1Loss\",epochs=50,val_bs=32,\n    warmup_prop=0.1\n):\n    avg_val_loss = 0.\n\n    if optimizer=='sgd':\n        optimizer = torch.optim.SGD(model.parameters(),\n                              lr=lr, momentum=0.9)\n    elif optimizer=='adam':\n        optimizer = torch.optim.Adam(model.parameters(),\n                              lr=lr)\n    elif optimizer=='adamw':\n        optimizer = AdamW(model.parameters(),\n                              lr=lr)\n    \n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=val_bs,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n    loss_fct = VentilatorLoss()\n    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n    num_training_steps = int(epochs * len(train_loader))\n\n    if scheduler=='linear':\n        scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n            )\n    elif scheduler=='cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, num_cycles=CFG.num_cycles\n            )\n    elif scheduler=='CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n    elif scheduler=='CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        \n    for epoch in range(epochs):\n        model.train()\n        model.zero_grad()\n        start_time = time.time()\n\n        avg_loss = 0\n        for data in train_loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n\n            loss = loss_fct(\n                pred,\n                data['p'].to(device),\n                data['u_out'].to(device),\n            ).mean()\n            loss.backward()\n            avg_loss += loss.item() \/ len(train_loader)\n\n            optimizer.step()\n            scheduler.step()\n\n            for param in model.parameters():\n                param.grad = None\n\n        model.eval()\n        mae, avg_val_loss = 0, 0\n        preds = []\n\n        with torch.no_grad():\n            for data in val_loader:\n                pred = model(data['input'].to(device)).squeeze(-1)\n\n                loss = loss_fct(\n                    pred.detach(), \n                    data['p'].to(device),\n                    data['u_out'].to(device),\n                ).mean()\n                avg_val_loss += loss.item() \/ len(val_loader)\n\n                preds.append(pred.detach().cpu().numpy())\n        \n        preds = np.concatenate(preds, 0)\n        mae = compute_metric(val_dataset.df, preds)\n        wandb.log({\"MAE\": mae, \"epoch\": epoch}) \n        \n        elapsed_time = time.time() - start_time\n        if (epoch + 1) % verbose == 0:\n            elapsed_time = elapsed_time * verbose\n            lr = scheduler.get_last_lr()[0]\n            print(\n                f\"Epoch {epoch + 1:02d}\/{epochs:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n                f\"loss={avg_loss:.3f}\",\n                end=\"\\t\",\n            )\n\n            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == epochs):\n                print(f\"val_loss={avg_val_loss:.3f}\\tmae={mae:.3f}\")\n            else:\n                print(\"\")\n\n    del (val_loader, train_loader, loss, data, pred)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n","64d46b4c":"from sklearn.model_selection import GroupKFold\ngkf = GroupKFold(n_splits=Config.k)\nsplits = list(gkf.split(X=df_train, y=df_train , groups=df_train[\"breath_id\"]))","66e6f20f":"df=df_train\nfor i, (train_idx, val_idx) in enumerate(splits):\n        if i in Config.selected_folds:\n            print(f\"\\n-------------   Fold {i + 1} \/ {Config.k}  -------------\\n\")\n\n            df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n            df_val = df.iloc[val_idx].copy().reset_index(drop=True)","6273f14c":"def train(con=None,conf=Config, df_train=df_train, df_val=df_val):\n    with wandb.init(config=con):\n        con=wandb.config\n        \n        seed_everything(conf.seed)\n        \n        model = RNNModel(\n        dropout=con.dropout,\n        rnn_cell=con.rnn_cell,\n        input_dim=conf.input_dim,\n        lstm_dim=con.lstm_dim,\n        dense_dim=conf.dense_dim,\n        logit_dim=conf.logit_dim,\n        num_classes=conf.num_classes).to(conf.device)\n        \n        model.zero_grad()\n\n        train_dataset = VentilatorDataset(df_train)\n        val_dataset = VentilatorDataset(df_val)    \n        \n        fit(\n        model,\n        conf,\n        train_dataset,\n        val_dataset,\n        optimizer=con.optimizer,\n        scheduler=con.scheduler,\n        batch_size=con.batch_size,\n        lr=con.learning_rate,\n        verbose=conf.verbose,\n        first_epoch_eval=conf.first_epoch_eval,\n        device=conf.device,\n        loss_name=conf.loss,\n        epochs=conf.epochs,\n        val_bs=conf.val_bs,\n        warmup_prop=conf.warmup_prop\n        )\n        \n    del (model, train_dataset, val_dataset)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    ","9259ca50":"run=wandb.agent(sweep_id, train, count=6)","04968f8a":"* We also need to specify the metric name ,so that it can be found in the model outputs and also if your goal is to minimize it or maximize it.\n\n","85085fae":"<p style=\"color:#159364; font-family:cursive;\">SEED FUNCTIONS<\/center><\/p>\n","d3808b67":"\n<p style=\"color:#159364; font-family:cursive;\">Metric & Loss<\/center><\/p>\n\n","6772b5c4":"<p style=\"color:#159364; font-family:cursive;\">LOAD THE DATA<\/center><\/p>\n","f2553af5":"<p style=\"color:#159364; font-family:cursive;\">\ud83d\udcca Hyperparameter Importance Plot<\/center><\/p>\nThe hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics. We report feature importance (from a random forest model) and correlation (implicitly a linear model).\n\n![Screenshot (663).png](attachment:dbe5d84e-bad2-469d-8c7a-8bb7eff2e592.png)\n\n![Screenshot (665).png](attachment:5cf18d27-19bd-4420-9c1c-115f58afab61.png)","295ea638":"\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">Step 3\ufe0f\u20e3. Run the Sweep agent<\/h2>\n\n* Before we can actually execute the sweep, we need to define the training procedure that uses those values.\n\n<p style=\"color:#159364; font-family:cursive;\">TRAIN<\/center><\/p>\nIn the train function below,we add the following wandb tools to log model metrics,visualize performance and track our experiments.\n\n* **wandb.init()**\u2013 Initializes a new W&B Run. Each Run is a single execution of the training function.\n\n* **wandb.config** \u2013 Saves all our hyperparameters in a configuration object so they can be logged.\n\n* **wandb.log()** \u2013 logs model behavior to W&B. Here, we just log the performance","2da06c29":"![Screenshot (662).png](attachment:88c8ba32-4912-46ba-90a0-647d76aded1e.png)","d6477f61":"\n <h1 style = \"font-size:40px;font-family:verdana;text-align: center;background-color:#DCDCDC\">GOOGLE BRAIN:VENTILATOR PRESSURE PREDICTION<\/h1>\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">ORGANIZING HYPERPARAMETER SWEEPS USING W&B<\/h2>\n<center><img src=\"https:\/\/wandb.me\/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center>\n\n[Reference tutorial from wandb](https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)\n\n[Sweeps docs](https:\/\/docs.wandb.ai\/guides\/sweeps)\n\n<h4 style=\"font-size:15px;font-family:verdana\">Weights & Biases Hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most accurate model. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.<\/h4>\n<h4 style=\"font-size:15px;font-family:verdana\">Running a Hyperparameter sweep with Weights & Biases requires some simple steps that will be illustrated in this notebook.Feel free to fork the notebook and tweak the sweep configuration to suit your requirements<\/h4>\n\n\n![Blank diagram (1).png](attachment:bcfec536-bee7-4c1c-b074-1202ca20b2b9.png)\n\n<a href=#sec1>Results Visualization<\/a><br>\n","630bb2be":"\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">Training<\/h2>","85377d2f":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","19c633c9":"<p style=\"color:#159364; font-family:cursive;\">CROSS VALIDATION:GROUP K FOLD<\/center><\/p>\n\n* Refer [THIS](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/274137) discussion for better insights on cross validation and type of split.","05fbd441":"* For a random search, we can also specify a named distribution,plus its parameters, like the mean mu and standard deviation sigma of a normal distribution.\n","fd8f23a5":"<p style=\"color:#159364; font-family:cursive;\">IMPORT THE REQUIRED LIBRARIES<\/center><\/p>\n","e5fea852":"<p style=\"color:#159364; font-family:cursive;\">Add W&B in your code<\/center><\/p>\n","93c3b4d2":"<p style=\"color:#159364; font-family:cursive;\">\ud83d\udc48 Pick a method<\/center><\/p>\n\n*   **grid Search** \u2013 Iterate over every combination of hyperparameter values.\nVery effective, but can be computationally costly.\n*   **random Search** \u2013 Select each new combination at random according to provided distributions. Surprisingly effective!\n*   **bayesian Search** \u2013 Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n\nI will use random search.","81a90e21":"Sweep Controllers, like the one we made by running **wandb.sweep**,\nsit waiting for someone to ask them for a **config** to try out.\n\nThat someone is an agent, and they are created with **wandb.agent**.\nTo get going, the agent just needs to know\n1. which Sweep it's a part of (sweep_id)\n2. which function it's supposed to run (here, train)\n3. (optional) how many configs to ask the Controller for (count)\n\nThe cell below will launch an agent that runs train 5 times,usingly the randomly-generated hyperparameter values returned by the Sweep Controller. Execution takes under 5 minutes.","e9287933":"\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">Step 1\ufe0f\u20e3. Define the Sweep<\/h2>","f76acaca":"<p style=\"color:#159364; font-family:cursive;\">FIT<\/center><\/p>\n\n* NOTE THE DECLARATION OF **OPTIMIZER** AND **SCHEDULER** AS THEY ARE IN THE HYPERPARAMETER SEARCH","f4634c7d":"<p style=\"color:#159364; font-family:cursive;\">\ud83d\udcc3 Name the hyperparameters<\/center><\/p>\n\n*  This step is straightforward: you just give the parameter a name and specify a list of legal values of the parameter.","f432cc9a":"<p style=\"color:#159364; font-family:cursive;\">CONFIGURATION WITH HYPERPARAMETERS OTHER THAN THOSE IN THE HYPERPARAMETER SEARCH<\/center><\/p>","077226a8":"<p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS and THE CUSTOM RNN <\/center><\/p>\n\n* DATASET CLASS: Note that we are using 5 input features,one is cummulative sum of u_ins feature that is seen to improve the LB score in the competion.Refer to this [discussion](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/273974 ) for more insights.\n\n* RNN:3 layer MLP,BIDIRECTIONAL LSTM\/GRU(type of rnn cell is a hyperparameter) and a Prediction Dense Layer\n","22b8e187":"<a id='sec1'><\/a>\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\"> Visualize Sweep Results<\/h2>\n<p style=\"color:#159364; font-family:cursive;\">\ud83d\udd00 Parallel Coordinates Plot<\/center><\/p>\nThis plot maps hyperparameter values to model metrics. It\u2019s useful for honing in on combinations of hyperparameters that led to the best model performance.\n\n","eb421999":"\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">Step 2\ufe0f\u20e3. Initialize the Sweep<\/h2>\n\n\n\n* The clockwork taskmaster in charge of our Sweep is known as the **Sweep Controller**.As each run completes, it will issue a new set of instructions describing a new run to execute.These instructions are picked up by agents who actually perform the runs.\n\n* In a typical Sweep, the Controller lives on W&B machine,while the agents who complete runs live on our machine(s),like in the diagram below.This division of labor makes it super easy to scale up Sweeps by just adding more machines to run agents!\n\n![zlbw3vQ.png](attachment:a2eca4db-f257-4488-a6c2-21dcfbb3f6cd.png)\n\n* We can wind up a Sweep Controller by calling wandb.sweep with the appropriate sweep_config and project name.This function returns a sweep_id that we will later user to assign agents to this Controller."}}