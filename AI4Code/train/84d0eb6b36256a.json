{"cell_type":{"d1dfdd7e":"code","f0289496":"code","0dbf7c92":"code","21e15fb1":"code","d6bf18e6":"code","2c73d83c":"code","84c12984":"code","1dd8a129":"code","a74bb619":"code","0bea5334":"code","455f8c81":"code","98d93762":"code","a82cf350":"code","7540629e":"code","fb0916a3":"code","25b1b685":"code","3045f9d1":"code","d9ceeaf8":"code","fdec8cd4":"code","fb839387":"code","b9450df6":"code","a1c78d26":"code","ef61631a":"code","f196621f":"code","e8faf8be":"code","5356ff37":"code","5fbc3ff1":"code","3ae2b36e":"code","97f97206":"code","fbae0f9d":"code","d1351cb5":"code","71940979":"code","8cb8f42e":"code","2472285b":"code","28220aa0":"code","35645d6d":"code","8201f0d0":"code","97390dbd":"code","3ee9f744":"code","20af7f73":"code","580b754b":"code","0ac6c1df":"code","6c22e438":"code","87e6776a":"code","2ac173da":"code","0f1c2914":"code","7f842760":"code","08012d25":"code","4e00b016":"code","fe000aa5":"code","2b01bcea":"code","74673e7f":"code","dd6e83a0":"code","02021676":"code","7316dc6f":"code","b43e27e3":"code","3bd16ddf":"code","13dafad1":"code","74c8845f":"code","ba430e02":"code","0b252262":"code","5eae9efd":"code","47ba09b8":"code","4959f023":"code","f36166ff":"code","24c21bb7":"code","534d4bf6":"code","2ea7122d":"code","8235d33f":"code","e3513a14":"code","e4f4ac9c":"code","59cf8e07":"code","6f2da42d":"code","64c0c113":"code","7698251d":"code","3ccf4528":"code","658a90bd":"code","9760daab":"markdown","8cd7d739":"markdown","107f41d0":"markdown","7b9ca17f":"markdown","31891fb0":"markdown","70b23d4e":"markdown","d6a3a9a8":"markdown","6fe3531c":"markdown","c3f98614":"markdown","1f2d2dde":"markdown","57a65bf8":"markdown","5ec94b79":"markdown","35879776":"markdown","38c5a6db":"markdown","ed630dc0":"markdown","f1af85ec":"markdown","5c6dbfaf":"markdown","b2080843":"markdown","17dcd7e7":"markdown","d443af28":"markdown","0521d51d":"markdown","3176c2ff":"markdown","a8c4c27e":"markdown","1dd30893":"markdown","200e7923":"markdown","cfa2ad47":"markdown","caa74844":"markdown","da9d5589":"markdown","b2100569":"markdown","1f69b534":"markdown","90304e84":"markdown","6c8b6572":"markdown","70e07b87":"markdown","10b2bfcb":"markdown","5e1565ad":"markdown","c2a94ed2":"markdown","7c4bf72d":"markdown","691cd28b":"markdown","ba7374bc":"markdown","c28b616a":"markdown","c1d0f0dc":"markdown","82590989":"markdown","dff2b17f":"markdown"},"source":{"d1dfdd7e":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tensorflow.keras import models, layers, utils, optimizers, callbacks\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport tensorflow as tf","f0289496":"from numpy.random import seed\nseed(1)\ntf.random.set_seed(1)","0dbf7c92":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","21e15fb1":"ndls_Temp_org_full=pd.read_csv(\"\/kaggle\/input\/delhi-weather-data\/testset.csv\")\nndls_Temp_org_full","d6bf18e6":"print(ndls_Temp_org_full.dtypes)\nprint(\"================================================\")\nprint(ndls_Temp_org_full.info())","2c73d83c":"ndls_Temp_org_full.describe()","84c12984":"print(\"Before modification\")\nprint(ndls_Temp_org_full.columns)\nndls_Temp_org_full.columns=ndls_Temp_org_full.columns.str.replace(\"_\",\"\")\nndls_Temp_org_full.columns=ndls_Temp_org_full.columns.str.replace(\" \",\"\")\nndls_Temp_org_full.columns\nprint(\"=================================\")\nprint(\"After modification\")\nprint(ndls_Temp_org_full.columns)","1dd8a129":"print(ndls_Temp_org_full[\"conds\"].value_counts())\nprint(\"No of unique conditions:\",len(ndls_Temp_org_full[\"conds\"].unique()))","a74bb619":"ndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Widespread Dust\", \"Blowing Sand\", \n                                                           \"Sandstorm\", \"Volcanic Ash\" ,\n                                                            \"Light Sandstorm\"], \"Dust\")\nndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Fog\", \"Shallow Fog\", \"Partial Fog\",\n                                                                \"Light Fog\", \"Mist\", \"Heavy Fog\", \"Light Haze\",\n                                                                \"Patches of Fog\"], \"Fog\")\nndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Scattered Clouds\", \"Partly Cloudy\", \n                                                                 \"Mostly Cloudy\" ,\"Overcast\",\n                                                                 \"Funnel Cloud\"], \"Cloudy\")\nndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Light Rain\", \"Light Drizzle\",\"Rain\", \"Drizzle\", \"Light Rain Showers\"\n                                                                 ,\"Drizzle\" ,\"Rain Showers\"], \"Rain\")\nndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Thunderstorms and Rain\", \"Light Thunderstorms and Rain\",\n                                                                 \"Light Thunderstorm\" ,\"Heavy Thunderstorms and Rain\",\n                                                                \"Heavy Rain\"], \"Thunderstorm\")\nndls_Temp_org_full[\"conds\"]=ndls_Temp_org_full[\"conds\"].replace([\"Thunderstorms with Hail\", \"Squalls\",\n                                                                 \"Light Hail Showers\" ,\"Light Freezing Rain\",\n                                                                \"Heavy Thunderstorms with Hail\", \"Unknown\"], \"Others\")","0bea5334":"print(\"No of unique conditions for GROUPING:\",len(ndls_Temp_org_full[\"conds\"].unique()))\nplt.figure(figsize=(10,5))\nndls_Temp_org_full[\"conds\"].value_counts().plot(kind=\"bar\")","455f8c81":"le=LabelEncoder()\ncol=\"conds\"\nndls_Temp_org_full[col] = ndls_Temp_org_full.apply(lambda x: le.fit_transform(ndls_Temp_org_full[col].astype(str)), axis=0, result_type='expand')","98d93762":"ndls_Temp_org_full[\"conds\"]","a82cf350":"ndls_Temp_org_full[\"conds\"].value_counts()","7540629e":"ndls_Temp_org_full=ndls_Temp_org_full.replace(0, np.nan)","fb0916a3":"print(ndls_Temp_org_full.isnull().sum())\nprint(\"TOTAL NAs:\",ndls_Temp_org_full.isnull().sum().sum())","25b1b685":"halfrows=0.5*ndls_Temp_org_full.shape[0]\nfor i in ndls_Temp_org_full.columns:\n    totalNA=ndls_Temp_org_full[i].isnull().sum()\n    if(totalNA<halfrows):\n        if(ndls_Temp_org_full[i].dtypes==\"object\"):\n            tempmode=ndls_Temp_org_full[i].mode()[0]\n            print(i,\" is categorical has\",str(totalNA),\" NA values replacing is mode: \",tempmode)\n            ndls_Temp_org_full[i].fillna(tempmode, inplace=True)\n        else:\n            tempmedian=ndls_Temp_org_full[i].median()\n            print(i,\" is continuous has\",str(totalNA),\" NA values replacing is median: \",tempmedian)\n            ndls_Temp_org_full[i].fillna(tempmedian, inplace=True)\n    else:\n        print(\"Column to drop:\", i, \"Total NAs\", totalNA)\n        ndls_Temp_org_full.drop([i], axis=1, inplace=True)","3045f9d1":"ndls_Temp_org_full","d9ceeaf8":"print(ndls_Temp_org_full[\"wdire\"].value_counts())\nprint(\"No of unique conditions:\",len(ndls_Temp_org_full[\"wdire\"].unique()))","fdec8cd4":"ndls_Temp_org_full[\"wdire\"]=ndls_Temp_org_full[\"wdire\"].replace([\"WNW\", \"WSW\", \"ESE\", \"ENE\", \"NNW\", \"SSE\", \"NNE\" ,\"SSW\", \"Variable\"], \n                                                                [\"West\", \"West\", \"East\", \"East\", \"North\", \"South\", \"North\", \"South\", \"North\"])","fb839387":"ndls_Temp_org_full[\"wdire\"].value_counts()","b9450df6":"deg=45\nndls_Temp_org_full[\"wdire\"]=ndls_Temp_org_full[\"wdire\"].replace([\"North\",\"NE\", \"East\",\"SE\", \"South\",\"SW\", \"West\", \"NW\"],\n                                                                [0, deg, 2*deg, 3*deg, 4*deg, 5*deg, 6*deg, 7*deg])","a1c78d26":"ndls_Temp_org_full[\"wdire\"].value_counts()","ef61631a":"ndls_Temp_org_full","f196621f":"timeseries_fulldata=ndls_Temp_org_full.copy()","e8faf8be":"timeseries_fulldata[\"datetimeutc\"].dtype","5356ff37":"timeseries_fulldata[\"datetimeutc\"]=pd.to_datetime(timeseries_fulldata[\"datetimeutc\"])","5fbc3ff1":"timeseries_fulldata.set_index(\"datetimeutc\", inplace=True)","3ae2b36e":"timeseries_fulldata","97f97206":"ndls_daily=timeseries_fulldata.resample(\"D\").mean()","fbae0f9d":"ndls_daily","d1351cb5":"ndls_daily.isnull().sum() #nulls created due to rollup","71940979":"ndls_daily.fillna(ndls_daily.mean(), inplace=True)","8cb8f42e":"ndls_daily.isnull().sum()","2472285b":"ndls_daily_temp=pd.DataFrame(list(ndls_daily['tempm']), columns=['temp'])\nndls_daily_temp","28220aa0":"plt.figure(figsize=(20,8))\nplt.plot(ndls_daily_temp)\nplt.grid()\nplt.title(\"Delhi Temp variation (Yearly)\") \nplt.show()","35645d6d":"scaler=MinMaxScaler(feature_range=(-1,1))\nndls_daily_temp_scaled=scaler.fit_transform(ndls_daily_temp)","8201f0d0":"print(ndls_daily_temp_scaled)\nprint(ndls_daily_temp_scaled.shape)","97390dbd":"steps=30\nX_part=[]\nY_part=[]\nfor i in range(len(ndls_daily_temp_scaled)-(steps)):\n    X_part.append(ndls_daily_temp_scaled[i:i+steps])\n    Y_part.append(ndls_daily_temp_scaled[i+steps])\n    \nX_part=np.array(X_part)\nY_part=np.array(Y_part)\n\nprint(X_part.shape)\nprint(Y_part.shape)","3ee9f744":"train_X=X_part[:7300,::]\ntest_X=X_part[7300:,::]\nprint(\"train_X Shape:\",train_X.shape, \",test_X Shape:\", test_X.shape)\n\ntrain_Y=Y_part[:7300]\ntest_Y=Y_part[7300:]\nprint(\"test_Y Shape:\",train_Y.shape, \",test_Y Shape:\", test_Y.shape)","20af7f73":"model1=models.Sequential()\nmodel1.add(layers.Conv1D(filters=128, kernel_size=2, activation=\"relu\", input_shape=(30,1)))\nmodel1.add(layers.Conv1D(filters=128, kernel_size=2, activation=\"relu\"))\nmodel1.add(layers.MaxPooling1D(pool_size=2))\nmodel1.add(layers.Conv1D(filters=256, kernel_size=2, activation=\"relu\"))\nmodel1.add(layers.Flatten())\nmodel1.add(layers.RepeatVector(30)) #same as input (30,1)\nmodel1.add(layers.LSTM(units=100, return_sequences=True, activation=\"relu\"))\nmodel1.add(layers.Dropout(0.2))\nmodel1.add(layers.LSTM(units=100, return_sequences=True, activation=\"relu\"))\nmodel1.add(layers.Dropout(0.2))\nmodel1.add(layers.Bidirectional(layers.LSTM(units=128, activation=\"relu\")))\nmodel1.add(layers.Dense(100, activation=\"relu\"))\nmodel1.add(layers.Dense(1))","580b754b":"model1.compile(loss=\"mae\", optimizer=optimizers.Adam(lr=0.0001))","0ac6c1df":"model1.summary()","6c22e438":"utils.plot_model(model1)","87e6776a":"#callbacks\nEarlystp=callbacks.EarlyStopping(monitor=\"loss\", mode=\"min\", patience=5, restore_best_weights=True)\nSavemod=callbacks.ModelCheckpoint(filepath=\"model1_ts.h5\", monitor=\"loss\", save_best_only=True)","2ac173da":"history=model1.fit(train_X, train_Y, epochs=200, verbose=0, callbacks=[Earlystp, Savemod])","0f1c2914":"hist=history.history\ntrain_loss=hist[\"loss\"]\nepoch=range(1,len(train_loss)+1)  #hist is a dict","7f842760":"plt.plot(epoch, train_loss)","08012d25":"model1_pt=models.load_model(\"model1_ts.h5\")","4e00b016":"ts_temp=model1_pt.predict(test_X)","fe000aa5":"ts_temp=scaler.inverse_transform(ts_temp)","2b01bcea":"ts_temp","74673e7f":"test_Y_inv=scaler.inverse_transform(test_Y)","dd6e83a0":"plt.figure(figsize=(20,9))\nplt.plot(test_Y_inv , 'blue', linewidth=5)\nplt.plot(ts_temp,'r' , linewidth=4)\nplt.xlabel(\"Time\", fontsize=20)\nplt.ylabel(\"Temperature (C)\", fontsize=20)\nplt.legend(('Test','Predicted'))\nplt.show()","02021676":"mse=mean_squared_error(test_Y_inv, ts_temp)\nmae=mean_absolute_error(test_Y_inv, ts_temp)\nprint(\"Mean Squared Error:\", str(mse), \"and Mean Absolute Error:\", str(mae))","7316dc6f":"X_part=ndls_daily.drop([\"tempm\"], axis=1)\nY_part=ndls_daily[\"tempm\"]\nX_part=np.array(X_part)\nY_part=np.array(Y_part).reshape(-1,1)","b43e27e3":"print(\"X shape:\",X_part.shape)\nprint(\"Y shape:\",Y_part.shape)","3bd16ddf":"scaler2=MinMaxScaler(feature_range=[-1,1])\nX_part_scaled=scaler2.fit_transform(X_part)\nY_part_scaled=scaler2.fit_transform(Y_part)","13dafad1":"print(X_part)\nprint(\"===================Post Scalling====================\")\nprint(X_part_scaled)","74c8845f":"print(Y_part)\nprint(\"===================Post Scalling====================\")\nprint(Y_part_scaled)","ba430e02":"step=30\ninput=[]\noutput=[]\nfor i in range(len(X_part_scaled)-(step)):\n    input.append(X_part_scaled[i:i+step])\n    output.append(Y_part_scaled[i+step])\n \ninput=np.array(input)\noutput=np.array(output)\n\nprint(input.shape)\nprint(output.shape)","0b252262":"trainR_X=input[:7300,::]\ntestR_X=input[7300:,::]\nprint(\"train_X Shape:\",trainR_X.shape, \",test_X Shape:\", testR_X.shape)\n\ntrainR_Y=output[:7300]\ntestR_Y=output[7300:]\nprint(\"test_Y Shape:\",trainR_Y.shape, \",test_Y Shape:\", testR_Y.shape)","5eae9efd":"model2=models.Sequential()\nmodel2.add(layers.Conv1D(filters=128, kernel_size=2, activation=\"relu\", input_shape=(30,8)))\nmodel2.add(layers.Conv1D(filters=128, kernel_size=2, activation=\"relu\"))\nmodel2.add(layers.MaxPool1D(pool_size=2))\nmodel2.add(layers.Conv1D(filters=128, kernel_size=2, activation=\"relu\"))\nmodel2.add(layers.Flatten())\nmodel2.add(layers.RepeatVector(30)) #same as input (30,1)=30\nmodel2.add(layers.LSTM(units=100, return_sequences=True, activation=\"relu\"))\nmodel2.add(layers.Dropout(0.2))\nmodel2.add(layers.LSTM(units=100, return_sequences=True, activation=\"relu\"))\nmodel2.add(layers.Dropout(0.2))\nmodel2.add(layers.Bidirectional(layers.LSTM(units=128, activation=\"relu\")))\nmodel2.add(layers.Dense(100, activation=\"relu\"))\nmodel2.add(layers.Dense(1))","47ba09b8":"utils.plot_model(model2)","4959f023":"model2.compile(optimizer=optimizers.Adam(lr=0.001), loss=\"mae\", metrics=[\"mse\"])","f36166ff":"EarlyStp=callbacks.EarlyStopping(monitor=\"loss\", mode=\"min\", patience=5)\nSavemod=callbacks.ModelCheckpoint(filepath=\"model2_R.h5\", monitor=\"loss\", save_best_only=True)","24c21bb7":"history=model2.fit(trainR_X, trainR_Y, epochs=200, verbose=0, callbacks=[Savemod, EarlyStp])","534d4bf6":"hist=history.history\ntrain_loss=hist[\"loss\"]\nepoch=range(1,len(train_loss)+1)  #hist is a dict","2ea7122d":"plt.plot(epoch, train_loss)","8235d33f":"model2=models.load_model(\"model2_R.h5\")","e3513a14":"temp_rs=model2.predict(testR_X)","e4f4ac9c":"temp_rs=scaler2.inverse_transform(temp_rs)\ntemp_rs","59cf8e07":"testR_Y_inv=scaler2.inverse_transform(testR_Y)","6f2da42d":"plt.figure(figsize=(20,9))\nplt.plot(testR_Y_inv , 'blue', linewidth=5)\nplt.plot(temp_rs,'r' , linewidth=4)\nplt.xlabel(\"Time\", fontsize=20)\nplt.ylabel(\"Temperature (C)\", fontsize=20)\nplt.legend(('Test','Predicted'))\nplt.show()","64c0c113":"mse=mean_squared_error(testR_Y_inv, temp_rs)\nmae=mean_absolute_error(testR_Y_inv, temp_rs)\nprint(\"Mean Squared Error:\", str(mse), \"and Mean Absolute Error:\", str(mae))","7698251d":"final_pred=(temp_rs+ts_temp)\/2","3ccf4528":"plt.figure(figsize=(20,9))\nplt.plot(testR_Y_inv , 'red', linewidth=5)\nplt.plot(final_pred,'blue' , linewidth=4)\nplt.xlabel(\"Time\", fontsize=20)\nplt.ylabel(\"Temperature (C)\", fontsize=20)\nplt.legend(('Test','Predicted'))\nplt.show()","658a90bd":"mse=mean_squared_error(testR_Y_inv, final_pred)\nmae=mean_absolute_error(testR_Y_inv, final_pred)\nprint(\"Mean Squared Error:\", str(mse), \"and Mean Absolute Error:\", str(mae))","9760daab":"At a glace we can see \"hum\" or humidity, \"dewpoint\",  \"pressurem\", and even \"tempm\" has outliers. ","8cd7d739":"## Timeseries using Neural Networks.\n\n### A brief introduction","107f41d0":"## Delhi Temperature prediction","7b9ca17f":"### Prediting the Daily temperature using the Best model Saved (Savemod)\n\nThe predicted temprature will be saved for **ensembling** with Regression model","31891fb0":"### Conclusion\n\nThus we note that out regression model adds some bias to the prediction which helps in preventing overfitting and games the model more generalised. The MSE is around 3 to 4 and on absolute scale our prediction if off by around 1.3 to 2 deg C which is quite in the acceptable range.\n\nThus this model can predict Delhi's temperature based on the data provided with a difference of 1.5 deg C.","70b23d4e":"### Handling Date and time varibles","d6a3a9a8":"### Train the model- .fit() method","6fe3531c":"We have loaded the data from the path and print it on the notebook. On the first glance we see that the data has lots of **NULL** values which we need to get ridoff.","c3f98614":"![download.png](attachment:download.png)","1f2d2dde":"We also note that out regression model also works as expected. But it doesn't overfit the model. This is what we want as it introduces some amount of bias into the prediction when we average out with our timeseries data.","57a65bf8":"Hereby we note wind-direction (wdire) has 17 unique variables.\n\nFirstly, we will group them by 8 major directions like \"WNW\" is more inclined towards West to we will group them to as \"West\". Similarly \"NNE\" has been grouped as \"North\".","5ec94b79":"Now we see that the dimensionality of the variable \"cond\" has reduced to 10 or a 4 times reduction.","35879776":"### Plotting of Predicted temperature and Actual temperature","38c5a6db":"An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs.\n\n\nTraining in Neural networks happens by adjusting the weights and bias terms by forward propagation and backward propagation using gradient decent and the concept of differenciation.\nA basic NN equation:\n    \n$$ Z=Activation(Weight.X + bias) $$\n\nWhere activation can be Sigmoid, Tanh or Relu (rectified Linear unit)\n\nThe weights and bias are adjusted to minimise the error in prediction (differece between predicted output and actual output)\n\n\n![NN.jpeg](attachment:NN.jpeg)\n\n\n\nPic: **Dense Neural network**\n","ed630dc0":"### Rollup\n\nThis data is in hourly format but we want to predict daily temperature so we will rollup the data to a Daily scale. It will be an average of each hourly temperature.","f1af85ec":"Based on Mean Squared Error we see a sqaured error of (3.8 deg C) and absolute error of 1.54 deg C\n\nThus our model predicts the daily temperature with less than 2 deg of error.\n\nBut somehow the model overfits the training data and by using regression we will introduce some **bias** into the model to make it more generalised.","5c6dbfaf":"**Scaling of Data**","b2080843":"### Plotting the loss curves.","17dcd7e7":"To convert them to continuous type variable we label encode them. We use Sklearns LabelEncoder().","d443af28":"We begin by importing the dependencies:-\n\n1. OS: for file navigation from storage devices.\n2. Numpy for array and calculations.\n3. Pandas for data management including dataframes and series.\n4. Matplotlib Pyplot for visual representation.\n5. Sklearns MinMaxScaler for Data scaling.\n6. From Keras(tensorflow) we will be importing Layers (Dense, Maxpooling, Conv1D, Bidirection and LSTM) for both Timeseries and Regression.\n7. From Sklearn Metrics we will import mean_squared_error and mean_absolute_error to assess the performance of the model.","0521d51d":"### Scaling of Data\n\nWe will use **MixMax Scaler** to scale the data between -1 to +1\n\nScaling is necessary as the values are not same scale. Some in tens to some in hundreds. Thus creates issues  while learning, weight and bias adjustments in the model.","3176c2ff":"For time series we will use \n\n### **LSTM and Conv1D layers with MaxPooling1D.**\n\n### LSTM\n\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture. Unlike standard feedforward neural networks, LSTM has feedback connections which is helpful to process entire sequences of data instead of a single data point.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n\n![LSTM.png](attachment:LSTM.png)\n\n\n### Conv1D\n\nThe Conv1D model extracts features from sequences data and maps the internal features of the sequence. A 1D CNN is very effective for deriving features from a fixed-length segment of the overall dataset, and location of the feature in the segment is not so important.\n![conv1d.png](attachment:conv1d.png)\n\n\n### Maxpooling\nDownsamples the input representation by taking the maximum value over the window defined by pool_size. The window is shifted by strides. The resulting output when using \"valid\" padding option has a shape of: output_shape = (input_shape - pool_size + 1) \/ strides)\n\n![maxpooling1d.png](attachment:maxpooling1d.png)\n\n\n### Flatten\nThe Flatten() operator unrolls the values beginning at the last dimension\n\n--Source: Wikipedia, Keras.io, Medium","a8c4c27e":"After grouping them to 8 major directions, now we will use directional compass to convert this categorical variable to continuous one. The direction will be replaced by their degrees based on North. Like North=0 degree, East=90 degree, South=180, and West=270. We will use a incrementor variable \"deg=45 (360\/8)\" to encode the 8 directions.","1dd30893":"### Train Test Split. \n\n**(Non Random)**","200e7923":"On the above **dtypes** and **info()** output we take a note of the different variable types and the count of \"non-null values\". Some categories like \"precipm\", \"wgustm\", \"windchillm\", etc almost have 80% and above of null values, imputing these columns will not help significantly and thus we will drop these columns in a short while.","cfa2ad47":"\"datetimeutc\" variable has a data type \"object\" which needs to be converted to Date-time so that we can take benefit of Day or month or yearwise rollups, convert it into a timeindex for timeseries etc.\nWe will use pandas \"to_datetime\" function.","caa74844":"Thus we see we have **40** unique condition types.So lets try to reduce them.\n\nWe know that \"Sand\", \"Dust, \"Ash\" etc are all dusty conditions we will replace them with one unique key \"Dust\"\nSimilary we will modifiy keys like \"Fog\", \"Shallow Fog\", \"Partial Fog\", etc as \"Fog\" and so on and so forth.","da9d5589":"## Timeseries Analysis","b2100569":"Hi, welcome to my notebook on Delhi's temperature prediction.\n\nThe goal of this notebook is to predict temperature of the following based on a certain number of prevous day's temperature provided (**timeseires**) and **In addition too**, I have used **Regression** analysis to **predict the temperature** based on other **climatic factors** like **Humidity**, **Air direction**,**Rain**, Smoke, Dust, Storm etc.\n\nIn both cases (timeseries and regression) we will use Neural Network libraries like **Keras** with **Tensorflow backend**.","1f69b534":"### Re-modifying X and Y based on the timeseries pattern\n\nWe are doing this so that we can average out the output of both the models to predict the final output.","90304e84":"The rollup induces some null values towards the end but we will takecare of it by taking a mean of daily temperatures.","6c8b6572":"Before we proceed first we will remove the the underscores from the column names.","70e07b87":"We do notice that the model performs quite well with the red line (predicted) almost superimposed on the blue line (actual)","10b2bfcb":"## Regression model\n\nWhy we use a Regression model in a temperature prediction task?\n\nSimple, weather, temperature doesn't work in isolation does it? No, temperature is dependent on humidity, winddirection, pollution level, dust, cloud cover etc. \n\nSo we will use a regression model to study the connection between tempature and other independent variables and try to predict the temparature based on that and average out both the prediction to get a final temperature prediction for our capital's temperature.","5e1565ad":"### Removing of NULL values\n\nWe saw a lot of Null values present in our data from the overview and subsequest analysis. Let us proceed to remove them.\n\nA quick note here that we will be converting all \"0\" to \"np.nan\" or NAN values to remove them in one go.","c2a94ed2":"### Callbacks\n\n\"Keras callbacks\" return(s) information from a training algorithm while training is taking place. This is used to control the learning (pace) of the model preventing it from overfitting or underfitting or \"keep us from waiting while the model doesn't learn anything.\n\n\nHere we are going to use two calls backs:-\n\n**Earlystop** which will monitor loss with a patience of \"5\" targetting to minimize the loss to prevent overfitting.\n\n**Savemod** to save the best model from all the iterations.\n","7c4bf72d":"### Creating the timeseries datatimeline\n\nLogic & Pseudocode:\n\nUsing a timeline of 30(step) past days to predict current day's temperature.\n    \n    A loop upto fulldataset except last 30 days \n\n    X will have temperature range from i+30(steps)days or 30 days eg: 1 to 29 dates inclusive (this will capture the full  \n    dataset)\n    \n    Y will be the temp of 30th day (i+step)\n    \nThe code as follows:-","691cd28b":"### Dimension Reduction: Grouping Categorical variable \"Cond\"\n\nWe note that some of the variables \"cond\" (condition) and \"wdire\" (wind direction)  has lots of unique keys or \"high dimension\". These will be an memory management issue if we try to convert this categorical variable by **One hot encoding** using pandas(get_dummies) or any other library function. So we will group similar values together to manually reduce the dimension.","ba7374bc":"So basically we used a for loop to find the total number of null values per variable (columns). If the total null values is less than 50% of the data (row length) then we will impute the null data points based on the following criteria:\n1. Categorical: impute with First Mode\n2. Continuous :  impute with median.\n\nAny column which has more than 50% of null values are dropped.\n\nWe approx drop 10 columns which is evident from the next output","c28b616a":"We begin by separating the **dependent variable (tempm) and independent variables**","c1d0f0dc":"### Regression model\n\nWe will use almost the same model with **no activation** at the last **Dense** layer to get the regression output.","82590989":"## Ensembling the model 2 model's outputs\n\nWe will take a mean of each days temperature as predicted by the models.\n\n   $$ temp.Day1=(temp.Day1.m1+temp.Day1.m2)\/2$$\n   $$ temp.Day2=(temp.Day2.m1+temp.Day2.m2)\/2$$  \n   \netc","dff2b17f":"### Dimension Reduction: Grouping Categorical variable \"wdire\" (Wind direction)"}}