{"cell_type":{"ccf3af73":"code","fcc3d75a":"code","33d6571a":"code","745e4917":"code","50e51c15":"code","fa248b48":"code","91e8c068":"code","d0ce22ae":"code","7e76e776":"code","a2dc4179":"code","afe804b4":"code","4fe81eb1":"code","c409530e":"code","447ecaf0":"code","d9017328":"code","ba15d7a6":"code","ae85f79e":"code","99d171f3":"code","0386b28f":"code","61176ba4":"code","f5887884":"code","e96560df":"code","e5204c97":"code","068a32d2":"code","09824ad4":"code","42b1287b":"code","c4f822a9":"code","b28bf8d6":"code","13f5d402":"code","27ddee3d":"code","6c8ba659":"code","f8ec1f95":"markdown","558c0d02":"markdown","af246824":"markdown","4717b615":"markdown","8763e64a":"markdown"},"source":{"ccf3af73":"#Import Packages\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport warnings\nimport nltk\nimport string\nimport seaborn as sns\nfrom nltk.stem.porter import * \nfrom wordcloud import WordCloud\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","fcc3d75a":"#import data\ntrain = pd.read_csv('..\/input\/train_E6oV3lV.csv')\ntest = pd.read_csv('..\/input\/test_tweets_anuFYb8.csv')\n","33d6571a":"train.head(5)","745e4917":"test.head(5)","50e51c15":"\nprint('Shape of Train Dataset:',train.shape)\nprint('Shape of Test Dataset:',test.shape)\n\n","fa248b48":"train[train['label'] == 0].head(5)\n","91e8c068":"train[train['label'] == 1].head(5)","d0ce22ae":"positive = train['label'].value_counts()[0]\nnegative = train['label'].value_counts()[1]\n\nflatui = [\"#15ff00\", \"#ff0033\"]\nsns.set_palette(flatui)\nsns.barplot(['Positive','Negative'],[positive,negative])\nplt.xlabel('Tweet Classification')\nplt.ylabel('Count of Tweets')\nplt.title('Balanced or Unbalanced Dataset')\nplt.show()\n\nprint('No of Tweets labelled as Non-Sexist:',positive)\nprint('No of Tweets labelled as Sexist:',negative)\n\nprint('Data is highly unbalanced with only',round(((negative\/(negative+positive))*100),2),'% negative points and ',\n      round(((positive\/(negative+positive))*100),2),'% positive points')","7e76e776":"tweetLengthTrain = train['tweet'].str.len()\ntweetLengthTest = test['tweet'].str.len()\n\nplt.hist(tweetLengthTrain,bins=20,label='Train_Tweet')\nplt.hist(tweetLengthTest,bins=20,label='Test_Tweet')\nplt.legend()\nplt.show()","a2dc4179":"#Comining both Train and Test Data Set before Data Cleaning\n# since Label Column is not present in test Dataset, the values are filled with NaN\ncombine = train.append(test,ignore_index=True)\nprint('Shape of new Dataset:',combine.shape)\ncombine.tail()","afe804b4":"#User Defined Function to clean unwanted text patterns from all tweets\n# input - text to clean,pattern to replace\ndef cleantext(inputword,pattern):\n    r = re.findall(pattern=pattern,string=inputword)\n    for i in r:\n        inputword = re.sub(pattern=i,repl='',string=inputword)\n    return inputword\n    ","4fe81eb1":"#Removing all twitter handles because they are already masked as @user due to privacy concerns.\n#These twitter handles hardly give any information about the nature of the tweet.\ncombine['cleanedText'] = np.vectorize(cleantext)(combine['tweet'],'@[\\w]*')\ncombine.head()\n","c409530e":"combine['cleanedText'] = combine['cleanedText'].str.replace(\"[^a-zA-Z#]\",\" \")\ncombine.head()\n\n","447ecaf0":"combine['cleanedText'] = combine['cleanedText'].str.replace(r'\\b(\\w{1,2})\\b', '')\ncombine.head()","d9017328":"#Tokenize the tweets\ntokenized_tweets = combine['cleanedText'].apply(lambda x:x.split())\ntokenized_tweets.head()","ba15d7a6":"#Stemming the words to remove words with similar meaning\nstemmer = PorterStemmer()\ntokenized_tweets = tokenized_tweets.apply(lambda x : [stemmer.stem(i) for i in x]  )","ae85f79e":"tokenized_tweets.head()","99d171f3":"#Joining the tokenized tweets\n\nfor i in range(len(tokenized_tweets)):\n    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])    \ncombine['cleanedText'] = tokenized_tweets","0386b28f":"combine.head()","61176ba4":"# Creating word Cloud for all Words in all tweets\nallWords = ' '.join([text for text in combine['cleanedText']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(allWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","f5887884":"# Creating word Cloud for all Words in all positive tweets\npositiveWords = ' '.join([text for text in combine['cleanedText'][combine['label'] == 0]])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positiveWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","e96560df":"# Creating word Cloud for all Words in all negative tweets\npositiveWords = ' '.join([text for text in combine['cleanedText'][combine['label'] == 1]])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positiveWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","e5204c97":"#Function Collecting HashTag\ndef collectHashtag(x):\n    hashtags = []    \n    for i in x:        \n        ht = re.findall(r\"#(\\w+)\", i)        \n        hashtags.append(ht)     \n    return hashtags","068a32d2":"#Collect all the hashtags in positive and negative tweets\nHT_positive = collectHashtag(combine['cleanedText'][combine['label'] == 0])\n#Nested List to Un-nested List\nHT_positive = sum(HT_positive,[])\n\nHT_negative = collectHashtag(combine['cleanedText'][combine['label'] == 1])\nHT_negative = sum(HT_negative,[])\n","09824ad4":"corpus_positive = nltk.FreqDist(HT_positive)\ncorpus_negative = nltk.FreqDist(HT_negative)","42b1287b":"d = pd.DataFrame({'Hashtag':list(corpus_positive.keys()),'Count':list(corpus_positive.values())})\nd = d.nlargest(columns='Count',n=20)","c4f822a9":"ax = sns.barplot(data = d,x = 'Hashtag',y = 'Count')\nplt.figure(figsize=(16,5))\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.show()","b28bf8d6":"d = pd.DataFrame({'Hashtag':list(corpus_negative.keys()),'Count':list(corpus_negative.values())})\nd = d.nlargest(columns='Count',n=20)\n\nax = sns.barplot(data = d,x = 'Hashtag',y = 'Count')\nplt.figure(figsize=(16,5))\nplt.setp(ax.get_xticklabels(), rotation=90)\n\nplt.show()","13f5d402":"#Vectorization\n#Importing Required Packages\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport gensim","27ddee3d":"#Applying Bag of Words Vectorization to the Tweets\nbow_vectorizer = CountVectorizer(stop_words= 'english')\nbow = bow_vectorizer.fit_transform(combine['cleanedText'])\n","6c8ba659":"#Applying TF-IDF Vectorization to the Tweets\ntfidf_vectorizer = TfidfVectorizer(stop_words= 'english')\ntfidf = tfidf_vectorizer.fit_transform(combine['cleanedText'])","f8ec1f95":"Check the Distribution of Length of Tweets in train and Test Dataset","558c0d02":"**Data Cleaning**","af246824":"Checking whether the Training Data is Balanced or Unbalanced","4717b615":"Checking the Train Dataset for Tweets labelled as Racist\/Sexist or Not\nLabel = 1 means Tweet is Sexist\/Racist\nLabel = 0 means Tweet is neither Sexist nor Racist","8763e64a":"Check the first five rows of both Train and Test dataset"}}