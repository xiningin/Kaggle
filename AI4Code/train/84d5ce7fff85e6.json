{"cell_type":{"978b52e4":"code","95067624":"code","852c6e46":"code","52bc0949":"code","be6cbbb3":"code","5173ad19":"code","472e793a":"code","c9e1b2c0":"code","52cc59cb":"code","eed62b6c":"code","1fb638c7":"code","aaf42d9b":"code","6abc9e3b":"code","93c994f1":"code","8ce10bf7":"code","c72c84ba":"code","f4963681":"code","b307ed1e":"code","3a31ab18":"code","d4780d73":"code","4dd9deda":"code","f1aac8d0":"markdown","42c0c0c6":"markdown","0c43a272":"markdown"},"source":{"978b52e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95067624":"df = pd.read_excel(\"\/kaggle\/input\/BreastCancer_Prognostic_v1.xlsx\")","852c6e46":"df.head()\n","52bc0949":"del df[\"Lymph_Node_Status\"]\n","be6cbbb3":"df.isnull().sum()","5173ad19":"df.info()","472e793a":"df.describe()","c9e1b2c0":"col = df.columns\nprint(col)","52cc59cb":"del df[\"ID\"]","eed62b6c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nax = sns.countplot(df[\"Outcome\"],label=\"Count\")     \nB, M = df[\"Outcome\"].value_counts()\nprint('Number of Recurrent: ',B)\nprint('Number of Non-Recurrent : ',M)\n","1fb638c7":"df['Outcome'] = df['Outcome'].map({'R':1,'N':0})","aaf42d9b":"df.head()","6abc9e3b":"features_mean=list(df.columns[1:34])\n# split dataframe into two based on diagnosis\ndfM=df[df['Outcome'] ==1]\ndfB=df[df['Outcome'] ==0]","93c994f1":"#Stack the data\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=16, ncols=2, figsize=(16,16))\naxes = axes.ravel()\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))\/50\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True,  label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","8ce10bf7":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold   #For K-fold cross validation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics","c72c84ba":"traindf, testdf = train_test_split(df, test_size = 0.3)","f4963681":"def classification_model(model, data, predictors, outcome):\n  #Fit the model:\n  model.fit(data[predictors],data[outcome])\n  \n  #Make predictions on training set:\n  predictions = model.predict(data[predictors])\n  \n  #Print accuracy\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\n  print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n  #Perform k-fold cross-validation with 5 folds\n  kf= KFold( n_splits=5)\n  error = []\n  for train, test in kf.split(data):\n    # Filter training data\n    train_predictors = (data[predictors].iloc[train,:])\n    \n    # The target we're using to train the algorithm.\n    train_target = data[outcome].iloc[train]\n    \n    # Training the algorithm using the predictors and target.\n    model.fit(train_predictors, train_target)\n    \n    #Record error from each cross-validation run\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n    \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n    \n  #Fit the model again so that it can be refered outside the function:\n  model.fit(data[predictors],data[outcome]) ","b307ed1e":"predictor_var = [ 'Time', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_std_dev', 'texture_std_dev', 'perimeter_std_dev',\n       'area_std_dev', 'smoothness_std_dev', 'compactness_std_dev',\n       'concavity_std_dev', 'concave_points_std_dev', 'symmetry_std_dev',\n       'fractal_dimension_std_dev', 'Worst_radius', 'Worst_texture',\n       'Worst_perimeter', 'Worst_area', 'Worst_smoothness',\n       'Worst_compactness', 'Worst_concavity', 'Worst_concave_points',\n       'Worst_symmetry', 'Worst_fractal_dimension', 'Tumor_Size']\noutcome_var='Outcome'\nmodel= LogisticRegression()\nclassification_model(model,traindf,predictor_var,outcome_var)","3a31ab18":"predictor_var = [ 'Time', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_std_dev', 'texture_std_dev', 'perimeter_std_dev',\n       'area_std_dev', 'smoothness_std_dev', 'compactness_std_dev',\n       'concavity_std_dev', 'concave_points_std_dev', 'symmetry_std_dev',\n       'fractal_dimension_std_dev', 'Worst_radius', 'Worst_texture',\n       'Worst_perimeter', 'Worst_area', 'Worst_smoothness',\n       'Worst_compactness', 'Worst_concavity', 'Worst_concave_points',\n       'Worst_symmetry', 'Worst_fractal_dimension', 'Tumor_Size']\noutcome_var='Outcome'\nmodel = RandomForestClassifier(n_estimators=600,min_samples_split=25)\nclassification_model(model, traindf,predictor_var,outcome_var)","d4780d73":"featimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\nprint(featimp)\n","4dd9deda":"predictor_var = ['Time','Worst_area', 'Tumor_Size',\"Worst_perimeter\",\"Worst_radius\",\"Worst_smoothness\"]\noutcome_var='Outcome'\nmodel = RandomForestClassifier(n_estimators=600,min_samples_split=25)\nclassification_model(model, traindf,predictor_var,outcome_var)","f1aac8d0":"**Random Forest**","42c0c0c6":"**Using Top 5 features**","0c43a272":"**Logistic Regression**"}}