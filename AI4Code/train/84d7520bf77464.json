{"cell_type":{"a724147d":"code","f26ec52c":"code","f8ee0a9a":"code","ab943051":"code","44a33110":"code","10e3207c":"code","f2037e5e":"code","e4f50469":"code","db403726":"code","fe3f27b8":"code","bdb4215e":"code","fe164864":"code","82030ece":"code","7ba03c8b":"code","c976eee2":"code","2f33cb9b":"code","42283284":"code","6fa61e5d":"code","becccc0e":"code","8762f95c":"code","ca42037f":"code","de7dfc2c":"code","46c4e378":"code","f1f25d5b":"code","35d8d006":"code","67a5a17a":"code","8c06c3a6":"code","d4383eac":"code","2e7d151b":"markdown","01cb1138":"markdown","647397f3":"markdown","c8eae36a":"markdown","05b0b484":"markdown","f16dddf5":"markdown","9bc192cc":"markdown","32d12098":"markdown","94561c0f":"markdown","cbac2fbc":"markdown","9b54da9d":"markdown","0d4b7b24":"markdown","4f70e0a4":"markdown","bc25c10a":"markdown","35b65e87":"markdown","b885f243":"markdown"},"source":{"a724147d":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f26ec52c":"#Load dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain.head(5)","f8ee0a9a":"#Dataset shape\nprint('Train %s\\nTest %s' % (train.shape, test.shape))","ab943051":"#Check the data types\ntrain.dtypes.value_counts().plot(kind='barh')\nplt.title('Data types present with counts')\nplt.show()","44a33110":"#Describe data set\ntrain.describe()","10e3207c":"#Feature to predict\nlist(set(train.columns) - set(test.columns))","f2037e5e":"#save and drop id\ntrain_id = train[\"Id\"]\ntrain.drop(columns='Id',inplace=True)\n\ntest_id = test[\"Id\"]\ntest.drop(columns='Id',inplace=True)\n\n#select object columns\nobj_col = train.columns[train.dtypes == 'object'].values\n\n#select non object columns\nnum_col = train.columns[train.dtypes != 'object'].values\nnum_col_test = test.columns[test.dtypes != 'object'].values\n\n#replace null value in obj columns with None\ntrain[obj_col] = train[obj_col].fillna('None')\ntest[obj_col] = test[obj_col].fillna('None')\n\n#replace null value in numeric columns with 0\ntrain[num_col] = train[num_col].fillna(0)\ntest[num_col_test] = test[num_col_test].fillna(0)","e4f50469":"#Ordinal features\nordinal_features = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\", \n                    \"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\",\"Electrical\",\"KitchenQual\", \n                    \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\ntrain[ordinal_features].head(5)","db403726":"#Map values\nmap_1 = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0}\nmap_2 = {\"Gd\":5,\"Av\":4,\"Mn\":3,\"No\":2,\"None\":1}\nmap_3 = {\"GLQ\":6,\"ALQ\":5,\"BLQ\":4,\"Rec\":3,\"LwQ\":2,\"Unf\":1,\"None\":0}\nmap_4 = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\nmap_5 = {\"SBrkr\":5,\"FuseA\":4,\"FuseF\":3,\"FuseP\":2,\"Mix\":1,\"None\":0}\n\n#Encoding\ntrain[\"ExterQual\"] = train[\"ExterQual\"].map(map_1)\ntrain[\"ExterCond\"] = train[\"ExterCond\"].map(map_1)\ntrain[\"BsmtQual\"]  = train[\"BsmtQual\"].map(map_1)\ntrain[\"BsmtCond\"]  = train[\"BsmtCond\"].map(map_1)\ntrain[\"BsmtExposure\"] = train[\"BsmtExposure\"].map(map_2)\ntrain[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].map(map_3)\ntrain[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].map(map_3)\ntrain[\"HeatingQC\"]   = train[\"HeatingQC\"].map(map_4)\ntrain[\"Electrical\"]  = train[\"Electrical\"].map(map_5)\ntrain[\"KitchenQual\"] = train[\"KitchenQual\"].map(map_1)\ntrain[\"FireplaceQu\"] = train[\"FireplaceQu\"].map(map_1)\ntrain[\"GarageQual\"]  = train[\"GarageQual\"].map(map_1)\ntrain[\"GarageCond\"]  = train[\"GarageCond\"].map(map_1)\ntrain[\"PoolQC\"]    = train[\"PoolQC\"].map(map_1)\n\ntest[\"ExterQual\"] = test[\"ExterQual\"].map(map_1)\ntest[\"ExterCond\"] = test[\"ExterCond\"].map(map_1)\ntest[\"BsmtQual\"]  = test[\"BsmtQual\"].map(map_1)\ntest[\"BsmtCond\"]  = test[\"BsmtCond\"].map(map_1)\ntest[\"BsmtExposure\"] = test[\"BsmtExposure\"].map(map_2)\ntest[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].map(map_3)\ntest[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].map(map_3)\ntest[\"HeatingQC\"]   = test[\"HeatingQC\"].map(map_4)\ntest[\"Electrical\"]  = test[\"Electrical\"].map(map_5)\ntest[\"KitchenQual\"] = test[\"KitchenQual\"].map(map_1)\ntest[\"FireplaceQu\"] = test[\"FireplaceQu\"].map(map_1)\ntest[\"GarageQual\"]  = test[\"GarageQual\"].map(map_1)\ntest[\"GarageCond\"]  = test[\"GarageCond\"].map(map_1)\ntest[\"PoolQC\"]   = test[\"PoolQC\"].map(map_1)","fe3f27b8":"#after encoding\ntrain[ordinal_features].head(5)","bdb4215e":"#Nominal features\nnominal_features = [x for x in obj_col if x not in ordinal_features]\n\ntrain[nominal_features].head(5)","fe164864":"#Transfer object to int\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#for loop nominal feature column\nfor i in train[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    train[i] = labelencoder.fit_transform(train[i])\n    \n#for loop nominal feature column\nfor i in test[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    test[i] = labelencoder.fit_transform(test[i])\n    \n#Get dummy variable for nominal features\ntrain = pd.get_dummies(train,columns=nominal_features,drop_first=True)\ntest = pd.get_dummies(test,columns=nominal_features,drop_first=True)","82030ece":"#Only for test set\n#Check if any null values\nprint(train.isnull().any().sum())\nprint(test.isnull().any().sum())\n\n#Get missing columns in the training test\nmissing_cols = set(train.drop(columns=\"SalePrice\").columns) - set(test.columns)\n\n#Add a missing column in test set with default value equal to 0\nfor cols in missing_cols:\n    test[cols] = 0\n    \n#Ensure the order of column in the test set is in the same order than in train set\ntest = test[train.drop(columns=\"SalePrice\").columns]","7ba03c8b":"#TotalBath\ntrain['TotalBath'] = (train['FullBath'] + (0.5 * train['HalfBath']) + \n                         train['BsmtFullBath'] + (0.5 * train['BsmtHalfBath']))\ntest['TotalBath'] = (test['FullBath'] + (0.5 * test['HalfBath']) + \n                         test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath']))","c976eee2":"#HasPool\ntrain['HasPool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['HasPool']  = test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)","2f33cb9b":"#HasFireplaces\ntrain['HasFirePlace'] = train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntest['HasFirePlace'] = test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","42283284":"#Importing Packages\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn import svm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","6fa61e5d":"#define a evaluation function, Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle_score(train, pred):\n    rmsle_score = (np.sum((np.log1p(train)-np.log1p(pred))**2)\/len(pred))**0.5\n    return rmsle_score","becccc0e":"#Test Model\ndef test_model(model, X_train, y_train):\n    rmsle  = make_scorer(rmsle_score)\n    cv = KFold(n_splits=5,shuffle=True,random_state=45)\n    rmsle_score_cv = cross_val_score(model, X_train, y_train, cv = cv ,scoring = rmsle)\n    score=[rmsle_score_cv.mean()]\n    \n    return score","8762f95c":"#Fit models\ndef fit_models(X_train, y_train):\n    results={}\n    \n    model = linear_model.LinearRegression()\n    results[\"Linear\"]=test_model(model, X_train, y_train)\n    \n    model = linear_model.Ridge()\n    results[\"Ridge\"]=test_model(model, X_train, y_train)\n    \n    model = linear_model.BayesianRidge()\n    results[\"Bayesian Ridge\"]=test_model(model, X_train, y_train)\n    \n    model = linear_model.HuberRegressor()\n    results[\"Hubber\"]=test_model(model, X_train, y_train)\n    \n    model = linear_model.Lasso(alpha=1e-4)\n    results[\"Lasso\"]=test_model(model, X_train, y_train)\n    \n    model = BaggingRegressor()\n    results[\"Bagging\"]=test_model(model, X_train, y_train)\n    \n    model = svm.SVR()\n    results[\"SVM RBF\"]=test_model(model, X_train, y_train)\n    \n    model = svm.SVR(kernel=\"linear\")\n    results[\"SVM Linear\"]=test_model(model, X_train, y_train)\n    \n    results = pd.DataFrame.from_dict(results,orient='index')\n    results.columns=[\"Square Score\"] \n    results = results.sort_values(\"Square Score\", ascending=True)\n    \n    return results","ca42037f":"# Split data to X(features) and y(target)\nX = train.drop(columns=\"SalePrice\")\ny = train[\"SalePrice\"]","de7dfc2c":"#fit all models\nfit_models(X, y)","46c4e378":"#Split data to train dataset and validation dataset\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=0)","f1f25d5b":"#Create model\nmodel = BaggingRegressor()\n\n#Train the model using the val set\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)\n\nprint('RMSLE:', rmsle_score(y_val, y_pred))\nprint('MSE:', mean_squared_error(y_val, y_pred))","35d8d006":"#Importing Packages\nfrom sklearn.decomposition import PCA","67a5a17a":"#reduction of dimensionality to plot\ndf = X_val\n\n#Applying PCA\npca = PCA(n_components=1,whiten=True).fit(df)\ndfPCA = pca.transform(df)\n\n#Join Y val and predict\ndfPCA = pd.DataFrame(dfPCA, columns = ['X'])\ndfPCA['y_val'] = y.reset_index(drop=True)\ndfPCA['y_pred'] = y_pred","8c06c3a6":"# Plot outputs\n\nplt.figure(figsize=(15, 5)) #Chart size\n\n#Chart Real and Predict\nplt.subplot(1, 2, 1)\nplt.scatter(dfPCA.X, dfPCA.y_val,  color='blue', label='Real', alpha=0.5)\nplt.scatter(dfPCA.X, dfPCA.y_pred,  color='red', label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.xlabel(\"Feature\")\nplt.ylabel(\"SalePrice\")\nplt.legend(loc='best')\nplt.xticks(())\nplt.yticks(())\n\n#ChartReal vs Predict\nplt.subplot(1, 2, 2)\nplt.scatter(dfPCA.y_val, dfPCA.y_pred,  color='blue')\nplt.title(\"Real vs Predict\")\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predict\")\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","d4383eac":"# Split data to X(features)\ntest_X = test.values\n\ny_pred_test = model.predict(test_X)\n\nsubmission = pd.DataFrame({'Id':test_id,'SalePrice':y_pred_test})\n\n# Save results\nsubmission.to_csv(\"submission.csv\",index=False)","2e7d151b":"# 5. Models","01cb1138":"# 1. Importing Packages","647397f3":"# 3. Data preprocessing\n\n<ul>\n    <li>First I'll replace the numeric missing values (NaN's) with 0 and non numeric with none.\n    <li>Create Dummy variables for the categorical features.\n    <li>transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal.\n<\/ul>","c8eae36a":"# 6. Plot Results","05b0b484":"<br>FullBath: Full bathrooms above grade \n<br>HalfBath: Half baths above grade\n<br>totalbath = FullBath + HalfBath","f16dddf5":"# 4. Feature Engineering","9bc192cc":"\n# Regression Models\n\n\n<br>Reference:\n<br>https:\/\/www.kaggle.com\/alexzhuzk\/linear-xgb-regression-novice\n<br>https:\/\/www.kaggle.com\/alfredmaboa\/advanced-regression-techniques-regularization\n<br>https:\/\/www.kaggle.com\/miguelangelnieto\/pca-and-regression","32d12098":"### 5.2.1 All models","94561c0f":"# 2. Loading and Inspecting Data","cbac2fbc":"## 5.1 Definitions","9b54da9d":"## 3.1 Fill NaN values","0d4b7b24":"## 3.3 Encode nominal features","4f70e0a4":"# 7. Predic Test & Submission","bc25c10a":"## 5.2 Fit models","35b65e87":"## 3.2 Encoding ordinal features","b885f243":"### 5.2.2 Best model"}}