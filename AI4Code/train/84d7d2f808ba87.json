{"cell_type":{"22a50ccd":"code","4c25a764":"code","f6374005":"code","f3e87f26":"code","f6103159":"code","25cbf0ca":"code","2d66c63c":"code","ee45534c":"code","fcedd9a8":"code","3fa9fcfe":"code","fd618f30":"code","da869f67":"code","48d5840b":"code","e14adb0f":"code","7eb9e45e":"code","acba70fc":"code","5cf95eb3":"code","c3243dff":"code","1397831c":"code","45183391":"code","3fc56757":"code","af0ff1c5":"code","14ac87f8":"code","fc59d89c":"code","e1c5ad9f":"code","9a263159":"code","117a9623":"code","b9f12147":"code","9fddf454":"markdown","f45e63ca":"markdown","f9f1d514":"markdown","236cad8f":"markdown","e2805643":"markdown","e238c4db":"markdown","d71c6b40":"markdown","2f61739f":"markdown","1fac26f1":"markdown","686b7f21":"markdown","2d3af5ab":"markdown","60455b5a":"markdown","004f2489":"markdown"},"source":{"22a50ccd":"!pip install pylops","4c25a764":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport matplotlib.cm as cm\nfrom skimage import io\nimport pylops\n\nimport tensorflow as tf \n\nimport os\nimport shutil\nfrom tqdm import tqdm\nfrom random import shuffle\n\nimport cv2\nfrom glob import glob\n\nfrom tensorflow.keras import backend as K\nimport random\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.applications.efficientnet import *","f6374005":"image_dir = Path('..\/input\/flowers-recognition\/flowers')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.jpg'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)","f3e87f26":"image_df.head(5)","f6103159":"image_df.shape","25cbf0ca":"# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head(5)","2d66c63c":"# Display 20 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","ee45534c":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.9, shuffle=True, random_state=1)","fcedd9a8":"train_df.shape","3fa9fcfe":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","fd618f30":"train_df.head(5)","da869f67":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","48d5840b":"def create_model(input_shape=(224, 224, 3)):\n    \n    inputs = Input(input_shape)\n    base_model = EfficientNetB1(input_shape=input_shape, include_top=False, classes=5)\n    \n    x = base_model(inputs)\n    \n    x = GlobalAveragePooling2D()(x)\n#     x = Dropout(0.1)(x)\n    \n    x = Dense(56, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    outputs = Dense(5, activation='sigmoid')(x)\n    \n    model = Model(inputs, outputs)\n    \n    return model","e14adb0f":"K.clear_session()\nmetrics = [\n    'accuracy',\n    'AUC'\n]\n\n# model = create_model((224, 224, 3))\n# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=metrics)\n# model.save('models\/checkpoint\/EfficientNetB0.h5')\n\n\nmodel = load_model('..\/input\/trained-model\/EfficientNetB0.h5')\n","7eb9e45e":"checkpoint_path = 'models\/EfficientNetB1\/model_224.h5'\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', mode='min', patience=15, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n    ModelCheckpoint(monitor='val_loss', mode='min', filepath=checkpoint_path, verbose=1, save_best_only=True, save_weights_only=False)\n]","acba70fc":"history = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=30,\n    callbacks=callbacks\n)","5cf95eb3":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","c3243dff":"results = model.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","1397831c":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","45183391":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","3fc56757":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","af0ff1c5":"# Display 15 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","14ac87f8":"image =io.imread('https:\/\/www.lovingly.com\/wp-content\/uploads\/2019\/09\/red-rose-on-black-background.jpg',as_gray=True)\n\ndef detect_blur_fft(image, size=60, thresh=10):\n    (h,w) = image.shape\n    (cX, cY) = (int(w \/ 2.0), int(h \/ 2.0))\n    fft = np.fft.fft2(image)\n    fftShift = np.fft.fftshift(fft)\n    fftShift[cY - size:cY + size, cX - size:cX + size] = 0\n    fftShift = np.fft.ifftshift(fftShift)\n    recon = np.fft.ifft2(fftShift)\n    magnitude = 20 * np.log(np.abs(recon))\n    mean = np.mean(magnitude)\n    if mean <= thresh:\n        return \"Blurry\"\n    else:\n        return \"Not Blurry\"\n\ndetect_blur_fft(image,60,10)","fc59d89c":"if detect_blur_fft(image) == 'Blurry':\n    im = io.imread('https:\/\/www.lovingly.com\/wp-content\/uploads\/2019\/09\/red-rose-on-black-background.jpg')[::5, ::5, 0]\n    \n    Nz, Nx = im.shape\n\n\n    nh = [15, 25]\n    hz = np.exp(-0.1*np.linspace(-(nh[0]\/\/2), nh[0]\/\/2, nh[0])**2)\n    hx = np.exp(-0.03*np.linspace(-(nh[1]\/\/2), nh[1]\/\/2, nh[1])**2)\n    hz \/= np.trapz(hz) \n    hx \/= np.trapz(hx) \n    h = hz[:, np.newaxis] * hx[np.newaxis, :]\n\n\n    Cop = pylops.signalprocessing.Convolve2D(Nz * Nx, h=h,\n                                             offset=(nh[0] \/\/ 2,\n                                                     nh[1] \/\/ 2),\n                                             dims=(Nz, Nx), dtype='float32')\n\n","e1c5ad9f":"imblur = Cop * im.flatten()\n\nimdeblur = \\\n    pylops.optimization.leastsquares.NormalEquationsInversion(Cop, None,\n                                                              imblur,\n                                                              maxiter=50)\n\nWop = pylops.signalprocessing.DWT2D((Nz, Nx), wavelet='haar', level=3)\nDop = [pylops.FirstDerivative(Nz * Nx, dims=(Nz, Nx), dir=0, edge=False),\n       pylops.FirstDerivative(Nz * Nx, dims=(Nz, Nx), dir=1, edge=False)]\nDWop = Dop + [Wop, ]\n\n\n\n# Reshape images\nimblur = imblur.reshape((Nz, Nx))\nimdeblur = imdeblur.reshape((Nz, Nx))\n\nfig = plt.figure()\nfig.suptitle('Deblurring', fontsize=14, fontweight='bold', y=0.97 ,x =0.5)\nplt.title(\"Blurred and Deblurred image\")\nax = plt.subplot(121)\nax.set_title(\"Input Image\")\nplt.imshow(imblur)\n\nax = plt.subplot(122)\nax.set_title(\"Deburred Image\")\nplt.imshow(imdeblur)\nplt.subplots_adjust(top=0.8)","9a263159":"plt.imshow(imdeblur)\nplt.savefig('deblured.jpg')","117a9623":"img_path = '.\/deblured.jpg'\nfilepaths = pd.Series([img_path], name='Filepath').astype(str)\nlabels = pd.Series(['rose'], name='Label')\n\n# Concatenate filepaths and labels\ndf = pd.concat([filepaths, labels], axis=1)\n","b9f12147":"test_image = test_generator.flow_from_dataframe(\n    dataframe=df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)\n\n\npredictions = model.predict(test_image)\n\npred = np.argmax(predictions)\n\n# Map the label\nlabels = test_image.class_indices\nlabels = dict((v,k) for k,v in labels.items())\n\nprint(labels[0])\n","9fddf454":"## Training and Testing Data","f45e63ca":"## Importing Libraries","f9f1d514":"## Testing the model","236cad8f":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","e2805643":"# Testing Our Model with a Random Image from Internet\n\n## Blur Detection","e238c4db":"## Deblurring ","d71c6b40":"## Image Recognition","2f61739f":"## A Few Images from Our Dataset","1fac26f1":"## Setting path and other variables","686b7f21":"## Installing Libraries","2d3af5ab":"## Accuracy Graph of our Model","60455b5a":"# Flower Identification Model","004f2489":"## Creating the testing model"}}