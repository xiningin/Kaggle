{"cell_type":{"37a6536a":"code","74472adf":"code","23817fd9":"code","a8ec6261":"code","e10d6d8b":"code","2d32630e":"code","3ea393c6":"code","59249e24":"code","e98a6d1f":"code","15a88f69":"code","8a1247d9":"code","96733246":"code","08768807":"code","b6c67097":"code","349b1dbc":"code","1b7b0b2e":"code","07cc15a7":"code","3c35d9cd":"code","25660307":"code","f8795e83":"code","93034131":"code","347c5aa3":"code","9e9874af":"code","b75816e2":"code","40203b07":"code","9b354b58":"code","3e00195b":"code","d5c87f94":"code","39e73cb7":"code","3d6e2266":"code","4d94094f":"code","9f5aa319":"code","50db5603":"code","ad5e72ca":"code","49bdc8fd":"code","89c406ff":"code","7dd7eb8e":"code","88bb57a9":"code","c1be0feb":"code","9b67ac55":"code","d1b7e816":"code","6b93dca8":"code","de54a5f0":"code","03b23f3e":"code","9a2fe46b":"code","e19c3612":"code","f258848e":"code","837e317c":"code","ff836742":"code","18ec9000":"code","e8bcf16d":"code","1312f4d1":"code","9269573c":"code","141b23cf":"code","1fdae511":"code","81b8d9c5":"code","14706143":"code","f4b74356":"code","f31ac385":"code","a639984e":"code","8e573738":"code","9a18e8d6":"code","c22a01e2":"code","09c9801d":"code","fb2fe328":"code","ebafab41":"code","76ef5a61":"code","5f58b198":"code","7cc69c97":"code","3ad9844b":"code","c01287d6":"code","fa001301":"code","42bc58e4":"code","6f839ccf":"code","08cd5410":"code","deb70fcc":"code","e12662b7":"code","7e19a3da":"code","d57e0602":"code","62a73525":"code","3d057659":"code","70af6e99":"code","79637621":"code","1513f89b":"code","f398acda":"code","d2e81dfe":"code","3e33a96d":"code","64f5fb8d":"code","ceeddc8a":"code","4f5008a3":"code","973318fd":"code","34c03f89":"code","71f30088":"code","5b5242db":"code","f5915d76":"markdown","600d7f7e":"markdown","14af3363":"markdown","96dfde7f":"markdown","508c1543":"markdown","2bd30212":"markdown","61e3728c":"markdown","3eed89d4":"markdown","4c3bb8ef":"markdown","ab499146":"markdown","c559b541":"markdown","c6e065bc":"markdown","cf14ecb1":"markdown","5d4063cf":"markdown","0f421785":"markdown","c72b1448":"markdown","a5076397":"markdown","5c1d608d":"markdown","61e82cfa":"markdown","761f78ce":"markdown","276c6d5a":"markdown","67096117":"markdown","bc6fc146":"markdown","86c6eceb":"markdown","c69941c8":"markdown","2ffa2c3e":"markdown","d1650a09":"markdown","7e4b0eeb":"markdown","62af3ae6":"markdown","f40e7cb8":"markdown","099f7708":"markdown","19a10c13":"markdown","f1a7ebe9":"markdown","0cebfcf4":"markdown","468334d5":"markdown","2312ba77":"markdown","8f4dfed2":"markdown","fe105f5c":"markdown","511a8f36":"markdown","81b8b760":"markdown","b9b19050":"markdown","d7c73e7b":"markdown","dfd239df":"markdown","9d37df8e":"markdown","91d6406e":"markdown","1c25ca72":"markdown","8e36906a":"markdown","6c6f0026":"markdown","2333433b":"markdown","128fc0ef":"markdown","6d34d774":"markdown","9f44e09c":"markdown","c5d93546":"markdown","c3ebf898":"markdown","af22527b":"markdown","deb28014":"markdown","b9f5d55e":"markdown","04c824c1":"markdown","528597e1":"markdown","9eba6d49":"markdown","6c10bae3":"markdown","bb581b72":"markdown","2e7c0e25":"markdown","bda85d8e":"markdown"},"source":{"37a6536a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","74472adf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\n# for seaborn issue:\nimport warnings\nwarnings.filterwarnings(\"ignore\")","23817fd9":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a8ec6261":"train.head()","e10d6d8b":"train.shape","2d32630e":"train.dtypes","3ea393c6":"train.describe()","59249e24":"train.describe(include=['O'])","e98a6d1f":"train.info()","15a88f69":"train.isnull().sum()","8a1247d9":"test.shape","96733246":"test.head()","08768807":"test.describe()","b6c67097":"test.describe(include=['O'])","349b1dbc":"test.info()","1b7b0b2e":"test.isnull().sum()","07cc15a7":"survived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n\nprint ('Survived: %i (%.1f%%)' %(len(survived), float(len(survived))\/len(train)*100.0))\nprint ('Not Survived: %i (%.1f%%)' %(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint('Total: %i' %(len(train)))","3c35d9cd":"train[['Pclass', 'Survived']].groupby(['Pclass'],as_index=False).mean()","25660307":"sns.barplot(x='Pclass',y='Survived', data=train, ci=None)","f8795e83":"train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean()","93034131":"sns.barplot(x='Sex', y='Survived',data=train, ci=None)","347c5aa3":"tab = pd.crosstab(train['Pclass'], train['Sex'])\nprint(tab)\n\ntab.div(tab.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')","9e9874af":"sns.factorplot('Sex','Survived',hue='Pclass', size=4, aspect=2, data=train)","b75816e2":"train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean()","40203b07":"sns.barplot(x='Embarked',y='Survived',data=train, ci=None)","9b354b58":"sns.factorplot(x='Pclass', y='Survived',hue='Sex',col='Embarked',data=train)","3e00195b":"train[['Parch','Survived']].groupby(['Parch'], as_index=False).mean()","d5c87f94":"sns.barplot(x='Parch',y='Survived',data=train, ci=None)","39e73cb7":"train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()","3d6e2266":"sns.barplot(x='SibSp',y='Survived',data=train, ci=None)","4d94094f":"fig=plt.figure(figsize=(8,11))\nax1=fig.add_subplot(131)\nax2=fig.add_subplot(132)\nax3=fig.add_subplot(133)\n\nsns.violinplot(x='Embarked', y='Age', hue='Survived', data=train, ax=ax1, split=True)\nsns.violinplot(x='Pclass', y='Age', hue='Survived', data=train, ax=ax2, split=True)\nsns.violinplot(x='Sex', y='Age', hue='Survived', data=train, ax=ax3, split=True)","9f5aa319":"total_survived = train[train['Survived']==1]\ntotal_not_survived =  train[train['Survived']==0]\nplt.figure(figsize=(12,12))\nplt.subplot(111)\nsns.distplot(total_survived['Age'].dropna().values, bins=range(0,81,1), kde=False, color='blue')\nsns.distplot(total_not_survived['Age'].dropna().values, bins=range(0,81,1), kde=False, color='red', axlabel='Age')","50db5603":"male_survived = train[(train['Survived']==1) & (train['Sex']=='male')]\nfemale_survived = train[(train['Survived']==1) & (train['Sex']=='female')]\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']=='male')]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']=='female')]\n\nplt.figure(figsize=(15,15))\nplt.subplot(121)\nsns.distplot(female_survived['Age'].dropna().values, bins=range(0,81,1),kde=False, color='blue')\nsns.distplot(female_not_survived['Age'].dropna().values, bins=range(0,81,1),kde=False, color='red', axlabel='Female Age')\n\nplt.subplot(122)\nsns.distplot(male_survived['Age'].dropna().values, bins=range(0,81,1),kde=False, color='blue')\nsns.distplot(male_not_survived['Age'].dropna().values, bins=range(0,81,1),kde=False, color='red', axlabel='Female Age')","ad5e72ca":"plt.figure(figsize=(15,15))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.6, square=True, annot=True)","49bdc8fd":"train_test_data=[train, test] # combining train and test set","89c406ff":"# extract title from Name column\nfor dataset in train_test_data:\n    dataset['Title']=dataset.Name.str.extract(' ([A-Za-z]+)\\.')","7dd7eb8e":"for dataset in train_test_data:\n    dataset['Title']=dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","88bb57a9":"## Distribution Age by Title (Master, Miss, Mr, Mrs, Other)\ndist_age_title=train['Age'].hist(by=train['Title'], bins=np.arange(0,81,1))","c1be0feb":"# max Age passengers with Title Master (training)\ntrain[train['Title']=='Master']['Age'].max()","9b67ac55":"# max Age passengers with Title Master (testing)\ntest[test['Title']=='Master']['Age'].max()","d1b7e816":"def get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex\n    \nfor dataset in train_test_data:\n    dataset['Person']=dataset[['Age','Sex']].apply(get_person, axis=1)","6b93dca8":"# Impute Person column for missing value in Age\nfor dataset in train_test_data:\n    dataset['Person'][ np.isnan(dataset['Age']) & (dataset['Title']=='Master') ] = 'child'\n    dataset['Person'][ np.isnan(dataset['Age']) & (dataset['Title']!='Master') ] = dataset['Sex']","de54a5f0":"# convert Title into numerical value\n\ntitle_mapping={\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"Master\":4,\"Other\":5}\nfor dataset in train_test_data:\n    dataset['Title']=dataset['Title'].map(title_mapping)\n    dataset[\"Title\"]=dataset[\"Title\"].fillna(0)","03b23f3e":"# convert Person to numerical value\n\nfor dataset in train_test_data:\n    dataset['Person']=dataset['Person'].map({'child':0,\"female\":1,\"male\":2}).astype(int)","9a2fe46b":"train[['Person', 'Survived']].groupby(['Person'], as_index=False).mean()","e19c3612":"# convert Sex to numerical value\nfor dataset in train_test_data:\n    dataset['Sex']=dataset['Sex'].map({'female':0,\"male\":1}).astype(int)","f258848e":"train.Embarked.value_counts()","837e317c":"# Impute the missing value by the mode value which is 'S'\nfor dataset in train_test_data:\n    dataset['Embarked']=dataset['Embarked'].fillna('S')","ff836742":"# convert Embarked to numerical value\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","18ec9000":"# impute the missing value with random integer between (mean +\/- std)\nfor dataset in train_test_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['AgeBand'] = pd.cut(train['Age'], 5)","e8bcf16d":"# map the ageBand\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","1312f4d1":"# impute the missing value with median\nfor dataset in train_test_data:\n    dataset['Fare']=dataset['Fare'].fillna(dataset['Fare'].median())","9269573c":"# create FareBand divided into 4 parts\ntrain['FareBand']=pd.qcut(train['Fare'],4)\nprint(train[['FareBand','Survived']].groupby(['FareBand'],as_index=False).mean())","141b23cf":"# Map the Fare according to FareBand\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)","1fdae511":"train.head()","81b8d9c5":"# Create New Feature named FamilySize by combining SibSp & Parch\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1\n\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","14706143":"# Create Feature named IsAlone\nfor dataset in train_test_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","f4b74356":"cols = ['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Title','Person','FamilySize','IsAlone']\ng = sns.pairplot(data=train, vars=cols, size=1.5,\n                 hue='Survived', palette=['red',\"blue\"])\ng.set()","f31ac385":"fig=plt.figure(figsize=(15,15))\nsns.heatmap(train.corr(), vmax=0.6, square=True, annot=True)","a639984e":"# drop unnecessary features\nfeatures_drop=['Name','Age','SibSp','Parch','Ticket','Cabin','FamilySize','Sex','Title']\ntrain=train.drop(features_drop,axis=1)\ntrain=train.drop(['PassengerId','AgeBand','FareBand'],axis=1)\ntest=test.drop(features_drop,axis=1)","8e573738":"train.head()","9a18e8d6":"test.head()","c22a01e2":"# defining the training and testing dataset\nX_train = train.drop('Survived',axis=1)\ny_train = train['Survived']\nX_test=test.drop('PassengerId',axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","09c9801d":"# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score","fb2fe328":"clf_log_reg = LogisticRegression()\nclf_log_reg.fit(X_train, y_train)\nacc_log_reg=round((cross_val_score(clf_log_reg,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_log_reg)+\"%\")","ebafab41":"clf_svc=SVC() # Support Vector Classification\nclf_svc.fit(X_train, y_train)\nacc_svc=round((cross_val_score(clf_svc,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_svc)+\"%\")","76ef5a61":"clf_linear_svc = LinearSVC() # Linear Support Vector Classification\nclf_linear_svc.fit(X_train, y_train)\nacc_linear_svc=round((cross_val_score(clf_linear_svc,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_linear_svc)+\"%\")","5f58b198":"clf_knn=KNeighborsClassifier(n_neighbors=3)\nclf_knn.fit(X_train, y_train)\nacc_knn=round((cross_val_score(clf_knn,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_knn)+\"%\")","7cc69c97":"clf_dt = DecisionTreeClassifier()\nclf_dt.fit(X_train, y_train)\nacc_decision_tree=round(clf_dt.score(X_train, y_train)*100,2)\nacc_decision_tree=round((cross_val_score(clf_dt,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_decision_tree)+\"%\")","3ad9844b":"clf_rf = RandomForestClassifier(n_estimators=100)\nclf_rf.fit(X_train, y_train)\nacc_random_forest=round((cross_val_score(clf_rf,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_random_forest)+\"%\")\n#clf_rf.score(X_train, y_train)","c01287d6":"y_train.head()","fa001301":"X_train.head()","42bc58e4":"clf_ext_rf = ExtraTreesClassifier(\n    max_features='auto',\n    bootstrap=True,\n    oob_score=True,\n    n_estimators=1000,\n    max_depth=None,\n    min_samples_split=10\n    #class_weight=\"balanced\",\n    #min_weight_fraction_leaf=0.02\n    )\nclf_ext_rf.fit(X_train,y_train)\nacc_ext_rf=round(clf_ext_rf.score(X_train, y_train)*100,2)\nacc_ext_rf=round((cross_val_score(clf_ext_rf,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_ext_rf)+\"%\")","6f839ccf":"clf_bagging = BaggingClassifier(\n    KNeighborsClassifier(\n        n_neighbors=3,\n        weights='distance'\n        ),\n    oob_score=True,\n    max_samples=0.5,\n    max_features=1.0\n    )\nclf_bagging.fit(X_train,y_train)\nacc_bagging=round((cross_val_score(clf_bagging,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_bagging)+\"%\")","08cd5410":"clf_gnb=GaussianNB()\nclf_gnb.fit(X_train, y_train)\nacc_gnb=round((cross_val_score(clf_gnb,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_gnb)+\"%\")","deb70fcc":"clf_perceptron=Perceptron(max_iter=5, tol=None)\nclf_perceptron.fit(X_train, y_train)\nacc_perceptron=round((cross_val_score(clf_perceptron,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_perceptron)+\"%\")","e12662b7":"clf_sgd = SGDClassifier(max_iter=5, tol=None)\nclf_sgd.fit(X_train, y_train)\nacc_sgd=round((cross_val_score(clf_sgd,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint (str(acc_sgd)+\"%\")","7e19a3da":"#import warnings\n#warnings.filterwarnings(\"ignore\")\n\nclf_gb = GradientBoostingClassifier(\n            #loss='exponential',\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.5,\n            random_state=0)\nclf_gb.fit(X_train,y_train)\nacc_gb=round((cross_val_score(clf_gb,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint (str(acc_gb)+\"%\")","d57e0602":"clf_xgb = xgb.XGBClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_xgb.fit(X_train,y_train)\nacc_xgb=round((cross_val_score(clf_xgb,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint (str(acc_xgb)+\"%\")","62a73525":"clf_lgb = lgb.LGBMClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_lgb.fit(X_train,y_train)\nacc_lgb=round((cross_val_score(clf_lgb,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint (str(acc_lgb)+\"%\")","3d057659":"clf_ada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nclf_ada.fit(X_train,y_train)\nacc_ada=round((cross_val_score(clf_ada,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint (str(acc_ada)+\"%\")","70af6e99":"clf_vote=VotingClassifier(\nestimators=[\n    #('tree',clf_dt),\n    #('knn',clf_knn),\n    ('svm',clf_svc),\n    #('extra',clf_ext_rf),\n   #('gb',clf_gb),\n    ('xgb',clf_xgb),\n    ('ada',clf_ada),\n    #('bagging',clf_bagging),\n    #('percep',clf_perceptron),\n    #('logistic',clf_log_reg),\n    ('lightgbm', clf_lgb),\n    ('RF',clf_rf)\n],\nweights=[3,2,1,3,3],\nvoting='hard')\n\nclf_vote.fit(X_train,y_train)\nacc_vote=cross_val_score(clf_vote,X_train, y_train, cv=5,scoring='accuracy')\nprint(\"Voting Accuracy: {:.2%}\".format(acc_vote.mean())+\" (+\/-{:.2%})\".format(acc_vote.std()))","79637621":"models=pd.DataFrame({'Model':['Logistic Regression', \"Support Vector Machine\",'Linear SVM','KNN','Decision Tree','Random Forest','Extremely Randomised Trees','Bagging','Naive Bayes','Perceptron','Stochastic Gradient Descent'\n                             ,'Gradient Boosting','XGBoost','LightGBM','Ada Boosting','Stacking'],\n                     'Score':[acc_log_reg, acc_svc,acc_linear_svc,acc_knn,acc_decision_tree,acc_random_forest, acc_ext_rf ,acc_bagging,acc_gnb, acc_perceptron,acc_sgd,acc_gb,acc_xgb,acc_lgb,acc_ada,(acc_vote.mean()*100)]})\nmodels.sort_values(by='Score',ascending=False)","1513f89b":"# As the example using the Random Forest Classifier\nfrom sklearn.metrics import confusion_matrix\nimport itertools","f398acda":"# We use Stacking Method as an example because the classifier provides good accuracy and stability\nclf=VotingClassifier(\nestimators=[\n    #('tree',clf_dt),\n    #('knn',clf_knn),\n    ('svm',clf_svc),\n    #('extra',clf_ext_rf),\n   #('gb',clf_gb),\n    ('xgb',clf_xgb),\n    ('ada',clf_ada),\n    #('bagging',clf_bagging),\n    #('percep',clf_perceptron),\n    #('logistic',clf_log_reg),\n    ('lightgbm', clf_lgb),\n    ('RF',clf_rf)\n],\nweights=[3,2,1,3,3],\nvoting='hard')\nclf.fit(X_train, y_train)\ny_pred_vote_training_set = clf.predict(X_train)\nacc_vote_training_set=round((cross_val_score(clf,X_train, y_train, cv=5,scoring='accuracy').mean())*100,2)\nprint(str(acc_vote_training_set)+\"%\")","d2e81dfe":"# Confusion matrix in number\ntrue_class_names = ['True Survived','True Not Survived']\npred_class_names = ['Predicted Survived','Predicted Not Survived']\ncnf_matrix = confusion_matrix(y_train, y_pred_vote_training_set)\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, index=true_class_names, columns=pred_class_names)\ndf_cnf_matrix","3e33a96d":"# Confusion matrix in percentage\ntrue_class_names = ['True Survived','True Not Survived']\npred_class_names = ['Predicted Survived','Predicted Not Survived']\ncnf_matrix_percent = cnf_matrix.astype('float')\/cnf_matrix.sum(axis=1)[:,np.newaxis]\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, index=true_class_names, columns=pred_class_names)\ndf_cnf_matrix_percent","64f5fb8d":"plt.figure(figsize = (10,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)","ceeddc8a":"# Check the contribution of each features\n\nsummary_df=pd.DataFrame(list(zip(X_test.columns,\n                      #clf_log_reg.feature_importances_,\n                      #clf_svc.feature_importances_,\n                      #clf_linear_svc.feature_importances_,\n                      #clf_knn.feature_importances_,\n                      clf_dt.feature_importances_,\n                     clf_rf.feature_importances_,\n                     clf_ext_rf.feature_importances_,\n                     #clf_bagging.feature_importances_,\n                     #clf_gnb.feature_importances_,\n                     #clf_perceptron.feature_importances_,\n                     #clf_sgd.feature_importances_,\n                     clf_gb.feature_importances_,\n                     clf_xgb.feature_importances_,\n                     clf_lgb.feature_importances_,\n                                 #clf_vote.feature_importances_,\n                     clf_ada.feature_importances_\n                     )),\n             columns=['Feature','Tree','RF','Ext RF','GBoost','XGBoost','lightGB','AdaBoost'])\nsummary_df['Median']=summary_df.median(1)\nsummary_df.sort_values('Median',ascending=False)","4f5008a3":"test.head()","973318fd":"y_pred_vote=clf_vote.predict(X_test)","34c03f89":"submission = pd.DataFrame({'PassengerId':test['PassengerId'],\n                           'Survived':y_pred_vote})","71f30088":"submission.head()","5b5242db":"submission.to_csv('submission_output.csv', index=False)","f5915d76":"## Relationship between Features","600d7f7e":"# Model Classification","14af3363":"> Females have better survival chance","96dfde7f":"## Embarked","508c1543":"### LightGBM","2bd30212":"## Classification & Accuracy","61e3728c":"# Loading Dataset","3eed89d4":"### Gaussian Naive Bayes","4c3bb8ef":"### Gradient Boosting","ab499146":"## Pclass & Sex vs Survival","c559b541":"# Checking the Dataset Available","c6e065bc":"## Age","cf14ecb1":"## Parch vs Survival","5d4063cf":"### XGBoost","0f421785":"> travelling alone has only 30% survival chance","c72b1448":"# Exploratory Data Analysis (EDA)","a5076397":"## Sex vs Survival","5c1d608d":"It will be better to simplify based on the Title and create new feature named 'Person' containing categorical value:\n- *Child* (passenger with Age less than 16) - value:0\n- *Female* (passenger with Age greater than to 16 and Sex='Female') - value: 1\n- *Male* (passenger with Age greater than to 16 and Sex='Male') - value: 2","61e82cfa":"# Loading Module","761f78ce":"### Logistic Regression","276c6d5a":"## Correlating Features","67096117":"> The features that are strongly correlated with 'Survived' so far are:\n- Pclass\n- Fare\n\n> But at the same time those features (Pclass and Fare) are having correlation with:\n- Age\n- Parch\n- SibSp","bc6fc146":"### Perceptron","86c6eceb":"## Age vs Survival","c69941c8":"> Decided to use *Stacking Methods* Classifier as it provides high accuracy and more stability","2ffa2c3e":"## Embarked vs Survival","d1650a09":"> - There are more males among the 3rd Pclass\n> - Women from 1st and 2nd Pclass have almost 100% survival chance\n> - Male from 2nd and 3rd Pclass have only around 10% survival chance","7e4b0eeb":"> Passengers embarked from C (Cherbourg) have better survival chance than other ambarkation places (Southampton and Queenstown)","62af3ae6":"### Linear SVM","f40e7cb8":"# General Data Exploration","099f7708":"## SibSp vs Survival","19a10c13":"## FamilySize vs Survival","f1a7ebe9":"## Features Extraction","0cebfcf4":"# Model Validation","468334d5":"### Bagging (with estimator KNN)","2312ba77":"> Several classifier algorithms used as following:\n- Logistic Regression\n- Support Vector Machine (SVM)\n- Linear SVM\n- K-Nearest Neigbor (KNN)\n- Decision Tree\n- Random Forest\n- Naive Bayes (Gaussian NB)\n- Perceptron\n- Stochastic Gradient Descent (SGD)","8f4dfed2":"> from above **Pclass** violinplot:\n- 1st Pclass has very few children as compared to other two classes\n- 1st Pclass has more old people as compared to other two classes\n- Almost all children (between age 0 to 10) of 2nd Pclass survived\n- Most children of 3rd Pclass survived\n- Younger people of 1st Pclass survived as compared to its older people","fe105f5c":"### Support Vector Machine (SVM)","511a8f36":"## Pclass vs Survival","81b8b760":"## IsAlone","b9b19050":"> - almost all females from Pclass 1 and 2 survived\n> - Female dying were mostly from 3rd Pclass\n> - Males from Pclass 1 only have slightly higher survival chance than 2nd and 3rd class","d7c73e7b":"## Fare","dfd239df":"## Title","9d37df8e":"### Stochastic Gradient Descent (SGD)","91d6406e":"> - Having FamilySize upto 4 (from 2 to 4) has better survival chance\n> - FamilySize = 1, i.e. travelling alone has less survival chance\n> - Large FamilySize (size of 5 and above) also have less survival chance","1c25ca72":"## Explore Testing Dataset","8e36906a":"> - Combining both male and female , we can see that children with age between 0 to 5 have better survival chance\n> - Females with age \"18 to 40\" and \"50 and above\" have higher survival chance\n> - Males with age between 0 to 14 have better chance of survival","6c6f0026":"### K-Nearest Neighbors","2333433b":"### Ada Boosting","128fc0ef":"> Training and Testing Procedurs:<br>\n1. First, we train these classifiers with our training dataset\n2. Using trained classifier, predict the survival outcome of test data\n3. Calculate the accuracy score (in %) of trained classifier","6d34d774":">from above **Sex** violinplot:\n- Most male children (age 0 to 14) survived\n- Female with age between 18 to 40 have better survival chance","9f44e09c":"> Higher Pclass have better survival chance","c5d93546":"## Sex","c3ebf898":"## Pclass, Sex & Embarked vs Survival","af22527b":"### Decision Tree","deb28014":"# Submission File Output","b9f5d55e":"## Comparing Model","04c824c1":"# Feature Engineering (Feature Selection)","528597e1":"### Stacking Method (Voting Mechanism Principal)","9eba6d49":"> The features that are strongly correlated with 'Survived' so far are:\n- Pclass\n- Fare\n\n> But at the same time those features (Pclass and Fare) are having correlation with:\n- Age\n- Parch\n- SibSp","6c10bae3":"### Random Forest","bb581b72":"## Explore Training Dataset","2e7c0e25":"### Extremely Randomised Trees","bda85d8e":"## Confusion Matrix"}}