{"cell_type":{"7a202eb1":"code","ebde558e":"code","ef108b48":"code","4415e98f":"code","5a97e36f":"code","9cf297c8":"code","e150813c":"code","d08d8caa":"code","b8b0a33b":"code","93f9a0a7":"code","aca89f90":"code","4d78fa25":"code","c8988b90":"code","d66be289":"code","f7211782":"code","192a4ac8":"code","0c4988b2":"code","3888ddfa":"code","a6029490":"code","50196d34":"code","93cd3514":"code","2eb1240b":"code","77736d0c":"code","433c7cb7":"code","8dc3aaba":"code","938a2601":"code","47548b07":"code","9b900e39":"code","f835f4f6":"code","32aa95e1":"code","f2582231":"code","624846e4":"code","21b29efa":"code","df67194d":"code","262dcc42":"code","e57cbf80":"code","b54c429d":"code","9aedb959":"code","159edd33":"code","18d2726c":"code","19184e6a":"code","43e9af57":"code","7c1107db":"markdown","2e9a51d8":"markdown","52fca311":"markdown","29299384":"markdown","833de6e6":"markdown","6444281d":"markdown","8b14d927":"markdown","abff2fe1":"markdown","0ac5f519":"markdown","d2acb7e6":"markdown","819b53c1":"markdown","abc9f2ce":"markdown","1e078fe7":"markdown","5554c4e5":"markdown","c674f5f6":"markdown","19d6661c":"markdown"},"source":{"7a202eb1":"# Warnings & Display options\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n\n# Load the Data\n# We work on a sample to prevent some algorithms from taking too much time. \npath = '..\/input\/mse-pysupport-hackathon\/simulation.csv'\ndf = pd.read_csv(path).sample(frac = 0.1)","ebde558e":"# We may work with log SalePrice as that is much less skewed and helps in interpretation\ndf['TSalePrice'] = np.log1p(df['SalePrice']) \n\n# Key Variables\ndf['PastMV'] = df['EstimateLand'] + df['EstimateBuilding']\ndf['BuildingArea'] = df['BuildingSquareFeet']\ndf['BedroomShare'] = df['Bedrooms']\/df['Rooms']\ndf['SaleYear'] = df['SaleDate'].str[-4:]\n\n# (Interior) Furnishing Quality - a simple score that is higher if the house has high quality interiors\ndf['FurnishingQuality'] = 0\ndf.loc[df.ConstructionQuality == 'Deluxe', 'FurnishingQuality'] += 1\ndf.loc[df.BasementFinish == 'FormalRecRoom', 'FurnishingQuality'] += 1\ndf.loc[df.CentralHeating == 'HotWaterStream', 'FurnishingQuality'] += 1\ndf.loc[df.AtticFinish == 'LivingArea', 'FurnishingQuality'] += 1\ndf.loc[df.WallMaterial.isin(['Stucco', 'Masonry']), 'FurnishingQuality'] += 1\ndf.loc[df.RoofMaterial.isin(['Shake', 'Slate', 'Tile']), 'FurnishingQuality'] += 1\ndf.loc[df.DesignPlan == 'ArchitectPlan', 'FurnishingQuality'] += 1\ndf.loc[df.RepairCondition == 'AboveAvg', 'FurnishingQuality'] += 1\ndf.loc[df.Garage1Material.isin(['Stucco', 'Masonry']) |\n       df.Garage2Material.isin(['Stucco', 'Masonry']), 'FurnishingQuality'] += 1\n\n# (Interior) Furshining Quantity - a score that captures if the house has a porch, garage, basement, etc.\ndf['Furnishing'] = 0\ndf.loc[df.GarageIndicator == 1.0, 'Furnishing'] += 1\ndf.loc[df.Porch != 'None', 'Furnishing'] += 1\ndf.loc[df.CathedralCeiling != 'None', 'Furnishing'] += 1\ndf.loc[df.AtticType != 'None', 'Furnishing'] += 1\ndf.loc[df.Fireplaces != 0.0, 'Furnishing'] += 1\ndf.loc[df.Basement.isin(['Full', 'Partial']), 'Furnishing'] += 1","ef108b48":"# Both Seem evenly distributed\ndf.FurnishingQuality.value_counts()","4415e98f":"df.Furnishing.value_counts()","5a97e36f":"# Log transforming SalePrice reveals a bimodal distribution. \n# Some values below k, where log(k) = 5 are too low to be considered. \nimport seaborn as sns\nimport numpy as np\nsns.distplot(df.TSalePrice)","9cf297c8":"# Arms Length Transactions are sales made between family or known members.\n# They are more erratic. \ndf[['SalePrice', 'ArmsLengthTransaction']].groupby('ArmsLengthTransaction').describe()","e150813c":"# Also remove Condos and Apartments. \n# Condos are close knit shared community living (their price depends on % ownership, and the data for them is lesser).\n# Apartments can house multiple families in them (the rooms in them are much larger).\n# We retain only Single Family Houses for simplicity. \ndf2 = df[(df.TSalePrice > 5) & (df.ArmsLengthTransaction == 1) & \n       (~df.PropertyDesc.isin(['Residential condominium', \n                               'Apartment building with 2 to 6 units, any age', \n                               'Mixed-use commercial\/residential building with apartment and commercial area totaling 6 units or less with a square foot area less than 20,000 square feet, any age']))]\n\n\n#df2 = df2[(df2.TSalePrice < df2.TSalePrice.quantile(.9)) | (df2.TSalePrice > df2.TSalePrice.quantile(.1))]","d08d8caa":"# Optionally you way want to truncate the data to remove all extreme values based on price\n#df2 = df2[(df2.TSalePrice < df2.TSalePrice.quantile(.9)) | (df2.TSalePrice > df2.TSalePrice.quantile(.1))]","b8b0a33b":"# Attributes of Interest - a small set to keep things simple\ncols = ['Age', 'BuildingArea', 'BedroomShare', 'Furnishing','FurnishingQuality',\n        'OHareNoise','Floodplain', 'RoadProximity', 'SaleYear', 'TownshipName', 'NeighborhoodCode']\n\n#cols = ['BuildingArea', 'Furnishing','FurnishingQuality']\n#'OtherRoomsShare','AvgRoomSize', \nx = df2[cols]\n#x = df2[list(df2.select_dtypes(exclude = 'object').columns) + ['SaleYear', 'TownshipName', 'NeighborhoodCode']].dropna()\n\n# SalePrice\ny = df2[['SalePrice']]\n\n# Filling Nulls - all these are indicator variables with \nna1 = ['OHareNoise','Floodplain', 'RoadProximity']\nx[na1] = x[na1].fillna(0)","93f9a0a7":"x.isnull().sum()","aca89f90":"# Creating dummies for the three Categorical variables\n# Keep the most common categorical value as base\ndum = [ 'OHareNoise','Floodplain', 'RoadProximity','TownshipName', 'SaleYear', 'NeighborhoodCode']\ndum_base = {}\nfor i in dum:\n    base = str(i) + '_' + str(x[i].value_counts().index[0])\n    print(str(i), ' base is ', base)\n    x[i] = pd.Categorical(x[i], x[i].value_counts().index)\n    x.sort_values(i, inplace = True)\n    for col in pd.get_dummies(x[i], drop_first = True, prefix = str(i)).columns:\n        dum_base[str(col)] = base\n    x = pd.concat([x.drop(i, axis = 1), pd.get_dummies(x[i], drop_first = True, prefix = str(i))], axis = 1, ignore_index = False)\n    \nx.sort_index(inplace = True)\ny.sort_index(inplace = True)","4d78fa25":"# Sanity Checks\n\n# Zillow claims Cook County area has a market value of 200-250k$\nprint(y.median())\n\n# Shapes\nprint(x.shape, y.shape)\n\nx.head()","c8988b90":"# We use the GLS to correct for heteroskedasticity, since SalePrice is skewed. \nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import GLS\nx = sm.add_constant(x, prepend=False)\nmod = GLS(y, x)\nres = mod.fit()\nres.summary()","d66be289":"#res2 = mod.fit_regularized()\nnumerics = 6\nfor i in range(len(res.params)-1):\n    if i < numerics: \n        var = res.params.index[i]\n        coeff = res.params.iloc[i]\n        mean = x[var].mean()\n        pvalue = res.pvalues[i]\n        if pvalue<0.05:\n            print(f'Average {var} of a house is {mean:0.2f} and a 10% (about {mean*0.1:0.2f}) increase in {var}, will change house price by {mean*0.1 * coeff:0.2f} dollars')\n    if i >= numerics:\n        var = res.params.index[i]\n        mean = x[var].mean()\n        coeff = res.params.iloc[i]\n        base = dum_base[var]\n        pvalue = res.pvalues[i]\n        if pvalue<0.01:\n            print(f'For {mean:0.4f} of houses, we have {var}. As compared to {base} the avg house price is greater by {1 * coeff:0.2f} dollars.')\n    if i == len(res.params) - 2:\n        print('constant')","f7211782":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\nmodel = CatBoostRegressor(n_estimators = 1000, depth = 8, eval_metric = 'R2', verbose = 100)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\nmodel.fit(x_train, y_train, eval_set =  (x_test, y_test), cat_features = np.where(x.dtypes == 'object')[0])\npd.DataFrame(np.c_[x_train.columns, model.feature_importances_]).sort_values(by = 1, ascending = False)","192a4ac8":"import shap\nshap.initjs()\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(x)","0c4988b2":"houseNumber = 5450\nshap.force_plot(explainer.expected_value, shap_values[houseNumber,:], x.iloc[houseNumber,:])","3888ddfa":"i = 7000\nshap.force_plot(explainer.expected_value, shap_values[i,:], x.iloc[i,:])","a6029490":"# Overall feature importance from Shap\n# Similar to catboost native feature importance\nshap.summary_plot(shap_values, x, plot_type=\"bar\")","50196d34":"# single feature across the whole dataset\nshap.summary_plot(shap_values, x)","93cd3514":"import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale, normalize\nxPCA = df2[['Age', 'BuildingArea', 'Bedrooms', 'FullBaths','BedroomShare', 'Furnishing','FurnishingQuality']]\n\n# ensure these are lowly correlated variables. \nxPCA.corr()","2eb1240b":"# Scaling is needed to prevent any feature from dominating the process\nx_scaled = normalize(xPCA, axis = 0)\n\n# We compress the data into 3 dimensions\n# tSNE can give you a nonlinear decomposition\nfrom sklearn.manifold import TSNE\n#pca = TSNE(n_components = 3)\npca = PCA(n_components = 3)\n\nx_transformed = pca.fit_transform(x_scaled)\nprint('Correl b\/w PCA vecs:', np.corrcoef(x_transformed[:, 0], x_transformed[:, 1])[1, 0])","77736d0c":"print('Variance explained by pricipal components:', pca.explained_variance_ratio_)","433c7cb7":"print('Total variance explained:', sum(pca.explained_variance_ratio_))","8dc3aaba":"# Correlation between componants and raw features\n# Unable to make out any distinctive details\nfor j in range(3):\n    for i in xPCA.columns: \n        print(f'PCA-{j} with {i}: {np.corrcoef(xPCA[i].values, x_transformed[:, j])[1, 0]}')\n    print('\\n')","938a2601":"# Scatter plots of features vs components\n# Components are uncorrelated\nimport matplotlib.pyplot as plt\nplt.scatter(x_scaled[:, 3], x_scaled[:, 4])\nplt.show()\n\nplt.scatter(x_transformed[:, 0], x_transformed[:, 1])\nplt.show()","47548b07":"xKMEANS = x_transformed.copy()\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.metrics import silhouette_samples, silhouette_score as SS\nfrom sklearn.metrics import calinski_harabasz_score as CH, davies_bouldin_score as DB\nfor i in range(2, 10):\n    clusterer = KMeans(n_clusters=i, random_state=10)\n    X = normalize(xKMEANS)\n    Y = clusterer.fit_predict(X)\n    print(i, SS(X, Y), CH(X, Y), DB(X, Y))\n","9b900e39":"# taking the best silouhetter score for clusters = 5\nimport plotly.express as px\ndfPCA = xPCA.copy()\nfrom sklearn.cluster import KMeans\nclusterer = KMeans(n_clusters=5, random_state=10, algorithm = 'full')\ncluster_labels = clusterer.fit_predict(x_transformed)","f835f4f6":"# Check the Mean of Variables accross clusters, including SalePrice\ndfPCA['clusters'] = np.array(cluster_labels)\ndfPCA['SalePrice'] = df2['TSalePrice'].values\ndfPCA.groupby('clusters').mean()","32aa95e1":"# Visualise clusters on actual features\nfrom sklearn.cluster import KMeans\nfig = px.scatter_3d(dfPCA, x='Age', y='BuildingArea', z='FurnishingQuality', size = 'SalePrice', \n                    color = 'clusters', opacity = 1)\nfig.show()","f2582231":"# Visualise clusters on PCA components\n# They are more evident here, but less interpretable.\ndfPCA_transformed = pd.DataFrame(x_transformed, columns = ['PCA0', 'PCA1', 'PCA2'])\ndfPCA_transformed['clusters'] = np.array(cluster_labels)\ndfPCA_transformed['SalePrice'] = df2['TSalePrice'].values\nfrom sklearn.cluster import KMeans\nfig = px.scatter_3d(dfPCA_transformed, x='PCA0', y='PCA1', z='PCA2', size = 'SalePrice', \n                    color = 'clusters', opacity = 1)\nfig.show()","624846e4":"xFACTOR = df2[['Age', 'BuildingArea','LandSquareFeet', 'Bedrooms', 'Rooms', 'FullBaths', 'Furnishing','FurnishingQuality']].dropna()\nxFACTOR.head()","21b29efa":"import numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.multivariate.factor import Factor, FactorResults\nX = normalize(xFACTOR.values, axis = 0)\nmod = Factor(X, \n             n_factor = 2, \n             endog_names = xFACTOR.columns, \n             method = 'pa')\nres = mod.fit()\n\n# We \"rotate\" to enable factors to be correlated with certain features\n# https:\/\/community.alteryx.com\/t5\/Data-Science\/Ghost-Hunting-Factor-Analysis-with-Python-and-Alteryx\/ba-p\/566434\nFR = FactorResults(mod)\n\n#Rotations available - varimax, quartimax, biquartimax, equamax, oblimin, parsimax, parsimony, biquartimin, promax.\nFR.rotate(method = 'oblimin')\nprint(FR.summary())","df67194d":"# The scree plot tells us how much variance is explained with more and more factors\nres.plot_scree(ncomp=4).show()","262dcc42":"# We plot the loadings, or the direction of features vis-a-vis 'factors'\n# This tells us the direction in which features move\n# age, furnishing tend to move together i.e Quality\n# area, rooms go together i.e quantity\nres.plot_loadings()","e57cbf80":"# After rotation this is more magnified\nFR.plot_loadings()","b54c429d":"FACTORS = np.dot(X, mod.loadings)\nxFACTOR['FACTOR0'] = FACTORS[:, 0] * 100\nxFACTOR['FACTOR1'] = FACTORS[:, 1] * 100\nxFACTOR.head()","9aedb959":"# Factors 0 correlates to Size variables, while factor 1 correlates to quantity. \nxFACTOR.corr()","159edd33":"xFACTOR.mean()","18d2726c":"xFACTOR['HighQuality'] = 0\nxFACTOR.loc[xFACTOR.FACTOR1 > 0.592254,  'HighQuality'] = 1\nxFACTOR['HighQuantity'] = 0\nxFACTOR.loc[xFACTOR.FACTOR0 > 2.663217,  'HighQuantity'] = 1\n\nxFACTOR['Clusters'] = 0\nxFACTOR.loc[(xFACTOR.HighQuality == 1) & (xFACTOR.HighQuantity == 0), 'Clusters'] = 1\nxFACTOR.loc[(xFACTOR.HighQuality == 0) & (xFACTOR.HighQuantity == 1), 'Clusters'] = 2\nxFACTOR.loc[(xFACTOR.HighQuality == 1) & (xFACTOR.HighQuantity == 1), 'Clusters'] = 3\n","19184e6a":"xFACTOR.shape","43e9af57":"# Manually create clusters - high and low, quantity and quality\nxFACTOR['SalePrice'] = df2['TSalePrice'].values\nfig = px.scatter(xFACTOR, x='FACTOR0', y='FACTOR1', color = 'Clusters', opacity = 1)\nfig.show()","7c1107db":"<div id='21'\/>\n\n## Dimensionality Reduction\n\n* Select a handful of features to attempt summarization on. \n* Ignore location because we cannot use dummies. ","2e9a51d8":"<div id='13'\/>\n\n# GBMs: Local Explanations with Shapley Values\n\n* A better idea is to use Shapley values. https:\/\/github.com\/slundberg\/shap\n* [Nature paper](https:\/\/www.nature.com\/articles\/s42256-019-0138-9.epdf?shared_access_token=RCYPTVkiECUmc0CccSMgXtRgN0jAjWel9jnR3ZoTv0O81kV8DqPb2VXSseRmof0Pl8YSOZy4FHz5vMc3xsxcX6uT10EzEoWo7B-nZQAHJJvBYhQJTT1LnJmpsa48nlgUWrMkThFrEIvZstjQ7Xdc5g%3D%3D)\n* [Simpler Explanation](https:\/\/christophm.github.io\/interpretable-ml-book\/shapley.html)\n* The Shapley value for feature - Building Area for house number number 5450 (ID), is to obtain multiple predictions with different feature subsets (small and large). Each time you calculate two predictions - (a) using the exact Building area for that house and (b) using the wrong Building Area (randomly chosen). Then you subtract the prediction at (a) with (b). This gives you the marginal contribution of Building Area to house number 5450, for that particular feature subset (say Age, Building Area, Township). \n* You repeat this for many subsets and average to get the shapley local contribution of building area to house number 5450. You also find the same for other features. Then you are in a position to split the total prediction for house no. 5450 into its local determinants. \n* you can aggregate again to arrive at global explanations","52fca311":"# Table of Contents\n#### [Preprocessing](#0)\n\n#### [Determinants of House Prices](#1)\n\n   - [Generalized Least Squares](#11)\n   - [GBMs: Global Features Importances](#12)\n   - [GBMs: Local Explanations with Shapley Values](#13)\n    \n    \n#### [Clustering Houses](#2)\n\n   - [Dimensionality Reduction](#21)\n   - [Clustering](#22)\n   - [Factor Analysis](#23)","29299384":"* Nearly statistically significant variables have correct sign - except OHareNoise.\n* Practically however, Size and Location turn out to be the most important. A quick search on Google reveals that they location dummies are performing correctly. Places like Jefferson and River Forest do have much more expensive homes than say, Calumet. \n","833de6e6":"* Can plot the shap value obtained for every feature for every datapoint. \n* So against building area, we see a scatter of houses. \n* when building area is Red (greater than mean), then its shap value on SalePrice is positive and fairly large. The converse is also true, but not to that extent\n* When Age is low, on the other hand, the impact on model output is high. ","6444281d":"<div id='12'\/>\n\n# GBMs: Global Features Importances\n\n* We can use features importances (calculated by avg RMSE decrease during recursive splits) from GBM models. \n* From catboost we get the similar results as above -> building area and location dummies playing the larger role in \n* But these face the problem that in the face of correlated features - only one of them is split on and so only one of them is given the importance score. And they also only give 'importance' and not direction or magnitude of impact. They also provide a global explanation and no local explanation. \n* but these are useful for feature selection at a large scale. \n","8b14d927":"<div id='2'\/>\n\n# Clustering Houses","abff2fe1":"<div id='23'\/>\n\n# Factor Analysis\n\n* PCA has this draw back that it creates components sequentially. The first component is allowed to explain as much variance. The others grab whatever is left. The focus is on data compression with orthogonal unit vectors. The focus is not on having these components represent anything. \n* Factor analysis allows us to make adjustments to the data compression so as to allow 'latent factors' instead of 'components'. Factors are supposed to have meaning and correlation with subsets of features. E.g. a students grades accross many exams can be reduced to 'effort' and 'intelligence'. \n* factor analysis is a more interpretable PCA. \n* We will try to decompose every thing into 'quantity' and 'quality'","0ac5f519":"For House no 7000, being in Thornton causes the largest reduction in its value.","d2acb7e6":"For House no 5450, building area explains the largest increase in its value. Other factors like being in Township Barrington and good furnishing quality. ","819b53c1":"<div id='11'\/>\n\n### Generalized Least Squares","abc9f2ce":"<div id='22'\/>\n\n## Clustering Algorithms\n\n* We hunt for clusters in the compressed dimensions\n* We evaluate the performance using Silouhette Coefficient which compares inter and intra cluster differences. Values close to 1 represent that clusters are distinctive. ","1e078fe7":"<div id='0'\/>\n\n# Preprocessing","5554c4e5":"Once rotated, the 'factors' are now behaving nicely. The numbers represent correlations between 'factors' and features. \n* Factor 0 correlates with building area, land area, bedrooms and rooms. It represents \"quantity\"\n* Factor 1 corrleates with Furnishing, Furnishing Quality. It represents \"quality\". ","c674f5f6":"* Cluster 0 (Blue) -> old and small houses, with lot of furnishing but of poorer quality. Lesser number of Bedrooms. Cheap. \n* Cluster 1 (Red) -> young and large houses, with lot of furnishing but of poorer quality. More number of Bedrooms. Costlier than cluster 1. \n* Cluster 4 (Yellow) -> young and large houses, with lot of furnishing but with high quality. More number of Bedrooms. Expensive. ","19d6661c":"<div id='1'\/>\n\n# Determinants of House Prices"}}