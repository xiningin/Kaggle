{"cell_type":{"143e1b5c":"code","aa7bacda":"code","10ea7b66":"code","86bbf4d8":"code","0be1468b":"code","3a070c0d":"code","c2626160":"code","e41e0dc8":"code","01353804":"code","9a6310d2":"code","801fab80":"code","57852967":"code","d6f7a2a4":"code","1ade5ad8":"code","91e1e477":"code","ef1e8266":"code","d6c4d738":"code","bd79ad12":"code","bc904c00":"code","0946ab0e":"code","a5719588":"code","0b4f72d9":"code","a796f47d":"code","b67a2a7f":"code","c93ef891":"code","e4c76cde":"code","c2354113":"code","7b975a9b":"code","d3e791e5":"code","27313b7e":"code","987543f7":"code","dc1a1414":"code","238582fd":"code","839f995a":"code","c5f986a8":"code","baea87b2":"code","a4fbe00c":"code","12ac6272":"code","9f0f97a9":"code","0abc5033":"code","27e14dda":"markdown"},"source":{"143e1b5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa7bacda":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","10ea7b66":"df = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding = 'latin1',header = None)\ndf.head(3)","86bbf4d8":"df = df[[5,0]]","0be1468b":"pd.set_option('display.max_columns', 15000)\npd.set_option('display.max_rows', 15000)","3a070c0d":"df.columns = ['twitts','sentiment']\ndf.head()","c2626160":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')","e41e0dc8":"df['sentiment'].value_counts()","01353804":"#Count words\ndf['word_count']=df['twitts'].apply(lambda x :len(str(x).split()))\n#count character\ndf['char_count']=df['twitts'].apply(lambda x :len(x))\ndf.head()","9a6310d2":"#Avg_word\ndef avg_word(x):\n    words = x.split()\n    word_len = 0\n    for word in words:\n        word_len = word_len + len(word)\n    return word_len\/len(words)  \ndf['Avg_count'] = df['twitts'].apply(lambda x :avg_word(x))    \ndf.head()    ","801fab80":"print(STOP_WORDS)","57852967":"#Stop_word_len\ndf['stop_word_len']= df['twitts'].apply(lambda x :len([t for t in x.split() if t in STOP_WORDS]))\ndf.head()","d6f7a2a4":"for t in df['twitts'][0].split():\n    if t.startswith('@'):\n        print(t)\n        print(len(t))","1ade5ad8":"#Count hastag & @\ndf['hash_len']= df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\ndf['mention_len']= df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\ndf.head()","91e1e477":"#count numeric digit\ndf['numeric']= df['twitts'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\ndf['upper_case']= df['twitts'].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))\ndf['lower_case']= df['twitts'].apply(lambda x: len([t for t in x.split() if t.islower() and len(x)>3]))\ndf.head()","ef1e8266":"df['twitts'][1]","d6c4d738":"#Change all character to lower case\ndf['twitts']=df['twitts'].apply(lambda x:x.lower())","bd79ad12":"#contraction to Expansion\ncontractions = {\n\"aight\": \"alright\",\n\"ain't\": \"am not\",\n\"amn't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"can not\",\n\"cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"daren't\": \"dare not\",\n\"daresn't\": \"dare not\",\n\"dasn't\": \"dare not\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"d'ye\": \"do you\",\n\"e'er\": \"ever\",\n\"everybody's\": \"everybody is\",\n\"everyone's\": \"everyone is\",\n\"finna\": \"fixing to\",\n\"g'day\": \"good day\",\n\"gimme\": \"give me\",\n\"giv'n\": \"given\",\n\"gonna\": \"going to\",\n\"gon't\": \"go not\",\n\"gotta\": \"got to\",\n\"hadn't\": \"had not\",\n\"had've\": \"had have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'dn't've'd\": \"he would not have had\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"he've\": \"he have\",\n\"how'd\": \"how would\",\n\"howdy\": \"how do you do\",\n\"how'll\": \"how will\",\n\"how're\": \"how are\",\n\"I'll\": \"I will\",\n\"I'm\": \"I am\",\n\"I'm'a\": \"I am about to\",\n\"I'm'o\": \"I am going to\",\n\"innit\": \"is it not\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"may've\": \"may have\",\n\"methinks\": \"me thinks\",\n\"mightn't\": \"might not\",\n\"might've\": \"might have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"must've\": \"must have\",\n\"needn't\": \"need not\",\n\"ne'er\": \"never\",\n\"o'clock\": \"of the clock\",\n\"o'er\": \"over\",\n\"ol'\": \"old\",\n\"oughtn't\": \"ought not\",\n\"'s\": \"is\",\n\"shalln't\": \"shall not\",\n\"shan't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she shall\",\n\"she'll\": \"she will\",\n\"she's\": \"she has\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"somebody's\": \"somebody has\",\n\"somebody's\": \"somebody is\",\n\"someone's\": \"someone has\",\n\"someone's\": \"someone is\",\n\"something's\": \"something has\",\n\"something's\": \"something is\",\n\"so're\": \"so are\",\n\"that'll\": \"that shall\",\n\"that'll\": \"that will\",\n\"that're\": \"that are\",\n\"that's\": \"that has\",\n\"that's\": \"that is\",\n\"that'd\": \"that would\",\n\"that'd\": \"that had\",\n\"there'd\": \"there had\",\n\"there'd\": \"there would\",\n\"there'll\": \"there shall\",\n\"there'll\": \"there will\",\n\"there're\": \"there are\",\n\"there's\": \"there has\",\n\"there's\": \"there is\",\n\"these're\": \"these are\",\n\"these've\": \"these have\",\n\"they'd\": \"they had\",\n\"they'd\": \"they would\",\n\"they'll\": \"they shall\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they're\": \"they were\",\n\"they've\": \"they have\",\n\"this's\": \"this has\",\n\"this's\": \"this is\",\n\"those're\": \"those are\",\n\"those've\": \"those have\",\n\"'tis\": \"it is\",\n\"to've\": \"to have\",\n\"'twas\": \"it was\",\n\"wanna\": \"want to\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd\": \"we would\",\n\"we'd\": \"we did\",\n\"we'll\": \"we shall\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'd\": \"what did\",\n\"what'll\": \"what shall\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what're\": \"what were\",\n\"what's\": \"what has\",\n\"what's\": \"what is\",\n\"what's\": \"what does\",\n\"what've\": \"what have\",\n\"when's\": \"when has\",\n\"when's\": \"when is\",\n\"where'd\": \"where did\",\n\"where'll\": \"where shall\",\n\"where'll\": \"where will\",\n\"where're\": \"where are\",\n\"where's\": \"where has\",\n\"where's\": \"where is\",\n\"where's\": \"where does\",\n\"where've\": \"where have\",\n\"which'd\": \"which had\",\n\"which'd\": \"which would\",\n\"which'll\": \"which shall\",\n\"which'll\": \"which will\",\n\"which're\": \"which are\",\n\"which's\": \"which has\",\n\"which's\": \"which is\",\n\"which've\": \"which have\",\n\"who'd\": \"who would\",\n\"who'd\": \"who had\",\n\"who'd\": \"who did\",\n\"who'd've\": \"who would have\",\n\"who'll\": \"who shall\",\n\"who'll\": \"who will\",\n\"who're\": \"who are\",\n\"who's\": \"who has\",\n\"who's\": \"who is\",\n\"who's\": \"who does\",\n\"who've\": \"who have\",\n\"why'd\": \"why did\",\n\"why're\": \"why are\",\n\"why's\": \"why has\",\n\"why's\": \"why is\",\n\"why's\": \"why does\",\n\"won't\": \"will not\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd've\": \"you all would have\",\n\"y'all'dn't've'd\": \"you all would not have had\",\n\"y'all're\": \"you all are\",\n\"you'd\": \"you had\",\n\"you'd\": \"you would\",\n\"you'll\": \"you shall\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\" u \": \"you\",\n\" ur \": \"your\",\n\" n \": \"and\"\n}","bc904c00":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x","0946ab0e":"df['twitts'] = df['twitts'].apply(lambda x: cont_to_exp(x))\ndf.head(3)","a5719588":"#email remove \nimport re\ndf['email']=df['twitts'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)',x))\ndf['email_count']=df['email'].apply(lambda y:len(y))\ndf.head()","0b4f72d9":"df[df['email_count']>0]","a796f47d":"#count URL and remove it\ndf['ulr_flag_count']= df['twitts'].apply(lambda x:len([re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)]))","b67a2a7f":"df.head()","c93ef891":"#remove the URL from the message of twitts\ndf['twitts']=df['twitts'].apply(lambda x: re.sub('(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?',\"\", x))","e4c76cde":"df.head()","c2354113":"#remove the RT\ndf['twitts']=df['twitts'].apply(lambda x: re.sub('RT',\"\", x))","7b975a9b":"df.head()","d3e791e5":"#Remove special character and punctuation\ndf['twitts'] = df['twitts'].apply(lambda x: re.sub('[^a-z A-Z 0-9-]+','', x))","27313b7e":"df.head()","987543f7":"df['twitts'][0]","dc1a1414":"#remove multiple white space\ndf['twitts'] = df['twitts'].apply(lambda x: ' '.join(x.split()))","238582fd":"df.head()","839f995a":"#remove html tags\n#from bs4 import BeautifulSoup\n#df['twitts'] = df['twitts'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())","c5f986a8":"df.head()","baea87b2":"tk = df['twitts'][1]\ntk","a4fbe00c":"c = tk.split()\nc","12ac6272":"print(STOP_WORDS)","9f0f97a9":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\ndef nltk1(text):\n    text_tokens = word_tokenize(text)\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    #print(tokens_without_sw)\n    return (\"\").join(tokens_without_sw)","0abc5033":"df['twitts'].apply(lambda x: nltk1(x))","27e14dda":"# *PreProcessing and cleaning the data*"}}