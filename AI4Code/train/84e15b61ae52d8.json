{"cell_type":{"345ada21":"code","297a3ec0":"code","d0312632":"code","6927ffdd":"code","f4457404":"code","b723d7a8":"code","36b9759f":"code","be41a2a8":"code","31beb49c":"code","e7dbde5d":"code","36e6b698":"code","ae3476a3":"code","ff7945de":"code","11302b94":"code","e60cd984":"code","98e1390d":"code","98910d77":"code","3c4d723b":"code","8c829dbf":"code","cd7401e2":"code","be6406e0":"code","386d8893":"code","bcc4637d":"code","e3137366":"code","5c826265":"code","6ef075ca":"code","3693561f":"code","350049f2":"code","6287c0e1":"code","7b1c5eca":"code","3477757e":"code","9c49c7c2":"code","b4271e34":"code","db7a36e0":"code","137e6aed":"code","8158bff6":"code","37677864":"code","e4505fe8":"code","3c0d119a":"code","fe897d62":"code","25e0fcba":"code","a8a01d55":"code","ca8512f2":"code","ce3cfa8f":"code","a2b5808c":"code","83580084":"code","b4b8a573":"code","29094040":"code","693be523":"code","46633c85":"code","3ffcb3eb":"code","3409e66b":"code","3e188a10":"code","55338dfa":"code","09d34f9d":"code","53f2d3fe":"code","d69b4aad":"code","4a505f53":"code","8c04d57f":"code","e2c695e0":"code","25d1e80a":"code","106db63b":"code","0f0897d9":"code","4f57fa57":"code","66b58d0f":"markdown","71f638e1":"markdown","0ed34fc8":"markdown","ab4dc755":"markdown","855ce4c8":"markdown","6a309b11":"markdown","945e921a":"markdown","65f41029":"markdown","0db338db":"markdown","67fcee4c":"markdown","10008b97":"markdown","4f5a52ee":"markdown","595d486f":"markdown","aca780c9":"markdown","08f659be":"markdown","5618451b":"markdown","bdd31a1d":"markdown","8e303d42":"markdown","bf5c96f7":"markdown","005a1b2c":"markdown","953832f1":"markdown","d7213d24":"markdown","c169bf07":"markdown","4c57a367":"markdown","90a854b2":"markdown","7be513af":"markdown","a8a9ae57":"markdown","38397d21":"markdown","75456ffd":"markdown","c4e17c84":"markdown","df8c3f27":"markdown","143812a5":"markdown","e6f2c7ee":"markdown","8ff0cdb6":"markdown","cd2b4d1f":"markdown","e55ba879":"markdown","d5c7d499":"markdown","f402406e":"markdown","fe266c28":"markdown","454baf51":"markdown","0ae7f80a":"markdown","5e2a6606":"markdown","b3712154":"markdown","28ed087c":"markdown","c7ff7ed7":"markdown","a9e77bb3":"markdown","ebfafcbf":"markdown","e945575d":"markdown","ffed8904":"markdown"},"source":{"345ada21":"%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import RandomizedSearchCV","297a3ec0":"data = pd.read_csv(\"..\/input\/housing-raw-data\/housing_raw.csv\")\ndata.head()","d0312632":"data.info()","6927ffdd":"data.describe()","f4457404":"col = data.columns       # .columns gives columns names in data \nprint(col)","b723d7a8":"data[\"ocean_proximity\"].value_counts()","36b9759f":"data[\"total_bedrooms\"].isna().sum()","be41a2a8":"data.hist(bins=50, figsize=(20,15))","31beb49c":"data['ocean_proximity'].value_counts().plot(kind='bar')","e7dbde5d":"for column in data.columns:\n    if data[column].dtype == np.float64:\n        plt.figure(figsize = (20, 3))\n        ax = sns.boxplot(x = data[column])","36e6b698":"def remove_outleirs(dataframe, column_name, threshold):\n    outliers_index = (dataframe[column_name] > threshold).values.nonzero()\n    \n    return dataframe.drop(labels = outliers_index[0], axis = 0)","ae3476a3":"clean_data = remove_outleirs(data, column_name = \"housing_median_age\", threshold = 100)","ff7945de":"plt.figure(figsize = (20, 3))\nax = sns.boxplot(x = clean_data[\"housing_median_age\"])","11302b94":"def remove_duplicates(dataframe):\n    duplicated_indexes = dataframe.duplicated(keep = \"first\")\n    return dataframe[~duplicated_indexes]","e60cd984":"print(\"duplicated =>\", clean_data.duplicated(keep = \"first\").sum())","98e1390d":"clean_data = remove_duplicates(clean_data)","98910d77":"print(\"duplicated =>\", clean_data.duplicated(keep = \"first\").sum())","3c4d723b":"def remove_inconsistencies(dataframe, columns):\n    inconsistent_indexes = dataframe.duplicated(subset = columns, keep = False)\n    return dataframe[~inconsistent_indexes]","8c829dbf":"features_columns = list(clean_data.columns)\nfeatures_columns.remove('median_house_value')","cd7401e2":"print(\"Inconsistency =>\", clean_data.duplicated(subset = features_columns, keep = False).sum())","be6406e0":"indexes = clean_data.duplicated(subset = features_columns, keep = False).values.nonzero()\nclean_data.iloc[indexes]","386d8893":"clean_data = remove_inconsistencies(clean_data, columns = features_columns)","bcc4637d":"print(\"Inconsistency =>\", clean_data.duplicated(subset = features_columns, keep = False).sum())","e3137366":"X = clean_data.drop(\"median_house_value\", axis=1)\ny = clean_data[\"median_house_value\"].copy()","5c826265":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","6ef075ca":"data_filler = SimpleImputer(strategy=\"median\")\nnumerical_data = X_train.drop(\"ocean_proximity\", axis=1)\ndata_filler.fit(numerical_data)\nnp.set_printoptions(suppress=True)\nprint(data_filler.statistics_)","3693561f":"X_train.median()","350049f2":"X_train_process = data_filler.transform(numerical_data)","6287c0e1":"print( np.isnan(np.sum(X_train_process)) )","7b1c5eca":"ocean_proximity = X_train[\"ocean_proximity\"].values\nocean_proximity = ocean_proximity.reshape(-1, 1)\nocean_proximity","3477757e":"ordinal_encoder = OrdinalEncoder()\nocean_proximity_encoded = ordinal_encoder.fit_transform(ocean_proximity)","9c49c7c2":"viz_data = X_train.copy()\nviz_data[\"ocean_proximity_ordinal\"] = ocean_proximity_encoded\nviz_data","b4271e34":"print(ordinal_encoder.categories_)","db7a36e0":"one_hot_encoder = OneHotEncoder()\nocean_proximity_hot_encoded = one_hot_encoder.fit_transform(ocean_proximity)","137e6aed":"viz_data = X_train.copy()\nviz_data[\"<1H OCEAN\"] = ocean_proximity_hot_encoded.toarray()[:, 0]\nviz_data[\"INLAND\"] = ocean_proximity_hot_encoded.toarray()[:, 1]\nviz_data[\"ISLAND\"] = ocean_proximity_hot_encoded.toarray()[:, 2]\nviz_data[\"NEAR BAY\"] = ocean_proximity_hot_encoded.toarray()[:, 3]\nviz_data[\"NEAR OCEAN\"] = ocean_proximity_hot_encoded.toarray()[:, 4]\nviz_data","8158bff6":"print(one_hot_encoder.categories_)","37677864":"numerical_columns = list(numerical_data)\ncategorical_columns = [\"ocean_proximity\"]","e4505fe8":"numerical_pipeline = Pipeline([\n        ('data_filler', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])\n\nX_train_numerical = numerical_pipeline.fit_transform(numerical_data)","3c0d119a":"pipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])","fe897d62":"def rmse_r2(model,y,y_predict):    \n    rmse = (np.sqrt(mean_squared_error(y, y_predict)))\n    r2 = r2_score(y, y_predict)\n    print('RMSE is {}'.format(rmse))\n    print('R2 score is {}'.format(r2))","25e0fcba":"def score_rmse_r2(model,X,y):    \n    rmse = cross_val_score(clf, X, y, cv=5, scoring='neg_root_mean_squared_error').mean()\n    r2 = cross_val_score(clf, X, y, cv=5, scoring='r2').mean()\n    print('RMSE is {}'.format(rmse))\n    print('R2 score is {}'.format(r2))","a8a01d55":"def get_predict(model,X_train,y_train,X_test,y_test):\n    print(\"\\nThe model performance for training set\")\n    print(\"--------------------------------------\")\n    y_predict = model.predict(X_train)\n    rmse_r2(model,y_train,y_predict)\n    print(\"\\nThe model performance for testing set\")\n    print(\"--------------------------------------\")\n    y_predict = model.predict(X_test)\n    rmse_r2(model,y_test,y_predict)","ca8512f2":"def get_score_predict(model,X_train,y_train,X_test,y_test):\n    print(\"\\nThe model performance for training set\")\n    print(\"--------------------------------------\")\n    score_rmse_r2(model,X_train,y_train)\n    print(\"\\nThe model performance for validation set\")\n    print(\"--------------------------------------\")\n    score_rmse_r2(model,X_test,y_test)","ce3cfa8f":"def test_score(model,X,y):\n    print(\"\\nThe model performance for testing set\")\n    print(\"--------------------------------------\")\n    score_rmse_r2(model,X,y)","a2b5808c":"def get_model_grid_search(model, parameters, X, y, pipeline):\n    \n    X = pipeline.fit_transform(X)    \n    \n    random_search = RandomizedSearchCV(model,\n                            param_distributions=parameters,\n                            scoring='r2',\n                            verbose=1, n_jobs=-1,\n                            n_iter=1000)\n    \n    grid_result = random_search.fit(X, y)\n    \n    print('Best R2: ', grid_result.best_score_)\n    print('Best Params: ', grid_result.best_params_)  \n    \n    return random_search.best_estimator_","83580084":"def get_model_random_search(model, parameters, X, y, pipeline):\n    \n    X = pipeline.fit_transform(X)    \n    clf = GridSearchCV(model, parameters, scoring='r2',cv=5,verbose=1, n_jobs=-1)\n    grid_result = clf.fit(X, y)\n    \n    print('Best R2: ', grid_result.best_score_)\n    print('Best Params: ', grid_result.best_params_)  \n    \n    return clf.best_estimator_","b4b8a573":"def k_fold_score(model, X ,y):\n    kf = KFold(n_splits = 5)\n    rmse_list = []\n    r2_list = []\n    for train_index, test_index in kf.split(X, y):\n        X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n        y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n\n        X_train = pipeline.fit_transform(X_train)\n        X_test = pipeline.transform(X_test)\n        \n        model.fit(X_train,y_train)\n        y_predict = model.predict(X_test)\n\n        rmse = (np.sqrt(mean_squared_error(y_test, y_predict)))\n        r2 = r2_score(y_test, y_predict)\n        rmse_list.append(rmse)\n        r2_list.append(r2)\n\n\n    rmse_list = np.array(rmse_list)\n    r2_list = np.array(r2_list)\n\n    print(\"--------------------------------------\")\n    print('RMSE is {}'.format(rmse_list.mean()))\n    print('R2 score is {}'.format(r2_list.mean()))","29094040":"X_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","693be523":"data_gs, data_cv, target_gs, target_cv = train_test_split(X, y, test_size=0.95, random_state=42)","46633c85":"lin_model = LinearRegression()\nlin_model.fit(X_train, y_train)\nget_predict(lin_model,X_train,y_train,X_test,y_test)","3ffcb3eb":"params = {\n    'alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000],       \n    'l1_ratio':[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n}\n\nen = ElasticNet()\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\nen_model = get_model_grid_search(en, params, data_gs, target_gs, pipeline)","3409e66b":"k_fold_score(en_model,data_cv, target_cv)","3e188a10":"params = {\n    'alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000],       \n    'l1_ratio':[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n}\n\nen = ElasticNet()\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\nen_rs_model = get_model_random_search(en, params, data_gs, target_gs, pipeline)","55338dfa":"k_fold_score(en_rs_model, data_cv, target_cv)","09d34f9d":"svr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\nget_predict(svr,X_train,y_train,X_test,y_test)","53f2d3fe":"params = {  'C': [0.1, 1, 100, 1000],\n            'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\n\nsvr = SVR(kernel='rbf')\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\nsvr_model = get_model_grid_search(svr, params, data_gs, target_gs, pipeline)","d69b4aad":"k_fold_score(svr_model,data_cv, target_cv)","4a505f53":"params = {  'C': [0.1, 1, 100, 1000],\n            'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\n\nsvr = SVR(kernel='rbf')\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\nsvr_rs_model = get_model_random_search(svr, params, data_gs, target_gs, pipeline)","8c04d57f":"k_fold_score(svr_rs_model, data_cv, target_cv)","e2c695e0":"tree = DecisionTreeRegressor()\ntree.fit(X_train, y_train)\nget_predict(tree, X_train, y_train, X_test, y_test)","25d1e80a":"params = {'min_samples_split': range(2, 10)}\n\ntree = DecisionTreeRegressor()\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\ntree_model = get_model_grid_search(tree, params, data_gs, target_gs, pipeline)","106db63b":"k_fold_score(tree_model, data_cv, target_cv)","0f0897d9":"params = {'min_samples_split': range(2, 10)}\n\ntree = DecisionTreeRegressor()\n\npipeline = ColumnTransformer([\n        (\"numerical\", numerical_pipeline, numerical_columns),\n        (\"categorical\", OneHotEncoder(), categorical_columns),\n    ])\n\ntree_rs_model = get_model_random_search(tree, params, data_gs, target_gs, pipeline)","4f57fa57":"k_fold_score(tree_rs_model, data_cv, target_cv)","66b58d0f":"## Data Analysis","71f638e1":"### Missing data","0ed34fc8":"## Support Vector Machine Regressor","ab4dc755":"## Acknowledgements","855ce4c8":"## Models","6a309b11":"### Cross Validation","945e921a":"* 5 categories of districts\n\n* Most districts are located less than 1H from the ocean, while the smallest part are located on islands.","65f41029":"### Outlier detection","0db338db":"## Imports","67fcee4c":"### Categorical columns","10008b97":"### Missing data","4f5a52ee":"### K Fold","595d486f":"### Outleirs","aca780c9":"**Elastic-Net Regression**\n\nElastic-net is a linear regression model that combines the penalties of Lasso and Ridge.","08f659be":"## Decision Tree Regressor","5618451b":"## Conclusion","bdd31a1d":"## Description","8e303d42":"#### Ordinal Encoder","bf5c96f7":"### Random Search","005a1b2c":"This data was initially featured in the following paper: Pace, R. Kelley, and Ronald Barry. \"Sparse spatial autoregressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n\nand I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aur\u00e9lien G\u00e9ron. Aur\u00e9lien G\u00e9ron wrote: This dataset is a modified version of the California Housing dataset available from: Lu\u00eds Torgo's page (University of Porto)","953832f1":"This is the dataset used in the second chapter of Aur\u00e9lien G\u00e9ron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n\nThe data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.","d7213d24":"## Read dataset","c169bf07":"### Inconsistency data","4c57a367":"## Clean Dataset","90a854b2":"### K Fold","7be513af":"* The attributes have different scales. \n    * It is recommended to rescale all the attributes\n\n\n* The medium house value has a sudden peak around 500000, which is very different from others. \n    * It is recommended to remove these data in training the model.\n\n\n* The medium income is centered around 3, where the unit is unknown. \n    * Probably, 3 means $30,000.","a8a9ae57":"### Random Search","38397d21":"### Redundancy data","75456ffd":"The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:\n\nlongitude\n\nlatitude\n\nhousing_median_age\n\ntotal_rooms\n\ntotal_bedrooms\n\npopulation\n\nhouseholds\n\nmedian_income\n\nmedian_house_value\n\nocean_proximity","c4e17c84":"### Grid Search","df8c3f27":"### Grid Search","143812a5":"9 numerical columns and 1 categorical columns","e6f2c7ee":"## Data Visualization","8ff0cdb6":"### Cross Validation","cd2b4d1f":"Random search differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.\nThere is a tradeoff to decreased processing time, however. We aren\u2019t guaranteed to find the optimal combination of hyperparameters.\n\nLet\u2019s give random search a try with sklearn\u2019s RandomizedSearchCV. Very similar to grid search above, we define the hyperparameters to search over before running the search.\n\nAn important additional parameter to specify here is n_iter. This specifies the number of combinations to randomly try.\n\nSelecting too low of a number will decrease our chance of finding the best combination. Selecting too large of a number will increase our processing time.\n","e55ba879":"Oficial Repository: https:\/\/www.kaggle.com\/camnugent\/california-housing-prices\n\nAlso can be found on Repository: https:\/\/www.kaggle.com\/fsalvagnini\/housing-raw-data?select=housing_raw.csv\n","d5c7d499":"**Linear Regression**\n\nCross Validation\n\n    RMSE is 87413    \n    R2 score is 0.42\n\nGrid Search\n\n    Best R2:  0.42\n    \nK Fold\n\n    RMSE is 88428\n    R2 score is 0.41\n\nRandom Search\n\n    Best R2:  0.42\n    \nK Fold\n\n    RMSE is 88428\n    R2 score is 0.41\n\n**Support Vector Machine Regressor**\n\nCross Validation\n\n    RMSE is 118241\n    R2 score is -0.05\n\nGrid Search\n\n    Best R2:  0.024\n    \nK Fold\n\n    RMSE is 92407\n    R2 score is 0.35\n\nRandom Search\n\n    Best R2:  0.02\n\nK Fold\n\n    RMSE is 92407\n    R2 score is 0.35\n\n**Decision Tree Regressor**\n\nCross Validation\n\n    RMSE is 71042\n    R2 score is 0.62\n\nGrid Search\n\n    Best R2:  0.44\n\nK Fold\n\n    RMSE is 65741\n    R2 score is 0.67\n\nRandom Search\n\n    Best R2:  0.44\nK Fold\n\n    RMSE is 65878\n    R2 score is 0.67","f402406e":"### Cross validation","fe266c28":"### Categorical columns","454baf51":"### Random Search","0ae7f80a":"### LinearRegression","5e2a6606":"# California Housing Prices","b3712154":"### K Fold","28ed087c":"#### One Hot Encoder","c7ff7ed7":"## Content","a9e77bb3":"### Grid Search","ebfafcbf":"## Pipeline","e945575d":"## Data Preprocessing","ffed8904":"Median house prices for California districts derived from the 1990 census."}}