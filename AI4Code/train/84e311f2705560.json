{"cell_type":{"b8023dee":"code","657fdd2a":"code","f419f40c":"code","b1a53d75":"code","b44d7e1e":"code","f2f5fdc2":"code","907097c0":"code","e14f39c6":"code","1bc8682a":"code","47738609":"code","1a30a270":"code","aff1ab0d":"code","a7ac22a5":"code","6b267839":"code","e3d08ab4":"code","c34f8297":"code","f79cbfaa":"code","54c0c708":"code","44fb1488":"code","ad20306c":"code","245d840b":"code","35dc696f":"code","480ed07c":"code","41b317dd":"code","47a8220a":"code","90537509":"code","14727fb9":"code","7de2dd12":"code","f530fe33":"code","7d821825":"code","49f6eb80":"code","2e580e5d":"code","29762f62":"code","c0ba831a":"code","3d0bd3db":"code","c09401ca":"markdown","e58b03e9":"markdown","36e9165f":"markdown","b1d4678b":"markdown","148144d2":"markdown","32d6aae3":"markdown","725cc887":"markdown","39fbb579":"markdown","f69a5d54":"markdown","52ad71d2":"markdown","e5f57e68":"markdown","8a01dbc3":"markdown","3be60850":"markdown","f671b7e8":"markdown","498b7af1":"markdown","1405dfb5":"markdown","17627caf":"markdown","621c27c2":"markdown","d7a0988c":"markdown","42bae55c":"markdown","38282749":"markdown","11d872dc":"markdown","65271946":"markdown","8dd3b24e":"markdown","b2719b99":"markdown","da973851":"markdown","9895f512":"markdown","fa7cc784":"markdown","33ca178f":"markdown","9439dbf8":"markdown","f807d4c3":"markdown","3a9dfe83":"markdown","00d7e7c2":"markdown","7a74736a":"markdown","5f7f25a8":"markdown","dd41732a":"markdown","9d9e36f0":"markdown","1527a03d":"markdown"},"source":{"b8023dee":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split","657fdd2a":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest = test_full.copy()","f419f40c":"train.head()","b1a53d75":"print('-----Information-----')\nprint(train.info())","b44d7e1e":"sns.distplot(train['Survived'], kde=False)","f2f5fdc2":"sns.heatmap(train.isna(), cmap='viridis', cbar=None, yticklabels=False)","907097c0":"sns.heatmap(test.isna(), cmap='viridis', cbar=None, yticklabels=False)","e14f39c6":"train['Age'].fillna(train['Age'].median(), inplace=True)\ntest['Age'].fillna(test['Age'].median(), inplace=True)\n\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","1bc8682a":"train['Cabin'].fillna(0, inplace=True)\ntrain['Cabin'] = train['Cabin'].map(lambda p: 1 if p is not 0 else 0)\n\ntest['Cabin'].fillna(0, inplace=True)\ntest['Cabin'] = test['Cabin'].map(lambda p: 1 if p is not 0 else 0)","47738609":"sns.heatmap(train.isna(), cmap='viridis', cbar=None, yticklabels=False)","1a30a270":"sns.heatmap(test.isna(), cmap='viridis', cbar=None, yticklabels=False)","aff1ab0d":"for i, col in enumerate(['SibSp','Parch']):\n    plt.figure(i)\n    sns.catplot(x=col, y='Survived', data=train, kind='point', aspect=2)","a7ac22a5":"train['Family_count'] = train['SibSp'] + train['Parch']\ntest['Family_count'] = test['SibSp'] + test['Parch']\ntrain.head()","6b267839":"train.drop(columns=['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch', 'Embarked'], inplace=True)\ntest.drop(columns=['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch', 'Embarked'], inplace=True)\n\ntrain.head()","e3d08ab4":"label = LabelEncoder()\ntrain['Sex_conv'] = label.fit_transform(train['Sex'])\ntrain.drop(columns='Sex', inplace=True)\n\ntest['Sex_conv'] = label.fit_transform(test['Sex'])\ntest.drop(columns='Sex', inplace=True)\n\ntrain.head()","c34f8297":"sns.heatmap(np.abs(train.corr()), cmap='Blues', annot=True)","f79cbfaa":"X_train = train.drop(columns='Survived')\nX_test = test\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif[\"features\"] = X_train.columns\nvif.round(2)","54c0c708":"X_train.head()","44fb1488":"scaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nX_test = pd.DataFrame(scaler.fit_transform(X_test), columns = X_test.columns)\n\nprint('----Training Set-----')\nprint(X_train.head())\nprint('----Test Set-----')\nprint(X_test.head())","ad20306c":"y = train['Survived']\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size = 0.4, stratify=y, shuffle=True)","245d840b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV","35dc696f":"params = {'C':[0.001, 0.01, 0.1, 1, 10, 100]}\n\nlr = GridSearchCV(LogisticRegression(), params, cv=5)\nlr.fit(X_train, y_train)\nlr.best_params_","480ed07c":"LR_model = lr.best_estimator_","41b317dd":"params = {'C':[0.001, 0.01, 0.1, 1, 10, 100], \n          'kernel':['rbf', 'linear', 'poly', 'sigmoid']}\n\nsvc = GridSearchCV(SVC(), params, cv=5)\nsvc.fit(X_train, y_train)\nsvc.best_params_","47a8220a":"SVC_model = svc.best_estimator_","90537509":"params = {'n_estimators':[5, 50, 250, 500], \n          'max_depth':[2, 4, 8, 16, 32, None]}\n\nrf = GridSearchCV(RandomForestClassifier(), params, cv=5)\nrf.fit(X_train, y_train)\nrf.best_params_","14727fb9":"RF_model = rf.best_estimator_","7de2dd12":"params = {'learning_rate': [0.001, 0.1, 1, 10, 100], \n          'n_estimators':[5, 50, 250, 500], \n          'max_depth':[1, 3, 5, 7, 9]}\n\ngb = GridSearchCV(GradientBoostingClassifier(), params, cv=5)\ngb.fit(X_train, y_train)\ngb.best_params_","f530fe33":"GB_model = gb.best_estimator_","7d821825":"params = {'hidden_layer_sizes': [(10,), (50,), (100,)],\n          'activation': ['relu', 'tanh', 'logistic'],\n          'learning_rate': ['constant', 'invscaling', 'adaptive']}\n\nmlp = GridSearchCV(MLPClassifier(), params, cv=5)\nmlp.fit(X_train, y_train)\nmlp.best_params_","49f6eb80":"MLP_model = mlp.best_estimator_","2e580e5d":"from sklearn.metrics import accuracy_score\nfrom time import time\n\nsummary = pd.DataFrame(columns=['Model', 'Accuracy Score', 'Prediction Time (ms)'])\nmodels = {'LR':LR_model, 'SVC':SVC_model, 'RF':RF_model, 'GB':GB_model, 'MLP':MLP_model}","29762f62":"def evaluate_model(name, model):\n    start=time()\n    y_pred = model.predict(X_val)\n    stop=time()\n    accuracy = accuracy_score(y_val, y_pred)\n    global summary\n    summary = summary.append({'Model':name, 'Accuracy Score':np.round(accuracy,4), 'Prediction Time (ms)': (stop-start)*1000}, ignore_index=True)","c0ba831a":"for name, model in models.items():\n    evaluate_model(name, model)\n    \nsummary","3d0bd3db":"predictions = GB_model.predict(X_test)\noutput=pd.DataFrame({'PassengerId': test_full.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)","c09401ca":"Most important parameters: learning_rate, max_depth, n_estimators","e58b03e9":"### Drop unneccessary variables","36e9165f":"For the final model, we will select Gradient Boost since it gave the highest accuracy and it is also fast in predicting (2nd to MLP).","b1d4678b":"There is almost equal number of labels so we can use accuracy as our scoring method in the model selection later on.","148144d2":"It is important to check that our variables are independent from each other since our models assume independence.","32d6aae3":"Aside from the hyperparameter \"C\", the parameter \"kernel\" is also important.","725cc887":"From the information above, we see that there are null values and there are also object data types which we may need to convert later on so we can use in our model.","39fbb579":"# 2. Fitting and evaluating a model","f69a5d54":"### Split train data into train and validation set","52ad71d2":"We see that our data features has different scales. Let us standardize this using the StandardScaler from sklearn.","e5f57e68":"# 3. Comparison and final model selection","8a01dbc3":"# 1. Data Exploration, Cleaning and Preparation","3be60850":"We see from the plot that generally, survival decreases with increasing number of relatives inside the ship. So we create 1 variable called 'Family_count'","f671b7e8":"We convert categorical variables to numerical so we can use in our model. We use the LabelEncoder from sci-kit learn.","498b7af1":"### Check for correlation and multicolliniarity","1405dfb5":"### Random Forest","17627caf":"Next we fill the missing values for the Cabin. From the data, we can see that the passenger either has a cabin or no cabin. So for those who do not have a cabin, we assign a value of 0 while we assign a value of 1 for those who have.","621c27c2":"### Gradient Boosting","d7a0988c":"1. We can drop PassengerId, Name, and Ticket since these are unique values associated to each passenger.\n2. We can also drop SibSp and Parch since we already created a combined variable 'Family_count'.\n3. The column embarked which indicates the point of embarkation can also be dropped since this is not important for survival.","42bae55c":"### Convert categorical variables","38282749":"The most important hyperparameter for Logistic Regression is \"C\" which is the regularization parameter.","11d872dc":"### Combine related variables","65271946":"### Scale the data","8dd3b24e":"Let us check for missing values through heatmap visualization.","b2719b99":"### Multi Layer Perceptron","da973851":"# 4. Evaluating the final model","9895f512":"We will use different classification models namely\n    1. Logistic Regression\n    2. Support Vector Machines\n    3. Random Forest\n    4. Gradient Boosting\n    5. Multi Layer Perceptron\n   \nWe will also be using 5-fold cross validation using GridSearchCV to get the best hyperparameters for each model.","fa7cc784":"From the data, we can see that we have columns 'SibSp' (number of siblings and spouses) and 'Parch' (number of parents and children) which are both related and we can combine to create 1 variable. To confirm, let us check through visualization the trend on Survival.","33ca178f":"Most important parameters: max_depth and n_estimators","9439dbf8":"1. Only Age and Cabin has missing values for train set while the test set has 1 missing value for Fare. Let us impute the Age and Fare with the median.","f807d4c3":"Let us split into 70% train and 30% validation. We use the train_test_split from sklearn.","3a9dfe83":"No 2 variables are strongly correlated with each other and all VIF Factors are less than 10 which indicates that there is no multicollinearity.","00d7e7c2":"Most important parameters: activation, hidden_layer_sizes, learning_rate","7a74736a":"Both the train and test set is now free from missing values.","5f7f25a8":"### Support Vector Machines","dd41732a":"### Load the data","9d9e36f0":"### Logistic Regression","1527a03d":"### Fill missing values"}}