{"cell_type":{"8bc1ed5a":"code","53d61df4":"code","0b540c7e":"code","7cb9c88a":"code","819893ca":"code","0066cc88":"code","f6fc49d8":"code","ffb02e9e":"code","ed6b8dec":"code","f32b778a":"code","1b462131":"code","a98007ce":"code","b0b1efa4":"code","8ab2a96b":"code","de8eed6d":"code","19bc1964":"code","1741d7d3":"code","ea5f6c4c":"markdown","45f95c9d":"markdown","69e93965":"markdown","af7e6833":"markdown","faf2bce4":"markdown","97191c78":"markdown","eef3d35c":"markdown","f9119428":"markdown","9c27e5b9":"markdown","45fc3b27":"markdown","f99cc5dd":"markdown","a22e66f5":"markdown","d513143b":"markdown","b9510779":"markdown","678fc190":"markdown","bc15a964":"markdown","ac2d0f71":"markdown","20b1a1e1":"markdown","2def3fc9":"markdown","faa308d0":"markdown","f290e173":"markdown","8d409b0e":"markdown","a30dfc5e":"markdown","5000c4ca":"markdown","87412d70":"markdown","071ea9a7":"markdown","3e8cd940":"markdown","58cace12":"markdown","e1ff153b":"markdown","87386e6a":"markdown","c0b32eac":"markdown","e65091ed":"markdown"},"source":{"8bc1ed5a":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nfrom scipy.sparse import csr_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom surprise import Reader, Dataset, SVD\nsns.set_style(\"darkgrid\")","53d61df4":"# Skip date\ndf1 = pd.read_csv('..\/input\/combined_data_1.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\ndf1['Rating'] = df1['Rating'].astype(float)\n\nprint('Dataset 1 shape: {}'.format(df1.shape))\nprint('-Dataset examples-')\nprint(df1.iloc[::5000000, :])","0b540c7e":"#df2 = pd.read_csv('..\/input\/combined_data_2.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n#df3 = pd.read_csv('..\/input\/combined_data_3.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n#df4 = pd.read_csv('..\/input\/combined_data_4.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\n\n#df2['Rating'] = df2['Rating'].astype(float)\n#df3['Rating'] = df3['Rating'].astype(float)\n#df4['Rating'] = df4['Rating'].astype(float)\n\n#print('Dataset 2 shape: {}'.format(df2.shape))\n#print('Dataset 3 shape: {}'.format(df3.shape))\n#print('Dataset 4 shape: {}'.format(df4.shape))","7cb9c88a":"# load less data for speed\n\ndf = df1\n#df = df1.append(df2)\n#df = df.append(df3)\n#df = df.append(df4)\n\ndf.index = np.arange(0,len(df))\nprint('Full dataset shape: {}'.format(df.shape))\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])","819893ca":"p = df.groupby('Rating')['Rating'].agg(['count'])\n\n# get movie count\nmovie_count = df.isnull().sum()[1]\n\n# get customer count\ncust_count = df['Cust_Id'].nunique() - movie_count\n\n# get rating count\nrating_count = df['Cust_Id'].count() - movie_count\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)\nplt.axis('off')\n\nfor i in range(1,6):\n    ax.text(p.iloc[i-1][0]\/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 \/ p.sum()[0]), color = 'white', weight = 'bold')\n\n","0066cc88":"df_nan = pd.DataFrame(pd.isnull(df.Rating))\ndf_nan = df_nan[df_nan['Rating'] == True]\ndf_nan = df_nan.reset_index()\n\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\n# Account for last record and corresponding length\n# numpy approach\nlast_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)\n\nprint('Movie numpy: {}'.format(movie_np))\nprint('Length: {}'.format(len(movie_np)))","f6fc49d8":"# remove those Movie ID rows\ndf = df[pd.notnull(df['Rating'])]\n\ndf['Movie_Id'] = movie_np.astype(int)\ndf['Cust_Id'] = df['Cust_Id'].astype(int)\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])\n","ffb02e9e":"f = ['count','mean']\n\ndf_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\ndf_movie_summary.index = df_movie_summary.index.map(int)\nmovie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)\ndrop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n\nprint('Movie minimum times of review: {}'.format(movie_benchmark))\n\ndf_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)\ndf_cust_summary.index = df_cust_summary.index.map(int)\ncust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)\ndrop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n\nprint('Customer minimum times of review: {}'.format(cust_benchmark))","ed6b8dec":"print('Original Shape: {}'.format(df.shape))\ndf = df[~df['Movie_Id'].isin(drop_movie_list)]\ndf = df[~df['Cust_Id'].isin(drop_cust_list)]\nprint('After Trim Shape: {}'.format(df.shape))\nprint('-Data Examples-')\nprint(df.iloc[::5000000, :])","f32b778a":"df_p = pd.pivot_table(df,values='Rating',index='Cust_Id',columns='Movie_Id')\n\nprint(df_p.shape)\n\n# Below is another way I used to sparse the dataframe...doesn't seem to work better\n\n#Cust_Id_u = list(sorted(df['Cust_Id'].unique()))\n#Movie_Id_u = list(sorted(df['Movie_Id'].unique()))\n#data = df['Rating'].tolist()\n#row = df['Cust_Id'].astype('category', categories=Cust_Id_u).cat.codes\n#col = df['Movie_Id'].astype('category', categories=Movie_Id_u).cat.codes\n#sparse_matrix = csr_matrix((data, (row, col)), shape=(len(Cust_Id_u), len(Movie_Id_u)))\n#df_p = pd.DataFrame(sparse_matrix.todense(), index=Cust_Id_u, columns=Movie_Id_u)\n#df_p = df_p.replace(0, np.NaN)\n","1b462131":"df_title = pd.read_csv('..\/input\/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\ndf_title.set_index('Movie_Id', inplace = True)\nprint (df_title.head(10))","a98007ce":"reader = Reader()\n\n# get just top 100K rows for faster run time\ndata = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:100000], reader)\n#data.split(n_folds=3)\n\nsvd = SVD()\n#evaluate(svd, data, measures=['RMSE', 'MAE'])","b0b1efa4":"df_785314 = df[(df['Cust_Id'] == 785314) & (df['Rating'] == 5)]\ndf_785314 = df_785314.set_index('Movie_Id')\ndf_785314 = df_785314.join(df_title)['Name']\nprint(df_785314)\n","8ab2a96b":"user_785314 = df_title.copy()\nuser_785314 = user_785314.reset_index()\nuser_785314 = user_785314[~user_785314['Movie_Id'].isin(drop_movie_list)]\n\n# getting full dataset\ndata = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']], reader)\n\n#trainset = data.build_full_trainset()\n#svd.train(trainset)\n\nuser_785314['Estimate_Score'] = user_785314['Movie_Id'].apply(lambda x: svd.predict(785314, x).est)\n\nuser_785314 = user_785314.drop('Movie_Id', axis = 1)\n\nuser_785314 = user_785314.sort_values('Estimate_Score', ascending=False)\nprint(user_785314.head(10))","de8eed6d":"def recommend(movie_title, min_count):\n    print(\"For movie ({})\".format(movie_title))\n    print(\"- Top 10 movies recommended based on Pearsons'R correlation - \")\n    i = int(df_title.index[df_title['Name'] == movie_title][0])\n    target = df_p[i]\n    similar_to_target = df_p.corrwith(target)\n    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n    corr_target.dropna(inplace = True)\n    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n    corr_target.index = corr_target.index.map(int)\n    corr_target = corr_target.join(df_title).join(df_movie_summary)[['PearsonR', 'Name', 'count', 'mean']]\n    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))","19bc1964":"recommend(\"What the #$*! Do We Know!?\", 0)","1741d7d3":"recommend(\"X2: X-Men United\", 0)","ea5f6c4c":"Evalute performance of [collaborative filtering](https:\/\/en.wikipedia.org\/wiki\/Collaborative_filtering), with just first 100K rows for faster process:","45f95c9d":"The data set now is super huge. I have tried many different ways but can't get the Kernel running as intended without memory error. Therefore I tried to reduce the data volumn by improving the data quality below:\n\n* Remove movie with too less reviews (they are relatively not popular)\n* Remove customer who give too less reviews (they are relatively less active)\n\nHaving above benchmark will have significant improvement on efficiency, since those unpopular movies and non-active customers still occupy same volumn as those popular movies and active customers in the view of matrix (NaN still occupy space). This should help improve the statistical signifiance too.\n\nLet's see how it is implemented:","69e93965":"## Recommend with Collaborative Filtering","af7e6833":"## Data loading","faf2bce4":"# Objective\n<br>\nLearn from data and recommend best TV shows to users, based on self & others behaviour\n<br>","97191c78":"Let's pivot the data set and put it into a giant matrix - we need it for our recommendation system:","eef3d35c":"Let's give a first look on how the data spread:","f9119428":"# Data manipulation","9c27e5b9":"Below is what user 783514 liked in the past:","45fc3b27":"## Recommend with Pearsons' R correlations","f99cc5dd":"Each data file (there are 4 of them) contains below columns:\n\n* Movie ID (as first line of each new movie record \/ file)\n* Customer ID\n* Rating (1 to 5)\n* Date they gave the ratings\n\nThere is another file contains the mapping of Movie ID to the movie background like name, year of release, etc","a22e66f5":"Now we load the movie mapping file:","d513143b":"## Data cleaning","b9510779":"Let's predict which movies user 785314 would love to watch:","678fc190":"A recommendation for you if you like 'What the #$*! Do We Know!?'","bc15a964":"## Table of Content:\n\n* Objective\n\n* Data manipulation\n    -  Data loading\n    -  Data viewing\n    -  Data cleaning\n    -  Data slicing\n    -  Data mapping\n    \n* Recommendation models\n    -  Recommend with Collaborative Filtering (*Edit on 2017\/11\/07*)\n    -  Recommend with Pearsons' R correlation","ac2d0f71":"We can see that the rating tends to be relatively positive (>3). This may be due to the fact that unhappy customers tend to just leave instead of making efforts to rate. We can keep this in mind - low rating movies mean they are generally really bad","20b1a1e1":"Hope it is a good read. I will keep updating this Kernel (more models etc). Welcome any suggestions!\n","2def3fc9":"Well all data required is loaded and cleaned! Next let's get into the recommendation system.","faa308d0":"## Data mapping","f290e173":"Let's import the library we needed before we get started:","8d409b0e":"Let's try to load the 3 remaining dataset as well:","a30dfc5e":"The way it works is we use Pearsons' R correlation to measure the linear correlation between review scores of all pairs of movies, then we provide the top 10 movies with highest correlations:","5000c4ca":"Next let's load first data file and get a feeling of how huge the dataset is:","87412d70":"Now we combine datasets:","071ea9a7":"*Last edit by DLao - 2019\/10*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<br>\n<br>\n\n\n![](https:\/\/m.media-amazon.com\/images\/M\/MV5BNTFhOTk1NTgtYWM1ZS00NWI1LTgzYzAtYmE5MjZiMDE0NzlhXkEyXkFqcGdeQXVyMTkxNjUyNQ@@._V1_SY1000_CR0,0,675,1000_AL_.jpg)\n# Netflix Analytics - Movie Recommendation through Correlations \/ CF\n<br>\n\nI love Netflix! Everyone does?\n\nThis project aims to build a movie recommendation mechanism within Netflix. The dataset I used here come directly from Netflix. It consists of 4 text data files, each file contains over 20M rows, i.e. over 4K movies and 400K customers. All together **over 17K movies** and **500K+ customers**! \n\n<br>\nOne of the major challenges is to get all these data loaded into the Kernel for analysis, I have encountered many times of Kernel running out of memory and tried many different ways of how to do it more efficiently. Welcome any suggestions!!!\n\nThis kernel will be consistently be updated! Welcome any suggestions! Let's get started!\n\n<br>\nFeel free to fork and upvote if this notebook is helpful to you in some ways!\n","3e8cd940":"X2: X-Men United:","58cace12":"# Recommendation models","e1ff153b":"## Data slicing","87386e6a":"## Data viewing","c0b32eac":"Movie ID is really a mess import! Looping through dataframe to add Movie ID column WILL make the Kernel run out of memory as it is too inefficient. I achieve my task by first creating a numpy array with correct length then add the whole array as column into the main dataframe! Let's see how it is done below:","e65091ed":"Now let's trim down our data, whats the difference in data size?"}}