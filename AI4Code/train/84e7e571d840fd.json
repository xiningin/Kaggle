{"cell_type":{"a1d1d2a9":"code","f151f60d":"code","acff81cb":"code","a274b01a":"code","4a6e9272":"code","681f4ee5":"code","07e3e0a7":"code","57c6066c":"code","a0c9692e":"code","56488f41":"code","a5c8c212":"code","235a1302":"code","721fa5cd":"code","11fb86ee":"code","afd54308":"code","b2d1fffc":"code","072947f7":"code","d646419e":"code","cd412380":"code","f78275c7":"code","3d641781":"code","20fb29c5":"code","2752d2bf":"code","12214031":"code","c73b4e8a":"code","785e15b2":"code","bd9e1653":"code","6fa1527d":"code","0058a187":"code","54a29baa":"code","7c41358e":"code","c08d0bf8":"code","1b963cd9":"code","7d65fbe0":"code","60a32bb7":"code","e60c6ab4":"code","e65f2647":"code","f1ca1ffb":"code","667c63ca":"code","beb1221b":"code","9525112f":"code","1b4ea6d1":"code","dbdc4b4e":"code","973ad1c7":"code","717cf168":"code","c72e7214":"code","0aaab7a4":"code","069650af":"code","31f187f7":"code","f7ba8a73":"code","cb4b4ccf":"code","0426c759":"code","bdff322a":"code","fc8966c1":"code","c64f798c":"code","e4623bab":"code","0fe5e66a":"code","4aed8420":"code","bfe1e437":"code","aeff63a6":"code","9e352f71":"code","2916f490":"code","b1d12dbc":"code","9de450ec":"code","2f030125":"code","c8665694":"code","8a59a11a":"code","529a1374":"code","ac535c14":"code","0f39a8d2":"code","3cd1b24d":"code","0e01c0ad":"code","138ce29c":"code","f1b85609":"code","7cc22729":"code","c4a1d6ca":"code","a6ee981e":"code","4c01ee11":"code","e4a7191f":"code","f47b2e57":"code","6588ed8d":"code","48128d6f":"code","cc5a4c6b":"code","ad38feba":"code","ee952448":"code","6da83e35":"code","bd07a05c":"code","1cb26ec2":"code","f515e11b":"code","eacf5cc7":"code","04ca96c6":"code","f018df02":"code","082fd1b5":"code","ff82767a":"code","eae4e82a":"code","772d553d":"code","80da1107":"code","b74cf45c":"code","b3a11605":"code","02aaca4a":"code","151e9d49":"code","7264f3a6":"code","0b6b773a":"markdown","62f45497":"markdown","4363f63f":"markdown","89e88b0b":"markdown","61454a8c":"markdown","f5baa506":"markdown","cedf5f9b":"markdown","e130f206":"markdown","282c43f0":"markdown","612ffed0":"markdown","dd69e3bd":"markdown","59f6dd22":"markdown","f07e5c90":"markdown","5a401794":"markdown","23095704":"markdown","663ba9a1":"markdown","9cb71e13":"markdown","71c462f1":"markdown","451d317c":"markdown","f896a100":"markdown","872edc09":"markdown","58dd5565":"markdown","51ae7168":"markdown","90cb3193":"markdown","f67dc7f5":"markdown","d47d3384":"markdown","d4c4e209":"markdown","88bc6252":"markdown","da009ecc":"markdown","98dde087":"markdown","a498e46f":"markdown","e5776c21":"markdown","8bdcbe3d":"markdown","7a5c632d":"markdown","c18c3ed4":"markdown","2767c873":"markdown","8b681567":"markdown","b03b2166":"markdown","26074f0d":"markdown","6e480e55":"markdown","747cf47d":"markdown","0980ae1b":"markdown","6666136c":"markdown","08d4220e":"markdown","43d2068a":"markdown","1dbb1e6f":"markdown","4e9ac434":"markdown","36e6e220":"markdown","519b95a0":"markdown","97d9074a":"markdown","6007b4ad":"markdown","e904f514":"markdown","e2cf2e40":"markdown","1100b25a":"markdown","c8f7e3c3":"markdown","946fac54":"markdown","5df3d61b":"markdown","0c4ba69d":"markdown","56334a98":"markdown","dd4b5df2":"markdown","006c6835":"markdown","665e2e9a":"markdown","ebbed2ff":"markdown","05286b92":"markdown","a368d709":"markdown","89d05a2d":"markdown","c2e6816e":"markdown","d4f70d13":"markdown","cf7afd8f":"markdown","8f5a56b2":"markdown","f4459510":"markdown","2947d39d":"markdown","2e785cb3":"markdown","152b7dd9":"markdown","a271af7b":"markdown","5512b8f9":"markdown","547aae20":"markdown","b789eb24":"markdown"},"source":{"a1d1d2a9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# plotly library\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\")\n#warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n#warnings.filterwarnings(action='once')\n\nfrom sklearn.utils.testing import ignore_warnings\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","f151f60d":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","acff81cb":"def get_best_score(model):\n    \n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_\n\n\ndef plot_feature_importances(model, columns):\n    nr_f = 10\n    imp = pd.Series(data = model.best_estimator_.feature_importances_, \n                    index=columns).sort_values(ascending=False)\n    plt.figure(figsize=(7,5))\n    plt.title(\"Feature importance\")\n    ax = sns.barplot(y=imp.index[:nr_f], x=imp.values[:nr_f], orient='h')","a274b01a":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","4a6e9272":"df_train.head()","681f4ee5":"df_test.head()","07e3e0a7":"df_train.info()","57c6066c":"df_test.info()","a0c9692e":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_train.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","56488f41":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_test.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","a5c8c212":"cols = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']","235a1302":"nr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(df_train[cols[i]], hue=df_train[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=14, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()   ","721fa5cd":"bins = np.arange(0, 80, 5)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Age', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","11fb86ee":"df_train['Fare'].max()","afd54308":"bins = np.arange(0, 550, 50)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Fare', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","b2d1fffc":"sns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass\")\nplt.show()","072947f7":"sns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass and Sex\")\nplt.show()","d646419e":"sns.barplot(x='Embarked', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","cd412380":"sns.barplot(x='Embarked', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","f78275c7":"sns.countplot(x='Embarked', hue='Pclass', data=df_train)\nplt.title(\"Count of Passengers as function of Embarked Port\")\nplt.show()","3d641781":"sns.boxplot(x='Embarked', y='Age', data=df_train)\nplt.title(\"Age distribution as function of Embarked Port\")\nplt.show()","20fb29c5":"sns.boxplot(x='Embarked', y='Fare', data=df_train)\nplt.title(\"Fare distribution as function of Embarked Port\")\nplt.show()","2752d2bf":"cm_surv = [\"darkgrey\" , \"lightgreen\"]","12214031":"fig, ax = plt.subplots(figsize=(13,7))\nsns.swarmplot(x='Pclass', y='Age', hue='Survived', split=True, data=df_train , palette=cm_surv, size=7, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","c73b4e8a":"fig, ax = plt.subplots(figsize=(13,7))\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue='Survived', data=df_train, split=True, bw=0.05 , palette=cm_surv, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","785e15b2":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"swarm\", split=True, palette=cm_surv, size=7, aspect=.9, s=7)","bd9e1653":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"violin\", split=True, bw=0.05, palette=cm_surv, size=7, aspect=.9, s=7)","6fa1527d":"for df in [df_train, df_test] :\n    \n    df['FamilySize'] = df['SibSp'] + df['Parch'] +1\n    \n    df['Alone']=0\n    df.loc[(df.FamilySize==1),'Alone'] = 1\n    \n    df['NameLen'] = df.Name.apply(lambda x : len(x)) \n    df['NameLenBin']=np.nan\n    for i in range(20,0,-1):\n        df.loc[ df['NameLen'] <= i*5, 'NameLenBin'] = i\n    \n    \n    df['Title']=0\n    df['Title']=df.Name.str.extract(r'([A-Za-z]+)\\.') #lets extract the Salutations\n    df['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","0058a187":"print(df_train[['NameLen' , 'NameLenBin']].head(10))","54a29baa":"grps_namelenbin_survrate = df_train.groupby(['NameLenBin'])['Survived'].mean().to_frame()\ngrps_namelenbin_survrate","7c41358e":"plt.subplots(figsize=(10,6))\nsns.barplot(x='NameLenBin' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of NameLenBin\")\nplt.show()","c08d0bf8":"fig, ax = plt.subplots(figsize=(9,7))\nsns.violinplot(x=\"NameLenBin\", y=\"Pclass\", data=df_train, hue='Survived', split=True, \n               orient=\"h\", bw=0.2 , palette=cm_surv, ax=ax)\nplt.show()","1b963cd9":"g = sns.factorplot(x=\"NameLenBin\", y=\"Survived\", col=\"Sex\", data=df_train, kind=\"bar\", size=5, aspect=1.2)","7d65fbe0":"grps_title_survrate = df_train.groupby(['Title'])['Survived'].mean().to_frame()\ngrps_title_survrate","60a32bb7":"plt.subplots(figsize=(10,6))\nsns.barplot(x='Title' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Title\")\nplt.show()","e60c6ab4":"pd.crosstab(df_train.FamilySize,df_train.Survived).apply(lambda r: r\/r.sum(), axis=1).style.background_gradient(cmap='summer_r')","e65f2647":"plt.subplots(figsize=(10,6))\nsns.barplot(x='FamilySize' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of FamilySize\")\nplt.show()","f1ca1ffb":"for df in [df_train, df_test]:\n\n    # Title\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n\n    # Age: use Title to fill missing values\n    df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title==\"Mr\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title==\"Mrs\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title==\"Master\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title==\"Miss\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title==\"Other\"].mean()\n    df = df.drop('Name', axis=1)\n\n\n","667c63ca":"# Embarked\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode().iloc[0])\ndf_test['Embarked'] = df_test['Embarked'].fillna(df_test['Embarked'].mode().iloc[0])\n\n# Fare\ndf_train['Fare'] = df_train['Fare'].fillna(df_train['Fare'].mean())\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())","beb1221b":"for df in [df_train, df_test]:\n    \n    df['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        df.loc[ df['Age'] <= i*10, 'Age_bin'] = i\n        \n    df['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        df.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i        \n    \n    # convert Title to numerical\n    df['Title'] = df['Title'].map( {'Other':0, 'Mr': 1, 'Master':2, 'Miss': 3, 'Mrs': 4 } )\n    # fill na with maximum frequency mode\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n    df['Title'] = df['Title'].astype(int)        ","9525112f":"df_train_ml = df_train.copy()\ndf_test_ml = df_test.copy()\n\npassenger_id = df_test_ml['PassengerId']","1b4ea6d1":"df_train_ml.info()","dbdc4b4e":"df_test_ml.info()","973ad1c7":"df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\ndf_test_ml = pd.get_dummies(df_test_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n\ndf_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\ndf_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\n\n#df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n#df_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n","717cf168":"df_train_ml.dropna(inplace=True)","c72e7214":"for df in [df_train_ml, df_test_ml]:\n    df.drop(['NameLen'], axis=1, inplace=True)\n\n    df.drop(['SibSp'], axis=1, inplace=True)\n    df.drop(['Parch'], axis=1, inplace=True)\n    df.drop(['Alone'], axis=1, inplace=True)","0aaab7a4":"df_train_ml.head()","069650af":"df_test_ml.fillna(df_test_ml.mean(), inplace=True)\ndf_test_ml.head()","31f187f7":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# for df_train_ml\nscaler.fit(df_train_ml.drop(['Survived'],axis=1))\nscaled_features = scaler.transform(df_train_ml.drop(['Survived'],axis=1))\ndf_train_ml_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])\n\n# for df_test_ml\ndf_test_ml.fillna(df_test_ml.mean(), inplace=True)\n#scaler.fit(df_test_ml)\nscaled_features = scaler.transform(df_test_ml)\ndf_test_ml_sc = pd.DataFrame(scaled_features) # , columns=df_test_ml.columns)","f7ba8a73":"df_train_ml_sc.head()","cb4b4ccf":"df_test_ml_sc.head()","0426c759":"df_train_ml.head()","bdff322a":"X = df_train_ml.drop('Survived', axis=1)\ny = df_train_ml['Survived']\nX_test = df_test_ml\n\nX_sc = df_train_ml_sc\ny_sc = df_train_ml['Survived']\nX_test_sc = df_test_ml_sc","fc8966c1":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\n\n\nfrom sklearn.metrics import accuracy_score\n","c64f798c":"from sklearn.model_selection import cross_val_score","e4623bab":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores_svc)\nprint(scores_svc.mean())","0fe5e66a":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc_sc = cross_val_score(svc, X_sc, y_sc, cv=10, scoring='accuracy')\nprint(scores_svc_sc)\nprint(scores_svc_sc.mean())","4aed8420":"rfc = RandomForestClassifier(max_depth=5, max_features=6)\nscores_rfc = cross_val_score(rfc, X, y, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())","bfe1e437":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform","aeff63a6":"model = SVC()\nparam_grid = {'C':uniform(0.1, 5000), 'gamma':uniform(0.0001, 1) }\nrand_SVC = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100)\nrand_SVC.fit(X_sc,y_sc)\nscore_rand_SVC = get_best_score(rand_SVC)","9e352f71":"param_grid = {'C': [0.1,10, 100, 1000,5000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\nsvc_grid = GridSearchCV(SVC(), param_grid, cv=10, refit=True, verbose=1)\nsvc_grid.fit(X_sc,y_sc)\nsc_svc = get_best_score(svc_grid)","2916f490":"pred_all_svc = svc_grid.predict(X_test_sc)\n\nsub_svc = pd.DataFrame()\nsub_svc['PassengerId'] = df_test['PassengerId']\nsub_svc['Survived'] = pred_all_svc\nsub_svc.to_csv('svc.csv',index=False)","b1d12dbc":"knn = KNeighborsClassifier()\nleaf_range = list(range(3, 15, 1))\nk_range = list(range(1, 15, 1))\nweight_options = ['uniform', 'distance']\nparam_grid = dict(leaf_size=leaf_range, n_neighbors=k_range, weights=weight_options)\nprint(param_grid)\n\nknn_grid = GridSearchCV(knn, param_grid, cv=10, verbose=1, scoring='accuracy')\nknn_grid.fit(X, y)\n\nsc_knn = get_best_score(knn_grid)","9de450ec":"pred_all_knn = knn_grid.predict(X_test)\n\nsub_knn = pd.DataFrame()\nsub_knn['PassengerId'] = df_test['PassengerId']\nsub_knn['Survived'] = pred_all_knn\nsub_knn.to_csv('knn.csv',index=False)","2f030125":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n\nparam_grid = {'min_samples_split': [4,7,10,12]}\ndtree_grid = GridSearchCV(dtree, param_grid, cv=10, refit=True, verbose=1)\ndtree_grid.fit(X_sc,y_sc)\n\nprint(dtree_grid.best_score_)\nprint(dtree_grid.best_params_)\nprint(dtree_grid.best_estimator_)","c8665694":"pred_all_dtree = dtree_grid.predict(X_test_sc)\n\nsub_dtree = pd.DataFrame()\nsub_dtree['PassengerId'] = df_test['PassengerId']\nsub_dtree['Survived'] = pred_all_dtree\nsub_dtree.to_csv('dtree.csv',index=False)","8a59a11a":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparam_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [6,7,8,9,10],  \n              'min_samples_split': [5, 6, 7, 8]}\n\nrf_grid = GridSearchCV(rfc, param_grid, cv=10, refit=True, verbose=1)\nrf_grid.fit(X_sc,y_sc)\nsc_rf = get_best_score(rf_grid)","529a1374":"plot_feature_importances(rf_grid, X.columns)","ac535c14":"pred_all_rf = rf_grid.predict(X_test_sc)\n\nsub_rf = pd.DataFrame()\nsub_rf['PassengerId'] = df_test['PassengerId']\nsub_rf['Survived'] = pred_all_rf\nsub_rf.to_csv('rf.csv',index=False)","0f39a8d2":"from sklearn.ensemble import ExtraTreesClassifier\nextr = ExtraTreesClassifier()\n\nparam_grid = {'max_depth': [6,7,8,9], 'max_features': [7,8,9,10],  \n              'n_estimators': [50, 100, 200]}\n\nextr_grid = GridSearchCV(extr, param_grid, cv=10, refit=True, verbose=1)\nextr_grid.fit(X_sc,y_sc)\nsc_extr = get_best_score(extr_grid)","3cd1b24d":"plot_feature_importances(extr_grid, X.columns)","0e01c0ad":"pred_all_extr = extr_grid.predict(X_test_sc)\n\nsub_extr = pd.DataFrame()\nsub_extr['PassengerId'] = df_test['PassengerId']\nsub_extr['Survived'] = pred_all_extr\nsub_extr.to_csv('extr.csv',index=False)","138ce29c":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\n\nparam_grid = {'n_estimators': [50, 100], \n              'min_samples_split': [3, 4, 5, 6, 7],\n              'max_depth': [3, 4, 5, 6]}\ngbc_grid = GridSearchCV(gbc, param_grid, cv=10, refit=True, verbose=1)\ngbc_grid.fit(X_sc,y_sc)\nsc_gbc = get_best_score(gbc_grid)","f1b85609":"plot_feature_importances(gbc_grid, X.columns)","7cc22729":"pred_all_gbc = gbc_grid.predict(X_test_sc)\n\nsub_gbc = pd.DataFrame()\nsub_gbc['PassengerId'] = df_test['PassengerId']\nsub_gbc['Survived'] = pred_all_gbc\nsub_gbc.to_csv('gbc.csv',index=False)","c4a1d6ca":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nparam_grid = {'max_depth': [5,6,7,8], 'gamma': [1, 2, 4], 'learning_rate': [0.1, 0.2, 0.3, 0.5]}\n\nwith ignore_warnings(category=DeprecationWarning):\n    xgb_grid = GridSearchCV(xgb, param_grid, cv=10, refit=True, verbose=1)\n    xgb_grid.fit(X_sc,y_sc)\n    sc_xgb = get_best_score(xgb_grid)","a6ee981e":"plot_feature_importances(xgb_grid, X.columns)","4c01ee11":"with ignore_warnings(category=DeprecationWarning):\n    pred_all_xgb = xgb_grid.predict(X_test_sc)\n\nsub_xgb = pd.DataFrame()\nsub_xgb['PassengerId'] = df_test['PassengerId']\nsub_xgb['Survived'] = pred_all_xgb\nsub_xgb.to_csv('xgb.csv',index=False)","e4a7191f":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\n\nparam_grid = {'n_estimators': [30, 50, 100], 'learning_rate': [0.08, 0.1, 0.2]}\nada_grid = GridSearchCV(ada, param_grid, cv=10, refit=True, verbose=1)\nada_grid.fit(X_sc,y_sc)\nsc_ada = get_best_score(ada_grid)\n\npred_all_ada = ada_grid.predict(X_test_sc)","f47b2e57":"plot_feature_importances(ada_grid, X.columns)","6588ed8d":"sub_ada = pd.DataFrame()\nsub_ada['PassengerId'] = df_test['PassengerId']\nsub_ada['Survived'] = pred_all_ada\nsub_ada.to_csv('ada.csv',index=False)","48128d6f":"from catboost import CatBoostClassifier\ncat=CatBoostClassifier()\n\nparam_grid = {'iterations': [100, 150], 'learning_rate': [0.3, 0.4, 0.5], 'loss_function' : ['Logloss']}\n\ncat_grid = GridSearchCV(cat, param_grid, cv=10, refit=True, verbose=1)\ncat_grid.fit(X_sc,y_sc, verbose=False)\nsc_cat = get_best_score(cat_grid)\n\npred_all_cat = cat_grid.predict(X_test_sc)","cc5a4c6b":"plot_feature_importances(cat_grid, X.columns)","ad38feba":"sub_cat = pd.DataFrame()\nsub_cat['PassengerId'] = df_test['PassengerId']\nsub_cat['Survived'] = pred_all_cat\nsub_cat['Survived'] = sub_cat['Survived'].astype(int)\nsub_cat.to_csv('cat.csv',index=False)","ee952448":"import lightgbm as lgb\nlgbm = lgb.LGBMClassifier(silent=False)\nparam_grid = {\"max_depth\": [8,10,15], \"learning_rate\" : [0.008,0.01,0.012], \n              \"num_leaves\": [80,100,120], \"n_estimators\": [200,250]  }\nlgbm_grid = GridSearchCV(lgbm, param_grid, cv=10, refit=True, verbose=1)\nlgbm_grid.fit(X_sc,y_sc, verbose=True)\nsc_lgbm = get_best_score(lgbm_grid)\n\npred_all_lgbm = lgbm_grid.predict(X_test_sc)","6da83e35":"plot_feature_importances(lgbm_grid, X.columns)","bd07a05c":"sub_lgbm = pd.DataFrame()\nsub_lgbm['PassengerId'] = df_test['PassengerId']\nsub_lgbm['Survived'] = pred_all_lgbm\nsub_lgbm.to_csv('lgbm.csv',index=False)","1cb26ec2":"from sklearn.ensemble import VotingClassifier","f515e11b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}\n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid = GridSearchCV(estimator=eclf, param_grid=params, cv=10)\n    votingclf_grid.fit(X_sc,y_sc)\n    sc_vot1 = get_best_score(votingclf_grid)","eacf5cc7":"clf4 = GradientBoostingClassifier()\nclf5 = SVC()\nclf6 = RandomForestClassifier()\n\neclf_2 = VotingClassifier(estimators=[('gbdt', clf4), \n                                      ('svc', clf5), \n                                      ('rf', clf6)], voting='soft')\n\nparams = {'gbdt__n_estimators': [50], 'gbdt__min_samples_split': [3],\n          'svc__C': [10, 100] , 'svc__gamma': [0.1,0.01,0.001] , 'svc__kernel': ['rbf'] , 'svc__probability': [True],  \n          'rf__max_depth': [7], 'rf__max_features': [2,3], 'rf__min_samples_split': [3] } \n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid_2 = GridSearchCV(estimator=eclf_2, param_grid=params, cv=10)\n    votingclf_grid_2.fit(X_sc,y_sc)\n    sc_vot2_cv = get_best_score(votingclf_grid_2)","04ca96c6":"with ignore_warnings(category=DeprecationWarning):    \n    pred_all_vot2 = votingclf_grid_2.predict(X_test_sc)\n\nsub_vot2 = pd.DataFrame()\nsub_vot2['PassengerId'] = df_test['PassengerId']\nsub_vot2['Survived'] = pred_all_vot2\nsub_vot2.to_csv('vot2.csv',index=False)","f018df02":"from mlxtend.classifier import StackingClassifier","082fd1b5":"# Initializing models\nclf1 = xgb_grid.best_estimator_\nclf2 = gbc_grid.best_estimator_\nclf3 = rf_grid.best_estimator_\nclf4 = svc_grid.best_estimator_\n\nlr = LogisticRegression()\nst_clf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3, clf4], meta_classifier=lr)\n\nparams = {'meta_classifier__C': [0.1,1.0,5.0,10.0] ,\n          #'use_probas': [True] ,\n          #'average_probas': [True] ,\n          'use_features_in_secondary' : [True, False]\n         }\nwith ignore_warnings(category=DeprecationWarning):\n    st_clf_grid = GridSearchCV(estimator=st_clf, param_grid=params, cv=5, refit=True)\n    st_clf_grid.fit(X_sc, y_sc)\n    sc_st_clf = get_best_score(st_clf_grid)","ff82767a":"with ignore_warnings(category=DeprecationWarning):    \n    pred_all_stack = st_clf_grid.predict(X_test_sc)\n\nsub_stack = pd.DataFrame()\nsub_stack['PassengerId'] = df_test['PassengerId']\nsub_stack['Survived'] = pred_all_stack\nsub_stack.to_csv('stack_clf.csv',index=False)","eae4e82a":"list_scores = [sc_knn, sc_rf, sc_extr, sc_svc, sc_gbc, sc_xgb, \n               sc_ada, sc_cat, sc_lgbm, sc_vot2_cv, sc_st_clf]\n\nlist_classifiers = ['KNN','RF','EXTR','SVC','GBC','XGB',\n                    'ADA','CAT','LGBM','VOT2','STACK']","772d553d":"score_subm_svc   = 0.80861\nscore_subm_vot2  = 0.78947\nscore_subm_ada   = 0.78468\nscore_subm_lgbm  = 0.78468\nscore_subm_rf    = 0.77990\nscore_subm_xgb   = 0.77033\nscore_subm_dtree = 0.76076\nscore_subm_extr  = 0.76076\nscore_subm_gbc  = 0.74641\nscore_subm_cat   = 0.74162\nscore_subm_knn   = 0.69856\n\nscore_subm_stack = 0.76076","80da1107":"subm_scores = [score_subm_knn, score_subm_rf, score_subm_extr, score_subm_svc, \n               score_subm_gbc, score_subm_xgb, score_subm_ada, score_subm_cat, \n               score_subm_lgbm, score_subm_vot2, score_subm_stack]","b74cf45c":"trace1 = go.Scatter(x = list_classifiers, y = list_scores,\n                   name=\"Validation\", text = list_classifiers)\ntrace2 = go.Scatter(x = list_classifiers, y = subm_scores,\n                   name=\"Submission\", text = list_classifiers)\n\ndata = [trace1, trace2]\n\nlayout = dict(title = \"Validation and Submission Scores\", \n              xaxis=dict(ticklen=10, zeroline= False),\n              yaxis=dict(title = \"Accuracy\", side='left', ticklen=10,),                                  \n              legend=dict(orientation=\"v\", x=1.05, y=1.0),\n              autosize=False, width=750, height=500,\n              )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","b3a11605":"predictions = {'KNN': pred_all_knn, 'RF': pred_all_rf, 'EXTR': pred_all_extr, \n               'SVC': pred_all_svc, 'GBC': pred_all_gbc, 'XGB': pred_all_xgb, \n               'ADA': pred_all_ada, 'CAT': pred_all_cat, 'LGBM': pred_all_lgbm, \n               'VOT2': pred_all_vot2, 'STACK': pred_all_stack}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","02aaca4a":"plt.figure(figsize=(9, 9))\nsns.set(font_scale=1.25)\nsns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns\n            )\nplt.yticks(rotation=0)\nplt.show()","151e9d49":"imp_rf   = pd.Series(data = rf_grid.best_estimator_.feature_importances_, index=X.columns)\nimp_extr = pd.Series(data = extr_grid.best_estimator_.feature_importances_, index=X.columns)\nimp_gbc = pd.Series(data = gbc_grid.best_estimator_.feature_importances_, index=X.columns)\nimp_xgb = pd.Series(data = xgb_grid.best_estimator_.feature_importances_, index=X.columns)\nimp_ada = pd.Series(data = ada_grid.best_estimator_.feature_importances_, index=X.columns)\nimp_cat = pd.Series(data = cat_grid.best_estimator_.feature_importances_ \/ 100, index=X.columns)\nimp_lgbm = pd.Series(data = lgbm_grid.best_estimator_.feature_importances_ \/ 10000, index=X.columns)\nimp_lgbm \/= imp_lgbm.sum()","7264f3a6":"models=['RF', 'EXTR', 'GBC', 'XGB', 'ADA', 'CAT', 'LGBM']\n\nfig = go.Figure(data=[\n    go.Bar(name=models[0],  x=imp_rf.index,   y=imp_rf.values),\n    go.Bar(name=models[1],  x=imp_extr.index, y=imp_extr.values),\n    go.Bar(name=models[2],  x=imp_gbc.index,  y=imp_gbc.values),   \n    go.Bar(name=models[3],  x=imp_xgb.index,  y=imp_xgb.values),    \n    go.Bar(name=models[4],  x=imp_ada.index,  y=imp_ada.values),        \n    go.Bar(name=models[5],  x=imp_cat.index,  y=imp_cat.values),  \n    go.Bar(name=models[6],  x=imp_lgbm.index, y=imp_lgbm.values),  \n])\n\nfig.update_layout(barmode='group')\nfig.update_layout(title=go.layout.Title(\n                  text=\"Feature importances for Ensemble models\",\n                  xref=\"paper\",\n                  x=0.5))\nfig.show()","0b6b773a":"### New Feature: NameLenBin","62f45497":"### SVC : GridSearchCV","4363f63f":"### Review: k fold cross validation  \njust a short review of this technique that we already studied in the first kernel","89e88b0b":"* With this kernel we studied EDA with **Seaborn** including some unusual plots like violin and swarm.   \n* Based on the EDA we filled missing values according to related features and developed new features (**Feature Engineering**) to improve model performance.  \n* In Part 3 we learned basics of applying **ensemble models** for classification like **Boosting**, **Stacking** and **Voting**.   \n* For this we applied libraries like: **sklearn, mlxtend, lightgbm, catboost, xgboost**","61454a8c":"### Swarm and Violin plots\nAlthough the following swarm and violin plots show the same data like the countplots or distplots before,  \nthey can reveal ceratin details that disappear in other plots. However, it takes more time to study these plots in detail.","f5baa506":"### Correlation of prediction results","cedf5f9b":"**References**  \nWhile this notebook contains some work work based on my ideas, it is also a collection of approaches  \nand techniques from these kaggle notebooks:","e130f206":"### RFC, features not scaled  ","282c43f0":"This violinplot shows exactly the same info like the swarmplot before.","612ffed0":"### Compare feature importances","dd69e3bd":"Here, the high survival rate for kids in Pclass 2 is easily observed.  \nAlso, it becomes more obvious that for passengers older than 40 the best chance to survive is in Pclass 1,  \nand smallest chance in Pclass 3   ","59f6dd22":"### Seaborn Countplots  \nfor all categorical columns","f07e5c90":"**In the following we apply GridSearchCV and RandomizedSearchCV for these Classification models:**  \n**KNN, Decision Tree, Random Forest, SVC**","5a401794":"# Part 2: Data Wrangling and Feature Engineering","23095704":"### Standard Scaler","663ba9a1":"### Comparison plot for best models","9cb71e13":"### ExtraTreesClassifier","71c462f1":"### submission scores","451d317c":"### scores from GridSearchCV","f896a100":"### First Voting  \nfor the first voting ensemble I use three simple models (LR, RF, GNB)","872edc09":"**submission for GradientBoostingClassifier**","58dd5565":"**submission for ExtraTreesClassifier**","51ae7168":"### Bining for Age and Fare, convert Title to numerical","90cb3193":"* **TODO:**  \n \ncompare feature importances  \ncomplete documentation","f67dc7f5":"### SVC, features not scaled  \nSupport Vector Machine Classifier","d47d3384":"# Titanic Survival: Seaborn and Ensembles\n**My second Titanic kernel**\n\n**[Part 0: Imports, Functions](#Part-0:-Imports,-Functions)** \n\n**[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)** \n\n* Seaborn [heatmaps](#Seaborn-heatmaps) : missing data in df_train and df_test\n* Seaborn [countplots](#Seaborn-Countplots) : Number of (Non-)Survivors as function of features\n* Seaborn [distplots](#Seaborn-Distplots) : Distribution of Age and Fare as function of Pclass, Sex and Survived  \n* [Bar and Box plots](#Bar-and-Box-plots) for categorical features : Pclass and Embarked\n* Seaborn [violin and swarm plots](#Swarm-and-Violin-plots) : Survivals as function of Age, Pclass and Sex\n\n**[Part 2: Data Wrangling and Feature Engineering](#Part-2:-Data-Wrangling-and-Feature-Engineering)**  \n\n* [Feature Engineering](#Feature-Engineering): include new features to improve the performance of the classifiers and to fill missing values:  \nFamily size, Alone, Name length, Title\n* [Data Wrangling](#Data-Wrangling): fill NaN, convert categorical to numerical, [Standard Scaler](#Standard-Scaler), create X, y and X_test for Part 3\n\n\n**[Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking](#Part-3:-Optimization-of-Classifier-parameters,-Boosting,-Voting-and-Stacking)**  \n\n* Review: [k fold cross validation](#Review:-k-fold-cross-validation) for SVC and Random Forest: \n * SVC, features not scaled \n * SVC, features scaled \n * Random Forest Classifier, RFC, features not scaled \n* Hyperparameter tuning with GridSearchCV and RandomizedSearchCV for: \n * Support Vector Machine Classifier, [SVC](#SVC-:-RandomizedSearchCV) \n * K Nearest Neighbor, [KNN](#KNN)\n * [Decision Tree](#Decision-Tree)\n * [Random Forest Classifier](#Random-Forest), RFC\n\n* study Ensemble models like Boosting, Stacking and Voting:  \n * [ExtraTreesClassifier](#ExtraTreesClassifier)\n * Gradient Boost Decision Tree - [GBDT](#Gradient-Boost-Decision-Tree-GBDT)\n * eXtreme Gradient Boosting - [XGBoost](#eXtreme-Gradient-Boosting---XGBoost)   \n * Adaptive Boosting - [AdaBoost](#Ada-Boost)\n * [CatBoost](#CatBoost)\n * lightgbm [LGBM](#lightgbm-LGBM)\n * Voting: [VotingClassifier 1](#First-Voting), [VotingClassifier 2](#Second-Voting)  \n * Stacking : [StackingClassifier](#StackingClassifier)  \n* Compare Classifier performance based on the validation score : [comparison plot](#Comparison-plot-for-best-models)\n* Correlation of prediction results : [correlation matrix](#Correlation-of-prediction-results)","d4c4e209":"Default mode for seaborn barplots is to plot the mean value for the category.  \nAlso, the standard deviation is indicated.  \nSo, if we choose Survived as y-value, we get a plot of the survival rate as function   \nof the categories present in the feature chosen as x-value.","88bc6252":"### New Feature: Family size","da009ecc":"### Gradient Boost Decision Tree GBDT \n","98dde087":"**Disribution of Fare as function of Pclass, Sex and Survived**","a498e46f":"### CatBoost\nlibrary for gradient boosting on decision trees with categorical features support","e5776c21":"### Decision Tree","8bdcbe3d":"### Ada Boost  ","7a5c632d":"**Boxplot**","c18c3ed4":"### SVC : RandomizedSearchCV","2767c873":"### Random Forest","8b681567":"Looks like the feature Cabin has lots of missing data, also some data for Age and Embarked is missing.  \nLets plot the seaborn heatmap of the isnull matrix for the train and test data","b03b2166":"## Data Wrangling","26074f0d":"**submission for svc**","6e480e55":"# Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking","747cf47d":"# Part 1: Exploratory Data Analysis","0980ae1b":"### eXtreme Gradient Boosting - XGBoost","6666136c":"Best chances to survive for male passengers was in Pclass 1 or being below 5 years old.  \nLowest survival rate for female passengers was in Pclass 3 and being older than 40.  \nMost passengers were male, in Pclass 3 and between 15-35 years old.","08d4220e":"But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.","43d2068a":"### Seaborn Distplots \n**Distribution of Age as function of Pclass, Sex and Survived**","1dbb1e6f":"**submission for random forest**","4e9ac434":"## Hyperparameter tuning with RandomizedSearchCV and GridSearchCV","36e6e220":"Of the 891 passengers in df_test, less than 350 survive.  \nMuch more women survive than men.  \nAlso, the chance to survive is much higher in Pclass 1 and 2 than in Class 3.  \nSurvival rate for passengers travelling with SibSp or Parch is higher than for those travelling alone.  \nPassengers embarked in C and Q are more likely to survie than those embarked in S.","519b95a0":"**Looks like there is very strong correlation of Survival rate and Name length**","97d9074a":"## Feature Engineering\n**New Features: 'FamilySize'  ,  'Alone' , 'NameLen' and 'Title'**","6007b4ad":"**This is my second notebook for the Titanic classification competition.**\n\nIf you are new to Machine Learning, have a look at  **[my first Titanic notebook](https:\/\/www.kaggle.com\/dejavu23\/titanic-survival-for-beginners-eda-to-ml)** where  I studied the  \nbasics of EDA with Pandas and Matplotlib and how to do Classification with the scikit-learn library.  ","e904f514":"Passengers embarked in \"S\" had the lowest survival rate, those embarked in \"C\" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass.","e2cf2e40":"Passengers embarked in \"C\" had largest proportion of Pclass 1 tickets.  \nAlmost all Passengers embarked in \"Q\" had Pclass 3 tickets.  \nFor every class, the largest count of Passengers  embarked in \"S\".","1100b25a":"### StackingClassifier","c8f7e3c3":"### SVC, features scaled  ","946fac54":"### lightgbm LGBM","5df3d61b":"### **Conclusion**","0c4ba69d":"**submission for decision tree**","56334a98":"# Part 0: Imports, Functions","dd4b5df2":"### VotingClassifier","006c6835":"### New Feature: Title","665e2e9a":"**Fill NaN with mean or mode**","ebbed2ff":"**loading the data**","05286b92":"### KNN","a368d709":"### Seaborn heatmaps  \nmissing data in df_train and df_test","89d05a2d":"**RandomizedSearchCV  and GridSearchCV apply k fold cross validation on a chosen set of parameters**\n**and then find the parameters that give the best performance.**  \nFor GridSearchCV, all possible combinations of the specified parameter values are tried out, resulting in a parameter grid.  \nFor RandomizedSearchCV, a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.","c2e6816e":"### Bar and Box plots","d4f70d13":"**double-check for missing values**","cf7afd8f":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg\" width=\"520\">","8f5a56b2":"**submission for knn**","f4459510":"Mean fare for Passengers embarked in \"C\" was higher.","2947d39d":"**some useful functions**","2e785cb3":"**convert categorical to numerical : get_dummies**","152b7dd9":"**Increase of survival rate with length of name most important for male passengers**","a271af7b":"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. ","5512b8f9":"**Chance to survive increases with length of name for all Passenger classes**","547aae20":"Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.","b789eb24":"### Second Voting\n\nfor the 2nd voting ensemble I use the three models (together with the optimal parameters found by GridSearchCV)  \nthat had the best test score based on the cross validations above "}}