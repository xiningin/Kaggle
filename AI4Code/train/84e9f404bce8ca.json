{"cell_type":{"28853416":"code","e0281c2d":"code","ea53748f":"code","d9efe49a":"code","cb962b6f":"code","eb85c8de":"code","4270229b":"code","2547db89":"code","e62302ff":"code","367b97f9":"code","4bd44950":"code","61602ee5":"code","f19c6c33":"code","a9a72d17":"code","9705c11c":"code","21dff418":"code","f5e06a81":"code","d1d6a89a":"code","5b99ea15":"code","59a9f3f4":"code","aa325c71":"code","dd19ee71":"code","65297046":"code","0a4f29c0":"code","8aee170a":"code","33da390e":"code","dc6d796c":"code","0e254e2d":"code","3eec72cc":"code","b8a88e75":"code","772bbf1b":"code","765d4f19":"code","e8542827":"code","f5442e74":"code","7505dfbf":"markdown","bf3a2064":"markdown","1932003b":"markdown","4743fc79":"markdown","c87e7ec8":"markdown","f5ad0522":"markdown","7ba84075":"markdown","ab7eacbe":"markdown","5640315e":"markdown","6e43a655":"markdown","4afa3765":"markdown","fabab0de":"markdown","3856da85":"markdown","46625841":"markdown","f30b4fac":"markdown","de63286b":"markdown","f80ba6e3":"markdown","f6b9513c":"markdown","6778f5c1":"markdown","b687e50b":"markdown","1e7e9ea7":"markdown","593de332":"markdown","6a3ed63b":"markdown","93bfd5dc":"markdown","47664ed9":"markdown","098b0d76":"markdown"},"source":{"28853416":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0281c2d":"! head \/kaggle\/input\/netflix-prize-data\/combined_data_1.txt","ea53748f":"if not os.path.isfile('all_data.csv'):\n    #read all txt file and store them in one big file\n    data = open('all_data.csv', mode='w')\n    \n    files = ['..\/input\/netflix-prize-data\/combined_data_1.txt']#, '..\/input\/netflix-prize-data\/combined_data_2.txt']\n            #'..\/input\/netflix-prize-data\/combined_data_3.txt', '..\/input\/netflix-prize-data\/combined_data_4.txt']\n    for file in files:\n        print('reading ratings from {}...'.format(file))\n        with open(file) as f:\n            for line in f:\n                line = line.strip()\n                if line.endswith(':'):\n                    #all are rating\n                    movid_id = line.replace(':', '')\n                else:\n                    row = [x for x in line.split(',')]\n                    row.insert(0, movid_id)\n                    data.write(','.join(row))\n                    data.write('\\n')\n        print('Done.\\n')\n    data.close()","d9efe49a":"! head all_data.csv","cb962b6f":"print('Loading csv file to dataframe...')\ndf = pd.read_csv(\n        'all_data.csv',\n        names=['movieId', 'userId', 'rating', 'date'],\n        dtype={'movieId': str, 'userId': str, 'rating': float},\n        parse_dates = ['date']\n)\n\n# We'll sample to only 150 movies and 50% of the reviews on them to make our lives easier\nnp.random.seed(42)\nrandom_movies = np.random.choice(df.movieId.unique(), 500, replace=False)\n\ndf = df[df.movieId.isin(random_movies)]#.sample(frac=.5, random_state=42)\nprint('Done!')","eb85c8de":"df.head()","4270229b":"# visualization library\nimport seaborn as sns","2547db89":"df.describe()","e62302ff":"print('number of Null\/Missing values in our dataset:', sum(df.isnull().any()))","367b97f9":"dup = df.duplicated(['movieId','userId','rating'])\ndups = sum(dup) #considering by column\nprint('there are {} duplicate rating entries in the data'.format(dups))","4bd44950":"print('\\nTotal number of ratings:', df.shape[0])\nprint('Total number of users:', len(np.unique(df.userId)))\nprint('total number of movies:', len(np.unique(df.movieId)))","61602ee5":"sns.displot(df, x='rating', bins=range(1,7))","f19c6c33":"ratings_per_user = df.groupby('userId').size()\nsns.displot(ratings_per_user, bins=range(50))","a9a72d17":"print(f'Average number of ratings per user: {ratings_per_user.mean()}')","9705c11c":"avg_daily_ratings = df.groupby('date').mean()\navg_daily_ratings.head()","21dff418":"avg_daily_ratings.plot()","f5e06a81":"num_monthly_ratings = df.groupby(pd.Grouper(key='date', freq='M')).size()\nnum_monthly_ratings.plot()","d1d6a89a":"from scipy import sparse","5b99ea15":"df_pivoted = df.pivot(index='userId', \n                      columns='movieId', \n                      values='rating')\n\ndf_pivoted.head(10)","59a9f3f4":"df_pivoted_filled = df_pivoted.fillna(0)","aa325c71":"df_pivoted_filled.head()","dd19ee71":"df_sparse = sparse.csr_matrix(df_pivoted_filled.to_numpy())","65297046":"from scipy.sparse.linalg import svds","0a4f29c0":"# de-meaning (account for average user rating)\nuser_ratings_mean = np.mean(df_sparse, axis = 1)\ndf_sparse_demeaned = df_sparse - user_ratings_mean.reshape(-1, 1)","8aee170a":"#The number of factors to factor the user-item matrix.\nnum_factors = 20\n#Performs matrix factorization of the original user item matrix\nU, sigma, Vt = svds(df_sparse_demeaned, k=num_factors)\nsigma = np.diag(sigma)\n\nU, sigma, Vt = U.astype('float32'), sigma.astype('float32'), Vt.astype('float32')","33da390e":"pred_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1,1)\n\n# This method gives us negative values so we need to squeeze it into a (0, 1) range \npred_ratings_norm = (\n    (pred_ratings - pred_ratings.min()) \n    \/ (pred_ratings.max() - pred_ratings.min())\n)                ","dc6d796c":"cf_preds_df = pd.DataFrame(pred_ratings_norm, columns = df_pivoted.columns, index=df_pivoted.index)\ncf_preds_df","0e254e2d":"movie_names=pd.read_csv('\/kaggle\/input\/netflix-prize-data\/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['movieId', 'Year', 'Name'])\nmovie_names.set_index(['movieId', 'Name'],inplace=True)\nmovie_names.head()","3eec72cc":"name_index = movie_names.reindex(cf_preds_df.columns.astype(int), level=0).index.astype('object')\ncf_preds_df.columns = name_index\ncf_preds_df","b8a88e75":"# take a user with >= 5 ratings so we can get a feel for what they like\n\ntest_user_id = (\n    cf_preds_df[df_pivoted.notnull().sum(axis=1) >= 5]\n    .sample(n=1, random_state=41)\n    .index[0]\n)","772bbf1b":"def get_user_ratings(df, movie_names, user_id):\n    \"\"\"Get the ratings a given user has already made\"\"\"\n    \n    user_test = df[df['userId'] == user_id]\n    user_test['movieId'] = user_test['movieId'].astype(int)\n    \n    user_test_titles = (\n        user_test\n        .merge(movie_names.reset_index(drop=False), how='left', on='movieId')\n        .loc[:, (['movieId', 'date', 'Name', 'rating'])]\n    )\n    \n    return user_test_titles","765d4f19":"user_ratings = get_user_ratings(df, movie_names, test_user_id)\nuser_ratings","e8542827":"def make_user_recs(cf_preds_df, user_ratings, user_id, num_recs):\n    \"\"\"Make new movie recommendations based on the SVD matrix factorization\"\"\"\n    \n    user_preds = cf_preds_df.loc[user_id]\n    # we have to remove movies the user has already rated\n    recs = (\n        user_preds[~user_preds.index.get_level_values(0).isin(user_ratings.movieId)]\n        .sort_values(ascending = False)\n    )\n    \n    return recs.iloc[:num_recs] ","f5442e74":"make_user_recs(cf_preds_df, user_ratings, test_user_id, 5)","7505dfbf":"Later on we can join the movieId to the title using the linkage in the file `movie_titles.csv`","bf3a2064":"# Step 1: Find a dataset that may solve our problem\n\nNot applicable in this case","1932003b":"We see lots of similar genres, although the recommendations lean a bit towards the horror genre which isn't really present in our user's rated movies.  \n\n**I kind of cherry-picked this example which is why testing your model quantitatively (Step 6) is usually very important**","4743fc79":"# Step 3: Explore and Visualize\n\nThere isn't as much to do here as there might be in more complex datasets with lots of different columns, but we should still get a general feel for our data. ","c87e7ec8":"We see exponential growth in ratings starting in 2003. This makes sense considering Netflix's growth in subscribers during this period:\n\n![](https:\/\/www.cislm.org\/wp-content\/uploads\/2019\/09\/image-17.png)","f5ad0522":"Let's bring in the actual movie names now:","7ba84075":"Now that our file is set, we'll begin working with it in the form of a pandas dataframe. The pandas library is one of the most commonly used data science tools and provide ways to view and manipulate data in tables called 'dataframes'. Think an excel table that you can easily change with code.","ab7eacbe":"## Distribution of Ratings","5640315e":"This person makes a good example as:\n - they only rated things that they liked (4's and 5's)\n - Their taste is pretty limited to a few genres\n \n |Genre|Examples|\n |-----|--------|\n |Comedy|That 70's show, The Sandlot, King of the Hill|\n |Action\/Thriller| The Professional, Planet of the Apes, Scorpion King, Universal Soldier|\n |Science Fiction \/ Fantasy| Planet of the Apes Universal Soldier|\n |Drama| Universal Soldier, Planet of the Apes, The Professional|","6e43a655":"The formatting of the data is as follows\n\n\\[movie id\\]:\n\nCustomer Id, Customer Rating, Date\n\n\n\nAll the ratings under a movie id are for that movie until a new movie id is shown. We need to account for this in how we read the data. In this example we're not going to use all of the provided files to reduce processing time, so we won't have all movies available.","4afa3765":"The instability in earlier years suggests we may have less data then: let's check.","fabab0de":"# Step 5: Develop a Model Based On Our Data\n\n## Latent Factor Models\n\n- break down high dimensional data into lower dimensional 'latent factors' that capture broader categorizations of the data\n- Instead of data on how each user likes each movie, we can learn how much their taste appeals to certain \"profiles\"\n\nRather than learning how each user rated each movie, we can instead learn users preferences for these profiles (of which there are many less than movies). Then we could learn two tables like this, one for users (call it **U**):\n\n||\"Action\"|\"Romance\"|\n|---|---|---|\n|Lazlo|4.2 | 3.1|\n|Nadja|3.7 | 1.5 |\n|Colin|.1 |.7 |\n\nAnd one for movies (call it **V**):\n\n||\"Action\"|\"Romance\"| \n|---|---|---|\n|Nosferatu |2.5 | 1.7|\n|Titanic|2.3 | 4.9 |\n|Paul Blart: Mall Cop 2| 3.5|1.2 |\n\nKeep in mind that the algorithm doesn't learn actual genres as these profiles, and we aren't able to define what the profiles are (only how many of them we learn). What they learn are connections that exist in the data based on common ratings which can sometimes be very similar to genres. \n\n\n## Matrix Factorization and the SVD\n\nThe same way we can factor the number 10 into (5)(2), we can factor our ratings matrix into 2 smaller matrices that, when multiplied together create an approximation of our full ratings data (**R**), except *with the missing values now filled in*. We do this via the Singular Value Decomposition, or SVD. There's a lot of confusing math behind it, but you just need to understand that it lets us factor our matrix the same way you'd factor any number.  \n\nInstead of 10 = (5)(2), we get:  \n\n**R** = **(U)(V)**\n\n**R** is our original matrix. rows = users, columns = movies\n**U** is our users latent factors matrix. rows = users, columns = latent factors (basically categories)\n**V** is our movies latent factors matrix. rows = movies, columns = latent factors \n\nSo our process is this:\n\n1. Pick a number for the 'rank' of our factorization, which amounts to how many latent factors we have.\n2. Use the scipy package to calculate the SVD of our original matrix **R** based on the rank we've chosen\n3. Remultiply our latent factors \"category\" matrices to get a filled in version of our original matrix which predicts how each user would rate each movie.\n\nThen we can use these predicted ratings to recommend new movies based on which are predicted highest for each user!\n\n![](https:\/\/www.researchgate.net\/profile\/Jun-Xu-67\/publication\/321344494\/figure\/fig1\/AS:702109309751298@1544407312766\/Diagram-of-matrix-factorization.png)\n\n**Please Note: This explanation simplifies \/ glosses over a lot of details of the process**","3856da85":"# Step 2: Get the Dataset formatted and loaded","46625841":"Sources:\n\nhttps:\/\/www.kaggle.com\/shubham47\/eda-on-netflix-prize  \nhttps:\/\/www.kaggle.com\/reintegrated\/netflix-movie-prediciton-using-mf  \nhttps:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101  ","f30b4fac":"## Ratings Per User","de63286b":"The data we'll be working with exists in the several combined_data files. We'll need to concatenate the files first. Let's take a look at the beginning of the first file:","f80ba6e3":"# Step 7: Use Our Model\n\nLet's actually recommend some movies!\n\nWe'll pick a user randomly and view the movies that they rated. Then we'll view our recs for them:","f6b9513c":" |Genre|Examples|\n |-----|--------|\n |Comedy|Scary Movie 2, Evolution|\n |Action\/Thriller| Evolution, Lawnmower Man |\n |Science Fiction \/ Fantasy| Star Trek, Evolution, Mothman, Lawnmower Man|\n |Drama| Mothman, Star Trek|\n |Horror| Mothman, Lawnmower Man|","6778f5c1":"We can now reconstruct the original matrix by multiplying the factors together.","b687e50b":"Overall","1e7e9ea7":"# The Data Science Problem-Solving Process\n\n1. Find a dataset that may solve our problem\n2. Get the data formatted and loaded into memory\n3. Explore and Visualize\n4. Clean data and reformat as necessary (often performed iteratively with (3))\n5. Develop a model based on our data\n6. Test our model's performance\n7. Use our model","593de332":"Over time","6a3ed63b":"# Step 4: Clean Data and Reformat As Necessary\n\nWe don't have any missing items or weird formattings, which makes our problem easier. However, we need to reformat our data to a form that will work with the models we're interested in running.\n\n||Nosferatu|Titanic| Paul Blart: Mall Cop 2\n|---|---|---|---|\n|Lazlo|4 | |\n|Nadja| | 1 |\n|Colin| | | 5\n\nWe need one row for each userId and one column for each movie. Because most users won't rate most movies, the vast majority of entries will be missing. Our job, based on the Netflix Prize, will be to predict how each user would rate each movie and fill in all of these blank spaces.","93bfd5dc":"This dataset is almost entirely missing values, which is an issue for the algorithm we're going to talk about in the next step. For now, we'll fill in a missing rating as 0.","47664ed9":"# Step 6: Test Our Model\n\nIn practice we would define a measure of how well our model is performing by testing it to recommend movies for a user based on known ratings to see how it does. So, for example, if we showed it a new users and it was commonly predicting low scores for movies people liked, we'd know it wasn't doing very well. There are quantitative ways to capture this, and we could also test different values for the number of latent factors ('or categories') we'd use. We'll skip this for now.","098b0d76":"You'll notice that these numbers are very similar and very small This is because we filled in all missing values with zero. Despite this, our algorithm should still capture differences in users preferences for different movies. Even if we can't correctly guess *how a user will rate a movie* exactly, we can guess *which movies they will rate more highly than others*, and this is what is truly important for making recommendations."}}