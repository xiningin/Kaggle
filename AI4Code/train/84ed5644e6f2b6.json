{"cell_type":{"74ecd206":"code","7a71bef0":"code","03cb7101":"code","d2434128":"code","e026e412":"code","2ddefde2":"code","7f43f627":"code","c4ecbb0f":"code","3114bcdc":"code","044bb4d7":"code","3b6245ff":"code","f5f1e0fc":"code","d0b36b3a":"code","924a52f0":"code","21047e59":"code","b2c64693":"code","0fd99f43":"code","f07dec28":"code","50aa76ba":"code","41ea24e7":"code","6e968f25":"code","c1200679":"code","f1c5bf63":"code","561f3203":"markdown","6dfc9232":"markdown"},"source":{"74ecd206":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","7a71bef0":"import numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n# import tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\n# Work with phash\nimport imagehash\n\nimport cv2, os\nimport skimage.io as io\nfrom PIL import Image\n","03cb7101":"# train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n# data = '..\/input\/shopee-product-matching\/train_images\/'\n# a = train[train['posting_id'] == 'train_4062952348'].index.values\n# b = train[train['posting_id'] == 'train_2876255851'].index.values\n# import cv2\n\n# import matplotlib.pyplot as plt\n# print(data+train.iloc[a].image.values)\n# img1 = cv2.imread(data + train.iloc[a].image.values[0])\n# img2 = cv2.imread(data + train.iloc[b].image.values[0])\n\n# img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n# img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n\n\n# plt.figure(figsize=(10,5))\n# plt.subplot(1,2,1)\n# plt.imshow(img1)\n\n# plt.subplot(1,2,2)\n# plt.imshow(img2)","d2434128":"# print(train.iloc[a,])\n# print(train.iloc[b,])","e026e412":"class CFG:\n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes1 = 11014\n    classes2 = 28735\n    \n    model_name1 = 'eca_nfnet_l0'\n    model_path1 = '..\/input\/shopee-pytorch-models\/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    model_name2 = 'tf_efficientnet_b5'\n    model_path2 = '..\/input\/shopee-hash-model-eff-b5\/arcface_512x512_tf_efficientnet_b5(mish).pt'\n    \n    model_name3 = 'resnext50_32x4d'\n    model_path3 = '..\/input\/shopee-pytorch-models\/arcface_512x512_resnext32x4d.pt'\n    \n    scale = 30 \n    margin = 0.5","2ddefde2":"def read_dataset():\n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n    return df, df_cu, image_paths","7f43f627":"df, df_cu, image_paths = read_dataset()\ndf_cu.head()","c4ecbb0f":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","3114bcdc":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","044bb4d7":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","3b6245ff":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","f5f1e0fc":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self, model_name,\n        n_classes = CFG.classes1,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","d0b36b3a":"class Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1.\/h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1.\/v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","924a52f0":"def get_image_embeddings(image_paths, model_name, model_path, classes):\n    embeds = []\n    \n    model = ShopeeModel(model_name=model_name, n_classes=classes)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","21047e59":"def get_text_predictions(df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)\/\/CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","b2c64693":"df,df_cu,image_paths = read_dataset()\ndf.head()","0fd99f43":"image_embeddings1 = get_image_embeddings(image_paths.values, CFG.model_name1, CFG.model_path1, classes=CFG.classes1)\nimage_embeddings2 = get_image_embeddings(image_paths.values, CFG.model_name2, CFG.model_path2, classes=CFG.classes2)\n# image_embeddings3 = get_image_embeddings(image_paths.values, CFG.model_name3, CFG.model_path3)\n\n\n# image_embeddings2 = get_image_embeddings(image_paths.values, CFG.model_name2, CFG.model_path2)\n# image_embeddings3 = get_image_embeddings(image_paths.values, CFG.model_name3, CFG.model_path3)\n\nimage_embeddings = np.concatenate([image_embeddings1 ,image_embeddings2], axis=1)\n\n# image_embeddings = ( image_embeddings1 + image_embeddings2)\/2","f07dec28":"image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.3)\n\ntext_predictions = get_text_predictions(df, max_features = 25_000)","50aa76ba":"# def simple_match(dataset, element):\n#     \"\"\"\n#     A function that returns match names.\n#     Takes dataset and i element.\n#     \"\"\"\n#     match_list = []\n#     matches = dataset[dataset['image_phash'] == \n#                       dataset['image_phash'][element]]['posting_id'].drop(element).values\n#     str_matches = ''\n#     str_matches = str_matches + dataset.iloc[element, 0] + ' '\n#     for j in matches:\n#         str_matches = str_matches + j + ' '\n#     match_list.append(str_matches[:-1])\n    \n#     return match_list","41ea24e7":"# test_for_s = df.loc[:2, ['posting_id', 'image_phash']]\n# test_matches = []\n\n# for i in tqdm(range(len(test_for_s)), desc = 'Progress:', \n#                    position = 0, leave = True):\n#     test_matches.append(np.array(simple_match(test_for_s, i)))\n    \n# test_matches","6e968f25":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x))\n    ","c1200679":"df['image_predictions'] = image_predictions\n\n\ndf['text_predictions'] = text_predictions\n\ndf['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","f1c5bf63":"df.head()","561f3203":"Thanks you so much for reading this notebook. If you have any suggestions or ideas on ensembling models together then do let me know. \ud83d\ude01","6dfc9232":"# Architecture : Eca-nfnet-l0 \/ Efficient-B3 + Arcface Module\n\nHello everyone, this notebook is just an ensemble of two high performing models trained by [Parth Dhameliya](https:\/\/www.kaggle.com\/parthdhameliya77)\n\n### Important Links:\n- [Original inference notebook](https:\/\/www.kaggle.com\/parthdhameliya77\/pytorch-eca-nfnet-l0-image-tfidf-inference)\n- You can find the training notebook [here](https:\/\/www.kaggle.com\/parthdhameliya77\/shopee-pytorch-eca-nfnet-l0-image-training)\n- All the models are present [here](https:\/\/www.kaggle.com\/parthdhameliya77\/shopee-pytorch-models)\n\nA huge thanks to [Parth Dhameliya](https:\/\/www.kaggle.com\/parthdhameliya77) for all his work. I will highly recommend to upvote his notebook as well, if you find this notebook helpful.\n\nLets get started . . . \n\nI tried different ways of ensembling the model. The below approach worked for me. \n\n# > Refer Link https:\/\/www.kaggle.com\/ankursingh12\/shopee-ensemble-b3-nfnet-l0-inference\n# the notebook above is original notebook, you can upvote this!\n# I just copy his notebook! this is his effort!"}}