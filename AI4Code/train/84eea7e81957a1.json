{"cell_type":{"2bedb86c":"code","98648fff":"code","be4720a0":"code","2ff54227":"code","2fba973f":"code","ed882eee":"code","8d5fafdd":"code","f894c439":"code","16c95ecc":"code","b4a83681":"code","e45e2c0c":"code","9a3b60bf":"code","1066da05":"code","59954854":"code","693ab841":"code","e85fa688":"code","c171fcc4":"code","82b541b3":"code","27fe71f4":"code","7914a6fe":"code","35158f4f":"code","646a3e37":"markdown","3869a53f":"markdown","f893262c":"markdown","2104be71":"markdown","e6be5689":"markdown","1c685a69":"markdown","a1e84236":"markdown","3b2fc0bf":"markdown","b8a14ae1":"markdown","826a034c":"markdown","51349f69":"markdown","3a0183e0":"markdown","56e906dd":"markdown","2a6f8a11":"markdown"},"source":{"2bedb86c":"import pandas as pd\nimport numpy as np\nimport re\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n%matplotlib inline\n\nwarnings.simplefilter(action='ignore', category=Warning)\npd.options.mode.chained_assignment = None","98648fff":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\", usecols=[\"title\", \"subject\", \"date\"]).copy()\nreal = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\", usecols=[\"title\", \"subject\", \"date\"]).copy()","be4720a0":"fake[\"label\"] = \"fake\"\nreal[\"label\"] = \"real\"\nfake.head()","2ff54227":"real.head()","2fba973f":"news = pd.concat([fake, real], axis=0).sample(frac=1, random_state=1).reset_index(drop=True)\nnews.head()","ed882eee":"news.label.value_counts(dropna=False)","8d5fafdd":"news.subject.value_counts(dropna=False)","f894c439":"news[\"subject\"] = news[\"subject\"].replace({\"politicsNews\": \"Politics\",\n                                           \"worldnews\": \"World\",\n                                           \"politics\": \"Politics\",\n                                           \"News\": \"All\",\n                                           \"left-news\": \"Left\", \n                                           \"Government News\": \"Government\",\n                                           \"US_News\": \"US\",\n                                           \"Middle-east\": \"Middle East\"})\nnews.subject.value_counts()","16c95ecc":"news.date.value_counts(dropna=False)","b4a83681":"news[news.date.str.extract(r\"^((?!\\w+ \\d+, \\d+))*\", expand=False).notnull()]","e45e2c0c":"news.loc[news.date.str.extract(r\"^((?!\\w+ \\d+, \\d+))*\", expand=False).notnull(), \"date\"] = np.nan","9a3b60bf":"news[news.date.isnull()].shape[0]","1066da05":"news.info()","59954854":"news.date = pd.to_datetime(news.date, errors=\"coerce\")\nnews_grouped = news[[\"date\", \"subject\", \"label\"]].groupby([\"date\", \"label\"]).count().reset_index()\n\nfig, ax = plt.subplots(figsize=(16,10))\nsns.lineplot(x=\"date\", y=\"subject\", hue=\"label\", data=news_grouped, palette=\"Set2\", ax=ax)\nplt.title(\"News Articles Labelled Fake vs. Real\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Count\")","693ab841":"news_group_by_subj_and_label = news.groupby(by=[\"label\", \"subject\"]).count().reset_index()\n\nfig1, ax1 = plt.subplots(figsize=(16, 8))\nsns.barplot(x=\"subject\", y=\"title\", hue=\"label\", data=news_group_by_subj_and_label, palette=\"Set2\", saturation=0.5, ax=ax1)\nplt.title(\"Fake vs. Real News Articles by Subjects\")\nplt.xticks(rotation=45, horizontalalignment='center', fontweight='light', fontsize='x-large')\nplt.yticks(horizontalalignment='center', fontweight='light', fontsize='large')\nplt.xlabel(\"Subjects\", fontsize=\"large\")\nplt.ylabel(\"Amount\", fontsize=\"large\")\nplt.legend(fontsize=\"large\")","e85fa688":"news_clean = news[news.date > dt.datetime(2016,1,1)]\nnews_clean.date.value_counts().sort_index()","c171fcc4":"news_clean = news_clean[news_clean.subject.isin([\"All\", \"Politics\", \"World\"])]\nnews_clean.subject.value_counts()","82b541b3":"news_clean.shape[0]","27fe71f4":"training_data, testing_data = train_test_split(news_clean, random_state=1) #seed for reproducibility\n\nY_train = training_data[\"label\"].values\nY_test = testing_data[\"label\"].values\n\ndef word_counter(data, column, training_set, testing_set):\n\n    cv = CountVectorizer(binary=False, max_df=0.95)\n    cv.fit_transform(training_data[column].values)\n    \n    train_feature_set = cv.transform(training_data[column].values)\n    test_feature_set = cv.transform(testing_data[column].values)\n    \n    return train_feature_set, test_feature_set, cv\n\nX_train, X_test, feature_transformer = word_counter(news_clean, \"title\", training_data, testing_data)","7914a6fe":"classifier = LogisticRegression(solver=\"newton-cg\", C=5, penalty=\"l2\", multi_class=\"multinomial\", max_iter=1000)\nmodel = classifier.fit(X_train, Y_train)","35158f4f":"predictions = model.predict(X_test)\naccuracy = accuracy_score(Y_test, predictions, normalize=True)\nprint(\"Our model has {}% prediction accuracy.\".format(round(accuracy, 2) * 100))","646a3e37":"# Analyzing, Visualizing & Classifying Fake & Real News Data with Logistic Regression\n\nIn this project, we aim to analyze and visualize news data before predicting whether the article is fake or real news. We will be using Logistic Regression for classification purposes since it's good at text-classifying. Data used can be found [here](https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset).\n\nThe data is split into two datasets, containing 44898 articles in total. The columns include:\n\n* title: Contains the title of each article.\n* text: Contains the context of each article.\n* subject: Contains the subject of each article.\n* date: Contains the date each article was posted in a Month DD, YYYY format.\n\n## 1. Reading the data in\n\nWe will not be using the text column for this project. For other columns, we will be familiarizing ourselves with them in this step. Let's import, concatenate and explore the datasets.","3869a53f":"## 6. Training the model, prediction & accuracy\n\nNow that we have prepared our features, we will be training our model. Then, we will predict values for our testing set. We will be using the accuracy metric after that. Accuracy is measured using the formula below:\n\n\\begin{equation}\n\\text{Accuracy} = \\frac{\\text{number of correctly classified articles}}{\\text{total number of classified articles}}\n\\end{equation}","f893262c":"As we can see, \"real\" article group contains articles that have either \"Politics\" or \"World\" as their subjects while \"fake\" articles consist of a variety of subjects. Fake news articles seem to have a tendency to belong into the subject category \"All\", followed by \"Politics\" whereas not showing any interest in \"World\" category. In the meanwhile, real news articles belong into either one of these categories: \"Politics\" and \"World\". Both real and fake articles have many articles in \"Politics\" category. \n\n## 4. Further Preprocessing\n\nBefore we move on to the classification, we will be working on the data further.\n\nOur first step will be dropping some data. We will drop articles that were released before 2016 because there are no real news articles in that period, therefore, the data is not representative enough. Talking about representation, we will also be dropping articles from subjects \"Government\", \"Left\", \"Middle East\" and \"US\". That's because the only common subject both fake and real articles have is \"Politics\" and we will be dropping too much data if we drop articles with \"All\" and \"World\" subjects. Also, their frequencies and names also indicate that the \"All\" subject for fake articles might be the equivalent of the \"World\" subject in real news. ","2104be71":"Here are some observations about our recent exploration:\n\n* The articles that have falsely formatted dates and other values for dates are all labelled fake.\n* They are mostly about politics.\n\nWhat we will do is, we will replace these values with null values.","e6be5689":"## 7. Conclusion\n\nThrough this project, we have cleaned, analyzed, visualized and classified fake and real news articles. \n\n* We have cleaned our data to prepare it for further analysis.\n* Later, we have visualized and analyzed the data before fitting it to make sure that our data is ready.\n* We assigned term-frequencies of the words in the title column as their weights while feature weighting.\n* We have used Multinomial Logistic Regression for classification and accuracy as our metric. Our model successfully classified 96% of the testing data in the end.","1c685a69":"We have labelled the data, we can now combine the datasets. We will do sampling on the entire dataset after combining with a seed. We will use the seed \"1\" for the random number generator for reproducable results.","a1e84236":"Now that we have combined the data, we will first clean and prepare it.\n\n## 2. Preprocessing the data\n\nWe will first take a look at the columns subject and date.","3b2fc0bf":"Before classification, we will visualize our data to see if we're missing anything so far. We will do some further preprocessing later on.","b8a14ae1":"As we can see, there are some values which are not dates. We will explore these values.","826a034c":"As we are able to see from the line chart above, we have no real news article data prior to the beginning of 2016. However, even after that, the amount of fake news articles is dominating compared to the amount of real news articles for a while. We see a sudden increase in number of real news articles in the last quarter of 2016 followed by another unexpected peak nearing May, 2017. Afterwards, we see a sudden and massive increase in the amount of real articles' data with a more subtle decrease in number of fake articles over time. \n\nLet's take a look at the subjects.","51349f69":"As we can see, there are no missing values in this column. However, some subject names are written in camel case with others written in snake case and more... We will replace the column names to fix this problem.","3a0183e0":"As we can see, we still ended up with a decent amount of data! As a next step, we will be working on our features. \n\n## 5. Feature Weighting\n\nAs our features, we are going to use words in the article titles. We will be assigning weights to these words using term-frequencies of them. We will begin with splitting the dat into training and testing sets.","56e906dd":"We will now take a look at the dates.","2a6f8a11":"## 3. Visualizing the data for gaining further insights\n\nWe will now visualize and analyze the data we have to see if there are any patterns we might be missing. We will begin with visualizing the time series data for frequencies of fake vs. real news articles."}}