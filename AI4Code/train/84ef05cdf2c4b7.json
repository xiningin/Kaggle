{"cell_type":{"ce399b09":"code","d5765fe6":"code","94fdefbd":"code","8ba22c9f":"code","af68b467":"code","91d78ccb":"code","401a3f37":"code","2f95ba73":"code","f18b60d1":"code","550cfe23":"code","ae78efc9":"code","f268f746":"code","60e884f0":"code","9c25390c":"code","baa98776":"code","85561e6c":"code","cba6130d":"code","44a60b86":"code","a22d129c":"code","6d33ebc5":"code","7a05d104":"code","2a84395b":"code","f097d830":"code","8c987694":"code","153cb7cf":"code","13abe3f2":"code","5a5ed3f1":"code","c01a9ac8":"code","05371eaf":"code","4b42bdff":"markdown"},"source":{"ce399b09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.|\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier","d5765fe6":"X_train=pd.read_csv('..\/input\/X_train.csv')\ny_train=pd.read_csv('..\/input\/y_train.csv')\nX_test=pd.read_csv('..\/input\/X_test.csv')\ny_test=pd.read_csv('..\/input\/sample_submission.csv')","94fdefbd":"X_train.describe()","8ba22c9f":"y_train.describe()","af68b467":"X_test.describe()","91d78ccb":"y_test.describe()","401a3f37":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","2f95ba73":"def perform_euler_factors_calculation(df):\n    df['total_angular_velocity'] = np.sqrt(np.square(df['angular_velocity_X']) + np.square(df['angular_velocity_Y']) + np.square(df['angular_velocity_Z']))\n    df['total_linear_acceleration'] = np.sqrt(np.square(df['linear_acceleration_X']) + np.square(df['linear_acceleration_Y']) + np.square(df['linear_acceleration_Z']))\n    df['total_xyz'] = np.sqrt(np.square(df['orientation_X']) + np.square(df['orientation_Y']) +\n                              np.square(df['orientation_Z']))\n    df['acc_vs_vel'] = df['total_linear_acceleration'] \/ df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angle'] = np.sqrt(np.square(df['euler_x']) + np.square(df['euler_y']) + np.square(df['euler_z']))\n    df['angle_vs_acc'] = df['total_angle'] \/ df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] \/ df['total_angular_velocity']\n    return df","f18b60d1":"def perform_feature_engineering(df):\n    df_out = pd.DataFrame()\n    \n    for col in df.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        df_out[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        df_out[col + '_min'] = df.groupby(['series_id'])[col].min()\n        df_out[col + '_max'] = df.groupby(['series_id'])[col].max()\n        df_out[col + '_std'] = df.groupby(['series_id'])[col].std()\n        df_out[col + '_mad'] = df.groupby(['series_id'])[col].mad()\n        df_out[col + '_med'] = df.groupby(['series_id'])[col].median()\n        df_out[col + '_skew'] = df.groupby(['series_id'])[col].skew()\n        df_out[col + '_range'] = df_out[col + '_max'] - df_out[col + '_min']\n        df_out[col + '_max_to_min'] = df_out[col + '_max'] \/ df_out[col + '_min']\n        df_out[col + '_mean_abs_change'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df_out[col + '_mean_change_of_abs_change'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.diff(np.abs(np.diff(x)))))\n        df_out[col + '_abs_max'] = df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        df_out[col + '_abs_min'] = df.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        df_out[col + '_abs_mean'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(x)))\n        df_out[col + '_abs_std'] = df.groupby('series_id')[col].apply(lambda x: np.std(np.abs(x)))\n        df_out[col + '_abs_avg'] = (df_out[col + '_abs_min'] + df_out[col + '_abs_max'])\/2\n        df_out[col + '_abs_range'] = df_out[col + '_abs_max'] - df_out[col + '_abs_min']\n\n    return df_out","550cfe23":"X_train = perform_euler_factors_calculation(X_train)","ae78efc9":"X_test = perform_euler_factors_calculation(X_test)","f268f746":"X_train.shape, X_test.shape","60e884f0":"X_train = perform_feature_engineering(X_train[X_train.columns.values[:13]])","9c25390c":"X_test = perform_feature_engineering(X_test[X_test.columns.values[:13]])","baa98776":"print(\"Train X: {}\\nTrain y: {}\\nTest X: {}\".format(X_train.shape, y_train.shape, X_test.shape))","85561e6c":"le = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","cba6130d":"scaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))","44a60b86":"folds = StratifiedKFold(n_splits=49, shuffle=True, random_state=41)","a22d129c":"folds","6d33ebc5":"sub_preds_rf = np.zeros((X_test_scaled.shape[0], 9))\noof_preds_rf = np.zeros((X_train_scaled.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled, y_train['surface'])):\n    clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n    clf.fit(X_train_scaled.iloc[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_rf[val_idx] = clf.predict(X_train_scaled.iloc[val_idx])\n    sub_preds_rf += clf.predict_proba(X_test_scaled) \/ folds.n_splits\n    score += clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_,clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","7a05d104":"sub_preds_rf","2a84395b":"from xgboost import XGBClassifier\nsub_preds_xgboost = np.zeros((X_test_scaled.shape[0], 9))\noof_preds_xgboost = np.zeros((X_train_scaled.shape[0]))\nscore = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled, y_train['surface'])):\n    xgb_clf =  XGBClassifier(n_jobs = -1)\n    xgb_clf.fit(X_train_scaled.iloc[trn_idx], y_train['surface'][trn_idx])\n    oof_preds_xgboost[val_idx] = xgb_clf.predict(X_train_scaled.iloc[val_idx])\n    sub_preds_xgboost += xgb_clf.predict_proba(X_test_scaled) \/ folds.n_splits\n    score += xgb_clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])\n    print('Fold: {} score: {}'.format(fold_,xgb_clf.score(X_train_scaled.iloc[val_idx], y_train['surface'][val_idx])))\nprint('Avg Accuracy', score \/ folds.n_splits)","f097d830":"sub_preds_xgboost","8c987694":"y_test_pred_final = np.array(sub_preds_rf.argmax(axis=1)*0.5+sub_preds_xgboost.argmax(axis=1)*0.5)","153cb7cf":"y_test_pred_final.shape","13abe3f2":"y_test_pred_final = np.array([int(val) for val in y_test_pred_final])","5a5ed3f1":"y_test_pred_final","c01a9ac8":"y_test['surface'] = le.inverse_transform(y_test_pred_final)\ny_test.to_csv('submission_18.csv', index=False)","05371eaf":"y_test","4b42bdff":"**Importing Data Set**"}}