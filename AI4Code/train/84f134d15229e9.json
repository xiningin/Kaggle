{"cell_type":{"d22f4e41":"code","bd820d6e":"code","275c29eb":"code","be646f98":"code","0b788b7f":"code","a6060ce5":"code","fca7127e":"code","33aae84a":"code","a59b6dda":"code","4de9636f":"code","103896e5":"code","6db2f8b5":"code","1902daa5":"markdown"},"source":{"d22f4e41":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\n\nimport random\nimport time\nimport os\n\n\nfrom scipy import signal\nfrom scipy.fft import fftshift\nimport numpy as np\n\n\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n\n# Binary Relevance\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Performance metric\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import cross_val_score\n#from sklearn.metrics import multilabel_confusion_matrix\n#from mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","bd820d6e":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import pinv2","275c29eb":"ACTIONS = [\"kiri\", \"maju\",\"idle\",\"kanan\"]\n#reshape = (-1,8, 60)\n","be646f98":"def create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\"):\n    training_data = {}\n    for action in ACTIONS:\n        if action not in training_data:\n            training_data[action] = []\n        data_dir = os.path.join(starting_dir,action)\n        for item in os.listdir(data_dir):\n            data = np.load(os.path.join(data_dir, item))\n            for item in data:\n                training_data[action].append(item)\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n\n    for action in ACTIONS:\n        np.random.shuffle(training_data[action])  \n        training_data[action] = training_data[action][:min(lengths)]\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n    combined_data = []\n    for action in ACTIONS:\n        for data in training_data[action]:\n            if action == \"kiri\":\n                combined_data.append([data, [1, 0, 0,0]])\n            elif action == \"maju\":\n                combined_data.append([data, [0, 1, 0, 0]])\n            elif action == \"idle\":\n                combined_data.append([data, [0, 0, 1, 0]])\n            elif action == \"kanan\":\n                combined_data.append([data, [0, 0, 0, 1]])\n\n    np.random.shuffle(combined_data)\n    print(\"length:\",len(combined_data))\n    return combined_data\n","0b788b7f":"print(\"creating training data\")\ntraindata = create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\")\ntrain_X = []\ntrain_y = []\n\nfor X, y in traindata:\n    train_X.append(X)\n    train_y.append(y)\n\n","a6060ce5":"train_X =  np.array(train_X)\ntrain_y = np.array(train_y)\n#print(train_X.shape)\n\nx_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nxtn=np.array(x_train).reshape(800,480)\nxtt =np.array(x_test).reshape(200,480)\nytt =np.array(y_test)\nytn =np.array(y_train)\n","fca7127e":"onehotencoder = OneHotEncoder(categories='auto')\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(xtn[:,1:])\ny_train = onehotencoder.fit_transform(ytn[:,:1]).toarray()\nX_test = scaler.fit_transform(xtt[:,1:])\ny_test = onehotencoder.fit_transform(ytt[:,:1]).toarray()\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","33aae84a":"input_size = X_train.shape[1]\nhidden_size = 100\ninput_weights = np.random.normal(size=[input_size,hidden_size])\nbiases = np.random.normal(size=[hidden_size])","a59b6dda":"def relu(x):\n    return np.maximum(x, 0, x)","4de9636f":"def hidden_nodes(X):\n    G = np.dot(X, input_weights)\n    G = G + biases\n    H = relu(G)\n    return H\n\noutput_weights = np.dot(pinv2(hidden_nodes(X_train)), y_train)","103896e5":"def predict(X):\n    out = hidden_nodes(X)\n    out = np.dot(out, output_weights)\n    return out","6db2f8b5":"prediction = predict(X_test)\ncorrect = 0\ntotal = X_test.shape[0]\n\n\nfor i in range(total):\n    predicted = np.argmax(prediction[i])\n    actual = np.argmax(y_test[i])\n    correct += 1 if predicted == actual else 0\naccuracy = correct\/total\nprint('Accuracy for ', hidden_size, ' hidden nodes: ', accuracy)","1902daa5":"# DATA"}}