{"cell_type":{"085c9db0":"code","b04fcc23":"code","1244147d":"code","72839e40":"code","176e7cef":"code","f4aca0cd":"code","cefc7e95":"code","17f5071f":"code","9d558800":"code","fb3d98a2":"code","9a7171fe":"code","8a8ee7d1":"code","97e9f1f9":"code","1ffb2c87":"code","255b5523":"code","b0096e84":"code","4c1ef965":"code","e1f5dfc9":"code","c81da497":"code","2f3f4260":"code","65fc4fbc":"code","5288e30e":"code","3318927b":"code","8dc7ece2":"code","945339f2":"code","1310edbd":"code","3bab8a8d":"code","8af8c47a":"code","3f0307d4":"code","19ffa73f":"code","32a41d30":"code","4dde52a5":"code","672cccea":"code","763e4762":"code","33abafaa":"code","622651c6":"code","f56cb382":"code","557d5e06":"code","e11f9f3e":"code","ffcf4405":"markdown","6dde6d2a":"markdown","b72d82b6":"markdown","d7ba201a":"markdown","e860b9a2":"markdown","b5b755d9":"markdown","f3c7c054":"markdown","72681eb5":"markdown","1a80ac98":"markdown","b81cd7ed":"markdown","5cbade0a":"markdown","fcd9110c":"markdown","7ea66bee":"markdown","bc1c1de8":"markdown","a12fbac0":"markdown","2815409b":"markdown","52e11c4a":"markdown","7e78b1a9":"markdown","1690c521":"markdown","a227e750":"markdown","1b9ef030":"markdown","62310464":"markdown","b4fbd5f2":"markdown","523597b6":"markdown","b32ee27b":"markdown","20fb867b":"markdown","37fd0cc7":"markdown","3d5bd270":"markdown"},"source":{"085c9db0":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b04fcc23":"data = pd.read_csv(\"..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv\")","1244147d":"data.head()","72839e40":"\nplt.figure(figsize=[15,17])\nfft=['Country','Firstname','Sex','Age','Category','Survived']\nn=1\nfor f in fft:\n    plt.subplot(5,3,n)\n    sns.countplot(x=f, hue='Survived', edgecolor=\"black\", alpha=0.7, data=data)\n    sns.despine()\n    plt.title(\"Countplot of {}  by Age\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()\n\n\n    \n","176e7cef":"plt.figure(figsize=[15,4])\nsns.countplot(x='Category', hue='Survived',edgecolor=\"black\", alpha=0.7, data=data)\nsns.despine()\nplt.title(\"Countplot of category by survived\")\nplt.show()","f4aca0cd":"plt.figure(figsize=[15,4])\nsns.countplot(x='Sex', hue='Survived',edgecolor=\"black\", alpha=0.7, data=data)\nsns.despine()\nplt.title(\"Countplot of sex by survived\")\nplt.show()","cefc7e95":"plt.figure(figsize=[15,4])\nsns.countplot(x='Age', hue='Survived',edgecolor=\"black\", alpha=0.7, data=data)\nsns.despine()\nplt.title(\"Countplot of age by survived\")\nplt.show()","17f5071f":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\ncd = data['Survived'].value_counts().reset_index()\ncd.columns = [\n    'Survived', \n    'count'\n]\ncd['Survived'] = cd['Survived'].astype(str) + '-'\ncd = cd.sort_values(['count']).tail(50)\n\nfig = px.bar(\n    cd, \n    x='count', \n    y='Survived', \n    orientation='h', \n    title='Count: Survived', \n    width=1000,\n    height=900 \n)\n\nfig.show()","9d558800":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"Age\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of age\")","fb3d98a2":"sns.set_style(\"ticks\")\nsns.pairplot(data,hue=\"Survived\",size=3);\nplt.show()","9a7171fe":"#plot outcome by age\nsns.boxplot(x=\"Survived\",y=\"Age\",data=data)\nplt.show()","8a8ee7d1":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nep = data['Survived'].value_counts().reset_index()\nep.columns = [\n    'Survived', \n    'percent'\n]\nep['percent'] \/= len(data)\n\nfig = px.pie(\n    ep, \n    names='Survived', \n    values='percent', \n    title='Survived', \n    width=800,\n    height=500 \n)\n\nfig.show()","97e9f1f9":"display(data[[\"PassengerId\",\"Country\",\"Age\",\"Survived\"]].groupby([\"PassengerId\",\"Age\",\"Country\"]).agg(\"sum\").sort_values(by=\"PassengerId\",\n                                                          ascending = False).head(100).style.background_gradient(cmap='autumn'))","1ffb2c87":"data = data[['Sex','Age','Category', 'Survived']] #Subsetting the data\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","255b5523":"#C : Crew, P :  Passenger\nCategory = {'C': 1,'P': 2} \n  \n# traversing through dataframe \n# values where key matches \ndata.Category = [Category[item] for item in data.Category] \nprint(data)","b0096e84":"#M: Male , F:Female\nSex = {'M': 1,'F': 2} \n  \n# traversing through dataframe \n# Gender column and writing \n# values where key matches \ndata.Sex = [Sex[item] for item in data.Sex] \nprint(data)","4c1ef965":"from sklearn.model_selection import train_test_split\nY = data['Survived']\nX = data.drop(columns=['Survived'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","e1f5dfc9":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","c81da497":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","2f3f4260":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","65fc4fbc":"test_acc_rfcla = round(rfcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","5288e30e":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","3318927b":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","8dc7ece2":"test_acc_knncla = round(knncla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_knncla = round(knncla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","945339f2":"#Accuracy\nmodel1 = pd.DataFrame({\n    'Model': ['Random Forest','KNN'],\n    'Train Score': [train_acc_rfcla, train_acc_knncla],\n    'Test Score': [test_acc_rfcla, test_acc_knncla]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","1310edbd":"Y1 = data['Survived']\nX1 = data.drop(columns=['Category'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","3bab8a8d":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Sex, Age, Survived')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","8af8c47a":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(0,3))) \nprint(variance[10:90])","3f0307d4":"X1 = data[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=10)","19ffa73f":"# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm1 = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)","32a41d30":"\nTestscores1 = pd.Series([score1_rfcla], index=['Random Forest Score']) \nprint(Testscores1)","4dde52a5":"Y1 = data['Survived']\nX1 = data.drop(columns=['Age'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","672cccea":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Sex, Category, Survived')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","763e4762":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(0,3))) \nprint(variance[10:90])","33abafaa":"X1 = data[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=10)","622651c6":"# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm2 = confusion_matrix(Y1_test, Y1_predict5)\nscore2_rfcla = rfcla.score(X1_test, Y1_test)","f56cb382":"Testscores2 = pd.Series([score2_rfcla], index=['Random Forest Score']) \nprint(Testscores2)","557d5e06":"All_Score = pd.Series([test_acc_rfcla,score1_rfcla,score2_rfcla], index=['Random Forest: all features','Random Forest : Age, Sex',\n                                                                        'Random Forest: Sex and Category']) \nprint(All_Score)","e11f9f3e":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Random Forest: all features') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('Random Forest : Age, Sex')\nax5 = fig.add_subplot(3, 3,3)\nax5.set_title('Random Forest: Sex and Category')\n\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=rfcla_cm1, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=rfcla_cm2, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax5)\nplt.show()","ffcf4405":"## 1. Random Forest Classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","6dde6d2a":"# Comparison of feature selection\n","b72d82b6":"**Traversing through dataframe**\nwe traversing category and sex to know the correlation","d7ba201a":"## c. Accuracy","e860b9a2":"# Prediction","b5b755d9":"# Prepocessing","f3c7c054":"In here I drop Category from data and then use features :Age and Sex\n","72681eb5":"# 5.Features Selection\n\n","1a80ac98":"Data for training and testing to select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20%, assumed it ideal ratio between training and testing","b81cd7ed":"![https:\/\/storage.googleapis.com\/kaggle-datasets-images\/800154\/1372289\/dc03fd2f456a647790cc4feaee35b2bc\/dataset-cover.jpg?t=2020-07-27-08-58-20](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/800154\/1372289\/dc03fd2f456a647790cc4feaee35b2bc\/dataset-cover.jpg?t=2020-07-27-08-58-20)","5cbade0a":"* The classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","fcd9110c":"# Countplot of category by survived","7ea66bee":"Introduction\nOn September 27 1994 the ferry Estonia set sail on a night voyage across the Baltic Sea from the port of Tallin in Estonia to Stockholm. She departed at 19.00 carrying 989 passengers and crew, as well as vehicles, and was due to dock at 09.30 the following morning, Tragically, the Estonia never arrived.\n\nThe weather was typically stormy for the time of year but, like all the other scheduled ferries on that day, the Estonia set off as usual. At roughly 01:00 a worrying sound of screeching metal was heard, but an immediate inspection of the bow visor showed nothing untoward. The ship suddenly listed 15 minutes later and soon alarms were sounding, including the lifeboat alarm. Shortly afterwards the Estonia rolled drastically to starboard. Those who had reached the decks had a chance of survival but those who had not were doomed as the angled corridors had become death traps. A Mayday signal was sent but power failure meant the ship\u2019s position was given imprecisely. The Estonia disappeared from the responding ships\u2019 radar screens at about 01:50.\n\nThe Marietta arrived at the scene at 02:12 and the first helicopter at 03:05. Of the 138 people rescued alive, one died later in hospital.\n\nOf the 310 people who had reached the decks, almost a third died of hypothermia. The final death toll was shockingly high \u2013 more than 850 people.\n\nAn official inquiry found that failure of the locks on the bow visor, which broke away under the punishing waves, caused water to flood the car deck and quickly capsize the ship. The report also noted a lack of action, delay in sounding the alarm, lack of guidance from the bridge and a failure to light distress flares.\n\nsource dataset :https:\/\/www.kaggle.com\/christianlillelund\/passenger-list-for-the-estonia-ferry-disaster","bc1c1de8":"## b.KNN","a12fbac0":"* The classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","2815409b":"# 2. K-Nearest Neighbour (KNN) ?\n\nis a non-parametric method proposed by Thomas Cover used for classification and regression.[[1](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n* In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n* In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, normalizing the training data can improve its accuracy dramatically. \n\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1\/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n\n**\nExample**\n\n![https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/220px-KnnClassification.svg.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e7\/KnnClassification.svg\/220px-KnnClassification.svg.png)\n\n\nAs you can see above on the Example of k-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).\n\n\n**Parameter selection**\n\nThe best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification[[1](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)] but make boundaries between classes less distinct. \n\n\n**The 1-nearest neighbor classifier**\n\n\nThe most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is ![https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f815904edbff2ce82502172ec0dce3311d57f2bb](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f815904edbff2ce82502172ec0dce3311d57f2bb)\n\nAs the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).","52e11c4a":"# Countplot of age by survived","7e78b1a9":"# Model\n\nHere I used Random Forest Classifier and K-Nearest Neighbour (KNN)","1690c521":"## a. Random Forest","a227e750":"## Plotting Heatmap\n\nAs you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation","1b9ef030":"In here I drop Age from data and then use features :Category and Sex\n ","62310464":"# train_test_split","b4fbd5f2":"read dataset","523597b6":"# Visualize the data","b32ee27b":"## [](http:\/\/)","20fb867b":"import library","37fd0cc7":"Variable Prediction:\n\n\n\n* Sex\tGender of passenger: M = Male, F = Female\n* Age\t                   :Age of passenger at the time of sinking\t\n* Category\t               :The type of passenger\tC = Crew, P = Passenger\n* Survived\t               : Survival\t0 = No, 1 = Yes","3d5bd270":"# Countplot of sex by survived"}}