{"cell_type":{"e51b22ee":"code","87a3e4ab":"code","c8620d00":"code","75973864":"code","c9596e6f":"code","691f16ad":"code","69cb9fb3":"code","7130edad":"code","d8ef4d8d":"code","ed59d7eb":"markdown"},"source":{"e51b22ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom scipy.ndimage import affine_transform\nfrom PIL import Image as pil_image\nfrom PIL import ImageDraw as pil_draw\n\nfrom keras.models import load_model\nfrom keras.preprocessing.image import img_to_array, array_to_img\nimport keras.backend as K\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n# Any results you write to the current directory are saved as output.","87a3e4ab":"TRAIN_PATH = \"..\/input\/humpback-whale-identification\/train\"\nTEST_PATH = \"..\/input\/humpback-whale-identification\/test\"\nMODEL_PATH = \"..\/input\/humpback-bb-martinpiotte\/cropping.model\"","c8620d00":"# load the pretrained model\nmodel = load_model(MODEL_PATH)","75973864":"train_paths = [img for img in os.listdir(TRAIN_PATH)]\ntest_paths = [img for img in os.listdir(TEST_PATH)]","c9596e6f":"# define useful constants\nimg_shape = (128,128,1)\nanisotropy = 2.15\n\ndef center_transform(affine, input_shape):\n    hi, wi = float(input_shape[0]), float(input_shape[1])\n    ho, wo = float(img_shape[0]), float(img_shape[1])\n    top, left, bottom, right = 0, 0, hi, wi\n    if wi\/hi\/anisotropy < wo\/ho: # input image too narrow, extend width\n        w     = hi*wo\/ho*anisotropy\n        left  = (wi-w)\/2\n        right = left + w\n    else: # input image too wide, extend height\n        h      = wi*ho\/wo\/anisotropy\n        top    = (hi-h)\/2\n        bottom = top + h\n    center_matrix   = np.array([[1, 0, -ho\/2], [0, 1, -wo\/2], [0, 0, 1]])\n    scale_matrix    = np.array([[(bottom - top)\/ho, 0, 0], [0, (right - left)\/wo, 0], [0, 0, 1]])\n    decenter_matrix = np.array([[1, 0, hi\/2], [0, 1, wi\/2], [0, 0, 1]])\n    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n\ndef transform_img(x, affine):\n    matrix   = affine[:2,:2]\n    offset   = affine[:2,2]\n    x        = np.moveaxis(x, -1, 0)\n    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n                                 mode='constant', cval=np.average(channel)) for channel in x]\n    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n\ndef read_raw_image(p):\n    return pil_image.open(p)\n\ndef read_for_validation(x):\n    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t  = center_transform(t, x.shape)\n    x  = transform_img(x, t)\n    x -= np.mean(x, keepdims=True)\n    x \/= np.std(x, keepdims=True) + K.epsilon()\n    return x, t\n\ndef coord_transform(list, trans):\n    result = []\n    for x,y in list:\n        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n        result.append((x,y))\n    return result\n\ndef read_array(p):\n    img = read_raw_image(p).convert('L')\n    return img_to_array(img)\n\ndef make_bbox(p):\n    raw = read_array(p)\n    width, height = raw.shape[1], raw.shape[0]\n    img,trans         = read_for_validation(raw)\n    a                 = np.expand_dims(img, axis=0)\n    x0, y0, x1, y1    = model.predict(a).squeeze()\n    (u0, v0),(u1, v1) = coord_transform([(x0,y0),(x1,y1)], trans)\n    bbox = [max(u0,0), max(v0,0), min(u1,width), min(v1,height)]\n    if bbox[0] >= bbox[2] or bbox[1] >= bbox[3]:\n        bbox = [0,0,width,height]\n    return bbox\n\ndef transform_coordinate(coords, trans):\n    result = []\n    for x,y in coords:\n        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n        result.append((x,y))\n    return result","691f16ad":"bbox_df = pd.DataFrame(columns=['Image','x0','y0','x1','y1']).set_index('Image')","69cb9fb3":"for img in tqdm(train_paths):\n    bbox_df.loc[img] = make_bbox(TRAIN_PATH+\"\/\"+img)\n              \nfor img in tqdm(test_paths):\n    bbox_df.loc[img] = make_bbox(TEST_PATH+\"\/\"+img)","7130edad":"bbox_df.to_csv(path_or_buf='bounding_box.csv')","d8ef4d8d":"from numpy.linalg import inv as mat_inv\n\n# Type any image name here \nimg_name = \"32318c344.jpg\"\nimg = pil_image.open(TRAIN_PATH+\"\/\"+img_name)\n\nprint(img.size)\n\nx0,y0,x1,y1 = bbox_df.loc[img_name]\nprint(\"Original BB Coordinates\",x0,y0,x1,y1)\n\nimgArr,trans = read_for_validation(read_array(TRAIN_PATH+\"\/\"+img_name))\n(xt0,yt0),(xt1,yt1) = transform_coordinate([(x0,y0),(x1,y1)],mat_inv(trans))\nprint(\"New BB Coordinates\",xt0,yt0,xt1,yt1)\n\nimgNew = array_to_img(imgArr)\nimgNew = imgNew.convert('RGB')\n\n\nimgDraw = pil_draw.Draw(imgNew)\nimgDraw.rectangle([(xt0,yt0),(xt1,yt1)],outline='red')\n\nplt.imshow(imgNew)","ed59d7eb":"## Humpback whale- bounding boxes\n\nThis kernel calculates the co-ordinates of the bounding boxes for train dataset and test dataset. \nThe results are stored in bounding_box.csv.\n\nThis kernel extends @suicaokhoailang 's [this](https:\/\/www.kaggle.com\/suicaokhoailang\/generating-whale-bounding-boxes) kernel by adding utility to visualize the calculated bounding box. "}}