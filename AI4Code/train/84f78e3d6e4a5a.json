{"cell_type":{"d1d86bda":"code","e0564158":"code","6aecddb3":"code","dba8dd74":"code","b3214a9c":"code","7a6d0bf3":"code","bc9b8789":"code","b4fde177":"code","e72c04de":"code","185c544c":"code","13aaaf60":"code","dc82dd7f":"code","e92aefe0":"code","5705c2eb":"code","694a1928":"code","6d3ce18d":"code","024338cf":"code","b56f847a":"code","89344e82":"code","e47ee84b":"code","b6ef9a5e":"code","7234d95d":"code","1522a7ed":"code","102774ec":"code","0df5f536":"code","73332714":"code","114a7c67":"code","dd56feaf":"code","2995d661":"code","951a603b":"code","245fc0ad":"code","e6d76745":"code","fea42278":"code","00793e16":"code","da3c6d67":"code","05b73b31":"markdown","c6d8f8c1":"markdown","7567cd8a":"markdown","e27e5034":"markdown","5306c0bf":"markdown","38fd198f":"markdown","d8719bc4":"markdown","4fbf309c":"markdown","ebfb27c2":"markdown","c1769b46":"markdown","5036741b":"markdown","6b797368":"markdown","a7945450":"markdown","6db8c31e":"markdown","83ed30b0":"markdown","226a0681":"markdown","e202aeed":"markdown","e8afc27f":"markdown","f38b5a62":"markdown","9d9d7a79":"markdown","fc59199d":"markdown","6b63da04":"markdown","c6ea6953":"markdown","4bcfc91b":"markdown","89da2955":"markdown","b4b77fec":"markdown","16479cd5":"markdown"},"source":{"d1d86bda":"# i think we should implement CV on surface and group_id.\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom seaborn import countplot,lineplot, barplot\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom random import shuffle\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\n\n%matplotlib inline","e0564158":"train = pd.read_csv('..\/input\/X_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\ntarget = pd.read_csv('..\/input\/y_train.csv')\nss = pd.read_csv('..\/input\/sample_submission.csv')","6aecddb3":"train.head()","dba8dd74":"train.shape, test.shape","b3214a9c":"countplot(y='surface', data=target)\nplt.show()","7a6d0bf3":"train.shape[0]\/128, test.shape[0]\/128","bc9b8789":"target.groupby('surface')['group_id'].nunique()","b4fde177":"# columns to use for filtering\ncols = train.columns[3:]\ncols","e72c04de":"# HP\/LP Filter\nfrom scipy import signal\n\ndef HP_filter(sig, critical_freq=0.05):\n    b, a = signal.butter(4, critical_freq, analog=False, btype='highpass')\n    out = signal.filtfilt(b, a, sig)\n    sig[:] = out\n    return sig\n\ndef LP_filter(sig, critical_freq=0.2):\n    #b, a = signal.butter(4, critical_freq, analog=False, btype='highpass')\n    b, a = signal.butter(4, critical_freq, analog=False, btype='lowpass')\n    out = signal.filtfilt(b, a, sig)\n    sig[:] = out\n    return sig\n\n\ntemp_sid_sample = 0\nHPF_enabled = True\nif HPF_enabled:\n    fig, ax = plt.subplots(3, 4, figsize=(18, 16) )\n    \n    temp_sid_sample = [train.series_id.sample(1)+ii for ii in np.arange(10)]\n    \n    i=0\n    for col in cols:\n        ax[int(i\/4), int(i%4)].plot( train.loc[ train.series_id.isin(temp_sid_sample), col ].tolist() )\n        if col.startswith('orient'):\n            train[col+'_f'] = train.groupby(['series_id'])[col].apply(LP_filter)\n            test[col+'_f']  = test.groupby(['series_id'])[col].apply(LP_filter)\n        elif col.startswith('angular'):\n            train[col+'_f'] = train.groupby(['series_id'])[col].apply(LP_filter, 0.9)\n            test[col+'_f']  = test.groupby(['series_id'])[col].apply(LP_filter, 0.9)\n        else:\n            train[col+'_f'] = train.groupby(['series_id'])[col].apply(HP_filter)\n            test[col+'_f']  = test.groupby(['series_id'])[col].apply(HP_filter)\n\n        ax[int(i\/4), int(i%4)].plot( np.array(train.loc[ train.series_id.isin(temp_sid_sample), col+'_f' ]) )\n        ax[int(i\/4), int(i%4)].set_title(col)\n        i+=1\n    plt.title(cols[0])","185c544c":"train[train.columns[3:]] = train[train.columns[3:]].abs()\ntest[test.columns[3:]]   = test[test.columns[3:]].abs()","13aaaf60":"# Define is we want to pick filtered values or unfiltered. Remove \"_f\" if unfiltered is desired.\n# Orientation:filtered, angular:filtered, linear:filtered\nfeature_cols = ['orientation_X_f', 'orientation_Y_f', 'orientation_Z_f', 'orientation_W_f',\n       'angular_velocity_X_f', 'angular_velocity_Y_f', 'angular_velocity_Z_f',\n       'linear_acceleration_X_f', 'linear_acceleration_Y_f', 'linear_acceleration_Z_f']","dc82dd7f":"all_cols = ['row_id', 'series_id', 'measurement_number'] + feature_cols\nif HPF_enabled==False:\n    all_cols = train.columns\ntrain = train[all_cols]\ntest  = test[all_cols]\ntrain.shape, test.shape, train.columns","e92aefe0":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef fe(actual, feature_cols):\n    new = pd.DataFrame()\n    if 'angular_velocity_X' in feature_cols:\n        actual['total_angular_velocity'] = (actual['angular_velocity_X'] ** 2 + actual['angular_velocity_Y'] ** 2 + actual['angular_velocity_Z'] ** 2) ** 0.5\n    if 'angular_velocity_X_f' in feature_cols:\n        actual['total_angular_velocity'] = (actual['angular_velocity_X_f'] ** 2 + actual['angular_velocity_Y_f'] ** 2 + actual['angular_velocity_Z_f'] ** 2) ** 0.5\n\n    if 'linear_acceleration_X' in feature_cols:\n        actual['total_linear_acceleration'] = (actual['linear_acceleration_X'] ** 2 + actual['linear_acceleration_Y'] ** 2 + actual['linear_acceleration_Z'] ** 2) ** 0.5\n    if 'linear_acceleration_X_f' in feature_cols:\n        actual['total_linear_acceleration'] = (actual['linear_acceleration_X_f'] ** 2 + actual['linear_acceleration_Y_f'] ** 2 + actual['linear_acceleration_Z_f'] ** 2) ** 0.5\n\n\n    #actual['acc_vs_vel'] = actual['total_linear_acceleration'] \/ actual['total_angular_velocity']\n    \n    if 'orientation_X' in feature_cols:\n        x, y, z, w = actual['orientation_X'].tolist(), actual['orientation_Y'].tolist(), actual['orientation_Z'].tolist(), actual['orientation_W'].tolist()\n    if 'orientation_X_f' in feature_cols:        \n        x, y, z, w = actual['orientation_X_f'].tolist(), actual['orientation_Y_f'].tolist(), actual['orientation_Z_f'].tolist(), actual['orientation_W_f'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    \n    actual['total_angle'] = (actual['euler_x'] ** 2 + actual['euler_y'] ** 2 + actual['euler_z'] ** 2) ** 5\n    \n    actual['angle_vs_acc'] = actual['total_angle'] \/ actual['total_linear_acceleration']\n    actual['angle_vs_vel'] = actual['total_angle'] \/ actual['total_angular_velocity']\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in actual.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        new[col + '_mean'] = actual.groupby(['series_id'])[col].mean()\n        new[col + '_min'] = actual.groupby(['series_id'])[col].min()\n        new[col + '_max'] = actual.groupby(['series_id'])[col].max()\n        new[col + '_std'] = actual.groupby(['series_id'])[col].std()\n        #new[col + '_max_to_min'] = new[col + '_max'] \/ new[col + '_min']\n        \n        # Change. 1st order.\n        new[col + '_mean_abs_change'] = actual.groupby('series_id')[col].apply(f2)\n        \n        # Change of Change. 2nd order.\n        new[col + '_mean_change_of_abs_change'] = actual.groupby('series_id')[col].apply(f1)\n        \n        new[col + '_abs_max'] = actual.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        new[col + '_abs_min'] = actual.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n\n    return new","5705c2eb":"%%time\ntrain_stats = fe(train, train.columns).copy()\ntest_stats  = fe(test, test.columns).copy()\ntrain_stats.head()","694a1928":"train_stats.head()","6d3ce18d":"train_stats.shape, test_stats.shape","024338cf":"le = LabelEncoder()\ntarget['surface'] = le.fit_transform(target['surface'])","b56f847a":"train_stats.fillna(0, inplace = True)\ntest_stats.fillna(0, inplace = True)","89344e82":"train_stats.replace(-np.inf, 0, inplace = True)\ntrain_stats.replace(np.inf, 0, inplace = True)\ntest_stats.replace(-np.inf, 0, inplace = True)\ntest_stats.replace(np.inf, 0, inplace = True)","e47ee84b":"df = pd.merge(train_stats, target, how='inner', on='series_id')","b6ef9a5e":"# Split train\/test data by group_id so that train and test don't have overlapping group_id.\n# For each surface, randomly sample 20% of group_id values and assign them to test samples.\n# The rest goes to train samples. For surface=3, we have 1 group_id. So use 20% of series_id to split.\ndef sample_split_by_groupID(df, test_size=0.2, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    test_gid = np.array([])\n    for k, G in df.groupby('surface'):\n        if k==3:   # Skip 3 since it has only 1 group.\n            continue\n        ch = np.random.choice(G['group_id'].unique(), int(max(1, test_size*G['group_id'].nunique())), replace=False )\n        test_gid=np.append(test_gid, ch)\n\n    # series_id chosen from surface=3\n    test_sid_s3 = np.random.choice( df[df.surface==3].series_id.unique(), int(max(1, 0.2*df[df.surface==3].series_id.nunique())) )\n    X_test  = df[df.group_id.isin(test_gid) | df.series_id.isin(test_sid_s3) ]\n    X_train = df[~df.series_id.isin(X_test.series_id)]\n\n    #print('Train size: ', X_train.shape, '\\tTest size: ', X_test.shape)\n    if (pd.merge(X_train, X_test, how='inner', on='series_id').shape[0] == 0):\n        pass\n    else:\n        raise(ValueError('Train and Test have overlapping series_id. This was not intended by design.'))\n    return X_train, X_test","7234d95d":"clf =  RandomForestClassifier(n_estimators = 200, n_jobs = -1, class_weight=\"balanced_subsample\")\nfeature_cols = df.columns.drop(['group_id', 'series_id', 'surface'])\nclf.fit(df[feature_cols], df['surface'])\nprint('RF training score:', clf.score(df[feature_cols], df['surface']))","1522a7ed":"### Random Forest\n# y_pred and y_test are only for the last X_test. \n# setting max_depth to low values.\n# return score or f_score if needed.\ndef run_RandomForest_CV(df, test_stats, cv=5, test_size=0.2, verbose=True, seed=None):\n    preds_final  = np.zeros((cv, test_stats.shape[0], df.surface.nunique()))\n    y_pred = []\n    y_test = []\n    score = []\n    f_score = []\n    clf =  RandomForestClassifier(n_estimators = 200, n_jobs = -1, class_weight=\"balanced_subsample\", max_depth=10)\n\n    for i in np.arange(cv):\n        X_train, X_test = sample_split_by_groupID(df, test_size, seed)\n        if seed is not None:\n            seed += 1\n\n        if verbose:\n            print('-'*20, i, '-'*20, X_train.shape, X_test.shape)\n            \n        feature_cols = X_train.columns.drop(['group_id', 'series_id', 'surface'])\n\n        clf.fit(X_train[feature_cols], X_train['surface'])\n        \n        y_pred = clf.predict(X_test[feature_cols])\n        y_test = X_test['surface']\n        preds_final += clf.predict_proba(test_stats[feature_cols]) \/ cv\n\n        score = score + [clf.score(X_test[feature_cols], X_test['surface'])]\n        if verbose:\n            print('Train score ', clf.score(X_train[feature_cols], X_train['surface']))\n            print('Test score ', clf.score(X_test[feature_cols], X_test['surface']))\n\n        importances = clf.feature_importances_\n        FI = pd.DataFrame([x for x in zip(clf.feature_importances_, feature_cols)], columns=['FI', 'feature'])\n        if verbose:\n            FI.sort_values(by='FI', ascending=False).iloc[0:min(30, len(FI))].plot.barh(x='feature', y='FI', figsize=(5,10))\n            plt.show()\n        \n        #pdb.set_trace()\n        [_,_, f_x, _] = precision_recall_fscore_support(X_test['surface'].values, y_pred, average='weighted')\n        f_score = f_score + [f_x]\n    if verbose:\n        print('Test Accuracy', [ '%.2f' % elem for elem in score ] )\n        print('F-1 score', [ '%.2f' % elem for elem in f_score ] )\n        print('Min Accuracy\/F-1:', '%.2f' % min(score), '%.2f' % min(f_score))\n    \n    return clf, FI, y_test, y_pred, score  #, y_pred, y_test, val_score","102774ec":"clf, FI, y_test, y_pred, score = run_RandomForest_CV(df, test_stats, cv=5)","0df5f536":"# https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n# Plot Confusion Matrix\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","73332714":"# test data\nplot_confusion_matrix(y_test, y_pred, le.inverse_transform(clf.classes_))","114a7c67":"# Goes through all_cols items one by one. Drops item from features. Runs RF CV. \n# Takes the score as minimum score returned from CV.\n#\ndef feature_elimination(all_cols, cv=5, seed=65789):\n    select_cols = list(all_cols)\n    val_score_base=0.0\n    RFE_df = pd.DataFrame(columns=['Column', 'Val_Score', 'Dropped'])\n\n    _, _, _, _, score = run_RandomForest_CV(df[select_cols+['group_id', 'series_id', 'surface']], test_stats, cv=cv, test_size=0.3, verbose=False, seed=seed)\n    val_score_base = min(score)\n    print('Base score:', val_score_base)\n    print('Column'.ljust(52) +'\\tMin_score\\tAvg Score\\tDropped\\tfeatures')\n\n    for col in all_cols:\n        select_cols.remove(col)\n        dropped = True\n        _, _, _, _, score = run_RandomForest_CV(df[select_cols+['group_id', 'series_id', 'surface']], test_stats, cv=cv, test_size=0.3, verbose=False, seed=seed)\n        val_score = min(score)\n        if val_score_base>val_score * 1.02:\n            # We could not afford to drop this columns since val_score dropped bad.\n            select_cols.append(col)\n            dropped = False\n        # Keep the best score as base\n        val_score_base = max(val_score, val_score_base)\n\n        print(col.ljust(52)+ '\\t'+\"{0:.2f}\".format(val_score)+\"\\t\\t{0:.2f}\".format(np.average(score))+ '\\t\\t'+str(dropped)+'\\t'+str(len(select_cols)) )\n        RFE_df = RFE_df.append( {'Column': col, 'Val_Score':val_score, 'Dropped':dropped} , ignore_index=True)\n    return select_cols, RFE_df","dd56feaf":"select_cols = df.columns.drop(['group_id', 'series_id', 'surface']).tolist()","2995d661":"from random import shuffle\nshuffle(select_cols)\nselect_cols, RFE_df = feature_elimination(select_cols, cv=5, seed=24876)","951a603b":"from random import shuffle\nshuffle(select_cols)\nselect_cols, RFE_df = feature_elimination(select_cols, cv=5, seed=24876)","245fc0ad":"# Lets see which features are remaining.\nselect_cols","e6d76745":"clf, FI, y_test, y_pred, score = run_RandomForest_CV(df[select_cols+['group_id', 'series_id', 'surface']], test_stats, cv=10)","fea42278":"np.average(score)","00793e16":"tr = df[select_cols]\nte = test_stats[select_cols]\ntarget = df[['surface', 'group_id']]\n\n# Taken from https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=546789)\nsub_preds_rf = np.zeros((te.shape[0], 9))\noof_preds_rf = np.zeros((tr.shape[0]))\nscore = 0\nclf =  RandomForestClassifier(n_estimators = 400, n_jobs = -1, class_weight=\"balanced_subsample\", max_depth=12)\nfor i, (train_index, test_index) in enumerate(folds.split(tr, target['surface'])):\n    print('-'*20, i, '-'*20)\n\n    clf.fit(tr.iloc[train_index], target['surface'][train_index])\n    oof_preds_rf[test_index] = clf.predict(tr.iloc[test_index])\n    sub_preds_rf += clf.predict_proba(te) \/ folds.n_splits\n    score += clf.score(tr.iloc[test_index], target['surface'][test_index])\n    print('Train score ', clf.score(tr.iloc[train_index], target['surface'][train_index]))\n    print('Test score ' , clf.score(tr.iloc[test_index], target['surface'][test_index]))\n    importances = clf.feature_importances_\n    indices = np.argsort(importances)\n    features = tr.columns\n\n    FI = pd.DataFrame([x for x in zip(clf.feature_importances_, features)], columns=['FI', 'feature'])\n    FI.sort_values(by='FI', ascending=False).iloc[0:30].plot.barh(x='feature', y='FI', figsize=(5,10))\n    plt.show()\n\nprint('Avg Accuracy', score \/ folds.n_splits)","da3c6d67":"ss['surface'] = le.inverse_transform(sub_preds_rf.argmax(axis=1))\nss.to_csv('submission.csv', index=False)\nss.head(10)","05b73b31":"Blue is the original data. \nOrange is filtered data. \n\nOrientation: \n    - filtered data is similar to unfiltered. Probably some very high frequency points are removed.\n    \nangular velocity:\n    - filtered data mostly similar to unfiltered. Some high frequency points removed.\n    \nlinear acceleration:\n    - we removed bias from data. Only kept the high frequency response. Here, I wanted the information from driving pattern of the driver to be eliminated and it seems that it worked. ","c6d8f8c1":"# I'm leaving this one out. I think this would be great for detecting frequnecy shift on different surfaces. I might come back to this later.\n# FFT plots\nfig, ax = plt.subplots(3, 4, figsize=(18, 16) )\ni=0\nfor temp in ['orientation', 'angular', 'linear']:\n    cols = train.columns[train.columns.str.startswith(temp)]\n    cols = cols[ ~cols.str.endswith('_f')  ]\n    j=0\n    for col in cols:\n        y    = np.array(train[col])\n        Y    = np.fft.fft(y)\n        freq = np.fft.fftfreq(len(y), 1)\n        ps   = np.abs(Y)**2\n        \n        y2    = LP_filter(y, 0.1)        \n        Y2    = np.fft.fft(y2)\n        freq2 = np.fft.fftfreq(len(y2), 1)\n        ps2   = np.abs(Y2)**2\n        \n        ax[i,j].plot(freq, np.abs(Y))\n        ax[i,j].plot(freq2, np.abs(Y2))\n        ax[i,j].set_title(col)\n        #ax[i,j].set_yscale('log')\n        j+=1\n    i+=1","7567cd8a":"This is a multi-class classification problem. It's supervised and has imbalanced classes.\nEach measurement has 128 data points taken over time for each sensor. The identifier for each measurement is series_id. Then each measurement is repeated on the same surface multiple times which is identified by group_id.\n\nThe chart above shows the number of group_id values for each surface in training data. ","e27e5034":"We have 3810 train series, and 3816 test series.","5306c0bf":"Let's remove features one by one and see if the test score improves. \nIf dropping a feature does not degrade the score, then remove it. ","38fd198f":"Let's see what we learned. Depending on seed, this data might be different.\nBut it shows that some features can predict some classes much better.\n\nClasses with good score:\n\n    soft_pvc, soft_tiles, wood\n\nClasses with ok score:\n    \n    concrete, tiled\n\nClasses with terrible score:\n    \n    carpet, fine_concrete, hard_tiles, hard_tiles_large_space\n    ","d8719bc4":"### Let's applyour high pass and low pass filters. \nWhy do we need this? \n\nThe data is noisy as you'll see below. My guess is that most of the important information should be in linear acceleration, because that's what measuring the movements which the robot feels from different surfaces.\nWe don't usually care about low freqeuncy signal in linear acceleration. That's just the driver applying acceleration. Driving on a different surface should not add a bias to linear acceleration so let's remove low frequencies. But the high frequency components should give us what we want. \n\nFor velocity and orientation, I will apply low pass filter to remove noise. We will be using different critical frequncy for orientation and velocity.\n\nLet's see how this goes and if my hunch is justified.","4fbf309c":"## Feature Engineering","ebfb27c2":"That's no good! We have a perfect training score, which means we are overfitting to training data. We will need to eliminate this by using better features, eliminating most of unnecessary features and limiting the random forest depth.  ","c1769b46":"With the selected features, we run a StratifiedKFold on \"surface\" to train and generate output. This one will not be split on group_id, only on surface.","5036741b":"Now that we have a split strategy, we can focuse on our Random Forest algorithm. Let's do a quick RF and see where we are.","6b797368":"Let's repeat feature elimination again.  ","a7945450":"# Feature Elimination","6db8c31e":"Let's pick the filtered columns only. ","83ed30b0":"I used a specific seed so the same set of samples are compared for every feature.\nEvery time we run the above code, it's better to use a different seed. \n\nSo we started with 96 features and base score of 0.37.\nWe ended up with 28 features and score of 0.39. We improved the algo. Note that this was done for a specific seed. If we randomize the seed, it will behave different.","226a0681":"# Let's detect them surfaces!\n\nIn this project our task is to detect the floor surface the robots are moving on. Data is collected from Inertial Measurement Units (IMU). \n\n# What's new here:\n    - I add High Pass and Low Pass filters to filter the IMU data.\n    - I use group_id for better cross validation.\n    - I make the random forest not overfit to data intentionally by limiting it's depth\/impurity stuff.\n    - I use feature elimination to remove most of the features not needed.","e202aeed":"Let's create new features by taking mean, abs, std, etc.\nThis part is taken from other notebooks. See below. ","e8afc27f":"Ok, so we were able to lower training accuracy a bit by limiting max_depth. \nTest accuracy is low because it contains group_id which is not in training. \n\nThe fact that orientation is among the top important features is a bit weird for me. But maybe that's the data. I would have discussed this more in detail with customers to see if it makes sense. ","f38b5a62":"# After feature elimination","9d9d7a79":"Merging train and taget data into a single dataframe. ","fc59199d":"select_cols = ['linear_acceleration_X_f_mean',\n 'orientation_Y_f_min',\n 'orientation_X_f_min',\n 'linear_acceleration_Y_f_abs_max',\n 'angular_velocity_Z_f_abs_max',\n 'angular_velocity_X_f_abs_min',\n 'linear_acceleration_Z_f_abs_max',\n 'orientation_Y_f_mean_change_of_abs_change',\n 'angular_velocity_X_f_mean',\n 'linear_acceleration_X_f_min',\n 'angular_velocity_X_f_std',\n 'orientation_Z_f_abs_min',\n 'orientation_Z_f_max',\n 'angular_velocity_X_f_min',\n 'angular_velocity_Z_f_abs_min',\n 'linear_acceleration_Z_f_mean_change_of_abs_change',\n 'angular_velocity_Y_f_std',\n 'linear_acceleration_Y_f_std',\n 'orientation_W_f_mean_abs_change',\n 'angular_velocity_X_f_max',\n 'total_angular_velocity_abs_max',\n 'orientation_W_f_abs_max',\n 'linear_acceleration_Z_f_mean_abs_change',\n 'angular_velocity_Y_f_min',\n 'angular_velocity_X_f_mean_change_of_abs_change',\n 'total_angular_velocity_min',\n 'angular_velocity_X_f_mean_abs_change',\n 'angular_velocity_Z_f_min']","6b63da04":"## Submission\n\nThis algorithm gives a LB score of 0.6 to 0.7.","c6ea6953":"Let's use absolute values only","4bcfc91b":"First we need to come up with a train\/test split strategy. Most kernels I've seen are using Stratified KFold over surface to ensure an even distribution of data over classes. While this is smart and yields very good test score, it does not generalize well. From my tests on this dataset, if we split the data by group_id, the prediction becomes much harder.\nSince we want this model to generalize and it's fair to assume that group_id will be different in future data (unseen data), it's a good idea to split on group_id and take the hard approach. \n\nWhy not use KFold on group_id? Here's we are trying to balance by surface and group_id. KFold only balances on one of them. ","89da2955":"# StratifiedKFold","b4b77fec":"Performance seems to have improved after feature elimination. We can run feature elimination 1 more time and see if we get better results. I'll skip that today.","16479cd5":"## Random Forest"}}