{"cell_type":{"a5800c3c":"code","7048ff38":"code","b3076fe0":"code","c224b996":"code","03cd03ab":"code","c32c6c0e":"code","b7b9aaf2":"code","9d844f0f":"code","d2b0610c":"code","4835a306":"code","ba42e62e":"code","5952a894":"code","90ea2cd9":"code","cc9d0da5":"code","69fc67c0":"code","1c666d4d":"code","de1e9554":"code","ba7ccef2":"code","57a56690":"code","c20e6bae":"code","98c3ac50":"code","09b31eff":"code","364abcab":"code","9e4c8b53":"code","124adc6e":"code","8568b626":"code","f9854be3":"code","a0f1c478":"code","63e01b6f":"code","311c1dc9":"code","371ba1b0":"code","4190c285":"code","67e1ab4d":"code","8e2f1f35":"code","c0abf9bb":"code","88ee1499":"code","04c8e639":"code","62c76914":"code","e5a58c34":"code","dc455fa3":"code","53e8c803":"code","edac1cfc":"code","64e4ee04":"code","f83cff00":"code","f6157cc5":"code","d46c75d1":"code","fcc28e5e":"code","2f107c23":"code","50a64796":"code","fd3cc84e":"markdown","f694b7ac":"markdown","dca3d672":"markdown","2bd18143":"markdown","c8a5b831":"markdown","6fe0c16b":"markdown","d588dd93":"markdown","3b0823a0":"markdown","5210fd99":"markdown","2275e3f5":"markdown","ab24ef57":"markdown","ea1457c3":"markdown","22a47d39":"markdown","7c26301b":"markdown","035bd42a":"markdown","411e1318":"markdown","b1bcb644":"markdown","1e94b97c":"markdown","af2e4007":"markdown","43d8c576":"markdown","e46184f3":"markdown","bcc506c4":"markdown","4ce72aee":"markdown"},"source":{"a5800c3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7048ff38":"import matplotlib.pylab as plt","b3076fe0":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0]) #en argument : (label_True, label_Pred).","c224b996":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","03cd03ab":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","c32c6c0e":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","b7b9aaf2":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","9d844f0f":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","d2b0610c":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","4835a306":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","ba42e62e":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))","5952a894":"import numpy as np\n\n#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\nX.shape","90ea2cd9":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')\n\nplt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')\n\n","cc9d0da5":"from sklearn import cluster\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)","69fc67c0":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","1c666d4d":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])","de1e9554":"plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","ba7ccef2":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","57a56690":"Z=clf.predict(data) # returns the labels of the data\nprint (Z)","c20e6bae":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","98c3ac50":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data","09b31eff":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n\nx = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","364abcab":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","9e4c8b53":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0,\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))\n","124adc6e":"\nclf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","8568b626":"#Read and check the dataset downloaded from the EuroStat\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('\/kaggle\/input\/ense3-lesson\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","f9854be3":"edu.tail()","a0f1c478":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","63e01b6f":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","311c1dc9":"#Extract 2010 set of values\nedu2010=pivedu.loc[2010]\nedu2010.head()","371ba1b0":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\n\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head()","4190c285":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","67e1ab4d":"#Remove non info countries\nwrk_countries = nan_countries<4\n\neduclean=edu2010.loc[wrk_countries] #.ix - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\nprint (na_features)\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((8,4))","8e2f1f35":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","c0abf9bb":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()","88ee1499":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","04c8e639":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","62c76914":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","e5a58c34":"plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","dc455fa3":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","53e8c803":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","edac1cfc":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\n#the distance of the elements of cluster 0 to the center of cluster 1\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","64e4ee04":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1)\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","f83cff00":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","f6157cc5":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')","d46c75d1":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","fcc28e5e":"from scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","2f107c23":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","50a64796":"X_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 4,labels = wrk_countries_names, leaf_font_size=20);\n\nplt.savefig(\"\/kaggle\/working.png\",dpi=300, bbox_inches='tight')\nplt.show()\n\n#plt.tight_layout() # fixes margins","fd3cc84e":"300 it\u00e9rations ont \u00e9t\u00e9 n\u00e9c\u00e9ssaires pour la m\u00e9thode Kmeans. Et les centroids \u00e9taient \u00e9loign\u00e9s les un des autres au d\u00e9but. C'est la configuration optimale pour l'algorithme.\nLes \u00e9lements de chaque clusters sont \u00e9loign\u00e9s les uns des autres, ce qui explique que l'inertie est \u00e9lev\u00e9e. \nLe adjusted_rand_score est proche de 1 donc les clusters pr\u00e9dits sont proches des clusters r\u00e9els. ","f694b7ac":"La distance d0 va des pays du cluster 0 jusqu'au  centroid du cluster 0. De m\u00eame la distance d1 va des pays du cluster 1 jusqu'au centroid du cluster 1. On note que d0 est inf\u00e9rieur \u00e0 d1, ce qui est logique d'apr\u00e8s la d\u00e9finition du cluster 0. La Norv\u00e8ge et la Su\u00e8de sont les pays les plus proche du cluster 1 donc ceux qui d\u00e9pensent le moins, alors que le Danemark et l'Iceland sont ceux qui d\u00e9pensent le plus. ","dca3d672":"Ci-dessus, on note qu'en \u00e9changeant les deux arguments de completeness_score(label-true,label_pred) on obtient le homogeneity_score\n\nLabelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harm the completeness and thus penalise V-measure as well","2bd18143":"On remarque qu'avec la deuxi\u00e8me m\u00e9thode on enl\u00e8ve 4 caract\u00e9ristiqes contrairement \u00e0 la premi\u00e8re m\u00e9thode qui garde toutes les caract\u00e9ristiques.\n\nOn applique maintenant une technique de regroupement K-means sur ces donn\u00e9es afin de r\u00e9partir les pays en fonction de leur investissement dans l'\u00e9ducation et de v\u00e9rifier leurs profils.","c8a5b831":"Ici la completeness est totale mais l'homog\u00e9n\u00e9it\u00e9 est nulle donc le v_mesure_score vaut toujours 0.","6fe0c16b":"On fait cette fois-ci que 2 it\u00e9rations et les centroids sont choisis al\u00e9atoirement, c'est \u00e0 dire une configuration non optimale pour la m\u00e9thode Kmeans, et pourtant les valeurs sont tr\u00e8s proches des valeurs pr\u00e9c\u00e9dentes.\nL'efficacit\u00e9 de la m\u00e9thode Kmeans est donc tr\u00e8s peu d\u00e9pendante du nombre d'it\u00e9rations et du choix initial des centroids.","d588dd93":"On se concentre ensuite sur l'ann\u00e9e 2010 pour plus de simplicit\u00e9 :","3b0823a0":"Le label_true met tous les \u00e9lements dans la meme classe, mais le label pred le range chacun dans une classe diff\u00e9rentes, donc l'homog\u00e9n\u00e9it\u00e9 est totale mais aucune classe n'est compl\u00e8te. Donc v_mesure_score =0 car completness=0\n\nClusters that include samples from totally different classes totally destroy the harmonic mean between homogeneity and completeness (v-measure) of the labelling, hence:","5210fd99":"On remarque que l'on a 3 clusters, regroup\u00e9s selon leurs investissements dans l'\u00e9ducation.","2275e3f5":"L'agglomerative clustering am\u00e8ne \u00e0 la meme d\u00e9finition des clusters qu'avec la m\u00e9thode K-means. Dans cet exemple, les deux m\u00e9thodes ont des performances \u00e9quivalentes. ","ab24ef57":"On remarque que la m\u00e9thode des K_means donnent d'aussi bons r\u00e9sultats que la m\u00e9thode pr\u00e9c\u00e9dente.","ea1457c3":"Ci dessus, on voit que les \u00e9lements de label_pred sont bien rang\u00e9es dans les classes de label_True donc le v_mesure_score=1.\nDans chaque groupe de label_pred, on a donc que des \u00e9l\u00e9ments de la m\u00eame classe (homog\u00e9n\u00e9it\u00e9 parfaite) et tous les \u00e9l\u00e9ments d'un classe de label_true sont tous dans une classe de label_pred (completensess totale)\n\nLabelings that assign all classes members to the same clusters are: complete, but not homogeneous","22a47d39":"Si dans chaque cluster label_pred il y a uniquement des donn\u00e9es appartenant \u00e0 la m\u00eame classe (d\u00e9finit dans label_True), alors l'homog\u00e9n\u00e9it\u00e9 est totale. \n\nAu contraire, si label_True d\u00e9finit de mettre toutes les donn\u00e9es dans une m\u00eame classe, et que label_Pred met chaque \u00e9lement dans une classe diff\u00e9rente, alors la classe n'est pas compl\u00e8te. ","7c26301b":"On voit ci dessus les indicateurs de chaque cluster. On remarque que les pays sont bien regroup\u00e9 par indicateur : le cluster 0 a d\u00e9pens\u00e9 plus pour l'\u00e9ducation dans chaque indicateur. L'Espagne est dans le cluster qui a d\u00e9pens\u00e9 le moins pour l'\u00e9ducation.","035bd42a":"On remarque qu'il y a des valeurs non renseign\u00e9es, et on enl\u00e8ve donc les pays o\u00f9 il y a plus de 3 valeurs non renseign\u00e9es","411e1318":"How many \u201cmisclusterings\u201d do we have? On voit qu'il y a 2 points bleus dans la zone rose, 3 verts dans la zone bleu et 2 bleus dans la zone vert. Tous les autres points sont dans la bonne zone de couleur. Il y a 7 misclustering d'apr\u00e8s la figure ci dessous. ","b1bcb644":"On r\u00e9partit ensuite les datas en 3 clusters d\u00e9sign\u00e9s respectivement par les nombres 0,1 et 2. Chaque donn\u00e9e est associ\u00e9e \u00e0 l'un de ces cluster.\n","1e94b97c":"On remarque qu'avec la m\u00e9thode des Kmeans certains points ne sont pas class\u00e9 dans le bon cluster, tous les points bleus qui chevauchent la zone verte dans le graphe des original labels.","af2e4007":"Shall the centroids belong to the original set of points? Le centroid correspond \u00e0 une moyenne des points d'un cluster, ce n'est pas forc\u00e9ment un point du cluster lui m\u00eame.","43d8c576":"On note comme pr\u00e9c\u00e9demment que le Danemark, l'Islande et Chypre sont les pays qui d\u00e9pensent le plus en \u00e9ducation.","e46184f3":"Pour le Danemarl et l'Iceland, la distance au centre du cluter 1 est la plus grande. Ce sont ces pays qui d\u00e9pensent le plus en \u00e9ducation de leur cluster.","bcc506c4":"On voit que la caract\u00e9ristique 0 n'est pas renseign\u00e9e pour un pays, la caract\u00e9ristique 2 n'est pas renseign\u00e9e pour 3 pays, etc.","4ce72aee":"Les pays ne sont pas rang\u00e9 dans le m\u00eame ordre et le cluster 1 est devenu le cluster 0 et inversement. Donc il est difficile de comparer l'effet d'enlever les valeurs non renseign\u00e9es. Mais on note que la classe 1 qui contenait 7 pays avant en contient 9 maintenant. "}}