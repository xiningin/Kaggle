{"cell_type":{"13d5b21b":"code","9e3146dd":"code","d90da931":"code","7d31de58":"code","0269c038":"code","37a8f423":"code","cdf6283a":"code","69131c55":"code","b023c32e":"code","e00820fe":"code","ffdaa2be":"code","2fd0a83b":"code","6dcacd60":"code","192d0032":"code","b781a8a7":"code","1b0498cb":"code","be5a149e":"code","745ea9a1":"code","3626f7cd":"code","d92165da":"code","741030d3":"code","88a8881e":"code","a39d3552":"code","3acc336c":"code","541c09a4":"code","5ea037a2":"code","5b3da192":"code","14e4098e":"code","89f7959f":"code","81140bb0":"code","e390ef2b":"code","b6b5e122":"code","c9be7611":"code","88a38c57":"code","2ce0988a":"code","f06e976d":"code","ab1406d9":"code","1386784a":"code","0cc60206":"code","9c3bde23":"code","f4db34f0":"code","862e14d8":"code","bdbac6ca":"code","f900d314":"code","31d5b617":"code","8afd0daf":"code","c7f47241":"code","641a4dd9":"code","28b11e01":"code","da50fbaa":"code","e1c366e7":"code","d97ea0ca":"markdown"},"source":{"13d5b21b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n\nfrom keras.utils.all_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras import losses, metrics\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9e3146dd":"df = pd.read_csv(\"\/kaggle\/input\/dish-network-hackathon\/Train_Dataset.csv\", low_memory=False)\ndf.head()","d90da931":"# this is a generic function to fix values based on a map passed\n\ndef fix_value_by_map(current_value, default, value_map):\n    if str(current_value) == 'nan':\n        return value_map[default]\n    else:\n        return current_value","7d31de58":"# this is generic function converting to float\n\ndef get_floats(values):\n    vals = []\n    for v in values:\n        try:\n            vals.append(float(v))\n        except:\n            vals.append(np.nan)\n    return vals\n\n# this is generic function converting to int\n\ndef get_integers(values):\n    vals = []\n    for v in values:\n        try:\n            vals.append(int(v))\n        except:\n            vals.append(np.nan)\n    return vals","0269c038":"# check for dtypes\n\n# most of data is imported as object and needs to be converted to numeric\ndf.dtypes","37a8f423":"#fix Client_Income\n\n# convert to float\ndf['Client_Income'] = get_floats(df.Client_Income)\n        \n# replace nan with proper values\nvalue_map = df.groupby('Default').median()['Client_Income']\ndf['Client_Income'] = df.apply(lambda row : fix_value_by_map(row['Client_Income'], row['Default'], value_map), axis=1)","cdf6283a":"# fix Credit_Amount\n\n# convert to float\ndf['Credit_Amount'] = get_floats(df.Credit_Amount)\n        \n# remove rows where there is no the amount of the credit\ndf.dropna(subset=['Credit_Amount'], inplace = True)","69131c55":"# fix Loan_Annuity\n\n# convert to float\ndf['Loan_Annuity'] = get_floats(df.Loan_Annuity)\n\n# try to get a picture of loan annuity for nan, based on client income and default\nvalue_map = df.groupby(['Client_Income','Default']).mean()['Loan_Annuity']\n\n\ndef get_Loan_Annuity_by_Client_Income_and_Default(loan_annuity, client_income, default, value_map):\n    \n    new_loan_annuity = loan_annuity\n    \n    if str(new_loan_annuity) == 'nan':\n        for key in value_map.keys():\n            if default == key[1]:\n                if client_income < key[0]:\n                    continue\n                else:\n                    new_loan_annuity = value_map[key]\n                    break\n            else:\n                continue\n    return new_loan_annuity\n          \ndf['Loan_Annuity'] = df.apply(lambda row : get_Loan_Annuity_by_Client_Income_and_Default(row['Loan_Annuity'], row['Client_Income'], row['Default'], value_map), axis=1)","b023c32e":"# fix Accompany_Client\ndf[['Accompany_Client']] = df[['Accompany_Client']].fillna(\"Alone\")\n\nencoder = LabelEncoder()\n\ndf.Accompany_Client = encoder.fit_transform(df.Accompany_Client)","e00820fe":"# fix Client_Income_Type\ndf[['Client_Income_Type']] = df[['Client_Income_Type']].fillna(\"Unknown\")\n\nencoder = LabelEncoder()\n\ndf.Client_Income_Type = encoder.fit_transform(df.Client_Income_Type)          ","ffdaa2be":"# fix Car_Owned: consider nan as no car\n\ndf[['Car_Owned']] = df[['Car_Owned']].fillna(value=0)","2fd0a83b":"# fix Bike_Owned: consider nan as no bike\n\ndf[['Bike_Owned']] = df[['Bike_Owned']].fillna(value=0)","6dcacd60":"# fix Active_Loan: consider nan as no active loan\n\ndf[['Active_Loan']] = df[['Active_Loan']].fillna(value=0)","192d0032":"# fix House_Own: consider nan as no house owned\n\ndf[['House_Own']] = df[['House_Own']].fillna(value=0)","b781a8a7":"# fix Child_Count: consider nan as no house owned\n\ndf[['Child_Count']] = df[['Child_Count']].fillna(value=0)","1b0498cb":"# fix Client_Education\ndf[['Client_Education']] = df[['Client_Education']].fillna(value='None')\n\nencoder = LabelEncoder()\n\ndf.Client_Education = encoder.fit_transform(df.Client_Education)  ","be5a149e":"# fix Client_Marital_Status\ndf[['Client_Marital_Status']] = df[['Client_Marital_Status']].fillna(value='Other')\n\nencoder = LabelEncoder()\n\ndf.Client_Marital_Status = encoder.fit_transform(df.Client_Marital_Status)  ","745ea9a1":"# fix Client_Gender\ndf[['Client_Gender']] = df[['Client_Gender']].fillna(value='Unknown')\n\nencoder = LabelEncoder()\n\ndf.Client_Gender = encoder.fit_transform(df.Client_Gender)  ","3626f7cd":"# fix Loan_Contract_Type\ndf[['Loan_Contract_Type']] = df[['Loan_Contract_Type']].fillna(value='Other')\nencoder = LabelEncoder()\n\ndf.Loan_Contract_Type = encoder.fit_transform(df.Loan_Contract_Type)  ","d92165da":"#fix Client_Housing_Type\ndf[['Client_Housing_Type']] = df[['Client_Housing_Type']].fillna(value='Other')\n\nencoder = LabelEncoder()\n\ndf.Client_Housing_Type = encoder.fit_transform(df.Client_Housing_Type)  ","741030d3":"# fix Population_Region_Relative\ndf['Population_Region_Relative'] = get_floats(df.Population_Region_Relative)\n\ndf[['Client_Housing_Type']] = df[['Client_Housing_Type']].fillna(value=0)\n\nencoder = LabelEncoder()\n\ndf.Population_Region_Relative = encoder.fit_transform(df.Population_Region_Relative)  ","88a8881e":"# fix Age_Days\ndf['Age_Days'] = get_integers(df.Age_Days)\n        \n# map missing ages by 'default' column \nvalue_map = df.groupby('Default').median()['Age_Days']\ndf['Age_Days'] = df.apply(lambda row : fix_value_by_map(row['Age_Days'], row['Default'], value_map), axis=1)","a39d3552":"# fix Employed_Days\ndf['Employed_Days'] = get_integers(df.Employed_Days)\n\n# map missing Employed Days by 'default' column\nvalue_map = df.groupby('Default').median()['Employed_Days']\ndf['Employed_Days'] = df.apply(lambda row : fix_value_by_map(row['Employed_Days'], row['Default'], value_map), axis=1)","3acc336c":"#fix Registration_Days\ndf['Registration_Days'] = get_integers(df.Registration_Days)\n\n# map missing Registration Days by 'default' column\nvalue_map = df.groupby('Default').median()['Registration_Days']\ndf['Registration_Days'] = df.apply(lambda row : fix_value_by_map(row['Registration_Days'], row['Default'], value_map), axis=1)","541c09a4":"#fix ID_Days\ndf['ID_Days'] = get_integers(df.ID_Days)\n        \n# map missing Id Days by 'default' column\nvalue_map = df.groupby('Default').median()['ID_Days']\ndf['ID_Days'] = df.apply(lambda row : fix_value_by_map(row['ID_Days'], row['Default'], value_map), axis=1)","5ea037a2":"# fix Client_Occupation\ndf[['Client_Occupation']] = df[['Client_Occupation']].fillna(value='Other')\n\nencoder = LabelEncoder()\n\ndf.Client_Occupation = encoder.fit_transform(df.Client_Occupation)  ","5b3da192":"# fix Client_Permanent_Match_Tag\nencoder = LabelEncoder()\n\ndf.Client_Permanent_Match_Tag = encoder.fit_transform(df.Client_Permanent_Match_Tag)  \ndf.Client_Permanent_Match_Tag.unique()","14e4098e":"# fix Own_House_Age\n\n# remove for now due to high nans\ndf.drop(['Own_House_Age'], inplace=True, axis=1)","89f7959f":"# fix family members\n\n# suppose nan means zero\ndf[['Client_Family_Members']] = df[['Client_Family_Members']].fillna(value=0)","81140bb0":"#fix Cleint_City_Rating\n\nvalue_map = df.groupby('Default').median()['Cleint_City_Rating']\n\ndf['Cleint_City_Rating'] = df.apply(lambda row : fix_value_by_map(row['Cleint_City_Rating'], row['Default'], value_map), axis=1)","e390ef2b":"# fix Application_Process_Day\n\nvalue_map = df.groupby('Default').median()['Application_Process_Day']\n\ndf['Application_Process_Day'] = df.apply(lambda row : fix_value_by_map(row['Application_Process_Day'], row['Default'], value_map), axis=1)","b6b5e122":"# fix Application_Process_Hour\n\nvalue_map = df.groupby('Default').median()['Application_Process_Hour']\n\ndf['Application_Process_Hour'] = df.apply(lambda row : fix_value_by_map(row['Application_Process_Hour'], row['Default'], value_map), axis=1)","c9be7611":"# fix Client_Contact_Work_Tag\nencoder = LabelEncoder()\n\ndf.Client_Contact_Work_Tag = encoder.fit_transform(df.Client_Contact_Work_Tag)  ","88a38c57":"# fix Type organization by splitting between sector and type\ndf.Type_Organization.unique()\n\ndef get_sector(organization_plus_type_value):\n    return str(organization_plus_type_value).lower().split('type')[0]\n\ndef get_sector_type(organization_plus_type_value):\n    try:\n        return int(str(organization_plus_type_value).lower().split('type')[1])\n    except:\n        return 0\n\n    \n# TODO eventually try to join 'XNA' with 'nan' and\/or 'Other'?\n\ndf['Employment_Sector'] = df.apply(lambda row : get_sector(row['Type_Organization']), axis=1)\ndf['Employment_Sector_Type'] = df.apply(lambda row : get_sector_type(row['Type_Organization']), axis=1)\ndf.drop(['Type_Organization'], inplace=True, axis=1)\n\nencoder = LabelEncoder()\n\ndf['Employment_Sector'] = encoder.fit_transform(df['Employment_Sector'])","2ce0988a":"#fix Score_Source_1\n\ndf['Score_Source_1'] = get_floats(df.Score_Source_1)\n\nvalue_map = df.groupby('Default').median()['Score_Source_1']\n\ndf['Score_Source_1'] = df.apply(lambda row : fix_value_by_map(row['Score_Source_1'], row['Default'], value_map), axis=1)","f06e976d":"#fix Score_Source_2\n\ndf['Score_Source_2'] = get_floats(df.Score_Source_2)\n\nvalue_map = df.groupby('Default').median()['Score_Source_2']\n\ndf['Score_Source_2'] = df.apply(lambda row : fix_value_by_map(row['Score_Source_2'], row['Default'], value_map), axis=1)","ab1406d9":"#fix Score_Source_3\n\ndf['Score_Source_3'] = get_floats(df.Score_Source_3)\n\nvalue_map = df.groupby('Default').median()['Score_Source_3']\n\ndf['Score_Source_3'] = df.apply(lambda row : fix_value_by_map(row['Score_Source_3'], row['Default'], value_map), axis=1)","1386784a":"# fix Social_Circle_Default\nvalue_map = df.groupby('Default').mean()['Social_Circle_Default']\n\ndf['Social_Circle_Default'] = df.apply(lambda row : fix_value_by_map(row['Social_Circle_Default'], row['Default'], value_map), axis=1)","0cc60206":"# fix Phone_Change\nvalue_map = df.groupby('Default').median()['Phone_Change']\n\ndf['Phone_Change'] = df.apply(lambda row : fix_value_by_map(row['Phone_Change'], row['Default'], value_map), axis=1)","9c3bde23":"# fix Credit_Bureau\nvalue_map = df.groupby('Default').median()['Credit_Bureau']\n\ndf['Credit_Bureau'] = df.apply(lambda row : fix_value_by_map(row['Credit_Bureau'], row['Default'], value_map), axis=1)","f4db34f0":"# check all dtypes are fine\ndf.dtypes","862e14d8":"# check for nan : we should not have any of them now...\ndf.isna().sum()","bdbac6ca":"# dataset in unbalanced\ndf.groupby('Default').count()","f900d314":"# oversample the dataset\n\nX = df.drop(['ID','Default'], axis=1)\nY = df.Default\n\n# oversample dataset\noversample = RandomOverSampler(sampling_strategy='minority', random_state=0)\nX_over, y_over = oversample.fit_resample(X, Y)\n\nscaler = StandardScaler()\nX_over = scaler.fit_transform(X_over)\n\nlen(X_over)","31d5b617":"# generate train test and validation set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_over, y_over, test_size=0.2, stratify=y_over, shuffle=True, random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, shuffle=True, random_state=42)\n\n# transform y to categorical\ny_train = to_categorical(y_train, 2)\ny_test = to_categorical(y_test, 2)\ny_valid = to_categorical(y_valid, 2)","8afd0daf":"# compose the NN model\n\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\nmodel.fit(X_train, y_train, batch_size = 256,  epochs = 80, verbose = True, \n          validation_data=(X_test, y_test), shuffle=True)    \n# Evaluate the model accuracy on the validation set.\nscore = model.evaluate(X_valid, y_valid, verbose=0)","c7f47241":"# predict for train and validation (will be used later for confusion matrix)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\ny_valid_pred = model.predict(X_valid)","641a4dd9":"# plot confusion matrix train TRAIN\n\ny_train_for_cmatrix = []\ny_train_preds_for_cmatrix = []\nfor v in y_train:\n    if v[1] > 0.5:\n        y_train_for_cmatrix.append(1)\n    else:\n        y_train_for_cmatrix.append(0)\nfor v in y_train_pred:\n    if v[1] > 0.5:\n        y_train_preds_for_cmatrix.append(1)\n    else:\n        y_train_preds_for_cmatrix.append(0)\n\n        \nconfusion_mtx = confusion_matrix(y_train_for_cmatrix, y_train_preds_for_cmatrix)\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, xticklabels=list(['Ok','Default']), yticklabels=list(['Ok','Default']),annot=True, cmap='Blues')\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()","28b11e01":"# plot confusion matrix train TEST\n\ny_test_for_cmatrix = []\ny_test_preds_for_cmatrix = []\nfor v in y_test:\n    if v[1] > 0.5:\n        y_test_for_cmatrix.append(1)\n    else:\n        y_test_for_cmatrix.append(0)\nfor v in y_test_pred:\n    if v[1] > 0.5:\n        y_test_preds_for_cmatrix.append(1)\n    else:\n        y_test_preds_for_cmatrix.append(0)\n\n        \nconfusion_mtx = confusion_matrix(y_test_for_cmatrix, y_test_preds_for_cmatrix)\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, xticklabels=list(['Ok','Default']), yticklabels=list(['Ok','Default']),annot=True, cmap='Blues')\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()","da50fbaa":"# plot confusion matrix train VALIDATION (data never seen by the algorithm)\n\ny_valid_for_cmatrix = []\ny_valid_preds_for_cmatrix = []\nfor v in y_valid_pred:\n    if v[1] > 0.5:\n        y_valid_preds_for_cmatrix.append(1)\n    else:\n        y_valid_preds_for_cmatrix.append(0)\nfor v in y_valid:\n    if v[1] > 0.5:\n        y_valid_for_cmatrix.append(1)\n    else:\n        y_valid_for_cmatrix.append(0)\n        \nconfusion_mtx = confusion_matrix(y_valid_for_cmatrix,y_valid_preds_for_cmatrix)\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, xticklabels=list(['Ok','Default']), yticklabels=list(['Ok','Default']),annot=True, cmap='Blues')\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()","e1c366e7":"# print accuracy and f1 score for training data\nprint(accuracy_score(y_train_for_cmatrix, y_train_preds_for_cmatrix), f1_score(y_train_for_cmatrix, y_train_preds_for_cmatrix))\n# print accuracy and f1 score for test data\nprint(accuracy_score(y_test_for_cmatrix, y_test_preds_for_cmatrix), f1_score(y_test_for_cmatrix, y_test_preds_for_cmatrix))\n# print accuracy and f1 score for validation data\nprint(accuracy_score(y_valid_for_cmatrix, y_valid_preds_for_cmatrix), f1_score(y_valid_for_cmatrix, y_valid_preds_for_cmatrix))","d97ea0ca":"COMMENTS\nThis is a base notebook (no tuning, very basic feature engineering\nConfusion matrix is quite balanced, but the\nbehavior of this model for False Negative (risk of default predicted as fine) is worst than False positive ( no risk predicted as default).\nTo be improved."}}