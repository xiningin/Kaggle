{"cell_type":{"55ba88d3":"code","b5302d73":"code","ab626827":"code","cbf138b7":"code","c9cd725a":"code","327b376a":"code","6e3c4dbc":"code","5e1833ba":"code","3dc107cd":"code","6300c25e":"code","370ce4ea":"code","f7292828":"code","fe41e7fd":"code","5937996f":"code","262290c1":"code","163864b4":"code","0f561c77":"code","c40b0a60":"code","0b4dcd00":"code","f208ccd5":"code","be8e793d":"markdown","8bea1d08":"markdown","5cb6496b":"markdown","ba367a7c":"markdown","970dbf51":"markdown","c91b2837":"markdown","ec9703fe":"markdown","b6472f5d":"markdown"},"source":{"55ba88d3":"# This is a bit of code to make things work on Kaggle\nimport os\nfrom pathlib import Path\n\nif os.path.exists(\"\/kaggle\/input\/ucfai-supplementary-fa19-app-nns\/data\"):\n    DATA_DIR = Path(\"\/kaggle\/input\/ucfai-supplementary-fa19-app-nns\/data\")\nelse:\n    DATA_DIR = Path(\"data\/\")\n\n!pip install torchsummary","b5302d73":"# general imports\nimport numpy as np\nimport time\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# torch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\n\n# torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torchvision import models as pretrained_models","ab626827":"try:\n    import torchsummary\nexcept:\n    torchsummary = None\n\nfrom tabulate import tabulate\n\nBATCH_TEMPLATE = \"Epoch [{} \/ {}], Batch [{} \/ {}]:\"\nEPOCH_TEMPLATE = \"Epoch [{} \/ {}]:\"\nTEST_TEMPLATE = \"Epoch [{}] Test:\"\n\ndef print_iter(curr_epoch=None, epochs=None, batch_i=None, num_batches=None, writer=None, msg=False, **kwargs):\n    \"\"\"\n    Formats an iteration. kwargs should be a variable amount of metrics=vals\n    Optional Arguments:\n        curr_epoch(int): current epoch number (should be in range [0, epochs - 1])\n        epochs(int): total number of epochs\n        batch_i(int): current batch iteration\n        num_batches(int): total number of batches\n        writer(SummaryWriter): tensorboardX summary writer object\n        msg(bool): if true, doesn't print but returns the message string\n\n    if curr_epoch and epochs is defined, will format end of epoch iteration\n    if batch_i and num_batches is also defined, will define a batch iteration\n    if curr_epoch is only defined, defines a validation (testing) iteration\n    if none of these are defined, defines a single testing iteration\n    if writer is not defined, metrics are not saved to tensorboard\n    \"\"\"\n    if curr_epoch is not None:\n        if batch_i is not None and num_batches is not None and epochs is not None:\n            out = BATCH_TEMPLATE.format(curr_epoch + 1, epochs, batch_i, num_batches)\n        elif epochs is not None:\n            out = EPOCH_TEMPLATE.format(curr_epoch + 1, epochs)\n        else:\n            out = TEST_TEMPLATE.format(curr_epoch + 1)\n    else:\n        out = \"Testing Results:\"\n\n    floatfmt = []\n    for metric, val in kwargs.items():\n        if \"loss\" in metric or \"recall\" in metric or \"alarm\" in metric or \"prec\" in metric:\n            floatfmt.append(\".4f\")\n        elif \"accuracy\" in metric or \"acc\" in metric:\n            floatfmt.append(\".2f\")\n        else:\n            floatfmt.append(\".6f\")\n\n        if writer and curr_epoch:\n            writer.add_scalar(metric, val, curr_epoch)\n        elif writer and batch_i:\n            writer.add_scalar(metric, val, batch_i * (curr_epoch + 1))\n\n    out += \"\\n\" + tabulate(kwargs.items(), headers=[\"Metric\", \"Value\"], tablefmt='github', floatfmt=floatfmt)\n\n    if msg:\n        return out\n    print(out)\n\ndef summary(model, input_dim):\n    if torchsummary is None:\n        raise(ModuleNotFoundError, \"TorchSummary was not found!\")\n    torchsummary.summary(model, input_dim)","cbf138b7":"folders = {\"train\": DATA_DIR \/ 'chest_xray' \/ 'train', \"test\": DATA_DIR \/ 'chest_xray' \/ 'test'}\n\nnum_normal = 0\nnum_pne = 0\nfor f in folders.values():\n    num_normal += len(glob.glob(str(f \/ 'NORMAL' \/ '*')))\n    num_pne += len(glob.glob(str(f \/ 'PNEUMONIA' \/ '*')))\n\n# plot number of cases\nplt.figure(figsize=(10,8))\nplt.bar([0, 1], [num_normal, num_pne], color=[(1, 0.5, 0), (0, 0.5, 1)])\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks([0, 1], ['Normal', 'Pneumonia'])\nplt.show()","c9cd725a":"sample_imgs = glob.glob(str(folders[\"train\"] \/ 'NORMAL' \/ '*'))[:5] + glob.glob(str(folders[\"train\"] \/ 'PNEUMONIA' \/ '*'))[:5]\n\n# plot some normal and pneumonia images\nf, ax = plt.subplots(2,5, figsize=(30,10))\nfor i in range(10):\n    img = cv2.imread(sample_imgs[i])\n    print(img.shape)\n    ax[i\/\/5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i\/\/5, i%5].set_title(\"Pneumonia\")\n    else:\n        ax[i\/\/5, i%5].set_title(\"Normal\")\n    ax[i\/\/5, i%5].axis('off')\n    ax[i\/\/5, i%5].set_aspect('auto')\nplt.show()","327b376a":"class CustomImageFolder(Dataset):\n    \n    def __init__(self, root, transforms=None):\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def __getitem__(self, index):\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def __len__(self):\n        # YOUR CODE HERE\n        raise NotImplementedError()","6e3c4dbc":"input_size = (224, 224)\nnum_workers = 4\nbatch_size = 16\n\ntransform = transforms.Compose([transforms.Resize(input_size),\n                                        transforms.ColorJitter(brightness=15, contrast=15, saturation=15),\n                                        transforms.RandomHorizontalFlip(p=0.5),\n                                        transforms.RandomRotation(90),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n                                    \ntrain_dataset = ImageFolder(folders[\"train\"], transform=transform)\ntest_dataset = ImageFolder(folders[\"test\"], transform=transform)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\n\nprint(f\"Number of: Train Images: {len(train_dataset)}, Test Images: {len(test_dataset)}\")\nprint(f\"Dataloader Sizes: Train: {len(train_dataloader)}, Test: {len(test_dataloader)}\")","5e1833ba":"def get_padding(output_dim, input_dim, kernel_size, stride):\n    \"\"\"\n    Calculates padding given in output and input dim, and parameters of the convolutional layer\n\n    Arguments should all be integers. Use this function to calculate padding for 1 dimesion at a time.\n    Output dimensions should be the same or bigger than input dimensions\n\n    Returns 0 if invalid arguments were passed, otherwise returns an int or tuple that represents the padding.\n    \"\"\"\n\n    padding = (((output_dim - 1) * stride) - input_dim + kernel_size) \/\/ 2\n\n    if padding < 0:\n        return 0\n    else:\n        return padding\n\n# can use this to help with padding calculations, or use in model directly!\nprint(get_padding(224, 224, 4, 1))","3dc107cd":"def cnn_block(input_channels, output_channels, kernel_size, stride, padding):\n    layers = [nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding)]\n    layers += [nn.BatchNorm2d(output_channels)]\n    layers += [nn.ReLU(inplace=True)]\n    \n    return layers","6300c25e":"class CNN_Model(nn.Module):\n    def __init__(self, use_pretrained=False, input_dim=224):\n        super(CNN_Model, self).__init__()\n        \n        self.dim = input_dim\n        \n        if use_pretrained:\n            self.features = pretrained_models.resnet18(pretrained=True)\n            #for layer in self.features.parameters()\n            #    layer.requires_grad = False\n            \n            self.classifier = nn.Sequential(nn.Linear(1000, 1), nn.Sigmoid())\n        else:\n            self.features, out_c = self.build_layers()\n            self.classifier = self.build_classifier(out_c)\n    \n    \n    def forward(self, x):\n        # squeeze out those features! It removes extra dimensions of 1 from shape.\n        x = self.features(x)\n        return self.classifier(x.squeeze()).squeeze()\n    \n    \n    def build_classifier(self, in_c):\n        layers = []\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n    \n    def build_layers(self):\n        layers = []\n        in_c = 3\n        out_c = 64\n        dim = self.dim\n        \n        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n        in_c = out_c\n        out_c = 64\n        \n        layers += [nn.MaxPool2d(2, stride=2)]\n        dim = dim \/\/ 2\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        layers += [nn.AdaptiveAvgPool2d(1)]\n        \n        return nn.Sequential(*layers), out_c\n        ","370ce4ea":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device: {}\".format(device))\nlearn_rate = 0.0001\nepochs = 20\n\nmodel = CNN_Model(use_pretrained=True).to(device)\n\nopt = optim.Adam(model.parameters(), lr=learn_rate)\ncriterion = nn.BCELoss()\n\nsummary(model, (3, 224, 224))","f7292828":"# defines a test run through data\n# epoch of -1 is just a test run\ndef test(epoch):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, targets in test_dataloader:\n            inputs, targets = inputs.to(device), targets.float().to(device)\n            outputs = model(inputs)\n\n            loss = criterion(outputs, loss_const)\n\n            test_loss += loss.item()            \n            # round off decimal predictions to either 0 or 1\n            preds = torch.round(outputs)\n            total += targets.size(0)\n            # sum correct predictions\n            correct += torch.sum(preds == targets.data)\n\n        if epoch == -1:\n            print_iter(test_loss=test_loss\/len(test_dataloader), val_acc=(correct \/ total) * 100.0)\n        else:\n            print_iter(curr_epoch=epoch, writer=writer, val_loss=test_loss\/len(test_dataloader), val_acc=(correct \/ total) * 100.0)\n\n            return test_loss\/len(test_dataloader), (correct \/ total) * 100.0\n","fe41e7fd":"print_step = 5\nbest_loss = 0\nbest_acc = 0\n\nprint(\"Training Starting...\")\nfor e in range(epochs):\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    epoch_start_time = time.time()\n\n    for inputs, targets in train_dataloader:\n        inputs, targets = inputs.to(device), targets.float().to(device)\n        outputs = model(inputs)\n\n        opt.zero_grad()\n        \n        # backward\n        loss = criterion(outputs, targets)\n        loss.backward()\n        opt.step()\n        \n        train_loss += loss.item() # .item() extracts the raw loss value from the tensor object\n        \n        # round off decimal predictions to either 0 or 1\n        preds = torch.round(outputs)\n        total += targets.size(0)\n        # sum correct predictions\n        correct += torch.sum(preds == targets.data)\n\n        if i % print_step == 0:\n            print_iter(curr_epoch=e, epochs=epochs, batch_i=i, num_batches=len(train_dataloader), loss=train_loss\/(i+1), acc=(correct \/ total) * 100.0)\n\n    print_iter(curr_epoch=e, epochs=epochs, writer=writer, loss=train_loss\/len(train_dataloader), acc=(correct \/ total) * 100.0, time=(time.time() - epoch_start_time) \/ 60)\n\n    val_loss, val_acc = test(e)\n    \n    if best_acc < val_acc:\n        print('Saving Checkpoint..')\n        checkpoint_path = \"best.weights.pt\"\n        state = {'net': model.state_dict(), 'acc': val_acc}\n        torch.save(state, checkpoint_path)\n        best_acc = val_acc","5937996f":"data = pd.read_csv(DATA_DIR \/ 'cervical.csv')\ndata.head()","262290c1":"data.info()","163864b4":"# Replace '?' with nans\ndata = data.replace('?', np.nan)\ndata.head()","0f561c77":"data.isnull().sum()","c40b0a60":"data = data.convert_objects(convert_numeric=True)\ndata.info()","0b4dcd00":"data['Number of sexual partners'] = data['Number of sexual partners'].fillna(data['Number of sexual partners'].median())\ndata['Smokes'] = data['Smokes'].fillna(1)\n# Fill the rest here!","f208ccd5":"data.isnull().sum()","be8e793d":"### Building our model\nAs we learned in our CNN lecture, we can use a custom model or a pretrained model with our own classifier layers attached. Lets try doing both and comparing the results! Feel free to use the cnn_block function or create your own custom functions. We can have one class that will either use a pretrained model or one we build.","8bea1d08":"### Diagnose Pneumonia in Patients\nOur first dataset is going to be xray images of people with and without pneumonia. Our job is going to be able to correctly diagnose pneumonia in these patients. \n\nFirst, lets take a look at our data. We should look at the number of normal cases vs. pneumonia cases and the pictures themselves.","5cb6496b":"<img src=\"https:\/\/ucfai.org\/supplementary\/fa19\/2019-09-23-app-nns\/app-nns\/banner.png\">\n\n<div class=\"col-12\">\n    <span class=\"btn btn-success btn-block\">\n        Meeting in-person? Have you signed in?\n    <\/span>\n<\/div>\n\n<div class=\"col-12\">\n    <h1> Applications of Neural Networks <\/h1>\n    <hr>\n<\/div>\n\n<div style=\"line-height: 2em;\">\n    <p>by: \n        <strong> Brandon<\/strong>\n        (<a href=\"https:\/\/github.com\/brandons209\">@brandons209<\/a>)\n     on 2019-09-23<\/p>\n<\/div>","ba367a7c":"#### NaNs\nSince we have NaNs all throughout the data, lets fill them instead of dropping them.\n\nFor continuous variable, we fill the median value for that column.\nFor categorical variable, we fill with a 1.","970dbf51":"### Cervical Cancer Risk Classification\nIn this tabular dataset, medical information if given alongside a biosopy, telling whether a woman has cervical cancer or not. Use this information to build a model that can accurately predict whether a woman has cervical cancer or not. I'll leave this one up to you! But I'll provide help and get you started with loading the data. Refer to the NNs lecture if you need help.\n\nSome things to remember: Make sure to look at your data first! Split your data into X and Y, then use train_test_split to get your splits for your data. Then create a custom dataset and dataloader for training and testing.","c91b2837":"### Train Model\nSince we have binary labels, we will use Binary CrossEntropy loss function. The Adam optimizer as always is an excellent optimizer to use for updating our model parameters. You can choose here to load a pretrained model or the one you have built.","ec9703fe":"# NN Applications\nToday, we are going to create models for a few different datasets. This will be pretty barebones, as we are going to jump right in! You can view solutions on our github for this notebook.","b6472f5d":"As you might see, there are much more pneumonia cases than normal, so we need to balance these classes or our model will overfit. We can subsample our data, so we only take 1500 normal and 1500 pneumonia cases, or augment our data using transforms to increase the number of each case's pictures. \n\nLets augment the number of pictures to increase our data, we do that with our pytorch transforms. We will also use the [ImageFolder](https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/datasets\/folder.html#ImageFolder) dataset for loading our images. \n\n#### Challenge\nWrite your own custom image folder dataset that loads images and **only** augments the normal pictures, not both pictures. Since there are more pneumonia pictures then normal, augmenting normal only will help even out the classes more."}}