{"cell_type":{"56f6ed6b":"code","2062c49a":"code","a8500947":"code","24c0d4ae":"code","3538ccf1":"code","d31c8237":"code","fada4d4b":"code","a5e9c48e":"code","5ec15b7b":"code","ee5cf6f0":"code","2dfc2819":"code","ab51985e":"code","4eb10136":"code","7864594d":"code","1666d60e":"code","2398b61b":"code","ea142a4c":"code","25c98daa":"code","eff5f78a":"code","9ed5b354":"code","edd536d6":"code","330c12a7":"code","7947f543":"code","a7cea51b":"code","865847ae":"code","20ec2a8b":"code","219b531e":"code","e7a50037":"code","74c8b35c":"code","9ad5579d":"code","62e5a604":"code","bd0cac67":"code","a7598b57":"code","fa75598e":"code","35c9a47d":"code","1dafa8b3":"code","78be3274":"code","38f2669b":"code","af7b7bf2":"code","0117c044":"code","0a26eaa2":"code","44283d57":"code","a6b9f098":"code","a703ebd4":"code","1ce7cc53":"code","4e76bae3":"code","bc9da9e3":"code","b0f6963b":"code","489b9c06":"code","2b88dacd":"code","d80c9701":"markdown","dfaf7f8a":"markdown","c8e26fa5":"markdown","14a50f0f":"markdown","d25e4127":"markdown","590073f7":"markdown","440e7022":"markdown","041f88bb":"markdown","1f57b367":"markdown","3d5393f4":"markdown","60c67a97":"markdown","2f475d4a":"markdown","7598495f":"markdown","2aba8c15":"markdown"},"source":{"56f6ed6b":"# Importing the necessary library\n\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense","2062c49a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8500947":"data = pd.read_csv('\/kaggle\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv')","24c0d4ae":"data.head()","3538ccf1":"# Checking for if there is any null values\ndata.isnull().sum()","d31c8237":"data.drop(['author_flair_text', 'removed_by', 'total_awards_received', 'awarders'],axis = 1, inplace = True)","fada4d4b":"data.drop(['id', 'created_utc', 'full_link'], axis=1, inplace=True)","a5e9c48e":"data.head()","5ec15b7b":"# filling the missing value in the column 'title'\ndata['title'].fillna(\" \", inplace=True)","ee5cf6f0":"# Finally checking if there is any null values\ndata.isnull().sum()","2dfc2819":"data['text'] = data['title'] + ' ' + data['author']\ndata.drop(['title', 'author'], axis = 1, inplace = True)","ab51985e":"data.head()","4eb10136":"# Let's categorize the data with over_18 as 1 and if not then with 0\n\ndata.over_18.replace([True, False], [1,0], inplace=True)\n\n# Counting the values \ndata.over_18.value_counts()","7864594d":"data.head()","1666d60e":"# Splitting the data for over_18 as False and True\n\nover_18_False = data[data['over_18']==0.0].text\nover_18_True = data[data['over_18']==1.0].text\n\ntrain_text = data['text'].values[:10000]\ntest_text = data['text'].values[10000:]\n\ntrain_category = data['over_18'][:10000]\ntest_category = data['over_18'][10000:]","2398b61b":"plt.figure(figsize = (20,20))\nW_C = WordCloud(min_font_size=3, max_words=3200, width=1600, height=850, stopwords=STOPWORDS).generate(str(\" \".join(over_18_False)))\nplt.imshow(W_C, interpolation='bilinear')","ea142a4c":"plt.figure(figsize=(20,20))\nW_C = WordCloud(min_font_size=3, max_words=3200, width=1600, height=850, stopwords=STOPWORDS).generate(str(\" \".join(over_18_True)))\nplt.imshow(W_C, interpolation='bilinear')","25c98daa":"# Using the W_C described above lets get the most frequently words used from Wordcloud of over_18_False\n\ntext_false = W_C.process_text(str(\" \".join(over_18_False)))\ntext_false","eff5f78a":"# Using the W_C described above lets get the most frequently words used from Wordcloud of over_18_True\n\ntext_true = W_C.process_text(str(\" \".join(over_18_True)))\ntext_true","9ed5b354":"len(text_false)","edd536d6":"len(text_true)","330c12a7":"# Lets sort the dictionary on the basis of key_values\n\ntext_false = sorted(text_false.items(),key = lambda kv:(kv[1], kv[0]))","7947f543":"text_false","a7cea51b":"text_true = sorted(text_true.items(),key = \n             lambda kv:(kv[1], kv[0]))","865847ae":"text_true","20ec2a8b":"ans_true = []\nfor i in text_true:\n    ans_true.append(i[0])\nans_true [:5] ","219b531e":"# Now for every word in each test data point, lets check that if any word of that test data point is either present, if word is present, then I will simply predict 1, else 0.\n\npredictions = []\nfor i in test_text:\n    x = i.split()  # splitting the word in test_text\n    for j in x:\n        if j in ans_true:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\n            \nlen(predictions)","e7a50037":"len(test_category)","74c8b35c":"# Counting for how many times predictions is same as test_category\n\ncount = 0\nfor i in range(len(predictions)):\n    test_category = list(test_category)\n    if (predictions[i] == int(test_category[i])):\n        count += 1\n        \nprint(count)","9ad5579d":"# Checking the accuracy of the match in predictions and test_category\n\naccuracy = (count\/len(predictions))*100\n\nprint(\"Accuracy obtained using the WordCloud is: \", accuracy, \"%\")","62e5a604":"stop = set(stopwords.words('english'))\npunctuation = list(punctuation)\nstop.update(punctuation)","bd0cac67":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","a7598b57":"stemmer = PorterStemmer\ndef stem_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = stemmer.stem(i.strip(), get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text","fa75598e":"lemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text ","35c9a47d":"data['text'] = data['text'].apply(lemmatize_words)","1dafa8b3":"def join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string","78be3274":"data['text'] = data['text'].apply(join_text)","38f2669b":"data.head()","af7b7bf2":"train_text, test_text, train_category, test_category = train_test_split(data['text'], data['over_18'], random_state=0)\n\nprint(train_text.shape)\nprint(test_text.shape)\nprint(train_category.shape)\nprint(test_category.shape)","0117c044":"# Defining the model\nC_V = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,2))\n\n# Transforming the train reviews\nC_V_train_reviews = C_V.fit_transform(train_text)\n\n# Transforming the test reviews\nC_V_test_reviews = C_V.transform(test_text)\n\nprint('Shape of C_V_train:', C_V_train_reviews.shape)\nprint('Shape of C_V_test:', C_V_test_reviews.shape)","0a26eaa2":"# Definig the Model\nT_V = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\n# Transforming the train reviews\nT_V_train_reviews = T_V.fit_transform(train_text)\n\n# Transforming the test reviews\nT_V_test_reviews = T_V.transform(test_text)\n\nprint('Shape of T_V_train:', T_V_train_reviews.shape)\nprint('Shape of T_V_test:', T_V_test_reviews.shape)","44283d57":"# Using the Logistic Regression as the First Model\nlr = LogisticRegression(penalty='l2', max_iter = 500, C = 1, random_state = 3)\n\n# Fitting the Model \nlr_cv = lr.fit(C_V_train_reviews, train_category)\nlr_tfidf = lr.fit(T_V_train_reviews, train_category)\n\nprint(lr_cv)\nprint(lr_tfidf)","a6b9f098":"#Predicting the model for bag of words\nlr_cv_predict=lr_cv.predict(C_V_test_reviews)\n##Predicting the model for tfidf features\nlr_tfidf_predict=lr_tfidf.predict(T_V_test_reviews)","a703ebd4":"# Calculating the Accuracy Score now\n\nlr_cv_score = accuracy_score(test_category, lr_cv_predict)\nlr_tfidf_score=accuracy_score(test_category, lr_tfidf_predict)\n\nprint(\"lr_Cv_score: \", lr_cv_score)\nprint(\"lr_tfidf_score :\", lr_tfidf_score)","1ce7cc53":"# Classification report\n\nlr_cv_report = classification_report(test_category, lr_cv_predict)\nlr_tfidf_report = classification_report(test_category, lr_tfidf_predict)\n\nprint(\"Classification Report for Bag of Words \\n \", lr_cv_report)\nprint(\"Classification Report for TfIdf features \\n \", lr_tfidf_report)","4e76bae3":"# Plotting the Confusion Matrix\nplot_confusion_matrix(lr_cv, C_V_test_reviews, test_category, values_format='')\nplot_confusion_matrix(lr_tfidf, T_V_test_reviews, test_category,  values_format='')","bc9da9e3":"# Using the Multinomial Naive Bayes\n\n# Creating the model\nmnb = MultinomialNB()\n\n#fitting the model\nmnb_cv = mnb.fit(C_V_train_reviews, train_category)\nmnb_tfidf = mnb.fit(T_V_train_reviews, train_category)\n\n# Predicting the model\nmnb_cv_predict = mnb_cv.predict(C_V_test_reviews)\nmnb_tfidf_predict = mnb_tfidf.predict(T_V_test_reviews)","b0f6963b":"# Calculating the Score\n\nmnb_cv_score = accuracy_score(test_category, mnb_cv_predict)\nmnb_tfidf_score = accuracy_score(test_category, mnb_tfidf_predict)\n\n# Printing the Accuracy Score\n\nprint(\"mnb_score: \", mnb_cv_score)\nprint(\"mnb_tfidf_score: \", mnb_tfidf_score)","489b9c06":"# Classification report\n\nmnb_cv_report = classification_report(test_category, mnb_cv_predict)\nmnb_tfidf_report = classification_report(test_category, mnb_tfidf_predict)\n\nprint(\"Classification Report for Bag of Words \\n \", mnb_cv_report)\nprint(\"Classification Report for TfIdf features \\n \", mnb_tfidf_report)","2b88dacd":"# Plotting the Confusion Matrix\nplot_confusion_matrix(mnb_cv, C_V_test_reviews, test_category, values_format='')\nplot_confusion_matrix(mnb_tfidf, T_V_test_reviews, test_category,  values_format='')","d80c9701":"# StopWords\n\nIn computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools,","dfaf7f8a":"Guys if you found this insighful please upvote!","c8e26fa5":"We can see that the values in the over_18 has been removed with 0 for False similarly it should have been with 1 for True values","14a50f0f":"Also deleting the some other columns which are not making sense, columns like 'id', 'created_utc', 'full_link'","d25e4127":"* We can see that the columns 'author_flair_text', 'removed_by', 'totla_award_recieved' and 'awrders' have lots of missing values so it would be better to drop them","590073f7":"Wow it gave a very high accuracy score ","440e7022":" Splitting the data into training and testing data","041f88bb":"We have already seen that there is one missing value in the column title","1f57b367":"There is no null values in the data so we can move ahead","3d5393f4":"# WordCloud Visualization","60c67a97":"# Using Different Classifier for training and then analysing after the testing","2f475d4a":"# Splitting ","7598495f":"# Stemming and Lemmatization\n\nStemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n\nSearch engines use stemming for indexing the words. That\u2019s why rather than storing all forms of a word, a search engine can store only the stems. In this way, stemming reduces the size of the index and increases retrieval accuracy.\n\n![image.png](attachment:image.png)\n\nLemmatization technique is like stemming. The output we will get after lemmatization is called \u2018lemma\u2019, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n\nNLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma.\n\n![image.png](attachment:image.png)","2aba8c15":"Its giving around an accuracy of almost 85% or if saying the exact valu its 84.56% accurate which is good"}}