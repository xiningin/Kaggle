{"cell_type":{"90975092":"code","4ee16505":"code","ac69c679":"code","006bb9f0":"code","5eb263de":"code","6b64c51b":"code","dea429be":"code","0b09e66f":"code","2d14a93e":"code","d1348590":"code","61e69115":"code","16c3d4e0":"code","82897d42":"code","bb7ca0e2":"code","eff21f71":"code","eb7f8910":"code","f74f7c2d":"code","a269914b":"code","3cc69743":"code","c6a6a6de":"code","666663c5":"code","0d036c29":"code","eff39ed5":"code","56007ecc":"code","c01c3658":"code","fcebb047":"code","0ed736b0":"code","19993ca6":"code","ac6e7edc":"code","367279ff":"code","d7e3da09":"code","80fb8a6f":"code","29b7a512":"code","30bcc3b5":"code","10cd5879":"markdown","4bd43f70":"markdown","2fa8949d":"markdown","30314e9c":"markdown","3a08a0f2":"markdown","f8ee4b4a":"markdown","5935a26d":"markdown","13ec7324":"markdown","603e237b":"markdown","bf519c0d":"markdown"},"source":{"90975092":"# # Uncomment and execute this code to clear the Kaggle kernel's working\n# # directory of all files and directories produced by this notebook:\n# !rm -r train\n# !rm -r test\n# !rm train_annotations.csv\n# !rm label_key.csv\n# !rm Model_state_dict.pt","4ee16505":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport cv2","ac69c679":"np.random.seed(1)\nrandom.seed(1)","006bb9f0":"src_dir = '..\/input\/captcha-images\/captcha_images'\nimg_paths = [f for f in os.listdir(src_dir) if f.lower().endswith(('.png', '.jpg'))]","5eb263de":"# View an example CAPTCHA\n\nimg = cv2.imread(os.path.join(src_dir, img_paths[0]))\nplt.imshow(img)\nplt.title(f'Shape: {img.shape}')\nplt.show()","6b64c51b":"# Get list of ground-truth labels from image filenames\nlabels = [img_path.split('.')[0] for img_path in img_paths]\n\n# Get count of each unique char\nchar_counts = {}\nfor label in labels:\n    for char in label:\n        if char not in char_counts:\n            char_counts[char] = 1\n        else:\n            char_counts[char] += 1\n            \n# Sort by character\nchar_counts = sorted(char_counts.items())\n# Convert items to two stand-alone lists\nchars = [char_count[0] for char_count in char_counts]\ncounts = [char_count[1] for char_count in char_counts]\n# Want first char on top, so reverse order\nchars.reverse()\ncounts.reverse()","dea429be":"plt.figure(figsize=(10, 10))\n\nbars = plt.barh(y=chars, width=counts, height=0.6)\nfor bar in bars:\n    w = bar.get_width()\n    y = bar.get_y()\n    h = bar.get_height()\n    plt.text(w+40, y+h\/2, w, ha='center', va='center')\n    \nplt.xlim(0, 1500)\nplt.grid(linestyle='-', axis='x')\nplt.title(f'Character counts in CAPTCHA labels ({len(chars)} unique characters)')\nplt.show()","0b09e66f":"fig = plt.figure(figsize=(14, 10))\n\nplt_rows = 4\nplt_cols = 2\nplt_iter = 1\n\nplt.subplots_adjust(hspace=0.5)\n\nfor i in range(plt_rows*plt_cols):\n    plt.subplot(plt_rows, plt_cols, plt_iter)\n    \n    img_index = np.random.randint(0, len(img_paths))\n    # Load random image\n    img = cv2.imread(os.path.join(src_dir, img_paths[img_index]))\n    # Covert to grayscale\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Take binary threshold\n    ret, thresh = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY)\n    # Invert image\n    bit_not = cv2.bitwise_not(thresh)\n    # Find contours\n    contours, hierarchy = cv2.findContours(bit_not, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n    \n    # Draw on orignal image\n    cv2.drawContours(img, contours, -1, (0, 255, 0), 1)\n    # Get bounding rect of each contour\n    rects = [cv2.boundingRect(c) for c in contours]\n    # Sort rects by their width\n    rects.sort(key=lambda x: x[2])\n    \n    # Deal with touching letters where one wide bounding box\n    # envlopes two letters. split these in half\n    while len(rects) < 4:\n        # Pop widest rect\n        wide_rect = rects.pop()\n        x, y, w, h = wide_rect\n        # Split in two\n        first_half = (x, y, w\/\/2, h)\n        second_half = (x+w\/\/2, y, w\/\/2, h)\n        rects.append(first_half)\n        rects.append(second_half)\n        # Re-sort rects by their width\n        rects.sort(key=lambda x: x[2])\n    \n    for rect in rects:\n        x, y, w, h = rect\n        # Buffer rect by 1 pixel\n        cv2.rectangle(img, (x-1, y-1), (x+w+1, y+h+1), (255, 0, 0), 1)\n    \n    plt.imshow(img)\n    plt_iter += 1","2d14a93e":"from cnn_captcha_solver_utility_segmenter import Segmenter","d1348590":"# Demonstrate on random image\nimg_index = np.random.randint(0, len(img_paths))\n\nSegmenter = Segmenter()\nsegmented_chars = Segmenter.segment_chars(os.path.join(src_dir, img_paths[img_index]), plot=True)","61e69115":"from cnn_captcha_solver_utility_data_generator import DataGenerator","16c3d4e0":"DataGenerator = DataGenerator(src_dir, train_size=0.75, random_seed=1)","82897d42":"# Extract training data from training set CAPTCHA images,\n# generating ~4*len(test_set) individual character images\n\nDataGenerator.extract_train_set('train', train_annotation_file='train_annotations.csv')","bb7ca0e2":"# Copy the test set CAPTCHA images to test folder, images\n# will remain unsegmented until model testing\n\nDataGenerator.save_test_set('test')","eff21f71":"# Save a label key file that shows match between CAPTCHA\n# characters and their corresponding integer label, e.g.\n# '2' = 0, 'G' = 14, 'Q' = 22\n\nDataGenerator.save_label_dict('label_key.csv')","eb7f8910":"from cnn_captcha_solver_utility_chars_dataset import CharsDataset\nfrom torch.utils.data import random_split, DataLoader","f74f7c2d":"CharsDataset = CharsDataset('train_annotations.csv', 'train')","a269914b":"# What is 80% of dataset size?\nround(len(CharsDataset)*0.8)","3cc69743":"train_set, val_set = random_split(CharsDataset, [24000, len(CharsDataset)-24000])","c6a6a6de":"train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset=val_set, batch_size=32, shuffle=True)","666663c5":"from cnn_captcha_solver_utility_model import Model\nimport torch\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.autograd import Variable","0d036c29":"Model = Model()","eff39ed5":"val_iter = iter(val_loader)","56007ecc":"optimizer = Adam(Model.parameters(), lr=0.001)\ncriterion = CrossEntropyLoss()\nepochs = 5","c01c3658":"# Training and validation of model\n\n# Will accumulate loss for learning curve plot\ntrain_loss_history = []\nval_loss_history = []\n\nfor epoch in range(epochs):        \n    for step, (x, y) in enumerate(train_loader):\n        b_x = Variable(x)   # batch x (char image)\n        b_y = Variable(y)   # batch y (target label, as int)\n        output = Model(b_x)\n        loss = criterion(output, b_y) \n        train_loss_history.append(loss.data)\n        optimizer.zero_grad()           \n        loss.backward()                 \n        optimizer.step()\n        \n        # Evaluate on validation set every 50 batches\n        if step % 50 == 0:\n            val_x, val_y = next(val_iter)\n            val_output = Model(val_x)\n            val_loss = criterion(val_output, val_y)\n            val_loss_history.append(val_loss.data)\n            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data, '| val loss: %.4f' % val_loss.data)","fcebb047":"# Plotting learning curve\n\ntrain_loss_history_x = np.arange(0, len(train_loss_history))\n\n# Only have validation loss every 50 batches\nval_loss_history_x = np.arange(0, len(val_loss_history)*50, 50)\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_loss_history_x, train_loss_history, label='Training loss')\nplt.plot(val_loss_history_x, val_loss_history, label='Validation loss')\nplt.legend()\nplt.title('Learning curves for CNN')\nplt.xlabel('Training step')\nplt.ylabel('Loss')\nplt.show()","0ed736b0":"# Save state dictionary of trained model\ntorch.save(Model.state_dict(), 'Model_state_dict.pt')","19993ca6":"from cnn_captcha_solver_utility_evaluator import Evaluator","ac6e7edc":"Evaluator = Evaluator(Model,\n                      transform=CharsDataset.transform,\n                      label_key='label_key.csv',\n                      model_state_dict='Model_state_dict.pt')","367279ff":"EvaluatorResults = Evaluator.evaluate_test_set('test')","d7e3da09":"EvaluatorResults.captcha_accuracy","80fb8a6f":"EvaluatorResults.char_accuracy","29b7a512":"EvaluatorResults.plot_confusion_matrix()","30bcc3b5":"prediction = Evaluator.predict(os.path.join(src_dir, 'L4MJ.png'), print_results=True, plot_segmentation=True)","10cd5879":"# Prepare PyTorch Dataset and Data Loaders\nA custom PyTorch dataset will be defined using the ```CharsDataset``` class ([see on GitHub](https:\/\/github.com\/t-ott\/cnn-captcha-solver\/blob\/master\/cnn_captcha_solver\/chars_dataset.py#L8)).","4bd43f70":"# Basic EDA","2fa8949d":"# Character Segmentation","30314e9c":"# Generate Training Data\nUtilizing the ```DataGenerator``` class ([see on GitHub](https:\/\/github.com\/t-ott\/cnn-captcha-solver\/blob\/master\/cnn_captcha_solver\/data_generator.py#L9)) to extract and save individual character images to a target directory to be used as training data for the CNN model.","3a08a0f2":"# Potential Improvements\n\nThe error rate of the character classification model is quite low. In order to correctly solve a given CAPTCHA, every character needs to be correctly classified. If only one of the four characters is incorrectly classified, the CAPTCHA prediction will be incorrect. With this current approach, the only way to improve the overall accuracy of CAPTCHA predictions is to:\n1) Improve the character segmentation process to provide cleaner\/more reliable training data for the classification model and re-train, or\n2) Improve the character classification model itself.\n\nFurther alterations to the classification model architecture may improve the classification accuracy. No systematic hyperparameter tuning has been performed on the model thus far. My gut says that character segmentation improvements would likely be more effective. Dealing with \"smushed\" pairs of letters is currently done by splitting the pair in half. This does not work very well with letters that have inconsisent widths. For example:","f8ee4b4a":"This segementation process was written to the ```Segmenter``` class in the ```cnn_captcha_solver``` package ([see GitHub repo](https:\/\/github.com\/t-ott\/cnn-captcha-solver)). ***To make it easier to integrate with the Kaggle kernel, this package was split into Kaggle utility scripts and added to this notebook.***","5935a26d":"# Evaluate Model\nEvaluate the trained model on the test set using the ```Evaluator``` class of the ```cnn_captcha_solver``` package ([see on GitHub](https:\/\/github.com\/t-ott\/cnn-captcha-solver\/blob\/master\/cnn_captcha_solver\/evaluator.py#L11)).","13ec7324":"This notebook demonstrates data pre-processing, model training, and model evaluation for an optical character recognition (OCR) system powered by a convolution neural network character classifier. The system predicts the text shown in basic four-character CAPTCHA images generated by the [Really Simple CAPTCHA WordPress plugin](https:\/\/wordpress.org\/plugins\/really-simple-captcha\/#description).\n\nThe approach I took was directly informed by [Adam Geitgey's blog post \"How to break a CAPTCHA system in 15 minutes with Machine Learning\"](https:\/\/medium.com\/@ageitgey\/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710). The technique is **very** similar (OpenCV character segementation pre-processing to train a CNN character classifier), but the model built here uses PyTorch rather than Keras. It also took me much longer than 15 minutes.\n\n**Check out the [corresponding GitHub repo](https:\/\/github.com\/t-ott\/cnn-captcha-solver) for this project as well, which contains the source code of the Python package used in this notebook.**","603e237b":"Another problem with this system is it only manages CAPTCHA text that is four characters in length. It could not manage variable length CAPTCHA text.","bf519c0d":"# Train CNN Model\nTraining a convolutional neural network model built with PyTorch. Architecture is defined in the ```Model``` class of the ```cnn_captcha_solver``` package ([see on GitHub](https:\/\/github.com\/t-ott\/cnn-captcha-solver\/blob\/master\/cnn_captcha_solver\/model.py#L3))."}}