{"cell_type":{"698fd160":"code","4db6c35b":"code","ccecca94":"code","b625549f":"code","caeb52c5":"code","4767e34f":"code","702144bf":"code","ea2049a1":"code","9ecdf7ea":"code","78598916":"code","458aba2d":"code","378ceb50":"code","c61b5a7f":"code","a63e0560":"code","9f2f0474":"code","0fcb9d65":"code","40403c43":"code","42cc55cb":"code","99963f7c":"code","360ea986":"code","7ea4800a":"code","56d168df":"code","2a799d91":"code","6b5b5f32":"code","ded83a18":"code","bfcb5eec":"code","709fee82":"code","2b7741b2":"code","be05a8e3":"code","01962383":"code","cd7585d5":"markdown","17700a54":"markdown","ed0e00a0":"markdown","96dbc2f8":"markdown","b2039e3c":"markdown","b2366c88":"markdown","2ba6cad0":"markdown","bf955ac9":"markdown","15c12280":"markdown","e4b72727":"markdown","b90cb2f4":"markdown","dd5d1202":"markdown","9a0bd561":"markdown","e6380623":"markdown","524cd9d9":"markdown","036f9d2a":"markdown","c6ff3de5":"markdown","b643f754":"markdown","c35a2a79":"markdown","5deeee88":"markdown"},"source":{"698fd160":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\nimport seaborn as sns\n\n# Any results you write to the current directory are saved as output.","4db6c35b":"import xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score","ccecca94":"df = pd.read_csv('..\/input\/bigml_59c28831336c6604c800002a.csv')\ndf.head(5)","b625549f":"df = df.drop(['phone number'],axis=1)\ndf.shape","caeb52c5":"df.isnull().sum()","4767e34f":"print(\"------  Data Types  ----- \\n\",df.dtypes)\nprint(\"------  Data type Count  ----- \\n\",df.dtypes.value_counts())","702144bf":"cate = [key for key in dict(df.dtypes) if dict(df.dtypes)[key] in ['bool', 'object']]","ea2049a1":"le = preprocessing.LabelEncoder()\nfor i in cate:\n    le.fit(df[i])\n    df[i] = le.transform(df[i])\n    ","9ecdf7ea":"corrmat = df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","78598916":"y = df['churn']\ndf = df = df.drop(['churn'],axis=1)","458aba2d":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(df, y)\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(clf, max_num_features=50, height=0.8, ax=ax)\nplt.show()","378ceb50":"xtrain, xvalid, ytrain, yvalid = train_test_split(df, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","c61b5a7f":"print(xtrain.shape, xvalid.shape, ytrain.shape, yvalid.shape)","a63e0560":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\nlr = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","9f2f0474":"algo = pd.DataFrame([lr])","0fcb9d65":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\nxg = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","40403c43":"algo = algo.append([xg])","42cc55cb":"clf = MultinomialNB()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\nmnb = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","99963f7c":"algo = algo.append([mnb])","360ea986":"clf = AdaBoostClassifier()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\nabc = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","7ea4800a":"algo = algo.append([abc])","56d168df":"clf = KNeighborsClassifier()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\nknc = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","2a799d91":"algo = algo.append([knc])","6b5b5f32":"clf = GradientBoostingClassifier()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\ngbc = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","ded83a18":"algo = algo.append([gbc])","bfcb5eec":"clf = ExtraTreesClassifier()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\netc = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","709fee82":"algo = algo.append([etc])","2b7741b2":"clf = DecisionTreeClassifier()\nclf.fit(xtrain, ytrain)\npredictions = clf.predict(xvalid)\nprint(\"accuracy_score\",accuracy_score(yvalid, predictions))\nprint(\"auc\",roc_auc_score(yvalid, predictions))\ndtc = [clf.__class__,accuracy_score(yvalid, predictions),roc_auc_score(yvalid, predictions)]","be05a8e3":"algo = algo.append([dtc])","01962383":"algo.sort_values([1], ascending=[False])","cd7585d5":"### MultinomialNB","17700a54":"### LogisticRegression","ed0e00a0":"### AdaBoostClassifier","96dbc2f8":"### Load Data set","b2039e3c":"### Cover all Classification Algorithm\n* LogisticRegression\n* XGBClassifier\n* MultinomialNB\n* AdaBoostClassifier\n* KNeighborsClassifier\n* GradientBoostingClassifier\n* ExtraTreesClassifier\n* DecisionTreeClassifier ","b2366c88":"### Label Encoding for Catergorical Variable ","2ba6cad0":"### ExtraTreesClassifier","bf955ac9":"### Load all Classification Packages and Accuracy Packages","15c12280":"### Correlation Plot","e4b72727":"## Road Map \n* Library for Preprocessing and Cleaning\n* Load all Classification Packages and Accuracy Packages\n* Load Data Set\n* Analyse the Data \n* LabelEncoder\n* Split the Data Train and Validation\n* Train Model and Check Validation Data Accuracy","b90cb2f4":"### KNeighborsClassifier","dd5d1202":"# Approaching (Almost) Any Churn Predication Problem for Classification on Kaggle\n\n![](https:\/\/i0.wp.com\/www.everythingai.co.in\/wp-content\/uploads\/2018\/01\/Churn.png?resize=900%2C450&ssl=1)\n\nIn this post, I'll talk about approaching churn predication problems on Kaggle. As an example, we will use the data from this competition. I have create a very basic all classification model first and then improve algorithm parameter.\n\n## Cover all Classification Algorithm\n* LogisticRegression\n* XGBClassifier\n* MultinomialNB\n* AdaBoostClassifier\n* KNeighborsClassifier\n* GradientBoostingClassifier\n* ExtraTreesClassifier\n* DecisionTreeClassifier","9a0bd561":"###  Remove Column, Shape, Null Value and Data Type :---- Over_View","e6380623":"### Split Train and Validation Dataset","524cd9d9":"### Library for Preprocessing and Cleaning","036f9d2a":"### Feature Important by XGB\n\nusing XGBClassifier i have achive great accurcy so i have take insight of which one feature  ","c6ff3de5":"### GradientBoostingClassifier","b643f754":"### XGBClassifier","c35a2a79":"### The Importance of Predicting Customer Churn\n\nThe ability to predict that a particular customer is at a high risk of churning, while there is still time to do something about it, represents a huge additional potential revenue source for every online business. Besides the direct loss of revenue that results from a customer abandoning the business, the costs of initially acquiring that customer may not have already been covered by the customer\u2019s spending to date. (In other words, acquiring that customer may have actually been a losing investment.) Furthermore, it is always more difficult and expensive to acquire a new customer than it is to retain a current paying customer.\n\nReference : [Link](https:\/\/www.optimove.com\/learning-center\/customer-churn-prediction-and-prevention)","5deeee88":"### DecisionTreeClassifier"}}