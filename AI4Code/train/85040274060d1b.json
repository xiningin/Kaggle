{"cell_type":{"fc341da8":"code","46ef2197":"code","c3293749":"code","c9e0e018":"code","1bb4b982":"code","3252414f":"code","504d5b68":"code","a1358a32":"code","47fe433f":"code","fa499264":"code","115e37ae":"code","0dfbbf5c":"code","4db95c6d":"code","18342789":"code","5bdf7a48":"code","95accbd6":"code","326c1730":"code","5c4ad86c":"code","881427e2":"code","793d475e":"code","1ed1404a":"code","99b1a810":"code","695e9a32":"code","3dece550":"code","4be87c75":"code","ce0d2963":"code","db14217b":"code","ffda0326":"code","2d7cdfad":"code","53b8a83b":"code","5c3fec8f":"code","ef0e8180":"code","42d54d11":"code","468f01f9":"code","095f1602":"code","edc657a1":"code","5b929d75":"code","6fd4e9ed":"code","84a5498b":"code","6dd20b3e":"code","7409eb46":"code","a522b04d":"code","940a622a":"code","9006f9d0":"code","47cf3e1a":"code","0cfc99bf":"code","0d61d822":"code","a7be80e0":"code","827f968c":"code","bbde777f":"code","cbd3e020":"markdown","40485769":"markdown","fcb2bbfa":"markdown","0f62f919":"markdown","f863372e":"markdown","c78f4399":"markdown","5029c316":"markdown","ce69babd":"markdown","d3dbdf16":"markdown","6bae355b":"markdown","1b1ea899":"markdown","f49d7c18":"markdown","ce0d1f0a":"markdown","1cab7456":"markdown","93bffec2":"markdown","752d02a2":"markdown","d4a08d91":"markdown","448a20c9":"markdown","a4bb8419":"markdown","461ad801":"markdown","bda8f7a9":"markdown","b7d2de3f":"markdown"},"source":{"fc341da8":"import pandas\nimport torch\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport os","46ef2197":"train_transforms = [\n    transforms.RandomRotation(degrees=(-90,90)),\n    transforms.ColorJitter(),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(),\n    transforms.Resize(size=[224,224]),\n    transforms.ToTensor(),\n        transforms.Normalize([0.5],[0.5])\n]","c3293749":"dataset = datasets.ImageFolder(root='..\/input\/dogs-data', transform=transforms.Compose(train_transforms)) \n\ndataset_size = len(dataset)\n\nindices = list(range(dataset_size))\nvalidation_split = int(np.floor(dataset_size*0.2))\n\nnp.random.shuffle(indices)\ntrain_indices, validation_indices = indices[validation_split:], indices[:validation_split]\n\ndataset_size = len(train_indices)\nindices = list(range(dataset_size))\nholdout_split = int(np.floor(dataset_size*0.25))\n\nnp.random.shuffle(indices)\ntrain_indices, holdout_indices = indices[holdout_split:], indices[:holdout_split]\nprint(\"The size of holdout dataset \" + str(len(holdout_indices)))\nprint(\"The size of training dataset \" + str(len(train_indices)))\nprint(\"The size of validation dataset \" + str(len(validation_indices)))","c9e0e018":"train = torch.utils.data.SubsetRandomSampler(train_indices) \nvalid = torch.utils.data.SubsetRandomSampler(validation_indices)\nholdout = torch.utils.data.SubsetRandomSampler(holdout_indices)","1bb4b982":"train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n                                           sampler=train, drop_last=False)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n                                                sampler=valid, drop_last=False)\nholdout_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n                                                sampler=holdout, drop_last=False)","3252414f":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))","504d5b68":"device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)","a1358a32":"def train(model, train_loader, validation_loader):\n    epochs = 20\n    training_losses = []\n    validation_losses = []\n\n    training_acc = []\n    validation_acc = []\n    \n    loss_criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n    for e in range(epochs):\n        print(\"In epoch\", e)\n        running_loss = 0\n        running_acc = 0\n        model.train()\n        for images, labels in train_loader: \n            images, labels = images.to(device), labels.to(device)\n            predictions = model(images).to(device) \n            _, preds = torch.max(predictions, 1)\n            loss = loss_criterion(predictions, labels) \n            optimizer.zero_grad() \n            loss.backward() \n            optimizer.step()\n\n            running_acc += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n            running_loss += loss.item()\n        valid_loss = 0\n        valid_acc = 0\n        model.eval()\n        for images, labels in validation_loader: \n            images, labels = images.to(device), labels.to(device)\n            with torch.no_grad():\n                predictions = model(images).to(device)\n                _, preds = torch.max(predictions, 1)\n                loss = loss_criterion(predictions, labels)\n            valid_loss += loss.item()\n            valid_acc += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        print(f\"\\tTraining loss: {running_loss\/len(train_loader)}\")\n        print(f\"\\tValidation loss: {valid_loss\/len(validation_loader)}\")   \n        print(f\"\\n\\tTraining accuracy: {running_acc\/len(train_loader)}\")\n        print(f\"\\tValidation accuracy: {valid_acc\/len(validation_loader)}\")  \n        training_losses.append(running_loss\/len(train_loader))\n        validation_losses.append(valid_loss\/len(validation_loader))\n        training_acc.append(running_acc\/len(train_loader))\n        validation_acc.append(valid_acc\/len(validation_loader))\n    return training_losses, validation_losses, training_acc, validation_acc, model, loss_criterion","47fe433f":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","fa499264":"random_ds = datasets.ImageFolder(root='..\/input\/randdogs2\/random imgs', transform=transforms.Compose(train_transforms)) \nrandom_dl = torch.utils.data.DataLoader(random_ds, batch_size=1, drop_last=False)","115e37ae":"for images, labels in random_dl:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n#         _, preds = torch.argmax(predictions)\n    plt.figure()\n    plt.title('Actual: ' + str(labels[0].cpu().numpy()) + ' Predicted: ' + str(np.argmax(predictions.cpu().numpy())))\n    plt.imshow(images[0].cpu().permute(1, 2, 0) )","0dfbbf5c":"def create_loader(root):\n    dataset = datasets.ImageFolder(root=root, transform=transforms.Compose(train_transforms)) \n\n    dataset_size = len(dataset)\n\n    indices = list(range(dataset_size))\n    validation_split = int(np.floor(dataset_size*0.2))\n\n    np.random.shuffle(indices)\n    train_indices, validation_indices = indices[validation_split:], indices[:validation_split]\n\n    dataset_size = len(train_indices)\n    indices = list(range(dataset_size))\n    holdout_split = int(np.floor(dataset_size*0.25))\n\n    np.random.shuffle(indices)\n    train_indices, holdout_indices = indices[holdout_split:], indices[:holdout_split]\n    print(\"The size of holdout dataset \" + str(len(holdout_indices)))\n    print(\"The size of training dataset \" + str(len(train_indices)))\n    print(\"The size of validation dataset \" + str(len(validation_indices)))\n\n    train = torch.utils.data.SubsetRandomSampler(train_indices) \n    valid = torch.utils.data.SubsetRandomSampler(validation_indices)\n    holdout = torch.utils.data.SubsetRandomSampler(holdout_indices)\n\n    train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, \n                                               sampler=train, drop_last=False)\n    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n                                                    sampler=valid, drop_last=False)\n    holdout_loader = torch.utils.data.DataLoader(dataset, batch_size=32,\n                                                sampler=holdout, drop_last=False)\n    return train_loader, validation_loader, holdout_loader","4db95c6d":"train_loader, validation_loader, holdout_loader = create_loader('..\/input\/norfolkdane\/Norfolk-Dane')","18342789":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","5bdf7a48":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","95accbd6":"hold_loss = 0\naccuracy = 0\nfor images, labels in holdout_loader:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        accuracy += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        hold_loss += loss.item()\ntest_accuracy = accuracy.double() \/ len(holdout_loader)\ntest_loss = hold_loss \/ (len(holdout_loader))\nprint('Holdout accuracy: ', test_accuracy.item())\nprint('Holdout loss: ', test_loss)","326c1730":"plt.plot(training_losses, label='Training loss')\nplt.plot(validation_losses, label='Validation loss')\nplt.show()","5c4ad86c":"plt.plot(training_accuracys, label='Training accuracy')\nplt.plot(validation_accuracys, label='Validation accuracy')\nplt.show()","881427e2":"train_loader, validation_loader, holdout_loader = create_loader('..\/input\/norfnorw\/Norfolk-Norwich')","793d475e":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","1ed1404a":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","99b1a810":"hold_loss = 0\naccuracy = 0\nfor images, labels in holdout_loader:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        accuracy += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        hold_loss += loss.item()\ntest_accuracy = accuracy.double() \/ len(holdout_loader)\ntest_loss = hold_loss \/ (len(holdout_loader))\nprint('Holdout accuracy: ', test_accuracy.item())\nprint('Holdout loss: ', test_loss)","695e9a32":"plt.plot(training_losses, label='Training loss')\nplt.plot(validation_losses, label='Validation loss')\nplt.show()","3dece550":"plt.plot(training_accuracys, label='Training accuracy')\nplt.plot(validation_accuracys, label='Validation accuracy')\nplt.show()","4be87c75":"train_loader, validation_loader, holdout_loader = create_loader('..\/input\/norwichdane\/Norwich-Dane')","ce0d2963":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","db14217b":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","ffda0326":"hold_loss = 0\naccuracy = 0\nfor images, labels in holdout_loader:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        accuracy += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        hold_loss += loss.item()\ntest_accuracy = accuracy.double() \/ len(holdout_loader)\ntest_loss = hold_loss \/ (len(holdout_loader))\nprint('Holdout accuracy: ', test_accuracy.item())\nprint('Holdout loss: ', test_loss)","2d7cdfad":"plt.plot(training_losses, label='Training loss')\nplt.plot(validation_losses, label='Validation loss')\nplt.show()","53b8a83b":"plt.plot(training_accuracys, label='Training accuracy')\nplt.plot(validation_accuracys, label='Validation accuracy')\nplt.show()","5c3fec8f":"train_loader, validation_loader, holdout_loader = create_loader('..\/input\/mislabelled\/')","ef0e8180":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","42d54d11":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","468f01f9":"hold_loss = 0\naccuracy = 0\nfor images, labels in holdout_loader:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        accuracy += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        hold_loss += loss.item()\ntest_accuracy = accuracy.double() \/ len(holdout_loader)\ntest_loss = hold_loss \/ (len(holdout_loader))\nprint('Holdout accuracy: ', test_accuracy.item())\nprint('Holdout loss: ', test_loss)","095f1602":"plt.plot(training_losses, label='Training loss')\nplt.plot(validation_losses, label='Validation loss')\nplt.show()","edc657a1":"plt.plot(training_accuracys, label='Training accuracy')\nplt.plot(validation_accuracys, label='Validation accuracy')\nplt.show()","5b929d75":"import cv2","6fd4e9ed":"def get_avg_image(base_dir, dog_name):\n    #read 1st image\n    img1 = cv2.imread(base_dir + os.listdir(base_dir)[0])\n    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n    img1 = cv2.resize(img1, (224, 224))\n\n    #read other images and accumulate\n    mean_img = np.array(img1)\n    for img_path in os.listdir(base_dir)[1:]:\n        img = cv2.imread(base_dir+img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (224, 224))\n        mean_img += np.array(img)\n        \n    #find mean and plot\n    mean_img = mean_img \/ len(os.listdir(base_dir))\n    mean_img = np.round(mean_img)\n    plt.figure()\n    plt.title(dog_name)\n    plt.imshow(mean_img)","84a5498b":"get_avg_image('\/kaggle\/input\/dogs-data\/n02094114-Norfolk_terrier\/', 'Norfolk Terrier')\nget_avg_image('\/kaggle\/input\/dogs-data\/n02094258-Norwich_terrier\/', 'Norwich Terrier')\nget_avg_image('\/kaggle\/input\/dogs-data\/n02109047-Great_Dane\/', 'Great Dane')","6dd20b3e":"train_loader, validation_loader, holdout_loader = create_loader('..\/input\/dogs-data\/')","7409eb46":"model = models.densenet201(pretrained=False)\nFREEZE = False\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","a522b04d":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","940a622a":"hold_loss = 0\naccuracy = 0\nfor images, labels in holdout_loader:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        accuracy += torch.sum(torch.argmax(nn.Softmax(dim=1)(predictions), dim=1) == labels.data) \/ len(labels)\n        hold_loss += loss.item()\ntest_accuracy = accuracy.double() \/ len(holdout_loader)\ntest_loss = hold_loss \/ (len(holdout_loader))\nprint('Holdout accuracy: ', test_accuracy.item())\nprint('Holdout loss: ', test_loss)","9006f9d0":"plt.plot(training_losses, label='Training loss')\nplt.plot(validation_losses, label='Validation loss')\nplt.show()","47cf3e1a":"plt.plot(training_accuracys, label='Training accuracy')\nplt.plot(validation_accuracys, label='Validation accuracy')\nplt.show()","0cfc99bf":"model = models.densenet201(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeatures_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(features_num, 3))\n\nmodel = model.to(device)","0d61d822":"training_losses, validation_losses, training_accuracys, validation_accuracys, model, loss_criterion = train(model, \n                                                                                            train_loader, \n                                                                                            validation_loader)","a7be80e0":"from PIL import Image","827f968c":"def saliency_map(img_path, model, transform):\n    #open and preprocess images\n    img = Image.open(img_path)\n    img = transforms.Compose(transform)(img)\n    img = img.reshape(1, 3, 224, 224)\n    img = img.to(device)\n    img.requires_grad_()\n    output = model(img)\n    output_idx = output.argmax()\n    output_max = output[0, output_idx]\n    output_max.backward()\n    saliency, _ = torch.max(img.grad.data.abs(), dim=1) \n    saliency = saliency.reshape(224, 224)\n    image = img.reshape(-1, 224, 224)\n    \n    #plot images\n    fig, ax = plt.subplots(1, 2)\n    fig.set_figheight(3)\n    fig.set_figwidth(3)\n    ax[0].imshow(image.cpu().detach().numpy().transpose(1, 2, 0))\n    ax[0].axis('off')\n    ax[1].imshow(saliency.cpu(), cmap='hot')\n    ax[1].axis('off')\n    plt.tight_layout()\n    plt.show()","bbde777f":"for folder in os.listdir('..\/input\/dogs-data\/'):\n    for img in os.listdir('..\/input\/dogs-data\/' + folder):\n        saliency_map('..\/input\/dogs-data\/' + folder + \"\/\" + img, model, train_transforms)\n        ","cbd3e020":"1. I think if the model has only heads of dogs it will perform better, since images will be similar and it would be easier to distinguish them. However, then it would mean that it would only work on images with just heads. What if we have some different images on test set. Then this could be a problem.\n2. Having just backgrounds would hurt performance of the model and would give no information about dog breeds. The performance of the model cannot be better than random.\n3. I think if the model is not trained on pre-trained weights it would need to train a lot more. Having more number of epochs would help to improve performance. Otherwise, performance would be bad. Demonstration is given below:","40485769":"Now I train separately for different dogs:","fcb2bbfa":"As expected, this mislabelled datasets causes the model to perform worse than the initial version.","0f62f919":"2. Metamorphic Testing\n\nThis study focuses on hypotheses connected with metamorphic relations. They provide an extensive list of metamorphic relations, which can be described as what one might expect the model to do if the condition is present. For example, users expect that when we add additional valuable information, the model should perform better, and vice versa. The study shows how we can use these common expectations to validate the models.","f863372e":"1. Saliency Maps\n\nThis study shows how saliency maps are used and try to test whether this an effective method. They analyse CNNs and make an experiment where people try to guess the performance of the model. Some of them are given saliency maps and classification metrics, some just one of them, and some none. The results show that the highest guess rate is for those cases when both are given. The study supports the benefit of saliency maps in testing and validating machine learning models, especially CNNs.","c78f4399":"Here we see much worse performance since those two breeds are very alike.","5029c316":"The perfomance of this model is also very good since these breeds are not very alike:","ce69babd":"Saliency maps show where the gradients are of high value and which parts make most effect on results. More red parts are those points. Overall, the model does not a bad job. Over a half of images and their saliency maps make sense. You can see below:","d3dbdf16":"### Average Images","6bae355b":"Here i show labels create by my model:","1b1ea899":"### Saliency Maps","f49d7c18":"### Norfolk terrier and Great Dane","ce0d1f0a":"### Issues with dataset:\n1. Different backgrounds\n2. Different sizes of dogs\n3. Only heads in some images, whole body on others.\n4. Different colors of dogs\n5. Different lights\n6. People on some images.\n7. Other dogs present.","1cab7456":"### Biases in the Dataset","93bffec2":"### Scenarios","752d02a2":"### Norfolk - Norwich","d4a08d91":"As we can see from below, the model does much better in distinguishing Norfolk Terrier and Dane:","448a20c9":"### Norwich - Dane","a4bb8419":"I don't think there are any biases in dataset, since all three folders contain similar amount of dogs. There are quality problems present in the dataset, but I don't see any biases. \nPossible biases that could be present in the dataset: \n- prevalence of one dog category over other in terms of quantity, \n- better quality of images for some dogs, worse for other, etc.","461ad801":"### Mislabelled Images","bda8f7a9":"### Papers","b7d2de3f":"I am adding model from project 2:"}}