{"cell_type":{"91231681":"code","5d3a482b":"code","5db230ee":"code","2125b70f":"code","a87a627d":"code","77cd3124":"code","32d6546e":"code","121b472d":"code","498e1262":"code","af5cee6d":"code","84613c08":"code","3f7e3f08":"code","507f475d":"code","433a9fcd":"code","8f532cd7":"code","97f039e8":"code","1b766d13":"code","a3278c9c":"code","b2d4e31d":"code","6f103ed3":"code","aef306f4":"code","904341b5":"code","110af66b":"code","76b18d0b":"code","d7ee0c37":"markdown","1c1797a6":"markdown","d5a8243b":"markdown","26336010":"markdown","89d763b2":"markdown","b436ee00":"markdown","fb74b36d":"markdown","9d14d600":"markdown","d99f093c":"markdown","a5160c37":"markdown","0d4cdfe5":"markdown","b28fe954":"markdown","079f755f":"markdown","391dc434":"markdown","5ece478e":"markdown","d8d916b0":"markdown","0d383e25":"markdown","b388487e":"markdown","01760f87":"markdown","8bebf31a":"markdown"},"source":{"91231681":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow\timport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import InputLayer\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom scipy.stats import randint as sp_randint","5d3a482b":"# Reading admissions dataset and viewing the first few rows\ndataset=pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')\ndataset.head()","5db230ee":"# Dropping serial number as it has no influence on our prediction\ndataset.drop('Serial No.', axis=1, inplace=True)","2125b70f":"# Extracting feature values and labels(last column includes admission percentage)\nfeatures=dataset.iloc[:,0:-1]\nlabels=dataset.iloc[:,-1]","a87a627d":"# Creating training and testing sets\n# test_size set at 0.30 as a modest split for evaluation of model;\n# random_state set to an int for easily-reproducible results\nfeatures_train,features_test,labels_train,labels_test=train_test_split(\nfeatures,labels,test_size=0.30,random_state=1)","77cd3124":"# Performing standardisation to scale all numeric feature values\nsc=StandardScaler()\nfeatures_train_scale=sc.fit_transform(features_train)\nfeatures_test_scale=sc.transform(features_test)\nfeatures_train_scale=pd.DataFrame(features_train_scale,columns=features_train.columns)\nfeatures_test_scale=pd.DataFrame(features_test_scale,columns=features_test.columns)","32d6546e":"# Number of input features that form the number of input nodes \n# Used to construct the model in the next step\nfeatures_train_scale.shape[1]","121b472d":"# Defining model\n# Options: number of hidden layers, number of nodes per layer, activation functions\n        # type of optimisation function, loss function, evaluation metrics\n# Given a linear regression problem, loss function=mean squared error and\n# evaluation metrix=mean absolute error\ndef design_model():\n    model=Sequential()\n    inputL=InputLayer(input_shape=(7,))\n    model.add(inputL)\n    model.add(Dense(16,activation='relu')) #relu activation function for linear regression\n    #------Alternatives------\n    #model.add(layers.Dropout(0.1)) \n    #model.add(Dense(16,activation='relu'))\n    #model.add(layers.Dropout(0.1))\n    #------Alternatives------\n    model.add(Dense(1))\n    opt=Adam(learning_rate=0.001) # Choosing the Adam optimizer as a starter\n    model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n    return model","498e1262":"# Carrying out grid search to optimise batch-size and epochs\nbatch_size=[2,4,8]\nepochs=[20,40,80]\nmodel=KerasRegressor(build_fn=design_model)\nparam_grid=dict(batch_size=batch_size, epochs=epochs)\ngrid=GridSearchCV(estimator=model, param_grid=param_grid,\n                  scoring=make_scorer(mean_squared_error, greater_is_better=False),\n                  return_train_score=True)\ngrid_result_grid=grid.fit(features_train_scale, labels_train, verbose=0)\nprint(grid_result_grid)","af5cee6d":"print(\"Results from grid search show that\")\nprint(\"Best: %f using %s\" % (grid_result_grid.best_score_, grid_result_grid.best_params_))","84613c08":"# Carrying out randomised search to optimse batch-size and epochs\nparam_grid={'batch_size':randint(2,8),'nb_epoch':randint(20,80)}\nmodel=KerasRegressor(build_fn=design_model)\ngrid=RandomizedSearchCV(estimator=model,param_distributions=param_grid,\n                        scoring=make_scorer(mean_squared_error,greater_is_better=False), \n                        n_iter=50)\ngrid_result_random=grid.fit(features_train_scale,labels_train,verbose=0)\nprint(grid_result_random)","3f7e3f08":"print(\"Results from randomised search show that\")\nprint(\"Best: %f using %s\" % (grid_result_random.best_score_, grid_result_random.best_params_))","507f475d":"# Using early stopping to save computaion time\nes=EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=50)\n\n# Creating and training model\nmy_model=design_model()\nresults=my_model.fit(features_train_scale, labels_train, \n                     epochs=80, batch_size=2, verbose=1, validation_split=0.25,\n                     callbacks=[es])\n\n# Evaluating mse(loss), mae and R2 metrics of the model\nval_mse,val_mae=my_model.evaluate(features_test_scale,labels_test,verbose=0)\ny_pred=my_model.predict(features_test_scale)\nprint('Final MAE metric:',val_mae)\nprint('R2 metric:',r2_score(labels_test,y_pred))","433a9fcd":"# Plotting MAE and loss metrics against epochs\nprint(\"patience level=50\")\nfig=plt.figure()\nax1=fig.add_subplot(2, 1, 1)\nax1.plot(results.history['mae'])\nax1.plot(results.history['val_mae'])\nax1.set_title('model mae')\nax1.set_ylabel('MAE')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper right')\n\nax2=fig.add_subplot(2, 1, 2)\nax2.plot(results.history['loss'])\nax2.plot(results.history['val_loss'])\nax2.set_title('model loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","8f532cd7":"# Using early stopping to save computaion time\nes=EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=30)\nmy_model=design_model()\nresults=my_model.fit(features_train_scale, labels_train, \n                     epochs=100, batch_size=2, verbose=1, validation_split=0.25,\n                     callbacks=[es])\nval_mse,val_mae=my_model.evaluate(features_test_scale,labels_test,verbose=0)\ny_pred=my_model.predict(features_test_scale)\nprint('Final MAE metric:',val_mae)\nprint('R2 metric:',r2_score(labels_test,y_pred))","97f039e8":"# Plotting MAE and loss metrics against epochs\nprint(\"patience level=30\")\nfig=plt.figure()\nax1=fig.add_subplot(2, 1, 1)\nax1.plot(results.history['mae'])\nax1.plot(results.history['val_mae'])\nax1.set_title('model mae')\nax1.set_ylabel('MAE')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper right')\n\nax2=fig.add_subplot(2, 1, 2)\nax2.plot(results.history['loss'])\nax2.plot(results.history['val_loss'])\nax2.set_title('model loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","1b766d13":"# Using early stopping to save computaion time\nes=EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=5)\nmy_model=design_model()\nresults=my_model.fit(features_train_scale, labels_train, \n                     epochs=100, batch_size=4, verbose=1, validation_split=0.25,\n                     callbacks=[es])\nval_mse,val_mae=my_model.evaluate(features_test_scale,labels_test,verbose=0)\ny_pred=my_model.predict(features_test_scale)\nprint('Final MAE metric:',val_mae)\nprint('R2 metric:',r2_score(labels_test,y_pred))","a3278c9c":"# Plotting MAE and loss metrics against epochs\nprint(\"patience level=5\")\nfig=plt.figure()\nax1=fig.add_subplot(2, 1, 1)\nax1.plot(results.history['mae'])\nax1.plot(results.history['val_mae'])\nax1.set_title('model mae')\nax1.set_ylabel('MAE')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper right')\n\nax2=fig.add_subplot(2, 1, 2)\nax2.plot(results.history['loss'])\nax2.plot(results.history['val_loss'])\nax2.set_title('model loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","b2d4e31d":"# Adjusting model architecture\ndef design_model_v2():\n    model=Sequential()\n    inputL=InputLayer(input_shape=(7,))\n    model.add(inputL)\n    model.add(Dense(16,activation='relu')) #relu activation function for linear regression\n    model.add(Dense(16,activation='relu')) #extra layer of neurons\n    #------Alternatives------\n    #model.add(layers.Dropout(0.1)) \n    #model.add(Dense(16,activation='relu'))\n    #model.add(layers.Dropout(0.1))\n    #------Alternatives------\n    model.add(Dense(1))\n    opt=Adam(learning_rate=0.001) # Choosing the Adam optimizer as a starter\n    model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n    return model","6f103ed3":"# Testing new model layout\nmy_model=design_model_v2()\nresults=my_model.fit(features_train_scale, labels_train, \n                     epochs=100, batch_size=4, verbose=1, validation_split=0.25,)\nval_mse,val_mae=my_model.evaluate(features_test_scale,labels_test,verbose=0)\ny_pred=my_model.predict(features_test_scale)\nprint('Final MAE metric:',val_mae)\nprint('R2 metric:',r2_score(labels_test,y_pred))","aef306f4":"# Plotting MAE and loss metrics against epochs\nfig=plt.figure()\nax1=fig.add_subplot(2, 1, 1)\nax1.plot(results.history['mae'])\nax1.plot(results.history['val_mae'])\nax1.set_title('model mae')\nax1.set_ylabel('MAE')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper right')\n\nax2=fig.add_subplot(2, 1, 2)\nax2.plot(results.history['loss'])\nax2.plot(results.history['val_loss'])\nax2.set_title('model loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","904341b5":"# Adjusting model architecture\ndef design_model_v3():\n    model=Sequential()\n    inputL=InputLayer(input_shape=(7,))\n    model.add(inputL)\n    model.add(Dense(16,activation='relu')) #relu activation function for linear regression\n    model.add(layers.Dropout(0.1)) \n    model.add(Dense(16,activation='relu'))\n    model.add(layers.Dropout(0.1))\n    #------Alternatives------\n    #model.add(layers.Dropout(0.1)) \n    #model.add(Dense(16,activation='relu'))\n    #model.add(layers.Dropout(0.1))\n    #------Alternatives------\n    model.add(Dense(1))\n    opt=Adam(learning_rate=0.001) # Choosing the Adam optimizer as a starter\n    model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n    return model","110af66b":"# Testing new model layout\nmy_model=design_model_v3()\nresults=my_model.fit(features_train_scale, labels_train, \n                     epochs=100, batch_size=4, verbose=1, validation_split=0.25,)\nval_mse,val_mae=my_model.evaluate(features_test_scale,labels_test,verbose=0)\ny_pred=my_model.predict(features_test_scale)\nprint('Final MAE metric:',val_mae)\nprint('R2 metric:',r2_score(labels_test,y_pred))","76b18d0b":"# Plotting MAE and loss metrics against epochs\nfig=plt.figure()\nax1=fig.add_subplot(2, 1, 1)\nax1.plot(results.history['mae'])\nax1.plot(results.history['val_mae'])\nax1.set_title('model mae')\nax1.set_ylabel('MAE')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper right')\n\nax2=fig.add_subplot(2, 1, 2)\nax2.plot(results.history['loss'])\nax2.plot(results.history['val_loss'])\nax2.set_title('model loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","d7ee0c37":"#### 4) Tuning Hyperparameters (Early stopping)\n\nToo small a patience level for the early stopping function could be vulnerable to error fluctuation associated with a large learning rate or too small a batch size (both could lead to large jumps of the cost function). On the safe side, choosing a larger patience ensures more confidence in observing that cost is monotonically decreasing. \n\nWe start with a large patience value of 50 epochs and observe the learning curves.","1c1797a6":"#### Randomized Search Method (batch size and epochs)","d5a8243b":"#### Patience level:30; Batch size:2; Epochs:100","26336010":"Early stopping is triggered at last but lowering the patience level to 5 seems to have worsened our result. Given the number of epochs of 100,  introducing a reasonable early stopping is rather futile. As 100 epochs is not an extremely large number worth reducing at the same time a patience level of 5 is rather pointless, we might as well just leave the model as 100 epochs with no early stopping. The MAE metric and R2 metric show pretty good performance anyway.","89d763b2":"#### Adding in drop out layers","b436ee00":"Given a relatively small randomised range for the batch size, randomised search has concluded the same result as grid search giving a batch size of 2. The number of epochs obtained (50 in our run) is quite far off from the grid search result given the randomness of the search. To get a more coherent solution, one could increase the number of iterations randomised search operates on but that would demand more computaion which defeats the purpose of randomised search.\n\nThe result from grid search [batch_size:2, epochs:80] seems to suggest more epochs is better. However, this does not mean a threshold of 80 is the upper bound number of epochs necessary. Perhaps adding even more epochs would reduce costs significantly. This could be explored with early stopping next.","fb74b36d":"#### 6) Future work\n\nFuture imporvements of the model could entail tuning of all the hyperparameters in one single grid search\/ randomised search method that will guarantee the best result. More attention on data volume and data quality could lead to more accurate models as well. Due to the limited amount of computing power on my personal laptop, this has been an interesting analysis to look at.","9d14d600":"#### 2) Creating model\n\nCreating a separate design_model() function that involves adding layers to the ANN model.","d99f093c":"#### Patience level:5; Batch size:4; Epochs: 100","a5160c37":"Learning curve seems to have spanned the entire 80 epochs meaning the model is not terminated earlier. This could mean our patience value is too large or model still has room for improvement with lower losses. However as shown with the curves, the MAE score and loss function only seem to have drastic decrease in the early iterations. Epochs after around 30 seem to be giving diminishin marginal returns. Hence, we could try to reduce the patience level while increasing number of epochs observe results.","0d4cdfe5":"#### Grid Search Method (batch size and epochs)","b28fe954":"#### Patience level=50; Batch size: 2; Epochs: 80","079f755f":"Again, early stopping is not triggered. Perhaps we could reduce patience value even more. However, we could observe the noise associated with the MAE curves. This is a sign of instability that is associated with low batch size. If we increase the batch size, each optimisation could better follow a downward trend of the loss function. Though this may contradict with our best_param obtained using grid search, the earlier analysis was only an initial guess of the best paramaters. We could manually fine-tune the model more now.\n\nGood news is that loss function has reduced even more and the coeffecient of determination has increased showing increasing epochs leads to a lower loss function.","391dc434":"Adding drop out layers clearly solved the issue of overfitting. The noise with the learning curves however remain an issue. This could be addressed with more fine-tuning of the learning rate or the optimsation strategy. This could involve a lot more work and time. For the purpose of our analysis, our final model has achieved relatively good results with a coefficient of determination of 0.8066. ","5ece478e":"#### Extra hidden layer of 16 neurons","d8d916b0":"#### 5) Adjusting models\n\nAttempting to adjust the model architecture to see if results would be better. Available options are to add more layers or neurons per layer to see if a better fit to the data is available. ","0d383e25":"### Analysis of Graduate Admission using Deep Learning\n\nI utilised tensorflow\/keras to build a neural network model to predict the admission rate of Masters Programs based on students' undergraduate performance. The following analysis involves data cleaning, data analysis and data visualisation.","b388487e":"#### 3) Tuning Hyperparameters (Batch size and epochs)\n\nAutomatically tuning the hyperparameters using a grid search method and a randomised search method and comparing the results. Grid search requires picking specific values to test. This is rather arbitary but we follow the usual convention of doubling each trial value and limiting to 9 chooses. Randomised search picks integer value given a range. The number of iterations picked is larger to test if it converges towards the grid search method to show a general trend.","01760f87":"Adding an extra layer has led to high variance issue where our model has overfitted the training the dataset. This led to a higher loss function associated with the cross validation set as opposed to the training dataset. We could adjust this by chaning the cross validation split or introducing regularisation terms such as drop outs.","8bebf31a":"#### 1) Initilisation and Data Cleaning\n\nLoading the necessary packages from the virtual environment setup."}}