{"cell_type":{"0ceee546":"code","b8f1d334":"code","417b2c73":"code","07646315":"code","d70e02d9":"code","7933f014":"code","b79f878f":"code","74f3c17c":"code","df822581":"code","ac717cf3":"code","9ba725d7":"code","636edf01":"code","74953e4b":"code","66c36323":"code","a986a574":"code","b2a5e1fb":"code","0d353620":"code","751258eb":"code","2eb4ad03":"code","2c66cd98":"code","bf188b47":"code","b310d0f3":"code","7ac14629":"code","b7ffa071":"code","3e3ac80a":"code","46861e23":"code","043d30ad":"code","cc0262ce":"code","0e0e268d":"code","5bab89b7":"code","ec4250c5":"code","0d00bf25":"code","90ac39cf":"code","6834b998":"code","690d344b":"code","68af6d29":"code","d21a35c5":"code","f6266df6":"code","50547ca8":"code","fb3bd599":"code","de799c82":"code","46948ecd":"code","e6e6a933":"code","20577d54":"code","1bbffd65":"code","b93e293b":"code","a2b2b3a0":"code","c96a4fd5":"markdown","f78a6b14":"markdown","cedc037d":"markdown","2ac8f955":"markdown","b3e10b2a":"markdown","338be7f7":"markdown","e01cd45a":"markdown","18e11b3e":"markdown","8a35b139":"markdown","c7b811d7":"markdown","93b1950d":"markdown","a42e8a85":"markdown","1a5d4b12":"markdown","b40e4d72":"markdown","73c4cacd":"markdown","a0f40a53":"markdown","7697f4e7":"markdown","7857ce52":"markdown","bf213d2a":"markdown","9a000620":"markdown","8051772c":"markdown","c263c8c2":"markdown","e37de772":"markdown","df651dc4":"markdown","9b4e6094":"markdown","8d6c4658":"markdown","c87c7b28":"markdown","6da13be3":"markdown","7bf983eb":"markdown","1da2eb4e":"markdown"},"source":{"0ceee546":"import pandas as pd\nimport numpy as np\nimport math\n\n# Use this cell to begin, and add as many cells as you need to complete your analysis!\n# Libaries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nfrom scipy import stats\nfrom scipy.special import boxcox, inv_boxcox\n#from scipy.stats import norm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_log_error as MSLE\n\n","b8f1d334":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   HEADER = BOLD + UNDERLINE\n   END = '\\033[0m'","417b2c73":"# Get the default NA values from Pandas and remove \"NA\". Use this as the default list of NA's\n_na_values = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N\/A N\/A', '#N\/A', 'N\/A', 'n\/a', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']\n\n# Now we can read the files\nsrc_path      = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\ntest_df       = pd.read_csv(src_path + \"test.csv\", keep_default_na = True)#, na_values = _na_values)\ntrain_df      = pd.read_csv(src_path + \"train.csv\", keep_default_na = True)#, na_values = _na_values)\nsample_sub_df = pd.read_csv(src_path + \"sample_submission.csv\")\n\n#train_df.head().style.set_properties(**{\"background-color\": \"#98FB98\",\"color\": \"black\", \"border-color\": \"black\"})\ntrain_df.head()","07646315":"# Using Ids as Indexes\nfor df in [train_df, test_df]:\n    df.set_index(\"Id\", inplace=True)\ntrain_df.head()","d70e02d9":"# Data Type Check\ndf = pd.DataFrame({\"Column\": train_df.columns, \"Dtype\": train_df.dtypes.astype(\"str\").tolist(), \n                   \"Sample1\": train_df.loc[1].tolist(),\n                   \"Sample2\": train_df.loc[50].tolist(), \n                   \"Sample3\": train_df.loc[500].tolist()})\nprint(color.BOLD + color.UNDERLINE + \"Data Types for all features in the training data frame\" + color.END)\nprint(df.to_string())","7933f014":"def fix_category_NA(df):\n    # Array of features with NA as a valid option\n    features_with_NA = [\"Alley\",\n                        \"BsmtQual\",\n                        \"BsmtCond\",\n                        \"BsmtExposure\",\n                        \"BsmtFinType1\",\n                        \"BsmtFinType2\",\n                        \"FireplaceQu\",\n                        \"GarageType\",\n                        \"GarageFinish\",\n                        \"GarageQual\",\n                        \"GarageCond\",\n                        \"PoolQC\",\n                        \"Fence\",\n                        \"MiscFeature\"]\n    \n    for feature in features_with_NA:\n        df.replace({feature: {np.NAN : \"NA\"}}, inplace=True)","b79f878f":"for df in [train_df, test_df]:\n        fix_category_NA(df)\nprint(color.HEADER + \"Head of some of the columns we amended\" + color.END)\ntrain_df[[\"Alley\", \"BsmtQual\", \"GarageType\", \"PoolQC\", \"MiscFeature\"]].head()","74f3c17c":"categorical_features = train_df.select_dtypes(exclude = [np.number, bool]).columns\nprint(color.HEADER + \"Unique values for each categorical feature\" + color.END)\nfor categories in categorical_features:\n    print(color.BOLD + categories + color.END)\n    print(pd.concat([train_df, test_df])[categories].sort_values().unique())","df822581":"def fix_categories_integers(df):\n    df.replace({\"MSSubClass\": {20: \"SC20\", 30: \"SC30\", 40: \"SC40\", 45: \"SC45\", 50: \"SC50\", 60: \"SC60\", 70: \"SC70\", 75: \"SC75\",\n                               80: \"SC80\", 85: \"SC85\", 90: \"SC90\", 120: \"SC120\", 150: \"SC150\", 160: \"SC160\", 180: \"SC180\", 190: \"SC190\"}, \n                \"MoSold\": {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n                           7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"},\n                \"CentralAir\": {\"Y\": True, \"N\": False}},\n                inplace=True)\n    df[\"YrSold\"] = pd.Categorical(df.YrSold)","ac717cf3":"for df in [train_df, test_df]:\n    fix_categories_integers(df)\nprint(color.HEADER + \"Head of the features we've fixed\" + color.END)\ntrain_df[[\"MSSubClass\", \"YrSold\", \"MoSold\", \"CentralAir\"]].head()","9ba725d7":"_train_df      = train_df.drop(columns = \"SalePrice\")\ncombined_df = pd.concat([_train_df, test_df])\n\nprint(color.BOLD + \"Which features contain null values?\" + color.END)\nprint(combined_df.isnull().sum()[combined_df.isnull().sum()>0])","636edf01":"def fix_missing_values(df):\n    MSZoning_series    = df.groupby(\"Neighborhood\").MSZoning.agg(lambda x:x.value_counts().index[0])\n    LotFrontage_series = df.groupby(\"Neighborhood\").LotFrontage.median()\n\n    df.fillna({\"Utilities\": \"AllPub\",\n                    \"Exterior1st\":\"Other\",\n                    \"Exterior2nd\":\"Other\",\n                    \"MasVnrType\":\"None\",\n                    \"MasVnrArea\":0,\n                    \"BsmtFinSF1\":0,\n                    \"BsmtFinSF2\":0,\n                    \"BsmtUnfSF\":0,\n                    \"TotalBsmtSF\":0,\n                    \"Electrical\":\"SBrkr\",\n                    \"BsmtFullBath\":0,\n                    \"BsmtHalfBath\":0,\n                    \"KitchenQual\":\"TA\",\n                    \"Functional\":\"Typ\",\n                    \"GarageYrBlt\":\"None\",\n                    \"GarageCars\":0,\n                    \"GarageArea\":0,\n                    \"SaleType\":\"Oth\",\n                    \"MSZoning\": df[\"Neighborhood\"].apply(lambda x: MSZoning_series[x]),\n                    \"LotFrontage\": df[\"Neighborhood\"].apply(lambda x: LotFrontage_series[x])}, inplace = True)\n\n    df[\"GarageYrBlt\"] = pd.Categorical(df.GarageYrBlt)","74953e4b":"for df in [train_df, test_df]:\n    fix_missing_values(df)\n\nprint(color.BOLD + color.RED + \"Number of null values across both train and test data frames?\")\nprint(f\"{pd.concat([train_df.drop(['SalePrice'], axis = 1), test_df]).isnull().sum().sum()}\"+color.END)","66c36323":"# Figure\nplt.figure(figsize=(12, 4))\nplt.suptitle(\"Visualising the skewness of the SalePrice target variable\")\n\n# Distribution Plot\nplt.subplot(1, 2, 1)\nsns.histplot(train_df[\"SalePrice\"], stat = \"density\", kde = True)\nplt.title('Distribution Plot')\n\n# Probability Plot\nplt.subplot(1, 2, 2)\nstats.probplot(train_df['SalePrice'], plot=plt)\n\nplt.tight_layout()\nplt.show()\nplt.clf()","a986a574":"train_df['SalePrice'] = np.log1p(train_df.SalePrice)\n\n# Figure\nplt.figure(figsize=(12, 4))\nplt.suptitle(\"Visualisaing the skewnewss of the SalePrice target variable following a log1p transformation\")\n# Distribution Plot\nplt.subplot(1, 2, 1)\nsns.histplot(train_df[\"SalePrice\"], stat = \"density\", kde = True)\nplt.title('Distribution Plot')\n\n# Probability Plot\nplt.subplot(1, 2, 2)\nstats.probplot(train_df['SalePrice'], plot=plt)\n\nplt.tight_layout()\nplt.show()\nplt.clf()","b2a5e1fb":"numerical_features = train_df.select_dtypes(include = [np.number]).columns\nprint(color.BOLD + f'Numerical Features ({len(numerical_features)}):' + color.END + f'\\n{numerical_features}')\ncategorical_features = train_df.select_dtypes(exclude = [np.number, bool]).columns\nprint(color.BOLD + f'Categorical Features ({len(categorical_features)}):' + color.END + f'\\n{categorical_features}')","0d353620":"# We want to split the numerical and categorical features into groups to view the data better\n# To do this, we'll group these in sets of 10\n\n# How many groups are needed?\n#  Each will be a 4x4 grid. Total of 16 charts per plot\n#  Each plot will have two charts, total of 8 features per plot\nnumerical_groups   = math.ceil(len(numerical_features.values)\/8)\ncategorical_groups = math.ceil(len(categorical_features.values)\/8)\n\ntotal_groups       = numerical_groups + categorical_groups\n\nnumerical_step     = 8\ncategorical_step   = 8\n\ngroup_num = np.empty(int(numerical_groups), dtype = pd.Series)\nfor grp in np.arange(numerical_groups):\n#  print(grp * numerical_step)\n  st = int(grp * numerical_step)\n  en = int((grp+1) * numerical_step - 1)+1\n  group_num[int(grp)] = numerical_features[st:en]\n\n\ngroup_cat = np.empty(int(categorical_groups), dtype = pd.Series)\nfor grp in np.arange(categorical_groups):\n  #print(grp * numerical_step)\n  st = int(grp * categorical_step)\n  en = int((grp+1) * categorical_step - 1)+1\n  group_cat[int(grp)] = categorical_features[st:en]\n\n\n# EDA of all groups\nprint(color.BOLD + color.UNDERLINE + \"Visualisation of distribution and relationship of numerical features vs SalePrice\" + color.END)\ngroups = group_num\nfor grp in groups:\n    plt.figure(figsize=(12, 12))\n    i = 1\n    for feature in grp:\n        # Distribution Plot\n        width  = 4\n        height = 4\n        _=plt.subplot(height, width, i)\n        _=sns.histplot(train_df[feature], kde=True, stat=\"density\", linewidth=0)\n        _=plt.title(\"Distribution\")\n        i += 1\n\n        # Scatter Plot\n        _=plt.subplot(height, width, i)\n        _=sns.scatterplot(data=train_df, x=feature, y=\"SalePrice\", alpha=0.5)\n        _=plt.title(\"Relationship\")\n        i += 1\n    plt.tight_layout()\n    plt.show()\n    plt.clf()\n\n","751258eb":"old_length = len(train_df)\ntrain_df   = train_df.drop(train_df[(train_df.LotFrontage > 200)|\n                                          (train_df.LotArea > 100000)|\n                                          (train_df.BsmtFinSF1 > 4000)|\n                                          (train_df.BsmtFinSF2 > 1200)|\n                                          (train_df.TotalBsmtSF > 5000)|\n                                          (train_df.GrLivArea > 4000)|\n                                          (train_df.KitchenAbvGr == 0)|\n                                          (train_df.WoodDeckSF > 750)|\n                                          (train_df.OpenPorchSF > 500)|\n                                          (train_df.EnclosedPorch > 500)|\n                                          (train_df.MiscVal > 5000)].index)\nnew_length = len(train_df)\nprint(color.HEADER + color.RED + f'Reduction in training data from removing outliers is {np.round(100*(old_length-new_length)\/old_length, 2)}%' + color.END)","2eb4ad03":"def numerical_feature_engineering(df):\n    # Create New Features\n    df[\"Has_LowQualFinSF\"] = df[\"LowQualFinSF\"].apply(lambda x: False if x==0 else True)\n    df[\"Has_Pool\"]         = df[\"PoolArea\"].apply(lambda x: False if x==0 else True)\n    df[\"LivAreaRatio\"]     = df.GrLivArea \/ df.LotArea\n    df[\"SpaceRatio\"]       = (df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]) \/ df[\"TotRmsAbvGrd\"]\n    df[\"TotalBath\"]        = df.BsmtFullBath + df.BsmtHalfBath\n    df[\"TotalRoom\"]        = df.TotRmsAbvGrd + df.FullBath + df.HalfBath\n\n    # Feature Engineering: Numerical Features\n    df[\"BsmtFullBath\"] = df[\"BsmtFullBath\"].apply(lambda x: False if x==0 else True)\n    df[\"BsmtHalfBath\"] = df[\"BsmtHalfBath\"].apply(lambda x: False if x==0 else True)\n    df[\"HalfBath\"]     = df[\"HalfBath\"].apply(lambda x: False if x==0 else True)\n    df[\"BedroomAbvGr\"] = df[\"BedroomAbvGr\"].apply(lambda x: x if x<5 else 5)\n    df[\"KitchenAbvGr\"] = df[\"KitchenAbvGr\"].apply(lambda x: x if x<2 else 2)\n    df[\"Fireplaces\"]   = df[\"Fireplaces\"].apply(lambda x: x if x<2 else 2)\n    df[\"GarageCars\"]   = df[\"GarageCars\"].apply(lambda x: x if x<3 else 3)\n    df[\"NhbdRank\"]     = df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    df[\"GrLivAreaPlusBsmtSF\"] = df.GrLivArea + df.TotalBsmtSF\n    df[\"RecentRemodLargeBsmt\"] = df.YearRemodAdd * df.TotalBsmtSF\n\n  # Drop replaced features\n    df.drop(columns = [\"LowQualFinSF\", \"PoolArea\"], inplace = True)","2c66cd98":"for df in [train_df, test_df]:\n    numerical_feature_engineering(df)","bf188b47":"print(color.HEADER + \"Shape of our training dataframe, after numerical feature engineering\" + color.END)\ntrain_df.shape","b310d0f3":"# EDA of categorical groups\nprint(color.HEADER + \"Visualisation of distribution and relationship of categorical features vs SalePrice\" + color.END)\ngroups = group_cat\nfor grp in groups:\n    plt.figure(figsize=(12, 12))\n    i = 1\n    for feature in grp:\n        # Distribution Plot\n        width  = 4\n        height = 4\n        _=plt.subplot(height, width, i)\n        _=sns.countplot(x = train_df[feature])\n        _=plt.xticks(rotation=90)\n        _=plt.title(\"Distribution\")\n        i += 1\n\n        # Scatter Plot\n        _=plt.subplot(height, width, i)\n        _=sns.stripplot(data=train_df, x=feature, y=\"SalePrice\", alpha=0.5)\n        _=plt.xticks(rotation=90)\n        _=plt.title(\"Relationship\")\n        i += 1\n    plt.tight_layout()\n    plt.show()\n    plt.clf()","7ac14629":"def categorical_feature_engineering(df):\n    # Update existing features\n    df[\"RoofMatl\"]    = df[\"RoofMatl\"].apply(lambda x: x if x==\"CompShg\" else \"Other\")\n    df[\"ExterQual\"]   = df[\"ExterQual\"].apply(lambda x: \"Good\" if x in [\"Gd\", \"Ex\"] else \"Average\")\n    df[\"Heating\"]     = df[\"Heating\"].apply(lambda x: x if x==\"GasA\" else \"Other\")\n    df[\"Electrical\"]  = df[\"Electrical\"].apply(lambda x: x if x==\"SBrkr\" else \"Other\")\n    df[\"KitchenQual\"] = df[\"KitchenQual\"].apply(lambda x: \"Good\" if x in [\"Gd\", \"Ex\"] else \"Average\")\n    df[\"Functional\"]  = df[\"Functional\"].apply(lambda x: x if x==\"Typ\" else \"Other\")\n    df[\"SaleType\"]    = df[\"SaleType\"].apply(lambda x: x if x in [\"WD\", \"New\"] else \"Other\")\n    \n    # Add new features\n    df[\"FrontageType\"] = df[[\"WoodDeckSF\",\n                             \"OpenPorchSF\",\n                             \"EnclosedPorch\",\n                             \"3SsnPorch\",\n                             \"ScreenPorch\"\n                            ]].gt(0.0).sum(axis=1)\n\n    # Drop replaced features\n    df.drop(columns = [\"Street\", \"Utilities\", \"Condition2\"], inplace = True)","b7ffa071":"for df in [train_df, test_df]:\n    categorical_feature_engineering(df)\n\nprint(color.HEADER + \"Head of the categorical features we've amended \/ added\" + color.END)    \n#train_df[[\"RoofMatl\", \"ExterQual\", \"Heating\", \"Electrical\", \"KitchenQual\", \"Functional\", \"SaleType\", \"FrontageType\"]].head()","3e3ac80a":"# Are there any features that are highly correlated to each other now that we've encoded categorical\n#    data to numeric? If so, can we drop them?\ncorr = train_df.drop(columns=[\"SalePrice\"]).corr()\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(22, 12))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, annot = False).set(title=\"Correlation of features\")\n_ = plt.xlabel(\"Feature\")\n_ = plt.ylabel(\"Feature\")\n_ = plt.show()\n_ = plt.clf()","46861e23":"# Let's encode the categorical features\ntest_df_pre          = test_df.copy()\ntrain_df_pre         = train_df.copy()\n_train_df            = train_df.drop(columns = \"SalePrice\")\n\ncategorical_features = pd.concat([_train_df, test_df_pre]).select_dtypes(exclude = [np.number, bool]).columns\ncombined_df_cat      = pd.concat([_train_df, test_df_pre])[categorical_features].reset_index(drop=True)\n\nencoder_mapping      = pd.DataFrame(index = categorical_features, columns = {\"encoder\", \"mapping\"})\n\nfor i in np.arange(len(categorical_features)):\n    le = LabelEncoder()\n    encoder_mapping.iloc[i][\"encoder\"] = le.fit(list(combined_df_cat.iloc[:,i]))\n    encoder_mapping.iloc[i][\"mapping\"] = dict(zip(le.classes_, range(len(le.classes_))))\n\n\nfor feature in encoder_mapping.index:\n    train_df_pre.replace({feature: encoder_mapping.loc[feature][\"mapping\"]}, inplace=True)\n    test_df_pre.replace({feature: encoder_mapping.loc[feature][\"mapping\"]}, inplace=True)","043d30ad":"SEED      = 42\ntest_size = 0.3   #  30% test, 70% train\ncv        = 5     #  5 fold cross vailidation\n\nX = train_df_pre.drop([\"SalePrice\"], axis = \"columns\")    # Independant columns (all the features used for prediction)\ny = train_df_pre[\"SalePrice\"]                             # Target Variable","cc0262ce":"# Create training and test sets\nX_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=SEED)","0e0e268d":"# Set up the scaler\nscaler                    = RobustScaler()\n\n# Fit and Transforn the scaling to both the train and test dataset\nX_train_scaled            = pd.DataFrame(scaler.fit_transform(X_train_unscaled))\nX_test_scaled             = pd.DataFrame(scaler.transform(X_test_unscaled))\ntest_df_scaled            = pd.DataFrame(scaler.transform(test_df_pre))\n\n\n# Amend the columns of the scaled data to match those of the original data frame\nX_train_scaled.columns    = X_train_unscaled.columns.values\nX_test_scaled.columns     = X_test_unscaled.columns.values\ntest_df_scaled.columns    = test_df_pre.columns.values\n\n\n# Amend the index of the scaled data to match those of the original data frame\nX_train_scaled.index      = X_train_unscaled.index.values\nX_test_scaled.index       = X_test_unscaled.index.values\ntest_df_scaled.index      = test_df_pre.index.values\n\n\n# Output the final data frames. \nX_train                   = X_train_scaled\nX_test                    = X_test_scaled\ntest_df_processed         = test_df_scaled\n\ntrain_df_processed        = pd.concat([pd.concat([X_train, y_train], axis = 1), pd.concat([X_test, y_test], axis = 1)]).sort_index()\n\n\nprint(color.BOLD + color.UNDERLINE + \"Head of the scaled, encoded and clened training data frame\" + color.END)\nprint(X_train.head(n=3))\nprint()\nprint(color.BOLD + color.UNDERLINE + \"Head of the scaled, encoded and clened test data frame\" + color.END)\nprint(test_df_processed.head(n=3))","5bab89b7":"# Ridge Regression (L2 Regularization)\nalphas = np.arange(1, 10, 1)\nridge = RidgeCV(alphas, normalize=True)\nridge.fit(X_train, y_train)\nbest_alpha = ridge.alpha_\n\niterations = 5\nfor i in range(iterations):\n    alphas = [best_alpha*x for x in np.arange(0.1, 2, 0.1)]\n    ridge = RidgeCV(alphas, normalize=True)\n    ridge.fit(X_train, y_train)\n    best_alpha = ridge.alpha_\n\nridge_score = np.sqrt(-cross_val_score(ridge, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))","ec4250c5":"# Lasso Regression (L1 Regularization)\nlasso = LassoCV(alphas=None, max_iter=100000, normalize=True)\nlasso.fit(X_train, y_train)\nbest_alpha = lasso.alpha_\nlasso_score = np.sqrt(-cross_val_score(lasso, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))","0d00bf25":"forest = RandomForestRegressor(n_estimators = 20, random_state = SEED)\nforest.fit(X_train, y_train)\n\nada = AdaBoostRegressor(n_estimators = 20, random_state = SEED)\nada.fit(X_train, y_train)\n\nbagging = BaggingRegressor(n_estimators = 20, random_state = SEED)\nbagging.fit(X_train, y_train)\n\nETR = ExtraTreesRegressor(n_estimators = 20, random_state = SEED)\nETR.fit(X_train, y_train)\n\nGBR = GradientBoostingRegressor(n_estimators = 20, random_state = SEED)\nGBR.fit(X_train, y_train)\n\nforest_score = np.sqrt(-cross_val_score(forest, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))\nada_score = np.sqrt(-cross_val_score(ada, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))\nbagging_score = np.sqrt(-cross_val_score(bagging, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))\nETR_score = np.sqrt(-cross_val_score(ETR, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))\nGBR_score = np.sqrt(-cross_val_score(GBR, X_train, y_train, cv=cv, scoring='neg_mean_squared_log_error'))","90ac39cf":"results = pd.DataFrame({\"Ridge\":[round(np.mean(ridge_score),5)],\n                        \"Lasso\":[round(np.mean(lasso_score),5)],\n                        \"Forest\":[round(np.mean(forest_score),5)],\n                        \"Ada\":[round(np.mean(ada_score),5)],\n                        \"Bagging\":[round(np.mean(bagging_score),5)],\n                        \"ETR\":[round(np.mean(ETR_score),5)], \n                        \"GBR\":[round(np.mean(GBR_score),5)]},  \n                       index = [\"RMSLE\"])\nresults","6834b998":"lasso_param_grid = {\n    \"n_alphas\": [50, 100, 200, 500, 1000],\n    \"max_iter\": [500, 1000, 10000, 25000, 50000],\n    \"selection\": [\"cyclic\", \"random\"],\n    \"random_state\": [SEED]\n}\nprint(color.UNDERLINE + \"Lasso Hyper Parameter Grid\" + color.END)\nlasso_param_grid","690d344b":"grid_search_lasso = GridSearchCV(estimator = lasso,\n                                param_grid = lasso_param_grid,\n                                cv = cv,\n                                n_jobs = -1,\n                                verbose = 0)\n_ = grid_search_lasso.fit(X_train, y_train)\n\nprint(color.BOLD + color.UNDERLINE + \"Best parameters are:\" + color.END)\nprint(color.BOLD + f\"{grid_search_lasso.best_params_}\" + color.END)","68af6d29":"lasso_best_grid = grid_search_lasso.best_estimator_\nlasso_score_2 = np.sqrt(-cross_val_score(lasso_best_grid, X_train, y_train, scoring = \"neg_mean_squared_log_error\", cv = cv))","d21a35c5":"bagging_param_grid = {\n    \"bootstrap\":[True, False],\n    \"max_features\":[1, 4, 10],\n    \"max_samples\":[1, 4],\n    \"n_estimators\":[10, 75, 250],\n    \"random_state\": [SEED]\n}\n\nprint(color.UNDERLINE + \"Bagging Regressor Hyper Parameter Grid\" + color.END)\nbagging_param_grid","f6266df6":"grid_search_bagging = GridSearchCV(estimator = bagging,\n                                param_grid = bagging_param_grid,\n                                cv = cv,\n                                n_jobs = -1,\n                                verbose = 0)\n_ = grid_search_bagging.fit(X_train, y_train)\n\nprint(color.BOLD + color.UNDERLINE + \"Best parameters are:\" + color.END)\nprint(color.BOLD + f\"{grid_search_bagging.best_params_}\" + color.END)","50547ca8":"bagging_best_grid = grid_search_bagging.best_estimator_\nbagging_score_2 = np.sqrt(-cross_val_score(bagging_best_grid, X_train, y_train, scoring = \"neg_mean_squared_log_error\", cv = cv))","fb3bd599":"forest_param_grid = {\n    \"bootstrap\":[True, False],\n    \"max_depth\":[80, 100, None],\n    \"max_features\":[4, 10, \"auto\"],\n    \"min_samples_leaf\":[1, 4],\n    \"min_samples_split\":[2, 6, 10],\n    \"n_estimators\":[75, 250],\n    \"random_state\":[SEED]\n}\n\nprint(color.UNDERLINE + \"Random Forest Regressor Hyper Parameter Grid\" + color.END)\nforest_param_grid","de799c82":"grid_search_forest = GridSearchCV(estimator = forest,\n                                param_grid = forest_param_grid,\n                                cv = cv,\n                                n_jobs = -1,\n                                verbose = 0)\n_ = grid_search_forest.fit(X_train, y_train)\n\nprint(color.BOLD + color.UNDERLINE + \"Best parameters are:\" + color.END)\nprint(color.BOLD + f\"{grid_search_forest.best_params_}\" + color.END)","46948ecd":"forest_best_grid = grid_search_forest.best_estimator_\nforest_score_2 = np.sqrt(-cross_val_score(forest_best_grid, X_train, y_train, scoring = \"neg_mean_squared_log_error\", cv = cv))","e6e6a933":"results = pd.DataFrame({\"Lasso\":[round(np.mean(lasso_score),7), round(np.mean(lasso_score_2),7)],\n                        \"Forest\":[round(np.mean(forest_score),7), round(np.mean(forest_score_2),7)],\n                        \"Bagging\":[round(np.mean(bagging_score),7), round(np.mean(bagging_score_2),7)]},  \n                       index = [\"Before\", \"After\"])\nprint(color.BOLD + color.UNDERLINE + \"RMSLE score\" + color.END)\nresults","20577d54":"y_pred_lasso_log1p       = lasso_best_grid.predict(X_test)\ny_pred_bagging_log1p     = bagging_best_grid.predict(X_test)\ny_pred_forest_log1p      = forest_best_grid.predict(X_test)\n\n\nlasso_test_RMSLE         = np.sqrt(MSLE(y_test, y_pred_lasso_log1p))\nbagging_test_RMSLE       = np.sqrt(MSLE(y_test, y_pred_bagging_log1p))\nforest_test_RMSLE        = np.sqrt(MSLE(y_test, y_pred_forest_log1p))\n\ny_pred_lasso             = np.expm1(y_pred_lasso_log1p)\ny_pred_bagging           = np.expm1(y_pred_bagging_log1p)\ny_pred_forest            = np.expm1(y_pred_forest_log1p)\ny_test_expm1             = np.expm1(y_test)\n\nlasso_test_RMSLE_expm1   = np.sqrt(MSLE(y_test_expm1, y_pred_lasso))\nbagging_test_RMSLE_expm1 = np.sqrt(MSLE(y_test_expm1, y_pred_bagging))\nforest_test_RMSLE_expm1  = np.sqrt(MSLE(y_test_expm1, y_pred_forest))\n\nresults = pd.DataFrame({\"Lasso\":[round(np.mean(lasso_score),7), round(np.mean(lasso_score_2),7), lasso_test_RMSLE, lasso_test_RMSLE_expm1],\n                        \"Forest\":[round(np.mean(forest_score),7), round(np.mean(forest_score_2),7), forest_test_RMSLE, forest_test_RMSLE_expm1],\n                        \"Bagging\":[round(np.mean(bagging_score),7), round(np.mean(bagging_score_2),7), bagging_test_RMSLE, bagging_test_RMSLE_expm1]},  \n                       index = [\"No Tuning\", \"Hyper Tuning\", \"Test Data\", \"Test Data (expm1)\"])\nprint(color.BOLD + color.UNDERLINE + \"RMSLE score\" + color.END)\nresults","1bbffd65":"t = np.linspace(min(y_test_expm1), max(y_test_expm1), len(y_test_expm1))\n\n# Figure\nplt.figure(figsize=(20, 8))\n\n# Distribution Plot\nplt.subplot(1, 2, 1)\nplt.title(\"Lasso | Difference in predicted value vs expected value\")\nplt.plot(t, np.linspace(0,0,len(t)), c = \"red\")\nplt.scatter(y_test_expm1, y_pred_lasso - y_test_expm1, alpha = 0.3)\n\nplt.subplot(1,2,2)\nplt.title(\"Random Forest | Difference in predicted value vs expected value\")\nplt.plot(t, np.linspace(0,0,len(t)), c = \"red\")\nplt.scatter(y_test_expm1, y_pred_forest - y_test_expm1, alpha = 0.3)\n\nplt.show()\n\nplt.figure(figsize=(20, 8))\nplt.title(\"Mean of Lasso and Random Forest | Difference in predicted value vs expected value\")\nplt.plot(t, np.linspace(0,0,len(t)), c = \"red\")\nplt.scatter(y_test_expm1, np.mean([y_pred_forest, y_pred_lasso],axis=0) - y_test_expm1, alpha = 0.3)\nplt.show()\n\n\nprint(color.BOLD + color.RED + \"RMSLE of the mean of the lasso and random forest predictions\" + color.END)\nprint(\n    round(np.sqrt(MSLE(y_test_expm1, np.mean([y_pred_forest, y_pred_lasso],axis=0))),4)\n)","b93e293b":"final_y_pred_lasso_log1p       = lasso_best_grid.predict(test_df_scaled)\nfinal_y_pred_forest_log1p      = forest_best_grid.predict(test_df_scaled)\n\nfinal_y_pred_lasso             = np.expm1(final_y_pred_lasso_log1p)\nfinal_y_pred_forest            = np.expm1(final_y_pred_forest_log1p)\n\nfinal_y_pred = np.mean([final_y_pred_lasso, final_y_pred_forest], axis = 0)\n\n# Submitting Prediction\nsubmission = pd.DataFrame({'Id': test_df_scaled.index, 'SalePrice': final_y_pred})\nsubmission.head()","a2b2b3a0":"submission.to_csv('submission.csv', index=False)\nprint('Submission saved.')","c96a4fd5":"There are certainly differences to the data description, but no multiple spellings or errors that will impact our model.\n\nWe'll now replace various integers to appropriate categories:","f78a6b14":"### 2. EDA (Exploratory Data Analysis)\n###### 2.1 Data Check","cedc037d":"**Summary**\n\nWhilst a good result, it's clear that properties with a low sale price, or (more significantly) properties with a higher sale price are being predicted incorrectly.\n\nWould training a model on only high-end properties make a difference?\n\nFor now, I'll submit my results and delve deeper another day.","2ac8f955":"Fantastic! There are no null values.","b3e10b2a":"### 3. Pre-Processing\n###### 3.1 Categorical encoding\n\nWe'll now encode the categorical data to numbers, which is needed for the prediction models to work.\n\n","338be7f7":"Now lets implement our numerical feature engineering.","e01cd45a":"We can fill most of these in by looking at the data description by using defaults such as \"Other\" or \"None.\nThe numeric values can have 0, such as those with square feet metrics.\n\n**GarageYrBlt** contains a year, or is blank if there is no garage.  We can change this to a categorical feature.","18e11b3e":"##### 2.2 Missing Values\nWe now need to review and clean any missing value.","8a35b139":"### 4. Machine Learning - Prediction\n##### 4.1 Configuation\n\nWe're now ready to build a prediction model.  First, we'll need to set up our configuation, where we'll set our seed and test size.\n\nWe'll also get training data ready:\n* X = Independant columns (features)\n* Y = Target Variable (SalePrice)\n\nWe'll then need to scale the data before proceeding.  This is completed after the train test split, as we don't want any leakage of training data into the test data.","c7b811d7":"# House Prices - Advanced Regression Techniques\n\n## James Morgan (jhmmorgan)\n*2021-11-04*\n\n### 1. Set Up\n##### 1.1 Import Libraries","93b1950d":"##### 2.3 Visualisation of the data\n###### 2.3.2 Numerical and Categorical Features\n\nWe now need to visualise the numerical and categorical features.\n\n* We want to extract the names of the numerical and categorical features\n* We want to visualise the density and relationship of the numerical values against the SalePrice target variable.\n    * We'll achieve this by producing an 4x4 grid of visualisation.\n    * This gives us 16 subplots per plot.\n    * We'll show 8 features per plot (each feature has two subplots)","a42e8a85":"##### 4.2 Train Test Split\nWe now need to split the training data into a further train test split, using the configuation above.","1a5d4b12":"##### 1.2 Default classes\nUsed for better printing outputs.","b40e4d72":"There is certainly some correlation, but no more than +- 0.5, which isn't enough for me to remove features.","73c4cacd":"We can see that both Lasso and Random Forest are poor at predicting the more expensive properties.  This isn't surprising considering we have much less data.\n\nHowever, by taking the mean of both results, we get a better result and an improved RMSLE.","a0f40a53":"##### 4.3 First Model\n###### Linear and Ensemble Mix\n\nTo start, we'll try several Linear and Ensemble models.\n* Ridge Regression\n* Lasso\n* Random Forest Regression\n* Ada\n* Extra Trees Regression\n* Gradiant Boosting Regression\n\nFor the Ensemble's, I'll use an n_estimator of 20.\nWe'll use negative mean squred log error as the scoring mechanism and apply a cross validation 5.","7697f4e7":"The SalePrice appears skewed. A log1p transformation smooths this better than the square root.  The boxcox is only marginally better and personally less preferred.","7857ce52":"Our best score is from a **Lasso Regression**, which gives us an RMSE of 0.0092.\nThe best ensemble method is **Bagging**, closely followed by **Random Forest**.\n\nI'll take these three models forward and see if we can improve them using hyper parameters.\n\n##### 4.4 Hyper Parameter Tuning\n###### Tuning Lasso Regression","bf213d2a":"##### 1.3 Load Data\nLets read in the data...\n\nHowever, before we do, the data description shows NA as being a valid value for many of the categories\nNormally meaning none, i.e. Alley == NA means no alley access, not missing data. We'll therefore override the default NA's to ignore \"NA\" from the NA list.\n\n","9a000620":"Pretty good.  The test data has performed similar to the hyper tuned training data.\nWe applied a log1p transformation to the SalePrice, due to skewness.  When we revere this (expm1), the score is slightly worse but still good, except for Bagging.\n\nWe'll proceeed with Lasso and Forest only.\n\nHow does this look when visualised?\n\n##### 4.6 Visualisation of results","8051772c":"##### 2.3 Visualisation of the data\n###### 2.3.2.1 Numerical Outliers\n* **LotFrontage** > 250\n* **LotArea** > 100000\n* **BsmtFinSF1** > 4000\n* **BsmtFinSF2** > 1200\n* **TotalBsmtSF** > 5000\n* **GrLivArea** > 4000\n* **KitchenAbcGr** = 0\n* **WoodDeckSF** > 750\n* **OpenPorchSF** > 500\n* **EnclosedPorch** > 500\n* **MiscVal** > 5000\n\n###### 2.3.2.2 Numerical Feature Engineering\n* **LowQualFin**   | if LowQualFinSF == 0 then False, =>1 then True\n* **BsmtFullBath** | 0 then False, =>1 then True\n* **BsmtHalfBath** | 0 then False, =>1 then True\n* **HalfBath**     | 0 then False, =>1 then True\n* **BedroomAbvGr** | >= 5 then 5\n* **KitchenAbvGr** | >=2 then 2\n* **Fireplaces**   | >= 2 then 2\n* **GarageCars**   | >= 3 then 3\n* **HasPool**      | if PoolArea == 0 then False, >0 then True\n* **LivAreaRatio** | Living Area Ratio (GrLivArea \/ LotArea)\n* **SpaceRatio**   | Space ( (FirstFlrSF + SecondFlrSF) \/ df.TotRmsAbvGrd)\n* **TotalBath**    | BsmtFullBath + BsmtHalfBath\n* **TotalRoom**    | TotRmsAbvGrd + FullBath + HalfBath\n* **NhbdRank**     | Neighbour Rank - The median GrLivArea for the Neighbourhood\n* **GrLivAreaPlusBsmtSF**  | Total living area (df.GrLivArea + df.TotalBsmtSF)\n* **RecentRemodLargeBsmt** | df.YearRemodAdd * df.TotalBsmtSF\n\nLet's remove these outliers as not to skew our data \/ predictions then move onto the feature engineering:","c263c8c2":"Great! The Lasso and Forest models have marginally improved. The bagging has got slightly worse!\n\nHow do these look across the test data?","e37de772":"##### 2.3 Visualisation of the data\n###### 2.3.2.4 Categorical Feature Engineering\n\n* **Drop** Street, Utilities, Condition2\n* **RoofMatl** = ClyTile or Other\n* **ExterQual** = Gd\/Ex = Good, TA\/FA = Average\n* **Heating** = GasA or Other\n* **Electrical** = SBrkr or Other\n* **KitchenQual** = Gd\/Ex = Good, TA\/FA = Average\n* **Functional** = Typ, Other\n* **SaleType** = WD, New, Other\n\n* **FrontageType** = Count of (how many of)\n    * 'WoodDeckSF', \n    * 'OpenPorchSF',\n    * 'EnclosedPorch',\n    * 'Threeseasonporch',\n    * 'ScreenPorch'\n\nLet's make these amendments.","df651dc4":"###### Observations:\n* **MSSubClass**, **MoSold**, **YrSold** are categorical, but stored as numbers\n* **OverallQal**, **OverallCond** are also categorical, however with a scale of 1 to 10 so are ok to remain numbers\n* **CentralAir** is a Y\/N and so should be a boolean\n* Various features have missing values, however **NA** was one of their options.\n\nWe need to change these in both the training and test datasets.\n\nWe'll first replace any null value in the specified columns to the string \"NA\":","9b4e6094":"##### 2.3 Visualisation of the data\n###### 2.3.1. SalePrice\nWe now need to visualise the data.\n\n**How evenly distributed is the sale price (target) in our training data?**","8d6c4658":"I now want to check each categorical feature's possible value.  I'm not looking to fix this to match the list provided (although this would be a good idea in a real life scenario), however I'd like to ensure there are no mistakes that could lead to incorrect categorisations, such as multiple spellings for one value e.g. **Exm One** and **ExmOne**.","c87c7b28":"###### 3.2 Scaling the data\n\nNow that each feature is encoded to a number, we should scale the data.","6da13be3":"Finally, lets have a look at a correlation of features to see if there are any strong correlations that we can remove?","7bf983eb":"##### 2.3 Visualisation of the data\n###### 2.3.2.3 Categorical dimension reduction","1da2eb4e":"##### 4.5 Results of the Hyper Parameter Tuning"}}