{"cell_type":{"b8e1b037":"code","98eb8eb7":"code","24cb6114":"code","aab2b38b":"code","a00b49d3":"code","71bff46f":"code","caf9fb4f":"code","150eb493":"code","dd425956":"code","344a6eb1":"code","09ebb93e":"code","7bc2bcae":"code","3f385e8d":"code","bda6aebc":"code","0424e4be":"code","aced1d26":"code","f9ee501c":"code","f575a7b5":"code","b081df44":"code","3444c351":"code","76c6526f":"code","b7a019c7":"code","74d6c355":"code","dcb739ca":"code","da462bd0":"code","a9b85508":"code","46d70148":"code","0be1737e":"code","c086faf4":"code","12ce74ea":"code","ee90dc1b":"code","678cedad":"code","c00f2fe2":"code","f43de3f0":"code","60962c80":"code","fd2cca47":"code","2ec718ff":"code","bd196e1e":"code","ae0babc3":"code","2b0224e7":"code","0aa41cfb":"code","524ba234":"code","4771c0fa":"code","e2260dc9":"code","a0ee4a4d":"code","6bb351e5":"markdown","391593f6":"markdown","26c7d8e1":"markdown","5ffb8cb5":"markdown","d69f9eda":"markdown","921af0b7":"markdown","c37c68ea":"markdown","034e3a17":"markdown","8d164284":"markdown","f5d3a3b4":"markdown","5746968d":"markdown","6fe50b33":"markdown","272159a3":"markdown","fc100bfa":"markdown"},"source":{"b8e1b037":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport gc\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98eb8eb7":"items_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_categories_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsales_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","24cb6114":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","aab2b38b":"# get the type of the items and label them\nitem_categories_df['type'] = item_categories_df['item_category_name'].str.split('-').map(lambda x:x[0])\nitem_categories_df['type_code'] = LabelEncoder().fit_transform(item_categories_df['type'])\n\n# get the sub type of the items and label them\nitem_categories_df['sub_type'] = item_categories_df['item_category_name'].str.split('-').map(lambda x:x[1].strip() \n                                                                                             if len(x) > 1 else x[0].strip())\nitem_categories_df['sub_type_code'] = LabelEncoder().fit_transform(item_categories_df['sub_type'])\nitem_categories_df.head()","a00b49d3":"shops_df.loc[shops_df['shop_name']=='\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"','shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x:x[0])\nshops_df.loc[shops_df['city'] == '!\u042f\u043a\u0443\u0442\u0441\u043a','city']='\u042f\u043a\u0443\u0442\u0441\u043a'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.head()","71bff46f":"# replace the negative item_price to mean value\nmean = sales_df[(sales_df[\"shop_id\"] == 32) & (sales_df[\"item_id\"] == 2973) & \n                (sales_df[\"date_block_num\"] == 4) & (sales_df[\"item_price\"] > 0)][\"item_price\"].mean()\nsales_df.loc[sales_df.item_price < 0, 'item_price'] = mean\n\n# clean the outliers\nsales_df = sales_df[sales_df[\"item_price\"] < np.percentile(sales_df[\"item_price\"], q = 100)]\nsales_df = sales_df[(sales_df[\"item_cnt_day\"] >= 0) & (sales_df[\"item_cnt_day\"] < np.percentile(sales_df[\"item_cnt_day\"], q = 100))]\n\n# change the format of date\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%d.%m.%Y\")\nsales_df.head()","caf9fb4f":"for i in [(0, 57), (1, 58), (10, 11)]:\n    sales_df.loc[sales_df['shop_id'] == i[0], 'shop_id'] = i[1]\n    test_df.loc[test_df['shop_id'] == i[0], 'shop_id'] = i[1]","150eb493":"sales_df['revenue'] = sales_df['item_cnt_day'] * sales_df['item_price']","dd425956":"def name_correction(x):\n    x = x.lower() #lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x","344a6eb1":"# split item names by first bracket\nitems_df['name1'], items_df['name2'] = items_df['item_name'].str.split('[', 1).str\nitems_df['name1'], items_df['name3'] = items_df['item_name'].str.split('(', 1).str\n\n# replace special characters and turn to lower case\nitems_df['name2'] = items_df['name2'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems_df['name3'] = items_df['name3'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems_df = items_df.fillna('0')\n\nitems_df['item_name'] = items_df['item_name'].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems_df['name2'] = items_df['name2'].apply(lambda x: x[:-1] if x != '0' else '0')","09ebb93e":"items_df['type'] = (items_df['name2'].apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0]))\n\nitems_df.loc[(items_df['type'] == 'x360')|(items_df['type'] == 'xbox360')|(items_df['type'] == 'xbox 360'),'type'] = 'xbox 360'\nitems_df.loc[items_df['type'] == '', 'type'] = 'mac'\nitems_df.type = (items_df['type'].apply(lambda x: x.replace(' ', '')))\nitems_df.loc[(items_df['type'] == 'pc' )|(items_df['type'] == 'p\u0441')|(items_df['type'] == 'p\u0441'),'type'] = 'p\u0441'\n\nitems_df.loc[items_df['type'] == '\u0440s3' , 'type'] = '\u0440s3'","7bc2bcae":"group_sum = (items_df.groupby('type').agg({'item_id': 'count'}).reset_index())\n\ndrop_cols = []\nfor categ in group_sum['type'].unique():\n    if group_sum.loc[(group_sum['type'] == categ), 'item_id'].values[0] <= 39:\n        drop_cols.append(categ)\n\nitems_df['name2'] = (items_df['name2'].apply(lambda x: 'other' if x in drop_cols else x))\nitems_df = items_df.drop(['type'], axis=1)","3f385e8d":"items_df['name2'] = LabelEncoder().fit_transform(items_df['name2'])\nitems_df['name3'] = LabelEncoder().fit_transform(items_df['name3'])\n\nitems_df.drop(['item_name', 'name1'], axis=1, inplace=True)\nitems_df.head()","bda6aebc":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor month in sales_df['date_block_num'].unique():\n    cur_shops = sales_df.loc[sales_df['date_block_num'] == month, 'shop_id'].unique()\n    cur_items = sales_df.loc[sales_df['date_block_num'] == month, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [month]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_df.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\ngb.rename(columns ={'item_cnt_day':'item_cnt_month'},inplace = True)\ntrain_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n#Clip target values\ntrain_data['item_cnt_month'] = np.clip(train_data['item_cnt_month'],0,20)\ntrain_data.sort_values(index_cols, inplace = True)\n\n# Downcast dtypes from 64 to 32 bit to save memory\ntrain_data = downcast_dtypes(train_data)\ndel grid, gb \ngc.collect();","0424e4be":"# combine test set by date_block_num\ntest_df.insert(loc=3, column='date_block_num', value=34)\ntrain_data = train_data.append(test_df.drop('ID', axis = 1)).fillna(0)\ntrain_data.head().append(train_data.tail())","aced1d26":"# add shops_df\ntrain_data = pd.merge(train_data,shops_df.drop(['city','shop_name'], axis = 1), on=['shop_id'], how='left') \n\n# add items_df\ntrain_data = pd.merge(train_data, items_df, on=['item_id'], how='left') \n\n# add item_categories_df\ntrain_data = pd.merge(train_data, item_categories_df.drop(['item_category_name','type','sub_type'], axis = 1), on=['item_category_id'], how='left') ","f9ee501c":"# add month, days, and holidays\ntrain_data['month'] = train_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ntrain_data['days'] = train_data['month'].map(days)","f575a7b5":"train_data.head()","b081df44":"def generate_lag(df, lags, lag_col):\n    for i in lags:\n        shifted = df[['date_block_num','shop_id','item_id',lag_col]].copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', lag_col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","3444c351":"train_data = generate_lag(train_data, [1,2,3,6,12], 'item_cnt_month')","76c6526f":"group = train_data.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().rename('date_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3,6,12], 'date_item_avg_cnt')\ntrain_data.drop(['date_item_avg_cnt'], axis=1, inplace=True)","b7a019c7":"group = train_data.groupby(['date_block_num','shop_id'])['item_cnt_month'].mean().rename('date_shop_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3,6,12], 'date_shop_avg_cnt')\ntrain_data.drop(['date_shop_avg_cnt'], axis=1, inplace=True)","74d6c355":"group = train_data.groupby(['date_block_num','item_category_id'])['item_cnt_month'].mean().rename('date_itemcat_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','item_category_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2], 'date_itemcat_avg_cnt')\ntrain_data.drop(['date_itemcat_avg_cnt'], axis=1, inplace=True)","dcb739ca":"group = train_data.groupby(['date_block_num'])['item_cnt_month'].mean().rename('date_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_avg_cnt')\ntrain_data.drop(['date_avg_cnt'], axis=1, inplace=True)","da462bd0":"group = train_data.groupby(['date_block_num','city_code'])['item_cnt_month'].mean().rename('date_city_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'city_code'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_city_avg_cnt')\ntrain_data.drop(['date_city_avg_cnt'], axis=1, inplace=True)","a9b85508":"group = train_data.groupby(['date_block_num','city_code','item_id'])['item_cnt_month'].mean().rename('date_city_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'city_code','item_id'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_city_item_avg_cnt')\ntrain_data.drop(['date_city_item_avg_cnt'], axis=1, inplace=True)","46d70148":"group = train_data.groupby(['date_block_num','shop_id','item_id'])['item_cnt_month'].mean().rename('date_shop_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id','item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'date_shop_item_avg_cnt')\ntrain_data.drop(['date_shop_item_avg_cnt'], axis=1, inplace=True)","0be1737e":"group = train_data.groupby(['date_block_num','shop_id','sub_type_code'])['item_cnt_month'].mean().rename('date_shop_subtype_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num', 'shop_id','sub_type_code'], how='left')\ntrain_data = generate_lag(train_data, [1], 'date_shop_subtype_avg_cnt')\ntrain_data.drop(['date_shop_subtype_avg_cnt'], axis=1, inplace=True)","c086faf4":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","12ce74ea":"group = train_data.groupby(['month','item_id'])['item_cnt_month'].mean().rename('month_item_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month', 'item_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'month_item_avg_cnt')\ntrain_data.drop(['month_item_avg_cnt'], axis=1, inplace=True)","ee90dc1b":"group = train_data.groupby(['month','shop_id'])['item_cnt_month'].mean().rename('month_shop_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month', 'shop_id'], how='left')\ntrain_data = generate_lag(train_data, [1,2,3], 'month_shop_avg_cnt')\ntrain_data.drop(['month_shop_avg_cnt'], axis=1, inplace=True)","678cedad":"group = train_data.groupby(['month'])['item_cnt_month'].mean().rename('month_avg_cnt').reset_index()\ntrain_data = pd.merge(train_data, group, on=['month'], how='left')\ntrain_data = generate_lag(train_data, [1], 'month_avg_cnt')\ntrain_data.drop(['month_avg_cnt'], axis=1, inplace=True)","c00f2fe2":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","f43de3f0":"# price\n# get the mean by item_id and item_price\ngroup = sales_df.groupby(['item_id'])['item_price'].mean().rename('item_avg_price').reset_index()\ntrain_data = pd.merge(train_data, group, on=['item_id'], how='left')\n\n# get the mean by date_block_num, item_id and item_price\ngroup = sales_df.groupby(['date_block_num','item_id'])['item_price'].mean().rename('date_item_avg_price').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','item_id'], how='left')\n\n# calculate the trend of price and add lag\nlags = [1,2,3,4,5,6]\ntrain_data = generate_lag(train_data, lags, 'date_item_avg_price')\n\nfor i in lags:\n    train_data['trend_price_lag_'+str(i)] = \\\n        (train_data['date_item_avg_price_lag_'+str(i)] - train_data['item_avg_price']) \/ train_data['item_avg_price']\n    \ndef select_trend(row):\n    for i in lags:\n        if row['trend_price_lag_'+str(i)]:\n            return row['trend_price_lag_'+str(i)]\n    return 0\n    \ntrain_data['trend_price_lag'] = train_data.apply(select_trend, axis=1)\n\n# drop all the columns\nfetures_to_drop = ['item_avg_price', 'date_item_avg_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_price_lag_'+str(i)]\n    fetures_to_drop += ['trend_price_lag_'+str(i)]\n\ntrain_data.drop(fetures_to_drop, axis=1, inplace=True)","60962c80":"# get the sum by date_block_num, shop_id and revenue\ngroup = sales_df.groupby(['date_block_num','shop_id',])['revenue'].sum().rename('sum_date_shops_revenue').reset_index()\ntrain_data = pd.merge(train_data, group, on=['date_block_num','shop_id'], how='left')\n\n# get the mean by with shop_id and revenue\ngroup = group.groupby(['shop_id',])['sum_date_shops_revenue'].mean().rename('mean_shops_revenue').reset_index()\ntrain_data = pd.merge(train_data, group, on=['shop_id'], how='left')\n\n# calculate the trend of revenue and add lag\ntrain_data['trend_revenue'] = (train_data['sum_date_shops_revenue'] - train_data['mean_shops_revenue']) \/ train_data['mean_shops_revenue']\ntrain_data = generate_lag(train_data, [1], 'trend_revenue')\n\n# drop all the columns \ntrain_data.drop(['sum_date_shops_revenue'], axis=1, inplace=True)\ntrain_data.drop(['mean_shops_revenue'], axis=1, inplace=True)\ntrain_data.drop(['trend_revenue'], axis=1, inplace=True)","fd2cca47":"# add the month of each shop and item first sale\ntrain_data['item_shop_first_sale'] = (\n    train_data['date_block_num'] - train_data.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\n)\ntrain_data['item_first_sale'] = (\n    train_data['date_block_num'] - train_data.groupby(['item_id'])['date_block_num'].transform('min')\n)","2ec718ff":"#Cleaning works\ntrain_data.fillna(0,inplace = True)\ntrain_data = downcast_dtypes(train_data)\ngc.collect()","bd196e1e":"train_data = train_data[train_data['date_block_num'] > 3]\ntrain_data.head().append(train_data.tail())","ae0babc3":"X_train = train_data[train_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = train_data[train_data.date_block_num < 33]['item_cnt_month']\nX_valid = train_data[train_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = train_data[train_data.date_block_num == 33]['item_cnt_month']\nX_test = train_data[train_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","2b0224e7":"del shops_df\ndel items_df\ndel item_categories_df\ndel sales_df\ndel train_data\n\ngc.collect()","0aa41cfb":"from xgboost import XGBRegressor, plot_importance\nimport matplotlib.pyplot as plt","524ba234":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,    \n    seed=42)\n\nmodel.fit(\n    X_train,\n    Y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n    verbose=True,\n    early_stopping_rounds = 10)\n","4771c0fa":"submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","e2260dc9":"y_pred = model.predict(X_test).clip(0,20)\nsubmission['item_cnt_month'] = y_pred \nsubmission.to_csv('future_sales_pred.csv', index=False)","a0ee4a4d":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10, 14))","6bb351e5":"# **XGboost**","391593f6":"**Part two: lag feature with month**","26c7d8e1":"**Trend Feature**","5ffb8cb5":"**Preprcoess sales_df**","d69f9eda":"**Preprcoess item_categories_df**","921af0b7":"# **Feature Engineering**","c37c68ea":"**Preprcoess shops_df**","034e3a17":"**Preprcoess items_df**","8d164284":"**Part one: lag feature with date_block_num**","f5d3a3b4":"# **EDA**","5746968d":"**Lag Feature**","6fe50b33":"# **Preprocessing Data**","272159a3":"# **Load Data**","fc100bfa":"**Combine tables**"}}