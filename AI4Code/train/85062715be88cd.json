{"cell_type":{"79e51841":"code","77fc88c4":"code","f11cf9c0":"code","88b27016":"code","82deddea":"code","0d61797f":"code","65a72f04":"code","491eae81":"code","9f6d6acc":"code","f7f9630e":"code","6a6b88a6":"code","4a93605e":"code","b77464ca":"code","fc14f5ef":"code","157f9429":"code","dca25733":"code","f5194d18":"code","b0d5c5ec":"code","647c40b4":"code","a3cfd39e":"code","2bf729e4":"code","58d2b72d":"code","0af56bfa":"code","77db3cde":"code","bf512ac7":"code","b26c6d36":"code","ed2ba677":"code","7a61c007":"code","85d6f061":"code","a0d9c8c2":"code","4d720b98":"code","b6744a53":"code","ddc9f5aa":"code","a58f9e37":"markdown","f2bf881e":"markdown","b7b9c242":"markdown","4719b280":"markdown","e2b511f8":"markdown","ce65ac12":"markdown","baeeacbf":"markdown","031e2a86":"markdown","2d494600":"markdown"},"source":{"79e51841":"#import library\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\nimport pandas as pd\nimport matplotlib.pyplot as plt","77fc88c4":"#load dataset\ndata = pd.read_csv('..\/input\/indian_liver_patient.csv')","f11cf9c0":"#view dataset\ndata.head()","88b27016":"#visualization age-albumin and globulin ratio\nplt.scatter(data.Age, data.Albumin_and_Globulin_Ratio, color = 'navy',alpha = 0.3)\nplt.xlabel('Age')\nplt.ylabel('Albumin_and_Globulin_Ratio')\nplt.title('Visualization for Age-Albumin_and_Globulin_Ratio')\nplt.show()","82deddea":"data.info()","0d61797f":"corr = data.corr()\ncorr.style.background_gradient()","65a72f04":"# we created x and y array. So we understand very well.\ny = np.array([2,5,10,14,15,16,20,25,30,35,36,38,40,45,50,52,55,60,61,62]).reshape(-1,1)\nx = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]).reshape(-1,1)","491eae81":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(x,y)\nb_zero = linear_reg.intercept_\nb_one = linear_reg.coef_\nprint('b_zero: {}, b_one: {}'.format(b_zero,b_one))\n#visualization\nplt.scatter(x,y)\nx_predict = linear_reg.predict(x)\nplt.plot(x,x_predict,color='red')","9f6d6acc":"#Now we visualization for dataset\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nx = data.Total_Bilirubin.values.reshape(-1,1)\ny = data.Direct_Bilirubin.values.reshape(-1,1)\nlinear_reg.fit(x,y)\nb_zero = linear_reg.intercept_\nb_one = linear_reg.coef_\nprint('b_zero: {}, b_one: {}'.format(b_zero,b_one))\n#visualization\nplt.scatter(x,y)\nx_predict = linear_reg.predict(x)\nplt.plot(x,x_predict,color='red')","f7f9630e":"# This regression may involve x^2,x^3,....\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ny = np.array([100,95,93,90,86,85,80,82,75,70,65,55,40,45]).reshape(-1,1)\nx = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14]).reshape(-1,1)\nplt.scatter(x,y)\npolynomial_regression = PolynomialFeatures(degree=4)\nx_polynomial = polynomial_regression.fit_transform(x)\nlinear_regressionn = LinearRegression()\nlinear_regressionn.fit(x_polynomial,y)\ny_head2 = linear_regressionn.predict(x_polynomial)\nplt.plot(x,y_head2,color = 'purple',label=\"ploy\")\nplt.legend()\nplt.show()","6a6b88a6":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]","4a93605e":"#normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","b77464ca":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\n","fc14f5ef":"# create initialize weights and bias values\ndef initialize_weights_and_bias(dimension):\n    #create weights\n    w = np.full((dimension,1),0.01)\n    #create bias\n    b = 0.0\n    return w,b\n#w,b = initialize_weights_and_bias(5)\n#w,b","157f9429":"# create activation function\ndef sigmoid(z):\n    #sigmoid function\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","dca25733":"#create forward backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","f5194d18":"# for values update\ndef update(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w-learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n       # if block created for visualization\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n            \n    parameters = {\"weight\":w, \"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"number of iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n","b0d5c5ec":"#prediction\ndef predict(w,b,x_test):\n    t = np.dot(w.T,x_test)+b\n    z = sigmoid(t)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            Y_prediction[0,i] = 2\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction\n","647c40b4":"# %% logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  number_of_iterarion):\n    # initialize\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_of_iterarion)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, number_of_iterarion = 30)    ","a3cfd39e":"data1 = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin','Dataset']]","2bf729e4":"data1.head()","58d2b72d":"#patient has liver disease or not. One =liver disease, two =liver disease not.\none = data1[data1.Dataset == 1]\ntwo = data1[data1.Dataset == 2]","0af56bfa":"plt.scatter(one.Age, one.Total_Protiens, color = \"purple\", label = \"one\", alpha = 0.4)\nplt.scatter(two.Age, two.Total_Protiens, color = \"orange\", label = \"two\", alpha = 0.4)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Total_Protiens\")\nplt.legend()\nplt.show()","77db3cde":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]","bf512ac7":"# normalization\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n# (x-minx)\/(maxx-minx)","b26c6d36":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","ed2ba677":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","7a61c007":"print(\"{} n\u0131n score: {}\".format(2,knn.score(x_test,y_test)))","85d6f061":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list,color='red')\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","a0d9c8c2":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\n#svm\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"svm algoritmas\u0131n\u0131n do\u011frulu\u011fu: \",svm.score(x_test, y_test))","4d720b98":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\n#navie bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint(\"bayes algoritmas\u0131n\u0131n sonucu: \", nb.score(x_test, y_test))","b6744a53":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\nprint(\"score: \", dt.score(x_test, y_test))","ddc9f5aa":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 10, random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest result: \", rf.score(x_test,y_test))\n#confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\n#visualization\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"prediction values\")\nplt.ylabel(\"original values\")\nplt.show()","a58f9e37":"# Polynomial Regression","f2bf881e":"# Linear Regression\n\nLinear Regression is a way of predicting a response Y on the basis of a single predictor variable X. It is assumed that there is approximately a linear relationship between X and Y.\n\ny = b0 + b1*x","b7b9c242":"# Logistic Regression\n\nLike all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","4719b280":"# Support Vector Machine\n\nSupport vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n","e2b511f8":"# Navie Bayes Classifications","ce65ac12":"# Random Forest","baeeacbf":"# KNN Algorithm\n\nA case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a distance function. If K = 1, then the case is simply assigned to the class of its nearest neighbor. ","031e2a86":"# MACHINE LEARNING\n\nThis tutorial created for observe the result of algorithms.","2d494600":"# Decision Tree"}}