{"cell_type":{"e66d36f5":"code","61c586c4":"code","675a4af9":"code","98600a47":"code","d6dd6028":"code","afe3f4b5":"code","d540b8f4":"code","ff1044e5":"code","fbc6eb25":"code","adff26fc":"code","6af8f7f1":"code","0877c4b4":"code","52439dc7":"code","bccfd3c5":"code","de8a631e":"code","1b7598a0":"code","1a470c49":"code","cd5bb9aa":"markdown","8d4bf54b":"markdown","720ee74c":"markdown","41781359":"markdown","c01b3382":"markdown","c2790cc1":"markdown","88bcb76f":"markdown","6b4d4a83":"markdown"},"source":{"e66d36f5":"train_dir = \"..\/input\/face-mask\/face-mask-database\/Train\/\"\nvalid_dir = \"..\/input\/face-mask\/face-mask-database\/Valid\/\"\ntest_dir  = \"..\/input\/face-mask\/face-mask-database\/Test\/\"","61c586c4":"import numpy as np\nimport cv2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.utils import to_categorical\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","675a4af9":"target_size=(96,96)\nbatch_size = 16","98600a47":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    color_mode='rgb',    \n    shuffle=True,\n    seed=42,\n    class_mode='categorical')","d6dd6028":"valid_datagen = ImageDataGenerator(rescale=1.\/255)\n\nvalid_generator = valid_datagen.flow_from_directory(\n    valid_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    color_mode='rgb',\n    shuffle=False,    \n    class_mode='categorical')","afe3f4b5":"test_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    color_mode='rgb', \n    shuffle=False,    \n    class_mode='categorical')","d540b8f4":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model,save_model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, LeakyReLU, Concatenate\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix","ff1044e5":"num_classes = 2 # WithMask, WithoutMask","fbc6eb25":"input_shape = (96,96,3)","adff26fc":"# Build Model\ninput_image = Input(shape=input_shape)\n# 1st Conv layer\nmodel = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape)(input_image)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 2nd Conv layer\nmodel = Conv2D(32, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 3rd Conv layer\nmodel = Conv2D(64, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 4th Conv layer\nmodel = Conv2D(128, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# 5th Conv layer\nmodel = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\nmodel = MaxPooling2D((2, 2),padding='same')(model)\n# FC layers\nmodel = Flatten()(model)\n#model = Dense(1024, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(model)\nmodel = Dense(1024)(model)\n#model = Dropout(0.2)(model)\n\n#model = Dense(64, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(model)\nmodel = Dense(64)(model)\n#model = Dropout(0.2)(model)\n\noutput= Dense(num_classes, activation='softmax')(model)\n\nmodel = Model(inputs=[input_image], outputs=[output])\n\nmodel.summary()","6af8f7f1":"# Compile Model\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","0877c4b4":"STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\nSTEP_SIZE_TEST =test_generator.n\/\/test_generator.batch_size\nnum_epochs = 100","52439dc7":"# Train Model\nmodel.fit_generator(train_generator,steps_per_epoch=STEP_SIZE_TRAIN,epochs=num_epochs, validation_data=valid_generator, validation_steps=STEP_SIZE_VALID) #, callbacks=[checkpoint])","bccfd3c5":"save_model(model, \"facemask_cnn.h5\")","de8a631e":"score = model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\nprint(score)","1b7598a0":"predY=model.predict_generator(test_generator)\ny_pred = np.argmax(predY,axis=1)\n#y_label= [labels[k] for k in y_pred]\ny_actual = test_generator.classes\ncm = confusion_matrix(y_actual, y_pred)\nprint(cm)","1a470c49":"# report\nlabels = ['withMask', 'withoutMask']\nprint(classification_report(y_actual, y_pred, target_names=labels))","cd5bb9aa":"## Build Model","8d4bf54b":"## Train Model","720ee74c":"## Data Augmentation","41781359":"## Save Model","c01b3382":"# FaceMask Image Classification\n### withMask, withoutMask","c2790cc1":"## Confusion Matrix","88bcb76f":"## [Dataset](https:\/\/www.kaggle.com\/ashishjangra27\/face-mask-12k-images-dataset)\n![image.png](attachment:image.png)","6b4d4a83":"## Evaluate Model"}}