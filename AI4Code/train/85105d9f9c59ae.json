{"cell_type":{"e5c22576":"code","310aa7ed":"code","355a263c":"code","71f5c481":"code","f12ee3c4":"code","8ebc7606":"code","d41edff5":"code","21b5b8e2":"code","abf3475d":"code","48ea11db":"code","88bd5dde":"code","f43c75be":"code","6172c68e":"code","35548576":"code","b1750c1e":"code","6a21859d":"code","6ecc7a4f":"code","08f44bf1":"code","8edf1ad7":"code","e1927178":"code","454d1e0a":"code","6eb89068":"code","cd0a14f8":"code","68342281":"code","934bcafe":"code","605e6c7e":"code","233471a4":"code","e25d44fa":"code","1b3256bf":"code","3bb4b528":"code","8854bb15":"code","51cadd29":"code","845f4f2f":"code","7620efa9":"code","f23d468c":"code","cbf29f9a":"code","8a9b75ab":"code","f8fbcfac":"code","6ded0f59":"code","82853e6f":"code","b27536c7":"code","e55637ef":"code","c90dd5b9":"code","dda4f31e":"code","79a1c343":"code","e1118bbb":"code","f516cb73":"code","1c2fb7d5":"code","212cbc7b":"code","0c3dbd26":"code","dc9146b7":"code","6eda57d9":"code","27812d1d":"code","e9289e54":"code","afc22232":"code","8be350d4":"code","74c23441":"code","9d6556d7":"code","0bc6723c":"code","addb4be0":"code","5dc2243e":"code","ffe1cbea":"code","c719dc75":"code","96418158":"code","091e1a25":"code","15d11498":"code","f5d97d13":"code","0d688651":"code","b4fc8480":"code","122b15bc":"code","fc05c941":"code","2e58bf28":"code","fd8a287b":"code","a5983d9e":"code","5c9af980":"code","a6182991":"code","37efa84d":"code","26b94ff2":"code","c47f92a3":"code","6d418be9":"code","fccdc79e":"code","3877325d":"code","a9855e30":"code","2391f057":"code","67316e9f":"code","2d773a7c":"code","957293aa":"code","48039e95":"code","410c562f":"code","d82f15ba":"code","59df3e69":"code","0fb192e5":"code","d798d669":"code","dca37309":"code","7374fa5d":"code","c3165528":"code","cce57af1":"code","9bc2e27e":"code","3d905f9b":"code","e96f14fb":"code","55c62954":"code","7547a82a":"code","b442163f":"code","8fceeffa":"code","f0b25fc1":"code","290f8b9a":"code","07c0abb9":"code","841a5c2d":"code","5f5ac400":"code","d5aee02b":"code","86ec40ed":"code","51e8c2ea":"code","97d59aad":"code","e89133bd":"markdown","e7c79002":"markdown","26f712b6":"markdown","af856c79":"markdown","ea6e142b":"markdown","8727c055":"markdown","e53aa69f":"markdown","f2c164b4":"markdown","8b05cf2a":"markdown","7ff021f5":"markdown","f3e70f0c":"markdown","1b619fd7":"markdown","08af671d":"markdown","6790880a":"markdown","a076f704":"markdown","37d4122e":"markdown","46439a05":"markdown","ca09ee39":"markdown","a0e217b4":"markdown","ba19c718":"markdown","b0e5d505":"markdown","aa01c166":"markdown","a1755703":"markdown","e3033d5f":"markdown","c71074eb":"markdown","adc9aef4":"markdown","dd634297":"markdown","b1d952d6":"markdown","c0e4ae15":"markdown","8db6a7f4":"markdown","f415e126":"markdown","7f79a74c":"markdown"},"source":{"e5c22576":"import pandas as pd\nimport numpy as np","310aa7ed":"data = 'https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-02-car-price\/data.csv'","355a263c":"!wget $data ","71f5c481":"df = pd.read_csv('data.csv')","f12ee3c4":"# Making the columns uniform in terms of capitalisation and having no spaces in the columns\ndf.columns = df.columns.str.lower().str.replace(' ', '_')","8ebc7606":"# We're doing the same for the 'make' column of the dataset \ndf['make'].str.lower().str.replace(' ', '_')","d41edff5":"# Finding categorical variables so that we can standardise the inputs. \nstrings = list(df.dtypes[df.dtypes == 'object'].index)\nstrings","21b5b8e2":"# Standardising the inputs \nfor col in strings:\n    df[col] = df[col].str.lower().str.replace(' ', '_')","abf3475d":"df.dtypes","48ea11db":"# There are a few datatypes here, let's count how many of each datatype there is here! \ndf.dtypes.value_counts()","88bd5dde":"for col in df.columns:\n    print(col)\n    print(df[col].unique()[:5])\n    print(df[col].nunique())\n    print()","f43c75be":"df","6172c68e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","35548576":"# Visualisation of MSRP \nsns.histplot(df.msrp, bins=50)","b1750c1e":"sns.histplot(df.msrp[df.msrp < 100000], bins=50)","6a21859d":"# Transformation of certain numbers using log(1+x)\nnp.log1p([0, 1, 10, 1000, 100000])","6ecc7a4f":"# This is the same as using log1p, except that we are manually adding the one 1 before applying logarithms on it.\nnp.log([0 + 1, 1+ 1, 10 + 1, 1000 + 1, 100000])","08f44bf1":"# Transformation of the MSRPs\nprice_logs = np.log1p(df.msrp)","8edf1ad7":"# Look at the scale - it has drastically reduced! This is especially wonderful for linear regression since linear regression is rather sensitive to differences :)\nsns.histplot(price_logs, bins=50)","e1927178":"# Okay, let's take a look at the dataset and see if there are any missing values! \ndf.isnull().sum()","454d1e0a":"# Splitting of dataset into the train, validation, and the test dataset \nn = len(df)\n\n# Validation dataset\nn_val = int(n * 0.2)\n\n# Test dataset \nn_test = int(n * 0.2)\n\n# Train dataset\nn_train = n - n_val - n_test","6eb89068":"n","cd0a14f8":"n_val, n_test, n_train","68342281":"df.iloc[[10, 0, 3, 5]]","934bcafe":"df_train = df.iloc[n_train:]\ndf_val = df.iloc[n_train:n_train+n_val]\ndf_test = df.iloc[n_train+n_val:]","605e6c7e":"# Gives an array from 0 to the specified number itself. This will help in what we're trying to do.\nidx = np.arange(n)\nidx","233471a4":"# We're shuffling the indexes so that we are able to get random datapoints out of the dataset \n# This is so that we are able to reduce any bias by any previous points\nnp.random.seed(2)\nnp.random.shuffle(idx)","e25d44fa":"df_train = df.iloc[idx[:n_train]]\ndf_val = df.iloc[idx[n_train:n_train+n_val]]\ndf_test = df.iloc[idx[n_train+n_val:]]","1b3256bf":"# To visualise above, this is what's above:\nidx[:n_train]","3bb4b528":"df_train.head()","8854bb15":"len(df_train), len(df_val), len(df_test)","51cadd29":"# As seen above, the index of the train data is mixed up, so let's reset the index back to 0 - 7150.\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","845f4f2f":"# Transformation of the y values\ny_train = np.log1p(df_train.msrp.values)\ny_val = np.log1p(df_val.msrp.values)\ny_test = np.log1p(df_test.msrp.values)","7620efa9":"del df_train['msrp']\ndel df_val['msrp']\ndel df_test['msrp']","f23d468c":"len(y_train)","cbf29f9a":"df_train.iloc[10]","8a9b75ab":"# From index number 10: We choose engine_hp, city_mpg and popularity \nxi = [453, 11, 86]\nw0 = 7.17\nw = [0.01, 0.04, 0.002]\n\n# How did you get the initial bias values and the weights? ","f8fbcfac":"# Python code for how the linear regression works, but in a more general sense \ndef linear_regression(xi):\n    # number of features in the feature matrix\n    n = len(xi)\n    \n    # We have an initial bias term W0, so that is the starting of our prediction.\n    pred = w0\n    \n    # Recursive feature whereby the feature + the weight\/bias is being added.\n    # Range(n) starts at 0 and ends at n-1 by default\n    for j in range(n):\n        pred = pred + w[j] * xi[j]\n        \n    # End product \n    return pred","6ded0f59":"xi = [453, 11, 86]\nw0 = 7.17\nw = [0.01, 0.04, 0.002]","82853e6f":"linear_regression(xi)","b27536c7":"# Since we did log1p just now, we have to reverse it so we do a exponential first and then we minus 1.\nnp.expm1(12.312)","e55637ef":"# Let's double check the value! \nnp.log1p(222347.2221101062)","c90dd5b9":"# Do note that this dot product only starts from weight 1 and feature 1. We have to consider w0, which will be covered.\ndef dot(xi, w):\n    n = len(xi)\n    \n    res = 0.0\n    \n    for j in range(n):\n        res = res + xi[j] * w[j]\n    \n    return res","dda4f31e":"# Inclusion of weight 0 \ndef linear_regression(xi):\n    return w0 + dot(xi, w)","79a1c343":"# w is a list in this case, we can join two lists together to get the full matrix. \nw_new = [w0] + w","e1118bbb":"w_new","f516cb73":"# Why do we add [1] here? It is to consider w0. \ndef linear_regression(xi):\n    xi = [1] + xi\n    return dot(xi, w_new)","1c2fb7d5":"linear_regression(xi)","212cbc7b":"w0 = 7.17\nw = [0.01, 0.04, 0.002]\nw_new = [w0] + w","0c3dbd26":"# We always have 1s in the beginning because of w0.\n# If we do 0 instead, it means that there will be no w0 when we do the multiplication, which makes it incorrect.\nx1  = [1, 148, 24, 1385]\nx2  = [1, 132, 25, 2031]\nx10 = [1, 453, 11, 86]\n\nX = [x1, x2, x10]\nX = np.array(X)\nX\n","dc9146b7":"def linear_regression(X):\n    return X.dot(w_new)\n\n# Why is this possible?\n# This is because X is a matrix with size (3,4) while w_new is a matrix with size (4,1)\n# This produces the results in a matrix with size (3,1) from vector multiplication.","6eda57d9":"linear_regression(X)","27812d1d":"def train_linear_regression(X, y):\n    pass","e9289e54":"X = [\n    [148, 24, 1385],\n    [132, 25, 2031],\n    [453, 11, 86],\n    [158, 24, 185],\n    [172, 25, 201],\n    [413, 11, 86],\n    [38,  54, 185],\n    [142, 25, 431],\n    [453, 31, 86],\n]\n\nX = np.array(X)\nX","afc22232":"# Why do we need this? This is for the initial bias term. \nones = np.ones(X.shape[0])\nones","8be350d4":"# Joining of the two matrices together to ensure there is an inclusion of the initial bias term \n# X = np.column_stack([ones, X])\n# X","74c23441":"y = [10000, 20000, 15000, 20050, 10000, 20000, 15000, 25000, 12000]","9d6556d7":"# Finding of the weights \nXTX = X.T.dot(X)\nXTX_inv = np.linalg.inv(XTX)\nw_full = XTX_inv.dot(X.T).dot(y)","0bc6723c":"w0 = w_full[0]\nw = w_full[1:]","addb4be0":"w0, w","5dc2243e":"# Note: We should be using the X without the ones for the linear regression so that the ones will not stack.\ndef train_linear_regression(X, y):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]","ffe1cbea":"train_linear_regression(X, y)","c719dc75":"df_train.columns","96418158":"base = ['engine_hp', 'engine_cylinders', 'highway_mpg',\n        'city_mpg', 'popularity']\n\nX_train = df_train[base].fillna(0).values\n\nw0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w0 + X_train.dot(w)","091e1a25":"w0","15d11498":"w","f5d97d13":"sns.histplot(y_pred, color='red', alpha=0.5, bins=50)\nsns.histplot(y_train, color='blue', alpha=0.5, bins=50)\n","0d688651":"def rmse(y, y_pred):\n    se = (y - y_pred) ** 2\n    mse = se.mean()\n    return np.sqrt(mse)","b4fc8480":"rmse(y_train, y_pred)","122b15bc":"# This is the same code as the steps taken in 2.8, but applied to X_train and X_val. \ndef prepare_X(df):\n    df_num = df[base]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","fc05c941":"X_train = prepare_X(df_train)\nw0, w = train_linear_regression(X_train, y_train)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred)","2e58bf28":"df['year'].max()","fd8a287b":"# We're simply just adding an 'age' category\n# As seen above, the max year = 2017, therefore we are able to use '2017' and then derive the age from there.\ndef prepare_X(df):\n    # We use a copy so as to prevent any changes for the old dataframe \n    df = df.copy()\n    \n    df['age'] = 2017 - df['year']\n    features = base + ['age']\n    \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n\n    return X","a5983d9e":"X_train = prepare_X(df_train)\nw0, w = train_linear_regression(X_train, y_train)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred)","5c9af980":"sns.histplot(y_pred, label='prediction', color='red', alpha=0.5, bins=50)\nsns.histplot(y_val, label='target', color='blue',  alpha=0.5, bins=50)\nplt.legend()","a6182991":"categorical_columns = [\n    'make', 'model', 'engine_fuel_type', 'driven_wheels', 'market_category',\n    'vehicle_size', 'vehicle_style']\n\ncategorical = {}\n\nfor c in categorical_columns:\n    categorical[c] = list(df_train[c].value_counts().head().index)","37efa84d":"def prepare_X(df):\n    df = df.copy()\n    \n    df['age'] = 2017 - df['year']\n    features = base + ['age']\n    \n    # Refer to footnote 1\n    for v in [2, 3, 4]:\n        df['num_doors_%d' % v] = (df.number_of_doors == v).astype(int)\n        features.append('num_doors_%d' % v)\n        \n    # Refer to footnote 2 \n    for name, values in categorical.items():\n        for value in values:\n            df['%s_%s' % (name, value)] = (df[name] == value).astype(int)\n            features.append('%s_%s' % (name, value))\n\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n\n    return X","26b94ff2":"# Footnote 2: What's really going on in this for loop? It seems rather confusing. Let's try to dissect it.\ntest = categorical.items()\nprint(test)\nprint('\\n')\n# items() allow a dictionary to be converted into a 2-dimensional list\n\nfor name, values in categorical.items():\n    print(name)\n    print(values)\n# the first element in the list is the name of the column itself, while the second element are the values themselves\n# therefore, we need a second for loop for the list itself to extract the different values \n# ","c47f92a3":"X_train = prepare_X(df_train)\nw0, w = train_linear_regression(X_train, y_train)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred)","6d418be9":"w0, w","fccdc79e":"# Assume we have a matrix X with two columns of the same values.\n\nX = [\n    [4, 4, 4],\n    [3, 5, 5],\n    [5, 1, 1],\n    [5, 4, 4],\n    [7, 5, 5],\n    [4, 5, 5],\n]\nX = np.array(X)\nX\n\n# IF we have two columns with the same values, the gram matrix cannot be inversed as shown below.\n# This is known as a singular matrix, which is a matrix containing the same rows and\/or columns.\n# Singular matrices are not inversible.","3877325d":"XTX = X.T.dot(X)\nXTX","a9855e30":"# Let's test if this is inversible or not!\ntry:\n    print(np.linalg.inv(XTX))\nexcept np.linalg.LinAlgError:\n    print('Singular matrix, can\\'t be inversed')","2391f057":"# Sometimes, there is noise in the dataset itself like in the last element of the last row. \n# In the original notebook, it is 5.00000001, but I still got a LinAlgError so I decided to make it a little noisier.\nX = [\n    [4, 4, 4],\n    [3, 5, 5],\n    [5, 1, 1],\n    [5, 4, 4],\n    [7, 5, 5],\n    [4, 5, 5.0000001],\n]\nX = np.array(X)\nX","67316e9f":"y= [1, 2, 3, 1, 2, 3]","2d773a7c":"# In this case, we can see that the matrix is now different and that there there are no columns with the same values.\nXTX = X.T.dot(X)\nXTX","957293aa":"# Let's test if this is inversible or not!\ntry:\n    print(np.linalg.inv(XTX))\nexcept np.linalg.LinAlgError:\n    print('Singular matrix, can\\'t be inversed')","48039e95":"# Storing the XTX inverse as a variable.\nXTX_inv = np.linalg.inv(XTX)\nXTX","410c562f":"# Finding the weights of the metric\nXTX_inv.dot(X.T).dot(y)","d82f15ba":"XTX = [\n    [1, 2, 2],\n    [2, 1, 1.0000001],\n    [2, 1.0000001, 1]\n]\n\nXTX = np.array(XTX)","59df3e69":"np.linalg.inv(XTX)","0fb192e5":"XTX = [\n    [1.0001, 2, 2],\n    [2, 1.0001, 1],\n    [2, 1, 1.0001]\n]\n\nXTX = np.array(XTX)","d798d669":"np.linalg.inv(XTX)","dca37309":"# Recall np.eye gives you an identity  matrix with the size n x n with the right parameters\nXTX = XTX + 0.01 * np.eye(3)","7374fa5d":"np.linalg.inv(XTX)","c3165528":"# Let's implement the regularization into our linear regression\ndef train_linear_regression_reg(X, y, r=0.001):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n\n    XTX = X.T.dot(X)\n    XTX = XTX + r * np.eye(XTX.shape[0])\n\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]","cce57af1":"X_train = prepare_X(df_train)\nw0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred)","9bc2e27e":"# We're tuning the model with regularization, using 7 different values of r.\nfor r in [0.0, 0.00001, 0.0001, 0.001, 0.1, 1, 10]:\n    X_train = prepare_X(df_train)\n    w0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\n    X_val = prepare_X(df_val)\n    y_pred = w0 + X_val.dot(w)\n    score = rmse(y_val, y_pred)\n    \n    print(r, w0, score)","3d905f9b":"r = 0.001\nX_train = prepare_X(df_train)\nw0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nscore = rmse(y_val, y_pred)\nscore","e96f14fb":"# Recall that for the Model Selection process, we combine train data and val data after using val data.\ndf_full_train = pd.concat([df_train, df_val])","55c62954":"df_full_train = df_full_train.reset_index(drop=True)","7547a82a":"X_full_train = prepare_X(df_full_train)","b442163f":"X_full_train","8fceeffa":"# We have to do that for y too.\ny_full_train = np.concatenate([y_train, y_val])","f0b25fc1":"w0, w = train_linear_regression_reg(X_full_train, y_full_train, r=0.001)","290f8b9a":"X_test = prepare_X(df_test)\ny_pred = w0 + X_test.dot(w)\nscore = rmse(y_test, y_pred)\nscore","07c0abb9":"# Using the model for one entry\ncar = df_test.iloc[20].to_dict()\ncar","841a5c2d":"df_small = pd.DataFrame([car])\ndf_small","5f5ac400":"X_small = prepare_X(df_small)","d5aee02b":"y_pred = w0 + X_small.dot(w)\ny_pred = y_pred[0]\ny_pred","86ec40ed":"prediction = np.expm1(y_pred)","51e8c2ea":"test = np.expm1(y_test[20])","97d59aad":"print('RMSE: ' + str((prediction - test)** 0.5))","e89133bd":"We can tell that there is a very big difference in the 2 by 2 sub matrix at the bottom right corner itself. This shows the effect of regularization itself and how useful it is! \n\nYou might be wondering how to implement regularization in the model itself - here's how! ","e7c79002":"We can tell that there is an even further decrease in the RMSE as compared to the previous RMSE, which is great! ","26f712b6":"Missing values","af856c79":"## 2.5 Linear regression","ea6e142b":"## 2.10 Validating the model","8727c055":"What we can do is that we can use regularization, which is adding a small number to the diagonal of our matrix. This allows us to counteract the big numbers itself instead. We have an example like that below. ","e53aa69f":"## 2.6 Linear regression vector form\nThe general formula, as mentioned above, is: \n# g(xi) = W0 + summation(1,n) (w[j] * xi[j]) \n\nHowever, it can be written as another form. Specifically if we look at (w[j] * xi[j]),\n# g(xi) = w0 + xi^T * W \n\nwhereby xi^T is the transpose of xi. ","f2c164b4":"There are definitely some outliers out there in the visualisation, if not we will be looking at a smaller scale. Let's delve further! ","8b05cf2a":"Recall that using the feature matrix (xi), we put it into a model and produce yi. \n\nIn other words, \n\ng(xi) = yi","7ff021f5":"## 2.8 Car price baseline model","f3e70f0c":"## 2.11 Simple feature engineering","1b619fd7":"## 2. Machine Learning for Regression\n","08af671d":"## 2.4 Setting up the validation framework","6790880a":"## 2.7 Training a linear regression model","a076f704":"## 2.17 Summary\n\n* EDA - looking at data, finding missing values\n* Target variable distribution - long tail => bell shaped curve\n* Validation framework: train\/val\/test split (helped us detect problems)\n* Normal equation - not magic, but math\n* Implemented it with numpy\n* RMSE to validate our model\n* Feature engineering: age, categorical features\n* Regularization to fight numerical instability","37d4122e":"Distribution of price","46439a05":"Why is this so? \n- Recall that both the weights and feature matrices are vectors, with size (n,1) whereby n is the number of features.\n- Since the number of weights = number of features, they both have the same size.\n- However, in order for vector-vector multiplication to occur, the **first vector** needs to have the **same number of columns** as the **number of rows of the second vector**. \n- We can either transpose the weights or we can transpose the feature matrix. \n- In this case, we transpose xi so that we are able to get a matrix with size (1,n).\n- Since we want to get the inner product, we will use xi^T * W so we get a product of (1,1) aka the prediction (instead of W * Xi ^T)\n- Finally, the vector-vector multiplication occurs, and we get the 'prediction'. \n\n(Note: We can transpose either vector, but since we're going with the transpose of xi, we have to make sure that we change the position of the matrices as the position affects the product of matrix-matrix multiplication, or in this case, vector-vector multiplication.) ","ca09ee39":"We can tell that 0.00001 has the lowest RMSE, followed by 0.0001, and lastly 0.001. Honestly, we can use either of the regularization values since the RMSE score has been about the same. In the example below, we use r = 0.001. ","a0e217b4":"## 2.2 Data preparation","ba19c718":"## 2.13 Regularization","b0e5d505":"This is a problem as the gram matrix NEEDS to be inversed so that we can find the weights vector.","aa01c166":"## 2.12 Categorical variables","a1755703":"## 2.3 Exploratory data analysis","e3033d5f":"Comparing with 2.10 and 2.11, we can tell that there is a slight change to the RMSE itself - 0.76165 vs 0.51721.\nYou can tell that there also is a difference between the previous histogram and the histogram below. \n","c71074eb":"## 2.14 Tuning the model","adc9aef4":"## 2.9 RMSE\nRMSE is defined as Root Mean Squared Error.\n\nTo take this step by step:\n1. Find the squared error, aka the square of error between the actual value of y and the predicted values of y.\n2. The mean squared error the mean of the squared errors themselves.\n3. Lastly, root means to square root the MSE. ","dd634297":"## 2.15 Using the model","b1d952d6":"For this specific example: \n\n# g(xi) = W0 + W1 * X1 + W2 * X2 + W3 * X3\n\nwhereby X is the value of the feature\nwhereby W is the weight\/bias of the feature.\n\nW0 is the bias term itself without knowing anything about the car. \n\nW1 * X1 + W2 * X2 + W3 * X3 is basically a summation\/sigma (refer to the video to see how the summation looks like)\n\nTherefore, mathematically, it can be simplified to \n# g(xi) = W0 + summation(1,3) (w[j] * xi[j])  \nIf we use python, we have to start from 0 instead, therefore \n# g(xi) = W0 + summation(0,2) (w[j] * xi[j])\n\nDo note that the parameters for the summation are the lower and upper limits.","c0e4ae15":"* We included only 5 top features. What happens if we include 10?\n\nOther projects\n\n* Predict the price of a house - e.g. boston dataset\n* https:\/\/archive.ics.uci.edu\/ml\/datasets.php?task=reg\n* https:\/\/archive.ics.uci.edu\/ml\/datasets\/Student+Performance","8db6a7f4":"However, even though there is an inverse, we can tell that the inverse has very, very big numbers which is a problem. It directly affects the weights matrix.","f415e126":"Recall that:\n# g(X) = X * w = y\n\nIf we want to find out the vector w (aka the weights), we can do so by adding an inverse of X to both Xw and y, so that we can find w. \n\nHowever, that's not necessarily possible as X might not have an inverse due to it being rectangular in shape. What we can do, however, is to multiply the transpose of X for both sides, therefore creating:\n# X^T * X * w = X^T * y \n\nX^T * X is also known as a gram matrix with size (n+1, n+1), therefore allowing for an inverse and allowing us to find w as shown below: \n\n# (X^T * X)^-1 * X^T * X * w = (X^T * X)^-1 * X^T * y \nwhich is then simplified to \n# I * w =  (X^T * X)^-1 * X^T * y \n\nand I * w is w. \n\nDo note that w is not the exact solution, but rather the closest solution.\n","7f79a74c":"## 2.16 Next steps"}}