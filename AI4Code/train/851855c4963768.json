{"cell_type":{"13e2d819":"code","6ab8d47f":"code","f1a176b3":"code","c51b2c28":"code","f40deb7c":"code","28547a91":"code","879c06be":"code","e0b02982":"code","10c4e6da":"code","c56a7ee9":"code","80a29d13":"code","213cf9b4":"code","b78c1ce4":"code","9578ed49":"code","d1f37179":"code","df47c4a7":"code","911e4cb0":"code","70845ba1":"code","beb240e6":"code","9e6a2a8e":"code","25771303":"code","7b21d83d":"code","8c4c6f99":"code","85790b20":"code","12a66ac7":"code","2a925a91":"code","06425a96":"code","ff6d2306":"code","1c9f7b7e":"code","b8467731":"code","2e5575d6":"code","acf00273":"code","31f0a1ee":"markdown","3da3eeb9":"markdown","d5631e17":"markdown","7dc12737":"markdown","773af9f3":"markdown","1b158ec3":"markdown","c71fbc34":"markdown","3026cd5f":"markdown","5f8ba9ed":"markdown","e3d4c167":"markdown","b4c9e63d":"markdown","47825d0f":"markdown","ebb0d1c6":"markdown","a29d51dc":"markdown","0833b29e":"markdown","34971fd9":"markdown","9c89b7b9":"markdown","c94679fe":"markdown","75cb9f83":"markdown","a5bf2168":"markdown","7a8da5ed":"markdown","9bd844a2":"markdown"},"source":{"13e2d819":"#This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6ab8d47f":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns # for making plots with seaborn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import ensemble","f1a176b3":"# Read the data\nX = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')\nX_sample = pd.read_csv('\/kaggle\/input\/learn-together\/sample_submission.csv', index_col='Id')","c51b2c28":"X.head()","f40deb7c":"X.describe()","28547a91":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of forest categories(Target Variable)\")\nax = sns.distplot(X[\"Cover_Type\"])","879c06be":"# Get list of categorical variables\ns = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","e0b02982":"# Remove rows with missing target, create a target dataset and remove Target from X dataset\nX.dropna(axis=0, subset=['Cover_Type'], inplace=True)\ny = X.Cover_Type              \nX.drop(['Cover_Type'], axis=1, inplace=True)","10c4e6da":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)","c56a7ee9":"# Define the model\nmy_model_1 = RandomForestClassifier()\n\n# Fit the model or training your model\nmy_model_1.fit(X_train, y_train)","80a29d13":"# Get predictions or get the target output based on 'unseen' data in this case validation data.\npredictions_1 = my_model_1.predict(X_valid)\n\n# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid) # Your code here\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_1)","213cf9b4":"\naccuracy = accuracy_score(y_valid, predictions_1)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","b78c1ce4":"# Define the model\nmy_model_2 = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train) \n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid) \n\n# Calculate MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid) \n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_2)\n\naccuracy = accuracy_score(y_valid, predictions_2)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","9578ed49":"# Define the model\nmy_model_3 = RandomForestClassifier(n_estimators=1000, max_depth=20,random_state=0)\n\n# Fit the model\nmy_model_3.fit(X_train, y_train)\n\n# Get predictions\npredictions_3 = my_model_3.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(predictions_3, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, predictions_3)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","d1f37179":"# Basic XGB classifier\nmodel_4 = XGBClassifier(learning_rate=0.01,n_estimators=1000,max_depth=20,random_state=0)\nmodel_4.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds4_test = model_4.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds4_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds4_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","df47c4a7":"# Basic XGB classifier\nmodel_5 = XGBClassifier(learning_rate=0.01,n_estimators=1000,max_depth=5,random_state=0)\nmodel_5.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds5_test = model_5.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds5_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds5_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","911e4cb0":"# Basic XGB classifier\nmodel_6 = XGBClassifier(learning_rate=0.01,n_estimators=1000,max_depth=10,random_state=0)\nmodel_6.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds6_test = model_6.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds6_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds6_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","70845ba1":"# Basic XGB classifier\nmodel_7 = XGBClassifier(learning_rate=0.01,n_estimators=1000,max_depth=15,random_state=0)\nmodel_7.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds7_test = model_7.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds7_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds7_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","beb240e6":"# Basic XGB classifier\nmodel_8 = XGBClassifier(learning_rate=0.01,n_estimators=1000,max_depth=25,random_state=0)\nmodel_8.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds8_test = model_8.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds8_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds8_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","9e6a2a8e":"model_9 = ensemble.ExtraTreesClassifier(n_estimators=350)\nmodel_9.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds9_test = model_9.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds9_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds9_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","25771303":"trying using neural network with KERAS API","7b21d83d":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","8c4c6f99":"from tensorflow.python.data import Dataset\nimport tensorflow as tf\nfrom tensorflow import keras","85790b20":"model_10 = keras.Sequential([\n keras.layers.Dense(256, activation=tf.nn.relu, input_shape=(X_train.shape[1],)), # neurons with relu activation, first layer with input \n #keras.layers.Dropout(0.5), # dropout for reducing the overfitting problem\n #keras.layers.Dense(512, activation=tf.nn.relu), # 2nd hidden layer\n #keras.layers.Dropout(0.5),\n #keras.layers.Dense(256, activation=tf.nn.relu), # 3rd hidden layer\n #keras.layers.Dropout(0.5),\n keras.layers.Dense(7, activation=tf.nn.softmax)]) #  output layer with 7 categories\n\nmodel_10.compile(loss='sparse_categorical_crossentropy', #this loss method is useful for multiple categories, otherwise our model does not work\n optimizer=tf.train.AdamOptimizer(learning_rate=0.0043, beta1=0.9), metrics=['accuracy'])","12a66ac7":"# train the model\nhistory1 = model_10.fit(X_train, y_train, epochs = 300, batch_size = 32, verbose=2, validation_data = (X_valid, y_valid))","2a925a91":"model_9 = ensemble.ExtraTreesClassifier(n_estimators=350)\nmodel_9.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds9_test = model_9.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds9_test, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds9_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","06425a96":"import sklearn\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.experimental import enable_hist_gradient_boosting\n# Going to use these  base models for the stacking\nfrom sklearn.ensemble import (  BaggingClassifier, ExtraTreesClassifier,  HistGradientBoostingClassifier)\nfrom tqdm import tqdm\nfrom mlxtend.classifier import StackingCVClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","ff6d2306":"clf_lgbm = LGBMClassifier(n_estimators=400,num_leaves=100,verbosity=0)\nclf_knc = KNeighborsClassifier(n_jobs = -1, n_neighbors =1)\nclf_etc = ExtraTreesClassifier(random_state = 1, n_estimators = 900, max_depth =50,max_features = 30)\nclf_hbc = HistGradientBoostingClassifier(random_state = 1, max_iter = 500, max_depth =25)","1c9f7b7e":"ensemble = [\n            ('clf_knc', clf_knc),\n            ('clf_hbc', clf_hbc),\n            ('clf_etc', clf_etc),\n            ('clf_lgbm', clf_lgbm)\n           \n           ]\n\nmodel_stack = StackingCVClassifier(classifiers=[clf for label, clf in ensemble],\n                             meta_classifier=clf_lgbm,\n                             cv=3,\n                             use_probas=True, \n                             use_features_in_secondary=True,\n                             verbose=-1,\n                             n_jobs=-1)","b8467731":"model_stack.fit(X_train, y_train)\n\n# Preprocessing of test data, fit model\npreds_stack = model_stack.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(preds_stack, y_valid)\n\n#print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\naccuracy = accuracy_score(y_valid, preds_stack)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","2e5575d6":"preds_test = model_stack.predict(X_test)","acf00273":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'Cover_Type': preds_test})\noutput.to_csv('submission.csv', index=False)","31f0a1ee":"let's try with max_depth = 15","3da3eeb9":"just looking quick at the training data, looks like they are all elready encoded in numbers so there is no need to do categorical conversion to numbers using label encoding or one hot encoding!","d5631e17":"so we got 0.43, low, high ..i am not sure ? but what i know the lower the score the better it is the model since this is computing error not accuracy.","7dc12737":"Turns out to be worst than the first model! Why is that? I think the n_estimators is too low! Let's try a bigger n_estimators!","773af9f3":"woow, this third model looks good! Let's try again, this time with XGBClassifier() but the same n_estimators as RandomForestClassifier()","1b158ec3":"just a preprocessing stuff: removing missing target which is Cover_Type, Separating the target answer on separate placeholder: y, and dropping the target answer from the training dataset","c71fbc34":"Now organising those predictions with ID and saving it out the submission file","3026cd5f":"So looks like the stacking model gives the best result! So, I am going to use the stacking model for prediction and submit the result for submission!","5f8ba9ed":"Calling the RandomForestClassifier algorithm since this is classification problem NOT a regression (number problem), and train the model using the training and target dataset","e3d4c167":"this is interesting, why is it not working with neural network? is it too small the dataset ... my guess is.","b4c9e63d":"from the above .describe() looks like there is no categorical variable that need to converted into number so we don't have to worry about it. Just to be sure we are checking it one more time:","47825d0f":"So after we trained the model, now we want to know how good is the model is. So how do we do that? We need a way to measure how good is the model is to 'unseen' data. For that we are calling mean_absolute_error to compute how 'good' our model is","ebb0d1c6":"let's try with max_depth = 5","a29d51dc":"we will try a stacking method this time!","0833b29e":"let's try with max_depth = 10","34971fd9":"OK, let's try max_depth = 25","9c89b7b9":"Computing the predictions using the test dataset","c94679fe":"So for the sake of fun we will try another metrics, accuracy ... so ideally the higher the better it is","75cb9f83":"wallaa, there is none of categorical variable!","a5bf2168":"importing the necessary libraries","7a8da5ed":"Calling the train_test_split algorithm for the training and validation purposes and putting the training and validation dataset on the proper placeholders","9bd844a2":"Trying for the second model with different parameters"}}