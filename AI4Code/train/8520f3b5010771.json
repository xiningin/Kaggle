{"cell_type":{"c7bf1fb2":"code","dc9bc549":"code","91a4ee27":"code","9a7c3553":"code","e874aa05":"code","8707d139":"code","8d586e2a":"code","46434247":"code","b84e6ae5":"code","e0b51b64":"code","68806986":"code","2cd4e3a0":"code","18552b7f":"code","d9ecb237":"code","abda83bb":"code","661a7d6f":"code","ebf2f534":"code","c041cd42":"code","854adb70":"code","ace07a49":"code","8f864b7c":"code","2b75f66e":"code","f27b9b0b":"code","86683fa4":"code","bdb23263":"code","19bd2a25":"code","e8853d2e":"code","bb510aaa":"code","034d72e9":"code","b25536da":"code","0fd3f229":"code","70c2f8e3":"code","1dd3d97b":"code","edb7c3ea":"code","8e2b6ab2":"code","0fea9014":"code","e2879199":"code","e66d6454":"code","25b39fc8":"code","7214e7c2":"code","76d3b70f":"code","3044e764":"code","af3d1f05":"code","208a8486":"markdown","3b97f168":"markdown","0148ef7b":"markdown","d0533b60":"markdown","8d415526":"markdown","76f7eddc":"markdown","747287f6":"markdown","4c848d2b":"markdown","96c9f88f":"markdown","42fbd8de":"markdown","2bbb4da2":"markdown","a0af4480":"markdown","c187859b":"markdown","5b8925ef":"markdown","fe5fe2a4":"markdown"},"source":{"c7bf1fb2":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))","dc9bc549":"data_train = pd.read_csv('..\/input\/train.csv')","91a4ee27":"data_train.head()","9a7c3553":"data_train.shape","e874aa05":"data_train.isnull().sum().any()","8707d139":"data_train['target'].value_counts()","8d586e2a":"X = data_train.drop(['ID_code', 'target'], axis=1)\ny = data_train['target'].values","46434247":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","b84e6ae5":"scaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\npca = PCA(0.99, random_state=1)\npca.fit(X_scaled)","e0b51b64":"pca.explained_variance_ratio_[:198].sum()","68806986":"pca.n_components_","2cd4e3a0":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, \n                                                    y, \n                                                    test_size=0.1, \n                                                    stratify=y, \n                                                    random_state=1)","18552b7f":"kfolds = StratifiedShuffleSplit(n_splits=5, random_state=1)","d9ecb237":"model_lr = LogisticRegressionCV(Cs=10, cv=kfolds, random_state=1, class_weight='balanced', scoring='roc_auc')","abda83bb":"model_lr.fit(X_train, y_train)","661a7d6f":"model_lr.score(X_test, y_test)","ebf2f534":"model_lr.C_","c041cd42":"from tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\n\n#model_rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=1)\nmodel_lr = LogisticRegression(C=0.0001, solver='lbfgs', random_state=1)\nmodel = model_lr\nscores = []\nscores_xtest = []\npreds_xtest = np.empty((X_test.shape[0], 5))\nfor i, (id_train, id_val) in tqdm(enumerate(kfolds.split(X_train, y_train))):\n    model.fit(X_train[id_train], y_train[id_train])\n    pred_i = model.predict_proba(X_train[id_val])[:, 1]\n    score_i = roc_auc_score(y_train[id_val], pred_i)\n    scores.append(score_i)\n    pred_xtest = model.predict_proba(X_test)[:, 1]\n    preds_xtest[:, i] = pred_xtest\n    score_xtest = roc_auc_score(y_test, pred_xtest)\n    scores_xtest.append(score_xtest)","854adb70":"print(scores)","ace07a49":"print(\"Mean validation auc: {:.4f} +\/- {:.4f}\".format(np.mean(scores), np.std(scores)))","8f864b7c":"print(\"Mean test auc: {:.6f} +\/- {:.6f}\".format(np.mean(scores_xtest), np.std(scores_xtest))) ","2b75f66e":"mean_preds_test = preds_xtest.mean(axis=1)","f27b9b0b":"roc_auc_score(y_test, mean_preds_test)","86683fa4":"model.fit(X_train, y_train)","bdb23263":"full_preds_test = model.predict_proba(X_test)[:, 1]\nroc_auc_score(y_test, full_preds_test)","19bd2a25":"cv_scores = cross_val_score(model_lr, X_train, y_train, scoring='roc_auc', cv=kfolds, n_jobs=-1)","e8853d2e":"cv_scores","bb510aaa":"print(\"Mean validation auc: {:.4f} +\/- {:.4f}\".format(np.mean(cv_scores), np.std(cv_scores)))","034d72e9":"from sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression(class_weight='balanced', solver='lbfgs', random_state=1)\nparam_grid = {'C': np.logspace(-4, 4, base=10, num=4)}\ngrid = GridSearchCV(model, cv=kfolds, param_grid=param_grid, n_jobs=-1, scoring='roc_auc')","b25536da":"grid.fit(X_train, y_train)","0fd3f229":"grid.best_params_","70c2f8e3":"grid.best_score_","1dd3d97b":"cv_results = list(zip(grid.cv_results_['mean_test_score'], \n                      grid.cv_results_['std_test_score']))\n\nfor i, param in enumerate(grid.cv_results_['params']):\n    print(\"C: {:.4f} => mean validation auc: {:.4f} +\/- {:.4f}\"\n          .format(param['C'], cv_results[i][0], cv_results[i][1]))","edb7c3ea":"grid.best_estimator_","8e2b6ab2":"grid.best_estimator_.score(X_test, y_test)","0fea9014":"grid.score(X_test, y_test)","e2879199":"(grid.best_estimator_.predict(X_test) != grid.predict(X_test)).sum()","e66d6454":"from sklearn.model_selection import RandomizedSearchCV\n\nmodel = LogisticRegression(class_weight='balanced', solver='lbfgs', random_state=1)\nparam_grid = {'C': np.logspace(-4, 4, base=10, num=100)}\ngrid_random = RandomizedSearchCV(model, cv=kfolds, n_iter=4, \n                                 param_distributions=param_grid, n_jobs=-1, \n                                 random_state=1, scoring='roc_auc')","25b39fc8":"grid_random.fit(X_train, y_train)","7214e7c2":"grid_random.best_params_","76d3b70f":"grid_random.best_score_","3044e764":"grid_random.cv_results_['params']","af3d1f05":"cv_results = list(zip(grid_random.cv_results_['mean_test_score'], \n                      grid_random.cv_results_['std_test_score']))\n\nfor i, param in enumerate(grid_random.cv_results_['params']):\n    print(\"C: {:.4f} => mean validation auc: {:.4f} +\/- {:.4f}\"\n          .format(param['C'], cv_results[i][0], cv_results[i][1]))","208a8486":"We need a lot of features to explain most of the variance, so pca looks like no very useful here:","3b97f168":"## Custom cross validation\nAt the same time that we create each model in the cross validation process we evaluate it on the validation data and create predictions for the testing set. We will ensamble those predictions and we'll evaluate them.","0148ef7b":"## GridSearchCV\nThis helps also to explore the hyperparameter space","d0533b60":"If we obtain the mean of the scores of all build models during the cross validation, on the testing data, we have:","8d415526":"The grid object can be used as an estimator because by default the refit param is True. So we can have predicions directly:","76f7eddc":"So, if we want to get the auc metric we can do:","747287f6":"Logistic Regression score method return the mean accuracy:","4c848d2b":"## Cross validation with cross_val_score","96c9f88f":"We are going to split our data into a training and testing sets:","42fbd8de":"Let's check if pca can help:","2bbb4da2":"Other alternative is to build a model using all the training data and apply it on the testing data (the result in this case is quite similar):","a0af4480":"## RandomSearchCV\nThis chooses randomly different n_iter groups of hyperparameters from the defined space and try their performance with cross validation","c187859b":"## Cross validation with LogisticRegressionCV\nThis method is specific for LogisticRegression, although other models have also their own \"CV\" version. This method let's explore differnt values of hyperparameters at the same time.","5b8925ef":"We have applied cross validation using different techniques:\n- **LogisticRegressionCV**\n- **Custom cross validation**\n- **cross_val_score**\n- **GridSearchCV**\n- **RandomSearchCV**\n\nIn all cases the evaluated model (Logistic Regression) has a not very big variance (the score is very similar in the different folds)","fe5fe2a4":"This is quite similar to what we obtain if we get a mean ensamble of all of the predictions on the testing data and we obtain the score:"}}