{"cell_type":{"2cc6b65a":"code","fe7b1268":"code","25783378":"code","2eb0a7a8":"code","8915b5ad":"code","918ba9bb":"code","3bc9b265":"code","849e66dc":"code","8ec58d29":"code","fb412d2c":"code","f119f676":"code","e9655b95":"code","2cc98a2f":"code","cefeb983":"code","f8d4278f":"code","b077ba71":"code","95d0721b":"markdown","78af4a5f":"markdown","8722b3cb":"markdown","e6a19d2e":"markdown","d7b83607":"markdown","cc717c81":"markdown","e8648e38":"markdown","ec09d4ef":"markdown","df63b9f3":"markdown","1c7c1027":"markdown","64d66431":"markdown","16764161":"markdown","6e23f273":"markdown","7267312f":"markdown","635bfa68":"markdown","6360004c":"markdown","96d7efc5":"markdown"},"source":{"2cc6b65a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nf = \"\"\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        f = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe7b1268":"data = pd.read_csv(f)\nprint(data)","25783378":"print(data.isnull().sum())","2eb0a7a8":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nX = data[['weight','age','height']]\nX = imp.fit_transform(X)\nprint(X)","8915b5ad":"from sklearn.preprocessing import LabelEncoder\n\ny = np.array(data[['size']]).ravel()\nle = LabelEncoder()\nle_y = le.fit_transform(y)\ny_classes = le.classes_\nprint(\"Encoded labels: \" + str(le_y))\nprint(\"Class labels: \" + str(le.classes_))","918ba9bb":"from tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import Callback\nfrom keras.optimizers import SGD\n\nX_train, X_test, y_train, y_test = train_test_split(X,le_y, test_size=0.33, random_state=42)\n\n#Prevent overfitting\nearly_stopping = keras.callbacks.EarlyStopping(\nmin_delta=0.001,\npatience=5,\nrestore_best_weights=True)\n\n#Build model\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[3]), #Inputs=weights,age,height\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.25),\n    \n    layers.Dense(64, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.25),\n    \n    layers.Dense(64, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.25),\n    \n    layers.Dense(len(y_classes), activation='softmax') #Multi-class classification\n])\n\n#Optimizer + Loss\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    # optimizer=SGD(lr=0.005),\n    metrics=['accuracy'],\n)\n\n#Train model\nhistory = model.fit(\nX_train, y_train,\nvalidation_data=(X_test,y_test),\nbatch_size=512,\nepochs=50,\ncallbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:,['loss', 'val_loss']].plot(title=\"Sparse Categorical Cross-entropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=('Accuracy'))\n\n","3bc9b265":"print(\"Max validation accuracy: \" + str(np.max(history_df.loc[:,['val_accuracy']])))\nprint(\"Min validation loss: \" + str(np.min(history_df.loc[:,['val_loss']])))","849e66dc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=0)\n\nprint(X_train)\nprint(y_train)\n\nprint(X_test)\nprint(y_test)","8ec58d29":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlr = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\ny_pred = lr.predict(X_test)\naccuracy_lr = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Logistic Regression: %.2f\" % accuracy_lr)","fb412d2c":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\naccuracy_nb = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Naive Bayes: %.2f\" % accuracy_nb)","f119f676":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss='modified_huber', shuffle=True, random_state=0)\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_test)\naccuracy_sgd = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Stochastic Gradient Descent: %.2f\" % accuracy_sgd)","e9655b95":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\naccuracy_knn = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Random Forest: %.2f\" % accuracy_knn)","2cc98a2f":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=5, random_state=0, max_features=None, min_samples_leaf=5)\ndtree.fit(X_train, y_train)\ny_pred = dtree.predict(X_test)\naccuracy_dt = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Decision Tree: %.2f\" % accuracy_dt)","cefeb983":"from sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators=50, oob_score=True, n_jobs=3, random_state=0, max_features=None, min_samples_leaf=15)\nrfm.fit(X_train, y_train)\ny_pred = rfm.predict(X_test)\naccuracy_rfm = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Random Forest: %.2f\" % accuracy_rfm)","f8d4278f":"from sklearn.svm import SVC\nsvm = SVC(kernel=\"linear\", C=1, random_state=0)\nsvm.fit(X_train,y_train)\ny_pred=svm.predict(X_test)\naccuracy_svm = accuracy_score(y_test, y_pred)\nprint(\"Accuracy for Support Vector Machine: %.2f\" % accuracy_svm)","b077ba71":"import matplotlib.pyplot as plt\n\nlabels = ['LR', 'NB', 'KNN', 'DT', 'RF', 'SVM']\naccuracies = [accuracy_lr, accuracy_nb, accuracy_knn, accuracy_dt, accuracy_rfm, accuracy_svm]\n\nx = [0,1,2,3,4,5]\nwidth=0.35\n\nfig, ax = plt.subplots()\nax.bar(x=labels,height=accuracies)\n\nax.set_ylabel('Accuracy')\nax.set_title('Classification Accuracy of ML models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\nplt.show()","95d0721b":"# Machine Learning Classification","78af4a5f":"6. Random Forest","8722b3cb":"2. Fill in the missing values via mean (missing values are numerical)","e6a19d2e":"4. K-Nearest Neighbour","d7b83607":"3. Stoachastic Gradient Descent","cc717c81":"# Evaluation","e8648e38":"7. Support Vector Machine","ec09d4ef":"1. Check null values","df63b9f3":"## Baseline\n\n1. 3 layers\n2. dropout=0.3\n3. relu\n4. adam\n5. batch norm\n6. 128 neurons\n7. batch_size=512,\n8. epochs=50,\n\n|Accuracy|Validation loss|\n|--------|---------------|\n|0.518285| 1.121909      |\n\n## Changed\n\n|Parameters|Accuracy|Validation loss|\n|----------|--------|---------------|\n| elu | 0.51502 | 1.136109 |\n| dropout=0.25| 0.518437 | 1.122018 |\n| dropout=0.5| 0.517627 | 1.122494|\n| 4 layers | 0.517652 | 1.122734|\n| 64,32 neurons | 0.519019 | 1.124153|\n| 32,16 neurons | 0.516311 | 1.127735|\n| 64 neurons| 0.518488 | 1.127987| \n| 128 neurons | 0.515906 | 1.128714 |\n| 64.64 | 0.516918 | 1.125243|\n| 64,32,16| 0.517172 | 1.126445|\n| 64,32 no batch norm | 0.518007 | 1.127214|","1c7c1027":"5. Decision Tree","64d66431":"0. Split into testing and training sets","16764161":"3. (Optional) Convert categorical class to numeric - label encoder","6e23f273":"# Data Cleaning","7267312f":"2. Naive Bayes","635bfa68":"# Deep Learning Classification","6360004c":"1. Logistic Regression","96d7efc5":"# Input the Data set"}}