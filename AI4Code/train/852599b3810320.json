{"cell_type":{"d60f6c6b":"code","425fa8e0":"code","bfcf7156":"code","bb733ef6":"code","eb947951":"code","e7c03602":"code","66de53f0":"code","9d371717":"code","009a337c":"code","fa1c792c":"code","684c73f4":"code","44ad3e25":"code","3d6b1949":"code","d7d8b945":"code","83ab69cf":"code","5b762bf7":"code","a952155f":"code","4cc0d5e0":"code","000e2f24":"code","acb2d066":"code","ae30b81c":"code","0a4a4497":"code","40182de5":"code","d2526f9e":"code","82437c0a":"code","b10e8833":"code","14b9f331":"markdown","fd295313":"markdown","be15ed67":"markdown","89b8cede":"markdown","435413be":"markdown","f3d8e8ab":"markdown","e44dd75e":"markdown","90b9bd8c":"markdown","a471f07c":"markdown","acacea20":"markdown","9d67c0a9":"markdown","bd3361a2":"markdown","d7faf250":"markdown","676c54d6":"markdown"},"source":{"d60f6c6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# keras\nimport keras\n\n# matplotlib\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","425fa8e0":"df = pd.read_csv('..\/input\/spam_or_not_spam.csv')\ndf.head()","bfcf7156":"df.label.value_counts()","bb733ef6":"df.info()","eb947951":"df.dropna(inplace=True)","e7c03602":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\ndef tokenizer_sequences(num_words, X):\n    \n    # when calling the texts_to_sequences method, only the top num_words are considered while\n    # the Tokenizer stores everything in the word_index during fit_on_texts\n    tokenizer = Tokenizer(num_words=num_words)\n    # From doc: By default, all punctuation is removed, turning the texts into space-separated sequences of words\n    tokenizer.fit_on_texts(X)\n    sequences = tokenizer.texts_to_sequences(X)\n    \n    return tokenizer, sequences","66de53f0":"max_words = 10000 \n# for the tokenizer, we configure it to only take into account the 1000 most common words when calling the texts_to_sequences method.\n\nmaxlen = 300\n# maxlen is the dimension that each email will have a fixed word sequence, in this case each email will be of a 1-d tensor (300,).","9d371717":"tokenizer, sequences = tokenizer_sequences(max_words, df.email.copy())","009a337c":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# # We will pad all input sequences to have the length of 300. Each email will be the same length of sequence.\nX = pad_sequences(sequences, maxlen=maxlen)\n\ny = df.label.copy()\n\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', y.shape)","fa1c792c":"max_words = len(tokenizer.word_index) + 1 # 33672 + 1\n# 0 is reserved for padding \/no data. The word indexes (i.e. tokenizer.word_index) are 1-offset.\n# max_words is the size of the vocabulary, you can think of a book is of max_words pages\n\nembedding_dim = 100 # the dimension of the word dictinory, i.e. this will be 100-dimensional word vector\n# you can think of a book that each page has embedding_dim words.","684c73f4":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Embedding\n\nmodel = Sequential()\n\n# embedding dictionary = 33673 * 100 = 3_367_300 parameters\n# we have a 33673 x 100 word vector, Embedding accepts 2D input and returns 3D output as shown in the summary\n# input_length = the length of input sequences (i.e. e-mails)\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen)) # 33673, 100, input_length=300 = (None, 300,100)\n# the activations have shape of (33673, 300, embedding_dim=100)\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","44ad3e25":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","3d6b1949":"X_train.shape, y_train.shape","d7d8b945":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.2)","83ab69cf":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","5b762bf7":"model.evaluate(X_test, y_test)\n# loss value & acc metrics","a952155f":"model.save_weights('pre_trained_model_100D.h5')","4cc0d5e0":"# just curiosity, let's explore the shape of the trained word embedding\nmodel.layers[0].get_weights()[0].shape\n# (vocabulary len x dimension of word vector) = word vector!","000e2f24":"model2 = Sequential()\n\nmodel2.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel2.add(Flatten())\nmodel2.add(Dense(32, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.summary()","acb2d066":"model2.layers[0].set_weights(model.layers[0].get_weights()) # load\nmodel2.layers[0].trainable = False # freeze","ae30b81c":"model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory2 = model2.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2)","0a4a4497":"from sklearn.metrics import precision_score, recall_score\n\ny_test_pred = np.where(model2.predict(X_test) > .5, 1, 0).reshape(1, -1)[0]","40182de5":"print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_test_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred)))","d2526f9e":"acc = history2.history['acc']\nval_acc = history2.history['val_acc']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","82437c0a":"# predict the test score\nmodel2.evaluate(X_test, y_test)\n# loss value & acc metrics","b10e8833":"model2.save_weights('pre_trained_model2100D_dense.h5')","14b9f331":"The model2 assumes that the pre-trained vocabulary is of shape (33673, 100), otherwise we receive the `not compatible with provided weight shape` error.\nSince we load the embedding from outside, and do not want it to be trained, we freeze the first layer as follows: ","fd295313":"Let's now create a new model and add the pre-trained word vector is of shape (33673, 100). ","be15ed67":"You can now save the model2 along with the fully connected layers and word vector.\n","89b8cede":"# Neural Network Word Embedding Using Keras\n\nIn this kernel, I have tried to discover how to train our own word embedding and use it with another architecture. To make this possible, Keras  offers an Embedding layer that can be used to train our own word embedding on text data.\n\nNote that, this kernel is just an introductory purpose and allows you to get some ideas about how this could work.","435413be":"Thank you for examining the kernel :)","f3d8e8ab":"Remove one row where the email is null","e44dd75e":"You should see the architecture, it uses 4,327,365parameters, of which 3,367,300 (the word embeddings) are non-trainable, and the remaining are 960,032 + 33. Because our vocabulary size has 33,673 words (with valid indices from 0 to 33,673) there are 33,673*100 = 3,367,300 non-trainable parameters.","90b9bd8c":"References :\n    \n1. https:\/\/www.tensorflow.org\/tutorials\/keras\/basic_text_classification\n2. https:\/\/spamassassin.apache.org\/old\/publiccorpus\/\n3. https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n4. https:\/\/www.tensorflow.org\/tutorials\/sequences\/text_generation\n5. https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n6. http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","a471f07c":"## Get the Data","acacea20":"We can now save the trained word vector and use it later","9d67c0a9":"We use a Tokenizer class of Keras to convert email text to sequences consistently:","bd3361a2":"## Train a Model Using a Pre-Trained Word Vector","d7faf250":"## The Keras Embedding Layer[](http:\/\/)\n\nIn the first example we use the embedding layer to train the word embedding alone so that we can save and use it in another model later.","676c54d6":"The number of spam and non-spam labels"}}