{"cell_type":{"215c64ec":"code","77f4a2ed":"code","da131653":"code","4ca6c601":"code","28230591":"code","71142771":"code","f065a416":"code","258e1b6c":"code","a6b276d9":"code","ef9a5fa7":"code","5be73595":"code","5890056d":"code","72d189d8":"code","0bd1cf99":"code","9be6eeba":"code","e392ba5e":"code","d349f5dc":"code","6752132a":"markdown","1b09bd12":"markdown","c3f0362f":"markdown","ce06c893":"markdown","6e8f4aa4":"markdown","ce5f870e":"markdown","69f3f727":"markdown","51bbb71f":"markdown","f4e2a49c":"markdown","7672696a":"markdown","928adf50":"markdown","2aaa2bea":"markdown","a77d9311":"markdown","84063854":"markdown"},"source":{"215c64ec":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom matplotlib import pyplot as plt\n\nfrom tqdm import tqdm\nfrom PIL import Image as Img\nfrom keras import Input\nfrom keras.layers import Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n","77f4a2ed":"#\nPIC_DIR = f'..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/'\n\nIMAGES_COUNT = 10000\n\nORIG_WIDTH = 178\nORIG_HEIGHT = 208\ndiff = (ORIG_HEIGHT - ORIG_WIDTH) \/\/ 2\n\nWIDTH = 128\nHEIGHT = 128\n\ncrop_rect = (0, diff, ORIG_WIDTH, ORIG_HEIGHT - diff)\n\nimages = []\nfor pic_file in tqdm(os.listdir(PIC_DIR)[:IMAGES_COUNT]):\n    pic = Image.open(PIC_DIR + pic_file).crop(crop_rect)\n    pic.thumbnail((WIDTH, HEIGHT), Image.ANTIALIAS)\n    images.append(np.uint8(pic))","da131653":"#Image shape\nimages = np.array(images) \/ 255\nprint(images.shape)","4ca6c601":"#Display first 25 images\nplt.figure(1, figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(images[i])\n    plt.axis('off')\nplt.show()","28230591":"LATENT_DIM = 32\nCHANNELS = 3\n\ndef create_generator():\n    gen_input = Input(shape=(LATENT_DIM, ))\n\n    x = Dense(128 * 16 * 16)(gen_input)\n    x = LeakyReLU()(x)\n    x = Reshape((16, 16, 128))(x)\n\n    x = Conv2D(256, 5, padding='same')(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(512, 5, padding='same')(x)\n    x = LeakyReLU()(x)\n    x = Conv2D(512, 5, padding='same')(x)\n    x = LeakyReLU()(x)\n    x = Conv2D(CHANNELS, 7, activation='tanh', padding='same')(x)\n\n    generator = Model(gen_input, x)\n    return generator","71142771":"def create_discriminator():\n    disc_input = Input(shape=(HEIGHT, WIDTH, CHANNELS))\n\n    x = Conv2D(256, 3)(disc_input)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(256, 4, strides=2)(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(256, 4, strides=2)(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(256, 4, strides=2)(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(256, 4, strides=2)(x)\n    x = LeakyReLU()(x)\n\n    x = Flatten()(x)\n    x = Dropout(0.4)(x)\n\n    x = Dense(1, activation='sigmoid')(x)\n    discriminator = Model(disc_input, x)\n\n    optimizer = RMSprop(\n        lr=.0001,\n        clipvalue=1.0,\n        decay=1e-8\n    )\n\n    discriminator.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy'\n    )\n\n    return discriminator","f065a416":"from IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot","258e1b6c":"generator = create_generator()\ngenerator.summary()","a6b276d9":"Image(model_to_dot(generator, show_shapes=True).create_png())","ef9a5fa7":"discriminator = create_discriminator()\ndiscriminator.trainable = False\ndiscriminator.summary()","5be73595":"Image(model_to_dot(discriminator, show_shapes=True).create_png())","5890056d":"gan_input = Input(shape=(LATENT_DIM, ))\ngan_output = discriminator(generator(gan_input))\ngan = Model(gan_input, gan_output)","72d189d8":"optimizer = RMSprop(lr=.0001, clipvalue=1.0, decay=1e-8)\ngan.compile(optimizer=optimizer, loss='binary_crossentropy')","0bd1cf99":"gan.summary()","9be6eeba":"import time\niters = 15000\nbatch_size = 16\n\nRES_DIR = 'res2'\nFILE_PATH = '%s\/generated_%d.png'\nif not os.path.isdir(RES_DIR):\n    os.mkdir(RES_DIR)\n\nCONTROL_SIZE_SQRT = 6\ncontrol_vectors = np.random.normal(size=(CONTROL_SIZE_SQRT**2, LATENT_DIM)) \/ 2\n\nstart = 0\nd_losses = []\na_losses = []\nimages_saved = 0\nfor step in range(iters):\n    start_time = time.time()\n    latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n    generated = generator.predict(latent_vectors)\n\n    real = images[start:start + batch_size]\n    combined_images = np.concatenate([generated, real])\n\n    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n    labels += .05 * np.random.random(labels.shape)\n\n    d_loss = discriminator.train_on_batch(combined_images, labels)\n    d_losses.append(d_loss)\n\n    latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n    misleading_targets = np.zeros((batch_size, 1))\n\n    a_loss = gan.train_on_batch(latent_vectors, misleading_targets)\n    a_losses.append(a_loss)\n\n    start += batch_size\n    if start > images.shape[0] - batch_size:\n        start = 0\n\n    if step % 50 == 49:\n        gan.save_weights('\/gan.h5')\n\n        print('%d\/%d: d_loss: %.4f,  a_loss: %.4f.  (%.1f sec)' % (step + 1, iters, d_loss, a_loss, time.time() - start_time))\n\n        control_image = np.zeros((WIDTH * CONTROL_SIZE_SQRT, HEIGHT * CONTROL_SIZE_SQRT, CHANNELS))\n        control_generated = generator.predict(control_vectors)\n        \n        for i in range(CONTROL_SIZE_SQRT ** 2):\n            x_off = i % CONTROL_SIZE_SQRT\n            y_off = i \/\/ CONTROL_SIZE_SQRT\n            control_image[x_off * WIDTH:(x_off + 1) * WIDTH, y_off * HEIGHT:(y_off + 1) * HEIGHT, :] = control_generated[i, :, :, :]\n        im = Img.fromarray(np.uint8(control_image * 255))#.save(StringIO(), 'jpeg')\n        im.save(FILE_PATH % (RES_DIR, images_saved))\n        images_saved += 1\n","e392ba5e":"plt.figure(1, figsize=(12, 8))\nplt.subplot(121)\nplt.plot(d_losses, color='red')\nplt.xlabel('epochs')\nplt.ylabel('discriminant losses')\nplt.subplot(122)\nplt.plot(a_losses)\nplt.xlabel('epochs')\nplt.ylabel('adversary losses')\nplt.show()","d349f5dc":"import imageio\nimport shutilimages_to_gif = []\nfor filename in os.listdir(RES_DIR):\n    images_to_gif.append(imageio.imread(RES_DIR + '\/' + filename))\nimageio.mimsave('trainnig_visual.gif', images_to_gif)\nshutil.rmtree(RES_DIR)","6752132a":"https:\/\/giphy.com\/gifs\/QuDO8a4UBfyZrLP331?utm_source=iframe&utm_medium=embed&utm_campaign=Embeds&utm_term=https%3A%2F%2Fwww.kdnuggets.com%2F","1b09bd12":"### How does GANs work?\n\n**GANs** learn a probability distribution of a dataset by pitting two neural networks against each other.\n\n\nOne neural network, called the **Generator**, generates new data instances, while the other, the **Discriminator**, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data that it reviews belongs to the actual training dataset or not.\n\n\nMeanwhile, the generator is creating new, synthetic\/fake images that it passes to the discriminator. It does so in the hopes that they, too, will be deemed authentic, even though they are fake. The fake image is generated from a 100-dimensional noise (uniform distribution between -1.0 to 1.0) using the inverse of convolution, called transposed convolution.\n\n\nThe goal of the generator is to generate passable images: to lie without being caught. The goal of the discriminator is to identify images coming from the generator as fake.\n\n\nHere are the steps a GAN takes:\n\n1. The generator takes in random numbers and returns an image.\n\n2. This generated image is fed into the discriminator alongside a stream of images taken from the actual, ground-truth dataset.\n\n3. The discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\n\nSo you have a double feedback loop:\n\n1. The discriminator is in a feedback loop with the ground truth of the images, which we know.\n\n2. The generator is in a feedback loop with the discriminator.","c3f0362f":"### Training the GAN model:\n\n\nTraining is the hardest part and since a GAN contains two separately trained networks, its training algorithm must address two complications:\n\nGANs must juggle two different kinds of training (generator and discriminator).\n\nGAN convergence is hard to identify.\n\nAs the generator improves with training, the discriminator performance gets worse because the discriminator can\u2019t easily tell the difference between real and fake. If the generator succeeds perfectly, then the discriminator has a 50% accuracy. In effect, the discriminator flips a coin to make its prediction.\n\n\nThis progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its quality may collapse.","ce06c893":"### Load the data and resize the iamges","6e8f4aa4":"### Define a GAN Model\n\nNext, a GAN model can be defined that combines both the generator model and the discriminator model into one larger model. This larger model will be used to train the model weights in the generator, using the output and error calculated by the discriminator model. The discriminator model is trained separately, and as such, the model weights are marked as not trainable in this larger GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the discriminator weights only affects when training the combined GAN model, not when training the discriminator standalone.\n\nThis larger GAN model takes as input a point in the latent space, uses the generator model to generate an image, which is fed as input to the discriminator model, then output or classified as real or fake.\n\nSince the output of the Discriminator is sigmoid, we use binary cross-entropy for the loss. RMSProp as an optimizer generates more realistic fake images compared to Adam for this case. The learning rate is 0.0001. Weight decay and clip value stabilize learning during the latter part of the training. You have to adjust the decay if you want to adjust the learning rate.\n\nGANs try to replicate a probability distribution. Therefore, we should use loss functions that reflect the distance between the distribution of the data generated by the GAN and the distribution of the real data.","ce5f870e":"![3eee0b_462bf4d9aa534e1ebdeca7c9d8739a0e_mv2.webp](attachment:3eee0b_462bf4d9aa534e1ebdeca7c9d8739a0e_mv2.webp)","69f3f727":"### The next step is to create a Generator\n\nThe generator goes the other way: It is the artist who is trying to fool the discriminator. This network consists of 8 convolutional layers. Here first, we take our input, called gen_input and feed it into our first convolutional layer. Each convolutional layer performs a convolution and then performs batch normalization and a leaky ReLu as well. Then, we return the tanh activation function.","51bbb71f":"### Let us also make the GIF of the output images that have been generated.","f4e2a49c":"You can read the full blog on [www.theaidream.com](https:\/\/www.theaidream.com\/post\/generate-realistic-human-face-using-gan-1)","7672696a":"### Display first 25 images","928adf50":"### Why were GANs developed in the first place?\n\nIt has been noticed most of the mainstream neural nets can be easily fooled into misclassifying things by adding only a small amount of noise into the original data. Surprisingly, the model after adding noise has higher confidence in the wrong prediction than when it predicted correctly. The reason for such an adversary is that most machine learning models learn from a limited amount of data, which is a huge drawback, as it is prone to overfitting. Also, the mapping between the input and the output is almost linear. Although it may seem that the boundaries of separation between the various classes are linear, in reality, they are composed of linearities and even a small change in a point in the feature space might lead to misclassification of data.","2aaa2bea":"### Next, create a Discriminator\n\nThe discriminator network consists of convolutional layers the same as the generator. For every layer of the network, we are going to perform a convolution, then we are going to perform batch normalization to make the network faster and more accurate and finally, we are going to perform a Leaky ReLu.","a77d9311":"# Generate Realistic Human Face using GAN","84063854":"Check the output here."}}