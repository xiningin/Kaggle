{"cell_type":{"e16bd7f7":"code","d171ad52":"code","1797f95e":"code","553dd54d":"code","ed8ffbd0":"code","3f232631":"code","27e7b050":"code","2dcec068":"code","0d54ba58":"code","41744378":"code","80c4e6e2":"code","ae2d0028":"code","e3aed1e2":"code","b2f1405d":"code","9b1eca9c":"code","f297673d":"code","0f8dde62":"code","6f53660c":"markdown","d5a63b6a":"markdown","c1b6d8f1":"markdown","70063329":"markdown","31a1ecea":"markdown","9a58db57":"markdown","24946a76":"markdown","5524ec8c":"markdown","5394a26e":"markdown","5c43e7cb":"markdown","359e5615":"markdown","8efa5773":"markdown"},"source":{"e16bd7f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d171ad52":"train_path='\/kaggle\/input\/titanic\/train.csv'\ntest_path='\/kaggle\/input\/titanic\/test.csv'\ntrain_data=pd.read_csv(train_path)\ntest_data=pd.read_csv(test_path)\ntrain_data.info()\ntest_data.info()","1797f95e":"train = train_data.copy()\ntest = test_data.copy()\ntrain.dropna(subset = [\"Embarked\"], inplace=True)\ntrain.describe()","553dd54d":"y_train=train.Survived\nfeatures=['Pclass','Sex','Age','Fare','SibSp','Parch','Embarked']\nX_train=train[features]\nX_test=test[features]\n\nX_train.columns","ed8ffbd0":"plt.figure(figsize=(6,6))\nsns.barplot(x=X_train.Sex, y=y_train)","3f232631":"plt.figure(figsize=(6,6))\nsns.barplot(x=X_train.Embarked, y=y_train)","27e7b050":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n\n#Unique string present in Cols \n\nprint(\"Unique values in 'Sex' column in training data:\", X_train['Sex'].unique())\nprint(\"Unique values in 'Embarked' column in training data:\", X_train['Embarked'].unique())\n\nprint(\"\\nUnique values in 'Sex' column in validation data:\", X_test['Sex'].unique())\nprint(\"Unique values in 'Embarked' column in validation data:\", X_test['Embarked'].unique())","2dcec068":"from sklearn.preprocessing import OneHotEncoder\n\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_val = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\n\nOH_cols_train.index = X_train.index\nOH_cols_val.index = X_test.index\n\nnum_X_train =X_train.drop(object_cols, axis=1)\nnum_X_val = X_test.drop(object_cols, axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_val, OH_cols_val], axis=1)\n        \n \nOH_X_train.describe()","0d54ba58":"#Missing data in train data\nmissing_val_count_by_column = (OH_X_train.isnull().sum())\nprint(\"Missing data in train data:\")\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n#Missing data in test data\nmissing_val_count_by_column = (OH_X_test.isnull().sum())\nprint(\"Missing data in test data:\")\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","41744378":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(OH_X_train))\nimputed_X_test = pd.DataFrame(my_imputer.transform(OH_X_test))\n\nimputed_X_train.columns = OH_X_train.columns\nimputed_X_test.columns = OH_X_test.columns\n\nimputed_X_train.head","80c4e6e2":"#Missing data in train data after using SimpleImputer\nnotavailable_val_count_by_column = (imputed_X_train.isnull().sum())\nprint(\"Missing data after imputed in train data:\")\nprint(missing_val_count_by_column[notavailable_val_count_by_column > 0])\n\n#Missing data in test data after using SimpleImputer\nnotavailable_val_count_by_column = (imputed_X_test.isnull().sum())\nprint(\"Missing data after imputed in test data:\")\nprint(missing_val_count_by_column[notavailable_val_count_by_column > 0])","ae2d0028":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\ntrain_imputed_X_train, val_imputed_X_train, train_y_train, val_y_train = train_test_split(imputed_X_train, y_train, random_state=1)\n\ndef get_mae(max_leaf_nodes, train_imputed_X_train, val_imputed_X_train, train_y_train, val_y_train):\n    \n    train_model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n    \n    train_model.fit(train_imputed_X_train,train_y_train)\n    \n    val_predictions=train_model.predict(val_imputed_X_train)\n    val_mae = mean_absolute_error(val_predictions, val_y_train)    \n    return val_mae","e3aed1e2":"candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes,train_imputed_X_train, val_imputed_X_train, train_y_train, val_y_train)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n\nscores = {leaf_size: get_mae(leaf_size,train_imputed_X_train, val_imputed_X_train, train_y_train, val_y_train) for leaf_size in candidate_max_leaf_nodes}\nprint(\"\\n\",scores)\n\nbest_tree_size = min(scores, key=scores.get)\n\nprint(\"\\n\", best_tree_size, scores[best_tree_size])","b2f1405d":"train_model = RandomForestRegressor(max_leaf_nodes=best_tree_size,random_state=0)\ntrain_model.fit(train_imputed_X_train,train_y_train)\nval_predictions=train_model.predict(val_imputed_X_train)\nval_mae = mean_absolute_error(val_predictions, val_y_train)\nprint(val_mae)","9b1eca9c":"predict=train_model.predict(imputed_X_test)\nprint(predict)","f297673d":"predict[predict > 0.5] = int(1)\npredict[predict < 0.5] = 0\nprint(type(predict))\npredict = predict.astype(int)\nprint(predict)","0f8dde62":"output = pd.DataFrame({'PassengerId': test_data.PassengerId,'Survived': predict})\nprint(output)\n\noutput.to_csv('submission.csv', index=False)\nprint(\"Your Output has been saved\")\n","6f53660c":"# **COMPLETING THE MISSING DATA**\n* Identify the missing data\n* Imputation fills in the missing values with mean value","d5a63b6a":"Selecting the best tree size\n* Write loop to find the ideal tree size from candidate_max_leaf_nodes\n* Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)","c1b6d8f1":"# **CONVERTING THE DATA**","70063329":"Selecting Features","31a1ecea":"# **CREATING MODEL**\n* Specifying the Model\n* Fiting Model\n* Make validation predictions and calculate mean absolute error\n","9a58db57":"Creating model using Best tree size","24946a76":"Removing data with nan in Embarked","5524ec8c":"# **LOAD TRAIN AND TEST DATA**","5394a26e":"Converting the prediction value into categories(i.e 1 or 0)","5c43e7cb":"Exporting predicted values to csv file ","359e5615":"Apply one-hot encoder to each column with categorical data","8efa5773":"Making the prediction"}}