{"cell_type":{"aab3a97e":"code","d14cfce2":"code","8243b812":"code","46c52797":"code","1ff60f23":"code","2ba6178f":"code","310e286a":"code","e3010970":"code","c95816e9":"code","401f5510":"code","e970622a":"code","49bd6cab":"code","6e3fcf6b":"code","dc619c8d":"code","5fa82d81":"code","7ea47ceb":"code","bdf51d21":"code","f49f0247":"code","fa247773":"code","8152070e":"code","97cd555c":"code","c8eb3e35":"code","5b73cd0b":"code","8d980a2a":"code","99c8ac08":"code","2e9bf35c":"code","6a84c242":"code","780d5d23":"code","456c83c5":"code","90229869":"code","9faaee75":"code","f59bb751":"code","ed75ecbf":"code","e3f314d0":"code","d975645a":"code","527917ed":"code","5aa2dc8b":"code","1fb0ae04":"code","b0da71cd":"code","6f3bfce9":"code","e48bc20a":"code","e8f14c67":"code","2106c643":"code","216d7098":"code","b55ec07f":"code","1cc1ab2a":"code","77692b10":"code","9d4e61d3":"code","a48d1d16":"code","4acc4ebe":"code","d8a1897c":"code","11dab39e":"code","28f6b326":"markdown","643a3644":"markdown"},"source":{"aab3a97e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d14cfce2":"import pandas as pd\n#from catboost import CatBoostClassifier\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import Imputer","8243b812":"#import data\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","46c52797":"train_df=train\ntest_df=test\ntest_df.Fare.fillna(test_df.Fare.mean(), inplace=True)\ndata_df = train_df.append(test_df) # The entire data: train + test.\npassenger_id=test_df['PassengerId']\n\n## We will drop PassengerID and Ticket since it will be useless for our data. \ntrain_df.drop(['PassengerId'], axis=1, inplace=True)\ntest_df.drop(['PassengerId'], axis=1, inplace=True)\ntest_df.shape","1ff60f23":"print (train_df.isnull().sum())\nprint (''.center(20, \"*\"))\nprint (test_df.isnull().sum())\nsns.boxplot(x='Survived',y='Fare',data=train_df)","2ba6178f":"train_df=train_df[train_df['Fare']<400]","310e286a":"train_df['Sex'] = train_df.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest_df['Sex'] = test_df.Sex.apply(lambda x: 0 if x == \"female\" else 1)","e3010970":"train_df.head()","c95816e9":"pd.options.display.max_columns = 99\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)\ntrain_df.head()","401f5510":"for name_string in data_df['Name']:\n    data_df['Title']=data_df['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n    \nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata_df.replace({'Title': mapping}, inplace=True)\n\ndata_df['Title'].value_counts()\ntrain_df['Title']=data_df['Title'][:891]\ntest_df['Title']=data_df['Title'][891:]\n\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\nfor title in titles:\n    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n    #print(age_to_impute)\n    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\ndata_df.isnull().sum()\n\n\n\ntrain_df['Age']=data_df['Age'][:891]\ntest_df['Age']=data_df['Age'][891:]\ntest_df.isnull().sum()","e970622a":"## Family_size seems like a good feature to create\ntrain_df['family_size'] = train_df.SibSp + train_df.Parch+1\ntest_df['family_size'] = test_df.SibSp + test_df.Parch+1","49bd6cab":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain_df['family_group'] = train_df['family_size'].map(family_group)\ntest_df['family_group'] = test_df['family_size'].map(family_group)","6e3fcf6b":"train_df['child'] = [1 if i<16 else 0 for i in train_df.Age]\ntest_df['child'] = [1 if i<16 else 0 for i in test_df.Age]\ntrain_df.child.value_counts()","dc619c8d":"train_df['calculated_fare'] = train_df.Fare\/train_df.family_size\ntest_df['calculated_fare'] = test_df.Fare\/test_df.family_size","5fa82d81":"def fare_group(fare):\n    a= ''\n    if fare <= 5:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a","7ea47ceb":"train_df['fare_group'] = train_df['calculated_fare'].map(fare_group)\ntest_df['fare_group'] = test_df['calculated_fare'].map(fare_group)","bdf51d21":"#comment after use\ntrain_df = pd.get_dummies(train_df, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntrain_df.drop(['Cabin', 'family_size','Ticket','Name', 'Fare'], axis=1, inplace=True)\ntest_df.drop(['Ticket','Name','family_size',\"Fare\",'Cabin'], axis=1, inplace=True)","f49f0247":"def age_group_fun(age):\n    a = ''\n    if age <= 2:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 15:\n        a = 'child'\n    elif age <= 19:\n        a = 'teenager'\n    elif age <= 32:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a","fa247773":"train_df['age_group'] = train_df['Age'].map(age_group_fun)\ntest_df['age_group'] = test_df['Age'].map(age_group_fun)","8152070e":"train_df = pd.get_dummies(train_df,columns=['age_group'], drop_first=True)\ntest_df = pd.get_dummies(test_df,columns=['age_group'], drop_first=True)\n#Lets try all after dropping few of the column.\ntrain_df.drop(['Age','calculated_fare'],axis=1,inplace=True)\ntest_df.drop(['Age','calculated_fare'],axis=1,inplace=True)","97cd555c":"train_df.head()\ntrain_df.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)\ntest_df.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)","c8eb3e35":"X = train_df.drop('Survived', 1)\ny = train_df['Survived']\n#testing = test_df.copy()\n#testing.shape","5b73cd0b":"from catboost import CatBoostClassifier","8d980a2a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    svm.SVC(probability=True),\n    DecisionTreeClassifier(),\n    CatBoostClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n    \n\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog= pd.DataFrame(columns=log_cols)","99c8ac08":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n\nSSplit=StratifiedShuffleSplit(test_size=0.2,random_state=7)\nacc_dict = {}\n\nfor train_index,test_index in SSplit.split(X,y):\n    X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test=y.iloc[train_index],y.iloc[test_index]\n    \n    for clf in classifiers:\n        name = clf.__class__.__name__\n          \n        clf.fit(X_train,y_train)\n        predict=clf.predict(X_test)\n        acc=accuracy_score(y_test,predict)\n        if name in acc_dict:\n            acc_dict[name]+=acc\n        else:\n            acc_dict[name]=acc","2e9bf35c":"log['Classifier']=acc_dict.keys()\nlog['Accuracy']=acc_dict.values()\n#log.set_index([[0,1,2,3,4,5,6,7,8,9]])\n%matplotlib inline\nsns.set_color_codes(\"muted\")\nax=plt.subplots(figsize=(10,8))\nax=sns.barplot(y='Classifier',x='Accuracy',data=log,color='b')\nax.set_xlabel('Accuracy',fontsize=20)\nplt.ylabel('Classifier',fontsize=20)\nplt.grid(color='r', linestyle='-', linewidth=0.5)\nplt.title('Classifier Accuracy',fontsize=20)","6a84c242":"\n\n## Necessary modules for creating models. \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix","780d5d23":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\ntestframe = std_scaler.fit_transform(test_df)\ntestframe.shape","456c83c5":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=1000)","90229869":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score,recall_score,confusion_matrix\nlogreg = LogisticRegression(solver='liblinear', penalty='l1')\nlogreg.fit(X_train,y_train)\npredict=logreg.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","9faaee75":"C_vals = [0.0001, 0.001, 0.01, 0.1,0.13,0.2, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 4.0,4.5,5.0,5.1,5.5,6.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\n\nparam = {'penalty': penalties, 'C': C_vals, }\ngrid = GridSearchCV(logreg, param,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=10,shuffle=True), n_jobs=1,scoring='accuracy')","f59bb751":"grid.fit(X_train,y_train)\nprint (grid.best_params_)\nprint (grid.best_score_)\nprint(grid.best_estimator_)","ed75ecbf":"#grid.best_estimator_.fit(X_train,y_train)\n#predict=grid.best_estimator_.predict(X_test)\n#print(accuracy_score(y_test,predict))\nlogreg_grid=LogisticRegression(C=0.33, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l1',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n#logreg_grid = LogisticRegression(penalty=grid.best_params_['penalty'], C=grid.best_params_['C'])\nlogreg_grid.fit(X_train,y_train)\ny_pred = logreg_grid.predict(X_test)\nlogreg_accy = round(accuracy_score(y_test, y_pred), 3)\nprint (logreg_accy)\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))","e3f314d0":"AdaC=AdaBoostClassifier()\n\nAdaC.fit(X_train,y_train)\npredict=AdaC.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))","d975645a":"from sklearn.tree import DecisionTreeClassifier\nn_estimator=[50,60,100,150,200,300]\nlearning_rate=[0.001,0.01,0.1,0.2,0.3]\nhyperparam={'n_estimators':n_estimator,'learning_rate':learning_rate}\ngridBoost=GridSearchCV(AdaC,param_grid=hyperparam,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=15,shuffle=True), n_jobs=1,scoring='accuracy')","527917ed":"gridBoost.fit(X_train,y_train)\nprint(gridBoost.best_score_)\nprint(gridBoost.best_estimator_)","5aa2dc8b":"gridBoost.params = gridBoost.best_params_\ngridBoost.params","1fb0ae04":"gridBoost.best_estimator_.fit(X_train,y_train)\npredict_grid=gridBoost.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,predict))","b0da71cd":"xgb=XGBClassifier(max_depth=2, n_estimators=700, learning_rate=0.009,nthread=-1,subsample=1,colsample_bytree=0.8)\nxgb.fit(X_train,y_train)\npredict=xgb.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","6f3bfce9":"paramsxgb = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","e48bc20a":"folds = 3\nparam_comb = 5\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)","e8f14c67":"grid = GridSearchCV(estimator=xgb, param_grid=paramsxgb, scoring='recall', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3 )\ngrid.fit(X_train, y_train)","2106c643":"xgb = XGBClassifier(base_score=0.5, booster='gbtree',\n                                     colsample_bylevel=1, colsample_bynode=1,\n                                     colsample_bytree=1, gamma=0,\n                                     learning_rate=0.02, max_delta_step=0,\n                                     max_depth=3, min_child_weight=1,\n                                     missing=None, n_estimators=600, n_jobs=1,\n                                     nthread=1,\n                                     random_state=0, reg_alpha=0, reg_lambda=1,\n                                     scale_pos_weight=1, seed=None, silent=True,\n                                     subsample=1, verbosity=1)","216d7098":"xgb.fit(X_train,y_train)\npredict=xgb.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","b55ec07f":"lda=LinearDiscriminantAnalysis()\nlda.fit(X_train,y_train)\npredict=lda.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","1cc1ab2a":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100,max_depth=5,min_samples_split=20,max_features=0.2, min_samples_leaf=8,random_state=20)\n#randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nrandom_accy = round(accuracy_score(y_pred, y_test), 3)\nprint (random_accy)\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","77692b10":"from sklearn.ensemble import VotingClassifier","9d4e61d3":"# Prediction with catboost algorithm.\nfrom catboost import CatBoostClassifier\nmodel = CatBoostClassifier(verbose=False, one_hot_max_size=3)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nacc = round(accuracy_score(y_pred, y_test), 3)\nprint(acc)","a48d1d16":"y_predict=lda.predict(testframe)","4acc4ebe":"temp = pd.DataFrame(pd.DataFrame({\n        \"PassengerId\": passenger_id,\n        \"Survived\": y_predict\n    }))","d8a1897c":"temp.to_csv('submission_3.csv',index = False)","11dab39e":"temp.head()","28f6b326":"This kernel has covered 4 topics:\n\n- Basic Introduction\n- EDA\n- Feature Engineering\n- Model Building\n\n\n\nThe name of the competition is **Titanic**\nStart here! Predict survival on the Titanic and get familiar with ML basics\n\n\n\nThis is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n\n\nThe Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","643a3644":"### **Notebooks taken help from!**\n\nhttps:\/\/www.kaggle.com\/heyytanay\/commonlit-eda-understanding-the-competition\n\nhttps:\/\/www.kaggle.com\/tungmphung\/commonlit-readability-eda\/notebook#Target-and-sentence-length\n\n\n### **If there are any suggesion for the notebook please comment, that would be helpful. Also please upvote if you liked it! Thank you!!**\n\n### **Some of my other works:**\n\nhttps:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\/data\n\nhttps:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations \n\nhttps:\/\/www.kaggle.com\/udbhavpangotra\/what-do-people-use-youtube-for-in-great-britain\n\n\n\n### Do **drop a comment or follow me** for more content!!! "}}