{"cell_type":{"7c170bc3":"code","dd353c97":"code","494e0457":"code","7611faba":"code","7b379be9":"code","ec3d193b":"code","1b644568":"code","a41e7a84":"code","af25bb92":"code","2bdfa31e":"code","b3456e2b":"code","04c7f05b":"code","1e7cbed3":"code","6e78530b":"code","14d482e6":"code","7644ebf6":"code","c1b6c751":"code","ee3d2368":"code","2ebb1a08":"code","f835086e":"code","3f18bdac":"markdown","65ce5ae8":"markdown","00ddaa62":"markdown","1151d912":"markdown","91ef3288":"markdown","3f20b72b":"markdown","995f208b":"markdown","af4d7c1c":"markdown","36cc8197":"markdown","9cf856fa":"markdown","6759462f":"markdown","37f4dc8e":"markdown","d9fa520e":"markdown","10605531":"markdown"},"source":{"7c170bc3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, Binarizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet","dd353c97":"# Load data\nhouse = pd.read_csv('..\/input\/amsterdam-house-price-prediction\/HousingPrices-Amsterdam-August-2021.csv')\nhouse","494e0457":"# Check whether 'house' contains any Null or NaN\nhouse.isnull().sum()","7611faba":"# Fill missing value with median\nhouse.fillna(house.median(), inplace=True)\nhouse","7b379be9":"# Check Dtype of 'house'\nhouse.info()","ec3d193b":"# Extract road names\nhouse['Road'] = house['Address'].str.split(',', expand=True)[0]\nhouse['Road'] = house['Road'].str.split(' ')\nhouse['Road_Extract'] = pd.Series()\n\nfor i in range(0, len(house), 1):\n    lst = house.iloc[i, 8]\n    lst_extract = [j for j in lst if j.isalpha()]\n    lst_extract = ''.join(lst_extract)\n    house.iloc[i, 9] = lst_extract\n\nhouse","1b644568":"# Compare which columns has the least unique values.\ncolumns_names = ['Address', 'Zip', 'Road_Extract']\n\nfor name in columns_names:\n    print(\"Length of {0}: {1}\".format(name, len(house[name].unique())))","a41e7a84":"# Drop unnecessary columns\nhouse.drop(['Unnamed: 0', 'Address', 'Zip', 'Road'], axis=1, inplace=True)\nhouse.rename(columns={'Road_Extract':'Road'}, inplace=True)\nhouse.reset_index(drop=True, inplace=True)\nhouse = house[['Road', 'Area', 'Room', 'Lat', 'Lon', 'Price']]\nhouse","af25bb92":"# Check distribution of Area, Room and Price\nfig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n\nsns.distplot(house['Area'], ax=ax1)\nax1.set_title('Distribution of Area')\nsns.distplot(house['Room'], ax=ax2)\nax2.set_title('Distribution of Room')\nsns.distplot(house['Price'], ax=ax3)\nax3.set_title('Distribution of Price')\n\nplt.suptitle('Distribution of features', fontweight='bold')\nplt.tight_layout\nplt.show()","2bdfa31e":"# Apply Standard Scaling\narea_scaler = StandardScaler()\nroom_scaler = StandardScaler()\n\narea_n = area_scaler.fit_transform(house['Area'].values.reshape(-1, 1))\nroom_n = room_scaler.fit_transform(house['Room'].values.reshape(-1, 1))\n\nhouse.insert(3, 'Area_Scaled', area_n)\nhouse.insert(4, 'Room_Scaled', room_n)\n\nhouse","b3456e2b":"# Check distribution of Area_Scaled, Room_Scaled\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\nsns.distplot(house['Area_Scaled'], ax=ax1)\nax1.set_title('Distribution of Area_Scaled')\nsns.distplot(house['Room_Scaled'], ax=ax2)\nax2.set_title('Distribution of Room_Scaled')\n\nplt.suptitle('Distribution of features scaled', fontweight='bold')\nplt.tight_layout\nplt.show()","04c7f05b":"# Standard Scaling doesn't work\n# Maybe conerting into log1p can be another good idea.\n\narea_n = np.log1p(house['Area'])\nroom_n = np.log1p(house['Room'])\n\nhouse.insert(5, 'Area_Log', area_n)\nhouse.insert(6, 'Room_Log', room_n)\n\nhouse","1e7cbed3":"# Check distribution of Area_Log, Room_Log\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\nsns.distplot(house['Area_Log'], ax=ax1)\nax1.set_title('Distribution of Area_Log')\nsns.distplot(house['Room_Log'], ax=ax2)\nax2.set_title('Distribution of Room_Log')\n\nplt.suptitle('Distribution of features applied of log', fontweight='bold')\nplt.tight_layout\nplt.show()","6e78530b":"# Extract needed features for training\nhouse_train = house[['Road', 'Area_Log', 'Room_Log', 'Price', 'Lat', 'Lon']]\nhouse_train","14d482e6":"# Process One-Hot Encoding\nhouse_train = pd.get_dummies(house_train)\nhouse_train","7644ebf6":"# Set feature and label dataset as X, y\nX = house_train.drop('Price', axis=1, inplace=False)\ny = house_train['Price']\n\nprint('Shape of X: ', X.shape)\nprint('Shape of y: ', y.shape)","c1b6c751":"# Split X, y into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of X_test: ', X_test.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of y_test: ', y_test.shape)","ee3d2368":"# Logistic Regression is one of the most fundamental estimator.\n# So, let's try with Logistic Regression, first.\n\nlr_reg = LogisticRegression(solver='liblinear')\nlr_reg.fit(X_train, y_train)\nlr_preds = lr_reg.predict(X_test)\n\nlr_mse = mean_squared_error(y_test, lr_preds)\nlr_rmse = np.sqrt(lr_mse)\n\nprint('MSE : {0:.3f}, RMSE : {1:.3f}'.format(lr_mse, lr_mse))\nprint('Variance score : {0:.3f}'.format(r2_score(y_test, lr_preds)))","2ebb1a08":"ridge = Ridge(alpha=10)\nneg_mse_scores = cross_val_score(ridge, X, y, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)","f835086e":"print('Individual Negative MSE scores of cross validation 5 times: ', np.round(neg_mse_scores, 2))\nprint('Individual RMSE scores of cross validation 5 times: ', np.round(rmse_scores, 2))\nprint('Average of RMSE scores of cross validation 5 times : {0:.3f} '.format(avg_rmse))","3f18bdac":"# Report\n**Summary**  \n* Features of datasets are few for training\n* If I had more various features, I could have made better results.\n\n**Comment**  \n1. I tried to reduce the number of unique values of 'Address' (919 --> 759)\n- Length of Address: 919\n- Length of Zip: 834\n- Length of Road_Extract: 759\n\n2. I also tried to make features more scaled as much as Normal Distribution by comapring 3 methods.\n3. I made Ridge model to regularize features.  \nComapring with Logistic Regression, the RMSE score was decreased from 323252587891.892 to 376525.286.  \nStill, the evaulation score of my models are low. I need to improve them.","65ce5ae8":"# Data Preprocessing","00ddaa62":"# Import libraries and data","1151d912":"## Extract road names from address","91ef3288":"# Regression","3f20b72b":"### **Comment**\nPreprocessed column has the least length.  \nA variety sorts can cause overfitting on prediction.  \nTherefore, choosing for the least one can be efficient way.","995f208b":"## Check Distribution","af4d7c1c":"## Logistic Regression","36cc8197":"## Process Missing Values","9cf856fa":"# Create Datasets","6759462f":"### **Comment**\nWe plotted each distribution of original, Standard Scaled and Log Scaled.  \nAt the last plot, we could see better distribution which scaler was Log (similar to Normal Distribution).","37f4dc8e":"## Dataset Overview\n|  Column |              Comment              |\n|:-------:|:---------------------------------:|\n| Address | Residential address               |\n|   Zip   | Residential Zip code              |\n|  Price  | Residential price in Euros        |\n|   Area  | Residential area in square meters |\n|   Room  | Number of rooms at residence      |\n|   Lon   | Longitude coordinate              |\n|   Lat   | Latitude coordinates.             |","d9fa520e":"### **Comment**\nOverall, as you can see, most of features have biased values  \nSo, if we process Standard Scaling, we can have better results.  \n100 square meters, 3 rooms were the heighest in each feature: Area, Room","10605531":"## Ridge"}}