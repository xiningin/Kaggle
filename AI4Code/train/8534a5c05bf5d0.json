{"cell_type":{"f7443ad8":"code","e28f8885":"code","28b738ca":"code","59a41fe9":"code","f0dced4f":"code","cec2130c":"code","7aaedb40":"code","544d0c3d":"code","ef972bbd":"code","f6e035c1":"code","d1a48b28":"code","2de2a9ee":"code","6009bd76":"code","a32b8256":"code","6801b39c":"code","6a747f11":"code","24166d04":"code","f31385b1":"code","fc982201":"code","7a6296fc":"code","812a99d3":"markdown","b4c03cb7":"markdown","45f72710":"markdown","812b3b87":"markdown","0ccae7ba":"markdown","93bb4417":"markdown","6ccb99d2":"markdown","8c2412a8":"markdown","24ac3612":"markdown","690655d8":"markdown","cf3d4d2b":"markdown","df64968c":"markdown","00c153e9":"markdown","aec50afa":"markdown","effa768b":"markdown","96d8d374":"markdown","0873803f":"markdown","eb5766ab":"markdown","3a70d833":"markdown","78882d47":"markdown","3af55ef8":"markdown","0ef8ceeb":"markdown","062438cb":"markdown","5335aa2d":"markdown","525e0514":"markdown","56ab3700":"markdown"},"source":{"f7443ad8":"# import pandas as pd\n# import glob\n# import json","e28f8885":"# # reading a csv file as a DataFrame in python\n# data = pd.read_csv('datasets\/big_mart_sales.csv')\n\n# # SKIPROWS\n# # (If some initial lines of csv are comments)\n# data_with_comments = pd.read_csv('datasets\/big_mart_sales_top_row_error.csv', skiprows= 5)\n\n# # To read data having a specific delimiter\n# data_delimiter = pd.read_csv('datasets\/big_mart_sales_delimiter.csv',delimiter='\\t')\n\n# # To read first n rows\n# read_sample_from_data = pd.read_csv('datasets\/big_mart_sales.csv',nrows=100)\n\n# # To Read specific columns\n# read_specific_columns = pd.read_csv('datasets\/big_mart_sales.csv',usecols=['Item_Identifier','Item_Type','Item_MRP'])\n","28b738ca":"# # reading an excel sheet\n# data = pd.read_excel('datasets\/big_mart_sales.xlsx')\n\n# # read the data with multiple sheets\n# sheet_1985 = pd.read_excel('datasets\/big_mart_sales_with_multiple_sheets.xlsx',sheet_name='1985')\n\n# # read the data with skiprows\n# sheet_with_comments = pd.read_excel('datasets\/big_mart_sales_comments.xlsx',skiprows=3)","59a41fe9":"# # read the json file\n# data = pd.read_json('datasets\/simple.json')\n\n# # Some of the json files are written as records i.e each json object is written in a line.\n# # For example:\n# # { 'name' : 'Lakshay', 'roll_no' : '100' } # line 1\n# # { 'name' : 'Sanad' , 'roll_no' : '101' } # line 2\n\n# # read json files with records \n# data_with_records = pd.read_json('datasets\/simple_records.json',lines=True)\n\n# # JSON Module of Standard Library\n# # Most of the json files are nested and we cannot directly import them into a dataframe. \n# # We first need to clean and filter the json file in order to convert it into a dataframe.\n# # open and load the data in json file\n# with open('datasets\/nested.json') as f :\n#     my_json_data = json.load(f)\n# print(my_json_data)\n# # use pprint or (pretty print) to print the data in the structured format\n# from pprint import pprint\n# pprint(my_json_data)","f0dced4f":"# # To read files from multiple directories:\n# print(\"Folder names: \")\n# for directory in glob.glob('datasets\/multi-directory\/*'):\n#     print(directory)\n    \n# # iterate through each of the files\n# print(\"File names in each folder: \")\n# for directory in glob.glob('datasets\/multi-directory\/*'):\n#     for files in glob.glob(directory + '\/*'):\n#         print(files)\n        \n# #Concatenate the files\n# data_frame_list = []\n# for directory in glob.glob('datasets\/multi-directory\/*'):\n#     for files in glob.glob(directory + '\/*'):\n#         data_frame_list.append(pd.read_csv(files))\n        \n# # concatenate the dataframes\n# final_data = pd.concat(data_frame_list)","cec2130c":"# # Store the csv\n# final_data.to_csv('datasets\/merged_big_mart_sales.csv',index=False)\n\n# # Writing a json file\n# # put the filtered data into the new json file\n# with open('datasets\/filtered.json','w') as f:\n#     json.dump(filtered_data, f, indent=4)","7aaedb40":"# data = pd.read_csv('datasets\/big_mart_sales.csv')\n\n# # check for the null values\n# data.isna().sum()\n\n# # select rows with null values in the Item_Weight\n# data.loc[data.Item_Weight.isna() == True]\n\n# # drop the rows with atleast one NaN\n# data.dropna()\n\n# # fill the null values in Item_Weight by mean\n# data.loc[(data.Item_Weight.isna() == True) , 'Item_Weight'] = data.Item_Weight.mean() \n\n# # fill the null values in Outlet_Size by mode\n# # data.Outlet_Size.mode() gives mode\n# # data.Outlet_Size.value_counts() gives count of each\n# data.loc[(data.Outlet_Size.isna() == True) , 'Outlet_Size'] = 'Medium'\n# # alternative: data.Outlet_Size.fillna('Medium', inplace=True)","544d0c3d":"# # Create a new mapping (dictionary) \n# mapping = {\n#     'Low Fat' : 'LF',\n#     'Regular' : 'R',\n#     'LF' : 'LF',\n#     'reg': 'R',\n#     'low fat' : 'LF'\n# }\n# # use the  map function to update the values\n# data.Item_Fat_Content = data.Item_Fat_Content.map(mapping)","ef972bbd":"# def convert(price):\n#     price = price\/74\n#     price = price + 1.28\n#     return price\n# data['Item_MRP_in_USD_UPDATED'] = data.Item_MRP.apply(lambda x : convert(x))","f6e035c1":"\n# import random\n# data = pd.read_csv('datasets\/big_mart_sales.csv')\n\n# # index of the dataframe\n# data.index\n# # So, In this case, we have the numeric index value start from the 0 and ends at 8523\n\n# # Changing index to a random set of numbers\n# random_list = [random.randint(1, 8523) for i in range(8523)] \n# data.index = random_list\n# # reset the index\n# data.reset_index(inplace=True)","d1a48b28":"# sample_df = pd.DataFrame({\n#     'gender' : ['M', 'F', 'M', 'M', 'F'],\n#     'grade'  : ['A', 'A', 'B', 'B', 'A'],\n#     'marks'  : [ 22,  21,  12,  14,  20],\n#     'id'     : ['A101', 'A102', 'A103', 'A104', 'A105']\n# })\n# # sample_df.loc[2:4] # gives from row 2 to row 4 \n# # sample_df.iloc[2:4] # gives from row 2 to row 3\n\n# sample_df = sample_df.sort_values(by=['marks'])\n# # sample_df.loc[2:4] # gives from label 2 to label 4\n# # sample_df.iloc[2:4] # gives from 2nd position to 4th position\n\n# sample_df.set_index('id',inplace=True)\n# # sample_df.iloc[2:4] # gives from 2nd position to 4th position\n# # sample_df.loc[2:4] # gives error because label=2,3,4 not possible\n# # sample_df.loc['A104':'A102'] # gives from label A104 to index A102","2de2a9ee":"# data = pd.read_csv('datasets\/big_mart_sales.csv')\n\n# # view top 7 rows\n# data.head(7)\n# # view bottom 3 rows\n# data.tail(3)\n\n# # select the rows from range 10-15\n# data[10:15]\n\n# # select specific rows by index number\n# data.iloc[[1,5,2,4,6,14]]\n\n# # select specific rows and columns from the data using their position\n# # In the iloc function pass the first list as the order of rows by their index and pass the second list as the order of columns\n# data.iloc[[1,4,5,2],[1,3,5]]\n","6009bd76":"# # Set the Item_Identifier as the index of the dataframe.\n# # 1. change the index of the dataframe\n# # 2. drop=True is used to drop the column that's set as index\n# # 3. inplace=True is used to make changes in the original dataframe\n# data.set_index('Item_Identifier', drop=True, inplace=True)\n\n# # select rows with index value 'FDA15'\n# data.loc['FDA15']\n\n# # select rows with index value 'FDA15' or 'FDA03'\n# data.loc[['FDA15', 'FDA03']]\n\n# # select rows and columns\n# data.loc[['FDA15', 'FDA03'], ['Item_Fat_Content', 'Item_Type', 'Item_MRP']]","a32b8256":"# data = pd.read_csv('datasets\/big_mart_sales.csv')\n\n# # Filter rows based on condition\n# data.loc[data.Outlet_Establishment_Year == 1987]\n\n# # Multiple conditions\n# data.loc[(data.Outlet_Establishment_Year == 2009) & (data.Outlet_Size == 'Medium')]\n\n# #using isin\n# data.loc[data.Outlet_Establishment_Year.isin([1987, 1988, 1999])]\n\n# # alternative: data[data.Outlet_Establishment_Year == 1987]\n\n# # Select specific columns\n# select_columns = ['Item_Identifier', 'Item_MRP', 'Outlet_Establishment_Year', 'Outlet_Size']\n# data[10:15][select_columns]\n\n# #Combined\n# data.loc[(data.Outlet_Establishment_Year == 1987) & (data.Outlet_Size == 'High'), select_columns]\n# # alternative: data[(data.Outlet_Establishment_Year == 1987) & (data.Outlet_Size == 'High')][select_columns]\n\n# # Select the columns where data type = object\n# data.select_dtypes('object')\n\n# # df[] sometimes has unwanted behavior, hence as a good practice it is recommended to use df.loc[]","6801b39c":"# # sample dataframe\n# data_frame = pd.DataFrame({\n#     'roll_no': [ 102, 101, 104, 103, 105],\n#     'name' : ['Aravind', 'Rahul', 'Prateek', 'Piyuesh', 'Kartik'],\n#     'grade': ['B', 'B', 'A', 'C', 'A'],\n#     'marks': [ 15, 15, 20, 4, 22],\n#     'city' : ['Gurugram', 'Delhi', 'Delhi', 'Gurugram', 'Hyderabad']\n# })\n\n# # sort the dataframe by grades\n# data_frame.sort_values(by=['grade'])\n\n# # sort the dataframe by asc grades and desc marks\n# data_frame.sort_values(by=['grade','marks'],ascending=[True,False])\n\n# # save the sorted state by using inplace =True\n# data_frame.sort_values(by=['grade','marks'], ascending= [True, False], inplace=True)\n# # But now, index will be messy. Hence, we should reset index\n# data_frame.reset_index(inplace=True, drop=True)","6a747f11":"# # read the datasets\n# outlet_size_small = pd.read_csv('datasets\/outlet_size_small.csv')\n# outlet_size_medium = pd.read_csv('datasets\/outlet_size_medium.csv')\n# outlet_size_large = pd.read_csv('datasets\/outlet_size_high.csv')\n# #concatenate by adding rows (axis=0)\n# all_dataframes = [outlet_size_small, outlet_size_medium, outlet_size_large]\n# data = pd.concat(all_dataframes, axis=0)","24166d04":"# # Creating a student data frame\n# student_df = pd.DataFrame({\n#     'college': ['ZU UNIVERSITY', 'ZU UNIVERSITY', 'ZU UNIVERSITY', 'ZU UNIVERSITY', 'ZU UNIVERSITY'],\n#     'roll_no': [ 102, 101, 104, 103, 105],\n#     'name' : ['Aravind', 'Rahul', 'Prateek', 'Piyuesh', 'Kartik'],\n#     'grade': ['B', 'B', 'A', 'C', 'A'],\n#     'marks': [ 15, 15, 20, 4, 22],\n#     'city' : ['Gurugram', 'Delhi', 'Delhi', 'Gurugram', 'Hyderabad']\n# })\n# # mapping data frame for left join\n# city_state_mapping = pd.DataFrame({\n#     'city' :  ['Gurugram', 'Delhi', 'Hyderabad', 'Faridabad'],\n#     'state' : ['Haryana',  'Delhi', 'Telangana', 'Haryana']\n# })\n# # mapping data frame for right join\n# roll_no = pd.DataFrame({\n#     'roll_no' : [ 102, 103]\n# })\n# # mapping data frame for outer join\n# student_selection = pd.DataFrame({\n#     'roll_no' : [102, 105, 101],\n#     'company' : ['ABC', 'XYZ', 'ABC'],\n#     'package (lpa)' : [ 8, 14.5, 11 ]\n# })\n# # mapping data frame for inner join\n# pool = pd.DataFrame({\n#         'college': ['ZU UNIVERSITY', 'ZU UNIVERSITY', 'AB UNIVERSITY', 'ZU UNIVERSITY','AB UNIVERSITY'],\n#         'name' : ['Aravind', 'Rahul', 'Rahul', 'Prateek', 'Harsh'],\n#         'company' : ['ABC', 'XYZ', 'ABC', 'AEP', 'ABC'],\n#         'package (lpa)' : [ 8, 14.5, 11, 6, 6 ]\n# })\n\n\n# # Now, we want to add another column state to the student_df using the city_state_mapping. \n# # We can do this by doing a left join. We need to use the merge function and set the parameters how='left' and on='city\n# student_df.merge(city_state_mapping, how='left', on='city')\n\n# # Now, we have another dataframe that contains roll_no of some students\n# # We need to find out the other details of the students. We can do this by using right join. \n# # You just need to set how='right'\n# student_df.merge(roll_no, how='right', on='roll_no')\n\n# # Now, we want to combine the student_df and student_selection. We can do this by using outer\/full join. \n# # You need to set parameter how = 'outer'\n# student_df.merge(student_selection, how='outer')\n\n# # Now, we have 2 columns common college and name in both the dataframes. So, here we will use the inner join.\n# # You just need to set the parameters how='inner' and as we have 2 common columns therefore, set the parameter on=['college','name']\n# student_df.merge(pool, how='inner', on=['college', 'name'])","f31385b1":"# data = pd.read_csv('..\/datasets\/big_mart_sales.csv')\n\n# # SINGLE COLUMN\n# # group by Item Type\n# d = data.groupby(['Item_Type'])\n# # first element of each column for each group\n# d.first()\n# # mean of each column for each group\n# d.mean()\n# # d.max(), d.min()\n\n# # TWO COLUMNS\n# data.groupby(['Outlet_Size', 'Item_Type']).mean()\n# # alternative: pd.pivot_table(data, index= ['Outlet_Size', 'Item_Type'], values= 'Item_MRP', aggfunc= 'mean')","fc982201":"# # The crosstab() function is used to compute a frequency table of two or more factors\n# pd.crosstab(data['Outlet_Size'], data['Item_Type'])","7a6296fc":"# # calculate the sum of all MRP\n# data.Item_MRP.sum()\n\n# # calculate the average Item_MRP\n# data.Item_MRP.mean()\n\n# # calculate the median of Item_MRP\n# data.Item_MRP.median()\n\n# # find out the most frequent outlet type\n# data.Outlet_Type.mode()\n\n# #find out the count of each outlet type\n# data.Outlet_Type.value_counts()\n\n# # get the summary\n# data.describe()","812a99d3":"### 4: <span id=\"c14\">Reading files from multiple directories and concatenating<\/span>","b4c03cb7":"## <span id=\"d2\">Crosstab for joint frequency of two columns<\/span>","45f72710":"## <span id=\"c1\">Reading Data Files<\/span>","812b3b87":"## Contents:\n\n### <a href=\"#c1\">A) Reading Data Files<\/a>\n   #### 1:  <a href=\"#c11\">CSV<\/a>\n   #### 2:  <a href=\"#c12\">Excel<\/a>\n   #### 3:  <a href=\"#c13\">Json<\/a>\n   #### 4:  <a href=\"#c14\">Reading files from multiple directories and concatenating<\/a>\n### <a href=\"#c2\">B) Saving Data Files<\/a>\n### <a href=\"#c3\">C) Dealing with Missing Values<\/a>\n### <a href=\"#c4\">D) Updating values of a column by mapping<\/a>\n### <a href=\"#c5\">E) Derive a new column from an existing column<\/a>\n### <a href=\"#c6\">F) Selecting rows based on index and position<\/a>\n   #### 1:  <a href=\"#c61\">Index<\/a>\n   #### 2:  <a href=\"#c62\">Difference between loc and iloc<\/a>\n   #### 3:  <a href=\"#c63\">Selecting rows based on position of index (or simply position)<\/a>\n   #### 4:  <a href=\"#c64\">Selecting rows based on label of index<\/a>\n### <a href=\"#c7\">G) Selecting rows based on condition<\/a>\n### <a href=\"#c8\">H) Sorting the data frame by values of specific column(s)<\/a>\n### <a href=\"#c9\">I) Combining and Joining Multiple Data frames<\/a>\n   #### 1:  <a href=\"#c91\">Combining multiple dataframes<\/a>\n   #### 2:  <a href=\"#c92\">Performing SQL-like Joints<\/a>\n### <a href=\"#d1\">J) Grouping by using columns having categorical values<\/a>\n### <a href=\"#d2\">K) Crosstab for for joint frequency of two columns<\/a>\n### <a href=\"#d3\">L) Summarizing the dataframe<\/a>","0ccae7ba":"### 2: <span id=\"c62\">Difference between loc and iloc<\/span>","93bb4417":"## <span id=\"c6\">Selecting rows based on index and position<\/span>","6ccb99d2":"## <span id=\"c2\">Saving Data Files<\/span>","8c2412a8":"## How to use this notebook?\nGo to Contents. <br>\nSelect the tool you want to use, uncomment a code cell using Ctrl + \"\/\" .<br>\nNow, change the dataframe and proceed <br>","24ac3612":"## <span id=\"d3\">Summarizing the dataframe<\/span>","690655d8":"## <span id=\"c8\">Sorting the data frame by values of specific column(s)<\/span>","cf3d4d2b":"## <span id=\"c4\">Updating the values of a column by mapping<\/span>","df64968c":"## <span id=\"c5\">Derive a New column from an existing column<\/span>","00c153e9":"## <span id=\"c3\">Dealing with Missing Values<\/span>","aec50afa":"## <span id=\"c7\">Selecting rows based on condition<\/span>","effa768b":"### 3: <span id=\"c63\">Selecting rows based on position of index (or simply position)<\/span>","96d8d374":"### 2: <span id=\"c12\">Excel<\/span>","0873803f":"# DATA PREPROCESSING TEMPLATE AND TOOLBOX","eb5766ab":"### 3: <span id=\"c13\">Json<\/span>","3a70d833":"### 1: <span id=\"c61\">Index<\/span>","78882d47":"## <span id=\"d1\">Grouping by using columns having categorical values<\/span>","3af55ef8":"### 1: <span id=\"c91\">Combining Multiple Dataframes<\/span>","0ef8ceeb":"### 4: <span id=\"c64\">Selecting rows based on label of index<\/span>","062438cb":"## <span id=\"c9\">Combining and Joining Multiple Dataframes<\/span>","5335aa2d":"### 1: <span id=\"c11\">CSV<\/span>","525e0514":"### 2: <span id=\"c92\">Performing SQL like joints<\/span>","56ab3700":"<B> Please upvote and comment if you like it <\/B><br>\n<B> I will be updating this notebook regularly <\/B><br>\n<B> Contact me if you want to improve this notebook <\/B> "}}