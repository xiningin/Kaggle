{"cell_type":{"70fec906":"code","039888fa":"code","233f006a":"code","deb53e0d":"code","7ae9a34d":"code","ec26ccb2":"code","31c3c8fc":"code","d0d6e112":"code","be0136fb":"code","56d94056":"code","5d15f3da":"code","5e23d1b0":"code","a6a070fd":"code","887c9fb4":"code","e9019be8":"code","2ac32dc5":"code","87522567":"code","5b9c5a4c":"code","c22e664c":"code","ebf18a76":"code","d1db787e":"code","d0f14fbe":"code","781eadd7":"code","67a18b48":"code","961fb4b9":"code","626341d6":"code","18172bb1":"code","4b135974":"code","0c16988b":"code","eab7c1df":"code","6f4a0a5e":"code","da6863be":"code","5a4561f9":"code","e93f6b7a":"code","ef06442c":"code","bf0370a1":"code","b61d79ad":"code","a44bb571":"code","83624af6":"code","7b0c597e":"code","c183372d":"code","e32e91fc":"code","e067452b":"code","a4a94496":"code","55b13d84":"code","2eaa5659":"markdown","07849f1c":"markdown","311d442c":"markdown","e55beedf":"markdown","832325bc":"markdown","9b274801":"markdown","53027c69":"markdown","e9f1fc2e":"markdown","699a2924":"markdown","57a091ff":"markdown","5271e1eb":"markdown","9e4a4043":"markdown","89063502":"markdown","67c763fd":"markdown","f1a235cb":"markdown","4c32f0b8":"markdown","c05af268":"markdown","6ff54ad0":"markdown","7b6a96ee":"markdown","201fc6f0":"markdown"},"source":{"70fec906":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport glob","039888fa":"print(os.listdir('..\/input\/'))\nprint(os.listdir('..\/input\/datafiles'))","233f006a":"masseyordinals = pd.read_csv('..\/input\/masseyordinals\/MasseyOrdinals.csv')\nsub = pd.read_csv('..\/input\/SampleSubmissionStage1.csv')\nsub.head()","deb53e0d":"data_dict = {}\nfor i in glob.glob('..\/input\/datafiles\/*'):\n    name = i.split('\/')[-1].split('.')[0]\n    print(i)\n    if name != 'TeamSpellings':\n        data_dict[name] = pd.read_csv(i)\n    else:\n        data_dict[name] = pd.read_csv(i, encoding='latin-1')        ","7ae9a34d":"data_dict['Teams'].head()","ec26ccb2":"data_dict['TeamSpellings'].head()","31c3c8fc":"team_counts = data_dict['TeamSpellings'].groupby('TeamID')['TeamNameSpelling'].count().reset_index()\nteam_counts.columns = ['TeamID', 'TeamSpellingCount']","d0d6e112":"plt.title('Count of team counts');\nteam_counts['TeamSpellingCount'].value_counts().sort_index().plot(kind='barh', color='teal');","be0136fb":"data_dict['NCAATourneySeeds'].head()","56d94056":"data_dict['NCAATourneySeeds']['Seed'] = data_dict['NCAATourneySeeds']['Seed'].apply(lambda x: int(x[1:3]))\ndata_dict['NCAATourneySeeds'] = data_dict['NCAATourneySeeds'][['Season', 'TeamID', 'Seed']]\ndata_dict['NCAATourneySeeds'].head()","5d15f3da":"data_dict['NCAATourneyCompactResults'] = data_dict['NCAATourneyCompactResults'][['Season','WTeamID', 'LTeamID']]\ndata_dict['NCAATourneyCompactResults'].head()","5e23d1b0":"df = pd.merge(data_dict['NCAATourneyCompactResults'], data_dict['NCAATourneySeeds'], how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf = pd.merge(df, data_dict['NCAATourneySeeds'], how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\ndf = df.drop(['TeamID_x', 'TeamID_y'], axis=1)\ndf['seed_diff'] = df['Seed_x'] - df['Seed_y']\ndf.head()","a6a070fd":"data_dict['RegularSeasonCompactResults'].head()","887c9fb4":"plt.title('Mean scores of winning teams by season');\ndata_dict['RegularSeasonCompactResults'].groupby(['Season'])['WScore'].mean().plot();","e9019be8":"data_dict['RegularSeasonCompactResults']['Season'] += 1","2ac32dc5":"team_win_score = data_dict['RegularSeasonCompactResults'].groupby(['Season', 'WTeamID'])['WScore'].mean().reset_index()\nteam_loss_score = data_dict['RegularSeasonCompactResults'].groupby(['Season', 'LTeamID'])['LScore'].mean().reset_index()\ndf = pd.merge(df, team_win_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'WTeamID'])\ndf = pd.merge(df, team_loss_score, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'LTeamID'])\ndf = pd.merge(df, team_loss_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'])\ndf = pd.merge(df, team_win_score, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\ndf.drop(['LTeamID_y', 'WTeamID_y'], axis=1, inplace=True)\ndf = df.loc[(df['Season'] > 1985) & (df['Season'] < 2014)]\ndf.head()","87522567":"data_dict['RegularSeasonDetailedResults'].head()","5b9c5a4c":"data_dict['RegularSeasonDetailedResults']['Season_join'] = data_dict['RegularSeasonDetailedResults']['Season'] + 1","c22e664c":"plt.title('Mean number of field goals made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGM'].mean().plot();","ebf18a76":"plt.title('Mean number of field goals attempted by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGA'].mean().plot();","d1db787e":"plt.title('Mean number of three pointers made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGM3'].mean().plot();","d0f14fbe":"plt.title('Mean number of three pointers attempted by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFGA3'].mean().plot();","781eadd7":"plt.title('Mean number of free throws made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFTM'].mean().plot();","67a18b48":"plt.title('Mean number of free throws made by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WFTA'].mean().plot();","961fb4b9":"plt.title('Mean number of offensive rebounds pulled by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WOR'].mean().plot();","626341d6":"plt.title('Mean number of defensive rebounds pulled by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WDR'].mean().plot();","18172bb1":"plt.title('Mean number of assists by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WAst'].mean().plot();","4b135974":"plt.title('Mean number of turnovers committed by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WTO'].mean().plot();","0c16988b":"plt.title('Mean number of steals accomplished by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WStl'].mean().plot();","eab7c1df":"plt.title('Mean number of blocks accomplished by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WBlk'].mean().plot();","6f4a0a5e":"plt.title('Mean number of personal fouls committed by winning teams by season');\ndata_dict['RegularSeasonDetailedResults'].groupby(['Season'])['WPF'].mean().plot();","da6863be":"df.head()","5a4561f9":"loss_df = df[df['WTeamID_x'] > df['LTeamID_x']]\nwin_df = df[df['WTeamID_x'] < df['LTeamID_x']]\nwin_df['target'] = 1\nwin_df.columns = ['Season', 'Team1', 'Team2', 'Seed_1', 'Seed_2', 'seed_diff',\n       'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'target']\nloss_df['target'] = 0\nloss_df = loss_df[['Season', 'LTeamID_x', 'WTeamID_x', 'Seed_y', 'Seed_x', 'seed_diff',\n       'LScore_y', 'WScore_y', 'WScore_x', 'LScore_x', 'target']]\nloss_df.columns = ['Season', 'Team1', 'Team2', 'Seed_1', 'Seed_2', 'seed_diff',\n       'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'target']\nloss_df['seed_diff'] = -1 * loss_df['seed_diff']\nnew_df = win_df.append(loss_df)","e93f6b7a":"new_df.head()","ef06442c":"test = sub.copy()\nsub['Season'] = sub['ID'].apply(lambda x: int(x.split('_')[0]))\nsub['Team1'] = sub['ID'].apply(lambda x: int(x.split('_')[1]))\nsub['Team2'] = sub['ID'].apply(lambda x: int(x.split('_')[2]))\nsub = pd.merge(sub, data_dict['NCAATourneySeeds'], how='left', left_on=['Season', 'Team1'], right_on=['Season', 'TeamID'])\nsub = pd.merge(sub, data_dict['NCAATourneySeeds'], how='left', left_on=['Season', 'Team2'], right_on=['Season', 'TeamID'])\nsub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'WTeamID'])\nsub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'LTeamID'])\nsub = pd.merge(sub, team_loss_score, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'LTeamID'])\nsub = pd.merge(sub, team_win_score, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'WTeamID'])\nsub['seed_diff'] = sub['Seed_x'] - sub['Seed_y']\nsub.head()","bf0370a1":"new_df = pd.merge(new_df, team_counts, how='left', left_on='Team1', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\nnew_df = pd.merge(new_df, team_counts, how='left', left_on='Team2', right_on='TeamID')\nnew_df = new_df.drop(['TeamID'], axis=1)\n\nsub = pd.merge(sub, team_counts, how='left', left_on='Team1', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)\nsub = pd.merge(sub, team_counts, how='left', left_on='Team2', right_on='TeamID')\nsub = sub.drop(['TeamID'], axis=1)\n\nnew_df = new_df.drop(['Season', 'Team1', 'Team2'], axis=1)\nsub = sub.drop(['Pred', 'Season', 'Team1', 'Team2', 'TeamID_x', 'TeamID_y', 'WTeamID_x', 'WTeamID_y', 'LTeamID_x', 'LTeamID_y'], axis=1)\nsub.columns = ['ID', 'Seed_1', 'Seed_2', 'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'seed_diff', 'TeamSpellingCount_x', 'TeamSpellingCount_y']\nsub = sub[['ID', 'Seed_1', 'Seed_2', 'seed_diff', 'WScore_1', 'LScore_1', 'LScore_2', 'WScore_2', 'TeamSpellingCount_x', 'TeamSpellingCount_y']]\nnew_df = new_df.fillna(0)\nsub = sub.fillna(0)","b61d79ad":"X = new_df.drop(['target'], axis=1)\ny = new_df['target']\nX_test = sub.drop(['ID'], axis=1)","a44bb571":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","83624af6":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y.values[train_index], y.values[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:, 1].reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.1, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(log_loss(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","7b0c597e":"X1 = X.copy()\nX_test1 = X_test.copy()","c183372d":"scaler = StandardScaler()\nX1[['WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']] = scaler.fit_transform(X1[['WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']])\nX_test1[['WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']] = scaler.transform(X_test1[['WScore_1', 'LScore_1', 'LScore_2', 'WScore_2']])","e32e91fc":"%%time\nmodel = linear_model.LogisticRegression(C=0.0001)\noof_lr, prediction_lr, scores = train_model(X1, X_test1, y, params=None, folds=folds, model_type='sklearn', model=model)","e067452b":"params = {'num_leaves': 8,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 5,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}\noof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","a4a94496":"test['Pred'] = prediction_lr\ntest.to_csv('submission.csv', index=False)","55b13d84":"test['Pred'] = prediction_lgb\ntest.to_csv('lgb.csv', index=False)","2eaa5659":"### Target variable\n\n**Important**\n\nThe idea is the following: currently we have pairs of teams and we know which of them is loser, which is winner. For predictions we have pairs of teams, where we won't know who is winner, but we know that the teams will be sorted (like 1214_1246 and not the other way). So we can modify the training dataframe in the following way:\n- create columns not for winning\/losing team, but for teams with lower and higher number;\n- of team with lower number of the winner, the target is 1 and 0 otherwise.","07849f1c":"## Data overview and loading\n\nWe have a lof of files and data. Let's have a look at them!","311d442c":"In `TeamSpellings` we have info about all spellings of all teams. Let's use this as a feature!","e55beedf":"Let's read all files in `datafiles` into one dictionary for convenience.","832325bc":"## Regular seasons detailed results","9b274801":"We have a folder `datafiles` with main data, we have playbyplay info (play-by-play event logs) and masseyordinals (various rankings over years).\n\nIn the submission we must predict probabilities of victory for all pairs of teams.","53027c69":"In `Teams` we have info on first and last season of each team.","e9f1fc2e":"Scaling is necessary for linear models!","699a2924":"Let's try using mean scores of teams!","57a091ff":"I think that using scores in current season as a feature can be considered a leak, so let's add `1` to a year - this way we'll get scores of previous year while merging.","5271e1eb":"We can see that mean scores peaked in ~1900 and steadily decreased since that time, but in recent years mean scores rise again. Sadly, I don't know enough info about history of matches in NCAA, so I don't have an explanation for such a phenomenon.","9e4a4043":"Seed is very important for determining which team is better, so we will use it as a feature. Let's convert it into number and rearrange the columns.","89063502":"We also have information about the results of season matches.","67c763fd":"Notice that submission has data since 2014 year.","f1a235cb":"We can create a new dataframe which will have info about matches and seeds of teams","4c32f0b8":"### Regular season results\n\nLet's try using some info from regular seasons","c05af268":"## General information\n\nIn this kernel I'm working with data from Google Cloud & NCAA\u00ae ML Competition 2019-Men's Challenge. We'll try to predict winners of NCAA based on previous tournaments! We have a lot of data, so let's start with EDA.\n\n![](https:\/\/www.ncaa.com\/sites\/default\/files\/public\/styles\/large\/public-s3\/images\/2018-11-06\/2019-NCAA-bracket-March-Madness-tournament-PDF.png?itok=etptsRdC)\n\nWork in progress.\n","6ff54ad0":"## Preparing data for training","7b6a96ee":"## Preparing test data","201fc6f0":"## Creating training data\n\nAt first we need to create some training data. We need to predict the results of matches between teams. So the data should have season (year), two teams and the result. Features should be based on teams or on teams and season.\nAlso we need to remember that we will need to predict the performance of teams before the beginning of the tournament, so we won't have information on teams performance in that tournament and need to use previously available information.\n\nIn stage 1 we need to make predictions for certain matches since 2014 year. To prevent leaks I'll use the data up to 2013 for training.\n\nLet's use NCAATourneySeeds data as a blueprint!"}}