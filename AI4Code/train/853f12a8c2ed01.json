{"cell_type":{"41e55d89":"code","7038995a":"code","adc98576":"code","87e82ede":"code","d4c83081":"code","541b7e6a":"code","6598aaa6":"code","24aae82e":"code","d85cd72b":"code","9d6cf844":"code","a4a76857":"code","5efc5a05":"code","0499e601":"code","80187b44":"code","8820b5ec":"code","4b03691f":"code","8e097f32":"code","6e792c8c":"code","8553ee26":"code","02ae2c99":"code","fb066fa2":"code","70c12ef7":"code","67932516":"code","37a9aa10":"code","3f50f11b":"markdown","732b2173":"markdown","20598433":"markdown"},"source":{"41e55d89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7038995a":"!pip install bert-for-tf2 \n\n!pip install sentencepiece\n","adc98576":"import tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert","87e82ede":"#Training data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","d4c83081":"import re\n\ntest_str = train.loc[417, 'text']\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'#\\w+', '', text) # Remove hashtag\n    text = re.sub(r'@\\w+', '', text) # Remove mentions\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\nprint(\"Original text: \" + test_str)\nprint(\"Cleaned text: \" + clean_text(test_str))","541b7e6a":"# Testing data \ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nprint('Testing data shape: ', test.shape)\ntest.head()","6598aaa6":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    # df['hashtags'].fillna(value='no', inplace=True)\n    # df['mentions'].fillna(value='no', inplace=True)\n    \n    return df\n    \ntrain = process_text(train)\ntest = process_text(test)","24aae82e":"#Missing values in training set\ntrain.isnull().sum()","d85cd72b":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","9d6cf844":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","a4a76857":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=True)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","5efc5a05":"tokenizer.tokenize(\"don't be so judgmental\")","0499e601":"train =train.fillna(' ')\ntest = test.fillna(' ')\ntrain['text_final'] = train['text_clean']+' '+ train['keyword']+' '+ train['location']\ntest['text_final'] = test['text_clean']+' '+ test['keyword']+' '+ test['location']","80187b44":"train['lowered_text'] = train['text_final'].apply(lambda x: x.lower())\ntest['lowered_text'] = test['text_final'].apply(lambda x: x.lower())","8820b5ec":"train_input = bert_encode(train.lowered_text.values, tokenizer, max_len=320)\ntest_input = bert_encode(test.lowered_text.values, tokenizer, max_len=320)\ntrain_labels = train.target.values","4b03691f":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n","8e097f32":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    hidden1 = Dense(100, activation='relu')(clf_output)\n    hidden2 = Dense(50, activation='relu')(hidden1)\n    out = Dense(1, activation='sigmoid')(hidden2)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","6e792c8c":"model = build_model(bert_layer, max_len=320)\nmodel.summary()","8553ee26":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=10,\n    batch_size=16\n)\n","02ae2c99":"model.save('bert100_50.h5')","fb066fa2":"print(train_history)","70c12ef7":"test_pred = model.predict(test_input)","67932516":"#from sklearn.metrics import confusion_matrix, classification_report\n#print(confusion_matrix(y_test, y_pred))\n#print(classification_report(y_test, y_pred.round().astype(int)))","37a9aa10":"submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","3f50f11b":"## References\nhttps:\/\/towardsdatascience.com\/bert-in-keras-with-tensorflow-hub-76bcbc9417b\nhttps:\/\/medium.com\/analytics-vidhya\/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8\nhttps:\/\/github.com\/strongio\/keras-bert\/blob\/master\/keras-bert.ipynb\nhttps:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/\nhttp:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\nhttps:\/\/medium.com\/@vineet.mundhra\/loading-bert-with-tensorflow-hub-7f5a1c722565\nhttps:\/\/towardsdatascience.com\/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a\nhttps:\/\/github.com\/google-research\/bert\/blob\/master\/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\nhttps:\/\/gist.github.com\/vineetm\n\n## Bert Google Research\nhttps:\/\/github.com\/google-research\/bert\n\n## Bert Models\nhttps:\/\/tfhub.dev\/s?q=bert\n\n## Note\nFeel free to ask questions, initiate discussion on this topic. Please upvote the discussion and notebook to help me make more such contributions.","732b2173":"## Exploratory Data Analysis","20598433":"## Creating Traing Model"}}