{"cell_type":{"46b0b502":"code","21a6fc08":"code","5c863f3a":"code","f1e3ce99":"code","68596f2e":"code","ddb39614":"code","5d2b3f8e":"code","f5fa0fb3":"code","470cfcf7":"code","0f83019d":"code","64f5f4a2":"code","f198f96a":"code","72d70198":"code","3980da48":"code","96051901":"code","56b89186":"code","df280897":"code","d5780b49":"code","76037387":"code","5b90e50d":"code","5dd9d0e8":"code","45e496fe":"code","886ceec3":"code","ff1959a6":"code","69fff72e":"code","ab738ddd":"code","113517ef":"code","5ee36db3":"code","a695b1a6":"code","3940cb46":"code","55ebfb93":"code","405a68c6":"code","06ad208e":"code","dd9bcbf3":"code","e4ed3fca":"code","3f4050e1":"code","dc67d950":"code","459ce6e2":"code","07fdcf59":"code","b7e2b1f0":"code","22705cd8":"code","885d9705":"code","6e526235":"code","b2a4a45d":"code","c074da1c":"code","dd0c2b62":"code","2ffce62d":"code","b47c303a":"code","2f1b60d2":"code","43c80f5e":"code","95d8a5d8":"code","6948042e":"code","f65fc6ec":"code","6f3ced90":"code","246aec56":"code","a8a35972":"code","7d28c0ae":"code","c1ff23b8":"markdown","302fbe39":"markdown","ce4f0492":"markdown","1dcf8d77":"markdown","23c07cbb":"markdown","1205f1cc":"markdown","68de76d5":"markdown","60ce7874":"markdown","91f00c41":"markdown","dfb7b4a9":"markdown","5f544955":"markdown","90add8e7":"markdown","9a9b71df":"markdown","d132ca01":"markdown","217a7d28":"markdown","b362082e":"markdown","54378b9e":"markdown","3c2b2ca9":"markdown","9c1adac4":"markdown","1db9d442":"markdown","7aa5b6c2":"markdown","285920a5":"markdown","03c9abb6":"markdown","56ca1183":"markdown","bcbd8c2b":"markdown","0b6ad142":"markdown","c44fd8b3":"markdown","c0948670":"markdown","a15d0b3c":"markdown","9a30bd93":"markdown","9d5f1c6e":"markdown","30bc615f":"markdown","d06b74bf":"markdown","fc1b750a":"markdown","c3bc5335":"markdown","78216379":"markdown","0bd673a8":"markdown","e4d11404":"markdown","92c68fae":"markdown","99393b36":"markdown","bd18e375":"markdown","f01cd00f":"markdown","6087d1ab":"markdown","5fb6504e":"markdown","e7f3d332":"markdown","dc26dd6e":"markdown","196f50c2":"markdown"},"source":{"46b0b502":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","21a6fc08":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head()","5c863f3a":"df.dtypes","f1e3ce99":"print(\"total number of NaN in column 'Age' is:\", df['Age'].isna().sum())","68596f2e":"fcount = 0\nmcount = 0\n\nfor i in df['Sex']:\n    if i == 'female':\n        fcount +=1\n    else:\n        mcount +=1\n        \nprint('We have: ', fcount, 'females')\nprint('We have: ', mcount, 'males')","ddb39614":"df['Sex'].replace({'female':1, 'male':0}, inplace=True)\ndf.head()","5d2b3f8e":"df['Pclass'].value_counts()","f5fa0fb3":"df['Family'] = df['SibSp'] + df['Parch']\ndf.drop(['SibSp', 'Parch'], axis=1, inplace=True)\ndf.head()","470cfcf7":"df['Age'].fillna(29, inplace=True) # replace missing values with average age value\ndf['Age'].apply(np.ceil) # round age up to the closest integer\ndf.describe()","0f83019d":"plt.figure(figsize=(14, 3.5), dpi=80)\n\nplt.subplot(1,3,1)\nplt.hist(df['Age'], color='firebrick', alpha=.85)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\nplt.subplot(1,3,2)\nplt.hist(df['Fare'], color='firebrick', alpha=.85)\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\n\nplt.subplot(1,3,3)\nplt.hist(df['Family'], color='firebrick', alpha=.85)\nplt.xlabel('Family Size')\nplt.ylabel('Frequency')","64f5f4a2":"cclass = pd.get_dummies(pd.Series(list(df['Pclass'])), drop_first=True)\ncclass","f198f96a":"df = pd.concat([df, cclass], axis=1)\ndf.rename(columns={2: 'SecondClass', 3: 'ThirdClass'},inplace=True)\ndf.tail()","72d70198":"df.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\n\ndf = df.set_index('PassengerId')\ndf.head(5)","3980da48":"df.tail(5)","96051901":"df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean().sort_values(by='Survived', ascending=False)","56b89186":"df1 = df.copy()\ndf1['Sex'].replace({1:'Female', 0:'Male'}, inplace=True)\ndf1[['Sex', 'Survived']].groupby(['Sex'],\n                                as_index=True).mean().sort_values(by='Survived', ascending=False)","df280897":"df[['Family', 'Survived']].groupby(['Family'],\n                                   as_index=True).mean().sort_values(by='Survived', ascending=False)","d5780b49":"# group ages based on 10 year increments for a table\nbins = [0, 10, 20, 30, 40, 50, 60, 90]\nlabels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60+']\ndf['Age_gr'] = pd.cut(df1.Age, bins, labels = labels, include_lowest = True)\n\ndf.head()","76037387":"df[['Age_gr', 'Survived']].groupby(['Age_gr'],\n                                as_index=True).mean().sort_values(by='Age_gr', ascending=True)","5b90e50d":"import matplotlib as mpl \nimport matplotlib.pyplot as plt\nprint('Matplotlib imported!')","5dd9d0e8":"df['Fare'] = df['Fare'].replace(0, np.nan)\ndf['Fare'] = df['Fare'].dropna(how='all', axis=0)\nprint((df['Fare'] == 0).sum())\ndf.tail()","45e496fe":"female = df[['Sex', 'Age', 'Fare', 'Survived', 'Family']].loc[df['Sex'] == 1]\nmale = df[['Sex', 'Age', 'Fare', 'Survived', 'Family']].loc[df['Sex'] == 0]\nfemale.describe()","886ceec3":"plt.figure(figsize=(14, 3.5), dpi=80)\nplt.suptitle('Female & Male', fontsize=16)\nlabels='Female','Male'\n\nplt.subplot(1, 3, 1)\nfemale['Age'].plot(kind='density', linewidth=3, alpha=.7, color='darkred')\nmale['Age'].plot(kind='density', linewidth=3, alpha=.7, color='orange')\nplt.xlim(0, 90)\nplt.ylim(0, 0.05)\nplt.xlabel('Age')\nplt.legend(labels)\n\nplt.subplot(1, 3, 2)\nfemale['Fare'].plot(kind='density', alpha=.7, color='darkred', linewidth=3)\nmale['Fare'].plot(kind='density', alpha=.7, color='orange', linewidth=3)\nplt.xlim(-10, 500)\nplt.ylim(0, .025)\nplt.xlabel('Fare')\nplt.legend(labels)\n\nplt.subplot(1, 3, 3)\nfemale['Family'].plot(kind='density', alpha=.7, color='darkred', linewidth=3)\nmale['Family'].plot(kind='density', alpha=.7, color='orange', linewidth=3)\nplt.xlim(-1, 10)\nplt.ylim(0, .7)\nplt.xlabel('Family Size')\nplt.legend(labels)","ff1959a6":"g = sns.catplot(x=\"Fare\", y=\"Survived\", row=\"Pclass\", kind='box', palette='YlOrRd',\n                orient=\"h\", height=1.8, aspect=3.5, hue_order='Ascending',\n                data=df1)\ng.set(xscale='log')\ng.set(xlim=(3, 500))","69fff72e":"h = sns.catplot(x=\"Age\", y=\"Survived\", row=\"Pclass\", kind='box', palette='YlOrRd',\n                orient=\"h\", height=1.8, aspect=3.5, hue_order='Ascending',\n                data=df1)\n","ab738ddd":"f = sns.catplot(x=\"Age_gr\", y=\"Survived\", col=\"Pclass\", palette='YlOrRd',\n                data=df, kind=\"bar\", ci=None, height=6, aspect=1.33)\nf.set_axis_labels('', 'Survival Rate')\nf.set(ylim=(0,1))","113517ef":"aw = sns.regplot(x='Fare', y='Survived', data=df, logistic=True,\n                 line_kws={\"color\":\"orange\",\"alpha\":0.3,\"lw\":4},\n                 scatter_kws={\"color\":\"red\", \"edgecolor\":'darkred', 'alpha':.7, \"s\":100} )\naw.set_ylabel('Survival Probability')\naw.set_xlabel('Fare')\naw.set_title('Logistic Model',fontsize=16)","5ee36db3":"ax = sns.regplot(x='Age', y='Survived', data=df, logistic=True, marker='o', \n                 line_kws={\"color\":\"orange\",\"alpha\":0.3,\"lw\":4},\n                 scatter_kws={\"color\":\"red\", \"edgecolor\":'darkred', 'alpha':.7, \"s\":100} )\nax.set_ylabel('Survival Probability')\nax.set_xlabel('Age')\nax.set_title('Logistic Model',fontsize=16)","a695b1a6":"# importing necessary packages\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\nprint('statsmodels imported!')","3940cb46":"df['lnFare'] = np.log(df['Fare'])\ndf.head()","55ebfb93":"logit1 = smf.logit('Survived ~ Sex + Age', data=df).fit()\nprint(logit1.summary())","405a68c6":"logit2 = smf.logit('Survived ~ Sex + Age + lnFare', data=df).fit()","06ad208e":"logit3 = smf.logit('Survived ~ Sex + Age + lnFare + Family', data=df).fit()","dd9bcbf3":"logit4 = smf.logit('Survived ~ Sex + Age + lnFare + Family + SecondClass + ThirdClass', data=df).fit()","e4ed3fca":"# we build table charachteristics and plug regressions\ninfo_dict={'Pseudo R-squared' : lambda x: f\"{x.prsquared:.4f}\",\n           'No. observations' : lambda x: f\"{int(x.nobs):d}\"}\n\nresults_table = summary_col(results=[logit1, logit2, logit3, logit4],\n                            float_format='%0.4f',\n                            stars = True,\n                            model_names=['Model 1',\n                                         'Model 2',\n                                        'Model 3',\n                                        'Model 4'],\n                            info_dict=info_dict,)\n\nresults_table.add_title('Logit Regressions - Table')\n\nprint(results_table)","3f4050e1":"print('SURVIVED = ', logit4.params[0].round(4),'+', logit4.params[1].round(4),'* SEX',\n      logit4.params[2].round(4),'* AGE +', logit4.params[3].round(4),'* lnFARE', logit4.params[4].round(4), '* FAMILY',\n     logit4.params[4].round(4),'* 2NDCLASS', logit4.params[6].round(4), '* 3RDCLASS')","dc67d950":"df = df.dropna()\nXset = df[['Sex','Age','lnFare','Family','SecondClass','ThirdClass']]\ny = df[['Survived']]\n\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(Xset, y)\npredictions = logmodel.predict(Xset)","459ce6e2":"# this function will generate confusion matrix needed to evaluate success rate of our logistic model\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Reds):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"grey\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y, predictions, labels=[1,0]))","07fdcf59":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y, predictions, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Survived=1','Survived=0'],normalize= False,  title='Confusion matrix')","b7e2b1f0":"from sklearn.metrics import classification_report\nprint(classification_report(y, predictions))","22705cd8":"acc_log = round(logmodel.score(Xset, y) * 100, 2)\nprint('Accuracy of our train data via Logistic model can be summed as: ', acc_log,'%')","885d9705":"sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","6e526235":"print('Test dataset shape:', test.shape)\nprint('Outcomes\\' shape:', sub.shape)","b2a4a45d":"# we combine those two datasets\ntest_model = pd.merge(test, sub, on='PassengerId')\ntest_model.head()","c074da1c":"test_model.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\n\ntest_model = test_model.set_index('PassengerId')\ntest_model.head(10)","dd0c2b62":"# update Sex to dummy variables; male=1 and female=1\ntest_model['Sex'].replace({'female':1, 'male':0}, inplace=True)\ntest_model.head()","2ffce62d":"test_model['Age'].fillna(30, inplace=True) # replace missing values with average values\ntest_model['Age'].apply(np.ceil) # roung age up to the closest integer\ntest_model.describe()","b47c303a":"test_class1 = pd.get_dummies(pd.Series(list(test_model['Pclass'])), drop_first=True)\ntest_class1","2f1b60d2":"test_model = test_model.reset_index()\ntest_model = pd.concat([test_model, test_class1], axis=1)\ntest_model.rename(columns={2: 'SecondClass', 3: 'ThirdClass'}, inplace=True)\ntest_model.head()","43c80f5e":"test_model['Family'] = test_model['SibSp'] + test_model['Parch']\ntest_model.drop(['SibSp', 'Parch'], axis=1, inplace=True)\ntest_model.head()","95d8a5d8":"test_model['Fare'] = test_model['Fare'].replace(0, np.nan)\ntest_model['Fare'] = test_model['Fare'].dropna(how='all', axis=0)\nprint((df['Fare'] == 0).sum())\n\ntest_model['lnFare'] = np.log(test_model['Fare'])\nprint('Shape: ', test_model.shape)\ntest_model.tail()","6948042e":"test_model.dropna(inplace=True)\ntest_model.reset_index(drop=True, inplace=True)\nprint(test_model.shape)\ntest_model.tail()","f65fc6ec":"Xtest1 = test_model[['Sex','Age','lnFare','Family','SecondClass','ThirdClass']]\nyhat1 = logmodel.predict(Xtest1)\nprint(yhat1.shape)\nprint(test_model.shape)","6f3ced90":"yhat11 = pd.DataFrame(yhat1)\nyhat11.tail()","246aec56":"test_model.tail()","a8a35972":"finalll = pd.concat([test_model, yhat11], axis=1)\nfinalll.rename(columns={0: 'Predicted'}, inplace=True)\nfinalll.tail()","7d28c0ae":"count = 0\nwrong = 0\ntotal = 0\n\nfor index, row in finalll.iterrows():\n    if row['Survived'] == row['Predicted']:\n        count += 1\n        total += 1\n    else:\n        wrong += 1\n        total += 1\n        \nprint('correct prediction ratio:', count\/ total)\nprint('wrong prediction ratio:', wrong \/ total )","c1ff23b8":"We'll create two data sets, one for female passengers and one for male passengers as they will be visualized separately to better see the effect of gender and their subsequent probabilities","302fbe39":"Importing necessary packages...","ce4f0492":"### Test Data Cleaning and Organization\nFirst, we need to import clean and organize our train dataset the exact same way we did with our train data and then we can run our model.","1dcf8d77":"Now I will run the same logit model but using the data from the test dataset. We will create a list with the predicted values for the survival (survived=1; not survived=0) and then append to our test dataset for the comparison of how precise our model was on test data.","23c07cbb":"![Imgur](https:\/\/i.imgur.com\/N8ETwjW.png 'Header')","1205f1cc":"As we have a binary dependent variable which gives only Yes=1, No=0 outcomes, we\u2019ll test some of the explanatory variables and its explanatory power.","68de76d5":"We plot the boxplot again but this time for the <code>'Age'<\/code> variable","60ce7874":"We import matplotlib libraries","91f00c41":"Cleaning <code>'Fare'<\/code> data for this section","dfb7b4a9":"We managed to achieve the amazing 93% precision on our test set. This is even better than the result on the train-data. Maybe I made some technical mistake while comparing my predictions to the actual values, however, it can also be attributed to the neat cleaning of the data and test-set being more explicit in terms of characteristics than the train set.\n\nAnyway, the results confirm the assumptions I made at the beginning of the project. Being female and being the passenger of the first-class massively increases the chances of surviving. Being under 10 years old also contributes to a positive outcome. However, paying higher fare doesn't necessarily mean the passenger ended in the first class, as this would have increased his or her survival chances. This can be attributed to the fact that some tickets were maybe sold in the aftermarket with higher rates. Or some tickets could have been more expansive in the different ports. For this project, I didn't go into details explaining how passengers embarking port influenced the outcome of placing him or her in a certain class or if the fare for the tickets significantly differed in those ports.\n\nAs for the model, logit regression was initially my first choice and it didn't disappoint so I didn't see the reason to try linear regression or classification. 80% succession rate on the train set can be assumed to be satisfactory, would I have gone figuring out the ages of the passengers, whose data was missing, instead of just filling them with average rate, could have increased the success rate. I was thinking of approximating age based on the title passenger name contained (for example, if it was Mrs. I bet she was more than 10. If no title maybe under 20 and so on)\n\n\n-----------\n> *Beka Modebadze 2019* **[LinkedIn](https:\/\/www.linkedin.com\/in\/bmodebadze\/); [Github](https:\/\/github.com\/bexxmodd)**","5f544955":"* Even though it's not as noticeable as the correlation of the <code>'Fare'<\/code> trend still appears that as <code>'Age'<\/code> increases the probability of survival decreases, however, the slope of the regression is nearly flat.","90add8e7":"* Our model predicted the survivability of the passengers with the average precision of 80% (we have been more precise on passengers who have not survived)\n\n* Overall success of our model is satisfactory and we can move on testing our model on our train test and let's see how our model performs there","9a9b71df":"We'll count the number of females and number of males in our data","d132ca01":"Filling 'NaN' values in <code>'Age'<\/code> column and rounding up to closes integers to reduce minor variability in the data and to round estimated ages (which are float values) in style other ages are entered to keep data consistent","217a7d28":"We'll look at the number of passengers in each class. Class 1 is a first-class (highest location), 2 is a second-class (middle location), and 3 is a third-class (bottom location)","b362082e":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/8CLs1D2.png\">","54378b9e":"* We see that only <code>'Age'<\/code> is more or less normally distributed and we'll keep it that way\n\n* Fare is skewed to the left with a substantial difference in each bin. We know that <code>'Fare'<\/code> represents currency - the amount paid for the ticket - thus we will convert it by taking **ln** of the variable to use it as a percentage change, thus normalizing its distribution\n\n* <code>'Family'<\/code> is also skewed to the left but the difference between the minimum and the maximum value is marginal thus we'll keep it that way\n\n----\nWe'll create two dummy variables for the second and third classes from the <code>'Pclass'<\/code>, treating the first class as the base for our model. However, we'll still keep <code>'Pclass'<\/code> which will be used for pivot tables and data visualization","3c2b2ca9":"#### In order to evaluate precision of our model we'll create a function to compare our predictions with the test's survival results and convert to percentage or correct and wrong predictions","9c1adac4":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/2zoAhUc.png\">","1db9d442":"We'll check how many <code>NaN<\/code> results we have in our data and later we'll decide what to do with them","7aa5b6c2":"We will create new column which we'll name <code>'Family'<\/code>. This will be used to determin if person traveled alone or with certain number of family members (Siblinds\/Parents). In order to do that we will combine <code>'SibSp'<\/code> which is the number of siblinds and <code>'Parch'<\/code> number of parents","285920a5":"As we are using a logit model Adjusted R-squared will do no good for us as the logit model is build using MLE (maximum likelihood estimation) not regular OLS estimators. To analyze the significance of the model, that's why we have reported Pseudo R-squared, which does the same job for logit models what OLS does for linear regression.\nWith the addition of each explanatory variable, the pseudo R-squared went up and all the explanatory variables are statistically significant at 5% confidence. Thus, we'll keep 'model 4' and proceed with the predictions by testing our trained model on test data.\n\nBut before we do that first see how precise our model explains the given data. And we'll do that by using the confusion matrix in conjunction with classification report\n\nAnyway this our model, which will be used further in this project:","03c9abb6":"## Observe by pivot tables\n\nNow we start observing probabilities based on various categories.\n\n* First, we will look at the probability to survive the crash based on the class passenger was suited.\n\n* Second, we will observe survivability based on gender\n\n* Third, based on the effect of the family size present on the ship\n\n* Fourth, we will divide passengers into age groups and observe at the probability of survival\n\n#### Passengers Class and Survival Probability:","56ca1183":"---------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/ULf7YHt.png\">","bcbd8c2b":"* Here we have an odd situation where somehow if you were traveling with three family members chance of surviving is 72%. This can be just as a matter of chance, but we still need to include in our model to control for any controllable bias, even if it's by just a matter of chance\n\n#### Passengers Age Group and Survival Probability:\nWe'll group ages by 10-year increments for a table, which will give us seven bins","0b6ad142":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/7oKFkBF.png\">","c44fd8b3":"* If a passenger was in a first-class there was a ~70% surviving, 50% if a passenger was in the 2nd class and only 24% change if they were in the third class. Unfortunately, as we saw in a previous section third class was the class, where the majority of the travelers were located.\n\n#### Passengers Sex and Survival Probability:","c0948670":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/QP6dM4K.png\">","a15d0b3c":"**RMS Titanic** was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of April 15, 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n\n### Hypothesis:\nAs I remember from the movie titanic, after the crash, priority was given to females and children to fill the lifeboats however I assume that's the not only case. My main assumption for this research is that the passenger's class location had the biggest influence on the outcome if he or she survived. Higher the class closer the passenger would have been located to the deck, thus front row in the line for the lifeboats. Also, the actual fare passenger paid will not correlate with the chances of survival, hearsay the higher fare doesn't necessarily mean that passenger was in the better class.","9a30bd93":"Using a bar chart, we can compare the probability to survive in each class based on the age group","9d5f1c6e":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/AsArJE1.png\">","30bc615f":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/DemYpQE.png\">","d06b74bf":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/wOpSJMe.png\">","fc1b750a":"* Bad news for males. The probability of surviving if you were male was only 19%. This was expected as if you remember there was a rule that females and babies went first in the rescue boats.\n\n#### Passengers Family Size and Survival Probability:","c3bc5335":"Previously we looked at the distribution of the <code>'Age'<\/code>, <code>'Fare'<\/code> and <code>'Family'<\/code> in combination for both genders.\nNow we'll separate them based on gender, but now using the k-density graph for distribution instead of the histogram","78216379":"We start our session by doing the descriptive analysis of our data. Cleaning, wrangling and keeping what we assume would be influential in predicting the outcome.\n\nWe check for data types:","0bd673a8":"We convert <code>'Sex'<\/code> to dummy variables which will be used for our model; male=0 and female=1","e4d11404":"As mentioned in the beginning, we'll convert <code>'Fare<\/code> into the ln of Fare and run first **logit regression**\n\nWe'll continue adding explanatory variables one by one and later summarize all the models in a table for the comparison and inferences","92c68fae":"The ship once proclaimed as 'unsinkable' bears one of the most profound tragedies. For the Kaggle challenge, we have the data of the passengers on the ship. Data is split into halves for the train-set and for test-set. Our goal is to build the model which will predict survivability of the passengers based on provided characteristics with the precision of > 80%. We have multiple columns, and our goal is to decide which variables to keep and which variables to drop.\n\n---------\nWe start by importing all the necessary packages and importing the train set and taking look at the first five rows","99393b36":"Next, we look at the survivability based on Fare paid in each class for each gender. <code>'Fare'<\/code> data is represented in the log form for a better scaling","bd18e375":"* This logit model shows that <code>'Fare'<\/code> is strongly correlated with survivability, which contradicts the assumption made at the beginning of the research. However, this is not conclusive, this gives an idea of how the model should look like.","f01cd00f":"<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/rXnNidY.png\">","6087d1ab":"We have three continiuous explanatory variables <code>'Age'<\/code>, <code>'Fare'<\/code> and <code>'Family'<\/code>. We plan to implement these variables in our model. That's why we will look at their distribution and characteristics.","5fb6504e":"<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/Nl1r4be.png\">","e7f3d332":"----------\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/4VFBdPl.png\">","dc26dd6e":"-----\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/Mnrrfx9.png\">","196f50c2":"We convert 'yhat1' into the dataframe so we can easily append it to our test dataframe"}}