{"cell_type":{"c90e2216":"code","c62cfca5":"code","2b834688":"code","1cbdd8ef":"code","9a13a27c":"code","9b71e055":"code","2b08ff49":"code","92a7a31b":"code","540db9bf":"code","9850fc23":"code","6d140ec5":"code","6a790c4d":"code","dbeb7dca":"code","da09f3f3":"code","c779c663":"code","aef252d2":"code","76c78529":"code","aff4a99a":"code","1738dbc3":"code","0fef0e4a":"code","37630469":"code","ca9b37df":"code","6567d7f8":"code","f5046bef":"code","fc8c8749":"code","2236ef89":"code","b9810be6":"code","0b520dfb":"code","67b8ca43":"code","8a4c4f4e":"code","c180e8f9":"code","2e967663":"code","50a92289":"code","dd1c4766":"code","d25ef09c":"code","ebf30cda":"code","7688fc05":"code","c47414a3":"code","f75f98e1":"code","08efd0c5":"code","02823f03":"code","4965f3ed":"code","852ce473":"code","ffa84fa3":"code","4a3051c2":"code","5cbc15d6":"code","920adb06":"code","d7af4525":"code","053722d5":"code","dc33a699":"code","c43ec179":"code","367f5121":"code","da4ea69d":"code","0c998ade":"code","c9978b2a":"code","842daf03":"code","0eaf99c1":"code","b4eb59eb":"code","e631b42a":"code","5471dc4e":"code","5f2e0ce4":"markdown","d6a85447":"markdown","72ca7751":"markdown","a5348f2f":"markdown","5480e15c":"markdown","abf7998a":"markdown","05bcbdfa":"markdown","63762a12":"markdown","b207c935":"markdown","4bcefe35":"markdown","ee0a22f9":"markdown","ab6b30e0":"markdown","e011b3ca":"markdown","d92e2d06":"markdown","b19f863c":"markdown","b067a0f5":"markdown","fbc9e5cc":"markdown","d5914f3c":"markdown","9633045c":"markdown","70ac25fe":"markdown","3cd086d1":"markdown","18648ee3":"markdown","1aec0486":"markdown","95fc6662":"markdown","4fea884e":"markdown","417bae10":"markdown","4aba5329":"markdown","b55ebd75":"markdown","3ad28ac6":"markdown","f4202add":"markdown","de4d051a":"markdown","f7f208d1":"markdown","c2a05a21":"markdown","b940b3c3":"markdown"},"source":{"c90e2216":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use('dark_background')\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n\nimport warnings\nwarnings.simplefilter('ignore')","c62cfca5":"train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntrain.head()","2b834688":"temp = train[train['target'] == 1]\ntemp.head()","1cbdd8ef":"test = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\ntest.head()","9a13a27c":"submission = pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')\nsubmission.head()","9b71e055":"train.isnull().sum()","2b08ff49":"test.isnull().sum()","92a7a31b":"train.shape, test.shape","540db9bf":"df_train = train.drop(['qid', 'question_text'], axis = 1)\nlabel_counts = df_train.sum()\ndf_counts = pd.DataFrame(label_counts)\ndf_counts.rename(columns = {0:'counts'}, inplace = True)\ndf_counts = df_counts.sort_values('counts', ascending = False)\ndf_counts","9850fc23":"labels = np.round(df_train.sum()\/len(df_train)*100, 1)\nlabels","6d140ec5":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    #text = text.lower()\n    \n    #pattern = [zero or more character]\n    text = re.sub('\\[.*?\\]', '', text)\n    \n    #pattern = with or without(http),:\/\/, one or more non-white space character, OR www, .,one or more non-white space character\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    #pattern = <, zero or more characters, >, (one or more occurance of >)\n    text = re.sub('<.*?>+', '', text)\n    \n    #pattern = any punctionation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    #pattern = any new line\n    text = re.sub('\\n', '', text)\n    \n    #pattern = any from[a-zA-Z0-9_], any from[0-9], any from [a-zA-Z0-9_]\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","6a790c4d":"%%time\ntrain = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv', nrows = 50000)\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\ntrain['clean_text'] = train['question_text'].apply(str).apply(lambda x: clean_text(x))\ntest['clean_text'] = test['question_text'].apply(str).apply(lambda x: clean_text(x))","dbeb7dca":"kfold = 5\ntrain['kfold'] = train.index % kfold\ntrain.index % kfold","da09f3f3":"p_train = train[train[\"kfold\"] != 0].reset_index(drop = True)\np_valid = train[train[\"kfold\"] == 0].reset_index(drop = True)","c779c663":"p_train.head()","aef252d2":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')","76c78529":"%%time\nsenten_len = []\nfor sentence in tqdm(p_train['clean_text']):\n    token_words = tokenizer.encode_plus(sentence)['input_ids']\n    senten_len.append(len(token_words))","aff4a99a":"sns.distplot(senten_len)\nplt.xlim([0, 50])\nplt.xlabel('Tocken count')","1738dbc3":"max_len = 30","0fef0e4a":"class BertDataSet(Dataset):\n    \n    def __init__(self, sentences, toxic_labels):\n        self.sentences = sentences\n        self.targets = toxic_labels.to_numpy()\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_senten = tokenizer.encode_plus(sentence, \n                                            add_special_tokens = True, # [CLS],[SEP]\n                                            max_length = max_len,\n                                            pad_to_max_length = True,\n                                            truncation = True,\n                                            return_attention_mask = True\n                                             )\n        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        \n        return {\n            'ids' : ids,\n            'mask' : mask,\n            'toxic_label':toxic_label\n        }","37630469":"train_dataset = BertDataSet(p_train['clean_text'], p_train['target'])\nvalid_dataset = BertDataSet(p_valid['clean_text'], p_valid['target'])","ca9b37df":"train_batch = 32\nvalid_batch = 32","6567d7f8":"train_dataloader = DataLoader(train_dataset, batch_size = train_batch, pin_memory = True, num_workers = 4, shuffle = True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size = valid_batch, pin_memory = True, num_workers = 4, shuffle = False)","f5046bef":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","fc8c8749":"%%time\nmodel = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 1)\nmodel.to(device)\nmodel.train()","2236ef89":"%%time\nfor a in train_dataloader:\n    ids = a['ids'].to(device)\n    mask = a['mask'].to(device)\n    output = model(ids, mask)\n    break","b9810be6":"output","0b520dfb":"func.softmax(output['logits'], dim = 1)","67b8ca43":"output_probs = func.softmax(output['logits'], dim = 1)","8a4c4f4e":"torch.max(output_probs, dim = 1)","c180e8f9":"epochs = 5\nLR = 2e-5 #Learning rate\noptimizer = AdamW(model.parameters(), LR, betas = (0.9, 0.999), weight_decay = 1e-2, correct_bias = False)","2e967663":"train_steps = int((len(train) * epochs)\/train_batch)\nnum_steps = int(train_steps * 0.1)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","50a92289":"le = []\nfor b in tqdm(range(epochs)):\n    for a in train_dataloader:\n        le.append(scheduler.get_last_lr())\n        scheduler.step()\nplt.plot(np.arange(len(le)), le)","dd1c4766":"loss_fn = nn.BCEWithLogitsLoss()\nloss_fn.to(device)","d25ef09c":"scaler = torch.cuda.amp.GradScaler()","ebf30cda":"def training(train_dataloader, model, optimizer, scheduler):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in train_dataloader:\n        losses = []\n        optimizer.zero_grad()\n        \n        #allpreds = []\n        #alltargets = []\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            #allpreds.append(output.detach().cpu().numpy())\n            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward() #Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n        scheduler.step() # Update learning rate schedule\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/len(p_train)\n    \n    return losses, accuracy","7688fc05":"def validating(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/len(p_valid)\n    \n    return losses, accuracy, all_output_probs","c47414a3":"%%time\n\nbest_score = 1000\ntrain_accs = []\nvalid_accs = []\ntrain_losses = []\nvalid_losses = []\n\nfor eboch in tqdm(range(epochs)):\n    \n    train_loss, train_acc = training(train_dataloader, model, optimizer, scheduler)\n    valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\n    \n    print('train losses: %.4f' % train_loss, 'train accuracy: %.3f' % train_acc)\n    print('valid losses: %.4f' % valid_loss, 'valid accuracy: %.3f' % valid_acc)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accs.append(train_acc)\n    valid_accs.append(valid_acc)\n    \n    \n    if valid_loss < best_score:\n        best_score = valid_loss\n        print('Found a good model!')\n        state = {\n            'state_dict': model.state_dict(),\n            'optimizer_dict': optimizer.state_dict(),\n            'best_score': best_score\n        }\n        torch.save(state, 'best_model.pth')\n    else:\n        pass","f75f98e1":"x = np.arange(epochs)\nfig, ax = plt.subplots(1, 2, figsize = (15,4))\nax[0].plot(x, train_losses)\nax[0].plot(x, valid_losses)\nax[0].set_ylabel('Losses', weight = 'bold')\nax[0].set_xlabel('Epochs')\nax[0].grid(alpha = 0.3)\nax[0].legend(labels = ['train losses', 'valid losses'])\n\nax[1].plot(x, train_accs)\nax[1].plot(x, valid_accs)\nax[1].set_ylabel('Accuracy', weight = 'bold')\nax[1].set_xlabel('Epochs')\nax[1].legend(labels = ['train acc', 'valid acc'])\n\nax[1].grid(alpha = 0.3)\nfig.suptitle('Fold = 0', weight = 'bold') ","08efd0c5":"valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\nvalid_probs = np.asarray(valid_probs).flatten()\ny_valid = p_valid['target'].to_numpy().flatten()\nfpr, tpr, _ = roc_curve(y_valid, valid_probs)","02823f03":"fig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.set_title('ROC Curv')\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\nplt.show()","4965f3ed":"auc(fpr, tpr)","852ce473":"%%time\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use('dark_background')\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ntrain = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv', nrows = 50000)\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\nsubmission = pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')\n\n\ndef clean_text(text):\n\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ntrain['clean_text'] = train['question_text'].apply(str).apply(lambda x: clean_text(x))\ntest['clean_text'] = test['question_text'].apply(str).apply(lambda x: clean_text(x))\n\nkfold = 5\ntrain['kfold'] = train.index % kfold\n\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\nmax_len = 30\n\nclass BertDataSet(Dataset):\n    \n    def __init__(self, sentences, toxic_labels):\n        self.sentences = sentences\n        self.targets = toxic_labels.to_numpy()\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_senten = tokenizer.encode_plus(sentence, \n                                            add_special_tokens = True, # [CLS],[SEP]\n                                            max_length = max_len,\n                                            pad_to_max_length = True,\n                                            truncation = True,\n                                            return_attention_mask = True\n                                             )\n        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        \n        return {\n            'ids' : ids,\n            'mask' : mask,\n            'toxic_label':toxic_label\n        }\n\nepochs = 5\ntrain_batch = 32\nvalid_batch = 32\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nloss_fn = nn.BCEWithLogitsLoss()\nloss_fn.to(device)\nscaler = torch.cuda.amp.GradScaler()\n\ndef training(train_dataloader, model, optimizer, scheduler):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in train_dataloader:\n        losses = []\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/len(p_train)\n    \n    return losses, accuracy\n\ndef validating(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/len(p_valid)\n    \n    return losses, accuracy, all_output_probs","ffa84fa3":"%%time\n\nbest_scores = []\nfor fold in tqdm(range(0,5)):\n\n    # initializing the data\n    p_train = train[train['kfold'] != fold].reset_index(drop = True)\n    p_valid = train[train['kfold'] == fold].reset_index(drop = True)\n\n    train_dataset = BertDataSet(p_train['clean_text'], p_train['target'])\n    valid_dataset = BertDataSet(p_valid['clean_text'], p_valid['target'])\n\n    train_dataloader = DataLoader(train_dataset, batch_size = train_batch, shuffle = True, num_workers = 4, pin_memory = True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size = valid_batch, shuffle = False, num_workers = 4, pin_memory = True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 1)\n    model.to(device)\n    \n    LR = 2e-5\n    optimizer = AdamW(model.parameters(), LR,betas = (0.9, 0.999), weight_decay = 1e-2) # AdamW optimizer\n\n    train_steps = int(len(p_train)\/train_batch * epochs)\n    num_steps = int(train_steps * 0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    best_score = 1000\n    train_accs = []\n    valid_accs = []\n    train_losses = []\n    valid_losses = []\n    best_valid_probs = []\n    \n    print(\"-------------- Fold = \" + str(fold) + \"-------------\")\n    \n    for epoch in tqdm(range(epochs)):\n        print(\"Epoch = \" + str(epoch))\n\n        train_loss, train_acc = training(train_dataloader, model, optimizer, scheduler)\n        valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\n\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        valid_losses.append(valid_loss)\n        valid_accs.append(valid_acc)\n        \n        print('train losses: %.4f' %(train_loss), 'train accuracy: %.3f' %(train_acc))\n        print('valid losses: %.4f' %(valid_loss), 'valid accuracy: %.3f' %(valid_acc))\n\n        if (valid_loss < best_score):\n\n            best_score = valid_loss\n            print(\"Found an improved model! :)\")\n\n            state = {'state_dict': model.state_dict(),\n                     'optimizer_dict': optimizer.state_dict(),\n                     'best_score':best_score\n                    }\n\n            torch.save(state, \"model\" + str(fold) + \".pth\")\n            best_valid_prob = valid_probs\n            torch.cuda.memory_summary(device = None, abbreviated = False)\n        else:\n            pass\n\n\n    best_scores.append(best_score)\n    best_valid_probs.append(best_valid_prob)\n    \n    ##Plotting the result for each fold\n    x = np.arange(epochs)\n    fig, ax = plt.subplots(1, 2, figsize = (15,4))\n    ax[0].plot(x, train_losses)\n    ax[0].plot(x, valid_losses)\n    ax[0].set_ylabel('Losses', weight = 'bold')\n    ax[0].set_xlabel('Epochs')\n    ax[0].grid(alpha = 0.3)\n    ax[0].legend(labels = ['train losses', 'valid losses'])\n\n    ax[1].plot(x, train_accs)\n    ax[1].plot(x, valid_accs)\n    ax[1].set_ylabel('Accuracy', weight = 'bold')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(labels = ['train acc', 'valid acc'])\n\n    ax[1].grid(alpha = 0.3)\n    fig.suptitle('Fold = '+str(fold), weight = 'bold') ","4a3051c2":"best_scores","5cbc15d6":"print('Mean of',kfold, 'folds for best loss in', epochs, 'epochs cross-validation folds is %.4f.' %(np.mean(best_scores)))","920adb06":"def predicting(test_dataloader, model, pthes):\n    allpreds = []\n    \n    for pth in pthes:\n        state = torch.load(pth)\n        model.load_state_dict(state['state_dict'])\n        model.to(device)\n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for a in test_dataloader:\n                ids = a['ids'].to(device)\n                mask = a['mask'].to(device)\n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n                output_probs = torch.sigmoid(output)\n                preds.append(output_probs.cpu().numpy())\n            preds = np.concatenate(preds)\n            allpreds.append(preds)\n      \n    return allpreds","d7af4525":"pthes = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if \".pth\" in s]\npthes","053722d5":"allpreds = predicting(valid_dataloader, model, pthes)","dc33a699":"valid_probs = np.zeros(len(p_valid))\nfor i in range(kfold):\n    valid_probs += allpreds[i]\nvalid_probs = valid_probs \/ kfold","c43ec179":"valid_probs = np.asarray(valid_probs).flatten()","367f5121":"#valid_probs = allpreds[0].flatten() #This line is used when training for one model and not k-fold model \ny_valid = p_valid['target'].to_numpy().flatten()","da4ea69d":"fpr, tpr, _ = roc_curve(y_valid, valid_probs)\nprint('auc score for kfold =', kfold, 'models is: %.2f' %(auc(fpr, tpr)*100))","0c998ade":"fig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.set_title('ROC Curv')\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\nplt.show()","c9978b2a":"class BERTinferenceDataSet(Dataset):\n    \n    def __init__(self, sentences):\n        self.sentences = sentences\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_sent = tokenizer.encode_plus(sentence, \n                                         add_special_tokens = True, #[SEP][PAD]\n                                         max_length = max_len,\n                                         pad_to_max_length = True,\n                                         truncation = True)\n\n        ids = torch.tensor(bert_sent['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_sent['attention_mask'], dtype = torch.long)\n\n        return{\n            'ids' : ids,\n            'mask' : mask\n             }","842daf03":"test_batch = 32\ntest_dataset = BERTinferenceDataSet(test['clean_text'])\ntest_dataloader = DataLoader(test_dataset, batch_size = test_batch, shuffle = False, num_workers = 4, pin_memory = True)\npthes","0eaf99c1":"allpreds = predicting(test_dataloader, model, pthes)","b4eb59eb":"allpreds[0][0]","e631b42a":"preds = np.zeros(len(test_dataset))\nfor i in range(kfold):\n    preds += allpreds[i]\npreds = preds \/ kfold","5471dc4e":"results = pd.DataFrame(preds)\nsubmission = pd.concat([test,results], axis = 1).drop(['question_text', 'clean_text'], axis = 1)\nsubmission.rename(columns = { 0:'target'}, inplace = True)\nsubmission.to_csv(\"submission.csv\", index = False)","5f2e0ce4":"L\u1ea5y trung b\u00ecnh c\u1ee7a x\u00e1c su\u1ea5t t\u1eeb k-m\u00f4 h\u00ecnh cho m\u1ed7i m\u1eabu trong t\u1eadp validation.","d6a85447":"\u0110\u1ecbnh ngh\u0129a l\u1edbp BertDataSet v\u00f3i Dataset l\u00e0 si\u00eau l\u1edbp v\u00e0 ghi \u0111\u00e8 h\u00e0m init, len v\u00e0 getitem. N\u00f3 s\u1ebd l\u1ea5y danh s\u00e1ch question v\u00e0 toxic label r\u1ed3i t\u1ea1o token ids v\u00e0 attention mask \u0111\u1ec3 ph\u00e2n bi\u1ec7t c\u00e1c c\u00e2u h\u1ecfi v\u1edbi zero padding.\n\ntorch.tensor vs np.ndarray:\nN\u1ebfu ch\u1ec9 quan t\u00e2m \u0111\u1ebfn c\u00e1ch hi\u1ec7u qu\u1ea3 v\u00e0 d\u1ec5 d\u00e0ng \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c ph\u00e9p to\u00e1n tr\u00ean ma tr\u1eadn, np.ndarray ho\u1eb7c torch.tensor c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng thay th\u1ebf cho nhau.\n\nTuy nhi\u00ean, torch.tensors \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 s\u1eed d\u1ee5ng trong b\u1ed1i c\u1ea3nh t\u1ed1i \u01b0u h\u00f3a \u0111\u1ed9 d\u1ed1c gradient, v\u00e0 do \u0111\u00f3 ch\u00fang kh\u00f4ng ch\u1ec9 gi\u1eef m\u1ed9t tensor v\u1edbi c\u00e1c gi\u00e1 tr\u1ecb s\u1ed1, m\u00e0 (quan tr\u1ecdng h\u01a1n) bi\u1ec3u \u0111\u1ed3 t\u00ednh to\u00e1n d\u1eabn \u0111\u1ebfn c\u00e1c gi\u00e1 tr\u1ecb n\u00e0y. Sau \u0111\u00f3, \u0111\u1ed3 th\u1ecb t\u00ednh to\u00e1n n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng (s\u1eed d\u1ee5ng quy t\u1eafc chu\u1ed7i c\u1ee7a \u0111\u1ea1o h\u00e0m) \u0111\u1ec3 t\u00ednh \u0111\u1ea1o h\u00e0m c\u1ee7a h\u00e0m t\u1ed5n th\u1ea5t w.r.t t\u1eebng bi\u1ebfn \u0111\u1ed9c l\u1eadp \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u00ednh to\u00e1n t\u1ed5n th\u1ea5t.\n\n\u0110\u1ed1i t\u01b0\u1ee3ng np.ndarray kh\u00f4ng c\u00f3 th\u00eam l\u1edbp \"\u0111\u1ed3 th\u1ecb t\u00ednh to\u00e1n\" n\u00e0y v\u00e0 do \u0111\u00f3, khi chuy\u1ec3n \u0111\u1ed5i torch.tensor th\u00e0nh np.ndarray, ta ph\u1ea3i x\u00f3a r\u00f5 r\u00e0ng \u0111\u1ed3 th\u1ecb t\u00ednh to\u00e1n c\u1ee7a tensor b\u1eb1ng l\u1ec7nh detach ().","72ca7751":"S\u1eed d\u1ee5ng 20% d\u1eef li\u1ec7u training l\u00e0m b\u1ed9 validation.","a5348f2f":"# \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh","5480e15c":"Xem x\u00e9t s\u1ef1 ph\u00e2n b\u1ed1 \u0111\u1ed9 d\u00e0i cho c\u00e1c question_text \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a \u0111\u1ec3 l\u1ef1a ch\u1ecdn max_length cho c\u00e1c d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a c\u1ee7a ch\u00fang ta.","abf7998a":"## H\u00e0m validating","05bcbdfa":"Ki\u1ec3m tra th\u00f4ng tin kh\u00f4ng x\u00e1c \u0111\u1ecbnh","63762a12":"### D\u1ecdn d\u1eb9p d\u1eef li\u1ec7u:\nCh\u00fang ta c\u1ea7n d\u1ecdn d\u1eb9p, m\u00e3 h\u00f3a v\u00e0 chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u sang tensor.\nH\u00e0m b\u00ean d\u01b0\u1edbi gi\u00fap ch\u00fang ta th\u1ef1c hi\u1ec7n:\n- X\u00f3a si\u00eau li\u00ean k\u1ebft, d\u1ea5u ch\u1ea5m c\u00e2u v\u00e0 s\u1ed1.\n- M\u00e3 h\u00f3a.\n- B\u1ecf qua vi\u1ec7c thay \u0111\u1ed5i t\u1ea5t c\u1ea3 th\u00e0nh ch\u1eef th\u01b0\u1eddng v\u00e0 gi\u1eef cho c\u00e1c ch\u1eef c\u00e1i ph\u00f9 h\u1ee3p v\u1edbi t\u1eebng tr\u01b0\u1eddng h\u1ee3p. \"BAD !!\" >> \"bad !!\".\n- Kh\u00f4ng x\u00f3a c\u00e1c t\u1eeb d\u1eebng, V\u00ec v\u1edbi c\u00e1c m\u00f4 h\u00ecnh ng\u1eef c\u1ea3nh nh\u01b0 BERT v\u00e0 ROBERTA, t\u1ed1t h\u01a1n h\u1ebft l\u00e0 (h\u1ea7u nh\u01b0) kh\u00f4ng x\u1eed l\u00fd c\u00e1c v\u0103n b\u1ea3n x\u00f3a t\u1eeb d\u1eebng. C\u00e1c m\u00f4 h\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c \u0111\u00e0o t\u1ea1o tr\u01b0\u1edbc v\u1edbi c\u00e1c t\u1eeb d\u1eebng: n\u1ebfu lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb d\u1eebng, m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 m\u1ea5t ng\u1eef c\u1ea3nh. \u0110i\u1ec1u n\u00e0y c\u0169ng \u0111\u00fang v\u1edbi c\u00e1c k\u1ef9 thu\u1eadt ti\u1ec1n x\u1eed l\u00fd g\u1ed1c v\u00e0 lemmatization. V\u00ec v\u1eady, ta c\u0169ng b\u1ecf qua ch\u00fang!","b207c935":"\u0110\u1ec3 c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh, ta l\u1eb7p l\u1ea1i quy tr\u00ecnh training t\u01b0\u01a1ng t\u1ef1 cho m\u1ed7i l\u1ea7n g\u1ea5p k-folds\n# L\u1eb7p l\u1ea1i training cho K-fold","4bcefe35":"# Train m\u00f4 h\u00ecnh","ee0a22f9":"### K\u1ebft qu\u1ea3 kh\u1ea3o s\u00e1t:\n- Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u kh\u00f4ng x\u00e1c \u0111\u1ecbnh (null).\n- T\u1eadp train c\u00f3 1306122 gi\u00e1 tr\u1ecb, t\u1eadp test c\u00f3 375806 gi\u00e1 tr\u1ecb.\n- D\u1eef li\u1ec7u target = 1 c\u00f3 80810 gi\u00e1 tr\u1ecb, chi\u1ebfm kho\u1ea3ng 6.2% t\u1eadp train.","ab6b30e0":"Ki\u1ec3m tra s\u1ed1 l\u01b0\u1ee3ng target = 1","e011b3ca":"T\u1ec9 l\u1ec7 tr\u1ef1c quan","d92e2d06":"## Chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u","b19f863c":"\u0110\u1eb7t train_batch v\u00e0 valid_batch l\u00e0 32. K\u00edch th\u01b0\u1edbc batch l\u1edbn h\u01a1n gi\u00fap t\u0103ng t\u1ed1c \u0111\u1ed9 t\u00ednh to\u00e1n. Tuy nhi\u00ean, k\u1ebft qu\u1ea3 x\u00e1c nh\u1eadn r\u1eb1ng vi\u1ec7c s\u1eed d\u1ee5ng k\u00edch th\u01b0\u1edbc batch nh\u1ecf \u0111\u1ea1t \u0111\u01b0\u1ee3c hi\u1ec7u su\u1ea5t t\u1ed5ng qu\u00e1t h\u00f3a t\u1ed1t nh\u1ea5t, v\u1edbi m\u1ed9t chi ph\u00ed t\u00ednh to\u00e1n nh\u1ea5t \u0111\u1ecbnh. Trong m\u1ecdi tr\u01b0\u1eddng h\u1ee3p, k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t \u0111\u00e3 thu \u0111\u01b0\u1ee3c v\u1edbi k\u00edch th\u01b0\u1edbc batch l\u00e0 32 ho\u1eb7c nh\u1ecf h\u01a1n. Th\u01b0\u1eddng k\u00edch th\u01b0\u1edbc bacth nh\u1ecf nh\u01b0 2 ho\u1eb7c 4 mang l\u1ea1i k\u1ebft qu\u1ea3 t\u1ed1i \u01b0u.","b067a0f5":"# \u0110\u00e1nh gi\u00e1 cho k m\u00f4 h\u00ecnh\nTa s\u1eed d\u1ee5ng t\u1eadp h\u1ee3p \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 k m\u00f4 h\u00ecnh trong validation set","fbc9e5cc":"## T\u1ed5ng h\u1ee3p v\u00e0 kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u","d5914f3c":"# Chu\u1ea9n b\u1ecb m\u00f4 h\u00ecnh training v\u00e0 validating","9633045c":"**amp**: automatic mixed precision.\n\nc\u00e2u l\u1ec7nh **with** \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong x\u1eed l\u00fd ngo\u1ea1i l\u1ec7 \u0111\u1ec3 l\u00e0m cho m\u00e3 s\u1ea1ch h\u01a1n v\u00e0 d\u1ec5 \u0111\u1ecdc h\u01a1n nhi\u1ec1u. N\u00f3 \u0111\u01a1n gi\u1ea3n h\u00f3a vi\u1ec7c qu\u1ea3n l\u00fd c\u00e1c t\u00e0i nguy\u00ean chung nh\u01b0 c\u00e1c lu\u1ed3ng t\u1ec7p.\n\n**Optimizer.step()**: th\u1ef1c hi\u1ec7n c\u1eadp nh\u1eadt tham s\u1ed1 d\u1ef1a tr\u00ean gradient hi\u1ec7n t\u1ea1i (\u0111\u01b0\u1ee3c l\u01b0u tr\u1eef trong thu\u1ed9c t\u00ednh .grad c\u1ee7a tham s\u1ed1) v\u00e0 quy t\u1eafc c\u1eadp nh\u1eadt.\n\n**Loss.backward()** g\u1ecdi .backward() nhi\u1ec1u l\u1ea7n t\u00edch l\u0169y gradient (b\u1eb1ng c\u00e1ch c\u1ed9ng) cho m\u1ed7i tham s\u1ed1. \u0110\u00e2y l\u00e0 l\u00fd do t\u1ea1i sao ta n\u00ean g\u1ecdi Optimizer.zero_grad() sau m\u1ed7i l\u1ea7n g\u1ecdi .step().","70ac25fe":"Ta s\u1eed d\u1ee5ng auc \u0111\u1ec3 \u0111\u00e1nh gi\u00e1.","3cd086d1":"**B\u00c1O C\u00c1O B\u00c0I T\u1eacP L\u1edaN H\u1eccC M\u00c1Y CU\u1ed0I K\u1ef2**\n\nM\u00e3 L\u1edbp: INT3405_1\n\nH\u1ecd v\u00e0 t\u00ean: Nguy\u1ec5n Huy Ho\u00e0n\n\nM\u00e3 s\u1ed1 sinh vi\u00ean:18020532\n\n# M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n\nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c, nh\u1eefng ng\u01b0\u1eddi \u0111\u00f3ng g\u00f3p th\u00f4ng tin chi ti\u1ebft \u0111\u1ed9c \u0111\u00e1o v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng.V\u00ec l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng m\u1edf ai c\u0169ng c\u00f3 th\u1ec3 \u0111\u1ecdc v\u00e0 h\u1ecfi \u0111\u00e1p m\u1ed9t c\u00e1ch d\u1ec5 d\u00e0ng n\u00ean s\u1ebd c\u00f3 nh\u1eefng ng\u01b0\u1eddi \u0111\u01b0a ra nh\u1eefng \u0111\u1ecbnh ki\u1ebfn, nh\u1eefng c\u00e2u h\u1ecfi mang t\u00ednh \u0111\u1ed9c h\u1ea1i chia r\u1ebd.\n\nTh\u00e1ch th\u1ee9c \u1edf \u0111\u00e2y l\u00e0 l\u00e0m sao lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch, qua \u0111\u00f3 c\u1ea3i thi\u1ec7n n\u1ec1n t\u1ea3ng \u0111\u1ec3 n\u00f3 th\u00e0nh 1 n\u01a1i m\u00e0 m\u1ecdi ng\u01b0\u1eddi d\u1ec5 d\u00e0ng trao \u0111\u1ed5i h\u01a1n kh\u00f4ng b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nh\u1eefng toxic question nh\u01b0 v\u1eady n\u1eefa\n\ninput: c\u00e2u h\u1ecfi d\u1ea1ng text\n\noutput: 1\/0(kh\u00f4ng ch\u00e2n th\u00e0nh\/ ch\u00e2n th\u00e0nh)\n\nPh\u1ea7n 1: C\u00e1c b\u01b0\u1edbc kh\u1ea3o s\u00e1t d\u1eef li\u00eau, x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh.\n\nPh\u1ea7n 2: Clean code v\u1edbi k-fold = 5.","18648ee3":"# D\u1eef li\u1ec7u\n## Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u","1aec0486":"pin_memory = True trong DataLoader s\u1ebd t\u1ef1 \u0111\u1ed9ng \u0111\u1eb7t Tensors d\u1eef li\u1ec7u \u0111\u00e3 t\u00ecm n\u1ea1p v\u00e0o b\u1ed9 nh\u1edb \u0111\u01b0\u1ee3c ghim, v\u00e0 do \u0111\u00f3 cho ph\u00e9p truy\u1ec1n d\u1eef li\u1ec7u nhanh h\u01a1n \u0111\u1ebfn c\u00e1c GPU h\u1ed7 tr\u1ee3 CUDA. \u0110i\u1ec1u n\u00e0y \u0111\u01b0\u1ee3c gi\u1ea3i th\u00edch r\u00f5 nh\u1ea5t trong b\u00e0i \u0111\u0103ng tr\u00ean blog c\u1ee7a NVIDIA.\n![t\u1ea3i xu\u1ed1ng.png](attachment:8ded059c-d47a-425f-b942-ba0a7c5e8bd0.png)","95fc6662":"## C\u00e0i \u0111\u1eb7t tham s\u1ed1","4fea884e":"## H\u00e0m training","417bae10":"# Ph\u1ea7n 2","4aba5329":"\u0110\u1ec3 gi\u1ea3m th\u1eddi gian train, ta ch\u1ec9 s\u1eed d\u1ee5ng 50000 gi\u00e1 tr\u1ecb","b55ebd75":"Ta s\u1eed d\u1ee5ng tham s\u1ed1 learning rate nh\u01b0 \u0111\u00e3 thi\u1ebft l\u1eadp \u1edf tr\u00ean cho 10% t\u1ed5ng th\u1eddi gian training. Sau \u0111\u00f3, gi\u1ea3m d\u1ea7n learning rate v\u1ec1 kh\u00f4ng.","3ad28ac6":"# M\u00f4 h\u00ecnh BERT","f4202add":"BERT l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a c\u1ee5m t\u1eeb **Bidirectional Encoder Representation from Transformer** c\u00f3 ngh\u0129a l\u00e0 m\u00f4 h\u00ecnh bi\u1ec3u di\u1ec5n t\u1eeb theo 2 chi\u1ec1u \u1ee9ng d\u1ee5ng k\u1ef9 thu\u1eadt Transformer. BERT \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 hu\u1ea5n luy\u1ec7n tr\u01b0\u1edbc c\u00e1c bi\u1ec3u di\u1ec5n t\u1eeb (pre-train word embedding). \u0110i\u1ec3m \u0111\u1eb7c bi\u1ec7t \u1edf BERT \u0111\u00f3 l\u00e0 n\u00f3 c\u00f3 th\u1ec3 \u0111i\u1ec1u h\u00f2a c\u00e2n b\u1eb1ng b\u1ed1i c\u1ea3nh theo c\u1ea3 2 chi\u1ec1u tr\u00e1i v\u00e0 ph\u1ea3i.\n\nC\u01a1 ch\u1ebf attention c\u1ee7a Transformer s\u1ebd truy\u1ec1n to\u00e0n b\u1ed9 c\u00e1c t\u1eeb trong c\u00e2u v\u0103n \u0111\u1ed3ng th\u1eddi v\u00e0o m\u00f4 h\u00ecnh m\u1ed9t l\u00fac m\u00e0 kh\u00f4ng c\u1ea7n quan t\u00e2m \u0111\u1ebfn chi\u1ec1u c\u1ee7a c\u00e2u. Do \u0111\u00f3 Transformer \u0111\u01b0\u1ee3c xem nh\u01b0 l\u00e0 hu\u1ea5n luy\u1ec7n hai chi\u1ec1u (bidirectional) m\u1eb7c d\u00f9 tr\u00ean th\u1ef1c t\u1ebf ch\u00ednh x\u00e1c h\u01a1n ch\u00fang ta c\u00f3 th\u1ec3 n\u00f3i r\u1eb1ng \u0111\u00f3 l\u00e0 hu\u1ea5n luy\u1ec7n kh\u00f4ng chi\u1ec1u (non-directional). \u0110\u1eb7c \u0111i\u1ec3m n\u00e0y cho ph\u00e9p m\u00f4 h\u00ecnh h\u1ecdc \u0111\u01b0\u1ee3c b\u1ed1i c\u1ea3nh c\u1ee7a t\u1eeb d\u1ef1a tr\u00ean to\u00e0n b\u1ed9 c\u00e1c t\u1eeb xung quanh n\u00f3 bao g\u1ed3m c\u1ea3 t\u1eeb b\u00ean tr\u00e1i v\u00e0 t\u1eeb b\u00ean ph\u1ea3i.\n\n![image.png](attachment:357c2d29-5fdc-4b91-ab07-8ba4e2832778.png)\n\nTo\u00e0n b\u1ed9 ti\u1ebfn tr\u00ecnh pre-training v\u00e0 fine-tuning c\u1ee7a BERT. M\u1ed9t ki\u1ebfn tr\u00fac t\u01b0\u01a1ng t\u1ef1 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng cho c\u1ea3 pretrain-model v\u00e0 fine-tuning model. Ch\u00fang ta s\u1eed d\u1ee5ng c\u00f9ng m\u1ed9t tham s\u1ed1 pretrain \u0111\u1ec3 kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh cho c\u00e1c t\u00e1c v\u1ee5 down stream kh\u00e1c nhau. Trong su\u1ed1t qu\u00e1 tr\u00ecnh fine-tuning th\u00ec to\u00e0n b\u1ed9 c\u00e1c tham s\u1ed1 c\u1ee7a layers h\u1ecdc chuy\u1ec3n giao s\u1ebd \u0111\u01b0\u1ee3c fine-tune. \u0110\u1ed1i v\u1edbi c\u00e1c t\u00e1c v\u1ee5 s\u1eed d\u1ee5ng input l\u00e0 m\u1ed9t c\u1eb7p sequence (pair-sequence) v\u00ed d\u1ee5 nh\u01b0 question and answering th\u00ec ta s\u1ebd th\u00eam token kh\u1edfi t\u1ea1o l\u00e0 [CLS] \u1edf \u0111\u1ea7u c\u00e2u, token [SEP] \u1edf gi\u1eefa \u0111\u1ec3 ng\u0103n c\u00e1ch 2 c\u00e2u.\n\nTi\u1ebfn tr\u00ecnh \u00e1p d\u1ee5ng fine-tuning s\u1ebd nh\u01b0 sau:\n\nB\u01b0\u1edbc 1: Embedding to\u00e0n b\u1ed9 c\u00e1c token c\u1ee7a c\u1eb7p c\u00e2u b\u1eb1ng c\u00e1c v\u00e9c t\u01a1 nh\u00fang t\u1eeb pretrain model. C\u00e1c token embedding bao g\u1ed3m c\u1ea3 2 token l\u00e0 [CLS] v\u00e0 [SEP] \u0111\u1ec3 \u0111\u00e1nh d\u1ea5u v\u1ecb tr\u00ed b\u1eaft \u0111\u1ea7u c\u1ee7a c\u00e2u h\u1ecfi v\u00e0 v\u1ecb tr\u00ed ng\u0103n c\u00e1ch gi\u1eefa 2 c\u00e2u. 2 token n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o \u1edf output \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh c\u00e1c ph\u1ea7n Start\/End Spand c\u1ee7a c\u00e2u output.\n\nB\u01b0\u1edbc 2: C\u00e1c embedding v\u00e9c t\u01a1 sau \u0111\u00f3 s\u1ebd \u0111\u01b0\u1ee3c truy\u1ec1n v\u00e0o ki\u1ebfn tr\u00fac multi-head attention v\u1edbi nhi\u1ec1u block code (th\u01b0\u1eddng l\u00e0 6, 12 ho\u1eb7c 24 blocks t\u00f9y theo ki\u1ebfn tr\u00fac BERT). Ta thu \u0111\u01b0\u1ee3c m\u1ed9t v\u00e9c t\u01a1 output \u1edf encoder.\n\nB\u01b0\u1edbc 3: \u0110\u1ec3 d\u1ef1 b\u00e1o ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t cho t\u1eebng v\u1ecb tr\u00ed t\u1eeb \u1edf decoder, \u1edf m\u1ed7i time step ch\u00fang ta s\u1ebd truy\u1ec1n v\u00e0o decoder v\u00e9c t\u01a1 output c\u1ee7a encoder v\u00e0 v\u00e9c t\u01a1 embedding input c\u1ee7a decoder \u0111\u1ec3 t\u00ednh encoder-decoder attention (c\u1ee5 th\u1ec3 v\u1ec1 encoder-decoder attention l\u00e0 g\u00ec c\u00e1c b\u1ea1n xem l\u1ea1i m\u1ee5c 2.1.1). Sau \u0111\u00f3 projection qua liner layer v\u00e0 softmax \u0111\u1ec3 thu \u0111\u01b0\u1ee3c ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t cho output t\u01b0\u01a1ng \u1ee9ng \u1edf time step .\n\nB\u01b0\u1edbc 4: Trong k\u1ebft qu\u1ea3 tr\u1ea3 ra \u1edf output c\u1ee7a transformer ta s\u1ebd c\u1ed1 \u0111\u1ecbnh k\u1ebft qu\u1ea3 c\u1ee7a c\u00e2u Question sao cho tr\u00f9ng v\u1edbi c\u00e2u Question \u1edf input. C\u00e1c v\u1ecb tr\u00ed c\u00f2n l\u1ea1i s\u1ebd l\u00e0 th\u00e0nh ph\u1ea7n m\u1edf r\u1ed9ng Start\/End Span t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u00e2u tr\u1ea3 l\u1eddi t\u00ecm \u0111\u01b0\u1ee3c t\u1eeb c\u00e2u input.","de4d051a":"# Ph\u1ea7n 1","f7f208d1":"M\u00f4 h\u00ecnh s\u1eed d\u1ee5ng id v\u00e0 mask t\u1eeb token encoding trong t\u1eadp train.","c2a05a21":"# Th\u01b0 vi\u1ec7n","b940b3c3":"H\u00e0m validating kh\u00e1 gi\u1ed1ng v\u1edbi h\u00e0m training. S\u1ef1 kh\u00e1c bi\u1ec7t l\u00e0 kh\u00f4ng c\u00f3 s\u1ef1 lan truy\u1ec1n ng\u01b0\u1ee3c v\u00e0 t\u1ed1i \u01b0u h\u00f3a cho c\u00e1c tham s\u1ed1 trong \u0111\u00f3."}}