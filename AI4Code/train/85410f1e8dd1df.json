{"cell_type":{"75268000":"code","5404f559":"code","f18e48bb":"code","4713b73f":"code","7750ec6e":"code","54624060":"code","5ff15b29":"code","3dbc083e":"code","dc06c803":"code","27ed40e4":"code","9d2eb867":"code","29b2f3ea":"code","13867e22":"code","66d58de3":"code","1fe4127f":"code","69ad2da1":"code","62153267":"code","7f2f1354":"code","f3bbf044":"code","57094894":"code","d897c875":"code","efc1525b":"code","0548f279":"code","c207cdc2":"code","76ba99b3":"code","288339aa":"code","355fcc1f":"code","2ce4921c":"code","fec711d5":"code","d30b55e1":"code","a97f238c":"code","2740e749":"code","535901f8":"markdown","49cd938d":"markdown","203fe7ca":"markdown","0d09aaff":"markdown","420df510":"markdown","e79d3d5d":"markdown","2b576b51":"markdown","ecea1966":"markdown","b5315a90":"markdown","c9d53942":"markdown","19aa03e5":"markdown","53986c75":"markdown","a2acd7b3":"markdown","d4d08d50":"markdown","0c4759a6":"markdown","8abcaaf1":"markdown","71cea4e9":"markdown","294b475e":"markdown","40d954c4":"markdown","7f561653":"markdown","b2d2ad4d":"markdown","e7b56dc7":"markdown","784851f5":"markdown","c392375b":"markdown","622a81a2":"markdown","e3c8d582":"markdown","71422c61":"markdown","b321cd3e":"markdown","ac988381":"markdown","248cfda7":"markdown","909b1808":"markdown"},"source":{"75268000":"# Built-in packages\nimport json\nimport warnings\nimport re\nwarnings.filterwarnings(\"ignore\")\n\n# Third party packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import model_selection\n\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n\nsns.set(context= \"notebook\", color_codes=True)\nplt.style.use('bmh')\n\n%matplotlib inline\npd.set_option('display.max_columns', None)","5404f559":"df = pd.read_csv(\"\/kaggle\/input\/regression-with-neural-networking\/concrete_data.csv\")\ndf.columns = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarse_agg', 'fine_agg', 'age', 'strength']\ndf.head()","f18e48bb":"print(f\"The shape of the DataFrame is: {df.shape}, which means there are {df.shape[0]} rows and {df.shape[1]} columns.\")","4713b73f":"df.info()","7750ec6e":"df_summary = df.describe()\ndf_summary","54624060":"fig, axes = plt.subplots(3, 3, figsize = (20,20));\ncol_cnt = 0\ncol_names = df.columns.tolist()\nfor r in range(3):\n    for c in range(3):\n        try:\n            sns.boxplot(df[col_names[col_cnt]], ax=axes[r][c], orient=\"h\");\n            col_cnt += 1\n        except:\n            pass","5ff15b29":"outlier_cols = [\"slag\", \"water\", \"superplasticizer\", \"fine_agg\", \"age\"]\ndef cap_outliers(df, col):\n    q1 = df_summary[col].loc[\"25%\"]\n    q3 = df_summary[col].loc[\"75%\"]\n    iqr = q3 - q1\n    lower_bound = q1 - (1.5 * iqr)\n    upper_bound = q3 + (1.5 * iqr)\n    df.loc[df[col] < lower_bound, col] = df_summary[col].loc[\"mean\"]\n    df.loc[df[col] > upper_bound, col] = df_summary[col].loc[\"mean\"]\n    return df\n    \n    \nfor col in outlier_cols:\n    df = cap_outliers(df, col)","3dbc083e":"sns.pairplot(df, diag_kind=\"kde\");","dc06c803":"fig, axes = plt.subplots(1, 3, figsize = (20,10));\n\nsns.scatterplot(y=\"slag\", x=\"cement\", data=df, ax=axes[0], alpha=0.5);\nsns.scatterplot(y=\"flyash\", x=\"cement\", data=df, ax=axes[1], alpha=0.5);\nsns.scatterplot(y=\"superplasticizer\", x=\"cement\", data=df, ax=axes[2], alpha=0.5);","27ed40e4":"def impute_values(alg, df, a, cols):\n    df[a] = df[a].replace({0: np.nan})\n    df_notnull = df[df[a].notna()]\n    \n    X = df_notnull[cols]\n    y = df_notnull[a]\n    \n    df_null = df[df[a].isna()]\n    test_X = df_null[cols]\n\n    regr = alg\n    regr.fit(X, y)\n\n    pred = regr.predict(test_X)\n    df_null = df[df[a].isna()]\n    df_null[a] = pred\n    \n    xx = df[df[a].notna()]\n    xx[\"null\"] = 0\n    df_null[\"null\"] = 1\n    df = pd.concat([xx, df_null], axis=0)\n    cols.append(a)\n    return df","9d2eb867":"for col, params in {\"flyash\": [\"cement\", \"water\", \"fine_agg\"], \"slag\": [\"cement\", \"coarse_agg\", \"fine_agg\"], \"superplasticizer\" : [\"cement\", \"water\", \"coarse_agg\"]}.items():\n    alg = LinearRegression()\n    df = impute_values(alg, df, col, params)\n    \nfig, axes = plt.subplots(1, 3, figsize = (20,10));\n\nsns.scatterplot(y=\"slag\", x=\"cement\", data=df, hue=\"null\", ax=axes[0], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nsns.scatterplot(y=\"flyash\", x=\"cement\", data=df, hue=\"null\", ax=axes[1], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nsns.scatterplot(y=\"superplasticizer\", x=\"cement\", data=df, hue=\"null\", ax=axes[2], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nplt.show()","29b2f3ea":"df.drop(columns=[\"null\"], inplace=True)","13867e22":"sns.jointplot(x=\"flyash\", y=\"slag\", data=df[[\"flyash\", \"slag\"]], kind=\"reg\");","66d58de3":"df_copy = df.copy()\ndf_copy[\"cement_water_ratio\"] = df_copy[\"cement\"]\/df_copy[\"water\"]\ndf_copy[\"average_agg\"] = (df_copy[\"coarse_agg\"] + df_copy[\"fine_agg\"])\/2\ndf_copy.drop(columns=[\"cement\", \"water\", \"coarse_agg\", \"fine_agg\"], inplace=True)","1fe4127f":"target_col = [\"strength\"]\ncol_names = df_copy.columns.to_list()\ncol_names.remove(target_col[0])","69ad2da1":"plt.figure(figsize=(10,7))\nsns.heatmap(df_copy[col_names + target_col].corr(), fmt=\".2g\", annot=True);","62153267":"g = sns.PairGrid(df_copy, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=15)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=2)","7f2f1354":"sns.jointplot(x=\"strength\", y=\"age\", data=df_copy[[\"age\", \"strength\"]], kind=\"kde\");","f3bbf044":"# Prepare data for training\n\nX = df_copy[col_names]    # Contains the independent columns \ny = df_copy[target_col]     # Our target column\n\nseed = 16\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = seed)\ntrain_X_poly, train_y_poly, test_X_poly, test_y_poly = (None,) * 4\ntrain_y = train_y[target_col[0]]\ntest_y = test_y[target_col[0]]","57094894":"evaluation = {\"Metrics\" : ['Root Mean Squared Error (RMSE)', 'Mean Absolute Error (MAE)', 'Mean Squared Error (MSE)', 'R2 score', 'CV-mean']}\ndef evaluate_model(name, train_X, train_y, test_X, test_y, types, alg, plot=True):\n    global evaluation, pred\n    \n    alg.fit(train_X, train_y)\n    print(f\"Score: {alg.score(test_X, test_y)}\")\n    if plot:\n        fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n        try:\n            if types == \"Coefs\":\n                print(f\"Intercept: {alg.intercept_}\")\n                try:\n                    coefs = pd.DataFrame({\"coefs\" : alg.coef_, \"col\" : col_names})\n                except:\n                    coefs = pd.DataFrame({\"coefs\" : alg.coef_[0], \"col\" : col_names})\n                sns.barplot(x=\"col\", y=\"coefs\", data=coefs, ax=axes[1]);\n            else:\n                features = pd.DataFrame({\"features\" : alg.feature_importances_, \"col\" : col_names})\n                sns.barplot(x=\"col\", y=\"features\", data=features, ax=axes[1]);\n        except:\n            pass\n    else:\n        plt.figure(figsize=(20,5));\n        axes = [None]\n\n    pred = alg.predict(test_X)\n    rmsecm = np.sqrt(metrics.mean_squared_error(test_y,pred))\n    mae = mean_absolute_error(test_y, pred)\n    mse = mean_squared_error(test_y, pred)\n    r2 = r2_score(test_y, pred)\n\n    p = pd.DataFrame(pred, columns=[0])\n    p[\"Type\"] = \"Predictions\"\n    p[\"n\"] = list(range(p.shape[0]))\n    t = test_y.copy()\n    t = t.reset_index().set_index(\"index\")\n    t.columns = [0]\n    t[\"Type\"] = \"Actual\"\n    t = t[t[0] != \"Actual\"]\n    t[\"n\"] = list(range(p.shape[0]))\n    x = pd.concat([p,t], axis=0).reset_index()\n    sns.lineplot(x=\"n\", y=0, hue=\"Type\", data=x, markers=[\"o\", \"o\"], style=\"Type\", ax=axes[0]);\n    \n    cv = cross_val_score(alg, X, y, cv=10)\n    cv_mean = cv.ravel().mean()\n    \n    evaluation[name] = [rmsecm, mae, mse, r2, cv_mean]\n    df_ev = pd.DataFrame(evaluation)\n    plt.show()\n    return df_ev, cv","d897c875":"lr = linear_model.LinearRegression()\nevaluation, cv_scores = evaluate_model(\"Multiple Regression\", train_X, train_y, test_X, test_y, \"Coefs\", lr)\nevaluation.set_index(\"Metrics\")","efc1525b":"print(cv_scores)\nsns.distplot(cv_scores);\nprint(f\"Average score: {round(cv_scores.mean(),2)*100}%\")","0548f279":"rg = linear_model.Ridge()\nevaluation, cv_scores = evaluate_model(\"Ridge\", train_X, train_y, test_X, test_y, \"Coefs\", rg)\nevaluation.set_index(\"Metrics\")","c207cdc2":"# Fitting Polynomial Regression to the dataset\n\npoly_reg = PolynomialFeatures(degree=3)\nX_poly = poly_reg.fit_transform(X)\n\ntrain_X_poly, test_X_poly, train_y_poly, test_y_poly = train_test_split(X_poly, y, test_size = 0.3, random_state = seed)\ntrain_y_poly = train_y_poly[target_col[0]]\ntest_y_poly = test_y_poly[target_col[0]]\n\nplr = LinearRegression()\nevaluation, cv_scores = evaluate_model(\"Polynomial Regression\", train_X_poly, train_y_poly, test_X_poly, test_y_poly,  \"Coefs\", plr, plot=False)\nevaluation.set_index(\"Metrics\")","76ba99b3":"print(cv_scores)\nsns.distplot(cv_scores);\nprint(f\"Average score: {round(cv_scores.mean(),2)*100}%\")","288339aa":"svr = SVR(C=10, kernel=\"linear\")\nevaluation, cv_scores = evaluate_model(\"Support Vector Regression\", train_X, train_y, test_X, test_y, \"Coefs\", svr)\nevaluation.set_index(\"Metrics\")","355fcc1f":"knn = KNeighborsRegressor(n_neighbors=4, metric=\"manhattan\", weights=\"distance\")\nevaluation, cv_scores = evaluate_model(\"KNN\", train_X, train_y, test_X, test_y, \"Coefs\", knn, plot=False)\nevaluation.set_index(\"Metrics\")","2ce4921c":"dtr = DecisionTreeRegressor()\nevaluation, cv_scores = evaluate_model(\"Decision Tree Regression\", train_X, train_y, test_X, test_y, \"Features\", dtr)\nevaluation.set_index(\"Metrics\")","fec711d5":"rfr = RandomForestRegressor()\nevaluation, cv_scores = evaluate_model(\"Random Forest Regression\", train_X, train_y, test_X, test_y, \"Features\", rfr)\nevaluation.set_index(\"Metrics\")","d30b55e1":"xgc = XGBRegressor()\nevaluation, cv_scores = evaluate_model(\"XGBoost\", train_X, train_y, test_X, test_y, \"Features\", xgc)\nevaluation.set_index(\"Metrics\")","a97f238c":"model = XGBRegressor(n_jobs=4)\n\nparameters = {\n    'n_estimators': [50, 100, 500],\n    'max_depth': [2, 4, 6, 8, 10],\n    'gamma': [0.001, 0.01],\n    'learning_rate': [0.01, 0.1],\n    'booster': ['gbtree']\n}\n\ngrid_obj = GridSearchCV(\n    estimator=xgc,\n    param_grid=parameters\n)\n\ngrid_obj.fit(X, y)\n\nbest_model = grid_obj.best_estimator_\nprint(best_model)","2740e749":"xgc = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=5,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=500, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nevaluation, cv_scores =  evaluate_model(\"XGBoost Tuned\", train_X, train_y, test_X, test_y, \"Features\", xgc)\nevaluation.set_index(\"Metrics\")","535901f8":"### Multiple linear regression","49cd938d":"## Create the models","203fe7ca":"**cement**$~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**slag**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**flyash**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**water**$~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**superplasticizer**$~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**coarse_agg**$~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**fine_agg**$~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**age**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- day (1~365)\n____\n**strength**$~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____","0d09aaff":"## Multivariate analysis","420df510":"## Decide on complexity of the model\n\n### Polynomial regression","e79d3d5d":"The columns **slag**, **ash** and **superplasticizer** have zero values which could be thought of as missing values and fill them using regression.","2b576b51":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**.\n\n___________","ecea1966":"# Feature engineering","b5315a90":"## Support Vector Regressor","c9d53942":"The column **cement_water_ratio** has the highest coefficient value than the rest of the features which means it has the highest weight in the prediction of the **strength** column.\n\n___________","19aa03e5":"### Ridge regressor","53986c75":"### Perform necessary imputation","a2acd7b3":"There are 8 columns with float datatype and one column with int. There seem to be no null values in any of the columns","d4d08d50":"## Check for outliers","0c4759a6":"### Creating composite features\n\nWe can create 2 composite features using **cement** - **water** and **coarse_agg** - **fine_agg**","8abcaaf1":"For some of the data points in the above plot, there seems to be a direct correlation between the amount of slag and flyash with a slope of almost equal to 0.8 which is quite high. Amount of slag increases with increase in flyash.","71cea4e9":"* It looks like our composite feature **cement_water_ratio** and **flyash** are negatively correlated. \n* The target column **strength** has good amount of correlation with **age** and **cement_water_ratio**.","294b475e":"## Read the data as a data frame","40d954c4":"#### Let's make use of GridSearchCV to find the right set of parameters for our model","7f561653":"The column **cement_water_ratio** has the highest coefficient value than the rest of the features which means it has the highest weight in the prediction of the **strength** column.\n\n___________","b2d2ad4d":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**. Though **age** has almost half the importance as **cement_water_ratio**.\n\n___________","e7b56dc7":"##### Looking at the cross validation scores of both Multiple linear regressor and Polynomial regressor with parameter of higher degrees, they are exactly the same so we don't really need a complex model. Also, looking at the coefficients, it looks like the strength can be calculated with a simple linear equation using these coefficients (meaning it is made up simple proportions of the other features). \n##### On a separate not, it doesn't give us any extra accuracy, instead increases complexity and time required to give the same results.\n\n___________","784851f5":"Please consider upvoting if you found this useful and informative. Thank you.","c392375b":"The above plot shows the zero values imputed in green and the rest in blue. Looks like our imputed values fit the patterns.","622a81a2":"### Explore for Gaussians","e3c8d582":"# Import the necessary libraries.","71422c61":"There are a few outliers in **slag**, **water**, **superplasticizer**, **fine_agg** and **age**. \n\nRemoving the rows with outliers removes around 10% of the data so we will substitute them with the mean values.","b321cd3e":"* Strength vs age is very interesting to look at. We can clearly see four clusters, 2 big ones and 2 tiny. We can also see that as the age increases, the strength increases too so this might be a helpful feature in predicting the Strength. There is one cluster that is far away from the rest of the clusters. It could mean that the concrete that has an age range of 80-100 has average strength.\n\n* We will check if it really works by creating a KNeighbors Regressor model.","ac988381":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**.\n\n___________","248cfda7":"In the charts above, we see some sort of pattern in the data and a lot of columns with zero values. We shall impute them using Linear Regression by considering other features to predict them.\n\n**Observations:**\n\n* As the amount of cement increases, the amount of slag does too. A pattern is visible in the first chart.\n* If we look at the cement vs flyash closely, we can see pattern with a negative slope, the amount of cement is inversely proportional to the amount of flyash.\n* The cement vs superplasticizer trend is similar to that of cement vs slag.","909b1808":"* Looking at the kdeplots of strength vs other independent columns, we can see that the data is infact a mixture of Gaussians.\n\n* Strength vs flyash shows us two distinct Gaussians and 2 clusters but looking at the position of the two clusters, it might not be very helpful in determining\/predicting the strength and the same goes for Strength vs superplasticizer."}}