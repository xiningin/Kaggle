{"cell_type":{"e0cef1d9":"code","b45fd1e9":"code","2205bdaa":"code","d1d843ad":"code","111f8853":"code","25a2ca92":"code","05181b53":"code","5cc6bfbe":"code","a1627782":"code","0c74c3a4":"code","74b5fe28":"code","603e949c":"code","e7bef11f":"code","1bffcf36":"code","72157076":"code","092c37a9":"code","d4c2234c":"code","d1503687":"code","10bc9178":"code","ccec1281":"code","cd317ad8":"code","d27fdb9f":"code","030c1a93":"code","9f21f62f":"code","22c92425":"code","7c1d8cf4":"code","33b8b269":"code","c4d21e52":"code","10396072":"code","184397f6":"code","98db246c":"code","9d840c17":"code","73fb90b2":"code","2b40c7b4":"code","e9ca6412":"code","c82dc19c":"code","3c8baa4a":"code","5f47fd89":"code","8cfdab21":"code","558cff52":"code","f02b81d0":"code","c42d7c3a":"code","abe80c77":"code","0581ae15":"code","9b0702d3":"code","ac906ac2":"code","9031b1e1":"code","1cca4529":"code","96db52c9":"code","6295b9b8":"code","1b95b663":"code","1e368a56":"code","d0dddca4":"code","4424ce0b":"code","d27ac86f":"code","d35bb07b":"code","77f258cd":"code","6c8cc22f":"code","e82dbd0c":"code","aa231f48":"code","8fe4abf2":"code","af9d2b7e":"code","a3664aca":"code","e5bf92ab":"code","f5f623bd":"code","1a366225":"code","480f3a95":"code","5294263d":"code","1484174c":"code","a13bee90":"code","667059b7":"code","d19feb9f":"code","7e013d07":"code","a799614d":"code","f416e67b":"code","31327258":"code","e9aeed28":"code","19c9e6ff":"code","a1102134":"code","d013de8f":"code","ee41fed3":"code","8f6813a0":"code","fe930bb6":"code","22b57c57":"code","281886e4":"code","186e5c0a":"code","cc1ed989":"code","ce150c52":"code","a2fb0427":"code","37d36bf9":"code","511178b8":"code","825c8a9f":"code","a1a3be5e":"code","1731bf06":"code","a5cd326c":"code","efbf51a2":"code","7cce1b74":"code","270007f4":"code","56d60b6a":"code","3a14e442":"code","9c305764":"code","112ae93a":"code","10176ce0":"code","103bacb0":"code","6f3a32df":"code","f0530d51":"code","d888440f":"code","6410bfef":"code","a8e685a5":"code","fd2627e3":"code","cb136644":"code","64e063be":"code","2a8de364":"code","be4a1326":"code","50908a34":"markdown","76d2aa84":"markdown","be995f14":"markdown","30d96ca7":"markdown","6a2c239c":"markdown","5d6d0b02":"markdown","4f33312f":"markdown","6eff1bf6":"markdown","809ec045":"markdown","7c949f17":"markdown","e0a704d3":"markdown","652c0883":"markdown","ba7eb554":"markdown","c2502c59":"markdown"},"source":{"e0cef1d9":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport missingno as msno\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.impute import KNNImputer\nimport statsmodels.api as sm\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom lightgbm import LGBMRegressor\nimport re","b45fd1e9":"#Read the Data\ndf=pd.read_csv(\"\/kaggle\/input\/hitters-baseball-data\/Hitters.csv\")","2205bdaa":"data=df.copy()\ndf.head()","d1d843ad":"def data_understanding(df):\n    print('############shape##############')\n    print(df.shape)\n    print('############types##############')\n    print(df.dtypes)\n    print('############head###############')\n    print(df.head())\n    print('############info###############')\n    print(df.info())\n    print('############nunique###############')\n    print(df.nunique())","111f8853":"# There are 322 observations and int-float-object types of features in this data set\ndata_understanding(df)","25a2ca92":"print(\"Num of Object Variables:\", df.select_dtypes(object).shape[1])\nprint(\"Num of Integer Variables:\", df.select_dtypes(\"integer\").shape[1])\nprint(\"Num of Float Variables:\", df.select_dtypes(\"float\").shape[1])","05181b53":"df[\"League\"].value_counts()","5cc6bfbe":"df[\"League\"].value_counts().plot.barh()","a1627782":"df[\"NewLeague\"].value_counts()","0c74c3a4":"df[\"NewLeague\"].value_counts().plot.barh()","74b5fe28":"df[\"Division\"].value_counts()","603e949c":"df[\"Division\"].value_counts().plot.barh()","e7bef11f":"sns.distplot(df['Salary'])","1bffcf36":"#If the missing values don't come from Salary(target feature), i would have thought to assign mean according to these results.\n# Because, there seems to be a relation between categoric variables and Salary values for example there is an important differences between being E Division and W Devision.\nprint(\"New League= A\" ,df[df[\"NewLeague\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"New League= N\" ,df[df[\"NewLeague\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= A\" ,df[df[\"League\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= N\" ,df[df[\"League\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= E\" ,df[df[\"Division\"]==\"E\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= W\" ,df[df[\"Division\"]==\"W\"].agg({\"Salary\":\"mean\"}))","72157076":"#There are 59 null values in Hitters data set\ndf.isnull().sum().sum()","092c37a9":"# All these NA values comes from \"Salary\" feature\ndf.isnull().sum()","d4c2234c":"df[df.Salary.isnull()==True].head()","d1503687":"msno.bar(df)","10bc9178":"#Statistical view for all features\ndf.describe().T","ccec1281":"# Descriptive Analysis\ndf.describe([0.05,0.25,0.50,0.75,0.95,0.99]).T","cd317ad8":"sns.boxplot(x = df[\"Salary\"])\nplt.show()","d27fdb9f":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","030c1a93":"#When the quarters of 1% and quartiles of 99% were examined first, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.01, q3=0.99)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","9f21f62f":"#Then, when the quarters of 25% and quarters of 75% were examined, an outlier was found.\n#Conclusion: Observation analysis against the dependent variable is applied according to quartiles of 25 and 75. \n#Business sector information may remain untouched.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.25, q3=0.75)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","22c92425":"#Later, when quarters of 5% and quarters of 95% were examined, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary')\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","7c1d8cf4":"# numerical variables\ndef numeric_cols(df):\n    numeric_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    return numeric_cols","33b8b269":"#Here, how many outlier observations in all variables in quartiles of 25 and 75 are accessed.\nfor col in numeric_cols(df):\n    lower, upper=outlier_thresholds(df, col, 0.25, 0.75)\n    count=df[(df[col]<lower) | (df[col]>upper)].shape[0]\n    if count!=0:\n        print(col, 'yes')\n        print(count)\n    else:\n        print(col, 'no')","c4d21e52":"def replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n        \n    return dataframe","10396072":"df=replace_with_thresholds(df, 'Salary')","184397f6":"sns.boxplot(df['Salary'])","98db246c":"for i in numeric_cols(df):\n\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n    sns.histplot(df[i], bins=10, ax=axes[0])\n    axes[0].set_title(i)\n    \n    sns.boxplot(df[i], ax=axes[1])\n    axes[1].set_title(i)\n   \n    sns.kdeplot(df[i], ax=axes[2])\n    axes[2].set_title(i)\n    plt.show()","9d840c17":"# correlation analysis\ndf.corr()","73fb90b2":"def correlation(df, size=[20, 15]):\n    f, ax = plt.subplots(figsize= [20,15])\n    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\n    ax.set_title(\"Correlation Matrix\", fontsize=20)\n    plt.show()","2b40c7b4":"correlation(df)","e9ca6412":"# Correlation analysis of numerical variables was performed.\ndef find_corr(df, num_col_names, limit=0.55):\n    high_corrs={}\n    for col in num_col_names:\n        if col=='Salary':\n            pass\n        else:\n            corr=df[[col, 'Salary']].corr().loc[col, 'Salary']\n            print(col, corr)\n            if abs(corr)>limit:\n                high_corrs[col]=corr\n    return high_corrs","c82dc19c":"high_corrs = find_corr(df, numeric_cols(df))","3c8baa4a":"#Two variables with high correlation.\nprint(high_corrs)","5f47fd89":"sns.scatterplot(x= df['CRuns'], y=df.Salary)","8cfdab21":"sns.scatterplot(x= df['CRBI'], y=df.Salary)","558cff52":"def lof_scores(df):\n    clf=LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n    clf.fit_predict(df)\n    df_scores=clf.negative_outlier_factor_\n    sns.boxplot(df_scores)\n    plt.show()\n    return df_scores\n    \ndef lof(df, df_scores, threshold):\n    not_outlier = df_scores >threshold\n    value= df[df_scores == threshold]\n    outliers = df[~not_outlier] \n    res=outliers.to_records(index=False)\n    res[:] = value.to_records(index = False)\n    not_outlier_df = df[not_outlier]\n    outliers = pd.DataFrame(res, index = df[~not_outlier].index)\n    df_res = pd.concat([not_outlier_df, outliers], ignore_index = True)\n    return df_res","f02b81d0":"#drop NA values\ndf1=df.dropna()\ndf1.shape","c42d7c3a":"#Min-Max Scaler\ndef minmax_scaler(dataframe, col_names, feature_range=(0,1)):\n    minmax_scaler = MinMaxScaler(feature_range=feature_range)\n    col_names=[col for col in col_names if col !=\"Salary\"]\n    dataframe[col_names] = minmax_scaler.fit_transform(dataframe[col_names])\n    return dataframe","abe80c77":"df1=minmax_scaler(df1, numeric_cols(df1))","0581ae15":"df1.isnull().sum().sum()","9b0702d3":"# Variables with 2 categories\ndef var_two_cat(df):    \n    bins_cols=[col for col in df.columns if df[col].dtype=='O' and df[col].nunique()==2]\n    return bins_cols","ac906ac2":"print(var_two_cat(df1))","9031b1e1":"def label_encoder(df, bins_cols):\n    for col in bins_cols:\n        le=LabelEncoder()\n        df[col]=le.fit_transform(df[col])\n    return df","1cca4529":"df1=label_encoder(df1, var_two_cat(df1))","96db52c9":"df1.name='df1'\ndf1.head()","6295b9b8":"#This is second option and method is fill NA values with mean\ndf2=df.copy()","1b95b663":"#New variables were created with the most appropriate variables according to their proportions.\n#The data set includes the data obtained by the players in 1986 and throughout their careers and how many years of experience they have. \n#We add the annual averages of these data and the ratio of the 1986 data to the overall performance.\ndef new_var(df):\n    df['AtBat_new'] = df['AtBat'] \/ df['CAtBat']\n    df['Hits_new'] = df['Hits'] \/ df['CHits']\n    df['HmRun_new'] = (df['HmRun'] \/ df['CHmRun']).fillna(0)\n    df['Runs_new'] = df['Runs'] \/ df['CRuns']\n    df['RBI_new'] = (df['RBI'] \/ df['CRBI']).fillna(0)\n    df['Walks_new'] = (df['Walks'] \/ df['CWalks']).fillna(0)\n\n    df[\"CAtBat_rate\"] = df[\"CAtBat\"] \/ df[\"Years\"]\n    df[\"CHits_rate\"] = df[\"CHits\"] \/ df[\"Years\"]\n    df[\"CHmRun_rate\"] = df[\"CHmRun\"] \/ df[\"Years\"]\n    df[\"Cruns_rate\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df[\"CRBI_rate\"] = df[\"CRBI\"] \/ df[\"Years\"]\n    df[\"CWalks_rate\"] = df[\"CWalks\"] \/ df[\"Years\"]\n    \n    return df","1e368a56":"def new_year(df):\n    df['New_Year'] = pd.cut(x=df['Years'], bins=[0, 3, 6, 10, 15, 19, 24], ).astype(\"O\")\n    return df","d0dddca4":"df2=new_year(df2)","4424ce0b":"df2['New_Year'].value_counts().plot.barh()","d27ac86f":"df2.isnull().sum().sum()","d35bb07b":"msno.bar(df2)","77f258cd":"df2['Salary']=df2['Salary'].fillna(df2.groupby(['New_Year', \"League\", 'Division'])['Salary'].transform('mean'))","6c8cc22f":"df2.isnull().sum().sum()","e82dbd0c":"df2.head()","aa231f48":"df2=minmax_scaler(df2, numeric_cols(df2))","8fe4abf2":"df2=label_encoder(df2, var_two_cat(df2))","af9d2b7e":"df2.head()","a3664aca":"def one_hot_cols(df): \n    return [col for col in df.columns if 10>=df[col].nunique()>2]\nprint(one_hot_cols(df2))","e5bf92ab":"df2 = pd.get_dummies(df2, columns=one_hot_cols(df2), drop_first=True)","f5f623bd":"df2.head()\ndf2.name='df2'","1a366225":"df3=df.copy()","480f3a95":"df3=new_year(df3)","5294263d":"df3=minmax_scaler(df3, numeric_cols(df3))","1484174c":"df3=label_encoder(df3, var_two_cat(df3))","a13bee90":"print(one_hot_cols(df3))","667059b7":"df3 = pd.get_dummies(df3, columns=one_hot_cols(df3), drop_first=True)","d19feb9f":"df3.head()","7e013d07":"# We fill in the missing observations with the KNN algorithm and create the dataset named 'df_knn_imp':\ndef knn_imputer(df, n):\n    imputer = KNNImputer(n_neighbors = n)\n    df_filled = imputer.fit_transform(df)\n    df_knn_imp = pd.DataFrame(df_filled,columns = df.columns)\n    return df_knn_imp","a799614d":"df3=knn_imputer(df3, 4)\ndf3.head()\ndf3.name='df3'","f416e67b":"df3.isnull().sum().sum()","31327258":"#Filling Missing Data with KNN and Suppressing Outliers to create 'df4'\ndf4=df.copy()","e9aeed28":"df4.head()","19c9e6ff":"df4=new_year(df4)","a1102134":"df4=minmax_scaler(df4, numeric_cols(df4))","d013de8f":"df4=label_encoder(df4, var_two_cat(df4))","ee41fed3":"df4 = pd.get_dummies(df4, columns=one_hot_cols(df4), drop_first=True)","8f6813a0":"df4.head()","fe930bb6":"df4=knn_imputer(df4, 4)","22b57c57":"array=np.sort(lof_scores(df4))\n\narray_res=array[array>array[63]]","281886e4":"sns.boxplot(array_res)","186e5c0a":"df_scores=lof_scores(df4)\ndf4=lof(df4, df_scores, np.sort(df_scores)[63])","cc1ed989":"df4.isnull().sum().sum()","ce150c52":"df4.name='df4'\ndf4.head()","a2fb0427":"df5=df.copy()","37d36bf9":"df5=new_year(df5)\ndf5=new_var(df5)","511178b8":"df5=label_encoder(df5, var_two_cat(df5))","825c8a9f":"print(one_hot_cols(df5))","a1a3be5e":"df5 = pd.get_dummies(df5, columns=one_hot_cols(df5), drop_first=True)","1731bf06":"df5=knn_imputer(df5, 4)","a5cd326c":"df_scores=lof_scores(df5)","efbf51a2":"df5=lof(df5, df_scores, np.sort(df_scores)[110])   #90","7cce1b74":"df5=minmax_scaler(df5, numeric_cols(df5))","270007f4":"df5.isnull().sum().sum()","56d60b6a":"df5.name='df5'\ndf5.head()","3a14e442":"def reg_model(df, Y, algo, test_size=0.20):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=test_size, random_state=42)\n    model=algo.fit(X_train, Y_train)\n    Y_train_pred=model.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(df.name)\n    print(type(model).__name__)\n    print(\"Train RMSE: {}\".format(train_rmse))\n    \n    Y_test_pred=model.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE: {}\".format(test_rmse))\n    print('###################################')\n    return (df.name, type(model).__name__, train_rmse, test_rmse)","9c305764":"models=[LinearRegression(), Ridge(), Lasso(), ElasticNet()]\ndataframes=[df1, df2, df3, df4, df5]\nresults={'frame':[], 'model':[], 'train_error':[], 'test_error':[]}","112ae93a":"for frame in dataframes:\n    for model in models:\n        res=reg_model(frame, 'Salary', model)\n        results['frame'].append(res[0])\n        results['model'].append(res[1])\n        results['train_error'].append(res[2])\n        results['test_error'].append(res[3])","10176ce0":"results=pd.DataFrame(results)\nresults","103bacb0":"sns.barplot(x=results['frame'], y=results['test_error'], hue=results['model'])","6f3a32df":"def model_tuning(df, Y, algo_cv, algo, alphas, test_size=0.20, cv=10):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=test_size)\n    model_cv=algo_cv(alphas=alphas, cv=cv)\n    model_cv.fit(X_train, Y_train)\n    model_tuned=algo(alpha=model_cv.alpha_)\n    model_tuned.fit(X_train, Y_train)\n    print(df.name)\n    print(type(model_tuned).__name__)\n    Y_train_pred=model_tuned.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(\"Train RMSE:{}\".format(train_rmse))\n    Y_test_pred=model_tuned.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE:{}\".format(test_rmse))\n    print('#####################')\n    return (df.name, type(model_tuned).__name__, train_rmse, test_rmse)","f0530d51":"models={Ridge: RidgeCV, Lasso:LassoCV, ElasticNet:ElasticNetCV}\nresults_tuned={'frame':[], 'model':[], 'train_rmse':[], 'test_rmse':[]}\nalphas = [0.1,0.01, 0.005, 0.05, 0.001,0.2,0.3,0.5,0.8,0.9,1]","d888440f":"for frame in dataframes:\n    for model in models:\n        res=model_tuning(frame, 'Salary', models[model], model, alphas)\n        results_tuned['frame'].append(res[0])\n        results_tuned['model'].append(res[1])\n        results_tuned['train_rmse'].append(res[2])\n        results_tuned['test_rmse'].append(res[3])","6410bfef":"results_tuned=pd.DataFrame(results_tuned)\nresults_tuned","a8e685a5":"def light_gbm(df, Y):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    lgbm=LGBMRegressor()\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=0.20)\n    lgbm.fit(X_train, Y_train)\n\n    Y_pred=lgbm.predict(X_test,num_iteration=lgbm.best_iteration_)\n\n    print((np.sqrt(mean_squared_error(Y_test, Y_pred))))","fd2627e3":"light_gbm(df4, 'Salary')","cb136644":"light_gbm(df5, 'Salary')","64e063be":"def light_gbm_tuning(df, Y):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=0.20)\n    lgbm_grid={\n    'colsample_bytree':[0.4, 0.5, 0.6, 0.9, 1],\n    'learning_rate':[0.01, 0.1, 0.5, 1],\n           'n_estimators':[20, 40, 100, 200, 500, 1000],\n           'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]}\n\n    lgbm=LGBMRegressor()\n\n    lgbm_cv_model=GridSearchCV(lgbm, lgbm_grid, cv=10,\n                           n_jobs=-1, verbose=2)\n\n    lgbm_cv_model.fit(X_train, Y_train)\n\n    #lgbm_cv_model.best_params_\n    lgbm_tuned=LGBMRegressor(learning_rate=0.1,\n                         max_depth=2,\n                         n_estimators=100,\n                         colsample_bytree=0.9)\n\n    lgbm_tuned.fit(X_train, Y_train)\n\n    Y_pred=lgbm_tuned.predict(X_test)\n\n    print(np.sqrt(mean_squared_error(Y_test, Y_pred)))","2a8de364":"light_gbm_tuning(df4, 'Salary')","be4a1326":"light_gbm_tuning(df5, 'Salary')","50908a34":"# Local Outlier Factor","76d2aa84":"Salary Prediction Model\n\nDescription\n\nMajor League Baseball Data from the 1986 and 1987 seasons.\n\n\nFormat:\n\nAtBat: Number of times at bat in 1986\n\nHits: Number of hits in 1986\n\nHmRun: Number of home runs in 1986\n\nRuns: Number of runs in 1986\n\nRBI: Number of runs batted in in 1986\n\nWalks: Number of walks in 1986\n\nYears: Number of years in the major leagues\n\nCAtBat: Number of times at bat during his career\n\nCHits: Number of hits during his career\n\nCHmRun: Number of home runs during his career\n\nCRuns: Number of runs during his career\n\nCRBI: Number of runs batted in during his career\n\nCWalks: Number of walks during his career\n\nLeague: A factor with levels A and N indicating player's league at the end of 1986\n\nDivision: A factor with levels E and W indicating player's division at the end of 1986\n\nPutOuts: Number of put outs in 1986\n\nAssists: Number of assists in 1986\n\nErrors: Number of errors in 1986\n\nSalary: 1987 annual salary on opening day in thousands of dollars\n\nNewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n\n\nNeeded python libraries:\nPandas, matplotlib, seaborn, filterwarnings, missingno, numpy, sklearn, statsmodels, re, lightgbm.\n\nProcesses:\n1. Data understanding\n2. Data preprocessing\n3. Cleaning outliers\n4. Creating different scenarios\n5. Apply regression algorithms to all scenarios\n6. Model tuning\n\nScenarios:\n1. Dropping NULL values, min-max sclae numeric columns, encode object columns with label \nand one hot encoder depending on unique values in these colums.\n\n2.Filling NULL values with mean, based on categories. \n\n3.Min-max scale numeric colums, encode object columns, filling NULL values with KNN imputer adding new_year column.\n\n4.Filling Missing Data with KNN and Suppressing Outliers to create 'df4' and cleaning outliers with LOF(Local Outlier Factor).\n\n5.Creating new variables and filling NULL values with knn imputer. New variables were created with the most appropriate variables according \nto their proportions. The data set includes the data obtained by the players in 1986 and throughout their \ncareers and how many years of experience they have. The annual averages of these data and the ratio of the 1986 \ndata to the overall performance were added.\n","be995f14":"We will create different data sets for different scenarios that we will apply for salary estimation.","30d96ca7":"# MODEL TUNING","6a2c239c":"# Model","5d6d0b02":"# DATA UNDERSTANDING","4f33312f":"# Fifth option","6eff1bf6":"# First option","809ec045":"# Second option","7c949f17":"# Libraries","e0a704d3":"# Fourth option","652c0883":"# Outliers","ba7eb554":"# Data Preprocessing","c2502c59":"# Third option"}}