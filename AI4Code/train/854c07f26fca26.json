{"cell_type":{"842bdd1d":"code","98bb4820":"code","e357f6c4":"code","e829dbf4":"code","556ec1d9":"code","a1fd6f58":"code","f9b2e2da":"code","8a90f29e":"code","81abb56d":"code","309431cc":"code","ef868464":"code","f2351d44":"code","f0037032":"code","c214a500":"code","95fc2027":"code","6e2420ab":"code","abfb5bec":"code","abf0e77d":"code","569144b0":"code","22083049":"code","e2b4eb64":"code","3c5744ba":"code","e5949e69":"code","194af777":"code","31e50477":"code","64fc9876":"code","fb70e17a":"code","71d932f5":"code","74b585c6":"code","fb456ce1":"code","a8783a52":"code","fb15bb20":"code","b039c788":"code","45de95fb":"code","819b8634":"code","155573f0":"code","9fd67aa2":"code","c64424ee":"code","5828fc33":"code","9d719ca3":"code","8017f557":"code","e5c2d8f5":"code","61f32724":"code","a5c2f7e8":"code","7373c80c":"code","b5b18904":"code","d0aafa84":"code","a5c7239c":"code","7440137c":"markdown","f12f61d6":"markdown","0ae9cd82":"markdown","fc83e242":"markdown","cf3ce7e9":"markdown","9678ba97":"markdown","bfc8f75a":"markdown","9092e073":"markdown","5e855f7f":"markdown","598a1fff":"markdown","f4ea5c19":"markdown","dba54946":"markdown","38b0bb46":"markdown","8ec3612f":"markdown","f481b36b":"markdown","9b799450":"markdown","14abd7f6":"markdown","913b3922":"markdown","4a989e68":"markdown","8c56d239":"markdown","7b99923e":"markdown","871381b3":"markdown","897c97b1":"markdown","b4490541":"markdown","384e1568":"markdown","1cc59aec":"markdown","0e4a4bd8":"markdown","c081e050":"markdown","81b8e3a5":"markdown","142e3981":"markdown","ea7b10d7":"markdown","123a3b69":"markdown","9c3859d5":"markdown","84a4a9e2":"markdown","2c195359":"markdown","273319bc":"markdown","bb83589a":"markdown","f427b2c7":"markdown","d7b5af9e":"markdown","f288e4bb":"markdown","b6ec0ddd":"markdown","bdbdf117":"markdown","706f892d":"markdown","34574afa":"markdown","055f2815":"markdown","eb6ba212":"markdown","0ab2c2e8":"markdown","ad4d3c36":"markdown","5dd1864e":"markdown","4b5635c8":"markdown","b9b7f4e0":"markdown","1663284a":"markdown","fad67006":"markdown","6e9e2bb3":"markdown","bbb3d6c0":"markdown","293a1f6e":"markdown","94a1056f":"markdown","d2b2948f":"markdown","ea99917e":"markdown","161c115b":"markdown","b19413b6":"markdown","1b30d4ec":"markdown","3187873e":"markdown","100ecc87":"markdown","908d6935":"markdown","d66aa6b7":"markdown","0d014afe":"markdown","68581a18":"markdown","b82930e4":"markdown","37eea04b":"markdown","f93ddc0d":"markdown","edae2fd2":"markdown","dc846097":"markdown","1d44dee3":"markdown","ec22bf0a":"markdown","404ea62e":"markdown","d2bc4b3f":"markdown","9c1d2514":"markdown","caf4c256":"markdown","1d88b2e8":"markdown","830d4b45":"markdown"},"source":{"842bdd1d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, roc_auc_score\nfrom scipy.stats import ks_2samp\nfrom imblearn.over_sampling import SMOTE\nimport shap","98bb4820":"df = pd.read_excel('..\/input\/covid19\/dataset.xlsx')\n\ndf.head()","e357f6c4":"# Transform to numeric\ndf['SARS-Cov-2 exam result'] = df['SARS-Cov-2 exam result'].map({'positive': 1, 'negative': 0})\n\n# Map detected\/not_detected and positive\/negative to 1 and 0\ndf = df.replace({'positive': 1, 'negative': 0, 'detected': 1, 'not_detected': 0})\n\ndf['SARS-Cov-2 exam result'].value_counts(normalize=True)","e829dbf4":"df_null_pct = df.isna().mean().round(4) * 100\n\ndf_null_pct.sort_values(ascending=False)","556ec1d9":"df_null_pct.plot(kind='hist')","a1fd6f58":"nulls = df_null_pct[df_null_pct > 90]\n\ndf = df[[col for col in df.columns if col not in nulls]]\n\ndf.head()","f9b2e2da":"features = [col for col in df.columns if col not in ['Patient ID', \n                                                    'Patient addmited to regular ward (1=yes, 0=no)',\n                                                    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                                                    'Patient addmited to intensive care unit (1=yes, 0=no)',\n                                                    'SARS-Cov-2 exam result']]\n\ndf[features].var()","8a90f29e":"df.drop('Parainfluenza 2', axis=1, inplace=True)\nfeatures.remove('Parainfluenza 2')","81abb56d":"df['has_disease'] = df[df.columns[20:]].sum(axis=1)\n\ndf.loc[df['has_disease'] > 1, 'has_disease'] = 1\n\ndf['has_disease'].value_counts(normalize=True)","309431cc":"df[df['has_disease'] == 1]['SARS-Cov-2 exam result'].value_counts(normalize=True)","ef868464":"df_clean = df.copy()\n\ndf[df.columns[20:]] = df[df.columns[20:]].fillna(0)","f2351d44":"for feature in df[df.columns[6:20]]:\n    df_age_var = df.dropna(axis=0, subset=['Patient age quantile', feature]).loc[:, ['Patient age quantile',\n                                                                                          feature]]\n    missing = df[feature].isnull()\n    age_missing = pd.DataFrame(df['Patient age quantile'][missing])\n\n    X = df_age_var[['Patient age quantile']]\n    y = df_age_var[feature]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    lm = LinearRegression().fit(X_train, y_train)\n\n    df.loc[df[feature].isna(), feature] = lm.predict(age_missing)\n    \ndf.head()","f0037032":"df.isna().sum().sum()","c214a500":"df_age_var = df.dropna(axis=0, subset=['Patient age quantile', 'Hematocrit']).loc[:, ['Patient age quantile',\n                                                                                      'Hematocrit']]\n\nmissing_hem = df['Hematocrit'].isnull()\nage_missing_hem = pd.DataFrame(df['Patient age quantile'][missing_hem])\n\nX = df_age_var[['Patient age quantile']]\ny = df_age_var['Hematocrit']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlm = LinearRegression().fit(X_train, y_train)\ny_pred = lm.predict(X_test)\n\nplt.scatter(X_test, y_test)\nplt.plot(X_test, y_pred, color='red', lw=2)\nplt.xlabel('Patient age quantile')\nplt.ylabel('Hematocrit')\nplt.show()","95fc2027":"df_age_var = df.dropna(axis=0, subset=['Patient age quantile', 'Leukocytes']).loc[:, ['Patient age quantile',\n                                                                                      'Leukocytes']]\n\nmissing_leu = df['Leukocytes'].isnull()\nage_missing_leu = pd.DataFrame(df['Patient age quantile'][missing_leu])\n\nX = df_age_var[['Patient age quantile']]\ny = df_age_var['Leukocytes']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nlm = LinearRegression().fit(X_train, y_train)\ny_pred = lm.predict(X_test)\n\nplt.scatter(X_test, y_test)\nplt.plot(X_test, y_pred, color='red', lw=2)\nplt.xlabel('Patient age quantile')\nplt.ylabel('Leukocytes')\nplt.show()","6e2420ab":"def fit_and_print(model):\n    model.fit(X_train, y_train)  \n    y_pred = model.predict(X_test)\n    #CM and Acc\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))  \n    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))  \n    print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))   \n    print(\"Recall Score:\", recall_score(y_test, y_pred))\n    #AUC and KS\n    print(\"AUC: \", roc_auc_score(y_test, y_pred))\n    print(\"KS: \", ks_2samp(y_pred[y_test == 0], y_pred[y_test == 1]).statistic)\n    \nX = df[features].values \ny = df['SARS-Cov-2 exam result'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\nprint(\"Split between test and train!\")\n\n#Apply RF\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', random_state=42)  \n\nfit_and_print(rf)","abfb5bec":"print('Total Columns: ', df_clean.shape[1])\ndf_clean.isna().sum(axis=1).value_counts()","abf0e77d":"df_red = df_clean[df_clean.isna().sum(axis=1) < 26]\n\ndf_red.head()","569144b0":"df_null_pct = df_red.isna().mean().round(4) * 100\n\ndf_null_pct.sort_values(ascending=False)","22083049":"df_red = df_red[df_red['Leukocytes'].notna()]\n\ndf_null_pct = df_red.isna().mean().round(4) * 100\n\ndf_null_pct.sort_values(ascending=False)","e2b4eb64":"df_red.loc[df_red['Mean platelet volume '].isna(), 'Mean platelet volume '] = df_red['Mean platelet volume '].mean()\n\ndf_red.loc[df_red['Monocytes'].isna(), 'Monocytes'] = df_red['Monocytes'].mean()","3c5744ba":"cols_to_remove = [c for c in df_red.columns[20:-1]]\ndf_feat = df_red.drop(cols_to_remove, axis=1)\n\n#update features\nfeatures = [c for c in df_feat.columns if c not in ['Patient ID', 'SARS-Cov-2 exam result',\n                                                   'Patient addmited to regular ward (1=yes, 0=no)',\n                                                   'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                                                   'Patient addmited to intensive care unit (1=yes, 0=no)']]\n\ndf_feat.head()","e5949e69":"df_feat.isna().sum().sum()","194af777":"corr = df_feat.drop(['Patient ID', 'Patient addmited to regular ward (1=yes, 0=no)', \n             'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n             'Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1).corr()\n\nplt.figure(figsize=(20,8))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap=sns.color_palette('RdBu_r'), \n            annot=True)\n\nplt.show()","31e50477":"df_feat = df_feat.drop(['Mean corpuscular hemoglobin (MCH)', 'Hematocrit', 'Hemoglobin'], axis=1)\n#update features\nfeatures = [f for f in features if f not in ['Mean corpuscular hemoglobin (MCH)', 'Hematocrit', 'Hemoglobin']]","64fc9876":"X = df_feat[features]\ny = df_feat['SARS-Cov-2 exam result'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\nprint(\"Split between test and train!\")\n\n#Apply RF\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', random_state=42)  \nfit_and_print(rf)","fb70e17a":"random_grid = {'n_estimators': [10, 50, 100, 200, 500],\n            'max_features': ['auto', 'sqrt', 'log2', 5, 10, 30],\n            'max_depth': [2, 8, 16, 32, 64, 128],\n            'min_samples_split': [1,2,4,8,16,24],\n            'min_samples_leaf': [1,2,5,10,15,30]}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                            n_iter = 200, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nfit_and_print(rf_random)\nprint(rf_random.best_params_)","71d932f5":"smt = SMOTE(k_neighbors=5, random_state=42)\nX_train, y_train = smt.fit_sample(X_train, y_train)\n\nnp.bincount(y_train)","74b585c6":"rf = RandomForestClassifier(n_estimators=100, max_features='auto', random_state=42)  \nfit_and_print(rf)","fb456ce1":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                            n_iter = 300, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n# Fit the random search model\nfit_and_print(rf_random)\nprint(rf_random.best_params_)","a8783a52":"random_grid = {\n    'penalty': ['l1', 'l2'],\n    'C': [100, 10, 1, 0.1, 0.01, 0.001]\n    }\nlr = LogisticRegression()\n\nlr_random = RandomizedSearchCV(estimator = lr, param_distributions = random_grid, \n                            n_iter = 100, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nfit_and_print(lr_random)\nprint(lr_random.best_params_)","fb15bb20":"random_grid = {\n    'n_neighbors': [2, 3, 5, 8, 10, 12, 15, 20, 30],\n    'weights': ['uniform', 'distance'],\n    'p': [1, 2, 3]\n    }\nknn = KNeighborsClassifier()\n\nknn_random = RandomizedSearchCV(estimator = knn, param_distributions = random_grid, \n                            n_iter = 100, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nfit_and_print(knn_random)\nprint(knn_random.best_params_)","b039c788":"random_grid = {'C': [0.1, 1, 10, 100, 1000], \n               'gamma': ['auto', 1, 0.1, 0.01, 0.001, 0.0001], \n               'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \n\nsvc = SVC()\n\nsvc_random = RandomizedSearchCV(estimator = svc, param_distributions = random_grid, \n                            n_iter = 200, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nfit_and_print(svc_random)\nprint(svc_random.best_params_)","45de95fb":"import xgboost as xgb\n\nrandom_grid = {\"n_estimators\": [100, 500, 1000],\n              \"learning rate\": [0.1, 0.05, 0.01],\n              \"max_depth\": [2, 8, 16, 64, 128], \n              \"colsample_bytree\": [0.3, 0.8, 1],\n              \"gamma\": [0,1,5]}  \n\nxgb_clf = xgb.XGBClassifier(objective='binary:logistic')\n\nxgb_random = RandomizedSearchCV(estimator = xgb_clf, param_distributions = random_grid, \n                            n_iter = 200, cv = 5, scoring = 'recall', \n                            verbose=0, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nfit_and_print(xgb_random)\nprint(xgb_random.best_params_)","819b8634":"feature_importances = pd.DataFrame(rf_random.best_estimator_.feature_importances_,\n                                index = features,\n                                    columns=['importance']).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(12,6))\nfeature_importances.importance.plot(kind='barh', color='green')\nplt.title('Feature Importance')\nplt.show()\nfeature_importances.style.format({'importance': '{:.1%}'.format})\nprint(feature_importances.importance*100)","155573f0":"X = df_feat[features] \ny = df_feat['SARS-Cov-2 exam result'].values\n_, X_test, _, _ = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\ny_pred = rf_random.best_estimator_.predict(X_test)\n\nexplainer = shap.TreeExplainer(rf_random.best_estimator_)\nexpected_value = explainer.expected_value[1]\n\nshap_values = explainer.shap_values(X_test)[1]\nshap_interaction_values = explainer.shap_interaction_values(X_test)[1]\nnp.save('shap_values.npy', shap_values)\n\nprint(expected_value)","9fd67aa2":"shap.decision_plot(expected_value, shap_values[34], X_test.iloc[34])","c64424ee":"shap.decision_plot(expected_value, shap_values[87], X_test.iloc[87])","5828fc33":"shap.decision_plot(expected_value, shap_values[33], X_test.iloc[33])","9d719ca3":"shap.decision_plot(expected_value, shap_values[57], X_test.iloc[57])","8017f557":"shap.decision_plot(expected_value, shap_values[40], X_test.iloc[40])","e5c2d8f5":"shap.dependence_plot(\"Platelets\", shap_values, X_test, alpha=0.8, show=False)\nplt.title(\"Platelets dependence plot\")\nplt.axhline(0, lw=3)\nplt.show()","61f32724":"shap.dependence_plot(\"has_disease\", shap_values, X_test, alpha=0.8, show=False)\nplt.title(\"Has_disease dependence plot\")\nplt.axhline(0, lw=3)\nplt.show()","a5c2f7e8":"shap.dependence_plot(\"Leukocytes\", shap_values, X_test, alpha=0.8, show=False)\nplt.title(\"Leukocytes dependence plot\")\nplt.axhline(0, lw=3)\nplt.show()","7373c80c":"shap.dependence_plot(\"Monocytes\", shap_values, X_test, alpha=0.8, show=False)\nplt.title(\"Monocytes\")\nplt.axhline(0, lw=3)\nplt.show()","b5b18904":"shap.dependence_plot(\"Patient age quantile\", shap_values, X_test, alpha=0.8, show=False)\nplt.title(\"Patient age quantile dependence plot\")\nplt.axhline(0, lw=3)\nplt.show()","d0aafa84":"shap.summary_plot(shap_values, X_test)","a5c7239c":"X_fn = X_test[(y_pred==0) & (y_test == 1)]\nshap_fn = shap_values[(y_pred==0) & (y_test == 1)]\n\nshap.decision_plot(expected_value, shap_fn, X_fn, feature_order='hclust')","7440137c":"Using a correlation heatmap, we can get a sense of the most important variables for the target (second line). Leukocytes, Platelets and has_disease have near -0.3 correlation - they will be important variables. Monocytes and age, on the other hand, have a slight positive correlation. \n\nSome features have a high correlation within themselves, so we have to make decisions in order to reduce noise. Hematocrit and Hemoglobin basically mean the same thing (0.97 correlation). They have very high correlation with Red blood Cells as well (0.87 and 0.84, respectively). Their correlations with the target are very similar as well, so I'll keep the one with the highest (Red Blood Cells, 0.12). The other two hightly correlated variables are MCV and MCH - in this case, MCV is more correlated to the target (-0.055 vs. -0.028), so I'll keep it.","f12f61d6":"We can see that in both examples that the low value of Leukocytes had the greatest influence in order to classify them as positives. Age and Lymphocytes, on the other hand, had minimal impact. Their \"prediction path\" is similar, but the order of the variables change (for instance, Platelets was the 2nd most important for the first patient, whereas the 4th for the second). This kind of analysis is very helpful to understand the impact of each feature for each patient.","0ae9cd82":"Even better!\n\nI think this is very close to the limit we can achieve, given dataset complications. Recall (71%) is not overly high but still acceptable, and the model is still able to correctly classify non-infected (91% Accuracy). This means that we have reduced the False Negatives (very costly), without having to sacrifice too many False Positives (less expensive, but still would have to test healthy people).\n\nStill, let's try some other models.","fc83e242":"Out of 5644 rows, 3596 have 32 columns with missing values (out of 39). This gives us a place to start.\n\nThe first reduction will be to trim the dataset to one whose rows don't have more than 26 null columns.","cf3ce7e9":"### Remove Variables","9678ba97":"### Remove Features with > 90% null values","bfc8f75a":"### All False Negative (FN) Predictions","9092e073":"### Check Variances","5e855f7f":"Not helpful.","598a1fff":"### False Negative Example","f4ea5c19":"The situation is bad. Most of the features have at least 80% of null values, with a high amout above 90%. One could choose to try to input those values, but that would be a colossal assumption (to input 90% based on 10%) and render our model useless. Garbabe In, Garbage Out. It may hurt, but I chose to drop all columns containing at least 90% of null values.","dba54946":"### Simple Random Forest Model","38b0bb46":"### Summary Plot","8ec3612f":"### Try Logistic Regression ","f481b36b":"### GridSearch","9b799450":"As one can see, after imputation, there are no nulls left on the dataset. It is ready for modeling.\n\nHowever, let's check how our regression is imputing the values. Let's take, for instance, Hematocrit:","14abd7f6":"Another good sign that our model is calibrated is that all people with a previous disease helped the prediction to be not-infected. And, given that the person has a disease, the higher the value of Platelets (blue points), the more the prediction is pushed towards \"not-infected\".","913b3922":"## Target Distribution\n<a id=\"eda\"><\/a>","4a989e68":"For the other variables, which I called 'blood variables' and are all continuous, I'll attempt to input missing values via Linear Regression, using Age as my independent variable. This is a big hypothesis and will only work if there is a clear relationship between variable and age, which I don't expect to happen. Still, I'll give it a try nonetheless.","8c56d239":"For Monocytes, the trend is clear as well. Higher values push the predictions to \"infected\".","7b99923e":"### True Negative Example","871381b3":"Here, I am making a pretty big assumption: in cases where it was not informed if the patient had the virus\/disease, I will assume that he didn't have it.","897c97b1":"The same analysis can be done to the false negatives. In this case, the two variables that greatly pushed the prediction to \"healthy\" were Platelets and Red blood Cells. This indicates that, for this individual, even though he was infected, this two variables indicate otherwise.","b4490541":"## SHAP Values\n\n<a id=\"shap\"><\/a>","384e1568":"Nothing changed. As a next step, let's try to overcome our imbalance problem with an *oversampling* technique called SMOTE. Basically, SMOTE synthetically creates minority class samples to deal with imbalanced classification. More on the technique can be found on https:\/\/arxiv.org\/abs\/1106.1813","1cc59aec":"For platelets, there is a clear relationship between low values (help \"infected\") and high values (help \"non-infected\"), though it is not linear and somewhat muddy around 0.","0e4a4bd8":"### Percentage of Nulls by Column","c081e050":"Let's check null percentage by column once again.","81b8e3a5":"- Keep: MCV, Red blood Cells\n- Remove: MCH, Hematocrit, Hemoglobin","142e3981":"Expected. The model basically predicts all the time that the individual is not infected (ridiculous Recall). We won't be able to use the whole dataset, much less inpute the missing values this way.","ea7b10d7":"We can look at a Summary Plot as well - this gives all SHAP values for every feature, displayed horizontally. Feature value is used as color and features are ranked by importance.","123a3b69":"## Read Data","9c3859d5":"This notebook aims to predict if a patient is infected with COVID-19, given a set of clinical features.\n\nThe dataset used was kindly provided by Hospital Israelita Albert Eistein, consisting of 5644 records and 101 variables.\n\nThe study is divided as followed:\n\n- [Basic EDA: Target Distribution and Missing Data](#eda)\n- [Attempt to input missing data with regression](#input)\n- [Use smaller dataset containing no nulls (602 records and 18 variables)](#smaller)\n- [Perform SMOTE to oversample minority class](#smote)\n- [Conduct GridSearch with some ML models (RF, LR, KNN, SVM)](#model)\n- [Inspect model precitions using SHAP](#shap)\n- [Conclusion](#conclusion)","84a4a9e2":"### Inpute 'disease' variables with 0","2c195359":"Basically the same behavior of Platelets is observed for Leukocytes.","273319bc":"### Dependence Plots","bb83589a":"We can see that blood variables have equal amount of nulls. For my modeling, I will be considering only the rows containing full data on blood variables. The effect of the other columns, hopefully, will be concentrated on the \"has_disease\" variable. This way, I can keep both \"types\" of variables in modeling.","f427b2c7":"Very similar to RF, but less able to correctly classify negative class.","d7b5af9e":"The same for Leukocytes. Even though the line slightly captures the trend, it is still not even close to ideal.\n\nLet's try to build a simple Random Forest model and see the tragedy that awaits us.","f288e4bb":"## Improve the dataset\n\n<a id=\"smaller\"><\/a>","b6ec0ddd":"Much better! Oversampling with SMOTE helps the model to distinguish between positive and negative results. We reached a Recall of 65%, which is somewhat acceptable. \n\nFinally, we will try to improve this result using GridSearch + SMOTE.","bdbdf117":"### Create 'has_disesase' variable","706f892d":"## SMOTE\n\n<a id=\"smote\"><\/a>","34574afa":"## Description","055f2815":"I've realized that the last 19 columns of the dataset are all related to the presence of antigens - all those variables are binary. When we analyze them one by one, the amount of nulls is very high. But, if we take the sum rowise, the null percentage decreases. This variable indicates, for each patient, if *at least* one of those variables is positive. \n\nHere I am making an assumption to fill the remaining nulls: if all those columns are null, I'll assume that the patient doesn't have any of those diseases (has_disease = 0). There might be cases where this is false and the person simply wasn't tested, but it may be a good approximation.","eb6ba212":"Similar result to Random Forest, but not better. XGB probably would have achieved the same result as RF (only 1 FN more) with a more thorough GridSearch. Problem is, dataset is too small!","0ab2c2e8":"Another way SHAP can help us understand model predictions is to use Dependence Plots. They plot the SHAP values for a given variable on y axis and variable value on x axis. The plot automatically colors the points with a variable estimated to have the higher correlation. Positive SHAP values indicate that the feature is \"helping\" the positive class (i.e., pushing the prediction to be \"infected\"), whereas negative SHAP values indicate that the feature is pushing the prediction to be \"not-infected\". This way, we can see how SHAP values vary with feature value.","ad4d3c36":"That left us with 39 variables out of 101.\n\nNext, I will check the variances to see if there is any variable with a single value.","5dd1864e":"### Try KNN","4b5635c8":"The main takeaway is that there is no \"single path\" that leads to False Negative predictions. Two of the classifications were close to the decision threshold (two lines near 0.4), while the other 3 were \"pretty sure the patient is not infected\". Once again, I have no medical expertise to thoroughly analyze this, but hopefully a doctor can make sense of this.","b9b7f4e0":"In this example, we can see that considering all other variables besides has_disease, this patient would be classified as positive. The fact that he\/she has a disease, however, \"abruptly\" changes the decision to not infected. This is an indication that blood variables are similar to a COVID-19 infected person, but this is due to the fact that he\/she has another disease.","1663284a":"This is an unbalanced dataset. As we can see, only 10% of the observations have tested positive. Will have to take this into account.","fad67006":"Keep only blood variables and has_disease.","6e9e2bb3":"Still far from ideal, but we can see an improvement! Obviously our test set is very small, but we went from 4% Recall to 35%. At least the model has *some* capacity to distinguish classes.\n\nNext, let's see if we can improve results using GridSearch.\n\nThe metric chosen is **Recall**, since we are more interested in minimizing the *False Negatives* (people predicted to be healthy, when in fact are infected).","bbb3d6c0":"### Correlations","293a1f6e":"## Imports","94a1056f":"And for the age, the known relationship of increasing risk is observed as well. There is a clear trend: model pushes predictions towards \"infected\" for older patients. All patients below age quantile 10 pushed prediction towards \"not infected\". However, for older people sometimes the model pushes observations towards \"not infected\", meaning that age does not have a perfect linear relationship with infection.","d2b2948f":"Once again, ready for modeling.\n\nBut first, let's take a look at correlations.","ea99917e":"### True Positive Examples","161c115b":"### Try SVM","b19413b6":"False positives are tricky, at least to me, to analyze, since I have little medical expertise. But hopefully a doctor can extract knowledge from this kind of plot (from what I understand, this person has Platelets and Leukocytes very similar to healthy people, but was infected).","1b30d4ec":"Terrible, terrible, terrible! We observe that the variable has very high *heteroskedasticity*, i.e., high dispersion. Since y values vary greatly for the same value of x (age), we are basically just inputing by the mean - which, of course, won't represent real data. ","3187873e":"We can use Decision Plots to see, for each instance, the impact of each variable on the base value. In all subsequent plots the base value is 0.5; values above it are classified as positive and below it, negative. Features are ranked in the plot by their importance to that specific observation.","100ecc87":"Aside from target unbalance, it seems that we have a lot of nulls in our dataset. In fact, we have columns with all null values. Let's check this distribution:","908d6935":"Indeed, this variable is promising. Of all the patients with a disease, only ~2% tested positive for COVID-19, as opposed to ~10% on the whole dataset. This indicates a strong relationship between having another disease and *not* being infected by COVID-19.","d66aa6b7":"There are still two blood variables with a minor percentage of nulls (0.5% and 0.17%). In this case, since it's just very few samples, I will inpute by the mean.","0d014afe":"We can see that variable \"Parainfluenza 2\" has 0 variance, meaning it has only one value. Will drop that column.","68581a18":"I conducted an analysis, based on 101 variables from 5644 records kindly provided by Hospital Israelita Albert Einstein, to predict if a patient was infected by COVID-19 or not. I chose to maximize the *Recall* metric, given how costly a False Negative is.\n\nAfter dealing with missing data, conducting oversampling with SMOTE and conducting GridSearch with a lot of models, the best result came from **Random Forest**, with 71% Recall, 91% Accuracy and 82% AUC. This is an acceptable value for Recall without sacrificing Accuracy (thus, few False Positives), and the model should perform much better with more data.\n\nI also used SHAP to better understand model predictions, with the main takeaways being:\n\n- Low values for Leukocytes, Platelets and Eosinophils are a strong indicator of COVID-19 presence;\n- If a person tested positive for any of the diseases contained in the dataset, it is highly likely that he\/she will *not* have the COVID-19; \n- High values of Monocytes are a strong indicator of COVID-19 presence;\n- For the other variables SHAP Values are highly skewed towards \"not infected\", becoming less certain for neutral and positive contributions for \"infected\";\n- There is no \"single path\" that leads to False Negative predictions. The implications and meanings have to be better assessed by a doctor.\n\nI hope this work can be helpful in any way :D\n\nStay home and be safe!","b82930e4":"## Conclusion\n\n<a id=\"conclusion\"><\/a>","37eea04b":"This confirms and summarizes all of our previous findings. Low values for Leukocytes, Platelets and Eosinophils are a strong indicator of COVID-19 presence, as well as the has_disease variable. High values of Monocytes are a strong indicator of COVID-19 presence. For the other variables SHAP Values are highly skewed towards \"not infected\" (negative values), with a somewhat \"muddy terrain\" around 0.","f93ddc0d":"### False Positive Example","edae2fd2":"Now our training set has class balance, thanks to synthetic oversampling of minority class.","dc846097":"### Feature Importances","1d44dee3":"The most important features are very similar to the ones we saw on the correlation heatmap.\n\n- Leukocytes\n- Platelets\n- has_disease\n- Eosionophils\n- Mean platelet volume\n- Monocytes\n\nAnother good thing about the model is that age is the third less important feature - I think this is good, given that we know that there is an age relationship, but would like other features to be more relevant.\n\nLastly, let's use SHAP Values to help us understand our model's predictions. You can read more about this wonderful technique on https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html","ec22bf0a":"## Input 'blood variables' via regression (using Age)\n\n<a id=\"input\"><\/a>","404ea62e":"Inverse of the first Random Forest we fitted. Here it basically tells that everyone is infected. Even though we wouldn't have any False Negatives, we would have to test everyone - this is useless.","d2bc4b3f":"### Retrain simple Random Forest with SMOTE","9c1d2514":"We again use a decision plot, this time to look at the prediction paths of all the 5 False Negatives.","caf4c256":"## GridSearch + SMOTE\n\n<a id=\"model\"><\/a>","1d88b2e8":"### Retrain Simple Random Forest","830d4b45":"13% percent of the patients have tested positive for at least one of the antigens. Now let's see how the target behaves for those people:"}}