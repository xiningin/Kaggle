{"cell_type":{"cfc5a7fa":"code","ba63130f":"code","53e1d23d":"code","c990c935":"code","98e49792":"code","271d328f":"code","08781714":"code","29853b3a":"code","4483f23b":"code","5f6d64a3":"code","45871112":"code","fc68951c":"code","766511bb":"code","fc5048fe":"code","c669c6db":"code","ba51508f":"code","56635442":"code","4f3747da":"code","71c15995":"markdown","a82e171f":"markdown","067f9939":"markdown","e6df5761":"markdown","44f4fc36":"markdown","d1be77d9":"markdown","310d29be":"markdown","09bfbf01":"markdown"},"source":{"cfc5a7fa":"#imports\nimport os,random\nfrom shutil import copyfile,copytree,rmtree\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\nimport numpy as np\n\n#visualization\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(tf.__version__)\ntf.test.is_gpu_available()","ba63130f":"#lets set the train & validation path\nimage_dir = \"\/kaggle\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/\"\ntest_dir = \"\/kaggle\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/\"","53e1d23d":"#I noticed that simpson_dataset is already inside simpsons folder which should be removed\n#Image should be copied to working to have delete permission\ncopytree(image_dir,\"\/kaggle\/working\/simpsons\")","c990c935":"train_dir = \"\/kaggle\/working\/simpsons\/\"\nrmtree('\/kaggle\/working\/simpsons\/simpsons_dataset\/')","98e49792":"#Lets go with 32x32 pixels for faster training\nIMG_SHAPE = (32,32,3)\nnum_classes = 42","271d328f":"#Now lets augement our image data\ndatagen_train = ImageDataGenerator(rescale=1.\/255,\n                                   rotation_range=30,\n                                   width_shift_range=0.3,\n                                   height_shift_range=0.3,\n                                   horizontal_flip=True,fill_mode='nearest')\ndatagen_test = ImageDataGenerator(rescale=1.\/255)","08781714":"#We are converting all our training data to size of 32x32 pixels for reducing training time \n#Test set are allowed to have 224x224 for easy visualization and we can convert to 32x32 during predictions\ntrain_generator = datagen_train.flow_from_directory(train_dir,target_size=(32,32))\ntest_generator = datagen_test.flow_from_directory(test_dir,target_size=(224,224))\nclass_names = {v:k for k,v in train_generator.class_indices.items()}","29853b3a":"#Now lets take a sample from the train set\nX_train,y_train = next(train_generator)\nfor i in range(5):\n    plt.imshow(X_train[i])\n    plt.show()\n    print(\"Label:\",class_names[np.argmax(y_train[i])])","4483f23b":"#model building\nmodel = tf.keras.models.Sequential()\n\n#Conv 1\nmodel.add(tf.keras.layers.Conv2D(64,(3,3),padding='same',input_shape = IMG_SHAPE,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n#Conv 2\nmodel.add(tf.keras.layers.Conv2D(64,(3,3),padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.Dropout(0.2))\n#Conv 3\nmodel.add(tf.keras.layers.Conv2D(128,(3,3),padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n#Conv 4\nmodel.add(tf.keras.layers.Conv2D(128,(3,3),padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.Dropout(0.2))\n#Conv 5\nmodel.add(tf.keras.layers.Conv2D(256,(3,3),padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n#Conv 6\nmodel.add(tf.keras.layers.Conv2D(256,(3,3),padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nplot_model(model,show_shapes=True)","5f6d64a3":"#lets compile the model\nmodel.compile(loss='categorical_crossentropy',\n             metrics=['acc'],\n             optimizer=tf.keras.optimizers.Adam(lr = 0.001))","45871112":"#We are not going for model checkpoints\/earlystopping since squeezing out the best model is not the scope of this kernel\n#So we haven't made any train\/validation splits here as well\n#lets train the model\nepochs = 30\nbatch_size = 32\nmodel.fit_generator(train_generator,epochs=epochs)","fc68951c":"#Lets save the model fro later use\nmodel.save('simpsons_model.h5')","766511bb":"#Now model building is completed.\n#So lets see how good our model preforms on the test data\nimg,label = next(test_generator)\nfor i in range(5):\n    #reshaping the image as per the model input shape\n    pred_img = cv2.resize(img[i],(32,32))\n    res = model.predict(np.expand_dims(pred_img,axis=0))\n    plt.imshow(img[i])\n    plt.show()\n    print(\"Predicted :\",class_names[np.argmax(res)])            \n","fc5048fe":"#Save the model\nexport_dir = 'simpson_saved_model'\ntf.saved_model.save(model,export_dir)","c669c6db":"#Now lets choose the optimzation strategy\noptimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY","ba51508f":"#Generate the tflite model\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\nconverter.optimizations = [optimization]\ntflite_model = converter.convert()\n\n#Now lets save the TFlite model\nwith open('simpsons.tflite','wb') as f:\n    f.write(tflite_model)\n","56635442":"#Time to test the TFlite mode\ninterpreter = tf.lite.Interpreter(model_content = tflite_model)\ninterpreter.allocate_tensors()\n\ninput_index = interpreter.get_input_details()[0][\"index\"]\noutput_index = interpreter.get_output_details()[0][\"index\"]","4f3747da":"#let us take an image from the test set\nrand_item = random.randint(0,len(img))\npred_img = cv2.resize(img[rand_item],(32,32))\n\ninterpreter.set_tensor(input_index, pred_img.reshape(-1,32,32,3))\ninterpreter.invoke()\nres = interpreter.get_tensor(output_index)\nprint(\"Predicted :\",class_names[np.argmax(res)])  \nplt.imshow(img[rand_item])","71c15995":"### Converting Model to Lite\nNow we have our everything we need to convert our model and build the tflite model","a82e171f":"### On device inference\n\nNow since our lite model is ready lets see how it actually performed on an android device\nI took the inference kotlin scripts from tensorflow github\n\n![inference](https:\/\/i.imgur.com\/ybboFlP.png?1)\n\nThe predictions are okay but it may vary from device to device based on its resources(CPU,camera,memory)\n\nCredits\n\nhttps:\/\/www.tensorflow.org\/lite\n\nhttps:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/lite\n\nFor more info \n\nhttps:\/\/github.com\/anandsm7\/simpsons_classifier_tflite\n\n#### I know its a bit unconventional kernel from other data science kernel but i clearly hate seeing great models sitting on local machines.If there any mistakes\/doubts please feel free to comment.\n\n#### If you find this kernel useful please upvote \n\n","067f9939":"### CNN Model Architecture\n\nWe are going to go with a model simpler yet very similar architecture to that of famous VGG16 as shown\n\n![CNN](https:\/\/www.researchgate.net\/profile\/Saikat_Roy9\/publication\/322787849\/figure\/fig1\/AS:588338117488642@1517282146532\/VGG16-Architecture-with-Softmax-layer-replaced-used-as-the-base-model-for-each-classifier.png)\nimage copyright : www.researchgate.net\/profile\/Saikat_Roy9\n\nThe model is very similar to that of VGG with two convolution followed by maxpooling. So tweaks in the model\n* Added batch normalization for better convergence and reduced overfitting\n* Model is made simpler by not adding three conv is in the final convolution layers","e6df5761":"#### Over predictions looks almost great :) \n![](https:\/\/media2.giphy.com\/media\/A6aHBCFqlE0Rq\/giphy.gif?cid=790b761197a14552e72bd33d3a1bd7f479b2129158ff652d&rid=giphy.gif)\ncredits:https:\/\/giphy.com\n\nSince our model is ready and seems to work just fine its time to see how to quanitize it and make it ready for resource constrained devices.\nAfter saving the model the save of .h5 file is around 25MB which is not the big given the model architecure and image sizes we can still decrease and size without having that much of degradation on the accuracy\n","44f4fc36":"### Optimization\n\nTF lite provide several optimization strategy like\n* DEFAULT\n* OPTIMIZE_FOR_LATENCY\n* OPTIMIZE_FOR_SIZE\n In which has its own behaviour and we will be using OPTIMIZE_FOR_LATENCY as we give more importance to ion device inference accuracy and time\n \n Check\n https:\/\/www.tensorflow.org\/lite\/performance\/model_optimization\n https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/lite\/Optimize\n ","d1be77d9":"### Lite model Interpretation\n\n  We have checked our model efficiency on non quantized model and were very satisfied with result.But we are not sure the our lite model is up to the mark of its parent model. Thanks to tensorflow which provides something called an interpreter to see the efficiency  of our lite model.If the lite model is not up to the mark there is no point of deploying to any devices. So lets see how it goes\n","310d29be":"# Simpsons Classifier - Building a Quantized Model\n##### Author : Anand\n\nThis is a fun kernel to give a brief idea about post training model quantization and the major use of it in real life\n\nA model is as good as the real world value it can provide.So we need to find a way to show case our model to the Users.Since ML models are very resource demanding deploying our model to cloud sounds like a great idea, but this comes with a fair share of cons as well like\n* Security Concerns\n* Cloud cost for model deployment\n* Need of strong connection \n..etc\n\nAnother way is to deploy our model on resource constrained edge devices (tiny IOT devices,android,IOS devices ..etc).So we need a way to minimize the size our models so that it could run smoothly on all these resource constrained devices.\n\nSo this kernel is on how to build a keras image classifier then quantize this model so that this model could be deployed on smaller devices.\n\n#### Model Quantization\n\nLets discuss on the topic model quantization, its basically a a way of conversion which could drastically a reduce the size of model without creating much degradation on the model accuracy.Creating an android app with a more than 200MB size model would be really bad idea, so post model quantization would really helpful in such situations.Model quantization process can reduce the model size 4x and can considerable increase the inference speed which are really useful in real life applications.\n\n## TF-Lite\nTensorflow lite is framework for quantizing the model and on device inference.So we can convert any tensorflow or keras model to lite model so that there will considerable decrease in the model size and we could use the tf lite interpreter for on device inference.The working TF-lite is as follows\n\n![tf](https:\/\/sdtimes.com\/wp-content\/uploads\/2017\/11\/tensorflow-490x464.jpg)\nsdtimes.com\/embedded-devices\/google-previews-tensorflow-lite\/\n\nFor details\nhttps:\/\/www.tensorflow.org\/lite\n \nOur kernel plan of action\n* Build a simpsons image classifier using keras\n* Perform post model quantization\n* Create a .tflite model\n* Using TF-lite interpreter for inference","09bfbf01":"## Hurrayyy :)\n\n### The lite model is working as expected\n![](https:\/\/media1.giphy.com\/media\/3orieRXneF4pvFAoAo\/giphy.gif?cid=790b76113276d6395f379cd8dfc912b44d042a0f32fedaba&rid=giphy.gif)\n\n"}}