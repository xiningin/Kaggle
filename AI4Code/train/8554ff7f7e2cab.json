{"cell_type":{"eed54f0c":"code","1535ee56":"code","7489eedc":"code","6e360c02":"code","c196a717":"code","3e4c42c5":"code","d549397c":"code","b3cb408f":"code","8ed41ded":"code","6bc29f97":"code","cc48d000":"code","3fad7310":"code","7be6d494":"code","1fe2b792":"code","797bc406":"code","b027283c":"code","a33c6184":"markdown","c7a50edd":"markdown","802e45a9":"markdown","952bcc0f":"markdown","324cee28":"markdown","69187db4":"markdown","a512998a":"markdown","98c7b4e9":"markdown","b5a86c4b":"markdown","13429360":"markdown","aa6189fa":"markdown","76cea5d1":"markdown","75ddbc47":"markdown","151224ac":"markdown","540eb7ca":"markdown","09968898":"markdown","742d7cdd":"markdown","7ae4fe6b":"markdown","897b5662":"markdown","a83bb4ff":"markdown","699c59a8":"markdown","283be2a0":"markdown"},"source":{"eed54f0c":"import pandas as pd\nfrom sklearn.datasets import load_breast_cancer","1535ee56":"data = load_breast_cancer()","7489eedc":"data_X = pd.DataFrame(data[\"data\"], columns=data['feature_names'])\ndata_X","6e360c02":"data_Y = data[\"target\"]\ndata_Y[:30]","c196a717":"data_X.shape","3e4c42c5":"data_Y.shape","d549397c":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score","b3cb408f":"etc = ExtraTreesClassifier(random_state=0)\ncv_score = cross_val_score(etc, data_X, data_Y, cv=5).mean()\nprint(\"The avergae Cross validation score is \", cv_score)","8ed41ded":"from sklearn.model_selection import GridSearchCV","6bc29f97":"parameters = {'n_estimators':[50, 100, 500, 1000], 'max_depth':[5, 10, 50, 100, 500]}\netc = ExtraTreesClassifier()\nclf = GridSearchCV(etc, parameters, cv=5)\nclf.fit(data_X, data_Y)","cc48d000":"clf.best_estimator_","3fad7310":"clf.best_score_","7be6d494":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nimport time","1fe2b792":"start_time = time.time()\n\ndtc = DecisionTreeClassifier(random_state=0)\ncv_score = cross_val_score(dtc, data_X, data_Y, cv=5).mean()\nprint(\"Average Cross Validation Score = \", cv_score)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","797bc406":"start_time = time.time()\n\nrfc = RandomForestClassifier(random_state=0)\ncv_score = cross_val_score(rfc, data_X, data_Y, cv=5).mean()\nprint(\"Average Cross Validation Score = \", cv_score)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","b027283c":"start_time = time.time()\n\netc = ExtraTreesClassifier(random_state=0)\ncv_score = cross_val_score(etc, data_X, data_Y, cv=5).mean()\nprint(\"Average Cross Validation Score = \", cv_score)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","a33c6184":"Wow! Without any hyperparameter tuning we achieved a validation score of 97% The model is working pretty well on its own, still lets look at some parameters that we can tweak.","c7a50edd":"## 5. Implementation\n\nLets create our Extra Trees model!  \n### 5.1. Importing dataset \nWe will be using breast cancer data present in Scikit-Learn datasets. This dataset has 30 features and 569 instances.","802e45a9":"Random Forest took 1.52 seconds which is 25 times slower than a single Decision Tree which is obvious due to more number of estimators(trees). Validation score is 96.3%","952bcc0f":"Here, the first node divides the dataset by sex, at the second level the dataset is divided by the features Age and Pclass and so on. The important thing to note here is that the threshold to divide the tree at a particular node is selected by the algorithm on its own. It calculates this threshold by analysing the feature and threshold that will give the least gini impurity. In simple terms, if we have a dataset which classifies students into pass or fail and we have only one feature i.e Marks, the decision tree will find the passing score(threshold) on its own and train a model. The next time we enter a student's marks, the algorithm classifies the student into pass or fail using the computed passing score.","324cee28":"## 4. Extra Trees\nThe \"Random\" in Random Forest is bceause we are using a random subset of the dataset. But what if instead of choosing the best possible threshold for each tree at each node, we simply choose a random threshold too. If you remember the example taken in Decision Trees, imagine the algorithm is not bothered to compute the passing score in the first attempt, instead it takes a random marks value which divides the dataset(let that value be 90). Now at the first level, the students are divided into two categories (greater than and less than 90). If a student scores more than 90 he will be declared as pass, but if he scores less than 90, then the algorithm will find another random threshold (between 0 and 90) and categorize the remaining instances further. This algorithm too will eventually create an accurate model but it will have far more number of levels and nodes than a Decision Tree. However, the random nature of choosing the threshold value will make it much more faster.","69187db4":"## 7. Comparison\nFinally, lets see how the Extra Trees algorithm compares with Decision Trees and Random Forest in terms of Accuracy and Time. We will use default model parameters instead of hypertuning.","a512998a":"Turns out that max_depth=500 and n_estimators=50 gives the best score of 97.3% which is a slight improvement over default model.","98c7b4e9":"We will use 5-fold cross validation and take its average to compute the accuracy of model.","b5a86c4b":"## 1. Overview\nScikit-Learn is packed with amazing algoritms which serves a wide variety of purposes. Needless to say one algoritm does not fit all datasets. Each algoritm comes with its pros and cons and our role is to keep experimenting and pick the algorithm which serve our needs.\nIn this notebook, we will look at a special case of the famous Random Forest ensemble called Extremely Randomized Trees or Extra Trees and will see how can this ensemble become helpful in our projects. We will also compare it with the traditional Random Forest in terms of computational complexity. To get started lets first look at a basic Decision Tree.","13429360":"### 5.2. Creating a basic Extra Trees Classifier","aa6189fa":"### Please upvote this notebook if you found it to be useful!","76cea5d1":"This algoritm is incredibly fast and took only 0.06 seconds! Reason being that it just trains a single Tree and predicts. Validation score is 91.7%","75ddbc47":"Wow! With the same default parameters, validation score of Extra Trees is very close(slightly better) than that of Random Forest i.e 97%. But the important thing to note here is the time complexity, it trains the model in somewhat 60% time as that taken by Random Forest.","151224ac":"# Random Forest vs Extra Trees","540eb7ca":"### 7.3. Extra Trees","09968898":"## 6. Hypertuning\n\nHere are some of the popular tuning parameters provided by Scikit-Learn :- \n1. <b>n_estimators<\/b> : The number of trees in a forest. Default = 100\n2. <b>max_depth<\/b> : The maximum depth of the forest. Default = None i.e the tree will continue to expand untill all nodes are pure.\n3. <b>min_samples_split<\/b> : The minimum number of samples required to split a node. Default = 2\n4. <b>min_samples_leaf<\/b> : The minimum number of samples required to be at leaf node. This parameter becomes important specially if max_depth=None. Default = 1.\n\nCheck out other parameters in Scikit-Learn's documentation(<a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html?highlight=extra#sklearn.ensemble.ExtraTreesClassifier\">here<\/a>).\n\nHere, we will hypertune n_estimators and max_depth by using GridSerchCV","742d7cdd":"### 7.2. Random Forest","7ae4fe6b":"### 7.1. Decision Tree","897b5662":"Reference - Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aur\u00e9lien G\u00e9ron","a83bb4ff":"## 3. Random Forest\nGoing from Decision Tree to Random Forest is simple. Instead of a single tree on an empty land(let's say apple tree), imagine a forest with 1000s of trees(but not every tree is an apple tree). Each tree bear a slightly different fruit with different taste, colour, or altogether a different fruit. This variety usually makes the harvest of the forest much more high-yielding. \n\nThe Decision Tree takes the whole dataset and creates a tree. While Random Forest produces many trees but each tree is not shown the entire training dataset, it is shown only a part of the dataset and then it predicts the class or value. Later, all predicted values of all trees are cumulated to make the final prediction. \n\nThe random partial data is shown to each tree to bring diversity. Imagine if all the trees are shown the entire dataset, then each tree will bear the same identical apple fruit, which will add no additional value than just using a single Decision Tree algoritm. Look at the following image for better understanding (<a href=\"https:\/\/towardsdatascience.com\/random-forest-classification-and-its-implementation-d5d840dbead0\">Source<\/a>).\n\n<img src=\"https:\/\/miro.medium.com\/max\/1148\/0*a8KgF1IINziv7KIQ.png\" \/>","699c59a8":"## 2. Decision Tree\nAs the name suggest, Decision Tree is a tree with levels (or a depth), each level has nodes and each node takes a decision based on a certain threshold. Each node splits the dataset into two which helps in classifying any instance into categories and collectively into target classes. Look at the following figure for a better understanding (<a href=\"https:\/\/towardsdatascience.com\/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f\">Source<\/a>)\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*fGX0_gacojVa6-njlCrWZw.png\" width=600 height=400\/>\n\n","283be2a0":"## 8. Conclusion\nIts hard to pin out which algorithm is better in terms of accuracy and would depend on testing. However, Extremely Randomized Trees are surely faster than Random Forest due to the random nature of picking up thresholds. Extra Trees can become a very useful algorithm if your dataset is huge and you want to quickly run a Decision Tree ensemble and check how your model performs on the dataset. Check out this link to know the exact time complexities of different machine learning algoritms(<a href=\"https:\/\/www.thekerneltrip.com\/machine\/learning\/computational-complexity-learning-algorithms\/\">Link<\/a>)."}}