{"cell_type":{"1543da2a":"code","29270793":"code","3b589ad4":"code","c36adc96":"code","c2eb4d5a":"code","82f2328f":"code","a0ea3958":"code","fcbc2d22":"code","f6e110c2":"code","bebb27cf":"code","d7b1e102":"code","5b455d30":"code","653a69a1":"code","c71e4aa9":"code","002baaf2":"code","7098a697":"code","2e024248":"code","b01bb420":"code","06f9fe22":"code","bb1778c4":"markdown","3fbd8386":"markdown","55239c6f":"markdown","9b1823c6":"markdown","6255ad54":"markdown","38e7d371":"markdown","96535054":"markdown","f76b1897":"markdown"},"source":{"1543da2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29270793":"from sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot as plt\nimport gc","3b589ad4":"train_data =  pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data =  pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","c36adc96":"X = train_data.drop('target',axis=1).set_index('id')\ny = train_data.target\nX_test = test_data.set_index('id')","c2eb4d5a":"del train_data, test_data\ngc.collect()","82f2328f":"ss = StandardScaler().fit(pd.concat([X,X_test],axis=0))\nX = pd.DataFrame(ss.transform(X),index=X.index,columns=X.columns)\nX_test = pd.DataFrame(ss.transform(X_test),index=X_test.index,columns=X_test.columns)","a0ea3958":"X.head()","fcbc2d22":"X_test.head()","f6e110c2":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)","bebb27cf":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nos.environ[\"KMP_SETTINGS\"] = \"0\" \n\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nlogging.getLogger(\"tensorflow\").addHandler(logging.NullHandler(logging.ERROR))","d7b1e102":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\ntfpl = tfp.layers","5b455d30":"def logistic_regression(input_shape):\n    tf.keras.backend.clear_session()\n    model = Sequential()\n    model.add(InputLayer(input_shape=input_shape))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', \n                  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  metrics=['accuracy',tf.keras.metrics.AUC(name='auc')])\n    model.summary()\n    return model  ","653a69a1":"def train_model(model, X_train, y_train):\n    \n    callbacks = [EarlyStopping(monitor='val_loss',mode='min',patience=20,restore_best_weights=True)]\n    \n    X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n    history = model.fit(x=X_t, y=y_t,\n                batch_size=1024,\n                epochs=5000,\n                validation_data=(X_v,y_v),\n                callbacks=callbacks,\n                verbose=1)\n    # plot graphs\n    fig = plt.figure(figsize=(18, 6))\n    figno = [131,132,133]\n    metrics = ['loss','auc','accuracy']\n    legend_pos = ['upper right','lower right','lower right']\n    for i in range(3):\n        fig.add_subplot(figno[i])\n        metric = metrics[i]\n        plt.plot(history.history[metric])\n        plt.plot(history.history[F'val_{metric}'])\n        plt.title(F'{metric} vs epochs')\n        plt.ylabel(metric)\n        plt.xlabel('Epoch')\n        plt.legend(['Training', 'Validation'], loc=legend_pos[i])\n    plt.show()\n        \n    return model","c71e4aa9":"from sklearn.metrics import accuracy_score, roc_auc_score","002baaf2":"y_pred = train_model(logistic_regression((X.shape[1],)), X_train, y_train).predict(X_valid)\nprint(F'Test accuracy: {accuracy_score(y_valid,y_pred>0.5)}')\nprint(F'Test AUC: {roc_auc_score(y_valid,y_pred)}')\n\n_=plt.hist(y_pred,bins=100)\n      ","7098a697":"def mixture_of_bernoullis(input_shape):\n    tf.keras.backend.clear_session()\n    model = Sequential()\n    model.add(InputLayer(input_shape=input_shape))\n    # replace last layer with sigmoidal activation by a probabilistic layer\n    comps = 2\n    event_shape = [1]\n    params_size = tfpl.MixtureSameFamily.params_size(comps,\n                                                     component_params_size=tfpl.IndependentBernoulli.params_size(event_shape))              \n    model.add(Dense(params_size, activation=None))\n    model.add(tfpl.MixtureSameFamily(comps, tfpl.IndependentBernoulli(event_shape),\n                                    convert_to_tensor_fn=tfp.distributions.Distribution.mean))\n    \n    model.compile(loss=lambda y_true, y_pred: -y_pred.log_prob(y_true), # loss function \"binary cross-entropy\" replaced by \"minus log likelihood\"\n                  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  metrics=['accuracy',tf.keras.metrics.AUC(name='auc')])\n    model.summary()\n    return model","2e024248":"model = train_model(mixture_of_bernoullis((X.shape[1],)), X_train, y_train)\ny_pred = model.predict(X_valid)\nprint(F'Test accuracy: {accuracy_score(y_valid,y_pred>0.5)}')\nprint(F'Test AUC: {roc_auc_score(y_valid,y_pred)}')\n\n_=plt.hist(y_pred,bins=100)","b01bb420":"predictions_test = model.predict(X_test).flatten()","06f9fe22":"pd.DataFrame({'id': X_test.index, 'target': predictions_test}).to_csv('submission.csv', index=False)\nprint(\"Submission saved!\")","bb1778c4":"More than a thousand epochs later, we get the fitted model, and the distribution of predicted probabilities is bimodal as expected.\n\nLet's try our luck and submit the predictions for the test data using this model.","3fbd8386":"# Logistic Regression Model","55239c6f":"# Mixture of Bernoulli Distributions","9b1823c6":"As a warm-up, we first implement logistic regression as a neural network with a single output layer of 1 unit with a sigmoidal activation and no regularization. ","6255ad54":"As can be seen, the logistic model outputs a unimodal distribution of predictions (probabilities). We would expect 2 modes around 0.25 and 0.75, but the logistic model, being a linear model, is not able to capture that. \n\nA common way to improve the fit is to add additional layers into the network, essentially increasing the number of model parameters. We know a deeper neural network is able to capture the bimodal distribution, but there is another direction to generalize the logistic model.","38e7d371":"For simplicity, instead of doing a standard cross-validation, we simply split the training data into half, using one half for training and the half for validating\/testing. Our models will be small, so there should be enough data for training without overfitting.","96535054":"The logistic model (and other multi-layer neural networks) outputs the probability of class \"1\". We can also interpret this output as the parameter (mean) of a Bernoulli distribution. Viewed this way, one way to generalize the model is to replace the Bernoulli distribution by another probability distribution of binary random variable. One such distribution is mixture of Bernoulli distributions. \n\nModeling the probability distribution as a mixture of 2 Bernoullis has the following nice interpretation. There is a common belief that approximately 25% of the original target values of this dataset have been \"flipped\". So the categorical distribution of the mixture (which is another Bernoulli distribution) models the probability $p(1 \\mbox{ before flip})$. The 2 Bernoulli distributions model respectively $p(1|1\\mbox{ before flip})$ and $p(1|0\\mbox{ before flip})$.\n\nTo this end, we want to build a neural network that outputs a probability distribution, not a tensor. The TensorFlow Probability library makes this easy. We simply replace the last dense layer (with single unit) by a dense layer with appropriate number of units *and no activation* that will be fed into a final, non-trainable layer `MixtureSameFamily` that encapsulates the mixture of Bernoullis probability model.","f76b1897":"# Loading Data and Preprocessing"}}