{"cell_type":{"18adcf59":"code","1170ac88":"code","0f5b8be4":"code","735bf85d":"code","cbcd04d7":"code","406c32f2":"code","8300f7ce":"code","30ed4d13":"code","0733fbe3":"code","820a8e36":"code","90cb07fe":"code","1f0e3cf1":"code","730a3229":"code","c078ef52":"code","2ad41f82":"code","0aa88a24":"code","e1c35110":"code","bf8a4050":"code","8fb8e108":"code","ed961426":"code","ac9c59b1":"code","ca6497fa":"code","ce95a150":"code","1dd9d3c7":"markdown","395b3c45":"markdown","5b67a7c7":"markdown","d35a297b":"markdown","5b86e1f9":"markdown","69ffa61c":"markdown","a269e797":"markdown","5b26d923":"markdown","787459d6":"markdown","1a66836d":"markdown","07326894":"markdown","a20210ac":"markdown","cb56584b":"markdown","bdf92a3d":"markdown","d308b26f":"markdown","c39a5ab7":"markdown","5dc65864":"markdown","56a7c72c":"markdown","98ab0228":"markdown","d45b4466":"markdown","2daeb4d4":"markdown","b8ba9946":"markdown","99eebffa":"markdown","08dce2d3":"markdown","6185f337":"markdown","d524798c":"markdown","92454bb8":"markdown","2ce27d9b":"markdown","cb4a074d":"markdown","6d5f034b":"markdown","ff18200b":"markdown"},"source":{"18adcf59":"import numpy as np \nimport pandas as pd \n\nimport os\nimport re\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1170ac88":"import torch\nimport torch.optim as optim\nimport os\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, load_model\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nimport seaborn as sns\nimport scipy\nfrom scipy import spatial\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom keras import regularizers\nimport matplotlib.pyplot as plt\n","0f5b8be4":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","735bf85d":"print(train.head())\nprint(test.head())","cbcd04d7":"test_id = test['id']\ntest.drop(labels = ['id','keyword','location'], axis = 1, inplace = True)\ntrain.drop(labels = ['id','location','keyword'], axis = 1, inplace = True)","406c32f2":"def clean_data(data):\n    data = data.lower()\n\n    # remove unknow characters\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data = emoji_pattern.sub(r'', data)\n\n    # remove http links\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    data = url.sub(r'',data)\n\n    # remove other symbols\n    data = data.replace('#',' ')\n    data = data.replace('@',' ')\n    symbols = re.compile(r'[^A-Za-z0-9 ]')\n    data = symbols.sub(r'',data)\n\n    return data\n    ","8300f7ce":"train['text'] = train['text'].apply(lambda x: clean_data(x)).apply(lambda x:x.split(' '))\ntest['text'] = test['text'].apply(lambda x:clean_data(x)).apply(lambda x:x.split(' '))\n","30ed4d13":"stop = set(stopwords.words('english'))\ntrain['text'] = train['text'].apply(lambda x: [word for word in x if word not in stop])\ntest['text'] = test['text'].apply(lambda x: [word for word in x if word not in stop])","0733fbe3":"print(train.head())\nprint(test.head())","820a8e36":"sns.catplot('target',data = train, kind = 'count',palette = 'Set3')\ntrain.target.value_counts()","90cb07fe":"sample_size = 3271\n\ntrain_norm = train[train.target == 1].sample(sample_size).append(train[train.target == 0].sample(sample_size)).reset_index()","1f0e3cf1":"sns.catplot('target',data = train_norm, kind = 'count',palette = 'Accent')\ntrain_norm.target.value_counts()","730a3229":"train_x = train_norm.text.values\ntrain_y = train_norm.target.values\ntest_x = test.text.values","c078ef52":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(train_x)\nvocab_length = len(word_tokenizer.word_index) + 1","2ad41f82":"word_tokenizer_test = Tokenizer()\nword_tokenizer_test.fit_on_texts(test_x)","0aa88a24":"max_sentence_length = len(max(train_x, key = lambda sentence:len(word_tokenize(str(sentence)))))\n\n# integer encode the data\ndef sequence(corpus):\n  return word_tokenizer.texts_to_sequences(corpus)\n\ntrain_pad = pad_sequences(sequence(train_x),\n                          max_sentence_length,\n                          padding = 'post')\ntest_pad = pad_sequences(sequence(test_x),\n                         max_sentence_length,\n                         padding = 'post')","e1c35110":"embedding_dict={}\n\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f: \n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word]= vectors\nf.close()","bf8a4050":"embedding_dim = 100\nembedding_matrix =  np.zeros((vocab_length, embedding_dim))\nhits = 0\nmisses = 0\n\nfor word, index in word_tokenizer.word_index.items():\n  # If word is in our vocab, then update the corresponding weights\n  embedding_vector = embedding_dict.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[index] = embedding_vector\n    hits += 1\n  else:\n    misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","8fb8e108":"tsne_2d = TSNE(n_components=2, random_state=0)\nwords =  list(embedding_dict.keys())\nvectors = [embedding_dict[word] for word in words]\nembeddings = tsne_2d.fit_transform(vectors[:500])\n\nsns.set_palette('summer')\nplt.figure(figsize=(20,15))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.5)\nfor label, x, y in zip(words, embeddings[:, 0], embeddings[:, 1]):\n    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\nplt.show()","ed961426":"x_train, x_val, y_train, y_val = train_test_split(train_pad,\n                                                  train_y,\n                                                  test_size = 0.25)","ac9c59b1":"def BLSTM():\n  model = Sequential()\n  model.add(Embedding(input_dim = embedding_matrix.shape[0],\n                      output_dim = embedding_matrix.shape[1],\n                      weights = [embedding_matrix],\n                      trainable=False,\n                     input_length = max_sentence_length))\n  model.add(Bidirectional(LSTM(max_sentence_length,\n                               return_sequences = True,\n                               recurrent_dropout = 0.5)))\n  model.add(GlobalMaxPool1D())\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(max_sentence_length, activation = 'relu',kernel_regularizer=regularizers.l2(0.01)))\n  model.add(Dropout(0.5))\n  model.add(Dense(max_sentence_length, activation = 'relu', kernel_regularizer=regularizers.l2(0.001)))\n  model.add(Dropout(0.5))\n  model.add(Dense(1, activation = 'sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n  return model","ca6497fa":"# define the model\nmodel = BLSTM()\n\n# early stopping to avoid overfitting\nes = EarlyStopping(monitor='val_loss', mode='min', \n                   verbose=1, patience= 3)\n\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', \n                     mode='max', verbose=1, save_best_only=True)\n\n# fit the model\ntrain_history = model.fit(x_train, y_train, epochs = 15,\n                          validation_data = (x_val, y_val), \n                          verbose = 0, callbacks=[es, mc])\n\n# load the saved model\nsaved_model = load_model('best_model.h5')\n\n# evaluate the model\n_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n_, val_acc = model.evaluate(x_val, y_val, verbose = 0)\nprint('Train: %.3f,  Validation: %.3f' % (train_acc, val_acc))\n\n# plot training history\nplt.figure(figsize=(10,10))\nsns.set_palette('Pastel1')\nplt.plot(train_history.history['loss'], label='train')\nplt.plot(train_history.history['val_loss'], label='validation')\nplt.legend()\nplt.show()","ce95a150":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission.target = model.predict_classes(test_pad)\nsubmission.to_csv('submission00.csv', index = False)","1dd9d3c7":"- We can see that our training set is imbalanced that needs rebalancing.","395b3c45":"# 1. Introduction\n> One of the critical components in Natural Langauge Processing (NLP) is to encode text information in a numerical format that can be fed into an NLP model. Such technique, representing words in a numerical vector space, is called **Vector Space Modeling**. It is often synonymous to **word embedding**.   \n> **Word embedding** is defining a word by the company (say, the context) that it keeps that allows the word embedding to learn something about the meaning of words. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space. ","5b67a7c7":"# 7. Modelling","d35a297b":"# 0. References:  \n1. https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/   \nby Dr. Jason Brownlee.  \n2. https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/  \nby Dr. Jason Brownle.  \n2. https:\/\/aegis4048.github.io\/understanding_multi-dimensionality_in_vector_space_modeling  \nby Eric Kim.  \n3. https:\/\/mlexplained.com\/2018\/04\/29\/paper-dissected-glove-global-vectors-for-word-representation-explained\/  \nby Keitakurita.  \n4. https:\/\/medium.com\/analytics-vidhya\/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db   \nby Sebastian Theiler.  \n5. http:\/\/speech.ee.ntu.edu.tw\/~tlkagk\/courses_ML20.html  \nby Professor Hung-yi Lee.  \n  \n\nThank you \ud83d\udc67\ud83c\udffd\n","5b86e1f9":"# 5. GloVe - pretrained dataset","69ffa61c":"> Once inside of the `with` statement, we need to loop through each line in the file, and split the line by every space, into each of its components.  \n> After splitting the line, we make the assumption the word does not have any spaces in it, and set it equal the first (or zeroth) element of the split line.  \n> Then we can take the rest of the line, and convert it into a Numpy array. This is the vector of the word\u2019s position.  \n> We can then update our dictionary with the new word and its corresponding vector.","a269e797":"## 4.2 Feature & label splitting","5b26d923":"As decribed:  \n- `id`: unique identifier of each tweet  \n- `text`: content of a tweet  \n- `location`: where the tweet has been sent from\n- `keyword`: keyword from that tweet  \n- `target`: labeled whether a tweet is a real disaster(`1`) or not(`0`)","787459d6":"# 6. Visualization\n> Nothing helps to find insights in data more than visualizing it.  \n\nIn this section, we are going to reduce the dimension of our original word vector from 100-dimension to 3-dimension so as to visualize on a 2D plane.","1a66836d":"## 7.3 Model training   \n> - **`Earlystopping` callback:**   \nas soon as the loss of the model begins to increase on the test dataset, we will stop training.  \n`patience` means that we will allow training to continue for up to an additional 5 epochs after  \nthe point that validation loss started to degrade, giving the training process an opportunity to  \nget across flat spots or find some additional improvement.\n> - **`ModelChecckpoint` callback:**  \nin this case, we are interested in saving the model with the best accuracy on the test dataset,  \n> therefore, we will use accuracy on the validation in the ModelCheckpoint callback to save the   \nbest model observed during training.  \n>  The entire model will be saved to the file \u201cbest_model.h5\u201d only when accuracy on the validation  \ndataset improves overall across the entire training process.","07326894":"- Check resampled distribution.","a20210ac":"## 7.7 Prediction & submission","cb56584b":"- Check data.","bdf92a3d":"## 7.1 Data splitting  \nSplit our training dataset into training and validation.","d308b26f":"# 3. Clean data","c39a5ab7":"**This kernel is going to build a Bidirectional RNN(LSTM) that can improve model performance on sequence classification problems.**\n> In our NLP with Disaster Tweets problem where all timesteps of the input sequence are available,  \nBidirectional LSTMs train two instead of one LSTMs on the input sequence.   \nThe first on the input sequence as-is and the second on a reversed copy of the input sequence.   \nThis can provide additional context to the network and result in faster and even fuller learning on the problem.\n\n1. The Embedding layer is defined as the first hidden layer of our network:  \n  \n    - `input_dim`: the size of the vocabulary in the text data.(how your data have been encoded)   \n    - `output_dim`: the size of the vector space in which word will be embedded. For GloVe 100d, it is 100.    \n    - `input_length`: length of input sequence.\n2. The second hidden layer of has 52 (`max_sentence_length`) memory units and the output layer is a fully connected layer that outputs one value per timestep.  \n","5dc65864":"## 4.3 Tokenization\n> Tokenization is a process of splitting sentences into separate words.  \n  \n> Tokenization usually isn't as simple as splitting senteces based on space bars. It is usually a very complicated process that involves heavy regex manipulation, lemmatization, stemming, and some hard-coding with different rules for different cases.   \n\n> Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the `texts_to_sequences()` method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a `word_index` attribute.","56a7c72c":"- All words in lowercase\n- Remove http links\n- Removing unkonwn characters \\x89U0 etc.\n- Remove special characters ',:;. etc","98ab0228":"## 5.1 Embedding matrix\n> This section prepares a embedding matrix that will be used in a Keras Embedding layer.  \nIt is a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our `vectorizer`'s vocabulary (`word_tokenizer` in section 4.3).  \nThe result is a matrix of weights only for words we will see during training.","d45b4466":"A bit crowded but still can see the correlations.  \nPlease comment me if you have a better idea on this visualization \ud83d\ude48","2daeb4d4":"## 1.1 Word embedding techniques\nThere are two types of methods for word embedding: Count-Based Methods and Prediction-Based Methods.  \n> Count-Based:  \n> - If two words W<sub>i<\/sub> and W<sub>j<\/sub> frequently co-occur, V(W<sub>i<\/sub>) and V(W<sub>j<\/sub>) would be close to each other.  \n> - E.g. Glove Vector:\n> http:\/\/nlp.stanford.edu\/projects\/glove\/   \n\n> Prediction-Based:  \n> - Prediction-based methods use neural network algorithm.\n> - Their methodologies are based on predicting context words given a center word (Word2Vec Skip-Gram: P(w<sub>context<\/sub>\u2223w<sub>center<\/sub>)), or a center word given context words (Continuous Bag of Words: P(w<sub>center<\/sub>\u2223w<sub>context<\/sub>)).  \n> - Say we have 1-of-N encoding for W<sub>i-1<\/sub> as feature vectors of neural network's inputs, then our outputs will be the probability for each word as the next word W<sub>i<\/sub>.\n","b8ba9946":"# 2. Load data & packages","99eebffa":"## 4.4 Padding\n> `pad_sequences` is used to ensure that all sequences in a list have the same length.  \nBy default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.\n\n**Arguments**:  \n- `sequence`: List of sequences (each sequence is a list of integers).  \n- `maxlen`: maximum length of all sequences.  \n- `padding`: String, 'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence.\n","08dce2d3":"**Recurrent Neural Networks**","6185f337":"## 7.2 Model building","d524798c":"## 1.2 Global Vectors (GloVe) as pre-trained word embeddings  \n- This kernel is going to used Glove as pre-trained word embeddings.  \n\n> Pennington et al. present a simple example based on the words ice and steam that explain the basic idea of GloVe:  \n![image.png](attachment:image.png)   \n  \n> Ice and steam differ in their state but are the same in that they are both forms of water. Therefore, we would expect words related to water (like \"water\" and \"wet\") to appear equally in the context of \"ice\" and \"steam\". In contrast, words like \"cold\" and \"solid\" would probably appear near \"ice\" but would not appear near \"steam\".  \nThe probabilities shown here counts how often the word k appears when the words \"ice\" and \"steam\" are in the context, where k  refers to the words \"solid\", \"gas\", \"water\", and \"fashion\". ","92454bb8":"## 4.1 Class distribution & re-sampling","2ce27d9b":"- Remove stop words","cb4a074d":"![image.png](attachment:image.png)","6d5f034b":"For training data, we are only interested in `text` and `target` column.  \nAnd for testing data, we'll use `id` and `text` column.","ff18200b":"# 4. Data preprocess"}}