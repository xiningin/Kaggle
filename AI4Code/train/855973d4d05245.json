{"cell_type":{"62381f74":"code","f2d06ec5":"code","da4d73a8":"code","205d7293":"code","9bfa89e8":"code","a42c67cc":"code","321fd46c":"code","21c08390":"code","ee76d025":"code","1c59d51d":"code","6d55235a":"code","53f861dc":"code","8c80df6f":"code","74594d42":"code","bab0c16b":"code","870b2e3d":"code","284df66a":"code","402a79d7":"code","a39fcea2":"code","7e7b3d05":"code","b8905ced":"code","603ab190":"code","f7af02a3":"code","13a93123":"code","f005ced5":"code","29b977ef":"code","72fc530d":"code","ffedb8e6":"code","b2034054":"code","086e8f07":"markdown","9ba48a8f":"markdown","85dfd008":"markdown","3e56c78a":"markdown","6eb9e1c9":"markdown","f05aa72d":"markdown","7235a2df":"markdown","90c1cf9c":"markdown","8bb61c87":"markdown","04cac7b9":"markdown","4c91edd6":"markdown","0a52d886":"markdown","d7e16cfb":"markdown","b0f4173a":"markdown","993aef3d":"markdown","283c623e":"markdown","8c826f1d":"markdown","a70ce462":"markdown","d1c7b5c3":"markdown","b8202e59":"markdown","b75bae90":"markdown","ff6a34a5":"markdown","f601ffd4":"markdown","f641dbe4":"markdown","ec584e83":"markdown","a22cb3ad":"markdown","189cc88d":"markdown","e163db33":"markdown","aa3c5ef2":"markdown"},"source":{"62381f74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2d06ec5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom scipy.stats import skew, kurtosis\n%matplotlib inline\nsns.set()","da4d73a8":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","205d7293":"# checking the shape of both datasetes\nprint(train.shape)\nprint(test.shape)","9bfa89e8":"target = train[['SalePrice']]\n# dropping the target variable from train dataset\ntrain.drop(columns=['SalePrice'],axis=1, inplace=True) ","a42c67cc":"# Now concating both datasets\ndf = pd.concat([train,test])\ndf.shape","321fd46c":"df.info()","21c08390":"df.drop(columns=['Id'], axis=1, inplace=True)\ndf.shape","ee76d025":"plt.figure(figsize=(16,12))\nmsno.bar(df, labels=True, fontsize=(10))","1c59d51d":"#categorical variables having missing values more than 30%\nfor col in df:\n    if df[col].dtype == 'object':\n        if (df[col].isnull().sum()\/df[col].isnull().count())*100 >=30:\n            print(col)","6d55235a":"df.drop(columns=['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'], axis=1, inplace=True)","53f861dc":"for col in df:\n    if df[col].dtype == 'object':\n        df[col] = df[col].fillna(df[col].mode()[0])","8c80df6f":"# Now for numericals\ncheck = False\nfor col in df:\n    if df[col].dtype != 'object':\n        if (df[col].isnull().sum()\/df[col].isnull().count())*100 >=30:\n            print(col)\n        else:\n            check = True\nif check:\n    print('We do not have missing data more than 30%')","74594d42":"# let's see how much data we have missing in numerical data\nfor col in df:\n    if df[col].dtype != 'object':\n        print(df[col].isnull().sum(), end=' ')","bab0c16b":"for col in df:\n    if df[col].dtype != 'object':\n        df[col] = df[col].fillna(0)","870b2e3d":"plt.figure(figsize=(16,12))\nmsno.bar(df, labels=True, fontsize=(10))","284df66a":"df = pd.get_dummies(df)\ndf.shape","402a79d7":"from sklearn.preprocessing import MinMaxScaler\ndf_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df))\ndf_scaled.columns = df.columns\ndf_scaled","a39fcea2":"train = df_scaled.iloc[:1460,:]\ntest = df_scaled.iloc[1460:,:]\nprint(train.shape)\nprint(test.shape)","7e7b3d05":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(train,target,test_size=0.2,random_state=1)","b8905ced":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_preds = lr.predict(X_val)","603ab190":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=1)\ndt.fit(X_train,y_train)\ndt_preds = dt.predict(X_val)","f7af02a3":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=1)\nrf.fit(X_train,y_train)\nrf_preds = rf.predict(X_val)","13a93123":"from xgboost import XGBRegressor\nxgb = XGBRegressor(random_state=1)\nxgb.fit(X_train,y_train)\nxgb_preds = xgb.predict(X_val)","f005ced5":"from sklearn.metrics import mean_absolute_error","29b977ef":"compare = {'Models':['LinearRegression','DecissionTree','RandomForest','XGBoost'],\n          'MeanAbsoluteError':[mean_absolute_error(lr_preds,y_val), mean_absolute_error(dt_preds,y_val), mean_absolute_error(rf_preds,y_val),\n                              mean_absolute_error(xgb_preds,y_val)]}\npd.DataFrame(compare)","72fc530d":"test_values = xgb.predict(test)\ntest_values","ffedb8e6":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = test_values\nsubmission","b2034054":"submission.to_csv('Submission_3.csv', index=False)","086e8f07":"### Importing the required libraries","9ba48a8f":"**there are object type columns having catogarical data As we know that ML models work on numeric data not on catogarical so we will perfom the one hot encoding to convert catogarical features into numeric and for that pandas has a function named as get_dummies() which itself creates the features from catogaries. Then check their dimensions.**","85dfd008":"**In catogarical we have 5 columns with more than 30% of missing data so we're going to remove them**","3e56c78a":"**Now let's visualize the missing values of whole dataset**","6eb9e1c9":"**Decission Tree**","f05aa72d":"**Hopefully we don't have any missing value left in the dataset**","7235a2df":"**Now let's do it same for the Numerical data first let's see what percent missing values we have in Numerical columns** ","90c1cf9c":"#### After EDA we have to split the datsaet back into two datasets train and dataset","8bb61c87":"# House Price prediction for beginners","04cac7b9":"### This kernel includes following topics\n*     Importing required libraries\n*     Handling Missing values\n*     Data Wrangling, convert categorical to numerical\n*     Applying the basic Regression models of sklearn\n*     Comparing the performance of the Regressors and choosing the best one","4c91edd6":"**As per trend to replace with their mean i don't think so it's good choice to do so becuase missing data has the importance becuase as I earlier mentioned above that sometimes data is missing on purpose so rather than filling it with mean I am going to fill it with 0, near to me zero seems meaningful in handling the data**","0a52d886":"**Random Forest**","d7e16cfb":"![housesbanner.png](attachment:housesbanner.png)","b0f4173a":"**Linear Regression**","993aef3d":"### Normalizing the data\n**It's better to scale the data within the range of 0 and 1 before fitting to model**","283c623e":"**Now we will fill the rest of categorical columns with their most frequent value**","8c826f1d":"**Lets see if we are done with missing values**","a70ce462":"## Handling missing data\n**Handling missing data requires the great strategy and analysis to fit the model, **\n**Sometimes missing data is on purpose and sometimes it's not. So we will handle by looking at the number of missing data if there's missing data more than 30% then we will drop those columns and if less then we will replace the numerical columns with their mean and catogarical columns with their mode**\n\n**Rather than handling the missing data  of train and test dataset individually it's better to concat them but before that we have to seperate the target column from train dataset to make them have equal number of features.**","d1c7b5c3":"### **Loading datasets**","b8202e59":"**XGBoost**","b75bae90":"### We are going to apply few models and let's see which results best","ff6a34a5":"### loading the submission dataset to replace it's values of SalePrice column with our predicted values of test dataset","f601ffd4":"**Let's compare the models with their mean absolute error**","f641dbe4":"### Let's drop the Id column dataset","ec584e83":"**So we are going to test on xgboost model**","a22cb3ad":"**we see that there are a few columns having great number of missing values but let's check them percent wise**","189cc88d":"### Now for training the model we need to split up the train dataset and target variable into X_train,X_val,y_train,y_val variables.","e163db33":"**We see that we have columns with catogarical data and also numerical data but if we see that there's Id column which is nothing but representing the index so we will drop it becuase it has no relation with the target variable**\n\n**In supervised learning like classification and regression problems we have two type of variables**\n* **Feature variables** \n* **Target variables**\n\n**A function we say y=f(x) here y is dependent on the input of x and x has the effect on y as x changes so does y. likely in machine learning we call x the features and y the target variable. With the help of features we predict the target variable.**","aa3c5ef2":"**There's one more column in train dataset which is the target variable, we have to seperate it to make a whole dataframe of train and test dataset for the further analysis.**"}}