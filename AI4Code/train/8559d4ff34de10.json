{"cell_type":{"7dbe729f":"code","ee1a19aa":"code","6bb9d6cc":"code","bae667d6":"code","c5a7278c":"code","6db7828e":"code","a0dfcc33":"code","3a256258":"code","e591455d":"code","696b3d01":"code","590f22f1":"code","050ae62b":"code","29f99f62":"code","da9989f7":"code","d8dabe36":"code","6ca8fc0d":"code","cd55599d":"code","cf6f4fd4":"code","cdd1249e":"code","95f2b855":"code","c3390932":"code","dbb3aa7a":"code","b999c4b1":"code","7361e98d":"code","f10c6b1b":"code","2f28db68":"code","16b53c8b":"code","0762537d":"code","83b3b1dd":"code","65031664":"code","1cc680c4":"code","450271dc":"markdown","71a90b2a":"markdown","5f5c13cd":"markdown","53bbcc6c":"markdown","9af0d70f":"markdown","db57af68":"markdown","bceccf3e":"markdown","36374901":"markdown","fe15fcfe":"markdown","b3c891d8":"markdown","bd249f09":"markdown","6fe06a9f":"markdown","12e38089":"markdown","49ea2d3d":"markdown","be0bab80":"markdown","f1692deb":"markdown","2299023d":"markdown","a9b85bc0":"markdown","eaf75fb4":"markdown","feb3409c":"markdown","5c3fa3ad":"markdown","610ec05c":"markdown","dbad2757":"markdown","6b03604f":"markdown","6abe8b8d":"markdown","364a9d27":"markdown","416da67d":"markdown","7c64fa90":"markdown","85251241":"markdown","4b988758":"markdown","eb07ddfe":"markdown","bd667759":"markdown","23f93dda":"markdown","0797f13b":"markdown","c0e87055":"markdown"},"source":{"7dbe729f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ee1a19aa":"#\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","6bb9d6cc":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","bae667d6":"train_data.info()\ntest_data.info()","c5a7278c":"train_data.head()","6db7828e":"test_data.head()","a0dfcc33":"train_data.describe()","3a256258":"women  = len(train_data.loc[train_data[\"Sex\"]== \"female\"])\nmen = len(train_data.loc[train_data[\"Sex\"]== \"male\"])\nprint(\"number of women:\",women)\nprint(\"number of men:\" , men)","e591455d":"print(train_data.columns)\nprint(len(train_data.columns))","696b3d01":"sns.heatmap(train_data.corr(),annot= True)","590f22f1":"full_data = [train_data, test_data]\nsex_dict = {\"female\":0, \"male\":1}\nfor data_df in full_data:\n    data_df['Sex'] = data_df['Sex'].apply(lambda x:sex_dict[x])","050ae62b":"\nfor data_df in full_data:\n    data_df['Title']  = data_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    data_df['Title'] = data_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    data_df['Title'] = data_df['Title'].replace('Mlle', 'Miss')\n    data_df['Title'] = data_df['Title'].replace('Ms', 'Miss')\n    data_df['Title'] = data_df['Title'].replace('Mme', 'Mrs')\n\n    \ntitle_dict = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor data_df in full_data: \n    data_df['Title'] = data_df['Title'].apply(lambda x: title_dict[x])\n    data_df['Title'] = data_df['Title'].fillna(0)\n    \n\n","29f99f62":"def AgeGroup(age):\n    if(age <= 16):\n        return 0 \n    elif age > 16 and age <= 32:\n        return 1\n    elif age>32 and age <=48:\n        return 2 \n    elif age>48 and age <= 64:\n        return 3\n    else:\n        return 4\n    \nfor data_df in full_data:\n    age_avg = data_df['Age'].mean()\n    age_std = data_df['Age'].std()\n    age_null_count = data_df['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    data_df['Age'][np.isnan(data_df['Age'])] = age_null_random_list\n    data_df['Age'] = data_df['Age'].astype(int)\n    data_df['AgeGoup'] = data_df['Age'].apply(AgeGroup)","da9989f7":"def Alone(familysize):\n    if familysize ==1:\n        return 1 \n    else:\n        return 0\n\nfor data_df in full_data:\n    data_df['Family_size'] = data_df['SibSp'] + data_df['Parch'] + 1\n    data_df['IsAlone'] = data_df['Family_size'].apply(Alone)","d8dabe36":"embarked_dict= {'S': 0, 'C': 1, 'Q': 2}\nfor data_df in full_data:\n    data_df['Embarked'] = data_df['Embarked'].fillna('S')\n    data_df['Embarked'] = data_df['Embarked'].apply(lambda x: embarked_dict[x])","6ca8fc0d":"#type(train_data['Cabin'].iloc[1])\ndef HasCabin(cabin):\n    if type(cabin) == str:\n        return 1\n    else:\n        return 0\n    \nfor data_df in full_data:\n    data_df['HasCabin'] = data_df['Cabin'].apply(HasCabin)","cd55599d":"def FareGroup(fare):\n    if fare <= 7.91:\n        return 0;\n    elif fare >7.91 and fare <=14.454:\n        return 1\n    elif fare >14.454 and fare <=31:\n        return 2\n    else:\n        return 3\n\nfor data_df in full_data:\n    data_df['Fare'] = data_df['Fare'].fillna(data_df['Fare'].median())\n    data_df['FareGroup'] = data_df['Fare'].apply(FareGroup)","cf6f4fd4":"feature_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch']\nfor data_df in full_data:\n    data_df.drop(columns = feature_columns, inplace = True)\n\n    \n#train_data.info()\ntrain_data.head(10)","cdd1249e":"# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","95f2b855":"x_train = train_data.drop(\"Survived\", axis=1)\ny_train = train_data[\"Survived\"]\nx_test  = test_data.copy()\nx_train.shape,y_train.shape","c3390932":"logreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nacc_log","dbb3aa7a":"svc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\nacc_svc","b999c4b1":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nacc_knn","7361e98d":"gaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\nacc_gaussian","f10c6b1b":"perceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)\nacc_perceptron","2f28db68":"\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nacc_linear_svc","16b53c8b":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\nacc_sgd","0762537d":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","83b3b1dd":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nrandom_forest.score(x_train, y_train)\nacc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\nacc_random_forest","65031664":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","1cc680c4":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","450271dc":"## Drop Features\n","71a90b2a":"# Section 1 -- Problem definition\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n","5f5c13cd":"We can see the information given above. There are some missing data in our test and train data(eg: Cabin, Age, Embarked,fare), we will fix this issue later soon.","53bbcc6c":"Observation: People who are survived correlate with fare and Pclass. Passengers are more likely upper socio-economic status with expensive ticket fare, they got more chances to survive. We will dive more on this later on.","9af0d70f":"## Sex\nFemale: 0 \nMale: 1 \n","db57af68":"Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine","bceccf3e":"## Describe","36374901":"## Gaussian Naive Bayes\nIn machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. \nThe model generated confidence score is the lowest among the models evaluated so far.","fe15fcfe":"* 38.3838% people in the train data survived compared with 32.464029% in the real case\n* People are more likely to be middle class for socio-economic status\n* Most of them are young people, age range from 20 - 40\n* Most people travlled without parents, children, sibing and spouses.\n* 64.758698% are male and 35.241302% are female ","b3c891d8":"## Sibsp and Parch\n","bd249f09":"# Section 7 -- Model Deployment","6fe06a9f":"# Section 5 & 6 -- Model Traning, building and testing","12e38089":"## Fare","49ea2d3d":"## K-Nearest Neighbors\n\nIn pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\nKNN confidence score is better than Logistics Regression but worse than SVM.","be0bab80":"Here we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code\/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers.","f1692deb":"# Section 3 -- Data analyze and Exploration (EDA)\n","2299023d":"## Cabin\n","a9b85bc0":"## Linear SVC\n","eaf75fb4":"## Decision Tree\nThis model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference Wikipedia.\n\nThe model confidence score is the highest among models evaluated so far. ","feb3409c":"## Model evaluation\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. ","5c3fa3ad":"## Random Forest\nThe next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference Wikipedia.\n\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.","610ec05c":"Some of the data contain the string values, so we need convert these to numerical values.","dbad2757":"## Features","6b03604f":"## Support Vector Machines\nNext we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.\n\nNote that the model generates a confidence score which is higher than Logistics Regression model.","6abe8b8d":"## Embarked","364a9d27":"## Logistic Regression\nLogistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. \n\nNote the confidence score generated by the model based on our training dataset.","416da67d":"## Perceptron\n\nThe perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. ","7c64fa90":"# Introduction \nThis is my first work of machine learning and data science analyze. This notebook is just for learning :)\n","85251241":"## Correlation\nCorrelation is a statistical technique that can show whether and how strongly pairs of variables are related. \nThe main result of a correlation is called the correlation coefficient (or \"r\"). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.\n\nIf r is close to 0, it means there is no relationship between the variables. If r is positive, it means that as one variable gets larger the other gets larger. If r is negative it means that as one gets larger, the other gets smaller (often called an \"inverse\" correlation).","4b988758":"# Section 4 -- Data cleaning\n\n## Goal\n1. We need to fill out of null vaules\n2. We need to convert non-numeric values to numeric values. \n3. We need to drop some irrelevant values\n","eb07ddfe":"## Name","bd667759":"# Section 2 -- Data acquisition\nWe need to get our data, train data and test data.","23f93dda":"There are total 12 features: \n* 'PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'\n\nCategorical features:\n* Survived, Pclass, Sex, Embarked\n\nCumerical features:\n* Continous: Age, Fare. \n* Discrete: SibSp, Parch.\n \n","0797f13b":"# Workflow  \nMachine learing Process has the followsing 7 steps:\n1. Problem Definition\n2. Data Acquisition \n3. Data analyze and Exploration (EDA)\n4. Data Cleaning \n5. Model Traning and Building \n6. Model Testing and Results Visualization \n7. Model Deployment \n\n\n# Workflow goals\nThe data science solutions workflow solves for seven major goals.\n\n1. Classifying. We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n2. Correlating. One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a correlation among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n3. Converting. For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n4. Completing. Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n5. Correcting. We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n6. Creating. Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n7. Charting. How to select the right visualization plots and charts depending on nature of the data and the solution goals.","c0e87055":"## Age"}}