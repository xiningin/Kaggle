{"cell_type":{"d19cf698":"code","d8362c36":"code","c05502bc":"code","b26d44bc":"code","ac4f2eb5":"code","40e63fdb":"code","159fed7b":"code","5a269bfe":"code","4cb5d379":"code","f68bf2ed":"code","43543a8c":"code","4f532bf4":"code","5110d165":"code","da548e7b":"code","96f3902f":"code","de7baab1":"code","81b4802d":"code","1704ae25":"code","36fcc0bc":"code","e65801db":"code","4cd84687":"code","6704c2b3":"code","f2e81bb6":"markdown","b17f6573":"markdown","ba4e9af5":"markdown","7378568f":"markdown","2d2e749b":"markdown","a18b44e2":"markdown","5d89ca51":"markdown","db74e0ab":"markdown","4bd1a8ba":"markdown","5326d24b":"markdown","1b67056a":"markdown","d9fff117":"markdown","61b4bd0b":"markdown","e791e4fe":"markdown","f5c1736f":"markdown"},"source":{"d19cf698":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8362c36":"pip install openpyxl ","c05502bc":"import openpyxl\ndata = pd.read_excel(\"..\/input\/fyp-topics-2021\/FYP_priorities.xlsx\")\ndata.head()","b26d44bc":"Society_Culture = data[0:128][data['Topic']=='Society_Culture'].reset_index(drop=True)\nSOE = data[0:128][data['Topic']=='SOE'].reset_index(drop=True)\nReunification = data[0:128][data['Topic']=='Reunification'].reset_index(drop=True)\nStrategy = data[0:128][data['Topic']=='Strategy'].reset_index(drop=True)\nLocal_Gov = data[0:128][data['Topic']=='Local_Gov'].reset_index(drop=True)\nMilitary_defense = data[0:128][data['Topic']=='Military_defense'].reset_index(drop=True)\nDebt = data[0:128][data['Topic']=='Debt'].reset_index(drop=True)\nSci_innovation_tech = data[0:128][data['Topic']=='Sci_innovation_tech'].reset_index(drop=True)\nWelfare_health = data[0:128][data['Topic']=='Welfare_health'].reset_index(drop=True)\nGreen = data[0:128][data['Topic']=='Green'][0:128].reset_index(drop=True)\nemployment = data[0:128][data['Topic']=='employment'].reset_index(drop=True)\nChina_led_globalization = data[0:128][data['Topic']=='China_led_globalization'].reset_index(drop=True)\nPoverty_allev = data[0:128][data['Topic']=='Poverty_allev'].reset_index(drop=True)\nParty_building = data[0:128][data['Topic']=='Party_building'].reset_index(drop=True)\nEconomics = data[0:128][data['Topic']=='Economics'].reset_index(drop=True)\nEdu = data[0:128][data['Topic']=='Edu'].reset_index(drop=True)","ac4f2eb5":"from plotly import tools\nimport plotly.graph_objs as go\nimport plotly.express as px\ntrace1 = go.Scatter(x=Society_Culture['Year'],\n                    y=Society_Culture['True_weight'],\n                    mode='lines',\n                    name='Society',\n                    text='Culture',\n                    line=dict(width=3,color='darkgray'))\n\ntrace2 = go.Scatter(x = SOE['Year'],\n                    y = SOE['True_weight'],\n                    mode='lines',\n                    name='SOE',\n                    text='State Enterprise',\n                    line=dict(width=3,color='black'))\n\ntrace3 = go.Scatter(x = Reunification['Year'],\n                    y = Reunification['True_weight'],\n                    mode='lines',\n                    name='Reunification',\n                    text='HK & Taiwan',\n                    line=dict(width=3,color='lime'))\n                 \ntrace4 = go.Scatter(x = Strategy['Year'],\n                    y = Strategy['True_weight'],\n                    mode='lines',\n                    name='Strategy',\n                    text='Systemic',\n                    line=dict(width=3,color='dodgerblue'))\n                   \ntrace5 = go.Scatter(x = Local_Gov['Year'],\n                    y = Local_Gov['True_weight'],\n                    mode='lines',\n                    name='Local Governance',\n                    #text='',\n                    line=dict(width=3,color='purple'))\n                   \ntrace6 = go.Scatter(x = Military_defense['Year'],\n                    y = Military_defense['True_weight'],\n                    mode='lines',\n                    name='Military',\n                    text='Defense',\n                    line=dict(width=3,color='darkred'))\n\ntrace7 = go.Scatter(x = Debt['Year'],\n                    y = Debt['True_weight'],\n                    mode='lines',\n                    name='Financial Debt',\n                    #text='SOE',\n                    line=dict(width=3,color='silver'))\n\ntrace8 = go.Scatter(x = Sci_innovation_tech['Year'],\n                    y = Sci_innovation_tech['True_weight'],\n                    mode='lines',\n                    name='Science Tech',\n                    text='Innovation Tech',\n                    line=dict(width=3,color='skyblue'))\n\ntrace9 = go.Scatter(x = Green['Year'],\n                    y = Green['True_weight'],\n                    mode='lines',\n                    name='Green Dev.',\n                    text='Development',\n                    line=dict(width=3,color='springgreen'))\n\ntrace10 = go.Scatter(x = Welfare_health['Year'],\n                    y = Welfare_health['True_weight'],\n                    mode='lines',\n                    name='Welfare_health',\n                    #text='Welfare_health',\n                    line=dict(width=3,color='steelblue'))\n\ntrace11 = go.Scatter(x = employment['Year'],\n                    y = employment['True_weight'],\n                    mode='lines',\n                    name='Employment',\n                    #text='',\n                    line=dict(width=3,color='teal'))\n\ntrace12 = go.Scatter(x = China_led_globalization['Year'],\n                    y = China_led_globalization['True_weight'],\n                    mode='lines',\n                    name='Diplomacy',\n                    #text='China_led_globalization',\n                    line=dict(width=3,color='orange'))\n\ntrace13 = go.Scatter(x = Poverty_allev['Year'],\n                    y = Poverty_allev['True_weight'],\n                    mode='lines',\n                    name='Poverty',\n                    text='Alleviation',\n                    line=dict(width=3,color='turquoise'))\n\ntrace14 = go.Scatter(x = Party_building['Year'],\n                    y = Party_building['True_weight'],\n                    mode='lines',\n                    name='Party Building',\n                    text='Politics',\n                    line=dict(width=3,color='tomato'))\n\ntrace15 = go.Scatter(x = Economics['Year'],\n                    y = Economics['True_weight'],\n                    mode='lines',\n                    name='Economics',\n                    text='Industries',\n                    line=dict(width=3,color='deeppink'))\n\ntrace16 = go.Scatter(x = Edu['Year'],\n                    y = Edu['True_weight'],\n                    mode='lines',\n                    name='Education',\n                    #text='Education',\n                    line=dict(width=3,color='violet'))\n\nframes = [dict(data= [dict(type='scatter',\n                           x=Society_Culture['Year'][:k+1],\n                           y=Society_Culture['True_weight'][:k+1]),\n                      dict(type='scatter',\n                           x=SOE['Year'][:k+1],\n                           y=SOE['True_weight'][:k+1]),\n                      dict(type='scatter',\n                           x=Reunification['Year'][:k+1],\n                           y=Reunification['True_weight'][:k+1]),\n                      dict(type='scatter',\n                           x=Strategy['Year'][:k+1],\n                           y=Strategy['True_weight'][:k+1]), \n                    dict(type='scatter',\n                           x=Local_Gov['Year'][:k+1],\n                           y=Local_Gov['True_weight'][:k+1]), \n                     dict(type='scatter',\n                           x=Military_defense['Year'][:k+1],\n                           y=Military_defense['True_weight'][:k+1]),\n                      dict(type='scatter',\n                           x=Debt['Year'][:k+1],\n                           y=Debt['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Sci_innovation_tech['Year'][:k+1],\n                           y=Sci_innovation_tech['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Green['Year'][:k+1],\n                           y=Green['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Welfare_health['Year'][:k+1],\n                           y=Welfare_health['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=employment['Year'][:k+1],\n                           y=employment['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=China_led_globalization['Year'][:k+1],\n                           y=China_led_globalization['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Poverty_allev['Year'][:k+1],\n                           y=Poverty_allev['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Party_building['Year'][:k+1],\n                           y=Party_building['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Economics['Year'][:k+1],\n                           y=Economics['True_weight'][:k+1]),\n                    dict(type='scatter',\n                           x=Edu['Year'][:k+1],\n                           y=Edu['True_weight'][:k+1]),\n    \n                     ],\n               #traces= [0,1,2,3,4,5,6],  #2, 3\n              )for k  in  range(0, len(data)-1)]\n\nlayout = go.Layout(title = '<B>Historical FYP Topics<\/B> <br><i>Measuring Topical Weight Distribution<\/i>',\n                   titlefont = {\"size\": 20},\n                   yaxis = dict(title='Topic Distribution Weight'),\n                   xaxis = dict(title='Historical FYP'),\n                             #'range': [0, 12.5],\n                             #dtick: 1 )\n                   width=800,\n                   height=600,\n                   showlegend=True,\n                   hovermode='x unified',\n#                    updatemenus=[\n#                         dict(\n#                             type='buttons', showactive=False,\n#                             y=1.1,\n#                             x=1.15,\n#                             xanchor='right',\n#                             yanchor='top',\n#                             pad=dict(t=0, r=10),\n#                             buttons=[dict(label='PLAY',\n#                             method='animate',\n#                             args=[None,\n#                                   dict(frame=dict(duration=200,\n#                                                   redraw=False),\n#                                                   transition=dict(duration=0.02),\n#                                                   fromcurrent=True,\n#                                                   mode='immediate')]\n#                             )]\n#                         ),\n\n#                     ],\n                       images=[dict(\n                        source=\"https:\/\/media-exp1.licdn.com\/dms\/image\/C4D0BAQEdsy-Fo0jvtA\/company-logo_200_200\/0\/1611150327287?e=2159024400&v=beta&t=rS_2azuL19V4cx68707iSA7GTC8e2IPKEumJPBP2Dns\",\n                        xref=\"paper\", yref=\"paper\",\n                        x=1, y=1.05,\n                        sizex=0.2, sizey=0.2,\n                        xanchor=\"right\", yanchor=\"bottom\"\n                                  )],\n                   #template='ggplot2'\n                  )\n\nfig = go.Figure(data=[trace15,trace8,trace4,trace16,trace2,trace1,trace3,trace5,trace6,trace7,trace9,trace10,trace11,trace12,trace13,trace14], frames=frames, layout=layout) #,trace3,trace4,trace5,trace6\n\nannotations = []\nannotations.append(dict(xref='paper', yref='paper', x=1.0, y=0.1,\n                              xanchor='left', yanchor='bottom',\n                              text='<B>[Filter above by Topics]<br> \u00a9 Policybot.io',\n                              font=dict(family='Arial',\n                                        size=12,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\n\nannotations.append(dict(xref='paper', yref='paper', x=0.5, y=-0.1,\n                              xanchor='center', yanchor='top',\n                              text='Data source: State Council <br>http:\/\/www.gov.cn\/xinwen\/2016-03\/17\/content_5054992.htm',\n                              font=dict(family='Arial',\n                                        size=12,\n                                        color='rgb(150,150,150)'),\n                              showarrow=False))\n\nannotations.append(dict(\n  x = '12th',\n  y = 0.4,\n  text = '2012 - Xi Jinping takes Office',\n  xref = \"x\",\n  yref = \"y\"\n))\n\nfig.update_layout(annotations=annotations)\nfig.show()","40e63fdb":"# import plotly.plotly as py\n# import plotly.graph_objs as go\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nx=['0.095420407','0.00938455', '0.024825397', '0.236095954', '0.063893132', '0.083281265', '0.283451169',\n   '0.125493666', '0.123702056', '0.047249479', '0.059801556', '0.06762725',\n   '0.134483568', '0.309224763', '0.120163077', '0.041390367']\n\ny=['Society & Culture',\n   'Reunification (HK & Taiwan)',\n   'SOE Development', \n   'Strategy & Systemic Development',\n   'Local Governing', \n   'Military & Defense',\n   'Science, Innovation, & Tech', \n   'Green Sustainability Development',\n   'Social Welfare & Health Care',\n   'Employment',\n   'Foreign Diplomacy', \n   'Poverty Alleviation',\n   'Party Building', \n   'Economics & Industrial Development',\n   'Education Development',\n   'Financial Debt']\n\npred = pd.concat([pd.DataFrame(y),pd.DataFrame(x)],axis=1)\npred.columns = ['topic','perc']\npred['perc'] = pred.perc.astype('float')\npred = pred.sort_values(by=['perc']).reset_index(drop=True)\n\ndata = [\n    go.Bar(\n        x=list(pred.perc),\n        y=list(pred.topic),\n        marker=dict(\n            #color='rgb(220, 0, 0)',\n            color='#C0392B',\n            line=dict(color='rgb(0, 0, 0)',\n                      width=2)\n        ),\n        orientation='h',\n    )\n]\n\nlayout = go.Layout(\n    images=[dict(\n        source=\"https:\/\/media-exp1.licdn.com\/dms\/image\/C4D0BAQEdsy-Fo0jvtA\/company-logo_200_200\/0\/1611150327287?e=2159024400&v=beta&t=rS_2azuL19V4cx68707iSA7GTC8e2IPKEumJPBP2Dns\",\n        xref=\"paper\", yref=\"paper\",\n        x=1, y=1.05,\n        sizex=0.2, sizey=0.2,\n        xanchor=\"right\", yanchor=\"bottom\"\n      )],\n    autosize=True, height=800, width=700,\n    bargap=0.15, bargroupgap=0.1,\n    barmode='stack', hovermode='x',\n    margin=dict(r=20, l=300,\n                  b=75, t=125),\n    title=\"<br><B>Forecasting China's 14th Five Year Plan Priorities<\/B> <br><i>Predicting Topical Importance Ranking<\/i>\",\n    titlefont = {\"size\": 19},\n    xaxis=dict(\n        dtick=10, nticks=0,\n        #x=0.5, y=0.3,\n        gridcolor='rgba(102, 102, 102, 0.4)',\n        linecolor='#000', linewidth=1,\n        mirror=True,\n        showticklabels=True, tick0=0, tickwidth=1,\n        title='<i>Weight in Percent %<\/i> <br><B>Baseline Random-Forest Model 76.04% Accuracy<\/B>',\n    ),\n    yaxis=dict(\n        anchor='x',\n        gridcolor='rgba(102, 102, 102, 0.4)', gridwidth=1,\n        linecolor='#000', linewidth=1,\n        mirror=True, showgrid=False,\n        showline=True, zeroline=False,\n        showticklabels=True, tick0=0,\n        type='category',\n    ),\n    template='seaborn'\n)\nfig = go.Figure(data=data, layout=layout)\n\nannotations = []\nannotations.append(dict(xref='paper', yref='paper', x=0.5, y=0.0,\n                              xanchor='center', yanchor='top',\n                              text='Data source: State Council <br>http:\/\/www.gov.cn\/xinwen\/2016-03\/17\/content_5054992.htm',\n                              font=dict(family='Arial',\n                                        size=12,\n                                        color='rgb(150,150,150)'),\n                              showarrow=False))\nfig.update_layout(annotations=annotations)\nfig.show()","159fed7b":"import openpyxl\ndata = pd.read_excel(\"..\/input\/fyp-topics-2021\/FYP_priorities.xlsx\")\ndata[32:112].reset_index(drop=True)","5a269bfe":"features = pd.get_dummies(data)\nTRAIN = features[32:112].reset_index(drop=True)\nTEST = features[112:128].reset_index(drop=True)\n\ntrain_features = TRAIN.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1) # ,'Lag_draft_var','Lag_true_var'\ntest_features = TEST.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1) # ,'Lag_draft_var','Lag_true_var'\ntrain_labels = np.array(TRAIN['True_weight'])\ntest_labels = np.array(TEST['True_weight'])\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(train_features, train_labels);\n# Use the forest's predict method on the test data\npredictions_mod1 = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions_mod1 - test_labels)\n# Print out the mean absolute error (mae)\nprint('RMSE: ', mean_squared_error(test_labels, predictions_mod1, squared=False))\nMAPE = 100 * (errors \/ test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(MAPE)\nprint('Accuracy:', round(accuracy, 2), '%.')","4cb5d379":"features = pd.get_dummies(data)\nTRAIN = features[32:112].reset_index(drop=True)\nTEST = features[112:128].reset_index(drop=True)\n\ntrain_features = TRAIN.drop(['True_weight'], axis = 1) \ntest_features = TEST.drop(['True_weight'], axis = 1) \ntrain_labels = np.array(TRAIN['True_weight'])\ntest_labels = np.array(TEST['True_weight'])\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(train_features, train_labels);\n# Use the forest's predict method on the test data\npredictions_mod2 = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions_mod2 - test_labels)\n# Print out the mean absolute error (mae)\nprint('RMSE: ', mean_squared_error(test_labels, predictions_mod2, squared=False))\nMAPE = 100 * (errors \/ test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(MAPE)\nprint('Accuracy:', round(accuracy, 2), '%.')","f68bf2ed":"# Ensembling the results from the two models by average the estimations for each topic\npredictions = (predictions_mod1+predictions_mod2)\/2\npredictions","43543a8c":"print('RMSE: ', mean_squared_error(test_labels, predictions, squared=False))\nerrors = abs(predictions - test_labels)\nMAPE = 100 * (errors \/ test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(MAPE)\nprint('Accuracy:', round(accuracy, 2), '%.')","4f532bf4":"# Split the data into training and testing sets\nTRAIN = features[32:128].reset_index(drop=True)\nTEST = features[128:].reset_index(drop=True)\ntrain_features = TRAIN.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1)\ntest_features = TEST.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1)\ntrain_labels = np.array(TRAIN['True_weight'])\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(train_features, train_labels);\n# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n\nPRED_14th = data[128:].reset_index(drop=True)\nPRED_14th['Predicted_True_weight'] = pd.DataFrame(predictions)\nPRED_14th[['Year','Topic','draft_weight','Predicted_True_weight']]","5110d165":"# bootstrap 1000 times\n# Split the data into training and testing sets\nresults = []\nfor i in range(1,1000):\n\n    TRAIN = features[32:128].reset_index(drop=True)\n    TEST = features[128:].reset_index(drop=True)\n    train_features = TRAIN.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1)\n    test_features = TEST.drop(['True_weight','Lag_draft_var','Lag_true_var'], axis = 1)\n    train_labels = np.array(TRAIN['True_weight'])\n    from sklearn.ensemble import RandomForestRegressor\n    # Instantiate model with 1000 decision trees\n    rf = RandomForestRegressor(n_estimators = 100)\n    # Train the model on training data\n    rf.fit(train_features, train_labels)\n    # Use the forest's predict method on the test data\n    predictions = rf.predict(test_features)\n    PRED_14th = data[128:].reset_index(drop=True)\n    #PRED_14th['Predicted_True_weight'] = pd.DataFrame(predictions)\n    #PRED_14th[['Year','Topic','draft_weight','Predicted_True_weight']]\n    results.append([i,pd.DataFrame(predictions)])\n    ","da548e7b":"results","96f3902f":"#results[3][1].sort_values(0)\ndata = pd.concat([results[0][1],PRED_14th['Topic']],axis=1).sort_values(0,ascending=False)\n# data.assign(priority = lambda v: v.x.apply(lambda x: 'High' if x>=0.2 else None),\n#          priority = lambda v: v.y.apply(lambda x: 'Medium' if 0.1<=x<0.2 else None),\n#          priority = lambda v: v.z.apply(lambda x: 'Low' if x<=0.1 else None),)\ndata","de7baab1":"data.columns = ['score','Topic']","81b4802d":"# data.assign(priority = lambda v: v.x.apply(lambda x: 'High' if x>=0.2 else None),\n#          priority = lambda v: v.y.apply(lambda x: 'Medium' if 0.1<=x<0.2 else None),\n#          priority = lambda v: v.z.apply(lambda x: 'Low' if x<=0.1 else None),)\n#data[0] = data[0].astype('float')\n\ndef func(row):\n    if row[0] < 0.1:\n        return 'low'\n    elif (row[0] >= 0.1) & (row[0] < 0.2):\n        return 'medium' \n    elif row[0] >= 0.2:\n        return 'high' \n    \nCLASSES = []\nfor i in range(0,len(results)):\n    data = pd.concat([results[i][1],PRED_14th['Topic']],axis=1).sort_values(0,ascending=False)#.reset_index(drop=True)\n    data['priority'] = data.apply(func, axis=1)\n    print(data)\n    CLASSES.append(data)\n    \n    ","1704ae25":"ALL = pd.concat(CLASSES).reset_index(drop=True)\nALL","36fcc0bc":"ALL.to_csv(r'\/results.csv')","e65801db":"import os\n%cd \/kaggle\/working","4cd84687":"ECON= ALL[ALL['Topic']=='Strategy']\nECON['priority'].value_counts()","6704c2b3":"results[3][1]\nfor i in range(0,len(results)):\n    print(pd.concat([results[i][1],PRED_14th['Topic']],axis=1))","f2e81bb6":"# Exploring the Data","b17f6573":"> ### Model 2: with Lag weights from Draft_weights and True_weights\n- Underlying assuption about the documents: previous FYPs have an impact on how the current FYP will be formulated. The past decides the present, and the present influence the future. The Lag_vars are meant to caputure this autocorrelation between consecutive FYPs. \n- The Lag_variables of the drafted and actual weights of topics are there to capture structural nuances of the construct of FYPs. Our belief is that there are some structural nuances that could provide predictive power to forecasting actual topical weights of the 14th Plan. We see some of these signals when we randomized the prediction on the test set. ","ba4e9af5":"# How to Forecast China's 2021 Five Year Plan (FYP) \nUpdated February 28, 2021\nBy James Smith and Jian R.\n\nBig thanks to M. Gross from INWT Statistics and Policybot.io for helping us to brainstorm the methodolgies. ","7378568f":"# 1. Abstract and Motivation\n[Combining information from Multiple Sources in Bayesian Modeling](https:\/\/dukespace.lib.duke.edu\/dspace\/bitstream\/handle\/10161\/12840\/Schifeling_duke_0066D_13606.pdf?%20sequence=1) is not new in the statistical science literature. The application of Bayesian framework, however, within the greater domain of political science, particularly in China's political system, has been under-studied.  The [2008 U.S. election forecasts](https:\/\/datascience.foundation\/sciencewhitepaper\/big-data-analytics-and-predicting-election-results) has renaissanced an array of computational social science and political methodological tool kits for academia, civic technologists, and the greater public. However  prevalent they may be, the practice of these predictive applications has been saturated in democratic elections, as shown in the recent U.S. 2020 Election with [Nate Silver's FiveThirtyEight Forecasters](https:\/\/fivethirtyeight.com\/features\/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19\/) and [The Economist](https:\/\/projects.economist.com\/us-2020-forecast\/president\/how-this-works).  With the 2021 German *Bundestagswahl* democratic election coming up, many German forecasting enthusiasts, including \"[Wer-gewwint-die-Wahl](https:\/\/www.wer-gewinnt-die-wahl.de\/)\" and [Zweitstimme](http:\/\/zweitstimme.org\/) are also actively racing to accurately model multi-party political coalitions and government oppositions. Gro\u00df's [long-short term even memory state-space model](https:\/\/github.com\/INWTlab\/lsTerm-election-forecast\/blob\/master\/Poster\/190820_Poster_StanCon_2019a.pdf) is a Bayesian approach that utilize hybrid data sources. The predicitive analytics application to authoritarian soceities are few and far between. *Zhong et Chan's* [Reading China: Predicting Policy Change with Machine Learning](https:\/\/policychangeindex.org\/pdf\/Reading_China.pdf) was one of the first pivotal methodological products that  highlighted advanced quantitative methods to policymaking changes. The [Policy Influence Index](https:\/\/www.researchgate.net\/publication\/349608883_A_Machine_Learning_Approach_in_Predicting_China%27s_Industrial_Policy_Movements_Methodology_Section_3_Only?channel=doi&linkId=6038317b92851c4ed59922c4&showFulltext=true) borrowed a natural physics concept from Newton's Second Law of Motion and deployed a supervised classifier to predict industrial policy. \n\nThis research goes beyond harvesting objective documentary data provided by the [State Council](http:\/\/english.www.gov.cn\/) and the [National Development Reform Commission (NDRC)](https:\/\/en.ndrc.gov.cn\/); similar to Gro\u00df's overall approach ([Gro\u00df, 2021](https:\/\/github.com\/INWTlab\/lsTerm-election-forecast\/blob\/master\/Poster\/190820_Poster_StanCon_2019a.pdf)), it attempts to apply multi-sourced Bayesian approach that weighs human subjective judgment, as well as the objective topical distribution of FYP documents for prediction. Using bootstrap sampling techniques to simulate the prediction outcome 1000 times coupled with a robust anonymous survey of China experts, we hope to categorize sources of uncertainties and make more realistic estimations of the Five Year Plan topical weights.     ","2d2e749b":"> ### Model 1: Without Lag Variables","a18b44e2":"# Limitations: all models are wrong some are useful\n\nThere are a few limitations with our approach:\n* Test labels are clearly defined subjectively by a group of domain experts, who may not capture the nuances from evolving vocabulary, leading to an outdated set of Bag of Words.\n* **Uncertainties not captured in the model**: Since the FYP draft has been drafted in November 2020, two major events may drive how the Five Year Plan may play out:\n> 1. Biden's election: the Chinese national development planners have to make their own predictions under 4 more years of Trump or consider a new strategy of development if Biden wins. Now Biden won, they may have to alter the planning priorities based on new information.\n> 2. Covid-19 pandemic shocks: any major shocks in pandemic outbreaks can trigger enough concerns between November 2020 and March 2021, that the planners would reconsider current health capacities and shift priorities. This current model is oblivious to these shocks. We need a better information retreival system that captures these event data and calculates the probablities of shocks.\n\n## Next steps: Bayesian approach as a Solution\n### A survey directed towards domain experts that reflects their opinions on FYP priorities can be leveraged to improve the model. Thus a Bayesian approach adopting hybrid text-data and expert opinion data could potentially solve these uncertainty issues. \n\n# If you are a China policy expert, please take this [survey](https:\/\/forms.gle\/12fiCEea4hEZE7aA9) to help us improve the model! Thanks for you collaboration!","5d89ca51":"## Clearly Definining the Topical Dimensions\n![image.png](attachment:image.png)\n\n## Topical Dimensions for this Model Include:\n* 'Society & Culture',\n* 'Reunification (HK & Taiwan)',\n* 'SOE Development', \n* 'Strategy & Systemic Development',\n* 'Local Governing', \n* 'Military & Defense',\n* 'Science, Innovation, & Tech', \n* 'Green Sustainability Development',\n* 'Social Welfare & Health Care',\n* 'Employment',\n* 'Foreign Diplomacy', \n* 'Poverty Alleviation',\n* 'Party Building', \n* 'Economics & Industrial Development',\n* 'Education Development',\n* 'Financial Debt'","db74e0ab":"# Modeling Brainstorming\n\n- Hypothesis: Party Document can predict topical similarities of the actual Five Year Plan \n\n- Given: October Party Document publication already has suggested topics to focus on the real 5YP, but during the period between November and March, the drafting process will choose: \n\n\n\"We propose using association analysis and ensemble forecasting to automatically discover topics from a set of text documents and forecast their evolving trend in a near future. In order to discover meaningful topics, we collect publications from a particular research area, data mining and machine learning, as our data domain. An association analysis process is applied to the collected data to first identify a set of topics, followed by a temporal correlation analysis to help discover correlations between topics, and identify a network of topics and communities. After that, an ensemble forecasting approach is proposed to predict the popularity of research topics in the future.\" ([Hurtado, et al](https:\/\/link.springer.com\/article\/10.1186\/s40537-016-0039-2)) \n\n\n* For all these applications, the underlying technical problem is essentially twofold (1) how to summarize and generate meaningful topics from a set of documents (topic mining or discovery); and (2) how to forecast the trend of topics in the future (topic forecasting).\n\n* We propose a fine-grained topic discovery approach using sentence-level pattern mining to discover meaningful topics. Our method is highly scalable and efficient for large scale documents.\n\n* We propose to employ ensemble forecasting to predict future trends of research topics. Our empirical validation demonstrates strong dependency between topic correlations and the forecasting quality. By utilizing the topic dependency, our ensemble Random Forest forecasting model achieves good performance gain.\n\nOur model visually demonstrates the community of research topics, which help understand evolutionary relationships of different topics.\n\n\n* In order to forecast one specific topic, we need to select a target field (or target topic), which is the field of the data needs to be forecasted. We can select one target field (i.e., selecting one topic as the target). In this case, the forecasting model will only use the time series historical data of the selected field to predict the future value of the same field. In addition, we can also select multiple fields as targets, then the system will use time series of multiple topics to predict their future values.\n\n* A potential advantage of selecting multiple fields as targets is that, compared to forecasting using a single field, temporal correlations of different topics may help improve the forecasting accuracy. For example, if \u201csocial networks\u201d and \u201cgraph mining\u201d are strongly correlated, we may select both of them as target fields, and use their temporal correlations to help improve the forecasting accuracy. Accordingly, the fields to be selected as the targets will affect the final forecast for any given target topic. Due to this reason, we need to decide that when forecasting a future value for a topic Xi which topic(s) should also be selected to help forecast X i \u2019s future value.\n\n* This method assumes that only the highest correlated topic is necessary for the successful prediction of a topic. We also used the highest correlated field along with one randomly generated field along with the chosen topic as well. This method also uses a ensemble forecasting by generating 100 different forecasting where each forecaster varies by which topic was randomly chosen for target selection. The final forecast is the average of all forecasters.\n\n## Three types of outcomes to predict:\n\n1. which topics to leave in the plan (can give probabilities of topic survival, given the feedback docs) \n2. which new topics will be added (only gained these topical insights from feedback documents or \u610f\u89c1 \u5efa\u8bae between november and march) \n3. which topics to drop  \n\n \n## Leading Driving Questions: \n\n* What topics will be on the real 5 Year Plan in March 2021? \n* Can we predict the distribution of topics appearing on the plan? \n* Using baysian method to predict whether or not a topic will be on the real 5YP or not. ","4bd1a8ba":"# Assessing Predictive Performance\n\nBelow, we are calculating the accuracy by averaging the MAPE. This gives us a rough estimate of the model overall accuracy when we generalize the model to the 14th Five Year Plan after we have ensembled the two models. The reason we ensemble the results, or averaging the estimated predictions from the two models, is because we are uncertain whether *lag_draft_var* and *lag_true_weight* are both effective at predicting the true weights. On one hand, results show that the lag_vars do not add any predictive power to model, as shown by model 2, it has a lower accuracy rate than model 1, which contains no *lag_vars*. By averaging the results together, we can perhaps hedge against certain shocks in topical distribution. Whether the structural characteristics of the FYPs is still an open question, but what we know is that this could be captured by the Lag_variables of the weights. \n","5326d24b":"# **Training the Model: Testing on the 13th FYP**","1b67056a":"# Predicting 14th FYP Topical Weights\n","d9fff117":"* *Year*:: Plan #\n* *Topic*:: categorical variable for onehot encoding\n* *draft_weight*:: weights of topics from historical FYP \u5efa\u8bae released in November\n* *True_weight*:: weights of topics from Actual FYP published in March\n* *Lag_draft_var*:: lagged down by plan\n* *Lag_true_var*:: lagged down by plan","61b4bd0b":"### Mean Average Percentage Error (MAPE)\n\nMAPE is commonly used because it\u2019s easy to interpret and easy to explain. For example, a MAPE value of 11.5% means that the average difference between the predicted value and the actual value is 11.5%.\n\nThe lower the value for MAPE, the better a model is able to predict values. For example, a model with a MAPE of 5% is more accurate than a model with a MAPE of 10%.","e791e4fe":"# 2. Data Preparation\nThere are two main data sources the model attempts to learn from, namely, through capturing the structural objective components from the topical distributions and subjective perceptions via an anonyomous expert opinion survey.\n\n### Objective Reality: Documents Matters \nPredicting topics on China's Five Year Plan is like predicting US election. Whereas poll data, arguably, is the single most important data source as an election approaches. BUT for China's FYP forecastig, the single most important document that gives the heaviest weight is, not surprisingly, the Five Year Plan Draft (\u5efa\u8bae) which usually is published in Nomvember prior to the March official FYP release. Previous plans from 8th-13th FYP are included in the modeling training proess because they are the fundamental manifold of empirical topical distributions and they show structural components of policy priorities.\n\n### Subjective Judgment: Expert Opinions\nExpert opinions provides insightful biases. In statistics, we need biases to make good predictions, not too much, not too little, but just enough signals to make a generalizable prediction . The Bayesian model also makes use of expert attitudes via surveys, that reflect expert opinions regarding the priorities rankings. Experts are defined as political scientists, politicians, academia, or people working for a think-tank institute or foundation. A list of potential topics was given to the experts, then the experts ranked these priorities according to *low, medium, or high*. Up to date, the model considers ___ survey responses with ___ rankings. This data also provides critical priors for our Bayesian workflow. \n\n\n## 2.1 Tagging Topical Labels\nDebatably the most controversial part of the Bayesian framework is how we set our assumptions (or Priors, apriori knowledge), a feature different from the [Frequentist approach](https:\/\/towardsdatascience.com\/frequentist-vs-bayesian-approaches-in-machine-learning-86ece21e820e). The way we think about the world is actually, what I argue, a continuous updating process of ever-changing external phenomenon, from which we generalize subjective judgments that impact our decision making process. The intelligent thing, however aritificial or organic it is, thus uses a system of complex labeling system to quantify and update its model based on its time-space frame of reference. Thus, this step the classification of known topics is partially about declaring on what grounds is our model going to be established. It is Kant himself who says, *if we know all the qualities of a thing, then we know the thing itself.*  Thus, we must be able to label clearly the ontological existence of concepts and the sub-features that comprise them. For validating the labeling process, interviews with China domain experts were conducted to collectively store these policy labels and saved them in a Python function.  In order to teach a concept to the machine,  keywords which define that concept must be deposited into \u201cBag of Words (BoW) algorithm.\u201d  This research uses BoW technique to define the concept for the machine to cross reference. It is essentially a set of predefined elements operating under the set theory. A set is a collection of elements or members.  The mathematical concept used in the algorithm is called \u201cBag Algebra\u201d or Multiset, which essentially is the union of two documents (the \u201cdefined\u201d keywords and the new policy document) in the BoW notation: \n\n![image.png](attachment:image.png)\n \nFor example, if Automobile is the concept, then one can define this concept with a set of keywords such as {\u201csteering wheel\u201d, \u201cengine\u201d, and \u201ctires\u201d}. This set of keywords, for the sake of definition, is called the \u201cDefinition Set\u201d or \u201cDefinition BoW\u201d of the Automobile .  \n\n\n","f5c1736f":"\n# **Methodology**\n\n"}}