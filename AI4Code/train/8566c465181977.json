{"cell_type":{"3b8a1313":"code","f1db33ac":"code","88079527":"code","46265c7d":"code","c6c0dca0":"code","9c5355e4":"code","98b8a98f":"code","44089a02":"code","0befdfe6":"code","e7482925":"code","57b5a0ed":"code","d691898a":"code","7de93375":"code","eb79f7f6":"code","91b0a451":"code","89ce1686":"code","c4cb33e2":"code","0dfd46a7":"code","b3abc958":"code","41a536c5":"code","872610b9":"code","b42291db":"code","5e23a3a6":"code","0332599e":"code","203d5f50":"code","3433c7f1":"code","a3f4cede":"code","843c0ac5":"code","d2f3ab10":"code","b20332f6":"code","1f8ec738":"code","8e9ab92d":"code","ce6e46cf":"code","13ef6653":"code","1c6d14e3":"code","52668ada":"code","9086d6d3":"code","7f5af147":"code","b89cb21c":"code","74b6b9ba":"code","bf8e77e0":"code","f3c25ad1":"code","b196fd75":"code","833c843a":"code","2a6d2076":"code","3e054804":"code","d3d5a23f":"code","d15a9f27":"code","11c0f481":"code","9aabc634":"code","cafeebb7":"code","86286c74":"code","64d7d825":"code","6c1e1237":"code","65a88d88":"markdown","beeb104f":"markdown","9e0c3fbf":"markdown","c99c1d57":"markdown","05c95c76":"markdown","3cbe8237":"markdown","e16a601e":"markdown","71bbf214":"markdown","88ac7dfc":"markdown","482d76eb":"markdown","f8b89777":"markdown","da13548e":"markdown","f629a510":"markdown","4fa01337":"markdown","57de98e0":"markdown","8ea61ed9":"markdown","4bb72eb2":"markdown","163b761a":"markdown","6b2c3605":"markdown","45258e97":"markdown","b9c5af3c":"markdown","396f8a0e":"markdown","1f8bd1c3":"markdown","6facd9fc":"markdown","228d6373":"markdown","848e3350":"markdown","e10172b8":"markdown","1d77cbb5":"markdown","8ebafeb6":"markdown","2d798316":"markdown","a7e028b5":"markdown","6414127e":"markdown","c51612b7":"markdown","12cd8919":"markdown","f9e711ce":"markdown","3bbdcfc1":"markdown"},"source":{"3b8a1313":"import string\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","f1db33ac":"metadata = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv')\nmetadata.head(2)","88079527":"len(metadata)","46265c7d":"print(metadata.columns)","c6c0dca0":"# Calculate mean of vote average column\nC = metadata['vote_average'].mean()\nC","9c5355e4":"# Calculate the minimum number of votes required to be in the chart, m\nm = metadata['vote_count'].quantile(0.90)\nm","98b8a98f":"# Filter out all qualified movies into a new DataFrame\nq_movies = metadata.copy().loc[metadata['vote_count'] >= m]\nq_movies.shape","44089a02":"# Function that computes the weighted rating of each movie\ndef weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # Calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","0befdfe6":"# Define a new feature 'score' and calculate its value with `weighted_rating()`\nq_movies['score'] = q_movies.apply(weighted_rating, axis= 1)","e7482925":"# Sort movies based on score calculated above\nq_movies.sort_values(by= 'score', ascending= False, inplace= True)\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(20)","57b5a0ed":"metadata['overview'].head()","d691898a":"metadata['overview'].isnull().sum()","7de93375":"metadata['overview'] = metadata['overview'].fillna('')\nprint(metadata['overview'].isnull().sum())","eb79f7f6":"tfidf = TfidfVectorizer(stop_words= 'english')\ntfidf_matrix = tfidf.fit_transform(metadata['overview'])\ntfidf_matrix.shape","91b0a451":"tfidf.get_feature_names()[7000:7010]\n","89ce1686":"from sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ntfidf_matrix = tfidf_matrix[:30000] # 30000 for prevent allocate more memory than is available. It has restarted.\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","c4cb33e2":"cosine_sim.shape\n","0dfd46a7":"cosine_sim[1]","b3abc958":"indices = pd.Series(index=metadata['title'], data = metadata.index).drop_duplicates()\nindices","41a536c5":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim= cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n    \n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    \n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key= lambda x : x[1], reverse= True)\n    \n    # Get the scores of the 10 most similar movies, we stared with 1 because index 0 will give the same name of the input title \n    sim_scores = sim_scores[1:11]\n    \n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n    \n    # Return the top 10 most similar movies\n    return metadata['title'][movie_indices]\n","872610b9":"get_recommendations('The Dark Knight Rises')","b42291db":"get_recommendations('The Godfather')","5e23a3a6":"# Load keywords and credits\ncredits = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\nkeywords = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')\n\n# Remove rows with bad IDs.\nmetadata = metadata.drop([19730, 29503, 35587])\n\n# Convert IDs to int. Required for merging\nkeywords['id'] = keywords['id'].astype('int')\ncredits['id'] = credits['id'].astype('int')\nmetadata['id'] = metadata['id'].astype('int')\n\n# Merge keywords and credits into your main metadata dataframe\nmetadata = metadata.merge(credits, on='id')\nmetadata = metadata.merge(keywords, on='id')\nmetadata.head(2)","0332599e":"metadata.columns","203d5f50":"len(metadata)","3433c7f1":"# # Limit the data for memory size\nmetadata = metadata[:20000]","a3f4cede":"features = ['cast', 'crew', 'keywords', 'genres']\nmetadata[features].head()    ","843c0ac5":"# Parse the striliteral_evalfied features into their corresponding python objects\nfrom ast import literal_eval\n\nfor feature in features:\n    metadata[feature] = metadata[feature].apply(literal_eval)\n\nmetadata[features].head()    ","d2f3ab10":"# metadata['crew'][0]","b20332f6":"# metadata['cast'][0]","1f8ec738":"# metadata['keywords'][0]","8e9ab92d":"# metadata['genres'][0]","ce6e46cf":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","13ef6653":"def get_list_or_top_3(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        # get just 3 or less\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    # return empty list if it missing data!\n    return []","1c6d14e3":"# Define new director\nmetadata['director'] = metadata['crew'].apply(get_director)\nmetadata.head(2)\n","52668ada":"# Define new director cast, genres and keywords features.\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    metadata[feature] = metadata[feature].apply(get_list_or_top_3)\n    \nmetadata[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","9086d6d3":"# Function to convert all strings to lower case and strip names of spaces\ndef clean_text(txt):\n    if isinstance(txt, list):\n        return [i.replace(\" \", \"\").lower() for i in txt]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(txt, str):\n            return txt.replace(\" \", \"\").lower()\n        else:\n            return ''\n        ","7f5af147":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    metadata[feature] = metadata[feature].apply(clean_text)\n\nmetadata[features].head(2)    ","b89cb21c":"def create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n","74b6b9ba":"# Create a new soup feature\nmetadata['soup'] = metadata.apply(create_soup, axis=1)\nmetadata[['soup']].head()","bf8e77e0":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(metadata['soup'])\ncount_matrix.shape\n","f3c25ad1":"from sklearn.metrics.pairwise import cosine_similarity\n# count_matrix = count_matrix[:30000] # 20000 for prevent allocate more memory than is available. It has restarted.\ncosine_sim_2 = cosine_similarity(count_matrix, count_matrix)\ncosine_sim_2.shape","b196fd75":"# Reset index of our main DataFrame and construct reverse mapping as before\nmetadata = metadata.reset_index()\nindices = pd.Series(index=metadata['title'], data = metadata.index )\nindices = pd.Series(index=metadata['title'], data = metadata.index).drop_duplicates()\nindices.head()","833c843a":"get_recommendations('The Dark Knight Rises', cosine_sim_2)","2a6d2076":"get_recommendations('The Godfather', cosine_sim_2)","3e054804":"ratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')\nratings.head()","d3d5a23f":"movies = metadata[['id', 'title', 'genres']]\nmovies['genres'] =  [\"\".join(st) for st in movies['genres']] \nmovies.head()","d15a9f27":"pd.crosstab(ratings['userId'], movies['id']).head()","11c0f481":"encoder = LabelEncoder()\nratings['user'] = encoder.fit_transform(ratings['userId'].values)\nn_users = ratings['user'].nunique()\n\nratings['movie'] = encoder.fit_transform(ratings['movieId'].values)\nn_movies = ratings['movie'].nunique()\n\n\nratings['rating'] = ratings['rating'].values.astype(np.float32)\n\nmin_rating = min(ratings['rating'])\nmax_rating = max(ratings['rating'])\n\nn_users, n_movies, min_rating, max_rating","9aabc634":"X = ratings[['user', 'movie']].values\ny = ratings['rating'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","cafeebb7":"# Number of latent factors\nemb_sz = 50\n\n# User embeddings\nuser = layers.Input(shape=(1,))\nuser_emb = layers.Embedding(n_users, emb_sz, embeddings_regularizer=regularizers.l2(1e-6))(user)\nuser_emb = layers.Reshape((emb_sz,))(user_emb)\n\n# Movie embeddings\nmovie = layers.Input(shape=(1,))\nmovie_emb = layers.Embedding(n_movies, emb_sz, embeddings_regularizer=regularizers.l2(1e-6))(movie)\nmovie_emb = layers.Reshape((emb_sz,))(movie_emb)\n\n# Dot product\nrating = layers.Dot(axes=1)([user_emb, movie_emb])\nrating = layers.Activation('sigmoid')(rating)\nrating = layers.Lambda(lambda x:x*(max_rating - min_rating) + min_rating)(rating) # scalling up\n\n# Model\nmodel = models.Model([user, movie], rating)\n\n# Compile the model\nmodel.compile(loss='mse', metrics=metrics.RootMeanSquaredError(),\n              optimizer=optimizers.Adam(lr=0.001))\n\n# Show model summary\nmodel.summary()\nplot_model(model)","86286c74":"model.fit(x=[X_train[:,0], X_train[:,1]], y=y_train,\n          batch_size=64, epochs=5, verbose=1,\n          validation_data=([X_test[:,0], X_test[:,1]], y_test))","64d7d825":"# Number of latent factors\nemb_sz = 50\n\n# User embeddings\nuser = layers.Input(shape=(1,))\nuser_emb = layers.Embedding(n_users, emb_sz, embeddings_regularizer=regularizers.l2(1e-6))(user)\nuser_emb = layers.Reshape((emb_sz,))(user_emb)\n\n# User bias\nuser_bias = layers.Embedding(n_users, 1, embeddings_regularizer=regularizers.l2(1e-6))(user)\nuser_bias = layers.Reshape((1,))(user_bias)\n\n# Movie embeddings\nmovie = layers.Input(shape=(1,))\nmovie_emb = layers.Embedding(n_movies, emb_sz, embeddings_regularizer=regularizers.l2(1e-6))(movie)\nmovie_emb = layers.Reshape((emb_sz,))(movie_emb)\n\n# Movie bias\nmovie_bias = layers.Embedding(n_movies, 1, embeddings_regularizer=regularizers.l2(1e-6))(movie)\nmovie_bias = layers.Reshape((1,))(movie_bias)\n\n# Dot product\nrating = layers.Concatenate()([user_emb, movie_emb])\n\n# Add biases\nrating = layers.Add()([rating, user_bias, movie_bias])\nrating = layers.Dense(10, activation='relu')(rating)\nrating = layers.Dense(1, activation='sigmoid')(rating)\nrating = layers.Lambda(lambda x:x*(max_rating - min_rating) + min_rating)(rating)\n\n# Model\nmodel = models.Model([user, movie], rating)\n\n# Compile the model\nmodel.compile(loss='mse',  metrics=metrics.RootMeanSquaredError(),\n              optimizer=optimizers.Adam(lr=0.001))\n\n# Show model summary\nprint(model.summary())\nplot_model(model, to_file='model.png')","6c1e1237":"model.fit(x=[X_train[:,0], X_train[:,1]], y=y_train,\n          batch_size=64, epochs=5, verbose=1,\n          validation_data=([X_test[:,0], X_test[:,1]], y_test))","65a88d88":"use the `.copy()` method to ensure that the new q_movies DataFrame created is independent of your original metadata DataFrame. In other words, any changes made to the q_movies DataFrame will not affect the original metadata data frame.","beeb104f":"Well, from the above output, we can see that the `simple recommender` did a great job!\n\nSince the chart has a lot of movies in common with the IMDB Top 250 chart: for example, your top two movies, \"Shawshank Redemption\" and \"The Godfather\", are the same as IMDB and we all know they are indeed amazing movies, in fact, all top 20 movies do deserve to be in that list, isn't it?\n\n","9e0c3fbf":"## Recommendation system Collaborative Filtering.\n**Now let's build our third version of recommendation system `Collaborative Filtering` Recommender.**\n \n","c99c1d57":"We will build a system that recommends movies that are `similar to a particular movie.` To achieve this, you will compute `pairwise cosine similarity` scores for all movies based on their descriptions and recommend movies based on that similarity score threshold.\n\nThe description is available to you as the `overview` feature in your metadata dataset","05c95c76":"**Here we make a regularization to help model generalization by `embeddings_regularizer` hyperparameter, and we should make this regularization `on all embedding tables` as a specific for avoiding `overfitting`**","3cbe8237":"Compute the pair-wise cosine similarity between all overviews of movies. \n\n\n$$x.y = ||x||.||y||.cos(\\theta)$$\n\nSo \n$$cos(\\theta) = \\frac{x.y}{||x||||y||}$$\n\nSo we can use the `linear_kernel` from sklearn, which is faster than `cosine_similarity`","e16a601e":"**Recommender systems can be classified into 3 types:**\n\n**1- Simple recommenders**: `offer generalized recommendations to every user, based on movie popularity and\/or genre`. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience. `An example could be IMDB Top 250.`\n\n**2- Content-based recommenders**: `suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc`. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person likes a particular item, he or she will also like an item that is similar to it. And to recommend that, it will make use of the user's past item metadata. `A good example could be YouTube, where based on your history, it suggests you new videos that you could potentially watch.`\n\n**2- Collaborative filtering engines**: these systems are widely used, and they `try to predict the rating or preference` that a user would give an item-based on past ratings and preferences of other users. `Collaborative filters do not require item metadata like its content-based counterparts`.\n","71bbf214":"From your new features, cast, crew, and keywords, you need to extract the three most important actors, the director and the keywords associated with that movie.\n\nBut first things first, your data is present in the form of \"stringified\" lists. You need to convert them into a way that is usable for you.\n* `literal_eval` evaluate strings containing Python code in the current Python environment.","88ac7dfc":"We need to `extract some kind of features` from the above text data before you can `compute the similarity` between them.\n\nYou will compute `(TF-IDF)` vectors for each document. This will give you a `matrix where each column represents a word in the overview vocabulary`.","482d76eb":"Get the director's name from the crew feature. If the director is not listed, return NaN\n\n","f8b89777":"We are now in a position to create our \"metadata soup\", which is a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords).","da13548e":"Next, you will write a function that will return the `top 3 elements or the entire list`, whichever is more. Here the list refers to the `cast`, `keywords`, and `genres`.\n\n* The `isinstance()` function returns True if the specified object is of the specified type, otherwise False.\n\n\n","f629a510":"make each row of genres as a one string, because it will work good as long as i not using the sequance.","4fa01337":"We need to map movie titles and DataFrame indices. In other words, you need a mechanism to identify the index of a movie in your metadata DataFrame, given its title.\n","57de98e0":"**Adding more features to our recommendation system.**","8ea61ed9":"## Recommendation system Content-Based.\n**Now let's build our second version of recommendation system Content-Based Recommender.**\n","4bb72eb2":"* Each one row of `crew` have a lot of dict which is here indicate to character like Directing, Writing, Screenplay, etc..\nse wee need to extract each `job` will be`Driector` and \n\n* Each one of `cast` have a lot of dict which is here indicate to echr start with his\/her playrole .. so we need to extract `name`.\n* Each one of `keyword` have a lot of dict which is here indicate movie keywords, so we need to extract `name`.\n* Each one of `generas` have a lot of dict which is here indicate the movies which type will follow.\n ","163b761a":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them.\n\nRemoving the spaces between words is an important preprocessing step. It is done so that your vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same. After this processing step, the aforementioned actors will be represented as \"johnnydepp\" and \"johnnygalecki\" and will be distinct to your vectorizer.\n\nAnother good example where the model might output the same vector representation is \"bread jam\" and \"traffic jam\". Hence, it is better to strip off any space that is present.\n\n","6b2c3605":"Extract the train and test data for the data","45258e97":"In the upove code, the latent factors are depend on the dot product of users and movie ratings.\nBut there are might be some prior movie or user factor, that dosen't depend on the relation between the user and hi ratings!\nLike `how much movie is popular` which is independant of the specific user rating.Say a user never clicks a certain category of movies (popular), how can we recommend to him?\nOr how much `user likes moives`, independant of his rating to specific movie.","b9c5af3c":"There are 539 null values in `overview` feature, so se must fill null values.","396f8a0e":"**Let's build our first version of recommendation system**\n## Simple Recommender.\nSimple Recommender System based on the metric below:\n- weightedRating(WR) = ((v\/v+m).R) + ((m\/v+m).C)\n * v --> is the number of votes for the movie. (vote_count)\n * m --> is the min votes required to be listed in chart. (based on negative vote)\n * R --> is the average rating of the movie. (vote_average)\n * C --> is the mean vote across the the whole report. (calculate from data)\n","1f8bd1c3":"**Here also make regularization on`user_bias` and`movie_bias` embeddings layers**.","6facd9fc":"We will make new two embeddings tables for `user_bias` and `movie_bias`, and **not not going to work with Dot layer, but adding after Dot**.","228d6373":"**Each movie will be a 1 * 30000 column vector where each column will be a similarity score with each movie.**","848e3350":"The quality of your recommender would be increased with the usage of better metadata and by capturing more good features. That is precisely what you are going to do in this section. We will build a recommender system based on the following metadata: the 3 top actors, the director, related genres, and the movie keywords.\n\nThe keywords, cast, and crew data are not available in our current dataset, so the first step would be to load and merge them into your main DataFrame metadata.\n\n","e10172b8":"We see that over 75827 different words were used to describe the 45466 movies in our dataset.\n\n","1d77cbb5":"The next steps are the same as what we did with our plot description based recommender. One important difference is that we use the **CountVectorizer()** instead of TF-IDF. This is because we do not want to down-weight the presence of an actor\/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense.\n\n","8ebafeb6":"Next, you will use the `cosine_similarity` to measure the distance between the embeddings.\n\n","2d798316":"Here `RMSE ~ 90%`","a7e028b5":"There are gaps in each user id and in each movie id, **It's not sequential numbers like 0,1,2,3,4 .. etc it something like 862, 8844, 15602, .. etc**, so we will use `LabelEncoder()` for making them in a sequential way like 0,1,2,3,4 .. etc.","6414127e":"You can now reuse your `get_recommendations()` function by passing in the new `cosine_sim_2` matrix as your second argument.\n\n","c51612b7":"You already have the values to `v (vote_count)` and `R (vote_average)` for each movie in the dataset. It is also possible to directly calculate C from this data.\n\n\nDetermining an appropriate value for `m` is a hyperparameter that you can choose accordingly since there is no right value for `m`. You can consider it as a preliminary negative filter that will simply remove the movies which have a number of votes less than a certain threshold `m`. The selectivity of your filter is up to your discretion.\n\nWe will use cutoff m as the **`90th`** percentile. In other words, for a movie to be featured in the charts, it must have more votes than at least 90% of the movies on the list. (In top of 10%)","12cd8919":"With this matrix in hand, you can now compute a similarity score. There are several similarity metrics that you can use for this, such as the manhattan, euclidean, the Pearson, and the cosine similarity scores. Again, there is no right answer to which score is the best. \n\nYou will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. You use the **cosine similarity** score since it is independent of magnitude and is `relatively easy and fast to calculate`.\n\n\n**Since we have used the TF-IDF vectorizer, calculating the dot product between each vector will directly give you the cosine similarity score. Therefore, you will use sklearn's `linear_kernel()` instead of cosine_similarities() since it is faster.**\n","f9e711ce":"Recommender systems are among the most popular applications of data science today. They are used to predict the \"rating\" or \"preference\" that a user would give to an item. Almost every major tech company has applied them in some form. Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow.\n\nWhat's more, for some companies like Netflix, Amazon Prime, Hulu, and Hotstar, the business model and its success revolves around the potency of their recommendations. Netflix even offered a million dollars in 2009 to anyone who could improve its system by 10%.\n\nThere are also popular recommender systems for domains like restaurants, movies, and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services. YouTube uses the recommendation system at a large scale to suggest you videos based on your history. For example, if you watch a lot of educational videos, it would suggest those types of videos.\n\n","3bbdcfc1":"Collaborative filters can further be classified into two types:\n\n* `User-based Filtering`: these systems **recommend products to a user that similar users have liked**. For example, let's say Alice and Bob have a similar interest in books (that is, they largely like and dislike the same books). Now, let's say a new book has been launched into the market, and Alice has read and loved it. It is, therefore, highly likely that Bob will like it too, and therefore, the system recommends this book to Bob.\n\n* `Item-based Filtering`: these systems are extremely similar to the content recommendation engine that we have been built. **These systems identify similar items based on how people have rated it in the past**. For example, if Alice, Bob, and Eve have given 5 stars to The Lord of the Rings and The Hobbit, the system identifies the items as similar. Therefore, if someone buys The Lord of the Rings, the system also recommends The Hobbit to him or her.\n"}}