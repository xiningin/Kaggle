{"cell_type":{"ee104122":"code","b80ce861":"code","af881da5":"code","e470d072":"code","821086e3":"code","92c28c9b":"markdown"},"source":{"ee104122":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection, metrics\nfrom catboost import CatBoostRegressor","b80ce861":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\nX = train.drop('loss', axis=1).iloc[:, :10] # only the ten first features will be used for the purpose of demonstration\ny = train.loss\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv').iloc[:, :10]","af881da5":"# just run this cell\n\nclass LOFO(object):\n    \n    def __init__(self, data, labels, model, n_splits, eval_metric):\n        self._data = data\n        self._labels = labels\n        self.model = model\n        self.n_splits = n_splits\n        self.eval_metric = eval_metric\n        \n        \n    def kfold(self, x, y, model, n_splits, eval_metric):\n        from sklearn import model_selection\n        \n        preds_valid_array = np.zeros(y.shape[0])\n        \n        train_scores = []\n        valid_scores = []\n        \n        kf = model_selection.KFold(n_splits=n_splits)\n        for fold, (train_idx, valid_idx) in enumerate(kf.split(x)):\n            \n            print(f\"===================== Fold {fold+1} =====================\")\n            x_train, y_train = x[train_idx, :], y[train_idx]\n            x_valid, y_valid = x[valid_idx, :], y[valid_idx]\n            \n            self.model.fit(\n                          x_train, y_train,\n                          eval_set=[(x_valid, y_valid)],\n                          verbose=100\n                         )\n            \n            preds_valid = model.predict(x_valid)\n            preds_train = model.predict(x_train)\n            \n            valid_score = np.sqrt(eval_metric(y_valid, preds_valid))\n            train_score = np.sqrt(eval_metric(y_train, preds_train))\n            \n            valid_scores.append(valid_score)\n            train_scores.append(train_score)\n            \n            preds_valid_array[valid_idx] += preds_valid\n            \n        print(\"Mean valid score =\", np.mean(valid_scores), \"STD valid score = \", np.std(valid_scores, ddof=1))\n        print(\"Mean train score =\", np.mean(train_scores), \"STD train score = \", np.std(train_scores, ddof=1))\n        \n        cv_score = np.mean(valid_scores)\n        return cv_score, preds_valid_array\n    \n    def selectionLoop(self, x, y, model, n_splits, eval_metric):\n        \n        print(\"All Features\")\n        cv_score, preds_valid = self.kfold(x, y, model, n_splits, eval_metric)\n        score = cv_score\n        scores = []\n        good_scores = []\n        scores.append(score)\n        good_scores.append(score)\n        harmful_features = []\n        print(\"=================================================\")\n        \n        \n        for i in range(x.shape[1]):\n            \n            print(f\"Drop Feature {i}\")\n            x2 = pd.DataFrame(x, columns=[f\"col_{i}\" for i in range(x.shape[1])])\n            x2 = x2.drop(x2.columns[i], axis=1)\n            x2 = x2.dropna(axis=1, how='all').values\n            cv_score, preds_valid = self.kfold(x2, y, model, n_splits, eval_metric)  \n            if cv_score < score:\n                score = cv_score\n                print(\"Improved Score =\", score)\n                good_scores.append(score)\n                harmful_features.append(i)\n                x = pd.DataFrame(x, columns=[f\"col_{i}\" for i in range(x.shape[1])])\n                x.iloc[:, i] = np.nan\n                x = x.values\n                print(\"=================================================\")\n                \n            else:\n                continue\n        \n        print(\"Good scores :\", good_scores)\n        print(\"Harmful features :\", harmful_features)\n        \n        return good_scores, harmful_features\n    \n    def transform(self, X, test):\n        \n        X = self._data\n        y = self._labels\n        model = self.model\n        n_splits = self.n_splits\n        eval_metric = self.eval_metric\n        \n        good_scores, harmful_features = self.selectionLoop(X.values, y.values, model, n_splits, eval_metric)\n        X = X.drop(list(X.columns.tolist()[i] for i in harmful_features), axis=1)\n        X = X.dropna(axis=1, how='all')\n        test = test.drop(list(test.columns.tolist()[i] for i in harmful_features), axis=1)\n        \n        return X, test","e470d072":"# pass your arguments here\n# X, y and test should be in the form of Pandas DataFrames    \nlofo = LOFO(X, y, model=CatBoostRegressor(\n                                   learning_rate=0.03,\n                                   iterations=10000,\n                                   loss_function='RMSE',\n                                   eval_metric='RMSE',\n                                   use_best_model=True,\n                                   early_stopping_rounds=100\n                                   ), n_splits=5, eval_metric=metrics.mean_squared_error)\n\nX, test = LOFO.transform(lofo, X, test)","821086e3":"# let's see the reduction in X and test sets after performing LOFO\nprint(\"X shape after transformation :\", X.shape)\nprint(\"test shape after transformation :\", test.shape)","92c28c9b":"# LeaveOneFeatureOut (LOFO):\n* This is one of the most powerful and guaranteed techniques for feature selection. It iteratively drops out one feature at a time and tests the performance of your selected model on your selected folds. If the dropout of the feature improves the CV score, then the training set gets updated and continues the iterations for furthur improvement. <br>\n* This notebook contains an easy-to-use code for the purpose of this competition, however, it can be used for any other tabular dataset and for any task with just little modifications. <br>\n* All you need is to pass your training set (X), label (y), model, n_splits (for KFold cross validation), and evaluation metric.\n* Because this might take a significant amount of time in the case of 100 features, this code will be run of the first 10 features only.\n* Moreover, you can use a simpler model to speedup the process of FS."}}