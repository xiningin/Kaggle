{"cell_type":{"d4d2e8a3":"code","0b5d8d2f":"code","4d8d2882":"code","3244ef79":"code","f04552b6":"code","badfbf84":"markdown"},"source":{"d4d2e8a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport tensorflow as tf\nimport unicodedata\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport os\nimport io\nimport time","0b5d8d2f":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, encode_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.encode_units = encode_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n        # parameters:\n        # return_sequences: Boolean. Whether to return the last output\n        # \t    in the output sequence, or the full sequence. Default: `False`.\n        # return_state: Boolean. Whether to return the last state in addition to the\n        #\t    output. Default: `False`.\n        self.gru = tf.keras.layers.GRU(self.encode_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        # x: (batch_size, max_seq_len)\n        x = self.embedding(x)\n\n        # x: (batch_size, max_seq_len, embed_dim)\n        output, state = self.gru(x, initial_state=hidden)\n\n        # output: (batch_size, max_seq_len, encode_units)\n        # state: (batch_size, encode_units); the last state in the sequence\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_size, self.encode_units))\n    \n\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, decode_units, batch_size):\n        super(Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.decode_units = decode_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.decode_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        self.attention = Attention(self.decode_units)\n\n    def call(self, x, hidden, encode_output):\n        \"\"\"\n            x: (batch_size, 1)\n            hidden: (batch_size, encode_units)\n            encode_output: (batch_size, max_seq_len, encode_units)\n        \"\"\"\n        # x (input): (batch_size, 1)\n        x = self.embedding(x)\n\n        # x (after embedding): (batch_size, 1, embed_dim)\n        x, state = self.gru(x)\n\n        # x (after gru): (batch_size, 1, decode_units)\n        # encode_output: (batch_size, max_seq_len, encode_units)\n        x, attention_weights = self.attention(x, encode_output)\n\n        # x (after attention): (batch_size, 1, encode_units)\n        # flatten x\n        x = tf.reshape(x, (-1, x.shape[2]))\n        # x (after reshape): (batch_size, encode_units)\n\n        x = self.fc(x)\n        # x (after fc): (batch_size, vocab_size)\n\n        return x, state, attention_weights\n    \n\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.dot = tf.keras.layers.Dot((2, 2))\n        self.fc = tf.keras.layers.Dense(units, activation='tanh')\n\n    def call(self, query, values):\n        \"\"\"\n            query is the decoder output: (batch_size, 1, decode_units)\n            values are the encoder output: (batch_size, max_seq_len, encode_units)\n        \"\"\"\n        scores = self.dot([query, values])\n\n        # scores: (batch_size, 1, max_seq_len)\n        attention_weights = tf.nn.softmax(scores)\n\n        # attention_weights: (batch_size, 1, max_seq_len)\n        # values: (batch_size, max_seq_len, encode_units)\n        context_vector = tf.keras.layers.dot([attention_weights, values], axes=(2, 1))\n\n        # context_vector: (batch_size, 1, encode_units)\n        # query: (batch_size, 1, decode_units)\n        y = tf.keras.layers.concatenate([context_vector, query])\n\n        # y (after concat): (batch_size, 1, encode_units*2)\n        y = self.fc(y)\n\n        # y (after fc): (batch_size, 1, encode_units)\n        return y, attention_weights","4d8d2882":"@tf.function\ndef train_step(inp, targ, targ_lang, enc_hidden, encoder, decoder, optimizer, loss_fn, loss_object):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n        dec_hidden = enc_hidden\n        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n        for t in range(1, targ.shape[1]):\n            # at each time step, enc_output is passed to the decoder\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n            loss += loss_fn(targ[:, t], predictions, loss_object)\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss \/ int(targ.shape[1]))\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss\n\n\ndef loss_function(real, pred, loss_object):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","3244ef79":"def convert(lang, tensor):\n    for t in tensor:\n        if t != 0:\n            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n\n\ndef load_dataset(path, num_examples=None):\n    targ_lang, inp_lang = create_dataset(path, num_examples)\n\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n\n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n\n\ndef tokenize(lang):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    lang_tokenizer.fit_on_texts(lang)\n\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n    return tensor, lang_tokenizer\n\n\ndef max_length(tensor):\n    return max(len(t) for t in tensor)\n\n\ndef create_dataset(path, num_examples):\n    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n    lines = [x.split('CC-BY')[0].strip() for x in lines]\n    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n\n    return zip(*word_pairs)\n\n\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n    w = w.rstrip().strip()\n    w = '<start> ' + w + ' <end>'\n    return w","f04552b6":"TEXT_FILE_PATH = '..\/input\/spaeng\/spa.txt'\n\npath_to_file = TEXT_FILE_PATH\n\nnum_examples = 10000\ninput_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n                                                                                                target_tensor,\n                                                                                                test_size=0.2)\n\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train) \/\/ BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word_index) + 1\nvocab_tar_size = len(targ_lang.word_index) + 1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\nexample_input_batch, example_target_batch = next(iter(dataset))\nprint(example_input_batch.shape, example_target_batch.shape)\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n\nEPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, targ_lang, enc_hidden, encoder, decoder, optimizer,\n                                loss_function, loss_object)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n\n    if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix=checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss \/ steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","badfbf84":"**Paper Implementation: Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)**\n\nIn the context of NMT, Bahdanau et al. (2015) has successfully applied attentional mechanism to jointly translate and align words.\n\nWe design two novel types of attention-based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time\n\nThe former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally.\n\n(This notebook only considered the global approach)\n\n\n**Neural Machine Translation**\n\nA neural machine translation system is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x_1, . . . , x_n, to a target sentence, y_1, . . . , y_m\n\nThe conditional probability can be decomposed as:\n    \n    log p(y|x) = sum[log p(y_j|y < j, s)]\nIn more detail, one can parameterize the probability of decoding each word y_j as:\n\n    p(y_j|y < j, s) = softmax(g(h_j))\nwith g being the transformation function that outputs a vocabulary-sized vector\n\nHere, h_j is the RNN hidden unit, abstractly computed as:\n\n    h_j = f(h_{j\u22121}, s)\nwhere f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit\n\nIn (Sutskever et al., 2014; Cho et al., 2014), the source representation s is only used once to initialize the decoder hidden state\n\nOn the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted **throughout the\nentire course** of the translation process\n\nSuch an approach is referred to as an **attention** mechanism\n\nOur training objective is formulated as follows:\n\n    J_t = sum_D(-log p(y|x))\n**Attention-based Models**\n\nSpecifically, given the target hidden state h_t and the source-side context vector c_t, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\n\n    h_t^hat = tanh(W_c[c_t;h_t])\n    [c_t;h_t] means concatenation of c_t and h_t\nThe attentional vector h_t^hat is then fed through the softmax layer to produce the predictive distribution formulated as:\n\n    p(y_t|y < t, x) = softmax(W_s h_t^hat)"}}