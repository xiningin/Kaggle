{"cell_type":{"090827b8":"code","841de517":"code","1bd19b72":"code","2ec277b9":"code","2338fc47":"code","df6ab6e8":"code","4fd990ed":"code","86eb763d":"code","ab2a7da6":"code","07f39cb7":"code","71254125":"code","1d1833f4":"code","ef46a27e":"code","bad764b4":"code","7af9beba":"code","d9f44d29":"code","c0a86b4f":"markdown","3ce510bf":"markdown","df16bffe":"markdown","3ee3e00e":"markdown","993615f2":"markdown","5baef798":"markdown","f79556b8":"markdown","691c4e74":"markdown","96fabf01":"markdown","bb33f7c7":"markdown","0b58ec31":"markdown","b9fba819":"markdown","c9797220":"markdown"},"source":{"090827b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport logging\nfrom tqdm import tqdm\nimport math\nimport argparse\nimport os","841de517":"!git clone https:\/\/github.com\/huggingface\/transformers\n!pip install transformers\/\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup","1bd19b72":"parser = argparse.ArgumentParser()\nparser.add_argument('--seed', type=int, default=88888)\nparser.add_argument(\"--model_name\", default=\"gpt2\", type=str)\nparser.add_argument(\"--max_seq_length\", default=512, type=int)\nparser.add_argument(\"--train_batch_size\", default=4, type=int)\nparser.add_argument(\"--valid_batch_size\", default=4, type=int)\nparser.add_argument(\"--num_train_epochs\", default=1, type=int)\nparser.add_argument(\"--warmup\", default=0.1, type=float)\nparser.add_argument(\"--learning_rate\", default=5e-5, type=float)\nparser.add_argument(\"--input_text_path\", default='..\/input\/story-text', type=str)\nargs, _ = parser.parse_known_args()","2ec277b9":"DATAPATH=args.input_text_path\ndef combinetext(prompt, story):\n    fp=open(os.path.join(DATAPATH,prompt),encoding='utf8')\n    fs=open(os.path.join(DATAPATH,story),encoding='utf8')\n    prompts=fp.readlines()\n    stories=fs.readlines()\n    assert len(prompts)==len(stories)\n    combine=[]\n    for i in range(len(prompts)):\n        combine.append(prompts[i].rstrip()+' <sep> '+\" \".join(stories[i].split()[:300]))\n    return combine\n\n#do a littel text clean with punctuations\ndef cleanpunctuation(s):\n    for p in '!,.:;?':\n        s=s.replace(' '+p,p)\n    s=s.replace(' '+'n\\'t','n\\'t')\n    s=s.replace(' '+'\\'s','\\'s')\n    s=s.replace(' '+'\\'re','\\'re')\n    s=s.replace(' '+'\\'ve','\\'ve')\n    s=s.replace(' '+'\\'ll','\\'ll')\n    s=s.replace(' '+'\\'am','\\'am')\n    s=s.replace(' '+'\\'m','\\'m')\n    s=s.replace(' '+'\\' m','\\'m')\n    s=s.replace(' '+'\\'m','\\'m')\n    s=s.replace(' '+'\\' ve','\\'ve')\n    s=s.replace(' '+'\\' s','\\'s')\n    s=s.replace('<newline>','\\n')\n    return s   \n\ntrain_text=combinetext('valid.wp_source', 'valid.wp_target')\ntrain_text=list(map(cleanpunctuation,train_text))\nvalid_text=combinetext('test.wp_source', 'test.wp_target')\nvalid_text=list(map(cleanpunctuation,valid_text))","2338fc47":"train_text[6]","df6ab6e8":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token=tokenizer.eos_token\n\ninputs_train = tokenizer(train_text, padding=True,truncation=True,max_length=args.max_seq_length)\ninputs_valid=tokenizer(valid_text, padding=True,truncation=True,max_length=args.max_seq_length)","4fd990ed":"def create_labels(inputs):\n    labels=[]\n    for ids,attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n        label=ids.copy()\n        real_len=sum(attention_mask)\n        padding_len=len(attention_mask)-sum(attention_mask)\n        label[:]=label[:real_len]+[-100]*padding_len\n        labels.append(label)\n    inputs['labels']=labels\n    \ncreate_labels(inputs_train)\ncreate_labels(inputs_valid)\n","86eb763d":"print(inputs_train['input_ids'][6])\nprint(inputs_train['attention_mask'][6])\nprint(inputs_train['labels'][6])\n","ab2a7da6":"class StoryDataset:\n    def __init__(self, inputs):\n        self.ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels=inputs['labels']\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, item):\n\n        return [torch.tensor(self.ids[item], dtype=torch.long),\n                torch.tensor(self.attention_mask[item], dtype=torch.long),\n                torch.tensor(self.labels[item], dtype=torch.long)]\n            ","07f39cb7":"train_batch_size=args.train_batch_size\nvalid_batch_size=args.valid_batch_size\ntraindata=StoryDataset(inputs_train)\ntrain_dataloader = torch.utils.data.DataLoader(\n    traindata,\n    shuffle=False,\n    batch_size=train_batch_size)\n\nvaliddata=StoryDataset(inputs_valid)\nvalid_dataloader = torch.utils.data.DataLoader(\n    validdata,\n    shuffle=False,\n    batch_size=valid_batch_size)","71254125":"model = GPT2LMHeadModel.from_pretrained('gpt2')","1d1833f4":"model.to('cuda')\nmodel.eval()\neval_loss=[]\nfor inputs in tqdm(valid_dataloader, desc=\"eval\"):\n    d1,d2,d3=inputs\n    d1=d1.to('cuda')        \n    d2=d2.to('cuda')\n    d3=d3.to('cuda')\n\n    with torch.no_grad():\n        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n        batch_loss=output[0]\n    eval_loss+=[batch_loss.cpu().item()]\n    del batch_loss\neval_loss=np.mean(eval_loss)\nperplexity=math.exp(eval_loss)\nprint(f'The average perplexity for valid dataset before fine-tuning is {perplexity}') ","ef46a27e":"prompt=valid_text[300][:valid_text[300].find('<sep>')]\ntarget=valid_text[300][valid_text[300].find('<sep>')+5:]\n\ndef generate_story(prompt,target,k=0,p=0.9,output_length=300,temperature=1,num_return_sequences=3,repetition_penalty=1.0):\n    print(\"====prompt====\\n\")\n    print(prompt+\"\\n\")\n    print('====target story is as below===\\n')\n    print(target+\"\\n\")\n    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n    model.to('cpu')\n    model.eval()\n    output_sequences = model.generate(\n        input_ids=encoded_prompt,\n        max_length=output_length,\n        temperature=temperature,\n        top_k=k,\n        top_p=p,\n        repetition_penalty=repetition_penalty,\n        do_sample=True,\n        num_return_sequences=num_return_sequences\n    )\n    if len(output_sequences.shape) > 2:\n        output_sequences.squeeze_()\n    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n        generated_sequence = generated_sequence.tolist()\n        # Decode text\n        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n        # Remove all text after eos token\n        text = text[: text.find(tokenizer.eos_token)]\n        print(text)\n\ngenerate_story(prompt,target)","bad764b4":"num_train_epochs = args.num_train_epochs\ntraining_steps_per_epoch=len(train_dataloader)\ntotal_num_training_steps = int(training_steps_per_epoch*num_train_epochs)\nweight_decay=0\nlearning_rate=args.learning_rate\nadam_epsilon=1e-8\nwarmup_steps=int(total_num_training_steps*args.warmup)\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": weight_decay,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_num_training_steps\n)","7af9beba":"print(\"***** Running training *****\")\nprint(\"  Total_num_training_step = {}\".format(total_num_training_steps))\nprint(\"  Num Epochs = {}\".format(num_train_epochs))\nprint(f\"  Train_batch_size per device = {train_batch_size}\")\nprint(f\"  Valid_batch_size per device = {valid_batch_size}\")\nmodel.to('cuda')\nfor epoch in range(num_train_epochs):\n    print(f\"Start epoch{epoch+1} of {num_train_epochs}\")\n    train_loss=0\n    epoch_iterator = tqdm(train_dataloader,desc='Iteration')\n    model.train()\n    model.zero_grad()    \n    for _, inputs in enumerate(epoch_iterator):        \n        d1,d2,d3=inputs\n        d1=d1.to('cuda')\n        d2=d2.to('cuda')\n        d3=d3.to('cuda')\n        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n        batch_loss=output[0]\n        batch_loss.backward()\n        optimizer.step()\n        scheduler.step()\n        model.zero_grad()\n        train_loss+=batch_loss.item()\n        epoch_iterator.set_description('(batch loss=%g)' % batch_loss.item())\n        del batch_loss\n    print(f'Average train loss per example={train_loss\/training_steps_per_epoch} in epoch{epoch+1}')    \n    print(f'Starting evaluate after epoch {epoch+1}')\n    eval_loss=[]    \n    model.eval()    \n    for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n        d1,d2,d3=inputs\n        d1=d1.to('cuda')        \n        d2=d2.to('cuda')\n        d3=d3.to('cuda')\n        with torch.no_grad():\n            output = model(input_ids=d1, attention_mask=d2,labels=d3)\n            batch_loss=output[0]\n        eval_loss+=[batch_loss.cpu().item()]\n        del batch_loss\n    eval_loss=np.mean(eval_loss)\n    perplexity=math.exp(eval_loss)\n    print(f'Average valid loss per example={eval_loss} in epoch{epoch+1}')    \n    print(f'Perplextiy for valid dataset in epoch{epoch+1} is {perplexity}')\n    ","d9f44d29":"prompt=valid_text[300][:valid_text[300].find('<sep>')]\ntarget=valid_text[300][valid_text[300].find('<sep>')+5:]\ngenerate_story(prompt,target)","c0a86b4f":"Let's pick a prompt from the valid dataset and input it into the model, have the model generate a 300 words long story. The output stories is really great!  I use the generate method comes with the model. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling. The meanings of key arguments are below:<br> \n1)**do_sample**: if set to False greedy decoding is used.<br>\n2)The **temperature** is used to module the next token probabilities.<br>\n3)**top_k** is the number of highest probability vocabulary tokens to keep for top-k-filtering.<br>\n4)**top_p** is the cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. <br>5)**repetition_penalty** is the parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty.<br>","3ce510bf":"An example of sequence of input_ids, attention_mask and labels.","df16bffe":"# 3. Model and optimizer\n\n## 3.1 Zero-shot story generate\n\nWith the amazing transfomers pacakge, we can easily download the pretrained GPT-2 model. **Before fine-tuning the model, I evaluate the model with valid dataset, and the average perplexity of evaluate results is 39**. Let's see what is the score of perplexity after fine-tuning later.","3ee3e00e":"## 2.2 Tokenize and load to dataloader\n\nGPT-2 uses BPE to tokenize the text squence.BPE merges frequently co-occurred byte pairs in a greedy manner. In order to let the sequences in the same batch have the same length, I set the max length of sequence as 512, and truncate the longer sequence and pad the shorter sequence. Since the tokenizer function only return the input_ids and attention_mask. For training purpose, I need to feed the labels(targets) to the model. So I create labels sequence for every input_ids squence. In the label sequence,I rule out the padding tokens by set it to -100 to avoid compute loss on them. And also GPT-2 will automatically shift the labels to the right to match the inputs_ids, so I don't need to deal with it.\n","993615f2":"The below is an example of a prompt and story. ","5baef798":"# 4. Conclusion\n\n","f79556b8":"# 1. Introduction\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\n**Zero-Shot Transfer**: The pre-training task for GPT-2 is solely language modeling. All the downstream language tasks are framed as predicting conditional probabilities and there is no task-specific fine-tuning.\n\nIn this notebook, I use the dataset of writing prompts and stories from [this paper](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/stories) to fine-tune GPT-2, then use the fine-tuned model to generate stories. I use [perplexity](https:\/\/en.wikipedia.org\/wiki\/Perplexity) as the metrics to check if fine-tuning imporves the performance or not.  \n\n![image.png](attachment:image.png)","691c4e74":"## 1.1 Pacakges and frameworks\n\n**Environment**: Kaggle Python 3 environment and GPU\n\n**Deep learning framework**: Pytorch\n\n**NLP Package**: Transformers 3.0.2","96fabf01":"## 3.2 Generate stories\nI use the fine-tuened model to generate stories with the same prompt I used before fine-tuning.","bb33f7c7":"## 1.2 Arguments","0b58ec31":"## 3.1 Fine-tune the model\n\nThe number of training samples is 15620. With one GPU to train the model, it tooks about 21 minutes to run 1 epoch. **After 1 epoche learning, the perplexity for valid dataset is about 24**, which is better than the score before fine- tuning.","b9fba819":"# 2. Prepare the data\n\n## 2.1 Combine the prompt and story, do a little text clean\nDownload the text file from the above link. There are train, valid and test dataset in the original dataset. And the prompts and stories are in seperate files. For a example, the valid.wp_source has the writing promots and valid.wp_target has the corresponding stories. The train dataset is very large. Since kaggle notebook limits the kernel running time to 3 hours. I decide to take the valid dataset as my train dataset, and the test dataset as valid dataset. \n\nIn order to feed the prompt an story together to GPT-2, I combine the prompts and stories togeter.Thus every line in the combined file includes the prompt and it's corresponding story.","c9797220":"From my experiment, fine-tuning GPT-2 with specific task domain dataset does improve the perplexity score. Howerver, from human evaluation, I could not tell which generated story is better. The task of generative language modeling is way too hard.The human writing ablilities are far more complex than the existing technologies are able to reach. We still have a long way to explore in this field.  "}}