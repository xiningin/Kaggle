{"cell_type":{"e765a2ef":"code","581f0900":"code","8c45a2bd":"code","0c2f863d":"code","b4a6f3ec":"code","9158bd7a":"code","a795465c":"code","7832ca6a":"code","c02c55cf":"code","af34d464":"code","5092230d":"code","9414d78a":"code","e4e83218":"code","dd7b8b39":"code","9f4f8d98":"code","ceaee598":"code","18b4f1b9":"code","3a4092b3":"code","7df3a362":"code","552f4282":"code","c170f616":"code","50e4bcac":"markdown","88980572":"markdown","9a256eb8":"markdown","066ec78d":"markdown","0005ca70":"markdown","afc95ed8":"markdown","b6bb02ab":"markdown","c2d4ce39":"markdown","3c92329f":"markdown","8ead7116":"markdown","f449f690":"markdown","e112b3be":"markdown","7ac34e73":"markdown","931f15f9":"markdown","25800b3a":"markdown","e855dd9b":"markdown"},"source":{"e765a2ef":"!pip install --upgrade fastaudio","581f0900":"import pandas as pd\nfrom fastaudio.all import *\nfrom fastai.vision.all import *","8c45a2bd":"path = Path(\"..\/input\/rfcx-species-audio-detection\")\npath.ls()","0c2f863d":"train_path = path \/ 'train'\ntest_path = path \/ 'test'","b4a6f3ec":"train_files = get_audio_files(train_path)\naudio = AudioTensor.create(train_files[0])\naudio.show();","9158bd7a":"df_train_tp = pd.read_csv(path \/ 'train_tp.csv')\ndf_train_tp[\"recording_id\"] = df_train_tp[\"recording_id\"].map(lambda x: \"train\/\"+x)\ndf_train_tp.head()","a795465c":"df_train_tp = df_train_tp.drop(['t_min', 't_max', 'f_min', 'f_max', 'songtype_id'], axis=1)\ndf_train_tp['species_id'] = df_train_tp['species_id'].astype(str)\ndf_train_tp.head()","7832ca6a":"# https:\/\/stackoverflow.com\/questions\/27298178\/concatenate-strings-from-several-rows-using-pandas-groupby\ndf_train_tp['species_id'] = df_train_tp.groupby('recording_id')['species_id'].transform(\",\".join)\ndf_train_tp = df_train_tp.reset_index()","c02c55cf":"# AudioToSpec is a Transform from fastaudio that runs on the GPU \n# (notice that it's passed as a a batch_tfms)\n# Also, there's multiple AudioConfig's ready to use with parameters that can be easily adjusted.\n\naudio_to_spec = AudioToSpec.from_cfg(AudioConfig.BasicMelSpectrogram(n_fft=512))\n\n# Adding some data augmentation\ndata_augmentation = [AddNoise(color=NoiseColor.White, noise_level=0.1), SignalShifter(max_pct=0.3)]\n\nblocks = DataBlock(blocks=(AudioBlock, MultiCategoryBlock),\n                  get_x = ColReader('recording_id', pref=str(path.resolve())+\"\/\", suff='.flac'),\n                  get_y = ColReader('species_id', label_delim=','),\n                  item_tfms = data_augmentation,\n                  batch_tfms = audio_to_spec,\n                  splitter=RandomSplitter(valid_pct=0.2, seed=42)\n                  )","af34d464":"# Creating the dataloaders\ndls = blocks.dataloaders(df_train_tp, bs=24)","5092230d":"dls.show_batch(ncols=3, nrows=2, figsize=(20, 10))","9414d78a":"learner = cnn_learner(dls, resnet18, n_in=1)","e4e83218":"learner.lr_find()","dd7b8b39":"learner.fine_tune(10, base_lr=5e-2)","9f4f8d98":"learner.recorder.plot_loss()","ceaee598":"submission_df = pd.read_csv(path \/ 'sample_submission.csv')\nsubmission_df[\"recording_id\"] = submission_df[\"recording_id\"].map(lambda x: \"test\/\"+x)\nsubmission_df","18b4f1b9":"# Easily create test dataloader and get the predictions\ntest_dl = dls.test_dl(submission_df)\npreds = learner.get_preds(dl = test_dl)","3a4092b3":"preds[0].shape","7df3a362":"# Copy the predictions into the submission dataframe\nsubmission_df.iloc[:, 1:] = preds[0]","552f4282":"# It's ready to submit\nsubmission_df[\"recording_id\"] = submission_df[\"recording_id\"].map(lambda x: x.split(\"\/\")[1])\nsubmission_df","c170f616":"submission_df.to_csv('submission.csv', index=False)","50e4bcac":"There are multiple lines with the same recording_id but different species_id. Now, we will group them and concat the species_id separated by commas `,`","88980572":"# Fastaudio starter kit","9a256eb8":"# Building the dataloaders","066ec78d":"After installing fastaudio, restart the env before importing the library\n\n![image.png](https:\/\/i.imgur.com\/xlAOnbW.png)","0005ca70":"# Learner and training","afc95ed8":"First we will build [datablocks](https:\/\/docs.fast.ai\/tutorial.datablock.html), that are a general way to specify how to load our data. Then, using this blocks, the train and validation dataloaders will be created. ","b6bb02ab":"We will drop all the columns that are not recording_id or species_id.","c2d4ce39":"Now let's vizualize one batch of data","3c92329f":"# Creating submission file","8ead7116":"Let's start updating the pytorch version and installing it","f449f690":"# Processing dataframes","e112b3be":"Let's open a training file and visualize\/hear it","7ac34e73":"This notebook tries to give you the basic steps to compete in the Rainforest Connection Species Audio Detection competition using fastaudio.\n\n[Fastaudio](https:\/\/github.com\/fastaudio\/fastaudio) is an community contributed module for building audio machine learning applications on top of fastai 2. ","931f15f9":"The model used is based on the torchvision resnet18, with the only modification that the input should have 1 channel, because that's what our spectrograms have.\n\nHere you can use all the standard computer vision tricks, in fact the cnn_learner comes from the fastai.vision module","25800b3a":"# Imports and initial data exploration","e855dd9b":"Here you have a lot of room to experiment. As this is a starter kit, only the baseline `.fine_tune(...)` is used, but it's recommended to train for some epochs (with `.fit_one_cycle(...)`), unfreeze the model, and continue training."}}