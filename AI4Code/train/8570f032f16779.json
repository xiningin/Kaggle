{"cell_type":{"37db242c":"code","7af916b9":"code","8aee1fb5":"code","0d4d9a44":"code","4ed37625":"code","7816fce2":"code","cfd5e70c":"code","4adf7405":"code","388d7cf0":"code","203ecd5f":"code","d741b28f":"code","a73da31d":"code","c108fb7d":"code","64e5c17d":"code","d9a28450":"code","143d1757":"code","bc60a10a":"code","df4cd3aa":"code","43598bfb":"code","57581e5a":"code","c6676ad8":"code","b072ed63":"code","79e2a3ea":"code","18aed3b7":"code","f005090c":"code","9ad91f42":"code","21e5c34d":"code","c6810a37":"code","edd78466":"code","5cbb1064":"code","0e0d86a0":"code","c50ee451":"code","48f1c154":"code","e7acaf9b":"code","d862ea41":"code","cb2549b7":"markdown","64f9895b":"markdown","6c99fe06":"markdown","1c262326":"markdown","8c13b1f4":"markdown","150c2c40":"markdown","9e889056":"markdown","4ef98444":"markdown","12c9c382":"markdown"},"source":{"37db242c":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom PIL import Image\nimport re\nimport numpy as np\nimport os\nfrom glob import glob\nORIG_DATA_DIR = \"..\/input\/kitti-road-dataset\/data_road\"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","7af916b9":"DATA_DIR = \"..\/data\/\"\nMODEL_DIR = \"..\/model\/\"\nNUM_CLASSES = 2\nDROPOUT = 0.5\nINPUT_SHAPE = tf.keras.Input((125,414,3))","8aee1fb5":"def fcn_8():\n    with strategy.scope():\n        model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None,\n                                            pooling=None, classes=1000, classifier_activation='softmax')\n        model.trainable = False\n        inp_0 = model.layers[0]\n\n        tensor = inp_0(INPUT_SHAPE)\n        imp_tensor_index = [10, 14, 18]\n        imp_tensor = [tensor]\n\n        count = 1\n        for layer in model.layers[1:]:\n            tensor = layer(tensor)\n            if count in imp_tensor_index:\n                imp_tensor.append(tensor)\n            count+=1\n        inp_layer, layer_3, layer_4, layer_5 = imp_tensor\n\n\n        fcn_8_from_layer_7 = tf.keras.layers.Conv2D(4096,(7,7), padding='same',\n                                                    activation = \"relu\",name = \"block6_conv1\")(layer_5)\n\n        fcn_8_from_layer_7 = tf.keras.layers.Dropout(DROPOUT, \n                                                     name = \"block6_dropout1\")(fcn_8_from_layer_7)\n\n        fcn_8_from_layer_7 = tf.keras.layers.Conv2D(4096,(1,1),activation = \"relu\",\n                                                    name = \"block7_conv1\")(fcn_8_from_layer_7)\n\n        fcn_8_from_layer_7 = tf.keras.layers.Dropout(DROPOUT,\n                                                     name = \"block7_dropout1\")(fcn_8_from_layer_7)\n\n        fcn_8_from_layer_7 = tf.keras.layers.Conv2D(NUM_CLASSES,(1,1),activation = \"relu\",\n                                                    name = \"fcn_layer_7\")(fcn_8_from_layer_7)\n\n        #Upsample 1\n        fcn_8_from_layer_7 = tf.keras.layers.Conv2DTranspose(layer_4.shape.as_list()[-1],4,(2,2))(fcn_8_from_layer_7)\n        fcn_8_from_layer_7 = tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0)))(fcn_8_from_layer_7)\n        layer_4_plus_layer_7 = tf.keras.layers.Add()([fcn_8_from_layer_7,layer_4])\n\n        #Upsample 2\n        layer_4_upsampled = tf.keras.layers.Conv2DTranspose(layer_3.shape.as_list()[-1],4,(2,2))(layer_4_plus_layer_7)\n        layer_4_upsampled = tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0)))(layer_4_upsampled)\n        layer_3_plus_layer_4 = tf.keras.layers.Add()([layer_4_upsampled,layer_3])\n\n        #Upsample 3\n        fcn_8 = tf.keras.layers.Conv2DTranspose(NUM_CLASSES,16,(8,8))(layer_3_plus_layer_4)\n        fcn_8 = tf.keras.layers.Cropping2D(cropping=((2, 1), (1, 1)))(fcn_8)\n        \n        fcn_model = tf.keras.Model(inputs=INPUT_SHAPE, outputs=fcn_8)\n        return fcn_model","0d4d9a44":"def fcn_32():\n    model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None,\n                                            pooling=None, classes=1000, classifier_activation='softmax')\n    model.trainable = False\n    inp_0 = model.layers[0]\n\n    tensor = inp_0(INPUT_SHAPE)\n    imp_tensor_index = [10, 14, 18]\n    imp_tensor = [tensor]\n\n    count = 1\n    for layer in model.layers[1:]:\n        tensor = layer(tensor)\n        if count in imp_tensor_index:\n            imp_tensor.append(tensor)\n        count+=1\n    inp_layer, layer_3, layer_4, layer_5 = imp_tensor\n\n\n    fcn_32_from_layer_7 = tf.keras.layers.Conv2D(4096,(7,7), padding='same',\n                                                activation = \"relu\",name = \"block6_conv1\")(layer_5)\n\n    fcn_32_from_layer_7 = tf.keras.layers.Dropout(DROPOUT, \n                                                 name = \"block6_dropout1\")(fcn_32_from_layer_7)\n\n    fcn_32_from_layer_7 = tf.keras.layers.Conv2D(4096,(1,1),activation = \"relu\",padding='same',\n                                                name = \"block7_conv1\")(fcn_32_from_layer_7)\n\n    fcn_32_from_layer_7 = tf.keras.layers.Dropout(DROPOUT,\n                                                 name = \"block7_dropout1\")(fcn_32_from_layer_7)\n\n    fcn_32_from_layer_7 = tf.keras.layers.Conv2D(NUM_CLASSES,(1,1),activation = \"relu\",padding=\"same\",\n                                                name = \"fcn_layer_7\")(fcn_32_from_layer_7)\n\n    #Upsample 1\n    print(fcn_32_from_layer_7.shape)\n\n    fcn_32 = tf.keras.layers.Conv2DTranspose(NUM_CLASSES,64,(42,35),padding=\"same\")(fcn_32_from_layer_7)\n    fcn_32 = tf.keras.layers.Cropping2D(cropping=((1, 0), (3, 3)))(fcn_32)\n    fcn_model = tf.keras.Model(inputs=INPUT_SHAPE, outputs=fcn_32)\n    return fcn_model","4ed37625":"def fcn_16():\n    model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None,\n                                    pooling=None, classes=1000, classifier_activation='softmax')\n    model.trainable = False\n    inp_0 = model.layers[0]\n\n    tensor = inp_0(INPUT_SHAPE)\n    imp_tensor_index = [10, 14, 18]\n    imp_tensor = [tensor]\n\n    count = 1\n    for layer in model.layers[1:]:\n        tensor = layer(tensor)\n        if count in imp_tensor_index:\n            imp_tensor.append(tensor)\n        count+=1\n    inp_layer, layer_3, layer_4, layer_5 = imp_tensor\n\n\n    fcn_8_from_layer_7 = tf.keras.layers.Conv2D(4096,(7,7), padding='same',\n                                                activation = \"relu\",name = \"block6_conv1\")(layer_5)\n\n    fcn_8_from_layer_7 = tf.keras.layers.Dropout(DROPOUT, \n                                                 name = \"block6_dropout1\")(fcn_8_from_layer_7)\n\n    fcn_8_from_layer_7 = tf.keras.layers.Conv2D(4096,(1,1),activation = \"relu\",\n                                                name = \"block7_conv1\")(fcn_8_from_layer_7)\n\n    fcn_8_from_layer_7 = tf.keras.layers.Dropout(DROPOUT,\n                                                 name = \"block7_dropout1\")(fcn_8_from_layer_7)\n\n    fcn_8_from_layer_7 = tf.keras.layers.Conv2D(NUM_CLASSES,(1,1),activation = \"relu\",\n                                                name = \"fcn_layer_7\")(fcn_8_from_layer_7)\n\n    #Upsample 1\n    fcn_8_from_layer_7 = tf.keras.layers.Conv2DTranspose(layer_4.shape.as_list()[-1],4,(2,2))(fcn_8_from_layer_7)\n    fcn_8_from_layer_7 = tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0)))(fcn_8_from_layer_7)\n    layer_4_plus_layer_7 = tf.keras.layers.Add()([fcn_8_from_layer_7,layer_4])\n\n    #Upsample 2\n    layer_4_upsampled = tf.keras.layers.Conv2DTranspose(2,16,(18,17))(layer_4_plus_layer_7)\n    fcn_16 = tf.keras.layers.Cropping2D(cropping=((1, 0), (6, 5)))(layer_4_upsampled)\n    fcn_model = tf.keras.Model(inputs=INPUT_SHAPE, outputs=fcn_16)\n\n    return fcn_model","7816fce2":"def data_preprocessing_and_load(image_shape,dataset_type = \"training\"):\n\n    data_folder = os.path.join(ORIG_DATA_DIR, dataset_type)\n    # save_folder = os.path.join(POST_PROCESSING_DATA_DIR, dataset_type)\n\n    image_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n    label_paths = {\n        re.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n        for path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n    background_color = np.array([1, 0, 0], dtype=np.float32)\n    count = 0\n    # save_img = os.path.join(save_folder,\"image_2\")\n    # save_gt = os.path.join(save_folder,\"gt_image_2\")\n    # os.makedirs(save_img,exist_ok=True)\n    # os.makedirs(save_gt,exist_ok = True)\n    lhs = []\n    rhs = []\n    \n    lhrh = []\n\n    for image_path in image_paths:\n\n        gt_image_file = label_paths[os.path.basename(image_path)]\n\n        image_import = Image.open(image_path)\n        gt_image_import = Image.open(gt_image_file)\n        image_import = image_import.resize(image_shape,Image.BICUBIC)\n        gt_image_import = gt_image_import.resize(image_shape,Image.BICUBIC)\n\n\n        image = np.array(image_import, dtype = np.float32) \/ 255\n        gt_image = np.array(gt_image_import, dtype = np.float32) \/ 255\n\n        # One-hot labels\n        gt_bg = np.all(gt_image == background_color, axis=2)\n        gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n\n        gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n        gt_image = gt_image.astype('float32')\n        # image_path = os.path.join(save_img, \"{}.png\".format(str(count_image)) )\n        # gt_image_path = os.path.join(save_gt,\"{}.png\".format(str(count_image)))\n#         lhs.append(image)\n#         rhs.append(gt_image)\n        lhrh.append((image, gt_image))\n#         lhs.append(np.flip(image,1))\n#         rhs.append(np.flip(gt_image,1))\n        lhrh.append((np.flip(image,1), np.flip(gt_image,1)))\n        count+=1\n\n#     shuffle(lhrh)\n    for lr in lhrh:\n        lhs.append(lr[0])\n        rhs.append(lr[1])\n    \n    lhs=np.stack(lhs)\n    rhs=np.stack(rhs)\n    return lhs, rhs\n        \nX, Y = data_preprocessing_and_load((414, 125))\nprint(len(X))","cfd5e70c":"print(X.shape, X[0].shape)\nprint(Y[0].shape, Y[1].shape)","4adf7405":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2)\n\n# X_train = X[:-len(X)\/\/5]\n# Y_train = Y[:-len(Y)\/\/5]\n# X_valid = X[-len(X)\/\/5:]\n# Y_valid = Y[-len(Y)\/\/5:]\nprint(len(X_train), len(X_valid))","388d7cf0":"from tensorflow.keras.optimizers import Nadam\nwith strategy.scope():\n    fcn_model = fcn_8()\n    print(fcn_model.summary())\n    fcn_model.compile(optimizer = Nadam(lr=0.001), loss = 'binary_crossentropy')","203ecd5f":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,LearningRateScheduler\nfrom math import floor,pow\nmodel_name = 'fcn_8.h5'\ndef step_lr(epoch):\n   init_lr = 0.00005\n   drop = 0.5\n   epoch_step =  25\n   new_lr = init_lr * pow(drop,floor((1+epoch)\/epoch_step))\n   return new_lr\nstep_learning_rate = LearningRateScheduler(step_lr)\n# early stopping should always be the last callback\nstopper = EarlyStopping(monitor='loss', patience=10)\ncheckpointer = ModelCheckpoint(filepath=model_name, monitor='val_loss', mode='min', save_best_only=True)\n\n# fcn_model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy')\n\nif os.path.isfile(model_name):\n    print('Loaded saved Model!')\n    fcn_model.load_weights(model_name)\n\n\n","d741b28f":"losses = list()\nval_losses = list()","a73da31d":"history = fcn_model.fit(\n    X_train, Y_train,\n    batch_size = 10 * strategy.num_replicas_in_sync,\n#     batch_size = 16,\n    epochs = 200,\n    validation_data = (X_valid, Y_valid),\n    callbacks = [checkpointer, stopper,step_learning_rate],\n    shuffle = True,\n    verbose = 1\n)\n\nlosses += list(history.history['loss'])\nval_losses += list(history.history['val_loss'])","c108fb7d":"plt.plot(losses)\nplt.plot(val_losses)\nplt.show()","64e5c17d":"fcn_model.save('entire_model.h5')","d9a28450":"from tensorflow.keras.optimizers import Nadam\nwith strategy.scope():\n    fcn_model = fcn_16()\n    fcn_model.compile(optimizer = Nadam(lr=0.001), loss = 'binary_crossentropy')","143d1757":"fcn_model.summary()","bc60a10a":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,LearningRateScheduler\nfrom math import floor,pow\nmodel_name = 'fcn_16.h5'\ndef step_lr(epoch):\n   init_lr = 0.0001\n   drop = 0.5\n   epoch_step =  25\n   new_lr = init_lr * pow(drop,floor((1+epoch)\/epoch_step))\n   return new_lr\nstep_learning_rate = LearningRateScheduler(step_lr)\n# early stopping should always be the last callback\nstopper = EarlyStopping(monitor='loss', patience=10)\ncheckpointer = ModelCheckpoint(filepath=model_name, monitor='val_loss', mode='min', save_best_only=True)\n\n# fcn_model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy')\n\nif os.path.isfile(model_name):\n    print('Loaded saved Model!')\n    fcn_model.load_weights(model_name)\n","df4cd3aa":"losses = list()\nval_losses = list()","43598bfb":"\nhistory = fcn_model.fit(\n    X_train, Y_train,\n    batch_size = 10 * strategy.num_replicas_in_sync,\n#     batch_size = 16,\n    epochs = 200,\n    validation_data = (X_valid, Y_valid),\n    callbacks = [checkpointer, stopper,step_learning_rate],\n    shuffle = True,\n    verbose = 1\n)\n\nlosses += list(history.history['loss'])\nval_losses += list(history.history['val_loss'])","57581e5a":"plt.plot(losses)\nplt.plot(val_losses)\nplt.show()","c6676ad8":"from tensorflow.keras.optimizers import Nadam\nwith strategy.scope():\n    fcn_model = fcn_32()\n    fcn_model.compile(optimizer = Nadam(lr=0.001), loss = 'binary_crossentropy')","b072ed63":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,LearningRateScheduler\nfrom math import floor,pow\nmodel_name = 'fcn_32.h5'\ndef step_lr(epoch):\n   init_lr = 0.001\n   drop = 0.5\n   epoch_step =  10\n   new_lr = init_lr * pow(drop,floor((1+epoch)\/epoch_step))\n   return new_lr\nstep_learning_rate = LearningRateScheduler(step_lr)\n# early stopping should always be the last callback\nstopper = EarlyStopping(monitor='loss', patience=10)\ncheckpointer = ModelCheckpoint(filepath=model_name, monitor='val_loss', mode='min', save_best_only=True)\n\n# fcn_model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy')\n\nif os.path.isfile(model_name):\n    print('Loaded saved Model!')\n    fcn_model.load_weights(model_name)\n","79e2a3ea":"losses = list()\nval_losses = list()","18aed3b7":"\nhistory = fcn_model.fit(\n    X_train, Y_train,\n    batch_size = 10 * strategy.num_replicas_in_sync,\n#     batch_size = 16,\n    epochs = 200,\n    validation_data = (X_valid, Y_valid),\n    callbacks = [checkpointer, stopper,step_learning_rate],\n    shuffle = True,\n    verbose = 1\n)\n\nlosses += list(history.history['loss'])\nval_losses += list(history.history['val_loss'])","f005090c":"def generate_output(image,gt_np_array):\n    image = image*255\n    print(image.shape)\n    image = image.astype('uint8')\n    gt_road = gt_np_array[:,:,1]\n    gt_not_road = gt_np_array[:,:,0]\n    mask_bits = np.zeros(gt_road.shape)\n    mask_bits[gt_road > gt_not_road] = 1\n    green = np.asarray([0,255,0,127])\n    mask = np.outer(mask_bits,green).reshape(mask_bits.shape[0],-1,len(green))\n    mask = mask.astype('uint8')\n    mask = Image.fromarray(mask)\n    image = Image.fromarray(image)\n    image.paste(mask,mask = mask)\n    image = np.array(image)\n    return image\n","9ad91f42":"def test_data(image_shape):\n    dataset_type = \"testing\"\n    data_folder = os.path.join(ORIG_DATA_DIR, dataset_type)\n    # save_folder = os.path.join(POST_PROCESSING_DATA_DIR, dataset_type)\n\n    image_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n    \n    background_color = np.array([1, 0, 0], dtype=np.float32)\n    count = 0\n    # save_img = os.path.join(save_folder,\"image_2\")\n    # save_gt = os.path.join(save_folder,\"gt_image_2\")\n    # os.makedirs(save_img,exist_ok=True)\n    # os.makedirs(save_gt,exist_ok = True)\n    lhs = []\n    \n\n    for image_path in image_paths:\n        image_import = Image.open(image_path)\n        image_import = image_import.resize(image_shape,Image.BICUBIC)\n        image = np.array(image_import, dtype = np.float32) \/ 255\n        \n        lhs.append(image)\n    \n    lhs=np.stack(lhs)\n    return lhs\n","21e5c34d":"fcn_model = fcn_8()\nfcn_model.load_weights(\"..\/input\/fcn8weights\/fcn_8 (1)\/fcn_8 (1).h5\")","c6810a37":"x_test = test_data((414, 125))\nprint(\"Prediction on test data\")\nresults = fcn_model.predict(x_test)\n","edd78466":"for i in range(10):\n    plt.figure()\n    plt.imshow(generate_output(x_test[i],results[i]))","5cbb1064":"fcn_model = fcn_16()\nfcn_model.load_weights(\"..\/input\/fcn8weights\/fcn_16.h5\")","0e0d86a0":"x_test = test_data((414, 125))\nprint(\"Prediction on test data\")\nresults = fcn_model.predict(x_test)","c50ee451":"for i in range(10):\n    plt.figure()\n\n    plt.imshow(generate_output(x_test[i],results[i]))\n","48f1c154":"fcn_model = fcn_32()\nfcn_model.load_weights(\"..\/input\/fcn8weights\/fcn_32.h5\")","e7acaf9b":"print(\"Prediction on test data\")\nresults = fcn_model.predict(x_test)","d862ea41":"for i in range(10):\n    plt.figure()\n\n    plt.imshow(generate_output(x_test[i],results[i]))","cb2549b7":"FCN 32 : Upsample directly from the 7th block | Expectation: More \"blocky\" images","64f9895b":"FCN 16 Training","6c99fe06":"FCN 8 Training","1c262326":"FCN 32 Results","8c13b1f4":"FCN-based network: Use a encoder, freeze the weights, and expand via using an decoder. Train the decoder using transfer learning)","150c2c40":"FCN 16: Take up upsampling from 4th block and 7th block. Expectation : Less blocky from the 2n d","9e889056":"FCN 8 Results","4ef98444":"FCN 8: Take the scalings from 7th, 4th as well as 3rd block","12c9c382":"FCN 16 Results"}}