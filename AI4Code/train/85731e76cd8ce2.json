{"cell_type":{"dc92755e":"code","528a1341":"code","d3d3d414":"code","532339a0":"code","fd5587dc":"code","e0cb3e31":"code","53958293":"code","f2dce43b":"code","9b10eaaa":"code","a747f338":"code","47976eb8":"code","827bbbae":"code","9196675f":"code","fde97b07":"code","1971fd6e":"code","6c9ebea7":"code","759a373d":"code","43aba07d":"code","c7edb735":"code","e7c0e691":"code","b4281180":"code","24a805af":"code","09549d5b":"code","2c73c604":"code","77a0b3a1":"code","3c336491":"code","f3fba601":"code","4a51d6b5":"code","eaef6075":"code","157a19e6":"code","863e5bf9":"code","e0357319":"code","ec148adf":"code","d21b8de8":"code","6dbdd3ee":"code","1aa70e40":"code","1ad4b42a":"code","c4037522":"code","5d0b06e5":"code","b3b0d2c6":"code","6db054ee":"code","cbe46143":"code","1101df34":"code","368032b2":"code","e61f3045":"code","16edbdaa":"code","70d34ea0":"code","d6516445":"code","05f8518e":"code","aa0388f9":"code","e638bcd5":"code","18360ea8":"code","351f8517":"code","0f0f5883":"code","d55070bd":"code","6d2f330d":"code","dedd3808":"markdown","d9ab7e60":"markdown","0c6d800c":"markdown","24b1b1df":"markdown","6b8766d7":"markdown","febf5fef":"markdown","720b66d5":"markdown","040029cf":"markdown","ae55727f":"markdown","420103a2":"markdown","6a541c4e":"markdown","3235daf2":"markdown","d7e0f625":"markdown","7b5c1518":"markdown","1bea77f6":"markdown","3776104c":"markdown","0ea2edda":"markdown","c2c252b3":"markdown","572d1b46":"markdown","beb4a923":"markdown"},"source":{"dc92755e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid', palette='muted')\nimport warnings\nwarnings.simplefilter(\"ignore\")\nnp.random.seed(99)","528a1341":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","d3d3d414":"# useful functions\n\ndef get_NaN(df):\n    N = df.shape[0]\n    ndf = pd.DataFrame({'any NaN': df.apply(lambda x: x.isna().any()),  \n                        '# NaN': df.apply(lambda x: x.isna().sum()), \n                        '% NaN': df.apply(lambda x: x.isna().sum() \/ N * 100),\n                        '# unique': df.apply(lambda x: x.nunique())\n                        })\n    ndf['% NaN'] = np.round(ndf['% NaN'], 1)\n    return ndf\n        \n        \ndef plot_dist(df, feat):\n    fig, ax = plt.subplots(1, 2, figsize=(18,4))\n    for i, n in enumerate(['no', 'yes']):\n        y = df[df['Survived']==i][feat]\n        sns.distplot(y, ax=ax[0], label=n)\n        sns.distplot(np.log1p(y), ax=ax[1], label=n)\n    ax[0].legend()\n    ax[1].legend()\n    ax[1].set_xlabel(f'log1p {feat}')","532339a0":"train.info()","fd5587dc":"train.head()","e0cb3e31":"# train set\nget_NaN(train)","53958293":"# test set\nget_NaN(test)","f2dce43b":"sns.countplot(train['Survived']);\n\ntrain['Survived'].value_counts() \/ len(train)","9b10eaaa":"# Pclass\nsns.countplot(x='Pclass', hue='Survived', data=train);","a747f338":"# Sex\nsns.countplot(x='Sex', hue='Survived', data=train);","47976eb8":"# Embarked\nsns.countplot(x='Embarked', hue='Survived', data=train);","827bbbae":"# Age\nplot_dist(train, 'Age')","9196675f":"# SibSp\nsns.countplot(x='SibSp', hue='Survived', data=train);","fde97b07":"# Parch\n# fig, ax = plt.subplots(figsize=(18,4))\nsns.countplot(x='Parch', hue='Survived', data=train);","1971fd6e":"# Fare\nplot_dist(train, 'Fare')","6c9ebea7":"# Ticket\n# Get the number of components separated by \"space\" in the ticket name\ntrain['Ticket'] = train['Ticket'].apply(lambda x: len(x.split(' ')))\ntest['Ticket'] = test['Ticket'].apply(lambda x: len(x.split(' ')))\nsns.countplot(x='Ticket', hue='Survived', data=train);","759a373d":"# Cabin\n\n# Define NaN as a new category (n)\ntrain['Cabin'].fillna('N', inplace=True)\ntest['Cabin'].fillna('N', inplace=True)\n\n# Get the first letter of the cabin name\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0])\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\nsns.countplot(x='Cabin', hue='Survived', data=train);","43aba07d":"# Data Imputation\n\n# Age\nmed = train['Age'].median()\ntrain['Age'].fillna(med, inplace=True)\ntest['Age'].fillna(med, inplace=True)\n\n# Fare (test data)\nmed = test['Fare'].median()\ntest['Fare'].fillna(med, inplace=True)","c7edb735":"cols = ['Sex', 'Ticket', 'Cabin', 'Embarked']\nfor col in cols:\n    encoding = {j:i for i,j in enumerate(train[col].unique())}\n    train[col] = train[col].map(encoding).values\n    test[col] = test[col].map(encoding).values","e7c0e691":"train.head()","b4281180":"corr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","24a805af":"cols = ['Ticket', 'Cabin', 'Embarked']\n\n# copy features\nfor col in cols:\n    train[f'{col}_LE'] = train[col]\n    test[f'{col}_LE'] = test[col]\n\ntrain = pd.get_dummies(train, columns=cols)\ntest = pd.get_dummies(test, columns=cols)","09549d5b":"train.columns","2c73c604":"# correlation\ncorr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","77a0b3a1":"# target\ny_train = train['Survived']\n\n# features\nX_train = train.drop(['Survived'], axis=1)\nX_test = test","3c336491":"len(X_train.columns), len(X_test.columns)","f3fba601":"[i for i in X_train.columns if i not in X_test.columns]","4a51d6b5":"# drop some features\nX_train.drop(['PassengerId', 'Name', 'Cabin_8', 'Embarked_3'], axis=1, inplace=True)\nX_test.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n\n# scale features\nfor col in X_train.columns:\n    min_ = X_train[col].min()\n    max_ = X_test[col].max()\n    intv = max_ - min_\n    X_train[col] = (X_train[col] - min_) \/ intv\n    X_test[col] = (X_test[col] - min_) \/ intv","eaef6075":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score","157a19e6":"def CV_Logistic_Regression(X_train, y_train, X_test, n_folds=5):\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 99)\n    \n    # Create oof sets for prediction storage.\n    oof_train = np.zeros(X_train.shape[0])\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    # Emptly list to store accuracy for each fold\n    fold_acc = []\n\n    for ifold, (tr_index, val_index) in enumerate(kf.split(X = X_train, y = y_train)):\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[tr_index,:]\n        X_val = X_train.iloc[val_index,:]\n        y_tr = y_train[tr_index]\n        y_val = y_train[val_index]\n\n        # Train model\n        model = LogisticRegression(random_state=0).fit(X_tr, y_tr)\n\n        # Predict validation and test data and store them in oof sets.\n        proba_val = model.predict_proba(X_val)[:,1]\n        oof_train[val_index] = np.where(proba_val>0.5, 1, 0)\n        proba_test = model.predict_proba(X_test)[:,1]\n        oof_test[:, ifold] = np.where(proba_test>0.5, 1, 0)\n        \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[val_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n\n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc = round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n    \n    return oof_train, oof_test, acc","863e5bf9":"oof_train_LogReg, oof_test_LogReg, acc_LogReg = CV_Logistic_Regression(X_train, y_train, X_test)","e0357319":"from sklearn.svm import SVC","ec148adf":"def CV_SVM(X_train, y_train, X_test, n_folds=5):\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 99)\n    \n    # Create oof sets for prediction storage.\n    oof_train = np.zeros(X_train.shape[0])\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    # Emptly list to store accuracy for each fold\n    fold_acc = []\n\n    for ifold, (tr_index, val_index) in enumerate(kf.split(X = X_train, y = y_train)):\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[tr_index,:]\n        X_val = X_train.iloc[val_index,:]\n        y_tr = y_train[tr_index]\n        y_val = y_train[val_index]\n\n        # Train model\n        model = SVC()\n        model.fit(X_tr, y_tr)\n\n        # Predict validation and test data and store them in oof sets.\n        oof_train[val_index] = model.predict(X_val)\n        oof_test[:, ifold] = model.predict(X_test)\n        \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[val_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n\n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc =round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n    \n    return oof_train, oof_test, acc","d21b8de8":"oof_train_SVM, oof_test_SVM, acc_SVM = CV_SVM(X_train, y_train, X_test)","6dbdd3ee":"from sklearn.tree import DecisionTreeClassifier","1aa70e40":"def CV_Decision_Tree(X_train, y_train, X_test, n_folds=5):\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 99)\n    \n    # Create oof sets for prediction storage.\n    oof_train = np.zeros(X_train.shape[0])\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    # Emptly list to store accuracy for each fold\n    fold_acc = []\n\n    for ifold, (tr_index, val_index) in enumerate(kf.split(X = X_train, y = y_train)):\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[tr_index,:]\n        X_val = X_train.iloc[val_index,:]\n        y_tr = y_train[tr_index]\n        y_val = y_train[val_index]\n\n        # Train model\n        model = DecisionTreeClassifier(random_state=0)\n        model.fit(X_tr, y_tr)\n\n        # Predict validation and test data and store them in oof sets.\n        oof_train[val_index] = model.predict(X_val)\n        oof_test[:, ifold] = model.predict(X_test)\n        \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[val_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n\n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc = round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n    \n    return oof_train, oof_test, acc","1ad4b42a":"oof_train_DecisionTree, oof_test_DecisionTree, acc_DecisionTree = CV_Decision_Tree(X_train, y_train, X_test)","c4037522":"from sklearn.ensemble import RandomForestClassifier","5d0b06e5":"def CV_Random_Forest(X_train, y_train, X_test, n_folds=5):\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 99)\n    \n    # Create oof sets for prediction storage.\n    oof_train = np.zeros(X_train.shape[0])\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    # Emptly list to store accuracy for each fold\n    fold_acc = []\n\n    for ifold, (tr_index, val_index) in enumerate(kf.split(X = X_train, y = y_train)):\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[tr_index,:]\n        X_val = X_train.iloc[val_index,:]\n        y_tr = y_train[tr_index]\n        y_val = y_train[val_index]\n\n        # Train model\n        model = RandomForestClassifier(max_depth=5, random_state=0)\n        model.fit(X_tr, y_tr)\n\n        # Predict validation and test data and store them in oof sets.\n        oof_train[val_index] = model.predict(X_val)\n        oof_test[:, ifold] = model.predict(X_test)\n        \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[val_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n\n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc = round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n    \n    return oof_train, oof_test, acc","b3b0d2c6":"oof_train_RandomForest, oof_test_RandomForest, acc_RandomForest = CV_Random_Forest(X_train, y_train, X_test)","6db054ee":"import tensorflow as tf","cbe46143":"def build_model(Nfeatures):\n    tf.keras.backend.clear_session()\n    # set random seed for reproducibility\n    tf.random.set_seed(99)\n\n    # define a model\n    model = tf.keras.Sequential()\n\n    # add the first hidden layer\n    model.add(tf.keras.layers.Dense(units = 8, activation = 'linear', input_dim = Nfeatures))\n    \n#     # add regularization\n#     model.add(tf.keras.layers.Dropout(0.2))\n\n#     # add the second hidden layer\n#     model.add(tf.keras.layers.Dense(units = 16, activation = 'linear'))\n    \n#     # add regularization\n#     model.add(tf.keras.layers.Dropout(0.2))\n    \n    # add the output layer\n    model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n\n    # compile the model\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n    \n    return model","1101df34":"model = build_model(X_train.shape[1])\nmodel.summary()","368032b2":"def CV_ANN(X_train, y_train, X_test, n_folds=5, seed=99):\n    \n    # Kfold split\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n\n    # Create oof sets for prediction storage.\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    # Save all models validation accuracy\n    fold_acc = []\n    \n    # Define early stopping\n    earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n        \n    for ifold, (train_index, valid_index) in enumerate(kf.split(X=X_train, y=y_train)):\n        \n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[train_index,:]\n        X_val = X_train.iloc[valid_index,:]\n        y_tr = y_train[train_index]\n        y_val = y_train[valid_index]\n        \n        # Restart model\n        model = build_model(X_tr.shape[1])\n        \n        # Train model\n        model.fit(\n            X_tr,\n            y_tr,\n            batch_size = 8,\n            epochs = 100,\n            validation_data = (X_val, y_val),\n            verbose = 0,\n            callbacks = [earlystop])\n        \n        # Prediction on validation data and store them in oof (out of folds) sets\n        oof_train[valid_index] = np.where(model.predict(X_val)[:,0]>0.5, 1, 0)\n        oof_test[:, ifold] = np.where(model.predict(X_test)[:,0]>0.5, 1, 0)\n        \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[valid_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n             \n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc = round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n\n    return oof_train, oof_test, acc","e61f3045":"oof_train_ANN, oof_test_ANN, acc_ANN = CV_ANN(X_train, y_train, X_test)","16edbdaa":"import lightgbm as lgb","70d34ea0":"def CV_LGBM(X_train, y_train, X_test, n_folds=5, early_stop=None):\n    kf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 99)\n    \n    # Create oof sets for prediction storage.\n    oof_train = np.zeros(X_train.shape[0])\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n\n    gbm_history, fi, fold_acc = {}, [], []\n    \n    for ifold, (tr_index, val_index) in enumerate(kf.split(X = X_train, y = y_train)):\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[tr_index,:]\n        X_val = X_train.iloc[val_index,:]\n        y_tr = y_train[tr_index]\n        y_val = y_train[val_index]\n\n        dtrain = lgb.Dataset(X_tr, y_tr)\n        dvalid = lgb.Dataset(X_val, y_val)\n\n        # Train LightGBM model\n        params ={\n            'task': 'train',\n            'boosting': 'gbdt',\n            'nthread': 2,\n            'objective': 'binary',\n            'metrics': 'binary',\n            'learning_rate': 0.01,\n            'num_leaves': 7,\n            'max_depth': 20,\n            'min_data_in_leaf': 100,\n            'seed': 0,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.6,\n            'bagging_freq': 1,\n            'verbose': -1}\n        model = lgb.train(params = params, train_set = dtrain, evals_result = gbm_history, \n                          num_boost_round = 100000, valid_sets = [dtrain, dvalid], \n                          early_stopping_rounds = early_stop, verbose_eval = 0)\n\n        # Predict validation and test data and store them in oof sets.\n        proba_train = model.predict(X_val, num_iteration = model.best_iteration)\n        oof_train[val_index] = np.where(proba_train > 0.5, 1, 0)\n        proba_test = model.predict(X_test, num_iteration = model.best_iteration)\n        oof_test[:, ifold] = np.where(proba_test > 0.5, 1, 0)\n\n        # feature importance\n        fi.append(model.feature_importance())\n    \n        # Accuracy\n        acc = accuracy_score(y_val, oof_train[val_index])\n        \n        print('FOLD: {} \\t Acc: {:.3f}'.format(ifold, acc))\n            \n        fold_acc.append(acc)\n             \n    print(f'\\nMEAN Acc\\t: {round(np.mean(fold_acc), 3)}')\n    acc = round(accuracy_score(y_train, oof_train), 3)\n    print(f'ACTUAL Acc\\t: {acc}')\n    print(f'STD Acc\\t\\t: {round(np.std(fold_acc), 3)}')\n    \n    return oof_train, oof_test, acc, fi","d6516445":"# drop one-hot-encoding\ndropf = ['Ticket_0', 'Ticket_1', 'Ticket_2', 'Cabin_0', 'Cabin_1', 'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5',\n         'Cabin_6', 'Cabin_7', 'Embarked_0', 'Embarked_1', 'Embarked_2']","05f8518e":"oof_train_LGBM, oof_test_LGBM, acc_LGBM, fi = CV_LGBM(X_train.drop(dropf, axis=1), \n                                            y_train, X_test.drop(dropf, axis=1), \n                                            early_stop=200)","aa0388f9":"usecols = [f for f in X_train.columns if f not in dropf]\nImp_feats = {}\nmean_vals = np.mean(fi,axis=0)\nfor col, val in zip(usecols, mean_vals):\n    Imp_feats.update({col: val})","e638bcd5":"data = pd.Series(Imp_feats).sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(18,12))\nsns.barplot(data.values, data.index, ax=ax)\nax.set_title('Features Importance');","18360ea8":"results = {\n    'Logistic_Regression': acc_LogReg,\n    'SVM': acc_SVM,\n    'Decision Tree': acc_DecisionTree,\n    'Random Forest': acc_RandomForest,\n    'Neural Networks': acc_ANN,\n    'LGBM': acc_LGBM\n}\npd.Series(results)","351f8517":"submit = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmit.head()","0f0f5883":"pred = np.mean(oof_test_LGBM, axis=1)\nsubmit['Survived'] = np.where(pred>0.5, 1, 0)","d55070bd":"submit.head()","6d2f330d":"# save\nsubmit.to_csv(\"pred.csv\", index=False)","dedd3808":"## <p style='color:blue'> Missing Values <\/p>","d9ab7e60":"## <p style='color:blue'> Numerical Features <\/p>","0c6d800c":"### <p style='color:blue'> Random Forest <\/p>","24b1b1df":"## <p style='color:blue'> General Information <\/p>","6b8766d7":"## <p style='color:blue'> Target <\/p>","febf5fef":"## <p style='color:blue'> Gradient Boosted Trees with LGBM <\/p>","720b66d5":"# Submission","040029cf":"# Summary","ae55727f":"# Features Engineering","420103a2":"## <p style='color:blue'> Categorical Features <\/p>","6a541c4e":"# Load Data","3235daf2":"# Label Encoding","d7e0f625":"## <p style='color:blue'> Logistic Regression <\/p>","7b5c1518":"## <p style='color:blue'> Decision Tree <\/p>","1bea77f6":"# Models","3776104c":"## <p style='color:blue'> Artificial Neural Networks <\/p>","0ea2edda":"## <p style='color:blue'> Support Vector Machine (SVM) <\/p>","c2c252b3":"# EDA","572d1b46":"# One Hot Encoding","beb4a923":"# Correlation"}}