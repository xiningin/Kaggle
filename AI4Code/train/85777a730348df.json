{"cell_type":{"f0fd2f4b":"code","7c584f65":"code","21f44e38":"code","72bd2c30":"code","0d6b70cc":"code","46583e8a":"code","2df8f3a3":"code","8077756a":"code","024bda1a":"code","cbd060a3":"code","a50d405e":"code","9fe03065":"code","c918d682":"code","230383a6":"code","2a2f21c5":"code","ffd93086":"code","fc5f2434":"code","10bed442":"code","291cd188":"code","58931e66":"code","d61e29a7":"code","c58be0cd":"code","37f93a79":"code","9c1df50c":"code","67aa7236":"code","58aadd18":"code","f42b35da":"code","411453ab":"code","0d534708":"code","af65f5ca":"code","9dacdcab":"code","bb3b3189":"code","e78d694e":"code","d3934be8":"code","82f9e1b8":"code","2eca7ff0":"code","564ef32d":"code","19bbabd9":"code","7ebae8db":"code","a6ceba5a":"code","ef2c2142":"code","cd1300c5":"code","35d81320":"code","65323bd1":"code","d70c22a9":"markdown","6b9c76e9":"markdown","9e5da06f":"markdown","2ac29202":"markdown","7c28f9e6":"markdown","2140b925":"markdown","e4bde532":"markdown","83182c2a":"markdown","b75ea14f":"markdown","6bf416b9":"markdown","1fe92736":"markdown","8aa4f39d":"markdown","b770fa74":"markdown","004faaf2":"markdown","f991ff31":"markdown","85fd1506":"markdown","24be5062":"markdown","5e5b2463":"markdown","c7accf6e":"markdown","857fa280":"markdown","52ca0630":"markdown","90dd15e7":"markdown","c204a8da":"markdown","43b78b0d":"markdown","9da676d3":"markdown","59b0ce6d":"markdown","df86122b":"markdown","602ff3aa":"markdown"},"source":{"f0fd2f4b":"!pip install -U vega_datasets notebook vega","7c584f65":"\nfrom __future__ import print_function, division\n\nimport numpy as np\nimport pandas as pd\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\npd.options.display.precision = 15\n\nimport time\nimport datetime\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py \nimport plotly.graph_objs as go \npy.init_notebook_mode(connected=True)\n\nalt.renderers.enable('notebook')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(15)\n","21f44e38":"import altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )","72bd2c30":"df_classes = pd.read_csv('\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_txs_classes.csv')\ndf_features = pd.read_csv('\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_txs_features.csv', header=None)\ndf_edgelist = pd.read_csv('\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_txs_edgelist.csv')","0d6b70cc":"df_classes['class'].value_counts()","46583e8a":"df_features.head()","2df8f3a3":"# renaming columns\ndf_features.columns = ['id', 'time step'] + [f'trans_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]","8077756a":"df_features.head()","024bda1a":"df_features['time step'].value_counts().sort_index().plot();\nplt.title('Number of transactions in each time step');","cbd060a3":"# merge with classes\ndf_features = pd.merge(df_features, df_classes, left_on='id', right_on='txId', how='left')","a50d405e":"plt.figure(figsize=(12, 8))\ngrouped = df_features.groupby(['time step', 'class'])['id'].count().reset_index().rename(columns={'id': 'count'})\nsns.lineplot(x='time step', y='count', hue='class', data=grouped);\nplt.legend(loc=(1.0, 0.8));\nplt.title('Number of transactions in each time step by class');","9fe03065":"bad_ids = df_features.loc[(df_features['time step'] == 37) & (df_features['class'] == '1'), 'id']\nshort_edges = df_edgelist.loc[df_edgelist['txId1'].isin(bad_ids)]","c918d682":"graph = nx.from_pandas_edgelist(short_edges, source = 'txId1', target = 'txId2', \n                                 create_using = nx.DiGraph())\npos = nx.spring_layout(graph)\nnx.draw(graph, cmap = plt.get_cmap('rainbow'), with_labels=True, pos=pos)","230383a6":"graph1 = nx.from_pandas_edgelist(short_edges, source = 'txId1', target = 'txId2', \n                                 create_using = nx.Graph())\npos1 = nx.spring_layout(graph1)\nnx.draw(graph1, cmap = plt.get_cmap('rainbow'), with_labels=False, pos=pos1)","2a2f21c5":"# grouped = df_features.groupby(['time step', 'class'])['trans_feat_0'].mean().reset_index()\n# chart = alt.Chart(grouped).mark_line().encode(\n#     x=alt.X(\"time step:N\", axis=alt.Axis(title='Time step', labelAngle=315)),\n#     y=alt.Y('trans_feat_0:Q', axis=alt.Axis(title='Mean of trans_feat_0')),\n#     color = 'class:N',\n#     tooltip=['time step:O', 'trans_feat_0:Q', 'class:N']\n# ).properties(title=\"Average trans_feat_0 in each time step by type\", width=600).interactive()\n# chart","ffd93086":"plt.figure(figsize=(12, 8))\ngrouped = df_features.groupby(['time step', 'class'])['trans_feat_0'].mean().reset_index()\nsns.lineplot(x='time step', y='trans_feat_0', hue='class', data=grouped);\nplt.legend(loc=(1.0, 0.8));\nplt.title('Average trans_feat_0 in each time step by type');","fc5f2434":"edges = pd.read_csv(\"\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_bitcoin_dataset\/elliptic_txs_edgelist.csv\")\nfeatures = pd.read_csv(\"\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_bitcoin_dataset\/elliptic_txs_features.csv\",header=None)\nclasses = pd.read_csv(\"\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/elliptic_bitcoin_dataset\/elliptic_txs_classes.csv\")\n","10bed442":"len(edges),len(features),len(classes)","291cd188":"display(edges.head(5),features.head(5),classes.head(5))\n","58931e66":"tx_features = [\"tx_feat_\"+str(i) for i in range(2,95)]\nagg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\nfeatures.columns = [\"txId\",\"time_step\"] + tx_features + agg_features\nfeatures = pd.merge(features,classes,left_on=\"txId\",right_on=\"txId\",how='left')\nfeatures['class'] = features['class'].apply(lambda x: '0' if x == \"unknown\" else x)","d61e29a7":"features.groupby('class').size()","c58be0cd":"count_by_class = features[[\"time_step\",'class']].groupby(['time_step','class']).size().to_frame().reset_index()\nillicit_count = count_by_class[count_by_class['class'] == '1']\nlicit_count = count_by_class[count_by_class['class'] == '2']\nunknown_count = count_by_class[count_by_class['class'] == \"0\"]","37f93a79":"x_list = list(range(1,50))\nfig = go.Figure(data = [\n    go.Bar(name=\"Unknown\",x=x_list,y=unknown_count[0],marker = dict(color = 'rgba(120, 100, 180, 0.6)',\n        line = dict(\n            color = 'rgba(120, 100, 180, 1.0)',width=1))),\n    go.Bar(name=\"Licit\",x=x_list,y=licit_count[0],marker = dict(color = 'rgba(246, 78, 139, 0.6)',\n        line = dict(\n            color = 'rgba(246, 78, 139, 1.0)',width=1))),\n    go.Bar(name=\"Illicit\",x=x_list,y=illicit_count[0],marker = dict(color = 'rgba(58, 190, 120, 0.6)',\n        line = dict(\n            color = 'rgba(58, 190, 120, 1.0)',width=1)))\n\n])\nfig.update_layout(barmode='stack')\npy.iplot(fig)","9c1df50c":"bad_ids = features[(features['time_step'] == 32) & ((features['class'] == '1'))]['txId']\nshort_edges = edges[edges['txId1'].isin(bad_ids)]\ngraph = nx.from_pandas_edgelist(short_edges, source = 'txId1', target = 'txId2', \n                                 create_using = nx.DiGraph())\npos = nx.spring_layout(graph)\n\nedge_x = []\nedge_y = []\nfor edge in graph.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.append(x0)\n    edge_x.append(x1)\n    edge_x.append(None)\n    edge_y.append(y0)\n    edge_y.append(y1)\n    edge_y.append(None)\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='blue'),\n    hoverinfo='none',\n    mode='lines')\n\nnode_x = []\nnode_y = []\nnode_text=[]\nfor node in graph.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n    node_text.append(node)\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        color=[],\n        size=10,\n        colorbar=dict(\n            thickness=15,\n            title='Transaction Type',\n            xanchor='left',\n            titleside='right',\n            tickmode='array',\n            tickvals=[0,1,2],\n            ticktext=['Unknown','Illicit','Licit']\n        ),\n        line_width=2))\nnode_trace.text=node_text\nnode_trace.marker.color = pd.to_numeric(features[features['txId'].isin(list(graph.nodes()))]['class'])\n\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title=\"Illicit Transactions\",\n                titlefont_size=16,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    showarrow=True,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002 ) ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\nfig.show()","67aa7236":"good_ids = features[(features['time_step'] == 32) & ((features['class'] == '2'))]['txId']\nshort_edges = edges[edges['txId1'].isin(good_ids)]\ngraph = nx.from_pandas_edgelist(short_edges, source = 'txId1', target = 'txId2', \n                                 create_using = nx.DiGraph())\npos = nx.spring_layout(graph)\n\nedge_x = []\nedge_y = []\nfor edge in graph.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.append(x0)\n    edge_x.append(x1)\n    edge_x.append(None)\n    edge_y.append(y0)\n    edge_y.append(y1)\n    edge_y.append(None)\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='blue'),\n    hoverinfo='none',\n    mode='lines')\n\nnode_x = []\nnode_y = []\nnode_text=[]\nfor node in graph.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n    node_text.append(node)\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        color=[],\n        size=10,\n        colorbar=dict(\n            thickness=15,\n            title='Transaction Type',\n            xanchor='left',\n            titleside='right',\n            tickmode='array',\n            tickvals=[0,1,2],\n            ticktext=['Unknown','Illicit','Licit']\n        ),\n        line_width=2))\nnode_trace.text=node_text\nnode_trace.marker.color = pd.to_numeric(features[features['txId'].isin(graph.nodes())]['class'])\n\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title=\"Licit Transactions\",\n                titlefont_size=16,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    showarrow=True,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002 ) ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\nfig.show()","58aadd18":"data = features[(features['class']=='1') | (features['class']=='2')]","f42b35da":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(15)","411453ab":"X = data[tx_features+agg_features]\ny = data['class']\ny = y.apply(lambda x: 0 if x == '2' else 1 )\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=15,shuffle=False)","0d534708":"reg = LogisticRegression().fit(X_train,y_train)\npreds = reg.predict(X_test)\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"Logistic Regression\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","af65f5ca":"clf = RandomForestClassifier(n_estimators=50, max_depth=100,random_state=15).fit(X_train,y_train)\npreds = clf.predict(X_test)\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"Random Forest Classifier\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","9dacdcab":"from torch.utils.data import Dataset, DataLoader","bb3b3189":"class LoadData(Dataset):\n    \n    def __init__(self,X,y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        features = self.X.iloc[idx]\n        features = np.array([features])\n        label = y.iloc[idx]\n\n        return features,label","e78d694e":"traindata = LoadData(X_train,y_train)\ntrain_loader = DataLoader(traindata,batch_size=128,shuffle=True)  ","d3934be8":"testdata = LoadData(X_test,y_test)\ntest_loader = DataLoader(testdata,batch_size=128,shuffle=False) ","82f9e1b8":"import torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.hidden = nn.Linear(165,50 )\n\n        self.output = nn.Linear(50,1)\n        self.out = nn.Sigmoid()\n        \n    def forward(self, x):\n\n        x = F.relu(self.hidden(x))\n\n        x = self.out(self.output(x))\n        \n        return x\n\nmodel = Network()","2eca7ff0":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion=nn.BCELoss()\nn_epochs=10","564ef32d":"for epoch in range(n_epochs):\n        model.to('cuda')\n        model.train()\n        running_loss = 0.\n        for data in train_loader:\n            x,label=data\n            x,label=x.cuda(),label.cuda()\n            output = model.forward(x.float())\n            output = output.squeeze()\n            loss = criterion(output.float(), label.float())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        else:\n            print(f\"Training loss: {running_loss\/len(train_loader)}\")","19bbabd9":"all_preds = []\nfor data in test_loader:\n    x,labels = data\n    x,labels = x.cuda(),labels.cuda()\n    preds = model.forward(x.float())\n    all_preds.extend(preds.squeeze().detach().cpu().numpy())\n\npreds = pd.Series(all_preds).apply(lambda x: round(x))\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"MLP\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","7ebae8db":"embed_names = [\"emb_\"+str(i) for i in range(1,51)]\nembeddings = pd.read_csv('\/kaggle\/input\/ellipticemb50d\/elliptic.emb',delimiter=\" \",skiprows=1,header=None)\nembeddings.columns = ['txId'] + [\"emb_\"+str(i) for i in range(1,51)]","a6ceba5a":"data = features[(features['class']=='1') | (features['class']=='2')]\ndata = pd.merge(data,embeddings,how='inner')\nX = data[tx_features+agg_features+embed_names]\ny = data['class']\ny = y.apply(lambda x: 0 if x == '2' else 1 )\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=15,shuffle=False)\n","ef2c2142":"reg = LogisticRegression().fit(X_train,y_train)\npreds = reg.predict(X_test)\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"Logistic Regression\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","cd1300c5":"clf = RandomForestClassifier(n_estimators=50, max_depth=100,random_state=15).fit(X_train,y_train)\npreds = clf.predict(X_test)\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"Random Forest Classifier\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","35d81320":"traindata = LoadData(X_train,y_train)\ntrain_loader = DataLoader(traindata,batch_size=128,shuffle=True)  \n\ntestdata = LoadData(X_test,y_test)\ntest_loader = DataLoader(testdata,batch_size=128,shuffle=False)  \n\nimport torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.hidden = nn.Linear(215,50 )\n\n        self.output = nn.Linear(50,1)\n        self.out = nn.Sigmoid()\n        \n    def forward(self, x):\n\n        x = F.relu(self.hidden(x))\n\n        x = self.out(self.output(x))\n        \n        return x\n\nmodel = Network()","65323bd1":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion=nn.BCELoss()\nn_epochs=10\n\nfor epoch in range(n_epochs):\n        model.to('cuda')\n        model.train()\n        running_loss = 0.\n        for data in train_loader:\n            x,label=data\n            x,label=x.cuda(),label.cuda()\n            output = model.forward(x.float())\n            output = output.squeeze()\n            loss = criterion(output.float(), label.float())\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        else:\n            print(f\"Training loss: {running_loss\/len(train_loader)}\")\n            \nall_preds = []\nfor data in test_loader:\n    x,labels = data\n    x,labels = x.cuda(),labels.cuda()\n    preds = model.forward(x.float())\n    all_preds.extend(preds.squeeze().detach().cpu().numpy())\n\npreds = pd.Series(all_preds).apply(lambda x: round(x))\nprec,rec,f1,num = precision_recall_fscore_support(y_test,preds, average=None)\nprint(\"MLP\")\nprint(\"Precision:%.3f \\nRecall:%.3f \\nF1 Score:%.3f\"%(prec[1],rec[1],f1[1]))\nmicro_f1 = f1_score(y_test,preds,average='micro')\nprint(\"Micro-Average F1 Score:\",micro_f1)","d70c22a9":"On display les premier \u00e9l\u00e9ments des 3 fichier ","6b9c76e9":"* MACHINE LEARNING PART ,\n\non ne s'int\u00e9resse QUE aux transactions licite et illicite, le reste on ignore","9e5da06f":"## General information\n\njeu de donn\u00e9e :  Elliptic Data Set. \nobjectif : explore the data - trouver des datas illicite int\u00e9ressante\n","2ac29202":"* Class `1` = la transaction est illicite \n* Class '2' = la transaction est licite\n* On a ici 42019 transactions licite et 4545 transactions illicite\n","7c28f9e6":"## Data loading and overview","2140b925":"Ici on va chercher \u00e0 voir la r\u00e9partition des transactions illicite. On voit ici des populations qui sont rapproch\u00e9. ","e4bde532":"# \nNous pouvons voir que cette feature peut s\u00e9parer efficacement les transactions illicites des transactions licites.","83182c2a":"Maintenant, nous pouvons voir qu'il y avait des pics de transactions illicites qui se produisaient g\u00e9n\u00e9ralement quand il y avait une augmentation g\u00e9n\u00e9rale du nombre de transactions.\n","b75ea14f":"Pyuis les licites. Tr\u00e8s dispers\u00e9 ","6bf416b9":"Consid\u00e9rant que le delta entre les pas de temps est de 2 semaines (comme dit dans la documentation), nous avons 98 semaines - presque 2 ans. Il y avait des tendances \u00e0 la hausse et \u00e0 la baisse, mais nous ne pouvons rien voir d'int\u00e9ressant sur une simple observation de ces volumes. Divisons les transactions par classe.","1fe92736":"On d\u00e9limite les p\u00e9riode et on regarde combien de transaction pour chaque ","8aa4f39d":"longueur des jeu de donn\u00e9es","b770fa74":"Class = licite ou illicite, ou on ne sait pas , voici les r\u00e9sultats ,\n* Class `0` = on ne sait pas\n* Class `1` = la transaction est illicite \n* Class '2' = la transaction est licite","004faaf2":"Jetons un coup d'\u0153il \u00e0 un graphique \u00e0 un moment donn\u00e9. \n","f991ff31":" ## exemples de graphs","85fd1506":"on compare la proportion de licite \/ illicite \/ unknow par rapport aux time step","24be5062":"## Explorons les features","5e5b2463":"ANALYSE V2 , graphiques plus explicites\n","c7accf6e":"premi\u00e8rement je r\u00e9cup\u00e8re les donn\u00e9es\n","857fa280":"etude de la premi\u00e8re feature","52ca0630":"## Nombre de transactions et classes","90dd15e7":"On d\u00e9limite ces colonnes ","c204a8da":"Le document avec les features est totalement anonimys\u00e9, il n'a pas de noms de colonnes, voici les indications de la description des datas :\n- premi\u00e8re colonne (avec comme nom `0`) est l' id de transaction;\n- la colonne '1' repr\u00e9sente la p\u00e9riode de chaque noeud. Ces dates \/ heures ont une intervale de 2 semaines. Chaque dates \/ heure contient des composant de transactions connect\u00e9s, qui apparaissent dans la blockchain apr\u00e8s moins de trois heures entre chaque.\n- Les 93 colonnes suivantes affichent des informations sur la transaction: nombre d'entr\u00e9es \/ sorties, frais de transaction, volume de sortie et chiffres agr\u00e9g\u00e9s tels que le BTC moyen re\u00e7u (d\u00e9pens\u00e9) par les entr\u00e9es \/ sorties et le nombre moyen de transactions entrantes (sortantes) associ\u00e9es aux entr\u00e9es \/ les sorties;\n- les 72 fonctionnalit\u00e9s restantes sont des fonctionnalit\u00e9s agr\u00e9g\u00e9es, obtenues en utilisant les informations de transaction un saut en arri\u00e8re \/ en avant \u00e0 partir du n\u0153ud central - donnant les coefficients maximum, minimum, d'\u00e9cart type et de corr\u00e9lation des transactions voisines pour les m\u00eames donn\u00e9es d'information (nombre d'entr\u00e9es \/ sorties, frais de transaction, etc.).\n\n- toutes ces informations sont int\u00e9ressante car gr\u00e2ce \u00e0 certaines de ces features nous allons pouvoir avoir des informations exacte sur les transaction et le trajet des BTC et donc faire un rapport entre celles ci et leur status frauduleux ou non.\n\n","43b78b0d":"on divise maintenant les transactions par classe","9da676d3":"On voit clairement ci-dessus des groupement de transactions \u00e0 caract\u00e8re illicite et d'autres isol\u00e9s","59b0ce6d":"le dataset comporte 3 fichiers, voici le load","df86122b":"Nous pouvons clairement voir que certains fraudeurs ont travaill\u00e9 seuls et certains ont travaill\u00e9 en groupe.\n","602ff3aa":"We have the following data:\n- 203769 transactions \/ graph nodes;\n- 234355 bitcoin flows \/ graph edges;\n- `elliptic_txs_edgelist.csv` contains graph edges information;\n- `elliptic_txs_classes.csv` contains information about legality of transactions;\n- `elliptic_txs_features.csv` contains information about transaction features;"}}