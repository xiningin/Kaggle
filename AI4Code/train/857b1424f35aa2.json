{"cell_type":{"154f54f4":"code","8b22f743":"code","7187353f":"code","83821206":"code","ab396497":"code","38f6be6e":"code","b806a1b0":"code","9a9f1f7f":"code","61f836e8":"code","b719a37d":"code","11d0dc12":"code","17484eba":"code","185c5023":"code","f21d4255":"code","63e95ea3":"code","3bf912ea":"code","a7ca2bde":"code","9615a6f9":"code","c64432f1":"code","a69deb57":"code","05c292c3":"code","e5484584":"markdown","dda8eb92":"markdown","5c654d32":"markdown","dd1c85be":"markdown","e7498f32":"markdown","96d657db":"markdown","ceec986c":"markdown","efc3335d":"markdown","09f09997":"markdown","f544e68c":"markdown","f3b63faf":"markdown","a1a4a7fc":"markdown","c562997f":"markdown","53ae48e6":"markdown","c44757de":"markdown","c3387019":"markdown","23502225":"markdown","413f33bb":"markdown","368e5a72":"markdown","cf030a0e":"markdown","35318e12":"markdown","ccbc2c03":"markdown","6fceeb13":"markdown","bf5b6dba":"markdown","4a808cd2":"markdown","ea02faa4":"markdown","7998d470":"markdown","19e6a65c":"markdown","40ae0e22":"markdown"},"source":{"154f54f4":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nplt.style.use(\"ggplot\")\nwarnings.filterwarnings(\"ignore\")\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","8b22f743":"train_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\n\ntrain = train_data.copy()","7187353f":"train_data.shape, test_data.shape","83821206":"train.head(2)","ab396497":"def dataDetails(df):\n    res = pd.DataFrame()\n    res[\"Columns\"] = list(df.columns)\n    res[\"Missing_Values\"] = list(df.isna().sum())\n    res[\"Data_Type\"] = list(df.dtypes)\n    res[\"Unique_Values\"] = [df[x].nunique() for x in df.columns]\n    \n    return res\n\ndetails = dataDetails(train)\ndetails","38f6be6e":"X, y = train.drop(\"target\",axis=1), train.target","b806a1b0":"def model(X,y):\n    from sklearn.model_selection import StratifiedKFold, cross_val_score\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import precision_score,recall_score,f1_score\n    \n    lr = LogisticRegression()\n    cv = StratifiedKFold(n_splits = 10, random_state = 3)\n    scores = cross_val_score(lr,X,y,cv=cv)\n    \n    print(\"Mean Score : {:.3f}\".format(np.mean(scores)))","9a9f1f7f":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\n\nohe_columns = [\"bin_3\",\"bin_4\",\"nom_0\",\"nom_1\",\"nom_2\",\"nom_3\",\"nom_4\"]\nohe_result = ohe.fit_transform(train[ohe_columns])\n\n# To help visualize the outputs of OneHotEncoder we will use Pandas get_dummies() method.\n# But from implementation-point-of-view, One-Hot Encoder from sklearn is better, as it could be directly used\n# to fit into the test data.\n\nohe_output = pd.get_dummies(train[ohe_columns])\nohe_output.head(2)","61f836e8":"train.ord_2.value_counts()","b719a37d":"categories = [[\"Freezing\",\"Cold\",\"Warm\",\"Hot\",\"Boiling Hot\",\"Lava Hot\"]]\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder(categories=categories, dtype = int)\nout = oe.fit_transform(np.array(train.ord_2).reshape(-1,1))","11d0dc12":"for i in range(10):\n    print(str(train.loc[i,\"ord_2\"])+\" --> \"+str(out[i][0]))","17484eba":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_out = le.fit_transform(np.array(train.nom_5).flatten())","185c5023":"for i in range(10):\n    print(str(train.loc[i,\"nom_5\"])+\" --> \"+str(le_out[i]))","f21d4255":"le_out_ohe = ohe.fit_transform(np.array(le_out).reshape(-1,1))","63e95ea3":"print(\"Shape: {}\".format(le_out_ohe.shape))","3bf912ea":"# if category_encoders isn't installed, use !pip install category_encoders\nbe_col = train[[\"nom_5\",\"nom_6\",\"nom_7\",\"nom_8\",\"nom_9\"]]\nimport category_encoders as ce\nbe = ce.binary.BinaryEncoder()\nbe_out = be.fit_transform(be_col,y)","a7ca2bde":"be_out.shape","9615a6f9":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(input_type='string')\nX_hash = X.ord_2.astype(str)\nfh_out = fh.fit_transform(X_hash.values)\nfh_out.shape","c64432f1":"fh = FeatureHasher(input_type='string',n_features=20) #n_features = 20\nX_hash = X.ord_2.astype(str)\nfh_out = fh.fit_transform(X_hash.values)\nfh_out.shape","a69deb57":"# One-Hot Encoding\nohe_columns = [\"bin_0\",\"bin_1\",\"bin_2\",\"bin_3\",\"bin_4\"]\nohe_out = pd.get_dummies(train[ohe_columns],drop_first=True)\n\n# Nominal Feature Encoding : Binary Encoding or (Ordinal Encoding + Label Binarizer)\nimport category_encoders as ce\nnom_columns = [\"nom_0\",\"nom_1\",\"nom_2\",\"nom_3\",\"nom_4\",\"nom_5\",\"nom_6\",\"nom_7\",\"nom_8\",\"nom_9\"]\nbe = ce.BinaryEncoder(return_df = True)\nbe_out = be.fit_transform(train[nom_columns],y)\n\n# Ordinal Feature Encoding : Ordinal Encoding\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder(dtype=int)\nord_columns = [\"ord_1\",\"ord_2\",\"ord_3\",\"ord_4\",\"ord_5\"]\nord_out = oe.fit_transform(train[ord_columns])\nord_out = pd.DataFrame(ord_out,columns=ord_columns)\n\nnew_train = pd.concat([ohe_out,be_out],axis=1)\nnew_train[\"ord_0\"] = train[\"ord_0\"]\nnew_train = pd.concat([new_train,ord_out],axis=1)","05c292c3":"model(new_train,y) #fitting and calculating model performance","e5484584":"## **Importing Data**","dda8eb92":"By default, number of columns in Feature Hashing is **2<sup>20<\/sup> = 1048576**\n\nIt can be changed via the parameter **n_features**","5c654d32":"### One-Hot Encoding ended up creating 222 new columns for a SINGLE feature. However, Binary Encoding added 60 new columns in total for 5 features.\n\n### How did this happen?\n\n## Working of Binary Encoding\n![be.png](attachment:be.png)\n\nIn Binary Encoding, the Label Encoded value is converted into **Binary Number System**.\n\nFor N categories in a feature, One-Hot Encoding would use atleast N-1 new columns. Let's see how many columns Binary Encoding would need.\nLet the number of columns needed be n\n\n\nTo represent all the categories, \n***2N*** $\\gt$ **$2^n$ $\\geq$ *N***\n$\\implies$ $\\log_2 2N$ $\\gt$ n $\\geq$ $\\log_2 N$\n\n\nIt can be easily interpreted from the [graph](https:\/\/imgur.com\/Uz8vd0U) that,\n\n$\\log_2 N$ $\\lt$ N-1, for all $N\\gt 2$\n\nand\n\n$\\log_2 2N$ $\\lt$ N-1, for all $N\\gt 4$\n\nIf you didn't get the logic behind the terms 2N and N, [Click Here](http:\/\/)\n## Hence, Binary Encoding will result in significantly lesser number of new columns as compared to One-Hot Encoding.\n\n","dd1c85be":"# Label Encoding\nIt encodes the target variables to numerical value according to alphabetical order (in case of strings).\n\n[Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) for the Label Encoder documentation.","e7498f32":"![intro_meme.jpeg](attachment:intro_meme.jpeg)","96d657db":"## Data Statistics","ceec986c":"# Combining Everything","efc3335d":"### **When number of categories is large, the number of newly created columns by One-Hot Encoder is huge too. Hence, in this tutorial only a selected features were used for One-Hot Encoding, which have less than 7 categories in it.**\n\n## Question : Why weren't the columns named \"ord_1\" and \"ord_2\" encoded via One-Hot Encoding? They too had frequency of categories less than 7.\n## Answer : Because they are Ordinal Features and they need to be handled through other encoding techniques.\n### If you are not familiar with \"**Ordinal**\" and \"**Nominal**\" data types, [Click Here](https:\/\/towardsdatascience.com\/data-types-in-statistics-347e152e8bee).","09f09997":"## What's the difference between Ordinal Encoder and Label Encoder? They seem to be performing the same task!\n\n### Ordinal Encoder and Label Encoder perform the same function. However, Ordinal Encoder is designed for features and can accept input in the shape of (n_samples, n_features), but Label Encoder is made for labels and hence they accept input in the shape of (n_samples,)\n\n### However, our work with the nominal feature encoding isn't done yet!\n# REMEMBER!\n## The nominal features don't have a sense of priority or order in them unlike Ordinal features.\n### Hence, simply leaving them after encoding with Label Encoder or Ordinal Encoder will give the ML model a wrong impression about those features. Unwanted weightage might be given to some of the categories in the nominal features. \nFor example, look at the following outputs from Label Encoding above.\n\n50f116bcf --> 78\n\nb3b4d25d0 --> 159\n\n### The model will interpret the above encoding as : Priority(b3b4d25d0) > Priority(50f116bcf), which is absurd and not at all relevant. This may lead to poor performance for the model.\n\n### Hence, for nominal features One-Hot Encoding is usually performed after Label\/Ordinal Encoding.\n","f544e68c":"![hmmm-wait-a-minute.jpg](attachment:hmmm-wait-a-minute.jpg)","f3b63faf":"## There are a lot of feature encoding techniques out there. In this notebook I have mentioned about a few of them, which are mostly used and their usefulness. \n\nDo access the following links for more details:\n\n1) https:\/\/towardsdatascience.com\/cyclical-features-encoding-its-about-time-ce23581845ca\n\n2) https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/\n\n## Also, I haven't focused much on the model and performance portion as the motive of this notebook inclined more towards the encoding techniques. \n\n## If you enjoyed reading it, do upvote.\n\n## Contact for any doubts or clarification on the content.","a1a4a7fc":"### Let's test Label Encoding on \"nom_5\" feature in the dataset.","c562997f":"### Some More Encoding Techniques","53ae48e6":"## Importing Libraries ","c44757de":"## So far, So good!\n### However, let's look at the shape of the output after One-Hot Encoding the Label-encoded nominal features.","c3387019":"### \"\"\" WHAT ABOUT OTHER FEATURES? YOU SELECTED ONLY A FEW OF THEM WITHOUT ANY CONTEXT OR REASON!! TRYING TO ESCAPE, HUH? \"\"\"\n\n## Definitely not!\nEach type of Encoding technique has it's own advantages and disadvantages, making it effiecient in **specific** conditions. If you focus on the output from One-Hot Encodings, you will observe there is an increase in the number of features.\nIf a feature **X** has **2 categories**, say \"**Yes**\" and \"**No**\", One-Hot Encoding would make **2 columns**; one for \"Yes\" and other for \"No\". Now let's assume, if **X** has an entry \"**Yes**\" in it, then corresponding to that row, \"**X_Yes**\" (*the new column created by One_Hot Encoder*) will have **1** while the \"**X_No**\" will have **0** in it.\n\nFor example, look at the image below.\n![ohe.png](attachment:ohe.png)\nWe have 3 categories in the \"Colour\" feature. Hence, One-Hot-Encoder makes 3 new columns against it, namely \"Colour Green\",\"Color Red\" and \"Color Blue\". For a record with Color as Green, the column \"Color Green\" contains 1 and the rest of the columns have 0 in them.\n## Did we really need 3 additional columns for the above example? NO!!\n\n### One-Hot encoding results in the famous Dummy Variable Trap and multicollinearity. [Click Here](https:\/\/towardsdatascience.com\/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a) to know more.\n\nIf you look at the last row of the above table,\nColor = Blue\nNow, the \"**Color Green**\" and \"**Color Red**\" have **0** in them. So, it's **obvious** that \"**Color Blue**\" will have **1**.\nGo to the documentation for Pandas Documentation for get_dummies() method. You will see a parameter called **drop_first**. If **drop_first** is set to **True** (*by default it's set to False*), the first level is dropped. In simple words, if you have **N** categories in a feature, setting **drop_first** to **True** will create **N-1 new columns instead of N.**\n\nA simple example,\n\nIf you proposed to your crush and they say \"**Yes**\", then it's obvious to you that they didn't say \"**No**\".\n![propose.jpg](attachment:propose.jpg)\nPS: I hope she says **YES**!","23502225":"# Ordinal Encoding\nIt is used in case of **Ordinal** features and is very useful if the priority of the categories is known beforhand.\n\n[Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html) for the documentation of Ordinal Encoder.","413f33bb":"If the problem statement demands that the categories in \"**ord_2**\" feature has **priority** according the level of \nhotness\/temperature, then\n\nPriority Order:\n#### Freezing < Cold < Warm < Hot < Boiling Hot < Lava Hot\n\nThis priority order could be easily fitted into the Ordinal Encoder object through the parameter called \"**categories**\". For more information [Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html).","368e5a72":"## Model Function and Performance Monitoring\n\n[Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) for Decision Tree Classifier documentation","cf030a0e":"# BINARY ENCODING","35318e12":"### Let's look at a few outputs from the Label Encoding","ccbc2c03":"## 222 columns!\n\nThat's a tremendous increase in the number of columns! Even if **drop_first** is set to **True**, there will be **221** columns!\n\n## Is there any other way? Obviously there is!","6fceeb13":"# One-Hot Encoding\nLet's look at the application of One-Hot Encoding first.\n\n[Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) for the Documentation.","bf5b6dba":"# Feature Encoding\n\n### Need for Encoding\nIt is not necessary that the data available to us is always in the form of numbers. There are various instances when the data maybe in the form of categories, texts and choices. \nFor example, when a person is asked about his\/her gender, the response would be *male*, *female* or *other*. It won't be something like 0 or 1. Hence, categorical variables usually find their way into the data most of the time.\n\n## So, what's the problem in that?\n\nThe machine learning algorithms developed can handle only numerical values. So, the categorical variables need to be pre-processed and converted or mapped into some numerical values before passing them to the model. This process of mapping categorical variables to numerical values is called **Feature Encoding.**","4a808cd2":"**Pandas get_dummies()** function performs One-Hot Encoding on the specified columns and returns a Data Frame containing the *One-Hot Encoded columns*.\n\n[Click Here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) for Pandas get_dummies() method documentation.","ea02faa4":"## Encoding Techniques","7998d470":"# Feature Hasher\n\nFeature Hashing is generally used in [Natural Language Processing](https:\/\/machinelearningmastery.com\/natural-language-processing\/), where it helps to counter and minimize the problem of **high dimensionality** and **sparse data** in the dataset. Refer to this [video](https:\/\/www.youtube.com\/watch?v=z9irRiTdDoE) for more guidance.\n\n[Click Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.FeatureHasher.html) for the Feature Hasher documentation.","19e6a65c":"### Let's look at a few outputs from the Ordinal Encoding","40ae0e22":"### Let's examine the categories present in the \"ord_2\" feature of the dataset."}}