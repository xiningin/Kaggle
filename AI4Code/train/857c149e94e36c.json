{"cell_type":{"8d39d1d2":"code","abc06efc":"code","3e849f2b":"code","8fa8523c":"code","2dfb9a46":"code","994ac0ed":"code","7148fd49":"code","0c6ba4b0":"code","060416a3":"code","ea8abcf1":"code","88a987e6":"code","ec04086d":"code","30964e9f":"code","61869ec7":"code","c2ac32c5":"code","414ce9ee":"code","a75696c2":"code","946a9ae4":"code","9355bc61":"code","b9697acb":"code","266ccf26":"code","8e1e4ad5":"code","741d63ef":"code","a4d04775":"code","e6f44c4c":"markdown","bb838851":"markdown","78163c7a":"markdown","ca6381fc":"markdown","256da56c":"markdown","0f763c49":"markdown","14e886da":"markdown"},"source":{"8d39d1d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abc06efc":"def count_values(df, colname, n_values=20, ascending=False):\n    counts = df\\\n        [colname]\\\n        .astype(str)\\\n        .replace(\"nan\", \"unknown\")\\\n        .value_counts()\\\n        .sort_values(ascending=ascending)\n    \n    plot = counts\\\n        .iloc[:n_values]\\\n        .plot\\\n        .bar(title=f\"{colname} ({counts.shape[0]} unique values)\")\n    \n    return plot\n\ndef hist_values(df, colname, bins=10):\n    if df[colname].dtype != \"object\":\n        plot = df.hist(column=colname, bins=bins)\n        return plot\n    \n\ndef plot_values(df, colname, n_values=20, top_n=True, bins=10):\n    if df[colname].dtype == \"float\":\n        return hist_values(df=df, colname=colname, bins=bins)\n    \n    else:\n        ascending = not top_n\n        return count_values(df=df, colname=colname, n_values=n_values, ascending=ascending)\n    ","3e849f2b":"# IMPORT DATASET\n\nfilename = \"\/kaggle\/input\/personal-cars-classifieds\/all_anonymized_2015_11_2017_03.csv\"\nraw_df = pd.read_csv(filename)\nraw_shape = raw_df.shape\n\nprint(f\"Raw data has {raw_shape[0]} rows, and {raw_shape[1]} columns\")","8fa8523c":"# PASS IN DATA TYPES WHEN READING\n# - SPEED UP READ\n# - AVOID READ ERRORS\n\nraw_dtypes = {\n    \"maker\": str,\n    \"model\": str,\n    \"mileage\": float,\n    \"manufacture_year\": float, # np.NaN doesn't work with int\n    \"engine_displacement\": float,\n    \"engine_power\": float,\n    \"body_type\": str,\n    \"color_slug\": str,\n    \"stk_year\": str, # None's cannot be converted by pandas here\n    \"transmission\": str,\n    \"door_count\": str,\n    \"seat_count\": str,\n    \"fuel_type\": str,\n    \"date_created\": str,\n    \"date_last_seen\": str,\n    \"price_eur\": float}\n\nraw_df = pd.read_csv(filename, dtype=raw_dtypes)\nraw_shape = raw_df.shape\n\n\n# AND CONVERT TYPES TO NUMERIC WHERE REQUIRED\n# THIS WILL ALSO GET RID OF ANY UNEXPECTED TEXT IN THESE FIELDS (e.g. \"None\")\nto_num = [\"stk_year\", \"door_count\", \"seat_count\"]\nfor col in to_num:\n    raw_df[col] = pd.to_numeric(raw_df[col], errors=\"coerce\")\n    \nprint(f\"Raw data has {raw_shape[0]} rows, and {raw_shape[1]} columns\")","2dfb9a46":"# SOME USEFUL METHODS TO START EXPLORING\n\nprint(raw_df.info(), \"\\n\\n\\n\")\nprint(raw_df.describe(), \"\\n\\n\\n\")\nprint(raw_df.isna().sum())\nraw_df.head()\n","994ac0ed":"# EXPLORE EACH COLUMN\ncolname = raw_df.columns[0]\nplot_values(raw_df, colname, n_values=20, top_n=True, bins=8)","7148fd49":"# DEALING WITH NA's (CAN'T MODEL WITH THEM)\n\n# CREATE EXISTS COLUMNS\nraw_df[\"stk_bool\"] = raw_df[\"stk_year\"].notnull().astype(int)\n\n\n# IMPUTE SOME NAs - manufacture_year\n# - USE MAKE\/MODEL TO ESTIMATE manufacture_year (IS THIS SENSIBLE)\nyear_of_scepticism = 1970\naverage_years = raw_df\\\n    .loc[raw_df[\"manufacture_year\"] >= year_of_scepticism]\\\n    .groupby([\"maker\", \"model\"])\\\n    [\"manufacture_year\"]\\\n    .mean()\\\n    .round()\\\n    .rename(\"avg_manufacture_year\")\\\n    .reset_index()\n\n\n# REPLACE SCEPTICAL VALUES AND NAs\nclean_df = raw_df.merge(average_years, how=\"left\", on=[\"maker\", \"model\"])\nclean_df.loc[raw_df[\"manufacture_year\"] < year_of_scepticism, \"manufacture_year\"] = np.nan\nclean_df[\"manufacture_year\"] = clean_df[\"manufacture_year\"].fillna(clean_df[\"avg_manufacture_year\"])\n\n\n# DROP COLUMNS AND CREATE NEW DF\ndrop_cols = [\"model\", \"body_type\", \"color_slug\", \"stk_year\", \"avg_manufacture_year\"]\nclean_df = clean_df.drop(drop_cols, axis=\"columns\")\n\n\n# DROP ALL ADVERTS WITH AT LEAST ONE NA\nclean_df = clean_df.dropna()\nclean_df","0c6ba4b0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","060416a3":"# FIT BENCHMARK MODEL (JUST NUMERIC FEATURES)\nfeatures_initial = [\"mileage\", \"manufacture_year\", \"engine_displacement\", \"engine_power\", \"door_count\", \"seat_count\", \"stk_bool\"]\ntarget = \"price_eur\"\n\nX_initial = clean_df[features_initial]\ny_initial = clean_df[target]\n\n\ndef fit_and_score(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        test_size=0.3,\n        random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    score = lr.score(X_test, y_test)\n    rmse = mean_squared_error(y_test, lr.predict(X_test))\n    actual_predicted = pd.DataFrame({\"predicted\": lr.predict(X_test), \"actual\": y_test})\n    \n    return (score, actual_predicted)\n\n\n\nscore_initial = fit_and_score(X_initial, y_initial)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))","ea8abcf1":"score_initial[1].plot.scatter(x=\"actual\", y=\"predicted\")","88a987e6":"# PRICE\nhist_values(clean_df, target, bins=100)\n# hist_values(clean_df.loc[clean_df[target] < 100000], target, bins=100)\n# hist_values(clean_df.assign(log=np.log1p(clean_df[target])), \"log\", bins=100)\n# clean_df[clean_df[target] == 1295.34]\n","ec04086d":"# MODEL ON LOG(PRICE)\nX_logged = clean_df[features_initial]\ny_logged = np.log1p(clean_df[target])\n\n\nscore_logged = fit_and_score(X_logged, y_logged)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))","30964e9f":"# FILTER OUT UNUSUAL DATA\nfiltered_df = clean_df.copy()\nfiltered_df = filtered_df[filtered_df[target] != 1295.34]\n\nX_filtered = filtered_df[features_initial]\ny_filtered = np.log1p(filtered_df[target])\n\nscore_filtered = fit_and_score(X_filtered, y_filtered)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))","61869ec7":"# count_values(filtered_df, \"maker\", ascending=False)\ncount_values(filtered_df, \"seat_count\", ascending=False)","c2ac32c5":"# CREATE MANUFACTURER CATEGORIES\nluxurious_makers = [\n    'bentley', 'bmw', 'chevrolet', 'dodge',\n    'hummer', 'mercedes-benz', 'rolls-royce']\n\nsports_makers = [\n    'alfa-romeo', 'aston-martin', 'audi', 'jaguar',\n    'lamborghini', 'lotus', 'maserati', 'porsche', 'tesla']\n\nfiltered_df[\"maker_type\"] = np.select(\n    condlist=[\n        filtered_df[\"maker\"].isin(luxurious_makers),\n        filtered_df[\"maker\"].isin(sports_makers)],\n    choicelist=[\n        \"luxurious\",\n        \"sports\"],\n    default=\"normal\")\n\n# CREATE CAR SIZE CATEGORIES\nfiltered_df[\"seat_str\"] = np.select(\n    condlist=[\n        (filtered_df[\"seat_count\"] >= 0) & (filtered_df[\"seat_count\"] < 4),\n        (filtered_df[\"seat_count\"] >= 4) & (filtered_df[\"seat_count\"] < 6),\n        (filtered_df[\"seat_count\"] >= 6) & (filtered_df[\"seat_count\"] < 10),\n        (filtered_df[\"seat_count\"] >= 10)],\n    choicelist=[\n        \"small\",\n        \"medium\",\n        \"large\",\n        \"very large\"],\n    default=\"unknown\")\n\nfiltered_df.head()","414ce9ee":"# CALCULATE TIME THE AD HAS BEEN POSTED FOR\nfiltered_df[\"date_created\"] = pd.to_datetime(filtered_df[\"date_created\"])\nfiltered_df[\"date_last_seen\"] = pd.to_datetime(filtered_df[\"date_last_seen\"])\nfiltered_df[\"post_duration\"] = (filtered_df[\"date_last_seen\"] - filtered_df[\"date_created\"]).dt.total_seconds() \/ 86400","a75696c2":"# ENCODING CATEGORICAL VARIABLES\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nstr_cols = [\"transmission\", \"fuel_type\", \"seat_str\", \"maker_type\"]\nmodel_df = pd.DataFrame(index=filtered_df.index)\nfor col in str_cols:\n    ohe = OneHotEncoder()\n    ohe.fit(filtered_df[[col]])\n    col_df = pd.DataFrame(\n        ohe.transform(filtered_df[[col]]).toarray(), \n        columns=ohe.get_feature_names([col]), \n        index=filtered_df.index)\n    \n    model_df = pd.concat([model_df, col_df], axis=1)","946a9ae4":"# ADD NUMERIC COLUMNS TO ENCODED COLUMNS\nnum_cols = [\"manufacture_year\", \"mileage\", \"engine_displacement\", \"engine_power\", \"door_count\", \"post_duration\", \"stk_bool\", target]\nmodel_df[num_cols] = filtered_df[num_cols]\nmodel_df.head()","9355bc61":"X_engineered = model_df.drop(target, axis=\"columns\")\ny_engineered = np.log1p(model_df[target])\n\nscore_engineered = fit_and_score(X_engineered, y_engineered)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))\nprint(\"Engineered model R Squared: %0.3f (%i observations)\" % (score_engineered[0], X_engineered.shape[0]))","b9697acb":"score_engineered[1].plot.scatter(x=\"actual\", y=\"predicted\")","266ccf26":"# STILL A FEW OUTLIERS\n# REMOVE ADVERTS THAT ARE OUTLIERS IN ANY COLUMN\nfrom scipy import stats\n\nfinal_df = model_df\\\n    .loc[:, lambda df: df.std() > 0.05]\\\n    .loc[lambda df: (np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\nX_final = final_df.drop(target, axis=\"columns\")\ny_final = np.log1p(final_df[target])\n\nscore_final = fit_and_score(X_final, y_final)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))\nprint(\"Engineered model R Squared: %0.3f (%i observations)\" % (score_engineered[0], X_engineered.shape[0]))\nprint(\"Final model R Squared: %0.3f (%i observations)\" % (score_final[0], X_final.shape[0]))\n","8e1e4ad5":"score_final[1].plot.scatter(x=\"actual\", y=\"predicted\")","741d63ef":"# COMPARE THE PERFORMANCE BY USING\n# SAME DATA FOR EACH MODEL\nindex_final = final_df.index\n\nX_initial1 = clean_df.loc[index_final, features_initial]\ny_initial1 = clean_df.loc[index_final, target]\n\nX_logged1 = clean_df.loc[index_final, features_initial]\ny_logged1 = np.log1p(clean_df.loc[index_final, target])\n\nX_filtered1 = filtered_df.loc[index_final, features_initial]\ny_filtered1 = np.log1p(filtered_df.loc[index_final, target])\n\nX_engineered1 = model_df.loc[index_final].drop(target, axis=\"columns\")\ny_engineered1 = np.log1p(model_df.loc[index_final, target])\n\nscore_initial1 = fit_and_score(X_initial1, y_initial1)\nscore_logged1 = fit_and_score(X_logged1, y_logged1)\nscore_filtered1 = fit_and_score(X_filtered1, y_filtered1)\nscore_engineered1 = fit_and_score(X_engineered1, y_engineered1)\n\nprint(\"All models with the same %i rows:\\n\" % X_final.shape[0])\nprint(\"Initial model R Squared: %0.3f (old: %0.3f)\" % (score_initial1[0], score_initial[0]))\nprint(\"Logged model R Squared: %0.3f (old: %0.3f)\" % (score_logged1[0], score_logged[0]))\nprint(\"Filtered model R Squared: %0.3f (old: %0.3f)\" % (score_filtered1[0], score_filtered[0]))\nprint(\"Engineered model R Squared: %0.3f (old: %0.3f)\" % (score_engineered1[0], score_engineered[0]))\nprint(\"Final model R Squared: %0.3f\" % (score_final[0]))","a4d04775":"score_engineered1[1].plot.scatter(x=\"actual\", y=\"predicted\")","e6f44c4c":"## Getting there...","bb838851":"# Engineering for AI\n\n## Kaggle Prelims and Function Definition","78163c7a":"## Data Description\n\nFrom the kaggle dataset page:\n\n* maker - normalized all lowercase\n* model - normalized all lowercase\n* mileage - in KM\n* manufacture_year\n* engine_displacement - in ccm\n* engine_power - in kW\n* body_type - almost never present, but I scraped only personal cars, no motorcycles or utility vehicles\n* color_slug - also almost never present\n* stk_year - year of the last emission control\n* transmission - automatic or manual\n* door_count\n* seat_count\n* fuel_type - gasoline, diesel, cng, lpg, electric\n* date_created - when the ad was scraped\n* datelastseen - when the ad was last seen. Our policy was to remove all ads older than 60 days\n* price_eur - list price converted to EUR","ca6381fc":"A few issues:\n\n* Maker, transmission and fuel type are categorical variables\n* Many different car models\n* Some \"unexpected\" years of manufacture, door and seat counts\n* Body type is mostly \"other\" or missing\n* Color Slug and STK year also mostly missing\n* Date created and date last seen both in string (date) format\n\nPossible solutions:\n* Encode these variables\n* Drop car model*\n* Clean the values in these columns\n* Drop Body Type and Color Slug (unlikely to improve model)\n* Convert STK year into a boolean column (exists\/not exists)\n* Convert these into dates, then take the difference","256da56c":"## That's more like it!","0f763c49":"## Make a simple regression model\n\nWe will use a standard linear regression model as the focus is on the features.","14e886da":"\n## Yikes."}}