{"cell_type":{"a81dbe0d":"code","8475173d":"code","595eefc2":"code","1c573c8f":"code","34365c4e":"code","20d65007":"code","7dad308e":"code","0d448ea9":"code","f60a3531":"code","ea8e05c2":"code","99166917":"code","60623355":"code","ad91fce9":"code","26dc7173":"code","0721a096":"code","14dc50f5":"code","0a127864":"code","d056444c":"code","6eb912f4":"code","3b47dd4e":"code","2f29284d":"code","8a8dee81":"code","2dd893ce":"code","827bdebe":"code","8e1ebfd2":"code","7f6fae6c":"code","34f64ab3":"code","6d3fd0e1":"code","888e2e19":"code","c43f4b7b":"code","6985d007":"code","68ad6d6b":"code","01928df3":"code","d0c366df":"code","78139d8d":"code","29d78517":"code","285bf1a3":"code","d513669f":"markdown","b3867f4c":"markdown","5abc72f7":"markdown","c652c7d2":"markdown","f9f73c46":"markdown","dcf765d6":"markdown","3995fcf2":"markdown","8eea318d":"markdown","92d51420":"markdown","86a962f0":"markdown","79c60169":"markdown","95b63ab0":"markdown","988aebf3":"markdown","c8521f27":"markdown","eca52752":"markdown","47be4762":"markdown","ff200206":"markdown","b2698a51":"markdown","47a450a0":"markdown","6f4ef9ba":"markdown","726e6941":"markdown","5a01ec4b":"markdown","09a52e60":"markdown","541c58aa":"markdown","b2a56eb5":"markdown","146e1cf0":"markdown","f7bfc241":"markdown"},"source":{"a81dbe0d":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","8475173d":"data=pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","595eefc2":"data.head(5)","1c573c8f":"data.shape","34365c4e":"data=data.drop([\"id\"], axis=1)","20d65007":"# Here we will replace one column with another and drop the original.\nbmi_median = data.bmi.median()\ndata['bmi_median'] = data.bmi.fillna(bmi_median)","7dad308e":"# Here I'm dropping the original column, it won't work for us anymore.\ndata=data.drop([\"bmi\"], axis=1)","0d448ea9":"data.isnull().sum()","f60a3531":"data[\"gender\"]=data[\"gender\"].map({\"Male\":0 , \"Female\":1 , \"Other\":2})\ndata[\"ever_married\"]=data[\"ever_married\"].map({\"Yes\":1 , \"No\":0 })\ndata[\"work_type\"]=data[\"work_type\"].map({'Private':0, 'Self-employed':1, 'Govt_job':2, 'children':3, 'Never_worked':4 })\ndata[\"smoking_status\"]=data[\"smoking_status\"].map({'formerly smoked':0, 'never smoked':1, 'smokes':2, 'Unknown':3 })\ndata[\"Residence_type\"]=data[\"Residence_type\"].map({'Urban':0, 'Rural':1})","ea8e05c2":"data.head()","99166917":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix","60623355":"# Here I separated the target column from the rest of the columns, that is, an initial separation of the data.\ntarget=data[\"stroke\"]\nfeatures=data.drop([\"stroke\"],axis=1)","ad91fce9":"scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX = scaler.fit_transform(features)","26dc7173":"# Here I complete the data separation\nx_train,x_test_and_val, y_train, y_test_and_val = train_test_split(X,target,test_size=0.25,random_state=0)","0721a096":"print(x_train.shape,x_test_and_val.shape,y_train.shape,y_test_and_val.shape)","14dc50f5":"# This subdivision is to support the neural network for the evaluation process of the network.\nx_val ,x_test ,y_val , y_test= train_test_split(x_test_and_val,y_test_and_val,test_size=0.2,random_state=0)","0a127864":"print(x_val.shape, y_val.shape ,x_test.shape ,y_test.shape )","d056444c":"pip install lazypredict","6eb912f4":"from lazypredict.Supervised import LazyClassifier","3b47dd4e":"model = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = model.fit(x_train, x_test, y_train, y_test)","2f29284d":"print(models)","8a8dee81":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","2dd893ce":"RandomForestClassifierModel=RandomForestClassifier()","827bdebe":"parameters = {\n    \"n_estimators\":[50,70,100,150,200],\n    \"max_depth\":[7,11,13,15,32,None]\n    \n}","8e1ebfd2":"#  I will pass the classifier and parameters and the number of iteration in the GridSearchCV method.\ncv = GridSearchCV(RandomForestClassifierModel,parameters,cv=5)\ncv.fit(x_train, y_train)","7f6fae6c":"#I have defined the method for printing all the iteration done and scores in each iteration.\ndef display(results):\n    print(f'Best parameters are: {results.best_params_}')\n    print(\"\\n\")\n    mean_score = results.cv_results_['mean_test_score']\n    std_score = results.cv_results_['std_test_score']\n    params = results.cv_results_['params']\n    for mean,std,params in zip(mean_score,std_score,params):\n        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')","34f64ab3":"display(cv)","6d3fd0e1":"RandomForestClassifierModel=RandomForestClassifier(n_estimators=70, criterion='gini', max_depth=7,\n                                min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n                                max_features='auto',max_leaf_nodes=7,min_impurity_decrease=0.0,\n                                min_impurity_split=None, bootstrap=True,oob_score=False, n_jobs=-1,\n                                random_state=0, verbose=0,warm_start=True)\nRandomForestClassifierModel.fit(x_train, y_train)","888e2e19":"#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(x_train, y_train))\n# And now we will see the accuracy of the model in the test data.\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(x_test, y_test))\n# This instruction calculates the percentage of importance for each of the features.\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)","c43f4b7b":"#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(x_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(x_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:15])\nprint(\"real values of y_test>>>>>>>>>>>>>>>>>>>>>>>>>>>is : \\n\" ,y_test[:15] )\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:1])","6985d007":"#Calculating Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nconfusion_matrix(y_test,y_pred)","68ad6d6b":"plot_confusion_matrix(RandomForestClassifierModel,x_test,y_test);","01928df3":"# Now we are going to use a neural network for classification\n# Here we will call the libraries that we need.\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # empty neural network\nfrom keras.layers import Dense # layer constitution\nimport keras \nfrom keras.layers import Dropout\nfrom keras import regularizers","d0c366df":"x_train = keras.utils.normalize(x_train, axis=1)","78139d8d":"model= Sequential([\n    Dense(100, activation='relu', input_shape=(10,)),\n    Dropout(0.5),\n    Dense(100, activation='relu'),\n    Dropout(0.5),\n    Dense(50, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n              \nhist = model.fit(x_train, y_train,\n          batch_size=5, epochs=5,\n          validation_data=(x_val, y_val))","29d78517":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","285bf1a3":"plt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","d513669f":"**Now we're going to test the model we built together, let's go.**","b3867f4c":"**It was a fast neural network to extract quick but good results.**","5abc72f7":"**Note We have explained why we chose this statistical function to substitute for null values in the first part of the project, for a quick reminder because of the normal distribution of both columns.**","c652c7d2":"**In the end, the model was wrong in 12 ratings out of 256 ratings, so we have 244 correct and successful ratings, and I would like to send a message to the model that we built and tell him, it\u2019s okay, you did a good job and tomorrow you will do a much better job than this.**","f9f73c46":"**Very excellent. The results we obtained from the test data exceeded the 95% barrier, with a difference of 0.002 percent from the results we obtained from the training data, and this makes us on the safe side, meaning that the model we built does not suffer from the problem of overfitting, and this is very good.**","dcf765d6":"# **Classification of stroke diseases**","3995fcf2":"**I applied some quick algorithms and extracted an accuracy rate of 95% of the evaluation data.\nthank you for your time**","8eea318d":"# **Let's build a deep learning algorithm**","92d51420":"> **In the first step we will do the usual things like reading the data and cleaning the data, and then we will move on to splitting the data and creating the models:**","86a962f0":"**Now let's calculate all the errors in the ratings and show them to our favorite Confusion Matrix.**","79c60169":"**I have also explained what I did to the data conversion process and also in the first part.**","95b63ab0":"**We will now drop some columns and replace some of them and transform the data.**","988aebf3":"# data scaling","c8521f27":"# **Let's start building some machine learning algorithms:**","eca52752":"**Very good The results we got tell us that there are a lot of good algorithms and the accuracy level of these models is great, so we will not use much but will apply some few algorithms just for the sake of proof in order to show what we can do \/ link to it with a little correction.**","47be4762":"# **1 . Random Forest Algorithm**","ff200206":"**In order to treat the column that suffers from data loss, we compensate for those missing data by using the median in the statistics, and then we make a projection of the original column after replacing it with the new column.**","b2698a51":"# The results we obtained.","47a450a0":"> **In the first stage, we conducted a complete and detailed exploration and analysis of stroke disease data, and the results we obtained are very useful information. You can view this kernel through the following link:\nhttps:\/\/www.kaggle.com\/alimohammedbakhiet\/eda-for-stroke-dataset**\n\n> **In the second stage, we will apply machine learning algorithms and neural networks to craft that data and we will work hard to get the highest results in accuracy for the test data.**\n\n> **Let's have fun...**","6f4ef9ba":"**Here we use this beautiful tool in order to test a set of values and filter the best for the data among those values that we will pass to the beautiful algorithm called GridSearchCV**","726e6941":"# **The end**","5a01ec4b":"**Before applying algorithms to the data, let's first look at the efficiency of the algorithms suitable for the data we have**","09a52e60":"# **Here I convert the data.**","541c58aa":"**I will now adjust the higher parameters of the model, which will play a major role in the accuracy of the model that we will reach.**","b2a56eb5":"**To begin with, I will not be interested in building the neural network, I will build a small and uncomplicated neural network that will fulfill the purpose only, and we will not exaggerate the tuning of the network to get higher results because the data we have will be appropriate for it if we use a small neural network.**","146e1cf0":"**Since we have some outliers in our data and I noted them in the first stage, I'm going to use Random Forest because they are good at dealing with outliers.**","f7bfc241":"**In the end, the results we got from that network are very good compared to the little effort we put into building it, which is very little.**\n**and, we made modifications to these simple codes and fixed some problems that these codes were experiencing relatively.**"}}