{"cell_type":{"144bb234":"code","a1ac1ca6":"code","b969c59c":"code","7fa4dfe6":"code","a5a440da":"code","16c73fb6":"code","afaa7a0c":"code","4dbd5a3e":"code","1bd8d2da":"code","d4883419":"code","52ed64e0":"code","5d5397d6":"code","627e4cea":"code","b67e5147":"code","48367e5d":"code","6ea2b46f":"code","6bc00b98":"code","399c7a69":"code","d35c7448":"code","5e3bcb84":"code","7cd4019d":"code","041f6224":"code","4d6af14d":"code","ae93e49f":"code","fc1c4886":"code","06330756":"code","fa6da415":"code","439e5333":"code","0fd60a9c":"code","eab4c937":"code","2d7ba81c":"code","efd89711":"code","d3178f79":"code","035a2ea5":"code","a2b4de89":"markdown","b9341c38":"markdown","a188158a":"markdown","786832c8":"markdown","527fe859":"markdown","74db2b46":"markdown","8d68e093":"markdown","225710f9":"markdown","aab4055c":"markdown","12298724":"markdown","f7634d0a":"markdown","f358fa11":"markdown","5f3834c5":"markdown","399f67fa":"markdown","1d23a2d1":"markdown","d8adb690":"markdown","1c9107b8":"markdown","a9a7bf60":"markdown","2861f58c":"markdown","0f3bb6d2":"markdown","ebb88fb4":"markdown","88a715f0":"markdown","3ae94c6b":"markdown","8b8f9d89":"markdown","e9d3cf14":"markdown","388f1acc":"markdown"},"source":{"144bb234":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1ac1ca6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","b969c59c":"df = pd.read_csv(\"..\/input\/cyberbullying-classification\/cyberbullying_tweets.csv\")\ndf","7fa4dfe6":"df.info()","a5a440da":"print(df[\"cyberbullying_type\"].value_counts())\nplt.figure(figsize=[8, 10])\ndf[\"cyberbullying_type\"].value_counts().plot(kind='pie', autopct='%1.0f%%')","16c73fb6":"stop_words = set(stopwords.words('english'))\ndef preprocess_text(tweet):\n    # Remove links and mentions\n    processed_tweet = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", tweet)\n    # Remove non utf8\/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    processed_tweet = re.sub(r'[^\\x00-\\x7f]',r'', processed_tweet)\n    # Remove punctuations and numbers\n    processed_tweet = re.sub('[^a-zA-Z]', ' ', processed_tweet)\n    # Single character removal\n    processed_tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', processed_tweet)\n    # Removing multiple spaces\n    processed_tweet = re.sub(r'\\s+', ' ', processed_tweet)\n    # Tokenize the sentences and remove the stop words\n    tokens = []\n    for word in processed_tweet.split():\n        if word.lower() not in stop_words:\n            tokens.append(word)\n    # Join the remaining words back\n    processed_tweet = ' '.join(tokens)\n\n    return processed_tweet","afaa7a0c":"x = []\ntweets = list(df[\"tweet_text\"])\nfor tweet in tweets:\n    x.append(preprocess_text(tweet))\ndf['processed_tweet'] = x\ndf.head()","4dbd5a3e":"df.shape","1bd8d2da":"df['processed_tweet'].duplicated().sum()","d4883419":"df.drop_duplicates('processed_tweet', inplace=True)\ndf.shape","52ed64e0":"x = np.array(df['processed_tweet'])\nx","5d5397d6":"y = df[\"cyberbullying_type\"]\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\ny = to_categorical(y)\ny","627e4cea":"len(y[1]) == len(df[\"cyberbullying_type\"].unique())","b67e5147":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\nx_train","48367e5d":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_train)\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","6ea2b46f":"x_train[1]","6bc00b98":"maxlen = 0\nfor s in x_train + x_test:\n    if len(s) > maxlen:\n        maxlen = len(s)\nmaxlen        ","399c7a69":"vocab_size = len(tokenizer.word_index) + 1\nmaxlen = 300\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)","d35c7448":"x_train[1]","5e3bcb84":"print(x_train.shape)\nprint(y_train.shape)","7cd4019d":"vocab_size","041f6224":"embedding_dict = {}\nwith open(\"..\/input\/glovetwitter27b-in-word2vec-format\/glove.twitter.27B.50d_wv.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vector","4d6af14d":"len(embedding_dict[\"hello\"])","ae93e49f":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","fc1c4886":"embedding_matrix.shape","06330756":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements \/ vocab_size","fa6da415":"from keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Input\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","439e5333":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=maxlen, weights=[embedding_matrix], trainable=True))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(6, activation='sigmoid'))","0fd60a9c":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.summary()","eab4c937":"history = model.fit(x_train, y_train, epochs=5, verbose=True, validation_data=(x_test, y_test), batch_size=10)","2d7ba81c":"score2 = model.evaluate(x_test, y_test, verbose=1)\n\nprint(\"Test Score:\", score2[0])\nprint(\"Test Accuracy:\", score2[1])","efd89711":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","d3178f79":"y_pred = model.predict(x_test)\ny_pred.shape","035a2ea5":"c = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\ndisp = ConfusionMatrixDisplay(confusion_matrix=c, display_labels=df[\"cyberbullying_type\"].unique())\ndisp.plot()","a2b4de89":"* After we process all the tweets we add them to a new column in the data frame ('processed_tweet')","b9341c38":"* Verify if there are duplicates in the processed tweets","a188158a":"* First we print the value counts to find out if the classes are balanced.\n* Then we plot the value counts in a pie chart using %","786832c8":"* We split the data into test and train using a test size of 20%","527fe859":"* Check if the dictionary is loaded corectly\n* Check if the length of the embedding list is 50","74db2b46":"# Embedding ","8d68e093":"* Check how many of the embeded lists are nonzero","225710f9":"* We find out the length of the longest tweet","aab4055c":"* We verify that the encoding was done propperly","12298724":"* Load the embedding dictionary from glove.\n* The dictionary will contain words as keys and their embedding list as values","f7634d0a":"* We define a function that:\n1. Removes links and mentions from the tweet\n2. Removes unwanted characters\n3. Removes all punctuation and numbers\n4. Removes all single characters\n5. Removes multiple spaces\n6. Removes stop words\n* This is to reduce the unimportant information to a minimum","f358fa11":"* We create a np array for the data in the target column ('cyberbullying_type')\n* We use the label encoder to transform the 6 different labels into numbers\n* We use the to_categorical function to transform the numbers into lists of length 6","5f3834c5":"* We use The Tokenizer class to create a word-to-index dictionary.\n* In the dictionary the keys are all the different words from the tweets and the values are unique indexes.\n* Then every tweet is transformed into a list that contains the indexes coresponding to every word in the tweet.","399f67fa":"* Check the shape of the embedded matrix = (vocab_size, 50)","1d23a2d1":"First model using GlobalMaxPool1D()\n# First Model Using GlobalMaxPool1D","d8adb690":"* We create a np array for the data in the 'processed_tweets' column in the database","1c9107b8":"# Read data","a9a7bf60":"* Check input and output shapes","2861f58c":"* This means that 81% of our word-to-index dictionary is covered by the pretrained model.","0f3bb6d2":"# Imports","ebb88fb4":"# Preprocessing","88a715f0":"* Since there are 2868 duplicates we need to remove them","3ae94c6b":"* We know that each tweet has a different length. The next step is to make all the inputs have the same size. To accomplish this we will add padding to each tweet so in the end every tweet has the same length.\n* (if the length of the sentence is smaller than the set size then the function will add 0 to the end so it reaches the set size. If the length is bigger the sentence will be truncated to the set size.) We will choose the set size to be bigger than the longest tweet beacuse we dont want to split tweets.\n* Also we will save the size of the vocabulary for later\n","8b8f9d89":"* Create the embedding matrix\n* The embedding matrix has the number of rows equal to the vocabulary size and the number of columns equal to the length of the embedding list\n* Each row of the matrix will correspond to the index of the word in the word-to-index dictionary and the columns will corespond to the embedding list of the word from the embedding_dictionary","e9d3cf14":"# Analyze the data","388f1acc":"# Import Keras"}}