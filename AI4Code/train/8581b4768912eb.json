{"cell_type":{"39442b1b":"code","5a39cb2e":"code","47e8263a":"code","102cb05e":"code","2abc0e70":"code","ee5d98ec":"code","424e3a21":"code","0778729e":"code","5af6d6ff":"code","2daf4aee":"code","20f2e727":"code","5f27a11a":"code","dcaffdba":"code","edb963a6":"code","866099f5":"code","ae802cf2":"code","38901ef1":"code","4888c620":"code","164888f5":"code","247aa300":"code","9a58cc59":"code","e2a7e4d8":"code","0ed73a50":"code","137410b3":"code","cb30e0a9":"code","d752651f":"code","97425c2f":"code","49132024":"code","6b70a51b":"code","b6264660":"code","37bd259a":"code","65d995be":"code","599af0c7":"code","277a46ba":"code","d94d8233":"code","abc5d8d4":"code","8743d35b":"code","95a2c9ec":"code","524836c1":"code","c69c8f77":"code","b98d660b":"code","5fbfa6cf":"code","f4e0dab1":"code","27e53cef":"code","acf59c95":"code","12f7bb41":"code","d84b2efc":"code","08d305ac":"code","cbbf7d04":"code","cdb19c1a":"code","774455cd":"code","b3374005":"code","ae72e718":"markdown","85a773b7":"markdown","8f72d075":"markdown","83f699f5":"markdown","e456da92":"markdown","b105c727":"markdown","31515a14":"markdown"},"source":{"39442b1b":"# Install to have latest seaborn version to use histplot\n!pip install seaborn==0.11.0","5a39cb2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords,wordnet\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47e8263a":"#Lemmatization\nnltk.download('wordnet')","102cb05e":"def load_data(path,file):\n    data = pd.read_csv(path+file)\n    return data","2abc0e70":"train_path = '\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/'\ntrain_file = 'Train.csv'\ntrain_data = load_data(train_path,train_file)\n\nvalid_path = '\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/'\nvalid_file = 'Valid.csv'\nvalid_data = load_data(valid_path,valid_file)\n\ntest_path = '\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/'\ntest_file = 'Test.csv'\ntest_data = load_data(test_path,test_file)","ee5d98ec":"train_data.head(10)","424e3a21":"# Statistics and details\nprint('Columns:',train_data.columns)\nprint('Shape:', train_data.shape)\nprint('Stats:',train_data.describe(include='object'))\nprint('Class Distribution:',train_data['label'].value_counts())\nprint('Info:',train_data.info())","0778729e":"#Class distribution\nplt.figure(figsize=(12,6))\nsns.countplot(x='label',data=train_data)","5af6d6ff":"#Get word count\ntrain_data['word_count']=train_data['text'].str.lower().str.len()\ntrain_data","2daf4aee":"plt.figure(figsize=(12,6))\nsns.kdeplot(train_data['word_count'],shade=True,color='r').set_title('Kernel Distribution of Number of words')","20f2e727":"#sns.histplot(train_data['word_count'],color='r')\nprint(sns.__version__)\nsns.histplot(data=train_data, x=\"word_count\").set_title('Word Count Distribution')","5f27a11a":"positive_wordcnt =  train_data['word_count'][train_data['label']==0]\npos_plot = sns.kdeplot(positive_wordcnt,color='b',shade=True)\nnegative_wordcnt = train_data['word_count'][train_data['label']==1]\nneg_plot = sns.kdeplot(negative_wordcnt,color='r',shade=True)","dcaffdba":"print(string.punctuation)","edb963a6":"# Remove stopwords\ndef preprocess_text(data):\n    stop = stopwords.words('english')\n    punct = '''!\"#$%&'()*+,-\/:;<=>?@[\\]^_`{|}~'''\n    #print(stop)\n    #Make lower\n    data['text'] = data['text'].str.lower()\n    #Remove stopwords\n    data['text'] = data['text'].apply(lambda x:' '.join([words for words in x.split() if words not in stop]))\n    #Remove punctuations\n    data['text'] = data['text'].str.translate(str.maketrans('', '', punct))\n    data['word_count'] = data['text'].str.split().str.len()\n    return data\ntrain_data = preprocess_text(train_data)\ntrain_data.head()","866099f5":"positive_wordcnt =  train_data['word_count'][train_data['label']==0]\npos_plot = sns.kdeplot(positive_wordcnt,color='b',shade=True)\nnegative_wordcnt = train_data['word_count'][train_data['label']==1]\nneg_plot = sns.kdeplot(negative_wordcnt,color='r',shade=True)","ae802cf2":"def pos_tag(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag,wordnet.NOUN)","38901ef1":"print(train_data.head())\nprint(train_data.shape)","4888c620":"def lemmatize(data):\n    lemmatizer=WordNetLemmatizer()\n    data['text'] = data['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w,pos_tag(w)) for w in word_tokenize(x)]))\n    return data","164888f5":"## Normalization;Lemmatization; No change as no pos_tag is there;hence add pos_tag\ntrain_data = lemmatize(train_data)\ntrain_data","247aa300":"max_count = max(train_data['word_count'])\nprint(max_count)","9a58cc59":"# view one lemmatized record\ntrain_data['text'][0]","e2a7e4d8":"## Plotting word cloud\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(40,30))\n    plt.imshow(wordcloud)","0ed73a50":"#get postive and negative sentiment records for wordcloud\npositive = train_data['text'][train_data['label']==1]\nnegative = train_data['text'][train_data['label']==0]\nstop = set(stopwords.words('english'))\nstop.update([\"br\", \"href\",\"film\",\"movie\",\"one\"])\nprint(stop)\n#negative\n## Wordcloud\npos_wordcloud = WordCloud(stopwords=stop).generate(' '.join(positive))\nneg_wordcloud = WordCloud(stopwords=stop).generate(' '.join(negative))\n# pos_wordcloud = WordCloud(stopwords=stop,width=800,height=800,min_font_size=10).generate(' '.join(positive))\n# neg_wordcloud = WordCloud(stopwords=stop,width=800,height=800,min_font_size=10).generate(' '.join(negative))\n","137410b3":"plot_cloud(pos_wordcloud)","cb30e0a9":"plot_cloud(neg_wordcloud)","d752651f":"#Remove word_count column\ntrain_data = train_data[['text','label']]\ntrain_data","97425c2f":"valid_data = preprocess_text(valid_data)\nvalid_data = lemmatize(valid_data)\n\ntest_data = preprocess_text(test_data)\ntest_data = lemmatize(test_data)\ntest_data","49132024":"x_valid_data = valid_data[['text']]\ny_valid_data = valid_data[['label']]\n\nx_test_data = test_data[['text']]\ny_test_data = test_data[['label']]","6b70a51b":"# Word technique\n#Create feature vectors using Bag of Words-TfIdf\ntfidf_converter = TfidfVectorizer(max_features=1000,min_df=5,max_df=0.7)\nx = tfidf_converter.fit_transform(train_data['text']).toarray()\ny = train_data['label']","b6264660":"#bi-grams\n# Word technique\n#Create feature vectors using Bag of Words-TfIdf\nntfidf_converter = TfidfVectorizer(max_features=1000,min_df=5,max_df=0.7,ngram_range=(2,2))\nnx = ntfidf_converter.fit_transform(train_data['text']).toarray()\nny = train_data['label']","37bd259a":"print(x.shape)\nprint(y)","65d995be":"y_valid=np.array(y_valid_data).reshape(-1,1)\nprint(y_valid)","599af0c7":"# Choose model\n#Naive Bayes Algorithm p(sent|word) = p(sent)p(word|sent)\/p(word)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\ngnb = GaussianNB()\ngnb.fit(x,y)\n\n\nngram_gnb = GaussianNB()\nngram_gnb.fit(nx,ny)\n","277a46ba":"#Save model\nimport pickle\ngnb_model = pickle.dumps(gnb)\n\nngram_gnb_model = pickle.dumps(ngram_gnb)","d94d8233":"# Use tfidf to transform test and valid data\nx_val_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_tst_data = tfidf_converter.transform(x_test_data['text']).toarray()","abc5d8d4":"#ngram\n# Use tfidf to transform test and valid data\nnx_valid_data = ntfidf_converter.transform(x_valid_data['text']).toarray()\nnx_test_data = ntfidf_converter.transform(x_test_data['text']).toarray()","8743d35b":"#Load the model\ngnb_model = pickle.loads(gnb_model)\n\nngram_gnb_model = pickle.loads(ngram_gnb_model)","95a2c9ec":"# Predict valid\nypred_valid = gnb_model.predict(x_val_data)\n#print(ypred_valid)\nprint('Valid Accuracy:',accuracy_score(y_valid_data,ypred_valid))\n\n#Predict Test\nypred_test = gnb_model.predict(x_tst_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,ypred_test))\nprint(confusion_matrix(y_test_data,ypred_test))\nprint('Classification Report:',classification_report(y_test_data,ypred_test))","524836c1":"#ngram\n# Predict valid\nnypred_valid = ngram_gnb_model.predict(nx_valid_data)\n#print(ypred_valid)\nprint('Valid Accuracy:',accuracy_score(y_valid_data,nypred_valid))\n\n#Predict Test\nnypred_test = ngram_gnb_model.predict(nx_test_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,nypred_test))\nprint(confusion_matrix(y_test_data,ypred_test))\nprint('Classification Report:',classification_report(y_test_data,nypred_test))","c69c8f77":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = tfidf_converter.transform(text).toarray()\nprint(gnb_model.predict(text))","b98d660b":"#Random text prediction\ntext = ['that is not bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = ntfidf_converter.transform(text).toarray()\nprint(ngram_gnb_model.predict(text))","5fbfa6cf":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(max_depth=4, random_state=0)\nrf.fit(x,y)","f4e0dab1":"rf_model=pickle.dumps(rf)\n# Use tfidf to transform test and valid data\nx_rf_valid_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_rf_test_data = tfidf_converter.transform(x_test_data['text']).toarray()\n","27e53cef":"#Load the model\nrf_model = pickle.loads(rf_model)","acf59c95":"# Predict valid\nypred_valid = rf_model.predict(x_rf_valid_data)\n#print(ypred_valid)\nprint('RF Valid Accuracy:',accuracy_score(y_valid_data,ypred_valid))\n\n#Predict Test\nypred_test = rf_model.predict(x_rf_test_data)\nprint('RF Test Accuracy:',accuracy_score(y_test_data,ypred_test))","12f7bb41":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really diid well']\ntext = tfidf_converter.transform(text).toarray()\nprint(rf_model.predict(text))","d84b2efc":"#Boosting\nimport sklearn\nprint(sklearn.__version__)\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nada.fit(x,y)","08d305ac":"# Use tfidf to transform test and valid data\nx_val_data = tfidf_converter.transform(x_valid_data['text']).toarray()\nx_tst_data = tfidf_converter.transform(x_test_data['text']).toarray()","cbbf7d04":"#Save model\nada_model = pickle.dumps(ada)\n","cdb19c1a":"#Predict valid and test data\nada_model = pickle.loads(ada_model)\ny_ada_val_pred = ada_model.predict(x_val_data)\nprint(\"Valid data Accuracy is :\",accuracy_score(y_valid_data,y_ada_val_pred))\n\n#Predict Test\ny_ada_tst_pred = ada_model.predict(x_tst_data)\nprint('Test Accuracy:',accuracy_score(y_test_data,y_ada_tst_pred))\nprint(confusion_matrix(y_test_data,y_ada_tst_pred))\nprint('Classification Report:',classification_report(y_test_data,y_ada_tst_pred))","774455cd":"#Random Text Prediction\ntext = ['not good','worth watching','bad','grew up listening to this.awful']\ntext = tfidf_converter.transform(text).toarray()\nprint(ada_model.predict(text))","b3374005":"#Random text prediction\ntext = ['that is too bad.but i can assure it can be made better','not sure if the climax really did well','as expected']\ntext = tfidf_converter.transform(text).toarray()\nprint(ada_model.predict(text))","ae72e718":"## 1) Load Data","85a773b7":"### 3) Do pre-processing for test and valid dataset","8f72d075":"### 3) Building model","83f699f5":"## 2) Data Exploration","e456da92":"## Boosting","b105c727":"## Random Forest","31515a14":"### Functions Defined"}}