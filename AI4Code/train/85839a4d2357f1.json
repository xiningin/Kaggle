{"cell_type":{"0434a768":"code","7d8f1ca7":"code","18640f0e":"code","facfd308":"code","15f68231":"code","70e468c5":"code","045ac1b1":"code","43c78f3c":"code","3741d650":"code","5dc362c5":"code","c185e575":"code","07de0a06":"code","38a623b7":"code","665b38f9":"code","203f3e93":"code","9543be5f":"code","76448380":"code","b92adca7":"code","57d37538":"code","c09b4396":"code","e7770108":"code","d29b77ec":"code","da1ad3e8":"code","ea90f9eb":"code","85309754":"code","f75ccdd8":"code","01fe1013":"code","087fda04":"code","83e579d3":"code","709a3351":"code","2cf409b4":"code","e585e8b6":"code","9ee1589c":"code","78c5db08":"code","ddbebb57":"code","a5e1600e":"code","d5bc8f67":"code","1e944115":"code","1865bf3d":"code","77c5f5f4":"code","e4c6b559":"code","9815f46c":"code","410e9e6d":"code","a4c14009":"code","e43e4565":"code","ad756171":"code","f94b7739":"code","84fb5f98":"code","619bc1c3":"code","ceeb0e13":"code","fc0b596a":"code","b13321b4":"code","b7f6352c":"code","38a45338":"code","3c3c999c":"code","d67101be":"code","cd30ed2c":"code","01015fd9":"code","260cde21":"code","0ed504ac":"code","11dc36b2":"code","21498bea":"code","7cd2fab3":"code","b67b5fd7":"code","7876ee19":"code","a61f67c9":"code","9af28d9d":"code","630fee0b":"code","74dd17e4":"code","5fabd125":"code","e2500733":"code","3e64738d":"code","03b2e4ef":"code","dff4f09f":"code","d8c9ed04":"code","038b45db":"code","962b8c61":"code","e8a353f0":"code","f8f2c203":"code","4f29b4f9":"code","9cd54ad7":"code","9352393b":"code","d934a716":"code","2ea421fe":"code","c570056a":"code","1ace8513":"code","2ccd8830":"code","f34e31f6":"code","d1c07110":"code","ff7c3780":"code","e3f1e6c8":"code","9de20fe0":"code","4e47a326":"code","b4ef9529":"markdown","f76f3566":"markdown","602e943c":"markdown","8b55a75e":"markdown","ff6bdbc4":"markdown","ec4db6f9":"markdown","41ae56af":"markdown","976d1ce1":"markdown","80ef3ce0":"markdown","c2767194":"markdown","28355f5c":"markdown","438709e7":"markdown","199d606f":"markdown","a126050f":"markdown","e4c2c3fc":"markdown","74a8ed70":"markdown","44c8545f":"markdown","7da9e0e2":"markdown","3c397e3f":"markdown","aebb3ee3":"markdown","09f0d3cc":"markdown","a058576e":"markdown","930cc245":"markdown","0cdcfed0":"markdown","bca1fb10":"markdown","25bc39a5":"markdown","993a2242":"markdown","f1dae881":"markdown","795f9dee":"markdown","89f2e688":"markdown","bd0450c8":"markdown","1d718303":"markdown","882fc412":"markdown","0e0bc196":"markdown","03160620":"markdown","4fdfcc6c":"markdown","2e61a9b2":"markdown","e1e9fb94":"markdown","c58b7f71":"markdown","f85ba0ff":"markdown","83476830":"markdown","e51fe428":"markdown","1a83c856":"markdown","d8c09ad9":"markdown","0be4b912":"markdown","73487232":"markdown","c1d86ad3":"markdown","f589f988":"markdown","bf749a3b":"markdown","031a5295":"markdown","6ffb4e3a":"markdown","10105c70":"markdown","792c0a38":"markdown","757bf6a7":"markdown","76f8f687":"markdown","5b362c9e":"markdown"},"source":{"0434a768":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport re\nfrom functools import reduce\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file\n\n# init_notebook_mode(connected = True)\n# color = sns.color_palette(\"Set2\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7d8f1ca7":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","18640f0e":"train_df.head()","facfd308":"train_df.target.value_counts()","15f68231":"sample_size = 3271 \n\n# Rebalancing the training set\ntrain_rebal = train_df[train_df.target == 1].sample(sample_size).append(train_df[train_df.target == 0].sample(sample_size)).reset_index()","70e468c5":"def remove_stopwords(words):\n    \"\"\"\n    Function to remove stopwords from the text\n    \"\"\"\n    stop_words = set(stopwords.words(\"english\"))\n    return [word for word in words if word not in stop_words]\n\ndef remove_punctuation(text):\n    \"\"\"\n    Function to remove punctuation from the text\n    \"\"\"\n    return re.sub(r'[^\\w\\s]', '', text)\n\ndef lemmatize_text(words):\n    \"\"\"\n    Function to lemmatize the text\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef stem_text(words):\n    \"\"\"\n    Function to stem th question text\n    \"\"\"\n    ps = PorterStemmer()\n    return [ps.stem(word) for word in words]","045ac1b1":"puncts=['\u2639', '\u0179', '\u017b', '\u1f30', '\u1f75', '\u0160', '\uff1e', '\u03be','\u0e09', '\u0e31', '\u0e19', '\u0e08', '\u0e30', '\u0e17', '\u0e33', '\u0e43', '\u0e2b', '\u0e49', '\u0e14', '\u0e35', '\u0e48', '\u0e2a', '\u0e38', '\u03a0', '\u092a', '\u090a', '\u00d6', '\u062e', '\u0628', '\u0b9c', '\u0bcb', '\u0b9f', '\u300c', '\u1ebd', '\u00bd', '\u25b3', '\u00c9', '\u0137', '\u00ef', '\u00bf', '\u0142', '\ubd81', '\ud55c', '\u00bc', '\u2206', '\u2265', '\u21d2', '\u00ac', '\u2228', '\u010d', '\u0161', '\u222b', '\u1e25', '\u0101', '\u012b', '\u00d1', '\u00e0', '\u25be', '\u03a9', '\uff3e', '\u00fd', '\u00b5', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '\/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '\u201c', '\u201d', '\u2019', '\u00e9', '\u00e1', '\u2032', '\u2026', '\u027e', '\u0303', '\u0256', '\u00f6', '\u2013', '\u2018', '\u090b', '\u0960', '\u090c', '\u0961', '\u00f2', '\u00e8', '\u00f9', '\u00e2', '\u011f', '\u092e', '\u093f', '\u0932', '\u0917', '\u0908', '\u0915', '\u0947', '\u091c', '\u094b', '\u0920', '\u0902', '\u0921', '\u017d', '\u017e', '\u00f3', '\u00ae', '\u00ea', '\u1ea1', '\u1ec7', '\u00b0', '\u0635', '\u0648', '\u0631', '\u00fc', '\u00b2', '\u20b9', '\u00fa', '\u221a', '\u03b1', '\u2192', '\u016b', '\u2014', '\u00a3', '\u00e4', '\ufe0f', '\u00f8', '\u00b4', '\u00d7', '\u00ed', '\u014d', '\u03c0', '\u00f7', '\u02bf', '\u20ac', '\u00f1', '\u00e7', '\u3078', '\u306e', '\u3068', '\u3082', '\u2191', '\u221e', '\u02bb', '\u2105''\u03b9', '\u2022', '\u00ec', '\u2212', '\u043b', '\u044f', '\u0434', '\u0644', '\u0643', '\u0645', '\u0642', '\u0627', '\u2208', '\u2229', '\u2286', '\u00e3', '\u0905', '\u0928', '\u0941', '\u0938', '\u094d', '\u0935', '\u093e', '\u0930', '\u0924', '\u00a7', '\u2103', '\u03b8', '\u00b1', '\u2264', '\u0909', '\u0926', '\u092f', '\u092c', '\u091f', '\u0361', '\u035c', '\u0296', '\u2074', '\u2122', '\u0107', '\u00f4', '\u0441', '\u043f', '\u0438', '\u0431', '\u043e', '\u0433', '\u2260', '\u2202', '\u0906', '\u0939', '\u092d', '\u0940', '\u00b3', '\u091a', '...', '\u231a', '\u27e8', '\u27e9', '\u2216', '\u02c2', '\u207f', '\u2154', '\u0c28', '\u0c40', '\u0c15', '\u0c46', '\u0c02', '\u0c26', '\u0c41', '\u0c3e', '\u0c17', '\u0c30', '\u0c3f', '\u0c1a', '\u09b0', '\u09dc', '\u09dd', '\u0ab8', '\u0a82', '\u0a98', '\u0ab0', '\u0abe', '\u0a9c', '\u0acd', '\u0aaf', '\u03b5', '\u03bd', '\u03c4', '\u03c3', '\u015f', '\u015b', '\u0633', '\u062a', '\u0637', '\u064a', '\u0639', '\u0629', '\u062f', '\u00c5', '\u263a', '\u2107', '\u2764', '\u2668', '\u270c', '\ufb01', '\u3066', '\u201e', '\u0100', '\u178f', '\u17be', '\u1794', '\u1784', '\u17d2', '\u17a2', '\u17bc', '\u1793', '\u1798', '\u17b6', '\u1792', '\u1799', '\u179c', '\u17b8', '\u1781', '\u179b', '\u17c7', '\u178a', '\u179a', '\u1780', '\u1783', '\u1789', '\u17af', '\u179f', '\u17c6', '\u1796', '\u17b7', '\u17c3', '\u1791', '\u1782', '\u00a2', '\u3064', '\u3084', '\u0e04', '\u0e13', '\u0e01', '\u0e25', '\u0e07', '\u0e2d', '\u0e44', '\u0e23', '\u012f', '\u06cc', '\u044e', '\u028c', '\u028a', '\u05d9', '\u05d4', '\u05d5', '\u05d3', '\u05ea', '\u1820', '\u1873', '\u1830', '\u1828', '\u1864', '\u1860', '\u1875', '\u1e6d', '\u1ebf', '\u0927', '\u095c', '\u00df', '\u00b8', '\u0447',  '\u1ec5', '\u1ed9', '\u092b', '\u03bc', '\u29fc', '\u29fd', '\u09ae', '\u09b9', '\u09be', '\u09ac', '\u09bf', '\u09b6', '\u09cd', '\u09aa', '\u09a4', '\u09a8', '\u09df', '\u09b8', '\u099a', '\u099b', '\u09c7', '\u09b7', '\u09af', '\u09bc', '\u099f', '\u0989', '\u09a5', '\u0995', '\u1fe5', '\u03b6', '\u1f64', '\u00dc', '\u0394', '\ub0b4', '\uc81c', '\u0283', '\u0278', '\u1ee3', '\u013a', '\u00ba', '\u0937', '\u266d', '\u093c', '\u2705', '\u2713', '\u011b', '\u2218', '\u00a8', '\u2033', '\u0130', '\u20d7', '\u0302', '\u00e6', '\u0254', '\u2211', '\u00be', '\u042f', '\u0445', '\u041e', '\u0437', '\u0641', '\u0646', '\u1e35', '\u010c', '\u041f', '\u044c', '\u0412', '\u03a6', '\u1ef5', '\u0266', '\u028f', '\u0268', '\u025b', '\u0280', '\u010b', '\u0585', '\u028d', '\u057c', '\u0584', '\u028b', '\u5170', '\u03f5', '\u03b4', '\u013d', '\u0252', '\u00ee', '\u1f08', '\u03c7', '\u1fc6', '\u03cd', '\u12a4', '\u120d', '\u122e', '\u12a2', '\u12e8', '\u129d', '\u1295', '\u12a0', '\u1201', '\u2245', '\u03d5', '\u2011', '\u1ea3', '\ufffc', '\u05bf', '\u304b', '\u304f', '\u308c', '\u0151', '\uff0d', '\u0219', '\u05df', '\u0393', '\u222a', '\u03c6', '\u03c8', '\u22a8', '\u03b2', '\u2220', '\u00d3', '\u00ab', '\u00bb', '\u00cd', '\u0b95', '\u0bb5', '\u0bbe', '\u0bae', '\u2248', '\u2070', '\u2077', '\u1ea5', '\u0169', '\ub208', '\uce58', '\u1ee5', '\u00e5', '\u060c', '\uff1d', '\uff08', '\uff09', '\u0259', '\u0a28', '\u0a3e', '\u0a2e', '\u0a41', '\ufe20', '\ufe21', '\u0251', '\u02d0', '\u03bb', '\u2227', '\u2200', '\u014c', '\u315c', '\u039f', '\u03c2', '\u03bf', '\u03b7', '\u03a3', '\u0923']\nodd_chars=[ '\u5927','\u80fd', '\u5316', '\u751f', '\u6c34', '\u8c37', '\u7cbe', '\u5fae', '\u30eb', '\u30fc', '\u30b8', '\u30e5', '\u652f', '\u90a3', '\u00b9', '\u30de', '\u30ea', '\u4ef2', '\u76f4', '\u308a', '\u3057', '\u305f', '\u4e3b', '\u5e2d', '\u8840', '\u2153', '\u6f22', '\u9aea', '\u91d1', '\u8336', '\u8a13', '\u8aad', '\u9ed2', '\u0159', '\u3042', '\u308f', '\u308b', '\u80e1', '\u5357', '\uc218', '\ub2a5', '\u5e7f', '\u7535', '\u603b', '\u03af', '\uc11c', '\ub85c', '\uac00', '\ub97c', '\ud589', '\ubcf5', '\ud558', '\uac8c', '\uae30', '\u4e61', '\u6545', '\u723e', '\u6c5d', '\u8a00', '\u5f97', '\u7406', '\u8ba9', '\u9a82', '\u91ce', '\u6bd4', '\u3073', '\u592a', '\u5f8c', '\u5bae', '\u7504', '\u5b1b', '\u50b3', '\u505a', '\u83ab', '\u4f60', '\u9171', '\u7d2b', '\u7532', '\u9aa8', '\u9673', '\u5b97', '\u9648', '\u4ec0', '\u4e48', '\u8bf4', '\u4f0a', '\u85e4', '\u9577', '\ufdfa', '\u50d5', '\u3060', '\u3051', '\u304c', '\u8857', '\u25e6', '\u706b', '\u56e2', '\u8868',  '\u770b', '\u4ed6', '\u987a', '\u773c', '\u4e2d', '\u83ef', '\u6c11', '\u570b', '\u8a31', '\u81ea', '\u6771', '\u513f', '\u81e3', '\u60f6', '\u6050', '\u3063', '\u6728', '\u30db', '\u062c', '\u6559', '\u5b98', '\uad6d', '\uace0', '\ub4f1', '\ud559', '\uad50', '\ub294', '\uba87', '\uc2dc', '\uac04', '\uc5c5', '\ub2c8', '\u672c', '\u8a9e', '\u4e0a', '\u624b', '\u3067', '\u306d', '\u53f0', '\u6e7e', '\u6700', '\u7f8e', '\u98ce', '\u666f', '\u00ce', '\u2261', '\u768e', '\u6ee2', '\u6768', '\u221b', '\u7c21', '\u8a0a', '\u77ed', '\u9001', '\u767c', '\u304a', '\u65e9', '\u3046', '\u671d', '\u0634', '\u0647', '\u996d', '\u4e71', '\u5403', '\u8bdd', '\u8bb2', '\u7537', '\u5973', '\u6388', '\u53d7', '\u4eb2', '\u597d', '\u5fc3', '\u6ca1', '\u62a5', '\u653b', '\u514b', '\u79ae', '\u5100', '\u7d71', '\u5df2', '\u7d93', '\u5931', '\u5b58', '\u0668', '\u516b', '\u201b', '\u5b57', '\uff1a', '\u522b', '\u9ad8', '\u5174', '\u8fd8', '\u51e0', '\u4e2a', '\u6761', '\u4ef6', '\u5462', '\u89c0', '\u300a', '\u300b', '\u8a18', '\u5b8b', '\u695a', '\u745c', '\u5b6b', '\u701b', '\u679a', '\u65e0', '\u6311', '\u5254', '\u8056', '\u90e8', '\u982d', '\u5408', '\u7d04', '\u03c1', '\u6cb9', '\u817b', '\u908b', '\u9062', '\u064c', '\u00c4', '\u5c04', '\u7c4d', '\u8d2f', '\u8001', '\u5e38', '\u8c08', '\u65cf', '\u4f1f', '\u590d', '\u5e73', '\u5929', '\u4e0b', '\u60a0', '\u5835', '\u963b', '\u611b', '\u8fc7', '\u4f1a', '\u4fc4', '\u7f57', '\u65af', '\u8339', '\u897f', '\u4e9a', '\uc2f1', '\uad00', '\uc5c6', '\uc5b4', '\ub098', '\uc774', '\ud0a4', '\u5922', '\u5f69', '\u86cb', '\u9c39', '\u7bc0', '\u72d0', '\u72f8', '\u9cf3', '\u51f0', '\u9732', '\u738b', '\u6653', '\u83f2', '\u604b', '\u306b', '\u843d', '\u3061', '\u3089', '\u3088', '\u60b2', '\u53cd', '\u6e05', '\u5fa9', '\u660e', '\u8089', '\u5e0c', '\u671b', '\u6c92', '\u516c', '\u75c5', '\u914d', '\u4fe1', '\u958b', '\u59cb', '\u65e5', '\u5546', '\u54c1', '\u767a', '\u58f2', '\u5206', '\u5b50', '\u521b', '\u610f', '\u68a6', '\u5de5', '\u574a', '\u06a9', '\u067e', '\u06a4', '\u862d', '\u82b1', '\u7fa1', '\u6155', '\u548c', '\u5ac9', '\u5992', '\u662f', '\u6837', '\u3054', '\u3081', '\u306a', '\u3055', '\u3044', '\u3059', '\u307f', '\u307e', '\u305b', '\u3093', '\u97f3', '\u7ea2', '\u5b9d', '\u4e66', '\u5c01', '\u67cf', '\u8363', '\u6c5f', '\u9752', '\u9e21', '\u6c64', '\u6587', '\u7cb5', '\u62fc', '\u5be7', '\u53ef', '\u932f', '\u6bba', '\u5343', '\u7d55', '\u653e', '\u904e', '\u300d', '\u4e4b', '\u52e2', '\u8bf7', '\u56fd', '\u77e5', '\u8bc6', '\u4ea7', '\u6743', '\u5c40', '\u6a19', '\u9ede', '\u7b26', '\u865f', '\u65b0', '\u5e74', '\u5feb', '\u4e50', '\u5b66', '\u4e1a', '\u8fdb', '\u6b65', '\u8eab', '\u4f53', '\u5065', '\u5eb7', '\u4eec', '\u8bfb', '\u6211', '\u7684', '\u7ffb', '\u8bd1', '\u7bc7', '\u7ae0', '\u6b22', '\u8fce', '\u5165', '\u5751', '\u6709', '\u6bd2', '\u9ece', '\u6c0f', '\u7389', '\u82f1', '\u5567', '\u60a8', '\u8fd9', '\u53e3', '\u5473', '\u5947', '\u7279', '\u4e5f', '\u5c31', '\u7f62', '\u4e86', '\u975e', '\u8981', '\u4ee5', '\u6b64', '\u4e3a', '\u4f9d', '\u636e', '\u5bf9', '\u4eba', '\u5bb6', '\u6279', '\u5224', '\u4e00', '\u756a', '\u4e0d', '\u5730', '\u9053', '\u554a', '\u8c22', '\u516d', '\u4f6c']\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","43c78f3c":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', ' ##### ', x)\n    x = re.sub('[0-9]{4}', ' #### ', x)\n    x = re.sub('[0-9]{3}', ' ### ', x)\n    x = re.sub('[0-9]{2}', ' ## ', x)\n    return x\n\ndef punct_add_space(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x  \n\ndef odd_add_space(x):\n    x = str(x)\n    for odd in odd_chars:\n        x = x.replace(odd, f' {odd} ')\n    return x \n\ndef clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","3741d650":"train_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: clean_numbers(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: punct_add_space(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: odd_add_space(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: clean_contractions(x, contraction_mapping))","5dc362c5":"train_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: word_tokenize(x))","c185e575":"train_rebal.head()","07de0a06":"tf_idf_vec = TfidfVectorizer(min_df=3,\n                             max_features = None, \n                             analyzer=\"word\",\n                             ngram_range=(1,3), # (1,6)\n                             stop_words=\"english\")\ntf_idf = tf_idf_vec.fit_transform(list(train_rebal[\"text\"].map(lambda tokens: \" \".join(tokens))))","38a623b7":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=50, random_state=2020)\nsvd_tfidf = svd.fit_transform(tf_idf)\nprint(\"Dimensionality of LSA space: {}\".format(svd_tfidf.shape))","665b38f9":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,0],\n           svd_tfidf[:,1],\n           svd_tfidf[:,2],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='o')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(0.0, 0.4)\nplt.ylim(-0.2,0.4)\nplt.show()","203f3e93":"fig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,20],\n           svd_tfidf[:,21],\n           svd_tfidf[:,22],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='o')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(-0.4, 0.4)\nplt.ylim(-0.3,0.4)\nplt.show()","9543be5f":"fig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,47],\n           svd_tfidf[:,48],\n           svd_tfidf[:,49],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='x')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(-0.2, 0.6)\nplt.ylim(-0.2,0.2)\nplt.show()","76448380":"!pip install MulticoreTSNE","b92adca7":"from MulticoreTSNE import MulticoreTSNE as TSNE\ntsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4, # Trying out exaggeration trick\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","57d37538":"tsne_tfidf_df = pd.DataFrame(data=tsne_tfidf, columns=[\"x\", \"y\"])\ntsne_tfidf_df[\"id\"] = train_rebal[\"id\"].values\ntsne_tfidf_df[\"text\"] = train_rebal[\"text\"].values\ntsne_tfidf_df[\"target\"] = train_rebal[\"target\"].values","c09b4396":"output_notebook()\nplot_tfidf = bp.figure(plot_width = 600, plot_height = 600, \n                       title = \"T-SNE applied to Tfidf_SVD space\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\ncolormap = np.array([\"darkblue\", \"red\"])\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\nsource = ColumnDataSource(data = dict(x = tsne_tfidf_df[\"x\"], \n                                      y = tsne_tfidf_df[\"y\"],\n                                      color = colormap[tsne_tfidf_df[\"target\"]],\n                                      text = tsne_tfidf_df[\"text\"],\n                                      id = tsne_tfidf_df[\"id\"],\n                                      target = tsne_tfidf_df[\"target\"]))\n\nplot_tfidf.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 1)\nhover = plot_tfidf.select(dict(type = HoverTool))\nhover.tooltips = {\"id\": \"@id\", \n                  \"text\": \"@text\", \n                  \"target\":\"@target\"}\n\nshow(plot_tfidf)","e7770108":"tsne_model_5 = TSNE(n_jobs=4, \n                    early_exaggeration=4,\n                  perplexity=5,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_5 = tsne_model_5.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_5 = pd.DataFrame(data=tsne_tfidf_5, columns=[\"x5\", \"y5\"])\ntsne_tfidf_df_5[\"target\"] = train_rebal[\"target\"][:6542].values","d29b77ec":"tsne_model_25 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=25,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_25 = tsne_model_25.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_25 = pd.DataFrame(data=tsne_tfidf_25, \n                             columns=[\"x25\", \"y25\"])\ntsne_tfidf_df_25[\"target\"] = train_rebal[\"target\"][:6542].values","da1ad3e8":"tsne_model_50 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=50,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_50 = tsne_model_50.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=50\ntsne_tfidf_df_50 = pd.DataFrame(data=tsne_tfidf_50, \n                                columns=[\"x50\", \"y50\"])\ntsne_tfidf_df_50[\"target\"] = train_rebal[\"target\"][:6542].values","ea90f9eb":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_5.x5, \n            tsne_tfidf_df_5.y5, \n            alpha=0.75,\n            c=tsne_tfidf_df_5.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=5)\")\nplt.legend()\nplt.show()","85309754":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_25.x25, \n            tsne_tfidf_df_25.y25, \n            c=tsne_tfidf_df_25.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=25)\")\nplt.legend()\nplt.show()","f75ccdd8":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_50.x50, \n            tsne_tfidf_df_50.y50, \n            c=tsne_tfidf_df_50.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=50)\")\nplt.legend()\nplt.show()","01fe1013":"from gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntexts = list(train_rebal[\"text\"])\n\n# Creating a list of terms and a list of labels to go with it\ndocuments = [TaggedDocument(doc, tags=[str(i)]) for i, doc in enumerate(texts)]","087fda04":"max_epochs = 100\nalpha=0.025\nmodel = Doc2Vec(documents,\n                size=10, \n                min_alpha=0.00025,\n                alpha=alpha,\n                min_count=1,\n                workers=4)","83e579d3":"tsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=300)\ntsne_d2v = tsne_model.fit_transform(model.docvecs.vectors_docs)\n\n# Putting the tsne information into sq\ntsne_d2v_df = pd.DataFrame(data=tsne_d2v, columns=[\"x\", \"y\"])\ntsne_d2v_df[\"id\"] = train_rebal[\"id\"].values\ntsne_d2v_df[\"text\"] = train_rebal[\"text\"].values\ntsne_d2v_df[\"target\"] = train_rebal[\"target\"].values","709a3351":"output_notebook()\nplot_d2v = bp.figure(plot_width = 500, plot_height = 500, \n                       title = \"T-SNE applied to Doc2vec document embeddings\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\ncolormap = np.array([\"darkblue\", \"cyan\"])\n\nsource = ColumnDataSource(data = dict(x = tsne_d2v_df[\"x\"], \n                                      y = tsne_d2v_df[\"y\"],\n                                      color = colormap[tsne_d2v_df[\"target\"]],\n                                      text = tsne_d2v_df[\"text\"],\n                                      id = tsne_d2v_df[\"id\"],\n                                      target = tsne_d2v_df[\"target\"]))\n\nplot_d2v.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 1.0)\nhover = plot_d2v.select(dict(type = HoverTool))\nhover.tooltips = {\"id\": \"@id\", \n                  \"text\": \"@text\", \n                  \"target\":\"@target\"}\n\nshow(plot_d2v)","2cf409b4":"import nltk\n\n#stop-words\nfrom nltk.corpus import stopwords\nstop_words=set(nltk.corpus.stopwords.words('english'))\n\n# tokenizing\nfrom nltk import word_tokenize,sent_tokenize\n\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n\n#keras\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import one_hot,Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten ,Embedding,Input\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D,Lambda\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom tensorflow.keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom keras.engine.topology import Layer\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.callbacks import *\n#custome function for f1 score\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","e585e8b6":"sample_text_1=\"Kaggle, a subsidiary of Google LLC, is an online community of Data scientists and machine learning practitioners\"\nsample_text_2=\"Data science is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data\"\ncorp=[sample_text_1,sample_text_2]\nno_docs=len(corp)","9ee1589c":"vocab_size=50 \nencod_corp=[]\nfor i,doc in enumerate(corp):\n    encod_corp.append(one_hot(doc,50))\n    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))","78c5db08":"maxlen=-1\nfor doc in corp:\n    tokens=nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen=len(tokens)\nprint(\"The maximum number of words in any document is : \",maxlen)","ddbebb57":"# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\npad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\nprint(\"No of padded documents: \",len(pad_corp))","a5e1600e":"for i,doc in enumerate(pad_corp):\n     print(\"The padded encoding for document\",i+1,\" is : \",doc)","d5bc8f67":"input=Input(shape=(no_docs,maxlen),dtype='float64')","1e944115":"word_input=Input(shape=(maxlen,),dtype='float64')  \n\n# creating the embedding\nword_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n\nword_vec=Flatten()(word_embedding) # flatten\nembed_model =Model([word_input],word_vec) # combining all into a Keras model","1865bf3d":"embed_model.compile(optimizer=Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) ","77c5f5f4":"print(type(word_embedding))\nprint(word_embedding)","e4c6b559":"print(embed_model.summary())","9815f46c":"embeddings=embed_model.predict(pad_corp)","410e9e6d":"print(\"Shape of embeddings : \",embeddings.shape)\nprint(embeddings)","a4c14009":"embeddings=embeddings.reshape(-1,maxlen,8)\nprint(\"Shape of embeddings : \",embeddings.shape) \nprint(embeddings)","e43e4565":"for i,doc in enumerate(embeddings):\n    for j,word in enumerate(doc):\n        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)","ad756171":"train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2020)\n\n## some config values \nembed_size = 128 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\ntrain_X = train_df[\"text\"].values\nval_X = val_df[\"text\"].values\ntest_X = test_df[\"text\"].values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","f94b7739":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(GRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy',f1])\n\nprint(model.summary())","84fb5f98":"model.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y))","619bc1c3":"pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)","ceeb0e13":"from tqdm import tqdm\ndef threshold_search(y_true, y_proba):\n#reference: https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.001 for i in range(1000)]):\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\nsearch_result = threshold_search(val_y, pred_noemb_val_y)\nsearch_result","fc0b596a":"train = train_df.drop('target',axis = 1)\ndf = pd.concat([train ,test_df])","b13321b4":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '\/kaggle\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","b7f6352c":"glove = '\/kaggle\/input\/embeddings\/glove-840B-300d.txt'\nprint(\"Extracting GloVe embedding\")\nembed_glove = load_embed(glove)\nprint('Loaded glove embeddings sucessfully...')","38a45338":"import operator \ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","3c3c999c":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","d67101be":"vocab = build_vocab(df['text'])","cd30ed2c":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","01015fd9":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","260cde21":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\nadd_lower(embed_glove, vocab)\noov_glove = check_coverage(vocab, embed_glove)","0ed504ac":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","11dc36b2":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","21498bea":"print(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(embed_glove))","7cd2fab3":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","b67b5fd7":"df['treated_text'] = df['text'].apply(lambda x: clean_contractions(x, contraction_mapping))","7876ee19":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","a61f67c9":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","9af28d9d":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","630fee0b":"print(\"Glove :\")\nprint(unknown_punct(embed_glove, punct))","74dd17e4":"punct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }","5fabd125":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","e2500733":"df['treated_text'] = df['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","3e64738d":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","03b2e4ef":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pok\u00e9mon': 'pokemon'}","dff4f09f":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","d8c9ed04":"df['treated_text'] = df['treated_text'].apply(lambda x: correct_spelling(x, mispell_dict))","038b45db":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","962b8c61":"train['treated_text'] = train['text'].apply(lambda x: x.lower())\n# Contractions\ntrain['treated_text'] = train['text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# Special characters\ntrain['treated_text'] = train['text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","e8a353f0":"def make_data(X):\n    t = Tokenizer(num_words=max_features)\n    t.fit_on_texts(X)\n    X = t.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    return X, t.word_index","f8f2c203":"X, word_index = make_data(train['text'])","4f29b4f9":"def make_treated_data(X):\n    t = Tokenizer(num_words=max_features, filters='')\n    t.fit_on_texts(X)\n    X = t.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    return X, t.word_index","9cd54ad7":"X_treated, word_index_treated = make_treated_data(train['treated_text'])","9352393b":"X_train, X_val, y_train, y_val = train_test_split(X, train_df['target'].values, test_size=0.1, random_state=2020)","d934a716":"X_t_train, X_t_val, _, _ = train_test_split(X_treated, train_df['target'].values, test_size=0.1, random_state=2020)","2ea421fe":"def make_embed_matrix(embeddings_index, word_index, len_voc):\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = word_index\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n    \n    for word, i in word_index.items():\n        if i >= len_voc:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","c570056a":"embedding = make_embed_matrix(embed_glove, word_index, max_features)\ndel word_index\ngc.collect()","1ace8513":"embedding_treated = make_embed_matrix(embed_glove, word_index_treated, max_features)\ndel word_index_treated\ngc.collect()","2ccd8830":"def modelling(embe_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, 300, weights=[embe_matrix])(inp)\n    x = Bidirectional(GRU(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(16, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy',f1])\n    return model","f34e31f6":"model = modelling(embedding)","d1c07110":"model_treated = modelling(embedding_treated)","ff7c3780":"history = model.fit(X_train, y_train, batch_size=512, epochs=10, \n                    validation_data=[val_X, val_y])\npred_val = model.predict(X_val, batch_size=512, verbose=1)","e3f1e6c8":"history = model_treated.fit(X_t_train, y_train, batch_size=512, epochs=10, \n                            validation_data=[X_t_val, y_val])\npred_t_val = model_treated.predict(X_t_val, batch_size=512, verbose=1)","9de20fe0":"search_result = threshold_search(y_val, pred_val)\nsearch_result","4e47a326":"search_result = threshold_search(y_val, pred_t_val)\nsearch_result","b4ef9529":"### Preparing data","f76f3566":"# 3. Resampling the training data.","602e943c":"### CREATING THE EMBEDDINGS USING KERAS EMBEDDING LAYER <br>","8b55a75e":"### 2.4.1 Word Embedding layer <br>\n*  A word embedding layer that is learned jointly with a neural network model on a specific natural language processing task classification,text generation etc..,\n* It requires that text be cleaned and prepared such that each word is one-hot encoded. The size of the vector space is specified as part of the model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm. <br>\n* The one-hot encoded words are mapped to the word vectors. If a multilayer Perceptron model is used, then the word vectors are concatenated before being fed as input to the model. If a recurrent neural network is used, then each word may be taken as one input in a sequence. <br>\n* [Read more here](https:\/\/machinelearningmastery.com\/what-are-word-embeddings\/)","ff6bdbc4":"#### Credits and refrences: <br>\nI have learned these techniques and implemented in this competitions from following kernels: <br>\n1. [Target Visualization - T-SNE and Doc2Vec](https:\/\/www.kaggle.com\/arthurtok\/target-visualization-t-sne-and-doc2vec) <br>\n2. [A Detailed Explanation of Keras Embedding Layer](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer) <br>\n3. [A look at different embeddings.!](https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings)<br>\n4. [Improve your Score with Text Preprocessing -- V2](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2)<br>\nThanks to authors of the above kernels :)","ec4db6f9":"### Correcting spellings:","41ae56af":"## 2.4 Word Embedding Algorithms <br>\n* Word embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text.The learning process is either joint with the neural network model on some task. <br>","976d1ce1":"### Building model without pretrained embeddings","80ef3ce0":"##### We can observe that f1-score without text preprocessing was 0.799","c2767194":"### Contractions:","28355f5c":"# Thats all for now.<font color='red'>Please consider Upvoting this kernel<\/font>.Suggestions are much appreciated to improve this kernel further. <br>\n# <font color='blue'>Happy learning :)<\/font>","438709e7":"# 1. Importing necessary modules.","199d606f":"<a id='models'><\/a>\n# <font color='orange'> Part three: Building basic models and text preprocessing to improve score<\/font>  ","a126050f":"##### We can observe that f1-score with text preprocessing was 0.82","e4c2c3fc":"# 8. Visualising T-SNE applied to LSA reduced space by changing Perplexity.","74a8ed70":"# 6.Scatter plots of the Latent Semantic Space","44c8545f":"### Taking our sample text corpus","7da9e0e2":"## Observations: <br>\n* From the above scatter plots, It is apparent that real disaster tweets and not real disaster tweets overlap quite significantly in the LSA semantic space. <br>\n* Also,there does not seem to be any clear or obvious pattern in segregating the class labels. <br>","3c397e3f":"## 6.3 Last 3 dimensions of the Latent Semantic Space","aebb3ee3":"## 2.5 Training our own embedding layer in keras <br>","09f0d3cc":"# 9. T-SNE applied on Doc2Vec embedding<br>\n* Moving forward with our T-SNE visual explorations, we next move away from semantic matrices into the realm of embeddings. Here we will use the Doc2Vec algorithm and much like its very well known counterpart Word2vec involves unsupervised learning of continuous representations for text. Unlike Word2vec which involves finding the representations for words (i.e. word embeddings), Doc2vec modifies the former method and extends it to sentences and even documents.<br>","a058576e":"* Having obtained our tf-idf matrix - a sparse matrix object, we now apply the TruncatedSVD method to first reduce the dimensionality of the Tf-idf matrix to a decomposed feature space, referred to in the community as the LSA (Latent Semantic Analysis) method.\n\n* LSA has been one of the classical methods in text that have existed for a while allowing \"concept\" searching of words whereby words which are semantically similar to each other (i.e. have more context) are closer to each other in this space and vice-versa.","930cc245":"# About competition: <br>\n* Twitter has become an important communication channel in times of emergency.The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). <br>\n* In this competition, we are challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.<br>","0cdcfed0":"<a id='word Embeddings'><\/a>\n# <font color='blue'>Part Two: Introduction to word Embeddings<\/font>","bca1fb10":"## Observations:<br>\n* It seems that the distribution of the real disaster tweets and not real disaster tweets overlap in certain regions of the T-SNE plots in concept space, which does not allow easy visual discernment between the two classes. <br>\n* This raises a question of how easy therefore, is it to a human to distinguish between an real disaster tweets and not real disaster tweets, when we see data from both class labels overlapping quite heavily across each other. <br>","25bc39a5":"### Punctuations:","993a2242":"# 2. Importing dataframes.","f1dae881":"<a id='top'><\/a>\n<h1 style=\"text-align:center;font-size:200%;;\">Real or Not? NLP with Disaster Tweets<\/h1>\n![](https:\/\/st.depositphotos.com\/1032753\/4674\/v\/950\/depositphotos_46741417-stock-illustration-twitter-and-social-media-concept.jpg)","795f9dee":"### Preparing data:","89f2e688":"## Therefore we can conclude that preprocessing the text with according to the embeddings we use,will help us to increase the score.","bd0450c8":"### Loading embeddings","1d718303":"# 4. Text processing.<br>\n**In this section, we will do some pre-processing of the text contained within the training data. The processing applied here are some of the standard NLP steps that one would implement in a text based problem, consisting of:**\n\n* Tokenization\n* Stemming or Lemmatization","882fc412":"# 5. T-SNE applied to Latent Semantic (LSA) space\n* To start off we look at the sparse representation of text documents via the Term frequency Inverse document frequency method. What this does is create a matrix representation that upweights locally prevalent but globally rare terms - therefore accounting for the occurence bias when using just term frequencies.","0e0bc196":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content:<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Target Visualization\" role=\"tab\" aria-controls=\"profile\">Part one: Target Visualization<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#word Embeddings\" role=\"tab\" aria-controls=\"messages\">Part Two: Introduction to word Embeddings<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#models\" role=\"tab\" aria-controls=\"settings\">Part three: Building basic models and text preprocessing to improve score<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n  ","03160620":"In this part one will be see an exploration into the target variable and how it is distributed across the structure of the training data to see if any potential information or patterns can be gleaned going forward. Since classical treatments of text data normally comes with the challenges of high dimensionality (using term frequencies or term frequency inverse document frequencies), the plan therefore in this kernel is to visually explore the target variable in some lower dimensional space.","4fdfcc6c":"## 2.1 What are word embeddings?<br>\n* A word embedding is a learned representation for text where words that have the same meaning have a similar representation.<br>\n* In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. <br>","2e61a9b2":"### Building model using Glove Embeddings","e1e9fb94":"# 7. Applying T-SNE(non-linear method) to LSA reduced space","c58b7f71":"## 6.2 Random 3 dimensions of the Latent Semantic Space","f85ba0ff":"* In this part we will build some basic models with simple architectures.Also,we will explore text processing techniques when we are using word embeddings.<br>\n* Also we will compare results of the model build on text processed text and text which was not processed and cleaned.","83476830":"## Observations: <br>\n*  Here also the distribution of the real disaster tweets and not real disaster tweets overlap in certain regions of the T-SNE plots in concept space, which does not allow easy visual discernment between the two classes.","e51fe428":"## Observations: <br>\n* The visual overlap between real disaster tweets and not real disaster tweets are pretty high in some regions and not that much high in some regions in the Doc2Vec plots.<br>\n* But,we cannot saggregate the labels using our eye ball. <br>","1a83c856":"## 2.2 Why do we need word embeddings?\n* As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications.<br>","d8c09ad9":"### Correcting the spellings had not done any trick for us. As we can observe that found embeddings percentage was decreased.","0be4b912":"### Vocabulary and Coverage functions","73487232":"## 2.3 Different types of word embeddings <br>\n* The different types of word embeddings can be broadly classified into two categories- <br>\n\n1. Frequency based Embedding <br>\n  1.1 Count Vector <br>\n  1.2 Tf-IDF Vector <br>\n  1.3 Co-Occurance Vector <br>\n  \n2. Prediction based Embedding <br>\n#### [Read more](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/)","c1d86ad3":"### INTEGER ENCODING ALL THE DOCUMENTS\n* After this all the unique words will be reprsented by an integer. For this we are using one_hot function from the Keras. Note that the vocab_size is specified large enough so as to ensure unique integer encoding for each and every word.\n\n* Note one important thing that the integer encoding for the word remains same in different docs. eg 'Data' is denoted by 21 in each and every document.","f589f988":"## Fitting a T-SNE model to the dense embeddings and overlaying that with the target visuals, we get:<br>","bf749a3b":"## Columns description\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","031a5295":"##### We can see the f1 score was 0.76 at threshold value of 0.159 ","6ffb4e3a":"### GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC TEXT <br>","10105c70":"<a id='Target Visualization'><\/a>\n# <font color='red'> Part one: Target Visualization<\/font> <br>","792c0a38":"The resulting shape is (2,34,8).\n\n2---> no of documents\n\n34---> each document is made of 34 words which was our maximum length of any document.\n\n& 8---> each word is 8 dimensional.","757bf6a7":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP<\/a>","76f8f687":"### PADDING THE DOCS (to make very doc of same length): <br>\n* The Keras Embedding layer requires all individual documents to be of same length. Hence we wil pad the shorter documents with 0 for now. Therefore now in Keras Embedding layer the 'input_length' will be equal to the length (ie no of words) of the document with maximum length or maximum number of words.\n\n* To pad the shorter documents I am using pad_sequences functon from the Keras library.","5b362c9e":"## 6.1 First 3 dimensions of the Latent Semantic Space"}}