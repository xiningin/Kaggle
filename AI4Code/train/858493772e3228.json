{"cell_type":{"1acea1bd":"code","e80dbc0e":"code","e229fe12":"code","0159dd78":"code","106d5781":"code","0b8d960a":"code","beedebb2":"code","28d6e4e3":"code","a6c3e165":"code","2c942548":"code","a5d5eb83":"code","96d01f1c":"code","e4cc7ee7":"markdown"},"source":{"1acea1bd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport lightgbm as lgb\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cluster import MiniBatchKMeans","e80dbc0e":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')","e229fe12":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","0159dd78":"#train = reduce_mem_usage(train)\n#test = reduce_mem_usage(test)","106d5781":"features = [x for x in train.columns.values if x[0]==\"f\"]","0b8d960a":"# Counting amount of missing values in each row and adding it as a new feature\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","beedebb2":"X = train.drop([\"claim\"], axis=1)\nX_test = test\ny = train[\"claim\"]","28d6e4e3":"del test, train\ngc.collect()","a6c3e165":"# Scaling all values\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","2c942548":"# Model hyperparameters\nlgbm_params = {'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 6,\n               'max_depth': 2,\n               'learning_rate': 0.1,\n               'n_estimators': 40000,\n               'reg_alpha': 25.0,\n               'reg_lambda': 76.7,\n               'random_state': 0,\n               'bagging_seed': 0, \n               'feature_fraction_seed': 0,\n               'n_jobs': -1,\n               'subsample': 0.98,\n               'subsample_freq': 1,\n               'colsample_bytree': 0.69,\n               'min_child_samples': 54,\n               'min_child_weight': 256,\n               'metric': 'AUC',\n               'verbosity': -1}","a5d5eb83":"%%time\n\nsplits = 7\nkf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=0)\n\noof_preds = np.zeros(len(X))\npreds = np.zeros(len(X_test))\ntotal_mean_auc = 0\n\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid)\n    \n    model = lgb.train(lgbm_params,\n                      lgb_train,\n                      verbose_eval=1000,\n                      early_stopping_rounds=200,\n                      valid_sets=[lgb_valid])\n    preds += model.predict(X_test) \/ splits\n    \n    #oof_preds[valid_idx] = model.predict(X_valid)\n    #fold_auc = roc_auc_score(y_valid, oof_preds[valid_idx])\n    #print(f\"Fold {num} ROC AUC: {fold_auc}\")\n    #total_mean_auc += fold_auc \/ splits\n#print(f\"\\nOverall ROC AUC: {total_mean_auc}\")","96d01f1c":"submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv', index_col='id')\nsubmission['claim'] = preds\nsubmission.to_csv('submission.csv')","e4cc7ee7":"The idea of adding the \"n_missing\" feature below is taken from [this notebook](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm) by [BIZEN](https:\/\/www.kaggle.com\/hiro5299834)."}}