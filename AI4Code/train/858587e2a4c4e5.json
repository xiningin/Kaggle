{"cell_type":{"ab50e1ad":"code","6a53545e":"code","a4d5fde9":"code","e9c323fa":"code","97c44dd8":"code","600906ef":"code","f5f407cb":"code","5b3a0cbf":"code","c118cfed":"code","c540a748":"code","d40efa98":"code","cd79bcef":"code","bfddafdd":"code","caeb2c08":"code","711205a7":"code","f840bf6c":"code","232b9102":"code","75d163e9":"code","cca5e534":"code","0bff3a9e":"code","3d6d5c59":"code","f8481bd6":"code","1e63a302":"code","6a242bde":"code","797c37e0":"code","2a7d4927":"code","cbb1acc0":"code","4b7140b4":"code","2d837f78":"code","ed496d74":"code","55942188":"code","65092aa8":"code","bb5cc819":"code","33ed2ff7":"code","d72c3ace":"code","316323cc":"code","39ce56cf":"code","31b3753d":"code","17d9bfca":"code","0bb1ab12":"code","33e7a299":"code","7ead05df":"code","e07de29f":"code","bedb1c15":"code","219cda34":"code","8b1bee80":"markdown","1c671c93":"markdown","18efe784":"markdown","a27ce318":"markdown","095a4cbc":"markdown","1fffc0a9":"markdown","cf78e8c1":"markdown","8b91243b":"markdown","db59a4da":"markdown","cee92dfc":"markdown","a25bb310":"markdown","c3c1cf63":"markdown","10e06f56":"markdown","8f4d8904":"markdown"},"source":{"ab50e1ad":"import numpy as np\nimport pandas as pd\nimport scipy\n\n#These are the visualization libraries. Matplotlib is standard and is what most people use.\n#Seaborn works on top of matplotlib, as we mentioned in the course.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n#For standardizing features. We'll use the StandardScaler module.\nfrom sklearn.preprocessing import StandardScaler\n#Hierarchical clustering with the Sci Py library. We'll use the dendrogram and linkage modules.\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n#Sk learn is one of the most widely used libraries for machine learning. We'll use the k means and pca modules.\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n# We need to save the models, which we'll use in the next section. We'll use pickle for that.\nimport pickle","6a53545e":"# Load the data, contained in the segmentation data csv file.\ndf_segmentation = pd.read_csv('data.csv', index_col = 0)","a4d5fde9":"# Descriptive analysis of the data set. Here we just look at the data to gain some insight. \n# We do not apply any transformations or changes to the data.\ndf_segmentation.head()","e9c323fa":"df_segmentation.describe()","97c44dd8":"# Compute Pearson correlation coefficient for the features in our data set.\n# The correlation method in pandas, it has the Pearson correlation set as default.\ndf_segmentation.corr()","600906ef":"# We'll plot the correlations using a Heat Map. Heat Maps are a great way to visualize correlations using color coding.\n# We use RdBu as a color scheme, but you can use viridis, Blues, YlGnBu or many others.\n# We set the range from -1 to 1, as it is the range of the Pearson Correlation. \n# Otherwise the function infers the boundaries from the input.\n# In this case they will be -0,25 to 0,68, as they are the minumum and maximum correlation indeces between our features.\nplt.figure(figsize = (12, 9))\ns = sns.heatmap(df_segmentation.corr(),\n               annot = True, \n               cmap = 'RdBu',\n               vmin = -1, \n               vmax = 1)\ns.set_yticklabels(s.get_yticklabels(), rotation = 0, fontsize = 12)\ns.set_xticklabels(s.get_xticklabels(), rotation = 90, fontsize = 12)\nplt.title('Correlation Heatmap')\nplt.show()","f5f407cb":"# We'll plot the data. We create a 12 by 9 inches figure.\n# We have 2000 data points, which we'll scatter acrros Age and Income, located on positions 2 and 4 in our data set. \nplt.figure(figsize = (12, 9))\nplt.scatter(df_segmentation.iloc[:, 2], df_segmentation.iloc[:, 4])\nplt.xlabel('Age')\nplt.ylabel('Income')\nplt.title('Visualization of raw data')","5b3a0cbf":"# Standardizing data, so that all features have equal weight. This is important for modelling.\n# Otherwise, in our case Income would be considered much more important than Education for Instance. \n# We do not know if this is the case, so we would not like to introduce it to our model. \n# This is what is also refered to as bias.\nscaler = StandardScaler()\nsegmentation_std = scaler.fit_transform(df_segmentation)","c118cfed":"# Perform Hierarchical Clustering. The results are returned as a linkage matrix. \nhier_clust = linkage(segmentation_std, method = 'ward')","c540a748":"# We plot the results from the Hierarchical Clustering using a Dendrogram. \n# We truncate the dendrogram for better readability. The level p shows only the last p merged clusters\n# We also omit showing the labels for each point.\nplt.figure(figsize = (12,9))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Observations')\nplt.ylabel('Distance')\ndendrogram(hier_clust,\n           truncate_mode = 'level', \n           p = 5, \n           show_leaf_counts = False, \n           no_labels = True)\nplt.show()","d40efa98":"# Perform K-means clustering. We consider 1 to 10 clusters, so our for loop runs 10 iterations.\n# In addition we run the algortihm at many different starting points - k means plus plus. \n# And we set a random state for reproducibility.\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(segmentation_std)\n    wcss.append(kmeans.inertia_)","cd79bcef":"# Plot the Within Cluster Sum of Squares for the different number of clusters.\n# From this plot we choose the number of clusters. \n# We look for a kink in the graphic, after which the descent of wcss isn't as pronounced.\nplt.figure(figsize = (10,8))\nplt.plot(range(1, 11), wcss, marker = 'o', linestyle = '--')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.title('K-means Clustering')\nplt.show()","bfddafdd":"# We run K-means with a fixed number of clusters. In our case 4.\nkmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)","caeb2c08":"# We divide our data into the four clusters.\nkmeans.fit(segmentation_std)","711205a7":"# We create a new data frame with the original features and add a new column with the assigned clusters for each point.\ndf_segm_kmeans = df_segmentation.copy()\ndf_segm_kmeans['Segment K-means'] = kmeans.labels_","f840bf6c":"# Calculate mean values for the clusters\ndf_segm_analysis = df_segm_kmeans.groupby(['Segment K-means']).mean()\ndf_segm_analysis","232b9102":"# Compute the size and proportions of the four clusters\ndf_segm_analysis['N Obs'] = df_segm_kmeans[['Segment K-means','Sex']].groupby(['Segment K-means']).count()\ndf_segm_analysis['Prop Obs'] = df_segm_analysis['N Obs'] \/ df_segm_analysis['N Obs'].sum()","75d163e9":"df_segm_analysis","cca5e534":"df_segm_analysis.rename({0:'well-off',\n                         1:'fewer-opportunities',\n                         2:'standard',\n                         3:'career focused'})","0bff3a9e":"# Add the segment labels to our table\ndf_segm_kmeans['Labels'] = df_segm_kmeans['Segment K-means'].map({0:'well-off', \n                                                                  1:'fewer opportunities',\n                                                                  2:'standard', \n                                                                  3:'career focused'})","3d6d5c59":"# We plot the results from the K-means algorithm. \n# Each point in our data set is plotted with the color of the clusters it has been assigned to.\nx_axis = df_segm_kmeans['Age']\ny_axis = df_segm_kmeans['Income']\nplt.figure(figsize = (10, 8))\nsns.scatterplot(x_axis, y_axis, hue = df_segm_kmeans['Labels'], palette = ['g', 'r', 'c', 'm'])\nplt.title('Segmentation K-means')\nplt.show()","f8481bd6":"# Employ PCA to find a subset of components, which explain the variance in the data.\npca = PCA()","1e63a302":"# Fit PCA with our standardized data.\npca.fit(segmentation_std)","6a242bde":"# The attribute shows how much variance is explained by each of the seven individual components.\npca.explained_variance_ratio_","797c37e0":"# Plot the cumulative variance explained by total number of components.\n# On this graph we choose the subset of components we want to keep. \n# Generally, we want to keep around 80 % of the explained variance.\nplt.figure(figsize = (12,9))\nplt.plot(range(1,8), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\nplt.title('Explained Variance by Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')","2a7d4927":"# We choose three components. 3 or 4 seems the right choice according to the previous graph.\npca = PCA(n_components = 3)","cbb1acc0":"#Fit the model the our data with the selected number of components. In our case three.\npca.fit(segmentation_std)","4b7140b4":"# Here we discucss the results from the PCA.\n# The components attribute shows the loadings of each component on each of the seven original features.\n# The loadings are the correlations between the components and the original features. \npca.components_","2d837f78":"df_pca_comp = pd.DataFrame(data = pca.components_,\n                           columns = df_segmentation.columns.values,\n                           index = ['Component 1', 'Component 2', 'Component 3'])\ndf_pca_comp","ed496d74":"# Heat Map for Principal Components against original features. Again we use the RdBu color scheme and set borders to -1 and 1.\nsns.heatmap(df_pca_comp,\n            vmin = -1, \n            vmax = 1,\n            cmap = 'RdBu',\n            annot = True)\nplt.yticks([0, 1, 2], \n           ['Component 1', 'Component 2', 'Component 3'],\n           rotation = 45,\n           fontsize = 9)","55942188":"pca.transform(segmentation_std)","65092aa8":"scores_pca = pca.transform(segmentation_std)","bb5cc819":"# We fit K means using the transformed data from the PCA.\nwcss = []\nfor i in range(1,11):\n    kmeans_pca = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans_pca.fit(scores_pca)\n    wcss.append(kmeans_pca.inertia_)","33ed2ff7":"# Plot the Within Cluster Sum of Squares for the K-means PCA model. Here we make a decission about the number of clusters.\n# Again it looks like four is the best option.\nplt.figure(figsize = (10,8))\nplt.plot(range(1, 11), wcss, marker = 'o', linestyle = '--')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.title('K-means with PCA Clustering')\nplt.show()","d72c3ace":"# We have chosen four clusters, so we run K-means with number of clusters equals four. \n# Same initializer and random state as before.\nkmeans_pca = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)","316323cc":"# We fit our data with the k-means pca model\nkmeans_pca.fit(scores_pca)","39ce56cf":"# We create a new data frame with the original features and add the PCA scores and assigned clusters.\ndf_segm_pca_kmeans = pd.concat([df_segmentation.reset_index(drop = True), pd.DataFrame(scores_pca)], axis = 1)\ndf_segm_pca_kmeans.columns.values[-3: ] = ['Component 1', 'Component 2', 'Component 3']\n# The last column we add contains the pca k-means clustering labels.\ndf_segm_pca_kmeans['Segment K-means PCA'] = kmeans_pca.labels_","31b3753d":"df_segm_pca_kmeans","17d9bfca":"# We calculate the means by segments.\ndf_segm_pca_kmeans_freq = df_segm_pca_kmeans.groupby(['Segment K-means PCA']).mean()\ndf_segm_pca_kmeans_freq","0bb1ab12":"# Calculate the size of each cluster and its proportion to the entire data set.\ndf_segm_pca_kmeans_freq['N Obs'] = df_segm_pca_kmeans[['Segment K-means PCA','Sex']].groupby(['Segment K-means PCA']).count()\ndf_segm_pca_kmeans_freq['Prop Obs'] = df_segm_pca_kmeans_freq['N Obs'] \/ df_segm_pca_kmeans_freq['N Obs'].sum()\ndf_segm_pca_kmeans_freq = df_segm_pca_kmeans_freq.rename({0:'standard', \n                                                          1:'career focused',\n                                                          2:'fewer opportunities', \n                                                          3:'well-off'})\ndf_segm_pca_kmeans_freq","33e7a299":"df_segm_pca_kmeans['Legend'] = df_segm_pca_kmeans['Segment K-means PCA'].map({0:'standard', \n                                                          1:'career focused',\n                                                          2:'fewer opportunities', \n                                                          3:'well-off'})","7ead05df":"# Plot data by PCA components. The Y axis is the first component, X axis is the second.\nx_axis = df_segm_pca_kmeans['Component 2']\ny_axis = df_segm_pca_kmeans['Component 1']\nplt.figure(figsize = (10, 8))\nsns.scatterplot(x_axis, y_axis, hue = df_segm_pca_kmeans['Legend'], palette = ['g', 'r', 'c', 'm'])\nplt.title('Clusters by PCA Components')\nplt.show()","e07de29f":"# We save the objects we'll need in the Purchase Analytics part of the course. We export them as pickle objects.\n# We need the scaler, pca and kmeans_pca objects to preprocess and segment the purchase data set.\npickle.dump(scaler, open('scaler.pickle', 'wb'))","bedb1c15":"pickle.dump(pca, open('pca.pickle', 'wb'))","219cda34":"pickle.dump(kmeans_pca, open('kmeans_pca.pickle', 'wb'))","8b1bee80":"## ${\\textbf{Hierarchical Clustering}}$","1c671c93":"### ${\\textbf{PCA}}$","18efe784":"## ${\\textbf{Correlation Estimate}}$","a27ce318":"### ${\\textbf{Data Export}}$","095a4cbc":"## ${\\textbf{Explore Data}}$","1fffc0a9":"## ${\\textbf{Import Data}}$","cf78e8c1":"## ${\\textbf{K-means Clustering}}$","8b91243b":"## ${\\textbf{Visualize Raw Data}}$","db59a4da":"## ${\\textbf{Standardization}}$","cee92dfc":"### ${\\textbf{K-means clustering with PCA}}$","a25bb310":"## ${\\textbf{Libraries}}$","c3c1cf63":"### ${\\textbf{PCA Results}}$","10e06f56":"### ${\\textbf{K-means clustering with PCA Results}}$","8f4d8904":"### ${\\textbf{Results}}$"}}