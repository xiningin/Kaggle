{"cell_type":{"25bbefc1":"code","81d31c46":"code","4e9744e4":"code","55db2414":"code","d3f8bed2":"code","9c12b1a9":"code","3aa083b3":"code","1b40a0be":"code","c9393158":"code","e156c3f8":"code","8af2b909":"code","3ac2b871":"code","dda69608":"code","4f271bda":"code","b18dd3cb":"code","93058c69":"code","a5f013e5":"code","ce92a296":"code","e3d23168":"code","4bbd66f2":"code","ef351235":"code","715063c2":"code","58f0adbb":"code","9358b744":"code","aff4f758":"code","8a51ed65":"code","33666324":"code","4d413b4c":"code","0ad25360":"code","cb6d0f9c":"code","398ca5e9":"code","f3b49e29":"code","d4b18ff2":"code","740c32f2":"code","e153323a":"code","bda548f7":"code","30ebfd01":"code","7c4a1d89":"code","c36fd6e1":"code","5c41e1ea":"code","c8fc86bd":"code","7620e2d4":"code","aba00175":"code","c58690a8":"code","636d6714":"code","d1461560":"code","28b49ed5":"markdown","77a0e57f":"markdown","49811123":"markdown","e88c8ff7":"markdown","3c123612":"markdown","89513e9b":"markdown","d725b246":"markdown","6a80ee33":"markdown","acf90d6f":"markdown","72929e6c":"markdown","bdc0b4cd":"markdown","f2dcb580":"markdown","c3bd3a35":"markdown","db24a833":"markdown","07435bcb":"markdown","b4b32f36":"markdown","66936283":"markdown","a28ea556":"markdown","c4f6b857":"markdown"},"source":{"25bbefc1":"import numpy as np#import all necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","81d31c46":"train=pd.read_csv(\"..\/input\/c\/titanic\/train.csv\")#import the dataset\ntest=pd.read_csv(\"..\/input\/c\/titanic\/train.csv\")","4e9744e4":"\n#store the passengerid of test data\npassenger_id=test[\"PassengerId\"]\n\ntrain.head()","55db2414":"#set the index as passengerid\ntrain.set_index([\"PassengerId\"],inplace=True)\ntest.set_index([\"PassengerId\"],inplace=True)\ntrain.head()","d3f8bed2":"train.isnull().sum()#show the missing datas","9c12b1a9":"100*(train.isnull().sum()\/len(train))\ndef missing_values_percent(train):#we can use this function in all dataframes.\n    nan_percent=100*(train.isnull().sum()\/len(train))\n    nan_percent=nan_percent[nan_percent>0].sort_values()\n    return(nan_percent)\n\nnan_percent=missing_values_percent(train)\nnan_percent\n","3aa083b3":"#deciding about age coloumn which have almost 19% missing datas.\n#Imputer age coloumn\nfrom sklearn.impute import SimpleImputer\n#train data             \nImp=SimpleImputer(strategy='median')\nnew_train=Imp.fit_transform(train.Age.values.reshape(-1,1))\ntrain['Age2'] = new_train\n\n#test data\nnew_test=Imp.fit_transform(test.Age.values.reshape(-1,1))\ntest['Age2'] = new_test\n\n\ntrain.drop('Age',axis=1,inplace=True)\ntest.drop('Age',axis=1,inplace=True)\n\n\ntrain.head()","1b40a0be":"train.isnull().sum()","c9393158":"train[\"Embarked\"].value_counts()","e156c3f8":"#So we can replace missing datas in Embarked with s\ntrain[\"Embarked\"].fillna(\"s\",inplace=True)","8af2b909":"#cabin has 687 missing datas so we can get rid of it by dropping this feature.\ntrain.drop(\"Cabin\",axis=1,inplace=True)","3ac2b871":"train.isnull().sum()","dda69608":"train[\"Survived\"].value_counts(normalize=True)#How many passengers survived?","4f271bda":"def bar_chart_stacked(dataset,feature,stacked=True):\n  survived=train[train[\"Survived\"]==1][feature].value_counts()\n  dead=train[train[\"Survived\"]==0][feature].value_counts()   \n  df_survived_dead=pd.DataFrame([survived,dead])  \n  df_survived_dead.index=[\"passengers survived\",\"passengers died\"]   \n  df_survived_dead.plot(kind=\"bar\",stacked=stacked,figsize=(8,5))\n                              ","b18dd3cb":"bar_chart_stacked(train,\"Survived\")","93058c69":"train[\"Sex\"].value_counts().to_frame()#passengers count on gender","a5f013e5":"bar_chart_stacked(train,\"Sex\")#compare the survived  and dead passengers counts on gender","ce92a296":"train.groupby([\"Pclass\"])[\"Survived\"].mean().to_frame()","e3d23168":"bar_chart_stacked(train,\"Pclass\")","4bbd66f2":"def bar_chart_compare(dataset,feature1,feature2=None):\n    plt.figure(figsize=(8,5))\n    plt.title(\"survived rate by sex and pclass\")\n    g=sns.barplot(x=feature1,y=\"Survived\",hue=feature2,data=dataset).set ","ef351235":"bar_chart_compare(train,\"Pclass\",\"Sex\")","715063c2":"sns.scatterplot(data=train,x=\"Age2\",y=\"Survived\")","58f0adbb":"    plt.figure(figsize=(8,5))\n    plt.title(\"survival swarmplot for fare and gender\")\n    sns.swarmplot(y=\"Fare\",x=\"Sex\",hue=\"Survived\",data=train)","9358b744":"sns.pairplot(train)","aff4f758":"corr = train.corr()\nplt.subplots(figsize=(12, 8))\nsns.heatmap(corr, annot=True)","8a51ed65":"train[\"Name\"]","33666324":"train['Title'] =train['Name'].apply(lambda x: x.split(', ')[1].split('. ')[0].strip())\ntest['Title'] =train['Name'].apply(lambda x: x.split(', ')[1].split('. ')[0].strip())\ntrain[\"Title\"].unique()","4d413b4c":"plt.figure(figsize=(18,3))\nsns.barplot(data=train,x=\"Title\",y=\"Survived\")","0ad25360":"train.info()","cb6d0f9c":"train[\"family_size\"]=train[\"SibSp\"]+train[\"Parch\"]+1","398ca5e9":"def family_group(size):\n    a=\"\"\n    if(size<=1):\n        a=\"alone\"\n    elif(size<=4):\n        a=\"small\"\n    else:\n        a=\"large\"\n    return a","f3b49e29":"train[\"family_group\"]=train[\"family_size\"].map(family_group)\ntrain[\"fare_per_person\"]=train[\"Fare\"]\/train[\"family_size\"]\ntrain.head()","d4b18ff2":"plt.figure(figsize=(8,5))\nsns.barplot(data=train,x=\"family_group\",y=\"Survived\")","740c32f2":"train.drop([\"Ticket\"],axis=1,inplace=True)#This feature doesnt give us any useful infomation.","e153323a":"train[\"Pclass\"].apply(str)","bda548f7":"#Divide dataframe to 2 parts(num and str)\ntrain_num=train.select_dtypes(exclude=\"object\")\ntrain_obj=train.select_dtypes(include=\"object\")","30ebfd01":"train_obj=pd.get_dummies(train_obj,drop_first=True)#use one-hot encoding to transform str to int and float\ntrain_obj.shape","7c4a1d89":"Final_train=pd.concat([train_num,train_obj],axis=1)\nFinal_train.info()","c36fd6e1":"#Determine the feature and lable\nX=Final_train.drop(\"Survived\",axis=1)\ny=Final_train[\"Survived\"]","5c41e1ea":"#Split the dataset to train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","c8fc86bd":"from sklearn.preprocessing import StandardScaler#scaling the features\nscaler= StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","7620e2d4":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)","aba00175":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","c58690a8":"accuracy_score(y_test, y_pred)","636d6714":"confusion_matrix(y_test, y_pred)","d1461560":"print(classification_report(y_test, y_pred))","28b49ed5":"# **Introduction**","77a0e57f":"## **Name feature**","49811123":"### We can see that even though the majority of passenger were men, the majority of survivors were women. The key observation here is that the survival rate for female passengers is 4 times higher than the survival rate of male passengers.Maybe the women were rescued earlier or the man helped the women and they didnt have enough time to save themselves. ","e88c8ff7":"### **Now we have no missing data**","3c123612":"### As we see small family survived more in comparison with alone and large family.on the other words the percentage of survived passenger for small family is three times bigger than large family and two times bigger than alone passenger.","89513e9b":"# **Exploratory Data Analysis**","d725b246":"### We see that 62% of passengers in class 1 were survived but this amount is reduced to 47% for class 2 and only 24% of passengers in class 3 were survived.On the other words the percentage of survived passengers in class 1 is 2 times bigger than the percentage of died passengers in this class.But in class 2 the percentage of survived people and died people is almost equal and for class 3 the percentage of died people is three times bigger than survived one.","6a80ee33":"# **Family feature**","acf90d6f":"### As expected the majority of passengers in the training data died. Only 38% survived the disaster. So the training data suffers from data imbalance.","72929e6c":"### Gender of all passengers with a fare above 500 dollar survived.The men who paid between 200 and 300 dollar died but the women paid between 200 and 300 dollar survived.","bdc0b4cd":"### As we see the passangers who were 1 to almost 57 years old,were survived but the paasengers who were above 60 years old mostly died.","f2dcb580":"# **Encoding str to int**","c3bd3a35":"### We see the titles which are women title have a high survival rate in comparison with men title.Master and Dr have a high survival rate, too even though both are male titles.but Mr have a low survival rate.","db24a833":"**In this notebook I've been dealing with taitanic data while learning data analysis.**","07435bcb":"This Dataset contains the information about Titanic ship and included 891Rows and 12 columns.In this notebook I try to find the best ML model to predict which passengers survived the Titanic shipwreck.    Variable Notes:\n\npclass: A proxy for socio-economic status (SES)(Ticket class) 1 = 1st, 2 = 2nd, 3 = 3rd\n\n1st = Upper\n\n2nd = Middle\n\n3rd = Lower\n\nage:Age in years\n\nSibsp: The dataset defines family relations in this way...(number of siblings \/ spouses aboard the Titanic)\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nParch: The dataset defines family relations in this way...(number of parents \/ children aboard the Titanic)\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.\n\nSurvival:0 = No, 1 = Yes\n\nSex:Sex\n\nTicket:Ticket number\n\nFare:Passenger fare\n\nCabin:Cabin number\n\nEmbarked:Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)","b4b32f36":"# **Checking the missing data**:\n### The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values. Now I want to identify the rows with the most number of missing values and drop or transform them.","66936283":"# **Feature Engineering**","a28ea556":"# **Logestic regression**","c4f6b857":"### We see that the number of men and women who were survived is decreasing according to class.In addition,men and women in class 1 had a significantly higher chance of survival if they bought class 1 tickets."}}