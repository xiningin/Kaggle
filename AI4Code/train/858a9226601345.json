{"cell_type":{"fa76875a":"code","54f522eb":"code","1d27d73f":"code","5bb64e46":"code","97be59e9":"code","4bd1fed1":"code","9ecfcf08":"code","127c0a09":"code","2d221cb4":"code","a0dc6a11":"code","92bb705c":"code","49b57324":"code","89d56696":"code","2101d1b3":"code","2a6a2846":"code","ae5b4e6d":"code","dd2be80c":"code","7069ef35":"code","5674d18c":"code","90577328":"code","21eac4ea":"code","b0ddca1b":"code","15d6382d":"code","3e1f9d81":"code","2e804ce1":"code","640e7a00":"code","685709ec":"code","18d32d52":"code","fae775e7":"code","7c818575":"code","b983956e":"code","c75a3903":"code","be149912":"code","338d2033":"code","d0008212":"code","d6debc5e":"markdown","e24c864f":"markdown","40ad28dc":"markdown","223a14cc":"markdown","83cbb746":"markdown","62e705bc":"markdown","1ffca0b4":"markdown","983a76ec":"markdown","c8e4f44f":"markdown","a401b671":"markdown","4ad49e51":"markdown","7ed1d36f":"markdown","76e07763":"markdown","db3bd075":"markdown","2d1f8d03":"markdown","d3adf384":"markdown","2d9924de":"markdown"},"source":{"fa76875a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm,skew\nfrom matplotlib.pyplot import figure\nfrom sklearn.linear_model import LinearRegression","54f522eb":"df=pd.read_csv('..\/input\/insurance-premium-prediction\/insurance.csv')\ndf.head()\n","1d27d73f":"df.isna().sum()","5bb64e46":"df['expenses'].describe()\nsns.distplot(df['expenses']);\n\n#plot Skewness and Kurtosis\nprint(\"Skewness: %f\" % df['expenses'].skew())\nprint(\"Kurtosis: %f\" % df['expenses'].kurt())","97be59e9":"fig =plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df['expenses'], fit=norm);\n(mu,sigma)= norm.fit(df['expenses'])\nprint('\\n mu= {:.2f}\\n'.format(mu,sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=${:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('Distribution of Charges')\nplt.subplot(1,2,2)\nres=stats.probplot(df['expenses'],plot=plt)\nplt.suptitle('Before Transformation')\n\ndf.expenses=np.log1p(df.expenses)\ny=df.expenses.values\ny_orig=df.expenses\n\nfig=plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df['expenses'], fit=norm);\n(mu,sigma)=norm.fit(df['expenses'])\nprint(' \\n mu={:.2f} and sigma = {:.2f}\\n'.format(mu,sigma))\nplt.legend(['Normal dist. ($\\mu=${:.2f} and $\\sigma=$ {:.2f})'.format(mu,sigma)], loc='best')\nplt.title('Distribution of expenses')\nplt.subplot(1,2,2)\nres=stats.probplot(df['expenses'], plot=plt)\nplt.suptitle('After Transformation')\n","4bd1fed1":"df","9ecfcf08":"#create a dictionary to map the features\nbin_dict = {'yes' :1, 'no': 0}\nbin_dict_2={'female':1, 'male':0}\n#map the category values in our dict\ndf['smoker']= df['smoker'].map(bin_dict)\ndf['sex']=df['sex'].map(bin_dict_2)\n#check if it has been converted\ndf.head()","127c0a09":"cat_dict = {'southwest':1,'southeast':2, 'northwest':3, 'northeast':4}\n#map the category values in our dict\n\ndf['region']=df['region'].map(cat_dict)","2d221cb4":"df.head()","a0dc6a11":"df.duplicated().sum()","92bb705c":"df=df.drop_duplicates()","49b57324":"df.age.plot(kind=\"hist\")","89d56696":"df.smoker.plot(kind=\"hist\")","2101d1b3":"df.children.plot(kind=\"hist\")","2a6a2846":"plt.figure(figsize = (12,8))\ng = sns.countplot(x=\"age\",data=df)\ng.set_title(\"different age groups\", fontsize=20)\ng.set_xlabel(\"age\", fontsize=15)\ng.set_ylabel(\"count\", fontsize=20)","ae5b4e6d":"df.region.plot(kind=\"hist\")","dd2be80c":"# finding correlation\ndf.corr()\ncorrMatrix = df.corr()\nsns.heatmap(corrMatrix,annot=True)","7069ef35":"vars = df.columns\n# vars = numerical_features\nfigures_per_time = 4\ncount = 0 \ny = df['expenses']\nfor var in vars:\n    x = df[var]\n    # print(y.shape,x.shape)\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,4)+1)\n    plt.scatter(x, y);\n    plt.title('f model: T= {}'.format(var))\n    count+=1","5674d18c":"sns.scatterplot(x=df['bmi'], y=df['expenses'], hue=df['smoker'])","90577328":"sns.lmplot(x=\"bmi\", y=\"expenses\", hue=\"smoker\", data=df)","21eac4ea":"cat_df=['sex', 'region', 'smoker']\nnum_df=['age','bmi','children', 'expenses']\n","b0ddca1b":"dum_sex = pd.get_dummies(df.sex)\ndum_sex.columns = ['female', 'male']\ndum_region = pd.get_dummies(df.region)\ndum_region.columns = ['southwest','southeast','northwest','northeast']\ndum_smoker = pd.get_dummies(df.smoker)\ndum_smoker.columns = ['smokeryes','smokerno']\ndummies = pd.concat([df, dum_sex,dum_region,dum_smoker],axis='columns')\ndf = dummies.drop(['sex','smoker','region'],axis='columns')\n","15d6382d":"df.shape","3e1f9d81":"from sklearn.model_selection import train_test_split","2e804ce1":"x=df.drop(columns='expenses')\ny=df[['expenses']]\nx_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.30,random_state=17)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","640e7a00":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer, mean_squared_error, r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost.sklearn import XGBRegressor\nfrom lightgbm import LGBMRegressor","685709ec":"k_folds = KFold(n_splits =18, shuffle=True, random_state=42)","18d32d52":"lightgbm = LGBMRegressor(objective='regression',num_leaves=4,learning_rate=0.01, n_extimators=9000,max_bin=200,bagging_fraction=0.75,bagging_seed=7,feature_fraction=0.2,feature_fraction_seed=7,verbose=-1)","fae775e7":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt,cv=k_folds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7,alphas=alphas2, random_state=42, cv=k_folds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                                                        alphas=e_alphas, cv=k_folds,l1_ratio=e_l1ratio))\nstack_gen = StackingCVRegressor(regressors=(ridge,lasso,elasticnet,lightgbm), meta_regressor=elasticnet,use_features_in_secondary=True)\n\nsvr= make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))","7c818575":"print('Elasticnet')\nelastic_model = elasticnet.fit(x_train, y_train)\nprint('Lasso')\nlasso_model = lasso.fit(x_train, y_train)\nprint('Ridge')\nridge_model = ridge.fit(x_train, y_train)\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(x_train, y_train)\nprint('Svr')\nsvr_model_full_data = svr.fit(x_train, y_train)\nprint('Stack_gen_model')\nstack_gen_model=stack_gen.fit(np.array(x_train), np.array(y_train))\n","b983956e":"def blend_models_predict(X):\n    return ((0.167 * elastic_model.predict(X)) + \\\n            (0.167 * lasso_model.predict(X)) + \\\n            (0.167 * ridge_model.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.30 * stack_gen_model.predict(np.array(X))))","c75a3903":"model = LinearRegression()\nmodel.fit(x_train,y_train)\ntrain_predict=model.predict(x_train)\ntest_predict=model.predict(x_test)\n\n\n","be149912":"blend_train=blend_models_predict(x_train)\nblend_train=np.expm1(blend_train.mean(axis=0))\nblend_test=blend_models_predict(x_test)\nblend_test=np.expm1(blend_test.mean(axis=0))","338d2033":"print(\"Predicting the train data\")\nprint(\"Predicting the test data\")\nprint(\"MAE\")\nprint(\"Train : \",mean_absolute_error(np.expm1(y_train),blend_train))\nprint(\"Test  : \",mean_absolute_error(np.expm1(y_test),blend_test))\nprint(\"Train_LR : \",mean_absolute_error(np.expm1(y_train),np.expm1(train_predict)))\nprint(\"Test_LR  : \",mean_absolute_error(np.expm1(y_test),np.expm1(test_predict)))\nprint(\"====================================\")\nprint(\"MSE\")\nprint(\"Train : \",mean_squared_error(np.expm1(y_train),blend_train))\nprint(\"Test  : \",mean_squared_error(np.expm1(y_test),blend_test))\nprint(\"Train_LR : \",mean_squared_error(np.expm1(y_train),np.expm1(train_predict)))\nprint(\"Test_LR  : \",mean_squared_error(np.expm1(y_test),np.expm1(test_predict)))\nprint(\"====================================\")\nprint(\"RMSE\")\nprint(\"Train : \",np.sqrt(mean_squared_error(np.expm1(y_train),blend_train)))\nprint(\"Test  : \",np.sqrt(mean_squared_error(np.expm1(y_test),blend_test)))\nprint(\"Train_LR : \",np.sqrt(mean_squared_error(np.expm1(y_train),np.expm1(train_predict))))\nprint(\"Test_LR  : \",np.sqrt(mean_squared_error(np.expm1(y_test),np.expm1(test_predict))))\nprint(\"====================================\")\nprint(\"R^2\")\nprint(\"Train : \",r2_score(np.expm1(y_train),blend_train))\nprint(\"Test  : \",r2_score(np.expm1(y_test),blend_test))\nprint(\"Train_LR : \",r2_score(y_train,train_predict))\nprint(\"Test_LR  : \",r2_score(y_test,test_predict))\n","d0008212":"plt.figure(figsize=(10,7))\nplt.title(\"Actual vs. predicted expenses\",fontsize=25)\nplt.xlabel(\"Actual expenses\",fontsize=18)\nplt.ylabel(\"Predicted expenses\", fontsize=18)\nplt.scatter(x=np.expm1(y_test),y=blend_test, c='b', label='Blended')\nplt.scatter(x=np.expm1(y_test),y=np.expm1(test_predict), c='r', label='LinReg')\nplt.plot([0,80000], [0,80000], '-g', label='perfect')\nplt.legend(loc='upper left')\nplt.show()\n\n\n","d6debc5e":"**Linear Regression**","e24c864f":"It is clear that smokers have a higher correlation to charges than those who arent","40ad28dc":"# Validating model","223a14cc":"# Preparing Data","83cbb746":"We will try to understand our database and its correlation towards each other and the target features","62e705bc":"Ridge Lasso elasticnet ","1ffca0b4":"I will try to compare against Logistic regression in time to come. Thank you for reading my kernel. please let me know how i can improve. ","983a76ec":"From what we can see, the target feature is postively skewed. to better predict we can actually perform a box-cox transformation","c8e4f44f":"LightGBM","a401b671":"conclusion: the location is evenly spread out.\n","4ad49e51":"Defining variables","7ed1d36f":"There is no null items inside of the data set","76e07763":"# Feature analysis ","db3bd075":"it looks better for our prediction","2d1f8d03":"# Modelling","d3adf384":"we can see that both age and BMI are indeed correlated however for BMI it seems that there is no correlation","2d9924de":"check for duplicate entries and remove them"}}