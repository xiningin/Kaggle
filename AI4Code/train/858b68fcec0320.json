{"cell_type":{"3baf2735":"code","a2bd636a":"code","d285337e":"code","637d9a89":"code","cfc767a6":"code","f08efb06":"code","c627dd00":"code","0d18d486":"code","ffebc58c":"code","41b67246":"code","f8b96bc0":"code","fa81be7c":"code","e5e2f277":"code","04372e5c":"code","201fc18b":"code","86a84c1a":"code","5e59e068":"code","30dd50ef":"code","3bb1bb0b":"code","62f7d2ef":"code","4ebee6ab":"code","75c33077":"code","587a6a1d":"code","2cefc63b":"code","c02aba09":"code","bad9a8e5":"code","5f509da6":"code","c6915261":"code","f25e6385":"code","493b7fa0":"code","e9decf1c":"code","47d89a3a":"code","2c153473":"code","17332005":"code","0f78bfc5":"code","a09cba38":"code","b02ca1a3":"code","c2dd116d":"code","376ecd43":"code","aeee46c5":"code","735f0499":"code","35234891":"code","ff67d009":"markdown","009a8cd0":"markdown","122a8721":"markdown","d4154046":"markdown","55b6c780":"markdown","2d80c19e":"markdown","ea5a23c7":"markdown","8a5568e0":"markdown","2786c72f":"markdown"},"source":{"3baf2735":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2bd636a":"# Read data\nimport pandas as pd                   # Linear Algebra (calculate the mean and standard deviation)\nimport numpy as np                    # manipulate data, data processing, load csv file I\/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib.pyplot as plt       # Visualization using matplotlib\nimport seaborn as sns                 # Visualization using seaborn\n\n# style\nplt.style.use(\"fivethirtyeight\")      # Set Graphs Background style using matplotlib\nsns.set_style(\"darkgrid\")             # Set Graphs Background style using seabornsns.set()\n\nimport warnings                       # To ignore any warnings\nwarnings.filterwarnings('ignore')","d285337e":"train = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')\ntest = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx')","637d9a89":"display(train.head())\ndisplay(test.head())","cfc767a6":"display(train.shape)\ndisplay(test.shape)","f08efb06":"display(train.info())\ndisplay(test.info())","c627dd00":"# Let\u2019s append the train and test data\ndf = train.append(test,sort=False)\ndf.head()","0d18d486":"df.shape","ffebc58c":"# Check the datatypes\ndf.dtypes","41b67246":"# Split Date_of_Journey column into date, month, year\ndf['Date'] = df['Date_of_Journey'].str.split('\/').str[0]\ndf['Month'] = df['Date_of_Journey'].str.split('\/').str[1]\ndf['Year'] = df['Date_of_Journey'].str.split('\/').str[2]","f8b96bc0":"df.head()","fa81be7c":"# Let\u2019s change the data dtype of date, month, year object to an integer.\ndf['Date'] = df['Date'].astype(int)\ndf['Month'] = df['Month'].astype(int)\ndf['Year'] = df['Year'].astype(int)","e5e2f277":"# Check the datatypes\ndf.dtypes","04372e5c":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n\ndf.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","201fc18b":"df.head()","86a84c1a":"df[df['Total_Stops'].isnull()]","5e59e068":"# We can see the null value is present is tota_stops columns, so we can fill that assuming as at least 1 stop.\ndf['Total_Stops'] = df['Total_Stops'].fillna('1 stop')","30dd50ef":"# Also, let\u2019s replace non stop values as 0 stop\ndf['Total_Stops'] = df['Total_Stops'].replace('non-stop','0 stop')","3bb1bb0b":"# Splits Departure_Hour and Departure_Minute to Dep_Time\ndf['Departure_Hour'] = df['Dep_Time'] .str.split(':').str[0]\ndf['Departure_Minute'] = df['Dep_Time'] .str.split(':').str[1]","62f7d2ef":"df.drop([\"Dep_Time\"], axis = 1, inplace = True)","4ebee6ab":"df.drop([\"Arrival_Time\"], axis = 1, inplace = True)","75c33077":"df.drop([\"Total_Stops\"], axis = 1, inplace = True)","587a6a1d":"df.head(3)","2cefc63b":"# Let\u2019s replace and split the Route as route_1,route_2,route_3,route_4,route_5\ndf['Route_1'] = df['Route'].str.split('\u2192 ').str[0]\ndf['Route_2'] = df['Route'].str.split('\u2192 ').str[1]\ndf['Route_3'] = df['Route'].str.split('\u2192 ').str[2]\ndf['Route_4'] = df['Route'].str.split('\u2192 ').str[3]\ndf['Route_5'] = df['Route'].str.split('\u2192 ').str[4]","c02aba09":"df['Price'].fillna((df['Price'].mean()),inplace=True)","bad9a8e5":"df['Route_1'].fillna(\"None\",inplace=True)\ndf['Route_2'].fillna(\"None\",inplace=True)\ndf['Route_3'].fillna(\"None\",inplace=True)\ndf['Route_4'].fillna(\"None\",inplace=True)\ndf['Route_5'].fillna(\"None\",inplace=True)","5f509da6":"# Now, drop the unwanted features\ndf = df.drop(['Route'],axis=1)\ndf = df.drop(['Duration'],axis=1)","c6915261":"df.head(3)","f25e6385":"df.isnull().sum()","493b7fa0":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ndf[\"Airline\"] = LE.fit_transform(df['Airline'])\ndf[\"Source\"] = LE.fit_transform(df['Source'])\ndf[\"Destination\"] = LE.fit_transform(df['Destination'])\ndf[\"Additional_Info\"] = LE.fit_transform(df['Additional_Info'])\ndf[\"Route_1\"] = LE.fit_transform(df['Route_1'])\ndf[\"Route_2\"] = LE.fit_transform(df['Route_2'])\ndf[\"Route_3\"] = LE.fit_transform(df['Route_3'])\ndf[\"Route_4\"] = LE.fit_transform(df['Route_4'])\ndf[\"Route_5\"] = LE.fit_transform(df['Route_5'])","e9decf1c":"df.head()","47d89a3a":"# Lasso used for regularization\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","2c153473":"train = df[0:10683]\ntest = df[10683:]","17332005":"# Splitting into Train and Test\nX = train.drop(['Price'],axis=1)\ny = train.Price","0f78bfc5":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\nmodel = SelectFromModel(Lasso(alpha=0.005,random_state=0))\nmodel.fit(X_train,y_train)\nmodel.get_support()\n\nselected_features = X_train.columns[(model.get_support())]","a09cba38":"from sklearn.model_selection import RandomizedSearchCV\n\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","b02ca1a3":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","c2dd116d":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()","376ecd43":"# cross-validation\n# Random search of parameters, using 3 fold cross validation, \n# search across 50 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n                               scoring='neg_mean_squared_error', n_iter = 50, cv = 5, verbose=2,\n                               random_state=42, n_jobs = 1)\nrf_random.fit(X_train,y_train)","aeee46c5":"# predict the model on test data set\ny_pred = rf_random.predict(X_test)","735f0499":"import seaborn as sns\n\nsns.distplot(y_test-y_pred)","35234891":"# Scatter plot on the predicted data point price\nplt.scatter(y_test,y_pred)","ff67d009":"### Dep_Time","009a8cd0":"<h2 style=\"color:blue\" align=\"left\"> 1. Import necessary Libraries <\/h2>","122a8721":"<h2 style=\"color:blue\" align=\"left\"> 3. EDA (Exploratory Data Analysis) <\/h2>\n\n- EDA is a way of **Visualizing, Summarizing and interpreting** the information that is **hidden in rows and column** format.\n\n\n- From description we can see that Date_of_Journey is a object data type. For this we require pandas **to_datetime** to convert object data type to datetime dtype.","d4154046":"### Route","55b6c780":"### Model Implementation\n#### Label Encoding","2d80c19e":"### Date_of_Journey","ea5a23c7":"<h2 style=\"color:blue\" align=\"left\"> 2. Load data <\/h2>","8a5568e0":"### RandomForestRegressor","2786c72f":"### Feature Selection\n- Before giving the feature to the model let\u2019s select important feature which will give us good accuracy."}}