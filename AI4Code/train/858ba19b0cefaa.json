{"cell_type":{"b36d1cc4":"code","639e33db":"code","86c08439":"code","5fa3aef5":"code","5f04c0a8":"code","999c7c76":"code","bbb690f0":"code","cabd7021":"code","68daec6c":"code","92dceeec":"code","b5512e82":"code","eae00822":"code","500c2f76":"code","e5a48e8c":"code","472458b0":"code","31058d1d":"code","78329c03":"code","debcfd5a":"markdown","8c72b69c":"markdown","842b5bcb":"markdown","b3487dd1":"markdown","115674b5":"markdown","51aa061b":"markdown","285e9c79":"markdown","a513f465":"markdown","23812172":"markdown","4143a356":"markdown","076958a4":"markdown","1345144b":"markdown","b64011cd":"markdown","9119e324":"markdown","c6549f21":"markdown","0de39675":"markdown","c78d71fe":"markdown","882ab4d4":"markdown","ec945974":"markdown","786af0ea":"markdown","656770fd":"markdown","aafa5d58":"markdown","f776f980":"markdown","4d01beab":"markdown","5d8a1e80":"markdown"},"source":{"b36d1cc4":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","639e33db":"import pandas as pd\n\ndata=pd.read_csv(r\"..\/input\/bbc-text.csv\")","86c08439":"data.head()","5fa3aef5":"data['category'].value_counts()","5f04c0a8":"from sklearn.preprocessing import LabelEncoder\ndf2 = pd.DataFrame()\ndf2[\"text\"] = data[\"text\"]\ndf2[\"label\"] = LabelEncoder().fit_transform(data[\"category\"])","999c7c76":"df2.head()","bbb690f0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df2[\"text\"].values, df2[\"label\"].values, test_size=0.2, random_state=42)","cabd7021":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","68daec6c":"import zipfile\n\nfolder = 'model_folder'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","92dceeec":"BERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{folder}\/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}\/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","b5512e82":"import os\nimport tensorflow as tf\n\ndef create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 50\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 100000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\nlabel_list = [str(num) for num in range(8)]\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(X_train, 'train', labels=y_train)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","eae00822":"import datetime\n\nprint('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))","500c2f76":"def input_fn_builder(features, seq_length, is_training, drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    print(params)\n    batch_size = 500\n\n    num_examples = len(features)\n\n    d = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"segment_ids\":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"label_ids\":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn","e5a48e8c":"predict_examples = create_examples(X_test, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","472458b0":"import numpy as np\npreds = []\nfor prediction in result:\n      preds.append(np.argmax(prediction['probabilities']))\nprint(len(preds))","31058d1d":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))","78329c03":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,preds))","debcfd5a":"## 1.1 Defination :","8c72b69c":"Download Bert Requirements","842b5bcb":"BBC articles dataset(2126 records) consist of two features text and the assiciated categories namely \n1. Sport \n2. Business \n3. Politics \n4. Tech \n5. Others\n\n**Our task is to train a multiclass classification model on the mentioned dataset.**","b3487dd1":">** Past Work mentioned on this dataset at max achieved 95.22 accuracies. BERT base model for the same, without any preprocessing and achieved 97.75 accuracies.**","115674b5":"# 2. Data Exploration","51aa061b":"### Step 2.5 Train BERT model","285e9c79":"## 1.4 Machine Learning Model Considered:","a513f465":"## 1.2 Problem Statement","23812172":"# 3. Implementation","4143a356":"https:\/\/www.kaggle.com\/thebrownviking20\/bert-multiclass-classification","076958a4":"**Accuracy** - Classification accuracy is the number of correct predictions made as a\nratio of all predictions made\n\n**Precision** - precision (also called positive predictive value) is the fraction of\nrelevant instances among the retrieved instances\n\n**F1_score** - considers both the precision and the recall of the test to compute the\nscore\n\n**Recall** \u2013 recall (also known as sensitivity) is the fraction of relevant instances that\nhave been retrieved over the total amount of relevant instances\n\n**Why these metrics?** - We took Accuracy, Precision, F1 Score and Recall as metrics\nfor evaluating our model because accuracy would give an estimate of correct prediction. Precision would give us an estimate about the positive category predicted value i.e. how much our model is giving relevant result. F1 Score gives a clubbed estimate of precision and recall.Recall would provide us the relevant positive category prediction to the false negative and true positive category recognition results.","1345144b":"### Step 2.6 Prediction on Test Dataset","b64011cd":"# 5. Future Improvements on this kernel:","9119e324":"> This kernel is based on the work of https:\/\/www.kaggle.com\/thebrownviking20\/bert-multiclass-classification","c6549f21":"# 1. Kernel Overview","0de39675":"### Step 2.4 Setting up BERT Configurations","c78d71fe":"# 4. Results","882ab4d4":"## 1.3 Metrics","ec945974":"* Explore preprocessing steps on data.\n* Explore other models as baseline.\n* Make this notebook more informative and illustrative.\n* Explaination on Bert Model.\n* More time on data exploration\nand many more...","786af0ea":"We will be using **BERT Base uncased model** for this use case. \n\nBert model working is not in the scope of this kernal. Kindly refer other external sources.","656770fd":"### Step 2.3 Divide dataset to test and train dataset","aafa5d58":"In today world** Text Classification\/Segmentation\/Categorization** (for example ticket categorization in a call centre, email classification, logs category detection etc.) is a common task. With humongous data out there, its nearly impossible to do this manually. Let's try to solve this problem automatically using machine learning and natural language processing tools.","f776f980":"### Step 2.2 Map Textual labels to numeric using Label Encoder","4d01beab":"# 6. References","5d8a1e80":"### Step 2.1 Load Dataset"}}