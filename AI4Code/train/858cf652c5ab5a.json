{"cell_type":{"0a07de1d":"code","8febd913":"code","3a6e565d":"code","bf15fd28":"code","97037247":"code","ec25d0e5":"code","7ea5aaf7":"code","13050033":"code","9ea7b68f":"code","a34ba6c0":"code","d624800e":"code","598fccf7":"code","85cbacad":"code","38f703c0":"code","332aadc4":"code","f9489bca":"code","da5aa4e0":"code","cea19f35":"code","47171183":"code","f8616462":"code","61b8860f":"code","6c002d0f":"code","1e453aa2":"code","3fb44a1d":"code","413f78a6":"code","47b74066":"code","44d4d501":"code","0b2a0f59":"code","b94f59f2":"code","d6c50044":"code","290b3828":"code","03cea34a":"code","b7ef7029":"code","7a3ff45b":"markdown","1686d358":"markdown","f9239874":"markdown","b755afdd":"markdown","5cab8d6a":"markdown","ac00f8d2":"markdown","195957e7":"markdown","ab7efd3d":"markdown","0268a6e3":"markdown","f8cee1e4":"markdown","471ca3cd":"markdown","b169f586":"markdown","50782872":"markdown","dd7b40e2":"markdown","1d696994":"markdown","df796096":"markdown","74c33ecb":"markdown"},"source":{"0a07de1d":"import os\nimport cv2\nimport time\nimport json\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n\n\nfrom PIL import Image\nfrom enum import Enum\nfrom glob import glob\nfrom tqdm import trange\nfrom pprint import pprint\nfrom colorama import Fore\nfrom tqdm import tqdm_notebook\nfrom sklearn.utils import shuffle\nfrom IPython.display import display\n\n\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\n\n\n# Neural Network Models\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import Input, Concatenate, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Dropout\n\n\n# Regression Models\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor,VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n\nwarnings.filterwarnings(\"ignore\")\nAUTOTUNE = tf.data.experimental.AUTOTUNE","8febd913":"# Set random seed to enable re-production\n\ndef set_random():\n    np.random.seed(100)\n    random.seed(100)\n    tf.random.set_seed(100)\n    os.environ['PYTHONHASHSEED'] = '100'","3a6e565d":"# Generate data folds for cross validation\n\ndef fold_generator(data, target='Pawpularity', cv = 5):\n    \n    # Fold generator\n    kf = StratifiedKFold(n_splits=cv)\n    \n    # Shuffle the dataset to generate folds\n    # Since the Pawpularity ranges from 1 to 100, we classify the scores into 10 levels\n    data = shuffle(data).reset_index(drop=True)\n    n_bins = 10\n    data['bins'] = data[target].map(lambda x: int(x\/10))\n    \n    # Stratified sampling to construct folds\n    for fid, (train_idx, valid_idx) in enumerate(kf.split(X=data, y=data['bins'])):\n        data.loc[valid_idx, 'Fold'] = fid\n        \n    data = data.drop([\"bins\"], axis = 1)         \n    return data ","bf15fd28":"# Self-defined cross validation function\n\ndef cross_valid(df, model, cv = 5):\n    \n    X = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n    Y = 'Pawpularity'\n\n    r2sr_train = 0\n    r2sr_valid = 0\n    rmse_train = 0\n    rmse_valid = 0\n    \n    # cv: the number of folds\n    for fold in range(cv):\n        train_df = df.loc[df['Fold'] != fold].reset_index(drop = True)\n        valid_df = df.loc[df['Fold'] == fold].reset_index(drop = True)\n\n        train_X = train_df[X]\n        train_Y = train_df[Y]\n\n        valid_X = valid_df[X]\n        valid_Y = valid_df[Y]\n        \n        if type(model).__name__ == 'CatBoostRegressor':\n            model.fit(train_X, train_Y, verbose=False)\n        else:\n            model.fit(train_X, train_Y)\n\n        train_pred = model.predict(train_X)\n        valid_pred = model.predict(valid_X)\n\n        rmse_train += np.sqrt(mean_squared_error(train_Y, train_pred))\n        rmse_valid += np.sqrt(mean_squared_error(valid_Y, valid_pred))\n        \n    return rmse_train\/cv, rmse_valid\/cv","97037247":"# Reshape image\n\ndef image_reshape(image_path, image_size):\n    # Load image\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    \n    # Resize to regularize the input shape\n    img = tf.image.resize(img, (image_size, image_size))\n    \n    # Normalize channels to range (0, 1)\n    img = tf.cast(img, tf.float32) \/ 255.0\n    \n    return img","ec25d0e5":"# Reshape image\n\ndef load_image(is_trainset):\n    def just_reshape(img_path):\n        img = tf.io.read_file(img_path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.cast(img, tf.float32)\n        img = tf.image.resize(img, (224, 224))\n        img = tf.keras.applications.efficientnet.preprocess_input(img) \n        return img\n    \n    def reshape_with_label(img_path, label):\n        return just_reshape(img_path), label\n\n    return reshape_with_label if is_trainset else just_reshape","7ea5aaf7":"# Image augmentation\n\ndef augment_image(is_trainset):\n    def just_augment(img):\n        img = tf.image.random_flip_left_right(img)\n        # img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        return img\n    \n    def augment_with_label(img, label):\n        return just_augment(img), label\n    \n    return augment_with_label if is_trainset else just_augment","13050033":"# Use TensorFlow Database to speed up dataset processing\n\ndef construct_tf_dataset(df, batch_size, is_trainset = False, \n                         use_augmentation = False, use_shuffle = False):\n    load_image_function = load_image(is_trainset)\n    augment_image_function = augment_image(is_trainset)\n    \n    if is_trainset:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Path'].values, df['Pawpularity'].values))\n        dataset = dataset.map(load_image_function, num_parallel_calls=AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Path'].values))\n        dataset = dataset.map(load_image_function, num_parallel_calls=AUTOTUNE)\n        \n    if use_augmentation:\n        dataset = dataset.map(augment_image_function, num_parallel_calls=AUTOTUNE)\n    \n    if use_shuffle:\n        dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n        \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","9ea7b68f":"# Set random seed to enable re-production\n\nset_random()","a34ba6c0":"# Dataset path\n\ntrain_csv = \"..\/input\/petfinder-pawpularity-score\/train.csv\"\ntest_csv = \"..\/input\/petfinder-pawpularity-score\/test.csv\"\nsubmission_csv = \"..\/input\/petfinder-pawpularity-score\/submission.csv\"\n\ntrain_dir = \"..\/input\/petfinder-pawpularity-score\/train\"\ntest_dir = \"..\/input\/petfinder-pawpularity-score\/test\"","d624800e":"# Load csv and image path to dataframe\n\ndata_df = pd.read_csv(train_csv)\ntest_df = pd.read_csv(test_csv)\n\ndata_df['Path'] = data_df['Id'].apply(lambda x : train_dir + '\/' + x + '.jpg')\ntest_df['Path'] = test_df['Id'].apply(lambda x : test_dir + '\/' + x + '.jpg')","598fccf7":"# Test\n\ndisplay(data_df)","85cbacad":"# Cross validation fold generation\n\ndata_df_cv = fold_generator(data_df, target = 'Pawpularity', cv = 5)","38f703c0":"# Test\n\ndisplay(data_df_cv)","332aadc4":"# # Result set\n\n# Result_set = {\n#     \"Model\" : [],\n#     \"rmse_train\" : [],\n#     \"rmse_valid\" : []\n# }","f9489bca":"# # def trainRegModels(df : \"data_file\", features : list, label: str):\n    \n# Model_dict = {\n#     \"Ridge\": Ridge(),\n#     \"ElasticNet\": ElasticNet(),\n#     \"XGBRegressor\": XGBRegressor(),\n#     \"LGBMRegressor\": LGBMRegressor(),\n#     \"LinearRegression\": LinearRegression(),\n#     \"BaggingRegressor\": BaggingRegressor(),\n#     \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n#     \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n#     \"RandomForestRegressor\": RandomForestRegressor(),\n#     \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n#     \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n#     \"AdaBoostRegressor\": AdaBoostRegressor(n_estimators=100),\n#     \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=5),\n#     \"CatBoostRegressor\": CatBoostRegressor(iterations=200, learning_rate=0.01),\n#     }\n\n# Model_list = list(Model_dict.keys())\n\n# for i in tqdm_notebook(range(len(Model_list))):\n#     model_name = Model_list[i]\n#     model = Model_dict[model_name]\n    \n#     rmse_train, rmse_valid = cross_valid(data_df_cv, model, cv = 5)\n\n#     Result_set[\"Model\"].append(model_name)\n#     Result_set[\"rmse_train\"].append(rmse_train)\n#     Result_set[\"rmse_valid\"].append(rmse_valid)","da5aa4e0":"# # Display result\n\n# Result_df = pd.DataFrame(Result_set)\n# Result_df.sort_values(\"rmse_valid\", axis = 0, ascending = True)","cea19f35":"# # Try the VotingRegressor to vote a prediction over the top 3 models\n\n# cbr = CatBoostRegressor(iterations=200, learning_rate=0.01)\n# gbr = GradientBoostingRegressor()\n# xgbr = XGBRegressor()\n\n# vr = VotingRegressor([('cbr', cbr),('gbr', gbr), ('xgbr', xgbr)], n_jobs=-1)\n\n# rmse_train, rmse_valid = cross_valid(data_df_cv, vr, cv = 5)\n\n# Result_set[\"Model\"].append('VotingRegressor')\n# Result_set[\"rmse_train\"].append(rmse_train)\n# Result_set[\"rmse_valid\"].append(rmse_valid)","47171183":"# # Display result again\n\n# Result_df = pd.DataFrame(Result_set)\n# Result_df.sort_values(\"rmse_valid\", axis = 0, ascending = True)","f8616462":"# # Reshape image\n\n# image_size = 128\n# x_data = []\n# x_test = []\n\n# for i, img in tqdm_notebook(enumerate(data_df['Path']), total=len(data_df)):\n#     x_data.append(image_reshape(img, image_size))\n    \n# for i, img in tqdm_notebook(enumerate(test_df['Path']), total=len(test_df)):\n#     x_test.append(image_reshape(img, image_size))","61b8860f":"# # Generate train and valid set\n\n# x_data = np.array(x_data)\n# x_test = np.array(x_test)\n# y_data = data_df['Pawpularity']\n\n# trn_X, val_X, trn_Y, val_Y = train_test_split(x_data, y_data, test_size=0.2)","6c002d0f":"# # Build CNN model\n\n# K.clear_session()\n\n# nn = Sequential()\n# nn.add(Conv2D(8 , (3,3), (2,2), activation='relu', padding='same', input_shape=(128,128,3)))\n# nn.add(Conv2D(16, (3,3), (2,2), activation='relu', padding='same'))\n# nn.add(Conv2D(32, (3,3), (2,2), activation='relu', padding='same'))\n# nn.add(Flatten())\n# nn.add(Dense(units=128, activation='relu'))\n# nn.add(Dropout(rate=0.5, seed=2021))\n# nn.add(Dense(units=1))\n\n# nn.summary()","1e453aa2":"# # Early stopping callback function\n\n# early_stop = tf.keras.callbacks.EarlyStopping(patience=5)\n# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, min_lr=1e-9)\n\n# callbacks = [early_stop, reduce_lr]","3fb44a1d":"# # Train CNN model\n\n# nn.compile(loss='mse', optimizer='Adam', \n#            metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"])\n\n# history = nn.fit(trn_X, trn_Y, epochs=25, batch_size=64, \n#                  validation_data = (val_X, val_Y), callbacks=callbacks)","413f78a6":"# cbr_cnn = CatBoostRegressor(iterations=200, learning_rate=0.01)\n# _, _ = cross_valid(data_df_cv, cbr_cnn, cv = 5)","47b74066":"# Self-defined cross validation function\n\nfold_number = 1\nbatch_size = 64\n\ntrn = data_df_cv.loc[data_df_cv['Fold'] != fold_number].reset_index(drop = True)\nval = data_df_cv.loc[data_df_cv['Fold'] == fold_number].reset_index(drop = True)\n\ntrain_set = construct_tf_dataset(trn, batch_size, is_trainset = True, use_augmentation = True, use_shuffle = True)\nvalid_set = construct_tf_dataset(val, batch_size, is_trainset = True, use_augmentation = False, use_shuffle = False)\ntest_set  = construct_tf_dataset(test_df, batch_size, is_trainset = False, use_augmentation = False, use_shuffle = False)","44d4d501":"# Load pre-trained model: EfficientNet\n\nEfficientNet_model = \"..\/input\/keras-applications-models\/EfficientNetB0.h5\"\nEfficientNet_model = tf.keras.models.load_model(EfficientNet_model)\nEfficientNet_model.trainable = False","0b2a0f59":"# Construct model on the top of the pre-trained EfficientNet with ReLu as the final layer (for regression)\n\ntransfer_nn = Sequential([\n    Input(shape=(224, 224, 3)),\n    EfficientNet_model,\n    BatchNormalization(),\n    Dropout(0.2),\n    Dense(units=64, activation=\"relu\"),\n    Dense(units=1, activation=\"relu\")\n])","b94f59f2":"# Early stopp parameters\n\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\nreduce_lr = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=100, decay_rate=0.96, staircase=True)","d6c50044":"# Train model\n\ntransfer_nn.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=reduce_lr), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\nhistory = transfer_nn.fit(train_set, epochs=25, validation_data=valid_set, callbacks=[early_stop])","290b3828":"# # Prediction, use model vr\n\n# X = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n\n# test_X = test_df[X]\n# test_X['Pawpularity'] = vr.predict(test_X)\n# test_X['Id'] = test_df['Id']\n\n# submission_vr = test_X[['Id', 'Pawpularity']]\n# submission_vr.to_csv(\"method-1-submission.csv\", index = False)\n\n# print(submission_vr)","03cea34a":"# # Prediction, use model nn\n\n# X = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n# test_X = test_df[X]\n\n# pred_cnn = nn.predict(x_test, verbose = False)\n# pred_cbr_cnn = cbr_cnn.predict(test_X).reshape(8, 1)\n\n# submission_cnn = pd.DataFrame()\n# submission_cnn['Id'] = test_df['Id']\n# submission_cnn['Pawpularity'] = (pred_cnn + pred_cbr_cnn)\/2.0\n\n# submission_cnn.to_csv(\"method-2-submission.csv\", index = False)\n\n# print(submission_cnn)","b7ef7029":"pred_transfer_nn = transfer_nn.predict(test_set)\n\nsubmission_transfer_nn = pd.DataFrame()\nsubmission_transfer_nn['Id'] = test_df['Id']\nsubmission_transfer_nn['Pawpularity'] = pred_transfer_nn\n\nsubmission_transfer_nn.to_csv(\"submission.csv\", index = False)\n\nprint(submission_transfer_nn)","7a3ff45b":"## Groups\nGroup projects should contain 2 students.  To sign up for a group, go to Canvas and under \"People\", join one of the existing \"Project Groups\".  _For group projects, the project report must state the percentage contribution from each project member._\n\n## Methodology\nYou are free to choose the methodology to solve the task.  In machine learning, it is important to use domain knowledge to help solve the problem.  Hence, instead of blindly applying the algorithms to the data you need to think about how to represent the data in a way that makes sense for the algorithm to solve the task. \n\n## Kaggle: Kaggle Notebooks\n\nThe Kaggle competitions have Kaggle Notebooks enabled, which provide free GPU\/TPU computing resources (up to a limit).  You can develop your model in the Kaggle Notebook, CS5489 JupyterHub, or on your own computers.\n\n## Kaggle: Evaluation on Kaggle\n\nFor Kaggle projects, the final evaluation will be performed on Kaggle. Note that for these competitions you need to submit your code via the Kaggle Notebook, which will then generate the submission file for processing. \n\n## Project Presentation\n\nEach project group needs to give a presentation at the end of the semester.  You will record your presentation and upload it to FlipGrid.  The presentation is limited to 5 minutes.  You _must_ give a presentation. See the details in the \"Project Presentations\" Canvas assignment.\n\n## What to hand in\n\nYou need to turn in the following things.\n\nThe following files should be uploaded to \"Course Project\" on Canvas:\n\n1. This ipynb file `CourseProject-2021A.ipynb` with your source code and documentation. **You should write about all the various attempts that you make to find a good solution.** You may also submit .py files, but your documentation should be in the ipynb file.\n2. A PDF version of your ipynb file.\n3. Presentation slides.\n4. (Kaggle projects) Your final submission file to Kaggle.\n5. (Kaggle projects) A downloaded copy of your Kaggle Notebook that is submitted to Kaggle. This file should contain the code that generates the final submission file on Kaggle. This code will be used to verify that your Kaggle submission is reproducible.\n\nOther things that need to be turned in:\n- Upload your Project presentation to FlipGrid and the submit the URL to the \"Project Presentations\" assignment on Canvas.  See the detailed instructions in the assignment.\n- Enter the percentage contribution for each project member using the \"Project Group Contribution\" assignment on Canvas.\n- (Student-defined projects) submit your project proposal to the \"Project Proposal\" assignment on Canvas. The project proposal is due 1 week after the course project is released. Kaggle projects do not need to submit a proposal.\n\n\n\n## Grading\nThe marks of the assignment are distributed as follows:\n- 40% - Results using various feature representations, dimensionality reduction methods, classifiers, etc.\n- 25% - Trying out feature representations (e.g. adding additional features, combining features from different sources) or methods not used in the tutorials.\n- 15% - Quality of the written report.  More points for insightful observations and analysis.\n- 15% - Project presentation\n- 5% - (Kaggle projects) Final ranking on the Kaggle test data, or (student-defined projects) Project proposal.\n\n**Late Penalty:** 25 marks will be subtracted for each day late.\n\n**Group contribution:** marks for a group member with less than equal contribution will be deducted according to the following formula:\n- Let A% and B% be the percentage contributions for group members Alice and Bob. A%+B%=100%\n- Let x be the group project marks.\n- If A>B, then Bob's marks will be reduced to be: x*B\/A\n<hr>","1686d358":"# Part 3. Prediction","f9239874":"## Method 3: Advanced Method -  Transfer Learning (data augmentation + use pre-trained network)","b755afdd":"# **Name:** \\_\\_\\_\\_\\_\n\n**EIDs:** \\_\\_\\_\\_\\_\n\n**Kaggle Competition:** \\_\\_\\_\\_\n\n**Kaggle Team Name:** \\_\\_\\_\\_\\_\n","5cab8d6a":"## Pre-processing","ac00f8d2":"## Method 2: Advanced Method  - CNN (use origin image data + meta data + CNN)","195957e7":"## Prediction 1: Baseline Method - Regression (use meta data + traditional regression model)","ab7efd3d":"# Part 4. Reflection, Justification and Conclusion","0268a6e3":"# YOUR METHODS HERE","f8cee1e4":"## Prediction 2: Advanced Method - CNN (use origin image data + meta data + CNN)","471ca3cd":"## Method 1: Baseline Method - Regression (use meta data + traditional regression model)","b169f586":"## Prediction 3: Advanced Method -  Transfer Learning (use origin image data + data augmentation + use pre-trained network)","50782872":"# Part 2.  Methods and Experiment","dd7b40e2":"# Part 1. Import Module and Help Functions","1d696994":"# Part 1. Dataset and Feature Exploration","df796096":"## Possible Projects\n\nFor the course project, you may select **one** of the following competitions on Kaggle **or** define your own course project:\n\n### [PetFinder.my - Pawpularity Contest](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score): Predict the popularity of shelter pet photos\n\n> A picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo\u2019s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.\n>\n>PetFinder.my is Malaysia\u2019s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n>\n>Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n>\n>In this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.\n>\n>If successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.\n>\n>Top participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.\n\n### [G-Research Crypto Forecasting](https:\/\/www.kaggle.com\/c\/g-research-crypto-forecasting): Use your ML expertise to predict real crypto market data\n\n>Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?\n>\n>In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.\n>\n>The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.\n>\n>G-Research is Europe\u2019s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. \n\n### Student-defined Course Project \n\nThe goal of the student-defined project is to get some hands-on experience using the course material on your own research problems. Keep in mind that there will only be about 4 weeks to do the project, so the scope should not be too large. Following the major themes of the course, here are some general topics for the project:\n- _regression_ (supervised learning) - use regression methods (e.g. ridge regression, Gaussian processes) to model data or predict from data.\n- _classification_ (supervised learning) - use classification methods (e.g., SVM, BDR, Logistic Regression, NNs) to learn to distinguish between multiple classes given a feature vector.\n- _clustering_ (unsupervised learning) - use clustering methods (e.g., K-means, EM, Mean-Shift) to discover the natural groups in data.\n- _visualization_ (unsupervised learning) - use dimensionality reduction methods (e.g., PCA, kernel-PCA, non-linear embedding) to visualize the structure of high-dimensional data.\n \nYou can pick any one of these topics and apply them to your own problem\/data. \n\n- *Can my project be my recently submitted or soon-to-be submitted paper?* If you plan to just\nturn in the results from your paper, then the answer is no. The project cannot be be work\nthat you have already done. However, your course project can be based on extending your\nwork. For example, you can try some models introduced in the course on your data\/problem.\n\nBefore actually doing the project, you need to write a **project proposal** so that we can make sure the project is doable within the 3-4 weeks. I can also give you some pointers to relevant methods, if necessary.  \n- The project proposal should be at most one page with the following contents: 1) an introduction that briefy states the problem; 2) a precise description of what you plan to do - e.g., What types of features do you plan to use? What algorithms do you plan to use? What dataset will you use? How will you evaluate your results? How do you define a good outcome for the project?\n- The goal of the proposal is to work out, in your head, what your project will be. Once the proposal is done, it is just a matter of implementation!\n- *You need to submit the project proposal to Canvas 1 week after the Course project is released.*","74c33ecb":"# CS5489 - Course Project (2021A)\n\nDue date: See canvas site."}}