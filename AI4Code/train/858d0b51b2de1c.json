{"cell_type":{"69c9a038":"code","6eeb8b7e":"code","61ee6bab":"code","76cbc379":"code","9490a42e":"code","61ac5159":"code","779868ae":"code","ddc1cf41":"code","d789db34":"markdown","a9950c5d":"markdown","0c69be4e":"markdown","a0382c68":"markdown","84d0c26d":"markdown","7fe7e5fc":"markdown"},"source":{"69c9a038":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom datetime import datetime\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6eeb8b7e":"df=pd.read_csv('\/kaggle\/input\/dutch-news-articles\/dutch-news-articles.csv')","61ee6bab":"#extract years from datetime data\ndf['datetime']=pd.to_datetime (df['datetime'])\ndf['year']=pd.DatetimeIndex(df['datetime']).year\n#manipulate with data to get a dataframe with number of articles by categories and years\nyear_category_df=df.groupby(['year','category']).nunique()\nyear_category_df.reset_index(inplace=True)\nyear_category_df=year_category_df.pivot(index='year',columns='category',values='url')\n#simply plot the pivot dataframe\nyear_category_df.plot(figsize=(30,10))","76cbc379":"#take 1\/10 of the content from random rows (time consuming...)\nrandom=np.random.randint(0,len(df.content), size=int(len(df.content)\/10))\n#take these rows from the content and sum it up\ntext = df.content[random].sum()\n\n#import packages and make a wordcloud\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)\nplt.figure(figsize=(40, 30))\nplt.imshow(wordcloud) \nplt.axis(\"off\")","9490a42e":"from nltk import FreqDist\nimport nltk\n#tokenize text into separate words\nwords = nltk.tokenize.word_tokenize(text)\n#make frequency dictionary of the words\nfdist = FreqDist(words)\n#transform data into df, drop dots and commas, calculate ratio of frequency of each word and plto the data\nfdf=pd.DataFrame(fdist, index=['freq']).transpose()\nfdf.sort_values(by='freq',ascending=False,inplace=True)\nfdf=fdf.drop(axis=0,labels=['.',','])\nfdf['ratio']=fdf['freq']\/len(words)\nfdf['word']=fdf.index\nfdf[0:20].plot(x='word',y='ratio',kind='bar',figsize=(10,10),title='20 most popular words ')\nplt.xticks(rotation=45)","61ac5159":"fdf.sort_values(by='freq')[['word','freq']].head(n=10)","779868ae":"fdf['length']=[len(fdf['word'][i]) for i in range(len(fdf['word']))]\n#drop outliers\nfdf1=fdf[fdf['length']<=100]\n#plot the scatterplot\nplt.figure(figsize=(20,10))\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.scatter(y=fdf1['freq'],x=fdf1['length'],c=fdf1['ratio'])","ddc1cf41":"fdf.reset_index(inplace=True)\nfdf.sort_values(by='length')['word'].tail(n=20)","d789db34":"Let's look on the longest words","a9950c5d":"I am wondering is there correlation between word length and its frequency?","0c69be4e":"And a list of 10 least popular words","a0382c68":"Let's plot numbers of articles by categories and years","84d0c26d":"Let's check ratio of words in the content and plot 20 most popular words ","7fe7e5fc":"Let's generate a wordcloud to undestand most popular words in articles"}}