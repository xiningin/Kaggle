{"cell_type":{"c948fa52":"code","9d030766":"code","ebb87d20":"code","67e3787a":"code","d6f06dae":"code","67672891":"code","49e2188d":"code","ce7d2f42":"code","abc8884d":"code","43ff4c14":"code","5b7674f9":"code","fb2453af":"code","b3363a19":"code","c420facf":"code","cc7ee042":"markdown","9de49a22":"markdown","683559d4":"markdown","5cf65c2f":"markdown","5149a373":"markdown","e98d96e2":"markdown","877724fc":"markdown","c6e25323":"markdown","9e4871e9":"markdown","d667c013":"markdown","5aad41c0":"markdown","8b8db190":"markdown","dcb4eb1c":"markdown"},"source":{"c948fa52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d030766":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku ","ebb87d20":"data = open('..\/input\/shakespear-sonnet\/shakespear_sonnet.txt').read()\ndata\n","67e3787a":"corpus = data.lower().split(\"\\n\")\ncorpus","d6f06dae":"oov_tok = \"<OOV>\"\nvocab_size=20000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1","67672891":"# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)","49e2188d":"max_sequence_len = max([len(x) for x in input_sequences])\nprint(max_sequence_len)","ce7d2f42":"input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))","abc8884d":"# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]","43ff4c14":"label = ku.to_categorical(label, num_classes=total_words)","5b7674f9":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150, return_sequences = True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words\/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","fb2453af":"history = model.fit(predictors, label, epochs=100, verbose=1)","b3363a19":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","c420facf":"seed_text = \"thereby beauty's rose\"\nnext_words = 5\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","cc7ee042":"Let's apply model:\n* Embeddings with 100 dimensions\n* Multi layer LSTM \n* kernel regularizer l2 with 0.01 learning rate","9de49a22":"Let's pad all sequences to same length and change it to array","683559d4":"Now predict the word of Shakespeare","5cf65c2f":"Now read the file","5149a373":"Let's create predictor and label i.e., we extract all of input sequence as predictor and leave the last character as label.","e98d96e2":"# **Let's import required libraries**","877724fc":"Let's fit with model using 100 epochs","c6e25323":"Now tokenize the data and find the total number of words.","9e4871e9":"*Let's split the data and convert the corpus in lower case*","d667c013":"Now perform one-hot encoding of labels using keras utility to convert a list to a catorical.","5aad41c0":"**Now we will create token list using tokenizer. It will convert a line of text to list of token representing words.**\n* Example: In the town of athy ===> [4 2 66 867 68]\n\n**Then we iterate over list of tokens and create n-grams sequences**:\n* Example: 1st two word 1st sequence\n         1st three word 2nd sequence \n         1st four words 3rd sequence\n         .....\n         ..... so on.\n         \n**Input sequences are simply the sentences being broken into phrases**","8b8db190":"Let's plot the graph and visualize","dcb4eb1c":"Now let's find out length of longest sentence in the corpus"}}