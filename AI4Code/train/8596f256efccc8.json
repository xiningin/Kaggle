{"cell_type":{"6e50bcce":"code","aa791ec4":"code","ff7c0e8e":"code","a2bb92f9":"code","71c89fd1":"code","222d302d":"code","49d5ba03":"code","e859e625":"code","3f57df28":"code","e701f8c5":"code","ea41c820":"markdown","d3c244fd":"markdown","ef058f98":"markdown","77b9ea51":"markdown","be8b42b4":"markdown","98175a19":"markdown","2a42b662":"markdown","4b6b18d6":"markdown","3e9e7309":"markdown","3073b19d":"markdown","7103ab4c":"markdown","130cbf60":"markdown","e1c89f12":"markdown","fddcd741":"markdown","722dcd6c":"markdown"},"source":{"6e50bcce":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","aa791ec4":"def generate_data(m):\n    '''plots m random points on a 3D plane'''\n\n    angles = np.random.rand(m) * 3 * np.pi \/ 2 - 0.5\n    data = np.empty((m, 3))\n    data[:,0] = np.cos(angles) + np.sin(angles)\/2 + 0.1 * np.random.randn(m)\/2\n    data[:,1] = np.sin(angles) * 0.7 + 0.1 * np.random.randn(m) \/ 2\n    data[:,2] = data[:, 0] * 0.1 + data[:, 1] * 0.3 + 0.1 * np.random.randn(m)\n    \n    return data","ff7c0e8e":"# use the function above to generate data points\nX_train = generate_data(100)\nX_train = X_train - X_train.mean(axis=0, keepdims=0)\n\n# preview the data\nax = plt.axes(projection='3d')\nax.scatter3D(X_train[:, 0], X_train[:, 1], X_train[:, 2], cmap='Reds');","a2bb92f9":"encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\ndecoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n\nautoencoder = keras.models.Sequential([encoder, decoder])","71c89fd1":"autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))","222d302d":"history = autoencoder.fit(X_train, X_train, epochs=200)","49d5ba03":"# encode the data\ncodings = encoder.predict(X_train)\n\n# see a sample input-encoder output pair\nprint(f'input point: {X_train[0]}')\nprint(f'encoded point: {codings[0]}')","e859e625":"# plot all encoder outputs\nfig = plt.figure(figsize=(4,3))\nplt.plot(codings[:,0], codings[:, 1], \"b.\")\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True)\nplt.show()","3f57df28":"# decode the encoder output\ndecodings = decoder.predict(codings)\n\n# see a sample output for a single point\nprint(f'input point: {X_train[0]}')\nprint(f'encoded point: {codings[0]}')\nprint(f'decoded point: {decodings[0]}')","e701f8c5":"# plot the decoder output\nax = plt.axes(projection='3d')\nax.scatter3D(decodings[:, 0], decodings[:, 1], decodings[:, 2], c=decodings[:, 0], cmap='Reds');","ea41c820":"You can then setup the model for training.","d3c244fd":"## Build the Model","ef058f98":"![Autoencoders1.JPG](attachment:47a3f016-7651-48aa-9e15-0738edea1613.JPG)","77b9ea51":"## Train the Model","be8b42b4":"You will configure the training to also use the input data as your target output. In our example, that will be `X_train`.","98175a19":"As mentioned, you can use the encoder to compress the input to two dimensions.","2a42b662":"## Compile the Model","4b6b18d6":"The decoder then tries to reconstruct the original input. See the outputs below. You will see that although not perfect, it still follows the general shape of the original input.","3e9e7309":"Now you will build the simple encoder-decoder model. Notice the number of neurons in each Dense layer. The model will contract in the encoder then expand in the decoder.","3073b19d":"## Plot the Decoder output","7103ab4c":"## Prepare and preview the dataset\n\nYou will first create a synthetic dataset to act as input to the autoencoder. You can do that with the function below.","130cbf60":"That's it for this simple demonstration of the autoencoder!","e1c89f12":"## Imports","fddcd741":"# Autoencoder\n\nWill build your first simple autoencoder. This will take in three-dimensional data, encodes it to two dimensions, and decodes it back to 3D.","722dcd6c":"## Plot the encoder output"}}