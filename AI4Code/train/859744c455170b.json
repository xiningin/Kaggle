{"cell_type":{"84235958":"code","0798ce29":"code","6ddc7962":"code","8088d829":"code","11ea88a8":"code","d0529dad":"code","b44cf8ce":"code","24aaf68a":"code","a9df96a1":"code","f0d56655":"code","0133e241":"code","8f52a18f":"code","73fb961f":"code","bd1f1ac7":"code","5478a58a":"code","fbdc6715":"code","e6b641a3":"code","9d92deaa":"code","2a7c73f7":"code","e5d1ec50":"code","4efc61b1":"code","640dac56":"code","699c1310":"code","be014e8a":"code","914cc493":"code","ee8e75e4":"code","581c8998":"code","13ebb20f":"code","6bee091a":"code","4976fc9d":"code","62c3d37b":"code","22c8c63b":"code","52a3f152":"code","2c3639fc":"code","b27162fb":"code","6ec4881c":"code","61ecba88":"code","a9bcb7f2":"code","d32ba5c4":"code","c9fbe653":"markdown","ab6b57f7":"markdown","6665866a":"markdown","ca174573":"markdown","3b2424d5":"markdown","26e4a9c2":"markdown","0e3fba4d":"markdown","d3b73004":"markdown","a6eaa13f":"markdown","286449ff":"markdown","238add9f":"markdown","174479d8":"markdown"},"source":{"84235958":"!pip install pytorch_pretrained_bert","0798ce29":"import pickle\nimport numpy as np\nimport torch\nfrom pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n\nwith torch.no_grad():\n    model = BertForMaskedLM.from_pretrained('bert-base-cased')\n    model.eval()\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')","6ddc7962":"# We should ignore the warning above, as the tokenizer is loaded correctly.\nbert_tokenizer.vocab['Hello']","8088d829":"import string\nmask_idx = bert_tokenizer.vocab['[MASK]']\ndef predict_OOV(text, words_to_replace):\n    for puncation in string.punctuation:\n        text = text.replace(puncation, f' {puncation} ')\n\n    for word in words_to_replace:\n        text = text.replace(word, '[MASK]')\n    \n    tokenize_input = [\"[CLS]\"] + bert_tokenizer.tokenize(text) + [\"[SEP]\"]\n    ids = np.array(bert_tokenizer.convert_tokens_to_ids(tokenize_input))\n    mask_ids = np.argwhere(ids==mask_idx)\n    input_ids = torch.tensor([ids], device='cpu')\n    next_word_preds =model(input_ids,\n                           token_type_ids=None,\n                           attention_mask=None,\n                           masked_lm_labels=None).data.cpu().numpy()\n\n    masked_lm_model = np.array([bert_tokenizer.ids_to_tokens[token] for token in np.argmax(next_word_preds[0], axis=-1)])\n    return masked_lm_model[mask_ids]","11ea88a8":"text = \"\"\"Quidditch , formerly known as Kwidditch and Cuaditch , is a wizarding sport played on broomsticks. It is the most popular game and most well-known game among wizards and witches, and, according to Rubeus Hagrid, the equivalent to Muggles' passion for football (Soccer). The object of the game is to score more points than your opponents. Each goal is worth ten points and catching the Golden Snitch is worth one-hundred and fifty points. The game ends when the Snitch is caught or an agreement is reached between the captains of both teams. Some games can go on for many days if the Snitch is not caught (the record, according to Quidditch Through the Ages, is six months, although no one caught the Snitch.) \"\"\"\npredict_OOV(text, ['Quidditch', 'Kwidditch', 'Cuaditch'])","d0529dad":"import numpy as np\nimport pandas as pd\nimport os\nimport time\n\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b44cf8ce":"tqdm.pandas()","24aaf68a":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar","a9df96a1":"def seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","f0d56655":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'","0133e241":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    # Only load words to save memory\n    return set(emb_arr.keys())","8f52a18f":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","73fb961f":"symbols_to_isolate = '.,?!-;*\"\u2026:\u2014()%#$&_\/@\uff3c\u30fb\u03c9+=\u201d\u201c[]^\u2013>\\\\\u00b0<~\u2022\u2260\u2122\u02c8\u028a\u0252\u221e\u00a7{}\u00b7\u03c4\u03b1\u2764\u263a\u0261|\u00a2\u2192\u0336`\u2765\u2501\u2523\u252b\u2517\uff2f\u25ba\u2605\u00a9\u2015\u026a\u2714\u00ae\\x96\\x92\u25cf\u00a3\u2665\u27a4\u00b4\u00b9\u2615\u2248\u00f7\u2661\u25d0\u2551\u25ac\u2032\u0254\u02d0\u20ac\u06e9\u06de\u2020\u03bc\u2712\u27a5\u2550\u2606\u02cc\u25c4\u00bd\u02bb\u03c0\u03b4\u03b7\u03bb\u03c3\u03b5\u03c1\u03bd\u0283\u272c\uff33\uff35\uff30\uff25\uff32\uff29\uff34\u263b\u00b1\u264d\u00b5\u00ba\u00be\u2713\u25fe\u061f\uff0e\u2b05\u2105\u00bb\u0412\u0430\u0432\u2763\u22c5\u00bf\u00ac\u266b\uff23\uff2d\u03b2\u2588\u2593\u2592\u2591\u21d2\u2b50\u203a\u00a1\u2082\u2083\u2767\u25b0\u2594\u25de\u2580\u2582\u2583\u2584\u2585\u2586\u2587\u2199\u03b3\u0304\u2033\u2639\u27a1\u00ab\u03c6\u2153\u201e\u270b\uff1a\u00a5\u0332\u0305\u0301\u2219\u201b\u25c7\u270f\u25b7\u2753\u2757\u00b6\u02da\u02d9\uff09\u0441\u0438\u02bf\u2728\u3002\u0251\\x80\u25d5\uff01\uff05\u00af\u2212\ufb02\ufb01\u2081\u00b2\u028c\u00bc\u2074\u2044\u2084\u2320\u266d\u2718\u256a\u25b6\u262d\u272d\u266a\u2614\u2620\u2642\u2603\u260e\u2708\u270c\u2730\u2746\u2619\u25cb\u2023\u2693\u5e74\u220e\u2112\u25aa\u2599\u260f\u215b\uff43\uff41\uff53\u01c0\u212e\u00b8\uff57\u201a\u223c\u2016\u2133\u2744\u2190\u263c\u22c6\u0292\u2282\u3001\u2154\u00a8\u0361\u0e4f\u26be\u26bd\u03a6\u00d7\u03b8\uffe6\uff1f\uff08\u2103\u23e9\u262e\u26a0\u6708\u270a\u274c\u2b55\u25b8\u25a0\u21cc\u2610\u2611\u26a1\u2604\u01eb\u256d\u2229\u256e\uff0c\u4f8b\uff1e\u0295\u0250\u0323\u0394\u2080\u271e\u2508\u2571\u2572\u258f\u2595\u2503\u2570\u258a\u258b\u256f\u2533\u250a\u2265\u2612\u2191\u261d\u0279\u2705\u261b\u2669\u261e\uff21\uff2a\uff22\u25d4\u25e1\u2193\u2640\u2b06\u0331\u210f\\x91\u2800\u02e4\u255a\u21ba\u21e4\u220f\u273e\u25e6\u266c\u00b3\u306e\uff5c\uff0f\u2235\u2234\u221a\u03a9\u00a4\u261c\u25b2\u21b3\u25ab\u203f\u2b07\u2727\uff4f\uff56\uff4d\uff0d\uff12\uff10\uff18\uff07\u2030\u2264\u2215\u02c6\u269c\u2601'\nsymbols_to_delete = '\\n\ud83c\udf55\\r\ud83d\udc35\ud83d\ude11\\xa0\\ue014\\t\\uf818\\uf04a\\xad\ud83d\ude22\ud83d\udc36\ufe0f\\uf0e0\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\\u200b\\u200e\ud83d\ude01\u0639\u062f\u0648\u064a\u0647\u0635\u0642\u0623\u0646\u0627\u062e\u0644\u0649\u0628\u0645\u063a\u0631\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\u0415\ud83d\udc4e\ud83d\ude00\ud83d\ude02\\u202a\\u202c\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\u1d0d\u028f\u0280\u1d07\u0274\u1d05\u1d0f\u1d00\u1d0b\u029c\u1d1c\u029f\u1d1b\u1d04\u1d18\u0299\u0493\u1d0a\u1d21\u0262\ud83d\ude0b\ud83d\udc4f\u05e9\u05dc\u05d5\u05dd\u05d1\u05d9\ud83d\ude31\u203c\\x81\u30a8\u30f3\u30b8\u6545\u969c\\u2009\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\\u200f\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\u05d0\u05e2\u05db\u05d7\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83c\udffc\u0b9c\ud83d\ude16\u1d20\ud83d\udeb2\u2010\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\\x7f\ud83d\udc4c\u1f10\u1f76\u03ae\u03b9\u1f72\u03ba\u1f00\u03af\u1fc3\u1f34\u03be\ud83d\ude44\uff28\ud83d\ude20\\ufeff\\u2028\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\\u3000\u062a\u062d\u0643\u0633\u0629\ud83d\udc6e\ud83d\udc99\u0641\u0632\u0637\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\\u2008\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\\x08\u2011\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud835\ude0a\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude24\ud835\ude3a\ud835\ude34\ud835\ude2a\ud835\ude27\ud835\ude2e\ud835\ude23\ud83d\udc97\ud83d\udc9a\u5730\u7344\u8c37\u0443\u043b\u043a\u043d\u041f\u043e\u0410\u041d\ud83d\udc3e\ud83d\udc15\ud83d\ude06\u05d4\ud83d\udd17\ud83d\udebd\u6b4c\u821e\u4f0e\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u043c\u03c5\u0442\u0455\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\\u200a\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\u044d\u043f\u0440\u0434\\x95\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\\u2002\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\\u2004\u02e2\u1d52\u02b3\u02b8\u1d3c\u1d37\u1d3a\u02b7\u1d57\u02b0\u1d49\u1d58\\x13\ud83d\udeac\ud83e\udd13\\ue602\ud83d\ude35\u03ac\u03bf\u03cc\u03c2\u03ad\u1f78\u05ea\u05de\u05d3\u05e3\u05e0\u05e8\u05da\u05e6\u05d8\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\\uf0b7\\uf04c\\x9f\\x10\u6210\u90fd\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\u0435\u0445\ud83d\ude32\u1f38\u1fb6\u1f41\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\\u202d\ud83d\udca4\ud83c\udf47\\ue613\u5c0f\u571f\u8c46\ud83c\udfe1\u2754\u2049\\u202f\ud83d\udc60\u300b\u0915\u0930\u094d\u092e\u093e\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\u8521\u82f1\u6587\ud83c\udf1e\ud83c\udfb2\u30ec\u30af\u30b5\u30b9\ud83d\ude1b\u5916\u56fd\u4eba\u5173\u7cfb\u0421\u0431\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\u044c\u044b\u0433\u044f\u4e0d\u662f\\x9c\\x9d\ud83d\uddd1\\u2005\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\u1e37\u0417\u0437\u25b1\u0446\ufffc\ud83e\udd23\u5356\u6e29\u54e5\u534e\u8bae\u4f1a\u4e0b\u964d\u4f60\u5931\u53bb\u6240\u6709\u7684\u94b1\u52a0\u62ff\u5927\u574f\u7a0e\u9a97\u5b50\ud83d\udc1d\u30c4\ud83c\udf85\\x85\ud83c\udf7a\u0622\u0625\u0634\u0621\ud83c\udfb5\ud83c\udf0e\u035f\u1f14\u6cb9\u522b\u514b\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\u0439\\u2003\ud83d\ude80\ud83e\udd34\u02b2\u0448\u0447\u0418\u041e\u0420\u0424\u0414\u042f\u041c\u044e\u0436\ud83d\ude1d\ud83d\udd91\u1f50\u1f7b\u03cd\u7279\u6b8a\u4f5c\u6226\u7fa4\u0449\ud83d\udca8\u5706\u660e\u56ed\u05e7\u2110\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\\u200d\ud835\udcd2\ud835\udcf2\ud835\udcff\ud835\udcf5\uc548\uc601\ud558\uc138\uc694\u0416\u0459\u041a\u045b\ud83c\udf40\ud83d\ude2b\ud83e\udd24\u1fe6\u6211\u51fa\u751f\u5728\u4e86\u53ef\u4ee5\u8bf4\u666e\u901a\u8bdd\u6c49\u8bed\u597d\u6781\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\u5929\u4e00\u5bb6\u26b2\\u2006\u26ad\u2686\u2b2d\u2b2f\u23d6\u65b0\u2700\u254c\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddec\ud83c\udde7\ud83d\ude37\ud83c\udde8\ud83c\udde6\u0425\u0428\ud83c\udf10\\x1f\u6740\u9e21\u7ed9\u7334\u770b\u0281\ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb\ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddef\ud835\ude01\ud835\uddf0\ud835\ude00\ud835\ude05\ud835\uddfd\ud835\ude04\ud835\uddf1\ud83d\udcfa\u03d6\\u2000\u04af\u057d\u1d26\u13a5\u04bb\u037a\\u2007\u0570\\u2001\u0269\uff59\uff45\u0d66\uff4c\u01bd\uff48\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc2e\ud835\udc1d\ud835\udc1a\ud835\udc03\ud835\udc1c\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\u0184\u1d28\u05df\u146f\u0ed0\u03a4\u13e7\u0be6\u0406\u1d11\u0701\ud835\udc2c\ud835\udc30\ud835\udc32\ud835\udc1b\ud835\udc26\ud835\udc2f\ud835\udc11\ud835\udc19\ud835\udc23\ud835\udc07\ud835\udc02\ud835\udc18\ud835\udfce\u051c\u0422\u15de\u0c66\u3014\u13ab\ud835\udc33\ud835\udc14\ud835\udc31\ud835\udfd4\ud835\udfd3\ud835\udc05\ud83d\udc0b\ufb03\ud83d\udc98\ud83d\udc93\u0451\ud835\ude25\ud835\ude2f\ud835\ude36\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud835\ude6c\ud835\ude56\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude61\ud835\ude6e\ud835\ude58\ud835\ude60\ud835\ude5a\ud835\ude59\ud835\ude5c\ud835\ude67\ud835\ude65\ud835\ude69\ud835\ude6a\ud835\ude57\ud835\ude5e\ud835\ude5d\ud835\ude5b\ud83d\udc7a\ud83d\udc37\u210b\ud835\udc00\ud835\udc25\ud835\udc2a\ud83d\udeb6\ud835\ude62\u1f39\ud83e\udd18\u0366\ud83d\udcb8\u062c\ud328\ud2f0\uff37\ud835\ude47\u1d7b\ud83d\udc42\ud83d\udc43\u025c\ud83c\udfab\\uf0a7\u0411\u0423\u0456\ud83d\udea2\ud83d\ude82\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u1fc6\ud83c\udfc3\ud835\udcec\ud835\udcfb\ud835\udcf4\ud835\udcee\ud835\udcfd\ud835\udcfc\u2618\ufd3e\u032f\ufd3f\u20bd\\ue807\ud835\udc7b\ud835\udc86\ud835\udc8d\ud835\udc95\ud835\udc89\ud835\udc93\ud835\udc96\ud835\udc82\ud835\udc8f\ud835\udc85\ud835\udc94\ud835\udc8e\ud835\udc97\ud835\udc8a\ud83d\udc7d\ud83d\ude19\\u200c\u041b\u2012\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\u516c\u5bd3\u517b\u5ba0\u7269\u5417\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\u64cd\u7f8e\ud835\udc91\ud835\udc9a\ud835\udc90\ud835\udc74\ud83e\udd19\ud83d\udc12\u6b22\u8fce\u6765\u5230\u963f\u62c9\u65af\u05e1\u05e4\ud835\ude6b\ud83d\udc08\ud835\udc8c\ud835\ude4a\ud835\ude6d\ud835\ude46\ud835\ude4b\ud835\ude4d\ud835\ude3c\ud835\ude45\ufdfb\ud83e\udd84\u5de8\u6536\u8d62\u5f97\u767d\u9b3c\u6124\u6012\u8981\u4e70\u989d\u1ebd\ud83d\ude97\ud83d\udc33\ud835\udfcf\ud835\udc1f\ud835\udfd6\ud835\udfd1\ud835\udfd5\ud835\udc84\ud835\udfd7\ud835\udc20\ud835\ude44\ud835\ude43\ud83d\udc47\u951f\u65a4\u62f7\ud835\udde2\ud835\udff3\ud835\udff1\ud835\udfec\u2981\u30de\u30eb\u30cf\u30cb\u30c1\u30ed\u682a\u5f0f\u793e\u26f7\ud55c\uad6d\uc5b4\u3138\u3153\ub2c8\u035c\u0296\ud835\ude3f\ud835\ude54\u20b5\ud835\udca9\u212f\ud835\udcbe\ud835\udcc1\ud835\udcb6\ud835\udcc9\ud835\udcc7\ud835\udcca\ud835\udcc3\ud835\udcc8\ud835\udcc5\u2134\ud835\udcbb\ud835\udcbd\ud835\udcc0\ud835\udccc\ud835\udcb8\ud835\udcce\ud835\ude4f\u03b6\ud835\ude5f\ud835\ude03\ud835\uddfa\ud835\udfee\ud835\udfed\ud835\udfef\ud835\udff2\ud83d\udc4b\ud83e\udd8a\u591a\u4f26\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u4e3a\u548c\u4e2d\u53cb\u8c0a\u795d\u8d3a\u4e0e\u5176\u60f3\u8c61\u5bf9\u6cd5\u5982\u76f4\u63a5\u95ee\u7528\u81ea\u5df1\u731c\u672c\u4f20\u6559\u58eb\u6ca1\u79ef\u552f\u8ba4\u8bc6\u57fa\u7763\u5f92\u66fe\u7ecf\u8ba9\u76f8\u4fe1\u8036\u7a23\u590d\u6d3b\u6b7b\u602a\u4ed6\u4f46\u5f53\u4eec\u804a\u4e9b\u653f\u6cbb\u9898\u65f6\u5019\u6218\u80dc\u56e0\u5723\u628a\u5168\u5802\u7ed3\u5a5a\u5b69\u6050\u60e7\u4e14\u6817\u8c13\u8fd9\u6837\u8fd8\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\u6279\u5224\u68c0\u8ba8\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\uc950\uc2a4\ud0f1\ud2b8\ub93c\ub3c4\uc11d\uc720\uac00\uaca9\uc778\uc0c1\uc774\uacbd\uc81c\ud669\uc744\ub835\uac8c\ub9cc\ub4e4\uc9c0\uc54a\ub85d\uc798\uad00\ub9ac\ud574\uc57c\ud569\ub2e4\uce90\ub098\uc5d0\uc11c\ub300\ub9c8\ucd08\uc640\ud654\uc57d\uae08\uc758\ud488\ub7f0\uc131\ubd84\uac08\ub54c\ub294\ubc18\ub4dc\uc2dc\ud5c8\ub41c\uc0ac\uc6a9\ud83d\udd2b\ud83d\udc41\u51f8\u1f70\ud83d\udcb2\ud83d\uddef\ud835\ude48\u1f0c\ud835\udc87\ud835\udc88\ud835\udc98\ud835\udc83\ud835\udc6c\ud835\udc76\ud835\udd7e\ud835\udd99\ud835\udd97\ud835\udd86\ud835\udd8e\ud835\udd8c\ud835\udd8d\ud835\udd95\ud835\udd8a\ud835\udd94\ud835\udd91\ud835\udd89\ud835\udd93\ud835\udd90\ud835\udd9c\ud835\udd9e\ud835\udd9a\ud835\udd87\ud835\udd7f\ud835\udd98\ud835\udd84\ud835\udd9b\ud835\udd92\ud835\udd8b\ud835\udd82\ud835\udd74\ud835\udd9f\ud835\udd88\ud835\udd78\ud83d\udc51\ud83d\udebf\ud83d\udca1\u77e5\u5f7c\u767e\\uf005\ud835\ude40\ud835\udc9b\ud835\udc72\ud835\udc73\ud835\udc7e\ud835\udc8b\ud835\udfd2\ud83d\ude26\ud835\ude52\ud835\ude3e\ud835\ude3d\ud83c\udfd0\ud835\ude29\ud835\ude28\u1f7c\u1e51\ud835\udc71\ud835\udc79\ud835\udc6b\ud835\udc75\ud835\udc6a\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\u14c7\u14a7\u152d\u1403\u1427\u1426\u1473\u1428\u14c3\u14c2\u1472\u1438\u146d\u144e\u14c0\u1423\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\u70b9\u51fb\u67e5\u7248\ud83c\udf6d\ud835\udc65\ud835\udc66\ud835\udc67\uff2e\uff27\ud83d\udc63\\uf020\u3063\ud83c\udfc9\u0444\ud83d\udcad\ud83c\udfa5\u039e\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\\x0b\ud83c\udf69\ud835\udc6f\ud835\udc92\ud83d\ude17\ud835\udfd0\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\u0686\u06cc\ud835\udc6e\ud835\uddd5\ud835\uddf4\ud83c\udf52\ua725\u2ca3\u2c8f\ud83d\udc11\u23f0\u9244\u30ea\u4e8b\u4ef6\u0457\ud83d\udc8a\u300c\u300d\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\u71fb\u88fd\u30b7\u865a\u507d\u5c41\u7406\u5c48\u0413\ud835\udc69\ud835\udc70\ud835\udc80\ud835\udc7a\ud83c\udf24\ud835\uddf3\ud835\udddc\ud835\uddd9\ud835\udde6\ud835\udde7\ud83c\udf4a\u1f7a\u1f08\u1f21\u03c7\u1fd6\u039b\u290f\ud83c\uddf3\ud835\udc99\u03c8\u0541\u0574\u0565\u057c\u0561\u0575\u056b\u0576\u0580\u0582\u0564\u0571\u51ac\u81f3\u1f40\ud835\udc81\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud835\ude2c\ud835\ude31\ud835\ude38\ud835\ude37\ud835\ude10\ud835\ude2d\ud835\ude13\ud835\ude16\ud835\ude39\ud835\ude32\ud835\ude2b\u06a9\u0392\u03ce\ud83d\udca2\u039c\u039f\u039d\u0391\u0395\ud83c\uddf1\u2672\ud835\udf48\u21b4\ud83d\udc92\u2298\u023b\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud835\udc0e\ud835\udc0d\ud835\udc0a\ud835\udc6d\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\uff47\uff52\uff4e\uff54\uff49\uff44\uff55\uff46\uff42\uff4b\ud835\udff0\ud83c\uddf4\ud83c\udded\ud83c\uddfb\ud83c\uddf2\ud835\uddde\ud835\udded\ud835\uddd8\ud835\udde4\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\\uf10a\u10da\u06a1\ud83d\udc26\\U0001f92f\\U0001f92a\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud835\uddf8\ud835\udddf\ud835\udde0\ud835\uddf7\ud83e\udd5c\u3055\u3088\u3046\u306a\u3089\ud83d\udd3c'","bd1f1ac7":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x","5478a58a":"train['comment_text'].head()","fbdc6715":"train['comment_text'] = train['comment_text'].progress_apply(lambda x:preprocess(x))\ntest['comment_text'] = test['comment_text'].progress_apply(lambda x:preprocess(x))","e6b641a3":"import operator \n\ndef check_coverage(vocab, embedding_words):\n    num_unique_words_found = 0\n    num_words_found = 0\n    num_words_not_found = 0\n    oov = {}\n\n    for word in tqdm(vocab):\n        if word in embedding_words:\n            num_unique_words_found += 1\n            num_words_found += vocab[word]\n        else:\n            oov[word] = vocab[word]\n            num_words_not_found += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(num_unique_words_found \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(num_words_found \/ (num_words_found + num_words_not_found)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence.split():\n            vocab[word] = vocab.get(word, 0) + 1\n            \n    return vocab","9d92deaa":"glove_words = load_embeddings(GLOVE_EMBEDDING_PATH)\nvocab = build_vocab(train['comment_text'])\noov = check_coverage(vocab, glove_words)","2a7c73f7":"oov[:20]","e5d1ec50":"oov_words = set([word for word, count in oov])","4efc61b1":"def contains_oov(text):\n    for word in text.split():\n        if word in oov_words:\n            return True\n    return False","640dac56":"train['contains_oov'] = train['comment_text'].apply(contains_oov)\ntest['contains_oov'] = train['comment_text'].apply(contains_oov)","699c1310":"len(train[train['contains_oov']])","be014e8a":"# We will have a look at this specific example how BERT replaces OOV tokens. In this case,\n# the following words are not found within the glove embedding: 'Ellmyer', 'Bretzing', 'backcountryhabitat', '980875531985426'\ntrain['comment_text'].iloc[117]","914cc493":"# Converting the lines to BERT format\n# Thanks to https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming\ndef convert_lines(texts, max_seq_length, tokenizer, words_to_replace):\n    for i, text in enumerate(texts):\n        for word in words_to_replace:\n            text = text.replace(word, '[MASK]')\n        texts[i] = text\n        \n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(texts):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) +[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","ee8e75e4":"def get_location2prediction(x_batch, text_preds, offset):\n    \"\"\"\n    location2prediction[line] contains the indices of the predicted\n    masked words. \n    \"\"\"\n    idxs = np.argwhere(x_batch == mask_idx)\n    locations = idxs[:, 0] + offset\n    preds = np.argmax(text_preds[x_batch == mask_idx], axis=-1)\n    location2prediction = {}\n    for location, pred in zip(locations, preds):\n        location2prediction[location] = location2prediction.get(location, []) + [pred]\n    return location2prediction","581c8998":"model = model.to('cuda')","13ebb20f":"def predict_OOV(texts,\n                max_seq_length,\n                words_to_replace,\n                batch_size=32):\n    \n    ids = convert_lines(texts, max_seq_length, bert_tokenizer, oov_words)\n    data_loader = torch.utils.data.DataLoader(torch.tensor(ids), batch_size=batch_size, shuffle=False)\n    \n    idx2prediction = {}\n    for i, x_batch  in tqdm(enumerate(data_loader)):\n        text_preds = model(x_batch.to('cuda'),\n                           token_type_ids=None,\n                           attention_mask=None,\n                           masked_lm_labels=None).data.cpu().numpy()\n        batch_preds = get_location2prediction(x_batch.numpy(), text_preds, offset = i * batch_size)\n        idx2prediction.update(batch_preds)\n    return idx2prediction","6bee091a":"idx2prediction = predict_OOV(train['comment_text'].values[:200],\n                    max_seq_length=220,\n                    words_to_replace=oov_words,\n                    batch_size=32\n                   )","4976fc9d":"print([bert_tokenizer.ids_to_tokens[idx] for idx in idx2prediction[117]])\nprint(train['comment_text'].iloc[117])","62c3d37b":"def replace_MASK(text, predicted_words):\n    if len(predicted_words) == 0:\n        return text\n    text = np.array(text.split())\n    locations = np.argwhere(text == '[MASK]').flatten()[: len(predicted_words)]\n    text[locations] = np.array(predicted_words)\n    return ' '.join(text)","22c8c63b":"def replace_MASK_in_dataframe(row, idx2prediction):\n    text = row['comment_text']\n    idx = row['idx']\n    predcited_OOV_tokens = idx2prediction.get(idx, [])\n\n    predcited_OOV_words = [bert_tokenizer.ids_to_tokens[idx] for idx in predcited_OOV_tokens]\n    corrected_text = replace_MASK(text, predcited_OOV_words)\n    return corrected_text","52a3f152":"train['idx'] = list(range(len(train)))\ntrain['comment_texts'] =  train.apply(lambda x: replace_MASK_in_dataframe(x, idx2prediction), axis=1).values","2c3639fc":"train['comment_texts'].iloc[117]","b27162fb":"idx2prediction = predict_OOV(test['comment_text'].values,\n                    max_seq_length=220,\n                    words_to_replace=oov_words,\n                    batch_size=32\n                   )","6ec4881c":"print([bert_tokenizer.ids_to_tokens[idx] for idx in idx2prediction[184]])\nprint(test['comment_text'].iloc[184])","61ecba88":"test['idx'] = list(range(len(test)))\ntest['comment_text'] = test.progress_apply(lambda x: replace_MASK_in_dataframe(x, idx2prediction), axis=1).values","a9bcb7f2":"print(test['comment_text'].iloc[184])","d32ba5c4":"train.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","c9fbe653":"So lets apply that preprocess function to our text","ab6b57f7":"In this notebook I build upon the the work of \n* https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda \n* https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part2-usage\n* https:\/\/www.kaggle.com\/bminixhofer\/speed-up-your-rnn-with-sequence-bucketing \n\nI will show how one can handle OOV words which are still present after the text preprocessing.\nI will not cover correct misspellings or similar methods, however have a look at the [third palce solution](https:\/\/www.kaggle.com\/wowfattie\/3rd-place) of Quora, how one can do this effectively (the idea goes back to [CPMP](https:\/\/www.kaggle.com\/cpmpml\/spell-checker-using-word2vec) ).\n\nHere, I want to use BERT to predict words which are OOV.\nAs BERT was trained to predict masked words (in the input sequence, a word is replaced by the token \"[MASK]\"), one can use masked language model out of the box to replace unknown words by words which are found inside BERT's vocabulary.\nLet me give you an example:","6665866a":"# Preface","ca174573":"In principle this functions just deletes some special characters. Which is not optimal and I will explain why in a bit. What is additionally inefficient is that later the keras tokenizer with its default parameters is used which has its own with the above function redundant behavior.","3b2424d5":"See part1 for an explanation how I came to the list of symbols and contraction function. I copied them from that kernel.","26e4a9c2":"This looks promising!\n\nWe will apply this idea to replace OOV words in the training set by BERT's predictions.\nFor this to be efficient, we need to rewrite predict_OOV, as this function currently only works with a batch size of 1 :)\n\nBefore doing this, I apply the methods found in [Dieter's](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda ) kernel to preprocess the text effectively.","0e3fba4d":"Of course we also need to adjust the load_embeddings function, to now handle the pickled dict.","d3b73004":"## Preprocessing","a6eaa13f":"Let us now implement the idea above, using code snppets from the [Toxic BERT plain vanila](https:\/\/www.kaggle.com\/yuval6967\/toxic-bert-plain-vanila) kernel.\n\nThe workflow is the following:\n* Replace OOV words by the [MASK] token and convert the text to BERT input ids.\n* Use the masked language model to argmax predict a distribution of possible candidate words for each time step of the input. \n(Side Remark: For words which are not masked, the prediction of the respective time step is (almost always)  the input id.).\n* Obtain the argmax prediction for those postitions which were marked. I then create a dictionary, where the keys are the ids of the texts and the keys are the argmax predictions for all the masked inputs of the respective text.\n* For each masked text, replace the masked words by the predicted words.","286449ff":"Let's discuss the function, which is most popular in most public kernels.","238add9f":"Here, compared to most other public kernels I replace the pretrained embedding files with their pickle corresponds. Loading a pickled version extremly improves timing ;)","174479d8":"The next function is really important. Although we put a lot of effort in making the preprocessing right there are stil some out of vocabulary words we could easily fix. One example I implement here is to try a \"lower\/upper case version of a\" word if an embedding is not found, which sometimes gives us an embedding. Sorry for the bad coding style in the loop"}}