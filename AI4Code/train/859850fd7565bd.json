{"cell_type":{"36162e65":"code","e5f3b85c":"code","94d72435":"code","077a3a44":"code","a042eb46":"code","43abfd2e":"code","a90ca09a":"code","4625b29b":"code","8b0842d2":"code","c1ada38e":"code","2f3b5c64":"code","2f1d19d5":"code","2de9aebb":"code","72aa6085":"code","02362f54":"code","ea66cc57":"code","f3cbdb0e":"code","feac477a":"code","d6d0cc01":"code","1d39fb08":"markdown","cd3fddd4":"markdown","63a23436":"markdown","9aaa6dc1":"markdown","e1071383":"markdown","41851c14":"markdown","d5630ee3":"markdown","f33bd7b6":"markdown","616910dc":"markdown","f7b7c3af":"markdown","ce96032b":"markdown","3d344333":"markdown","58aef3c3":"markdown","0872852b":"markdown"},"source":{"36162e65":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e5f3b85c":"df_h = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/history.csv', \n                   header=0).fillna('NA').set_index('EmployeeID')\ndf_e = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/employees.csv', \n                   header=0).set_index('EmployeeID')\ndf_s = pd.read_csv('..\/input\/softserve-ds-hackathon-2020\/submission.csv', \n                   header=0).set_index('EmployeeID')","94d72435":"df_h.head(5)","077a3a44":"df_e.head(5)","a042eb46":"df_s.head(5)","43abfd2e":"df_h= pd.merge(df_h, df_e, left_index=True, \n               right_index=True, how='left')\ndf_h= pd.merge(df_h, df_s, left_index=True, \n               right_index=True, how='left')\ndf_h.iloc[:10,-5:]","a90ca09a":"df_h['tgt'] = np.where((~df_h['DismissalDate'].isna()) & (df_h['target'].isna()), 1,\n                       np.where((df_h['DismissalDate'].isna()) & (df_h['target']==0), \n                                np.NaN, 0))\ndf_h = df_h.drop(columns='target').rename(columns={'tgt': 'target'})\ndf_h.loc[(pd.isnull(df_h.target)),'DismissalDate'] = '3\/1\/19'\ndf_h.DismissalDate = df_h.DismissalDate.fillna('3\/1\/19')","4625b29b":"df_h.iloc[45:55,-5:]","8b0842d2":"df_h.iloc[90:95,-5:]","c1ada38e":"df = pd.DataFrame()\ndf_h['DismissalDate']= df_h['DismissalDate'].astype('datetime64[ns]')\ndf_h['HiringDate']= df_h['HiringDate'].astype('datetime64[ns]')\ndf['months'] = df_h.groupby('EmployeeID')['DismissalDate'].last()-df_h.groupby('EmployeeID')['HiringDate'].last()\ndf.months =np.around(df['months']\/np.timedelta64(1, 'M')).astype(int)\n\n\ndf['SBUID_L'] = df_h.groupby('EmployeeID')['DevCenterID'].last()\ndf['SBUID_C'] = df_h.groupby('EmployeeID')['DevCenterID'].nunique()\n\ndf['SBUID_L'] = df_h.groupby('EmployeeID')['SBUID'].last()\ndf['SBUID_C'] = df_h.groupby('EmployeeID')['SBUID'].nunique()\n\n\ndf['PosID_L'] = df_h.groupby('EmployeeID')['PositionID'].last()\ndf['PosID_C'] = df_h.groupby('EmployeeID')['PositionID'].nunique()\n\ndf['PosLV_L'] = df_h.groupby('EmployeeID')['PositionLevel'].last()\ndf['PosLV_C'] = df_h.groupby('EmployeeID')['PositionLevel'].nunique()\n\ndf['IsTr_L'] = df_h.groupby('EmployeeID')['IsTrainee'].last()\ndf['IsTr_C'] = df_h.groupby('EmployeeID')['IsTrainee'].nunique()\n\ndf['Lang_L'] = df_h.groupby('EmployeeID')['LanguageLevelID'].last()\ndf['Lang_C'] = df_h.groupby('EmployeeID')['LanguageLevelID'].nunique()\n\ndf['CustomerID'] = df_h.groupby('EmployeeID')['CustomerID'].nunique()\ndf['ProjectID'] = df_h.groupby('EmployeeID')['CustomerID'].nunique()\n\ndf['Util_L'] = df_h.groupby('EmployeeID')['Utilization'].last()\ndf['Util_M'] = df_h.groupby('EmployeeID')['Utilization'].mean()\n\ndf['HourVac_L'] = df_h.groupby('EmployeeID')['HourVacation'].last()\ndf['HourVac_S'] = df_h.groupby('EmployeeID')['HourVacation'].sum()\n\ndf['HourMob_L'] = df_h.groupby('EmployeeID')['HourMobileReserve'].last()\ndf['HourMob_S'] = df_h.groupby('EmployeeID')['HourMobileReserve'].sum()\n\n\ndf['OnSite_S'] = df_h.groupby('EmployeeID')['OnSite'].sum()\ndf['CompGroup_L'] = df_h.groupby('EmployeeID')['CompetenceGroupID'].last()\n\ndf['Func_L'] = df_h.groupby('EmployeeID')['FunctionalOfficeID'].last()\ndf['Func_C'] = df_h.groupby('EmployeeID')['FunctionalOfficeID'].nunique()\n\ndf['Paym_L'] = df_h.groupby('EmployeeID')['PaymentTypeId'].last()\ndf['Paym_C'] = df_h.groupby('EmployeeID')['PaymentTypeId'].nunique()\n\ndf['Bonus_L'] = df_h.groupby('EmployeeID')['BonusOneTime'].last()\ndf['Bonus_S'] = df_h.groupby('EmployeeID')['BonusOneTime'].max()\n\ndf['APM_L'] = df_h.groupby('EmployeeID')['APM'].last()\ndf['APM_M'] = df_h.groupby('EmployeeID')['APM'].mean()\n\ndf['Wage_L'] = df_h.groupby('EmployeeID')['WageGross'].last()\ndf['Wage_M'] = df_h.groupby('EmployeeID')['WageGross'].mean()\n\ndf['MonthOnP'] = df_h.groupby('EmployeeID')['MonthOnPosition'].last()\ndf['MonthOnS'] = df_h.groupby('EmployeeID')['MonthOnSalary'].last()\n\ndf['target'] = df_h.groupby('EmployeeID')['target'].last()\ndf","2f3b5c64":"indc = df[(df.target.isna())].index.tolist()\ndf = df.reset_index().drop(columns=['EmployeeID'])\ndf_tr = df[(~df.target.isna())]\ndf_ts = df[(df.target.isna())].drop(columns=['target'])\ny = df_tr.iloc[:,-1]\nX = df_tr.iloc[:,:-1]","2f1d19d5":"df_tr.iloc[:10,-5:]","2de9aebb":"df_ts.iloc[:10,-5:]","72aa6085":"bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(pd.DataFrame(MinMaxScaler().fit_transform(X.copy())),y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(df_tr.iloc[:,:-1].columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Uni-rate']\nfeatureScores=featureScores.set_index('Features')\nres_uni = featureScores.nlargest(20,'Uni-rate')\nprint(res_uni)\nplt.figure(figsize=(6,6))\nres_uni.plot(kind='barh')","02362f54":"model = ExtraTreesClassifier()\nmodel.fit(X,y)\nfeat_importances = pd.Series(model.feature_importances_, index=df_tr.iloc[:,:-1].columns)\nres_imp = feat_importances.sort_values(ascending=False)[:21]\nprint(res_imp)\nplt.figure(figsize=(6,6))\nres_imp.plot(kind='barh')","ea66cc57":"corrmat = df_tr.corr()\ntop_corr_features = df_tr.corr().index\nres_cor= corrmat['target'].sort_values(ascending=False)[1:21]\nprint(res_cor)\nplt.figure(figsize=(6,6))\nres_cor.plot(kind='barh')","f3cbdb0e":"plt.figure(figsize=(20,20))\nsns.heatmap(df_tr[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","feac477a":"best_ft = res_uni.merge(res_imp.rename('Imp_rate'), \n                        left_index=True, \n                        right_index=True, \n                        how='outer')\nbest_ft = best_ft.merge(res_cor.rename('Corr_rate'), \n                        left_index=True, \n                        right_index=True, \n                        how='outer')\nbest_ft","d6d0cc01":"lst = ['APM_M', 'Bonus_S', 'CompGroup_L', 'CustomerID', \n       'Func_L', 'HourMob_S', 'HourVac_S', 'IsTr_C', \n       'Lang_C', 'Lang_L','Paym_L', 'MonthOnS', 'Paym_C', \n       'PosID_L', 'Wage_M', 'months', 'target']","1d39fb08":"**1. Univariate Selection**\n\n* Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n* The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n* The example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 20 of the best features from the Mobile Price Range Prediction Dataset.\n\nApply SelectKBest class to extract top 20 best features out of 33:","cd3fddd4":"So, now it's your choise. My list of featres looks like that. What's your list?","63a23436":"Changing values in the column 'target' where:\n* 0 - Employee will stay - training-dataset\n* 1 - Employee going to leave (actually alredy Dismissed)  - training-dataset\n* NaN - rows for our test-dataset\n\ncolumn 'tgt' is temporary, than it will be changed to 'target'","9aaa6dc1":"* Here I will group all features as per index 'EmployeeID' sometimes summing values or using mean. Sometimes just take the last value in the list, in other case count unique values. \n\nLet's create a DataFrame **df**, which going to be the main dataframe for prediction:","e1071383":"Let's open ours files...","41851c14":"And the heatmap. It makes it easy to identify which features are most related to each other, we will plot heatmap of correlated features using the seaborn library.","d5630ee3":"## Feature Engineering ##","f33bd7b6":"# Feature engineering & selection","616910dc":"So, our main DataFrame is full of grouped data, let's split it by train-set and test-set:","f7b7c3af":"# Feature selection #\nFeature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n\nI will share 3 Feature selection techniques that are easy to use and also gives good results.\n\n1. Univariate \n2. Feature Importance\n3. Correlation Matrix with Heatmap","ce96032b":"Merging all files in one DataFrame","3d344333":"All feature relation-rates in one DataFrame","58aef3c3":"**2. Importance**\n\nYou can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your \noutput variable.\n\nFeature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 20 features for the dataset:","0872852b":"**3. Correlation**\n\nCorrelation states how the features are related to each other or the target variable.\n\nFirst of all, let's find out how ours target is correlated to the Features.\n\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nTop 20 of most correlated features to our 'target':"}}