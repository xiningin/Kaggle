{"cell_type":{"c939efc9":"code","0fdf7ddb":"code","8506d4c6":"code","ae64c3e8":"code","e14f1302":"code","ff2951cd":"code","44bb0e4b":"code","0213d4c7":"code","2f1c0c5e":"code","62dfd119":"code","9db28315":"code","e62ea5a9":"code","3427deb0":"code","67027e91":"code","448f8552":"markdown","8543b8b3":"markdown","d09b4091":"markdown","f26bdb40":"markdown","9bafc1ca":"markdown"},"source":{"c939efc9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0fdf7ddb":"train_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest_df  = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission=pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","8506d4c6":"train_df.describe()","ae64c3e8":"train_df.head()","e14f1302":"### To have a look at the distribution of the target:\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train_df['pressure'], ax=ax[0])\nsns.countplot(train_df['pressure'], ax=ax[1])","ff2951cd":"test_df.describe()","44bb0e4b":"test_df.head()","0213d4c7":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_indexs = train_df.index\ntest_indexs = test_df.index\n\nall_df = pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\nall_df = all_df.drop('id', axis=1)\nall_df = all_df.drop('pressure', axis=1)","2f1c0c5e":"all_df.head()","62dfd119":"def simple_eda(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('---')\n    print('Common Information')\n    print('---')\n    print(df.info())\n    \n    # missing values\n    print('---')\n    if df.isna().sum().sum() == 0:\n        print('There are no missing values')\n    else:\n        print('Detected')\n        display(df.isna().sum())\n    \n    # same describe() method for continious features\n    print('---')\n    print('Continuous Columns')\n    print('Total {}'.format(len(df.select_dtypes(include=['int', 'float']).columns)))\n    print('---')\n    display(df.describe())\n    \n    #checking for duplicated rows\n    if df.duplicated().sum() == 0:\n        print('---')\n        print('There are no duplicates')\n        print('---')\n    else:\n        print('---')\n        print('Duplicates found')\n        print('---')\n        display(df[df.duplicated()])\n    \n    print('End of the report')","9db28315":"simple_eda(all_df)","e62ea5a9":"all_df.columns","3427deb0":"num_rows, num_cols = 3,2\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 12))\nf.suptitle('Distribution of Features', fontsize=16)\n\nfor index, column in enumerate(all_df.columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.kdeplot(all_df[column], color=\"m\", shade=True, label=\"%.2f\"%(all_df[column].skew()), ax=axes[i,j])\n    g = g.legend(loc=\"best\")\n\nplt.tight_layout()\nplt.show()","67027e91":"corr = all_df.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","448f8552":"## Please upvote if you like my work. \ud83d\udc4d\n## Thanks a lot for your encouragement. Have a nice day! \ud83d\ude09","8543b8b3":"### Train Dataset Analysis at First Glance:","d09b4091":"### Test Dataset Analysis at First Glance:","f26bdb40":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center\">Google Brain-Ventilator Pressure Prediction<\/p> Exploratory Data Analysis \ud83e\uddd0","9bafc1ca":"### Join Train and Test Datasets in order to Obtain An Overview:"}}