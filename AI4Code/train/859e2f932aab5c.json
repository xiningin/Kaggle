{"cell_type":{"7557ee13":"code","f141c4c0":"code","fd0ac960":"code","d74e59df":"code","e123e9e3":"code","432fde6d":"code","080b16a3":"code","9ed43328":"code","08e7979f":"code","f7f2c966":"code","2efcc905":"code","8f6dfdba":"code","38d64022":"code","d91c24d9":"code","873d205a":"code","5777254b":"code","1d3131c9":"code","ef18d10b":"code","8ca54553":"code","83ec9be4":"code","48dbd4b4":"code","99c3b008":"code","598e5db5":"code","08a6d623":"code","54e5319e":"code","260bc71f":"code","d1155092":"code","1dfc9abe":"code","13026e4d":"markdown","f8f90e76":"markdown","2bbe9648":"markdown","a84d0dcc":"markdown","dfe5cafb":"markdown","efbca8c8":"markdown","02c71e54":"markdown","6289ed70":"markdown"},"source":{"7557ee13":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nPATH = '\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv'  ","f141c4c0":"import numpy as np\nimport pandas as pd\nimport string\nimport spacy\nimport pickle\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\nfrom dask_ml.model_selection import train_test_split\nfrom dask_ml.feature_extraction.text import HashingVectorizer\nfrom dask_ml.wrappers import Incremental\nfrom dask_ml.metrics import accuracy_score\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils import class_weight\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report\n","fd0ac960":"from dask.distributed import Client\n\nclient = Client(threads_per_worker=2,\n                n_workers=5, memory_limit='3GB')\nclient","d74e59df":"reqd = ['Text', 'Score']\nReviews_df = dd.read_csv(PATH,\n                         usecols = reqd,\n                         blocksize=20e6,\n                         dtype={'Score': 'float'},\n                         engine='python',\n                         encoding='utf-8',\n                         error_bad_lines=False)","e123e9e3":"Reviews_df.info()","432fde6d":"Reviews_df","080b16a3":"# frac = 1.0\n# Reviews_df = Reviews_df.sample(frac=frac, replace=True)","9ed43328":"Reviews_df['Score'].value_counts().compute()","08e7979f":"X = Reviews_df['Text']\nylabels = Reviews_df['Score'] ","f7f2c966":"keys = np.unique(ylabels.compute())\nvalues = class_weight.compute_class_weight('balanced',\n                                           keys,\n                                           ylabels.compute())\nclass_weights = dict(zip(keys, values))","2efcc905":"class_weights","8f6dfdba":"X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2)","38d64022":"# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()","d91c24d9":"def spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","873d205a":"from dask_ml.feature_extraction.text import HashingVectorizer\nhw_vector = HashingVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1, 2), n_features=2**20)","5777254b":"%time\nFeature_pipeline = Pipeline([('vectorizer', hw_vector)])\nPipeline_Model = Feature_pipeline.fit(X_train.values)","1d3131c9":"Text_preprocess_pipe = pickle.dumps(Pipeline_Model)","ef18d10b":"Pipeline_Model = pickle.loads(Text_preprocess_pipe)","8ca54553":"%time\nX_transformed = Pipeline_Model.transform(X_train)","83ec9be4":"%%time\nimport joblib\nestimator = SGDClassifier(random_state=10, max_iter=200, loss='modified_huber',class_weight = class_weights, n_jobs=-1)\nclassifier = Incremental(estimator)\nModel = classifier.fit(X_transformed,\n               y_train,\n               classes=list(class_weights.keys()))","48dbd4b4":"predictions = Model.predict(Pipeline_Model.transform(X_test))\npredictions","99c3b008":"accuracy_score(y_test, predictions)","598e5db5":"ML_Model = pickle.dumps(Model)","08a6d623":"%time\nModel = pickle.loads(ML_Model)\n# X = Model.predict_proba(X_transformed).compute()","54e5319e":"%time\nx_test_transformed = Pipeline_Model.transform(X_test)\ny_pred = Model.predict(x_test_transformed).compute()","260bc71f":"%time\nprint(classification_report(y_train,\n                            Model.predict(Pipeline_Model.transform(X_train)).compute()))","d1155092":"%time\nprint(classification_report(y_test, y_pred))","1dfc9abe":"client.close()","13026e4d":"# Setup","f8f90e76":"#### Initially work with subsample of the data for rapid prototyping","2bbe9648":"### Test Accuracy","a84d0dcc":"## HashingVectorizer\n\n> This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n> This strategy has several advantages:\n* > it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n* > it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n* > it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n\n> There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n* > there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n* > there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n* > no IDF weighting as this would render the transformer stateful.","dfe5cafb":"## References\n* https:\/\/examples.dask.org\/machine-learning\/text-vectorization.html\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.class_weight.compute_class_weight.html\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.HashingVectorizer.html","efbca8c8":"# Amazon Food Reviews Analysis using Spacy & Dask\n\n## Purpose\nThe purpose of this kernel is to illustrate the application of Dask and Spacy for Multiclass Text classification problem where the classes are unbalanced. \n\n## Methodology\nSetup the Dask distributed to handle the text preprocessing and model building in parallel. The model utilizes spacy tokenizer, Hashing vectorizer for text preprocessing. For model building SGD classifier of scikit learn is then used within Incremental wrapper of Dask framework.\n\n\n## Improvements\n- Need to find better ways to partition the data for faster analysis;\n\n## Results\n\n1. Mix of Unigram + Bigram with Class Weights\n    \n       Class    precision    recall  f1-score   support\n\n         1       0.58      0.69      0.63     10327\n         2       0.35      0.45      0.39      6000\n         3       0.39      0.43      0.41      8638\n         4       0.53      0.29      0.38     16028\n         5       0.85      0.88      0.86     72748\n \n                    \n  - accuracy       0.72\n\n\n## Suggested next steps\n- More extensive text clean up can be done using spacy with better exploratory analysis of text;\n- Certain fields within the Dataset where ignored which can be incorporated for better accuracy;\n- Hyperparameter Tuning.","02c71e54":"### Train Accuracy","6289ed70":"# Data processing"}}