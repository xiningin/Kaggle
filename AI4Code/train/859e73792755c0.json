{"cell_type":{"8fd606b4":"code","289dafc4":"code","0329afe6":"code","c48fbf2b":"code","84d10962":"code","cacea4dc":"code","8501f8ef":"code","fcfa0f07":"code","eb062191":"code","97c3965d":"code","c400d95c":"code","0d3a2aab":"code","f704dd5c":"code","e6abf21b":"code","a60f61d4":"code","5ac6a5ea":"code","9491f493":"code","51f7ce02":"code","48ea3243":"code","52076630":"code","7702ac2e":"code","febf0524":"markdown","069a879a":"markdown","4e5f87fb":"markdown","78d7c671":"markdown","5e82c785":"markdown","7146437b":"markdown","f2b82091":"markdown","e8dfb7a8":"markdown","8d3f2036":"markdown","eba4497c":"markdown","73853a77":"markdown","7be8a27b":"markdown","3cebdec6":"markdown"},"source":{"8fd606b4":"import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","289dafc4":"# Read the data\ntex = pd.read_csv('..\/input\/texture.csv',index_col=0)\n","0329afe6":"# Visualizing the missing data with heatmap\nsns.heatmap(tex.isnull(),yticklabels=False,cbar=False,cmap='viridis')","c48fbf2b":"# Filling the NaN values with 0.00\nclean = tex.fillna(0.00)\nclean.head()","84d10962":"# Check the columns.\nclean.info()","cacea4dc":"sns.heatmap(clean.isnull(),yticklabels=False,cbar=False,cmap='viridis')","8501f8ef":"# Value counts for the Lithology \ntop = clean['LITHOLOGY'].value_counts().head(10)\ntop.head()","fcfa0f07":"clean ['AREA'].value_counts().head(10)","eb062191":"# Singeling out the Georges Bank columns. \nGeorges = clean[clean['AREA']=='GEORGES BANK']","97c3965d":"# Finding correlations.\nsns.heatmap(Georges.corr(method='pearson'))","c400d95c":"#Here is a better visuals for latitude and longitude.\nsns.jointplot(x='LATITUDE',y='LATITUDE',\n              data=Georges,kind='reg',\n              color='b')\nsns.set(style='white',color_codes=True)","0d3a2aab":"# More detailed correlations with scikit \nfrom sklearn.preprocessing import LabelEncoder\nlabe = LabelEncoder()\ndic = {}\n\nlabe.fit(Georges.MONTH_COLL.drop_duplicates())\ndic['MONTH_COLL'] = list(labe.classes_)\nGeorges.MONTH_COLL = labe.transform(Georges.MONTH_COLL)","f704dd5c":"cor = ['LATITUDE','LONGITUDE','DEPTH_M','T_DEPTH','B_DEPTH']","e6abf21b":"kor = np.corrcoef(Georges[cor].values.T)","a60f61d4":"sns.set(font_scale=1.5)\nmap = sns.heatmap(kor,cbar=True,\n                  cmap=\"YlGnBu\",\n                  annot = True, \n                  square= True,\n                  fmt = '.1f',\n                  annot_kws = {'size':10}, \n                 yticklabels = cor,\n                 xticklabels = cor)","5ac6a5ea":"clean ['AREA'].value_counts().head(10)","9491f493":"gulf = clean[clean['AREA']=='GULF OF MEXICO']","51f7ce02":"gulf.head()","48ea3243":"sns.heatmap(gulf.corr())","52076630":"#the correlations look the same for both gulf and Georges bank\nsns.jointplot(x='LATITUDE',y='LATITUDE',\n              data=gulf,kind='reg',\n              color='b')\nsns.set(style='white',color_codes=True)","7702ac2e":"# We see that some of the correlation values are different. \nfrom sklearn.preprocessing import LabelEncoder\nlabe = LabelEncoder()\ndic = {}\n\nlabe.fit(gulf.MONTH_COLL.drop_duplicates())\ndic['MONTH_COLL'] = list(labe.classes_)\ngulf.MONTH_COLL = labe.transform(gulf.MONTH_COLL)\n\ncor = ['LATITUDE','LONGITUDE','DEPTH_M','T_DEPTH','B_DEPTH']\nkor = np.corrcoef(gulf[cor].values.T)\nsns.set(font_scale=1.5)\nmap = sns.heatmap(kor,cbar=True,\n                  cmap=\"YlGnBu\",\n                  annot = True, \n                  square= True,\n                  fmt = '.1f',\n                  annot_kws = {'size':10}, \n                 yticklabels = cor,\n                 xticklabels = cor)","febf0524":"As we can se above, there is a lot of data collected for Massachusetts and Georges bank. I want to analyse the data for Georges Bank and Gulf of Mexico.","069a879a":"# Texture database ","4e5f87fb":"This sediment database contains location, description, and texture of samples taken by numerous marine sampling programs. Most of the samples are from the Atlantic Continental Margin of the United States, but some are from as diverse locations as Lake Baikal, Russia, the Hawaiian Islands region, Puerto Rico, the Gulf of Mexico, and Lake Michigan. The database presently contains data for over 27,000 samples, which includes texture data for approximately 3800 samples taken or analyzed by the Atlantic Continental Margin Program (ACMP), a joint U.S. Geological Survey\/Woods Hole Oceanographic Institution project conducted from 1962 to 1970. As part of the ACMP, some historical data from samples collected between 1955 and 1962 were also incorporated into the dataset.","78d7c671":"First we have to clean up all the missing data. Now we don't have any data that we can plug in so instead i will replace the NaN with floats with value of 0.","5e82c785":"# George Bank visualizations","7146437b":"### In this kernel i will: ","f2b82091":"# Analisis of the Gulf ","e8dfb7a8":"    - Clean up missing data \n    - Find which location who has the most findings\n    - Analyse Georges bank\n    - Find correlations using different heatmaps","8d3f2036":"Now all the missing data is set as 0, which basically isnt a value, so it wont affect any models. ","eba4497c":"# Georges Bank Correlations \n ","73853a77":"# Top Lithology ","7be8a27b":"Lets check the info to see what floats we want to pick out, or the columns that looks interesting to check out.","3cebdec6":"First on the list is Georges Bank. \n\nNOTE: print ut ett bilde og fortell basics om stein derifra. Finn hva som er gjennomsnittstemp. flest type stein osv.  "}}