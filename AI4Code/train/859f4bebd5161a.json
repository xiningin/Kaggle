{"cell_type":{"4af60cda":"code","cb5ee3e7":"code","229b63d6":"code","4d82fd07":"code","57dbb728":"code","ee898934":"code","ba1d9528":"code","a734a38c":"code","9136901b":"code","67ac5af8":"code","dd93b4db":"code","1de27a13":"code","fb0f0ddb":"code","1c5bea65":"code","aa6f5db6":"code","c732d2b9":"code","2089cc71":"code","aaadb360":"code","bc211f52":"code","70390066":"code","7885a4d3":"code","101bd931":"code","3c5d0bae":"code","dc35a3bf":"code","e0462b22":"code","1cbcca75":"code","357e93d4":"code","0e5804f1":"code","21adc221":"code","16132f51":"code","62f194d1":"code","19d4315b":"code","880d29a2":"code","60cb1451":"code","098c76a7":"code","6a2dee5f":"code","1e21357a":"code","0f1f1713":"code","d659b63a":"code","9aa3253b":"code","8d8568c4":"code","1a44eabb":"code","f99a344e":"code","9ee84a7f":"code","92187022":"code","446e0bfe":"code","cfa754af":"code","ac71e29c":"code","9d3827bc":"code","e4a1e97f":"code","5596cd17":"code","712e3396":"code","caeeeef7":"code","2b0e13f2":"code","447f8934":"code","a3da9b67":"code","fe701ac0":"code","124770b8":"markdown","ef20d36d":"markdown","ee745143":"markdown","63678e1a":"markdown","1f0b0b25":"markdown","ee80af88":"markdown","bd9d84dd":"markdown"},"source":{"4af60cda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re\nimport string\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb5ee3e7":"df = pd.read_csv('\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv', header=0,index_col=0)\ndf.head()","229b63d6":"df.sample(5)","4d82fd07":"df.duplicated().sum()","57dbb728":"df.describe()","ee898934":"df.drop_duplicates(inplace=True)","ba1d9528":"df.info()","a734a38c":"# check null value percent\n(df.isna().sum()\/df.shape[0])*100","9136901b":"df.isna().sum()","67ac5af8":"df.shape","dd93b4db":"df.columns","1de27a13":"#type of variables\ndf.dtypes","fb0f0ddb":"df['Review Text'][1]","1c5bea65":"review = df[['Review Text', 'Recommended IND']]\nreview.head()","aa6f5db6":"#columns rename\nreview = review.rename(columns={\"Recommended IND\": \"Recommended\", \"Review Text\": \"Review\"})\nreview.head()","c732d2b9":"review.isnull().sum()","2089cc71":"review.dropna(subset=['Review'], inplace=True)","aaadb360":"review.isnull().sum()","bc211f52":"review['Review'] = review['Review'].astype(str)","70390066":"review.shape","7885a4d3":"review['Recommended'].value_counts()","101bd931":"review[\"Recommended\"].value_counts().plot(kind='bar')\nplt.xlabel(\"Recommended\")\nplt.ylabel(\"Counts\")\nplt.title(\"Proportion Target Class\")","3c5d0bae":"df.isnull()","dc35a3bf":"plt.figure(figsize=(10,10))\nax =df.Rating.value_counts()\nlabels=df['Rating'].value_counts().index\nplt.pie(ax,labels=labels,autopct='%.2f')\nplt.title(\"Number in which figure shown\",fontsize=25,color='purple')\nplt.legend()\nplt.show()","e0462b22":"teju=df.cov()\nteju","1cbcca75":"sns.heatmap(teju,annot=True)","357e93d4":"plt.figure(figsize=(10,10))\nax=df.Rating.value_counts()[:10]\nax.plot(kind='bar')","0e5804f1":"sns.distplot(df.Rating, color = 'red')","21adc221":"def tokens(words):\n    words = re.sub(\"[^a-zA-Z]\",\" \", words)\n    text = words.lower().split()                   \n    return \" \".join(text)","16132f51":"review['Review_clear'] = review['Review'].apply(tokens)\nreview.head()","62f194d1":"review['Review_clear'] = review['Review_clear'].astype(str)","19d4315b":"import nltk\nnltk.download('stopwords')","880d29a2":"# Show some stop words\nstop_words = stopwords.words('english')\nprint(stop_words[::10])","60cb1451":"#clothing stopwords\nclothes =['dress','color','wear','top','sweater','material','shirt','jeans','pant',\n          'skirt','order','white','black','fabric','blouse','sleeve','even', 'jacket']","098c76a7":"def stopwords(review):\n    text = [word.lower() for word in review.split() if word.lower() not in stop_words and word.lower() not in clothes]\n    return \" \".join(text)","6a2dee5f":"review['Review_clear'] = review['Review_clear'].apply(stopwords)\nreview.head()","1e21357a":"review['Review_clear'][267]","0f1f1713":"import nltk\nnltk.download('wordnet')","d659b63a":"lem = WordNetLemmatizer()\n\ndef lemma(text):\n    lem_text = [lem.lemmatize(word) for word in text.split()]\n    return \" \".join(lem_text)","9aa3253b":"review['Review_clear'] = review['Review_clear'].apply(lemma)\nreview.head()","8d8568c4":"review['Review_clear'][1]","1a44eabb":"positive = review[review.Recommended== 1]\nnegative = review[review.Recommended== 0]\npositive.head()","f99a344e":"positive_words =[]\n\nfor review in positive.Review_clear:\n    positive_words.append(review) \npositive_words = ' '.join(positive_words)\npositive_words[:48]","9ee84a7f":"negative_words = []\n\nfor review in negative.Review_clear:\n    negative_words.append(review)\nnegative_words = ' '.join(negative_words)\nnegative_words[:455]","92187022":"wordcloud = WordCloud(background_color=\"white\", max_words=len(positive_words))\n\nwordcloud.generate(positive_words)\n\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","446e0bfe":"wordcloud = WordCloud(background_color=\"white\", max_words=len(negative_words), colormap='gist_heat')\n\nwordcloud.generate(negative_words)\n\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","cfa754af":"negative.head()","ac71e29c":"X = positive['Review_clear']\ny = positive['Recommended']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)","9d3827bc":"vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","e4a1e97f":"model_nb = Pipeline([('vect', CountVectorizer(min_df=5, ngram_range=(1,2))),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf',MultinomialNB()),\n                   ])\n\nmodel_nb.fit(X_train, y_train)\n\nytest = np.array(y_test)\npred_y = model_nb.predict(X_test)","5596cd17":"print('accuracy %s' % accuracy_score(pred_y, y_test))\nprint(classification_report(ytest, pred_y))","712e3396":"model_rf = Pipeline([('vect', CountVectorizer(min_df=5, ngram_range=(1,2))),\n                    ('tfidf', TfidfTransformer()),\n                    ('clf-rf', RandomForestClassifier(n_estimators=50)),\n                    ])\n\nmodel_rf.fit(X_train, y_train)\n\nytest = np.array(y_test)\npred = model_rf.predict(X_test)","caeeeef7":"print('accuracy %s' % accuracy_score(pred, y_test))\nprint(classification_report(ytest, pred))","2b0e13f2":"from sklearn.ensemble import AdaBoostClassifier\n\nada = Pipeline([('vect', CountVectorizer(min_df=5, ngram_range=(1,2))),\n                ('tfidf', TfidfTransformer()),\n                ('clf-ada', AdaBoostClassifier()),\n                ])\n\nada.fit(X_train, y_train)\nytest = np.array(y_test)\nada_pred = ada.predict(X_test)","447f8934":"print('accuracy %s' % accuracy_score(ada_pred, y_test))\nprint(classification_report(ytest, ada_pred))","a3da9b67":"nb_acc = accuracy_score(pred_y, y_test)\nrf_acc = accuracy_score(pred, y_test)\nada_acc = accuracy_score(ada_pred, y_test)","fe701ac0":"models = pd.DataFrame({\n                      'Model': ['Naive Bayes', 'Random Forest', 'AdaBoosting'],\n                      'Score': [nb_acc,rf_acc, ada_acc]})\nmodels.sort_values(by='Score', ascending=False)","124770b8":"# Ada Boosting","ef20d36d":"# Models\nWe use the following classification models:\n\nLogistic Regression,\nNaive Bayes,\nSupport Vector Machine,\nRandom Forest,\nAda Boosting.","ee745143":"# Which one is the best Model?","63678e1a":"# Random Forest","1f0b0b25":"# THANK YOU","ee80af88":"# Conclusion\nThis project was aimed to used sentiment analysis to determined product recommendation. We started with the data engineering and text mining, which cover change text into tokens, remove punctuation, numbers, stop words and normalization them by using lemmatization. Following we used bag of words model to convert the text into numerical feature vectors. ","bd9d84dd":"# Naive Bayes\nTo make the vectorizer => transformer => classifier easier to work with, we will use Pipeline class in Scilkit-Learn."}}