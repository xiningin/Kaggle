{"cell_type":{"f0527503":"code","647c953e":"code","b114aa63":"code","31c4188e":"code","a4a3d7c0":"code","fbb0622e":"code","9a53b7fd":"code","14a854dd":"code","d1931e77":"code","986f57e3":"code","64c24607":"code","23bdaa01":"code","539832fc":"code","3f9bcaa4":"code","e9152a72":"code","e3537ad2":"code","43d5fc60":"code","7dda94c8":"code","4f585175":"code","48fec7bf":"code","f6889d01":"code","cd2dc665":"code","f4fb2c65":"code","7ceef013":"code","8b041497":"code","2c8b730b":"code","13fb1da2":"code","e3246f17":"code","d8806b1f":"code","463013bf":"code","f8ee5ebe":"code","fe8d2880":"code","999091b6":"code","a411d2fa":"code","6b196af8":"code","c0babe55":"code","d6257137":"code","35ac0bcf":"code","3b1732ec":"code","409e7c89":"code","1bb50454":"code","7aed6f69":"code","030cf4c7":"code","5a01def2":"code","3651522d":"code","afe67935":"code","f516b3de":"code","cd1ffaeb":"code","63dab2f3":"code","28d78361":"code","552c0046":"code","8b0af541":"code","ed061ed0":"code","4b5ddd80":"code","cb16376b":"code","98f09127":"code","00ce5519":"code","a452db9a":"code","c4d2067d":"code","3174721f":"code","5d1f50f1":"code","ef6c4795":"code","62c8905c":"code","43d3f120":"code","55cda879":"code","c7b5d4cf":"code","2c6127eb":"code","d1364280":"code","f862821a":"code","4593a1c1":"code","a091a0d7":"code","74756917":"code","ac32ccdd":"code","d239bb14":"code","4a7ef46a":"code","ae52bf06":"code","68004a45":"code","dae36d03":"code","b944f808":"code","a2ad19da":"code","30565b78":"code","b8ff085b":"code","78e06799":"code","5c62edd2":"code","7a68a299":"code","4bf78aba":"code","ecc53c7c":"code","2eb4b065":"code","41c674ca":"code","3242c823":"code","96a65daa":"code","5bc9a28b":"code","28375b9e":"code","e305f323":"code","3a72690c":"code","16fdd51e":"code","d15bcafa":"code","06bd5ec3":"code","0b4f3633":"code","0ee2acb9":"code","1adee82d":"code","704b2173":"code","57fc3af2":"code","431de284":"code","7cf1aae5":"code","c03b9f40":"code","9eeaa4ee":"code","0d368722":"code","c2e67a87":"code","1e0d96e3":"code","d0f35c90":"code","29ccfaea":"code","9187cf63":"code","5d9d8a2a":"code","7cc826aa":"code","0b8f9b32":"code","c694d8ee":"code","f8befcdd":"code","b14866a4":"code","979f5514":"code","a607ac1c":"code","1099ea7d":"markdown","ae55ec67":"markdown","2a8d23cb":"markdown","2c47d3d0":"markdown","0dd0722c":"markdown","8b90f052":"markdown","dbc1356c":"markdown","2d89e54f":"markdown","2cbfc8e0":"markdown","d628056c":"markdown","c21ec530":"markdown","6b7b90f9":"markdown","f5b7ead5":"markdown","152df841":"markdown","34d45711":"markdown","05876bc2":"markdown","241be0d5":"markdown","a86b6a8d":"markdown","2c8d66ce":"markdown","198ce08d":"markdown","5a8306f1":"markdown","0bfe876a":"markdown","18a796b6":"markdown","5401f9df":"markdown","38238350":"markdown","46b68993":"markdown","19592d87":"markdown","1544c97e":"markdown","cb9bab32":"markdown","81fa7fe9":"markdown","202db9e9":"markdown","0010651a":"markdown","fd67b673":"markdown","c3d66906":"markdown","6d04701b":"markdown","6d88acdd":"markdown","73878253":"markdown","e9c12cc0":"markdown","27452a03":"markdown","855eb437":"markdown","c1318c24":"markdown","1a3fec48":"markdown","713b7711":"markdown"},"source":{"f0527503":"#Loading necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom collections import defaultdict, Counter\nimport warnings\nimport re\nimport random  \n\nplt.style.use('ggplot')\nwarnings.filterwarnings(\"ignore\")\nrandom.seed(42) ","647c953e":"#read the training data\ndf_train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n","b114aa63":"df_train.head()","31c4188e":"#let's get some info on our dataset!\ndf_train.info()","a4a3d7c0":"df_train.isnull().sum()","fbb0622e":"df_train.isnull().sum()\/df_train.shape[0]*100","9a53b7fd":"sns.countplot(df_train['target'])","14a854dd":"df_train['TweetLength'] = df_train['text'].apply(lambda x: len(x))\ndf_train.head()","d1931e77":"print(f\"Average tweet length: {np.round(df_train['TweetLength'].mean(),2)}\")","986f57e3":"fig = plt.figure(figsize=(8,5))\nsns.distplot(df_train.loc[df_train['target']==1,'TweetLength'],color='red', label='disaster')\nsns.distplot(df_train.loc[df_train['target']==0,'TweetLength'],color='skyblue', label='not disaster')\nfig.legend(labels=['disaster','not disaster'])","64c24607":"f, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,7), sharex=True)\nsns.distplot( df_train.loc[df_train['target']==1,'TweetLength'],color='red',  ax=ax1)\nax1.set_title('Disaster')\nsns.distplot( df_train.loc[df_train['target']==0,'TweetLength'],color='skyblue', ax=ax2)\nax2.set_title('Not disaster')\nplt.suptitle('Tweet length')\n","23bdaa01":"df_train['NumWordsPerTweet'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_train.head()","539832fc":"print(f\"Number of average number of words per tweet: {np.round(df_train['NumWordsPerTweet'].mean(),2)}\")","3f9bcaa4":"fig = plt.figure(figsize=(8,5))\nsns.distplot(df_train.loc[df_train['target']==1,'NumWordsPerTweet'],color='red', label='disaster')\nsns.distplot(df_train.loc[df_train['target']==0,'NumWordsPerTweet'],color='skyblue', label='not disaster')\nfig.legend(labels=['disaster','not disaster'])","e9152a72":"f, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5), sharex=True)\nsns.distplot( df_train.loc[df_train['target']==1,'NumWordsPerTweet'],color='red',  ax=ax1)\nax1.set_title('Disaster')\nsns.distplot( df_train.loc[df_train['target']==0,'NumWordsPerTweet'],color='skyblue', ax=ax2)\nax2.set_title('Not disaster')\nplt.suptitle('Tweet length')","e3537ad2":"df_train['AvgWordLength'] = df_train['text'].apply(lambda x: sum([len(i) for i in x.split()])\/len(x.split()))\ndf_train.head()","43d5fc60":"fig = plt.figure(figsize=(8,5))\nsns.distplot(df_train.loc[df_train['target']==1,'AvgWordLength'],color='red', label='disaster')\nsns.distplot(df_train.loc[df_train['target']==0,'AvgWordLength'],color='skyblue', label='not disaster')\nfig.legend(labels=['disaster','not disaster'])","7dda94c8":"f, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5), sharex=True)\nsns.distplot(df_train.loc[df_train['target']==1,'AvgWordLength'],color='red',  ax=ax1)\nax1.set_title('Disaster')\nsns.distplot( df_train.loc[df_train['target']==0,'AvgWordLength'],color='skyblue', ax=ax2)\nax2.set_title('Not disaster')\nplt.suptitle('Tweet length')","4f585175":"stop=set(stopwords.words('english'))\n","48fec7bf":"def create_corpus(target):\n    corpus=[]\n    \n    for x in df_train[df_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i.lower())\n    return corpus","f6889d01":"corpus_disaster = create_corpus(1)\ncorpus_non_disaster = create_corpus(0)","cd2dc665":"dic_disaster=defaultdict(int)\nfor word in corpus_disaster:\n    if word in stop:\n        dic_disaster[word]+=1\n        \ndic_non_disaster=defaultdict(int)\nfor word in corpus_non_disaster:\n    if word in stop:\n        dic_non_disaster[word]+=1        ","f4fb2c65":"top_disaster = sorted(dic_disaster.items(), key=lambda x:x[1],reverse=True)[:10] \ntop_non_disaster = sorted(dic_non_disaster.items(), key=lambda x:x[1],reverse=True)[:10] ","7ceef013":"x1,y1=zip(*top_disaster)\nx2,y2=zip(*top_non_disaster)\n\n\ntop_stop = pd.DataFrame({'Target':[1]*10+[0]*10, 'Word': [x for x in x1]+[x for x in x2], 'Occurances': [y for y in y1]+[y for y in y2] })\n","8b041497":"plt.figure(figsize=(12,7))\nsns.barplot(x='Word',y='Occurances',data = top_stop,hue='Target')","2c8b730b":"import string\npunct = string.punctuation\npunct","13fb1da2":"dic_disaster_punct=defaultdict(int)\nfor word in corpus_disaster:\n    if word in punct:\n        dic_disaster_punct[word]+=1\n        \ndic_non_disaster_punct=defaultdict(int)\nfor word in corpus_non_disaster:\n    if word in punct:\n        dic_non_disaster_punct[word]+=1   ","e3246f17":"top_disaster_punct = sorted(dic_disaster_punct.items(), key=lambda x:x[1],reverse=True)[:10] \ntop_non_disaster_punct = sorted(dic_non_disaster_punct.items(), key=lambda x:x[1],reverse=True)[:10] ","d8806b1f":"x1,y1=zip(*top_disaster_punct)\nx2,y2=zip(*top_non_disaster_punct)\n\n\ntop_punct = pd.DataFrame({'Target':[1]*10+[0]*10, 'Word': [x for x in x1]+[x for x in x2], 'Occurances': [y for y in y1]+[y for y in y2] })\n","463013bf":"plt.figure(figsize=(12,7))\n\nsns.barplot(x='Word',y='Occurances',data = top_punct,hue='Target')","f8ee5ebe":"counter=Counter(corpus_disaster)\nmost_disaster=counter.most_common()\nx1=[]\ny1=[]\nfor word,count in most_disaster[:50]:\n    if (word not in stop) :\n        x1.append(word)\n        y1.append(count)\n        \ncounter=Counter(corpus_non_disaster)\nmost_non_disaster=counter.most_common()\nx2=[]\ny2=[]\nfor word,count in most_non_disaster[:50]:\n    if (word not in stop) :\n        x2.append(word)\n        y2.append(count)        ","fe8d2880":"x1","999091b6":"len(x2)","a411d2fa":"plt.figure(figsize=(10,5))\nsns.barplot(x=x1,y=y1)\nplt.title('Disaster')\nplt.figure(figsize=(10,5))\nsns.barplot(x=x2,y=y2)\nplt.title('Not disaster')\n","6b196af8":"def generate_ngrams(text, n):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in stop]\n    ngrams = zip(*[token[i:] for i in range(n)])\n    return [' '.join(ngram) for ngram in ngrams]","c0babe55":"print(generate_ngrams(df_train.text[0],2))","d6257137":"disaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in df_train[df_train['target']==1]['text']:\n    for word in generate_ngrams(tweet,2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in df_train[df_train['target']==0]['text']:\n    for word in generate_ngrams(tweet,2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1],columns=['bigrams','frequency'])\n\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1],columns=['bigrams','frequency'])        ","35ac0bcf":"df_disaster_bigrams.head()","3b1732ec":"plt.figure(figsize = (12,5))\nsns.barplot(y='bigrams',x='frequency', data=df_disaster_bigrams[:20])\nplt.title('Common bigrams - Disaster')\n\nplt.figure(figsize = (12,5))\nsns.barplot(y='bigrams',x='frequency',data=df_nondisaster_bigrams[:20] )\nplt.title('Common bigrams - Not Disaster')","409e7c89":"disaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in df_train[df_train['target']==1]['text']:\n    for word in generate_ngrams(tweet,3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in df_train[df_train['target']==0]['text']:\n    for word in generate_ngrams(tweet,3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1],columns=['trigrams','frequency'])\n\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1],columns=['trigrams','frequency'])        ","1bb50454":"plt.figure(figsize = (12,5))\nsns.barplot(y='trigrams',x='frequency', data=df_disaster_trigrams[:20])\nplt.title('Common trigrams - Disaster')\n\nplt.figure(figsize = (12,5))\nsns.barplot(y='trigrams',x='frequency',data=df_nondisaster_trigrams[:20] )\nplt.title('Common trigrams - Not Disaster')","7aed6f69":"url_count = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\nurl_count.sum()\/df_train.shape[0]","030cf4c7":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","5a01def2":"print(df_train.iloc[31].text)","3651522d":"print(remove_URL(df_train.iloc[31].text))","afe67935":"#Let's use our newly created functio and remove URLs\ndf_train['text']=df_train['text'].apply(lambda x : remove_URL(x))","f516b3de":"df_train.iloc[31]['text']","cd1ffaeb":"my_text = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","63dab2f3":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n","28d78361":"print(remove_html(my_text))","552c0046":"df_train['text'] = df_train[\"text\"].apply(lambda x: remove_html(x))","8b0af541":"my_text = \"omg there was a huge eartquake in zone y \ud83d\ude2d\ud83d\ude2d\"","ed061ed0":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","4b5ddd80":"print(remove_emoji(my_text))","cb16376b":"df_train['text'] = df_train[\"text\"].apply(lambda x: remove_emoji(x))","98f09127":"def remove_contractions(text):\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"We're\", \"We are\", text)\n    text = re.sub(r\"That's\", \"That is\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"they're\", \"they are\", text)\n    text = re.sub(r\"Can't\", \"Cannot\", text)\n    text = re.sub(r\"wasn't\", \"was not\", text)\n    text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n    text = re.sub(r\"aren't\", \"are not\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"What's\", \"What is\", text)\n    text = re.sub(r\"haven't\", \"have not\", text)\n    text = re.sub(r\"hasn't\", \"has not\", text)\n    text = re.sub(r\"There's\", \"There is\", text)\n    text = re.sub(r\"He's\", \"He is\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"You're\", \"You are\", text)\n    text = re.sub(r\"I'M\", \"I am\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"wouldn't\", \"would not\", text)\n    text = re.sub(r\"i'm\", \"I am\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\"Isn't\", \"is not\", text)\n    text = re.sub(r\"Here's\", \"Here is\", text)\n    text = re.sub(r\"you've\", \"you have\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n    text = re.sub(r\"we're\", \"we are\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"we've\", \"we have\", text)\n    text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n    text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n    text = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", text)\n    text = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", text)\n    text = re.sub(r\"y'all\", \"you all\", text)\n    text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"it'll\", \"it will\", text)\n    text = re.sub(r\"we'll\", \"we will\", text)\n    text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n    text = re.sub(r\"We've\", \"We have\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"Y'all\", \"You all\", text)\n    text = re.sub(r\"Weren't\", \"Were not\", text)\n    text = re.sub(r\"Didn't\", \"Did not\", text)\n    text = re.sub(r\"they'll\", \"they will\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)\n    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n    text = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", text)\n    text = re.sub(r\"they've\", \"they have\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", text)\n    text = re.sub(r\"we'd\", \"we would\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"weren't\", \"were not\", text)\n    text = re.sub(r\"They're\", \"They are\", text)\n    text = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n    text = re.sub(r\"let's\", \"let us\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"i've\", \"I have\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"doesn't\", \"does not\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"ain't\", \"am not\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"I've\", \"I have\", text)\n    text = re.sub(r\"Don't\", \"do not\", text)\n    text = re.sub(r\"I'll\", \"I will\", text)\n    text = re.sub(r\"I'd\", \"I would\", text)\n    text = re.sub(r\"Let's\", \"Let us\", text)\n    text = re.sub(r\"you'd\", \"You would\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"Ain't\", \"am not\", text)\n    text = re.sub(r\"Haven't\", \"Have not\", text)\n    text = re.sub(r\"Could've\", \"Could have\", text)\n    text = re.sub(r\"youve\", \"you have\", text)  \n    text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)   \n    text = re.sub(r\"&gt;\", \">\", text)\n    text = re.sub(r\"&lt;\", \"<\", text)\n    text = re.sub(r\"&amp;\", \"&\", text)\n    return text","00ce5519":"my_example = \"That's the worst nightmare I've ever had!\"\n\nprint(remove_contractions(my_example))","a452db9a":"df_train['text'] = df_train[\"text\"].apply(lambda x: remove_contractions(x))","c4d2067d":"def remove_abreviations(sentence):\n    for text in sentence.split():\n    \n        text = re.sub(r\"$\" , \" dollar \", text)\n        text = re.sub(r\"\u20ac\" , \" euro \", text)\n        text = re.sub(r\"4ao\" , \"for adults only\", text)\n        text = re.sub(r\"a.m\" , \"before midday\", text)\n        text = re.sub(r\"a3\" , \"anytime anywhere anyplace\", text)\n        text = re.sub(r\"aamof\" , \"as a matter of fact\", text)\n        text = re.sub(r\"acct\" , \"account\", text)\n        text = re.sub(r\"adih\" , \"another day in hell\", text)\n        text = re.sub(r\"afaic\" , \"as far as i am concerned\", text)\n        text = re.sub(r\"afaict\" , \"as far as i can tell\", text)\n        text = re.sub(r\"afaik\" , \"as far as i know\", text)\n        text = re.sub(r\"afair\" , \"as far as i remember\", text)\n        text = re.sub(r\"afk\" , \"away from keyboard\", text)\n        text = re.sub(r\"app\" , \"application\", text)\n        text = re.sub(r\"approx\" , \"approximately\", text)\n        text = re.sub(r\"apps\" , \"applications\", text)\n        text = re.sub(r\"asap\" , \"as soon as possible\", text)\n        text = re.sub(r\"asl\" , \"age, sex, location\", text)\n        text = re.sub(r\"atk\" , \"at the keyboard\", text)\n        text = re.sub(r\"ave.\" , \"avenue\", text)\n        text = re.sub(r\"aymm\" , \"are you my mother\", text)\n        text = re.sub(r\"ayor\" , \"at your own risk\",  text)\n        text = re.sub(r\"b&b\" , \"bed and breakfast\", text)\n        text = re.sub(r\"b+b\" , \"bed and breakfast\", text)\n        text = re.sub(r\"b.c\" , \"before christ\", text)\n        text = re.sub(r\"b2b\" , \"business to business\", text)\n        text = re.sub(r\"b2c\" , \"business to customer\", text)\n        text = re.sub(r\"b4\" , \"before\", text)\n        text = re.sub(r\"b4n\" , \"bye for now\", text)\n        text = re.sub(r\"b@u\" , \"back at you\", text)\n        text = re.sub(r\"bae\" , \"before anyone else\", text)\n        text = re.sub(r\"bak\" , \"back at keyboard\", text)\n        text = re.sub(r\"bbbg\" , \"bye bye be good\", text)\n        text = re.sub(r\"bbc\" , \"british broadcasting corporation\", text)\n        text = re.sub(r\"bbias\" , \"be back in a second\", text)\n        text = re.sub(r\"bbl\" , \"be back later\", text)\n        text = re.sub(r\"bbs\" , \"be back soon\", text)\n        text = re.sub(r\"be4\" , \"before\", text)\n        text = re.sub(r\"bfn\" , \"bye for now\", text)\n        text = re.sub(r\"blvd\" , \"boulevard\", text)\n        text = re.sub(r\"bout\" , \"about\", text)\n        text = re.sub(r\"brb\" , \"be right back\", text)\n        text = re.sub(r\"bros\" , \"brothers\", text)\n        text = re.sub(r\"brt\" , \"be right there\", text)\n        text = re.sub(r\"bsaaw\" , \"big smile and a wink\", text)\n        text = re.sub(r\"btw\" , \"by the way\", text)\n        text = re.sub(r\"bwl\" , \"bursting with laughter\", text)\n        text = re.sub(r\"c\/o\" , \"care of\", text)\n        text = re.sub(r\"cet\" , \"central european time\", text)\n        text = re.sub(r\"cf\" , \"compare\", text)\n        text = re.sub(r\"cia\" , \"central intelligence agency\", text)\n        text = re.sub(r\"csl\" , \"can not stop laughing\", text)\n        text = re.sub(r\"cu\" , \"see you\", text)\n        text = re.sub(r\"cul8r\" , \"see you later\", text)\n        text = re.sub(r\"cv\" , \"curriculum vitae\", text)\n        text = re.sub(r\"cwot\" , \"complete waste of time\", text)\n        text = re.sub(r\"cya\" , \"see you\", text)\n        text = re.sub(r\"cyt\" , \"see you tomorrow\", text)\n        text = re.sub(r\"dae\" , \"does anyone else\", text)\n        text = re.sub(r\"dbmib\" , \"do not bother me i am busy\", text)\n        text = re.sub(r\"diy\" , \"do it yourself\", text)\n        text = re.sub(r\"dm\" , \"direct message\", text)\n        text = re.sub(r\"dwh\" , \"during work hours\", text)\n        text = re.sub(r\"e123\" , \"easy as one two three\", text)\n        text = re.sub(r\"eet\" , \"eastern european time\", text)\n        text = re.sub(r\"eg\" , \"example\", text)\n        text = re.sub(r\"embm\" , \"early morning business meeting\", text)\n        text = re.sub(r\"encl\" , \"enclosed\", text)\n        text = re.sub(r\"encl.\" , \"enclosed\", text)\n        text = re.sub(r\"etc\" , \"and so on\", text)\n        text = re.sub(r\"faq\" , \"frequently asked questions\", text)\n        text = re.sub(r\"fawc\" , \"for anyone who cares\", text)\n        text = re.sub(r\"fb\" , \"facebook\", text)\n        text = re.sub(r\"fc\" , \"fingers crossed\", text)\n        text = re.sub(r\"fig\" , \"figure\", text)\n        text = re.sub(r\"fimh\" , \"forever in my heart\",  text)\n        text = re.sub(r\"ft.\" , \"feet\", text)\n        text = re.sub(r\"ft\" , \"featuring\", text)\n        text = re.sub(r\"ftl\" , \"for the loss\", text)\n        text = re.sub(r\"ftw\" , \"for the win\", text)\n        text = re.sub(r\"fwiw\" , \"for what it is worth\", text)\n        text = re.sub(r\"fyi\" , \"for your information\", text)\n        text = re.sub(r\"g9\" , \"genius\", text)\n        text = re.sub(r\"gahoy\" , \"get a hold of yourself\", text)\n        text = re.sub(r\"gal\" , \"get a life\", text)\n        text = re.sub(r\"gcse\" , \"general certificate of secondary education\", text)\n        text = re.sub(r\"gfn\" , \"gone for now\", text)\n        text = re.sub(r\"gg\" , \"good game\",  text)\n        text = re.sub(r\"gl\" , \"good luck\", text)\n        text = re.sub(r\"glhf\" , \"good luck have fun\", text)\n        text = re.sub(r\"gmt\" , \"greenwich mean time\", text)\n        text = re.sub(r\"gmta\" , \"great minds think alike\", text)\n        text = re.sub(r\"gn\" , \"good night\", text)\n        text = re.sub(r\"g.o.a.t\" , \"greatest of all time\", text)\n        text = re.sub(r\"goat\" , \"greatest of all time\", text)\n        text = re.sub(r\"goi\" , \"get over it\", text)\n        text = re.sub(r\"gps\" , \"global positioning system\", text)\n        text = re.sub(r\"gr8\" , \"great\", text)\n        text = re.sub(r\"gratz\" , \"congratulations\", text)\n        text = re.sub(r\"gyal\" , \"girl\", text)\n        text = re.sub(r\"h&c\" , \"hot and cold\", text)\n        text = re.sub(r\"hp\" , \"horsepower\", text)\n        text = re.sub(r\"hr\" , \"hour\", text)\n        text = re.sub(r\"hrh\" , \"his royal highness\", text)\n        text = re.sub(r\"ht\" , \"height\", text)\n        text = re.sub(r\"ibrb\" , \"i will be right back\", text)\n        text = re.sub(r\"ic\" , \"i see\", text)\n        text = re.sub(r\"icq\" , \"i seek you\", text)\n        text = re.sub(r\"icymi\" , \"in case you missed it\", text)\n        text = re.sub(r\"idc\" , \"i do not care\", text)\n        text = re.sub(r\"idgadf\" , \"i do not give a damn fuck\", text)\n        text = re.sub(r\"idgaf\" , \"i do not give a fuck\", text)\n        text = re.sub(r\"idk\" , \"i do not know\", text)\n        text = re.sub(r\"ie\" , \"that is\", text)\n        text = re.sub(r\"i.e\" , \"that is\", text)\n        text = re.sub(r\"ifyp\" , \"i feel your pain\", text)\n        text = re.sub(r\"IG\" , \"instagram\", text)\n        text = re.sub(r\"iirc\" , \"if i remember correctly\", text)\n        text = re.sub(r\"ilu\" , \"i love you\", text)\n        text = re.sub(r\"ily\" , \"i love you\", text)\n        text = re.sub(r\"imho\" , \"in my humble opinion\", text)\n        text = re.sub(r\"imo\" , \"in my opinion\", text)\n        text = re.sub(r\"imu\" , \"i miss you\", text)\n        text = re.sub(r\"iow\" , \"in other words\", text)\n        text = re.sub(r\"irl\" , \"in real life\", text)\n        text = re.sub(r\"j4f\" , \"just for fun\", text)\n        text = re.sub(r\"jic\" , \"just in case\", text)\n        text = re.sub(r\"jk\" , \"just kidding\", text)\n        text = re.sub(r\"jsyk\" , \"just so you know\", text)\n        text = re.sub(r\"l8r\" , \"later\", text)\n        text = re.sub(r\"lb\" , \"pound\", text)\n        text = re.sub(r\"lbs\" , \"pounds\", text)\n        text = re.sub(r\"ldr\" , \"long distance relationship\", text)\n        text = re.sub(r\"lmao\" , \"laugh my ass off\", text)\n        text = re.sub(r\"lmfao\" , \"laugh my fucking ass off\", text)\n        text = re.sub(r\"lol\" , \"laughing out loud\", text)\n        text = re.sub(r\"ltd\" , \"limited\", text)\n        text = re.sub(r\"ltns\" , \"long time no see\", text)\n        text = re.sub(r\"m8\" , \"mate\", text)\n        text = re.sub(r\"mf\" , \"motherfucker\", text)\n        text = re.sub(r\"mfs\" , \"motherfuckers\", text)\n        text = re.sub(r\"mfw\" , \"my face when\", text)\n        text = re.sub(r\"mofo\" , \"motherfucker\", text)\n        text = re.sub(r\"mph\" , \"miles per hour\", text)\n        text = re.sub(r\"mr\" , \"mister\", text)\n        text = re.sub(r\"mrw\" , \"my reaction when\", text)\n        text = re.sub(r\"ms\" , \"miss\", text)\n        text = re.sub(r\"mte\" , \"my thoughts exactly\", text)\n        text = re.sub(r\"nagi\" , \"not a good idea\", text)\n        text = re.sub(r\"nbc\" , \"national broadcasting company\", text)\n        text = re.sub(r\"nbd\" , \"not big deal\", text)\n        text = re.sub(r\"nfs\" , \"not for sale\", text)\n        text = re.sub(r\"ngl\" , \"not going to lie\", text)\n        text = re.sub(r\"nhs\" , \"national health service\", text)\n        text = re.sub(r\"nrn\" , \"no reply necessary\", text)\n        text = re.sub(r\"nsfl\" , \"not safe for life\", text)\n        text = re.sub(r\"nsfw\" , \"not safe for work\", text)\n        text = re.sub(r\"nth\" , \"nice to have\", text)\n        text = re.sub(r\"nvr\" , \"never\", text)\n        text = re.sub(r\"nyc\" , \"new york city\", text)\n        text = re.sub(r\"oc\" , \"original content\", text)\n        text = re.sub(r\"og\" , \"original\", text)\n        text = re.sub(r\"ohp\" , \"overhead projector\", text)\n        text = re.sub(r\"oic\" , \"oh i see\", text)\n        text = re.sub(r\"omdb\" , \"over my dead body\", text)\n        text = re.sub(r\"omg\" , \"oh my god\", text)\n        text = re.sub(r\"omw\" , \"on my way\", text)\n        text = re.sub(r\"p.a\" , \"per annum\", text)\n        text = re.sub(r\"p.m\" , \"after midday\", text)\n        text = re.sub(r\"pm\" , \"prime minister\", text)\n        text = re.sub(r\"poc\" , \"people of color\", text)\n        text = re.sub(r\"pov\" , \"point of view\", text)\n        text = re.sub(r\"pp\" , \"pages\", text)\n        text = re.sub(r\"ppl\" , \"people\", text)\n        text = re.sub(r\"prw\" , \"parents are watching\", text)\n        text = re.sub(r\"ps\" , \"postscript\", text)\n        text = re.sub(r\"pt\" , \"point\", text)\n        text = re.sub(r\"ptb\" , \"please text back\", text)\n        text = re.sub(r\"pto\" , \"please turn over\", text)\n        text = re.sub(r\"qpsa\" , \"what happens\", text)\n        text = re.sub(r\"ratchet\" , \"rude\", text)\n        text = re.sub(r\"rbtl\" , \"read between the lines\", text)\n        text = re.sub(r\"rlrt\" , \"real life retweet\",  text)\n        text = re.sub(r\"rofl\" , \"rolling on the floor laughing\", text)\n        text = re.sub(r\"roflol\" , \"rolling on the floor laughing out loud\", text)\n        text = re.sub(r\"rotflmao\" , \"rolling on the floor laughing my ass off\", text)\n        text = re.sub(r\"rt\" , \"retweet\", text)\n        text = re.sub(r\"ruok\" , \"are you ok\", text)\n        text = re.sub(r\"sfw\" , \"safe for work\", text)\n        text = re.sub(r\"sk8\" , \"skate\", text)\n        text = re.sub(r\"smh\" , \"shake my head\", text)\n        text = re.sub(r\"sq\" , \"square\", text)\n        text = re.sub(r\"srsly\" , \"seriously\",  text)\n        text = re.sub(r\"ssdd\" , \"same stuff different day\", text)\n        text = re.sub(r\"tbh\" , \"to be honest\", text)\n        text = re.sub(r\"tbs\" , \"tablespooful\", text)\n        text = re.sub(r\"tbsp\" , \"tablespooful\", text)\n        text = re.sub(r\"tfw\" , \"that feeling when\", text)\n        text = re.sub(r\"thks\" , \"thank you\", text)\n        text = re.sub(r\"tho\" , \"though\", text)\n        text = re.sub(r\"thx\" , \"thank you\", text)\n        text = re.sub(r\"tia\" , \"thanks in advance\", text)\n        text = re.sub(r\"til\" , \"today i learned\", text)\n        text = re.sub(r\"tl;dr\" , \"too long i did not read\", text)\n        text = re.sub(r\"tldr\" , \"too long i did not read\", text)\n        text = re.sub(r\"tmb\" , \"tweet me back\", text)\n        text = re.sub(r\"tntl\" , \"trying not to laugh\", text)\n        text = re.sub(r\"ttyl\" , \"talk to you later\", text)\n        text = re.sub(r\"u\" , \"you\", text)\n        text = re.sub(r\"u2\" , \"you too\", text)\n        text = re.sub(r\"u4e\" , \"yours for ever\", text)\n        text = re.sub(r\"utc\" , \"coordinated universal time\", text)\n        text = re.sub(r\"w\/\" , \"with\", text)\n        text = re.sub(r\"w\/o\" , \"without\", text)\n        text = re.sub(r\"w8\" , \"wait\", text)\n        text = re.sub(r\"wassup\" , \"what is up\", text)\n        text = re.sub(r\"wb\" , \"welcome back\", text)\n        text = re.sub(r\"wtf\" , \"what the fuck\", text)\n        text = re.sub(r\"wtg\" , \"way to go\", text)\n        text = re.sub(r\"wtpa\" , \"where the party at\", text)\n        text = re.sub(r\"wuf\" , \"where are you from\", text)\n        text = re.sub(r\"wuzup\" , \"what is up\", text)\n        text = re.sub(r\"wywh\" , \"wish you were here\", text)\n        text = re.sub(r\"yd\" , \"yard\", text)\n        text = re.sub(r\"ygtr\" , \"you got that right\", text)\n        text = re.sub(r\"ynk\" , \"you never know\", text)\n        text = re.sub(r\"zzz\" , \"sleeping bored and tired\", text)\n\n\n    return sentence","3174721f":"df_train['text'] = df_train[\"text\"].apply(lambda x: remove_abreviations(x))","5d1f50f1":"def remove_acronyms(text):\n    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n    text = re.sub(r\"m\u00cc\u00bcsica\", \"music\", text)\n    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)    \n    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n    text = re.sub(r\"cawx\", \"California Weather\", text)\n    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)  \n    \n    return text\n\ndf_train['text'] = df_train[\"text\"].apply(lambda x: remove_acronyms(x))","ef6c4795":"def remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","62c8905c":"my_example = df_train.iloc[5][\"text\"]\nprint(my_example)","43d3f120":"print(remove_punctuation(my_example))","55cda879":"df_train['text'] = df_train[\"text\"].apply(lambda x: remove_punctuation(x))","c7b5d4cf":"cleaned_disaster_bigrams = defaultdict(int)\ncleaned_nondisaster_bigrams = defaultdict(int)\n\nfor tweet in df_train[df_train['target']==1]['text']:\n    for word in generate_ngrams(tweet,2):\n        cleaned_disaster_bigrams[word] += 1\n        \nfor tweet in df_train[df_train['target']==0]['text']:\n    for word in generate_ngrams(tweet,2):\n        cleaned_nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(cleaned_disaster_bigrams.items(), key=lambda x: x[1])[::-1],columns=['bigrams','frequency'])\n\ndf_nondisaster_bigrams = pd.DataFrame(sorted(cleaned_nondisaster_bigrams.items(), key=lambda x: x[1])[::-1],columns=['bigrams','frequency'])    \n\nplt.figure(figsize = (12,5))\nsns.barplot(y='bigrams',x='frequency', data=df_disaster_bigrams[:20])\nplt.title('Common bigrams - Disaster')\n\nplt.figure(figsize = (12,5))\nsns.barplot(y='bigrams',x='frequency',data=df_nondisaster_bigrams[:20] )\nplt.title('Common bigrams - Not Disaster')","2c6127eb":"cleaned_disaster_trigrams = defaultdict(int)\ncleaned_nondisaster_trigrams = defaultdict(int)\n\nfor tweet in df_train[df_train['target']==1]['text']:\n    for word in generate_ngrams(tweet,3):\n        cleaned_disaster_trigrams[word] += 1\n        \nfor tweet in df_train[df_train['target']==0]['text']:\n    for word in generate_ngrams(tweet,3):\n        cleaned_nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(cleaned_disaster_trigrams.items(), key=lambda x: x[1])[::-1],columns=['trigrams','frequency'])\n\ndf_nondisaster_trigrams = pd.DataFrame(sorted(cleaned_nondisaster_trigrams.items(), key=lambda x: x[1])[::-1],columns=['trigrams','frequency'])    \n\nplt.figure(figsize = (12,5))\nsns.barplot(y='trigrams',x='frequency', data=df_disaster_trigrams[:20])\nplt.title('Common trigrams - Disaster')\n\nplt.figure(figsize = (12,5))\nsns.barplot(y='trigrams',x='frequency',data=df_nondisaster_trigrams[:20] )\nplt.title('Common trigrams - Not Disaster')","d1364280":"vectorizer = CountVectorizer(lowercase=True,stop_words='english',max_df=0.95,min_df=2)","f862821a":"X = vectorizer.fit_transform(list(df_train['text']))","4593a1c1":"#let's look at the vocabulary word_name :word_id\nvectorizer.vocabulary_","a091a0d7":"#now let's check which are our features and how many of them there are\nprint(vectorizer.get_feature_names())","74756917":"len(vectorizer.get_feature_names())","ac32ccdd":"X = X.toarray()","d239bb14":"X.shape","4a7ef46a":"from sklearn.model_selection import train_test_split","ae52bf06":"y = df_train['target']\ny","68004a45":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)","dae36d03":"from sklearn.naive_bayes import MultinomialNB","b944f808":"#list of alphas we are going to try\nalpha_list = np.linspace(1, 100, 100)\nalpha_list","a2ad19da":"parameter_grid = [{\"alpha\":alpha_list}]","30565b78":"clf= MultinomialNB()\ngridsearch = GridSearchCV(clf,parameter_grid, scoring = 'neg_log_loss', cv = 5)\ngridsearch.fit(X_train, y_train)","b8ff085b":"import matplotlib\nresults = pd.DataFrame()\n# collect alpha list\nresults['alpha'] = gridsearch.cv_results_['param_alpha'].data\n# collect test scores\nresults['neglogloss'] = gridsearch.cv_results_['mean_test_score'].data\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results['alpha'], -results['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","78e06799":"print(\"Best parameter: \",gridsearch.best_params_)","5c62edd2":"print(\"Best score: \",gridsearch.best_score_) ","7a68a299":"alpha_list2 = np.linspace(13, 18, 100)\nparameter_grid2 = [{\"alpha\":alpha_list2}]\nclf= MultinomialNB()\ngridsearch2 = GridSearchCV(clf,parameter_grid2, scoring = 'neg_log_loss', cv = 5)\ngridsearch2.fit(X_train, y_train)","4bf78aba":"results2 = pd.DataFrame()\n# collect alpha list\nresults2['alpha'] = gridsearch2.cv_results_['param_alpha'].data\n# collect test scores\nresults2['neglogloss'] = gridsearch2.cv_results_['mean_test_score'].data\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results2['alpha'], -results2['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","ecc53c7c":"print(\"Best parameter: \",gridsearch2.best_params_)","2eb4b065":"print(\"Best score: \",gridsearch.best_score_) ","41c674ca":"clf= MultinomialNB(alpha=15.3)\nclf.fit(X_train,y_train)","3242c823":"from sklearn.metrics import classification_report,confusion_matrix","96a65daa":"predictions = clf.predict(X_test)","5bc9a28b":"print(classification_report(y_test,predictions))","28375b9e":"cf_matrix = confusion_matrix(y_test,predictions)\ncf_matrix","e305f323":"group_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","3a72690c":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,predictions)","16fdd51e":"from sklearn.feature_extraction.text import TfidfVectorizer","d15bcafa":"tfidf = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')","06bd5ec3":"X = df_train['text']\ny = df_train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)","0b4f3633":"X_train = tfidf.fit_transform(X_train)","0ee2acb9":"X_test = tfidf.transform(X_test)","1adee82d":"alpha_list1 =  np.linspace(0.001, 2, 100)\nparameter_grid1 = [{\"alpha\":alpha_list1}]\nclf= MultinomialNB()\ngridsearch1 = GridSearchCV(clf,parameter_grid1, scoring = 'neg_log_loss', cv = 5)\ngridsearch1.fit(X_train, y_train)","704b2173":"results1 = pd.DataFrame()\n# collect alpha list\nresults1['alpha'] = gridsearch1.cv_results_['param_alpha'].data\n# collect test scores\nresults1['neglogloss'] = gridsearch1.cv_results_['mean_test_score'].data\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results1['alpha'], -results1['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","57fc3af2":"alpha_list2 =  np.linspace(0.2, 1, 100)\nparameter_grid2 = [{\"alpha\":alpha_list2}]\nclf= MultinomialNB()\ngridsearch2 = GridSearchCV(clf,parameter_grid2, scoring = 'neg_log_loss', cv = 5)\ngridsearch2.fit(X_train, y_train)","431de284":"results2 = pd.DataFrame()\n# collect alpha list\nresults2['alpha'] = gridsearch2.cv_results_['param_alpha'].data\n# collect test scores\nresults2['neglogloss'] = gridsearch2.cv_results_['mean_test_score'].data\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results2['alpha'], -results2['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","7cf1aae5":"print(\"Best score: \",gridsearch2.best_score_) ","c03b9f40":"print(\"Best parameter: \",gridsearch2.best_params_)","9eeaa4ee":"clf= MultinomialNB(alpha=0.6)\nclf.fit(X_train,y_train)","0d368722":"predictions = clf.predict(X_test)\nprint(classification_report(y_test,predictions))","c2e67a87":"cf_matrix = confusion_matrix(y_test,predictions)\ncf_matrix","1e0d96e3":"group_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","d0f35c90":"accuracy_score(y_test,predictions)","29ccfaea":"vectorizer = CountVectorizer(lowercase=True,stop_words='english',max_df=0.95,min_df=2)\nX_train = vectorizer.fit_transform(list(df_train['text']))\nX_train = X_train.toarray()\ny_train = df_train['target']\n\nclf= MultinomialNB(alpha=15.3)\nclf.fit(X_train,y_train)","9187cf63":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","5d9d8a2a":"df_test.head()","7cc826aa":"df_test['text']=df_test['text'].apply(lambda x : remove_URL(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_html(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_emoji(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_contractions(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_abreviations(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_acronyms(x))\ndf_test['text'] = df_test[\"text\"].apply(lambda x: remove_punctuation(x))","0b8f9b32":"X_test =  vectorizer.transform(list(df_test['text']))\nX_test = X_test.toarray()","c694d8ee":"predictions = clf.predict(X_test)","f8befcdd":"output_matrix = pd.DataFrame(df_test['id'])\n#output['target'] = predictions\noutput_matrix","b14866a4":"df_test","979f5514":"output = pd.DataFrame(df_test.id[:3263])\noutput['target'] = predictions","a607ac1c":"output.to_csv('nlp_output.csv', index=False)","1099ea7d":"### Cleaned dataset bigrams","ae55ec67":"### Average word length in a tweet","2a8d23cb":"### Removing punctuations","2c47d3d0":"It seems we have some null values. Let's check where and how many.","0dd0722c":"Cool, now let's redo our bigram and trigram analysis","8b90f052":"Now let's try to understand what we see above:The model is much better at predicting non-disasters than disasters, with an f1-score for disasters of 0.75 and an accuracy of 0.8! ","dbc1356c":"### Removing acronyms","2d89e54f":"## Building a model to predict disaster tweets","2cbfc8e0":"### N-grams: bigrams and trigrams","d628056c":"### Cleaned text - trigrams","c21ec530":"### Removing contractions (I'm becomes I am)","6b7b90f9":"Now, let's look into a better approach for text processing, an embedding technique called tf-idf. I encourage you to read more on the topic, as this is quite a cool technique.","f5b7ead5":"### Number of words per tweet","152df841":"Now, we will split our training dataset into train and test, so that we can build our model on the training portion and test on the validation portion (we want to keep df_test as the 'not seen' portion of our dataset, so that we only apply our model on it when we are perfectly sure it works and are satisfied with the results). ","34d45711":"Now let's create a corpus for disaster and non-disaster tweets:","05876bc2":"# EDA time","241be0d5":"Now that we have trained our model, let's see its performance by building a classification report and a confusion matrix. These two reports will basically give us all the useful information about our model - precision, recall, f1-score. For those of you that are new to analysing the performance of a classifciation model, these are the 3 most import metrics, with accuracy being less important especially when we have imbalanced datasets. I found an article that explains quite well what each one of them means - https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9 and I dp encourage you to familiarize yourselves with those metrics by redaing more on the topic.","a86b6a8d":"Now, let's test on our test data!!! First, we will apply the same cleaning process on the train set, just as we did on the train set:","2c8d66ce":"### Removing emojis","198ce08d":"Based on the previous 6 graphs, we can conclude the following:\n- the tweets referring to disasters have slightly more words per tweet\n- the tweets referring to disasters have higher word length\n\nNow let's look into stopwords and punctuation for disaster and non-disaster tweets.","5a8306f1":"Hi all, so this is my first attempt at a kaggle dataset\/competition plus the first time applying NLP. What I did is go through the process of building a classification model: EDA - Exploratory Data Analysis, Data Cleaning, Feature Building, Model Selection & Hyperparameter tuning and then Results Interpretation\/Analysis. I hope it is also a good intro to beginners as I tried to explain as much as possible along the way!\n\nJust a quick note,for the data cleaning part I got those really lengthy texts from the below notebooks as I didn't see much point for reinventing the wheel, cheers to the owners! \n\nhttps:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert <br>\nhttps:\/\/www.kaggle.com\/itratrahman\/nlp-tutorial-using-python <br>\nhttps:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb <br>\n","0bfe876a":"For those of you that don't know, n-grams are groups of consecutive characters in a word - i.e. mother has bigrams mo, ot, th, he, er and trigrams mot, oth, the and her OR consecutive n-groups of words in a text - i.e. 'This is my text' would become 'this is', 'is my' and 'my text'. This is the traditional way of analysing text data and performing NLP, before the discovery of embedding models like GloVe, ELMO and BERT, so let's get an insight on these as well - we'll look into most common bigrams and trigrams.","18a796b6":"Now, as our first type of embedding we will use the vanilla bag of words - basically we will compute a matrix, in which the rows are the documents(tweets), and the columns are the words in our vocabulary, with the cell c(i,j) pointing to tweet i, word j and having a value of 1 if that word appears in that document, and 0 otherwise. For this, we will use CountVectorizer - however, we will use some specific parameters for it to make our life easier:\n1. lowercase=True --> this will basically lowercase all words\n2. stop_words = 'english' --> this will get rid of most common words in english ('the', 'are','I', 'a' etc.) as these would provide litte value in distinguishing wheter a tweet is referring to a disatser or not\n3. max_df = 0.95 --> this will only keep words that occur up to 95% of the documents, for a similar reason as above\n4. min_df = 2 --> this will only keep words which occur in at least 2 documents, as otherwise that specific word will be useless in our analysis","5401f9df":"### TF-IDF approach","38238350":"### Most common words","46b68993":"## Cleaning","19592d87":"### Removing URLs","1544c97e":"Now, we will use a  Naive Bayes classifier, as it is a good chocie for a medium-sized dataset and it is traditionally used in NLP tasks. If you are interested to learn more, i found this article quite good: https:\/\/medium.com\/@theflyingmantis\/text-classification-in-nlp-naive-bayes-a606bf419f8c . We are going to use Multinomial NB as it is suitable for classification with discrete features (in our case word counts for texts). We will use GridSeachCV to look for optimal alpha for our model, and will use 5 folds for cross-validtion (normally I use 10, but this would take too much time).","cb9bab32":"So it seems we have qute a lot of missing data for the location variable, which we will ignore for this analysys. There is a 0.8% of data missing for the keyword argument, but since we wil be concentrating on the text itself, this is fine.","81fa7fe9":"#### Most common bigrams (n = 2)","202db9e9":"### Common Stopwords","0010651a":"Conclusion:\n- most common punctuations are '-' and '|' for bot classes\n\nNow let's check the most common words for each class","fd67b673":"### Bag of words","c3d66906":"### Removing abbreviations","6d04701b":"If we compare the results of using tf-idf versus bag of words as embedding model, we can see that we just slightly improved the recall for disasters(0.68 to 0.7), whilst the precision slightly went down(0.84 to 0.81), and the f1-score stayed intact at 0.75...so unfortunately not much improvement. Sine f1-score stays the same, I will choose the bag of words embedding as it is really important to get the tweets about natural disasters right.  Let's train the model by using a bag-of-words embedding and a Multinomial Naive Bayes classifier with the best alpha found, 15.3","6d88acdd":"### Tweet Length","73878253":"It seems that our text is not that clean and if we take a clearer look into the individual texts we notice that they contain URLs, weird abbreviations, emojis, @ and #s, so the net task would be cleaning the text and then computing the n-grams again.","e9c12cc0":"First, let;s check how many tweets we have with urls in them","27452a03":"Conclusion:\n- 'the' is the most popular stopword in both classes\n- for disasters, the next most common words are 'in', 'of' and 'a'\n- for non-disasters, the next most common words are 'a', 'to' and 'and'","855eb437":"### Punctuation","c1318c24":"### Removing HTMLs","1a3fec48":"Next, we want to check how imbalanced our dataset is. Why does this matter? Well,let's supose at the end of this model we will have an accuracy of 98.5%. Great right? We put this in production (ie on twitter) and 3 weeks later we notice that our model is completely useless as it keeps answering 'not a disaster' all the time, even to tweets which are about disaster. Then, when we look back to our dataset we notice that we only have 5% of the total samples that are about disasters so obviously our model does not have enough information about how a disaster tweet is supposed to look like. Now, that we understand the importance of how balanced datasets are, let's check how ours looks like.","713b7711":"Great, the dataset is quite balanced, so our model should have enough information about the distinctiveness of disaster tweets. Now let's look into the text component of the tweets and check their length and the number of words per tweet."}}