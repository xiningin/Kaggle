{"cell_type":{"f9548ceb":"code","cb3b623e":"code","b72fcd19":"code","fdc1e6ef":"code","54c17c67":"code","f1eccba0":"code","8166c261":"code","c9c0f896":"code","8fd3c1cc":"code","76a7b7f7":"code","a508696f":"code","edcf8641":"code","7cc40cfd":"code","2ddd2efe":"code","de90640b":"code","2c51132e":"code","40b8f7b5":"code","9bfb6efc":"code","1be59626":"code","7d4d211e":"code","041bb2e2":"code","7e44ffaf":"code","629faff2":"code","d12a6f69":"code","23312f7b":"code","0487577d":"code","9a3defd8":"code","3c77cbdf":"code","e5af31f9":"code","3f8bcaa0":"code","30c7a1ef":"code","3e752658":"code","6770b09b":"code","bd366081":"code","443a3ec4":"code","44307b4c":"code","8fb5abd2":"code","7462e424":"code","2e79a0b6":"code","7bcfa0dd":"code","be21bf72":"code","1fb5685e":"markdown","95146fbd":"markdown","a1adaed4":"markdown","9046fb8f":"markdown","a15f11f1":"markdown","46085426":"markdown","eb970e28":"markdown","7915430b":"markdown","d6473f1e":"markdown","e0d15535":"markdown","2551d234":"markdown","d9413c0a":"markdown","718d908f":"markdown","5a87bae3":"markdown","99b41d70":"markdown","e0493e5c":"markdown","ad3b351b":"markdown"},"source":{"f9548ceb":"import pandas as pd\nimport numpy as np\nnp.random.seed(42)\n\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random as rn\nrn.seed(42)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport tensorflow as tf\ntf.random.set_seed(42)\n\nfrom imblearn.over_sampling import SMOTE","cb3b623e":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ncategorical_columns = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\ndf.head()","b72fcd19":"df_1 = df.copy()\ndf_1.head()","fdc1e6ef":"df_1.drop(['id'], axis=1, inplace=True)\ndf_1.head()","54c17c67":"imp = SimpleImputer()\ndf_1['bmi'] = imp.fit_transform(df_1['bmi'].to_numpy().reshape(-1,1))\n\ndf_1.isnull().sum()","f1eccba0":"s = (df_1.dtypes == 'object')\ncategorical_col = list(s[s].index)\n\ndef one_hot_encode(data, feature_name):\n    encoder = OneHotEncoder()\n    encoded = encoder.fit_transform(data[[feature_name]]).toarray()\n    encoded_df = pd.DataFrame(encoded, columns=[f'{feature_name}_{label}' for label in encoder.categories_[0]])\n    return encoded_df\n\nfor feat in categorical_col:\n    encoded = one_hot_encode(df_1, feat)\n    df_1 = pd.concat((df_1, encoded), axis=1)\n    df_1 = df_1.drop(columns=feat)\n\n# df_1.drop(categorical_col, axis=1, inplace=True)","8166c261":"# df_1['gender'] = df_1['gender'].replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n# df_1['Residence_type'] = df_1['Residence_type'].replace({'Rural':0,'Urban':1}).astype(np.uint8)\n# df_1['work_type'] = df_1['work_type'].replace({'Private':0,'Self-employed':1,'Govt_job':2,'children':-1,'Never_worked':-2}).astype(np.uint8)\n# df_1['ever_married'] = df_1['ever_married'].replace({'Yes':1, 'No':0}).astype(np.uint8)\n# df_1['smoking_status']=df_1['smoking_status'].replace({'never smoked': 0, 'Unknown':0, 'formerly smoked':1, 'smokes':1})\ndf_1.head()","c9c0f896":"X_1 = df_1.drop(['stroke'], axis=1)\ny_1 = df_1['stroke']\nX_1_train, X_1_test, y_1_train, y_1_test = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\ndisplay(X_1_train.shape)\ndisplay(X_1_test.shape)","8fd3c1cc":"oversample = SMOTE()\nX_1_train, y_1_train = oversample.fit_resample(X_1_train, y_1_train)","76a7b7f7":"y_1_train.value_counts()","a508696f":"df_2 = df.copy()\ndf_2.head()","edcf8641":"# belom diclean awokaowkwoa\ndf_2['smoking_status'] = df_2['smoking_status'].fillna(df_2['smoking_status'].value_counts().index[0])\ndf_2['bmi'] = df_2['bmi'].fillna(df_2['bmi'].mean())\ndisplay(sns.countplot(data=df_2, x='smoking_status'))","7cc40cfd":"df_2_scaled = df_2.copy()\ndf_2_scaled.drop(['id'], axis=1, inplace=True)\n\n# scale age\nage_scaler = MinMaxScaler()\ndf_2_scaled['age'] = age_scaler.fit_transform(df_2_scaled[['age']])\n\n# scale bmi\nbmi_scaler = StandardScaler()\ndf_2_scaled['bmi'] = bmi_scaler.fit_transform(df_2_scaled[['bmi']])\n\ndf_2_scaled.head()","2ddd2efe":"df_2_binned = df_2_scaled.copy()\n\nglucose = df_2_binned['avg_glucose_level']\n\nboundaries = [140, 200]\nboundaries = sorted({glucose.min(), glucose.max() + 1} | set(boundaries))\n\ndf_2_binned['avg_glucose_level'] = pd.cut(glucose, bins=boundaries, labels=range(len(boundaries) - 1), right=False)\n\ndf_2_binned.head()","de90640b":"sns.countplot(data=df_2_binned, x='avg_glucose_level')","2c51132e":"df_2_encoded = df_2_binned.copy()\ndf_2_encoded.head()","40b8f7b5":"ONE_HOT_ENCODED_FEATURES = ['work_type', 'smoking_status']\n\ndef one_hot_encode(data, feature_name):\n    encoder = OneHotEncoder()\n    encoded = encoder.fit_transform(data[[feature_name]]).toarray()\n    encoded_df = pd.DataFrame(encoded, columns=[f'{feature_name}_{label}' for label in encoder.categories_[0]])\n    return encoded_df\n\n# for feat in ONE_HOT_ENCODED_FEATURES:\n#     encoded = one_hot_encode(df_encoded, feat)\n#     df_encoded = pd.concat((df_encoded, encoded), axis=1)\n#     df_encoded = df_encoded.drop(columns=feat)\n\ndf_2_encoded.head()","9bfb6efc":"LABEL_ENCODED_FEATURES = ['work_type', 'smoking_status', 'gender', 'Residence_type', 'ever_married']\n\ndef label_encode(data, feature_name):\n    encoder = LabelEncoder()\n    encoded = encoder.fit_transform(data[[feature_name]])\n    return encoded\n\nfor feat in LABEL_ENCODED_FEATURES:\n    encoded = label_encode(df_2_encoded, feat)\n    df_2_encoded[feat] = encoded\n\ndf_2_encoded.head()","1be59626":"print(len(df_2_encoded.columns))","7d4d211e":"sns.heatmap(df_2_encoded.corr())","041bb2e2":"df_2_selection = df_2_encoded.drop(['ever_married'], axis=1)","7e44ffaf":"column_to_move = df_2_encoded.pop(\"stroke\")\ndf_2_encoded.insert(len(df_2_encoded.columns), \"stroke\", column_to_move)\ndf_2_encoded","629faff2":"column_to_move = df_2_selection.pop(\"stroke\")\ndf_2_selection.insert(len(df_2_selection.columns), \"stroke\", column_to_move)\ndf_2_selection","d12a6f69":"df_2 = df_2_encoded.copy()\n\nX_2 = df_2.drop(['stroke'], axis=1)\ny_2 = df_2['stroke']\nX_2_train, X_2_test, y_2_train, y_2_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\ndisplay(X_2_train.shape)\ndisplay(X_2_test.shape)","23312f7b":"X_2_selection = df_2_selection.drop(['stroke'], axis=1)\ny_2_selection = df_2_selection['stroke']\nX_2_train_s, X_2_test_s, y_2_train_s, y_2_test_s = train_test_split(X_2_selection, y_2_selection, test_size=0.2, random_state=42)\ndisplay(X_2_train_s.shape)\ndisplay(X_2_test_s.shape)","0487577d":"oversample = SMOTE()\nX_2_train, y_2_train = oversample.fit_resample(X_2_train, y_2_train)\ny_2_train.value_counts()\n\noversample_s = SMOTE()\nX_2_train_s, y_2_train_s = oversample_s.fit_resample(X_2_train_s, y_2_train_s)\ny_2_train_s.value_counts()","9a3defd8":"X_2_train","3c77cbdf":"def pca_dec(data):\n    pca = decomposition.PCA(n_components = 0.95, random_state=42)# PCA for dimensionality redcution (non-visualization)\n\n    scaler = StandardScaler()\n    vals_std = scaler.fit_transform(data)\n    bad_indices_inf = np.where(np.isinf(vals_std))\n    bad_indices_nan = np.where(np.isnan(vals_std))\n    vals_std[bad_indices_inf] = 0\n    vals_std[bad_indices_nan] = 0\n    principal_components = pca.fit_transform(vals_std)\n    \n    pca_cols = ['pc'+str(i) for i in range(1,(principal_components.shape[1]+1))]\n    reduced_data_pca = pd.DataFrame(data = principal_components\n                 , columns = pca_cols)\n    return reduced_data_pca","e5af31f9":"#Decomposing the train set:\nX_2_train_pca = pca_dec(X_2_train)\nX_2_train_s_pca = pca_dec(X_2_train_s)\n\n#Decomposing the test set:\nX_2_test_pca = pca_dec(X_2_test)\nX_2_test_s_pca = pca_dec(X_2_test_s)","3f8bcaa0":"X_2_train_pca","30c7a1ef":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=42)\nsvm.fit(X_1_train, y_1_train)\ny_1_pred_svm = svm.predict(X_1_test)\ny_1_pred_svm[:10]","3e752658":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(random_state=42)\nxgb.fit(X_1_train, y_1_train)\ny_1_pred_xgb = xgb.predict(X_1_test)\ny_1_pred_xgb[:10]","6770b09b":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(random_state=42)\nlgbm.fit(X_1_train, y_1_train)\ny_1_pred_lgbm = lgbm.predict(X_1_test)\ny_1_pred_lgbm[:10]","bd366081":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='liblinear', random_state=42)\nlogreg.fit(X_1_train, y_1_train)\ny_1_pred_logreg = logreg.predict(X_1_test)\ny_1_pred_logreg[:10]","443a3ec4":"linear_svm = SVC(kernel='linear', random_state=42)\nlinear_svm.fit(X_1_train, y_1_train)\ny_1_pred_svm_lin = linear_svm.predict(X_1_test)\ny_1_pred_svm_lin[:10]","44307b4c":"model_1 = Sequential()\nmodel_1.add(Dense(64, input_dim=21, activation='relu'))\n# model_1.add(Dropout(0.25))\nmodel_1.add(Dense(8, activation='relu'))\nmodel_1.add(Dropout(0.25))\nmodel_1.add(Dense(1, activation='sigmoid'))\nmodel_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.Recall())\nmodel_1.summary()","8fb5abd2":"model_1.fit(X_1_train, y_1_train, epochs=10)","7462e424":"from sklearn.metrics import recall_score, f1_score\n# Experiment 1\nprint('SVM: ', recall_score(y_1_test, y_1_pred_svm))\nprint('Linear SVM: ', recall_score(y_1_test, y_1_pred_svm_lin))\nprint('XGBoost: ', recall_score(y_1_test, y_1_pred_xgb))\nprint('LGBM: ', recall_score(y_1_test, y_1_pred_lgbm))\nprint('Logistic Regression: ', recall_score(y_1_test, y_1_pred_logreg))\nprint('FFNN: ', model_1.evaluate(X_1_test, y_1_test)[1])","2e79a0b6":"def model_first_stage():\n    model_2 = Sequential()\n    model_2.add(Dense(64, input_dim=9, activation='relu'))\n    # model_2.add(Dropout(0.25))\n    model_2.add(Dense(8, activation='relu'))\n    model_2.add(Dropout(0.25))\n    model_2.add(Dense(1, activation='sigmoid'))\n    model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.Recall())\n    return model_2\nmodel_2 = model_first_stage()\nmodel_2_s = model_first_stage()\nmodel_2.summary()\nmodel_2_s.summary()","7bcfa0dd":"model_2.fit(X_2_train_pca, y_2_train, epochs=10)\nmodel_2_s.fit(X_2_train_s_pca, y_2_train_s, epochs=10)","be21bf72":"print('FFNN: ', model_2.evaluate(X_2_test_pca, y_2_test)[1])\nprint('FFNN using feat selection: ', model_2_s.evaluate(X_2_test_s_pca, y_2_test_s)[1])","1fb5685e":"## Scaling","95146fbd":"Fill missing values of BMI with mean","a1adaed4":"## Cleaning","9046fb8f":"## Encoding","a15f11f1":"## Experiment 1","46085426":"# Evaluation\n\nUse recall as evaluation metrics","eb970e28":"## Experiment 1\n\nDrop id column, one hot encoding, fill missing values of BMI with mean","7915430b":"# Imports","d6473f1e":"## Binning","e0d15535":"# Model\n\nIn this section, we will experiment with different models.","2551d234":"## Feature Extraction","d9413c0a":"## Feature Selection","718d908f":"### Baseline Model","5a87bae3":"Drop ID Column","99b41d70":"## Experiment 2","e0493e5c":"# Data Preprocessing\n\nIn this section, we will do the data preprocessing stage.","ad3b351b":"## Experiment 2\nDrop id column, label encoding, fill missing values of BMI with mean, PCA"}}