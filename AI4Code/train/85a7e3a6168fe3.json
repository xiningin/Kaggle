{"cell_type":{"3c4eaf96":"code","3325c4f5":"code","41b3d75c":"code","59a28155":"code","7edc7556":"code","acff8669":"code","083af267":"code","c2ddb6ff":"code","c0b3fe74":"code","e1cb96de":"code","ffe266a3":"code","e02bbf77":"code","2e291b7e":"code","510dccd2":"code","5ddd7999":"code","484cdef9":"code","e792cd0f":"code","2a087484":"code","0646290a":"code","f8c2da88":"code","dc59508b":"code","2c6ed489":"code","3d4259ab":"code","e214438c":"code","a9f4dcef":"code","6193199d":"code","1dcf650d":"code","7da2b370":"code","6edc6911":"code","bfb98bd6":"code","d1ee5efa":"code","9b02aa7b":"code","17e6833f":"code","c574ed22":"code","9efb15d8":"code","9fda6c11":"code","44f6cbab":"code","e87cb9c2":"code","3ba60582":"code","14df6b17":"code","6b1a284f":"code","a2f50074":"code","55c462bf":"code","e5d7b347":"code","1223a26d":"code","42240eb0":"code","56da64b6":"code","92124eab":"code","0cc752cf":"code","e614992e":"code","240b45ca":"code","913c9eac":"code","8a09dc39":"code","d369df1b":"markdown","30263050":"markdown","5060e65a":"markdown","66298ce4":"markdown","029b7c27":"markdown","ab6ec272":"markdown","d0ecb82a":"markdown","7c67df60":"markdown","810f1b76":"markdown","8735b438":"markdown","644e5e6d":"markdown","f827dd13":"markdown","d8da5284":"markdown","0f9a3899":"markdown"},"source":{"3c4eaf96":"import numpy as np\nimport torch","3325c4f5":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')","41b3d75c":"inputs.shape","59a28155":"targets.shape","7edc7556":"# Convert inputs and targets to tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)","acff8669":"# Weights and biases\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)","083af267":"def model(x):\n    return x @ w.t() + b","c2ddb6ff":"# Generate predictions\npreds = model(inputs)\nprint(preds)","c0b3fe74":"# Compare with targets\nprint(targets)","e1cb96de":"# MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) \/ diff.numel()","ffe266a3":"# Compute loss\nloss = mse(preds, targets)\nprint(loss)","e02bbf77":"# Compute gradients\nloss.backward()","2e291b7e":"# Gradients for weights\nprint(w)\nprint(w.grad)","510dccd2":"w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)","5ddd7999":"# Generate predictions\npreds = model(inputs)\nprint(preds)","484cdef9":"# Calculate the loss\nloss = mse(preds, targets)\nprint(loss)","e792cd0f":"# Compute gradients\nloss.backward()\nprint(w.grad)\nprint(b.grad)","2a087484":"# Adjust weights & reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()","0646290a":"print(w)\nprint(b)","f8c2da88":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","dc59508b":"# Train for 100 epochs\nfor i in range(2000):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","2c6ed489":"w","3d4259ab":"b","e214438c":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","a9f4dcef":"# Predictions\npreds","6193199d":"# Targets\ntargets","1dcf650d":"# Train for 100 epochs\nfor i in range(4500):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","7da2b370":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","6edc6911":"# Predictions\npreds","bfb98bd6":"# Targets\ntargets","d1ee5efa":"!pip install jovian --upgrade -q","9b02aa7b":"import torch.nn as nn","17e6833f":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n                  dtype='float32')\n\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], \n                    [22, 37], [103, 119], [56, 70], \n                    [81, 101], [119, 133], [22, 37], \n                    [103, 119], [56, 70], [81, 101], \n                    [119, 133], [22, 37], [103, 119]], \n                   dtype='float32')\n\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","c574ed22":"inputs","9efb15d8":"from torch.utils.data import TensorDataset","9fda6c11":"# Define dataset\ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[0:3]","44f6cbab":"from torch.utils.data import DataLoader","e87cb9c2":"# Define data loader\nbatch_size = 5\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)","3ba60582":"for xb, yb in train_dl:\n    print(xb)\n    print(yb)\n    break","14df6b17":"# Define model\nmodel = nn.Linear(3, 2)\nprint(model.weight)\nprint(model.bias)","6b1a284f":"# Parameters\nlist(model.parameters())","a2f50074":"# Generate predictions\npreds = model(inputs)\npreds","55c462bf":"# Import nn.functional\nimport torch.nn.functional as F","e5d7b347":"# Define loss function\nloss_fn = F.mse_loss","1223a26d":"loss = loss_fn(model(inputs), targets)\nprint(loss)","42240eb0":"# Define optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)","56da64b6":"# Utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt, train_dl):\n    \n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n        \n        # Train with batches of data\n        for xb,yb in train_dl:\n            \n            # 1. Generate predictions\n            pred = model(xb)\n            \n            # 2. Calculate loss\n            loss = loss_fn(pred, yb)\n            \n            # 3. Compute gradients\n            loss.backward()\n            \n            # 4. Update parameters using gradients\n            opt.step()\n            \n            # 5. Reset the gradients to zero\n            opt.zero_grad()\n        \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}\/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","92124eab":"fit(100, model, loss_fn, opt, train_dl)","0cc752cf":"fit(1000, model, loss_fn, opt, train_dl)","e614992e":"# Generate predictions\npreds = model(inputs)\npreds","240b45ca":"# Compare with targets\ntargets","913c9eac":"import jovian","8a09dc39":"jovian.commit(project='linear-regression-pytorch')","d369df1b":"## Commit and update the notebook\n\nAs a final step, we can record a new version of the notebook using the `jovian` library.","30263050":"## nn.Linear\n\nInstead of initializing the weights & biases manually, we can define the model using the `nn.Linear` class from PyTorch, which does it automatically.","5060e65a":"## Linear regression using PyTorch built-ins\n\nThe model and training process above were implemented using basic matrix operations. But since this such a common pattern\u00a0, PyTorch has several built-in functions and classes to make it easy to create and train models.\n\nLet's begin by importing the `torch.nn` package from PyTorch, which contains utility classes for building neural networks.","66298ce4":"Some things to note above:\n\n* We use the data loader defined earlier to get batches of data for every iteration.\n\n* Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update, and `opt.zero_grad` to reset the gradients to zero.\n\n* We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. `loss.item` returns the actual value stored in the loss tensor.\n\nLet's train the model for 100 epochs.","029b7c27":"The `nn.functional` package contains many useful loss functions and several other utilities. ","ab6ec272":"## Dataset and DataLoader\n\nWe'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples, and provides standard APIs for working with many different types of datasets in PyTorch.","d0ecb82a":"## Optimizer\n\nInstead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. SGD stands for `stochastic gradient descent`. It is called `stochastic` because samples are selected in batches (often with random shuffling) instead of as a single group.","7c67df60":"Let's generate predictions using our model and verify that they're close to our targets.","810f1b76":"Let's compute the loss for the current predictions of our model.","8735b438":"We can use the model to generate predictions in the exact same way as before:","644e5e6d":"## Loss Function\n\nInstead of defining a loss function manually, we can use the built-in loss function `mse_loss`.","f827dd13":"Indeed, the predictions are quite close to our targets, and now we have a fairly good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall and humidity in a region.","d8da5284":"## Train the model\n\nWe are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n\n1. Generate predictions\n\n2. Calculate the loss\n\n3. Compute gradients w.r.t the weights and biases\n\n4. Adjust the weights by subtracting a small quantity proportional to the gradient\n\n5. Reset the gradients to zero\n\nThe only change is that we'll work batches of data, instead of processing the entire training data in every iteration. Let's define a utility function `fit` which trains the model for a given number of epochs.","0f9a3899":"Note that `model.parameters()` is passed as an argument to `optim.SGD`, so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which the parameters are modified."}}