{"cell_type":{"a6348d5c":"code","300e24c7":"code","8639ebb7":"code","bac81ef9":"code","ab7d94bb":"code","e1530fe4":"code","a0b94899":"code","927d99dc":"code","1f0bffcd":"code","a47286d0":"code","be992ef9":"code","e42a7ab9":"code","09f294f0":"code","6f27f524":"code","5ded0f0b":"code","c530eee2":"code","00f7c150":"code","fb1594db":"code","4e886be5":"code","7510433e":"code","fc99de4f":"code","6f229f89":"code","b7787ce4":"code","d071aa7f":"code","72d6d172":"code","b74cfaf8":"code","9c5b2ca4":"code","516835c4":"code","44c399e8":"code","02ccd6e4":"code","ea8a9552":"code","f4f6d866":"code","d7871ca1":"code","35597cd7":"code","e621c87f":"code","3a5216a8":"code","4d07bd1b":"code","90800c97":"code","66e342fe":"code","d9cb7aac":"code","fff08fba":"code","9de65a8a":"code","2edaf885":"code","95fd38b9":"code","9eb16374":"code","d4121208":"code","d3257164":"code","a8c0275f":"code","970bfc1f":"code","22f90c2d":"code","775f379d":"code","554254b5":"code","f30e55e8":"code","469f7a16":"code","2da4234b":"code","0083b6d6":"code","ddc95fd8":"code","25ab968f":"code","d2fefe8e":"code","c4f06963":"code","dc869eb0":"code","d3733a9b":"code","1c540d5a":"code","63800c2d":"code","9dd2b1bd":"code","026e13b7":"code","6d4c9730":"code","181f7467":"code","dfb89c3e":"code","2e359274":"code","3cb52970":"code","bb91accd":"code","b5f66a35":"code","8b2c032f":"code","cf39180e":"code","16ce399f":"code","d6fee496":"markdown","95de8d9e":"markdown","0f3b813b":"markdown","1c1b1699":"markdown","e40e75aa":"markdown","7077a973":"markdown","aa1ed7c4":"markdown","a3895de8":"markdown","e504c280":"markdown","e5cee1fe":"markdown","6f191a1e":"markdown","fab646f7":"markdown","8558d393":"markdown","ee31ca76":"markdown","742c7618":"markdown","17862be6":"markdown","49c090d9":"markdown","2d712337":"markdown"},"source":{"a6348d5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","300e24c7":"import matplotlib.pyplot as plt # --> for data visulization\nimport seaborn as sns # --> for data visulization\nfrom dateutil import parser # --> convert time in date time datatype\n","8639ebb7":"df = pd.read_csv('..\/input\/app-data\/appdata10.csv')\ndf.head()","bac81ef9":"df.tail()","ab7d94bb":"df.shape","e1530fe4":"for i in range(0,6):\n    print(df.loc[i,'screen_list'],'\\n') ","a0b94899":"df.isnull().sum()","927d99dc":"df.describe()","1f0bffcd":"df.info()","a47286d0":"features = df.columns\nfor i in features:\n    print('unique value of {} \\n {}  \\n len is {} '.format(i, df[i].unique(), len(df[i].unique())))","be992ef9":"df['hour'] = df.hour.str.slice(1,3).astype(int)","e42a7ab9":"df.head()","09f294f0":"df.dtypes","6f27f524":"df1 = df.drop(['first_open', 'screen_list','user','enrolled_date'], axis = 1)\ndf1.head()","5ded0f0b":"plt.figure(figsize=(14,8))\nsns.heatmap(df1.corr(),annot=True)\nplt.title('DF1 Heatmap correlatin matrix', fontsize=15)","c530eee2":"sns.pairplot(df1, hue='enrolled') ","00f7c150":"sns.countplot(df1.enrolled)","fb1594db":"not_enrolled_user = (df1.enrolled<1).sum()\nenrolled_user = (50000-not_enrolled_user).sum()\nnot_enrolled_user","4e886be5":"enrolled_user","7510433e":"plt.figure(figsize=(20,10))\nfeatures = df1.columns\nfor i, j in enumerate(features):\n    plt.subplot(3,3,i+1)\n    plt.title(f'Histogram of {j}', fontsize= 15)\n    bins = len(df1[j].unique())\n    plt.hist(df1[j], bins=bins,rwidth=0.8,linewidth=2, edgecolor= 'y')\nplt.subplots_adjust(hspace=0.5)\n    \n#   1) In first histogram easily find that wednesday (2) and thursday (3) less user enrolled \n#   2) In hour histogram near 8 to 12 less user enrolled \n#   3) Iin age histogram 20 to 40 aged user enrolled most \n#   4) In histogram of numscreen above 40 screen user is to less \n#   5) remaining all histogram is in 0 and 1","fc99de4f":"for i,j in enumerate(features):\n    print(i,j)","6f229f89":"sns.set()\nplt.figure(figsize=(15,7))\nplt.title('Correlation Bar Plot of Enrolled Features', fontsize= 20)\ndf2 = df1.drop(['enrolled'], axis= 1)\naxis = sns.barplot(df2.columns,df2.corrwith(df1.enrolled))\naxis.tick_params(labelsize=15, labelrotation = 20)","b7787ce4":"df.dtypes","d071aa7f":"df.head()","72d6d172":"screen_data = pd.read_csv('..\/input\/app-data\/top_screens.csv').top_screens.values\nscreen_data","b74cfaf8":"type(screen_data)","9c5b2ca4":"df['screen_list'] = df.screen_list.astype(str)+','\ndf.head()","516835c4":"for screen_name in screen_data:\n    df[screen_name] = df.screen_list.str.contains(screen_name).astype(int)\n    df['screen_list'] = df.screen_list.str.replace(screen_name+',',\"\")","44c399e8":"df.shape","02ccd6e4":"df.head()","ea8a9552":"df.loc[0,'screen_list']","f4f6d866":"df['remain_screen_list'] = df.screen_list.str.count(\",\")","d7871ca1":"# droped both time columns\ndf.shape","35597cd7":"df = df.drop(['first_open','enrolled_date','screen_list'], axis=1)","e621c87f":"df.columns\n","3a5216a8":"df.head()","4d07bd1b":"# sum of all saving screen at one place\nsaving_screen=[\n    'Saving9',\n    'Saving1',\n    'Saving8',\n    'Saving10',\n    'Saving4',\n    'Saving7',\n    'Saving2',\n    'Saving6',\n    'Saving5',\n    'Saving2Amount'\n]\ndf['saving_screen_count']= df[saving_screen].sum(axis=1)\ndf = df.drop(columns= saving_screen)\ndf.shape","90800c97":"# sum of all loan column at one place\nloan = [\n    'Loan2',\n    'Loan',\n    'Loan4',\n    'Loan3'\n]\ndf['loan_col']= df[loan].sum(axis=1)\n","66e342fe":"df = df.drop(columns= loan)","d9cb7aac":"# credit column sum\ncredit = [\n    'Credit3Container',\n    'Credit3',\n    'Credit2',\n    'Credit3Dashboard',\n    'Credit1'\n]\ndf['credit_col']= df[credit].sum(axis=1)","fff08fba":"df = df.drop(columns=credit)","9de65a8a":"# for cc \ncc = [\n    'CC1',\n    'CC1Category',\n    'CC3',\n]\ndf['cc_col'] = df[cc].sum(axis=1)","2edaf885":"df =df.drop(columns=cc)","95fd38b9":"df.shape","9eb16374":"df.describe()","d4121208":"df.head() # our pure cleaned data","d3257164":"# plt.figure(figsize=(20,10))\n# sns.heatmap(df.corr(),annot=True, linewidth=2) # --> its take a to much time for run","a8c0275f":"x= df.drop(columns = 'enrolled')\ny = df['enrolled']  # our target ","970bfc1f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=10)","22f90c2d":"x_train.shape","775f379d":"x_test.shape","554254b5":"y_train.shape","f30e55e8":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train_sc = sc.fit_transform(x_train)\nx_test_sc = sc.transform(x_test)","469f7a16":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score","2da4234b":"from sklearn.tree import DecisionTreeClassifier\nmodel_DTC = DecisionTreeClassifier(criterion='entropy', random_state=10)  # for gini entropy score is 0.7081\nmodel_DTC.fit(x_train,y_train)\ny_predict_DTC = model_DTC.predict(x_test)\naccuracy_score(y_test, y_predict_DTC) ","0083b6d6":"# standerd scaling data to build model\nmodel_DTC_SC =  DecisionTreeClassifier(criterion='entropy', random_state=10) # for gini 0.5502\nmodel_DTC_SC.fit(x_train_sc, y_train)\ny_predict_DTC_SC = model_DTC_SC.predict(x_test_sc)\naccuracy_score(y_test,y_predict_DTC_SC)","ddc95fd8":"from sklearn.neighbors import KNeighborsClassifier\nmodel_KN = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nmodel_KN.fit(x_train,y_train)\ny_predicted_KN = model_KN.predict(x_test)\naccuracy_score(y_test,y_predicted_KN)","25ab968f":"# standerd scaling data to build model\nmodel_KN_SC  = KNeighborsClassifier(n_neighbors=5,p=2, metric='minkowski')\nmodel_KN_SC.fit(x_train_sc,y_train)\ny_predicted_KN_SC = model_KN_SC.predict(x_test_sc)\naccuracy_score(y_test,y_predicted_KN_SC)","d2fefe8e":"from sklearn.naive_bayes import GaussianNB\nmod_G = GaussianNB()\nmod_G.fit(x_train,y_train)\ny_predicted_G = mod_G.predict(x_test)\naccuracy_score(y_test,y_predicted_G)","c4f06963":"# standerd scaling data to build model\nmod_G_SC = GaussianNB()\nmod_G_SC.fit(x_train_sc,y_train)\ny_predicted_G_SC = mod_G_SC.predict(x_test_sc)\naccuracy_score(y_test,y_predicted_G_SC)","dc869eb0":"from sklearn.ensemble import RandomForestClassifier\nmodel_RFC = RandomForestClassifier(n_estimators=10, criterion='gini')\nmodel_RFC.fit(x_train,y_train)\ny_predicted_RFC = model_RFC.predict(x_test)\naccuracy_score(y_test,y_predicted_RFC)","d3733a9b":"# standerd scaling data to build model\nmodel_RFC_SC = RandomForestClassifier(n_estimators=10, criterion='gini')\nmodel_RFC_SC.fit(x_train_sc,y_train)\ny_predicted_RFC_SC = model_RFC_SC.predict(x_test_sc)\naccuracy_score(y_test,y_predicted_RFC_SC)","1c540d5a":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty='l2', C = 1, random_state=10)\nmodel.fit(x_train,y_train)\ny_predicted_l = model.predict(x_test)\n# accuracy_score(y_test,y_predicted_l)\nmodel.score(x_test,y_test)","63800c2d":"# standerd scaling data to build model\nmodel_sc = LogisticRegression(penalty='l2', C = 1, random_state=10)\nmodel_sc.fit(x_train_sc, y_train)\ny_predicted_ls = model_sc.predict(x_test_sc)\naccuracy_score(y_test,y_predicted_ls)","9dd2b1bd":"from sklearn.svm import SVC\nmodel_svc = SVC()\nmodel_svc.fit(x_train,y_train)\nmodel_svc.predict(x_test)\nmodel.score(x_test,y_test)\n","026e13b7":"# standerd scaling data to build model\nmodel_svc_sc= SVC()\nmodel_svc_sc.fit(x_train_sc,y_train)\ny_predicted_svc_sc = model_svc_sc.predict(x_test_sc)\naccuracy_score(y_test,y_predicted_svc_sc)","6d4c9730":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n # eval_metric='mlogloss' parameter to not change behavior and not show a warning\nxgb_model.fit(x_train, y_train)\ny_pred_xgb = xgb_model.predict(x_test)\naccuracy_score(y_test, y_pred_xgb)","181f7467":"# train with Standert Scaling dataset\nxgb_model_sc = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n#  use_label_encoder=False to handle warning\nxgb_model_sc.fit(x_train_sc, y_train)\ny_pred_xgb_sc = xgb_model_sc.predict(x_test_sc)\naccuracy_score(y_test, y_pred_xgb_sc)","dfb89c3e":"data = { 'model_name' : ['Desicion Tree', 'K_Nearest', 'naive Bayes', 'Random forest','Logistic Regression', 'SVM', 'XGBoost'],\n        'Training Type' : ['Reguler','SC','SC','SC','SC','SC','SC'],\n        'score' : ['0.7106','0.7368','0.6967','0.7615','0.7522','0.7743','0.7814']\n}\nbest_model = pd.DataFrame(data)\nbest_model","2e359274":"# we use xgboost and svm give the best score than other ML algorithm\n# but we are going with xgboost because of xgboost is much more better then svm","3cb52970":"plt.figure(figsize=(7,4))\ncm_xgb = confusion_matrix(y_test,y_pred_xgb_sc)\nsns.heatmap(cm_xgb, annot=True, fmt='g')\nplt.title('XGBoost model Confusion Matrix')","bb91accd":"cr_xgb = classification_report(y_test,y_pred_xgb_sc)\nprint('Classification Report --------> \\n', cr_xgb)","b5f66a35":"from sklearn.model_selection import cross_val_score\nmodel = XGBClassifier(eval_metric='mlogloss',use_label_encoder=False)\ncross_validation = cross_val_score(model, x_train_sc, y_train, cv=5)\ncross_validation","8b2c032f":"cross_validation.mean()","cf39180e":"# save the model using pickle\nimport pickle\nwith open('app_data_pickle','wb') as file:\n    pickle.dump(xgb_model_sc,file)","16ce399f":"# save the model using joblib\nimport joblib\njoblib.dump(xgb_model_sc,'app_data_model_joblib')","d6fee496":"#  Logistic Regreesion","95de8d9e":"#  Support vactor Machine","0f3b813b":"# Cross Validation","1c1b1699":"# Heatmap using Correlation metrix","e40e75aa":"#  Naive Bayes","7077a973":"# Random Forest ","aa1ed7c4":"# countplot of enrolled","a3895de8":"# XGBoost ","e504c280":"# histogram of each feature of df1","e5cee1fe":"# correlation bar plot with 'enrolled ' features","6f191a1e":"#  Classification report","fab646f7":"#  K -  Nearest Neighbor ","8558d393":"**The mean value cross-validation and XGBoost model accuracy is 78%. That means our XGBoost model is a generalized model.**","ee31ca76":"# pair plot ","742c7618":"The multiple features in the different units so for the best accuracy need to convert all features in a single unit.\n\n","17862be6":"# Model Building With Ml algorithm","49c090d9":"# Decision Tree Classifier","2d712337":"Confusion Matrix"}}