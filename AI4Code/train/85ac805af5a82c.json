{"cell_type":{"e208c944":"code","f038016c":"code","f24a0968":"code","a5c64cce":"code","e4ec308b":"code","a50a0492":"code","1742717f":"code","27a49c04":"code","87db7bff":"code","343f357b":"code","22b222de":"code","615119fa":"code","79d4dc0b":"code","72904c7d":"code","45c7ee2d":"code","07ec8151":"code","f86a72d4":"code","075fcaa5":"code","e4ca3fea":"code","7bcb9cba":"code","52e74337":"code","9d1513e3":"code","d33ae241":"code","ca51971e":"code","64d76ce7":"code","06b8cc1a":"code","f661e9ce":"code","42bd057d":"code","ad32f701":"code","16409ada":"code","91017e55":"code","26466887":"code","a9dc534a":"code","095f7265":"code","647a9e08":"code","1f3a9490":"code","6f19d56d":"code","a00805d0":"code","7d15ecea":"code","b40d1f89":"code","fda88dae":"code","8619ed6e":"code","f88d42d1":"code","7f139328":"code","98522246":"code","6f1c5547":"code","786630db":"code","ad43b5aa":"code","5b39aabd":"code","8416a51a":"code","881a3664":"markdown","cc50016d":"markdown","f13d2639":"markdown","a4c79625":"markdown","33efd97e":"markdown","5f452a77":"markdown","61ed5283":"markdown","27bb8a8b":"markdown","144a0858":"markdown","2cf1e0f0":"markdown","3037c59a":"markdown","8f34007a":"markdown","08d59878":"markdown","2e2d60dd":"markdown","50d3ae86":"markdown","6a468eee":"markdown","10767951":"markdown","8b4483ba":"markdown","6e29c4ea":"markdown","3f5701f4":"markdown","a9c703c2":"markdown","bac6ed4b":"markdown","8f8b1894":"markdown","c34103e3":"markdown","9c28f502":"markdown","c5372eff":"markdown","9c8039cc":"markdown","d5079cee":"markdown","2ffedac6":"markdown","d1de3f21":"markdown","992361df":"markdown","d422d876":"markdown","cb592a12":"markdown","0131447c":"markdown","5b8016fb":"markdown","ba67e256":"markdown"},"source":{"e208c944":"def missing(df) : \n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_number', 'Missing_percent'])\n    return missing_values \n\ndef categorize(df) :\n    Quantitive_features = df.select_dtypes([np.number]).columns.tolist()\n    Categorical_features = df.select_dtypes(exclude = [np.number]).columns.tolist()\n    Discrete_features = [col for col in Quantitive_features if len(df[col].unique()) < 10]\n    Continuous_features = [col for col in Quantitive_features if col not in Discrete_features]\n    print(f\"Quantitive feautres : {Quantitive_features} \\nDiscrete features : {Discrete_features} \\nContinous features : {Continuous_features} \\nCategorical features : {Categorical_features}\\n\")\n    print(f\"Number of quantitive feautres : {len(Quantitive_features)} \\nNumber of discrete features : {len(Discrete_features)} \\nNumber of continous features : {len(Continuous_features)} \\nNumber of categorical features : {len(Categorical_features)}\")\n    \ndef unique(df) : \n    tb1 = pd.DataFrame({'Columns' : df.columns, 'Number_of_Unique' : df.nunique().values.tolist(),\n                       'Sample1' : df.sample(1).values.tolist()[0], 'Sample2' : df.sample(1).values.tolist()[0], \n                       'Sample3' : df.sample(1).values.tolist()[0],\n                       'Sample4' : df.sample(1).values.tolist()[0], 'Sample5' : df.sample(1).values.tolist()[0]})\n    return tb1\n    \ndef data_glimpse(df) :   \n    \n    # Dataset preview \n    print(\"1. Dataset Preview \\n\")\n    display(df.head())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Columns imformation\n    print(\"2. Column Imformation \\n\")\n    print(f\"Dataset have {df.shape[0]} columns and {df.shape[1]} rows\")\n    print(\"\\n\") \n    print(f\"Dataset Column name : {df.columns.values}\")\n    print(\"\\n\")\n    categorize(df)\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Basic imformation table \n    print(\"3. Missing data table : \\n\")\n    display(missing(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"4. Number of unique value by column : \\n\")\n    display(unique(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"5. Describe table : \\n\")\n    display(df.describe())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(df.info())\n    print(\"-------------------------------------------------------------------------------\\n\")","f038016c":"# Data Analysis\nimport warnings \nwarnings.filterwarnings('ignore')\n    \nimport pandas as pd\nimport numpy as np\nimport os \nimport missingno as msno\n    \n# Data View\npd.options.display.max_columns = 200\n\n# Import Basic Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# Data Visualization : Plotly library\nimport plotly.express as px\n    \nimport plotly.graph_objects as go\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n    \nfrom plotly.subplots import make_subplots \nimport plotly.figure_factory as ff ","f24a0968":"df_train_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col = \"PassengerId\")\ndf_test_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col = \"PassengerId\")\n\ndf_train_raw.head()","a5c64cce":"data_glimpse(df_train_raw)","e4ec308b":"df_train_raw = df_train_raw[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\ndf_train_raw.head()","a50a0492":"def count_per_table(fea) : \n    count = df_train_raw.groupby(by = fea).size()\n    percent = round(df_train_raw.groupby(by = fea).size()\/df_train_raw.shape[0]*100, 2)\n    indexs = count.index\n    \n    table = pd.concat([count, percent], axis = 1)\n    table.columns = ['Count', 'Percent(%)']\n    \n    return(table)","1742717f":"count_per_table('Survived')","27a49c04":"count_per_table('Pclass')","87db7bff":"count_per_table('Sex')","343f357b":"count_per_table('SibSp')","22b222de":"count_per_table('Parch')","615119fa":"count_per_table('Embarked')","79d4dc0b":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = df_train_raw['Age'],\n        xbins = dict(\n            start = 0,\n            end = 90,\n            size = 10 \n        )\n    )\n)\n\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Histogram of Age<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Age\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Count\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n)\n\nfig.show()","72904c7d":"fig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = df_train_raw['Fare'],\n        xbins = dict(\n            start = 0,\n            end = 100,\n            size = 1 \n        )\n    )\n)\n\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Histogram of Age<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Fare\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Count\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n)\n\nfig.show()","45c7ee2d":"# Correlation \n\ncorr_target = df_train_raw.corr(method = 'spearman')['Survived'].sort_values(ascending = False)\ndisplay(corr_target)","07ec8151":"# Age - Survived\n\nfig = px.box(df_train_raw, \n             x = 'Survived', y = 'Age',\n             color = 'Survived',\n             color_discrete_sequence = [\"red\", \"green\"]\n             )\n\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Difference of survived by age<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Survived\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Age\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n)\n\nfig.show()","f86a72d4":"# Fare - Survived\n\nfig = px.box(df_train_raw, \n             x = 'Survived', y = 'Fare',\n             color = 'Survived',\n             color_discrete_sequence = [\"red\", \"green\"],\n             log_y = True\n             )\n\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Difference of survived by Fare<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Survived\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Age\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n)\n\nfig.show()","075fcaa5":"# Function for checking effects of categorical values to target variable. \n\ndef uni_target(fea) : \n    \n    tb1 = round(df_train_raw.groupby(fea)['Survived'].mean()*100, 2)\n    display(tb1)\n\n    tb2 = df_train_raw.groupby([fea, 'Survived']).size().reset_index()\n    tb2.columns = [fea, 'Survived', 'Count']\n    tb2['Survived'] = tb2['Survived'].replace({0 : 'Non-Survived', 1 : 'Survived'})\n    \n    fig = px.bar(tb2, x = 'Survived', y = 'Count', \n                 color = 'Survived', \n                 facet_col = fea,\n                 color_discrete_map = {\n                     'Non-Survived' : 'red', \n                     'Survived' : 'green'\n                 }\n                )\n    \n    fig.update_layout(\n    {\n        \"yaxis\": {\n            \"title\": \"Survived\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n    )\n    fig.show()","e4ca3fea":"# Pclass \n\nuni_target('Pclass')","7bcb9cba":"uni_target('Sex')","52e74337":"uni_target('Embarked')","9d1513e3":"uni_target('SibSp')","d33ae241":"uni_target('Parch')","ca51971e":"# Age group anaylsis \n\nbins = [0, 20, 40, 60, 80, 100]\nlabels = ['0~20', '20~40', '40~60', '60~80', '80~100']\n\ndf_train_raw['Age_dec'] = pd.cut(df_train_raw['Age'], bins = bins, labels = labels)\ndf_train_raw['Age_dec'].head()","64d76ce7":"uni_target('Age_dec')","06b8cc1a":"# Load library\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.pipeline import Pipeline \nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix","f661e9ce":"# Check Full columns \n\ndf_train_raw.columns\n\n# Divide Target feature from datasets \n\ny = df_train_raw.Survived \n\nX_raw = df_train_raw.drop(columns = ['Survived', 'Age_dec'], axis = 1)\n\nX_test = df_test_raw[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n\n# Variable Grouping\n\ncat_cols = X_raw.select_dtypes(exclude = [np.number]).columns.tolist()\n\nnum_cols = X_raw.select_dtypes([np.number]).columns.tolist()\n\ndis_cols = [fea for fea in num_cols if len(X_raw[fea].unique()) < 10]\n\nnum_cols = [fea for fea in num_cols if fea not in dis_cols]\n\n# Check divided columns \n\nprint(f'Total Variables : {df_train_raw.columns.tolist()}\\nCategorical Variables : {cat_cols}\\nDiscrete Variables : {dis_cols}\\nNumerical Variables : {num_cols}')\n\n# Check size of train and test\n\nprint(\"\\n===================== Check Size Train vs Test =======================\")\nprint(f'Size of train = {X_raw.shape}\\nSize of test = {X_test.shape}')","42bd057d":"# Make pipeline for preprocessing \n\n# Categorical most frequency : Sex, Embarked + OneHotEncoder() \n# Numerical most frequency : Pclass, SibSp, Parch\n# Median : Age, Fare \n\n# Train test split \n\nX_train, X_valid, y_train, y_valid = train_test_split(X_raw, y, test_size = 0.2, random_state = 42) \n\n# Preprocessing for numerical data\n\nnum_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scale', StandardScaler())\n])\n\n# Preprocessing for categorical data\n\ncat_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore')),\n])\n\n# Preprocessing for discrete data\n\ndis_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('scale', StandardScaler())\n])\n\n# Bundle Preprocessing for all varaibles\n\npreprocessor = ColumnTransformer(transformers = [\n    ('num', num_transformer, num_cols),\n    ('cat', cat_transformer, cat_cols),\n    ('dis', dis_transformer, dis_cols)\n])","ad32f701":"# Score Metrics \n\ndef accuracy_table(best_model, y_valid, preds) : \n    \n    acc_score = accuracy_score(y_valid, preds) \n    f_score = f1_score(y_valid, preds)\n    prec_score = precision_score(y_valid, preds)\n    rec_score = recall_score(y_valid, preds)\n    cv_score = cross_val_score(best_model, X_raw, y, cv = 4).mean()\n    \n    table = pd.DataFrame([[acc_score, f_score, prec_score, rec_score, cv_score]], \n                         columns = ['accuracy_score', 'f1_score', 'precision_score', 'recall_score', 'cross_validation_score'])\n    \n    return table \n    \ndef make_confusion_matrix(y_valid, preds) : \n    \n    z = confusion_matrix(y_valid, preds)\n    x = ['TN', 'FP']\n    y = ['FN', 'TP']\n\n    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z, colorscale='Greys')\n\n    fig.update_layout(\n    {\n        \"title\": {\n            \"text\": f\"<b>Confusion Matrix of Result<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"template\":'plotly_dark',\n        \"height\" : 800,\n        \"width\" : 800\n    }\n    )\n\n    fig.show()","16409ada":"from sklearn.linear_model import LogisticRegression\n\n# Make pipeline for model \n\nlgr_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', LogisticRegression(random_state = 0))\n])\n\n# Parameters\n\nparams = {\n    'model__penalty' : ['l1', 'l2'],\n    'model__C' : [0.01, 0.02, 0.05, 0.1, 0.2]\n}\n\n# apply GridsearchCV\n\nlgr_tree = GridSearchCV(lgr_pipe, param_grid = params, cv = 4)\nlgr_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", lgr_tree.best_params_)\nprint(\"best score : \", lgr_tree.best_score_)","91017e55":"# Find best model and mad accuracy matrix \n\nbest_model = lgr_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nlgr_table = accuracy_table(best_model, y_valid, preds)\nlgr_table","26466887":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","a9dc534a":"from sklearn.naive_bayes import GaussianNB\n\n# Make pipeline for model \n\ngnb_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', GaussianNB())\n])\n\ngnb_pipe.fit(X_train, y_train)\n\nprint(f\"model score : {gnb_pipe.score(X_train, y_train)}\")","095f7265":"# Find best model and mad accuracy matrix \n\npreds = gnb_pipe.predict(X_valid)\n\ngnb_table = accuracy_table(best_model, y_valid, preds)\ngnb_table","647a9e08":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","1f3a9490":"from sklearn.neighbors import KNeighborsClassifier\n\n# Make pipeline for model \n\nknc_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsClassifier())\n])\n\n# Parameters\n\nparams = {\n    'model__n_neighbors' : [8, 9, 10, 11, 12, 20],\n    'model__weights' : ['uniform', 'distance'],\n    'model__leaf_size' : [10, 20, 30, 40, 50]\n}\n\n# apply GridsearchCV\n\nknc_tree = GridSearchCV(knc_pipe, param_grid = params, cv = 4)\nknc_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", knc_tree.best_params_)\nprint(\"best score : \", knc_tree.best_score_)","6f19d56d":"# Find best model and mad accuracy matrix \n\nbest_model = knc_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nknc_table = accuracy_table(best_model, y_valid, preds)\nknc_table","a00805d0":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","7d15ecea":"from sklearn.tree import DecisionTreeClassifier\n\n# Make pipeline for model \n\ndtc_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', DecisionTreeClassifier(random_state = 0))\n])\n\n# Parameters\n\nparams = {\n    'model__criterion' : ['gini', 'entropy'],\n    'model__splitter' : ['best', 'random'],\n    'model__max_depth' : [2, 3, 4, 5]\n}\n\n# apply GridsearchCV\n\ndtc_tree = GridSearchCV(dtc_pipe, param_grid = params, cv = 4)\ndtc_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", dtc_tree.best_params_)\nprint(\"best score : \", dtc_tree.best_score_)","b40d1f89":"# Find best model and mad accuracy matrix \n\nbest_model = dtc_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\ndtc_table = accuracy_table(best_model, y_valid, preds)\ndtc_table","fda88dae":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","8619ed6e":"from sklearn.svm import SVC\n\n# Make pipeline for model \n\nsvc_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', SVC(random_state = 0))\n])\n\n# Parameters\n\nparams = {\n    'model__C' : [2, 5, 10, 20],\n    'model__kernel' : ['linear', 'poly'],\n    'model__degree' : [2, 3]\n}\n\n# apply GridsearchCV\n\nsvc_tree = GridSearchCV(svc_pipe, param_grid = params, cv = 4)\nsvc_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", svc_tree.best_params_)\nprint(\"best score : \", svc_tree.best_score_)","f88d42d1":"# Find best model and mad accuracy matrix \n\nbest_model = svc_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nsvc_table = accuracy_table(best_model, y_valid, preds)\nsvc_table","7f139328":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","98522246":"from sklearn.ensemble import RandomForestClassifier\n\n# Make pipeline for model \n\nrfc_pipe = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', RandomForestClassifier(random_state = 0))\n])\n\n# Parameters\n\nparams = {\n    'model__n_estimators' : [10, 20, 50, 100, 200],\n    'model__max_depth' : [4, 5, 6]\n}\n\n# apply GridsearchCV\n\nrfc_tree = GridSearchCV(rfc_pipe, param_grid = params, cv = 4)\nrfc_tree.fit(X_train, y_train)\n\nprint(\"best parameter : \", rfc_tree.best_params_)\nprint(\"best score : \", rfc_tree.best_score_)","6f1c5547":"# Find best model and mad accuracy matrix \n\nbest_model = rfc_tree.best_estimator_\npreds = best_model.predict(X_valid)\n\nrfc_table = accuracy_table(best_model, y_valid, preds)\nrfc_table","786630db":"# Confusion matrix of result model \n\nmake_confusion_matrix(y_valid, preds)","ad43b5aa":"# Buntld Score of models \n\ntotal_result = pd.concat([lgr_table, gnb_table, knc_table, dtc_table, svc_table, rfc_table], axis = 0)\ntotal_result.index = ['Logistic Regression', 'Naive Bayes', 'K-Nearest Neighbors', 'Decision Tree', 'Support Vector Machine', 'RandomForest Classifier']\ntotal_result","5b39aabd":"colors = px.colors.sequential.RdBu[:5]\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x = total_result.index,\n    y = total_result.cross_validation_score,\n    text = round(total_result.cross_validation_score, 3),\n    marker_color = colors\n))\n\nfig.update_layout(\n{\n    \"title\": {\n        \"text\": \"<b>Cross Validation Score by model<\/b>\",   # Can add title value using f\" {}\" \n        \"x\": 0.5,\n        \"y\": 0.9,\n        \"font\": {\n            \"size\": 15\n        }\n    },\n    \"xaxis\": {\n        \"title\": \"Model Selection\",\n        \"showticklabels\":True,\n        \"tickfont\": {\n            \"size\": 10               \n        }\n    },\n    \"yaxis\": {\n        \"title\": \"Accuracy Score(Cross Validation)\",\n        \"tickfont\": {\n            \"size\": 10                \n        }\n    },\n    \"template\":'plotly_dark'\n}\n)\n\nfig.show()","8416a51a":"# Apply to test dataset\n\nbest_accuracy_model = knc_tree.best_estimator_\n\npreds_test = best_accuracy_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId' : X_test.index,\n                       'Survived' : preds_test})\n\noutput.to_csv('.\/submissions.csv', index = False)","881a3664":"## K-Nearest Neighbors ","cc50016d":"7. Age ","f13d2639":"## Library importing ","a4c79625":"As we see result of function 'data_glimpse()', we can see that there are NaN values more than 70% in feature 'Cabin'.   \nAnd also, we don't need categorical value which have unique value more than 500.   \n\nSo features what we will use is below.  \n- Survivde(Target) \n- Pclass \n- Sex\n- Age\n- SibSp \n- Parch\n- Fare \n- Embarked ","33efd97e":"male board on titanic more than feamle.","5f452a77":"# <center> Titanic Prediction \n\n# Data description\n## Datasource explaining \n- Datasets name : \"training.csv\", \"test.csv\"\n- Datasets source : [Kaggle Competition : Titanic Predictions](https:\/\/www.kaggle.com\/c\/titanic\/data)\n- Feature Explanation : \n\n| Feature type | Name | Data type | \n| : --- : | : --- : | : --- : | \n| Non-use | PassengerId | int64 | \n| Target | Survivded | boolean | \n| Numerical-Discrete | Pclass | int64 | \n| Non-use | Name | object | \n| Numerical-Continuous | Age | int64 | \n| Numerical-Discrete | SibSp | int64 | \n| Numerical-Discrete | Parch | int64 | \n| Non-use | Ticket | object | \n| Numerical-Continuous | Fare | float64 | \n| Categorical | Cabin | object | \n| Categorical | Embarked | object | \n\n## Data Dictionary\n    \n1. Sibsp : of siblings \/ spouse aboard the Titanic \n2. Parch : of paraents \/ children aboard the Titanic \n3. embakred : C = Cherboutg, Q = Queenstown, S = Southampton\n\n# Anaylsis Preparation\n## Function importing ","61ed5283":"Most people board without siblings and children or with only one siblings or spouse and thier children. ","27bb8a8b":"2. Pclass","144a0858":"# EDA + Visualization","2cf1e0f0":"## Data importing ","3037c59a":"## Feature Univariate Analysis","8f34007a":"Only 38.38% survived from Titanic. ","08d59878":"## Support Vector Machine","2e2d60dd":"## Decision Tree ","50d3ae86":"5. Parch ","6a468eee":"## RandomForest Classifier","10767951":"Women survived almost 74%, while men only survived 18.89%. Also Age group 0~20 survived almost 45.81%(highest among age group). It might be rules of Birkenhead(As i think). ","8b4483ba":"## Naive Bayes ","6e29c4ea":"3. Sex","3f5701f4":"1. Survived","a9c703c2":"More people survived when they pay more fare for titanic.","bac6ed4b":"Survived-0's IQR range is slightly higher than its Survived-1, but there isn't significant difference by age. ","8f8b1894":"6. Embarked","c34103e3":"4. SibSp","9c28f502":"People board on 3rd class most, while 2nd class least.","c5372eff":"As we can see, age group 20-30, 30-40 aboared on titanic most, and has same count in 0-10 : 50-60 and 10-20 and 40-50. It seems that the graph doen't have normality. ","9c8039cc":"Both 'SibSp' and 'Parch' variables have highest survive rate at 1 or 2. We will see more complicate reason in multivariate analysis. ","d5079cee":"## Logistic Regression","2ffedac6":"## Feature Analysis to Target  \nWe will see how independent variables affect target('Survived').  ","d1de3f21":"# Data glimpse ","992361df":"# Model selection and Conclusion","d422d876":"# Modeling ","cb592a12":"People survived more about their class of room (1st - 2nd - 3rd), People in 1st class survived almost 63% while people in 3rd class survived only 24.24%.","0131447c":"# Data Preprocessing ","5b8016fb":"**1. See How univariate features affects target**","ba67e256":"'Fare' has positive correlation about 0.324, while 'Pclass' has negative correlation about -0.340."}}