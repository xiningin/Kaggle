{"cell_type":{"c7c729db":"code","7d87b519":"code","a8727c88":"code","f3d710ee":"code","3d1d7a19":"code","c4ad3264":"code","cd4398f4":"code","336f297c":"code","a66f020b":"code","b8b7c7ca":"code","22540d5e":"code","06972c2c":"code","1072862a":"code","4aa78740":"code","c5e86f87":"code","8e874094":"code","f823751d":"code","1b55e42b":"markdown","bdfb5f00":"markdown","d1c8f0ec":"markdown","e86d9ede":"markdown","6128deb8":"markdown","80459f05":"markdown","f2240aa3":"markdown"},"source":{"c7c729db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d87b519":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import OrderedDict","a8727c88":"# coding: utf-8\ntry:\n    import urllib.request\nexcept ImportError:\n    raise ImportError('You should use Python 3.x')\nimport os.path\nimport gzip\nimport pickle\nimport os\nimport numpy as np\n\n\nurl_base = 'http:\/\/yann.lecun.com\/exdb\/mnist\/'\nkey_file = {\n    'train_img':'train-images-idx3-ubyte.gz',\n    'train_label':'train-labels-idx1-ubyte.gz',\n    'test_img':'t10k-images-idx3-ubyte.gz',\n    'test_label':'t10k-labels-idx1-ubyte.gz'\n}\n\ndataset_dir = '\/kaggle\/working'\nsave_file = dataset_dir + \"\/mnist.pkl\"\n\ntrain_num = 60000\ntest_num = 10000\nimg_dim = (1, 28, 28)\nimg_size = 784\n\n\ndef _download(file_name):\n    file_path = dataset_dir + \"\/\" + file_name\n    \n    if os.path.exists(file_path):\n        return\n\n    print(\"Downloading \" + file_name + \" ... \")\n    urllib.request.urlretrieve(url_base + file_name, file_path)\n    print(\"Done\")\n    \ndef download_mnist():\n    for v in key_file.values():\n       _download(v)\n        \ndef _load_label(file_name):\n    file_path = dataset_dir + \"\/\" + file_name\n    \n    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n    with gzip.open(file_path, 'rb') as f:\n            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n    print(\"Done\")\n    \n    return labels\n\ndef _load_img(file_name):\n    file_path = dataset_dir + \"\/\" + file_name\n    \n    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n    with gzip.open(file_path, 'rb') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n    data = data.reshape(-1, img_size)\n    print(\"Done\")\n    \n    return data\n    \ndef _convert_numpy():\n    dataset = {}\n    dataset['train_img'] =  _load_img(key_file['train_img'])\n    dataset['train_label'] = _load_label(key_file['train_label'])    \n    dataset['test_img'] = _load_img(key_file['test_img'])\n    dataset['test_label'] = _load_label(key_file['test_label'])\n    \n    return dataset\n\ndef init_mnist():\n    download_mnist()\n    dataset = _convert_numpy()\n    print(\"Creating pickle file ...\")\n    with open(save_file, 'wb') as f:\n        pickle.dump(dataset, f, -1)\n    print(\"Done!\")\n\ndef _change_one_hot_label(X):\n    T = np.zeros((X.size, 10))\n    for idx, row in enumerate(T):\n        row[X[idx]] = 1\n        \n    return T\n    \n\ndef load_mnist(normalize=True, flatten=True, one_hot_label=False):\n    \"\"\"\u8bfb\u5165MNIST\u6570\u636e\u96c6\n    \n    Parameters\n    ----------\n    normalize : \u5c06\u56fe\u50cf\u7684\u50cf\u7d20\u503c\u6b63\u89c4\u5316\u4e3a0.0~1.0\n    one_hot_label : \n        one_hot_label\u4e3aTrue\u7684\u60c5\u51b5\u4e0b\uff0c\u6807\u7b7e\u4f5c\u4e3aone-hot\u6570\u7ec4\u8fd4\u56de\n        one-hot\u6570\u7ec4\u662f\u6307[0,0,1,0,0,0,0,0,0,0]\u8fd9\u6837\u7684\u6570\u7ec4\n    flatten : \u662f\u5426\u5c06\u56fe\u50cf\u5c55\u5f00\u4e3a\u4e00\u7ef4\u6570\u7ec4\n    \n    Returns\n    -------\n    (\u8bad\u7ec3\u56fe\u50cf, \u8bad\u7ec3\u6807\u7b7e), (\u6d4b\u8bd5\u56fe\u50cf, \u6d4b\u8bd5\u6807\u7b7e)\n    \"\"\"\n    if not os.path.exists(save_file):\n        init_mnist()\n        \n    with open(save_file, 'rb') as f:\n        dataset = pickle.load(f)\n    \n    if normalize:\n        for key in ('train_img', 'test_img'):\n            dataset[key] = dataset[key].astype(np.float32)\n            dataset[key] \/= 255.0\n            \n    if one_hot_label:\n        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n    \n    if not flatten:\n         for key in ('train_img', 'test_img'):\n            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n\n    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) ","f3d710ee":"(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\nprint(x_train.shape, t_train.shape, x_test.shape, t_test.shape)","3d1d7a19":"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n    Parameters\n    ----------\n    input_data : \u7531(\u6570\u636e\u91cf, \u901a\u9053, \u9ad8, \u957f)\u76844\u7ef4\u6570\u7ec4\u6784\u6210\u7684\u8f93\u5165\u6570\u636e\n    filter_h : \u6ee4\u6ce2\u5668\u7684\u9ad8\n    filter_w : \u6ee4\u6ce2\u5668\u7684\u957f\n    stride : \u6b65\u5e45\n    pad : \u586b\u5145\n\n    Returns\n    -------\n    col : 2\u7ef4\u6570\u7ec4\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\n","c4ad3264":"def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n    Parameters\n    ----------\n    col :\n    input_shape : \u8f93\u5165\u6570\u636e\u7684\u5f62\u72b6\uff08\u4f8b\uff1a(10, 1, 28, 28)\uff09\n    filter_h :\n    filter_w\n    stride\n    pad\n\n    Returns\n    -------\n\n    \"\"\"\n    N, C, H, W = input_shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n    return img[:, :, pad:H + pad, pad:W + pad]\n","cd4398f4":"def softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) \/ np.sum(np.exp(x), axis=0)\n        return y.T\n    x = x - np.max(x) # \u6ea2\u51fa\u5bf9\u7b56\n    return np.exp(x) \/ np.sum(np.exp(x))","336f297c":"def cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    # \u76d1\u7763\u6570\u636e\u662fone-hot-vector\u7684\u60c5\u51b5\u4e0b\uff0c\u8f6c\u6362\u4e3a\u6b63\u786e\u89e3\u6807\u7b7e\u7684\u7d22\u5f15\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size","a66f020b":"class Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n\n        # \u4e2d\u95f4\u6570\u636e\uff08backward\u65f6\u4f7f\u7528\uff09\n        self.x = None\n        self.col = None\n        self.col_W = None\n\n        # \u6743\u91cd\u548c\u504f\u7f6e\u53c2\u6570\u7684\u68af\u5ea6\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        FN, C, FH, FW = self.W.shape #\u5377\u79ef\u6838\u7684\u5f62\u72b6\n        N, C, H, W = x.shape #\u8f93\u5165\u6570\u636e\u5f62\u72b6\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u83b7\u53d6\u8f93\u51fa\u6570\u636e\u7684\u9ad8\u548c\u5bbd\u5206\u522b\u5b58\u50a8\u5728\u53d8\u91cfout_h\u548c\u53d8\u91cfout_w\n        out_h = 1 + int((H + 2*self.pad - FH) \/ self.stride)\n        out_w = 1 + int((W + 2*self.pad - FW) \/ self.stride)\n\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u6765\u5b8c\u6210\u5377\u79ef\u8fd0\u7b97\n        ###(\u5176\u4e2dcol\u3001col_W\u3001out\u5206\u522b\u8868\u793a\u5c55\u5f00\u7684\u6570\u636e\u3001\u5377\u79ef\u6838\u3001\u5377\u79ef\u8fd0\u7b97\u7684\u7ed3\u679c)\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col_W = self.W.reshape(FN, -1).T\n        out = np.dot(col, col_W) + self.b\n\n        #\u8f93\u51fa\u5927\u5c0f\u8f6c\u6362\u4e3a\u5408\u9002\u7684\u5f62\u72b6\n        \n        #transpose\u4f1a\u66f4\u6539\u591a\u7ef4\u6570\u7ec4\u7684\u8f74\u7684\u987a\u5e8f\uff0c\u5c06\u8f93\u51fa\u6570\u636e\u5f62\u72b6\u7531(N,H,W,C)\u8f6c\u53d8\u4e3a(N,C,H,W)\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) \n\n        # \u66f4\u65b0backward\u8fc7\u7a0b\u9700\u8981\u7528\u5230\u7684\u4e2d\u95f4\u6570\u636e\n        self.x = x\n        self.col = col\n        self.col_W = col_W\n\n        return out\n\n    def backward(self, dout):\n        FN, C, FH, FW = self.W.shape\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n\n        self.db = np.sum(dout, axis=0)\n        self.dW = np.dot(self.col.T, dout)\n        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n\n        return dx\n","b8b7c7ca":"class Pooling:\n    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n        self.pool_h = pool_h\n        self.pool_w = pool_w\n        self.stride = stride\n        self.pad = pad\n\n        # \u5b58\u50a8backward\u9700\u7528\u5230\u7684\u4e2d\u95f4\u6570\u636e\n        self.x = None\n        self.arg_max = None\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n        ### \u8bf7\u8865\u5145\u4ee3\u7801\u8ba1\u7b97\u6c60\u5316\u5c42\u8f93\u51fa\u6570\u636e\u7684\u9ad8out_h\u548c\u5bbdout_w\n        out_h = int(1 + (H - self.pool_h) \/ self.stride)\n        out_w = int(1 + (W - self.pool_w) \/ self.stride)\n\n        ### \u8bf7\u8865\u5145\u4ee3\u7801\u5c06\u8f93\u5165\u6570\u636e\u5c55\u5f00\n        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n        col = col.reshape(-1, self.pool_h * self.pool_w)\n        \n        arg_max = np.argmax(col, axis=1)\n        ### \u8bf7\u901a\u8fc7\u8ba1\u7b97col\u77e9\u9635\u6bcf\u4e00\u884c\u6700\u5927\u503c\u6765\u5b9e\u73b0\u6c60\u5316\u64cd\u4f5c\n        out = np.max(col, axis=1)\n\n        # \u901a\u8fc7reshape\u65b9\u6cd5\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u5408\u9002\u7684\u5f62\u72b6\n        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n\n        # \u4fdd\u5b58backward\u8fc7\u7a0b\u4e2d\u9700\u8981\u7528\u5230\u7684\u4e2d\u95f4\u6570\u636e\n        self.x = x\n        self.arg_max = arg_max\n\n        return out\n\n    #\u53cd\u5411\u4f20\u64ad\n    def backward(self, dout):\n        dout = dout.transpose(0, 2, 3, 1)\n\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))\n\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n\n        return dx\n","22540d5e":"class Linear:\n    def __init__(self, W, b):\n        self.W = W # \u6743\u91cd\u53c2\u6570\n        self.b = b # \u504f\u7f6e\u53c2\u6570\n        self.x = None # \u7528\u4e8e\u4fdd\u5b58\u8f93\u5165\u6570\u636e\n        # \u5b9a\u4e49\u6210\u5458\u53d8\u91cf\u7528\u4e8e\u4fdd\u5b58\u6743\u91cd\u548c\u504f\u7f6e\u53c2\u6570\u7684\u68af\u5ea6\n        self.dW = None\n        self.db = None\n\n    #\u5168\u8fde\u63a5\u5c42\u7684\u524d\u5411\u4f20\u64ad\n    def forward(self, x):\n        # \u5bf9\u5e94\u5f20\u91cf\n        self.original_x_shape = x.shape\n        x = x.reshape(x.shape[0], -1)\n        self.x = x\n\n#         print(self.x.shape, self.W.shape, self.b.shape)\n        out = np.dot(self.x, self.W) + self.b\n\n        return out\n\n    #\u5168\u8fde\u63a5\u5c42\u7684\u53cd\u5411\u4f20\u64ad\n    def backward(self, dout):\n        ###\u8bf7\u540c\u5b66\u8865\u5145\u4ee3\u7801\u5b8c\u6210\u6c42\u53d6dx,dw,db\uff0cdw,db\u4fdd\u5b58\u5230\u6210\u5458\u53d8\u91cfself.dW,self.db\u4e2d\n        dx = np.dot(dout,self.W.T)\n        self.dW = np.dot(self.x.T,dout)\n        self.db = np.sum(dout,axis=0)\n        dx = dx.reshape(self.original_x_shape)  # \u8fd8\u539f\u8f93\u5165\u6570\u636e\u7684\u5f62\u72b6\uff08\u5bf9\u5e94\u5f20\u91cf\uff09\n        return dx","06972c2c":"class Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n    #\u83b7\u53d6x\u6570\u7ec4\u4e2d\u5c0f\u4e8e0\u7684\u5143\u7d20\u7684\u7d22\u5f15\n        self.mask = (x <= 0)\n        out = x.copy()    #out\u53d8\u91cf\u8868\u793a\u8981\u6b63\u5411\u4f20\u64ad\u7ed9\u4e0b\u4e00\u5c42\u7684\u6570\u636e\uff0c\u5373\u4e0a\u56fe\u4e2d\u7684y\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u5c06x\u6570\u7ec4\u4e2d\u5c0f\u4e8e0\u7684\u5143\u7d20\u8d4b\u503c\u4e3a0\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dout[self.mask] = 0\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u5b8c\u6210Relu\u5c42\u7684\u53cd\u5411\u4f20\u64ad\n        dx = dout\n        return dx","1072862a":"class SoftmaxWithLoss:    \n    def __init__(self):\n        self.loss = None\n        self.y = None  # softmax\u7684\u8f93\u51fa\n        self.t = None  # \u76d1\u7763\u6570\u636e\n\n    #SoftmaxWithLoss\u5c42\u7684\u524d\u5411\u4f20\u64ad\u51fd\u6570\n    def forward(self, x, t):\n#         print(x.shape, t.shape)\n        self.t = t\n        self.y = softmax(x)### \u8bf7\u8865\u5145\u4ee3\u7801\u83b7\u53d6\u9884\u6d4b\u503c\n        self.loss = cross_entropy_error(self.y, self.t)### \u8bf7\u8865\u5145\u4ee3\u7801\u83b7\u53d6\u6a21\u578b\u635f\u5931\n\n        return self.loss\n\n    #SoftmaxWithLoss\u5c42\u7684\u53cd\u5411\u4f20\u64ad\u51fd\u6570\n    def backward(self, dout=1):\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u5b8c\u6210\u6c42\u53d6SoftmaxWithLoss\u5c42\u7684\u53cd\u5411\u4f20\u64ad\u7684\u8f93\u51fa\n        #\u6ce8\u610f\uff1a\u53cd\u5411\u4f20\u64ad\u65f6\u5c06\u8981\u4f20\u64ad\u7684\u503c\u9664\u4ee5\u6279\u7684\u5927\u5c0f\uff0c\u4f20\u9012\u7ed9\u524d\u9762\u5c42\u7684\u662f\u5355\u4e2a\u6570\u636e\u7684\u8bef\u5dee\n#         batch_size = self.t.shape[0]\n#         dx = (self.y - self.t) \/ batch_size\n#         return dx\n        batch_size = self.t.shape[0]\n        if self.t.size == self.y.size: # \u76d1\u7763\u6570\u636e\u662fone-hot-vector\u7684\u60c5\u51b5\n            dx = (self.y - self.t) \/ batch_size\n        else:\n            dx = self.y.copy()\n            dx[np.arange(batch_size), self.t] -= 1\n            dx = dx \/ batch_size\n        \n        return dx","4aa78740":"class Adam:\n\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n        \n    def step(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n        \n        self.iter += 1\n        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) \/ (1.0 - self.beta1**self.iter)         \n        \n        for key in params.keys():\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n            \n            params[key] -= lr_t * self.m[key] \/ (np.sqrt(self.v[key]) + 1e-7)","c5e86f87":"class SimpleConvNet:\n    \"\"\"\u7b80\u5355\u7684ConvNet\n    conv - relu - pool - linear - relu - linear - softmax\n    Parameters\n    ----------\n    input_size : \u8f93\u5165\u5927\u5c0f\uff08MNIST\u7684\u60c5\u51b5\u4e0b\u4e3a784\uff09\n    conv_param : \u4fdd\u5b58\u5377\u79ef\u5c42\u7684\u8d85\u53c2\u6570\uff08\u5b57\u5178\uff09\n    hidden_size_list : \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u6570\u91cf\u7684\u5217\u8868\uff08e.g. [100, 100, 100]\uff09\n    output_size : \u8f93\u51fa\u5927\u5c0f\uff08MNIST\u7684\u60c5\u51b5\u4e0b\u4e3a10\uff09\n    activation : 'relu' or 'sigmoid'\n    weight_init_std : \u6307\u5b9a\u6743\u91cd\u7684\u6807\u51c6\u5dee\uff08e.g. 0.01\uff09\n        \u6307\u5b9a'relu'\u6216'he'\u7684\u60c5\u51b5\u4e0b\u8bbe\u5b9a\u201cHe\u7684\u521d\u59cb\u503c\u201d\n        \u6307\u5b9a'sigmoid'\u6216'xavier'\u7684\u60c5\u51b5\u4e0b\u8bbe\u5b9a\u201cXavier\u7684\u521d\u59cb\u503c\u201d\n    \"\"\"\n\n    def __init__(self, input_dim=(1, 28, 28), \n                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n                 hidden_size=100, output_size=10, weight_init_std=0.01):\n        \"\"\"\n        \u8fd9\u91cc\u5c06\u7531\u521d\u59cb\u5316\u53c2\u6570\u4f20\u5165\u7684\u5377\u79ef\u5c42\u7684\u8d85\u53c2\u6570\u4ece\u5b57\u5178\u4e2d\u53d6\u4e86\u51fa\u6765\uff08\u4ee5\u65b9\u4fbf\u540e\u9762\u4f7f\u7528\uff09\uff0c\u7136\u540e\uff0c\u8ba1\u7b97\n        \u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\u7684\u8f93\u51fa\u5927\u5c0f\u3002\n        \"\"\"\n        filter_num = conv_param['filter_num'] # conv_param\u2015\u5377\u79ef\u5c42\u7684\u8d85\u53c2\u6570\uff08\u5b57\u5178\uff09\u3002\n        filter_size = conv_param['filter_size'] # \u5377\u79ef\u6838\u7684\u5927\u5c0f\n        filter_pad = conv_param['pad'] # \u6b65\u5e45\n        filter_stride = conv_param['stride'] # \u586b\u5145\n        input_size = input_dim[1]\n        conv_output_size = (input_size - filter_size + 2*filter_pad) \/ filter_stride + 1\n        pool_output_size = int(filter_num * (conv_output_size\/2) * (conv_output_size\/2))\n\n        # \u521d\u59cb\u5316\u6743\u91cd\n        \"\"\"\n        \u5b66\u4e60\u6240\u9700\u7684\u53c2\u6570\u662f\u7b2c1\u5c42\u7684\u5377\u79ef\u5c42\u548c\u5269\u4f59\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u6743\u91cd\u548c\u504f\u7f6e\u3002\u5c06\u8fd9\u4e9b\u53c2\u6570\u4fdd\u5b58\u5728\u5b9e\u4f8b\u53d8\n        \u91cf\u7684params\u5b57\u5178\u4e2d\u3002\u5c06\u7b2c1\u5c42\u7684\u5377\u79ef\u5c42\u7684\u6743\u91cd\u8bbe\u4e3a\u53d8\u5206\u522b\u7528\u5173\u952e\u5b57W2\u3001b2\u548c\u5173\u952e\u5b57W3\u3001b3\n        \u6765\u4fdd\u5b58\u7b2c2\u4e2a\u548c\u7b2c3\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u6743\u91cd\u548c\u504f\u7f6e\u3002\n        \"\"\"\n        self.params = {}\n        self.params['W1'] = weight_init_std * \\\n                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n        self.params['b1'] = np.zeros(filter_num)\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u5b8c\u6210\u4e24\u4e2aLinear\u5c42\u7684\u53c2\u6570\u7684\u521d\u59cb\u5316\n        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n        self.params['b2'] = np.zeros(hidden_size)\n        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b3'] = np.zeros(output_size)\n\n        # \u751f\u6210\u5c42\n        self.layers = OrderedDict()\n\n        ###\u8bf7\u6309\u7167\u4e0a\u9762\u7ed9\u51fa\u7684\u7f51\u7edc\u6784\u6210\u8865\u5145\u4ee3\u7801\u5411\u6709\u5e8f\u5b57\u5178\uff08OrderedDict\uff09\u7684layers\u4e2d\u6dfb\u52a0\u5c42\n        ###\u7f51\u7edc\u5c42\u53ef\u4f9d\u6b21\u547d\u540d\u4e3a'Conv1'\u3001'Relu1'\u3001'Pool1'\u3001'Linear1'\u3001'Relu2'\u3001'Linear2'\n        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n                                           conv_param['stride'], conv_param['pad'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n        self.layers['Linear1'] = Linear(self.params['W2'], self.params['b2'])\n        self.layers['Relu2'] = Relu()\n        self.layers['Linear2'] = Linear(self.params['W3'], self.params['b3'])\n\n        self.last_layer = SoftmaxWithLoss()\n\n        \"\"\"\n        \u53c2\u6570x\u662f\u8f93\u5165\u6570\u636e\uff0ct\u662f\u6559\u5e08\u6807\u7b7e\u3002\u7528\u4e8e\u63a8\u7406\u7684predict\u65b9\u6cd5\u4ece\u5934\u5f00\u59cb\u4f9d\u6b21\u8c03\u7528\u5df2\u6dfb\u52a0\u7684\u5c42\uff0c\u5e76\u5c06\n        \u7ed3\u679c\u4f20\u9012\u7ed9\u4e0b\u4e00\u5c42\u3002\u5728\u6c42\u635f\u5931\u51fd\u6570\u7684loss\u65b9\u6cd5\u4e2d\uff0c\u9664\u4e86\u4f7f\u7528 forward\u65b9\u6cd5\u8fdb\u884c\u7684\u524d\u5411\u4f20\u64ad\u5904\u7406\u4e4b\n        \u5916\uff0c\u8fd8\u4f1a\u7ee7\u7eed\u8fdb\u884cforward\u5904\u7406\uff0c\u76f4\u5230\u5230\u8fbe\u6700\u540e\u7684SoftmaxWithLoss\u5c42\u3002\n        \"\"\"\n    def forward(self, x):\n        ### \u8bf7\u8865\u5145\u4ee3\u7801\u6c42\u7f51\u7edc\u524d\u5411\u4f20\u64ad\u7684\u8f93\u51fa\uff08\u6ce8\u610f\uff1a\u8fd9\u91cc\u65e0\u9700\u8003\u8651softmax\u5c42\uff09\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x, t):\n        \"\"\"\u6c42\u635f\u5931\u51fd\u6570\n        \u53c2\u6570x\u662f\u8f93\u5165\u6570\u636e\u3001t\u662f\u6570\u636e\u6807\u7b7e\n        \"\"\"\n        ### \u8bf7\u8865\u5145\u4ee3\u7801\u8ba1\u7b97\u5e76\u8fd4\u56de\u635f\u5931\u51fd\u6570\u503c\n        y = self.forward(x)\n        return self.last_layer.forward(y, t)\n\n    def accuracy(self, x, t, batch_size=100):\n        if t.ndim != 1 : t = np.argmax(t, axis=1)\n        \n        acc = 0.0\n        \n        for i in range(int(x.shape[0] \/ batch_size)):\n            tx = x[i*batch_size:(i+1)*batch_size]\n            tt = t[i*batch_size:(i+1)*batch_size]\n            y = self.forward(tx)\n            y = np.argmax(y, axis=1)\n            ### \u8bf7\u8865\u5145\u4ee3\u7801\u8ba1\u7b97\u8bad\u7ec3\u7cbe\u5ea6\n            acc += sum(y == tt)\n        \n        return acc \/ x.shape[0]\n\n        \"\"\"\n        \u53c2\u6570\u7684\u68af\u5ea6\u901a\u8fc7\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u6cd5\uff08\u53cd\u5411\u4f20\u64ad\uff09\u6c42\u51fa\uff0c\u901a\u8fc7\u628a\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u7ec4\u88c5\u5728\u4e00\u8d77\u6765\u5b8c\n        \u6210\u3002\u56e0\u4e3a\u5df2\u7ecf\u5728\u5404\u5c42\u6b63\u786e\u5b9e\u73b0\u4e86\u6b63\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u7684\u529f\u80fd\uff0c\u6240\u4ee5\u8fd9\u91cc\u53ea\u9700\u8981\u4ee5\u5408\u9002\u7684\u987a\u5e8f\u8c03\u7528\n        \u5373\u53ef\u3002\u6700\u540e\uff0c\u628a\u5404\u4e2a\u6743\u91cd\u53c2\u6570\u7684\u68af\u5ea6\u4fdd\u5b58\u5230grads\u5b57\u5178\u4e2d\u3002\n        \"\"\"\n    def backward(self, x, t):\n        #\u8fd0\u7528\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u6cd5\u6c42\u53d6\u68af\u5ea6\n        ### \u8bf7\u53c2\u8003\u5b9e\u9a8c6\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5b9e\u73b0\u7f51\u7edc\u8bad\u7ec3\u7684\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\n        self.loss(x, t)\n        dout = 1\n        dout = self.last_layer.backward(dout)\n\n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n        \n        # \u5c06\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u51fa\u7684\u6743\u91cd\u53c2\u6570\u68af\u5ea6\u4fdd\u5b58\u5230grads\u5b57\u5178\u4e2d\n        grads = {}       \n        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n        grads['W2'], grads['b2'] = self.layers['Linear1'].dW, self.layers['Linear1'].db\n        grads['W3'], grads['b3'] = self.layers['Linear2'].dW, self.layers['Linear2'].db\n\n        return grads\n","8e874094":"# \u8bfb\u5165\u6570\u636e\n(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n\n# \u5904\u7406\u82b1\u8d39\u65f6\u95f4\u8f83\u957f\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u6570\u636e\nx_train, t_train = x_train[:5000], t_train[:5000]\nx_test, t_test = x_test[:1000], t_test[:1000]\n\nclass Trainer:\n    \"\"\"\u8fdb\u884c\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u7684\u7c7b\n    \"\"\"\n\n    def __init__(self, network, x_train, t_train, x_test, t_test,\n                 epochs=20, mini_batch_size=100,lr = 0.001,\n                 evaluate_sample_num_per_epoch=None, verbose=True):\n        self.network = network\n        self.verbose = verbose\n        self.x_train = x_train\n        self.t_train = t_train\n        self.x_test = x_test\n        self.t_test = t_test\n        self.epochs = epochs\n        self.batch_size = mini_batch_size\n        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n\n        ### \u8bf7\u8865\u5145\u4ee3\u7801\u4f7f\u7528Adam\u8fdb\u884c\u4f18\u5316\n        self.optimizer = Adam()\n        #\u6ce8\u610f\uff1aAdam\u7c7b\u5df2\u5728\u4e0a\u4e00\u6b21\u5b9e\u9a8c\u5b66\u4e60\u8fc7\uff0c\u8bf7\u540c\u5b66\u4eec\u81ea\u5df1\u5b9e\u73b0\n\n        self.train_size = x_train.shape[0]\n        self.iter_per_epoch = max(self.train_size \/ mini_batch_size, 1)\n        self.max_iter = int(epochs * self.iter_per_epoch)\n        self.current_iter = 0\n        self.current_epoch = 0\n\n        self.train_loss_list = []\n        self.train_acc_list = []\n        self.test_acc_list = []\n\n    def train_step(self):\n        batch_mask = np.random.choice(self.train_size, self.batch_size)\n        x_batch = self.x_train[batch_mask]\n        t_batch = self.t_train[batch_mask]\n\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u8c03\u7528\u7f51\u7edc\u7684backward\u51fd\u6570\u83b7\u53d6\u68af\u5ea6\n        grads = self.network.backward(x_batch, t_batch)\n\n        ###\u8bf7\u8865\u5145\u4ee3\u7801\u4f7f\u7528\u4f18\u5316\u5668\u66f4\u65b0\u53c2\u6570\n        self.optimizer.step(self.network.params, grads)\n\n        loss = self.network.loss(x_batch, t_batch)\n        self.train_loss_list.append(loss)\n        if self.verbose: print(\"train loss:\" + str(loss))\n\n        if self.current_iter % self.iter_per_epoch == 0:\n            self.current_epoch += 1\n\n            x_train_sample, t_train_sample = self.x_train, self.t_train\n            x_test_sample, t_test_sample = self.x_test, self.t_test\n            if not self.evaluate_sample_num_per_epoch is None:\n                t = self.evaluate_sample_num_per_epoch\n                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n\n            ###\u8bf7\u8865\u5145\u4ee3\u7801\u8ba1\u7b97\u8bad\u7ec3\u7cbe\u5ea6train_acc\u548c\u6d4b\u8bd5\u7cbe\u5ea6test_acc\n            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n            self.train_acc_list.append(train_acc)\n            self.test_acc_list.append(test_acc)\n\n            if self.verbose: print(\n                \"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \n                                                     \", test acc:\" + str(test_acc) + \" ===\")\n        self.current_iter += 1\n\n    def train(self):\n        for i in range(self.max_iter):  #\u8bad\u7ec3\u5faa\u73af\n            self.train_step()\n\n        test_acc = self.network.accuracy(self.x_test, self.t_test)\n\n        if self.verbose:\n            print(\"=============== Final Test Accuracy ===============\")\n            print(\"test acc:\" + str(test_acc))\n\n\nmax_epochs = 20\n\n### \u8bf7\u8865\u5145\u4ee3\u7801\u5c06\u4e4b\u524d\u5b9a\u4e49\u7684SimpleConvNet\u7f51\u7edc\u5b9e\u4f8b\u5316\nnetwork = SimpleConvNet()\n\ntrainer = Trainer(network, x_train, t_train, x_test, t_test,\n                  epochs=max_epochs, mini_batch_size=100,lr = 0.001,\n                  evaluate_sample_num_per_epoch=1000)\ntrainer.train()\n","f823751d":"# Loss curve\nplt.plot(trainer.train_loss_list)\nplt.title('Loss')\nplt.legend(['train', 'test'])\nplt.savefig('loss.png')\nplt.show()\n\n# Accuracy curve\nmarkers = {'train': 'o', 'test': 's'}\nplt.plot(trainer.train_acc_list, marker='o')\nplt.plot(trainer.test_acc_list, marker='s')\nplt.title('Accuracy')\nplt.legend(['train', 'test'])\nplt.savefig('acc.png')\nplt.show()\n","1b55e42b":"## Layers\n- Convolution\n- Pooling\n- Linear\n- Relu\n- SoftmaxWithLoss","bdfb5f00":"## Useful Functions\n- im2col\n- col2im\n- softmax\n- cross_entropy_error","d1c8f0ec":"## Load Data","e86d9ede":"## Optimizer","6128deb8":"## NetWorks\n- SimpleConvNet","80459f05":"## Plot Loss && Acc","f2240aa3":"## Dependency"}}