{"cell_type":{"52633112":"code","51e5fce3":"code","635ad19f":"code","44ba9956":"code","e655b40b":"code","18b75dde":"code","29d6cca8":"code","cbc1751b":"code","3743ad38":"code","9f79f507":"code","1e864ab9":"code","1f444575":"code","1fe0d8e3":"code","032156d9":"code","ab022a86":"code","7d15af67":"code","6b0db6f0":"code","da3c9847":"code","0a7b53e3":"code","2de10129":"code","4e4b0ac7":"code","8f52d248":"code","d26b9e8d":"code","650cf2e0":"code","69069994":"code","b9e40636":"code","f0419322":"code","305dd42c":"code","f75f76a3":"code","7ab06971":"code","87403c7a":"code","537d1c6f":"code","11b0b287":"code","c0ed7126":"code","04c8a517":"code","0e5a35af":"code","180c850a":"code","673500ad":"code","37a08e0c":"code","47beea04":"code","73193f5a":"code","e0b51493":"code","b2b34041":"code","1062a4a8":"code","572d3fc0":"code","b2b9b75b":"code","eff67447":"code","d27de2e3":"code","c1906359":"code","1c1632c8":"code","a50bbcbb":"code","4ac9c99f":"code","e7d0d004":"code","9983c7cc":"code","b0524d80":"code","59493239":"code","c101c0ed":"code","6b50d380":"code","e4cb6cef":"code","f6126997":"code","61b446f1":"code","36ae0d34":"code","aac60abf":"code","e6201e86":"code","46c01bcd":"code","2fdc03a9":"code","b5169442":"code","0669ff9d":"code","dc6ef44b":"code","029fbff0":"markdown","094bfbfc":"markdown","42dc8f45":"markdown","3f80a911":"markdown","b7f731bc":"markdown","dbc699c2":"markdown","c0d8a78e":"markdown","bc6de660":"markdown","c398c3b6":"markdown","67833fab":"markdown","979c8733":"markdown","4085193d":"markdown","a222750d":"markdown","b4e156e8":"markdown","e4011931":"markdown","3b3de64f":"markdown","346920f1":"markdown","57ba2505":"markdown","e7136d2b":"markdown","0a42b268":"markdown","33155505":"markdown","5bcd8eb6":"markdown","fe09f793":"markdown","a271fdf1":"markdown","fd650cf2":"markdown","a950e3e2":"markdown","7cf3d859":"markdown"},"source":{"52633112":"import copy\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# WordCloud\nfrom wordcloud import WordCloud\n\n\n# preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# models\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","51e5fce3":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# Setting up visualisations\nsns.set_style(style='white') \nsns.set(rc={\n    'figure.figsize':(12,7), \n    'axes.facecolor': 'white',\n    'axes.grid': True, 'grid.color': '.9',\n    'axes.linewidth': 1.0,\n    'grid.linestyle': u'-'},font_scale=1.5)\ncustom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\nsns.set_palette(custom_colors)","635ad19f":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId') # Train data\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId') # Test data\ntd = pd.concat([copy.deepcopy(traindf), copy.deepcopy(testdf)], axis=0, sort  = False) # Train & Test data\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # Form for answers","44ba9956":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n# We check the train sample for balance\ntraindf['Survived'].value_counts(normalize=True)","e655b40b":"td.nunique()","18b75dde":"td.describe()","29d6cca8":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ntd = pd.concat([traindf, testdf], axis=0, sort=False)\ntd['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntd['Title'] = td.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntd['IsWomanOrBoy'] = ((td.Title == 'Master') | (td.Sex == 'female'))\ntd['LastName'] = td.Name.str.split(',').str[0]\nfamily = td.groupby(td.LastName).Survived\ntd['WomanOrBoyCount'] = family.transform(lambda s: s[td.IsWomanOrBoy].fillna(0).count())\ntd['WomanOrBoyCount'] = td.mask(td.IsWomanOrBoy, td.WomanOrBoyCount - 1, axis=0)\ntd['FamilySurvivedCount'] = family.transform(lambda s: s[td.IsWomanOrBoy].fillna(0).sum())\ntd['FamilySurvivedCount'] = td.mask(td.IsWomanOrBoy, td.FamilySurvivedCount - \\\n                                    td.Survived.fillna(0), axis=0)\ntd['WomanOrBoySurvived'] = td.FamilySurvivedCount \/ td.WomanOrBoyCount.replace(0, np.nan)\ntd.WomanOrBoyCount = td.WomanOrBoyCount.replace(np.nan, 0)\ntd['Alone'] = (td.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ntd['Title'] = td['Title'].replace('Ms','Miss')\ntd['Title'] = td['Title'].replace('Mlle','Miss')\ntd['Title'] = td['Title'].replace('Mme','Mrs')\n# Embarked\ntd['Embarked'] = td['Embarked'].fillna('S')\n# Cabin, Deck\ntd['Deck'] = td['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntd.loc[(td['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = td.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ntd['Fare'] = td['Fare'].fillna(med_fare)\n#Age\ntd['Age'] = td.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ntd['Family_Size'] = td['SibSp'] + td['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ntd = td.drop(cols_to_drop, axis=1)\n\ntd.WomanOrBoySurvived = td.WomanOrBoySurvived.fillna(0)\ntd.WomanOrBoyCount = td.WomanOrBoyCount.fillna(0)\ntd.FamilySurvivedCount = td.FamilySurvivedCount.fillna(0)\ntd.Alone = td.Alone.fillna(0)","cbc1751b":"target = td.Survived.loc[traindf.index]\ntd = td.drop(['Survived'], axis=1)\ntrain, test = td.loc[traindf.index], td.loc[testdf.index]","3743ad38":"# Thanks to: https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(td.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(td.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='gray',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);","9f79f507":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntraindf['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=traindf,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","1e864ab9":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntraindf['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=traindf,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","1f444575":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(1,2,figsize=(20,10))\ntraindf[traindf['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntraindf[traindf['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","1fe0d8e3":"# Thanks to: https:\/\/www.kaggle.com\/headsortails\/pytanic\ndummy = td[td['Title'].isin(['Mr','Miss','Mrs','Master'])]\nfoo = dummy['Age'].hist(by=dummy['Title'], bins=np.arange(0,81,1))","032156d9":"# Thanks to: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\nf,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=traindf,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=traindf,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=traindf,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=traindf,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","ab022a86":"td['Family'] = td.Parch + td.SibSp\ntd['Is_Alone'] = td.Family == 0","7d15af67":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf['Fare_Category'] = pd.cut(traindf['Fare'], bins=[0,7.90,14.45,31.28,120], labels=['Low','Mid', 'High_Mid','High'])","6b0db6f0":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\np = sns.countplot(x = \"Embarked\", hue = \"Survived\", data = traindf, palette=[\"C1\", \"C0\"])\np.set_xticklabels([\"Southampton\",\"Cherbourg\",\"Queenstown\"])\np.legend(labels = [\"Deceased\", \"Survived\"])\np.set_title(\"Training Data - Survival based on embarking point.\")","da3c9847":"# Thanks to: https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\ntraindf.Embarked.fillna(traindf.Embarked.mode()[0], inplace = True)","0a7b53e3":"traindf.info()","2de10129":"# Clone data for FE\ntrain_fe = copy.deepcopy(traindf)\ntarget_fe = train_fe['Survived']\ndel train_fe['Survived']","4e4b0ac7":"train_fe = train_fe.fillna(train_fe.isnull())","8f52d248":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train_fe.columns.values.tolist()\nfor col in features:\n    if train_fe[col].dtype in numerics: continue\n    categorical_columns.append(col)\nindexer = {}\nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    _, indexer[col] = pd.factorize(train_fe[col])\n    \nfor col in categorical_columns:\n    if train_fe[col].dtype in numerics: continue\n    train_fe[col] = indexer[col].get_indexer(train_fe[col])","d26b9e8d":"train_fe.info()","650cf2e0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nX = train_fe\nz = target_fe","69069994":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","b9e40636":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","f0419322":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","305dd42c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","f75f76a3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","7ab06971":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","87403c7a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","537d1c6f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","11b0b287":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Standardization for regression models\ntrain = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","c0ed7126":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","04c8a517":"len(coeff_logreg)","0e5a35af":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","180c850a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","673500ad":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()","37a08e0c":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","47beea04":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","73193f5a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('mean', ascending=False)","e0b51493":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","b2b34041":"#Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\nfeature_score.sort_values('total', ascending=False)","1062a4a8":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ntarget = traindf.Survived.loc[traindf.index]\ntrain, test = td.loc[traindf.index], td.loc[testdf.index]","572d3fc0":"train.head(3)","b2b9b75b":"test.head(3)","eff67447":"target[:3]","d27de2e3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","c1906359":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","1c1632c8":"train.info()","a50bbcbb":"test.info()","4ac9c99f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n#%% split training set to validation set\nSEED = 100\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)","e7d0d004":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc_random_forest = round(random_forest.score(train, target) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","9983c7cc":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nrandom_forest_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","b0524d80":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","59493239":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_xgb, best)\nparams","c101c0ed":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nXGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nacc_XGB_Classifier","6b50d380":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nxgb_submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","e4cb6cef":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\nplt.show();\nplt.close()","f6126997":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\ndef hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","61b446f1":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nparams = space_eval(space_gb, best)\nparams","36ae0d34":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nY_pred = gradient_boosting.predict(test).astype(int)\ngradient_boosting.score(train, target)\nacc_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nacc_gradient_boosting","aac60abf":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nsubmission_gradient_boosting = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})","e6201e86":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\nmodels = pd.DataFrame({\n    'Model': ['Random Forest',  'XGBClassifier', 'GradientBoostingClassifier'],\n    \n    'Score': [acc_random_forest, acc_XGB_Classifier, acc_gradient_boosting]})","46c01bcd":"models","2fdc03a9":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score'], label = 'Score')\nplt.legend()\nplt.title('Score  for 3 popular models')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","b5169442":"random_forest_submission.to_csv('submission_RandomForest.csv', index=False)","0669ff9d":"xgb_submission.to_csv('submission_XGB_Classifier.csv', index=False)","dc6ef44b":"submission_gradient_boosting.to_csv('submission_gradient_boosting.csv', index=False)","029fbff0":"# Titanic with 3 models","094bfbfc":"<a class=\"anchor\" id=\"10.3\"><\/a>\n### 10.3 GradientBoostingClassifier\n##### [Back to Table of Contents](#0.1)","42dc8f45":"<a class=\"anchor\" id=\"3\"><\/a>\n## 3. Data research\n##### [Back to Table of Contents](#0.1)","3f80a911":"## **Acknowledgements**\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n   - https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\n   - https:\/\/www.kaggle.com\/sumukhija\/tip-of-the-iceberg-eda-prediction-0-80861\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n   - https:\/\/www.kaggle.com\/headsortails\/pytanic","b7f731bc":"#### Pclass","dbc699c2":"<a class=\"anchor\" id=\"7.4\"><\/a>\n### 7.4 Linear Reagression\n##### [Back to Table of Contents](#0.1)","c0d8a78e":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE \n##### [Back to Table of Contents](#0.1)","bc6de660":"<a class=\"anchor\" id=\"7\"><\/a>\n## 7. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","c398c3b6":"<a class=\"anchor\" id=\"11\"><\/a>\n## 11. Models evaluation\n##### [Back to Table of Contents](#0.1)","67833fab":"<a class=\"anchor\" id=\"8\"><\/a>\n## 8. Comparison of the all feature importance diagrams\n##### [Back to Table of Contents](#0.1)","979c8733":"#### Survived","4085193d":"<a class=\"anchor\" id=\"10.1\"><\/a>\n### 10.1 Random Forest \n##### [Back to Table of Contents](#0.1)","a222750d":"#### Correlation Between The Features","b4e156e8":"<a class=\"anchor\" id=\"2\"><\/a>\n## 2. Download datasets \n##### [Back to Table of Contents](#0.1)","e4011931":"<a class=\"anchor\" id=\"12\"><\/a>\n## 12. Prediction & Output data\n##### [Back to Table of Contents](#0.1)","3b3de64f":"<a class=\"anchor\" id=\"9\"><\/a>\n## 9. Preparing to modeling\n##### [Back to Table of Contents](#0.1)","346920f1":"<a class=\"anchor\" id=\"7.1\"><\/a>\n### 7.1 LGBM\n##### [Back to Table of Contents](#0.1)","57ba2505":"<a class=\"anchor\" id=\"10\"><\/a>\n## 10. Tuning models\n##### [Back to Table of Contents](#0.1)","e7136d2b":"<a class=\"anchor\" id=\"9.1\"><\/a>\n### 9.1 Data for modeling \n##### [Back to Table of Contents](#0.1)","0a42b268":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. EDA & Visualization\n##### [Back to Table of Contents](#0.1)","33155505":"<a class=\"anchor\" id=\"10.2\"><\/a>\n### 10.2 XGB\n##### [Back to Table of Contents](#0.1)","5bcd8eb6":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## **Table of Contents**\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [Data research](#3)\n4. [FE](#4)\n5. [EDA & Visualization](#5)\n6. [Preparing data for building the feature importance diagrams](#6)  \n7. [FE: building the feature importance diagrams](#7)\n  -  [LGBM](#7.1)\n  -  [XGB](#7.2)\n  -  [Logistic Regression](#7.3)\n  -  [Linear Reagression](#7.4)\n8. [Comparison of the all feature importance diagrams](#8)\n9. [Preparing to modeling](#9)\n  - [Data for modeling](#9.1)\n  - [Encoding categorical features](#9.2)\n10. [Tuning models](#10)\n  -  [Random Forest](#10.1)\n  -  [XGB](#10.2)\n  -  [GradientBoostingClassifier](#10.3)\n11. [Models evaluation](#11)\n12. [Prediction & Output data](#11)","fe09f793":"<a class=\"anchor\" id=\"1\"><\/a>\n## 1. Import libraries \n##### [Back to Table of Contents](#0.1)","a271fdf1":"<a class=\"anchor\" id=\"7.3\"><\/a>\n### 7.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)","fd650cf2":"<a class=\"anchor\" id=\"7.2\"><\/a>\n### 7.2 XGB\n##### [Back to Table of Contents](#0.1)","a950e3e2":"<a class=\"anchor\" id=\"9.2\"><\/a>\n### 9.2 Encoding categorical features\n##### [Back to Table of Contents](#0.1)","7cf3d859":"<a class=\"anchor\" id=\"6\"><\/a>\n## 6. Preparing data for building the feature importance diagrams \n##### [Back to Table of Contents](#0.1)"}}