{"cell_type":{"a4a8da11":"code","dc01f6c5":"code","8b42e42b":"code","fd9e6edf":"code","a60c9213":"code","07a0b069":"code","ff34dc98":"code","5769bd45":"code","bf5221bc":"code","7df0191a":"code","9057c74d":"code","a3791266":"code","e680097f":"code","6af6788a":"code","4257fcec":"code","0389b1e1":"code","728255d0":"code","9af9710b":"code","b3667b37":"code","a700e895":"code","5f055999":"code","5fb765a6":"code","bd9b4a07":"code","df7f55c9":"code","a1aa979c":"code","64c31942":"code","ddd136ed":"code","e8089fc5":"code","525b8b9c":"code","0de130cf":"code","3f476091":"code","6c6e70ac":"code","8433b4df":"code","6feb5863":"code","2cfccd36":"code","d2797b1d":"markdown","1a2ce93a":"markdown","9e15b425":"markdown","a339bc53":"markdown","10c6ee16":"markdown"},"source":{"a4a8da11":"import numpy as np\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 100)","dc01f6c5":"data = pd.read_csv('..\/input\/smsspamdetection\/SMSSpamCollection.tsv', sep='\\t', header=None)\ndata.columns = ['label', 'body_text']\ndata.head()","8b42e42b":"data.tail()","fd9e6edf":"#Remove Punctuation\n\nimport string\n\ndef remove_punct(text):\n    text_nopunct = \"\".join(char for char in text if char not in string.punctuation)\n    return text_nopunct\n\ndata['body_text_clean'] = data['body_text'].apply(lambda x : remove_punct(x))\n\ndata.head()","a60c9213":"print(string.punctuation)","07a0b069":"#tokenize\n\nimport re\n\ndef tokenize(text):\n    token = re.split('\\W+', text)\n    return token\n\ndata['body_text_token'] = data['body_text_clean'].apply(lambda x : tokenize(x))\ndata.head()","ff34dc98":"#Remove Stopwords\n\nimport nltk\nnltk.download('stopwords')\n\nstopword = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(tokenize_list):\n    text = [word for word in tokenize_list if word not in stopword]\n    return text\n\ndata['body_text_nostop'] = data['body_text_token'].apply(lambda x : remove_stopwords(x))\ndata.head()","5769bd45":"print(stopword)\nprint(len(stopword))","bf5221bc":"#Stemming\n\nps = nltk.PorterStemmer()\n\ndef stemmer(tokenize_text):\n    text = [ps.stem(word) for word in tokenize_text]\n    return text\n\ndata['body_text_stem'] = data['body_text_nostop'].apply(lambda x : stemmer(x))\ndata.head()","7df0191a":"#lemmatize\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatize(tokenize_text):\n    text = [wn.lemmatize(word) for word in tokenize_text]\n    return text\n\ndata['body_text_lemma'] = data['body_text_nostop'].apply(lambda x : lemmatize(x))\ndata.head()","9057c74d":"# Apply CountVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nstopwords = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\n\ndf = pd.read_csv('..\/input\/smsspamdetection\/SMSSpamCollection.tsv', sep='\\t', header=None)\ndf.columns = ['label', 'body_text']\ndf.head()\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ncount_vect = CountVectorizer(analyzer=clean_text)\nX_counts = count_vect.fit_transform(df['body_text'])\nprint(X_counts.shape)\nprint(count_vect.get_feature_names())","a3791266":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df","e680097f":"X_counts_df.columns = count_vect.get_feature_names()\nX_counts_df","6af6788a":"def clean_txt(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n    return text\n\ndf['clean_text'] = df['body_text'].apply(lambda x : clean_txt(x))\ndf.head()","4257fcec":"from sklearn.feature_extraction.text import CountVectorizer\n\nngram_vect = CountVectorizer(ngram_range=(2,2))\nX_counts = ngram_vect.fit_transform(df['clean_text'])\nprint(X_counts.shape)\nprint(ngram_vect.get_feature_names())","0389b1e1":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.columns = ngram_vect.get_feature_names()\nX_counts_df","728255d0":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(df['body_text'])\nprint(X_tfidf.shape)\nprint(tfidf_vect.get_feature_names())","9af9710b":"X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\nX_tfidf_df.columns = tfidf_vect.get_feature_names()\nX_tfidf_df","b3667b37":"data = pd.read_csv('..\/input\/smsspamdetection\/SMSSpamCollection.tsv', sep='\\t', header=None)\ndata.columns = ['label', 'body_text']\ndata.head()","a700e895":"#Create feature for text message length\ndata['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n\ndata.head()","5f055999":"#Create feature for % of text that is punctuation\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndata['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n\ndata.head()","5fb765a6":"#Evaluate created features\n\nfrom matplotlib import pyplot\n%matplotlib inline","bd9b4a07":"bins = np.linspace(0,200,40)\n\npyplot.hist(data[data['label']=='spam']['body_len'], bins, alpha=0.5, label='spam')\npyplot.hist(data[data['label']=='ham']['body_len'], bins, alpha=0.5, label='ham')\npyplot.legend(loc='upper left')\npyplot.show()","df7f55c9":"bins = np.linspace(0, 50, 40)\n\npyplot.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, label='spam')\npyplot.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, label='ham')\npyplot.legend(loc='upper right')\npyplot.show()","a1aa979c":"bins = np.linspace(0,200,40)\n\npyplot.hist(data['body_len'], bins)\npyplot.title(\"Body Length Distribution\")\npyplot.show()","64c31942":"bins = np.linspace(0,50,40)\n\npyplot.hist(data['punct%'], bins)\npyplot.title(\"Punctuation % Distribution\")\npyplot.show()","ddd136ed":"data = pd.read_csv('..\/input\/smsspamdetection\/SMSSpamCollection.tsv', sep='\\t', header=None)\ndata.columns = ['label', 'body_text']\ndata.head()","e8089fc5":"data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\ndata['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\ndata.head()","525b8b9c":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n","0de130cf":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)","3f476091":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","6c6e70ac":"tfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","8433b4df":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport time","6feb5863":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nstart = time.time()\nrf_model = rf.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n\nstart = time.time()\ny_pred = rf_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","2cfccd36":"gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n\nstart = time.time()\ngb_model = gb.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n\nstart = time.time()\ny_pred = gb_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","d2797b1d":"### TF-IDF\n\nCreates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n","1a2ce93a":"### N-Grams \n\nCreates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n\n\"NLP is an interesting topic\"\n\n| n | Name      | Tokens                                                         |\n|---|-----------|----------------------------------------------------------------|\n| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |","9e15b425":"### Count vectorization\nCreates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document.","a339bc53":"### Model building","10c6ee16":"## Feature Engineering: Feature Creation"}}