{"cell_type":{"085beac1":"code","8c1b373b":"code","81240a03":"code","b5ca80b9":"code","72ab913f":"code","24985ed2":"code","5b5522c8":"code","8a33d89b":"code","fe8493d8":"code","97a73788":"code","950a8994":"code","e1628b72":"code","aac00446":"code","be70dfaa":"code","39f9f523":"code","54862441":"code","4ddbcae5":"code","04c9a728":"code","1c2b9010":"code","7ce1b6c1":"code","c25c28d4":"code","3891affb":"code","906b06d3":"code","4160ae2d":"code","b10a3fdd":"code","57e0a914":"code","9006982a":"code","f380ceef":"code","b56d5157":"code","7ee5f40e":"code","75e24a77":"markdown","bca0f006":"markdown","db7a3f44":"markdown","af16d751":"markdown","281af94a":"markdown","9294505f":"markdown","d9cfff79":"markdown","df62953d":"markdown","711e17b7":"markdown","cf555c51":"markdown","5e5e02a8":"markdown","41764cb7":"markdown","9e2cb517":"markdown","86f923f9":"markdown","ad2aa05e":"markdown","676af22a":"markdown","baee3bd8":"markdown","7f1e7802":"markdown","f3497d3b":"markdown","2d8df0bc":"markdown","d25aef3d":"markdown","ecb4c568":"markdown","c2414407":"markdown","02085f0c":"markdown","eb48971b":"markdown","0db96033":"markdown","cab19e5f":"markdown","2d7ce820":"markdown","7bc3fbaa":"markdown","f0d229c3":"markdown","de64afe1":"markdown","533dd21b":"markdown","8aaf9fa5":"markdown","1e2a07f0":"markdown","867363f1":"markdown","70a2c99f":"markdown","9c904d10":"markdown"},"source":{"085beac1":"import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style = \"darkgrid\")\n\nimport gc\n\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout, MaxPool2D, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import load_model\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport cv2","8c1b373b":"df_num = pd.read_csv(\"..\/input\/train-digit-recognition-mnist\/mnist_train.csv\")\ndf_alph = pd.read_csv(\"..\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data\/A_Z Handwritten Data.csv\")\ndf_alph[\"0\"] += 10","81240a03":"df_num.shape, df_alph.shape","b5ca80b9":"df_num.head(3)","72ab913f":"df_alph.head(3)","24985ed2":"pixel_array = [\"Label\"]\nfor i in range(1, 785):\n    pixel_array.append(f\"pixel_{i}\")\ndf_num.columns = pixel_array\ndf_alph.columns = pixel_array\ndel pixel_array\ngc.collect()","5b5522c8":"df = pd.concat([df_num, df_alph], axis = 0)\ndf.shape","8a33d89b":"SEED = 42\nnp.random.seed(SEED)","fe8493d8":"plt.rcParams[\"figure.figsize\"] = [10, 8]\ndf.Label.value_counts().plot(kind = \"bar\")\nplt.title(\"Target value distribution\")","97a73788":"a, b = df.Label[df.Label == 24].value_counts().sum(), df.Label[df.Label == 18].value_counts().sum()\nprint(f\"Maximum and Minimum frequency for any target value in the data: {a, b}\")","950a8994":"X = df.drop([\"Label\"], axis = 1)\ny = df[\"Label\"]\nX.shape","e1628b72":"X_reshaped = X.values.astype(\"float32\").reshape(X.shape[0], 28, 28)\ny_int = y.values.astype(\"int32\")\nprint(X.shape, \"***\", X_reshaped.shape)","aac00446":"X_reshaped = X_reshaped.reshape(-1, 28, 28, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_int, test_size = 0.3, stratify = y)\n\ndata_to_predict = X_test.reshape(-1, 28, 28)","be70dfaa":"def plot_grid(pred = False):\n    fig=plt.figure(figsize=(8, 8))\n    columns = 3\n    rows = 3\n    for i in range(1, columns*rows +1):\n        index = np.random.randint(data_to_predict.shape[0])\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(data_to_predict[index], cmap = plt.get_cmap(\"gray\"))\n        plt.xticks([])\n        plt.yticks([])\n        plt.xlabel(f\"Label: {y_test[index]}\")\n    plt.tight_layout()\n    plt.show()","39f9f523":"plot_grid()","54862441":"X_train_mean = X_train.mean().astype(np.float32)\nX_train_std = X_train.std().astype(np.float32)\nX_test_mean = X_test.mean().astype(np.float32)\nX_test_std = X_test.std().astype(np.float32)\n\nX_train = (X_train - X_train_mean)\/X_train_std\nX_test = (X_test - X_test_mean)\/X_test_std\n\ny_train = to_categorical(y_train, num_classes = 36)\ny_test = to_categorical(y_test, num_classes = 36)","4ddbcae5":"del X_train_mean, X_train_std, df_num, df_alph, X_reshaped, y_int, X, y, X_test_mean, X_test_std, data_to_predict, a, b\ngc.collect()","04c9a728":"# create CNN model for layers\ninput_shape = (28, 28, 1)\nnum_classes = 36\n\nmodel = Sequential()\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = \"relu\", padding = \"Same\", input_shape = input_shape))\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = \"relu\", padding = \"Same\"))\nmodel.add(MaxPool2D(pool_size = (3, 3)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, kernel_size = (3, 3), activation = \"relu\", padding = \"Same\"))\nmodel.add(Conv2D(128, kernel_size = (3, 3), activation = \"relu\", padding = \"Same\"))\nmodel.add(MaxPool2D(pool_size = (3, 3)))\nmodel.add(Dropout(0.40))\n\nmodel.add(Flatten())\nmodel.add(Dense(150, activation = \"relu\"))\nmodel.add(Dropout(0.30))\nmodel.add(Dense(36, activation = \"softmax\"))\nmodel.summary()","1c2b9010":"optimizer = Adam(lr = .0005, beta_1 = .9, beta_2 = .999, epsilon = 1e-07, decay = 0, amsgrad = False)","7ce1b6c1":"# Compile the model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\", tf.keras.metrics.AUC()])\n\n# learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3, verbose = 1, factor = .5, min_lr = .00001)\n\n# EarlyStopping\nes = EarlyStopping(monitor='val_categorical_accuracy', patience = 4)","c25c28d4":"epochs = 80\nbatch_size = 128","3891affb":"# Data Augmentation\ndatagen = ImageDataGenerator(featurewise_center = False, samplewise_center = False, \n                            featurewise_std_normalization = False, samplewise_std_normalization = False,\n                            zca_whitening = False, rotation_range = 10, zoom_range = .1, \n                            width_shift_range = .1, height_shift_range = .1, horizontal_flip = True, \n                            vertical_flip = False)\ntrain_batches = datagen.flow(X_train, y_train, batch_size = batch_size)\nval_batches = datagen.flow(X_test, y_test, batch_size = batch_size)","906b06d3":"# Fitting the model\nhistory = model.fit_generator(generator = train_batches, steps_per_epoch = train_batches.n\/\/batch_size, epochs=epochs, \n                    validation_data = val_batches, validation_steps = val_batches.n\/\/batch_size, verbose = 0,\n                    callbacks = [learning_rate_reduction, es])","4160ae2d":"model.save(\"model_0-10_a-z.h5\")","b10a3fdd":"print(f\"Total number of epochs for which the model trained: {len(history.history['loss'])}\")","57e0a914":"plt.rcParams['figure.figsize'] = [10, 8]\nplt.plot(history.history['categorical_accuracy'], \"b--\")\nplt.plot(history.history['val_categorical_accuracy'], \"r-\")\nplt.title(\"Training vs Validation accuracy\")\nplt.legend([\"Training\", \"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")","9006982a":"print(f\"Maximum Training Accuracy: {max(history.history['categorical_accuracy'])}, Maximum Validation Accuracy: {max(history.history['val_categorical_accuracy'])}\")","f380ceef":"plt.rcParams['figure.figsize'] = [10, 8]\nplt.plot(history.history['auc'], \"b--\")\nplt.plot(history.history['val_auc'], \"r-\")\nplt.title(\"Training vs Validation AUC score\")\nplt.legend([\"Training\", \"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"AUC score\")","b56d5157":"print(f\"Maximum Training AUC: {max(history.history['auc'])}, Maximum Validation AUC: {max(history.history['val_auc'])}\")","7ee5f40e":"plt.rcParams['figure.figsize'] = [10, 8]\nplt.plot(history.history['loss'], \"b--\")\nplt.plot(history.history['val_loss'], \"r-\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend([\"Training\", \"Validation\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss value\")","75e24a77":"## Analysing trends in Accuracy and AUC","bca0f006":"## Training + Cross Validation","db7a3f44":"> Now let's divide the data into features and target.","af16d751":"> Why not just plot the training and validation accuracy against each other, it will help us gain a lot of information about the training, like where our model stops learning, starts overfitting etc.","281af94a":"> Let's now change the column names and both the data and make them same so that joining them will be much easy.\n\n> What we are doing here is making column name as follows:\n> - Target -> Label\n> - Pixel value x -> pixel_x","9294505f":"> 57,825 for the majority class and 1120 for minority!!\n\n> Since the data is now unbalanced, we will have to choose our metric wisely, otherwise, we may get fooled by the accuracy, although we will keep an eye out for accuracy too! ;)","d9cfff79":"## Reading and Understanding data files","df62953d":"> We will be adding 10 to the target values of alphabet data, because 0-9 are already booked for digits and we don't want our model to confuse itself between \"A\" and \"0\" or \"B\" and \"1\" and so on, we will start the labels for alphabet targets from 10, so now, 0-9 will be digits and 10-35 will be alphabets.","711e17b7":"> Standardization of Training data with categorization training and test data.","cf555c51":"## Importing essential libraries","5e5e02a8":"> Till now, the data we had was 1-Dimensional, i.e. a 28x28 picture was flattened into 784 pixels.\n\n> Let's now reshape our data into 28x28 images!","41764cb7":"> Also, let's plot AUC scores for training and validation too!","9e2cb517":"> Let's now Save the model for future use.","86f923f9":"> Let's now see some of the data samples.","ad2aa05e":"### Reshaping the data","676af22a":"> Woah, You reached the end of it!\n\n> Hope you find reading this work worrthwhile. Please share your thoughts in the comment.\n\n> Thanks for reading and Until next time! :)","baee3bd8":"> Defining **Adam** optimizer with a learning rate of 5e-4 and no decay.","7f1e7802":"> Shape of both dataframes.","f3497d3b":"## Little Bit of Data Analysis","2d8df0bc":"> Let's now split the data into train and test data with a ratio of 7:3 and stratifying the target class.\n\n> What stratifying will do is it will maintain the ratio of unique target values in train and test set same.","d25aef3d":"**Compiling model** and defining **learning rate reduction** parameters","ecb4c568":"![](https:\/\/camo.githubusercontent.com\/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275\/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n\nHey, hope you are doing well and fine.\n\nIn this notebook, we will be adding both the MNIST and Alphabets dataset and train a model on them together. So, for this classification, we will have 36 classes in total, 10 for digits and 26 for alphabets.\n\nWe will be using Keras API in this notebook.\n\nI will try to keep it simple and beginner friendly, if you have any suggestions regarding improving the notebook, please leave a comment at the end of it. If you like the work, do appreciate in the way you like! :)\n\nSo, now, without telling more about what to come in this notebook, let's just dive right into it.","c2414407":"> Let's now see the count of maximum and minimum frequency labels.","02085f0c":"Let's see the number of epochs our model run for.","eb48971b":"Let's now the see the loss curve for both training and validation!","0db96033":"> Now, let'see the distribution of data over all the classes.","cab19e5f":"> They are a bit blurry but that's because of the low quality and figure size set by us too! Although they are very much recognizable.","2d7ce820":"> Although the MNIST data is all perfectly balanced, the other data i.e. Alphabetical data is very highly unbalanced.","7bc3fbaa":"## Defining the Model","f0d229c3":"> So we can see that we have 60000 samples for numerals and 3,72,450 samples for alphabets.","de64afe1":"> Let's now take some of the images from the data!","533dd21b":"> We will train this now for 80 epochs, since previously, when we trained it for 40 epochs, it was still converging, so increasing the number of epochs may get us better results.\n\n> Also, we will be using a 128 batch size for this training, i.e. 128 images will the model be trained  upon at a time.","8aaf9fa5":"> Let's set the value for seed too, so that reproducing the same work would be possible.","1e2a07f0":"> Since both of our dataframes have same name for both the columns, we can now simply concat them!","867363f1":"> Creating a simple CNN Classifier.\n\n> We will be using two blocks of [Conv2D -> Conv2d -> MaxPool2D -> Dropout] one after the another and then a [Flatten -> Dense -> Dropout -> Dense]. That's it.","70a2c99f":"> Data Augmentation\n\n> In data augmentation, we are going to do some modifications on the images like rotation, zoom, width shift, height shift and horizontal flip. Also, one must note that doing a vertical flip may be disastrous in some cases, for example, 6 will become 9 and vice versa.","9c904d10":"> Let's define a function to plot some of the images in a grid with their respective labels.\n\n> We will be plotting the images with **gray** cmap, you can try a lot of different too, I would recommend taking a look at \"inferno\"."}}