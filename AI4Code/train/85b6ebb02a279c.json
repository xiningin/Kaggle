{"cell_type":{"5b767d4e":"code","dd6ef656":"code","4071511e":"code","35966c0d":"code","9e9025d1":"code","5bfbde46":"code","e28c1957":"code","9da996ff":"code","a9f7ddff":"code","96e66ee8":"code","eadfe6e7":"code","5f9cd475":"code","5e221acb":"code","e3cdacaf":"code","2bfe34b7":"code","87451139":"code","c4389f8a":"code","9afaf752":"code","590aeacc":"code","a847423b":"code","13f1b488":"code","cdd3e5ee":"code","09ede464":"code","965c3f53":"markdown","53b433b1":"markdown","2844cac1":"markdown","be1575a7":"markdown","a8868557":"markdown","1ad5a0bb":"markdown","96a975ea":"markdown","fe5d885e":"markdown","afa87e8e":"markdown","c4f7994c":"markdown","5a627b65":"markdown","c64eb8ff":"markdown","d0d9410d":"markdown","daaf444a":"markdown","bf3e0fc5":"markdown","5e08e2e1":"markdown","65f68ff5":"markdown","54b2dec9":"markdown","f7e350cf":"markdown","d43e14c6":"markdown","61c3669a":"markdown","f93be069":"markdown","7cac844b":"markdown","ccbc1bc6":"markdown"},"source":{"5b767d4e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2","dd6ef656":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/building.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\nplt.subplot(1, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n\n# Create our shapening kernel, we don't normalize since the \n# the values in the matrix sum to 1\nkernel_sharpening = np.array([[-1,-1,-1], \n                              [-1,9,-1], \n                              [-1,-1,-1]])\n\n# applying different kernels to the input image\nsharpened = cv2.filter2D(image, -1, kernel_sharpening)\n\n\nplt.subplot(1, 2, 2)\nplt.title(\"Image Sharpening\")\nplt.imshow(sharpened)\n\nplt.show()","4071511e":"# Load our new image\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/Origin_of_Species.jpg', 0)\n\nplt.figure(figsize=(30, 30))\nplt.subplot(3, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n# Values below 127 goes to 0 (black, everything above goes to 255 (white)\nret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n\nplt.subplot(3, 2, 2)\nplt.title(\"Threshold Binary\")\nplt.imshow(thresh1)\n\n\n# It's good practice to blur images as it removes noise\nimage = cv2.GaussianBlur(image, (3, 3), 0)\n\n# Using adaptiveThreshold\nthresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5) \n\nplt.subplot(3, 2, 3)\nplt.title(\"Adaptive Mean Thresholding\")\nplt.imshow(thresh)\n\n\n_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\nplt.subplot(3, 2, 4)\nplt.title(\"Otsu's Thresholding\")\nplt.imshow(th2)\n\n\nplt.subplot(3, 2, 5)\n# Otsu's thresholding after Gaussian filtering\nblur = cv2.GaussianBlur(image, (5,5), 0)\n_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\nplt.title(\"Guassian Otsu's Thresholding\")\nplt.imshow(th3)\nplt.show()\n","35966c0d":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/LinuxLogo.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\nplt.subplot(3, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n\n# Let's define our kernel size\nkernel = np.ones((5,5), np.uint8)\n\n# Now we erode\nerosion = cv2.erode(image, kernel, iterations = 1)\n\nplt.subplot(3, 2, 2)\nplt.title(\"Erosion\")\nplt.imshow(erosion)\n\n# \ndilation = cv2.dilate(image, kernel, iterations = 1)\nplt.subplot(3, 2, 3)\nplt.title(\"Dilation\")\nplt.imshow(dilation)\n\n\n# Opening - Good for removing noise\nopening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\nplt.subplot(3, 2, 4)\nplt.title(\"Opening\")\nplt.imshow(opening)\n\n\n# Closing - Good for removing noise\nclosing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\nplt.subplot(3, 2, 5)\nplt.title(\"Closing\")\nplt.imshow(closing)","9e9025d1":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/fruits.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nheight, width,_ = image.shape\n\n# Extract Sobel Edges\nsobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\nsobel_y = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(3, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\nplt.subplot(3, 2, 2)\nplt.title(\"Sobel X\")\nplt.imshow(sobel_x)\n\n\nplt.subplot(3, 2, 3)\nplt.title(\"Sobel Y\")\nplt.imshow(sobel_y)\n\nsobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n\nplt.subplot(3, 2, 4)\nplt.title(\"sobel_OR\")\nplt.imshow(sobel_OR)\n\nlaplacian = cv2.Laplacian(image, cv2.CV_64F)\n\nplt.subplot(3, 2, 5)\nplt.title(\"Laplacian\")\nplt.imshow(laplacian)\n\n\n##  Then, we need to provide two values: threshold1 and threshold2. Any gradient value larger than threshold2\n# is considered to be an edge. Any value below threshold1 is considered not to be an edge. \n#Values in between threshold1 and threshold2 are either classi\ufb01ed as edges or non-edges based on how their \n#intensities are \u201cconnected\u201d. In this case, any gradient values below 60 are considered non-edges\n#whereas any values above 120 are considered edges.\n\n\n# Canny Edge Detection uses gradient values as thresholds\n# The first threshold gradient\ncanny = cv2.Canny(image, 50, 120)\n\nplt.subplot(3, 2, 6)\nplt.title(\"Canny\")\nplt.imshow(canny)","5bfbde46":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/scan.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n# Cordinates of the 4 corners of the original image\npoints_A = np.float32([[320,15], [700,215], [85,610], [530,780]])\n\n# Cordinates of the 4 corners of the desired output\n# We use a ratio of an A4 Paper 1 : 1.41\npoints_B = np.float32([[0,0], [420,0], [0,594], [420,594]])\n \n# Use the two sets of four points to compute \n# the Perspective Transformation matrix, M    \nM = cv2.getPerspectiveTransform(points_A, points_B)\n \nwarped = cv2.warpPerspective(image, M, (420,594))\n\nplt.subplot(1, 2, 2)\nplt.title(\"warpPerspective\")\nplt.imshow(warped)\n\n","e28c1957":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/fruits.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n# Let's make our image 3\/4 of it's original size\nimage_scaled = cv2.resize(image, None, fx=0.75, fy=0.75)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Scaling - Linear Interpolation\")\nplt.imshow(image_scaled)\n\n# Let's double the size of our image\nimg_scaled = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Scaling - Cubic Interpolation\")\nplt.imshow(img_scaled)\n\n# Let's skew the re-sizing by setting exact dimensions\nimg_scaled = cv2.resize(image, (900, 400), interpolation = cv2.INTER_AREA)\n\nplt.subplot(2, 2, 4)\nplt.title(\"Scaling - Skewed Size\")\nplt.imshow(img_scaled)","9da996ff":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/butterfly.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\nsmaller = cv2.pyrDown(image)\nlarger = cv2.pyrUp(smaller)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Smaller\")\nplt.imshow(smaller)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Larger\")\nplt.imshow(larger)\n","a9f7ddff":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/messi5.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\nheight, width = image.shape[:2]\n\n# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\nstart_row, start_col = int(height * .25), int(width * .25)\n\n# Let's get the ending pixel coordinates (bottom right)\nend_row, end_col = int(height * .75), int(width * .75)\n\n# Simply use indexing to crop out the rectangle we desire\ncropped = image[start_row:end_row , start_col:end_col]\n\n\nplt.subplot(2, 2, 2)\nplt.title(\"Cropped\")\nplt.imshow(cropped)","96e66ee8":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/home.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n# Creating our 3 x 3 kernel\nkernel_3x3 = np.ones((3, 3), np.float32) \/ 9\n\n# We use the cv2.fitler2D to conovlve the kernal with an image \nblurred = cv2.filter2D(image, -1, kernel_3x3)\n\nplt.subplot(2, 2, 2)\nplt.title(\"3x3 Kernel Blurring\")\nplt.imshow(blurred)\n\n# Creating our 7 x 7 kernel\nkernel_7x7 = np.ones((7, 7), np.float32) \/ 49\n\nblurred2 = cv2.filter2D(image, -1, kernel_7x7)\n\nplt.subplot(2, 2, 3)\nplt.title(\"7x7 Kernel Blurring\")\nplt.imshow(blurred2)\n","eadfe6e7":"# Let's load a simple image with 3 black squares\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/pic3.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\n\n# Grayscale\ngray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n\n# Find Canny edges\nedged = cv2.Canny(gray, 30, 200)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Canny Edges\")\nplt.imshow(edged)\n\n\n# Finding Contours\n# Use a copy of your image e.g. edged.copy(), since findContours alters the image\ncontours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Canny Edges After Contouring\")\nplt.imshow(edged)\n\nprint(\"Number of Contours found = \" + str(len(contours)))\n\n# Draw all contours\n# Use '-1' as the 3rd parameter to draw all\ncv2.drawContours(image, contours, -1, (0,255,0), 3)\n\nplt.subplot(2, 2, 4)\nplt.title(\"Contours\")\nplt.imshow(image)","5f9cd475":"# Load image and keep a copy\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/house.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\norig_image = image.copy()\n\n\n# Grayscale and binarize\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n\n# Find contours \ncontours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n# Iterate through each contour and compute the bounding rectangle\nfor c in contours:\n    x,y,w,h = cv2.boundingRect(c)\n    cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),2)    \n    plt.subplot(2, 2, 2)\n    plt.title(\"Bounding Rectangle\")\n    plt.imshow(orig_image)\n\ncv2.waitKey(0) \n    \n# Iterate through each contour and compute the approx contour\nfor c in contours:\n    # Calculate accuracy as a percent of the contour perimeter\n    accuracy = 0.03 * cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, accuracy, True)\n    cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n    \n    plt.subplot(2, 2, 3)\n    plt.title(\"Approx Poly DP\")\n    plt.imshow(image)\n\nplt.show()\n    \n# Convex Hull\n\n\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/hand.jpg')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original Image\")\nplt.imshow(image)\n\n# Threshold the image\nret, thresh = cv2.threshold(gray, 176, 255, 0)\n\n# Find contours \ncontours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n    \n# Sort Contors by area and then remove the largest frame contour\nn = len(contours) - 1\ncontours = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n\n# Iterate through contours and draw the convex hull\nfor c in contours:\n    hull = cv2.convexHull(c)\n    cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n\n    plt.subplot(1, 2, 2)\n    plt.title(\"Convex Hull\")\n    plt.imshow(image)","5e221acb":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/someshapes.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\nplt.figure(figsize=(20, 20))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Original\")\nplt.imshow(image)\n\nret, thresh = cv2.threshold(gray, 127, 255, 1)\n\n# Extract Contours\ncontours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\nfor cnt in contours:\n    \n    # Get approximate polygons\n    approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt,True),True)\n    \n    if len(approx) == 3:\n        shape_name = \"Triangle\"\n        cv2.drawContours(image,[cnt],0,(0,255,0),-1)\n        \n        # Find contour center to place text at the center\n        M = cv2.moments(cnt)\n        cx = int(M['m10'] \/ M['m00'])\n        cy = int(M['m01'] \/ M['m00'])\n        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n    \n    elif len(approx) == 4:\n        x,y,w,h = cv2.boundingRect(cnt)\n        M = cv2.moments(cnt)\n        cx = int(M['m10'] \/ M['m00'])\n        cy = int(M['m01'] \/ M['m00'])\n        \n        # Check to see if 4-side polygon is square or rectangle\n        # cv2.boundingRect returns the top left and then width and \n        if abs(w-h) <= 3:\n            shape_name = \"Square\"\n            \n            # Find contour center to place text at the center\n            cv2.drawContours(image, [cnt], 0, (0, 125 ,255), -1)\n            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n        else:\n            shape_name = \"Rectangle\"\n            \n            # Find contour center to place text at the center\n            cv2.drawContours(image, [cnt], 0, (0, 0, 255), -1)\n            M = cv2.moments(cnt)\n            cx = int(M['m10'] \/ M['m00'])\n            cy = int(M['m01'] \/ M['m00'])\n            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n            \n    elif len(approx) == 10:\n        shape_name = \"Star\"\n        cv2.drawContours(image, [cnt], 0, (255, 255, 0), -1)\n        M = cv2.moments(cnt)\n        cx = int(M['m10'] \/ M['m00'])\n        cy = int(M['m01'] \/ M['m00'])\n        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n        \n        \n        \n    elif len(approx) >= 15:\n        shape_name = \"Circle\"\n        cv2.drawContours(image, [cnt], 0, (0, 255, 255), -1)\n        M = cv2.moments(cnt)\n        cx = int(M['m10'] \/ M['m00'])\n        cy = int(M['m01'] \/ M['m00'])\n        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Identifying Shapes\")\nplt.imshow(image)","e3cdacaf":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/sudoku.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\n# Grayscale and Canny Edges extracted\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nedges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n\nplt.subplot(2, 2, 1)\nplt.title(\"edges\")\nplt.imshow(edges)\n\n# Run HoughLines using a rho accuracy of 1 pixel\n# theta accuracy of np.pi \/ 180 which is 1 degree\n# Our line threshold is set to 240 (number of points on line)\nlines = cv2.HoughLines(edges, 1, np.pi\/180, 200)\n\n# We iterate through each line and convert it to the format\n# required by cv.lines (i.e. requiring end points)\nfor line in lines:\n    rho, theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a * rho\n    y0 = b * rho\n    x1 = int(x0 + 1000 * (-b))\n    y1 = int(y0 + 1000 * (a))\n    x2 = int(x0 - 1000 * (-b))\n    y2 = int(y0 - 1000 * (a))\n    cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n\n\nplt.subplot(2, 2, 2)\nplt.title(\"Hough Lines\")\nplt.imshow(image)","2bfe34b7":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/blobs.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20, 20))\n\n\n# Intialize the detector using the default parameters\ndetector = cv2.SimpleBlobDetector_create()\n \n# Detect blobs\nkeypoints = detector.detect(image)\n \n# Draw blobs on our image as red circles\nblank = np.zeros((1,1)) \nblobs = cv2.drawKeypoints(image, keypoints, blank, (0,0,255),\n                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\nnumber_of_blobs = len(keypoints)\ntext = \"Total Number of Blobs: \" + str(len(keypoints))\ncv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 0, 255), 2)\n\n# Display image with blob keypoints\nplt.subplot(2, 2, 1)\nplt.title(\"Blobs using default parameters\")\nplt.imshow(blobs)\n\n\n# Set our filtering parameters\n# Initialize parameter settiing using cv2.SimpleBlobDetector\nparams = cv2.SimpleBlobDetector_Params()\n\n# Set Area filtering parameters\nparams.filterByArea = True\nparams.minArea = 100\n\n# Set Circularity filtering parameters\nparams.filterByCircularity = True \nparams.minCircularity = 0.9\n\n# Set Convexity filtering parameters\nparams.filterByConvexity = False\nparams.minConvexity = 0.2\n    \n# Set inertia filtering parameters\nparams.filterByInertia = True\nparams.minInertiaRatio = 0.01\n\n# Create a detector with the parameters\ndetector = cv2.SimpleBlobDetector_create(params)\n    \n# Detect blobs\nkeypoints = detector.detect(image)\n\n# Draw blobs on our image as red circles\nblank = np.zeros((1,1)) \nblobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\nnumber_of_blobs = len(keypoints)\ntext = \"Number of Circular Blobs: \" + str(len(keypoints))\ncv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)\n\n# Show blobs\nplt.subplot(2, 2, 2)\nplt.title(\"Filtering Circular Blobs Only\")\nplt.imshow(blobs)","87451139":"# Load image then grayscale\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/chessboard.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(10, 10))\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# The cornerHarris function requires the array datatype to be float32\ngray = np.float32(gray)\n\nharris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n\n#We use dilation of the corner points to enlarge them\\\nkernel = np.ones((7,7),np.uint8)\nharris_corners = cv2.dilate(harris_corners, kernel, iterations = 10)\n\n# Threshold for an optimal value, it may vary depending on the image.\nimage[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\n\nplt.subplot(1, 1, 1)\nplt.title(\"Harris Corners\")\nplt.imshow(image)","c4389f8a":"# Load input image and convert to grayscale\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/WaldoBeach.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(30, 30))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Where is Waldo?\")\nplt.imshow(image)\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Load Template image\ntemplate = cv2.imread('\/kaggle\/input\/opencv-samples-images\/waldo.jpg',0)\n\nresult = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n#Create Bounding Box\ntop_left = max_loc\nbottom_right = (top_left[0] + 50, top_left[1] + 50)\ncv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Waldo\")\nplt.imshow(image)","9afaf752":"import cv2 \nimport matplotlib.pyplot as plt\n\nalgo = 'MOG2'\n\nif algo == 'MOG2':\n    backSub = cv2.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv2.createBackgroundSubtractorKNN()\n\nplt.figure(figsize=(20, 20))\n\nframe = cv2.imread('\/kaggle\/input\/opencv-samples-images\/Background_Subtraction_Tutorial_frame.png')\nfgMask = backSub.apply(frame)\n\nplt.subplot(2, 2, 1)\nplt.title(\"Frame\")\nplt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\nplt.subplot(2, 2, 2)\nplt.title(\"FG Mask\")\nplt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n                       \nframe = cv2.imread('\/kaggle\/input\/opencv-samples-images\/Background_Subtraction_Tutorial_frame_1.png')\nfgMask = backSub.apply(frame)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Frame\")\nplt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\nplt.subplot(2, 2, 4)\nplt.title(\"FG Mask\")\nplt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))","590aeacc":"import cv2\nimport numpy as np\n\nalgo = 'MOG2'\ninputt = '\/kaggle\/input\/opencv-samples-images\/video_input\/Background_Subtraction_Tutorial_frame.mp4'\n\ncapture = cv2.VideoCapture(cv2.samples.findFileOrKeep(inputt))\nframe_width = int(capture.get(3))\nframe_height = int(capture.get(4))\n\nout = cv2.VideoWriter('Background_Subtraction_Tutorial_frame_output.mp4',cv2.VideoWriter_fourcc('M','J','P','G'),30, (frame_width,frame_height))\n\nif algo == 'MOG2':\n    backSub = cv2.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv2.createBackgroundSubtractorKNN()\n\n# If you want to run it on video and locally, you must set it to (While) True. (Do not try on Kaggle you will get the error)\nwhile False:\n    \n    ret, frame = capture.read()\n    \n    if frame is None:\n        break\n    \n    fgMask = backSub.apply(frame)\n    \n    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n\n    cv2.imshow('Frame', frame)\n    cv2.imshow('FG Mask', fgMask)\n    \n    out.write(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n    \n    keyboard = cv2.waitKey(1) & 0xFF;\n        \n    if (keyboard == 27 or keyboard == ord('q')):\n        cv2.destroyAllWindows()\n        break;\n        \ncapture.release()\nout.release()\ncv2.destroyAllWindows()","a847423b":"!pip install vcam","13f1b488":"import cv2\nimport numpy as np\nimport math\nfrom vcam import vcam,meshGen\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 20))\n\n# Reading the input image. Pass the path of image you would like to use as input image.\nimg = cv2.imread(\"\/kaggle\/input\/opencv-samples-images\/minions.jpg\")\nH,W = img.shape[:2]\n\n# Creating the virtual camera object\nc1 = vcam(H=H,W=W)\n\n# Creating the surface object\nplane = meshGen(H,W)\n\n# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*exp^((x\/w)^2 \/ 2*0.1*sqrt(2*pi))\n\nplane.Z += 20*np.exp(-0.5*((plane.X*1.0\/plane.W)\/0.1)**2)\/(0.1*np.sqrt(2*np.pi))\npts3d = plane.getPlane()\n\npts2d = c1.project(pts3d)\nmap_x,map_y = c1.getMaps(pts2d)\n\noutput = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n\nplt.subplot(1, 2,1)\nplt.title(\"Funny Mirror\")\nplt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))","cdd3e5ee":"plt.figure(figsize=(20, 20))\n\n# Reading the input image. Pass the path of image you would like to use as input image.\nimg = cv2.imread(\"\/kaggle\/input\/opencv-samples-images\/minions.jpg\")\nH,W = img.shape[:2]\n\n# Creating the virtual camera object\nc1 = vcam(H=H,W=W)\n\n# Creating the surface object\nplane = meshGen(H,W)\n\n# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*exp^((y\/h)^2 \/ 2*0.1*sqrt(2*pi))\nplane.Z += 20*np.exp(-0.5*((plane.Y*1.0\/plane.H)\/0.1)**2)\/(0.1*np.sqrt(2*np.pi))\n\npts3d = plane.getPlane()\n\npts2d = c1.project(pts3d)\nmap_x,map_y = c1.getMaps(pts2d)\n\noutput = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n\nplt.subplot(1, 2,1)\nplt.title(\"Funny Mirror\")\nplt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))","09ede464":"plt.figure(figsize=(20, 20))\n\n# Reading the input image. Pass the path of image you would like to use as input image.\nimg = cv2.imread(\"\/kaggle\/input\/opencv-samples-images\/minions.jpg\")\nH,W = img.shape[:2]\n\n# Creating the virtual camera object\nc1 = vcam(H=H,W=W)\n\n# Creating the surface object\nplane = meshGen(H,W)\n\n# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*[ sin(2*pi*(x\/w-1\/4))) + sin(2*pi*(y\/h-1\/4))) ]\n\nplane.Z += 20*np.sin(2*np.pi*((plane.X-plane.W\/4.0)\/plane.W)) + 20*np.sin(2*np.pi*((plane.Y-plane.H\/4.0)\/plane.H))\n\npts3d = plane.getPlane()\n\npts2d = c1.project(pts3d)\nmap_x,map_y = c1.getMaps(pts2d)\n\noutput = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n\nplt.subplot(1, 2,1)\nplt.title(\"Funny Mirror\")\nplt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))","965c3f53":"<a id=\"15.\"><\/a> \n# 15.Finding Corners","53b433b1":"<a id=\"5.\"><\/a> \n# 5.Perpsective Transform","2844cac1":"<a id=\"17.\"><\/a> \n# 17.Background Subtraction Methods","be1575a7":"So now as we know that by defining Z as a function of X and Y we can create different types of distortion effects. Let us create some more effects using the above code. We simply need to change the line where we define Z as a function of X and Y. This will further help you to create your own effects.","a8868557":"<a id=\"13.\"><\/a> \n# 13.Line Detection - Using Hough Lines\n\ncv2.HoughLines(binarized\/thresholded image, \ud835\udf0c accuracy, \ud835\udf03 accuracy, threshold)\n\n* Threshold here is the minimum vote for it to be considered a line\n","1ad5a0bb":"<a id=\"16.\"><\/a> \n# 16.Finding Waldo","96a975ea":"Let\u2019s create something using sine function !","fe5d885e":"<a id=\"9.\"><\/a> \n# 9.Blurring\n","afa87e8e":"<a id=\"18.\"><\/a> \n# 18.Funny Mirrors Using OpenCV\nSource: https:\/\/www.learnopencv.com\/funny-mirrors-using-opencv\/\n\nFunny mirrors are not plane mirrors but a combination of convex\/concave reflective surfaces that produce distortion effects that look funny as we move in front of these mirrors.\n\n### How does it work ?\nThe entire project can be divided into three major steps :\n\n* Create a virtual camera.\n* Define a 3D surface (the mirror surface) and project it into the virtual camera using a suitable value of projection matrix.\n* Use the image coordinates of the projected points of the 3D surface to apply mesh based warping to get the desired effect of a funny mirror.\n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2020\/04\/steps-for-funny-mirrors.jpg)\n\n","c4f7994c":"## The result you will get on video and locally\n\n![](https:\/\/iili.io\/JMXhdv.gif)","5a627b65":"<a id=\"11.\"><\/a> \n# 11.Approximating Contours and Convex Hull\n\ncv2.approxPolyDP(contour, Approximation Accuracy, Closed)\n\n* contour \u2013 is the individual contour we wish to approximate\n* Approximation Accuracy \u2013 Important parameter is determining the accuracy of the approximation. Small values give precise- approximations, large values give more generic approximation. A good rule of thumb is less than 5% of the contour perimeter\n* Closed \u2013 a Boolean value that states whether the approximate contour should be open or closed\n","c64eb8ff":"<a id=\"8.\"><\/a> \n# 8.Cropping","d0d9410d":"<a id=\"7.\"><\/a> \n# 7.Image Pyramids\nUseful when scaling images in object detection.","daaf444a":"<a id=\"6.\"><\/a> \n# 6.Scaling, re-sizing and interpolations\n\nRe-sizing is very easy using the cv2.resize function, it's arguments are: cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)","bf3e0fc5":"<a id=\"4.\"><\/a> \n# 4.Edge Detection & Image Gradients\n","5e08e2e1":"<a id=\"12.\"><\/a> \n# 12.Identifiy Contours by Shape","65f68ff5":"<a id=\"3.\"><\/a> \n# 3.Dilation, Erosion, Opening and Closing","54b2dec9":"<a id=\"1.\"><\/a> \n# 1.Sharpening\nBy altering our kernels we can implement sharpening, which has the effects of in strengthening or emphasizing edges in an image.","f7e350cf":"<a id=\"10.\"><\/a> \n# 10.Contours\n","d43e14c6":"<a id=\"2.\"><\/a> \n# 2.Thresholding, Binarization & Adaptive Thresholding","61c3669a":"# **About OpenCV**\n* Officially launched in 1999, OpenCV (Open Source Computer Vision) from an Intel initiative.\n* OpenCV\u2019s core is written in C++. In python we are simply using a wrapper that executes C++ code inside of python.\n* First major release 1.0 was in 2006, second in 2009, third in 2015 and 4th in 2018. with OpenCV 4.0 Beta.\n* It is an Open source library containing over 2500 optimized algorithms.\n* It is EXTREMELY useful for almost all computer vision applications and is supported on Windows, Linux, MacOS, Android, iOS with bindings to Python, Java and Matlab. \n\n\n## Update(19.05.2020)\n\nI will always try to improve this kernel. I made some additions to this version. Thanks for reading, I hope it will be useful\n#### Newly Added Content\n* 17.Background Subtraction Methods\n* 18.Funny Mirrors Using OpenCV\n\n# **Content**\n\n1. [Sharpening](#1.)\n1. [Thresholding, Binarization & Adaptive Thresholding](#2.)\n1. [Dilation, Erosion, Opening and Closing](#3.)\n1. [Edge Detection & Image Gradients](#4.)\n1. [Perpsective Transform](#5.)\n1. [Scaling, re-sizing and interpolations](#6.)\n1. [Image Pyramids](#7.)\n1. [Cropping](#8.)\n1. [Blurring](#9.)\n1. [Contours](#10.)\n1. [Approximating Contours and Convex Hull](#11.)\n1. [Identifiy Contours by Shape](#12.)\n1. [Line Detection - Using Hough Lines](#13.)\n1. [Counting Circles and Ellipses](#14.)\n1. [Finding Corners](#15.)\n1. [Finding Waldo](#16.)\n1. [Background Subtraction Methods](#17.)\n1. [Funny Mirrors Using OpenCV](#18.)\n\n\n### Background Subtraction Methods Output\n![](https:\/\/iili.io\/JMXhdv.gif)\n\n### Funny Mirrors Using OpenCV Output\n![](https:\/\/iili.io\/JMw3qF.png)\n\n### Some pictures from content\n![](https:\/\/iili.io\/JMXPkl.png)\n\n\n\n","f93be069":"## If you want to run it on video and locally, you must set it to (While) True. (Do not try on Kaggle you will get the error)\n\n","7cac844b":"<a id=\"14.\"><\/a> \n# 14.Counting Circles and Ellipses","ccbc1bc6":"source: https:\/\/docs.opencv.org\/3.4\/d1\/dc5\/tutorial_background_subtraction.html\n\n## How to Use Background Subtraction Methods\n\nBackground subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n\nAs the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n\n![](https:\/\/docs.opencv.org\/3.4\/Background_Subtraction_Tutorial_Scheme.png)\n\n"}}