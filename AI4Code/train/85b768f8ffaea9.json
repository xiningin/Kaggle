{"cell_type":{"0725ad73":"code","1db8ab51":"code","b6e7f3c2":"code","3ac76222":"code","4cad7aa4":"code","131c6dca":"code","d244df04":"code","a12bd2d4":"code","29c15ba4":"code","144923b8":"code","e2b70893":"code","764b40a7":"code","005fa53b":"code","cf57f0eb":"code","0f4b288b":"code","9f5c73a4":"code","bd3b551c":"code","5db5b393":"code","1ef48b29":"code","49319a38":"code","9dab2a2a":"code","60f308f5":"code","63c3539c":"code","48535a2d":"code","1d275374":"code","6573ebd6":"code","b108de2e":"code","07bafbf6":"code","08efab9b":"markdown","91ab6ce6":"markdown","94a448d8":"markdown","784df148":"markdown","43df791f":"markdown","790240a7":"markdown","b1ef3412":"markdown","47d85494":"markdown","f1be77a7":"markdown","97ad06d8":"markdown","f673ad03":"markdown","72313aea":"markdown","ffbe8dbc":"markdown","12a5d6b9":"markdown","11e28bc8":"markdown","669ab3ad":"markdown"},"source":{"0725ad73":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1db8ab51":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","b6e7f3c2":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","3ac76222":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","4cad7aa4":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")","131c6dca":"test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")","d244df04":"train.head()","a12bd2d4":"train.premise.values[1]","29c15ba4":"train.hypothesis.values[1]","144923b8":"train.label.values[1]","e2b70893":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","764b40a7":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","005fa53b":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","cf57f0eb":"encode_sentence(\"I love machine learning\")","0f4b288b":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","9f5c73a4":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","bd3b551c":"max_len = 259\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","5db5b393":"with strategy.scope():\n    model = build_model()\n    model.summary()","1ef48b29":"model.fit(train_input, train.label.values, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)","49319a38":"test.head()","9dab2a2a":"test2 = pd.DataFrame([['0', '[PAD] '*128, '[PAD] '*128,'en','English']],\n                   columns=['id', 'premise', 'hypothesis','lang_abv','language'])\ntest2\n#test2 = pd.DataFrame([['0', '[I hope you are fine]', '[You must be ill]', 'en','English']],\n        #columns=['id', 'premise', 'hypothesis','lang_abv','language'])","60f308f5":"test3 = test\ntest3 = test3.append(test2)\ntest3.shape","63c3539c":"test_input = bert_encode(test3.premise.values, test3.hypothesis.values, tokenizer)","48535a2d":"predictions = [np.argmax(i) for i in model.predict(test_input)]","1d275374":"predictions = predictions[:-1]","6573ebd6":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","b108de2e":"submission.head()","07bafbf6":"submission.to_csv(\"submission.csv\", index = False)","08efab9b":"#### This is an initial notebook, it's not completed yet. Please support.\n\n##### More versions will be updated soon with more explanations.","91ab6ce6":"Let's look at one of the pairs of sentences.","94a448d8":"## Creating & Training Model","784df148":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","43df791f":"To start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see: https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md\n\nFirst, we download the tokenizer.","790240a7":"Now, we can incorporate the BERT transformer into a Keras Functional Model. For more information about the Keras Functional API, see: https:\/\/www.tensorflow.org\/guide\/keras\/functional.\n\nThis model was inspired by the model in this notebook: https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert#BERT-and-Its-Implementation-on-this-Competition, which is a wonderful introduction to NLP!","b1ef3412":"## Downloading Data","47d85494":"## Preparing Data for Input","f1be77a7":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","97ad06d8":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data","f673ad03":"## Generating & Submitting Predictions","72313aea":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","ffbe8dbc":"BERT uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#tfbertmodel\n\nNow, we're going to encode all of our premise\/hypothesis pairs for input into BERT.","12a5d6b9":"Let's set up our TPU.","11e28bc8":"We can use the pandas head() function to take a quick look at the training set.","669ab3ad":"And now we've created our submission file, which can be submitted to the competition. Good luck!"}}