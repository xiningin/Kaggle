{"cell_type":{"7db57a79":"code","2b57c17a":"code","7b017740":"code","af3a6164":"code","ac873717":"code","be79977e":"code","5a6361ad":"code","1b6142e1":"code","dcc4e80c":"code","59955865":"code","a6171e47":"code","39957379":"code","9c29786b":"code","87572b1d":"code","cc327a24":"code","2390d7a5":"code","34617b7b":"code","5ad37cd0":"code","2af57569":"code","e9982e10":"code","8b65cede":"code","c3c95abc":"code","0f390caf":"code","df2ed4d0":"code","9e2b7877":"code","3871999c":"code","8f0b1e5e":"markdown","7d0d01c8":"markdown","36d68c13":"markdown","7e9201e8":"markdown","6a2a4bfc":"markdown","f3ac4c7b":"markdown","184bc3e2":"markdown","a863adbe":"markdown","f113ca9c":"markdown","fcad2406":"markdown","c488edaf":"markdown","74c6f7d8":"markdown","465a23b7":"markdown"},"source":{"7db57a79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b57c17a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","7b017740":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")","af3a6164":"train_df.info()\ntest_df.info()","ac873717":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","be79977e":"train_df = reduce_memory_usage(train_df)\ntest_df = reduce_memory_usage(test_df)","5a6361ad":"train_df.head()","1b6142e1":"train_df.info()","dcc4e80c":"print('Missing values in train dataset =',train_df.isna().sum().sum())\nprint('Missing values in test dataset =',test_df.isna().sum().sum())","59955865":"train_df['target'].value_counts()","a6171e47":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","39957379":"df = train_df.sample(n=10000)\nX = df.drop(['target','id'],axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","9c29786b":"from sklearn.ensemble import GradientBoostingClassifier\nXGB = GradientBoostingClassifier()","87572b1d":"XGB.fit(X_train,y_train)\nXGB_predict = XGB.predict(X_test)","cc327a24":"roc_auc_score(y_test, XGB_predict)","2390d7a5":"del df, X_train, X_test, y_train, y_test, XGB_predict","34617b7b":"cat_features=[feature for i,feature in enumerate(train_df.columns) if train_df[feature].dtype=='int8' and feature!='target']\ncont_features=[feature for i,feature in enumerate(train_df.columns) if train_df[feature].dtype=='float16']\n\nprint('Number of Categorical features excluding id and target: ',len(cat_features),\n      '\\nNumber of Continuous features: ',len(cont_features))","5ad37cd0":"corr=pd.DataFrame()\ncorr['target'] = train_df[cat_features].corrwith(train_df['target'])\nplt.subplots(figsize=(3,15))\ndf=corr.sort_values(by='target', ascending=False)\nheatmap = sns.heatmap(df,annot=True,cmap='mako',linewidth=0.5,xticklabels=df.columns,yticklabels=df.index)\n\nheatmap.set_title('Correlation - Categorical Features with target', fontdict={'fontsize':16}, pad=16)\nplt.show()","2af57569":"corr=pd.DataFrame()\ncorr['target'] = train_df[cont_features].corrwith(train_df['target'])\nplt.subplots(figsize=(3,50))\ndf=corr.sort_values(by='target', ascending=False)\nheatmap = sns.heatmap(df,annot=True,cmap='mako',linewidth=0.5,xticklabels=df.columns,yticklabels=df.index)\n\nheatmap.set_title('Correlation - Continous Features with target', fontdict={'fontsize':16}, pad=16)\nplt.show()","e9982e10":"imp_cat_features = {'f247', 'f43','f22'}\nimp_cont_features = {'f179', 'f69','f58','f14','f78','f8','f200','f134','f56','f192','f112','f72','f1','f201',\n                     'f150','f92','f95','f3','f77','f136','f156'}","8b65cede":"imp_features = imp_cont_features.union(imp_cat_features)\nX = train_df[imp_features]\ny = train_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","c3c95abc":"XGB = GradientBoostingClassifier()\nXGB.fit(X_train,y_train)\nXGB_predict = XGB.predict(X_test)\nroc_auc_score(y_test, XGB_predict)","0f390caf":"del df, X_train, X_test, y_train, y_test, XGB_predict","df2ed4d0":"df1 = train_df\nX = df1.drop(['target','id'],axis=1)\ny = df1['target']\n\nXGB.fit(X,y)\ndf2 = test_df.drop('id',axis=1).copy()\nXGB_target = XGB.predict(df2)","9e2b7877":"df2 = test_df.drop('id',axis=1).copy()\nXGB_target = XGB.predict(df2)\nXGB_target","3871999c":"sub = test_df['id'].copy()\nsub['target'] = XGB_target\nsub.to_csv(\"XGBsubmission.csv\", index=False)","8f0b1e5e":"* XGBoost gives a good roc_auc score above 0.75 on the train dataset.","7d0d01c8":"* There is a slight improvement in the roc_auc score after feature selection.","36d68c13":"#### **3.Understanding Data -**","7e9201e8":"**Data Summary -**\n* Total 1000000 datapoints and 287 features\n* 240 features are of float type\n* 45 features(other than id and target) are of int type \n* No missing datapoints in either of the datasets\n* The value counts of the target variable are almost equal","6a2a4bfc":"## **Basic XGBoost Prediction -**","f3ac4c7b":"* **Final Prediction -**","184bc3e2":"#### **2. Reduce Memory Usage -**\n* For every column we will reduce the datatype size if all datapoints in the column lie in the range of smaller sized datatype.\n\n* For eg- The target variable(which is int64) has only 2 values(0\/1) and can easily fit in int8(range -127 to 127) and help us save a lot of memory. ","a863adbe":"## **Feature Selection -**\n\n#### **1. Feature Importance -**\n\n* Categorize features into discrete(categorical) and continous types.","f113ca9c":"#### **4. XGBoost Modelling and Prediction -**","fcad2406":"* Let us check the correlation between all features and the target variable.","c488edaf":"# **TPS OCT'21**\n\n---\n\n- First we will simply implement XGBoost with any hyperparameter tuning and check the results. \n- Then we will select important feature and try to optimize our prediction.","74c6f7d8":"#### **1. Import Libraries and Dataset -**","465a23b7":"#### **2. Manual Feature Selction -**\n* Manually select only the features that have high correlation than a **threshold (say >|0.025|)** to the target variable.  "}}