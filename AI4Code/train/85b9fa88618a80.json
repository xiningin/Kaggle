{"cell_type":{"26c5c20f":"code","1203fe11":"code","dfef68ca":"code","f3feb32d":"code","381867c8":"code","d5a163f4":"code","680f32db":"code","bfcb9e08":"code","22d69c78":"code","f2d6a618":"code","3648a542":"markdown","6bbb587d":"markdown","6640acb4":"markdown","84f9dc61":"markdown","f3fac24c":"markdown"},"source":{"26c5c20f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport timeit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1203fe11":"data = pd.read_csv('..\/input\/KSI_CLEAN.csv')\ndf = pd.DataFrame(data)\nprint('The Original Data Size:',df.shape)","dfef68ca":"#Drop Unwanted Columns\ndrop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division', 'District', 'STREET1', 'STREET2', 'OFFSET', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND', 'PEDTYPE',\n       'PEDACT', 'PEDCOND', 'CYCLISTYPE', 'CYCACT', 'CYCCOND', 'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER','FATAL']\ndf_dropped = df.drop(columns=drop_colmns)\n\n#Pick only Automobile Related Accidents\ndf = df_dropped[df_dropped['AUTOMOBILE']==1]","f3feb32d":"#Considered Columns after dropping columns\ndf.columns\n","381867c8":"#Prepare the Final Dataframe with Neighbourhoods and their Number of Accidents Due to Causes of 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL' and 'DISABILITY'\ndf_g2=df.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\ndfspeed = df_g2.pivot(index='Hood_Name',columns='SPEEDING',values='count')\ndf_g2=df.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndfagdriv = df_g2.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\ndf_g2=df.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\ndfred = df_g2.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\ndf_g2=df.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndfalco = df_g2.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\ndf_g2=df.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndfdisb = df_g2.pivot(index='Hood_Name',columns='DISABILITY',values='count')\ndfspeed = dfspeed.drop(dfspeed.columns[0], axis=1)\ndfspeed[2] = dfagdriv.drop(dfagdriv.columns[0], axis=1)\ndfspeed[3] = dfred.drop(dfred.columns[0], axis=1)\ndfspeed[4] = dfalco.drop(dfalco.columns[0], axis=1)\ndfspeed[5] = dfdisb.drop(dfalco.columns[0], axis=1)\ndfspeed.columns.names = ['Cause'] \ndf = dfspeed\ndf = df.dropna()\nprint('Key for Causes are SPEEDING(1), AG_DRIV(2), REDLIGHT(3), ALCOHOL(4) and DISABILITY(5)')\ndf.columns = ['SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']\ndf.head(15)","d5a163f4":"#Scale the Data using Standard Scaler\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale = scaler.fit(df)\ndf_transformed = std_scale.transform(df)\npca = PCA(n_components=3)\npca = pca.fit(df_transformed)\nX = pca.transform(df_transformed)\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(df_transformed)\n    Sum_of_squared_distances.append(km.inertia_)","680f32db":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Deciding Optimal k')\nplt.show()","bfcb9e08":"\nfor n_clusters in range(2,6):\n    kmeans = KMeans(n_clusters=n_clusters , random_state=3425)\n    cluster_labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(10, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.2,-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=300, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","22d69c78":"#Plotting the cluster obtained using GMM\nkmeans = KMeans(n_clusters=2,random_state=3425)\ncolors = ['green','blue']\n\n\nplt.figure(figsize=(15, 5)) \n\nax = plt.subplot(121)\nkc =kmeans.fit(df_transformed)\nlabel = pd.DataFrame(kc.labels_)\ndf_result =pd.DataFrame(df_transformed)\n# label = label.sort_values(by=0)\ndf_result['label']=label\nprint(df_result.columns)\nscatter = plt.scatter(df_result[0],df_result[2],\n                     c=list(label.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering Without PCA')\nplt.xlabel('Speeding')\nplt.ylabel('Aggresive Driving')\nplt.colorbar(scatter)\n\n\nax = plt.subplot(122)\nstart = timeit.default_timer()\nkc =kmeans.fit(X)\nstop = timeit.default_timer()\nklabel0 = pd.DataFrame(kc.labels_)\ndf_result =pd.DataFrame(X)\n# klabel0 = klabel0.sort_values(by=0)\ndf_result['label']=klabel0\nscatter = plt.scatter(df_result[0],df_result[2],\n                      c = list(klabel0.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering With PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.colorbar(scatter)\nprint(\"running time is :\",stop-start)\nplt.show()\n","f2d6a618":"#Display Neighboorhoods\nprint('We Conclude that the Accident Prone Neighborhoods in Toronto by K-Means based on 2007-2017 Data')\nneighborhoods = df.index\nneighborhoods = np.array(neighborhoods)\nprint(neighborhoods[np.where(klabel0[0]==1)])\nksafe = neighborhoods[np.where(klabel0[0]==1)]\nkaccident = neighborhoods[np.where(klabel0[0]==0)]","3648a542":"# Clustering Visualization","6bbb587d":"### Hence We go with K=2 Major Accident Prone Zone and Minor Accident Prone Zone","6640acb4":"## Elbow Method to Determine Number of Clusters","84f9dc61":"## Silhouette analysis","f3fac24c":"# K-MEANS CLUSTERING"}}