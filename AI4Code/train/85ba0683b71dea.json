{"cell_type":{"6e8a83d5":"code","23df6712":"code","1483593c":"code","ac73f889":"code","20a72bda":"code","f6bd105a":"code","9de43236":"code","ba826918":"code","c1549ab7":"code","a3ff82c1":"code","597e192f":"code","aa6a8aef":"code","395c0001":"code","bcf32225":"code","f8374162":"code","4fa9a3ee":"code","9010a77f":"code","fecc7727":"code","7d1762f7":"code","56940ac6":"code","ec45ff04":"code","1eaaba9f":"code","42705819":"code","1845a1f7":"code","44ab5622":"code","d1e13d8b":"code","10f8ef19":"code","15f1054d":"code","ae7c5a21":"code","8d7d867e":"code","daa7c88c":"code","7293c0a2":"markdown","5cfbe2d3":"markdown","fdef667a":"markdown","4bbe247c":"markdown","ce29c81a":"markdown","e95ddf9a":"markdown","855a4cd6":"markdown","ba6f3b89":"markdown","fca76fe8":"markdown","5ded17ea":"markdown","b332066d":"markdown","be1c67ea":"markdown","220ef08b":"markdown","c2637caa":"markdown","bd1b33d6":"markdown","c9dd5f98":"markdown","65221c00":"markdown","7528465c":"markdown","287729ec":"markdown","3bbece29":"markdown","c8b5f607":"markdown","c78cbe58":"markdown"},"source":{"6e8a83d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23df6712":"import matplotlib.pyplot as plt\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential \nfrom tensorflow.keras.utils import Sequence, to_categorical, plot_model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, MaxPooling2D, concatenate, BatchNormalization, Activation, Dropout\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom random import sample, choice\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1483593c":"train_img_lst = os.listdir(\"..\/input\/camvid\/CamVid\/train\")\nval_img_lst = os.listdir(\"..\/input\/camvid\/CamVid\/val\")\ntest_img_lst = os.listdir(\"..\/input\/camvid\/CamVid\/test\")\nprint(len(train_img_lst),len(val_img_lst), len(test_img_lst))\nprint(type(train_img_lst[0].split('.')[0]))","ac73f889":"'''This function makes pairs of directories of Image and its mask '''\ndef make_pair(img_lst,image_dir,mask_dir):\n    pairs = []\n    #print(image_dir+img_lst[0])\n    for im in img_lst:\n        pairs.append((image_dir + im, mask_dir + im.split('.')[0]+'_L.png'))\n        \n    return pairs","20a72bda":"'''Here we create lists of pairs of images and corresponding masks for both train and validation Images'''\ntrain_pairs = make_pair(train_img_lst, \"..\/input\/camvid\/CamVid\/train\/\", \n                        \"..\/input\/camvid\/CamVid\/train_labels\/\")\n\nval_pairs = make_pair(val_img_lst, \"..\/input\/camvid\/CamVid\/val\/\", \n                        \"..\/input\/camvid\/CamVid\/val_labels\/\")\n\ntest_pairs = make_pair(test_img_lst, \"..\/input\/camvid\/CamVid\/test\/\", \n                        \"..\/input\/camvid\/CamVid\/test_labels\/\")\n\ntest_pairs[0]","f6bd105a":"'''We can simply plot and see the image and corresponding mask from above list of directories randomly'''\ntemp = choice(train_pairs)\nimg = img_to_array(load_img(temp[0]))\nmask = img_to_array(load_img(temp[1]))\n#mask_pil = np.asarray(Image.open(temp[1]))\n\nplt.figure(figsize=(12,12))\nplt.subplot(121)\nplt.title(\"Image\")\nplt.imshow(img\/255)\nplt.subplot(122)\nplt.title(\"Mask\")\nplt.imshow(mask\/255)\n#plt.subplot(123)\n#plt.imshow(mask_pil)\nplt.show()","9de43236":"class_map_df = pd.read_csv(\"..\/input\/camvid\/CamVid\/class_dict.csv\")\nclass_map_df.head()","ba826918":"class_map = []\nfor index,item in class_map_df.iterrows():\n    class_map.append(np.array([item['r'], item['g'], item['b']]))\n    \nprint(len(class_map))\nprint(class_map[0])","c1549ab7":"\"\"\"This function will be used later, to assert that mask should contains values that are class labels only.\n   Like, our example has 32 classes , so predicted mask must contains values between 0 to 31. \n   So that it can be mapped to corresponding RGB.\"\"\"\ndef assert_map_range(mask,class_map):\n    mask = mask.astype(\"uint8\")\n    for j in range(img_size):\n        for k in range(img_size):\n            assert mask[j][k] in class_map , tuple(mask[j][k])","a3ff82c1":"'''This method will convert mask labels(to be trained) from RGB to a 2D image whic holds class labels of the pixels.'''\ndef form_2D_label(mask,class_map):\n    mask = mask.astype(\"uint8\")\n    label = np.zeros(mask.shape[:2],dtype= np.uint8)\n    \n    for i, rgb in enumerate(class_map):\n        label[(mask == rgb).all(axis=2)] = i\n    \n    return label","597e192f":"lab = form_2D_label(mask,class_map)\nnp.unique(lab,return_counts=True)","aa6a8aef":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    \n    def __init__(self, pair,class_map,  batch_size=16, dim=(224,224,3), shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.pair = pair\n        self.class_map = class_map\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.pair) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [k for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.pair))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        batch_imgs = list()\n        batch_labels = list()\n\n        # Generate data\n        for i in list_IDs_temp:\n            # Store sample\n            img = load_img(self.pair[i][0] ,target_size=self.dim)\n            img = img_to_array(img)\/255.\n            batch_imgs.append(img)\n\n            label = load_img(self.pair[i][1],target_size=self.dim)\n            label = img_to_array(label)\n            #------ comment these two lines to see proper working of datagenerator in cell below----#\n            label = form_2D_label(label,self.class_map)\n            #label = np.asarray(to_categorical(label , num_classes = 32))\n            #------ But after that uncomment again them before training the model----------#\n            #------ comment them to just run the cell below and again uncomment these two lines----#\n            #print(label.shape)\n            batch_labels.append(label)\n        return np.array(batch_imgs) ,np.array(batch_labels)","395c0001":"img_size = 512\n#class_map = class_palette()\n\ntrain_generator1 = DataGenerator(train_pairs,class_map,batch_size=4, dim=(img_size,img_size,3) ,shuffle=True)\nX,y = train_generator1.__getitem__(0)\nprint(X.shape, y.shape)\n\n\nplt.figure(figsize=(12, 6))\nprint(\"Images\")\nfor i in range(4):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(X[i])\nplt.show()\n\nprint(\"Masks\")\nplt.figure(figsize=(12, 6))\nfor i in range(4):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(y[i])\nplt.show()","bcf32225":"train_generator = DataGenerator(train_pairs,class_map,batch_size=4, dim=(img_size,img_size,3) ,shuffle=True)\nval_generator = DataGenerator(val_pairs, class_map, batch_size=4, dim=(img_size,img_size,3) ,shuffle=True)\ntest_generator = DataGenerator(test_pairs, class_map, batch_size=4, dim=(img_size,img_size,3) ,shuffle=True)","f8374162":"def conv_block(tensor, nfilters, size=3, padding='same', initializer=\"he_normal\"):\n    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)):\n    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor)\n    y = concatenate([y, residual], axis=3)\n    y = conv_block(y, nfilters)\n    return y\ndef Unet(h, w, filters, num_classes = 32):\n# down\n    input_layer = Input(shape=(h, w, 3), name='image_input')\n    conv1 = conv_block(input_layer, nfilters=filters)\n    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = conv_block(conv1_out, nfilters=filters*2)\n    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = conv_block(conv2_out, nfilters=filters*4)\n    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = conv_block(conv3_out, nfilters=filters*8)\n    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)\n    conv4_out = Dropout(0.5)(conv4_out)\n    conv5 = conv_block(conv4_out, nfilters=filters*16)\n    conv5 = Dropout(0.5)(conv5)\n# up\n    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters*8)\n    deconv6 = Dropout(0.5)(deconv6)\n    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)\n    deconv7 = Dropout(0.5)(deconv7) \n    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)\n    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)\n    output_layer = Conv2D(filters=num_classes, kernel_size=(1, 1), activation='softmax')(deconv9)\n\n    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')\n    return model","4fa9a3ee":"model = Unet(img_size , img_size , 64)\nmodel.summary()","9010a77f":"model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy() ,metrics=['accuracy'])","fecc7727":"mc = ModelCheckpoint(mode='max', filepath='top-weights-na.h5', monitor='val_acc',save_best_only='True', verbose=1)\nes = EarlyStopping(monitor='val_acc', patience=10, verbose=0)","7d1762f7":"train_steps = train_generator.__len__()\nval_steps = val_generator.__len__()\n\nprint(train_steps, val_steps)","56940ac6":"results = model.fit_generator(train_generator , steps_per_epoch=train_steps ,epochs=30,\n                              validation_data=val_generator,validation_steps=val_steps,callbacks=[mc,es],\n                             verbose = 1)","ec45ff04":"model.save('..\/output\/kaggle\/working\/camvid_unet_model_na.h5')","1eaaba9f":"trained_model = keras.models.load_model(\"..\/output\/kaggle\/working\/camvid_unet_model_na.h5\")","42705819":"trained_model.evaluate_generator(test_generator, verbose=1)","1845a1f7":"x_test, y_test = test_generator.__getitem__(2)\nprint(x_test.shape, y_test.shape)","44ab5622":"y_pred = trained_model.predict(x_test, verbose = 1, batch_size = 4)\ny_pred.shape","d1e13d8b":"'''This converts predicted map to RGB labels'''\ndef map_this(y_pred,class_map):\n    y_pred_rgb = np.zeros((y_pred.shape[0],y_pred.shape[1],y_pred.shape[2],3))\n    for i in range(y_pred.shape[0]):\n        image = np.zeros((y_pred.shape[1],y_pred.shape[2],3))\n        for j in range(y_pred.shape[1]):\n            for k in range(y_pred.shape[2]):\n                image[j,k,:] = class_map[y_pred[i][j][k]]\n        y_pred_rgb[i] = image\n    return y_pred_rgb","10f8ef19":"\"\"\"This will plot original image, original mask and predicted mask\"\"\"\ndef plot_result(img , title):\n    plt.figure(figsize=(12, 6))\n    plt.title(title)\n    for i in range(4):\n        #print(pred[i].shape)\n        plt.subplot(2, 4, i+1)\n        plt.imshow(img[i])\n    plt.show()","15f1054d":"pred = np.argmax(y_pred, axis=3)\ny_pred_rgb = map_this(pred,class_map)\n#test = np.argmax(y_test, axis=3)\ny_test_rgb = map_this(y_test,class_map)","ae7c5a21":"plot_result(x_test,\"Test Images\")","8d7d867e":"plot_result(y_test_rgb,\"Original Masks\")","daa7c88c":"plot_result(y_pred_rgb,\"Predicted mask\")","7293c0a2":"**Let's compare the model predictions with original masks**","5cfbe2d3":"**So our above plotted mask Image will get converted into a 2D label with the class labels at corresponding pixels.**","fdef667a":"**A CSV file of Class map is also provided , which gives us the mapping, we will load it and see how mapping works**","4bbe247c":"**Creating objects of Class Datagenerator and trying to plot what it returns to validate its proper working.**","ce29c81a":"**This class map csv file defines 32 different classes and their RGB values to be mapped. So, we convert it into List whose index will give Class and entry at that index gives RGB values of that class\/index.**","e95ddf9a":"**So, we can clearly see that class map is a mapping which gives us , object->class mapping as for example Animal belongs to class 0. And object -> RGB values mapping for example Animal which belongs to class zero should have R=64, G=128 and B=64 as its pixel values.**","855a4cd6":"**In this notebook we will try to implement Image Segmentation using Deep Learning.\n  We will train a U-net model.\n  \"https:\/\/towardsdatascience.com\/understanding-semantic-segmentation-with-unet-6be4f42d4b47\",\n  on Camvid Dataset. you can find this dataset easily on kaggle. I have taken it from     https:\/\/www.kaggle.com\/carlolepelaars\/camvid**\n","ba6f3b89":"**So, We have 369 images in Training data(we are using lesser than actual CamVid Data) and 100 Validation Images and 232 test Images.**","fca76fe8":"**The CAMVID dataset consists of training(data and labels), and validation images(both data and label) and test data. so, we will make lists of directories of Train images and Val images which we will use to load the images later on whenever required.**","5ded17ea":"**Go and uncomment those lines in Datagenerator class, __data_generation() method and run that cell again.**","b332066d":"**Here We get a clear feel about what task we have to carry out, i.e. what is Image Segmentation.**","be1c67ea":"**This class is a custom datagenerator, which takes pairs list and return images and 2D labeled masks. We will use this generator to feed our model.**","220ef08b":"**From above output we see that every element of the lists train_pairs and test pairs is a pair of directories of image and its corresponding mask**","c2637caa":"**U-Net Model**","bd1b33d6":"**Classes present in the particular mask and no. of pixels belonging to that class**","c9dd5f98":"**I trained the model and saved it and loaded again to evaluate**","65221c00":"**Test Accuracy is near 84-85%. And loss is 0.57.**","7528465c":"## Segmentation ##\n**In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). The goal of segmentation is to simplify and\/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.**","287729ec":"**Thanks and please upvote if you get to learn something new.**","3bbece29":"**Custom Data Generator**","c8b5f607":"**Importing Libraries**","c78cbe58":"**Load the saved model to evaluate it**"}}