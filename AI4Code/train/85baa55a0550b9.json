{"cell_type":{"da60af6a":"code","b74e3bfe":"code","4e91daab":"code","0a291e51":"code","6a362e34":"code","6498a8f7":"code","d7e636e7":"code","ca3f8810":"code","cebf791e":"code","a4402ca4":"code","74e9d1f1":"code","e1da4031":"code","40fac2a0":"code","93e13c64":"code","3adc9838":"code","3b9b14f5":"code","c97734e5":"code","90f74ce0":"code","027267bb":"code","8ed23c7e":"code","83da589c":"code","600d340b":"code","f22343ec":"code","905b34e8":"code","38b946a6":"code","86f116dc":"code","5b2d900d":"code","6d3cce6c":"code","a3057d14":"code","dae1dd69":"code","053a2694":"code","6a8bc090":"code","4d3a2bf9":"code","45613166":"code","0851e63a":"code","5885af9b":"code","80d98f9c":"code","1ce6cc85":"code","289b8914":"code","b32e0d07":"code","c63cad1f":"code","1b79e34c":"code","2bb85547":"code","cbcc4770":"code","b34fbf5d":"code","5d5b9dc8":"code","48c59d54":"code","8b48ba02":"code","a0c396b0":"code","4c52098d":"code","0ef5c636":"code","fd39d6b3":"code","a57d6c52":"code","5668fd1d":"code","3bfd185b":"code","d28df8ce":"code","206f0974":"markdown","16e00a41":"markdown","0dad0b2f":"markdown","06b924a0":"markdown","bdbdc4ca":"markdown","1668cd63":"markdown","502c4c5f":"markdown","5d559d45":"markdown","4fa894ce":"markdown","abd99630":"markdown","959ae679":"markdown","727c0b81":"markdown","82890c28":"markdown","d1d96cbb":"markdown","0ac41447":"markdown","f6f14c1d":"markdown","b352257b":"markdown","5a1a5a94":"markdown","e3181732":"markdown","93a4fe68":"markdown","053fbb1d":"markdown","022ada61":"markdown","b6687acc":"markdown","2e26ff7c":"markdown","d99ae14f":"markdown","c1b393b1":"markdown","61002528":"markdown","92310cfd":"markdown","f1e53410":"markdown","fbe9f284":"markdown","ba90a1d5":"markdown","7eb31108":"markdown","b90c53b5":"markdown","4887a0a5":"markdown","b5e38c0c":"markdown","8da991c9":"markdown","28a2f188":"markdown","88be977e":"markdown","5a2e6328":"markdown"},"source":{"da60af6a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b74e3bfe":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix","4e91daab":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import RandomizedSearchCV","0a291e51":"from scipy.stats import pearsonr\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import r2_score","6a362e34":"# Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm","6498a8f7":"import time\nfrom collections import Counter","d7e636e7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca3f8810":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","cebf791e":"df = pd.concat([train, test], ignore_index=True)\ndf.head(10)","a4402ca4":"df.head(10)","74e9d1f1":"# copying for future purposes\ndata_0 = df.copy()","e1da4031":"train.info()","40fac2a0":"df = df.drop(['Id'], axis=1)","93e13c64":"df.info()","3adc9838":"i = 0\nfor x in range(len(df.columns)):\n    if df.iloc[:,x].isnull().sum() > 0:\n        i += 1\nprint(i)","3b9b14f5":"null_num = ['BsmtFullBath', 'BsmtHalfBath']\nnull_com = ['GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']\nnull_obj = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 'SaleType']","c97734e5":"for x in null_num:\n    df[x].fillna(df[x].median(), inplace = True)\n\nfor x in null_obj:\n    df[x].fillna(df[x].mode()[0], inplace = True)","90f74ce0":"for x in null_com:\n    df[x].fillna(0, inplace=True)","027267bb":"df[\"LotFrontage\"].fillna(0, inplace = True)\ndf[\"MasVnrArea\"].fillna(0, inplace=True)\ndf[\"BsmtFinSF1\"].fillna(0, inplace=True)\ndf[\"BsmtFinSF2\"].fillna(0, inplace=True)\ndf[\"BsmtUnfSF\"].fillna(0, inplace=True)\ndf[\"TotalBsmtSF\"].fillna(0, inplace=True)\ndf[\"GarageCars\"].fillna(0, inplace=True)\ndf[\"GarageArea\"].fillna(0, inplace=True)\ndf[\"LotFrontage\"].value_counts()","8ed23c7e":"df[\"Alley\"].fillna('No', inplace = True)\ndf[\"MasVnrType\"].fillna('No', inplace = True)\ndf[\"GarageType\"].fillna('No', inplace = True)\ndf[\"MiscFeature\"].fillna('No', inplace = True)\ndf[\"BsmtQual\"].fillna('No', inplace = True)\ndf[\"BsmtCond\"].fillna('No', inplace = True)\ndf[\"BsmtExposure\"].fillna('No', inplace = True)\ndf[\"BsmtFinType1\"].fillna('No', inplace = True)\ndf[\"BsmtFinType2\"].fillna('No', inplace = True)\ndf[\"FireplaceQu\"].fillna('No', inplace = True)\ndf[\"PoolQC\"].fillna('No', inplace = True)\ndf[\"Fence\"].fillna('No', inplace = True)","83da589c":"for x in range(df.shape[0]):\n    for y in null_com:\n        if df.iloc[x,df.columns.get_loc(\"GarageType\")] == 'No':\n            df.iloc[x,df.columns.get_loc(y)] = 0\n        elif df.iloc[x,df.columns.get_loc(\"GarageType\")] != 'No' and df.iloc[x,df.columns.get_loc(y)] == 'No':\n            df.iloc[x,df.columns.get_loc(y)] = df[y].median()","600d340b":"columns_numeric = list(df.dtypes[(df.dtypes=='int64') | (df.dtypes=='float64') ].index)\ncolumns_object = list(df.dtypes[df.dtypes=='object'].index)\nprint(f\"numeric columns: {len(columns_numeric)} \\nobject columns: {len(columns_object)}\")","f22343ec":"df2 = df.copy()\nfor x in columns_object:\n    temp = pd.get_dummies(df2[x],prefix=x)\n    df2 = pd.concat([df2,temp],axis=1)\n    df2.drop(x,axis=1,inplace=True)\ndf2.shape","905b34e8":"X = df2.drop(['SalePrice'], axis=1)\ny = df2['SalePrice']\ntrain = df2.iloc[:1460,:]","38b946a6":"corr = X.corr(method='pearson')\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = X.columns[columns]\nX2 = X[selected_columns]","86f116dc":"df3 = X2.copy()\ndf3['SalePrice'] = y\ncorr = df3.corr(method='pearson')['SalePrice']","5b2d900d":"flag = 0\nfor x in range(len(corr)):\n    if corr[x] < 0.05 and corr[x] > -0.05:\n        flag += 1\n        # print(f\"Dropping column: {df2.columns[x]}: {corr[x]}\")\n        df3 = df3.drop([X2.columns[x]], axis=1)\n        # print()\nprint(f\"Columns dropped: {flag}\")","6d3cce6c":"df11 = df3.copy()\nvar = df11.var()\ni = 0\nfor x in range(len(var)):\n    if var[x] < 0.005:\n        i += 1\n        df11 = df11.drop([df3.columns[x]], axis=1)\n        # print(f\"dropping: {df3.columns[x]}\")\nprint(f'Columns dropped: {i}')","a3057d14":"dataset = df11","dae1dd69":"X = dataset.drop(['SalePrice'], axis=1)\ny = dataset['SalePrice']\nX_t = X.iloc[:1460,:]\ny_t = y.iloc[:1460]\nX_test = X.iloc[1460:,:]\ny_test = y.iloc[1460:]","053a2694":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_t, y_t, test_size = 0.3, random_state = 0)","6a8bc090":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","4d3a2bf9":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nstart = time.time()\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_val)\ntime_ML = time.time() - start\nacc01 = round(r2_score(y_val, y_pred),4)\nprint('Linear regression accuracy : ' ,acc01)","45613166":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred)))\nRMLSE_ML = np.sqrt(mean_squared_log_error(y_val, y_pred))","0851e63a":"from sklearn.tree import DecisionTreeRegressor\nregr = DecisionTreeRegressor(max_depth=2, random_state=0, max_leaf_nodes=2)\nstart = time.time()\nregr.fit(X_train, y_train)\ny_pred01 = regr.predict(X_val)\ntime_DT = time.time() - start\nacc02 = round(r2_score(y_val, y_pred01),4)\nprint('Decision tree regression accuracy : ' ,acc02)","5885af9b":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred01))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred01))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred01)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred01)))\nRMLSE_DT = np.sqrt(mean_squared_log_error(y_val, y_pred01))","80d98f9c":"# randomforest = RandomForestRegressor(n_estimators=200, random_state=2)\nrandomforest = RandomForestRegressor(n_estimators=400, random_state=2, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=None, bootstrap=False)\n# randomforest = RandomForestRegressor(n_estimators=110, random_state=2, min_samples_split=6, min_samples_leaf=2, max_features='auto', max_depth=20, bootstrap=True)\nstart = time.time()\nrandomforest.fit(X_train, y_train)\ny_pred02= randomforest.predict(X_val)\ntime_RF = time.time() - start\nacc03 = round(r2_score(y_val, y_pred02),4)\nprint('Random Forest Regression accuracy : ' ,acc03)","1ce6cc85":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\nrf_random.best_params_","289b8914":"tuple(rf_random.best_params_)","b32e0d07":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred02))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred02))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred02)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred02)))\nRMLSE_RF = np.sqrt(mean_squared_log_error(y_val, y_pred02))","c63cad1f":"from sklearn.svm import SVR\n\nregr01 = SVR(kernel='linear')\nstart = time.time()\nregr01.fit(X_train, y_train)\ny_pred03 = regr01.predict(X_val)\ntime_SV = time.time() - start\nacc04 = round(r2_score(y_val, y_pred03),4)\nprint('SVR accuracy : ' ,acc04)","1b79e34c":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred03))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred03))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred03)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred03)))\nRMLSE_SV = np.sqrt(mean_squared_log_error(y_val, y_pred03))","2bb85547":"gb = GradientBoostingRegressor(n_estimators=1400, random_state=4, min_samples_split=10, min_samples_leaf=1, max_features='sqrt', max_depth=20, learning_rate=0.01)\nstart = time.time()\ngb.fit(X_train, y_train)\ny_pred04= gb.predict(X_val)\ntime_GB = time.time() - start\nacc05 = round(r2_score(y_val, y_pred04),4)\nprint('Gradient Boosting accuracy : ' ,acc05)","cbcc4770":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Learning rate\nlearning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'learning_rate': learning_rate}\nprint(random_grid)\n\ngb = GradientBoostingRegressor()\ngb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, n_iter = 1, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\ngb_random.fit(X_train, y_train)\ngb_random.best_params_","b34fbf5d":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred04))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred04))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred04)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred04)))\nRMLSE_GB = np.sqrt(mean_squared_log_error(y_val, y_pred04))","5d5b9dc8":"from sklearn.ensemble import AdaBoostRegressor\n\nadb = AdaBoostRegressor(random_state=101)\nstart = time.time()\nadb.fit(X_train, y_train)\ny_pred05= adb.predict(X_val)\ntime_AB = time.time() - start\nacc06 = round(r2_score(y_val, y_pred05),4)\nprint('ADA Boosting accuracy : ' ,acc06)","48c59d54":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred05))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred05))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred05)))\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred05)))\nRMLSE_AB = np.sqrt(mean_squared_log_error(y_val, y_pred05))","8b48ba02":"import lightgbm as lgb\n\nd_train = lgb.Dataset(X_train, label=y_train)\n\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 10\nparams['min_data'] = 50\nparams['max_depth'] = 10\n\nclf = lgb.train(params, d_train, 100)\n\ny_pred06= clf.predict(X_val)\n\nacc07 = round(r2_score(y_val, y_pred06),4)\nprint('ADA Boosting accuracy : ' ,acc07)\nprint('Root Mean Log Squared Error:', np.sqrt(mean_squared_log_error(y_val, y_pred06)))","a0c396b0":"models= pd.DataFrame({ \n\"Model\" : [\"MultipleLinearRegression\", \"DecisionTreeRegression\", \"RandomForestRegression\", \"SVR\",\"Gradient Boosting\", \"Adaboost\"],\n\"Accuracy\" : [acc01, acc02, acc03, acc04, acc05, acc06],\n\"Time\" : [time_ML, time_DT, time_RF, time_SV, time_GB, time_AB],\n\"RMLSE\" : [RMLSE_ML, RMLSE_DT, RMLSE_RF, RMLSE_SV, RMLSE_GB, RMLSE_AB]\n})\nmodel_notime = pd.DataFrame({ \n\"Model\" : [\"MultipleLinearRegression\", \"DecisionTreeRegression\", \"RandomForestRegression\", \"SVR\",\"Gradient Boosting\", \"Adaboost\"],\n\"Accuracy\" : [acc01, acc02, acc03, acc04, acc05, acc06]\n})\nmodel_time = pd.DataFrame({ \n\"Model\" : [\"MultipleLinearRegression\", \"DecisionTreeRegression\", \"RandomForestRegression\", \"SVR\",\"Gradient Boosting\", \"Adaboost\"],\n\"Time\" : [time_ML, time_DT, time_RF, time_SV, time_GB, time_AB]\n})\nmodels","4c52098d":"models.sort_values(by=\"RMLSE\")","0ef5c636":"gb = GradientBoostingRegressor(n_estimators=1400, random_state=2, min_samples_split=10, min_samples_leaf=1, max_features='sqrt', max_depth=20, learning_rate=0.01)\ngb.fit(X_t, y_t)\ny_pred = gb.predict(X_test)\ny_t","fd39d6b3":"submit = data_0.iloc[1460:,0]\nsubmit = pd.DataFrame(submit)","a57d6c52":"submit['SalePrice'] = y_pred","5668fd1d":"submit","3bfd185b":"submit.to_csv('Submission.csv', index=False)","d28df8ce":"submit.shape","206f0974":"# 1.0 Introduction\n","16e00a41":"### Hyperparameter Tuning(GB)","0dad0b2f":"# 3.4 Support Vector","06b924a0":"> - To Drop:\n    - Id (Irrelevant Data)","bdbdc4ca":"## 3.0.2 Standardizing the Data","1668cd63":"# 4.0 Final Result","502c4c5f":"## 2.2.2 Legit:\n> Values are not missing just to be replaced with some value\n### 2.2.2.1 Numeric:\n> Here null values are directly replaced by 0","5d559d45":"## 2.4.1 High Correlation Filter (Resolved the Dummy Variable Trap)\n> Calculated the correlation of all the feature variables with each other and then removed those having correlation above 0.9\n\n> Removed 10, Total Columns Remaining: 293","4fa894ce":"# 1.2 Collecting the data","abd99630":"## 2.4.2 Correlation of the target variable with all the features \n> Calculated the correlation of all the feature variables with the target variable and removed those with absolute val less than 0.05\n\n> Removed 110, Total Columns Remaining: 183","959ae679":"> - 1.0 Introduction\n    - 1.1 Importing libraries\n    - 1.2 Collecting the data\n  \n> - 2.0 Preprocessing\n    - 2.1 Dropping irrelevant features\n    - 2.2 Null Value Removal\n        - 2.2.1 Null Values\n        - 2.2.2 Legit\n            - 2.2.2.1 Numeric\n            - 2.2.2.2 Object\n            - 2.2.2.3 Complex\n    - 2.3 Data Encoding\n        - 2.3.1 One Hot Encoding\n    - 2.4 Feature Selection\n        - 2.4.1 High Correlation Filter (Resolved the Dummy Variable Trap)\n        - 2.4.2 Correlation of the target variable with all the features\n    - 2.5 Dimensionality Reduction\n        - 2.5.1 Low Variance Filter\n   \n> - 3.0 Model Training\n     - 3.0.1 Splitting the data\n     - 3.0.2 Standardizing the Data\n- 3.1 Multiple Linear Regression\n- 3.2 Decidion Tree\n- 3.3 Random Forest\n- 3.4 Support Vector Machine\n- 3.5 Gradient Boosting\n- 3.6 Ada Boosting\n- 3.7 Light GBM\n\n> - 4.0 Final Result\n\n> - 5.0 Submit","727c0b81":"# 2.5 Dimensionality Reduction\n\n## 2.5.1 Low Variance Filter\n\n> Calculating the variance of all feature columns and removing those with value less than 0.05\n\n> Removed 11, Total Columns Remaining: 172","82890c28":"# 2.3 Data Encoding:\n> Used OneHotEncoder over the whole the object features\n\n> Total Columns: 303","d1d96cbb":"# 1.1 Importing libraries","0ac41447":"# 5.0 Submissions\n> Used Gradient Boostiong as it is giving the highest accuracy","f6f14c1d":"# 3.1 Multiple Linear Regression","b352257b":"# 3.0 Model Training:","5a1a5a94":"# 3.2 Decision Tree","e3181732":"# House Price Prediction","93a4fe68":"### 2.2.2.2 Object:\n> Here null values are directly replaced by 'No'","053fbb1d":"*Not able to interpret anything from the heatmap as too many features therefore not using*\n> plt.subplots(figsize = (25,20))\nsns.heatmap(df2.corr(method='pearson'), annot=False, linewidths=0.2)","022ada61":"> - Null:\t\n\t- MSZoning, Utilities, Exterior1st, Exterior2nd, Electrical, BsmtFullBath(No Bsmt), BsmtHalfBath(No Bsmt), KitchenQual, Functional, GarageYrBlt(Not all), GarageFinish(Not all), GarageQual(Not all), GarageCond(Not all), SaleType\n\n\n> - Legit:\n\t- Numeric: LotFrontage, MasVnrArea, BsmtFinSF1, BsmtFinSF2, \n\t\tBsmtUnfSF, TotalBsmtSF, GarageYrBlt(Not all), \n\t\tGarageFinish(Not all), GarageCars, GarageArea, \n\t\tGarageQual(Not all), GarageCond(Not all) \n    - Object: Alley, MasVnrType, GarageType, MiscFeature, \n\t- Obj-Num: BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, PoolQC, Fence, ","b6687acc":"# 3.3 Random Forest","2e26ff7c":"### Hyperparameter Tuning(LGBM)","d99ae14f":"# 2.0 Preprocessing\n# 2.1 Dropping irrelevant features","c1b393b1":"> In this project we intend to predict the price of the houses with the various given features","61002528":"### 2.2.2.3 Complex\n> For the Values with complexity","92310cfd":"### Hyperparameter Tuning(RF)","f1e53410":"## 2.3.1 One Hot Encoder","fbe9f284":"# 3.5 Gradient Boosting\n> Model with highest accuracy","ba90a1d5":"## 3.0.1 Splitting the Data","7eb31108":"# Outliers Handling\n> Wasn't able to find a model yet to remove the outliers","b90c53b5":"## 2.2.1 Null Values:\n> Values which are missing thereby have to be filled with median\/mode","4887a0a5":"# 2.2 Null Value Removal:","b5e38c0c":"> There a lot of features with non-numeric data which will be required to be Encoded for our code to parse it.","8da991c9":"#### Splitting Target and Feature Variables","28a2f188":"# 3.7 Light GBM","88be977e":"# 3.6 Ada Boost","5a2e6328":"# 2.4 Feature Selection:"}}