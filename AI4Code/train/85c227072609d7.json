{"cell_type":{"1dae1259":"code","d5a54602":"code","7df3cd93":"code","4e696cb8":"code","635702f5":"code","d6b2d2aa":"code","b9dcfdfa":"code","23cd82e1":"code","e3124185":"code","0c9564fa":"code","8a336574":"code","294b1d2e":"code","dfb83430":"code","63193857":"code","fccd0f72":"code","42cf3c00":"code","c78bae0f":"code","8430ccc5":"code","f2c5b2d9":"code","e2af8935":"code","4e7b0dba":"code","1181d9df":"code","0d5bdacf":"code","55356e00":"code","b6dda918":"code","f2e0e9d7":"markdown","5cf9251a":"markdown","1bbb361e":"markdown","6a707217":"markdown","dc0c9fab":"markdown","d6ad175d":"markdown","bbc4fc56":"markdown","6f44ea10":"markdown","a19d42b2":"markdown","0ee6ae8e":"markdown","dbc78493":"markdown","5960ddf7":"markdown","6e72418e":"markdown","d4d635d1":"markdown","40c4bc53":"markdown","4803c349":"markdown","cd0fd8f0":"markdown","9370315a":"markdown","96dfd42f":"markdown","c26de3e5":"markdown","6b998450":"markdown","a1ee9e54":"markdown","aeffa487":"markdown","8f734529":"markdown","d3f4fcb1":"markdown","2f6f2768":"markdown","05c42008":"markdown","802cfc3b":"markdown","c6462f41":"markdown","96a234ed":"markdown","7c5a5555":"markdown","b71b7772":"markdown","5360f49e":"markdown","51e39110":"markdown"},"source":{"1dae1259":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","d5a54602":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport tokenization\nfrom plotnine import *\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom collections import Counter\n\nfrom sklearn.model_selection import StratifiedShuffleSplit","7df3cd93":"%%time\nimport tensorflow_hub as hub\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","4e696cb8":"input_dir = '\/kaggle\/input\/nlp-getting-started\/'\noutput_dir = '\/kaggle\/working\/'\ncheckpoint_dir = '\/kaggle\/input\/checkpoint-nlp-disaster\/'\nword1vec_dir = '\/kaggle\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin'\n\ndf_model, df_submission = pd.read_csv(input_dir + 'train.csv'), pd.read_csv(input_dir + 'test.csv')\ndf_model.head()","635702f5":"df_model['target'].mean()","d6b2d2aa":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","b9dcfdfa":"def tokenize_df(df_in):\n    \n    df_out = df_in.copy()\n    df_out['str_tokens'] = df_out['text'].apply(tokenizer.tokenize)\n    \n    apply_cls_sep = lambda X: ['[CLS]'] + X + ['[SEP]']\n    \n    df_out['str_tokens'] = df_out['str_tokens'].apply(apply_cls_sep)\n    df_out['id_tokens'] = df_out['str_tokens'].apply(tokenizer.convert_tokens_to_ids)\n    df_out['n_tokens'] = df_out['id_tokens'].apply(len)\n    \n    return df_out\n\ndf_model = tokenize_df(df_model)\ndf_submission = tokenize_df(df_submission)\ndf_model.head()","23cd82e1":"def define_mask(df_in, window_size=140):\n    df_out = df_in.copy()\n    df_out['pad_size'] = window_size - df_out['n_tokens']\n    \n    masked_list, masked_id_token = list(), list()\n    for i in range(0, df_out.shape[0]):\n        masked_list.append([0] * df_out['pad_size'][i] + [1] * df_out['n_tokens'][i])\n        masked_id_token.append([0] * df_out['pad_size'][i] + df_out['id_tokens'][i])\n        \n    df_out['masked_list'], df_out['masked_id_token'] = masked_list, masked_id_token\n    return df_out\n    \ndf_model = define_mask(df_model)\ndf_submission = define_mask(df_submission)\ndf_model.head()","e3124185":"def build_minimal_model(bert_layer, max_len=140):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","0c9564fa":"model = build_minimal_model(bert_layer, max_len=140)\nmodel.summary()","8a336574":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.8, random_state=42)\nfor train_index, test_index in sss.split(df_model, df_model['target']):\n    df_model = df_model.iloc[test_index.tolist() + train_index.tolist(), :]","294b1d2e":"def extract_inputs(df_in, max_len=140):\n    return np.array(df_in['masked_id_token'].tolist()),\\\n           np.array(df_in['masked_list'].tolist()),\\\n           np.array([[0] * max_len for i in range(0, df_in.shape[0])])\n\ntrain_input = extract_inputs(df_model)\ntest_input = extract_inputs(df_submission)\n\ntrain_labels = df_model['target']","dfb83430":"flag_fit_models = False","63193857":"if flag_fit_models:\n    train_history_minimal_model = model.fit(\n        train_input, train_labels,\n        validation_split=0.2,\n        epochs=3,\n        batch_size=16,\n        callbacks=[EarlyStopping(restore_best_weights=True, patience=1)])\nelse:\n    train_history_minimal_model = pd.read_csv(checkpoint_dir + \n                                              'train_history_minimal_model_no_label.csv')","fccd0f72":"if flag_fit_models:\n    pd.DataFrame.from_dict(train_history_minimal_model.history).\\\n        to_csv(output_dir + 'train_history_minimal_model_no_label.csv', index=False)\n\n    pd.DataFrame({\n        'id': df_submission['id'],\n        'target': [X[0] for X in list(model.predict(test_input))]\n    }).to_csv(output_dir + 'predictions_model_no_sentence_labels.csv', index=False)\nelse:\n    df_out = pd.read_csv(checkpoint_dir + 'predictions_model_no_sentence_labels.csv').loc[:, ['id', 'target']]\n    df_out['target'] = df_out['target'].apply(lambda X: 1 if X >= 0.5 else 0)\n    df_out.to_csv(output_dir + 'predictions_model_no_sentence_labels.csv', index=False)","42cf3c00":"df_eda = pd.DataFrame(dict(all_texts=df_model['text'].str.lower().str.split(), target=df_model['target'].tolist()))\ndf_eda.head()","c78bae0f":"df_eda['hashtag_count'] = df_eda['all_texts'].apply(lambda X: sum([1. for Y in X if Y[0] == '#']))\ndf_eda['at_count'] = df_eda['all_texts'].apply(lambda X: sum([1. for Y in X if Y[0] == '@']))\ndf_eda['exclamation_count'] = df_eda['all_texts'].apply(lambda X: sum([1. for Y in ''.join(X) if Y == '!']))\ndf_eda['interrogation_count'] = df_eda['all_texts'].apply(lambda X: sum([1. for Y in ''.join(X) if Y == '?']))\n\ndf_eda.head()","8430ccc5":"df_melt_eda = pd.melt(df_eda, id_vars=['target'], value_vars=['hashtag_count', 'at_count', 'exclamation_count'])\ndf_melt_eda['bigger_than_0'] = df_melt_eda['value'].apply(lambda X: 1. if X > 0 else 0)\ndf_melt_eda.head()","f2c5b2d9":"ggplot(aes(x='value', fill='variable'), data=df_melt_eda) + geom_bar() + facet_grid('target~variable')","e2af8935":"df_bigger_than_0_prob = df_melt_eda.groupby(['target', 'variable']).\\\n    agg({'bigger_than_0': ['mean', 'count']})\n\ndf_bigger_than_0_prob = df_bigger_than_0_prob['bigger_than_0'].reset_index()\ndf_bigger_than_0_prob['target'] = df_bigger_than_0_prob['target'].apply(lambda X: 'Disaster!' if X > 0 else 'Fake...')\n\ndf_bigger_than_0_prob['std'] = np.sqrt((df_bigger_than_0_prob['mean'] *\\\n                                       (1 - df_bigger_than_0_prob['mean'])) \/\\\n                                       df_bigger_than_0_prob['count'])\ndf_bigger_than_0_prob['mean']\ndf_bigger_than_0_prob","4e7b0dba":"ggplot(df_bigger_than_0_prob, aes(x='variable', fill = 'target', group='target')) +\\\n    geom_bar(aes(y='mean'), position = 'dodge', stat='identity', color='black') +\\\n    geom_errorbar(aes(ymin='mean - 3 * std', \n                      ymax='mean + 3 * std'), \n                  position = position_dodge(0.9), width=0.5) +\\\n    geom_point(aes(y='mean'), position = position_dodge(0.9), size=5) +\\\n    xlab('Variable') + ylab('P[Variable > 0]') + ggtitle('Features Influence over Disaster Tweets')","1181d9df":"df_model['n_characters'] = df_model['text'].apply(lambda X: len(''.join(X.split())))\ndf_model['n_']","0d5bdacf":"def index_to_row(df_in):\n    df_in['Epoch'] = df_in.index\n    return df_in\n\nhistory_extra_features = index_to_row(pd.read_csv(checkpoint_dir + 'train_history_extra_features.csv'))\nhistory_minimal_model_no_label = index_to_row(pd.read_csv(checkpoint_dir + 'train_history_minimal_model_no_label.csv'))\nhistory_minimal_model_with_label = index_to_row(pd.read_csv(checkpoint_dir + 'train_history_minimal_model_with_label.csv'))\nhistory_with_correction = index_to_row(pd.read_csv(checkpoint_dir + 'train_history_with_correction.csv'))","55356e00":"history_extra_features['Model'] = 'Extra Features'\nhistory_minimal_model_no_label['Model'] = 'Minimal'\nhistory_minimal_model_with_label['Model'] = 'Label'\nhistory_with_correction['Model'] = 'Auto Correction'\n\ndf_histories = pd.concat([\n    history_extra_features,\n    history_minimal_model_no_label,\n    history_minimal_model_with_label,\n    history_with_correction\n], axis=0, ignore_index=True)\n\ndf_histories.head()","b6dda918":"ggplot(df_histories, aes(x='Epoch', y='val_accuracy', color='Model', group='Model')) +\\\n    geom_line() + geom_point() + labs(title='Comparing Different Models') +\\\n    ylab('Validation Accuracy')","f2e0e9d7":"Thinking a little bit more about the structure of the BERT Neural Network, I got an idea: the BERT neural network also takes as inputs the segment ID's of each word. So, the last solution that I tried consists in just concatenate the location and the keyword of each row to the text, after zero-pad them with zeros.\n\nIn this new model, I didt not not use $0$ as the ID for all tokens:\n\n- The segment ID of the location tokens would be equal to 0\n- The segment ID of the keyword tokens would be equal to 1 and\n- The segment ID of the original text would be equal to 2\n\nBut I got the same \"problem\": I didn't get any improvement with this model. Let's compare the obtained results in the next section.","5cf9251a":"We will also check whether or not each of these counts are bigger than $0$:","1bbb361e":"We can check the structure of our Neural Network with the summary function:","6a707217":"And importing from the TensorFlow Hub the BERT model in the form of a Keras Neural Network layer:","dc0c9fab":"It doesn't seem to let us assure that there is a relationship among these characters and the target value. Let's try a different approach: let's see the proportions of tweets with no spetial characters and with at least $1$ special character:","d6ad175d":"Loading the tokenizer, which will be used to transform our words in ID numbers that will be processed by the BERT model (we have to assure that all words are in the lower case form in our problem):","bbc4fc56":"Importing the used libraries:","6f44ea10":"I'm working with a flag that says whether or not I will run the models. If it's false, then we will load the obtained results of the last execution (since we are working with deep learning here, it's really easy to overflow the GPU memory)","a19d42b2":"Since we are studying a proportion, we can visually apply a hypothesis test: the variance of a proportion variable is given by:\n\n$$\\sqrt{\\frac{p.(1-p)}{N}}$$\n\nIt happens because the mean of Bernouilli distributions follows a binominal distribution and we can take the binomial standard deviation formula to estimate the standard deviation of the mean, which will be plotted as error bars in the following plot:\n\n(if you didn't understand this part, it's ok! :) just continue to read and consider that the error bars must not cross one over another if we want to be sure that these features really have any influence over the target)","0ee6ae8e":"# 5. Conclusions\n\nThe conclusions, for this first public version of my notebook are:\n\n* It seems that it doesn't matter what you try to do: the minimal BERT model is aways better\n* Auto correction scripts also seem to not be useful at all: the maximum long path search algorithm from the Google Team's tokenizer script is capable to solve the problem of ortograph assuring a better final result\n* Special characters like #, !, @ and ? have an influence over the probability to have a fake news tweet but adding them to the BERT model didn't seem to improve the validation accuracy\n\nSo, my next step is to improve the EDA in a new version and try to write a stacked model.","dbc78493":"We will create a dataframe with the parameters that we want to plot. I will follow an R \"tidy\" format and I will use the ggplot library to show the results:","5960ddf7":"# 3. Comparing Models","6e72418e":"So, with this positive result, I tried to create a model that used these features as aditional inputs to the last Dense Layer of the minimal BERT model presented in the first section. But, as we will see in the **Section 3** (Comparing Models), the effect was not interesting.\n\nI also tried to write an autocorrector script using the concepts of [this notebook](https:\/\/www.kaggle.com\/cpmpml\/spell-checker-using-word2vec). It consists in three steps:\n\n1. Check the candidates that are similar to the words of the Word2Vec Google's Dictionary (we consider that 2 words are similar if we can transform the invalid word into a valid one with few insertions, deletions or permutations of characters)\n2. We take the most probable candidate (using the Word2Vec's inverse rank value as a proxy of the probability)\n3. The most likely candidate is used as correction\n\nI tried to apply this function to the words before the tokenization steps. The results didn't get better as we will see in the next section.","d4d635d1":"# Tweets Disaster Predictions\n\n![DISASTERS!](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSqsnrxzRLZFSSg9SNtj9SM2eAHoBJIvqCBWInda9hfuVMXXLu1)\n\n## A Basic Solution using BERT Deep Learning Google's Model\n\nHello everbody. The aim of this notebook is not to only share my solution of the NLP Disaster Prediction problem. I also want to show all the results that I got when I tried to improve the model.\n\nYou will se that all my tries were not better than the basic BERT and that's OK. We must remember that we are talking about Data **Science** and the \"Science\" part of the name is also composed by misleading results that can be reported to help in further studies :).\n\nSo, this guide is presented in the form of a really basic tutorial, as I usually like to do. It's also an opportunity to present the basic concepts of NLP and Transfer Learning to people who want to learn or review these concepts. So, let's start. But, before...\n\n**An Important Acknowledgement to xhlulu:** [His notebook was really, REALLY useful for me](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) - In his notebook, the minimal BERT model is presented and he shows that it's possible to get an excellent result by using the original Google's Article BERT model, with the same parameters.\n\nSo, this notebook is composed by $5$ steps:\n1. I show (again) the basic BERT model with a small guide to the concept of transfer learning and natural language processing, following the guideline proposed by xhulu.\n2. EDA (Exploratory Data Analysis) + Used Models\n3. Comparing the results of different models\n4. Conclusions","40c4bc53":"Well, let's continue our EDA and try to find more features:","4803c349":"Writting a function to add columns of tokenized sentences. The tokenized sentences will be used as inputs of our deep learning neural network:","cd0fd8f0":"Importing the saved histories:","9370315a":"If the flag is True, we output the new results:","96dfd42f":"Of course you can check the TensorFlow HUB website to find other models. You can, for example, take an alternative BERT version, the ALBERT, which is a smaller version of the model that is capable to also get nice results with a higher training speed.\n\n![TensorHub](https:\/\/miro.medium.com\/max\/1600\/1*3DSjKsIW08f3LBfGiNeovA.png)\n\nWe are working here with the concept of **Transfer Learning**: we take a pre-trained big deep learning neural network and add some layers to the original model (ocasionally we can also remove some layers of the pre-trained neural network) and with the adaptation of the learner we fit it to solve a new problem.\n\nIt's common to use this technique in Image Classification tasks and Natural Language Processing problems.\n\n![TransferLearning](https:\/\/www.researchgate.net\/profile\/Joseph_Lemley\/publication\/316748306\/figure\/fig2\/AS:491777640669185@1494260334992\/Illustration-of-transfer-learning-concept-where-the-rst-layers-in-network-A-and-network-B.png)","c26de3e5":"# 1. Construction of a Basic BERT Model\n## With a Fast Transfer Learning Guide","6b998450":"We have no choice: we will keep using the minimal Google's BERT model. So, for now, my submission will use the BERT original model with just one Dense layer. Anyway, I still have other plans to try to improve the estimator: I will try to create smaller models and blend them with the output of the obtained neural network. Maybe it can help us to achieve better results.","a1ee9e54":"The model results will be explored ahead in the models comparison section (**Section 5**). Before that, let's start our exploratory data analysis that lead me to alternative models.","aeffa487":"# 2. Exploratory Data Analysis (EDA) + Used Models","8f734529":"If the flag is false, we load the last results, otherwise, we start the fitting process:","d3f4fcb1":"And, before start fitting our model, let's split the training and testing set with a stratified shuffle and split. When I say \"stratified\" I mean that the percentual of zeroes and ones in the training and validation sets will be nearly the same so we work with balanced samples:\n\n![Stratification](https:\/\/doingsurveyresearch.files.wordpress.com\/2013\/01\/strata-sample.gif)","2f6f2768":"We start by taking the tokenization Google's Team script:","05c42008":"Here comes an important part: we know that our sentences will aways have less than $140$ words (which is the Twitter limit  before the last update). But some sentences will have less than $140$ words. For example: if I say \"deep learning is cool\", we will have just $4$ words.\n\nIn this case, after the tokenization process, we will have something like: (0, 0, 0,..., 0, 123, 321, 13, 51). With $140 - 4$ zeros and $4$ number at the end representing the tokens of the words \"deep\", \"learning\", \"is\" and \"cool\", respectively. So, we will not update the neural network weights that are linked to non used word positions.\n\nIt's the concept of **masking** and we will tell the neural network with an argument that is equal to $1$ in positions of used words and equal to zero in unused positions. It's much similar to the concept of IP Masking (if you are familiar with the area of networking).\n\nSo, let's define a function to create those masks after zero left padding the the tokens:","802cfc3b":"Here is a function to extract the model's inputs:","c6462f41":"Let's start! I aways start my storytelling scripts by loading the dataframes and taking a fast look at the training set:","96a234ed":"Plotting the number of special characters for each target value:","7c5a5555":"And we are ready to define our basic BERT model:","b71b7772":"The first auxiliary features that called me attention were: the number of hashtags in the sentence, the number of mentions (at's), the number of exclamation characters and the number of interrogations. These characters are eliminated from the text during the tokenization procedure and that's why I thought that, somehow, it would be possible to improve the model by adding these features to the input of the last Keras layer of the BERT model.\n\nIt makes sense since, in a certain way, these values are a proxy of the sentiment of the person who wrote the tweet:","5360f49e":"Noting that the autospelling script didn't work, I just applied the tokenization algorithm over the locations and keywords without concerning with the correctness of the new strings that will be added to the model.\n\nWhy the autocorrection didn't improve the model? My hypothesis can be found in a comment of the tokenization script imported at the beggining of the notebook:\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n      \nSo, the code breaks invalid words in a sequence of common suffixes and prefixes in order to get the longest match to valid words. That concept seems to work really well with the BERT neural network structure:\n\n![BERT Structure](https:\/\/cdn-images-1.medium.com\/max\/800\/0*ViwaI3Vvbnd-CJSQ.png)","51e39110":"Is our dataset balanced? The answer is YES! We have a distribution of nearly $40\\% - 60\\%$ of zeros and ones in our target column. So, the accuracy metric may be useful to our purposes:"}}