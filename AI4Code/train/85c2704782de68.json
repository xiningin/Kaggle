{"cell_type":{"1cbc86a7":"code","f1ae4cb7":"code","104f3309":"code","8376f928":"code","1e38941e":"code","c179fba1":"code","6c368764":"code","3cc895e5":"code","8600c208":"code","60d310af":"code","b769da33":"code","9a374af7":"code","46a2fd0a":"code","1794fa06":"code","bdd51bc1":"code","b5ed8b50":"code","c94343c9":"code","6e0e4dd8":"code","b7a0aabc":"code","df8f583b":"code","8fe72f63":"code","7f5419ac":"code","7b0ad8be":"code","46b40ab1":"code","4f0bc02c":"code","07c9083d":"code","974377f0":"code","46714069":"code","70c2132d":"code","86aa1ce3":"code","4e86aaba":"code","51b3c4dd":"code","9026aba0":"code","8ce6fa84":"code","008afd82":"code","e3a829e9":"code","2da0bb03":"code","8e4402b4":"code","0c05b6df":"code","4b8482c2":"code","4159807c":"code","97e7e500":"code","1e5692a4":"code","3bc899d2":"code","937bd6db":"code","d647b2f7":"code","393176f5":"code","0f175eac":"code","2e2bad58":"code","2bf9a0c6":"code","21e00631":"code","ca61919e":"code","1a40e190":"code","9be84c29":"code","5ba0504b":"code","b126de60":"code","6a467bd5":"code","2a414db5":"code","68daed49":"code","68b63f01":"markdown","b08f99ca":"markdown","e7cf7abe":"markdown","18d0f6bf":"markdown","97a9f5b8":"markdown","d12fac23":"markdown","15351f3d":"markdown","3a3b5252":"markdown","0a525c5c":"markdown","4c33370e":"markdown","b0bf36cf":"markdown","bfd2be4f":"markdown","d616d438":"markdown","aac896f7":"markdown","7daf5263":"markdown","2afc232f":"markdown","fc9949d5":"markdown","ab05ee69":"markdown","1efa21ca":"markdown","4b04a9a3":"markdown","da97d64e":"markdown","c40bb841":"markdown","f2b1594d":"markdown","58bb34e3":"markdown","0572ee96":"markdown","1d6c977d":"markdown","bb1621f0":"markdown","cf2067b3":"markdown","f6d0f952":"markdown","0d4b6967":"markdown","a57c62ef":"markdown","033cc1a7":"markdown","ab081b7c":"markdown","fca5a692":"markdown","65ea426b":"markdown","d45b8620":"markdown","a0e5c468":"markdown","2a6fd221":"markdown","65b5cf9c":"markdown","a5650bf8":"markdown","7556c45d":"markdown","9028ccfa":"markdown","d4fe9d76":"markdown","d76aec39":"markdown","8f4339b2":"markdown","281825a2":"markdown","ed29716e":"markdown","a074b4fe":"markdown","6db3cf98":"markdown","5537e290":"markdown"},"source":{"1cbc86a7":"%%time\n# times the whole cell\n\n# import pandas\nimport pandas as pd\n\n# set the path to read Train.csv\npath = '..\/input\/Train.csv'\n\n# read the data into a pandas DataFrame\ndf_raw = pd.read_csv(path,\n                     low_memory=False, # use as much memory as necessary to figure out dtypes\n                     parse_dates=[\"saledate\"]) # read saledate column as datetime dtype","f1ae4cb7":"# dimensions (rows, columns)\ndf_raw.shape","104f3309":"# print 5 first rows\ndf_raw.head()","8376f928":"# create a function that displays up to a thousand rows and columns of a dataframe\n# pd.option_context(*args) is a context manager to temporarily set options in the `with` statement context\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","1e38941e":"# show the tail (last 5 rows)\n# view transposed\ndisplay_all(df_raw.tail().T)","c179fba1":"# generate descriptive statistics of all columns (including object columns)\ndisplay_all(df_raw.describe(include='all'))","6c368764":"#import numpy\nimport numpy as np\n\n# replacing SalePrice column values with the calculated natural logarithm of SalePrice values.\ndf_raw['SalePrice'] = np.log(df_raw.SalePrice)\n\n# show head of column\ndf_raw.SalePrice.head()","3cc895e5":"# DataFrame with a column removed\nindependent = df_raw.drop('SalePrice', axis=1)\n\n# select a Series from the DataFrame\ndependent = df_raw.SalePrice","8600c208":"# prints first 5 rows of the column\ndf_raw.saledate.head()","60d310af":"# import add_datepart\nfrom fastai.structured import add_datepart\n\nadd_datepart(df_raw, 'saledate')","b769da33":"add_datepart??","9a374af7":"# column labels of the DataFrame.\ndf_raw.columns","46a2fd0a":"# acces the selected [rows, columns]. View transposed.\ndf_raw.loc[:,['saleYear', 'saleMonth',\n       'saleWeek', 'saleDay', 'saleDayofweek', 'saleDayofyear',\n       'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end',\n       'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start',\n       'saleElapsed']].T","1794fa06":"# count dtypes in the dataframe\ndf_raw.dtypes.value_counts()","bdd51bc1":"# select numerical columns\ndf_raw.select_dtypes(['int64','float64']).head()","b5ed8b50":"# import train_cats\nfrom fastai.structured import train_cats\n\ntrain_cats(df_raw)","c94343c9":"??train_cats","6e0e4dd8":"# examine all the data types\ndisplay_all(df_raw.dtypes)","b7a0aabc":"# categories of this categorical\ndf_raw.UsageBand.cat.categories","df8f583b":"# encoded variables\ndf_raw.UsageBand.cat.codes.head()","8fe72f63":"# sets the categories to the specified new_categories, ordered in place.\ndf_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)","7f5419ac":"# make a copy for demonstration purposes\ndf_codes = df_raw.copy()\n\n# iterate through column name and column values and if column values are categories, replace them with their numeric code.\nfor name, column in df_codes.items():\n  if column.dtype.name == 'category':\n    df_codes[name] = column.cat.codes\n    \n# count dtypes in the dataframe\ndf_codes.dtypes.value_counts()","7b0ad8be":"# display all columns with the percentage of missing values (NaN)\ndisplay_all(df_raw.isnull().sum().sort_index()\/len(df_raw))","46b40ab1":"# categories of this categorical\nprint(df_raw.Hydraulics.cat.categories)\n\n# encoded variables\ndf_raw.Hydraulics.cat.codes.head(6)","4f0bc02c":"# select numerical columns\nnum_col = df_raw.select_dtypes(include='number')\n\n# display numerical columns with the percentage of missing values (NaN)\nnum_col.isnull().sum().sort_index()\/len(df_raw)","07c9083d":"# make a copy for demonstration purposes\ndf_imputed = df_raw.copy()\n\n# iterate through column name and column values and if column values are numerical, replace NA values with the column median.\nfor name, column in df_imputed.items():\n  if column.dtype == np.number:\n    df_imputed[name] = df_imputed[name].fillna(column.median())\n\n# select numerical columns\nnum_col = df_imputed.select_dtypes(include='number')\n\n# display numerical columns with the percentage of missing values (NaN)\nnum_col.isnull().sum().sort_index()\/len(df_raw)","974377f0":"# Save to feather file\ndf_raw.to_feather('df_raw')","46714069":"# import proc_df and the functions it depends on\nfrom fastai.structured import numericalize, fix_missing, proc_df\n\nX, y, nas_dict = proc_df(df_raw, 'SalePrice')","70c2132d":"??proc_df","86aa1ce3":"nas_dict","4e86aaba":"# acces columns with names containing some strings\nX.loc[:, X.columns.str.contains('MachineHoursCurrentMeter|auctioneerID')].head()","51b3c4dd":"# set the path to read the Valid.csv\npath = '..\/input\/Test.csv'\n\n# dimensions of the dataframe (rows, columns)\npd.read_csv(path).shape","9026aba0":"# create a function for splitting X and y into train and test sets of customizable sizes\ndef split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\n# validation set size: 12000.\nvalidation = 12000\n\n# split point: length of dataset minus validation set size.\nsplit_point = len(X)-validation\n\n# split X\nX_train, X_valid = split_vals(X, split_point)\n\n# split y\ny_train, y_valid = split_vals(y, split_point)\n\n# dimensions (row, columns) of X, y and X_valid\nX_train.shape, y_train.shape, X_valid.shape","8ce6fa84":"# create a function that takes the RMSE\ndef rmse(pred,known): return np.sqrt(((pred-known)**2).mean())\n\n# create a function that rounds to 5 decimal places (like kaggle leaderboard)\ndef rounded(value): return np.round(value, 5)\n\n# create a function that prints a list of 4 scores, rounded:\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid]\ndef print_scores(model):\n    RMSE_train = rmse(model.predict(X_train), y_train)\n    RMSE_valid = rmse(model.predict(X_valid), y_valid)\n    R2_train = model.score(X_train, y_train)\n    R2_valid = model.score(X_valid, y_valid)\n    scores = [rounded(RMSE_train), rounded(RMSE_valid), rounded(R2_train), rounded(R2_valid)]\n    print(scores)","008afd82":"# import the class\nfrom sklearn.ensemble import RandomForestRegressor\n\n# instantiate the model \nm = RandomForestRegressor(n_jobs=-1, # n_jobs is a performance parameter. -1 is to paralelize the computations across the number of CPU cores.\n                          random_state=17190)\n# fit the model with data and calculate the running time\n%time m.fit(X_train, y_train)\n\n# print a list of 4 scores:\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid]\nprint_scores(m)","e3a829e9":"# randomly sample 32000 thousand rows\nX_subset, y_subset, nas_dict = proc_df(df_raw, 'SalePrice', subset=32000)\n\n# split the train subset and discard the last 12000 rows: X_train [:20000], _ [20000:].\nX_train, _ = split_vals(X_subset, 20000)\n\n# split the target subset and discard the last 12000 rows: X_train [:20000], _ [20000:].\ny_train, _ = split_vals(y_subset, 20000)","2da0bb03":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_scores(m)","8e4402b4":"m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1, random_state=17190)\nm.fit(X_train, y_train)","0c05b6df":"# import the export_graphviz exporter \nfrom sklearn.tree import export_graphviz\n\n# import draw_tree method\nfrom fastai.structured import draw_tree\n\n# draw a random forest\ndraw_tree(m.estimators_[0], X_train, precision=3)","4b8482c2":"# remove the max_depth parameter (by default to None) so the nodes expand until they can't do it anymore. \nm = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1, random_state=17190)\n\nm.fit(X_train, y_train)\n\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid]\nprint_scores(m)","4159807c":"# 10 trees: it's the default number of trees in the random forest\nm = RandomForestRegressor(n_jobs=-1, random_state=171900)\nm.fit(X_train, y_train)\nprint_scores(m)","97e7e500":"# use a list comprehension to loop through the random forest and concatenates the predictions of each individual tree on a new axis\npreds = np.stack([t.predict(X_valid) for t in m.estimators_])\n\n# dimensions of the predictions (rows, columns)\npreds.shape","1e5692a4":"# print the first prediction for each of the ten trees [all tree rows, first prediction column]\nprint(preds[:,0])\n\n# print the mean of the first ten predictions\nprint(np.mean(preds[:,0]))\n\n# the first value of the validation set\ny_valid[0]","3bc899d2":"# appends the oob score to the list of scores if the model has the parameter\ndef print_scores(model):\n    RMSE_train = rmse(model.predict(X_train), y_train)\n    RMSE_valid = rmse(model.predict(X_valid), y_valid)\n    R2_train = model.score(X_train, y_train)\n    R2_valid = model.score(X_valid, y_valid)\n    scores = [rounded(RMSE_train), rounded(RMSE_valid), rounded(R2_train), rounded(R2_valid)]\n    if hasattr(m, 'oob_score_'): scores.append(m.oob_score_) # appends OOB score (if any) to the list \n    print(scores)","937bd6db":"# 10 trees\nm = RandomForestRegressor(n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\n\nprint('10 trees:')\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid, OOB score]\nprint_scores(m)\n\n# 30 trees\nm = RandomForestRegressor(n_estimators=30, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\n\nprint('\\n30 trees:')\n# [RMSE of X_train, RMSE of X_valid, R Squared of X_train, R Squared of X_valid, OOB score]\nprint_scores(m)","d647b2f7":"X, y, nas_dict = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(X, split_point)\ny_train, y_valid = split_vals(y, split_point)","393176f5":"# import set_rf_samples\nfrom fastai.structured import set_rf_samples\n\nset_rf_samples(20000)","0f175eac":"# 10 trees\nm = RandomForestRegressor(n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","2e2bad58":"# 40 trees\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","2bf9a0c6":"# 100 trees\nm = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_scores(m)\n\n# use a list comprehension to loop through the random forest and concatenates the predictions of each individual tree on a new axis\npreds = np.stack([t.predict(X_valid) for t in m.estimators_])\n\n# dimensions of the predictions (rows, columns)\npreds.shape","21e00631":"# import matplotlib\nimport matplotlib.pyplot as plt\n\n# import metrics module\nfrom sklearn import metrics\n\n# plot the calculated r^2 of the true values and the predicted values up to [:i] trees (looping through a range from 1 to 100). \nplt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(100)]);","ca61919e":"# baseline for comparison\n\n# 40 trees: 1 minimum samples per leaf (default)\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","1a40e190":"# 40 trees: 2 minimum samples per leaf\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","9be84c29":"# 40 trees: consider all columns per split\nm = RandomForestRegressor(n_estimators=40, max_features=None, n_jobs=-1, oob_score=True, random_state=17190)\nm.fit(X_train, y_train)\nprint_scores(m)","5ba0504b":"# import reset_rf_samples\nfrom fastai.structured import reset_rf_samples\n\n# use full bootstrap sample\nreset_rf_samples()","b126de60":"# baseline for comparison\n\nm = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","6a467bd5":"# 40 trees: 3 minimum samples per leaf\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","2a414db5":"# 40 trees: half columns per split\nm = RandomForestRegressor(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True, random_state=17190)\n%time m.fit(X_train, y_train)\nprint_scores(m)","68daed49":"# 40 trees: 3 minimum samples per leaf and half columns per split\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True, random_state=1719)\n%time m.fit(X_train, y_train)\nprint_scores(m)","68b63f01":"We need to **split** the **dependent variable** from the dataset.\n- Later on, we'll use a method from the **fastai library** , **`proc_df`**,  to do just that and more.\n- We show some alternative code below to illustrate the process.","b08f99ca":"Most of the **categorical variables are currently stored as strings** (objects).\n- Apart from being inefficient, it doesn't provide the **numeric coding** required for our model.\n\n- We'll use a method from the **fastai library** , **`train_cats`**,  to convert strings to pandas categories.","e7cf7abe":"Some of the individual tree's predictions are better than the others, but **the mean of them is actually good enough**.\n### Out-of-bag (OOB) score\nOut-of-bag (**OOB**) **error**  is another method of measuring the **prediction performance** of models utilizing **bagging**.\n- Out-of-bag samples are **samples not used** during training for **any given tree**.\n- For **every tree** in the forest there is **unseen data** available to make **new predictions** on.\n- For each **training sample**, calculates the **mean prediction error** using only the trees that were not trained on that sample.\n- **Evaluates** the model on the training set **without** needing a separate **validation set**.\n\nSources: [1](https:\/\/en.wikipedia.org\/wiki\/Out-of-bag_error), [2](http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html), [3](https:\/\/stackoverflow.com\/questions\/18541923\/what-is-out-of-bag-error-in-random-forests).\n\nThis also has the benefit of allowing us to **assess** whether **our model generalizes** even if we **only have** a **small amount of data** so want to avoid separating some out to create a validation set.\n\nWe can add one more parameter to our model, **`oob_score`**, to use out-of-bag samples to estimate the **R^2** on unseen data.\n- There is **minimum number of trees** to be used for any given data that are **necessary** to compute a reliable OOB error (or **sklearn warns** us).\n- We'll also update the `print_scores` function to **print** the  **OOB error** last.","18d0f6bf":"### Variance\nAnother way to reduce over-fitting is to increase the amount of **variation amongst the trees**.\n- We can do this by specifying **`max_features`**, which is the **number of features** to consider at **each split**.\n- This way not only randomly selects a **sample of rows** for each tree, but **also** randomly selects a **sample of columns** for each split.\n- This can be critical for creating variance when there are features that are so much predictive than others and all the trees are very similar to each other because they are doing similar splits.\n\nPossible good values to try:\n- None: use all columns.\n- 0.5: use half of the columns\n- 'sqrt' (default): use the square root of total number of columns.","97a9f5b8":"### Initial processing\n#### SalePrice\n- SalePrice is the **dependent variable**, the target.\n- Kaggle tells us that the evaluation metric for this competition is the **RMSLE** (root mean squared log error) between the actual and predicted auction prices.\n- Therefore we take the **log of the prices**, so that RMSE will give us what we need.","d12fac23":"# Introduction to Random Forests\n## The data\nFor [this competition](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data), you are **predicting the sale price of bulldozers sold at auctions**.\n\nThe data for this competition is split into three parts:\n\n- **Train.csv** is the training set, which contains data through the end of 2011.\n- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\nThe key fields are in train.csv are:\n\n- **SalesID**: the uniue identifier of the sale\n- **MachineID**: the unique identifier of a machine. A machine can be sold multiple times\n- **saleprice**: what the machine sold for at auction (only provided in train.csv)\n- **saledate**: the date of the sale\n\n### Exploring the data","15351f3d":"The method **`fix_missing`** has returned `nas_dict`, which is a dictionary with the name of the columns that had missing data.\n- From the function docstring: Fill missing data in a column of df with the median, and add a {name}_na column which specifies if the data was missing (boolean).\n- Our model will **not be using it** but we can check it out.","3a3b5252":"# Lecture 1\nIt is recommended to [watch the video-lecture first](https:\/\/youtu.be\/CzdWqFTmn0Y), then follow the notebook.","0a525c5c":"We can check the **source code** of a function  by typing its name and **??** (before or after the name).","4c33370e":"There are a bunch of new columns. Let's take a look.","b0bf36cf":"# Random Forests\n## Base model\n### Validation set\nTo **avoid  overfitting** , we'll split the dataset to have two **separate training and validation sets**.\n- Since we are trying to predict new prices, we should pick the **latest samples** from our train set.\n- We'll use the **same size as** the **Kaggle** validation set, so when we evaluate our model it gives us an estimation of how will it perform in the **public leaderboard**.","bfd2be4f":"Finally, to fit this data to a random forest, we would need to **replace the text categories with their numeric codes**.\n- We'll use a method from the **fastai library** , **`proc_df`**,  to do just that and more.\n- We show some alternative code below to illustrate the process.","d616d438":"## Saving the progress\nWe will **save the dataset** in its current form so we can **save ourselves** the trouble of **repeating the process** of getting the data ready to be to be passed to a random forest in **future lessons**.\n- In kaggle, when we **commit** the kernel, everything that is writed to the current directory is saved as **output**.","aac896f7":"These four scores allows us to evaluate the accuracy of our model.\n- An **R Squared** of the validation set in the **high-80's** tells us that our model is significantly **better** than simply **predicting the average** price.\n- The large difference of the **RMSLE** of the training (**0.09**) and the validation sets (**0.25**) tells us that we're **over-fitting badly**, because the average error of the model predictions are much greater when dealing with data it hasn't been trained with.\n\n## Speeding things up\nIn order to make the model development process more **interactive**, we need to make sure that the model runs in a reasonable time to be able to **make some changes and see the results fast**.\n- We''ll use the paramater **`subset`** from **`proc_df`** that takes a random subset of the selected size from the dataframe.\n- We'll randomly sample **32,000** from the original 401,125 rows.\n- We'll use the **same validation set** as before.\n- To make sure our training set doesn't overlap with the dates of the validation set we'll **split again the subset** we've taken and **discard the last 12,000** rows.","7daf5263":"### Experimenting\nWe'll revert to using a full bootstrap sample in order to experiment with these hyperparameters further and to see their full impact.\n- The models will **train much slower**.\n- For more information, you can check this post on stackoverflow: [Practical questions on tuning Random Forests](https:\/\/stats.stackexchange.com\/questions\/53240\/practical-questions-on-tuning-random-forests).","2afc232f":"Since each additional tree allows the model to see more data, this approach can make **additional trees more useful without needing additional fine-tuning**.","fc9949d5":"# Lecture 2\nIt is recommended to [watch the video-lecture first](https:\/\/youtu.be\/blyXCk4sgEg), then follow the notebook.","ab05ee69":"If we look at the numeric columns, we can se that **most of them** are not cotinuous but **categorical**.\n- Even though it's not ideal that they stay that way, **random forest** works fine with it so there is **no problem**.","1efa21ca":"#### Categorical Data\nAs in most cases when dealing with datasets, this dataset contains a mix of **continuous** and **categorical** variables.\n- We need all of them to be **numeric** so they can be used by the model.\n- We must make the necessary changes (**feature engineering**).","4b04a9a3":"We'll grab the **predictions for each individual tree**, and look at one example.","da97d64e":"#### SaleDate\nWe already know that one of the features is a date and we've explicitly told pandas to read it as  **datetime dtype**.","c40bb841":"## Pre-processing\nWe'll use a method from the **fastai library** , **`proc_df`**,  to get the dataset ready for the random forest.\n- We'll **replace categories** with their numeric codes.\n- **Handle missing** continuous **values**.\n- **Split the dependent variable** into a separate variable.","f2b1594d":"We'll use the same subset size as before: **20,000** random samples, but this time will be a different subset for each tree.","58bb34e3":"The **OOB** is **better** than the **R Squared** of the validation set because the validation set is a **different time period** entirely, while the OOB is calculated on random samples of the same time period as the training set.\n- In this case the **validation set is much harder to predict** and the R Squared and will consistently be lower than the OOB.\n\n## Hyperparameter tuning\n### Subsampling\nEarlier we used **subsampling** to speed up the analysis, which basically consist in **limiting** the total **amount of data** that our **model can access** so it **trains faster**. There is a **better way** to do it that is actually **also** one of the easiest ways to **avoid overfitting**.\n- Rather than use **one random subset** of the data for our **entire model** (and for all its trees), we'll use a **different random subset per tree**.\n- **Given enough trees**, and enough random subsets, the model will **eventually** be able to **train with** much of **all the available data**.\n- Each **individual tree** it'll be **just as fast** as if we were **using one subset** for all the trees, **like** we did **before**.\n\nWe'll use a method from the **fastai library** , **`set_rf_samples`**, to do just that.\n- From the documentation: \"**changes Scikit learn's random forests** to give each tree a random sample of n random rows\".\n- It is **not compatible** with **OOB score** right now, so will simply do without it.\n- To turn off `set_rf_samples` to use random forest in the normal way, it's necessary to call **`reset_rf_samples`**.\n\nFirst we must **return** to using our **full dataset** so the **subsampling** is done on the **entire data**.","0572ee96":"Anyhow, now the **dataset is ready** to be to be passed to a **random forest**.","1d6c977d":"For the **numerical data**, we need to **replace** the **missing values** with the **median** (also called median imputation).\n- We'll use a method from the **fastai library** , **`proc_df`**,  to do just that and more.\n- We show some alternative code below to illustrate the process.","bb1621f0":"#  Intro to Machine Learning\nThis is a free course offered by [fast.ai](http:\/\/www.fast.ai\/) (currently [unlisted](http:\/\/forums.fast.ai\/t\/another-treat-early-access-to-intro-to-machine-learning-videos\/6826)). There's a github [repository](https:\/\/github.com\/fastai\/fastai\/tree\/master\/courses\/ml1).\n\n## About this course\nSome machine learning courses can leave you confused by the enormous range of techniques shown and can make it difficult to have a practical understanding of how to apply them.\n\nThe good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n\n- **Ensembles of decision trees** (i.e. Random Forests and Gradient Boosting Machines), mainly for **structured data** (such as you might find in a database table at most companies)\n- **Multi-layered neural networks learnt with Stochastic Gradient Descent** (SGD) (i.e. shallow and\/or deep learning), mainly for **unstructured data** (such as audio, vision, and natural language)\n\n### The lessons\nIn this course we'll be learning about:\n- **Random Forests** \n- **Stochastic Gradient Descent**.\n- **Gradient Boosting** \n- **Deep Learning**\n\n### The dataset\nWe will be teaching the course using the [Blue Book for Bulldozers Kaggle Competition](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers): \n- \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n\n### Note:\nThese are personal notes. For the original code, check the github repository of the course. Also, I will be importing things as I need them.\n","cf2067b3":"### Number of trees\nWe'll create a graphic to see what happens to the **R Squared** when using **1 tree up to 100 trees**.\n- We'll **store** the **predictions** of each tree in a **numpy array** like we did before for our base model.\n- We'll **loop** through the **predictions** of each tree to **plot the change** in **R Squared** as we add more trees.","f6d0f952":"Now we can **train our model with 20,000 randomly chosen samples** from the total 401,125 rows and we can be sure that we're not cheating because the validation set is a different set entirely.","0d4b6967":"Now we can display all the rows and columns whenever we need it.\n- For readability, we'll use the **.T attribute**, equal to the .transpose() method, which cause the rows and columns to exchange places.\n- Like so, we have to **scroll down** instead of sideways to see all the columns.\n","a57c62ef":"The shape of this curve clearly shows that the **model improves** the **more trees** we use, but it also suggests that adding more trees is beneficial **up to a point**, because it **flattens out** the closer it gets to 100 trees.\n- There is **no lineal relationship** between number of trees and our evaluation metric score.\n- The **number of trees** we use can **vary wether** we are:\n - **developing the model**, we'll propably use less trees but do lots of iterations, or\n - **finishing the model**, in that case, we'll probably use as much trees as our processor can handle to fit the model with all the available data and using the best found parameters.\n- **Without** our **subsampling** approach, building **lots of trees** on **lots of data** can be computationally expensive and very **time comsuming**.\n\n### Depth of the trees\nAnother way to reduce over-fitting is to grow our trees **less deeply**.\n- We can do this with **`min_samples_leaf`** parameter, which requires some minimum number of rows in every leaf node (by default 1).\n- For each tree there will be less levels and **less decisions** being made so it will result in simpler models.\n- The predictions are made by **averaging more samples** in the leaf node.\n- This can make the random forest to **generalize better**.\n\nPossible good values to try:\n- 1, 3, 5, 10, 25, 100","033cc1a7":"This is a **two step process** for being able to **fit categorical** variables to our model: first we convert **strings to categories**, then **categories to codes**\/numbers.\n- The result is that all variables are recoded in **only one column** using different integers for different categories, which is **useful for** columns with **lots of possible values**. \n- An **alternative** to this would be to **get dummy variables** for the categorical variables, but it would resolve it by recoding all the variables in **different columns** with **zeroes** and **ones**.\n\n#### Missing Data\nHandling missing data is important as **many** machine learning **algorithms do not support** data with **missing values** ([source](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/)).\n\nTypically, random forest methods\/packages encourage **two ways** of handling missing values ([source](https:\/\/medium.com\/airbnb-engineering\/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba)): \n- **Drop data points** with missing values (not recommended)\n- **Fill** in missing values with the **median** (for numerical values).","ab081b7c":"A tree consist of a **sequence of binary decisions** or splits.\n- Each step **partitions the data** into two subsets.\n- Each split point is a decision to split the data on a **particular variable** (feature) and in a **particular location** (value).\n- **All possible split points are evaluated** (all features and values).\n- The very **best split point is chosen** each time trying to **minimize the error** (for regression).\n- The error is calculated by **comparing the predicted values with the known values** of the subsets of a particular split point.\n\nEach box in the graphic that represents a split point consists of:\n- A **variable** and a **value** to split on.\n- **MSE** quantifies the error, the lower the better.\n- **Samples** is the size (rows) of the subset.\n- The **average** of the **target** value (log of price) of the subset.\n\nSources: [1](https:\/\/machinelearningmastery.com\/classification-and-regression-trees-for-machine-learning\/), [2](https:\/\/infocenter.informationbuilders.com\/wf80\/index.jsp?topic=%2Fpubdocs%2FRStat16%2Fsource%2Ftopic47.htm).\n\nLooking at our little tree, we can see that we **start with 20,000 rows** (the root node),  **split only three times** (trying to minimze the MSE) and end up with **many subsets of different sizes** (leaf nodes), which are the result of the last split.\n\nIf we create the largest (**deepest**) tree possible, it'll **keep splitting until each leaf node has only one sample**. Such tree would learn all there is to learn about the training data but would be a model that would **not be able to generalize a pattern** from it. And that is exactly what overfitting is: to **learn the noise instead of the signal**. Let's see it in practice.","fca5a692":"For more information, we can again check the **source code**:","65ea426b":"We'll use the [random_state](https:\/\/www.kaggle.com\/questions-and-answers\/49890) parameter for reproducibility.","d45b8620":"So `add_datepart` converts a column of a DataFramefrom a **datetime64 to many columns containing the information from the date** (changes occur inplace).","a0e5c468":"Pandas switchs to **truncate view** when there are too many rows and columns, but we can change the display options if we want to see all of them.","2a6fd221":"Now we'll **take a look inside** this random forest simplified to a **single small deterministic tree**.\n- We'll use a method fromt the **fastai library**, **`draw_tree`**, to draw a representation of the random forest in IPython.\n- This method uses the function **`export_graphviz`**, which generates a GraphViz representation of the decision tree.","65b5cf9c":" **What's in a date is one of the more important pieces of feaute engineering you can do** (i.e. Was it a holiday? Was it raining?).\n\n- We'll use a method from the **fastai library**, **`add_datepart`**, that extracts particular date fields from a complete datetime for the purpose of constructing categoricals. \n- You should always consider this **feature extraction** step when working with date-time dtypes. Without expanding your date-time into these additional fields, you can't capture any trend\/cyclical behavior as a function of time at any of these granularities.","a5650bf8":"### Model evaluation\n#### RMSE\nRemember the evaluation metric that Kaggle is going to use for this competition is the Root Mean Squared Log Error (**RMSLE**) between the actual and predicted auction prices.\n- Because we **already took the log** of the prices, we can use the Root Mean Squared Error (RMSE) instead.\n\nThe **RMSE** is an evaluation metric that expresses the **average error of the model predictions** by comparing the predicted values with the actual known values.\n- The score can range from **0** (best score possible, never achieved in practice) to **\u221e**, so the **lower the score the better**.\n\n#### R Squared\nThe coefficient of determination R^2 (**R squared**) is an evaluation metric for regression problems that essentially tells us **how good is our model** compared with **a model that just predicts the average** of the target (saleprice) all the time .\n- The **best** possible score is **1**.\n- The score of the **average** model is **0**.\n- A **negative** score means the model is even **worse** than simply predicting the average of the target.","7556c45d":"Indeed it has changed inplace all columns that contained **strings to category dtypes**.\n- It doesn't make the DataFrame look different but behind the scenes all string are encoded with **integers mapped to the strings**.\n- Let's look at UsageBand, our first category column.","9028ccfa":"There are 10 sets of predictions (trees) with 12,000 values (predictions), which corresponds to the size of the validation set.\n- Let's see what the **first prediction of each tree** does look like and how they **compare** with the actual value **when averaged out**.","d4fe9d76":"Well, that was fast.\n## Single tree\nTo **understand** why we were overfitting, we'll create a **random forest** so simple that we can actually **take a look inside**.\n- With the parameter `n_estimators`, we'll create a forest with **only one tree**.\n- With the parameter `max_depth`, we'll create a forest which trees **split only three times**.\n- With the parameter `bootstrap`, we'll **avoid using randomly generated training sets**.","d76aec39":"The scores are **great in the training set**: the RMSE is 0 and the R Squared is 1 (the best possible scores). This is because we can in fact **predict everything** from the training data.\n\nOur interest, of course, is to do **better in the validation set** to get more generalizable results and for that we need to use **multiple trees** and some kind of **model averaging approach** .\n## Bagging\n### Intro to bagging\nBootstrap aggregating (**Bagging**) is a way of doing repeated statistical analyses on the same data and combining them to form a single result.\n- The predictions that are combined are all different thanks to the use of **resampling** (bootstrap samples).\n- **Bootstraping** is a way of generating random non-repeated samples from a dataset.\n- The samples have the **same size** of the original dataset **but** they are all **different** to each other because each time there are rows that are left out and rows that appear more than once ([samples with replacement](https:\/\/www.emathzone.com\/tutorials\/basic-statistics\/sampling-with-replacement.html)).\n- These random differences make each sample unique so each model build on that sample can be different.\n\nSo bagging is a way of taking **many** training sets **from the single** original one and build a different prediction model using each training set in order to **average** the resulting **predictions** ([source](http:\/\/www2.stat.duke.edu\/~rcs46\/lectures_2017\/08-trees\/08-tree-advanced.pdf)).\n\nA **random forest** is simply a way of **bagging trees**.\n- It combines the **unique predictions** of different trees.\n- Each tree is trained on a **randomly generated training set**.\n- It grows **deep trees** that overfit their individual non-repeated subsets.\n- The prediction **error** of the trees are **random** so the **average of the predictions doesn't overfit**.\n\nSources: [1](https:\/\/onlinecourses.science.psu.edu\/stat857\/node\/181\/), [2](https:\/\/machinelearningmastery.com\/bagging-and-random-forest-ensemble-algorithms-for-machine-learning\/).\n\nTo learn about bagging in random forests, let's look at our **base model** again.","8f4339b2":"The set Kaggle used for creating the public leaderboard has roughtly **12,000 samples**, so that is the size we'll use for our validation set.","281825a2":"We can **specify the order** to use for categorical variables if we wish:","ed29716e":"Pandas has handled all **missing values** in the **categories** automatically by encoding  them with a **minus one**.","a074b4fe":"So we know the columns MachineHoursCurrentMeter' and 'auctioneerID' had missing values and according to the docstring, `fix_missing` has added **new columns** which specifies if the data was missing. Let's see.'","6db3cf98":"It's a relatively **large dataset**.","5537e290":"We can again check the **source code** of the function for more information."}}