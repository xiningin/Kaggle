{"cell_type":{"7f2e58b7":"code","df480b52":"code","e19ea222":"code","c5797b45":"code","a9e71d9b":"code","153dc069":"code","fa3249f0":"code","a142e706":"code","098d3dad":"code","87a85925":"code","b124b753":"code","c953c53e":"code","4d2ba9cd":"code","e62a47bf":"code","095fa3c4":"code","cfc483d4":"code","5d1c1cb2":"code","3ac6335f":"code","6c370534":"code","ac75ea3b":"code","7831cef4":"code","2c718f78":"code","e3e32859":"code","0777c2ef":"code","8c812b96":"code","5dca00bf":"markdown","2e710188":"markdown","2f253742":"markdown","6143e999":"markdown","c8656bf4":"markdown","f439cff5":"markdown","c64a378e":"markdown","6ea5da5e":"markdown"},"source":{"7f2e58b7":"!pip install -q \"monai-weekly[gdown, nibabel, tqdm, itk]\"","df480b52":"import os\nimport shutil\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\n\nfrom monai.apps import download_and_extract\nfrom monai.config import print_config\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\nfrom monai.transforms import *\nfrom monai.data import Dataset, DataLoader\nfrom monai.utils import set_determinism\n\n#print_config()","e19ea222":"data_dir = '..\/input\/digit-recognizer\/train.csv'\ntest_dir = '..\/input\/digit-recognizer\/test.csv'\nclass_names = list(range(10))\nnum_class = len(class_names)","c5797b45":"data=pd.read_csv(data_dir)\ntest=pd.read_csv(test_dir)","a9e71d9b":"image_label_list=data.iloc[:,0]\nimage_file_list=data.iloc[:,1:]","153dc069":"num_total = len(data)\nimage_width, image_height = 28,28\n\nprint('Total image count:', num_total)\nprint(\"Image dimensions:\", image_width, \"x\", image_height)\nprint(\"Label names:\", class_names)","fa3249f0":"plt.subplots(3,3, figsize=(8,8))\nfor i,k in enumerate(np.random.randint(num_total, size=9)):\n    im=image_file_list.iloc[k]\n    arr = np.array(im).reshape(28,28)\n    plt.subplot(3,3,i+1)\n    plt.xlabel(class_names[image_label_list[k]])\n    plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\nplt.tight_layout()\nplt.show()","a142e706":"plt.subplots(3,3, figsize=(8,8))\nfor i,k in enumerate(np.random.randint(num_total, size=9)):\n    im=image_file_list.iloc[k]\n    arr = np.array(im).reshape(28,28).astype('uint8')\n    arr2 = cv2.resize(arr,dsize=(56,56),interpolation=cv2.INTER_LINEAR)\n    plt.subplot(3,3,i+1)\n    plt.xlabel(class_names[image_label_list[k]])\n    plt.imshow(arr2, cmap='gray', vmin=0, vmax=255)\nplt.tight_layout()\nplt.show()\n# use cv2.resize","098d3dad":"plt.subplots(3,3, figsize=(8,8))\nfor i,k in enumerate(np.random.randint(num_total, size=9)):\n    im=image_file_list.iloc[k]\n    arr = np.array(im).reshape(28,28).astype('uint8')\n    arr2 = np.resize(arr,(56,56))\n    plt.subplot(3,3,i+1)\n    plt.xlabel(class_names[image_label_list[k]])\n    plt.imshow(arr2, cmap='gray', vmin=0, vmax=255)\nplt.tight_layout()\nplt.show()\n# do not use np.resize","87a85925":"valid_frac = 0.2\ntrainX, trainY = [], []\nvalX, valY = [], []\n\nfor i in range(num_total):\n    rann = np.random.random()\n    if rann < valid_frac:\n        valX.append(image_file_list.iloc[i].tolist())\n        valY.append(image_label_list[i].tolist())\n    else:\n        trainX.append(image_file_list.iloc[i].tolist())\n        trainY.append(image_label_list[i].tolist())\n\ntestX = test  \nprint(\"Training count =\",len(trainX),\", Validation count =\", len(valX), \", Test count =\",len(testX))","b124b753":"trainX=np.array(trainX).reshape(-1,28,28).astype('uint8')\ntrainY=np.array(trainY)\n\ntestX=np.array(testX).reshape(-1,28,28).astype('uint8')\ntestY=np.zeros((len(testX)))        #### dummy data\n\nvalX=np.array(valX).reshape(-1,28,28).astype('uint8')\nvalY=np.array(valY)","c953c53e":"class SumDimension(Transform):\n    def __init__(self, dim=1):\n        self.dim = dim\n\n    def __call__(self, inputs):\n        return inputs.sum(self.dim)","4d2ba9cd":"class MyResize(Transform):\n    def __init__(self, size=(56,56)):\n        self.size = size\n    def __call__(self, inputs):\n        image2=cv2.resize(inputs,dsize=(self.size[1],self.size[0]),interpolation=cv2.INTER_CUBIC)\n        return image2","e62a47bf":"class Astype(Transform):\n    def __init__(self, type='uint8'):\n        self.type = type\n    def __call__(self, inputs):\n        return inputs.astype(self.type)","095fa3c4":"train_transforms = Compose([\n    #LoadImage(image_only=True),\n    ScaleIntensity(),\n    MyResize(),     #to use DenseNet121\n    AddChannel(),    \n    ToTensor()\n])\n\nval_transforms = Compose([\n    #LoadImage(image_only=True),    \n    ScaleIntensity(),\n    MyResize(),     #to use DenseNet121\n    AddChannel(),\n    ToTensor()\n])\n\nact = Activations(softmax=True)\nto_onehot = AsDiscrete(to_onehot=True, n_classes=num_class)","cfc483d4":"class MedNISTDataset(Dataset):\n\n    def __init__(self, image_files, labels, transforms):\n        self.image_files = image_files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        return self.transforms(self.image_files[index]), self.labels[index]","5d1c1cb2":"train_ds = MedNISTDataset(trainX, trainY, train_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=100, shuffle=True, num_workers=2)\n\nval_ds = MedNISTDataset(valX, valY, val_transforms)\nval_loader = DataLoader(val_ds, batch_size=100, num_workers=2)\n\ntest_ds = MedNISTDataset(testX, testY, val_transforms)\ntest_loader = DataLoader(test_ds, batch_size=100, num_workers=2)","3ac6335f":"device = torch.device('cuda:0')     #\"cuda:0\"\nmodel = DenseNet121(\n    spatial_dims=2,\n    in_channels=1,               \n    out_channels=num_class\n).to(device)\n\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), 1e-5)\nepoch_num = 20\nval_interval = 1","6c370534":"best_metric = -1\nbest_metric_epoch = -1\nepoch_loss_values = list()\nauc_metric = ROCAUCMetric()\nmetric_values = list()","ac75ea3b":"for epoch in range(epoch_num):\n    print('-' * 10)\n    print(f\"epoch {epoch + 1}\/{epoch_num}\")\n    model.train()\n    epoch_loss = 0\n    step = 0\n    \n    for batch_data in train_loader:\n        step += 1\n        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)      \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        print(f\"{step}\/{len(train_ds) \/\/ train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n        epoch_len = len(train_ds) \/\/ train_loader.batch_size\n        \n    epoch_loss \/= step\n    epoch_loss_values.append(epoch_loss)\n    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n\n    if (epoch + 1) % val_interval == 0:\n        model.eval()\n        with torch.no_grad():\n            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n            y = torch.tensor([], dtype=torch.long, device=device)\n            for val_data in val_loader:\n                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n                y = torch.cat([y, val_labels], dim=0)\n            y_onehot = [to_onehot(i) for i in y]\n            y_pred_act = [act(i) for i in y_pred]\n            auc_metric(y_pred_act, y_onehot)\n            auc_result = auc_metric.aggregate()\n            auc_metric.reset()\n            del y_pred_act, y_onehot\n            metric_values.append(auc_result)\n            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n            acc_metric = acc_value.sum().item() \/ len(acc_value)\n            if acc_metric > best_metric:\n                best_metric = acc_metric\n                best_metric_epoch = epoch + 1\n                torch.save(model.state_dict(), 'best_metric_model.pth')\n                print('saved new best metric model')\n                \n            print(f\"current epoch: {epoch + 1} current AUC: {auc_result:.4f}\"\n                  f\" current accuracy: {acc_metric:.4f} best AUC: {best_metric:.4f}\"\n                  f\" at epoch: {best_metric_epoch}\")\n            \nprint(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")","7831cef4":"plt.figure('train', (12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Epoch Average Loss\")\nx = [i + 1 for i in range(len(epoch_loss_values))]\ny = epoch_loss_values\nplt.xlabel('epoch')\nplt.plot(x, y)\nplt.subplot(1, 2, 2)\nplt.title(\"Validation: Area under the ROC curve\")\nx = [val_interval * (i + 1) for i in range(len(metric_values))]\ny = metric_values\nplt.xlabel('epoch')\nplt.plot(x, y)\nplt.show()","2c718f78":"model.load_state_dict(torch.load('best_metric_model.pth'))\nmodel.eval()\ny_true = list()\ny_pred = list()\n\nwith torch.no_grad():\n    for test_data in test_loader:\n        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n        pred = model(test_images).argmax(dim=1)\n        for i in range(len(pred)):\n            y_pred.append(pred[i].item())","e3e32859":"print(y_pred[0:5])","0777c2ef":"sample=pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsample","8c812b96":"submit=sample\nsubmit['Label']=y_pred\nsubmit.to_csv('submission.csv',index=False)\nsubmit","5dca00bf":"## Evaluate the model on test dataset","2e710188":"\n## Read datafile ","2f253742":"## Prepare training, validation and test data","6143e999":"## Define MONAI transforms, Dataset and Dataloader to pre-process data","c8656bf4":"# MNIST Classify MONAI Pytorch\nThis notebook referred to MONAI's Image Classification Tutorial with the MedNIST Dataset\nhttps:\/\/colab.research.google.com\/drive\/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe\n\n## Introduction\n- Create a MONAI Dataset for training and testing\n- Use MONAI transforms to pre-process data\n- Use the DenseNet from MONAI for the classification task\n- Train the model with a PyTorch program\n- Evaluate on test dataset\n","f439cff5":"## Plot the loss and metric","c64a378e":"## Visualise some randomly picked examples from the dataset","6ea5da5e":"## Install MONAI"}}