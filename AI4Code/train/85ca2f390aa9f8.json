{"cell_type":{"cf74df7a":"code","b006aa07":"code","70e0ad00":"code","eaa7fe0c":"code","8818ea5f":"code","1da6d877":"code","c918f3fc":"code","6125ef70":"code","2480ee26":"code","4b2258ea":"code","ec887e3d":"code","c0563a52":"code","2572351c":"code","21394e81":"markdown","e033db70":"markdown","b2334b11":"markdown","45a4badd":"markdown","c683fd4b":"markdown","8a40b74c":"markdown","7dc5f348":"markdown"},"source":{"cf74df7a":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b006aa07":"train=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain.head()","70e0ad00":"target=train['label']\ntrain=train.drop('label',axis=1)\ntrain=train.values.reshape(-1,28,28)\ntrain=train\/255\nplt.imshow(train[1,:,:])\nprint(target[1])","eaa7fe0c":"from sklearn.model_selection import train_test_split\nx_train ,  x_test ,y_train, y_test= train_test_split(train,target,test_size=0.2,random_state=7)\nprint(x_train.shape)\nprint(x_test.shape)","8818ea5f":"test=test.values.reshape(-1,28,28)\ntest=test\/255\ntest.shape","1da6d877":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPool2D,BatchNormalization,Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","c918f3fc":"def get_model(input_shape,drop_rate):\n  model=Sequential([\n                    Conv2D(32,(3,3),input_shape=input_shape,padding='SAME',activation='relu'),\n                    Conv2D(32,(3,3),padding='SAME',activation='relu'),\n                    MaxPool2D((2,2)),\n                    Dropout(drop_rate),\n                    BatchNormalization(),\n                    Conv2D(64,(3,3),padding='SAME',activation='relu'),\n                    Conv2D(64,(3,3),padding='SAME',activation='relu'),\n                    MaxPool2D((2,2)),\n                    Dropout(drop_rate),\n                    BatchNormalization(),\n                    Flatten(),\n                    Dense(128,activation='relu'),\n                    Dense(10,activation='softmax')\n  ])\n  return model\n","6125ef70":"mod=get_model([28,28,1],0.3)\nmod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","2480ee26":"data_aug=ImageDataGenerator(rotation_range=20,shear_range=0.4,zoom_range=[0.75,1.3])\nhistory=mod.fit_generator(data_aug.flow(x_train[...,np.newaxis],y_train,batch_size=100),epochs=500,\n                          callbacks=[tf.keras.callbacks.EarlyStopping(patience=15),\n                                     tf.keras.callbacks.ReduceLROnPlateau(factor=0.2,patience=15)],validation_data=(x_test[...,np.newaxis],y_test))","4b2258ea":"pred=mod.predict(test[...,np.newaxis])\nlabels=np.argmax(pred,axis=1)\nresults=pd.DataFrame({'label':labels},index=range(1,test.shape[0]+1))","ec887e3d":"#results.to_csv('result3.csv')","c0563a52":"hist=pd.DataFrame(history.history)\nhist.plot(y='accuracy')\nplt.plot(hist['val_accuracy'])","2572351c":"hist.plot(y='loss')\nplt.plot(hist['val_loss'])","21394e81":"# Define Model Architecture\n","e033db70":"I have used two 3x3 convolutional layers with 32 filters each with padding so that image size doesnt vary. This is followed by max pool so that we reduce the image size making training easier and reduce overfiting. Dropout layer and batch normalization will further reduce overfitting.\nThen we repeat two more 3x3 convolutional layers with 64 filters. Finally the flatten layer converts the matrix to a one dimensional array which then goes through dense layers and finally a softmax activation which classifies it as a digit. \n\nThis achitecture can be further modified by adding more layers or altering the number of filters. There is always a better combination that would improve the results.","b2334b11":"# Data Preparation\n\nSplit the data into lables and images. Convert the pixels into an array with 28x28 matrix of pixel values","45a4badd":"# Import Libraries","c683fd4b":"# Conclusion\n\nUsing data augmentation we can see that we have achieved good accuracy and overfitting is reduced","8a40b74c":"# Import Data","7dc5f348":"model is compiled and a drop out rate of 0.3 is used.\n\n# Data Augmentation\n\nNow we shall use data augmentation to increase the size of test data.\nWhat happens in augmentation is the function randomly either rotates, zooms or shears the original image to create new images that are similar but different to the original. This way the algorithm gets many more training examples and can adapt to variations in the image. For example if a munber is tilted by 15 degrees its still the same number. So through data augmentation the model is better equiped to handle changes in the test data. So it can greatly reduce overfitting and make the model much more robust. This is a very good technique to use when there is less data available\n\nThe advantages of ImageDataGenerator is that it does not take any storage space the images are generated one by on as it is called by the fit fucnction. This way we can train the model on unlimited number of variations without worrying about storage constraints.\n\nHere we set a rotation range of 20 degrees sothat the image is rotated only +\/- 20 degrees similarly shear is selt to 0.45 and zoom range is set. The function pics a random number within the range and applies a particular transform at random to get a slightly varied image"}}