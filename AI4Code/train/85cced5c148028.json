{"cell_type":{"f3ddc948":"code","0e7c3c69":"code","e85a56ab":"code","605924ff":"code","6c0741a0":"code","b7f99315":"code","e03c4508":"code","0de758e2":"code","5e54df18":"code","b0a80516":"code","a5d1c2cc":"code","2d3edbfd":"code","14609411":"code","af9168b3":"code","592f3c80":"code","0986fcb3":"code","3163fc02":"code","063d48cc":"code","d543bb56":"code","866dc9d7":"code","15c62645":"code","963175f3":"code","0ac3b9b8":"code","d3e2e6ad":"code","f500cdb2":"code","5fdfaab1":"code","0ad9104a":"code","e5a35d7e":"code","8476c66a":"code","1b36d491":"code","0258a192":"code","fa4f84a6":"code","10ed2c17":"code","e1e362ca":"code","50d532eb":"code","69789f54":"code","3a7f6ff2":"code","ffedbf27":"code","f1fd6ca9":"code","3d545e34":"code","e5f5f202":"code","a3bb084d":"code","6533ff71":"code","af61438a":"code","0abd7bdf":"code","87a3da92":"code","4f566a87":"markdown","06b247a3":"markdown","fb2cc1c3":"markdown","27cc7ae6":"markdown","8e4c2e6d":"markdown","7f18e426":"markdown","5c8795dd":"markdown","4282bcbc":"markdown","75c07049":"markdown","32d51f84":"markdown","565d8120":"markdown","bbb47248":"markdown","ab3f7c7f":"markdown","4d7708b6":"markdown","eacfe98b":"markdown","c69a1e2f":"markdown","d838ce86":"markdown","b5ec3f41":"markdown","a64219b2":"markdown","ce914d7c":"markdown","e3d9baab":"markdown","5e1a129d":"markdown","77622e63":"markdown","0e3083e2":"markdown","216a4e8e":"markdown","1f01dc63":"markdown","e81fecb7":"markdown","5533eba9":"markdown","4ed07264":"markdown","18231e65":"markdown","4bef31ef":"markdown","439044a2":"markdown","bb7edf5c":"markdown","9b154ceb":"markdown","48bdc538":"markdown","ab8e4f52":"markdown","4ad7e94c":"markdown","5af56aea":"markdown","b5459d1f":"markdown","b028a719":"markdown","9855e35a":"markdown","2574f4a1":"markdown","2cbf7e00":"markdown","5e6fa585":"markdown","d932ff78":"markdown","70bfe60e":"markdown","ed76d84e":"markdown","45861732":"markdown","8949e4d4":"markdown","0957f2e7":"markdown","d894b1e6":"markdown","acfa67ba":"markdown","222d1265":"markdown","9ee99144":"markdown","38ecc26c":"markdown","d324236e":"markdown","003f878f":"markdown","aec849db":"markdown","32bfbbd7":"markdown","ec52ff4b":"markdown","12586cbf":"markdown","2c7a96a2":"markdown","22a11039":"markdown","203c40ce":"markdown"},"source":{"f3ddc948":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno # visualize the distribution of NaN values\nimport seaborn as sns #Visualization of data\nimport matplotlib.pyplot as plt #Visualization of data\n%matplotlib inline\nimport gc #Garbage collection\nimport datetime","0e7c3c69":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n\n#Reading the input Files from their respective Directory\ntrain_identity=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_transaction=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_transaction=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_identity=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")","e85a56ab":"train_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest_df= pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","605924ff":"del train_identity\ndel train_transaction\ndel test_transaction\ndel test_identity\ngc.collect()","6c0741a0":"def reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            #Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n               NAlist.append(col)\n               df[col].fillna(-999,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] =df[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df,NAlist","b7f99315":"train,train_Na=reduce_mem_usage(train_df)","e03c4508":"test,test_Na=reduce_mem_usage(test_df)","0de758e2":"del train_df,test_df\ngc.collect()","5e54df18":"cols=train.columns\nnum_cols=train._get_numeric_data().columns\ncat_cols=list(set(cols)-set(num_cols))\nprint(\"Numeric Columns:\",num_cols)\nprint(\"Categoric Columns:\",cat_cols)","b0a80516":"startdate = datetime.datetime.strptime('2017-12-01', '%Y-%m-%d')\ntrain['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))","a5d1c2cc":"fig, axes = plt.subplots(1, 1, figsize=(16, 6))\ntrain.set_index('TransactionDT').resample('D').mean()['isFraud'].plot(ax=axes).set_ylabel('isFraud mean', fontsize=14);\naxes.set_title('Mean of isFraud by day', fontsize=16);","2d3edbfd":"msno.bar(train[cat_cols])","14609411":"# Visualize the correlation between the number of \n# missing values in different columns as a heatmap \nmsno.heatmap(train[cat_cols])","af9168b3":"import seaborn as sns\ndomain=train.P_emaildomain.value_counts()[:5].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"P_emaildomain\"].isin(domain)]\nsns.catplot(x=\"P_emaildomain\", y=\"TransactionAmt\",hue=\"isFraud\",data=temp)","592f3c80":"import seaborn as sns\ndomain=train.R_emaildomain.value_counts()[:5].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"R_emaildomain\"].isin(domain)]\nsns.catplot(x=\"R_emaildomain\", y=\"TransactionAmt\", hue=\"isFraud\", data=temp)","0986fcb3":"del temp\ngc.collect()","3163fc02":"sns.catplot(x=\"DeviceType\", y=\"TransactionAmt\", hue=\"isFraud\",kind=\"violin\", data=train)","063d48cc":"sns.catplot(x=\"card6\", y=\"TransactionAmt\", hue=\"isFraud\", data=train)","d543bb56":"domain=train.DeviceInfo.value_counts()[:5].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"DeviceInfo\"].isin(domain)]\nsns.catplot(x=\"DeviceInfo\", y=\"TransactionAmt\", hue=\"isFraud\",kind=\"violin\", split=True, data=temp)","866dc9d7":"sns.catplot(x=\"ProductCD\", y=\"TransactionAmt\", hue=\"isFraud\", kind=\"bar\", data=train)","15c62645":"\nsns.catplot(x=\"card4\", y=\"TransactionAmt\", hue=\"isFraud\",kind=\"bar\", data=temp)","963175f3":"Id=['id_35','id_38','id_16','id_33','id_28','id_37','id_30','id_12','id_29','id_36','id_31','id_23','id_27','id_34','id_15']","0ac3b9b8":"f, axes = plt.subplots(8, 2,figsize=(30,24))\nfor i in range(0,len(Id)):\n    sns.countplot(y=train[Id[i]],data=train,ax=axes.flatten()[i])\n    axes.flatten()[i].set_title(Id[i])","d3e2e6ad":"domain=train.id_31.value_counts()[:3].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"id_31\"].isin(domain)]\nsns.catplot(x=\"id_31\", y=\"TransactionAmt\", hue=\"isFraud\",kind=\"violin\", split=True, data=temp)","f500cdb2":"domain=train.id_33.value_counts()[:3].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"id_33\"].isin(domain)]\nsns.catplot(x=\"id_33\", y=\"TransactionAmt\", hue=\"isFraud\", split=True, data=temp)","5fdfaab1":"domain=train.id_30.value_counts()[:3].index    #Selecting Top 5 most used domains\ntemp=train.loc[train[\"id_30\"].isin(domain)]\nsns.catplot(x=\"id_30\", y=\"TransactionAmt\", hue=\"isFraud\", split=True, data=temp)","0ad9104a":"Id=['M1','M2','M3','M4','M5','M6','M7','M8','M9']\nf, axes = plt.subplots(5, 2,figsize=(12,16))\nfor i in range(0,len(Id)):\n    sns.countplot(y=train[Id[i]],data=train,ax=axes.flatten()[i])\n    axes.flatten()[i].set_title(Id[i])","e5a35d7e":"\nfilter_col = [col for col in train if col.startswith('V')]","8476c66a":"import matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy as hc\ndataframe = train[filter_col]\ncorr = 1 - dataframe.corr() \n\ncorr_condensed = hc.distance.squareform(corr) # convert to condensed\nz = hc.linkage(corr_condensed, method='average')\nfig, axes = plt.subplots(1, 1, figsize=(76, 36))\ndendrogram = hc.dendrogram(z, labels=corr.columns)\naxes.margins(2, 2)   \nplt.show()","1b36d491":"\ndel dendrogram,dataframe,corr,corr_condensed,z\ngc.collect()","0258a192":"filter_col = [col for col in num_cols if col.startswith('id')]\nfilter_col.append('TransactionAmt')\nfilter_col.append('isFraud')","fa4f84a6":"for i in range(0, len(filter_col), 4):\n    sns.pairplot(data=train[filter_col],\n                x_vars=filter_col[i:i+4],\n                y_vars=['TransactionAmt'],\n                hue='isFraud')","10ed2c17":"filter_col = [col for col in num_cols if col.startswith('card')]\nfilter_col.append('TransactionAmt')\nfilter_col.append('isFraud')\nfor i in range(0, len(filter_col), 4):\n    sns.pairplot(data=train[filter_col],\n                x_vars=filter_col[i:i+4],\n                y_vars=['TransactionAmt'],\n                hue='isFraud')","e1e362ca":"filter_col = [col for col in num_cols if col.startswith('addr')]\nfilter_col.append('TransactionAmt')\nfilter_col.append('isFraud')\nfor i in range(0, len(filter_col), 4):\n    sns.pairplot(data=train[filter_col],\n                x_vars=filter_col[i:i+4],\n                y_vars=['TransactionAmt'],\n                hue='isFraud')","50d532eb":"for col in cat_cols:\n    train[col]=train[col].fillna('empty')\n    test[col]=test[col].fillna('empty')","69789f54":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')\n\n\n\ntrain[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","3a7f6ff2":"cols=train.columns\nnum_cols=train._get_numeric_data().columns\ncat_cols=list(set(cols)-set(num_cols))\nprint(\"Numeric Columns:\",num_cols)\nprint(\"Categoric Columns:\",cat_cols)","ffedbf27":"from sklearn.preprocessing import LabelEncoder\nfor i in cat_cols:\n    train[i]=LabelEncoder().fit_transform(train[i].astype('str')) \n    test[i]=LabelEncoder().fit_transform(test[i].astype('str')) \n    \n    #Reducing the memory\n    if train[i].max()<128: \n        train[i] = train[i].astype('int8')\n        test[i] = test[i].astype('int8')\n        \n    elif train[i].max()<32768: \n        test[i] = test[i].astype('int16')\n        train[i] = train[i].astype('int16')\n        \n    else: \n        test[i]=test[i].astype('int32')\n        train[i] = train[i].astype('int32')","f1fd6ca9":"cols=num_cols.drop('isFraud')\n#cols=cols.drop('isFraud')","3d545e34":"for col in cols:\n    try:\n        train[col] = ( train[col]-train[col].mean() ) \/ train[col].std() \n        test[col] = ( test[col]-test[col].mean() ) \/ test[col].std()\n    except:\n        print(col)","e5f5f202":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\n\ncols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionID')\ncols_to_drop.remove('TransactionDT')","a3bb084d":"train=train.drop(cols_to_drop,axis=1)\ntest=test.drop(cols_to_drop,axis=1)\n\ngc.collect()","6533ff71":"Y=train['isFraud']\ntrain=train.drop('isFraud',axis=1)\ntrain=train.drop('TransactionID',axis=1)\ngc.collect()","af61438a":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMClassifier(bagging_fraction=0.4181193142567742, bagging_seed=11,\n                               boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                               feature_fraction=0.3797454081646243, importance_type='split',\n                               learning_rate=0.006883242363721497, max_depth=-1, metric='auc',\n                               min_child_samples=20, min_child_weight=0.03454472573214212,\n                               min_data_in_leaf=106, min_split_gain=0.0, n_estimators=100,\n                               n_jobs=-1, num_boost_round=5090, num_leaves=491,\n                               objective='binary', random_state=47,\n                               reg_alpha=0.3899927210061127, reg_lambda=0.6485237330340494,\n                               silent=True, subsample=1.0, subsample_for_bin=200000,\n                               subsample_freq=0, verbosity=-1)\nmodel_lgb.fit(train,Y)","0abd7bdf":"submission=pd.DataFrame()\nsubmission['TransactionID']=test['TransactionID']\ntest=test.drop('TransactionID',axis=1)\nsubmission['isFraud'] = model_lgb.predict_proba(test)[:, 1]\nsubmission['TransactionID']=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")['TransactionID']\nsubmission.to_csv('Results.csv', index=False)","87a3da92":"from IPython.display import FileLink\nFileLink(r'Results.csv')","4f566a87":"There are 14 categorical columns representing ID's lets find out what they are about..","06b247a3":"**Reducing Memory for test data**","fb2cc1c3":"## About the competition\n\n                    \n\n\n\n\n\n","27cc7ae6":"**Device Info**","8e4c2e6d":"We have already processed the missing variables in numerical as -999 in numerical variables.\n\nWe have already processed the missing variables in categorical variable as empty \n\nThis is because you give np.nan to LGBM, then at each tree node split, it will split the non-NAN values and then send all the NANs to either the left child or right child depending on what\u2019s best. Therefore NANs get special treatment at every node and can become overfit.","7f18e426":"\n**Categorical Data Exploration**","5c8795dd":"**Observation**\n\nId_31 is about version of browser and type of device combined","4282bcbc":"**Address ** in Numerical data","75c07049":"These columns were selected based on the recursive feature elemination method.\n","32d51f84":"\n**Observation**\nThe distribution of Fraud's in Retailer's domain is pretty high.","565d8120":"**NAN Processing**","bbb47248":"** Quick Recap on Data **","ab3f7c7f":"**Data Merging**","4d7708b6":"**Numerical Data Exploration**","eacfe98b":"This is the first time dealing with this kind of large data.I have referred a lot of resources for creating this kernel. This kernel is also influenced by some kernels and discussion from this competition. So I can assure you that this kernel will help you how to deal with large dataset if you are new in dealing with large dataset.","c69a1e2f":"Frauds are distributed equally among all the devices","d838ce86":"**Card 6**\n\nIt is about whether the is Debit or Credit Card","b5ec3f41":"**Id_33**","a64219b2":"**Id_31**","ce914d7c":"**I will be working extenssively on Feature Engineering and Modelling part**","e3d9baab":"**Observation:**\n\nFraud's are using credit and debit card they are **not** using **charge card** and **debit or credit**","5e1a129d":"**How this helps?**\n\nHere I am reducing the memory of the dataset by 72% without losing the data. This will have a great impact while training our model. It will reduce the training time by a very large margin. ","77622e63":"**Breaking domain the purchaser domain**","0e3083e2":"\nDistribution of the data from Dec 2017 to Jun 2019 as we assumed We can see the pattern of transaction on each day","216a4e8e":"**Observations:**\n\n1) From the figure we are able to understand what the data is about. It gives some idea about their distrubution\n\n\n2) Id_31, Id_33, Id_30 the data is very micro level. The data is very much distributed. Let's further understand what they are about","1f01dc63":"![image.png](attachment:image.png)","e81fecb7":"**Key Take Aways**\n\n1) Effecting Data Minification\n\n2) Creative Feature Enginnering\n\n3) Exploratory Data Analysis\n\n4) Advanced Machine Learning Techniques\n","5533eba9":"ofcourse, we can't do normalization with timedelta as we converted to date format that's why it is excepted\n","4ed07264":"**Creating new Features**","18231e65":"**Card's in Numerical Data**","4bef31ef":"**Observation**\n\nWith Cards we can find that Fraud Transaction are made for the low amount's compared to the Non-Fraud Transaction's","439044a2":"**If you find this kernel useful just leave a upvote**","bb7edf5c":"**Stay tuned for more updates**","9b154ceb":"IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.","48bdc538":"Exploring the **M** series variable","ab8e4f52":"## Reading the Files and Data Merging","4ad7e94c":"**Breaking down the Retailer's Domain**","5af56aea":"**V** what is the relationship between them?\n","b5459d1f":"**Id's ** in numerical Feature","b028a719":"Observation:\n   The distribution of Fraud's in each user domain is very less","9855e35a":"**Observation**\n\nId_33 is about the resolution of the device you are using","2574f4a1":"Feature Engineering part of this kernel is highly influenced by this Discussion :https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-631034","2cbf7e00":"**Device Type**","5e6fa585":"## Reducing the memory of the data\n\nThe size of the dataset is pretty big, so we are trying to make the dataset smaller without losing information. \n\n**Reason behind memory Reduction:**\n\nInt16: 2 bytes\n\nInt32 and int: 4 bytes\n\nInt64 : 8 bytes\n\nThis is an example how different integer types are occupying the memory. In many cases it is not necessary to represent our integer as int64 and int32 it is just waste of memory. So I am trying to understand the necessaity of every numerical representation and try to convert the unnecessary higher numerical representation to lower one. In that, we can reduce the memory without losing the memory.\n ","d932ff78":"**Reducing Memory For training data**","70bfe60e":"**ProductCD**","ed76d84e":"**Observations:**\n\n1) From this pairplot we can clearly see that the transaction amount and Id's are for both Fraud and Non Fraud transaction's are mixed","45861732":"**Numeric AND Categorical Columns**","8949e4d4":"**Time period of the data**\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). Let's assume the start time is from December 1,2017 and see how the data is distributed between Train and Test data","0957f2e7":"## Exploratory Data  Analysis","d894b1e6":"\n## Feature Engineering","acfa67ba":"**Missing Nature of Categorical Data**","222d1265":"**Observations:**\n\n1)The dendogram can be zoomed.\n\n2) If you zoom and observe there are many cluster's in the **V** varaible.\n\n3) We can use this observation to create new feature while dealing with Feature Engineering\n    ","9ee99144":"**Noramlizing the columns**","38ecc26c":"**Label  Encoding**","d324236e":"## Modelling","003f878f":"\n**Id30**","aec849db":"**Observation**\n\nId_30 is about the windows\/Andriod version the user has used","32bfbbd7":"**How it works?**\n  \n  This heatmap tells the correlation between the missing nature of the data.**Example:** id_15 and id_28 has a correlation of 1 which means if one variable appears then the other variable is very likely to be **present**. ","ec52ff4b":"**Id's and their Nature**","12586cbf":"**Card 4**","2c7a96a2":"**Loading Necessary Libraries**","22a11039":"**Feature Selection**","203c40ce":"**Observation:**\n\n"}}