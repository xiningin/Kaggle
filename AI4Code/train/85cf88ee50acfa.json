{"cell_type":{"e6b807d7":"code","5b48cbeb":"code","dca5ce2f":"code","27190968":"code","454168c0":"code","4ca45b21":"code","8de5c7eb":"code","76aa0cc2":"code","4bdc2c47":"code","cb0a8123":"code","1acf6d47":"code","52dd82c9":"code","ba890bec":"code","72d982d7":"code","e455eef3":"code","105688c8":"code","16f38e8f":"code","54f649e7":"code","0abf812b":"code","aaeed86e":"code","3f571f87":"code","9e682324":"code","1ce31ef9":"code","7721c5e9":"code","0e2349cb":"code","87679438":"code","bf955c0c":"code","2a58b87b":"code","fb7f1d1c":"code","b789549a":"code","bed72a15":"code","35b02849":"code","47268aa7":"code","e2d90da0":"code","221b810c":"code","4300aafc":"code","45bc31b9":"code","81cc3627":"code","8957e663":"markdown","f70428f6":"markdown","3a2a75e2":"markdown","340b5d97":"markdown","7646b460":"markdown","5b366330":"markdown","5b30fb89":"markdown","aab0680b":"markdown"},"source":{"e6b807d7":"import glob\nimport pandas as pd\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport nltk.translate.bleu_score as bleu\nimport random\nimport string\nfrom sklearn.model_selection import train_test_split\nimport os\nimport time","5b48cbeb":"gpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)","dca5ce2f":"movie_df = pd.concat(map(pd.read_csv,\\\n                         glob.glob('..\/input\/movie-subtitles-csv-hindi-english\/*.csv')),\\\n                     ignore_index= True)","27190968":"movie_df.drop([\"Unnamed: 0\", \"Time\"], axis=1, inplace = True)\nmovie_df.dropna(inplace=True)","454168c0":"movie_df.head()","4ca45b21":"movie_df.rename(columns={\"en_list\": \"english\",\\\n                        \"hi_list\": \"hindi\"},inplace=True)","8de5c7eb":"movie_df.shape","76aa0cc2":"df=pd.read_csv('..\/input\/hindi-english-truncated-corpus\/Hindi_English_Truncated_Corpus.csv')\ndf.head()","4bdc2c47":"df.shape","cb0a8123":"df.dropna(inplace=True)\ndf.drop(['source'],axis=1,inplace=True)\ndf.shape","1acf6d47":"df.rename(columns={\"english_sentence\": \"english\",\\\n                        \"hindi_sentence\": \"hindi\"},inplace=True)","52dd82c9":"df.shape","ba890bec":"df = df.append(movie_df,ignore_index=True)","72d982d7":"df.shape","e455eef3":"punctuations = set(string.punctuation)\n# Creating a set of all punctuations and speial characters\ndigits = str.maketrans('', '', string.digits)\n# Creating a set of digits","105688c8":"def preprocess_english(text):\n    text = text.lower() # lower casing all characters\n    text = text.translate(digits) # removing the digits\n    text = ''.join(ch for ch in text if ch not in punctuations) # removing punctuation marks\n    text = re.sub(\"'\", '', text) # removing quotation marks\n    text = text.strip()\n    text = re.sub(\" +\", \" \", text) # removing extra spaces\n    text = '<start> ' + text + ' <end>' # Adding start and end tokens at the start of sentences\n    return text","16f38e8f":"def preprocess_hindi(text):\n    text = re.sub(\"[\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]\", \"\", text) # removing the hindi digits\n    text = ''.join(ch for ch in text if ch not in punctuations) # removing punctuation marks\n    text = re.sub(\"'\", '', text) # removing quotation marks\n    text = text.strip()\n    text = re.sub(\" +\", \" \", text) # removing extra spaces\n    text = '<start> ' + text + ' <end>' # Adding start and end tokens at the start of sentences\n    return text","54f649e7":"df['english'] = df['english'].apply(preprocess_english)\ndf['hindi'] = df['hindi'].apply(preprocess_hindi)\ndf.head()","0abf812b":"def tokenize(lang):\n\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\\\n                                                         padding='post',\\\n                                                         maxlen=20,\\\n                                                         dtype='int32')\n\n  return tensor, lang_tokenizer","aaeed86e":"def load_dataset():\n\n  input_tensor, inp_lang_tokenizer = tokenize(df['english'].values)\n  target_tensor, targ_lang_tokenizer = tokenize(df['hindi'].values)\n\n  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer","3f571f87":"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()","9e682324":"input_tensor","1ce31ef9":"max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]","7721c5e9":"BUFFER_SIZE = len(input_tensor)\nBATCH_SIZE = 64\nN_BATCH = BUFFER_SIZE\/\/BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nsteps_per_epoch = len(input_tensor)\/\/BATCH_SIZE\n\nvocab_inp_size =len(inp_lang.word_index.keys())\nvocab_tar_size =len(targ_lang.word_index.keys())\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","0e2349cb":"embeddings_index = dict()\nf = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.zeros((vocab_inp_size+1, 300))\nfor word, i in inp_lang.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","87679438":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        \n        # The embedding layer converts tokens to vectors\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\\\n                                                   output_dim=embedding_dim,\\\n                                                   name=\"embedding_layer_encoder\",\\\n                                                   trainable=False)\n        \n         # The GRU RNN layer processes those vectors sequentially.\n        self.gru = tf.keras.layers.GRU(units,\\\n                                       # Return the sequence and state\n                                       return_sequences=True, \\\n                                       return_state=True, \\\n                                       recurrent_activation='sigmoid',\\\n                                       recurrent_initializer='glorot_uniform')\n        \n    def call(self, x, hidden):\n        # The embedding layer looks up the embedding for each token.\n        x = self.embedding(x)\n        \n        # The GRU processes the embedding sequence.\n        # output shape: (batch, s, enc_units)\n        # state shape: (batch, enc_units)\n        output, state = self.gru(x, initial_state = hidden)\n        \n        # Returns the new sequence and its state.\n        return output, state\n    \n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_size, self.enc_units))","bf955c0c":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n        super(Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.dec_units = dec_units\n        \n        # The embedding layer convets token IDs to vectors\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        \n        # The RNN keeps track of what's been generated so far.\n        self.gru = tf.keras.layers.GRU(units, return_sequences=True,\\\n                                       return_state=True, recurrent_activation='sigmoid',\\\n                                       recurrent_initializer='glorot_uniform')\n        \n        # This fully connected layer produces the logits for each output token.\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        # used for attention\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, x, hidden, enc_output):\n\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        \n        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n        \n        attention_weights = tf.nn.softmax(score, axis=1)\n        \n        \n        # Use the RNN output as the query for the attention over the\n        # encoder output.\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n        x = self.embedding(x)\n        \n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        \n        # Process one step with the RNN\n        output, state = self.gru(x)\n        \n        output = tf.reshape(output, (-1, output.shape[2]))\n        \n        # Generate logit predictions\n        x = self.fc(output)\n        \n        return x, state, attention_weights\n        \n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_size, self.dec_units))","2a58b87b":"tf.keras.backend.clear_session()\n\nencoder = Encoder(vocab_inp_size+1, 300, units, BATCH_SIZE)\ndecoder = Decoder(vocab_tar_size+1, embedding_dim, units, BATCH_SIZE)","fb7f1d1c":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n                                                            reduction='none')\n\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n\n  # Calculate the loss for each item in the batch.\n  loss_ = loss_object(real, pred)\n \n  # Mask off the losses on padding.  \n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  \n # Return the total.\n  return tf.reduce_mean(loss_)","b789549a":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","bed72a15":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n        \n    # Encode the input\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    encoder.get_layer('embedding_layer_encoder').set_weights([embedding_matrix])\n    dec_hidden = enc_hidden\n\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n    for t in range(1, targ.shape[1]):\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss \/ int(targ.shape[1]))\n  \n    \n  # Apply an optimization step\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n  \n  return batch_loss","35b02849":"EPOCHS = 15\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix=checkpoint_prefix)\n\n  print(f'Epoch {epoch+1} Loss {total_loss\/steps_per_epoch:.4f}')\n  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","47268aa7":"for epoch in range(EPOCHS,20):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n  # saving (checkpoint) the model every 2 epochs\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix=checkpoint_prefix)\n\n  print(f'Epoch {epoch+1} Loss {total_loss\/steps_per_epoch:.4f}')\n  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","e2d90da0":"def evaluate(sentence):\n  attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n  sentence = preprocess_english(sentence)\n\n  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=20, padding='post')\n  inputs = tf.convert_to_tensor(inputs)\n\n  result = ''\n\n  hidden = [tf.zeros((1, units))]\n  enc_out, enc_hidden = encoder(inputs, hidden)\n\n  dec_hidden = enc_hidden\n  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n  for t in range(max_length_targ):\n    predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n    # storing the attention weights to plot later on\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    attention_plot[t] = attention_weights.numpy()\n    predicted_id = tf.argmax(predictions[0]).numpy()\n\n    result += targ_lang.index_word[predicted_id] + ' '\n\n    if targ_lang.index_word[predicted_id] == '<end>':\n      return result,attention_plot\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n\n  return result,attention_plot","221b810c":"input_sentence= 'what do you mean by this?'\nprint('Input sentence in english : ',input_sentence)\npredicted_output,attention_plot=evaluate(input_sentence)\nprint('Predicted sentence in hindi : ',predicted_output)","4300aafc":"input_sentence='do you live in this neighbourhood'\nprint('Input sentence in english : ',input_sentence)\npredicted_output,attention_plot=evaluate(input_sentence)\nprint('Predicted sentence in hindi : ',predicted_output)","45bc31b9":"input_sentence='you are taking him very lightly'\nprint('Input sentence in english : ',input_sentence)\npredicted_output,attention_plot=evaluate(input_sentence)\nprint('Predicted sentence in hindi : ',predicted_output)","81cc3627":"input_sentence='where do you think are the coins hidden?'\nprint('Input sentence in english : ',input_sentence)\npredicted_output,attention_plot=evaluate(input_sentence)\nprint('Predicted sentence in hindi : ',predicted_output)","8957e663":"For Text Preprocessing creating a list of Digits and Punctuations that will be removed from text while preprocessing","f70428f6":"### Define the loss function","3a2a75e2":"Defining functions to preprocess text data","340b5d97":"The encoder returns its internal state so that its state can be used to initialize the decoder.","7646b460":"## The encoder\nSource code: https:\/\/www.tensorflow.org\/text\/tutorials\/nmt_with_attention\n\nThe encoder:\n* Takes a list of token IDs (from input_text_processor).\n* Looks up an embedding vector for each token (Using a layers.Embedding).\n* Processes the embeddings into a new sequence (Using a layers.GRU).\n\nReturns:\n* The processed sequence. This will be passed to the attention head.\n* The internal state. This will be used to initialize the decoder","5b366330":"### Training\n\nNow that we  have all the model components, it's time to start training the model. You'll need:\n\n* A loss function and optimizer to perform the optimization.\n* A training step function defining how to update the model for each input\/target batch.\n* A training loop to drive the training and save checkpoints.","5b30fb89":"## The decoder\nThe decoder's job is to generate predictions for the next output token.\n\n* The decoder receives the complete encoder output.\n* It uses an RNN to keep track of what it has generated so far.\n* It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n* It combines the RNN output and the context vector using Equation 3 (below) to generate the \"attention vector\".\n* It generates logit predictions for the next token based on the \"attention vector\".","aab0680b":"1. https:\/\/github.com\/sunilbelde\/neural-machine-translation-english-to-hindi-telugu\n2. glove embeddings: https:\/\/nlp.stanford.edu\/projects\/glove\/\n3. https:\/\/www.kaggle.com\/thanakomsn\/glove6b300dtxt"}}