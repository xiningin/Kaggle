{"cell_type":{"ecdaf73f":"code","56f13379":"code","dcbf2689":"code","d8df1354":"code","e4586659":"code","0c3985b2":"code","f26735c5":"code","ed4d5308":"code","07dc8d4b":"code","fc6ad1e7":"code","6240724e":"code","1db118ce":"code","674b1aa2":"code","bf974c4e":"code","1a8934c5":"code","79a6cae6":"code","560eee7c":"code","03aad8fe":"code","5b84b728":"code","3698b6cd":"code","038704c6":"code","ae059b98":"markdown","0db37f5f":"markdown","4fc85152":"markdown","185eff8c":"markdown","4a9d521f":"markdown","96147cce":"markdown","a65e060a":"markdown","603b6b89":"markdown","020a2b7f":"markdown","9213a49f":"markdown","c77428be":"markdown","b60db68a":"markdown","4241890a":"markdown","e3110f89":"markdown"},"source":{"ecdaf73f":"import pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader,Dataset\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nimport torch\nfrom torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\nfrom collections import defaultdict\nplt.style.use('seaborn')\nseed = 42","56f13379":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# num_bins = int(np.floor(1 + np.log2(len(train_data))))\nnum_bins = 5\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n","dcbf2689":"x = train_data.bins.value_counts()*100\/train_data.shape[0]\nsns.barplot(list(map(str,x.index)),x.values,palette='flare') \nplt.plot(x.values,marker='*',color='blue')\nplt.gca().set_ylabel(\"percentage of samples\",fontsize=14)\nplt.gca().set_title(\"Bins distribution\",fontsize=18)\nplt.show()","d8df1354":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.bins = df['bins'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        curr_sent = {}\n        curr_sent['target'] = self.targets[idx]\n        curr_sent['bin'] = self.bins[idx]\n    \n        return curr_sent\n    \n    def __len__(self):\n        return len(self.excerpt)","e4586659":"tokenizer = AutoTokenizer.from_pretrained('roberta-base')","0c3985b2":"def get_fold_loader(sampler=None,indices=None):\n    \n\n    train_ds = CLRPDataset(train_data,tokenizer)\n    \n    if sampler=='random':\n        \n        train_sampler = RandomSampler(train_ds,replacement=True)\n        train_dataloader = DataLoader(train_ds,\n                                     sampler = train_sampler,\n                                     batch_size=128,\n                                     drop_last=False)\n        \n        \n    \n    elif sampler=='sequential':\n        \n        train_sampler = SequentialSampler(train_ds)\n        train_dataloader = DataLoader(train_ds,\n                                     sampler = train_sampler,\n                                     batch_size=128,\n                                     drop_last=False)\n        \n    elif sampler=='subset':\n        \n        train_sampler = SubsetRandomSampler(indices)\n        train_dataloader = DataLoader(train_ds,\n                                     sampler = train_sampler,\n                                     batch_size=128,\n                                     drop_last=False)\n        \n        \n        \n        \n        \n    elif sampler==\"weighted\":\n        \n        train_sampler = weightedsampler(train_ds)\n        train_dataloader = DataLoader(train_ds,\n                                     sampler = train_sampler,\n                                     batch_size=128,\n                                     drop_last=False)\n        \n    else:\n        train_dataloader = DataLoader(train_ds,\n                                      shuffle=True,\n                                     batch_size=128,\n                                     drop_last=False)\n    \n    \n    return train_dataloader\n        \n\n","f26735c5":"def get_batch_count(train_dataloader):\n    \n    all_batches={f'bin_{b}':[] for b in range(num_bins)}\n    for i,data in enumerate(train_dataloader):\n\n          curr = dict(Counter(data['bin'].numpy()))\n          for b in range(num_bins):\n                all_batches[f'bin_{b}'].append(curr.get(b,0))\n\n    return all_batches  \n        \n","ed4d5308":"import plotly.graph_objects as go\n\ndef plot_batches(all_batches):\n    \n    x = [str(x) for x in range(len(all_batches['bin_0']))]\n    data_append = []\n    for bin_no in range(num_bins):\n        data_append.append(go.Bar(\n                                   x=x,\n                                   y=all_batches[f'bin_{bin_no}'],\n                                   text=all_batches[f'bin_{bin_no}'],\n                                   textposition='auto',name=f'bin_{bin_no}'))\n        \n        \n\n        \n        \n    fig = go.Figure(data=data_append)\n    fig.update_layout(title=\"Batch bin frequency\", xaxis_title=\"Batch number\", yaxis_title=\"Bin frequency\",\n                      xaxis_visible=True,  xaxis_showticklabels=True,  yaxis_visible=True,  yaxis_showticklabels=False,\n                      xaxis_tickmode='linear', barmode='stack')\n    fig.show()","07dc8d4b":"\ntrain_dataloader = get_fold_loader()\nall_batches = get_batch_count(train_dataloader)\nplot_batches(all_batches)","fc6ad1e7":"train_data['Fold'] = -1\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    train_data.loc[valid_idx,'Fold'] = k\n","6240724e":"fold=1\nx_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\nindices_to_sample = x_train.index.tolist()","1db118ce":"train_dataloader = get_fold_loader(sampler='subset',indices=indices_to_sample)\nall_batches = get_batch_count(train_dataloader)\nplot_batches(all_batches)","674b1aa2":"train_dataloader = get_fold_loader(sampler='sequential')\nall_batches = get_batch_count(train_dataloader)\nplot_batches(all_batches)","bf974c4e":"class weightedsampler(Sampler):\n    \n    def __init__(self,dataset):\n        \n        self.indices = list(range(len(dataset)))\n        self.num_samples = len(dataset)\n        self.label_to_count = dict(Counter(dataset.bins))\n        weights = [1\/self.label_to_count[i] for i in dataset.bins]\n        \n        self.weights = torch.tensor(weights,dtype=torch.double)\n        \n    def __iter__(self):\n        count = 0\n        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n        while count < self.num_samples:\n            yield index[count]\n            count += 1\n    \n    def __len__(self):\n        return self.num_samples\n        \n        ","1a8934c5":"train_dataloader = get_fold_loader(sampler='weighted')\nall_batches = get_batch_count(train_dataloader)\nplot_batches(all_batches)","79a6cae6":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                padding=False,\n                                truncation=False,\n                                return_attention_mask=True,\n                                return_token_type_ids=True,\n                                )\n\n        \n        curr_sent = {}\n        curr_sent['input_ids'] = encode['input_ids']\n        curr_sent['attention_mask'] =encode['attention_mask'] \n        curr_sent['token_type_ids'] = encode['token_type_ids']\n        curr_sent['target'] = self.targets[idx]\n    \n        return curr_sent\n    \n    def __len__(self):\n        return len(self.excerpt)","560eee7c":"class CLMCollate:\n    \n    def __init__(self,config):\n        self.config = config\n        self.seq_dic = defaultdict(int)  ## used to track max_length\n        self.batch_record = defaultdict(list)\n        self.bn = 0\n        \n    def __call__(self,batch):\n                \n        out = {'input_ids' :[],\n               'attention_mask':[],\n               'token_type_ids':[],\n                'target':[]\n            \n        }\n        \n        for i in batch:\n            for k,v in i.items():\n                out[k].append(v)\n                \n        if self.config['bucket']:\n            max_pad =0\n            \n            for p in out['input_ids']:\n                if max_pad < len(p):\n                    max_pad = len(p)\n                    \n        else:\n            max_pad = self.config['max_len']\n            \n        \n        self.batch_record[str(self.bn)] = [len(x) for x in out['input_ids']]  \n        self.seq_dic[str(self.bn)] = max_pad\n        self.bn+=1\n\n        for i in range(len(batch)):\n            \n            input_id = out['input_ids'][i]\n            att_mask = out['attention_mask'][i]\n            token_type_id = out['token_type_ids'][i]\n            text_len = len(input_id)\n            \n            out['input_ids'][i] = (out['input_ids'][i] + [1] * (max_pad - text_len))[:max_pad]\n            out['attention_mask'][i] = (out['attention_mask'][i] + [0] * (max_pad - text_len))[:max_pad]\n            out['token_type_ids'][i] = (out['token_type_ids'][i] + [0] * (max_pad - text_len))[:max_pad]\n        \n        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n        out['token_type_ids'] = torch.tensor(out['token_type_ids'],dtype=torch.long)\n        out['target'] = torch.tensor(out['target'],dtype=torch.float)\n        \n        return out","03aad8fe":"config = {\n    'bucket':False,\n    'max_len':256,\n    'batch_size':256\n}\ntrain_ds = CLRPDataset(train_data,tokenizer)\nsequence = CLMCollate(config)\ntrain_dataloader = DataLoader(train_ds,\n                              batch_size=config['batch_size'],\n                             collate_fn=sequence,\n                             shuffle=True,)\nfor i,data in enumerate(train_dataloader):\n    pass","5b84b728":"def plot_sequence(sequence):\n    plt.figure(figsize=(15,15))\n    for i in sequence.seq_dic.keys():\n        fig = plt.subplot(3,4,int(i)+1)\n        plt.hist(sequence.batch_record[str(i)],density=True,color='yellow', edgecolor='white', alpha=0.5)\n        plt.axvline(sequence.seq_dic[str(i)],color='red')\n        plt.gca().set_title('batch '+str(i))\n        plt.gca().axes.get_yaxis().set_visible(False)\n        \n    plt.show()\n    \n\nplot_sequence(sequence)","3698b6cd":"config = {\n    'bucket':True,\n    'max_len':256,\n    'batch_size':256\n}\ntrain_ds = CLRPDataset(train_data,tokenizer)\nsequence = CLMCollate(config)\ntrain_dataloader = DataLoader(train_ds,\n                              batch_size=config['batch_size'],\n                             collate_fn=sequence,\n                             shuffle=True,)\nfor i,data in enumerate(train_dataloader):\n    pass","038704c6":"plot_sequence(sequence)","ae059b98":"You can see that as the samples are drawn randomly it does not keep any distribution and hence this could bring more randomness into the model. Class distrituion in different batches are also very different.","0db37f5f":"You can see that the max length is varying here for different batches according to the size of input sample.\nThat's what we wanted to do :)","4fc85152":"#### Let's try without sequence  bucketing\nFor this I have set `bucket=False` in config.","185eff8c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Data Samplers<\/p>\n\nThe purpose of data sampler is to determine how batches of data is formed from the given pool of data with given batch size. It also responsible for determining the order of the dataset which is fed into the model for learning.\n\n\n![](https:\/\/www.programmersought.com\/images\/340\/df42b44fcd8753e385e6413c3354ae8c.png)\n\n\n- When the dataloader is initialized, the sampler is also passed to it ( RandomSampler by default) which first create the sequence order in which the the samples in dataset is accessed using index.ie (1,2,3..N) where N = size of the dataset. \n- Then using this sequence of indices, the data is pulled from the dataset for each batch with given batch size.\n\nSo, Let's see some of the available data samplers in pytorch","4a9d521f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">Sequence Bucketing<\/p>\n\n\nAfter fetching a list of samples using the indices from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches.\n\nSequence bucketing is done by implementing a custom collate_fn for the dataloader. This ensures that the padding is done dynamically for each batch.","96147cce":"You can see when the max length is fixed, we are loosing important information.\n\n#### Now let's try with sequence bucketing","a65e060a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">SubsetRandom Sampler<\/p>\n\nSamples elements randomly from a given list of indices, without replacement.This can be used with K Fold to sample indices randomly from a list of indices.","603b6b89":"I will read the data and split it into bins like in this notebook.","020a2b7f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">Custom Sampler (weighted)<\/p>\n\nTo implement a custom sampler you can inherit from `Sampler` class in `utils.data`. \n\nThis sampler is used to ensure that each batch sees a proportional number of all classes.\n* Get all the target classes.\n* Get the class weights. Class weights are the reciprocal of the number of items per class.\n* Obtain corresponding weight for each target sample.\n\nThis will sample the data points with multinomial distribution. \n\n> the multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for each side of a k-sided die rolled n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.","9213a49f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Introduction<\/p>\n\n\nWhen using pytorch for dataloading, we mostly use the default data sampling technique. One of the factors that affect the the training stability is data sampling.In this notebook I will show you various data sampling methods available in pytorch and also how to write a custom data sampler function for your dataloader.\n\nSequence bucketing is another method which I will describe and implement in this notebook. This is a great method to increase model training speed by dynamic padding.","c77428be":"Let's visualize the the max length of batches. The red line shows the padded length and the distribution shows the token length distribution of the respective batch","b60db68a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">Sequential Sampler<\/p>\n\nThis samples the data samples sequentially,always in the same order","4241890a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">Dataset<\/p>\n\nI will make a simple pytorch dataset for our exercise which we can use to experiment with different sampling techniques.","e3110f89":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:left\">Random Sampler<\/p>\n\nFirst let's try the random sampler which shuffles the indices randomly and fetches the indices based on it for every batch.\n"}}