{"cell_type":{"470d866e":"code","82e6d07b":"code","de5fa41b":"code","f80268fd":"code","c90efa49":"code","5c2a21f6":"code","fbf92f48":"code","d8986901":"code","3132658d":"code","9c6ce8f7":"code","4e21dd88":"code","0804a8f6":"code","22aef84f":"code","42c0ea87":"code","511978f5":"code","2c287082":"code","fa0bf97d":"code","2ede01ea":"code","78d42d00":"code","668049bb":"code","15c3c9a7":"code","df2cdfc9":"code","1b6bacb4":"code","a5d33d37":"code","c14e1ea5":"code","5800380f":"code","a3c7c1e5":"code","3406f350":"code","17b365c9":"code","c3e21875":"code","2b0d61df":"code","fc00c337":"code","8f7a7fd7":"code","7ac90c98":"code","819afa6f":"code","0061bda2":"code","74b5ccc3":"code","e864236b":"code","6e3e066d":"code","338e1854":"code","eb80affd":"code","fd4332c7":"code","5e34ad34":"code","489ca811":"code","f8936aa1":"code","3c59babe":"code","352f7ddc":"code","3b499ccc":"code","46d2ee02":"code","d54d3702":"markdown","2e4c782f":"markdown","47ea282b":"markdown","f87a9dbb":"markdown","707cb223":"markdown","3419a060":"markdown","5b5ff9c4":"markdown","27a7132a":"markdown","7b7327b1":"markdown","9f6017e0":"markdown","54b8e642":"markdown"},"source":{"470d866e":"#It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82e6d07b":"import pandas as pd\ndf = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')","de5fa41b":"df.head()","f80268fd":"df.info()","c90efa49":"df.describe()","5c2a21f6":"df.shape","fbf92f48":"df[\"age\"] = df[\"Rings\"] + 1.5\n#df.drop(\"Rings\",axis = 1, inplace = True)","d8986901":"df.head()","3132658d":"df.isnull().sum()","9c6ce8f7":"df.info()","4e21dd88":"df.var()","0804a8f6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings","22aef84f":"plt.figure(figsize=(8,4))\nsns.countplot(x='Sex', data=df , palette= \"YlOrRd\")","42c0ea87":"plt.figure(figsize=(12,8))\nsn = sns.countplot(x='age',data=df, hue='Sex', palette=['pink','crimson',\"Yellow\"])","511978f5":"atributes_sex = df[['Sex','Length','Diameter',\t'Height',\t'Whole weight',\t'Shucked weight',\t'Viscera weight',\t'Shell weight'\t]].groupby('Sex').mean()\ncols = ['Length','Diameter',\t'Height',\t'Whole weight',\t'Shucked weight',\t'Viscera weight',\t'Shell weight']\natributes_sex.columns = cols","2c287082":"list(atributes_sex.iloc[0])","fa0bf97d":"atributes_sex.columns.values","2ede01ea":"atributes_sex","78d42d00":"from plotly.offline import iplot\ntrace1 = go.Bar(\n    y=list(atributes_sex.iloc[2]),\n    x=atributes_sex.columns.values,\n    name='Men',\n    marker=dict(\n        color='navy'\n    )\n)\ntrace2 = go.Bar(\n    y=list(atributes_sex.iloc[0]),\n    x=atributes_sex.columns.values,\n    name='Women',\n    marker=dict(\n        color='mediumslateblue'\n    )\n)\ntrace3 = go.Bar(\n    y=list(atributes_sex.iloc[1]),\n    x=atributes_sex.columns.values,\n    name='Infant',\n    marker=dict(\n        color='cornflowerblue'\n    )\n)\n\ndata = [trace1, trace2,trace3]\nlayout = go.Layout(\n    title='Features',\n    font=dict(\n        size=18\n    ),\n    legend=dict(\n        font=dict(\n            size=18\n        )\n    )\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)\nplt.show()","668049bb":"X = df[['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n       'Viscera weight', 'Shell weight']]\nY = df[\"Rings\"]\nsns.jointplot(x = \"Length\", y = \"age\", data=df, kind=\"reg\")\nplt.show()","15c3c9a7":"sns.lmplot(x='Diameter',y='age',data=df,col='Sex',palette = \"red\")","df2cdfc9":"sns.lmplot(x='Whole weight',y='age',data=df ,hue = \"Sex\",palette = \"YlOrRd\")","1b6bacb4":"sns.lmplot(x = 'Height', y = 'age', data = df, hue = 'Sex', palette = 'magma', scatter_kws={'edgecolor':'white', 'alpha':0.4, 'linewidth':0.5})","a5d33d37":"sns.pairplot(df,hue = \"Sex\",palette= \"coolwarm\")","c14e1ea5":"plt.figure(figsize = (8,6))\ncorr = df.corr()\nsns.heatmap(corr, annot = True)","5800380f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.33, random_state=42)","a3c7c1e5":"X_train.head()","3406f350":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit_transform(X_train)","17b365c9":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\nprint(\"Test score \"+str(reg.score(X_test, y_test)))\nprint(\"Train score \"+str(reg.score(X_train, y_train)))","c3e21875":"from sklearn.svm import SVR\nsvr_rbf = SVR(kernel='rbf', C=100, gamma=1, epsilon=.01).fit(X_train, y_train)\nsvr_lin = SVR(kernel='linear', C=100, gamma='auto').fit(X_train, y_train)\nsvr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1).fit(X_train, y_train)\n\nprint(\"Test score for rbf :\",str(svr_rbf.score(X_test, y_test)))\nprint(\"Test score for lin :\",str(svr_rbf.score(X_test, y_test)))\nprint(\"Test score for poly :\",str(svr_rbf.score(X_test, y_test)))","2b0d61df":"from sklearn.tree import DecisionTreeRegressor\n# create an estimator, optionally specifying parameters\nmodel = DecisionTreeRegressor()\n# fit the estimator to the data\nmodel.fit(X_train,y_train)\n# apply the model to the test and training data\npredicted_test_y = model.predict(X_test)\npredicted_train_y = model.predict(X_train)\ndef scatter_y(y_test, predicted_y):\n    \"\"\"Scatter-plot the predicted vs true number of rings\n    \n    Plots:\n       * predicted vs true number of rings\n       * perfect agreement line\n       * +2\/-2 number dotted lines\n\n    Returns the root mean square of the error\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(y_test, predicted_y, '.k')\n    \n    ax.plot([0, 30], [0, 30], '--k')\n    ax.plot([0, 30], [2, 32], ':k')\n    ax.plot([2, 32], [0, 30], ':k')\n    \n    rms = (y_test - predicted_y).std()\n    \n    ax.text(25, 3,\n            \"Root Mean Square Error = %.2g\" % rms,\n            ha='right', va='bottom')\n\n    ax.set_xlim(0, 30)\n    ax.set_ylim(0, 30)\n    \n    ax.set_xlabel('True age')\n    ax.set_ylabel('Predicted age')\n    \n    return rms\nscatter_y(y_train, predicted_train_y)\nplt.title(\"Training data\")\nscatter_y(y_test, predicted_test_y)\nplt.title(\"Test data\");","fc00c337":"model = DecisionTreeRegressor(max_depth=10)\n# fit the estimator to the data\nmodel.fit(X_train,y_train)\n# apply the model to the test and train data\npredicted_test_y = model.predict(X_test)\npredicted_train_y = model.predict(X_train)\n\nscatter_y(y_train, predicted_train_y)\nplt.title(\"Training data\")\nrms_decision_tree = scatter_y(y_test, predicted_test_y)\nplt.title(\"Test data\");","8f7a7fd7":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=10)\nmodel.fit(X_train, y_train)\npredicted_test_y = model.predict(X_test)\nrms_random_forest = scatter_y(y_test, predicted_test_y)","7ac90c98":"df[\"age\"].value_counts()","819afa6f":"df_1 = df.copy()\nAge = []\nfor i in df_1[\"age\"]:\n    if i < 9.33:\n        Age.append(\"1\")\n    if i > 9.33 and i< 18.66 :\n        Age.append(\"2\")\n    if i > 18.66:\n        Age.append(\"3\")\ndf_1[\"Age\"] = Age\ndf_1.drop(\"age\" , axis =1,inplace=True)\ndf_1.head()","0061bda2":"X = df_1[['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n       'Viscera weight', 'Shell weight']]\nY = df_1[\"Age\"]\nfrom sklearn.model_selection import train_test_split\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split( X, Y, test_size=0.33, random_state=42)","74b5ccc3":"scaler = StandardScaler()\nscaler.fit_transform(X_train_1)","e864236b":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train_1, y_train_1)\ny_pred = clf.predict(X_test)\nprint(\"Test score \"+str(clf.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(clf.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","6e3e066d":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train_1, y_train_1)\ny_pred = neigh.predict(X_test)\nprint(\"Test score \"+str(neigh.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(neigh.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))\n\nerror_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors= i)\n    knn.fit(X_train, y_train)\n    y_predi = knn.predict(X_test)\n    error_rate.append(np.mean(y_test != y_predi))\n    \nplt.figure(figsize = (10,8))\nplt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.show()","338e1854":"from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nregr = make_pipeline(StandardScaler(), SVC(gamma = 'auto'))\n\nregr.fit(X_train_1, y_train_1)\n\ny_pred = regr.predict(X_test_1)\nprint(\"Test score \"+str(regr.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(regr.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","eb80affd":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(X_train_1, y_train_1)\ny_pred = dtc.predict(X_test)\nprint(\"Test score \"+str(dtc.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(dtc.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))\n","fd4332c7":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_train_1, y_train_1)\ny_pred = clf.predict(X_test)\nprint(\"Test score \"+str(clf.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(clf.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","5e34ad34":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train_1, y_train_1)\ny_pred = clf.predict(X_test)\nprint(\"Test score \"+str(clf.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(clf.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","489ca811":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(random_state=0)\ngbc.fit(X_train_1, y_train_1)\ny_pred = gbc.predict(X_test_1)\nprint(\"Test score \"+str(gbc.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(gbc.score(X_train_1, y_train_1)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","f8936aa1":"from sklearn.utils import resample\nX_ = pd.concat([X_train_1, y_train_1], axis=1)\n\n# separate minority and majority classes\nlabel1 = X_[X_.Age== \"1\"]\nlabel2 = X_[X_.Age== \"2\"]\nlabel3 = X_[X_.Age== \"3\"]\n# upsample minority\nlabel3_upsampled = resample(label3,\n                          replace=True, # sample with replacement\n                          n_samples=len(label2), # match number in majority class\n                          random_state=27) # reproducible results\nlabel1_upsampled = resample(label1,\n                                replace = True, # sample without replacement\n                                n_samples = len(label2), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine majority and upsampled minority\nsampled = pd.concat([label2,label1_upsampled,label3_upsampled])\n\n# check new class counts\nprint(sampled.Age.value_counts())\n\ny_train_s= sampled.Age\nX_train_s = sampled.drop('Age', axis=1)","3c59babe":"scaler = StandardScaler()\nscaler.fit_transform(X_train_s)","352f7ddc":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train_s, y_train_s)\ny_pred = clf.predict(X_test_1)\nprint(\"Test score \"+str(clf.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(clf.score(X_train_s, y_train_s)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","3b499ccc":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train_s, y_train_s)\ny_pred = neigh.predict(X_test)\nprint(\"Test score \"+str(neigh.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(neigh.score(X_train_s, y_train_s)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))\n\nerror_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors= i)\n    knn.fit(X_train, y_train)\n    y_predi = knn.predict(X_test)\n    error_rate.append(np.mean(y_test != y_predi))\n    \nplt.figure(figsize = (10,8))\nplt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.show()","46d2ee02":"from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nregr = make_pipeline(StandardScaler(), SVC(gamma = 'auto'))\n\nregr.fit(X_train_s, y_train_s)\n\ny_pred = regr.predict(X_test_1)\nprint(\"Test score \"+str(regr.score(X_test_1, y_test_1)))\nprint(\"Train score \"+str(regr.score(X_train_s, y_train_s)))\nfrom sklearn.metrics import classification_report\nprint(\"Classification report :\")\nprint(classification_report(y_test_1, y_pred))","d54d3702":"# DESCRIPTION\nAbalone is a mollusc with a peculiar ear-shaped shell lined of mother of pearl. Its age can be estimated counting the number of rings in their shell with a microscope, but it is a time consuming process, in this tutorial we will use Machine Learning to predict the age using physical measurements.","2e4c782f":"# Data Visualisation :","47ea282b":"# DECISION TREE REGRESSOR\n*We can visualize the results with a scatter-plot of the true age against the predicted age:*","f87a9dbb":"# RANDOM FOREST REGRESSOR","707cb223":"# We specify a maximum depth of the decision tree of  10 , so that the estimator does not \"specialize\" too much on the training data.","3419a060":"# Heatmap\nTo Find Correlation :","5b5ff9c4":"# Scaling data","27a7132a":"# RESAMPLING DATA","7b7327b1":"# Split data in Training and Test sets\nWe can split the data into training and validation sets and use Machine Learning to create an estimator that can learn from the training set and then check its performance on the test set.","9f6017e0":"# Objective:\n    The objective of this project is to predicting the age of abalone from physical measurements \n    using the 1994   abalone data \"The Population Biology of Abalone (Haliotis species) in Tasmania. I.\n    Blacklip Abalone (H.rubra) from the North Coast and Islands of Bass Strait\".\n    \n\n# Key insights :\n\n        - No missing values in the dataset\n        - All float data type but 'sex'\n        - Though features are not normaly distributed, are close to normality\n        - None of the features have minimum = 0 except Height (requires re-check)\n        - Each feature has difference scale range\n        ","54b8e642":"# REGRESSION ALGORITHMS USED:\n* Linear Regression\n* Decision Tree Regressor\n* Support Vector Regressor\n\n# CLASSIFICATION ALGORITHMS USED:\n* Logistic Regression\n* K-Nearest Neighbour\n* Support Vector Classifier\n* Decision Tree Classifier\n* Ada Boosting Classifier\n* Gradient Boosting Classifier"}}