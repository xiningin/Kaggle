{"cell_type":{"5d9ee49c":"code","8ada3513":"code","80377778":"code","26abef07":"code","20f6d189":"code","a101fca9":"code","2f6f2fe8":"code","73fd6586":"code","52e4dbf1":"code","8da478dd":"code","7282e3f1":"code","7a94ba30":"code","3e6fc84a":"code","4a76dc49":"code","45003df4":"code","a1fcf45c":"code","8f384ee1":"code","1c1d5b30":"code","4b598eca":"code","d7ee59fa":"code","100bf868":"code","39b0f527":"code","71867a9f":"markdown","3e6884d5":"markdown","358eb7df":"markdown","0d5bf1f4":"markdown","d8ba657a":"markdown","617fb605":"markdown","523d68aa":"markdown","c391f2c4":"markdown","27033bc2":"markdown","9584003c":"markdown","8f77340d":"markdown","5bf45c66":"markdown","f9943910":"markdown","8d6d56ca":"markdown","a607e42f":"markdown","5e0be0d3":"markdown","d2b679da":"markdown","c0390985":"markdown","2c438bb3":"markdown","f67fc431":"markdown","0e5968d4":"markdown","108a23cd":"markdown","627389b6":"markdown","1475e793":"markdown","7b5aeecc":"markdown","77af99d3":"markdown","9f4a6db7":"markdown"},"source":{"5d9ee49c":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport urllib.request as urllib\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport statsmodels.stats.api as sms\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.graphics.gofplots import ProbPlot\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import summary_table\nfrom numpy.polynomial.polynomial import polyfit","8ada3513":"''' getting data '''\ndf_main = pd.read_csv('..\/input\/SkillCraft.csv')","80377778":"''' skimming data '''\ndf_main.head(20)","26abef07":"''' splitting data '''\n# using seed 111 for controlled test\ndf_train, df_test = train_test_split(df_main, test_size=0.2, random_state=111)","20f6d189":"''' checking if data looks like a normal distribution '''\ndf_league_index_distr = (df_train['LeagueIndex'].value_counts()).to_frame()\ndf_league_index_distr.sort_index(inplace=True)\ndf_league_index_distr.plot(kind='bar',y='LeagueIndex',colormap='Paired')\nplt.show()","a101fca9":"''' Linear Aggression : Full Model and exlcuding unsignificant p-value '''\n# Dependent Variable\ny = df_train[['LeagueIndex']]\n\n# 1st Model : Full Model\nremoved_cols = ['GameID','LeagueIndex']\nx_1 = df_train.drop(removed_cols, axis=1)\nmodel_1 = sm.OLS(y,x_1).fit()\nprint(model_1.summary())","2f6f2fe8":"# 2nd Model : Excluding Independent Variable w\/ p-values lower than 0.05\nremoved_cols = removed_cols + ['TotalHours','MinimapRightClicks','UniqueUnitsMade',\n                   'ComplexUnitsMade','ComplexAbilitiesUsed']\nx_2 = df_train.drop(removed_cols, axis=1)\nmodel_2 = sm.OLS(y,x_2).fit()\nprint(model_2.summary())","73fd6586":"''' checking for outliers and try to reduce outliers that are big in size and a bit extreme'''\n# checking for outliers\ndep_var = list(y)[0]\nindep_var_list = list(x_2)\n\nfor indep_var in indep_var_list:\n    df_train.boxplot(column=indep_var,by=dep_var)\n    plt.title(dep_var + ' vs ' + indep_var)\n    plt.suptitle('')\nplt.show()\n\n# # removing 2 variables : SelectByHotkeys and MinimapAttacks\n# removed_cols = removed_cols + ['SelectByHotkeys','MinimapAttacks']\n\n# # 3rd Model : Excluding certain variable with outliers that seem to be extreme\n# x_3 = df_train.drop(removed_cols, axis=1)\n# model_3 = sm.OLS(y,x_3).fit()\n# print(model_3.summary())\n\n# # 4th Model : Removing insignificant p-values\n# removed_cols = removed_cols + ['ActionLatency','TotalMapExplored']\n# x_4 = df_train.drop(removed_cols, axis=1)\n# model_4 = sm.OLS(y,x_4).fit()\n# print(model_4.summary())","52e4dbf1":"''' Checking for multicollinearity (using VIF) '''\n# adding a constant to correct output\nmc_x = add_constant(x_2)\npd.Series([variance_inflation_factor(mc_x.values, i) for i in range(mc_x.shape[1])],index=mc_x.columns)","8da478dd":"# 5th Model : using Model 2 and reducing variables\nremoved_cols = removed_cols + ['APM']\nx_5 = df_train.drop(removed_cols, axis=1)\nmodel_5 = sm.OLS(y,x_5).fit()\n# testing with VIF again\nmc_x = add_constant(x_5)\npd.Series([variance_inflation_factor(mc_x.values, i) for i in range(mc_x.shape[1])],index=mc_x.columns)","7282e3f1":"# reducing NumberOfPACs\nmodel_6_removed_cols = removed_cols + ['NumberOfPACs']\nx_6 = df_train.drop(model_6_removed_cols, axis=1)\nmodel_6 = sm.OLS(y,x_6).fit()\n# testing with VIF again\nmc_x = add_constant(x_6)\npd.Series([variance_inflation_factor(mc_x.values, i) for i in range(mc_x.shape[1])],index=mc_x.columns)","7a94ba30":"# reducing the other variable\nmodel_7_removed_cols = removed_cols + ['ActionLatency']\nx_7 = df_train.drop(model_7_removed_cols, axis=1)\nmodel_7 = sm.OLS(y,x_7).fit()\n# testing with VIF again\nmc_x = add_constant(x_7)\npd.Series([variance_inflation_factor(mc_x.values, i) for i in range(mc_x.shape[1])],index=mc_x.columns)","3e6fc84a":"# print(model_6.summary())\n# print(model_7.summary())\n\n''' Comparing Linear Models with ANOVA '''\nanova_result = anova_lm(model_6, model_7)\nprint(anova_result)","4a76dc49":"print(model_7.summary())","45003df4":"# Fitted vs Residual (weird...)\nfr_plot = plt.figure(1)\nfr_plot.axes[0] = sns.residplot(x=model_7.fittedvalues, y=model_7.resid,\n                          lowess=True, scatter_kws={'alpha': 0.5},\n                          line_kws={'color':'red', 'lw':1, 'alpha':0.8})\n\nfr_plot.axes[0].set_title('Residuals vs Fitted')\nfr_plot.axes[0].set_xlabel('Fitted values')\nfr_plot.axes[0].set_ylabel('Residuals')\nplt.show()","a1fcf45c":"# Normal QQ Plot\nQQ = ProbPlot(model_7.resid)\nqq_plot = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nqq_plot.axes[0].set_title('Normal QQ')\nqq_plot.axes[0].set_xlabel('Theoretical Quantiles')\nqq_plot.axes[0].set_ylabel('Standardized Residuals')\nplt.show()","8f384ee1":"anderson_result = stats.anderson(model_7.resid)\nanderson_result[1][2]","1c1d5b30":"# getting predicted values\nx_predict = df_test.drop(model_7_removed_cols, axis=1)\nprediction = model_7.predict(x_predict)\nprediction.head()","4b598eca":"# adding the difference between actual and predicted values in a new column\ndf_final = pd.concat([df_test['LeagueIndex'],prediction], axis=1)\ndf_final.columns = ['ActualLeagueIndex','PredictedLeagueIndex']\ndf_final = df_final.assign(Difference=abs(df_final.ActualLeagueIndex-df_final.PredictedLeagueIndex))\ndf_final.head(20)","d7ee59fa":"# skimming how much the average is\ndf_final['Difference'].mean()","100bf868":"plt.scatter(df_final['ActualLeagueIndex'], df_final['PredictedLeagueIndex'], edgecolors=(0, 0, 0))\nplt.plot([df_final['ActualLeagueIndex'].min(), df_final['ActualLeagueIndex'].max()], \n         [df_final['ActualLeagueIndex'].min(), df_final['ActualLeagueIndex'].max()], 'k-', lw=4)\nplt.xlabel('Measured')\nplt.ylabel('Predicted')\nplt.show()","39b0f527":"st, data, ss2 = summary_table(model_7, alpha=0.05)\n\nfitted_values = data[:, 2]\npredict_mean_low, predict_mean_upp = data[:, 4:6].T\npredict_low, predict_upp = data[:, 6:8].T\n\nplt.scatter(df_final['PredictedLeagueIndex'], df_final['ActualLeagueIndex'])\n# Fit with polyfit\nn_fit, m_fit = polyfit(df_train['LeagueIndex'], fitted_values, 1)\nplt.plot(df_train['LeagueIndex'], n_fit + m_fit * df_train['LeagueIndex'], '-', color='black', lw=1)\nn_low, m_low = polyfit(df_train['LeagueIndex'], predict_low, 1)\nplt.plot(df_train['LeagueIndex'], n_low + m_low * df_train['LeagueIndex'], ':', color='red', lw=1)\nn_upp, m_upp = polyfit(df_train['LeagueIndex'], predict_upp, 1)\nplt.plot(df_train['LeagueIndex'], n_upp + m_upp * df_train['LeagueIndex'], ':', color='red', lw=1)\nn_mean_low, m_mean_low = polyfit(df_train['LeagueIndex'], predict_mean_low, 1)\nplt.plot(df_train['LeagueIndex'], n_mean_low + m_mean_low * df_train['LeagueIndex'], ':', color='#ffa500', lw=1)\nn_mean_upp, m_mean_upp = polyfit(df_train['LeagueIndex'], predict_mean_upp, 1)\nplt.plot(df_train['LeagueIndex'], n_mean_upp + m_mean_upp * df_train['LeagueIndex'], ':', color='#ffa500', lw=1)\nplt.show()","71867a9f":"## Predicted Values with Final Model and Test Set\n\nSo I'd need to input the test inpendent variables into the Final Model and check if the predicted LeagueIndex is within the confidence or predicted interval. If this has good results, usually it means the Final Model is working pretty well.\n\n<font color='red'>This was the point when I realized doing a linear regression on this dataset is not the correct technique. So, from here the process and analysis I do on the numbers and graphs aren't supposed to work. However, I will keep doing the traditional process of linear regression for the sake of practice\/exercise.<\/font>\n\n","3e6884d5":"# Introduction\n**Author** Jinwoo Chung\n\n### Abstract\nI've tried a linear regession on R and thought it'd be a good experience to do one with Python, so here it is. Thought this dataset would work well with linear regression. So first, I started a Full Model.\n* **Dependent Variable** LeagueIndex\n* **Independent Variable** Other Variables\n\nI've gone through chopping variables with insignificant p-value first. After reduction, I've tried checking for weird or heavily affecting outliers, but did not improve the model. Stepped back to the model after insignficant p-value reduction, I've cut off variables according to high VIF values (i.e. variables with multicolinearity). In the end the final model was:\n\nLeagueIndex = 0.0212(Age) + 0.0099(HoursPerWeek) + 31.3489(SelectByHotkeys) + 874.1684(AssignToHotkeys) + 0.0297(UniqueHotkeys) + 1050.7058(MinimapAttacks) + 692.5465(NumberOfPACs) - 0.0104(GapBetweenPACs) + 0.1599(ActionsInPAC) - 0.0069(TotalMapExplored) + 185.1212(WorkersMade)\n\nThe final model's prediction with test value yielded on average, ~ \u00b11 LeagueIndex. The confidence\/predicted interval graph also showed that large chunks of data did not fit into the confidence interval. Predicted interval was able to fit majority of the data, but still had handful of data that wasn't within the interval.\n\nFinal model's residual analysis showed some problems. Normality, independence, and linearity tests showed strong evidence that these assumptions are satisfied, but the dataset had some heteroscedasticity issues (w\/ hindsight, this is expected since League Index is not continuous form of data).\n\n### Evaluation (After thoughts...)\nSince the independent variable is not linear (in R terms, the LeagueIndex is a factor type), using linear regression was a bad choice. Should've chose a different form of regression or use machine learning that is supposed to work on categorical\/discrete Y. But, I was already at the phase of checking if the predicted values were similar to the actual values, so I just made a kernel about how linear regression progress went. Based on [this post](http:\/\/blog.minitab.com\/blog\/michelle-paret\/regression-versus-anova-which-tool-to-use-when) I should've done a logistic regression or some other sort of machine learning techniques. <font color='red'>**Lesson learnt** : understand the data before tackling it blindly!<\/font>\n\n### Additionally...\nIf there are any ways I could improve this kernel and my model, please leave them in the comments! :^)","358eb7df":"## Residual Analysis\n\nNormally you have 4 different graph (which are generated pretty easily in R...) before going through each assumptions (Normality, Heteroscedasticity, Independence, Linearity). I'm just going to make the important 2 graphs (Fitted vs Residual and Normal Q-Q).","0d5bf1f4":"The average is not as good as I wanted it to be. Predicting ~ \u00b11 LeagueIndex isn't so good considering there are only 7 leagues. Let's look at it with graph.","d8ba657a":"By just looking through the numbers given by the OLS, most of the numbers seems pretty good (Adj. R Squared, Durbin_watson, Skew, Kurtosis, etc.). However, insignficiant p-values can be seen in few variables, so I'm going to remove them.","617fb605":"The Normal QQ plot seems pretty standard.","523d68aa":"It's a bit skewed to the right, but overall it seems close to a normal distribution.\n\n## Generating Models","c391f2c4":"### Independence\n\nLike Heteroscedasticity, I don't need to go in lengths for this one either. If Durbin-Watson value is close to 2, it shows strong evidence that Independence is supported. Since Final Model (7th Model) had Durbin-Watson value of 1.999, I don't need to go on further.","27033bc2":"Although they dont differ much, I chose 7th Model. Here is the Final Model summary.","9584003c":"### Linearity\n\nI'm not so sure about this. I've Googled about how to how to check for Linearity assumption and the one I read that sees the most reliable was this [post](http:\/\/people.duke.edu\/~rnau\/testing.htm). Says I should check the graph for Residuals (or Observed) vs Predicted Value. I have to get back to this later.","8f77340d":"Just by the look of VIF, 6th Model and 7th Model seems pretty similar, so we could either check by looking at their summaries or use ANOVA to see which Model is slightly better.","5bf45c66":"## Setting up and Skimming Data","f9943910":"Let's first get the predicted values and then see it with actual (observed) values","8d6d56ca":"Yikes! Time for intervals...","a607e42f":"## Final Words\n\nI learned linear regression in a very short time with only few practices, so I wanted to just try it again but with a different Programming Language (previuosly used R). In my opinion, R is a better and easier tool to use for linear regression, with the downside of being slow if the data is huge.\n\nI also wanted to try making a kernel on Kaggle.com. I'm going to work on Logistic Regression (if it works well with the data) or some machine learning technique that deals with categorical\/discrete Y with this next :P\n\nI appreciate any feedbacks and teachings, so please leave a comment if you have something to share! :^)","5e0be0d3":"It's linear regression, so the predicted LeagueIndex will not be a discrete value like the the actual results. Now onto looking at the data with the actual values and difference","d2b679da":"### Heteroscedasticity\n\nI don't even have to go through depths for this one because just by looking at the **Fitted vs Residual Graph**, we can already conclude that Heteroscedasticity exists within the residuals. In R, there's additional statistical way to check this assumption with NCV test, but I couldn't find it in Python (tried Dr. Google, but yielded no fruit). <font color='red'>**Hindsight** this was going to be a problem because the Y (dependent variable) is a discrete value!<\/font>\n","c0390985":"I've had to google a bit about doing VIF on Python. The most helpful post I've found was [this](https:\/\/stackoverflow.com\/questions\/42658379\/variance-inflation-factor-in-python). Basically it explains why I used add_constant to find the VIF.\n\nAPM has a **huge** VIF value and it somewhat makes sense. APM is how much keyboard and mouse inputs that the player does within a minute. If this number is large, other variables such as SelectByHotkeys, AssignToHotkeys, etc will be affected. Makes sense to remove APM.","2c438bb3":"The graph has 7 different linear like equations. I think it's because the LeagueIndex are exactly a whole number. I'm not too sure, so I will have to get back to this later.","f67fc431":"I wasn't sure how to get data from Kaggle, so I've uploaded the .csv file on my github and got it from there.","0e5968d4":"Let's look at the average of how much different it is from Predicted vs Actual","108a23cd":"On 2nd Model summary, it gives a warning about multicolinearity or other numerical problems, so I went at tested the independent variables with VIF (Variance Inflation Factor).","627389b6":"Now the overall model seems pretty good. At this point, I tried using outliers to reduce more variables, but yielded no fruit. It made the model worse. I will include the code, but will not run it on this kernel.","1475e793":"Even with dropping APM, there's are few variables with strong evidence for multicolinearity, so I tried reducing each one separately.","7b5aeecc":"### Normality\n\nUsually when you do normality test, you can either just look at the Histogram of the Residuals or Normal Q-Q Graph. There are other statistical ways to get strong evidence that the residuals supports normality. Popular ones include : Shapiro-Wilk, Kolmogorov-Smirnov, and Anderson-Darling. Usually Shaprio-Wilk is very sensitive, so this test is usually used for small data sets. I've decided to use Anderson-Darling.","77af99d3":"* Confidence Interval : Red\n* Predicted Interval : Orange\n\nOverall it's pretty bad... :( ... I don't think I need to statitically to see how many values it actually goes within the intervals.","9f4a6db7":"Since the p-value (```anderson_result[1][2]```) is higher than 0.05, it leans toward the Null Hypothesis, which means that there is a strong evidence that the residuals support normality."}}