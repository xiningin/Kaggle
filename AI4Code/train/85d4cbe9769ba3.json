{"cell_type":{"fe5e539e":"code","4961f1ac":"code","bc59fe45":"code","8cd207d9":"code","558ab377":"code","05e256b8":"code","0cedb8d2":"code","c54bd7fb":"code","45c18ae3":"code","409b8cbf":"code","aeb632ac":"code","bfb60573":"code","da0c561c":"code","ad0967d1":"code","c375d621":"code","d8063b94":"markdown","2d4b3f7b":"markdown","ea983ba1":"markdown","133b1dd3":"markdown","862f9a26":"markdown","6ac41a79":"markdown","d19f3daf":"markdown","c992462a":"markdown","855bf134":"markdown","a1306f18":"markdown","70312743":"markdown","4fd0e17f":"markdown","d313fc69":"markdown","043a7642":"markdown"},"source":{"fe5e539e":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression","4961f1ac":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_solution = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\ndf_train['kfold'] = -1\n\ny_train = df_train.claim\nX_train = df_train.drop('claim', axis=1)\n\nskf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (i_train, i_valid) in enumerate (skf.split(X_train, y_train)):\n    df_train.loc[i_valid, 'kfold'] = fold","bc59fe45":"m = 1\n\nfor i in range(20):\n    for s in range(5):\n        df_train_temp = pd.read_csv(f'..\/input\/tps-09-2021-base-predictions\/m{m}s{s}_valid_pred.csv')\n        df_test_temp = pd.read_csv(f'..\/input\/tps-09-2021-base-predictions\/m{m}s{s}_test_pred.csv')\n        df_train = df_train.merge(df_train_temp, on='id', how='left')\n        df_test = df_test.merge(df_test_temp, on='id', how='left')\n    preds_temp = [c for c in df_test.columns if f'm{m}' in c]\n    df_train[f'm{m}_mean_pred'] = df_train[preds_temp].mean(axis=1)\n    df_test[f'm{m}_mean_pred'] = df_test[preds_temp].mean(axis=1)\n    m += 1","8cd207d9":"# seed = 0\n\n# features = [c for c in df_train.columns if 'mean' in c]\n# df_test = df_test[features]\n\n# def objective(trial):\n#     fold = 0\n#     params = {\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 20),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1000, 20000),\n#         'max_depth': trial.suggest_int('max_depth', 0, 4),\n#         'max_bin': trial.suggest_int('max_bin', 200, 400),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5),\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.00001, 50),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.00001, 50),\n#         'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 4),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.75, 0.9),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 1)        \n#     }\n\n#     X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n#     X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n        \n#     y_train = X_train.claim\n#     y_valid = X_valid.claim\n    \n#     X_train = X_train[features]\n#     X_valid = X_valid[features]\n    \n#     model = LGBMClassifier(\n#             objective='binary',\n#             tree_learner='serial',\n#             seed=seed,\n#             n_estimators=50000,\n#             **params)\n    \n#     model.fit(X_train,\n#               y_train,\n#               early_stopping_rounds=500,\n#               eval_set=[(X_valid, y_valid)],\n#               eval_metric='auc',\n#               callbacks=[LightGBMPruningCallback(trial, 'auc')],\n#               verbose=1000)\n    \n#     valid_pred = model.predict_proba(X_valid)[:,1]\n        \n#     auc = roc_auc_score(y_valid, valid_pred)\n#     return auc\n\n# for i in range(10):\n#     study = optuna.create_study(direction=\"maximize\")\n#     study.optimize(objective, n_trials=500)","558ab377":"features = [c for c in df_train.columns if 'mean' in c]\ndf_test = df_test[features]\n\ntest_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    params = {'num_leaves': 16,\n              'min_data_in_leaf': 8056,\n              'max_depth': 0,\n              'max_bin': 355,\n              'learning_rate': 0.008529108246972048,\n              'lambda_l1': 9.855426024301865,\n              'lambda_l2': 3.584196138812915e-05,\n              'min_gain_to_split': 0.42289363518494794,\n              'feature_fraction': 0.42929842197137486,\n              'bagging_fraction': 0.8422719204115733,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=fold,\n        n_estimators=5000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'pred_1']\nvalid_preds.to_csv(f'level1_valid_pred_1.csv', index=False)\n\nsample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.columns = ['id', f'pred_1']\nsample_solution.to_csv(f'level1_test_pred_1.csv', index=False)","05e256b8":"test_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = {'num_leaves': 4,\n              'min_data_in_leaf': 11557,\n              'max_depth': 1,\n              'max_bin': 330,\n              'learning_rate': 0.009478273001225938,\n              'lambda_l1': 1.17273085019482e-05,\n              'lambda_l2': 1.1150847201215146,\n              'min_gain_to_split': 0.6849430970013546,\n              'feature_fraction': 0.17224803018739418,\n              'bagging_fraction': 0.8745864558511497,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=fold,\n        n_estimators=10000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'pred_2']\nvalid_preds.to_csv(f'level1_valid_pred_2.csv', index=False)\n\nsample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.columns = ['id', f'pred_2']\nsample_solution.to_csv(f'level1_test_pred_2.csv', index=False)","0cedb8d2":"test_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = {'num_leaves': 4,\n              'min_data_in_leaf': 1458,\n              'max_depth': 2,\n              'max_bin': 376,\n              'learning_rate': 0.006005480922506577,\n              'lambda_l1': 0.00021562065637690435,\n              'lambda_l2': 4.87408695501209,\n              'min_gain_to_split': 1.97067333984107,\n              'feature_fraction': 0.8666273347397625,\n              'bagging_fraction': 0.8743916816302966,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=fold,\n        n_estimators=10000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'pred_3']\nvalid_preds.to_csv(f'level1_valid_pred_3.csv', index=False)\n\nsample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.columns = ['id', f'pred_3']\nsample_solution.to_csv(f'level1_test_pred_3.csv', index=False)","c54bd7fb":"test_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    params = {'num_leaves': 15,\n              'min_data_in_leaf': 6512,\n              'max_depth': 3,\n              'max_bin': 310,\n              'learning_rate': 0.004125972490699574,\n              'lambda_l1': 0.003690793308308078,\n              'lambda_l2': 0.3051576539377836,\n              'min_gain_to_split': 0.2990942549901452,\n              'feature_fraction': 0.918317541041119,\n              'bagging_fraction': 0.7574750171832634,'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=fold,\n        n_estimators=10000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'pred_4']\nvalid_preds.to_csv(f'level1_valid_pred_4.csv', index=False)\n\nsample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.columns = ['id', f'pred_4']\nsample_solution.to_csv(f'level1_test_pred_4.csv', index=False)","45c18ae3":"test_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n\n    params = {'num_leaves': 20,\n              'min_data_in_leaf': 19960,\n              'max_depth': 4,\n              'max_bin': 331,\n              'learning_rate': 0.0019637756042711558,\n              'lambda_l1': 47.21817822491236,\n              'lambda_l2': 0.00045229975521610875,\n              'min_gain_to_split': 2.1533850359022284,\n              'feature_fraction': 0.10469850378121182,\n              'bagging_fraction': 0.8956146814653573,\n              'bagging_freq': 1}\n\n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=fold,\n        n_estimators=10000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')\n\nvalid_preds = pd.DataFrame.from_dict(valid_preds, orient='index').reset_index()\nvalid_preds.columns = ['id', f'pred_5']\nvalid_preds.to_csv(f'level1_valid_pred_5.csv', index=False)\n\nsample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.columns = ['id', f'pred_5']\nsample_solution.to_csv(f'level1_test_pred_5.csv', index=False)","409b8cbf":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_solution = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\ndf_train['kfold'] = -1\n\ny_train = df_train.claim\nX_train = df_train.drop('claim', axis=1)\n\nskf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (i_train, i_valid) in enumerate (skf.split(X_train, y_train)):\n    df_train.loc[i_valid, 'kfold'] = fold\n    \ndf_train1 = pd.read_csv('level1_valid_pred_1.csv')\ndf_train2 = pd.read_csv('level1_valid_pred_2.csv')\ndf_train3 = pd.read_csv('level1_valid_pred_3.csv')\ndf_train4 = pd.read_csv('level1_valid_pred_4.csv')\ndf_train5 = pd.read_csv('level1_valid_pred_5.csv')\n\ndf_test1 = pd.read_csv('level1_test_pred_1.csv')\ndf_test2 = pd.read_csv('level1_test_pred_2.csv')\ndf_test3 = pd.read_csv('level1_test_pred_3.csv')\ndf_test4 = pd.read_csv('level1_test_pred_4.csv')\ndf_test5 = pd.read_csv('level1_test_pred_5.csv')\n\ndf_train = df_train.merge(df_train1, on='id', how='left')\ndf_train = df_train.merge(df_train2, on='id', how='left')\ndf_train = df_train.merge(df_train3, on='id', how='left')\ndf_train = df_train.merge(df_train4, on='id', how='left')\ndf_train = df_train.merge(df_train5, on='id', how='left')\n\ndf_test = df_test.merge(df_test1, on='id', how='left')\ndf_test = df_test.merge(df_test2, on='id', how='left')\ndf_test = df_test.merge(df_test3, on='id', how='left')\ndf_test = df_test.merge(df_test4, on='id', how='left')\ndf_test = df_test.merge(df_test5, on='id', how='left')","aeb632ac":"# pred_1 = df_test1.pred_1\n# pred_2 = df_test2.pred_2\n# pred_3 = df_test3.pred_3\n# pred_4 = df_test4.pred_4\n# pred_5 = df_test5.pred_5\n\n# features = ['pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']\n# df_test = df_test[features]\n\n# def objective(trial):\n    \n#     w_1 = trial.suggest_uniform('w1', 0, 1)\n#     w_2 = trial.suggest_uniform('w2', 0, 1)\n#     w_3 = trial.suggest_uniform('w3', 0, 1)\n#     w_4 = trial.suggest_uniform('w4', 0, 1)\n#     w_5 = trial.suggest_uniform('w5', 0, 1)\n\n#     X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n#     X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n#     y_train = X_train.claim\n#     y_valid = X_valid.claim\n\n#     X_train = X_train[features]\n#     X_valid = X_valid[features]\n\n#     pred_1 = X_valid.pred_1\n#     pred_2 = X_valid.pred_2\n#     pred_3 = X_valid.pred_3\n#     pred_4 = X_valid.pred_4\n#     pred_5 = X_valid.pred_5\n\n#     pred = (w_1 * pred_1 + w_2 * pred_2 + w_3 * pred_3 + w_4 * pred_4 + w_5 * pred_5) \/ 5\n\n#     auc = roc_auc_score(y_valid, pred)\n    \n#     return auc\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=400)","bfb60573":"# seed = 0\n\n# features = ['pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']\n# df_test = df_test[features]\n\n# def objective(trial):\n#     fold = 0\n#     params = {\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 20),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1000, 20000),\n#         'max_depth': trial.suggest_int('max_depth', 0, 4),\n#         'max_bin': trial.suggest_int('max_bin', 200, 400),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5),\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.00001, 50),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.00001, 50),\n#         'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 4),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.75, 0.9),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 1)        \n#     }\n\n#     X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n#     X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n        \n#     y_train = X_train.claim\n#     y_valid = X_valid.claim\n    \n#     X_train = X_train[features]\n#     X_valid = X_valid[features]\n    \n#     model = LGBMClassifier(\n#             objective='binary',\n#             tree_learner='serial',\n#             seed=seed,\n#             n_estimators=50000,\n#             **params)\n    \n#     model.fit(X_train,\n#               y_train,\n#               early_stopping_rounds=500,\n#               eval_set=[(X_valid, y_valid)],\n#               eval_metric='auc',\n#               callbacks=[LightGBMPruningCallback(trial, 'auc')],\n#               verbose=1000)\n    \n#     valid_pred = model.predict_proba(X_valid)[:,1]\n        \n#     auc = roc_auc_score(y_valid, valid_pred)\n#     return auc\n\n# for i in range(1):\n#     study = optuna.create_study(direction=\"maximize\")\n#     study.optimize(objective, n_trials=5000)","da0c561c":"features = ['pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']\ndf_test = df_test[features]\n\ntest_preds = []\nvalid_preds = {}\nscores = []\n\nfor fold in range(5):\n    X_train = df_train[df_train.kfold != fold].reset_index(drop=True)\n    X_valid = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    X_test = df_test[features].copy()\n\n    valid_ids = X_valid.id.values.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n\n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n#     model = LogisticRegression()\n#     model.fit(X_train, y_train)\n\n    params = {'num_leaves': 20,\n              'min_data_in_leaf': 12875,\n              'max_depth': 0,\n              'max_bin': 268,\n              'learning_rate': 0.00480893584809694,\n              'lambda_l1': 0.00020929737984329072,\n              'lambda_l2': 0.0006179415934221924,\n              'min_gain_to_split': 2.7035405403285804,\n              'feature_fraction': 0.8117162013587371,\n              'bagging_fraction': 0.8175123327757648,\n              'bagging_freq': 1}\n    \n    model = LGBMClassifier(\n        objective='binary',\n        importance_type='split', #default=split. try gain\n        boosting_type='gbdt', #default=gbdt. try dart, goss, rf\n        tree_learner='serial',\n        num_threads=-1,\n        random_state=42,\n        n_estimators=10000,\n        **params)\n    \n    model.fit(X_train,\n          y_train,\n          early_stopping_rounds=500,\n          eval_set=[(X_valid, y_valid)],\n          eval_metric='auc',\n          verbose=1000)\n    \n    valid_pred = model.predict_proba(X_valid)[:,1]\n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    valid_preds.update(dict(zip(valid_ids, valid_pred)))\n    test_preds.append(test_pred)\n    \n    score = roc_auc_score(y_valid, valid_pred)    \n    scores.append(score)\n    \nprint(f'Mean auc {np.mean(scores)}, std {np.std(scores)}')","ad0967d1":"sample_solution.claim = np.mean(np.column_stack(test_preds), axis=1)\nsample_solution.to_csv('submission.csv', index=False)","c375d621":"# features = ['pred_1', 'pred_2', 'pred_3', 'pred_4', 'pred_5']\n# df_test = df_test[features]\n\n# df_test['mean'] = df_test[features].mean(axis=1)\n\n# sample_solution.claim = df_test['mean']\n# sample_solution.to_csv(f'stack_blend.csv', index=False)","d8063b94":"Model 5","2d4b3f7b":"# Importing libraries","ea983ba1":"# Creating submission files","133b1dd3":"\nModel 2","862f9a26":"# Hyperparameter tuning with Optuna","6ac41a79":"# Re-loading the data and adding the meta-model predictions","d19f3daf":"Model 4","c992462a":"# Loading the out-of-fold and test predictions","855bf134":"# Model training\n\nModel 1","a1306f18":"# Level 2 model","70312743":"# Calculating model weights with Optuna (does not work)\n\nI experimented with the calculation of model weight with Optuna, but couldn't get the percentages to add up to one.","4fd0e17f":"# Hyperparameter tuning the level 2 model","d313fc69":"# Loading the data and creating folds\n\nNo preprocessing is needed as we will be training out models on the out-of-fold predictions of our base models.","043a7642":"Model 3"}}