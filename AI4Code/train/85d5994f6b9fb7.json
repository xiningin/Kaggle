{"cell_type":{"4aa19051":"code","76a73e1c":"code","b442b861":"code","89cb92f1":"code","90e2cfcf":"code","f4ed4dfd":"code","32ab2e76":"code","5a7237fc":"code","fff3d59f":"code","86c79ad7":"code","5d544a61":"code","9062fdbe":"code","ef9f65fe":"code","d0142b6f":"code","fafc3a81":"code","4adc82a8":"code","e6637448":"code","3baf9185":"code","5a9cdc09":"code","036669d7":"code","869b3160":"code","0e6bbc7f":"code","eb54dc2e":"code","1b6fe2e5":"code","b3084272":"code","b0dc4045":"code","d5df9c94":"code","6389794f":"code","bca99309":"code","bccc4119":"code","fe0ecbd7":"code","1a677106":"code","23e9095f":"code","9ff23b0d":"code","85404e6a":"code","46d5f5a4":"code","fac7c961":"code","80cc418e":"code","30292c4f":"code","62309e8f":"code","5b1ac0b8":"code","5d387115":"code","6776a267":"code","d254371d":"code","7e3103ea":"code","a0bd21cf":"code","6df8dda8":"code","29c6a8bd":"code","0f0d2745":"code","75cff461":"code","8c2251d1":"code","0d60fcf6":"code","c7b1930c":"code","2b1f244a":"code","b9e649aa":"code","7258eb0d":"code","9f450994":"code","bb4a3101":"code","ffa50d2b":"code","81d7f80e":"code","8dc2fcf4":"code","237c5499":"code","a664e10e":"code","3a8120a2":"code","280aec80":"code","303cf193":"code","293089fc":"code","0625097e":"code","a1ad1c2d":"code","b0608a19":"code","1d184b0d":"markdown","ee9ed5a9":"markdown","6581c0f0":"markdown","191918ec":"markdown","f5d1e0d4":"markdown","99ef1221":"markdown","5462f16c":"markdown","60d04f4b":"markdown","9b751149":"markdown","c72b8858":"markdown","1776d51a":"markdown","3c708067":"markdown","1c6f252e":"markdown","c44c1046":"markdown","9b0e93f6":"markdown","02310f5d":"markdown","863a652b":"markdown","d20d2028":"markdown","0f2592f2":"markdown","70cccbe3":"markdown","8561e5ea":"markdown","50223522":"markdown","cc271847":"markdown","9a41cd77":"markdown","c62dfedc":"markdown","e93613e1":"markdown","1c65148a":"markdown","8a910453":"markdown","c72cf883":"markdown","c3082a44":"markdown","6025992f":"markdown"},"source":{"4aa19051":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# from nltk.tokenize import sent_tokenize, word_tokenize\n# from nltk.corpus import stopwords\n# from nltk.stem.wordnet import WordNetLemmatizer\n# from nltk.stem.porter import PorterStemmer\n\nfrom wordcloud import WordCloud\n\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport seaborn as sns\n\nimport plotly.io as pio\npio.renderers","76a73e1c":"pd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\",100)\npd.set_option(\"display.max_colwidth\", 200)\npd.set_option('precision', 4)\ncolor = sns.color_palette()","b442b861":"nlp = spacy.load('en_core_web_lg')","89cb92f1":"tw_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntw_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","90e2cfcf":"tw_train.info()","f4ed4dfd":"tw_train.sample(10)","32ab2e76":"# Pie chart - data is balanced:\nlabels = 'Disaster', 'Not a Disaster'\nsizes=[tw_train['target'].loc[tw_train['target']==1].count() \\\n            ,tw_train['target'].loc[tw_train['target']==0].count()]\n\nexplode = (0.1, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nfig1.set_size_inches(12.5, 7.5)\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%'\\\n        ,shadow=True, startangle=90,textprops={'fontsize': 14})\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Disaster or not Disaster out of \"+str(sum(sizes))+\" Tweets\",\\\n          fontdict={'fontsize': 14, 'fontweight':'bold'})\nplt.show()","5a7237fc":"\ndef plot_wordcloud(text, mask=None, figure_size=(15.0,10.0), \n                   title = None, title_size=40, image_color=False):\n    stop_words = STOP_WORDS\n    stop_words = stop_words.union({\"california\",\"wreck\"})\n    \n\n    wordcloud = WordCloud(stopwords=stop_words,background_color='white',      \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n   \nplot_wordcloud(tw_train['text'].loc[tw_train[\"target\"]==1], title=\"Word Cloud of disaster related tweets - Pre-Cleaning\")\nplot_wordcloud(tw_train['text'].loc[tw_train[\"target\"]==0], title=\"Word Cloud of disaster unrelated tweets - Pre-Cleaning\")","fff3d59f":"# creating spacy DOC objects\n\ntw_train['doc_obj'] = list(nlp.pipe(tw_train['text'])) \n","86c79ad7":"from nltk.corpus import stopwords\nadj_stop = stopwords.words('english')","5d544a61":"stop_list_real = []\nfor doc in tw_train['doc_obj'].loc[tw_train['target']==1]:\n    tweet_stop = [token.text for token in doc if token.text in adj_stop]\n    stop_list_real += tweet_stop\n\nstop_list_not = []\nfor doc in tw_train['doc_obj'].loc[tw_train['target']==0]:\n    tweet_stop = [token.text for token in doc if token.text in adj_stop]\n    stop_list_not += tweet_stop\n   \n","9062fdbe":"len(set(stop_list_not))","ef9f65fe":"stop_real_df = pd.DataFrame.from_dict(Counter(stop_list_real), orient='index',columns= ['real_count'])\nstop_not_df = pd.DataFrame.from_dict(Counter(stop_list_not), orient='index',columns= ['not_count'])\nstop_count = pd.concat([stop_real_df, stop_not_df], axis=1)\nstop_count['ratio'] = stop_count['real_count'] \/ stop_count['not_count']\nstop_count.head(15)","d0142b6f":"# Creating a list of stop words that may be usefull since these words are much more common on not related tweets\nadd_stop = list(stop_count.loc[((stop_count['not_count'] > 50) | (stop_count['real_count'] > 50)) & ((stop_count['ratio'] < 0.33)|(stop_count['ratio'] > 2.3))].index)\n\nadd_stop","fafc3a81":"# Remove the stop words (originaly loaded from spaCy)\n\nadj_stop = set(adj_stop).difference(add_stop)","4adc82a8":"print('stop words: ',len(stopwords.words('english')),' remove: ',len(add_stop),' len after: ',len(adj_stop))","e6637448":"tw_test['doc_obj'] = list(nlp.pipe(tw_test['text'])) ","3baf9185":"# Remove punctuation\ndef remove_punc(doc):\n  no_punc =  [token for token in doc if token.is_punct == False]\n  return no_punc\n\n# Remove stop words\ndef remove_stop(doc):\n  no_stop = [token for token in doc if token.text not in adj_stop]\n  return no_stop\n\n# Remove numbers\ndef remove_num(doc):\n  no_num = [token for token in doc if token.like_num == False]\n  return no_num\n\n# Remove url and @user:\ndef remove_url(doc):\n  no_url = [token for token in doc if ((token.like_url == False) & ('@' not in token.text))]\n  return no_url\n\n\n# Lemmatizing + to lower case\ndef lemma_text(doc):\n    tokens=[]\n    for token in doc:      \n        if token.lemma_ != \"-PRON-\":\n            lemma = token.lemma_.lower().strip()\n        else:\n            lemma = token.lower_\n        tokens.append(lemma)\n    return tokens\n\n\n#create_string\ndef create_string(doc):\n  new_string = ' '.join([str(token) for token in doc])\n  return new_string\n","5a9cdc09":"def clean_all(df):\n    df['clean'] = df['doc_obj'].apply(remove_url)\n    df['clean'] = df['clean'].apply(remove_punc)\n    df['clean'] = df['clean'].apply(remove_stop)\n    df['clean'] = df['clean'].apply(remove_num)\n    df['clean'] = df['clean'].apply(lemma_text)\n    df['clean_text_1'] = df['clean'].apply(create_string)\n\n","036669d7":"clean_all(tw_train)\ntw_train[['text','clean_text_1']].head(15)","869b3160":"clean_all(tw_test)\ntw_test[['text','clean_text_1']].head(15)","0e6bbc7f":"# plot_wordcloud(tw_train['clean_text_1'].loc[tw_train[\"target\"]==1], title=\"Word Cloud of disaster related tweets - Clean\")\n# plot_wordcloud(tw_train['clean_text_1'].loc[tw_train[\"target\"]==0], title=\"Word Cloud of disaster unrelated tweets - Clean\")","eb54dc2e":"number_unique_tweets=  tw_train['text'].nunique()\nnumber_of_tweets=len(tw_train)\nprint('There are {} not unique tweets at train data'.format(number_of_tweets-number_unique_tweets))\ndf_mislabeled = pd.DataFrame(tw_train.groupby('text')['target'].nunique())\ndf_mislabeled.sort_values(by = 'target', ascending=False).head(30)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]\nprint('There are {} tagged as disaster and also as not disaster'.format(len(df_mislabeled.index)))","1b6fe2e5":"tw_train['target'].loc[tw_train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit']= 0\ntw_train['target'].loc[tw_train['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife']= 0\ntw_train['target'].loc[tw_train['text'] == 'To fight bioterrorism sir.']= 0\ntw_train['target'].loc[tw_train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4']= 1\ntw_train['target'].loc[tw_train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring']= 1\ntw_train['target'].loc[tw_train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption']= 0\ntw_train['target'].loc[tw_train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!']= 0\ntw_train['target'].loc[tw_train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE']= 1\ntw_train['target'].loc[tw_train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG']= 1\ntw_train['target'].loc[tw_train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"Caution: breathing may be hazardous to your health.\"]= 1\ntw_train['target'].loc[tw_train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\"]= 0\ntw_train['target'].loc[tw_train['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\"]= 0\n","b3084272":"number_unique_tweets=  tw_train['text'].nunique()\nnumber_of_tweets=len(tw_train)\nprint('There are {} not unique tweets at train data'.format(number_of_tweets-number_unique_tweets))\ndf_mislabeled = pd.DataFrame(tw_train.groupby('text')['target'].nunique())\n# df_mislabeled = df_mislabeled.loc[df_mislabeled.count > 1]\ndf_mislabeled.sort_values(by = 'target', ascending=False).head(30)\n\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]\nprint('There are {} tagged as disaster and also as not disaster'.format(len(df_mislabeled.index)))","b0dc4045":"tw_train['tuples'] = tw_train.apply(\n    lambda row: (row['clean_text_1'],row['target']), axis=1)\ntrain = tw_train['tuples'].tolist()\ntrain[3:6]","d5df9c94":"def load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'Disaster_related': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f_score = 2 * (precision * recall) \/ (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts=len(tw_train)\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=12","6389794f":"# nlp = spacy.load('en_core_web_sm')  # create english Language class","bca99309":"# # add the text classifier to the pipeline if it doesn't exist\n# # nlp.create_pipe works for built-ins that are registered with spaCy\n# if 'textcat' not in nlp.pipe_names:\n#     textcat = nlp.create_pipe('textcat')\n#     nlp.add_pipe(textcat, last=True)\n# # otherwise, get it, so we can add labels to it\n# else:\n#     textcat = nlp.get_pipe('textcat')\n\n# # add label to text classifier\n# textcat.add_label('Disaster_related')\n\n# # load the dataset\n# print(\"Loading data...\")\n# (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n# print(\"Using {} examples ({} training, {} evaluation)\"\n#       .format(n_texts, len(train_texts), len(dev_texts)))\n# train_data = list(zip(train_texts,\n#                       [{'cats': cats} for cats in train_cats]))","bccc4119":"# # get names of other pipes to disable them during training\n# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n# with nlp.disable_pipes(*other_pipes):  # only train textcat\n#     optimizer = nlp.begin_training()\n#     print(\"Training the model...\")\n#     print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n#     for i in range(n_iter):\n#         losses = {}\n#         # batch up the examples using spaCy's minibatch\n#         batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n#         for batch in batches:\n#             texts, annotations = zip(*batch)\n#             nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n#                        losses=losses)\n#         with textcat.model.use_params(optimizer.averages):\n#             # evaluate on the dev data split off in load_data()\n#             scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n#         print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n#               .format(losses['textcat'], scores['textcat_p'],\n#                       scores['textcat_r'], scores['textcat_f']))","fe0ecbd7":"# # test the trained model\n\n# def model_score(doc):\n#     with nlp.disable_pipes(*other_pipes):\n#         tw_score = nlp(doc).cats.pop(\"Disaster_related\")\n#     return tw_score","1a677106":"# tw_train['model_score'] = tw_train['clean_text_1'].apply(model_score)","23e9095f":"# tw_train.loc[(((tw_train[\"model_score\"] > 0.99) & (tw_train[\"target\"] == 0)) | ((tw_train[\"model_score\"] < 0.01) & (tw_train[\"target\"] == 1)))\\\n#              ,[\"id\",\"keyword\",\"text\",'clean_text_1',\"model_score\",\"target\"]].tail(60)","9ff23b0d":"# # check if possible mistakes are keyword related\n# tw_train.loc[(((tw_train[\"model_score\"] > 0.9) & (tw_train[\"target\"] == 0)) | ((tw_train[\"model_score\"] < 0.1) & (tw_train[\"target\"] == 1)))\\\n#              ,[\"id\",\"keyword\",\"text\",'clean_text_1',\"model_score\",\"target\"]].groupby(['keyword','target']).count().sort_values('id',ascending=False).head(20)","85404e6a":"# this is a Categorizer based tag correction done by manualy checking labeles where model score was > 0.99 and label was 0 or model score was < 0.01 and label was 1 \nid_error = [1375,5903,5193,5564,8489,322,3879,6570,3679,3664,3650,3613,1821,3061,3414,3253,3240,9701,3221,3037,2993,3005,2774,1821,1192,1196,1349,1051,1040,805,1910,796,513,519,442,4043,4911,7174,7356,8721,8786,8912,9791,10034,10130,10239,10702,10729,9933,10682]\ntw_train['target'].loc[(tw_train['id'].isin(id_error))] = tw_train['target'].loc[(tw_train['id'].isin(id_error))].apply(lambda x: 0 if x == 1 else 1)","46d5f5a4":"# Filling NaN for keywords (in case of using keyword as stratifier)\ntw_train['keyword'] = tw_train['keyword'].fillna(value = 'none')","fac7c961":"# Split the Data\n\nX = tw_train['clean_text_1']  \ny = tw_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 53)","80cc418e":"count_vectorizer = CountVectorizer(stop_words = adj_stop, min_df = 0.003, ngram_range=(1, 2))\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\nprint(count_vectorizer.get_feature_names()[:10])","30292c4f":"tfidf_vectorizer = TfidfVectorizer(stop_words = adj_stop, min_df = 0.005, ngram_range=(1, 2))\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n","62309e8f":"count_df = pd.DataFrame(count_train.A, columns = count_vectorizer.get_feature_names())\n\ntfidf_df = pd.DataFrame(tfidf_train.A, columns =  tfidf_vectorizer.get_feature_names())","5b1ac0b8":"with nlp.disable_pipes():\n    list_docs_train = list(nlp.pipe(X_train))\n    doc_vectors_train = np.array([doc.vector for doc in list_docs_train])\n    \n    list_docs_test = list(nlp.pipe(X_test))\n    doc_vectors_test = np.array([doc.vector for doc in list_docs_test])\n                            \nprint(\"Train shape: \", doc_vectors_train.shape)\nprint(\"Test shape: \", doc_vectors_test.shape)","5d387115":"nb_classifier = MultinomialNB()\n\nnb_classifier.fit(count_train, y_train)\n\npred = nb_classifier.predict(count_test)\n\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\nprint(\"F1 Score: \", f1_score(y_test, pred))\nprint(confusion_matrix(y_test, pred, labels=[0,1]))","6776a267":"# Grid search \\ Cross Validation \n\nparam_grid = {'alpha': [0.001, 0.01, 0.05,0.1,0.5] } \n\nsvc = MultinomialNB()\n\ngs_cv = GridSearchCV(svc, param_grid=param_grid, cv=5, scoring='f1')\n\ngs_cv.fit(count_train, y_train)\n\ndf_results_train = pd.DataFrame(gs_cv.cv_results_)[['param_alpha', 'mean_test_score']]\n\ndf_results_train","d254371d":"svc = LinearSVC(random_state=1, dual=False, max_iter=1000)\n\nsvc.fit(doc_vectors_train, y_train)\n\ny_pred = svc.predict(doc_vectors_test)\n\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}%\", )\nprint(f\"Test F1 Score: {f1_score(y_test, y_pred):.3f}%\", )\nprint(confusion_matrix(y_test, y_pred, labels=[0,1]))","7e3103ea":"# Grid search \\ Cross Validation \n\n\nparam_grid = {'C': [1, 5, 10] } \n\nsvc = LinearSVC(random_state=1, dual=False)\n\ngs_cv = GridSearchCV(svc, param_grid=param_grid, cv=5, scoring='f1')\n\ngs_cv.fit(count_train, y_train)\n\ndf_results_train = pd.DataFrame(gs_cv.cv_results_)[['param_C', 'mean_test_score']]\n\ndf_results_train","a0bd21cf":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tokenization\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","6df8dda8":"class ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  ","29c6a8bd":"bert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","0f0d2745":"class DisasterDetector:\n    \n    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n        \n        \n    def encode(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    \n    \n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        \n        return model\n    \n    \n    def train(self, X):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['clean_text_1'], X['keyword'])):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.encode(X.loc[trn_idx, 'clean_text_1'].str.lower())\n            y_trn = X.loc[trn_idx, 'target']\n            X_val_encoded = self.encode(X.loc[val_idx, 'clean_text_1'].str.lower())\n            y_val = X.loc[val_idx, 'target']\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            # Model\n            model = self.build_model()        \n            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n                    \n                \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n        \n    def predict(self, X):\n        \n        X_test_encoded = self.encode(X['clean_text_1'].str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) \/ len(self.models)\n\n        return y_pred","75cff461":"SEED = 1337\nK = 2\nskf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)","8c2251d1":"#  Training, Evaluation and Prediction\nclf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=9, batch_size=32)\n\nclf.train(tw_train)","0d60fcf6":"clf.plot_learning_curve()","c7b1930c":"y_bert_pred = clf.predict(tw_test)\n\nmodel_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nmodel_submission['target'] = np.round(y_bert_pred).astype('int')\nmodel_submission.to_csv('model_submission.csv', index=False)\nmodel_submission.describe()","2b1f244a":"!pip install transformers","b9e649aa":"import torch\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","7258eb0d":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","9f450994":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\nprint(sent_id)","bb4a3101":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in X_train]\n\npd.Series(seq_len).hist(bins = 30)","ffa50d2b":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    X_train.tolist(),\n    max_length = 18,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    X_test.tolist(),\n    max_length = 18,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    tw_test['clean_text_1'].tolist(),\n    max_length = 18,\n    pad_to_max_length=True,\n    truncation=True\n)","81d7f80e":"## convert lists to tensors\n\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","8dc2fcf4":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","237c5499":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","a664e10e":"# Define model architecture\nclass BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,2)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n      \n      x = self.fc1(cls_hs)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n      # apply softmax activation\n      x = self.softmax(x)\n\n      return x","3a8120a2":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","280aec80":"# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer (AdamW - an improved version of the Adam optimizer)\noptimizer = AdamW(model.parameters(),\n                  lr = 1e-5)          # learning rate","303cf193":"# Needs further examination, the example is not balanced while the data is balanced\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n\nprint(\"Class Weights:\",class_weights)\n\n# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","293089fc":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss \/ len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds\nview rawtrain_bert.py hosted with \u2764 by GitHub\nWe will use the following function to evaluate the model. It will use the validation set data.\n\n# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss \/ len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds\nview rawevaluate_bert.py hosted with \u2764 by GitHub\nNow we will finally start fine-tuning of the model.\n\n# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","0625097e":"with nlp.disable_pipes():\n    list_docs = list(nlp.pipe(tw_train['clean_text_1']))\n    vectors = np.array([doc.vector for doc in list_docs])\n    \nvectors.shape","a1ad1c2d":"text = \"\"\"There was a wild fire in the forest. it was a disaster.\"\"\"\n#text = \"\"\"someone drowned in the sea.\"\"\"\nprint(text)\n\ndef my_cosine_similarity(a, b):\n    return np.dot(a, b)\/np.sqrt(a.dot(a)*b.dot(b))\n\ntext_vec = nlp(text).vector\n\n## Center the document vectors\n# Calculate the mean for the document vectors, should have shape (300,)\nvec_mean = vectors.mean(axis=0)\n# Subtract the mean from the vectors\ncentered = vectors - vec_mean\n\n# Calculate similarities for each document in the dataset\n# Make sure to subtract the mean from the text vector\nsims = np.array([my_cosine_similarity(text_vec - vec_mean, vec) for vec in centered])\n\n# Get the index for the most similar document\nmost_similar = sims.argmax()\nprint(\"most_similar\", most_similar)\nprint(tw_train.iloc[most_similar].text)","b0608a19":"# Hasgtags\ndef hashtags(doc):\n  matcher = Matcher(nlp.vocab)\n  hash_pattern = [{'IS_SPACE': False},{'ORTH': '#'},\n           {'IS_SPACE': False}]\n  matcher.add(\"hash_pattern\", None, hash_pattern)\n  matches = matcher(doc)\n  hash_list = []\n  for match_id, start, end in matches:\n    \n    hash_list = doc[start:end].text\n  return hash_list\n  ","1d184b0d":"## Cleaning - Adjusted","ee9ed5a9":"## Cleaning Using Spacy","6581c0f0":"## General Info","191918ec":"### Hashtags","f5d1e0d4":"# Cleaning and Preprocessing","99ef1221":"## Models","5462f16c":"**TfidfVectorizer**","60d04f4b":"## Spacy TextCategorizer","9b751149":"This model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow\/models\/official\/nlp\/bert.  \nIt uses L=12 hidden layers (Transformer blocks), a hidden size of H=768, and A=12 attention heads.\n","c72b8858":"BERT - Another version  \n[link](https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification\/\n)","1776d51a":"**Create DataFrames**","3c708067":"### Mislabeled Samples\n\nThere are 18 unique tweets in training set which are labeled differently in their duplicates. Those tweets are probably labeled by different people and they interpreted the meaning differently because some of them are not very clear. Tweets with two unique target values are relabeled since they can affect the training score.\n","1c6f252e":"**CountVectorizer**","c44c1046":"## Vectorize","9b0e93f6":"### Similarity","02310f5d":"### Categorizer based tag correction","863a652b":"# Miscellaneous  \nsome ideas which were not used as part of classification pipeline","d20d2028":"## Common Words - Pre cleaning, with worldcloud","0f2592f2":"**LinearSVC**","70cccbe3":"### MultinomialNB","8561e5ea":"## Stop Words","50223522":"# Word Embedding and quick models","cc271847":"DisasterDetector is a wrapper that incorporates the cross-validation and metrics stated above.\n\nThe tokenization of input text is performed with the FullTokenizer class from tensorflow.","9a41cd77":"## BERT  \nthanks to the work on this [notebook](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#7.-Model)\n","c62dfedc":"Define Model Architecture\n","e93613e1":"# Classification Models 1\n\n","1c65148a":"This notebook is for the competition https:\/\/www.kaggle.com\/c\/nlp-getting-started, for me it was a good experience with NLP and spaCy and Bert.\nIt is still a work in progress, hope you will find it interesting.","8a910453":"Tokenization","c72cf883":"**Fine-Tune BERT**   \ndefine a couple of functions to train (fine-tune) and evaluate the model, respectively","c3082a44":"SpaCy Vectors","6025992f":"# EDA"}}