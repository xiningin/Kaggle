{"cell_type":{"ff6d05f0":"code","884acbc0":"code","77b3849d":"code","fad06489":"code","8c204eff":"code","ba779603":"code","fe3f4709":"code","020afee9":"code","5cac2360":"code","405f6496":"code","8559b799":"code","a6fd334a":"code","60d4c239":"code","2a74e99a":"code","80b3cb5f":"code","5aeb9e7c":"code","0f455a8f":"code","b800c8ee":"code","84541965":"code","e9f8b1b5":"code","33b872ce":"code","0de77443":"code","ad36c210":"code","5badc9dc":"code","61d2bea8":"code","d8b478cf":"code","a695d774":"code","2629dd4c":"code","fc15b72c":"code","da191ad3":"code","60d8f610":"markdown","ae7c73ab":"markdown","c9b9ced1":"markdown","451e0cda":"markdown","05cebb36":"markdown","92cf1be5":"markdown","fce50469":"markdown","492da368":"markdown","a5c3915d":"markdown","a10b579d":"markdown","70be4777":"markdown","7ed84a3a":"markdown","a1bfdfda":"markdown","894fe745":"markdown","0782db5e":"markdown","c7568800":"markdown","27fa9677":"markdown","d772db75":"markdown","64c5d207":"markdown","08935f54":"markdown"},"source":{"ff6d05f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport math\nimport random\nimport os\n\n# Disable info and warning messages by TF\ntf.get_logger().setLevel('ERROR')\n\n# Seed to use for all random functions\nSEED = 42","884acbc0":"import zipfile\n\nwith zipfile.ZipFile('\/kaggle\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n   zip_obj.extractall('\/kaggle\/working\/')","77b3849d":"gpu_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(gpu_devices)","fad06489":"def show_image(path, title=None):\n    from PIL import Image\n    image = Image.open(path)\n    plt.title('' if title is None else title)\n    plt.imshow(image)\n    \ndef get_random_image_path_in_dir(dir):\n    files = os.listdir(dir)\n    files = list(filter(lambda x: x.endswith('.jpg'), files))\n    return os.path.join(dir, random.choice(files))","8c204eff":"show_image(\"\/kaggle\/working\/plates\/train\/cleaned\/0001.jpg\")","ba779603":"show_image(get_random_image_path_in_dir(\"\/kaggle\/working\/plates\/train\/cleaned\/\"))","fe3f4709":"import cv2\n\nclass BackgroundRemover:\n       \n    def __init__(self):\n        pass\n\n    def __call__(self, in_img):\n        # Convert PIL image to numpy array\n        in_img = np.array(in_img)\n        \n        height, width = in_img.shape[:2]\n\n        mask = np.zeros([height, width], np.uint8)\n        bgdModel = np.zeros((1, 65),np.float64)\n        fgdModel = np.zeros((1, 65),np.float64)\n        \n        # Bounding box of the foreground object\n        rect = (15, 15, width-30, height-30)\n        \n        # Use GrabCut algorithm to do automatic foreground extraction\n        cv2.grabCut(in_img, mask, rect, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_RECT)\n        mask = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n        out_img = in_img * mask[:, :, np.newaxis]\n\n        # Change all pixels that are part of the background to black\n        background = in_img - out_img\n        out_img[np.where((background > [0, 0, 0]).all(axis = 2))] = [0, 0, 0]\n\n        return out_img\n    \ndef remove_background(input_dir, output_dir, skip_existing=True):\n    background_remover = BackgroundRemover()\n\n    files = os.listdir(input_dir)\n    files = list(filter(lambda x: x.endswith('.jpg'), files))\n\n    print(f\"Removing background from {len(files)} images in {input_dir}\")\n    for i, file in enumerate(files):\n        infile = os.path.join(input_dir, file)\n        outfile = os.path.join(output_dir, file)\n        if skip_existing and os.path.isfile(outfile):\n            print('o', end='')\n            continue\n        img_original = cv2.imread(infile)\n        img_cleaned = background_remover(img_original)\n        cv2.imwrite(outfile, img_cleaned)\n        print('.', end='')\n        if (i + 1) % 50 == 0:\n            print()\n    print()","020afee9":"!mkdir -p preprocessed\/train\/cleaned\n!mkdir -p preprocessed\/train\/dirty\n!mkdir -p preprocessed\/test","5cac2360":"remove_background(\"\/kaggle\/working\/plates\/train\/cleaned\/\", \"\/kaggle\/working\/preprocessed\/train\/cleaned\/\")\nremove_background(\"\/kaggle\/working\/plates\/train\/dirty\/\", \"\/kaggle\/working\/preprocessed\/train\/dirty\/\")\nremove_background(\"\/kaggle\/working\/plates\/test\/\", \"\/kaggle\/working\/preprocessed\/test\/\")","405f6496":"show_image(\"\/kaggle\/working\/preprocessed\/train\/cleaned\/0001.jpg\")","8559b799":"show_image(get_random_image_path_in_dir(\"\/kaggle\/working\/preprocessed\/test\/\"))","a6fd334a":"TRAIN_VALIDATION_SPLIT = 0.1\nROOT_DATA_DIR=\"preprocessed\"\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    os.path.join(ROOT_DATA_DIR, \"train\"),\n    validation_split=TRAIN_VALIDATION_SPLIT,\n    subset=\"training\",\n    seed=SEED,\n    crop_to_aspect_ratio=True\n)\nvalid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    os.path.join(ROOT_DATA_DIR, \"train\"),\n    validation_split=TRAIN_VALIDATION_SPLIT,\n    subset=\"validation\",\n    seed=SEED,\n    crop_to_aspect_ratio=True\n)","60d4c239":"print(train_ds.element_spec)","2a74e99a":"def prediction_to_string(pred):\n    if pred > 0.7:\n        return \"dirty\"\n    return \"cleaned\"\n\ndef show_one_image(image, title, subplot, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image.astype(\"uint8\"))\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize), fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(images, labels=None):\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square\n    # or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else f\"{prediction_to_string(label)} ({label:.2f})\"\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = show_one_image(image, title, subplot, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\ndef get_image_and_label_batch(dataset, batch_size):\n    dataset_iter = iter(dataset.unbatch().batch(batch_size))\n    batch = next(dataset_iter)\n\n    images, labels = batch\n    images = images.numpy()\n    labels = labels.numpy()\n    return images, labels\n","80b3cb5f":"images, labels = get_image_and_label_batch(train_ds, 20)\n\ndisplay_batch_of_images(images, labels)","5aeb9e7c":"images, labels = get_image_and_label_batch(valid_ds, 5)\n\ndisplay_batch_of_images(images, labels)","0f455a8f":"from tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers.experimental.preprocessing as preprocessing\n\n# Augment the dataset with random transformations\ndata_augmentation = Sequential([\n    preprocessing.RandomFlip(),\n    preprocessing.RandomRotation(0.5, fill_mode='constant'),\n    preprocessing.RandomTranslation(0.1, 0.1, fill_mode='constant'),\n    preprocessing.RandomZoom(height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode='constant'),\n#     preprocessing.RandomContrast(0.2),\n], name='DataAugmentation')","b800c8ee":"def visualize(image, label):\n    plt.imshow(image.numpy().astype(\"uint8\"))\n    plt.title(label)  \n    plt.axis('off')\n\nIMAGES = 5\nAUGMENTATIONS = 9\nIMG_PER_ROW = AUGMENTATIONS + 1\n\nplt.figure(figsize=(12, 8))\ni = 0\nfor image, label in train_ds.unbatch().take(IMAGES):\n    ax = plt.subplot(IMAGES, IMG_PER_ROW, i * IMG_PER_ROW + 1)\n    visualize(image, f\"Original ({label})\")\n    \n    for j in range(AUGMENTATIONS):\n        augmented_image = data_augmentation(image)\n        ax = plt.subplot(IMAGES, IMG_PER_ROW, i * IMG_PER_ROW + j + 2)\n        visualize(augmented_image, f\"Aug {j}\")\n    i += 1    ","84541965":"from tensorflow.keras.layers import Input, BatchNormalization, GlobalAveragePooling2D, Flatten, Dense, Dropout, Lambda\n\ndef get_model():\n    pretrained_model = tf.keras.applications.ResNet152(\n        weights='imagenet',\n        include_top=False ,\n    )\n    pretrained_model.trainable = False\n    \n    model = Sequential([\n        Input(shape=(256,256,3)),\n        data_augmentation,\n        Lambda(tf.keras.applications.resnet.preprocess_input),\n        pretrained_model,\n        Flatten(),\n#         GlobalAveragePooling2D(),\n#         Dense(256, activation='relu'),\n#         Dropout(0.25),\n#         Dense(128, activation='relu'),\n#         Dropout(0.25),\n        Dense(1, activation='sigmoid')\n        ])\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss='binary_crossentropy',\n                  metrics=['binary_accuracy'])\n    return model\n\nmodel = get_model()\nmodel.summary()","e9f8b1b5":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=10\n)","33b872ce":"EPOCHS = 20\n\nextended_training_data = train_ds.repeat(10).shuffle(buffer_size=1000)\nextended_validation_data = valid_ds.repeat(10).shuffle(buffer_size=1000)\n\nmodel.optimizer.lr = 1e-3\n\nhistory = model.fit(\n        extended_training_data,\n        validation_data=extended_validation_data, \n        epochs=EPOCHS, \n        callbacks=[early_stopping]\n    )","0de77443":"pd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.xlabel('epochs')","ad36c210":"pd.DataFrame(history.history)[['binary_accuracy', 'val_binary_accuracy']].plot()\nplt.xlabel('epochs')","5badc9dc":"test_data = tf.keras.preprocessing.image_dataset_from_directory(\n    os.path.join(ROOT_DATA_DIR, \"test\"),\n    labels=None, \n    crop_to_aspect_ratio=True,\n    shuffle=False # must be false to maintain order for the submission!\n)","61d2bea8":"test_batch = next(iter(test_data.unbatch().batch(20)))\n\ntest_batch_predictions = model.predict(test_batch)\n\ndisplay_batch_of_images(test_batch.numpy(), test_batch_predictions.squeeze())","d8b478cf":"predictions = model.predict(test_data)","a695d774":"# Load pre-formatted file\nsubmission = pd.read_csv('..\/input\/platesv2\/sample_submission.csv')\n\n# Overwrite contents\nsubmission['label'] = predictions\nsubmission['label'] = submission['label'].map(prediction_to_string)\n\nsubmission.to_csv(\"submission.csv\",index=False)","2629dd4c":"submission['label'].value_counts()","fc15b72c":"submission.head(20)","da191ad3":"idx = random.randint(0, submission.shape[0])\nshow_image(f\"\/kaggle\/working\/plates\/test\/{str(idx).zfill(4)}.jpg\", title=submission['label'][idx])","60d8f610":"Load all data from the test set, predict classes and plot for visual inspection.","ae7c73ab":"# Predict and Verify","c9b9ced1":"# Setup\n\nImport frequently used libraries","451e0cda":"The training and test data is provided as zip-archive. Extract all files into the working directory:","05cebb36":"Thank you for reading! Please comment if you have any questions.","92cf1be5":"Check if GPU is available:","fce50469":"# Submit Results","492da368":"# Data Preparation\n\nShow one image from the dataset to understand its contents.","a5c3915d":"Run the cell below multiple times to see different images:","a10b579d":"Get one batch of from the training dataset and visualize it:","70be4777":"# Model Training","7ed84a3a":"# Data Augmentation\n\nSince the training dataset is very small, augmentation can help to increase the variety of data shown to the model during training. We can use rotation, flipping and so on to augment the available data.","a1bfdfda":"Do the same for the validation dataset:","894fe745":"Briefly investigate the resulting labels from the test set:","0782db5e":"Show image directly from the original, unprocessed file and use label from the submission CSV to ensure that the mapping between both is correct.","c7568800":"## Visualize Images and Labels\n\nDefine helper functions to plot a batch of images (hidden).","27fa9677":"## Training Convergence","d772db75":"## Train and Validation Split\n\nSplit the dataset into train and validation sets. Unfortunately, the training data pool is really small (just 40 images), so training with such a small dataset won't be easy.","64c5d207":"Predict on the complete test set and store results in `submission.csv` file to be submitted to the competition.","08935f54":"## Background Removal\n\nThe images contain various backgrounds with different textures. These make it more difficult for the classifier to be trained on the actual dirt present on the plates. Removing the background improves prediction accuracy significantly."}}