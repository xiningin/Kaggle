{"cell_type":{"93bfa1d7":"code","dac55f36":"code","594510f7":"code","a46d7c92":"code","04801f76":"code","754275c1":"code","c1f71ee6":"code","e1e8c925":"code","24e38c16":"code","ea987c09":"code","00713e74":"code","b6979fc2":"code","4876fdef":"code","3a9d50a5":"code","70be3f25":"code","fd923cbf":"code","c92ec967":"code","2fe77e28":"code","4c2c7562":"code","97bb09de":"code","c4d61363":"code","60bb26a7":"markdown","aea5761a":"markdown","0023709b":"markdown","a618949a":"markdown","937a9843":"markdown","78c27bf0":"markdown","efbf789a":"markdown","34195362":"markdown","348d76c2":"markdown","a7da07af":"markdown","123e5e05":"markdown","d9860525":"markdown","1e30c92c":"markdown","d191b6d3":"markdown","5f85e61b":"markdown","c759c8ab":"markdown","17f3a4af":"markdown"},"source":{"93bfa1d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dac55f36":"# Initializing the file paths\npath = '..\/input\/porto-seguro-safe-driver-prediction\/'\n\n# datasets\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')","594510f7":"train.head(10)","a46d7c92":"test.head(10)","04801f76":"# Creating a function to calculate and view the missing values in the dataset\ndef missing_values(df):\n        # Total missing values in dataframe\n        miss_val = df.isnull().sum()\n\n        # Percentage of missing values of each column\n        miss_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n        # Creating a table with them\n        miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n\n        # Renaming the columns\n        ren_columns = miss_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n\n        # Sorting the table by percentage of missing in descending order\n        ren_columns = ren_columns[\n            ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n        # Printing the summary information of missing values\n        print (\"The dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(ren_columns.shape[0]) +\n            \" columns that have missing values.\")\n\n        # Returning the dataframe with missing value information\n        return ren_columns","754275c1":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","c1f71ee6":"# Applying the function on the dataframes\nmissing_values_df1 = missing_values(train_copy)\nmissing_values_df1.head(10)","e1e8c925":"train_copy.drop(['ps_car_03_cat'], inplace=True, axis=1)","24e38c16":"test_copy = test\ntest_copy = test_copy.replace(-1, np.NaN)","ea987c09":"# Applying the function on the dataframes\nmissing_values_df2 = missing_values(test_copy)\nmissing_values_df2.head(10)","00713e74":"y = train_copy.target.values\ntrain_copy.drop(['id','target'], inplace=True, axis=1)\n\nx = train_copy.values","b6979fc2":"target_count = train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='target');","4876fdef":"# Creating training and validation sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","3a9d50a5":"# Creating the LightGBM data containers\ncategorical_features = [c for c, col in enumerate(train_copy.columns) if 'cat' in col]\ntrain_data = lightgbm.Dataset(x_train, label=y_train, categorical_feature=categorical_features)\ntest_data = lightgbm.Dataset(x_test, label=y_test)","70be3f25":"parameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}","fd923cbf":"model = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)","c92ec967":"feature_imp= pd.DataFrame({'Value':model.feature_importance(),'Feature':train_copy.columns})\nplt.figure(figsize=(40, 20))\nsns.set(font_scale = 5)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:20])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","2fe77e28":"lightgbm_tree = lightgbm.plot_tree(model, figsize=(20, 20))","4c2c7562":"ids = test_copy['id'].values\ntest_copy.drop('id', inplace=True, axis=1)","97bb09de":"# Applying the model on the test set \nx = test_copy.values\ny = model.predict(x, predict_disable_shape_check= True)","c4d61363":"output = pd.DataFrame({'id': ids, 'target': y})\noutput.to_csv(\"submission.csv\", index=False)","60bb26a7":"Porto Seguro is a dataset that contains the information about the insurance contracts of several automobiles and we have to predict their chances of initiating a claim in the following year","aea5761a":"## Submission","0023709b":"We can also view the indivitual trees from the model","a618949a":"We made a copy of the actual dataframe and replaced all the missing or null values with -1","937a9843":"## Datasets\nThis analysis is based on the dataset of Porto Seguro, Brazil's one of the largest homeowners and auto insurance companies. As they have the concern to increase the cost of insurance for good drivers and reduce the price for bad drivers for inaccuracies in the calculation of insurance premium. This model helps in predicting the chances of an auto insurance claim after one year of the contract initiation.\n","78c27bf0":"Creating the model","efbf789a":"Dropping 'ps_car_03_cat' from train_copy as 69.1% data of the column is missing. If more than 50% data are missing then the impact on the model is usually near to zero.","34195362":"Dropping 'id' and 'target' from train_copy to get the labels","348d76c2":"From the plot above, we can deduce that only the top 8 features are actually impacting the model, others are obvious on the machine.","a7da07af":"Feature Importance","123e5e05":"Calculating and visualising the number of claims opened within one year of starting the contract","d9860525":"## LightGBM\nLightGBM is very popular for tabular datasets. In most cases it is more accurate than XGBoost as it provides more optimized, scalable and fast implementation of GBMs.","1e30c92c":"setting the model configuration with the basic parameters","d191b6d3":"## Prediction","5f85e61b":"## Dealing with Missing values\nWe now define a function 'missing_values' to count the number of missing values in each dataset by counting the total number of missing values, calculating their percentage in each column, sorting them in descending order and adding the values to a column and to the datasets","c759c8ab":"So, here we can see that the score we got is at [129]valid_0's auc:0.630456 and it is not overfitting the data.","17f3a4af":"But still, we used the LightGBM\u2019s inbuilt \u2018feature importance\u2019 function to visually understand the 20 most important features which helped the model lean towards a particular classification."}}