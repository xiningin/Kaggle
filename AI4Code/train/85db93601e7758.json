{"cell_type":{"4eb65ce8":"code","e02f7617":"code","611228f4":"code","c1b2302b":"code","3ea32c63":"code","174abedb":"code","e1356821":"code","c46065c6":"code","924ef11d":"code","dfd1874b":"code","dc472115":"code","6e44e5fb":"code","27dbdfd1":"code","17b360c7":"code","66209221":"code","98894d9e":"code","a9470dba":"code","b9f9d32b":"code","b6d97197":"code","85e32dc7":"code","aff1822f":"code","4f25ace9":"code","e3df6c09":"code","234745f2":"code","2c03b8c1":"code","08b8fdff":"code","1c85170f":"code","cbc782d3":"code","94be3c96":"code","eccb0914":"code","c131086d":"code","539ec034":"code","fa608b3c":"code","7f798b24":"code","5698eb16":"code","81e24644":"code","9a75ebb5":"code","a657abd2":"code","a287dcb8":"code","cb008c1e":"code","137a3c1b":"code","b7d4a4c1":"code","95902a0f":"code","7b476ecf":"code","deef3718":"code","afe2213c":"code","2bf39d3b":"code","63b9d6ab":"code","aa3594d1":"code","e7ee6410":"code","8d92b534":"code","d75305a1":"code","b272935b":"code","6e6fa5b8":"markdown","60caa981":"markdown","5925dbfa":"markdown","b7371a0e":"markdown","4ccc6b4c":"markdown","dd61f7b4":"markdown","15eba925":"markdown","ad4ca734":"markdown","214b54bd":"markdown","c48e86f9":"markdown","fb37bd2a":"markdown","193701b8":"markdown","b4e174fb":"markdown","9089b002":"markdown","9385de43":"markdown","524bab89":"markdown","915bad10":"markdown","cb01de85":"markdown","fa99b880":"markdown","74b60462":"markdown","509a1d81":"markdown","099e0b48":"markdown","d61decee":"markdown","33a52aa4":"markdown","dd11a136":"markdown","60ef2031":"markdown","f1ace811":"markdown","59d40b53":"markdown","9341db89":"markdown","66820f64":"markdown","47cc40c7":"markdown","cd1800e5":"markdown","12fcc2af":"markdown"},"source":{"4eb65ce8":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.calibration import CalibratedClassifierCV\n\n#scaling \nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\nfrom scipy.stats import mode\n\n#model\nimport lightgbm as lgb\n\n# parameter tuning\nimport optuna\n\n# over and under sampling\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import CondensedNearestNeighbour\n\n# analysis and files\nfrom collections import Counter\nimport time\nimport os","e02f7617":"#Boosting\n#dart has dropout and should stop overfitting , -- very long to run\n#gdbt is default with good accuracy but tends to overfit\n# goss is fast to converge \n\n#Scaling \n#Robust is good with outliers \nSCALER_NAME = \"None\"   #None\nSCALER = MinMaxScaler() # RobustScaler() MinMaxScaler() StandardScaler()\n\n## additional data and sampling\nPSEUDOLABEL =2\nADD_DATA = False\nCLUSTERING = False \nsample_technique = \"class5\" #'class5' \"none\" \"SMOTEENN\" \"SMOTETomek\"  \"PCA\"   ----SMOTEEN and SMOTETomek needs Add Data to run \n\n#Training Type and Tuning \nOPTUNA = False\nNUM_TRIALS  = 200\n\n#Training Params\nMODEL_TYPE = \"Cross_validation\" #\"train\" \"classifier\" \"classifier_cal\" \"Cross_validation\"\nFull_Train = False # retrain model on full dataset\n\nFOLDS = 20\nBOOSTING = 'gbdt'  # \"goss\" 'dart'  'gbdt'\nEARLY_STOPPING_ROUNDS = 30 #20\nEPOCHS = 1000 #1000\n\n# Post processing \nCALIBRATION_METHOD = 'isotonic' #'sigmoid'","611228f4":"#Params \nDOWNCASTING = True\n\nDEVICE = \"cpu\" #\"gpu\" cpu\n\n# multi classification method - weightings \nUNBALANCED = True\n\nMETRIC = \"multi_logloss\"","c1b2302b":"title = \"CROSS - Best Params -no scaling- PSEUDO 2+ADD DATA (Complete)\"\nversion = 1\nminor = 0\n\nf= open(f\"log_file_{version}_{minor}.txt\",\"a\")\nf.write(f\"########################### {title} ########################### \")\nf.write(f\"\\n Boosting type: {BOOSTING}\")\nf.write(f\"\\n Sampling technique: {sample_technique}\")\nf.write(f\"\\n Scaler: {SCALER_NAME}\")\nf.write(f\"\\n Hyper Parameter Tuning: {OPTUNA}\")\nf.write(f\"\\n Model Type: {MODEL_TYPE}\")\nf.write(f\"\\n Pseudo labelling : {PSEUDOLABEL}\")\nf.close()","3ea32c63":"\"\"\"!pip uninstall -y lightgbm\n!apt-get install -y libboost-all-dev\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\"\"\"","174abedb":"\"\"\"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)\"\"\"","e1356821":"\"\"\"!cd LightGBM\/python-package\/;python setup.py install --precompile\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM\"\"\"","c46065c6":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\", index_col=0)","924ef11d":"train.shape","dfd1874b":"def additional_data():\n    original_data = pd.read_csv(\"..\/input\/forest-cover-type-dataset\/covtype.csv\")\n    #only class 4, 5, 6\n    #original_data = original_data [ (original_data[\"Cover_Type\"] ==4) | (original_data[\"Cover_Type\"] ==5)  | (original_data[\"Cover_Type\"] ==6) ]\n    original_data[\"Cover_Type\"].value_counts()\n    return original_data\n\noriginal_data =additional_data() \ndisplay(original_data.head())","dc472115":"def pseudo_labelling_1():\n    # obtain submissions\n    sub_files = pd.DataFrame()\n    for file in os.listdir(\"..\/input\/submission-files-tps-dec-2021\"):\n        sub_files = pd.concat([sub_files,pd.read_csv(\"..\/input\/submission-files-tps-dec-2021\/\"+file,index_col=0)],axis=1)    \n        \n    # get all rows where the values are the same for each column - num files =10\n    filter_vals =  sub_files.sum(axis =1 )\/ 10 == sub_files.iloc[:,0]\n    \n    # filter test and sub_files from above results & join \n    pseudo_df = test.copy(deep=True)[ filter_vals ] \n    pseudo_df[\"Cover_Type\"] = sub_files[ filter_vals ].iloc[:,0]\n    \n    return pseudo_df","6e44e5fb":"if PSEUDOLABEL ==1:\n    print(\"Pseudo 1 loaded\")\n    pseudo_df =  pseudo_labelling_1()\n    print(\"Pseudo shape: \",pseudo_df.shape)\n    \nif PSEUDOLABEL ==2 or PSEUDOLABEL ==0:\n    print(\"Pseudo 2 loaded\")\n    pseudo_df = pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv', index_col=0, dtype=train.dtypes.to_dict())\n    print(\"Pseudo shape: \",pseudo_df.shape)","27dbdfd1":"train[\"Cover_Type\"].value_counts()","17b360c7":"plt.figure(figsize = (10,7))\nsns.countplot(x = train[\"Cover_Type\"])\nplt.title(\"Count of Target (Cover_Type)\")","66209221":"cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Cover_Type']","98894d9e":"plt.figure(figsize = (15,10))\nsns.heatmap(train[cols].corr(), vmin = -1, vmax=1, annot = True, cmap=\"Spectral\") ","a9470dba":"def memory_usage_mb(df, *args, **kwargs):\n    \"\"\"Dataframe memory usage in MB. \"\"\"\n    return df.memory_usage(*args, **kwargs).sum() \/ 1024**2\n\ndef reduce_memory_usage(df, deep=True, verbose=True, categories=True):\n    # All types that we want to change for \"lighter\" ones.\n    # int8 and float16 are not include because we cannot reduce\n    # those data types.\n    # float32 is not include because float16 has too low precision.\n    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n    start_mem = 0\n    if verbose:\n        start_mem = memory_usage_mb(df, deep=deep)\n\n    for col, col_type in df.dtypes.iteritems():\n        best_type = None\n        if col_type == \"object\":\n            df[col] = df[col].astype(\"category\")\n            best_type = \"category\"\n        elif col_type in numeric2reduce:\n            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n            df[col] = pd.to_numeric(df[col], downcast=downcast)\n            best_type = df[col].dtype.name\n        # Log the conversion performed.\n        #if verbose and best_type is not None and best_type != str(col_type):\n         #   print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n\n    if verbose:\n        end_mem = memory_usage_mb(df, deep=deep)\n        diff_mem = start_mem - end_mem\n        percent_mem = 100 * diff_mem \/ start_mem\n        print(f\"Memory usage decreased from\"\n              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")","b9f9d32b":"if DOWNCASTING:\n    reduce_memory_usage(train)\n    reduce_memory_usage(test)\n    reduce_memory_usage(original_data)\n    reduce_memory_usage(pseudo_df)","b6d97197":"all_df = pd.concat([train.assign(ds=0), test.assign(ds=1),original_data.assign(ds=2),pseudo_df.assign(ds=3)]).reset_index(drop=True).drop(columns=['Soil_Type7', 'Soil_Type15'] )\n\ndef start_at_eps(series, eps=1e-10): return series - series.min() + eps\n\npos_h_hydrology = start_at_eps(all_df.Horizontal_Distance_To_Hydrology)\npos_v_hydrology = start_at_eps(all_df.Vertical_Distance_To_Hydrology)\n\nwilderness = all_df.columns[all_df.columns.str.startswith('Wilderness')]\nsoil_type = all_df.columns[all_df.columns.str.startswith('Soil_Type')]\nhillshade = all_df.columns[all_df.columns.str.startswith('Hillshade')]\n\nall_df = pd.concat([\n    all_df,\n\n    all_df[wilderness].sum(axis=1).rename('Wilderness_Sum').astype(np.float32),\n    all_df[soil_type].sum(axis=1).rename('Soil_Type_Sum').astype(np.float32),\n\n    (all_df.Aspect % 360).rename('Aspect_mod_360'),\n    (all_df.Aspect * np.pi \/ 180).apply(np.sin).rename('Aspect_sin').astype(np.float32),\n    (all_df.Aspect - 180).where(all_df.Aspect + 180 > 360, all_df.Aspect + 180).rename('Aspect2'),\n\n    (all_df.Elevation - all_df.Vertical_Distance_To_Hydrology).rename('Hydrology_Elevation'),\n    all_df.Vertical_Distance_To_Hydrology.apply(np.sign).rename('Water_Vertical_Direction'),\n\n    (pos_h_hydrology + pos_v_hydrology).rename('Manhatten_positive_hydrology').astype(np.float32),\n    (all_df.Horizontal_Distance_To_Hydrology.abs() + all_df.Vertical_Distance_To_Hydrology.abs()).rename('Manhattan_abs_hydrology'),\n    (pos_h_hydrology ** 2 + pos_v_hydrology ** 2).apply(np.sqrt).rename('Euclidean_positive_hydrology').astype(np.float32),\n    (all_df.Horizontal_Distance_To_Hydrology ** 2 + all_df.Vertical_Distance_To_Hydrology ** 2).apply(np.sqrt).rename('Euclidean_hydrology'),\n\n    all_df[hillshade].clip(lower=0, upper=255).add_suffix('_clipped'),\n    all_df[hillshade].sum(axis=1).rename('Hillshade_sum'),\n\n    (all_df.Horizontal_Distance_To_Roadways * all_df.Elevation).rename('road_m_elev'),\n    (all_df.Vertical_Distance_To_Hydrology * all_df.Elevation).rename('vhydro_elevation'),\n    (all_df.Elevation - all_df.Horizontal_Distance_To_Hydrology * .2).rename('elev_sub_.2_h_hydro').astype(np.float32),\n\n    (all_df.Horizontal_Distance_To_Hydrology + all_df.Horizontal_Distance_To_Fire_Points).rename('h_hydro_p_fire'),\n    (start_at_eps(all_df.Horizontal_Distance_To_Hydrology) + start_at_eps(all_df.Horizontal_Distance_To_Fire_Points)).rename('h_hydro_eps_p_fire').astype(np.float32),\n    (all_df.Horizontal_Distance_To_Hydrology - all_df.Horizontal_Distance_To_Fire_Points).rename('h_hydro_s_fire'),\n    (all_df.Horizontal_Distance_To_Hydrology + all_df.Horizontal_Distance_To_Roadways).abs().rename('abs_h_hydro_road'),\n    (start_at_eps(all_df.Horizontal_Distance_To_Hydrology) + start_at_eps(all_df.Horizontal_Distance_To_Roadways)).rename('h_hydro_eps_p_road').astype(np.float32),\n\n    (all_df.Horizontal_Distance_To_Fire_Points + all_df.Horizontal_Distance_To_Roadways).abs().rename('abs_h_fire_p_road'),\n    (all_df.Horizontal_Distance_To_Fire_Points - all_df.Horizontal_Distance_To_Roadways).abs().rename('abs_h_fire_s_road'),\n], axis=1)\n\ntypes = {'Cover_Type': np.int8}\ntrain = all_df.loc[all_df.ds == 0].astype(types).drop(columns=['ds'])\ntest = all_df.loc[all_df.ds == 1].drop(columns=['Cover_Type', 'ds'])\noriginal_data = all_df.loc[all_df.ds == 2].astype(types).drop(columns=['ds'])\npseudo_df = all_df.loc[all_df.ds == 3].astype(types).drop(columns=['ds'])\n\ndel all_df\ndel pos_h_hydrology\ndel pos_v_hydrology\ndel wilderness\ndel soil_type\ndel hillshade\n\nif not ADD_DATA:\n    del original_data\nif PSEUDOLABEL==0:\n    del pseudo_df","85e32dc7":"print(train.shape)\nprint(test.shape)\ndisplay(train.head())","aff1822f":"#start from 0 --> 6\ntrain[\"Cover_Type\"] = train[\"Cover_Type\"]-1\n\nif ADD_DATA:\n    original_data[\"Cover_Type\"] = original_data[\"Cover_Type\"]-1\n    \nif PSEUDOLABEL>0:\n    pseudo_df[\"Cover_Type\"] = pseudo_df[\"Cover_Type\"]-1","4f25ace9":"kfold = StratifiedKFold(n_splits= FOLDS, shuffle=True, random_state=42)","e3df6c09":"class oversample_techniques():\n    \n    def __init__ (self, df ,num):\n        self.df = df \n        self.num = num\n        \n    def sample_class5(self):\n        \n        #note that due to encoding class 5 = 4\n        class5 = self.df[self.df[\"Cover_Type\"]==4]\n        #append class 5 for each fold -1 \n        for i in range(self.num):\n            self.df = self.df.append(class5, ignore_index=True)\n            \n        f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n        f.write(\"\\n ##############################\")\n        f.write(\"\\n Oversampling Technique: oversample class 5\")\n        f.write(\"\\n ##############################\")\n        f.close()\n        \n        return self.df\n    \n    def no_change(self):\n        f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n        f.write(\"\\n ##############################\")\n        f.write(\"\\n Oversampling Technique: no sampling\")\n        f.write(\"\\n ##############################\")\n        f.close()\n        \n        \n    def over_under_SMOTEENN(self):\n        \n        smote_enn = SMOTEENN(random_state=42, n_jobs = -1)\n        X_resampled, y_resampled = smote_enn.fit_resample(self.df.drop(\"Cover_Type\",axis =1),\n                                                          self.df[\"Cover_Type\"])\n        X_resampled[\"Cover_Type\"] = y_resampled\n        \n        f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n        f.write(\"\\n ##############################\")\n        f.write(\"\\n Oversampling Technique: SMOTEENN\")\n        f.write(\"\\n ##############################\")\n        f.close()\n        \n        return X_resampled\n    \n    def over_under_SMOTETomek(self):\n\n        smote_tomek = SMOTETomek(random_state=42, n_jobs = -1)\n        X_resampled, y_resampled = smote_tomek.fit_resample(self.df.drop(\"Cover_Type\",axis =1),\n                                                            self.df[\"Cover_Type\"])\n        X_resampled[\"Cover_Type\"] = y_resampled\n        \n        f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n        f.write(\"\\n ##############################\")\n        f.write(\"\\n Oversampling Technique: SMOTETomek\")\n        f.write(\"\\n ##############################\")\n        f.close()\n        \n        return X_resampled","234745f2":"# run class 5 sample here as we are duplicatign the training data not the split data \nif sample_technique == \"class5\":\n    train = oversample_techniques(train,5).sample_class5()","2c03b8c1":"train[\"Cover_Type\"].value_counts()","08b8fdff":"X = train.drop(\"Cover_Type\",axis =1)\ny = train[\"Cover_Type\"]\n\nprint(\"X shape:\",X.shape)\n\nif MODEL_TYPE != \"Cross_validation\":\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    print(\"X_train shape:\",X_train.shape)","1c85170f":"if ADD_DATA:\n    print(\"Adding Original Data\")\n    \n    ## Cross val uses training data so add it to train\n    if MODEL_TYPE ==\"Cross_validation\":\n        print(\"X shape prior: \",X.shape)\n        train = pd.concat([X,original_data.drop(\"Cover_Type\",axis =1)])\n        print(\"X shape after: \",train.shape)\n    else:\n        print(\"X_train shape prior: \",X_train.shape)\n        X_train = pd.concat([X_train,original_data.drop(\"Cover_Type\",axis =1)])\n        y_train = pd.concat([y_train,original_data[\"Cover_Type\"]])\n        print(\"X_Train shape after: \",X_train.shape)\ndel train","cbc782d3":"# we concat here if not a cross validation session as cross validation concat will occur in fold\nif PSEUDOLABEL >0 and MODEL_TYPE!=\"Cross_validation\" :\n    print(\"Pseudo shape: \",pseudo_df.shape)\n    print(\"XTrain shape prior: \",X_train.shape)\n    \n    X_train = pd.concat([X_train,pseudo_df.drop(\"Cover_Type\",axis =1)])\n    y_train = pd.concat([y_train,pseudo_df[\"Cover_Type\"]])\n    \n    print(\"XTrain shape after: \",X_train.shape)","94be3c96":"def scale_data(X_train, X_test, test):\n    scaler= SCALER\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    test_scaled = scaler.transform(test)\n    \n    return X_train, X_test, test_scaled","eccb0914":"if SCALER_NAME != \"None\" and MODEL_TYPE !=\"Cross_validation\":\n    \n    print(f\"Scaling with {SCALER_NAME}\")\n    X_train, X_test, test_scaled = scale_data(X_train, X_test, test)\n    \n    display(test_scaled)","c131086d":"if sample_technique in [\"SMOTETEENN\", \"SMOTETomek\", \"PCA\"]:\n    \n    pca = PCA(n_components=10)\n    X_train = pca.fit_transform(X_train)\n    X_test = pca.transform(X_test)\n\n    test = pca.transform(test)\n\n    pca_cols = []\n    for i in range(10):\n        pca_cols.append(\"pca_\"+f\"{i}\")\n\n    X_train = pd.DataFrame(X_train, columns = pca_cols)\n    X_test = pd.DataFrame(X_test, columns = pca_cols)\n\n    test = pd.DataFrame(test, columns = pca_cols)\n    \n    #Boxplot & df \n    plt.figure(figsize= (15,7))\n    sns.boxplot(data = X_train)\n    \n    display(X_train.head())","539ec034":"print(f\"Applying Oversampling Technique: {sample_technique}\")\n\nif sample_technique == \"none\":\n    oversample  = oversample_techniques(train,kfold.n_splits)\n    train = oversample.no_change()\n    \nelif sample_technique ==\"SMOTEENN\":\n    X_train = oversample_techniques(X_train,kfold.n_splits).over_under_SMOTEENN()\n    X_test = oversample_techniques(X_test,kfold.n_splits).over_under_SMOTEENN()\n    \nelif sample_technique ==\"SMOTETomek\":\n    X_train = oversample_techniques(X_train,kfold.n_splits).over_under_SMOTETomek()\n    X_test = oversample_techniques(X_test,kfold.n_splits).over_under_SMOTETomek()","fa608b3c":"from sklearn.cluster import KMeans, DBSCAN\nfrom scipy.cluster.hierarchy import dendrogram, linkage","7f798b24":"if CLUSTERING:\n    cluster = KMeans(n_clusters= 7)\n    y_cluster =cluster.fit_predict(train.drop(\"Cover_Type\",axis =1))\n    train[\"cluster\"] = y_cluster\n\n    y_cluster_test = cluster.predict(test)\n    test[\"cluster\"] = y_cluster_test\n    test[\"cluster\"].value_counts()","5698eb16":"# 1. Define an objective function to be maximized.\ndef objective(trial):\n    # 2. Suggest values of the hyperparameters using a trial object.\n    lgb_params = {\n        \"is_unbalance\": UNBALANCED,\n        'objective': 'multiclass',\n        \"num_class\": 7,\n        'metric': \"multi_logloss\",\n        'verbosity': -1,\n        'num_iterations': EPOCHS,\n        \"num_threads\": -1,\n        #\"force_col_wise\": True\n        \"learning_rate\": trial.suggest_float('learning_rate',0.01,0.2),\n        'boosting_type': trial.suggest_categorical('boosting',[BOOSTING]),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        #'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1000, 10000),\n        'max_depth': trial.suggest_int('max_depth', 2,15),\n        #'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        #'bagging_freq': trial.suggest_int('bagging_freq', 1, 7)\n    }\n    \n    if BOOSTING == \"dart\":\n        lgb_params[\"drop_seed\"]= 42\n        \n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"multi_logloss\")\n    \n    model = lgb.train(params=lgb_params,train_set= train_data, \n                      valid_sets= [test_data], \n                      early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n                      callbacks=[pruning_callback]\n                     )\n    \n    \n    y_pred = model.predict(X_test)\n    test_preds = [np.argmax(x) for x in y_pred]\n    accuracy_s = accuracy_score(y_test,test_preds)\n    print(f\"Accuracy score of {accuracy_s}\")\n    print(classification_report(y_test,test_preds))\n    \n    return 1-accuracy_s","81e24644":"%%time\nif OPTUNA:\n            \n    train_data = lgb.Dataset(X_train, label=y_train)\n    test_data = lgb.Dataset(X_test, label=y_test)\n    \n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=NUM_TRIALS)\n    trial = study.best_trial\n    \n    \n    #Print our results\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    print(\" Accuracy Value: {}\".format(1- trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n    #write to log file\n    f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n    f.write(\"\\n ##################  Hyper paramter tuning ###############\")\n    f.write(\"\\n Number of finished trials: {}\".format(len(study.trials)))\n    f.write(f\"\\n Best trial accuracy score: {1-trial.value}\")\n    f.write(f\"\\n Best params: {trial.params}\")\n    f.close()","9a75ebb5":"if OPTUNA:\n    lgb_params = trial.params\n    lgb_params[\"is_unbalance\"]= UNBALANCED\n    lgb_params[\"objective\"]= \"multiclass\"\n    lgb_params[\"metric\"]= \"multi_logloss\"\n    lgb_params[\"num_class\"]= 7\n    lgb_params[\"device_type\"]= DEVICE\n    lgb_params[\"num_iterations\"]= EPOCHS\n    \nelse: #set to best params - see 'TEST DELETE notebook'\n    lgb_params = {\n    \"is_unbalance\": UNBALANCED,\n    \"objective\" : \"multiclass\",\n    \"metric\": \"multi_logloss\",\n    \"num_class\": 7,\n    #\"num_threads\": -1,\n    #\"force_col_wise\": True\n    \"device_type\": DEVICE,\n    'boosting': BOOSTING,  \n    'num_iterations': EPOCHS,\n    \"learning_rate\": 0.16704206649880823,\n    #\"lambda_l1\": 0.03469015403439412,\n    \"lambda_l2\": 9.993162304351474,\n    \"num_leaves\": 243,\n    \"max_depth\": 12\n                   }\n","a657abd2":"def cross_val(X,y, test):\n    \n    test_predictions = []\n    lgb_scores = []\n\n    for idx, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n\n        print(10*\"=\", f\"Fold={idx+1}\", 10*\"=\")\n        start_time = time.time()\n\n        x_train, y_train = X.iloc[train_idx,:], y.iloc[train_idx,]\n        x_valid, y_val = X.iloc[val_idx,:], y.iloc[val_idx,]\n\n        if PSEUDOLABEL >0:\n            #add pseduo data to each fold\n            x_train = np.concatenate([x_train, pseudo_df.drop(\"Cover_Type\", axis =1)], axis=0)\n            y_train = np.concatenate([y_train, pseudo_df[\"Cover_Type\"]], axis=0)\n        \n        if SCALER_NAME !=\"None\":\n            print(f\"Scaling with {SCALER_NAME}\")\n            x_train,x_valid, test_scaled = scale_data(x_train,x_valid, test)\n        else:\n            test_scaled = test\n        \n        train_dat = lgb.Dataset(x_train, label=y_train)\n        val_dat = lgb.Dataset(x_valid, label=y_val)\n        \n        del x_train\n        del y_train\n\n        model = lgb.train(params=lgb_params,\n                          train_set= train_dat, \n                          valid_sets= [val_dat], \n                          early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n                          verbose_eval = -1\n                     )\n\n        preds_valid = model.predict(x_valid)\n        preds_valid = [np.argmax(x) for x in preds_valid]\n        accuracy_val = accuracy_score(y_val,  preds_valid)\n        lgb_scores.append(accuracy_val)\n        \n        del x_valid\n        del y_val\n        \n        run_time = time.time() - start_time\n        print(f\"Fold={idx+1}, accuracy: {accuracy_val}, Run Time: {run_time:.2f}\")\n        f.write(f\"Fold={idx+1}, accuracy: {accuracy_val}, Run Time: {run_time:.2f}\")\n\n        test_preds = model.predict(test_scaled)\n        test_preds = [np.argmax(x) for x in test_preds]\n        test_predictions.append(test_preds)\n\n    print(\"Mean Validation Accuracy :\", np.mean(lgb_scores))\n    return test_predictions","a287dcb8":"if MODEL_TYPE == \"Cross_validation\":\n    \n    ##open log file\n    f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n    f.write(f\"########################### CROSSVAL SCORES ########################### \")\n    \n    test_predictions = cross_val(X,y, test)\n    y_pred_test = np.squeeze(mode(np.column_stack(test_predictions),axis = 1)[0]).astype('int')\n    \n    #Close logs\n    f.close()","cb008c1e":"def scoring(model ):\n    # train score \n    metric_score = model.best_score[\"valid_0\"][METRIC]\n    print(f\"{METRIC} score of {metric_score}\")\n    \n    train_probs = model.predict(X_train)\n    train_preds = [np.argmax(x) for x in train_probs]\n    accuracy_train = accuracy_score(y_train,train_preds)\n    print(f\"train accuracy score of {accuracy_train}\")\n    print(\"\\n Train Classification Report:\")\n    print(classification_report(y_train,train_preds))\n    \n    #test score\n    test_probs = model.predict(X_test)\n    test_preds = [np.argmax(x) for x in test_probs]\n    accuracy_s = accuracy_score(y_test,test_preds)\n    print(f\"Test accuracy score of {accuracy_s}\")\n    print(\"\\n Test Classification Report:\")\n    print(classification_report(y_test,test_preds))\n   \n    #Write log\n    f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n    f.write(\"\\n ##################  SCORE ###############\")\n    f.write(f\"\\n {METRIC} score: {metric_score}\")\n    f.write(f\"\\n accuracy score: {accuracy_s}\")\n    f.close()\n    return test_probs","137a3c1b":"%%time\ndef train_model():   \n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    test_data = lgb.Dataset(X_test, label=y_test)\n    \n    model = lgb.train(params=lgb_params,\n                  train_set= train_data, \n                  valid_sets= [test_data], \n                  early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n                  verbose_eval = -1\n                 )\n    \n    y_val_probs = scoring(model)\n    \n    #predict Test data\n    y_test_probs = model.predict(test_scaled)\n    \n    return model , y_test_probs,y_val_probs\n    \nif MODEL_TYPE == \"train\":\n    model, y_test_probs, y_val_probs = train_model()\n    y_pred_test = [np.argmax(x) for x in y_test_probs]","b7d4a4c1":"if MODEL_TYPE == \"train\":\n    lgb.plot_importance(booster= model, figsize=(20,10))\n    y_val = [np.argmax(x) for x in y_val_probs]\n    \n    #Confusion Matrix\n    cm = confusion_matrix(y_test, y_val)\n    ix = np.arange(cm.shape[0])\n    cm[ix, ix] = 0\n    col_names = [1,2,3,4,5,6,7]\n    cm = pd.DataFrame(cm, columns=col_names, index=col_names)\n    display(cm)","95902a0f":"def scoring_model (model_i):\n    y_pred_train = model_i.predict(X_train)\n    accuracy_train = accuracy_score(y_train,y_pred_train)\n\n    print(f\"Train accuracy score of {accuracy_train}\")\n    print(\"\\n Train classification report:\")\n    print(classification_report(y_train,y_pred_train))\n\n    y_pred = model_i.predict(X_test)\n    accuracy_test = accuracy_score(y_test,y_pred)\n\n    print(f\"Test accuracy score of {accuracy_test}\")\n    print(\"\\n Test classification report:\")\n    print(classification_report(y_test,y_pred))\n    \n    f= open(f\"log_file_{version}_{minor}.txt\",\"a\")\n    f.write(\"\\n ##################  Final Scoring ###############\")\n    f.write(f\"\\n Train accuracy score of {accuracy_train}\")\n    f.write(f\"\\n Test accuracy score of {accuracy_test}\")\n    f.write(f\"\\n Test Classification report: {classification_report(y_test,y_pred)}\")\n    f.close()\n    \n    return y_pred","7b476ecf":"%%time\nif MODEL_TYPE == \"classifier\" or MODEL_TYPE == \"classifier_cal\":\n        \n    lgb_clf = lgb.LGBMClassifier(**lgb_params)\n    lgb_clf.fit(\n        X_train,\n        y_train,\n        eval_set=(X_test,y_test),\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        eval_metric= METRIC\n    )\n    y_pred = scoring_model(lgb_clf)\n    \n    y_pred_test = lgb_clf.predict(test_scaled)","deef3718":"if MODEL_TYPE == \"classifier\" or MODEL_TYPE == \"classifier_cal\":\n    plt.figure(figsize=(20,7))\n    plt.bar(X.columns, lgb_clf.feature_importances_)\n    plt.xticks(rotation = 90)\n    plt.title(\"Feature Importance\")\n    plt.show()","afe2213c":"if MODEL_TYPE == \"classifier_cal\":\n    model = CalibratedClassifierCV(base_estimator = lgb_clf,  method = CALIBRATION_METHOD, cv=\"prefit\") \n    model.fit(X_test, y_test) \n\n    y_pred = scoring_model(model)\n    \n    #predict Test data\n    y_pred_test = model.predict(test_scaled)","2bf39d3b":"if PSEUDOLABEL == 3:\n    max_probabilities = []\n    for val in y_test_probs:\n        max_probabilities.append(max(val))\n        \n    new_train = test.copy(deep = True)\n    new_train[\"Cover_Type\"] = y_pred_test\n    new_train[\"Cover_Type\"] = new_train[\"Cover_Type\"].astype(\"int32\") +1\n    new_train[\"Max_Proba\"] = max_probabilities\n    \n    # Return only predictions > 0.99 \n    new_train = new_train[new_train[\"Max_Proba\"] >0.99]\n    new_train.drop(\"Max_Proba\", axis = 1)\n    \n    train = pd.concat([train,new_train])\n    new_train.to_csv(\"psedo_labels_2.csv\")","63b9d6ab":"if Full_Train:\n    \n    X = np.concatenate((np.array(X),test),axis =0)\n    y = np.concatenate((np.array(y),y_pred_test),axis =0)\n    \n    train_data = lgb.Dataset(X, label=y)\n    \n    model = lgb.train(params=lgb_params,\n                  train_set= train_data, \n                  verbose_eval = -1\n                 )\n    \n    y_probs = model.predict(X)\n    y_pred = [np.argmax(x) for x in y_probs]\n    print(\"accuracy check\", accuracy_score(y,y_pred))\n    print(classification_report(y,y_pred))\n    \n    #predict Test data\n    y_test_probs = model.predict(test_scaled)\n    y_pred_test = [np.argmax(x) for x in y_test_probs]","aa3594d1":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\", index_col=0)","e7ee6410":"sub[\"Cover_Type\"] = y_pred_test \n\n# Ensure interger column \nsub[\"Cover_Type\"] = sub[\"Cover_Type\"].astype(\"int32\")\n\n# reverse encoding on Target \nsub[\"Cover_Type\"] = sub[\"Cover_Type\"]+1","8d92b534":"sub.to_csv(\"submission.csv\")","d75305a1":"sub[\"Cover_Type\"].value_counts()","b272935b":"sub.head(10)","6e6fa5b8":"## Additional Data \nWe will use the [original data](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset) and concatenate so our training data - to as to keep validation data clean","60caa981":"# Downcasting \nWe note from train.info above that all columns are int64 \\\nWe should look at the min and max of the columns to check if they need full 64bit to store the integers ","5925dbfa":"## Experiments \nI ran multiple experiments using different parameters, the main experiments and submission scores are below:\n\n<style type=\"text\/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-vfn0{background-color:#efefef;border-color:#000000;text-align:left;vertical-align:top}\n.tg .tg-cjtp{background-color:#ecf4ff;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-l0ny{background-color:#FFCE93;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-pwrz{background-color:#FD6864;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-jio7{background-color:#FD6864;border-color:#000000;text-align:left;vertical-align:top}\n.tg .tg-tcpe{background-color:#ecf4ff;border-color:inherit;color:#333333;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-l6ss{background-color:#3531ff;border-color:#6200c9;color:#ffffff;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-mylm{background-color:#ecf4ff;border-color:#000000;color:#333333;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-zv36{background-color:#ffffff;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-7od5{background-color:#9aff99;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-jxgv{background-color:#FFF;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-dvid{background-color:#efefef;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-pdeq{background-color:#FFF;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-y698{background-color:#efefef;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-pidv{background-color:#ffce93;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}\n.tg .tg-112g{background-color:#67FD9A;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-q32f{background-color:#9AFF99;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-smvl{background-color:#fd6864;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-kbue{background-color:#9aff99;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-c6of{background-color:#ffffff;border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-fgdu{background-color:#ecf4ff;border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n<\/style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-l6ss\">No.<\/th>\n    <th class=\"tg-l6ss\">Name<\/th>\n    <th class=\"tg-l6ss\">Version<\/th>\n    <th class=\"tg-l6ss\">Model Type<\/th>\n    <th class=\"tg-l6ss\">Optuna<\/th>\n    <th class=\"tg-l6ss\">Additional Data<\/th>\n    <th class=\"tg-l6ss\">Boosting Type<\/th>\n    <th class=\"tg-l6ss\">Scaler<\/th>\n    <th class=\"tg-l6ss\">Score<\/th>\n    <th class=\"tg-l6ss\">To Rerun<\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0pky\">1<\/td>\n    <td class=\"tg-tcpe\" rowspan=\"4\">Scaler<\/td>\n    <td class=\"tg-0pky\">117<\/td>\n    <td class=\"tg-0pky\">Cross Val<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-0pky\">0.95562<\/td>\n    <td class=\"tg-pdeq\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">2<\/td>\n    <td class=\"tg-0pky\">212<\/td>\n    <td class=\"tg-0pky\">Cross Val<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">MinMax <\/td>\n    <td class=\"tg-0pky\">0.95570<\/td>\n    <td class=\"tg-pdeq\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">3<\/td>\n    <td class=\"tg-0pky\">213<\/td>\n    <td class=\"tg-0pky\">Cross Val<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Standard<\/td>\n    <td class=\"tg-0pky\">0.95570<\/td>\n    <td class=\"tg-pdeq\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">4<\/td>\n    <td class=\"tg-0pky\">251<\/td>\n    <td class=\"tg-0pky\">Cross Val<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">None<\/td>\n    <td class=\"tg-7od5\">0.95571<\/td>\n    <td class=\"tg-zv36\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">5<\/td>\n    <td class=\"tg-tcpe\" rowspan=\"2\">Optuna <\/td>\n    <td class=\"tg-y698\">173<\/td>\n    <td class=\"tg-pidv\">Train<\/td>\n    <td class=\"tg-pidv\">Yes<\/td>\n    <td class=\"tg-pidv\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-pidv\">GBDT<\/td>\n    <td class=\"tg-pidv\">None<\/td>\n    <td class=\"tg-pidv\">0.95496<\/td>\n    <td class=\"tg-l0ny\">Rerunning<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">6<\/td>\n    <td class=\"tg-y698\">240<\/td>\n    <td class=\"tg-y698\">Train<\/td>\n    <td class=\"tg-y698\">Yes<\/td>\n    <td class=\"tg-y698\">PSEUDO 2<\/td>\n    <td class=\"tg-y698\">GBDT<\/td>\n    <td class=\"tg-dvid\">MinMax<\/td>\n    <td class=\"tg-7od5\">BASELINE<\/td>\n    <td class=\"tg-zv36\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">9<\/td>\n    <td class=\"tg-tcpe\" rowspan=\"3\">Model Type<\/td>\n    <td class=\"tg-0pky\">86<\/td>\n    <td class=\"tg-0pky\">Train<\/td>\n    <td class=\"tg-0pky\">Yes<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-7od5\">0.95491<\/td>\n    <td class=\"tg-0pky\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">10<\/td>\n    <td class=\"tg-0pky\">112<\/td>\n    <td class=\"tg-0pky\">Classifier<\/td>\n    <td class=\"tg-0pky\">Yes<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-7od5\">0.95510<\/td>\n    <td class=\"tg-0pky\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\">11<\/td>\n    <td class=\"tg-0pky\">107<\/td>\n    <td class=\"tg-0pky\">Cross Val<\/td>\n    <td class=\"tg-0pky\">Yes<\/td>\n    <td class=\"tg-0pky\">NO<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-7od5\">0.95556<\/td>\n    <td class=\"tg-fymr\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-73oq\">12<\/td>\n    <td class=\"tg-mylm\">DART<\/td>\n    <td class=\"tg-vfn0\">238<\/td>\n    <td class=\"tg-vfn0\">Train<\/td>\n    <td class=\"tg-vfn0\">No<\/td>\n    <td class=\"tg-vfn0\">PSEUDO 2<\/td>\n    <td class=\"tg-vfn0\">DART<\/td>\n    <td class=\"tg-vfn0\">Robust<\/td>\n    <td class=\"tg-vfn0\">0.95532<\/td>\n    <td class=\"tg-jio7\">RE-RUNNING HOME<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-cjtp\" rowspan=\"11\"><span style=\"font-weight:bold\">CROSS VAL<\/span><\/td>\n    <td class=\"tg-fymr\">259<\/td>\n    <td class=\"tg-fymr\">Train <\/td>\n    <td class=\"tg-fymr\">No<\/td>\n    <td class=\"tg-fymr\">PSEUDO 2<\/td>\n    <td class=\"tg-fymr\">GBDT<\/td>\n    <td class=\"tg-fymr\">MinMax<\/td>\n    <td class=\"tg-112g\">0.95692<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">260<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<br>Original Data (4,5,6)<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">MinMax<\/td>\n    <td class=\"tg-7od5\">0.95692<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">267<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">MinMax<\/td>\n    <td class=\"tg-7od5\">0.95692<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">220<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-q32f\">0.95688<\/td>\n    <td class=\"tg-smvl\">RE-RUNNING 261<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">228<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<br>Original Data (4,5,6)<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Robust<\/td>\n    <td class=\"tg-q32f\">0.95689<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-pidv\">234<\/td>\n    <td class=\"tg-pidv\">Train <\/td>\n    <td class=\"tg-pidv\">No<\/td>\n    <td class=\"tg-pidv\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-pidv\">GBDT<\/td>\n    <td class=\"tg-pidv\">Robust<\/td>\n    <td class=\"tg-112g\">0.95690<\/td>\n    <td class=\"tg-pwrz\">RE-RUNNING 266<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Standard<\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-pwrz\">RUNNING 262<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">264<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">Standard<\/td>\n    <td class=\"tg-7od5\">0.95684<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">258<\/td>\n    <td class=\"tg-0pky\">Train <\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">None<\/td>\n    <td class=\"tg-7od5\">0.95691<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-fymr\">252<\/td>\n    <td class=\"tg-fymr\">Train <\/td>\n    <td class=\"tg-fymr\">No<\/td>\n    <td class=\"tg-fymr\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-fymr\">GBDT<\/td>\n    <td class=\"tg-fymr\">None<\/td>\n    <td class=\"tg-kbue\">0.95701<\/td>\n    <td class=\"tg-jxgv\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">253<\/td>\n    <td class=\"tg-0pky\">Train - unbalanced false<\/td>\n    <td class=\"tg-0pky\">No<\/td>\n    <td class=\"tg-0pky\">PSEUDO 2<br>Original Data (Complete)<\/td>\n    <td class=\"tg-0pky\">GBDT<\/td>\n    <td class=\"tg-0pky\">None<\/td>\n    <td class=\"tg-7od5\">NO Change<\/td>\n    <td class=\"tg-c6of\"><\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-fgdu\">Ensemble<\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\">Train, Class, Cross_val<\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n    <td class=\"tg-0pky\"><\/td>\n  <\/tr>\n<\/tbody>\n<\/table>\n\n**BEST Run** CROSS VAL v252 - No Scaling PSEUDO 2 +ADD DATA(Complete) **BEST**  ==0.95691 ","b7371a0e":"![image.png](attachment:97f75336-b8a0-4e8b-a220-719f126bac38.png)","4ccc6b4c":"# Train Model - with best parms\n\nIf using Calibration - need to use LGBMClassifier as this is compatible with probability calibration - although much slower","dd61f7b4":"## Sampling techniques\n\nSampling used:\n* none\n* Class 5 duplication \n* SMOTEEEN   --- due to memory issues we have to run pca \n* SMOTETomek --- due to memory issues we have to run pca ","15eba925":"# Post Processing - Pseudo Labelling 3\nThis creates pseudo data 2:\nFind the highest probabilities, set a threshold. If probabilities are > threshold \\\nKeep rows as ground truth and retrain model (import data into Kaggle and rerun)","ad4ca734":"# Split ","214b54bd":"## PCA \n* There are a certain features which may have low to no impact on the Target - PCA should have limit this impact and remove noise\n* Oversampling Techniques - Due to memory issues for sampling we need to run PCA prior to Sampling","c48e86f9":"# Submission ","fb37bd2a":"**Note**: Elevation seems to have the highest negative correlation to Cover_type although relativelty weak correlation ","193701b8":"## Load Additional & Pseudo data \n* Load original [forest Cover Data ](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset) \\\n   From previous experiments we noted classes 4,5, and 6 have the worst accuracy \\\n   We will add these classes only\n","b4e174fb":"## To  do: \n\n#### 1. Ensemble \n- create a model with Cross Val (find best)\n- Create model with Neural Network (seperate kernel)\n- Add probabilities together and argmax\n\n#### 2. DART Cross val\n- Run outside Kaggle as takes too long \n\n#### 3. Check Clipped values feature importance -- maybe remove\n\n#### 4. Fold Additional Data into Cross Val ","9089b002":"#### Over Sampling techniques \nTo check \n\n#### PCA\nTo check\n\n#### Summary \n* Hyperparameters tuning              ---- using Optuna \n* Oversampling                        ---- duplicated class5 increases accuracy -- checking SMOTEENN and SMOTEtomek\n* clustering                          ---- Kmeans reduces accuracy                  ---- CHECKING DBSCAN\n* Calibration                         ---- sigmoid reduces accuracy-- \"isotonic\" seems to improve accuracy\n* Cross Validation                    ---- Checking \n* pseudo labelling                    ---- CHECKING \n* Scaling                             ---- Unsure - need to confirm with standardised params","9385de43":"### Correlation analaysis \nWe will exclude soil types as these are boolean values ","524bab89":"# Project Description and goals \nIn this months Tabular Playground we have a synthetic [forest cover dataset ](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data). \n\n## Process:\n### 1. Feature engineering \n* feature extraction - as per notebooks and competition discussions \n* feature processing - duplicates, nulls etc \n\n### 2. Data Augmentation \n* Sampling  - over \/ under sampling \n* Additional data - external \/ synthetic(oversampling) \\   \n* Feature Selection  - PCA \n\n### 3. Model Build  \/ Transfer learning \n* Model type  **Note** will only use LIGHTGBM\n* Loss \/ error type - multilogloss\n* Model build - classifier, train, cross-val\n\n### 4. Training Schedule and Optimization\n* Optimize LIGHTGBM  ---needed for multiple experiments \n* Parameter Tuning - Optuna \n\n### 5. Post processing \n* Improve post predicted values - calibration\n\nScoring metric = accuracy","915bad10":"### SPlit & Scale\nSplit and scaling for **Train** and **Classifier** done here\n**Cross Val ** is done in fold ","cb01de85":"## lgb.Train model ","fa99b880":"# Cross Validation ","74b60462":"# Optuna hyperparameter tuning \nOptuna + cross val takes too long - will exclude any experiments with both enabled","509a1d81":"# EDA ","099e0b48":"## Encode the Target\nSeems like a weird interaction in lighgbm.train \\\nNeed to specify the num_classes for the model and it only works if the target class starts from 0 (for classification) ","d61decee":"## Enable LightGBM GPU","33a52aa4":"# POST Full Training \nAs we are trying to get the most out of the model, we will do one final training on the full training data (no splitting)","dd11a136":"# Post Processing - Probability Calibration \n\nsklearn calibration only works with LGBMClassifier","60ef2031":"# Libraries ","f1ace811":"### Note \nWe are looking at the inverse accuracy (1-accuracy) which we want to minimize \\\nwe are doing this to keep the prunning callback in line with the objective metric","59d40b53":"# Cluster  - Check ","9341db89":"# Data Augmentation \n\n1. Adding original data \n1. adding Pseudo labels\n1. Over\/ under sample data \n\n**Note**: Im adverse to drop data so we shall keep class 5 and either append new data, over\/undersample or duplicate class 5","66820f64":"## Experiments","47cc40c7":"## Pseudo Labelling \nTwo ways we can do this: \\\n**Process 1** \\\nWe can use historical submissions (10 submissions) and identify where all submissions predicted the same class \\\nWe can then append these rows to our training data \n   \n**Process 2** \\\nRun our a training model over the training data - where probability > threshold (i.e. 90%) then we add this data to training data and retrain model ","cd1800e5":"# Feature Engineering and splitting \nAll credit to their respecitve notebooks and discussion topics ","12fcc2af":"## lgb.LGBMClassifier model"}}