{"cell_type":{"7bdc223c":"code","cf3c19ab":"code","a3c6a9fd":"code","2c1df508":"code","61eeeddd":"code","023a9db1":"code","cf973dda":"code","19c59e35":"code","017129ad":"code","1569babb":"code","ea3730ec":"code","9f9cd343":"code","22008bac":"code","634a8d6e":"code","e9cc0529":"code","4c459115":"code","1354e858":"code","f7b4a3ac":"code","9e871458":"code","8b417ad0":"code","716c3316":"code","de3c6ded":"code","c2103c58":"code","73d05f7d":"code","7ea9991b":"code","26813956":"code","bf8eed5c":"markdown","12aeb911":"markdown","d49b587f":"markdown","ff687c13":"markdown","c14c5218":"markdown","0b748270":"markdown","e2bb4d95":"markdown","4c012431":"markdown","80da01d8":"markdown","2b72510f":"markdown","de44ce6d":"markdown","674a0fb4":"markdown","d01ef2cb":"markdown","7194a65e":"markdown","8c85ef72":"markdown","a93e5ede":"markdown","d1022ef8":"markdown","e306ec80":"markdown","f16b7820":"markdown","d4da79c1":"markdown","4b51c75d":"markdown","272a238a":"markdown","883805fa":"markdown","ef5abbf3":"markdown","c3db8ee1":"markdown","eb8dfe8a":"markdown","e02fd687":"markdown","d1911011":"markdown","c33b5b61":"markdown","51201d94":"markdown"},"source":{"7bdc223c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf3c19ab":"df = pd.read_csv(\"..\/input\/traffic-flow-data-in-ho-chi-minh-city-viet-nam\/train.csv\", index_col=\"_id\", parse_dates=[\"date\"])\n\nprint(df.head())\nprint(df.shape)","a3c6a9fd":"# Choose concerning cols\ncols = [\"segment_id\", \"street_id\", \"street_name\", \"date\", \"weekday\", \n        \"length\", \"max_velocity\", \"street_level\", \"street_type\", \n        \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\", \"period\", \"LOS\"]\ndf = df[cols]","2c1df508":"print(df.head())","61eeeddd":"import datetime\n\n# 6h-8h, 16h-19h\npeaks = [\"period_6_00\", \"period_6_30\", \n         \"period_7_00\", \"period_7_30\",\n         \"period_16_00\", \"period_16_30\", \n         \"period_17_00\", \"period_17_30\",\n         \"period_18_00\", \"period_18_30\"]\n\ndef is_special(date):\n    # holidays = [(day, month)]\n    holidays = [(1,1), (14,2), (8,3), (30,4), \n                (1,5), (1,6), (2,9), (20,10), \n                (20,11), (24,12), (25,12)]\n    for holiday in holidays:\n        if date.day == holiday[0] and\\\n           date.month == holiday[1]:\n            return True\n    return False","023a9db1":"df[\"is_weekend\"] = df[\"weekday\"].apply(lambda x: int(x in [5, 6]))\ndf[\"is_peak\"] = df[\"period\"].apply(lambda p: int(p in peaks))\ndf[\"special_day\"] = df[\"date\"].apply(lambda date: int(is_special(date)))\nprint(df.head())","cf973dda":"missing_df = pd.DataFrame((df.isna().sum() \/ df.shape[0]), columns=[\"missing_ratio\"]).sort_values(\"missing_ratio\", ascending=False)\nprint(missing_df)","19c59e35":"def plot_cat_cols_with_target(data, cols, target):\n    for col in cols:\n        pd.crosstab(data[col], data[target]).plot.bar(figsize=(20, 10), fontsize=18)\n    plt.show()\n        \ncat_cols = [\"weekday\", \"street_level\", \"street_type\",\n            \"period\", \"is_weekend\", \"is_peak\", \"special_day\"]\nplot_cat_cols_with_target(df, cat_cols, \"LOS\")","017129ad":"def plot_num_cols_with_target(data, cols, target):\n    for i, col in enumerate(cols):\n        plt.figure(figsize=(16, 8))\n        sns.violinplot(x=target, y=col, data=data)\n        plt.show()\n\n# Ignore 'max_velocity'\nnum_cols = [\"length\", \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\"]\nplot_num_cols_with_target(df, num_cols, \"LOS\")","1569babb":"def check_label(target_cols):\n    target_cols.value_counts().plot.bar(figsize=(12, 8))\n    plt.show()\ncheck_label(df[\"LOS\"])","ea3730ec":"from sklearn.preprocessing import LabelEncoder\n\nnum_cols = [\"length\", \"max_velocity\", \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\"]\ncat_cols = [\"street_id\", \"segment_id\", \"weekday\", \"street_level\", \"street_type\", \"period\", \"is_weekend\", \"is_peak\",\"special_day\"]\n\ndef plot_heatmap(data):\n    cols = [\"LOS\"] + num_cols + cat_cols\n    temp_df = data[cols].copy()\n    \n    encoder = LabelEncoder()\n    for col in cat_cols + [\"LOS\"]:\n        temp_df[col] = encoder.fit_transform(temp_df[col])\n        \n    corrmat = temp_df[cols].corr()\n    plt.figure(figsize=(12, 9))\n    sns.heatmap(corrmat, cbar=True, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, yticklabels=cols, xticklabels=cols)\n    plt.show()\n\nplot_heatmap(df)","9f9cd343":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\ndef classification_report_df(y_true, y_pred):\n    classes = np.unique(y_true)\n    true = label_binarize(y_true, classes=classes)\n    pred = label_binarize(y_pred, classes=classes)\n    \n    fpr, tpr, roc_auc = dict(), dict(), dict()\n    for i, c in enumerate(classes):\n        fpr[c], tpr[c], _ = roc_curve(true[:, i], pred[:, i])\n        roc_auc[c] = auc(fpr[c], tpr[c])\n        \n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true.ravel(), pred.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    return fpr, tpr, roc_auc\n\ndef plot_multiclass_roc(y_true, y_pred, title=\"Extension ROC to multi-class\"):\n    fpr, tpr, roc_auc = classification_report_df(y_true, y_pred)\n    classes = fpr.keys()\n    all_fpr = np.unique(np.concatenate([fpr[c] for c in classes]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for c in classes:\n        mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n    mean_tpr \/= len(classes)\n    \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n             color='deeppink', linestyle=':', linewidth=4)\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:0.2f})',\n             color='navy', linestyle=':', linewidth=4)\n    \n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    lw = 2\n    for c, color in zip(classes, colors):\n        plt.plot(fpr[c], tpr[c], color=color, lw=lw,\n                 label=f'ROC curve of class {c} (area = {roc_auc[c]:0.2f})')\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","22008bac":"from sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_features = make_column_selector(dtype_exclude=object)\ncat_features = make_column_selector(dtype_include=object)\n\nnum_pipeline = Pipeline([('numerical_imputer', SimpleImputer(strategy=\"median\")),\n                         ('numerical_scaler', RobustScaler())])\ncat_pipeline = Pipeline([('categorical_imputer', SimpleImputer(strategy=\"most_frequent\")),\n                         ('categorical_encoder', OneHotEncoder(handle_unknown=\"ignore\"))])\n\npreprocessor = make_column_transformer((num_pipeline, num_features), (cat_pipeline, cat_features))\n\n# Choose model\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = Pipeline([(\"preprocessor\", preprocessor),\n                  (\"classifier\", DecisionTreeClassifier(random_state=0))])","634a8d6e":"from sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\n\ndef train_and_evaluate_model(X, y, model):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    \n    plot_confusion_matrix(model, X_val, y_val)\n    print(classification_report(y_val, y_pred))\n    plot_multiclass_roc(y_val, y_pred)","e9cc0529":"features = [\"weekday\", \"length\", \"street_level\", \"street_type\", \"long_snode\", \"lat_snode\", \"period\"]\n\ntrain_and_evaluate_model(df[features], df[\"LOS\"], model)","4c459115":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX, y = preprocessor.fit_transform(df[features]), df[\"LOS\"]\nprint(\"Before:\", X.shape, y.shape)\nX, y = smote.fit_resample(X, y)\nprint(\"After:\", X.shape, y.shape)\ny.value_counts().plot.bar(figsize=(12, 8))\nplt.show()","1354e858":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ndef preprocess_dataset(X, y, preprocessor, resampler, test_size=0.2):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0)\n    X_train = preprocessor.fit_transform(X_train)\n    X_val = preprocessor.transform(X_val)\n    X_train, y_train = resampler.fit_resample(X_train, y_train)\n    return X_train, X_val, y_train, y_val\n\ndef train_and_validate(X_train, X_val, y_train, y_val, model, plot_title=\"Extension ROC to multi-class\"):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    print(classification_report(y_val, y_pred))\n    plot_confusion_matrix(model, X_val, y_val)\n    plot_multiclass_roc(y_val, y_pred, plot_title)","f7b4a3ac":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, SMOTE())\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with normal SMOTE\")","9e871458":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\nresampler = ImbPipeline(steps=[('o', SMOTE(sampling_strategy={\"B\":5000, \"C\":5000, \"D\":5000, \"E\":5000, \"F\":5000})),\n                               ('u', RandomUnderSampler(sampling_strategy={\"A\":8000}))])\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with Oversampling & Undersampling SMOTE\")","8b417ad0":"from imblearn.over_sampling import ADASYN\n\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, ADASYN())\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with ADASYN\")","716c3316":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with SMOTE Tomek Links\")","de3c6ded":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model)","c2103c58":"from sklearn.ensemble import RandomForestClassifier\n\nweights = {'F': 1.5, 'A': 0.8, 'B': 1.5, 'C': 1.5, 'D': 1.5, 'E': 1.5}\nforest = RandomForestClassifier(n_estimators=50, class_weight=weights)\nmodel = Pipeline([('preprocessor', preprocessor), ('classifier', forest)])\n\nX_train, X_val, y_train, y_val = train_test_split(df[features], df['LOS'], test_size=0.2, random_state=26)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model)","73d05f7d":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n# Model\nmodel = RandomForestClassifier(n_estimators=100)\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Random Forest with SMOTE Tomek Links\")","7ea9991b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\ndef get_models():\n    models, names = [], []\n    # LDA\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    # OAA SVC\n    # One-Against-All\n    models.append(OneVsRestClassifier(SVC()))\n    names.append('OAA-SVC')\n    # KNN\n    models.append(KNeighborsClassifier(n_neighbors=3))\n    names.append('KNN')\n    return zip(models, names)","26813956":"X_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nfor model, name in get_models():\n    train_and_validate(X_train.toarray(), X_val.toarray(), y_train, y_val, model, name)","bf8eed5c":"Some features may affect classification of LOS:\n\n* All columns seems not to be related to LOS, maybe these features aren't enough for classification.\n* Some cells show that long_snode ~ long_enode, lat_snode ~ lat_enode with full correlation. Easy to understand because segments connected by nodes so that a start node of a segment can also be an end node of other. Therefore, remove a pair of long\/lat of end node in each sample before train to prevent overestimating these features.\n* street_level somewhat relates to street_type, fair enough!\n\nThe dataset is pretty dark: outliers, unrelated features, severe imbalance; but diamond cuts diamond, I will try to mine it.","12aeb911":"## Baseline classification model","d49b587f":"## SMOTE\n\nAuthor suggested: one approach to addressing imbalanced datasets is to oversample the minority class. Simplest approach involves duplicating samples in the minority class which leaves out generating unnecessary information.\n\nSynthetic Minor Oversampling Technique(SMOTE): select examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\n[SMOTE for Imbalanced Classification](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)","ff687c13":"## Cost-sensitive random forest classifier\n\n[Multi-class Imbalanced Classification](https:\/\/machinelearningmastery.com\/multi-class-imbalanced-classification\/)\n\n[Cost-Sensitive Learning](https:\/\/machinelearningmastery.com\/cost-sensitive-learning-for-imbalanced-classification\/)","c14c5218":"Create a list of models to test","0b748270":"## 2. Numerical columns","e2bb4d95":"# I. Load dataset","4c012431":"# VI. Reference for improvement\n\nhttps:\/\/www.reddit.com\/r\/MachineLearning\/comments\/12evgi\/classification_when_80_of_my_training_set_is_of\/\n\nhttps:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data\n\n[Weka's CostSensitiveClassifier](https:\/\/www.youtube.com\/watch?v=LbZ9ROR1tQ0)\n\n[Sample Imbalanced Multiclass Classification](https:\/\/machinelearningmastery.com\/imbalanced-multiclass-classification-with-the-e-coli-dataset\/)","80da01d8":"## 1. Missing values","2b72510f":"## 5. Relationship","de44ce6d":"## 4. Label","674a0fb4":"## Random Forest with SMOTE Tomek Links","d01ef2cb":"# V. Check model","7194a65e":"### Why care about Imbalanced Classification? ([this blog](https:\/\/machinelearningmastery.com\/what-is-imbalanced-classification\/) for details and further reading)\n* Most ML algorithms for classification were designed around the assumption of an equal number of examples for each class; therefore imbalanced model will prone to majority class, which is bad for generalization.\n* In real world, we're mostly interested in minority class so it's useless if a model shows poor performance on minor population.\n\n### 3 advises when addressing imbalance problem:\n1. Use other classification metrics rather than just 'accuracy': 'precision', 'recall', 'F1-score', ie.\n2. Preprocess the raw data before feeding it into model: data augmentation, ie.\n3. Use variant of models\/ML algorithms that treat classification errors differently.","8c85ef72":"### SMOTE Tomek Links","a93e5ede":"## 2. Categorical columns","d1022ef8":"1. Outliers!!!\n2. Worth noting that plots for **long_snode & long_enode** look familiar to each other, as well as **lat_snode & lat_enode**","e306ec80":"This is how SMOTE balances the distribution of classes. Now let's feed it into model.","f16b7820":"### Normal SMOTE","d4da79c1":"Dataset is imbalanced which is not good for classification.","4b51c75d":"### Weighting-SMOTE: Oversampling & Undersampling","272a238a":"Model classifies A and B pretty well, others are not good since the dataset as shown to be imbalance towards 'A', or maybe data is not well distinguishable among class C, D, E, F.","883805fa":"## Feature Enrichment","ef5abbf3":"## Plot ROC curves for multiclass classification by computing macro-average ROC curve & ROC area","c3db8ee1":"The patterns show some similar distributions of LOS labels","eb8dfe8a":"# II. Simple EDA\n\n* LOS is the target we want to classify\n* Mostly data is categorical, except: velocity, max_velocity, long\/lat of 2 nodes","e02fd687":"# III. Implement metric for evaluation multiclass classification","d1911011":"A lot missing values in max_velocity, other columns are good.","c33b5b61":"# IV. Try to mining data in desperation","51201d94":"### Adaptive Synthetic Sampling (ADASYN)"}}