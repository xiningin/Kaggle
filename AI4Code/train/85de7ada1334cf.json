{"cell_type":{"983b65e5":"code","05f90185":"code","f0ab23c5":"code","4d414f01":"code","7eb21ed4":"code","5f430332":"code","c936a87c":"code","6c00dffe":"code","a5cc88a8":"code","7a23f7e1":"code","af27545e":"code","bc97f415":"code","14dcdc1f":"code","18e9c686":"markdown","98004a6f":"markdown","7a2d4a22":"markdown","93151ec3":"markdown","652b7f25":"markdown","c58069cd":"markdown","fc25d197":"markdown","951cc966":"markdown","ff6ecc8f":"markdown","ddf735cb":"markdown","af7b8c1f":"markdown"},"source":{"983b65e5":"import pandas as pd\n\ndf = pd.read_csv(\"\/kaggle\/input\/pubmed-abstracts\/pubmed_abstracts.csv\", index_col=0)\ndf.tail()","05f90185":"import re\n\n# function for removing html tags\ndef preprocess_text(text):\n    text = text.str.replace(r\"[^a-zA-Z\u0430-\u044f\u0410-\u042f1-9]+\", ' ') # remove () & []\n    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text\n\n\n\ndf = df[df.columns.tolist()[:8]].apply(lambda x: preprocess_text(x))\ndf.head()","f0ab23c5":"# Data\ndl = df.deep_learning.dropna()\ncovid_19 = df.covid_19.dropna()\nhuman_connectome = df.human_connectome.dropna()\nvirtual_reality = df.virtual_reality.dropna()\nbrain_machine_interfaces = df.brain_machine_interfaces.dropna()\nelectroactive_polymers = df.electroactive_polymers.dropna()\npedot_electrodes = df.pedot_electrodes.dropna()\nneuroprosthetics = df.neuroprosthetics.dropna()\n\ndata = list(pd.concat([dl, covid_19, \n                        human_connectome, virtual_reality, \n                        brain_machine_interfaces, electroactive_polymers, \n                        pedot_electrodes, neuroprosthetics]).unique())\n\n\nprint(data[:1])\n\n# array of texts\nnew_df = pd.DataFrame({\"data\": data})\nnew_df.head()","4d414f01":"from torchtext.data import Field\n\ntext_field = Field(init_token='<s>', eos_token='<\/s>', lower=True, tokenize=lambda line: list(line))\n\ntext_field.preprocess(data[0])","7eb21ed4":"import matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndf = df.fillna('')\ndl_lines = df.apply(lambda row: text_field.preprocess(row['deep_learning']), axis=1).tolist()\nc_lines = df.apply(lambda row: text_field.preprocess(row['covid_19']), axis=1).tolist()\nhc_lines = df.apply(lambda row: text_field.preprocess(row['human_connectome']), axis=1).tolist()\nvr_lines = df.apply(lambda row: text_field.preprocess(row['virtual_reality']), axis=1).tolist()\nbmi_lines = df.apply(lambda row: text_field.preprocess(row['brain_machine_interfaces']), axis=1).tolist()\nep_lines = df.apply(lambda row: text_field.preprocess(row['electroactive_polymers']), axis=1).tolist()\npe_lines = df.apply(lambda row: text_field.preprocess(row['pedot_electrodes']), axis=1).tolist()\nnp_lines = df.apply(lambda row: text_field.preprocess(row['neuroprosthetics']), axis=1).tolist()\ndl_lengths = [len(line) for line in dl_lines]\nc_lengths = [len(line) for line in c_lines]\nhc_lengths = [len(line) for line in hc_lines]\nvr_lengths = [len(line) for line in vr_lines]\nbmi_lengths = [len(line) for line in bmi_lines]\nep_lengths = [len(line) for line in ep_lines]\npe_lengths = [len(line) for line in pe_lines]\nnp_lengths = [len(line) for line in np_lines]\n\n\n# plot\nfig, ax = plt.subplots(nrows=4,  ncols=2, figsize=(20, 20))\n\nax[0, 0].hist(dl_lengths, bins=50, color='y')[-1]\nax[0, 0].set_title(\"deep learning\")\nax[0, 1].hist(c_lengths, bins=50, color='g')[-1]\nax[0, 1].set_title(\"covid 19\")\nax[1, 0].hist(hc_lengths, bins=50, color='r')[-1]\nax[1, 0].set_title(\"human connectome\")\nax[1, 1].hist(vr_lengths, bins=50, color='purple')[-1]\nax[1, 1].set_title(\"virtual reality\")\nax[2, 0].hist(bmi_lengths, bins=50)[-1]\nax[2, 0].set_title(\"brain machine interfaces\")\nax[2, 1].hist(ep_lengths, bins=50, color='pink')[-1]\nax[2, 1].set_title(\"electroactive polymers\")\nax[3, 0].hist(pe_lengths, bins=50, color='b')[-1]\nax[3, 0].set_title(\"pedot electrodes\")\nax[3, 1].hist(np_lengths, bins=50)[-1]\nax[3, 1].set_title(\"neuroprosthetics\")\nplt.suptitle(\"Text length distribution\", y=.95, fontsize=20)\nplt.show()","5f430332":"from torchtext.data import Example\nfrom torchtext.data import Dataset\n\nDEVICE = \"cpu\"#\"cuda:0\"\n\n# lines longer than 5k characters and less than 50 are eliminated.\nlines = new_df.apply(lambda row: text_field.preprocess(row['data']), axis=1).tolist()\nlines = [line for line in lines if 4000 > len(line) >= 50]\n\nfields = [('text', text_field)]\nexamples = [Example.fromlist([line], fields) for line in lines]\ndataset = Dataset(examples, fields)","c936a87c":"# split data into train and validation \ntrain_dataset, test_dataset = dataset.split(split_ratio=0.75)\n\ntext_field.build_vocab(train_dataset, min_freq=30)\n\nprint('Vocab size =', len(text_field.vocab))\nprint(text_field.vocab.itos)","6c00dffe":"# Iterative object\n\nfrom torchtext.data import BucketIterator\n\ntrain_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 192), \n                                              shuffle=True, device=DEVICE, sort=False)\n\nbatch = next(iter(train_iter))\nbatch","a5cc88a8":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nDEVICE = 'cuda:0'","7a23f7e1":"class RnnLM(nn.Module):\n    def __init__(self, vocab_size, emb_dim=32, lstm_hidden_dim=192, num_layers=256):  \n        super().__init__()\n\n        self._emb = nn.Embedding(vocab_size, emb_dim)\n        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n\n    def forward(self, inputs, hidden=None):\n        outputs = self._emb(inputs)\n        outputs, hidden = self._rnn(outputs, hidden)\n        outputs = self._out_layer(outputs)\n        \n        return outputs, hidden\n\nmodel = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)","af27545e":"\ndef sample(probs, temp):\n    probs = F.log_softmax(probs.squeeze(), dim=0)\n    probs = (probs \/ temp).exp()\n    probs \/= probs.sum()\n    probs = probs.cpu().numpy()\n\n    return np.random.choice(np.arange(len(probs)), p=probs)\n\n\ndef generate(model, temp=0.8):\n    model.eval()\n    with torch.no_grad():\n        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n        end_token = train_iter.dataset.fields['text'].vocab.stoi['<\/s>']\n        \n        hidden = None\n        for _ in range(1000):# After preprocessing text contains a length of at least 50 to 5000 characters.\n            probs, hidden = model(torch.LongTensor([[prev_token]]).to(DEVICE), \n                                  hidden)\n            prev_token = sample(probs[-1], temp)\n        \n            print(train_dataset.fields['text'].vocab.itos[prev_token], end='')\n            if prev_token == end_token:\n                return\n            \n\ngenerate(model)","bc97f415":"import math\nfrom tqdm import tqdm\n\n\ndef do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n    epoch_loss = 0\n    \n    is_train = not optimizer is None\n    name = name or ''\n    model.train(is_train)\n    \n    batches_count = len(data_iter)\n    \n    with torch.autograd.set_grad_enabled(is_train):\n        with tqdm(total=batches_count) as progress_bar:\n            for i, batch in enumerate(data_iter):\n    \n                labels = batch.text[1:, :]\n                labels = labels.view(-1).to(DEVICE)\n            \n                logits, _ = model(batch.text.to(DEVICE))\n                logits = logits[:-1, :, :]\n                logits = logits.view(-1, logits.shape[-1])\n                \n                target = ((labels != pad_idx) * (labels != unk_idx)).float()\n                loss = torch.sum(criterion(logits, labels.view(-1)) * target) \/ torch.sum(target)\n                \n                epoch_loss += loss.item()\n\n                if optimizer:\n                    optimizer.zero_grad()\n                    loss.backward()\n                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n                    optimizer.step()\n\n                progress_bar.update()\n                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n                                                                                         math.exp(loss.item())))\n                \n            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n                name, epoch_loss \/ batches_count, math.exp(epoch_loss \/ batches_count))\n            )\n\n    return epoch_loss \/ batches_count\n\n\ndef fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n    for epoch in range(epochs_count):\n        name_prefix = '[{} \/ {}] '.format(epoch + 1, epochs_count)\n        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n        \n        if not val_iter is None:\n            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n\n        generate(model)\n        print()","14dcdc1f":"model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n\npad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\nunk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\ncriterion = nn.CrossEntropyLoss().to(DEVICE)\n\noptimizer = optim.Adam(model.parameters())\n\nfit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)","18e9c686":"**Data tokenization** - turning a string into a character set with initialization Beginning Of Sentence\u0438 and End Of Sentence aka **BOS** & **EOS**.\n\n","98004a6f":"### Metric for language models - Perplexity\nBesides cross entropy loss, there is still **Perplexity** its cross-entropy losses erected by the exponent.\n\n[Intuition](https:\/\/towardsdatascience.com\/perplexity-intuition-and-derivation-105dd481c8f3)","7a2d4a22":"Generating the next sentence","93151ec3":"# What is data and what are labels?\nOur data its tokenized arrays of texts by which we can iterate.\nOur labels following words - that is, simply shifted by 1 input tensor.\n\nfor example: \n\n### sentence = \"I think this plastic box is very beautiful\"\n\ntext:\n\n+ **data = bos, i, think, this, plastic, box, is, very, beautiful**\n\n+ **target = i, think, this, plastic, box, is, very, beautiful, eos**\n\ntokens:\n+ **data =   33, 44, 22, 3, 4,  88, 12, 0,  22**\n+ **target = 44, 22, 3,  4, 88, 12,  0, 22, 19**\n\n(numbers are involuntary)\n\n","652b7f25":"# Data Pre-Processing","c58069cd":"# Train model","fc25d197":"# RNN with LSTM ","951cc966":"# Dataset and Datagenerator","ff6ecc8f":"Converting text data columns to single text column","ddf735cb":"# Character-level Language Model for abstracts generation.\n\n### Steps:\n+ Data Pre-Processing\n+ Dataset and DataGenerator\n+ RNN with LSTM\n+ What is data and what are labels?\n+ Train model","af7b8c1f":"Text length distribution"}}