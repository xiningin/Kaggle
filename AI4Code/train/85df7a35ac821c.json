{"cell_type":{"b2fc5f6e":"code","32619ce1":"code","9fd42b5c":"code","2b636373":"code","b7921d7c":"code","2649a9a6":"code","43accd69":"code","1a25bf02":"code","293d5a42":"code","982e0c13":"code","e7e96fbb":"code","3d4f7202":"code","da596e54":"markdown","c5aff16c":"markdown","d29ff826":"markdown","0f926dfa":"markdown","0b0c8a0f":"markdown","3ced1c65":"markdown","f1bb25ba":"markdown","792a0729":"markdown","838a456a":"markdown"},"source":{"b2fc5f6e":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","32619ce1":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv', index_col= 'id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv', index_col= 'id')\nsubmission = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv', index_col= 'id')","9fd42b5c":"target = train.pop('target')\ny = target","2b636373":"cat_features = [col for col in train.columns if train[col].dtype=='object']\nnum_features = [col for col in train.columns if train[col].dtype=='float']\n\n\nle = LabelEncoder()\n\nle_train = train.copy()\nle_test = test.copy()\n\nfor col in cat_features:\n    le_train[col] = le.fit_transform(train[col])\n    le_test[col] = le.transform(test[col])\n","b7921d7c":"train = le_train\ntest = le_test","2649a9a6":"features = train.columns\nlen(features)","43accd69":"Nfold = 5\nSEED = 100\n\nkfold = KFold(n_splits=Nfold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train.shape[0])\nsubm_preds_xgb = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train)):\n    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n    \n    xgb = XGBRegressor(max_depth=6,\n        learning_rate=0.005,\n        n_estimators=5000,\n        verbosity=1,\n        silent=None,\n        objective='reg:squarederror',\n        booster='gbtree',\n        n_jobs=-1,\n        nthread=None,\n        gamma=0.0,\n        min_child_weight= 133, \n        subsample=0.8,\n        colsample_bytree=0.5,\n        reg_alpha=7.5,\n        reg_lambda=0.25,\n        random_state=SEED,\n        tree_method = 'gpu_hist',\n        predictor = 'gpu_predictor',\n    )                      \n\n    \n    xgb.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y), (val_x, val_y)],\n           eval_metric=\"rmse\", verbose=1000, early_stopping_rounds=40\n           )\n   \n    oof_preds[val_idx] = xgb.predict(val_x)\n    subm_preds_xgb += xgb.predict(test[features])\/kfold.n_splits\n    \n    print('Fold {} MSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))   \n      \n    \nprint(\"*****************************************************************\")\nprint('{} fold local CV= {:.6f}'.format(Nfold, mean_squared_error(y, oof_preds, squared=False)))","1a25bf02":"# the following two code snippets are adapted from the \"feature engineering kaggle min-course\"\n\nfrom sklearn.feature_selection import mutual_info_regression\n\nfeatures = train.dtypes == int\n\ndef make_mi_scores(train, y, discrete_features):\n    mi_scores = mutual_info_regression(train, y, discrete_features=features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=train.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(train, y, features)\nmi_scores","293d5a42":"def plot_utility_scores(scores):\n    y = scores.sort_values(ascending=True)\n    width = np.arange(len(y))\n    ticks = list(y.index)\n    plt.barh(width, y, color='#d1aeab', alpha=0.9)\n    plt.yticks(width, ticks)\n    plt.grid()\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_utility_scores(mi_scores)","982e0c13":"# I tried other combinations of features based on the mutual information score \n# but these two gave the best improvement\n\ntrain['9t1'] = train['cat9']*train['cat1']\ntrain['8t1'] = train['cat8']*train['cat1']\n\ntest['9t1'] = test['cat9']*test['cat1']\ntest['8t1'] = test['cat8']*test['cat1']","e7e96fbb":"features = train.columns\nlen(features)","3d4f7202":"Nfold = 5\nSEED = 100\n\nkfold = KFold(n_splits=Nfold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train.shape[0])\nsubm_preds_xgb = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train)):\n    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n    \n    xgb = XGBRegressor(max_depth=6,\n        learning_rate=0.005,\n        n_estimators=5000,\n        verbosity=1,\n        silent=None,\n        objective='reg:squarederror',\n        booster='gbtree',\n        n_jobs=-1,\n        nthread=None,\n        gamma=0.0,\n        min_child_weight= 133, \n        subsample=0.8,\n        colsample_bytree=0.5,\n        reg_alpha=7.5,\n        reg_lambda=0.25,\n        random_state=SEED,\n        tree_method = 'gpu_hist',\n        predictor = 'gpu_predictor',\n    )                      \n\n    \n    xgb.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y), (val_x, val_y)],\n           eval_metric=\"rmse\", verbose=1000, early_stopping_rounds=40\n           )\n   \n    oof_preds[val_idx] = xgb.predict(val_x)\n    subm_preds_xgb += xgb.predict(test[features])\/kfold.n_splits\n    \n    print('Fold {} MSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))   \n      \n    \nprint(\"*****************************************************************\")\nprint('{} fold local CV= {:.6f}'.format(Nfold, mean_squared_error(y, oof_preds, squared=False)))","da596e54":"# 1. Load data","c5aff16c":"# 3. Base model: xgboost","d29ff826":"## 4.2 Additional Features Created","0f926dfa":"# 2. Data processing","0b0c8a0f":"### Thank you for your interest in this notebook!","3ced1c65":"# 4. Feature engineering\n## 4.1 Mutual Info Regression","f1bb25ba":"### Trying to create new features when the features are anonymised, is taking a stab in the dark. It is more of a hope than expectations. So I did!\n\n#### Highlights: \n* Features I combined: **cat1, cat8, cat9** (cat9 x cat1, cat8 x cat1)\n* Cat features are label encoded first\n\n||5 fold local CV|||\n|---|---|---|---|\n| Model|lr =0.1 | lr =0.01 |lr=0.005|\n| base-model | 0.843853 | 0.842680 |0.842935|\n| with new features | 0.843737 | 0.842648 |0.842927|\n|score gain|0.000116|0.000032|0.000008|\n\n### Remark:\nThe result is based on realtively coarse model. Outcome may be different for fine-tuned models. If you find something different with your models please let me know. \n\nThe score-gained due to the additional features has decreased with lowering the learning rate. Why?  \n\n         \n","792a0729":"# 0. Set-up","838a456a":"# 5. Model with the additional features"}}