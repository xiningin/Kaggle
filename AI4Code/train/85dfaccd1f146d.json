{"cell_type":{"f3c8e0e7":"code","c377b0e9":"code","fe279196":"code","2038e60f":"code","0246a2d0":"code","ba5488ec":"code","98d72f58":"code","fdfa22df":"code","c79559eb":"code","d112e3dd":"code","b0e1d63f":"code","d5da1028":"code","e419e001":"code","bc53ce28":"code","05798f98":"code","1e41307b":"code","5f9d3c4f":"code","933ec8f4":"code","184c8a11":"code","eb50f013":"code","8dd562df":"code","4db8a6e3":"code","14ce1984":"code","8c9abbc0":"code","c0723fc1":"code","2291e61c":"code","83ca4d96":"code","d357eef9":"code","99e6b6b6":"code","998f9341":"code","ad5c5c8f":"markdown","5bfac572":"markdown","786e51d3":"markdown","4940a350":"markdown","39b4c80f":"markdown","55148410":"markdown","bf2ee138":"markdown","2dddfd1c":"markdown","d8874e4b":"markdown","8fbed028":"markdown","104ab7af":"markdown","282ce96f":"markdown","65f09ba0":"markdown","9b65ce5a":"markdown","ae103370":"markdown","8205cea0":"markdown","03c31c92":"markdown"},"source":{"f3c8e0e7":"import os\n\n# Datatype \nimport numpy as np\nimport pandas as pd\n\n# Ploting\nfrom matplotlib import pyplot as plt","c377b0e9":"year = \"2019-2020\"\nratio = \"0\"\n\n# Local\n# dir_type = 0\n# ROOT_DIR = \"..\/\"\n\n# Colab + Google Drive\n# dir_type = 1\n# ROOT_DIR = \"\/content\/drive\/MyDrive\/Colab Notebooks\/Disk Failure Prediction\"\n\n# Kaggle\ndir_type = 2\nROOT_DIR = \"..\/input\/disk-smart\"\n\nif dir_type in [0, 1]:\n    DATA_DIR = os.path.join(ROOT_DIR, \"data\")\nelif dir_type in [2]:\n    DATA_DIR = os.path.join(ROOT_DIR)","fe279196":"train = pd.read_csv(os.path.join(DATA_DIR, f'test_{year}_{ratio}_1.csv'))\nprint(f\"train : {train.shape}\")\n\nvalid = pd.read_csv(os.path.join(DATA_DIR, f'valid_{year}_{ratio}_1.csv'))\nprint(f\"valid : {valid.shape}\")\n\ntest  = pd.read_csv(os.path.join(DATA_DIR, f'test_{year}_{ratio}_1.csv'))\nprint(f\"test  : {test.shape}\")","2038e60f":"def filter_feature(df: pd.DataFrame):\n    CRITICAL_STATS = [1, 5, 7, 9, 10, 184, 187, 188, 189, 190, 193, 194, 196, 197, 198, 240, 241, 242]\n    crit_cols_raw = ['smart_{}_raw'.format(i) for i in CRITICAL_STATS]\n    crit_cols_normalized = ['smart_{}_normalized'.format(i) for i in CRITICAL_STATS]\n    keep_cols = ['date', 'serial_number', 'capacity_bytes', 'failure'] + crit_cols_raw + crit_cols_normalized\n\n    df = df[keep_cols]\n    print(f\"=> Finished filtering features.\")\n    print(f\"   Dataframe has a shape of {df.shape}.\")\n    return df","0246a2d0":"train = filter_feature(train)\nvalid = filter_feature(valid)\ntest  = filter_feature(test)","ba5488ec":"def get_nan_count_percent(df, divisor=None):\n    \"\"\"\n    Calculates the number of nan values per column,both as an absolute amount and as a percentage of some pre-defined \"total\" amount\n        \n    [WARNING]\n\n    Return value is of the same type (pd.DataFrame or dask.dataframe) as the input. It is up to the caller to handle this accordingly.\n    \n    [Params]\n\n        df : {pandas.DataFrame\/dask.dataframe}\n\n            dataframe whose nan count to generate\n    \n        divisor : {int\/float} (default: {None})\n\n            The \"total\" amount for calculating percentage. \n            If value in count column is n, value in percent column will be n\/divisor.\n            If not provided, number of rows is used by default\n    \n    [Returns]\n\n        ret_df : {pandas.DataFrame\/dask.dataframe}\n        \n            Dataframe with counts and percentages of nans in each column of input df.\n            Column name is the index, \"count\" and \"percent\" are the two columns.\n    \"\"\"\n\n    # if total count is not provided, use the number of rows\n    if divisor is None:\n        '''\n        len() must be used, not .shape because in case of dask.dataframe.shape returns  delayed computation, not an actual value. But len() returns an actual value.\n        '''\n        divisor = len(df)\n\n    # Get count and convert series to dataframe\n    ret_df = df.isna().sum().to_frame(\"count\")\n\n    # add percent column\n    ret_df[\"percent\"] = ret_df[\"count\"] \/ divisor\n\n    return ret_df","98d72f58":"def deal_nan (df):\n\n    # Summerize NaNs in each column.\n    nan_count = get_nan_count_percent(df)\n\n    # ----------------------------------------------------------------------------------------------\n\n    # Remove columns with more than 97% of NaNs.\n    MAJORITY_THRESHOLD = 0.97\n    df = df.drop(\n        nan_count[nan_count['percent'] > MAJORITY_THRESHOLD].index, \n        axis=1\n        )\n        \n    # ----------------------------------------------------------------------------------------------\n\n    # Get the data points where 193 is null.\n    nan193_df = df[df['smart_193_raw'].isna()]\n    sers = nan193_df[nan193_df['failure'] == 1]['serial_number']\n\n    # Retain columns if they belong to a working drive or have non null values for 193 (and 1, 5, 7 \n    # .. by extension)\n    df = df[~df['smart_193_raw'].isna() | df['serial_number'].isin(sers)]\n\n    # ----------------------------------------------------------------------------------------------\n\n    # Use dummy value to fill up indecative features' NaNs.\n    DUMMY_VALUE = -100\n\n    cols_to_fill = [\n        'smart_240_raw',\n        'smart_240_normalized',\n        'smart_241_raw',\n        'smart_241_normalized',\n        'smart_242_raw',\n        'smart_242_normalized'\n    ]\n\n    for col in cols_to_fill:\n        df[col] = df[col].mask(df[col].isna(), DUMMY_VALUE)\n\n    # ----------------------------------------------------------------------------------------------\n\n    # Get all the serial numbers where 184 is seen to be nan at least once\n    nan184_serials = df[df['smart_184_raw'].isna()]['serial_number'].unique()\n\n    # Of these serial numbers, which ones are the ones for whom 184 is always nan?\n    isallnan184_serials = df[df['serial_number'].isin(\n        nan184_serials)][['serial_number', 'smart_184_raw']]\n    isallnan184_serials = isallnan184_serials.groupby(\n        'serial_number').apply(lambda g: g['smart_184_raw'].isna().all())\n\n\n    # Serial numbers which have all-nans for 184 and 189 and are therefore to be filled with dummy \n    # value\n    dummyfill_sers = isallnan184_serials[isallnan184_serials == True].index\n\n    # Fill columns with dummy value\n    cols_to_fill = [\n        'smart_184_raw',\n        'smart_184_normalized',\n        'smart_189_raw',\n        'smart_189_normalized'\n    ]\n    for col in cols_to_fill:\n        df[col] = df[col].mask(\n            df['serial_number'].isin(dummyfill_sers), DUMMY_VALUE)\n\n    # ----------------------------------------------------------------------------------------------\n\n    # Fill the rest of NaNs with the mean of each column.\n    data_col = [i for i in df.columns]\n    for rm in ['date', 'serial_number', 'capacity_bytes', 'failure']:\n        data_col.remove(rm)\n    \n    df = df.fillna(df[data_col].mean())\n\n    return df\n","fdfa22df":"train = deal_nan(train)\nvalid = deal_nan(valid)\ntest  = deal_nan(test)","c79559eb":"for i in [train, valid, test]:\n    nan_count = get_nan_count_percent(i)\n    print(any(nan_count['count'].values > 0))\n","d112e3dd":"# Deep Learning\nimport tensorflow as tf\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n# check keras version\nprint(keras.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","b0e1d63f":"# How many samples\/rows\/timesteps to look in the past in order to forecast the next sample\nn_input = 25\n# Number of timeseries samples in each batch\nb_size = 32\n\n# Learning rate\nlr = 0.00001\n# The amount of epochs does the network runs\nn_epoch = 1000","d5da1028":"def x_y_keras_gen(df: pd.DataFrame, n_input: int, b_size: int):\n    X = df.iloc[:, 4:]\n    Y = df[['failure']]\n\n    scaled_X = X.to_numpy()\n\n    scaled_Y = Y.to_numpy()\n    # remove the second dimention from y so the shape changes from (n,1) to (n,)\n    scaled_Y = scaled_Y.reshape(-1)\n        \n    # The following 2 lines of codes \u201cpushes\u201d the y data one step forward by adding zero in the \n    # first position and line 13 keeps the shape of y by deleting the last time step (last row).\n    # \n    # For example:\n    # y = [1,2,3,4]\n    # y = np.insert(y,0,0) --> [0,1,2,3,4]\n    # y = np.delete(y,-1) --> [0,1,2,3]\n    scaled_Y = np.insert(scaled_Y, 0, 0)\n    scaled_Y = np.delete(scaled_Y, -1)\n\n    # ----------------------------------------------------------------------------------------------\n\n    # Combine X and Y using the keras TimeseriesGenerator\n\n    # How many samples\/rows\/timesteps to look in the past in order to forecast the next sample\n    n_input = 25  \n    # How many predictors\/Xs\/features we have to predict y\n    n_features = X.shape[1]\n    # Number of timeseries samples in each batch\n    b_size = 32\n    generator = TimeseriesGenerator(scaled_X, scaled_Y, length=n_input, batch_size=b_size)\n\n    # The size should now be in the form of (batch_size, n_input, n_features).\n    print(generator[0][0].shape)\n\n    return generator","e419e001":"g_train = x_y_keras_gen(\n    df=train, \n    n_input=n_input, \n    b_size=b_size\n)","bc53ce28":"failure_count = train[train['failure'] == 1].shape[0]\nworking_count = train[train['failure'] == 0].shape[0]\n\n# tmp = failure_count * working_count \/ (2 * failure_count + 1 * working_count)\n# failure_weight = 1 * tmp \/ failure_count\n# working_weight = 2 * tmp \/ working_count\n\nfailure_weight = (working_count + failure_count) \/ failure_count\nworking_weight = (working_count + failure_count) \/ working_count","05798f98":"sample_weight = train['failure'].copy()\nsample_weight[sample_weight == 1] = failure_weight\nsample_weight[sample_weight == 0] = working_weight\nsample_weight = sample_weight.to_numpy()\nsample_weight","1e41307b":"class_weight = {\n    0: working_weight,\n    1: failure_weight\n}","5f9d3c4f":"# How many predictors\/Xs\/features we have to predict y\nn_features = g_train[0][0].shape[2]","933ec8f4":"model = Sequential()\n# model.add(\n#     LSTM(\n#         20, \n#         activation='tanh', \n#         input_shape=(n_input, n_features),\n#         return_sequences=True\n#     ))\nmodel.add(\n    LSTM(\n        20, \n        activation='tanh', \n        input_shape=(n_input, n_features),\n    ))\nmodel.add(\n    Dense(\n        1, \n        activation='sigmoid'\n    ))\nopt = keras.optimizers.Adam(learning_rate=lr)\nmodel.compile(optimizer=opt, loss='binary_crossentropy')\nmodel.summary()","184c8a11":"model.fit(\n    g_train, \n    epochs=n_epoch,\n    class_weight=class_weight\n)","eb50f013":"from datetime import datetime","8dd562df":"now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\nmodel_path = os.path.join(ROOT_DIR, \"models\", now)\nmodel.save(model_path)\nprint(f\"Model has been saved to {model_path}\")","4db8a6e3":"loss_per_epoch = model.history.history['loss']\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch);","14ce1984":"X_test = test.iloc[:, 4:]\nprint(f\"failure : {test[test['failure'] == 1].shape[0]}\")\nprint(f\"working : {test[test['failure'] == 0].shape[0]}\")","8c9abbc0":"X_test = X_test.to_numpy()\ng_test = TimeseriesGenerator(X_test, np.zeros(len(X_test)), length=n_input, batch_size=b_size)\nprint(g_test[0][0].shape)","c0723fc1":"y_pred = model.predict(g_test)","2291e61c":"y_pred[y_pred >  0.5] = 1\ny_pred[y_pred <= 0.5] = 0\ny_pred.shape","83ca4d96":"Y_test = test[['failure']]\nY_test = Y_test.to_numpy()\nY_test.shape","d357eef9":"def similar(results):\n    \n    same = results[results[\"y_true\"] == results[\"y_pred\"]]\n    diff = results[results[\"y_true\"] != results[\"y_pred\"]]\n\n    same_1 = same[same[\"y_true\"] == 1].shape[0]\n    same_0 = same[same[\"y_true\"] == 0].shape[0]\n\n    diff_1 = diff[diff[\"y_true\"] == 1].shape[0]\n    diff_0 = diff[diff[\"y_true\"] == 0].shape[0]\n\n    print(\"{0:10s}   {1:10s}   {2:10s} \".format(\"\", \"Pred_True\", \"Pred_False\"))\n    print(\"{0:10s}   {1:10s}   {2:10s} \".format(\"\", \"-\"*10, \"-\"*10))\n    print(\"{0:10s} | {1:10d} | {2:10d} |\".format(\"Real_True\", same_1, diff_1))\n    print(\"{0:10s}   {1:10s}   {2:10s} \".format(\"\", \"-\"*10, \"-\"*10))\n    print(\"{0:10s} | {1:10d} | {2:10d} |\".format(\"Real_False\", diff_0, same_0))\n\n    print(\"-\" * 38)\n\n    print(f\"ACC : {(same.shape[0] \/ results.shape[0]) * 100} %\")","99e6b6b6":"m = len(Y_test) - len(y_pred)\n\nif len(Y_test[m:, :].ravel()) == len(y_pred.ravel()):\n\n    results = pd.DataFrame(\n        {\n            'y_true': Y_test[m:, :].ravel(),\n            'y_pred': y_pred.ravel()\n        }\n    )\n    \n    similar(results)","998f9341":"results.plot(figsize=(200,10))","ad5c5c8f":"Class weight","5bfac572":"#### Fit The Model","786e51d3":"#### Pre-settings","4940a350":"## Data Preparation\n\n- Import Datasets\n- Filter Features\n- Deal With The `NaN` Values","39b4c80f":"Sample weight ( NOT fit for current nerual network setting )","55148410":"### Deal With The `NaN` Values\n\n*Details on each part of the codes can be referred from `(DL)methods.ipynb`.*","bf2ee138":"#### Construct The LSTM Model","2dddfd1c":"#### Test The Model","d8874e4b":"#### Save The Model","8fbed028":"### Import Datasets","104ab7af":"### Filter Features","282ce96f":"#### Define the weight for each sample\n\nThe weight being defined is based on the sequence of each label's showing up.","65f09ba0":"### RNN(LSTM)\n\nBased on Tensorflow Keras.","9b65ce5a":"#### Reshape Datasets to Fit Keras' Requirement","ae103370":"## Deep Learning","8205cea0":"#### Plot The Losses","03c31c92":"# (DL) Disk Failure Prediction via RNN(LSTM)\n\nThis notebook is intended to apply RNN(LSTM) method to disk failure prediction, with dataset from [BackBlaze](https:\/\/www.backblaze.com\/b2\/hard-drive-df-data.html) and reorganized by `(Data)preprocessing.ipynb`."}}