{"cell_type":{"68b03707":"code","dbc4ff57":"code","e9d34fee":"code","39cc8a88":"code","a0147a52":"code","61e400e7":"code","902ddc23":"code","4a439736":"code","7bd04b89":"code","ae0a3d70":"code","567d970d":"code","1482ae58":"code","e3882685":"code","00a82227":"code","80091d3c":"code","d30bb37c":"code","4d0d709a":"code","9ad74307":"code","02506681":"code","9e441bea":"code","b82a06e3":"code","8cf0d3e5":"code","7272e820":"code","8bbbecc6":"code","e0fbc957":"code","bdb3e50a":"code","34b13abd":"code","e5f61300":"code","a10bc4f9":"code","1eaee909":"code","bda4737b":"code","43410115":"code","309495d9":"markdown","a42d15dd":"markdown"},"source":{"68b03707":"#import\nimport librosa\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nimport IPython.display as ipd\n#Try to indentify its genre,tempo,instruments,mood,time signature,key signature,chord progression,tuning frequency,song structure","dbc4ff57":"cd ..\/input","e9d34fee":"ipd.YouTubeVideo('SEyzZw7xiPo')","39cc8a88":"garry, sr = librosa.load('LETS TALK (DO GALLAN ) _ Full Video _ GARRY SANDHU _ FRESH MEDIA RECORDS.mp3')","a0147a52":"ipd.Audio('LETS TALK (DO GALLAN ) _ Full Video _ GARRY SANDHU _ FRESH MEDIA RECORDS.mp3') # load a local WAV file","61e400e7":"librosa.get_duration(garry)","902ddc23":"yt, index = librosa.effects.trim(garry)\nprint(librosa.get_duration(garry), librosa.get_duration(yt))","4a439736":"ipd.Audio('LETS TALK (DO GALLAN ) _ Full Video _ GARRY SANDHU _ FRESH MEDIA RECORDS.mp3') # load a local WAV file","7bd04b89":"garry=yt\ngarry","ae0a3d70":"garry.shape\nsr","567d970d":"print(yt.shape,sr)#total length\/values in the array# sample rate of this music","1482ae58":"plt.figure(figsize=(25, 10))\nlibrosa.display.waveplot(yt, sr=sr)","e3882685":"n0 = 6500\nn1 = 7000\nplt.figure(figsize=(25, 10))\nplt.plot(garry[n0:n1])","00a82227":"Garry = librosa.stft(garry)#compute stft\nXdb = librosa.amplitude_to_db(abs(Garry))#computing log amplitude as the lower graph look srather boring\nplt.figure(figsize=(25, 10))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n#full spectrogram","80091d3c":"garry.shape","d30bb37c":"#let's look more closely\nGarry = librosa.stft(garry[20000:21000])#compute stft\nXdb = librosa.amplitude_to_db(abs(Garry))#computing log amplitude as the lower graph look srather boring\nplt.figure(figsize=(25, 10))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n#so it's a lot of bar graphs all in one","4d0d709a":"#Let's try to detect onsets now\nonset_frames = librosa.onset.onset_detect(garry, sr=sr)\nprint(onset_frames)","9ad74307":"onset_times = librosa.frames_to_time(onset_frames, sr=sr)\nprint(onset_times)","02506681":"onset_samples = librosa.frames_to_samples(onset_frames)\nprint(onset_samples)","9e441bea":"# Use the `length` parameter so the click track is the same length as the original signal\nclicks = librosa.clicks(times=onset_times, length=len(garry))","b82a06e3":"# Play the click track \"added to\" the original signal\nipd.Audio(garry+clicks, rate=sr)#other than for just feel using onset detection here is kind of mistake;)","8cf0d3e5":"#Segmentation show and then creating segments of 50 segments at beginning of each onset\n#concatanete also add 100 ms second into each segments then we concatenate them all into one signal\nframe_sz = int(0.1*sr)\nsegments = numpy.array([garry[i:i+frame_sz] for i in onset_samples])\ndef concatenate_segments(segments, sr=22050, pad_time=0.300):\n    padded_segments = [numpy.concatenate([segment, numpy.zeros(int(pad_time*sr))]) for segment in segments]\n    return numpy.concatenate(padded_segments)\nconcatenated_signal = concatenate_segments(segments, sr)\nipd.Audio(concatenated_signal, rate=sr)","7272e820":"#let's calculate zero crossing rate for that segments\n#zero crossing rate indicates the number of times that signal crosses the horizontal axis\nzcrs = [sum(librosa.core.zero_crossings(segment)) for segment in segments]\nprint(zcrs)","8bbbecc6":"plt.figure(figsize=(14, 5))\nplt.plot(zcrs)","e0fbc957":"ind = numpy.argsort(zcrs)\nprint(ind)","bdb3e50a":"concatenated_signal = concatenate_segments(segments[ind], sr)","34b13abd":"ipd.Audio(concatenated_signal, rate=sr)","e5f61300":"#finding MFCC\ngarry = librosa.feature.mfcc(yt, sr=sr)\nprint(garry.shape)\n#in this case,mff computed 20 MFCCS over 11343 frames","a10bc4f9":"\/plt.figure(figsize=(25, 10))\nlibrosa.display.specshow(garry, sr=sr, x_axis='time')","1eaee909":"import numpy, scipy, matplotlib.pyplot as plt, sklearn, librosa, urllib, IPython.display\nimport essentia, essentia.standard as ess","bda4737b":"garry = sklearn.preprocessing.scale(garry, axis=1)\nprint(garry.mean(axis=1))\nprint(garry.var(axis=1))","43410115":"plt.figure(figsize=(25, 10))\nlibrosa.display.specshow(garry, sr=sr, x_axis='time')","309495d9":"sampling of tasks found in music information retrieval:\n- fingerprinting\n- cover song detection\n- genre recognition\n- transcription\n- recommendation\n- symbolic melodic similarity\n- mood\n- source separation\n- instrument recognition\n- pitch tracking\n- tempo estimation\n- score alignment\n- song structure\/form\n- beat tracking\n- key detection\n- query by humming","a42d15dd":"The very first MFCC, the 0th coefficient, does not convey information relevant to the overall shape of the spectrum. It only conveys a constant offset, i.e. adding a constant value to the entire spectrum. Therefore, many practitioners will discard the first MFCC when performing classification. For now, we will use the MFCCs as is."}}