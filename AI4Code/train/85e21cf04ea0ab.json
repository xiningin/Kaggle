{"cell_type":{"39e11a6d":"code","54ac17d9":"code","fb4a516e":"code","c41df88b":"code","74200587":"code","e8891e36":"code","fd7df971":"code","0024601c":"code","112e9f5f":"code","3f65b6fd":"code","ca50a65d":"code","cf213647":"code","5274ea30":"code","60e6db64":"code","7886bc88":"code","36fb9150":"code","76214cbd":"markdown","62ff7386":"markdown"},"source":{"39e11a6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","54ac17d9":"df = pd.read_csv(\"..\/input\/mushrooms.csv\")","fb4a516e":"df.head()","c41df88b":"abt = pd.get_dummies(df)","74200587":"abt.head()","e8891e36":"#Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n#Building everything\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n#Evaluation\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n#Saving the model\nimport pickle","fd7df971":"df[\"class\"].replace([\"e\", \"p\"], [1, 0], inplace= True)","0024601c":"# Create separate object for target variable\ny = df[\"class\"]\n# Create separate object for input features\nX = abt.drop([\"class_e\", \"class_p\"], axis= 1).astype(float)","112e9f5f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 1234)","3f65b6fd":"print(len(X_train), len(X_test), len(y_train), len(y_test))","ca50a65d":"pipelines = {\n    \"rf\": make_pipeline(StandardScaler(), RandomForestClassifier(random_state=123)),\n    \"gb\": make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n}","cf213647":"rf_hyperparameters = {\n    \"randomforestclassifier__n_estimators\": [100, 200],\n    \"randomforestclassifier__max_features\": [\"auto\", \"sqrt\", 0.33]\n}\ngb_hyperparameters = {\n    \"gradientboostingclassifier__n_estimators\": [100, 200],\n    'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n    'gradientboostingclassifier__max_depth': [1, 3, 5]\n}\nhyperparameters = {\"rf\": rf_hyperparameters,\n                   \"gb\": gb_hyperparameters}","5274ea30":"# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv= 10, n_jobs= -1)\n    \n    # Fit model on X_train, y_train\n    model.fit(X_train, y_train)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, 'has been fitted.')","60e6db64":"for name, model in fitted_models.items():\n    print(name, model.best_score_)","7886bc88":"for name, model in fitted_models.items():\n    print(name)\n    print(\"-----------\")\n    pred = model.predict(X_test)\n    print('Acc:', accuracy_score(y_test, pred))\n    print(\"cm:\\n\", confusion_matrix(y_test, pred))","36fb9150":"with open('final_model.pkl', 'wb') as f:\n    pickle.dump(fitted_models['rf'].best_estimator_, f)","76214cbd":"### Finished!","62ff7386":"#  Random Forests and Gradient Boosting on Mushroom Classification\n\nThis is my first kernel on kaggle. This kernel is designed to classify mushrooms as edible or non edible based on the various features given.\nI have used classification models:\n* Random Forest Classifier\n* Gradient Boosted Classifier\n\nThis dataset has only categorical features and as I don't have any domain knowledge about mushrooms, I've skipped feature engineering for now."}}