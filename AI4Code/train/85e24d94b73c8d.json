{"cell_type":{"41578e11":"code","bce2df5c":"code","acad9936":"code","2f470521":"code","5b3fb9c5":"code","7c042162":"code","f7d5b507":"code","08bda33a":"code","63c20191":"code","d7646fac":"code","955d4514":"code","77b9394f":"code","ec5cb946":"code","e8cdf4f3":"code","1158a4f1":"code","8ee028a0":"code","73b73ae7":"code","6738a784":"code","05fdd6c6":"code","2141a7ef":"code","dfc7f139":"code","7db96a89":"code","5ae88f40":"code","860f0fc6":"code","e9043240":"code","90f03b43":"code","bfbaafaa":"code","dfad1e9c":"code","eac12ce9":"code","c931dedf":"code","65f1f75c":"code","59ddf8e2":"code","f2e2d8d5":"code","2ed9b34c":"code","a39e443b":"code","eb32d63f":"code","23fdfe92":"code","12f56781":"code","29a2150a":"code","66196b00":"code","5e1262ae":"code","70141f93":"code","22ab6793":"code","8371c59c":"code","4aea91ea":"code","ac838aff":"code","4d529a98":"code","c79bd9ff":"code","8772c867":"code","2aeb229c":"code","c3f94f59":"code","79845139":"code","166b2fd3":"code","b911283b":"code","18746580":"code","faa100a3":"code","e4742917":"code","58e91fd6":"code","e7c7a545":"code","e416b06a":"code","d2f6b4d2":"code","78142878":"code","0dda0f92":"code","2590498d":"code","970337ca":"code","22766830":"code","705ab882":"code","4cd250bf":"code","98097b49":"code","2a2fbbdc":"code","56594747":"code","c94e54e4":"code","f639f540":"code","7aa3cf0a":"code","e288e599":"code","398ab083":"code","5f6e8ef3":"code","3607b67c":"code","41519855":"code","ca5b4d6c":"code","e7b3106b":"code","a1e2238a":"code","069e1e09":"code","cef461f7":"code","ff77a4ae":"code","8782cc65":"code","344a80a3":"code","6521e3a5":"code","7102a08c":"code","8f5803c1":"code","eb641338":"code","d605019e":"code","8de857e1":"code","49c20c32":"code","a6de4068":"code","16fe3f49":"code","1dff36eb":"code","fdb6b1dd":"markdown","abb8a515":"markdown","bbd5ccf9":"markdown","b3b82753":"markdown","36c17cf9":"markdown","8ed0d482":"markdown","d7548643":"markdown","d23900d1":"markdown","e3795c52":"markdown","b0302d9a":"markdown","0eb68ef3":"markdown","d0821826":"markdown","582d49d9":"markdown","f87be38a":"markdown","630dae09":"markdown","5795c8b7":"markdown","a3d84a5e":"markdown","d355a812":"markdown","75e6593d":"markdown","8971ef70":"markdown","cc164841":"markdown","90e0f5e3":"markdown","1531820d":"markdown","d111fc49":"markdown","ab086d09":"markdown","cec8dd16":"markdown","1664b909":"markdown","80091315":"markdown","eff661a6":"markdown","3a2a8248":"markdown","76f05781":"markdown","93a792c3":"markdown","dc9f8912":"markdown","361f22e0":"markdown","293768ed":"markdown","372e9803":"markdown","48fea86e":"markdown","edb1f129":"markdown","c2bd52ed":"markdown","409c8624":"markdown","2fbda772":"markdown","2021bba4":"markdown","9f9db325":"markdown","36b4327c":"markdown","5bb9d1aa":"markdown","f16ae763":"markdown","f8277110":"markdown","12e1396e":"markdown","471d5262":"markdown","5b4fe186":"markdown","90da3e07":"markdown","0e7fc609":"markdown","4ac2b634":"markdown","7f602c22":"markdown","02a2915c":"markdown","1bff2693":"markdown","1ae1f28e":"markdown","0a7c6d3e":"markdown","5a46cee0":"markdown","52767ba5":"markdown","ca663cfb":"markdown","4f16c561":"markdown","4ae4a726":"markdown","aed43622":"markdown","13636a37":"markdown","57d5c1f3":"markdown","312500f2":"markdown","04cafc9b":"markdown","8dda6b00":"markdown","9ef48b6c":"markdown","fe415a4d":"markdown","ca2b2988":"markdown","c935b1db":"markdown","5f3d85f3":"markdown","ff72701d":"markdown","5b30186f":"markdown","41c68ef2":"markdown","5b8d0ede":"markdown","9a9434d2":"markdown","f2b53f42":"markdown","18a88f2c":"markdown","8f46a417":"markdown","63e71e40":"markdown","9009769b":"markdown","3a566f79":"markdown","4d31458c":"markdown","3b5b38b6":"markdown","15b20091":"markdown","f88da8fc":"markdown","a970d2aa":"markdown","25f8281a":"markdown","5cac3ddf":"markdown","a718aa83":"markdown","364c22d2":"markdown","9c8fb3aa":"markdown","26de5eb4":"markdown","2d1826ac":"markdown","62ab72c3":"markdown","3f67bf7c":"markdown","0e7a0904":"markdown","4a3b5da9":"markdown","b3e126a5":"markdown","197c9c77":"markdown","90e3b765":"markdown","d52b4851":"markdown","b2f2de3c":"markdown","8acd1ae5":"markdown","efd197c7":"markdown","5fd91091":"markdown","a95b631f":"markdown","48ea5525":"markdown","8a188635":"markdown","8bb3e2f8":"markdown","a47af13c":"markdown","2c44ef3e":"markdown","06935e15":"markdown","909b2275":"markdown","be839d74":"markdown","096f1b05":"markdown","ef691b93":"markdown","261204a8":"markdown","83503328":"markdown","730f494e":"markdown","80d04bd7":"markdown","86b47410":"markdown","b17471e2":"markdown","b9c20c68":"markdown","4cdfb0cc":"markdown","bfb0c724":"markdown","5357213b":"markdown","d377a3e5":"markdown","527b10ef":"markdown","894d4cf4":"markdown","be8f3508":"markdown","5c0b8cdc":"markdown","fb78c372":"markdown","a5f40274":"markdown","3d25620d":"markdown","46f9ee38":"markdown"},"source":{"41578e11":"# import comet_ml in the top of your file\n# from comet_ml import Experiment\n\n#Inspecting\nimport numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nfrom pprint import pprint\nfrom collections import Counter\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob\nfrom spacy import displacy\nimport gensim\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\nfrom wordcloud import WordCloud\nfrom PIL import Image\nimport collections\nfrom matplotlib import style\nplt.rcParams.update({'font.size': 18})\nplt.rcParams.update({'figure.figsize': [16, 12]})\nplt.style.use('seaborn-whitegrid')\nsns.set_style('dark')\n\n#Warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\n\n# Balance data\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\n\n#Cleaning\nimport nltk\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\n\n#Modeling\nfrom sklearn.pipeline import Pipeline\nfrom gensim.models import Word2Vec\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV,train_test_split, RandomizedSearchCV\n\n\n#metrics for analysis\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score, f1_score\n\n# Import Pickle for streamlit Application\nimport pickle\n","bce2df5c":"# Add the following code anywhere in your machine learning file\n# experiment = Experiment(api_key=\"ZO3kD6D1uVgIr9CHdhUPQaU3B\",\n#                         project_name=\"climate-change-belief-analysis\", workspace=\"helloaggregator\")","acad9936":"train = pd.read_csv('..\/input\/dataset\/train.csv')\ntest = pd.read_csv('..\/input\/dataset\/test.csv')","2f470521":"plt.title('Number of Characters Present in Tweet')\ntrain['message'].str.len().hist()","5b3fb9c5":"train['message'].str.split().\\\napply(lambda x : [len(i) for i in x]). \\\nmap(lambda x: np.mean(x)).hist()","7c042162":"# Fetch stopwords so it doesn't take away from Ngram analysis\nstop = set(stopwords.words('english'))","f7d5b507":"# Create corpus\ncorpus=[]\nnew= train['message'].str.split()\nnew=new.values.tolist()\ncorpus=[word for i in new for word in i]\n\nfrom collections import defaultdict\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1","08bda33a":"counter=Counter(corpus)\nmost=counter.most_common()\n\nx, y = [], []\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)\nplt.title('Most Common Words')\nplt.show()","63c20191":"def get_top_ngram(corpus, n=None):\n    \n    '''\n    Takes a list of words and groups then in terms of ngrams depending on how many words you want to group, returns \n    a word count based on the number of times ngram appears\n    \n    Parameters\n    -----------\n    corpus: list\n            input list of strings\n    n: int\n       input the number of ngrams needed\n       \n    Output\n    ----------\n    Output: Returns a tuple list with specified number of words grouped and counts the frequency\n    \n    '''\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]\n\ntop_n_bigrams = get_top_ngram(train['message'],2)[:40]\nx,y=map(list,zip(*top_n_bigrams))\nsns.barplot(x = y,y = x)\nplt.title('Bigram analysis')\nplt.show()","d7646fac":"top_tri_grams=get_top_ngram(train['message'],n=3)\nx,y=map(list,zip(*top_tri_grams))\nsns.barplot(x=y,y=x)\nplt.title('Trigram Analysis')\nplt.show()","955d4514":"def preprocess_train(df):\n    '''\n    Creates a list of lemmetized words that must have a length greater than 2 from an input of a dataframe\n    \n    Parameters\n    -----------\n    df: Dataframe\n        Input needs to be dataframe\n        \n    Output\n    -----------\n    corpus: Returns a list of lemmatized words\n    \n    '''\n    corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for train in df['message']:\n        words = [w for w in word_tokenize(train) if (w not in stop)]\n        \n        words = [lem.lemmatize(w) for w in words if len(w)>2]\n        \n        corpus.append(words)\n    return corpus","77b9394f":"#Create corpus\ncorpus = preprocess_train(train)\n\n# Create tuple vectorised words\ndic=gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]\n\n# creat a weight for topics of vectorized words\nlda_model =  gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 10, \n                                   id2word = dic,                                    \n                                   passes = 10,\n                                   workers = 2)","ec5cb946":"#visual the the top ten topics\nstyle.use('dark_background')\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\nvis","e8cdf4f3":"def ner(text):\n    '''\n    Takes in  text and returns entity label of text using the natural language processor on python\n    \n    Parameters\n    ----------\n    text: String\n          input a string\n    ent: String\n         Input a string ant it will return the entity you desire\n    \n    Output\n    ---------\n    output: Entity labelled string\n            Returns a label depending on the context of string\n    \n    '''\n    doc=nlp(text)\n    return [X.label_ for X in doc.ents]","1158a4f1":"#create labels for all the tweets\nent=train['message'].apply(lambda x : ner(x))\nent=[x for sub in ent for x in sub]\ncounter=Counter(ent)\ncount=counter.most_common()\n\n#Plot the labels that occurred the most from the tweets\nx,y=map(list,zip(*count))\nsns.barplot(x=y,y=x)\nplt.title('The most Occurring Entities')\nplt.show()","8ee028a0":"def ner(text,ent=\"GPE\"):\n    doc=nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]","73b73ae7":"gpe = train['message'].apply(lambda x: ner(x,\"GPE\"))\ngpe = [i for x in gpe for i in x]\ncounter = Counter(gpe)\n\nx,y = map(list,zip(*counter.most_common(40)))\nsns.barplot(y,x)\nplt.title('Most Common GPE')\nplt.show()","6738a784":"per = train['message'].apply(lambda x: ner(x,\"PERSON\"))\nper = [i for x in per for i in x]\ncounter = Counter(per)\n\nx,y = map(list,zip(*counter.most_common(40)))\nsns.barplot(y,x)\nplt.title('The most Common Person')\nplt.show()","05fdd6c6":"gpe = train['message'].apply(lambda x: ner(x,\"ORG\"))\ngpe = [i for x in gpe for i in x]\ncounter = Counter(gpe)\n\nx,y = map(list,zip(*counter.most_common(40)))\nsns.barplot(y,x)\nplt.title('The most Common Organisations')\nplt.show()","2141a7ef":"test.head()","dfc7f139":"train.head()","7db96a89":"sns.factorplot('sentiment',data = train, kind='count',size=6,aspect = 1.5, palette = 'PuBuGn_d') \nplt.suptitle(\"Climate Sentiment Bar Graph\",y=1)\nplt.show()","5ae88f40":"# total number of negative ,neutral, positive and news posts.\nclimate_sentiment  = train['sentiment'].value_counts()\n\n# pie plot for total percentage of climate change sentiment\nplt.figure(figsize=(5,5))\nlabels = 'Positive','News','Neutral','Negative'\nsizes = climate_sentiment.tolist()\ncolors = ['green', 'purple', 'blue','red']\nexplode = (0, 0, 0,0) \n\n# Plot\nplt.suptitle(\"Climate Sentiment Pie Chart\",y=1)\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()\n","860f0fc6":"#Analyse the text tweets for cleaning\nfor index,text in enumerate(train['message'][35:]):\n  print('Tweet %d:\\n'%(index+1),text)","e9043240":"# number of words in a tweet\ndf_eda = pd.DataFrame()\ndf_eda['count_words'] = train['message'].apply(lambda x: len(re.findall(r'\\w+',x)))\n\n# referrals to other twiiter accounts\ndf_eda['count_mentions'] = train['message'].apply(lambda x: len(re.findall(r'@\\w+',x)))\n\n# number of hashtags \ndf_eda['count_hashtags'] = train['message'].apply(lambda x: len(re.findall(r'#\\w+',x)))\n\n# Number of upper case words 3 or more to ignore RT\ndf_eda['count_capital_words'] = train['message'].apply(lambda x: len(re.findall(r'\\b[A-Z]{3,}\\b',x)))\n\n#count number of exclamation marks and questions marks \ndf_eda['count_exl_quest'] = train['message'].apply(lambda x: len(re.findall(r'!|\\?',x)))\n\n#count number of urls\ndf_eda['count_urls'] = train['message'].apply(lambda x: len(re.findall(r'http.?:\/\/[^\\s]+[\\s]?',x)))\n\n#count the number of emojis\ndf_eda['count_emojis'] = train['message'].apply(lambda x: emoji.demojize(x)).apply(lambda x: len(re.findall(r':[a-z_&]+:',x)))\n\n#add the dependent varaible for further analysis\ndf_eda['sentiment'] = train.sentiment\ndf_eda.head()","90f03b43":"# Comment\ncolumn_names = [col for col in df_eda.columns if col != 'sentiment']\nfor i in column_names:\n    bins = np.arange(df_eda[i].min(),df_eda[i].max()+1)\n    g = sns.FacetGrid(data=df_eda,col='sentiment',size=5, hue = 'sentiment',palette=\"PuBuGn_d\")\n    g = g.map(sns.distplot, i, kde= False, norm_hist = True,bins = bins)\n    plt.show()","bfbaafaa":"#Check for duplicates\nduplicate_rows_train = train['message'].duplicated().sum()\nduplicate_rows_test = test['message'].duplicated().sum()\nprint('There are ',duplicate_rows_train,' duplicated rows for the training set')\nprint('There are ',duplicate_rows_test,' duplicated rows for the test set')","dfad1e9c":"# Drop duplicate rows\/retweets\ntrain = train.drop_duplicates(subset='message', keep='first',)\ntrain = train.reset_index()\ntrain.drop('index',inplace=True,axis =1)\ntrain.head()","eac12ce9":"# Cleaning the data \ndef data_preprocessing(train,test):\n    '''\n    Cleaning the data based on analysis which includes removing capilised letters, changing contractions like don't to do not,\n    replace urls, replace emjicons, remove digits and lastly remove any funny characters in tweets\n    \n    Parameters\n    ----------\n    train: data frame\n          The data frame of training set\n    test: data frame\n          The data frame of test set\n          \n    Output\n    ---------\n    train: Adds column of tidy tweets to train dataframe\n    test: Adds column of tidy tweets to test dataframe\n    '''\n    def remove_capital_words(df,column):\n        df_Lower = df[column].map(lambda x: x.lower())\n        return df_Lower\n    train['tidy_tweet'] = remove_capital_words(train,'message')\n    test['tidy_tweet'] = remove_capital_words(test,'message')\n    contra_map = {\n                    \"ain't\": \"am not \",\n                    \"aren't\": \"are not \",\n                    \"can't\": \"cannot\",\n                    \"can't've\": \"cannot have\",\n                    \"'cause\": \"because\",\n                    \"could've\": \"could have\",\n                    \"couldn't\": \"could not\",\n                    \"couldn't've\": \"could not have\",\n                    \"didn't\": \"did not\",\n                    \"doesn't\": \"does not\",\n                    \"don't\": \"do not\",\n                    \"hadn't\": \"had not\",\n                    \"hadn't've\": \"had not have\",\n                    \"hasn't\": \"has not\",\n                    \"haven't\": \"have not\",\n                    \"he'd\": \"he would\",\n                    \"he'd've\": \"he would have\",\n                    \"he'll\": \"he will\",\n                    \"he'll've\": \"he will have\",\n                    \"he's\": \"he is\",\n                    \"how'd\": \"how did\",\n                    \"how'd'y\": \"how do you\",\n                    \"how'll\": \"how will\",\n                    \"how's\": \"how is\",\n                    \"i'd\": \"I would\",\n                    \"i'd've\": \"I would have\",\n                    \"i'll\": \"I will\",\n                    \"i'll've\": \"I will have\",\n                    \"i'm\": \"I am\",\n                    \"i've\": \"I have\",\n                    \"isn't\": \"is not\",\n                    \"it'd\": \"it would\",\n                    \"it'd've\": \"it would have\",\n                    \"it'll\": \"it will\",\n                    \"it'll've\": \"it will have\",\n                    \"it's\": \"it is\",\n                    \"let's\": \"let us\",\n                    \"ma'am\": \"madam\",\n                    \"mayn't\": \"may not\",\n                    \"might've\": \"might have\",\n                    \"mightn't\": \"might not\",\n                    \"mightn't've\": \"might not have\",\n                    \"must've\": \"must have\",\n                    \"mustn't\": \"must not\",\n                    \"mustn't've\": \"must not have\",\n                    \"needn't\": \"need not\",\n                    \"needn't've\": \"need not have\",\n                    \"o'clock\": \"of the clock\",\n                    \"oughtn't\": \"ought not\",\n                    \"oughtn't've\": \"ought not have\",\n                    \"shan't\": \"shall not\",\n                    \"sha'n't\": \"shall not\",\n                    \"shan't've\": \"shall not have\",\n                    \"she'd\": \"she would\",\n                    \"she'd've\": \"she would have\",\n                    \"she'll\": \"she will\",\n                    \"she'll've\": \"she will have\",\n                    \"she's\": \"she is\",\n                    \"should've\": \"should have\",\n                    \"shouldn't\": \"should not\",\n                    \"shouldn't've\": \"should not have\",\n                    \"so've\": \"so have\",\n                    \"so's\": \"so is\",\n                    \"that'd\": \"that would\",\n                    \"that'd've\": \"that would have\",\n                    \"that's\": \"that is\",\n                    \"there'd\": \"there would\",\n                    \"there'd've\": \"there would have\",\n                    \"there's\": \"there is\",\n                    \"they'd\": \"they would\",\n                    \"they'd've\": \"they would have\",\n                    \"they'll\": \"they will\",\n                    \"they'll've\": \"they will have\",\n                    \"they're\": \"they are\",\n                    \"they've\": \"they have\",\n                    \"to've\": \"to have\",\n                    \"wasn't\": \"was not\",\n                    \"we'd\": \"we would\",\n                    \"we'd've\": \"we would have\",\n                    \"we'll\": \"we will\",\n                    \"we'll've\": \"we will have\",\n                    \"we're\": \"we are\",\n                    \"we've\": \"we have\",\n                    \"weren't\": \"were not\",\n                    \"what'll\": \"what will\",\n                    \"what'll've\": \"what will have\",\n                    \"what're\": \"what are\",\n                    \"what's\": \"what is\",\n                    \"what've\": \"what have\",\n                    \"when's\": \"when is\",\n                    \"when've\": \"when have\",\n                    \"where'd\": \"where did\",\n                    \"where's\": \"where is\",\n                    \"where've\": \"where have\",\n                    \"who'll\": \"who will\",\n                    \"who'll've\": \"who will have\",\n                    \"who's\": \"who is\",\n                    \"who've\": \"who have\",\n                    \"why's\": \"why is\",\n                    \"why've\": \"why have\",\n                    \"will've\": \"will have\",\n                    \"won't\": \"will not\",\n                    \"won't've\": \"will not have\",\n                    \"would've\": \"would have\",\n                    \"wouldn't\": \"would not\",\n                    \"wouldn't've\": \"would not have\",\n                    \"y'all\": \"you all\",\n                    \"y'all'd\": \"you all would\",\n                    \"y'all'd've\": \"you all would have\",\n                    \"y'all're\": \"you all are\",\n                    \"y'all've\": \"you all have\",\n                    \"you'd\": \"you would\",\n                    \"you'd've\": \"you would have\",\n                    \"you'll\": \"you will\",\n                    \"you'll've\": \"you will have\",\n                    \"you're\": \"you are\",\n                    \"you've\": \"you have\"}\n    contractions_re = re.compile('(%s)' % '|'.join(contra_map.keys()))\n    def contradictions(s, contractions_dict=contra_map):\n        def replace(match):\n            return contractions_dict[match.group(0)]\n        return contractions_re.sub(replace, s)\n    train['tidy_tweet']=train['tidy_tweet'].apply(lambda x:contradictions(x))\n    test['tidy_tweet']=test['tidy_tweet'].apply(lambda x:contradictions(x))\n    def replace_url(df,column):\n        df_url = df[column].str.replace(r'http.?:\/\/[^\\s]+[\\s]?', 'urlweb ')\n        return df_url\n    train['tidy_tweet'] = replace_url(train,'tidy_tweet')\n    test['tidy_tweet'] = replace_url(test,'tidy_tweet')\n    def replace_emoji(df,column):\n        df_emoji = df[column].apply(lambda x: emoji.demojize(x)).apply(lambda x: re.sub(r':[a-z_&]+:','emoji ',x))\n        return df_emoji\n    train['tidy_tweet'] = replace_emoji(train,'tidy_tweet')\n    test['tidy_tweet'] = replace_emoji(test,'tidy_tweet')\n    def remove_digits(df,column):\n        df_digits = df[column].apply(lambda x: re.sub(r'\\d','',x))\n        return df_digits\n    train['tidy_tweet'] = remove_digits(train,'tidy_tweet')\n    test['tidy_tweet'] = remove_digits(test,'tidy_tweet')\t\n    def remove_patterns(df,column):\n        df_char = df[column].apply(lambda x:  re.sub(r'[^a-z# ]', '', x))\n        return df_char\n    train['tidy_tweet'] = remove_patterns(train,'tidy_tweet')\n    test['tidy_tweet'] = remove_patterns(test,'tidy_tweet')   \n    return train,test\n(train,test) = data_preprocessing(train,test)","c931dedf":"#Analyse the cleaned tweets\nfor index,text in enumerate(train['tidy_tweet'][35:]):\n  print('Tweet %d:\\n'%(index+1),text)","65f1f75c":"# Get Tokens of clean tweets\ntrain['token'] = train['tidy_tweet'].apply(lambda x: x.split())\ntest['token'] = test['tidy_tweet'].apply(lambda x: x.split())","59ddf8e2":"train['token'].head()","f2e2d8d5":"# use stemming process on clean tweets\nstemmer = PorterStemmer()\n\ntrain['stemming'] = train['token'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntest['stemming'] = test['token'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntrain['stemming'].head()","2ed9b34c":"#create list of all cleaned text appearing in tweets\nstemma_list_all = []\nfor index, rows in train.iterrows():\n    stemma_list_all.append(rows['stemming'])\nflatlist_all = [item for sublist in stemma_list_all for item in sublist]\nflatlist_all","a39e443b":"#Count the number of words apppearing in all the tweets\nfrequency_dist = FreqDist(flatlist_all)\nfreq_dist = dict(frequency_dist)\nsorted(freq_dist.items(), key= lambda x:-x[1])\n\n#Make Data frame \ndf_all = pd.DataFrame(freq_dist.items(),columns = ['Word','Occurrence'])\n# Sort values \ndf_all = df_all.sort_values('Occurrence', ascending=False)","eb32d63f":"fig, ax = plt.subplots(figsize=(20, 20))\n\n# Plot horizontal bar graph\ndf_all.iloc[:60].sort_values(by='Occurrence').plot.barh(x='Word',\n                      y='Occurrence',\n                      ax=ax,\n                      color=\"deepskyblue\")\n\nax.set_title(\"Plot 4: Common Words Found in all Tweets\")\n\nplt.show()","23fdfe92":"#check for stopwords in train\nstop = stopwords.words('english')\ntrain['stopwords'] = train['stemming'].apply(lambda x: len([i for i in x if i in stop]))\ntrain[['stemming','stopwords']].head()","12f56781":"#check for stopwords in test\nstop = stopwords.words('english')\ntest['stopwords'] = test['stemming'].apply(lambda x: len([i for i in x if i in stop]))\ntest[['stemming','stopwords']].head()","29a2150a":"#create my own stop words from analysis and comparing with general stopwords\nstopwords_own =[ 'i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him',\n                'his','himself','she','her','hers','herself','it','itself','they','them','their','theirs','themselves','what',\n                'which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has',\n                'had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while',\n                'of','at','by','for','with','about','against','between','into','through','during','before','after','above',\n                'below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here',\n                'there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','only',\n                'own','same','so','than','too','very','s','t','can','will','just','should','now','d','ll','m','o','re','ve','y',\n               #my own stopwords found from analysis\n                'u','doe','going','ha','wa','l', 'thi','becaus','rt']","66196b00":"# def remove_strop_words(df,column):\ndef remove_stopwords(df,column):\n    '''\n    Removing the stop words from the clean tweets\n    \n    Parameters\n    ----------\n    df: data frame\n        Input a dataframe\n    column: String\n        name of column from data frame\n        \n    Output\n    ----------\n    output: df\n            Returns a dataframe with no stopwords\n    \n    '''\n    df_stopwords = df[column].apply(lambda x: [item for item in x if item not in stopwords_own])\n    return df_stopwords\ntrain['stem_no_stopwords'] = remove_stopwords(train,'stemming')\ntest['stem_no_stopwords'] = remove_stopwords(test,'stemming')\ntrain['stem_no_stopwords'].head()","5e1262ae":"def convert_st_str(df,column):\n    '''\n    Changes list of strings into one string per row in dataframe\n    \n    Parameters\n    -----------\n    df: data frame\n        Takes  in a dataframe\n        \n    Output\n    -----------\n    output: df_str\n            Returns a dataframe with a string instead of list for each row \n    '''\n    df_str = df[column].apply(lambda x: ' '.join(x))\n    return df_str\ntrain['clean_tweet'] = convert_st_str(train,'stem_no_stopwords')\ntest['clean_tweet'] = convert_st_str(test,'stem_no_stopwords')\ntrain['clean_tweet'].head()","70141f93":"#Create WordCloud Plot\nnews_words =' '.join([text for text in train['clean_tweet'][train['sentiment'] == 2]])\nwordcloud = WordCloud(width=2000, height=1500, random_state=21, max_font_size=200, background_color='white').generate(news_words)\nprint(wordcloud)\nplt.figure(figsize=(12, 12))\nplt.title(\"Word Cloud for News Sentiment\")\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","22ab6793":"#Create WordCloud Plot\npro_words =' '.join([text for text in train['clean_tweet'][train['sentiment'] == 1]])\nwordcloud = WordCloud(width=2000, height=1500, random_state=21, max_font_size=200,background_color='white').generate(pro_words)\nplt.figure(figsize=(12, 12))\nplt.title(\"Word Cloud for postive Sentiment\")\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","8371c59c":"#Create WordCloud Plot\nneutral_words =' '.join([text for text in train['clean_tweet'][train['sentiment'] == 0]])\nwordcloud = WordCloud(width=2000, height=1500, random_state=21, max_font_size=200, background_color='white').generate(neutral_words)\nplt.figure(figsize=(12, 12))\nplt.title(\"Word Cloud for neutral Sentiment\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","4aea91ea":"#Create WordCloud Plot\nanti_words =' '.join([text for text in train['clean_tweet'][train['sentiment'] == -1]])\nwordcloud = WordCloud(width=2000, height=1500, random_state=21, max_font_size=200, background_color='white').generate(anti_words)\nplt.figure(figsize=(12, 12))\nplt.title(\"Word Cloud for negative Sentiment\")\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","ac838aff":"df_news = train[train.sentiment == 2]\ntop_bi_grams_news=get_top_ngram(df_news['clean_tweet'],n=2)\nx,y=map(list,zip(*top_bi_grams_news))\nsns.barplot(x=y,y=x).set(title = 'Common Words Found in Tweets of 2 sentiment')","4d529a98":"df_news = train[train.sentiment == 1]\ntop_bi_grams_pos=get_top_ngram(df_news['clean_tweet'],n=2)\nx,y=map(list,zip(*top_bi_grams_pos))\nsns.barplot(x=y,y=x).set(title = 'Common Words Found in Tweets of 1 sentiment')","c79bd9ff":"df_news = train[train.sentiment == 0]\ntop_bi_grams_neutr=get_top_ngram(df_news['clean_tweet'],n=2)\nx,y=map(list,zip(*top_bi_grams_neutr))\nsns.barplot(x=y,y=x).set(title = 'Common Words Found in Tweets of 0 sentiment')","8772c867":"df_news = train[train.sentiment == -1]\ntop_bi_grams_neg = get_top_ngram(df_news['clean_tweet'],n=2)\nx,y=map(list,zip(*top_bi_grams_neg))\nsns.barplot(x=y,y=x).set(title = 'Common Words Found in Tweets of -1 sentiment')","2aeb229c":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","c3f94f59":"# extracting hashtags from  tweets\n\nHT_neutral = hashtag_extract(train['clean_tweet'][train['sentiment'] == 0])\n\n\nHT_pro = hashtag_extract(train['clean_tweet'][train['sentiment'] == 1])\n\nHT_news = hashtag_extract(train['clean_tweet'][train['sentiment'] == 2])\n\n\nHT_anti = hashtag_extract(train['clean_tweet'][train['sentiment'] == -1])\n# unnesting list\nHT_neutral = sum(HT_neutral,[])\nHT_pro = sum(HT_pro,[])\nHT_news = sum(HT_news,[])\nHT_anti = sum(HT_anti,[])","79845139":"a = nltk.FreqDist(HT_news)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 5 most frequent hashtags  \nd = d.sort_values(by = 'Count',ascending = False)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d[0:5], x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.title(\"Hashtag plot for news (2) Sentiment\")\nplt.show()","166b2fd3":"a = nltk.FreqDist(HT_pro)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 5 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 5) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.title(\"Hashtag plot for positive (1) Sentiment\")\nplt.show()","b911283b":"a = nltk.FreqDist(HT_neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 5 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 5) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.title(\"Hashtag plot for neutral (0) Sentiment\")\nplt.show()","18746580":"a = nltk.FreqDist(HT_anti)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 5 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 5) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.title(\"Hashtag plot for negative (-1) Sentiment\")\nplt.show()","faa100a3":"#Create Count Vector\ncv = CountVectorizer(max_df = 0.90,min_df = 2, max_features = 1000)\nbow = cv.fit_transform(train['clean_tweet'])","e4742917":"#Plot Count Vector\nword_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\nword_counter = collections.Counter(word_freq)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\nfig, ax = plt.subplots(figsize=(20, 7))\nsns.barplot(x =\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.title(\"Count Vectorizer plot\")\nplt.show();","58e91fd6":"#Create TF -IDF\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000)\ntfidf = tfidf_vectorizer.fit_transform(train['clean_tweet'])","e7c7a545":"#Plot TF-IDF\nword_freq = dict(zip(cv.get_feature_names(), np.asarray(tfidf.sum(axis=0)).ravel()))\nword_counter = collections.Counter(word_freq)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\nfig, ax = plt.subplots(figsize=(20, 7))\nsns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.title(\"Plot 16: TD - IDF plot\")\nplt.show();","e416b06a":"#Create Word2Vec \ntokenised_tweet = train['clean_tweet'].apply(lambda x: x.split()) #tokenising\ntest_tokenised_tweets = test['clean_tweet'].apply(lambda x: x.split())\n\nmodel_w2v = Word2Vec(            \n            tokenised_tweet,\n            size=200, # desired no. of features\/independent variables \n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34) \nmodel_w2v.train(tokenised_tweet,total_examples= len(train['clean_tweet']), epochs=20)","d2f6b4d2":"model_w2v.wv.most_similar(positive=\"realdonaldtrump\")","78142878":"#The below function will be used to create a vector for each tweet by taking the average of the vectors\ndef word_vector(tokens, size):\n    '''\n    create a vector for each tweet by taking the average of the vectors of the words present in the tweet\n    \n    Parameters\n    ----------\n    tokens: list of strings\n            Input of tokens per tweet\n    size: int\n          how many words to vectorize\n          \n    Output\n    ---------\n    output: Average vector per token in tweets\n    \n    '''\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary\n                         \n            continue\n    if count != 0:\n        vec \/= count\n    return vec","0dda0f92":"#create word2vec dataframe\nwordvec_arrays = np.zeros((len(tokenised_tweet), 200))\n\nfor i in range(len(tokenised_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenised_tweet[i], 200)\n    \nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape","2590498d":"X = train['message']\ny = train['sentiment']\n# X_vec = wordvec_df\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# X_train_vec, X_test_vec, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)","970337ca":"# TF-IDF Features\npipe1 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',BernoulliNB())])\npipe1.fit(X_train,y_train)\n#prediction set\nprediction_nb = pipe1.predict(X_test)","22766830":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_nb),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","705ab882":"# print classification report\nprint(classification_report(y_test,prediction_nb))","4cd250bf":"# Print overall acuracy\nprint(accuracy_score(y_test,prediction_nb))","98097b49":"pipe2 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',LinearSVC(random_state = 42))])\npipe2.fit(X_train,y_train)\n#prediction set\nprediction_lsvc = pipe2.predict(X_test)","2a2fbbdc":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_lsvc),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","56594747":"# print classification report\nprint(classification_report(y_test,prediction_lsvc))","c94e54e4":"# Print overall acuracy\nprint(accuracy_score(y_test,prediction_lsvc))","f639f540":"pipe3 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',PassiveAggressiveClassifier(random_state = 42))])\npipe3.fit(X_train,y_train)\n#prediction set\nprediction_pas = pipe3.predict(X_test)","7aa3cf0a":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_pas),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","e288e599":"# print classification report\nprint(classification_report(y_test,prediction_pas))","398ab083":"# Print overall acuracy\nprint(accuracy_score(prediction_pas,y_test))","5f6e8ef3":"pipe4 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',LogisticRegression(random_state = 42))])\npipe4.fit(X_train,y_train)\n#prediction set\nprediction_lr = pipe4.predict(X_test)","3607b67c":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_lr),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","41519855":"# print classification report\nprint(classification_report(y_test,prediction_lr))","ca5b4d6c":"# Print overall acuracy\nprint(accuracy_score(y_test,prediction_lr))","e7b3106b":"pipe5 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',KNeighborsClassifier())])\npipe5.fit(X_train,y_train)\n#prediction set\nprediction_knnc = pipe5.predict(X_test)","a1e2238a":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_knnc),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","069e1e09":"# print classification report\nprint(classification_report(y_test,prediction_knnc))","cef461f7":"# Print overall acuracy\nprint(accuracy_score(y_test,prediction_knnc))","ff77a4ae":"pipe6 = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer()),('classifier',GradientBoostingClassifier(random_state = 42))])\npipe6.fit(X_train,y_train)\n#prediction set\nprediction_gbc = pipe6.predict(X_test)","8782cc65":"# adding labels to confusion matrix\nconfusion_matrix_df = pd.DataFrame(confusion_matrix(y_test,prediction_gbc),index=['-1','0','1','2'], columns=['-1','0','1','2'])\nconfusion_matrix_df","344a80a3":"# print classification report\nprint(classification_report(y_test,prediction_gbc))","6521e3a5":"# Print overall acuracy\nprint(accuracy_score(y_test,prediction_gbc))","7102a08c":"#Calculating f1 - scores\nnb_f1 = round(f1_score(y_test,prediction_nb, average='weighted'),2)\nlsvc_f1 = round(f1_score(y_test,prediction_lsvc, average='weighted'),2)\npac_f1 = round(f1_score(y_test,prediction_pas, average='weighted'),2)\nlr_f1 = round(f1_score(y_test,prediction_lr, average='weighted'),2)\nknnc_f1 = round(f1_score(y_test,prediction_knnc, average='weighted'),2)\ngbc_f1 = round(f1_score(y_test,prediction_gbc, average='weighted'),2)\n\ndict_f1 = {'BernoulliNB':nb_f1,'LinearSVC':lsvc_f1,'PassiveAggressiveClassifier':pac_f1,\n                      'LogisticRegression':lr_f1, 'KNeighborsClassifier':knnc_f1,'GradientBoostingClassifier':gbc_f1}\nf1_df = pd.DataFrame(dict_f1,index=['f1_score'])\nf1_df = f1_df.T\nf1_df.sort_values('f1_score',ascending = False)","8f5803c1":"#Model LSVC\nmodel_lsvc = LinearSVC(random_state = 42)\nmodel_lsvc.fit(X_train_vec,y_train)\n\n#Predict LSVC\npredict_vec_lsvc = model_lsvc.predict(X_test_vec)\n\n#Model LR\nmodel_lr = LogisticRegression(random_state = 42)\nmodel_lr.fit(X_train_vec,y_train)\n#Predict LR\npredict_vec_lr = model_lsvc.predict(X_test_vec)\n\n#Comparing f1 score and accuracy to see if model improved\nlsvc_vec_f1 = round(f1_score(predict_vec_lsvc,y_test, average='weighted'),2)\nlr_vec_f1 = round(f1_score(predict_vec_lr,y_test, average='weighted'),2)\nlsvc_vec_acc = accuracy_score(predict_vec_lsvc,y_test)\nlr_vec_acc = accuracy_score(predict_vec_lr,y_test)\n\n#Dict\ndict1 = {'Linear SVC vec2word':[lsvc_vec_f1,lsvc_vec_acc],'Logisitc Regression vec2word':[lr_vec_f1 ,lr_vec_acc]}\n\n#Dataframe\ngs_rs_df = pd.DataFrame(dict1,index =['f1 score','accuracy']).T\ngs_rs_df = gs_rs_df.sort_values('f1 score',ascending =False)\ngs_rs_df","eb641338":"#Tuning parameters for TD-IDF first\npipeline = Pipeline([('tfidf', TfidfVectorizer()),('clf',LinearSVC(random_state = 42))])\n\nparameters = {'tfidf': [TfidfVectorizer()],\n           'tfidf__max_df': [0.25,0.5,0.75],\n           'tfidf__ngram_range':[(1, 1),(1,2),(2, 2)],\n           'tfidf__min_df':(1,2),\n           'tfidf__norm':['l1','l2']},\n\ngrid_search_tune = RandomizedSearchCV(pipeline, parameters, cv=10, n_jobs=-1, verbose=3)\ngrid_search_tune.fit(X_train, y_train)\n\nprint(\"Best parameters set:\")\nprint(grid_search_tune.best_estimator_.steps)","d605019e":"#Tuning parameters for Linear Support Vector and Passive Agressive Classifiers\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_df=0.25, min_df=2, ngram_range=(1, 2))),\n    ('clf', LinearSVC(random_state = 42))])\n\nparameters = [{'clf':[LinearSVC(random_state = 42)],\n           'clf__penalty':['l1','l2'],\n           'clf__C':np.logspace(0, 4, 10),\n           'clf__class_weight':['balanced',None]},\n           {'clf':[LogisticRegression(random_state = 42)],\n            'clf__penalty' : ['l1', 'l2'],\n            'clf__C' : np.logspace(0, 4, 10),\n            'clf__solver' : [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n            'clf__class_weight':['balanced',None]}]\n\ngrid_search_tune = RandomizedSearchCV(pipeline, parameters, cv=10, n_jobs=-1, verbose=3)\ngrid_search_tune.fit(X_train, y_train)\n\nprint(\"Best parameters set:\")\nprint(grid_search_tune.best_estimator_.steps)","8de857e1":"#Tuning parameters for TD-IDF first\npipeline = Pipeline([('tfidf', TfidfVectorizer()),('clf',LinearSVC(random_state = 42))])\n\nparameters = {'tfidf': [TfidfVectorizer()],\n           'tfidf__max_df': [0.25,0.5,0.75],\n           'tfidf__ngram_range':[(1, 1),(1,2),(2, 2)],\n           'tfidf__min_df':(1,2),\n           'tfidf__norm':['l1','l2']},\n\ngrid_search_tune = GridSearchCV(pipeline, parameters, cv=4, n_jobs=-1, verbose=3)\ngrid_search_tune.fit(X_train, y_train)\n\nprint(\"Best parameters set:\")\nprint(grid_search_tune.best_estimator_.steps)","49c20c32":"#Tuning parameters for Linear Support Vector and Passive Agressive Classifiers\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1, 2))),\n    ('clf', LinearSVC(random_state = 42))])\n\nparameters = [{'clf':[LinearSVC(random_state = 42)],\n           'clf__penalty':['l1','l2'],\n           'clf__C':np.logspace(0, 4, 10),\n           'clf__class_weight':['balanced',None]},\n           {'clf':[LogisticRegression(random_state = 42)],\n            'clf__penalty' : ['l1', 'l2'],\n            'clf__C' : np.logspace(0, 4, 10),\n            'clf__solver' : [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n            'clf__class_weight':['balanced',None]}]\n\ngrid_search_tune = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=3)\ngrid_search_tune.fit(X_train, y_train)\n\nprint(\"Best parameters set:\")\nprint(grid_search_tune.best_estimator_.steps)","a6de4068":"#pipeline for random search cv\nrandomcv = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer(max_df=0.25, min_df=2, ngram_range=(1, 2))),('classifier',LinearSVC(C=7.742636826811269, random_state=42))])\nrandomcv.fit(X_train,y_train)\n#prediction set\nprediction_lsvc_best1 = randomcv.predict(test['message'])\n\n#pipeline for grid search cv\ngridcv = Pipeline(steps = [('tfidf_vectorisation',TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1, 2))),('classifier', LinearSVC(C=2.7825594022071245, random_state=42))])\ngridcv.fit(X_train,y_train)\n#prediction set\nprediction_lsvc_best2 = gridcv.predict(X_test)\n\n","16fe3f49":"#Calculating f1 - scores\nrandomcv_f1 = round(f1_score(y_test,prediction_lsvc_best1, average='weighted'),2)\ngridcv_f1 = round(f1_score(y_test,prediction_lsvc_best1, average='weighted'),2)\nrandomcv_acc = accuracy_score(y_test,prediction_lsvc_best1,)\ngridcv_acc = accuracy_score(y_test,prediction_lsvc_best2)\ndict1 = {'RandomSearch':[randomcv_f1,randomcv_acc],'GridSearch':[gridcv_f1,gridcv_acc]}\n\ngs_rs_df = pd.DataFrame(dict1,index =['f1 score','accuracy']).T\ngs_rs_df = gs_rs_df.sort_values('f1 score',ascending =False)\ngs_rs_df","1dff36eb":"my_submission = pd.DataFrame({'tweetid': test.tweetid, 'sentiment': prediction_lsvc_best1})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('finalsubmission.csv', index=False)","fdb6b1dd":"#### Most common person","abb8a515":"<a id='Table_Contents'><\/a><br>\n### Table of Contents\n\n1. [Introduction](#intro)\n * Abstract\n\n2. [Data](#imports_data)\n * Importing libraries\n * Comet\n * Loading Data\n \n3. [Exploratory Data Analysis of Raw tweets](#EDA_raw)\n * Text Statistics\n * Stop Words\n * Ngrams Analysis\n * Topic Modelling\n * NER Analysis\n\n4. [Preprocessing](#cleaning)\n * Target and input vairable\n * Data Cleaning\n\n5. [Exploratory Data Analysis after cleaning tweets](#EDA_clean)\n * Token\n * Stemming\n * Stopwords\n * Transformation\n * Visualing WordClouds\n * Visualising Bar Plots\n * Visualing Hashtags\n\n6. [Vectorization](#vector)\n * CountVector\n * TF-IDF\n * Word2Vec\n\n7. [Modelling](#modeling)\n * Trial and error\n * Split train \n * Naive Bayes Classifier\n * Linear Support Vector Classifier\n * Passive Aggressive Classifier\n * Logistic Regression Classifier\n * K Nearest Neighbours Clasiffier\n * Gradient Boosting Classifier\n * Performance Metrics of Best Model\n * Vec2Word on best models\n \n\n8. [Hypertuning](#tuning)\n * RandomSearchCV \n * GridSearchCV \n * Testing\n\n9. [Evaluation](#eval)\n\n10. [Conclusion](#con)\n\n11. [Submission](#pkl)","bbd5ccf9":"<a id='EDA_raw'><\/a><br>\n## 3.0 Exploratory Data Analysis of Raw Tweets\n[Back to Table of Contents](#Table_Contents)","b3b82753":"####  Analysis","36c17cf9":"The result of the f1-score shows that sentiment 1 and 2 averaged a score around 0.78, but had a problem with predicting scores of 0 and 1, averaging a score of 0.45 which shows a slight drop compared to Linear SVC. The imbalance in is still having a major effect. The accuracy has droped slightly comapared to linear svc","8ed0d482":"Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.","d7548643":"### Logistic Regression Classifier","d23900d1":"#####  Analysis","e3795c52":"#### Most common words","b0302d9a":"#### Sentiment of 1","0eb68ef3":"####  Analysis","d0821826":"####  Pipeline","582d49d9":"<p> When analysing each wordcloud it shows 'climat chang' which is 'climate change' as the most occuring word for any sentiment therefore stating that the data was collected with anyone referencing climate change to view each persons sentiment. The other interesting point to notice is that 'global warming' is mentioned way more in the neutral and negative sentiment than in the positve and news sentiments. The 'urlweb' seems to be appearing in every sentiment but more in sentiment 2 than any other sentiment. We will need to do some further analysis to see exactly which words are appearing the most in each sentiment. The same goes for the peoples names as it seems 'stevesgoddard' shows a lot in the negative sentiment where as for the positve sentiment of 2 shows 'stephenschelegel' appears substantially. Another interesting point, the word cloud for sentiment 2 shows no slang and the use of fuller phrases compared to others. For example instead of just saying 'Trump', 'president Donald Trump' is used.<\/p>","f87be38a":"* The positive sentiment counts are significantly higher followed by news, then neutral and lastly anti. \n* The categories for the labelled data that is going to predict unseen data is unbalanced and this will cause the model to predict sentiment = 1 very well as it dominates the four sentiment categories, however, this will be a problem in predicting the other sentiment (-1,0,2) accurately.","630dae09":"<p>Twitter is a platform widely used by people to express their opinions and show their sentiments based on a certain topic or situation. Sentiment analysis is a way to analyse the data in depth and get an understanding of how the tweet comes accross as positive, neutral or negative. One major topic in this world today is climate change and whether it exists. Companies are starting to offer products and services that are environmentally friendly and sustainable. It is very important for these companies to understand how people perceive climate change and whether a person believes in climate change based on what they tweet. The tweet format is very small, which generates many problems when analysing due to the slang, abbreviations, misspelled words etc. This notebook will do exploritory data analysis and preproccssing of data, in order to transform the data into a tidy tweet format and classify the user's sentiment by analysing the tweets into negative(-1), neutral(0), positive(1), news(2). This will be accomplished by building supervised learning models using python and natural language processing libraries<\/p>\n<p>The aim of this notebook is to detect sentiment in tweets to determine if people believe in climate change or not. A postive sentiment meaning they know climate change exists and a negative sentiment to express that climate change is not real.<\/p>","5795c8b7":"Topic 1 shows the words that appear the most, adding on from the words mentioned above are 'EPA', 'fight', 'paris', 'china', 'scientist' and 'SenSanders' seems to be the hot topics. Topic 3 shows the likes of 'amp', 'LeoDicaprio' and 'BeforeTheFlood' which shows postive words of climate change as Leonardo has strong views of helping climate change from his speech at the oscar awards. 'BeforeTheFlood', presented by National Geographic, features Leonardo DiCaprio on a journey as a United Nations Messenger of Peace.","a3d84a5e":"From looking the text above, we can create Textcounts from notcing a few trends, this will help compute some basic statistics on the text variables:\n\n<p><b>count_words:<\/b> Number of words in a tweet<\/p>\n<p><b>count_mentions:<\/b> referrals to other twitter accounts, starts with @<\/p>\n<p><b>count_hasgtags:<\/b> numner of tag words,starts with #<\/p>\n<p><b>count_capital_words:<\/b> Number of uppercase words are sometimes used as a way to express feelings<\/p>\n<p><b>count_number_exl_quest:<\/b> count number of question marks and exclamations<\/p>\n<p><b>count_urls:<\/b> number of links in tweet, starts with https<\/p>\n<p><b>count_emojis:<\/b> number of emojis, might be a good sign of the sentiment<\/p>","d355a812":"<a id='pkl'><\/a><br>\n## 11. Submission\n[Back to Table of Contents](#Table_Contents)","75e6593d":"### Comet","8971ef70":"It shows that the most entities tweeted are organisations, people then countries. This makes sense because these entities play a huge role climate change espcially organisations.","cc164841":"####  sentiment of 1","90e0f5e3":"##### Pipeline","1531820d":"####  Preparing word2vec feature set","d111fc49":"As noticed above, there seems to be quite a few stopwords for the train and specifically more for test. These could add noise in predicting sentiment so it is important to remove them as the length of the words average around 20 per tweet.","ab086d09":"#### Analysis","cec8dd16":"'Climate change' is the highest mentioned bigram. This is followed by a url, leading to an assumption that tweets also carry links to other content. 'Global warming' is another popular bigram.","1664b909":"#### Sentiment of -1","80091315":"### Testing","eff661a6":"<a id='EDA_clean'><\/a><br>\n## 5.0 Exploratory Data Analysis after cleaning tweets\n[Back to Table of Contents](#Table_Contents)","3a2a8248":"### Stopwords","76f05781":"The result of the f1-score shows that the Naive Bayes classifier predicted sentiment 1 and 2 very well, averaging a score around 0.75, but it could not predict the scores of sentiment 0 and 1, averaging a score of 0.045. This decreased the weighted score and shows why its at 0.58. The accuracy is also not high enough to predict unseen data. ","93a792c3":"* For years, Twitter has had a 140 character limit. \n* However, in 2017, the network increased the limit to 280 characters.\n* Our dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018 \n  - this probably explains why the majority of tweets are below 140 characters.\n* Sprout Social says the ideal Length of a Tweet is 71-100 characters.\n* According to Buddy Media, Tweets with 100 characters get 17% higher engagement rates than longer Tweets.\n\nKey maximum word count and character limits on Twitter:\n    * Maximum Tweet length: 280 characters\n    * DMs: 10,000 characters\n    * Handle maximum length: 15 characters\n    * Twitter profile name maximum length: 20","dc9f8912":"### Word2Vec on top performing models ","361f22e0":"### Gradient Boosting Classifier","293768ed":"Creating a string of tidy tweets for further analysis with visualisation","372e9803":"Firstly going to analyse the words after using the stemmatization process to see which words appear the most and to add to the stopword dictionary that has been created in the nltk library.","48fea86e":"Stemming is the process of reducing words to their base or root form. For example, if we were to stem the following words: \"runners\", \"running\", \"ran\", the result would be a single word \"run\"","edb1f129":"#### Input Variables","c2bd52ed":"###  Visualising WordClouds","409c8624":"###  Stemming","2fbda772":"#### Plot of sentiment -1","2021bba4":"Creating barplots will give show a very clear representation in terms of the amount of words for each sentiment, the horizontal plot will give insight and show clearly which words are used more dominantly ","9f9db325":"####  Pipeline","36b4327c":"<a id='cleaning'><\/a><br>\n## 4.0 Preprocessing\n[Back to Table of Contents](#Table_Contents)","5bb9d1aa":"####  Analysis","f16ae763":"The bag of words shows a representation of its words, disregarding grammar and word order but keeping multiplicity","f8277110":"####  Sentiment of 0","12e1396e":"### Linear Support Vector Classifier","471d5262":"This shows that the most common words are climate, change, rt, urlweb, global, warm and trump. Therefore some of these words like 'rt' can be added to the stopwords as it adds no sentiment. It going to be exciting to see which words show up the the most for each sentiment after cleaning the stopwords. What is notible here is the large amount of short words appearing in tweets","5b4fe186":"It is no surprise that 'climate' is the top word as that is the topic. The word appears over 12,000 times.\n* RT is the second word, appearing just shy of 10,000. A safe assumption here is that a large number of the \ntweets are not original, but just re-tweets.\n* Appearing under 9,000 times 'change' has the highest association with climate related tweets. This is       followed by global and then warming.\n* At just under 2,000 appearances, Trump is the highest mentioned person. This number will of course change as we come to realise that his mentions can be by title, first name or by surname.\n* Let's move to Ngram analysis for more insights.","90da3e07":"### Target and input variable","0e7fc609":"The word2vec performs slightly worse than the TF-IDF when comparing the f1 score and accuracy therefore the TF-IDF will be used for hypertuning on the models that preformed the best.","4ac2b634":"### Tokenisation","7f602c22":"To understand the imbalance of the data better, a pie plot will be shown with percentages of each sentiment","02a2915c":"#### Analysing word length","1bff2693":"Word2vec is a two-layer neural net processes that convert words into corresponding vectors in such way that the semantically similar vectors are close to each other in N-dimensional space. Where N refers to the dimensions of the vector, in other words it inputs text corpus and outputs text vector. There are two features with word2vec: Skip Gram and Continuous Bag of Words Models. In the Skip Gram model, the context words are predicted using the base word. For instance, given a sentence \"I love to dance in the rain\", the skip gram model will predict \"love\" and \"dance\" given the word \"to\" as input. Skip-gram was the type used for the wrd2vec created below.","1ae1f28e":"### Loading  data","0a7c6d3e":"As you can see the TF-IDF weights the words differently than the Count Vectorizer from analysing the plots, 'urlweb' seems to be waited much higher compared to the count vectorizer which just counts the amount of words for all tweets.","5a46cee0":"<a id='tuning'><\/a><br>\n## 8. Hypertuning\n[Back to Table of Contents](#Table_Contents)","52767ba5":"#### Pipeline","ca663cfb":"The result of the f1-score shows that sentiment 1 and 2 averaged a score around 0.72, but had a problem with predicting scores of 0 and 1, averaging a score of 0.36 which shows a slight drop compared to logistic regression. The imbalance in is still having a major effect. The accuracy has dropped quite a bit compared to logistic regression.","4f16c561":"####  Pipeline","4ae4a726":"<p>The goal is to find a hyperplane that seperates all the sentiment classes, on each round of analysing an observation and makes a prediction on the current hypothesis. It then compares prediction to true y and suffers a loss based on the difference. The goal is to make cumulative loss as small as possible. Finally, the hypothesis gets updated according to previous hypothesis and rhe current example.<\/p>","aed43622":"## RandomSearchCV","13636a37":"<p>This shows some good insight on paired words  per sentiment as the WorldCloud gave us the idea of how single words are used for each sentiment. This shows for positive sentiment of 2 that there are many links shared for giving a greater sentiment, this proves that the statistical analysis of url links was accurate, However that doesn't mean if someone put a link that it would automatically be a positive sentiment. The other interesting insights are 'donald trump', 'scott pruit' and 'fight climate' for sentiment of 2. Scott Pruit is the head of the EPA organisation that was mentioned above, which makes it clear why he would show signs of news sentiment, fight climate shows how people feel towards climate change.<\/p>\n<p> Sentiment of 1 shows new aspects of 'believe climate' and 'change denier', deniers work actively to mislead the public and delay policy action to address climate change. therefore the word change means that the public needs to stop being misleaded therefore it is intutively a positive sentiment.\n<p> Sentiment of 0 shows aspects like 'club penguin' which is not shown in any other sentiment, this speaks about intuitively the ice caps melting for penguins, which should show signs rather of positve sentiment, so its interesting that this pair of words is shown in the neutral sentiment.<\/p> \n<p> Sentiment of -1 shows some interesting paried features, 'man made', 'made climate' and 'al gore'. Al Gore is a environmentalist and politican in America, he has a campaign to teach people about global warming, so it is very interesting that he appears in the negative sentiment tweets. The other interesting debate and it may be the reason why man made showed up in the tweets, the debate is about is climate change a natural event meaning that humans have add no impact on it, or is it man made therefore humans have had an impact on the environment.<\/p>","57d5c1f3":"The result of the f1-score shows that the Linear SVC predicted sentiment 1 and 2 very well, averaging a score around 0.79, but had a problem with predicting scores of 0 and 1, averaging a score of 0.47 which is a major improvement from the Naive Bayers Classfier. The imbalance in data shows that it is predicting two classes better than the others. The accuracy is better than the Naives Bayers model.","312500f2":"#### Bigram Analysis","04cafc9b":"###  Visualising Barplots ","8dda6b00":"A classification model tries to draw some conclusion from the input values given for training. It will predict the class labels\/categories for the new data.\n* The Dataset being used here is a multiclass with four classes to predict namely negative, neutral and positive, news sentiments.\n* Hence we use various classification models to classify our data and test the accuracy of classification.\n* The Classifiers being used now are,\n    1. Naive Bayes Classifier\n    2. Linear Support Vector Classifier\n    3. Passive Aggressive Classifier\n    4. Logistic Regression Classifier\n    5. K Nearest Neighbours Clasiffier\n    6. Gradient Boosting Classifier\n* For evaluating the model we check for the Accuracy and F1 scores of the models for performance evaluation.","9ef48b6c":"###  TF-IDF Features","fe415a4d":"#### Most Common Organisation","ca2b2988":"<p>The target variable had unbalanced data, so it was imperative to fix this to improve the model in predicting all classes. Oversampling, Undersampling and a balance of Oversampling\/Undersamping were created in balancing the target variable in order to improve the scores. The scores for all the models improved in the training set, but as it was submitted in kaggle, there were scores less than 0.7 being achieved, therefore it was overfitting the training results. The conclusion was that we needed another method for balancing the data or more tweets to succeed in the methods mentioned above. This code was not included as it did not perform well and overfitted the models.<\/p> \n\n<p>The clean data was tested and the models performed worse on kaggle with f1 scores of around 0.69, trial and error was used to clean the tweets in various ways where a method was taken out from the precprocessing and other methods like text blogging and vader were added in. The score increased to 0.713 but did not achieve better results than the raw text. For this reason the raw tweets were used to train the models<\/p>","c935b1db":"Lets see how the text count analysis relates to the sentiment categoroies by plotting graphs","5f3d85f3":"<a id='modeling'><\/a><br>\n## 7.0 Modelling\n[Back to Table of Contents](#Table_Contents)","ff72701d":"The Bernoulli naive Bayes calssifier assumes that all features are binary such that they only take in two values. The classifier uses Bayes Theorem. It predicts membership probabilities for each class (in this case sentiment) such as the probability given a certain obervation belongs to a particular class. The class with highest probability is considered as the most likely class in this case sentiment 1. Given the features, it selects proability of class with highest outcome.","5b30186f":"The result of the f1-score shows that sentiment 1 and 2 averaged a score around 0.78, but had a problem with predicting scores of 0 and 1, averaging a score of 0.38 which shows a  drop compared to passive aggressive classifier. The imbalance in is still having a major effect. The accuracy has improved slightly compared to passive aggresive classifier.","41c68ef2":"<a id='imports_data'><\/a><br>\n## 2.0 Data\n[Back to Table of Contents](#Table_Contents)","5b8d0ede":"## GridSearchCV","9a9434d2":"#### Remove Duplicate Rows","f2b53f42":"####  Analysis","18a88f2c":"### Split train and test data","8f46a417":"### Preformance metric of best model","63e71e40":"### Importing libraries","9009769b":"####  sentiment of 2","3a566f79":"### NER Analysis","4d31458c":"# Team EN3 Classification Predict","3b5b38b6":"#### Most common GPE","15b20091":"<p>A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. Since it is a linear svc the hyperplane will be predicting each class with a linear kernal. It is vary similar to SVC with kernal = 'linear' and the multiclass support is handled according to a one-vs-the-rest scheme.<\/p> \n\n","f88da8fc":"### K Nearest Neighbours Classifier","a970d2aa":"From the entity analysis it shows that a lot of data needs to be cleaned as some of the organisations shown are links or latin symbols and not all the entity labels have been labelled correctly. However, the entity analysis has given good insight as the countries USA, China, UK and the city Paris seem to show the most in the tweets. The names that appear the most is first and for most Donald Trump, followed by Scott Pruit, Al Gore, Obama and Hilary Clinton which all have to do with mainly political figures in America. The top organisations found from the list are EPA which is the environmental Protection agency, Exxon which is an oil company and the UN known as the United Nations which has framework convention on climate change. NASA which stands for National Aeronautics and Space Administration, qhich have been collecting data with their Earth-orbiting satellites and other technological advances, have enabled scientists to see the big picture, collecting many different types of information about our planet and its climate on a global scale. Since tweets can use slang and mispelled words, this analysis can give you some good insight but not the whole picture, so further analysis will be done once the text is cleaned.","25f8281a":"#### Sentiment of 0","5cac3ddf":"Here we can analyse some clear topics. For example, #0 talking about how trump is impacting climate change for a better economy, #1 talking about how climate change is affecting the sea level because of melting ice caps and #6 talking about protecting the ebvironment and fight against trump city.","a718aa83":"The average amount of characters in a word is around 5","364c22d2":"<p>Plotting a WordCloud will help the common words used in a tweet. The most important analysis is understanding sentiment and the wordcloud will show the common words used by looking at the train dataset<\/p>\n<p>A word cloud is an image made of words that together resemble a cloudy shape. The clouds give greater prominence to words that appear more frequently in the source text. You can tweak your clouds with different fonts, layouts, and color schemes.<\/p>","9c8fb3aa":"<a id='con'><\/a><br>\n## 10. Conclusion\n[Back to Table of Contents](#Table_Contents)","26de5eb4":"### Stopwords","2d1826ac":"#### Trigram analysis","62ab72c3":"Frequency Inverse Document Frequency: Some words have high frequency but very little meaningful information. This feature takes into account both the frequency and how meaningful the information is.","3f67bf7c":"#### preparing word2vec features for models","0e7a0904":"<p>Before we start using the tweets to train the model for predictions. It is important to clean the data and check for repetions of rows in order to determine whether it will improve the base model of 0.75260 as the f1-Score. The base model is expected to improve once the tweet texts are have been cleaned to reduce the noise obtained within each tweet.<\/p>","4a3b5da9":"### Transformation","b3e126a5":"####  Pipeline","197c9c77":"Once again, climate change dominates and the sharing of urls is high. The aspect noticed is people using the word 'believe' in climate change or not.","90e3b765":"The summary of all the f1 scores which shows three models being the top performers, the logistic regression model will be used for futher analysis as it has a slightly higher accuracy compared to passive aggressive classfier, along side with the top performing model.","d52b4851":"###  Passive Aggressive Classifier","b2f2de3c":"Lets plot the target variable to understand if the sentiments are balanced or imbalanced data.","8acd1ae5":"####  Create a bigram for each sentiment","efd197c7":"### Hashtag plots","5fd91091":"<p>It is a statistical method for analysing a data set in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).<\/p>\n<p>The goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. This is better than other binary classification like nearest neighbor since it also explains quantitatively the factors that lead to classification.<\/p>","a95b631f":"<a id='vector'><\/a><br>\n## 6. Vectorization\n[Back to Table of Contents](#Table_Contents)","48ea5525":"The result of the f1-score shows that sentiment 1 and 2 averaged a score around 0.73, but had a problem with predicting scores of 0 and 1, averaging a score of 0.33 which shows a slight drop compared to K Nearest Neighbours classifier. The imbalance in is still having a major effect. The accuracy has improved slightly compared to K Nearest Neighbours.","8a188635":"####  Analysis","8bb3e2f8":"### Topic Modelling","a47af13c":"### Text Statistics","2c44ef3e":"####  Plot of sentiment 0","06935e15":"<a id='intro'><\/a><br>\n## 1. Introduction\n[Back to Table of Contents](#Table_Contents)","909b2275":"### Naive Bayes","be839d74":"#### Number of characters present in each message\/tweet.","096f1b05":"<h3> Summary of analysis before modelling<\/h3>\n<p> The exploratory data analysis gave insight into words used the most in this global topic, and revealed the most prominent personalities that drive and influence opinions.\n   \nPeople like Donald Trump, Barack Obama, Leonardo Dicaprio, Scott Pruitt, Al Gore Hilary Clinton and Steve Goddard featured mostly. These people do not share same views on climate change but their opinions drive the debate. For example, Donald Trump seems to resonate well with non-believers and his actions are largely seen as poralizing Notable as well is that climate change is largely a political topic.\n   \nOrganisations that appeared a lot in these tweets were the Environmental Protection Agency(EPA) where Scott Pruitt was the Administrator from February 17, 2017, to July 6, 2018. Exxon, an oil and and gas company also featured prominently. The United Nations (UN) also had a lot of mentions.\n   \nAnother discovery was that appeared a lot in the tweets had 'RT' which means that people retweeted their feelings of sentiment instead of saying anything personal.\n   \nThe countries that appeared the most were USA and China. Paris is the most prominent city driven by Paris Climate Agreement and the US walk out.\n   \nThe words that appeared the most from the WordClouds are fight climate, scientist, news, warm urlweb (urlweb means a link was originally here), change denier, not believe, believe climate, cause global and the realdonaldtrump.\n   \n<p> The people that are metioned above play a major role in climate change. Trump's first term has been a relentless drive for no restrictions on fossil energy development. This in other words was to create more jobs and GDP for America with no sentiment towards the environment. He changed all the laws that Obama started to protect the environment such as the safety rules for for offshore drilling operations. Leonardo Dicaprio has played a monumental movement in helping climate change where he has a foundation called the Leonardo DiCaprio Foundation, this is intuitively why his name appears a lot in the tweets. Steve Goddard is an environmentalist and is on both sides of the climate debate. He is famous for writing the article The Register, which describes Arctic Sea ice is not receding and claimed that data from the National Snow and Ice Data Center (NSIDC) showing the opposite was incorrect. China is the worlds largest emiiter of carbon dioxide since the economy is growing on a large scale and they have one of the hugest populations. The second country that emmits the most carbon dioxide is the USA. The Paris climate change agreement is the reasons for the city to appear so much in the tweets. The agreement includes commitments from all major emitting countries to cut their climate-altering pollution and to strengthen those commitments over time. This is some of the insight found from analysis and you can see why these organisations, places, people and choice of words have appeared on the topic of climate change.<\/p>\n\n<p> The preprocessing of data was done based on the statistical analysis of counting each of the specific characters to see if it added to sentiment. The only aspects left of the tweet were lowercase words and hashtags. The stopwords words used a process called stemming to get the routes of the words, this would help the process of vectorization. The stopwords were removed to decrease the noise from the the actual words that add sentiment. The repeated rows in the train set were removed as it would not add any insight in the training the model. The vectors that can be used to train the models were explored and it showed that TF-IDF and Vec2Word uses a more sophisticated weighting system rather than CountVectorizer therefore they were used in modelling section.\n\nIt is worth noting that text analysis of tweets would differ from normal literature. This is mainly driven by restrictions on characters per tweet which then influences language usage. So words are likely to be shortened and use of slang is common. Notably though, the language used for Sentiment 2, which is News - differs significantly as fuller phrases, peoples names and titles are used.\n\nThe use of raw data with minimal cleaning helps maintain the integrity and sentiment of each tweet.<\/p>\n\n<h3> Modelling <\/h3>\n<p> The models that were used for classification were Naive Bayes Classifier, Linear Support Vector Classifier, Passive Aggressive Classifier, Logistic Regression Classifier, K Nearest Neighbours Classifier and Gradient Boosting Classifier. The two best performing models were logistic regression and linear support vector classifier based off the clean tweets, the accuracy of the Logistic regression was slightly better and so was the f1-score. The raw data seems to achieve better results for the linear models than it did for any of the other models. This shows that the target variables were seperated linearly where a hyperplane could classify better which features distinguished each sentiment.<\/p>\n","ef691b93":"For the positve sentiments it seems to show more hashtags focus on the environment and the effects that climate change is having on the enviroment. For the neutral and negative sentiments it seems that the hashtags are focusing on  'trump hashtag' and 'maga' which means Make America Great Again, which could possibly state that they are believing in what trump is doing to he environment","261204a8":"#### Sentiment of -1","83503328":"### Data Cleaning","730f494e":"### Abstract","80d04bd7":"The imbalance of data seemed to have the biggest affect on the results of our models, it had a major problem with predicting sentiments of 0 and -1, the cleaning of data seemed to take away from the tweet messages since the length of the average tweets was around 15-25 words, taking away from this seemed to make it harder for the models to predict each of the sentiments. The hypertuning did not improve the models substantially and only showed slight improvement. In terms of recommendations, first to focus on finding a methods to balance the target variable. Secondly, to spend a lot more time on traning the models and tuning them.","86b47410":"### Ngrams Analysis","b17471e2":"The results showed Linear Support Vector Classifier and Logistic Regression model performing best, mainly observing the weighted f1 score and the accuracy of the model. The logistic regression had an accuracy of 0.71 and an average weighted f1 score of 0.69. The linear support vector classifier had an accuracy of 0.71 and a weighted f1 score of 0.71. Therefore by tuning the parameters of the TD-IDF and Support Vector Classifier, the f1-score should improve. There will be one more model tuned to see if the results improve for the second best performing models based on the weight f1-score","b9c20c68":"K-nearest neighbors (KNN) algorithm uses 'feature similarity' to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.","4cdfb0cc":"It is used to describe the process of converting each tweet into a list of tokens, words we actually want. Word tokenizer can be used to find the list of words in a string","bfb0c724":"#### Plot of Sentiment 2","5357213b":"### Word2Vec","d377a3e5":"#### Target Variable","527b10ef":"#### Sentiment of 2","894d4cf4":"### CountVector","be8f3508":"### Trial and Errors ","5c0b8cdc":"<a id='eval'><\/a><br>\n## 9. Evaluation\n[Back to Table of Contents](#Table_Contents)","fb78c372":"#### RandomSearchCV & GridSearchCV","a5f40274":"#### Plot of Sentiment 1","3d25620d":"<b> Number of words per tweet <\/b>\n<ul>\n<li> The number of words used in each tweet is relatively low, the largest number of words is 30 and the lowest number of words being 5. Therefore when cleaning data, be careful not to remove a lot of words. The distribution between 20-30 words seems to be the peak for sentiments = (-1, 0,1). However, the distribution for a sentiment = 2, shows a peak between 15-25 words. Therefore, no relationship was found when comparing number of words and the value of sentiment it portrays.<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of mentions<\/b>\n<ul>\n<li> Each sentiment has atleast one referral to another twitter account since the peak is at one, therefore seems to be no relationship with number of mentions and sentiment.<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of hashtags<\/b>\n<ul>\n<li> The distributions between graphs show the peak at zero therefore most of the tweets do not have hastags. Showing no change of number of hashtags with sentiment rating<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of Capital letters containig 3 or more consecutively<\/b>\n<ul>\n<li> Most of the tweets seem to show no Capatilized words therefore, no relationship between capitalized words and sentiment<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of exclamation and question marks<\/b>\n<ul>\n<li> Most of the tweets seem to show no exclamation or question marks therefore, no relationship between capitalized words and sentiment<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of URLs<\/b>\n<ul>\n<li> The tweets with no url link seem to have less sentiment than the tweets that have atleast one url link, this can be compared whe observing sentiment = 2 with the other sentiments. Therefore a relationship seems to show when comparing url links and sentiment<\/li>\n<\/ul>\n<p> <\/p>\n<b> Number of Emojis<\/b>\n<ul>\n<li> Most of the tweets seem to show no use of emojis therefore, no relationship between emojis and sentiment was detected<\/li>\n<\/ul>\n<p> <\/p>","46f9ee38":"#### Pipeline"}}