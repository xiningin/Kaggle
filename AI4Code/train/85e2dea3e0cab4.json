{"cell_type":{"4459cbf2":"code","c881d48d":"code","f8743c9f":"code","8d2c61e4":"code","9dc05e9e":"code","d86638f1":"code","467941ae":"code","407105fe":"code","e88455a1":"code","8db3727d":"code","6da4fada":"code","72704c5f":"code","ac0363cf":"code","6dd9a121":"code","1b019cd1":"code","57c7d6b5":"code","a750da9d":"code","6e4c0762":"code","62de07df":"code","ba4b889b":"markdown","8fe32eb8":"markdown","3a90fdfa":"markdown","9ae46d93":"markdown","53dd24fd":"markdown","392f9540":"markdown","be8ea8a1":"markdown","c3f8a798":"markdown","08baab46":"markdown","6ff9a82d":"markdown","edecfd97":"markdown","9f3d0b17":"markdown","e713e05e":"markdown","cafa9781":"markdown","22ce3a21":"markdown","7d9115d7":"markdown","7c909d30":"markdown","3a41d6a3":"markdown","46036a57":"markdown","a3b6f65f":"markdown","23f191fd":"markdown","6a3b40cd":"markdown","3df876ea":"markdown","9841a357":"markdown"},"source":{"4459cbf2":"# Import packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nimport matplotlib.patches as mpatches\nplt.style.use('ggplot')","c881d48d":"# Load the dataset\ndataset = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\n# Shape of dataset\nprint(\"This dataset has \" + str(dataset.shape[0]) + \" rows and \" + str(dataset.shape[1]) + \" columns.\")","f8743c9f":"# Look at a sample of the data\ndataset.head()","8d2c61e4":"# Information about the dataset\ndataset.info()","9dc05e9e":"# Check for missing data\nprint(dataset.isnull().sum())","d86638f1":"# Plot Time and Amount distributions\nplt.title(\"Time distribution\")\nsns.distplot(dataset.Time, color = \"blue\")\nplt.figure(figsize=(9,7))\nplt.title(\"Amount distribution\")\nsns.distplot(dataset.Amount, color = \"blue\")\nplt.figure(figsize=(9,7))","467941ae":"# Plot all features\ndataset.hist(figsize=(25,25), color = \"blue\")\nplt.show()","407105fe":"# Number of legal and fraud transactions\nnumber = dataset.Class.value_counts()\nlegal = number[0]\nfraud = number[1]\nlegal_perc = (legal\/(legal+fraud))*100\nfraud_perc = (fraud\/(legal+fraud))*100\nprint(\"There were {} legal transactions ({:.2f}%) and {} fraud transactions ({:.2f}%).\".format(legal, legal_perc, fraud, fraud_perc))","e88455a1":"# Plot legal vs fraud\nplt.figure(figsize=(9,7))\nsns.barplot(x=number.index, y=number, palette = [\"g\", \"r\"])\nplt.title(\"Number of Legal vs. Fraud Transactions\")\nplt.ylabel(\"Number of transactions\")\nplt.xlabel(\"Class Variable (0:Legal, 1:Fraud)\")\nplt.show()","8db3727d":"# Plot the correlation Matrix\ncorr_data = dataset.corr()\nplt.figure(figsize=(11,9))\nmatrix = sns.heatmap(data=corr_data)\nplt.title(\"Correlation Matrix\")\nplt.show()","6da4fada":"# Violin Plot to show distribution of features by class\nfor i in range(28):\n    sns.violinplot(x=\"Class\", y=\"V\"+str(i+1), data=dataset, palette = [\"g\", \"r\"])\n    plt.xlabel('Class')\n    plt.ylabel('V'+str(i+1))\n    plt.title(\"V\"+str(i+1)+\" by class\")\n    plt.show()","72704c5f":"# Standardise Time and Amount features\nstand1 = StandardScaler()\nstand2 = StandardScaler()\n\nstand_time = stand1.fit_transform(dataset[['Time']])\ntime_list = [item for sublist in stand_time.tolist() for item in sublist]\nstand_time = pd.Series(time_list)\n\nstand_amount = stand2.fit_transform(dataset[[\"Amount\"]])\namount_list = [item for sublist in stand_amount.tolist() for item in sublist]\nstand_amount = pd.Series(amount_list)\n\n# Add new standardised columns to dataset & drop old ones. \ndataset = pd.concat([dataset, stand_amount.rename(\"Stand_Amount\"), stand_time.rename(\"Stand_Time\")], axis=1)\ndataset.drop([\"Amount\", \"Time\"], axis=1, inplace=True)","ac0363cf":"# Split data into test and train datasets\nsplit = np.random.rand(len(dataset)) < 0.9\ntrain = dataset[split]\ntest = dataset[~split]\nprint(\"Train Number: \" + str(train.shape) + \" \\nTest Number: \" + str(test.shape))\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\n# Calculate no. of fraud transactions in train data \nfraud_in_train = train.Class.value_counts()[1]\nprint(\"There are \" + str(fraud_in_train) + \" fraud transactions in train data.\")\n\n# Randomly select the same no. of legal transactions\nnormal = train[train['Class'] == 0]\nfraudulent = train[train['Class'] == 1]\nselection = normal.sample(fraud_in_train)\nprint(\"There are \" + str(selection.shape[0]) + \" legal transactions selected for train data.\")","6dd9a121":"# Join both into dataset that is now balanced\nselection.reset_index(drop=True, inplace=True)\nfraudulent.reset_index(drop=True, inplace=True)\nbalanced_sample = pd.concat([selection, fraudulent])\n\n# Shuffle balanced dataset\nbalanced_sample = balanced_sample.sample(frac=1).reset_index(drop=True)\n\n# Plot new graph\nnew_numbers = balanced_sample.Class.value_counts()\nplt.figure(figsize=(9,7))\nsns.barplot(x=new_numbers.index, y=new_numbers, palette = [\"g\", \"r\"])\nplt.title(\"Legal vs. Fraud Transactions In Balanced Dataset\")\nplt.ylabel(\"Number\")\nplt.xlabel(\"Class (0:Legal, 1:Fraud)\")\nplt.show()","1b019cd1":"# Visualise correlation between all features and class\nbalanced_sample.corrwith(balanced_sample.Class).plot.bar(figsize = (20, 10), title = \"Correlation with Class\", fontsize = 15, rot = 45, grid = True)\nplt.show()","57c7d6b5":"# Identify features with relatively high correlation to class feature\nbal_corr = balanced_sample.corr()\nbal_corr = bal_corr[['Class']]\nprint(\"Features that have high negative correlation with class: \")\nprint(bal_corr[bal_corr.Class < -0.5])\nprint(\"Features that have high positive correlation with class: \")\nprint(bal_corr[bal_corr.Class > 0.5])","a750da9d":"# Plot features with high positive correlation\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(19,10))\nf.suptitle(\"Features With High Positive Correlation\", size=20)\nsns.violinplot(x=\"Class\", y=\"V4\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[0])\nsns.violinplot(x=\"Class\", y=\"V11\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[1])\nplt.show()\n\n# Plot features with high negative correlation\nf, axes = plt.subplots(nrows=3, ncols=3, figsize=(19,10))\nf.suptitle(\"Features With High Negative Correlation\", size=20)\nsns.violinplot(x=\"Class\", y=\"V3\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[0,0])\nsns.violinplot(x=\"Class\", y=\"V9\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[0,1])\nsns.violinplot(x=\"Class\", y=\"V10\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[0,2])\nsns.violinplot(x=\"Class\", y=\"V12\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[1,0])\nsns.violinplot(x=\"Class\", y=\"V14\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[1,1])\nsns.violinplot(x=\"Class\", y=\"V16\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[1,2])\nsns.violinplot(x=\"Class\", y=\"V17\", data=balanced_sample, palette = [\"g\", \"r\"], ax = axes[2,0])\nplt.show()","6e4c0762":"# Remove extreme outliers\nQ1 = balanced_sample.quantile(0.25)\nQ3 = balanced_sample.quantile(0.75)\nIQR = Q3 - Q1\ndataset2 = balanced_sample[~((balanced_sample < (Q1 - 3 * IQR)) |(balanced_sample > (Q3 + 3 * IQR))).any(axis=1)]\ndata_after = len(dataset2)\ndata_before = len(balanced_sample)\nlen_difference = len(balanced_sample) - len(dataset2)\nprint(\"Dataset size was reduced from \" +str(data_before)+\" transactions by \" +str(len_difference)+\" transactions to \"+str(data_after)+\" transactions.\")\nprint(\"There are now \" + str(dataset2.Class.value_counts()[0]) + \" legal transactions\")\nprint(\"There are now \" + str(dataset2.Class.value_counts()[1]) + \" fraud transactions\")\n\ndataset2.to_csv('dataset2.csv')","62de07df":"# Dimension Reduction using t-SNE\ny_target = dataset2[\"Class\"]\nX_data = dataset2.drop([\"Class\"], axis=1)\nX_dim_reduced = TSNE(n_components=2, random_state=42).fit_transform(X_data.values)\n\n# t-SNE scatter plot\nf, ax = plt.subplots(figsize=(20,14))\ngreen_cluster = mpatches.Patch(color='g', label='Legal')\nred_cluster = mpatches.Patch(color='r', label='Fraud')\nax.scatter(X_dim_reduced[:,0], X_dim_reduced[:,1], y_target == 0, c = 'g', label='Legal', linewidths=3)\nax.scatter(X_dim_reduced[:,0], X_dim_reduced[:,1], y_target == 1, c = 'r', label='Fraud', linewidths=3)\nax.set_title('t-SNE', fontsize=14)\nax.legend(handles=[green_cluster, red_cluster])\nplt.show()","ba4b889b":"Finally, we will use use a dimension reduction algorithm called T-distributed Stochastic Neighbor Embedding (t-SNE). t-SNE takes a high dimensional dataset and reduces it to the point where almost all of the information is captured but can also be visualised on a 2D graph. It does this by forming clusters of points that are similar and move each cluster away from one another in order to visualise clearly. Let's see how well this method works:","8fe32eb8":"Calculating the number of legal and fraud cases: ","3a90fdfa":"![Screenshot%202020-11-06%20at%2015.50.38.png](attachment:Screenshot%202020-11-06%20at%2015.50.38.png)","9ae46d93":"## Data Preparation","53dd24fd":"To summarise, we have explored the dataset and the correlations between variables. Balanced the dataset and potentially identified the most useful varaibles for classification. We have eliminated extreme outliers and produced a dimension reduction to check for clustering. Next we will need to employ ML algorithms on these cases and see if they improve classification. \n\nWithin 'Fraud Detection Using Machine Learning - Part 2', I use ML methods for classifying legal and fraud cases. This can be found within Kaggle under my notebooks. ","392f9540":"\nCredit Card Fraud includes any fraudulent activity committed by a criminal using credit or debit cards, with the intent to purchase goods, services or even transfer money to another account controlled by the fraudster. Credit card fraud has increased significantly on a global scale as online transactions and e-Commerce have become more popular. Modern fraud detection systems are better than ever at detecting fraudulent activity and yet billions of dollars are lost every year, creating considerable problems for banks and customers worldwide. Therefore, there is a need for more accurate detection systems. The Figure below comes from the Nison Report, 2019 and it shows the expected money loss to fraudsters over the next decade. ","be8ea8a1":"The figure above seems to show that V14, V4 and V11 are the variables most correlated with class. Therefore, it makes sense that these features may have the most impact when it comes to predicting whether a particular transaction is legal or fraud.\n\nLet's identify variables with relatively high correlation to class feature and produce violin plots for each to look at the distribution of instances:","c3f8a798":"# Fraud Detection Using Machine Learning - Part 1","08baab46":"The performance of ML methods will be evaluated against metrics such as accuracy, confusion matrix and ROC values to name a few. Armed with work done within this project, banks may be able to explain to customers why particular transactions were deemed as fraud rather than expecting them to simply trust in the algorithms. This can be hugely beneficial in building trust with current and prospective customers which is fundamental for a bank.\n\nThe dataset I have used here is the credit card fraud data that can be found here: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud. \n\nFirstly, lets do data analysis to get an idea of the dataset and any data cleaning if necessary. ","6ff9a82d":"Now there are 455 legal and 455 fraud transactions within the train dataset which will then be used to train machine learning models. The trained model will then look to classify legal from fraud as best it can. \n\n\nOnce the new dataset was produced, correlation between class and all other features need to be checked to see if there are any obvious relationships:","edecfd97":"We can see that the separation has worked fairly nicely and there are 2 distinct clusters of legal and fraud transactions. However, there seems to be quite a number of fraud cases bundled within the legal clusters as shown by the red dots within the green cluster. Around 35-40% of the transactions have ended up in the wrong cluster and so need to employ machine learning to see if ML methods improve classification.","9f3d0b17":"Here we can see the extreme imbalance present within the dataset, with the red bar for the number of fraud transactions barely visible. \n\nLet's look at the correlation between the variables within the dataset, by using a heatmap:","e713e05e":"## Summary","cafa9781":"No missing data.","22ce3a21":"![Screenshot%202020-11-06%20at%2016.06.28.png](attachment:Screenshot%202020-11-06%20at%2016.06.28.png)","7d9115d7":"Most of the variables don't show signs of high multicollinearity which is good. Multicollinearity occurs when independent variables in a model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results. The colour schemes shows that correlation between varaibles is very close to 0 between most varaibles. Time and amount are the 2 varibles that show most correlation with other features and this could be because they have not be scaled and so giving false correlation. \n\nNow let's explore the distribution of each variable by class (Legal transaction or fraud). Violin plots are used here to paint a more accurate picture compared to box-plots. ","7c909d30":"Firstly, we need to scale the amount and time variables by removing the old columns and replacing with new columns. ","3a41d6a3":"It is clear to see that the variables that have the most different value distribution with respect to legal vs fraud, are the most correlated to class.\n\nAs an extra step, lets remove extreme outliers in the dataset to improve the classification algorithm:","46036a57":"Time and amount distributions show that transactions took place over 2 days and that the vast majority of transaction amounts were relatively small. Let's plot all variables to get an idea of their distributions. ","a3b6f65f":"\nMachine learning (ML) techniques have been applied in the field of financial fraud detection with varying levels of success. This report explores ML algorithms, in order to determine the most effective fraud detection approach. A gap in this field is the lack of explanations about how ML algorithms come to a decision. Due to this, there is a lack of trust in the final decisions as the inner workings are complex. This project aims to explain the decisions made by ML algorithms by employing interpretation methods. This promises to take out much of the mystery associated with \u201cblack-box\u201d machine and deep learning frameworks.\n\n","23f191fd":"All variables had been anonymised except for \u2018Time\u2019 (seconds elapsed between each transaction and the first transaction in the dataset) and \u2018Amount\u2019(the transaction amount). This was done to protect the sensitive nature of financial data of customers. There are 31 attributes and the \u2018class\u2019 attribute is given value 1 if the instance is fraud and the 0 if it isn\u2019t. \n\nUnfortunately, this does make the work of a researcher more difficult as it will not be possible to determine the names of variables. This means that it is harder to judge whether results make sense or not. However, the principle remains the same because this report is looking at relationships between variables.","6a3b40cd":"Looking at the variable 'V1' for example, we can see that the vast majority of legal transactions (shown in green) had a value around 0 and above for the variable V1. However, the fraud transactions show a much smaller number of values around and above 0. There are many more values in the -10 to -20 range for the fraud transactions. This variable V1 could be useful is differentiating between legal and fraud transactions. \n\nVariables such as V6, V7 and V8 have very similar distributions for both legal and fraud cases and so are unlikely to offer help in terms of classification. \n\nThis dataset is mostly very good but there are a couple of things that need to be done before fraud detection can be performed. ","3df876ea":"As mentioned earlier, the dataset is heavily embalanced with legal transactions compared to fraud ones which is true to a real life dataset for fraud cases. To try and combat this issue, the dataset was balanced through the under-sampling technique. This is where the number of legal transactions were reduced in order to match the number of fraud transactions there are; creating a 50:50 ratio. This random under-sampling eliminates a lot of issues presented by imbalanced datasets. \n\nThe disadvantage of this technique is that there is a loss of information from the omitted number of legal transactions. However, the alternative is to produce more fake data for fraud transactions and this introduces another set of disadvantages. If the accuracy of predictions are relatively low, other techniques for dealing with imbalance can be employed.\n\nThe new balanced dataset was shuffled and visualised:","9841a357":"## Data Analysis"}}