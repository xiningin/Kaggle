{"cell_type":{"99a54634":"code","cfd5b9d1":"code","20f12924":"code","28b49467":"code","45142434":"code","32de7493":"code","88ec7108":"code","bc59a37a":"code","12d202fc":"code","f37ff852":"code","76a34ea5":"code","824c4e10":"code","0ebca779":"code","a761e9be":"code","0b0dd189":"code","093a260e":"code","a2597780":"code","c1e53fc1":"code","565d3c69":"code","60a95dd3":"code","7d23c908":"code","b1535db2":"code","1362bf47":"code","4cf3db18":"code","22e411e9":"code","0f2a705b":"code","7f934052":"code","65e5f52b":"code","fb739c4e":"code","6be20439":"code","e4ed6f14":"code","cd24d357":"code","8c2da82b":"code","44fb9eca":"code","19ae8b54":"code","3d89590a":"code","9c397112":"code","8b3c4cb2":"markdown","e9df84df":"markdown","40a07051":"markdown","a139f149":"markdown","f56d7fa4":"markdown","d26ba2e2":"markdown","61ec740b":"markdown","5caaeb41":"markdown","c4ebee30":"markdown","b89272a5":"markdown","51afd836":"markdown","5cfb7522":"markdown","94711a38":"markdown","a066739f":"markdown","35e6be78":"markdown","a1f82df3":"markdown"},"source":{"99a54634":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfd5b9d1":"import re\nimport string\n\n# Reset the output dimensions\nimport matplotlib.pyplot as plt\n\nfrom sklearn import decomposition\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.metrics import f1_score, accuracy_score, hamming_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\nfrom skmultilearn.problem_transform import BinaryRelevance, LabelPowerset\n\nfrom scipy import linalg\n\nfrom collections import Counter\n\nimport pickle\n\nimport nltk\nnltk.download('wordnet')\nfrom nltk import stem\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n\nfrom gensim import matutils, models\nfrom gensim.models import Word2Vec\n\nimport scipy.sparse\n\nfrom wordcloud import WordCloud\n\n\nimport warnings\n\nplt.rcParams['figure.figsize'] = [24, 12]\nplt.style.use('seaborn-darkgrid')","20f12924":"train = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/sample_submission_UVKGLZE.csv')\ntrain.columns = train.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ntest.columns = test.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\nprint('Train Data shape: ', train.shape, 'Test Data shape: ', test.shape)\n\ntrain.head(10)","28b49467":"def remove_pattern(text, pattern):\n    r = re.findall(pattern, text)\n    for i in r:\n        text = re.sub(i, \"\", text)\n    return text","45142434":"for column in ['title', 'abstract']:\n    #train[column] = train[column].apply(lambda x: x.lower())\n    train[column] = np.vectorize(remove_pattern)(train[column], \"@[\\w]*\")\n    train[column] = np.vectorize(remove_pattern)(train[column], \"#[\\w]*\")\n    train[column] = np.vectorize(remove_pattern)(train[column], '[0-9]')\n    train[column] = train[column].str.replace(\"[^a-zA-Z#]\", \" \")\n    train[column] = train[column].apply(lambda x: ' '.join([i for i in x.split() if len(i) > 3]))\n\n    #test[column] = test[column].apply(lambda x: x.lower())\n    test[column] = np.vectorize(remove_pattern)(test[column], \"@[\\w]*\")\n    test[column] = np.vectorize(remove_pattern)(test[column], \"#[\\w]*\")\n    test[column] = np.vectorize(remove_pattern)(test[column], '[0-9]')\n    test[column] = test[column].str.replace(\"[^a-zA-Z#]\", \" \")\n    test[column] = test[column].apply(lambda x: ' '.join([i for i in x.split() if len(i) > 3]))\n\ntrain['description'] = train['title'] + \" \" + train['abstract']\ntest['description'] = test['title'] + \" \" + test['abstract']\n\ntrain.head()","32de7493":"categories = ['computer_science', 'physics', 'mathematics', 'statistics', 'quantitative_biology', 'quantitative_finance']\n\ntrain_dict = {}\n\nfor column in categories:\n    a = train.loc[train[column] == 1, 'description'].tolist()\n    train_dict[column] = ' '.join(a)","88ec7108":"# We can either keep it in dictionary format or put it into a pandas dataframe\n\ndata_df = pd.DataFrame(train_dict.items())\ndata_df.columns = ['index', 'description']\ndata_df = data_df.set_index('index')\ndata_df = data_df.sort_index()\ndata_df.head()","bc59a37a":"def clean_text(text):\n    '''make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    #text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    return text","12d202fc":"data_df = pd.DataFrame(data_df['description'].apply(lambda x: clean_text(x)))\ndata_clean = data_df.copy()\ndata_df.head()","f37ff852":"cv = CountVectorizer(stop_words = 'english')\ndata_cv = cv.fit_transform(data_df['description'])\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\ndata_dtm.index = data_df.index\ndata_dtm = data_dtm.transpose()\ndata_dtm.head()","76a34ea5":"# Find the top 30 words on each category\n\ntop_dict = {}\nfor c in data_dtm.columns:\n    top = data_dtm[c].sort_values(ascending = False).head(30)\n    top_dict[c]= list(zip(top.index, top.values))\n\ntop_dict","824c4e10":"# Print the top 15 words said by each category\n\nfor category, top_words in top_dict.items():\n    print(category + \":\")\n    print(', '.join([word for word, count in top_words[0:14]]))\n    print('-----------------------------------------------------------------------------------------------------------------------')","0ebca779":"# Let's first pull out the top words for each category\n\nwords = []\nfor category in data_dtm.columns:\n    top = [word for (word, count) in top_dict[category]]\n    for t in top:\n        words.append(t)\n        \nwords\n\n# Let's aggregate this list and identify the most common words along with how many routines they occur in\nCounter(words).most_common()","a761e9be":"data_dtm","0b0dd189":"# Find the bottom 200 words on each category\n\nbottom_dict = {}\nfor c in data_dtm.columns:\n    bottom = data_dtm[c].sort_values(ascending = True).head(200)\n    bottom_dict[c]= list(zip(bottom.index, bottom.values))\n\n# Let's first pull out the bottom words for each category\n\nbottom_words = []\nfor category in data_dtm.columns:\n    bottom = [word for (word, count) in bottom_dict[category]]\n    for b in bottom:\n        bottom_words.append(b)\n        \nbottom_words\n\n# Let's aggregate this list and identify the most common words along with how many routines they occur in\nCounter(bottom_words).most_common()","093a260e":"# If less than =2 of the categories have it as a rare word, exclude it from the list\n\n#add_stop_words = [word for word, count in Counter(bottom_words).most_common() if count <= 2]\n\n# Let's update our document-term matrix with the new list of stop words\n\n# Add new stop words\n\n#stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)","a2597780":"# If more than 2 of the categories have it as a top word, exclude it from the list\n\nadd_stop_words = [word for word, count in Counter(words).most_common() if count > 2]\n\n# Let's update our document-term matrix with the new list of stop words\n\n# Add new stop words\n\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Recreate document-term matrix\n\ncv = CountVectorizer(stop_words = stop_words)\ndata_cv = cv.fit_transform(data_clean['description'])\ndata_stop = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\ndata_stop.index = data_clean.index\n\n# Pickle it for later use\n\npickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\ndata_stop.to_pickle(\"dtm_stop.pkl\")","c1e53fc1":"data_stop.head()","565d3c69":"# Let's make some word clouds!\n\nwc = WordCloud(stopwords = stop_words, background_color = \"white\", colormap = \"Dark2\", max_font_size = 150, random_state = 42)","60a95dd3":"# Create subplots for each category\n\nfor index, description in enumerate(data_dtm.columns):\n    wc.generate(data_clean.description[description])\n    \n    plt.subplot(3, 2, index + 1)\n    plt.imshow(wc, interpolation = \"bilinear\")\n    plt.axis(\"off\")\n    plt.title(categories[index])\n    \nplt.show()","7d23c908":"# Find the number of unique words that each category has\n\n# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\nunique_list = []\nfor category in data_dtm.columns:\n    uniques = data_dtm[category].to_numpy().nonzero()[0].size\n    unique_list.append(uniques)\n\n# Create a new dataframe that contains this unique word count\ndata_words = pd.DataFrame(list(zip(categories, unique_list)), columns = ['category', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by = 'unique_words')\ndata_unique_sort","b1535db2":"y_pos = np.arange(len(data_words))\n\nplt.figure(figsize = (8, 8))\nplt.barh(y_pos, data_unique_sort.unique_words, align = 'center')\nplt.yticks(y_pos, data_unique_sort.category)\nplt.title('Number of Unique Words', fontsize = 20)\nplt.show()","1362bf47":"data = pd.read_pickle('\/kaggle\/working\/dtm_stop.pkl')\ndata","4cf3db18":"# One of the required inputs is a term-document matrix\ntdm = data.transpose()\ntdm.head()","22e411e9":"# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm)\ncorpus = matutils.Sparse2Corpus(sparse_counts)","0f2a705b":"# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\ncv = pickle.load(open(\"\/kaggle\/working\/cv_stop.pkl\", \"rb\"))\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","7f934052":"# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n# we need to specify two other parameters as well - the number of topics and the number of passes\n\nlda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 6, passes = 10)\nlda.print_topics()","65e5f52b":"train_data = train.copy()","fb739c4e":"train_data = train_data.drop(['title', 'abstract'], axis = 1)\ntrain_data['description'] = train_data['description'].apply(lambda x: clean_text(x))\ntrain['description'] = train['description'].apply(lambda x: clean_text(x))\ntrain_data.head()","6be20439":"train_data, test_data = train_test_split(train_data, random_state = 42, test_size = 0.30, shuffle = True)\n\ntrainData = train_data['description'].values.astype('U')\ntestData = test_data['description'].values.astype('U')\n\nvectorizer = TfidfVectorizer(strip_accents = 'unicode', analyzer = 'word', ngram_range = (1, 3), norm = 'l2', max_features = 10000, use_idf = True, stop_words = stop_words)\n\n#vectorizer = TfidfVectorizer(norm = 'l2', stop_words = stop_words)\nvectorizer.fit(trainData)\nvectorizer.fit(testData)\nX_train = vectorizer.transform(trainData)\ny_train = train_data.drop(labels = ['id', 'description'], axis = 1)\nX_test = vectorizer.transform(testData)\ny_test = test_data.drop(labels = ['id', 'description'], axis = 1)","e4ed6f14":"# Label Powerset\n\nlp_classifier = LabelPowerset(LogisticRegression(max_iter = 250, verbose = 2))\nlp_classifier.fit(X_train, y_train)\nlp_predictions = lp_classifier.predict(X_test)\nprint(\"Accuracy = \", accuracy_score(y_test, lp_predictions))\nprint(\"F1 score = \", f1_score(y_test, lp_predictions, average = \"micro\"))\nprint(\"Hamming loss = \", hamming_loss(y_test, lp_predictions))","cd24d357":"Test_Data = test['description'].values.astype('U')\nTest_Data = vectorizer.transform(Test_Data)\nPredictions = lp_classifier.predict(Test_Data)","8c2da82b":"results = pd.DataFrame.sparse.from_spmatrix(Predictions)\nresults.columns = tdm.columns.tolist()\nresults['id'] = test['id'].tolist()\nresults = results[['id'] + tdm.columns.tolist()]\nresults.columns = submission.columns.tolist()\nresults.to_csv('results_v1.csv', index = False)\nresults.head()","44fb9eca":"#pipe = Pipeline([('TFidf', TfidfVectorizer(ngram_range = (1, 3), norm = 'l2', stop_words = stop_words, smooth_idf = True)), (\"multilabel\", OneVsRestClassifier(SVC(kernel = 'poly', random_state = 42)))])\n\npipe = Pipeline([('TFidf', TfidfVectorizer(ngram_range = (1, 3), norm = 'l2', stop_words = stop_words, smooth_idf = True)), \n                 (\"multilabel\", MultiOutputClassifier(LinearSVC(penalty = 'l2', random_state = 42, class_weight = 'balanced')))])","19ae8b54":"y_train = train[[i for i in train.columns if i not in ['id', 'title', 'abstract', 'description']]]","3d89590a":"pipe.fit(train['description'], y_train)","9c397112":"predicted1 = pipe.predict(test['description'])\nsubmit = pd.DataFrame({'ID': test['id'].tolist()})\nsubmission = pd.concat([submit, pd.DataFrame(predicted1, columns = tdm.columns.tolist())], axis = 1)\nsubmission.to_csv('submission_F3.csv', index = False)\nsubmission.head()","8b3c4cb2":"# Evaluation Results:\n\n### Logistic Regression gave a micro F1Score of 0.8083\n\n### Pipeline with Tfidf and OnevsRestClassifier - LinearSVC gave a micro F1Score of 0.84","e9df84df":"## Model 2: Pipeline with TfidVectorizer and OnevsRestClassifier - LinearSVC","40a07051":"## Model 1: TfidVectorizer with LabelPowerset - Logistic Regression","a139f149":"# Data Preprocessing and Cleaning","f56d7fa4":"#### NOTE: At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n\n### Look at the most common top words add them to the stop word list","d26ba2e2":"#### Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there.","61ec740b":"## Public Leaderboard Score - Micro F1 Score: 0.84","5caaeb41":"# Import and load the dataset","c4ebee30":"### Wordclouds for different research articles' categories","b89272a5":"### Public Leaderboard Score - Micro F1 Score: 0.80838","51afd836":"![](https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/jantahack_i-day-thumbnail-1200x1200-90.jpg)","5cfb7522":"# Model Building","94711a38":"# Exploratory Data Analysis","a066739f":"### Remove words if more than 2 of the research categories have it as a top word\n\n1. If more than 2 of the categories have it as a top word, exclude it from the list\n2. Let's update our document-term matrix with the new list of stop words\n3. Read in cleaned data\n4. Add new stop words\n5. Recreate document-term matrix\n6. Pickle it for later use","35e6be78":"**[Janatahack: Independence Day 2020 ML Hackathon](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-independence-day-2020-ml-hackathon\/)**\n\n**Topic Modeling for Research Articles**\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n2. Physics\n3. Mathematics\n4. Statistics\n5. Quantitative Biology\n6. Quantitative Finance","a1f82df3":"## Findings\n1. Result, neural and are most common in CS, QF, Stats\n2. network is a common word in QF, CS, Stats"}}