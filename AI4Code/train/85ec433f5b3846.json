{"cell_type":{"df1ddddc":"code","1d14d04c":"code","cc9b6b5c":"code","479a6564":"code","0396b9ae":"code","283dd98e":"code","3993e62c":"code","fd7f830e":"code","8a9b03b7":"code","51a9e2e8":"code","16a7b2ac":"code","24b40167":"code","8558c3b6":"code","ebac6899":"code","fa431265":"code","d1a69ddc":"code","93a10137":"code","e817ee77":"code","73989404":"code","b2f0a64a":"code","61fabcbf":"code","d7fa0715":"code","70a030fa":"code","d6700452":"code","64328008":"code","a67bf24f":"code","106cd976":"code","185249ed":"code","95bce7e3":"code","f5323d06":"code","0c521c5b":"code","2171745f":"code","e9e3451c":"code","a3c40de7":"code","aff0d165":"code","6bfdfa18":"code","6dae87bd":"code","ebee15a0":"code","76e05705":"code","6cabff76":"code","ea8d9dc6":"code","cf0a4d09":"code","acf02701":"code","36fcf254":"code","21c9b0c5":"code","589a0ae4":"code","024c48bd":"code","bcf24720":"code","638f5711":"code","93e69cb8":"code","44872bf0":"code","5d689828":"code","56cd77a1":"code","ef594c71":"code","bc0dc073":"code","2e64dc8d":"code","eaf29a65":"code","245517de":"code","935a7056":"code","5767db79":"code","8dae9e12":"code","33db06d8":"code","84d9f6fe":"code","dfd5a129":"code","f808b48b":"code","22e4ed3e":"code","35d12244":"markdown","e84a17ab":"markdown","f0d8a72c":"markdown","d6f5762c":"markdown","c8ae2c1d":"markdown","3b0436fc":"markdown","9c389a41":"markdown","8bd4ca81":"markdown","2290485a":"markdown","a3ddde68":"markdown","889ed1c0":"markdown","6979ff2f":"markdown","77f5c116":"markdown","a78c065e":"markdown","d1f85949":"markdown","af7590a0":"markdown","b801f34e":"markdown","376a8458":"markdown","4d9f4f4a":"markdown","f2875e4d":"markdown","9e7f8c5b":"markdown"},"source":{"df1ddddc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d14d04c":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","cc9b6b5c":"# Da ich alle Warnungen machgegangen bin, und sie nicht nutzlich sind, schalte ich an der Stelle sie aus. Ansonsten kann dieses Code deaktiviert werden.\nimport warnings\nwarnings.filterwarnings(\"ignore\")","479a6564":"df= pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head()","0396b9ae":"df.info()","283dd98e":"df.describe()","3993e62c":"list(df.select_dtypes(include=(\"object\")))","fd7f830e":"print(\n       df['Sex'].value_counts(),\n       df['Embarked'].value_counts(),\n       df['Pclass'].value_counts())","8a9b03b7":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.countplot(\"Survived\", data=df, alpha=0.7, ax=ax)\nax.set_title('Distribution of Survived')\nplt.close(2);\n\n#prozentwerte\np=df[df[\"Survived\"] == 1].count() \/ df[\"Survived\"].count() *100\nprint(\"Percentage of Survived:\", \n      p.iloc[0])\np1=df[df[\"Survived\"] == 0].count() \/ df[\"Survived\"].count() *100\nprint(\"Percentage of Notsurvived:\", \n      p1.iloc[0])","51a9e2e8":"# \nfig, ax=plt.subplots(2,2)\n\nsns.countplot(x='Sex', hue='Survived',  data=df, ax=ax[0, 0])\nax[0,0].set_title(\"Sex\")\n\nsns.countplot(x='Pclass', hue='Survived',  data=df, ax=ax[0, 1])\nax[0,1].set_title(\"Pclass\")\n\nsns.countplot(x='Embarked', hue='Survived', data=df, ax=ax[1, 0])\n#ax[1, 0].set_title(\"Embarked\")\n\nsns.countplot(x='Parch', hue='Survived',  data=df,  ax=ax[1, 1])\n#ax[1,1].set_title(\"Parch\");\n\n\n\n######################################################################################################\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", \n      df[\"Survived\"][df[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", \n      df[\"Survived\"][df[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n\n\n######################################################################################################\n#print percentages of first, second and third class passengers that survive\nprint(\"Percentage of first class who survived:\", \n      df[\"Survived\"][df[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of second class who survived:\", \n      df[\"Survived\"][df[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of second class who survived:\", \n      df[\"Survived\"][df[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)\n\n\n######################################################################################################\n#print percentages of passengers with different ports of embarkation that survive\nprint(\"Percentage of passengers who embarked in Cherbourg who survived:\", \n      df[\"Survived\"][df[\"Embarked\"] == 'C'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of passengers who embarked in Queenstown who survived:\", \n      df[\"Survived\"][df[\"Embarked\"] == 'Q'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of passengers who embarked in Southampton who survived:\", \n      df[\"Survived\"][df[\"Embarked\"] == 'S'].value_counts(normalize = True)[1]*100)\n\n\n######################################################################################################\n#print percentages of passengers with none, one, two, three or five children or parents that survive\nprint(\"Percentage of none children or parents who survived:\", \n      df[\"Survived\"][df[\"Parch\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of 1 children or parents  who survived:\", \n      df[\"Survived\"][df[\"Parch\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of 2 children or parents  who survived:\", \n      df[\"Survived\"][df[\"Parch\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of 3 children or parents  who survived:\", \n      df[\"Survived\"][df[\"Parch\"] == 3].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of 5 children or parents  who survived:\", \n      df[\"Survived\"][df[\"Parch\"] == 5].value_counts(normalize = True)[1]*100)\n","16a7b2ac":"# \nfig, ax=plt.subplots(figsize=(10,6))\nsns.countplot(x='SibSp', hue='Survived',  data=df, ax=ax)\nax.set_title(\"SibSp\")\n\n\n\n#print percentages of passengers with none, one, two, three or four siblings or a spouse that survive\nprint(\"Percentage of none sibling or spouse who survived:\", \n      df[\"Survived\"][df[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of one sibling or spouse who survived:\", \n      df[\"Survived\"][df[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of two sibling or spouse who survived:\", \n      df[\"Survived\"][df[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of three sibling or spouse who survived:\", \n      df[\"Survived\"][df[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of four sibling or spouse who survived:\", \n      df[\"Survived\"][df[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","24b40167":"df[\"SibSp\"].value_counts()","8558c3b6":"df[\"Parch\"].value_counts()","ebac6899":"for i in range(len(df)):\n    if df[\"SibSp\"][i]== 3:\n        df[\"SibSp\"][i]= 2\n    elif df[\"SibSp\"][i]== 4:\n        df[\"SibSp\"][i]= 2         \n    elif df[\"SibSp\"][i]== 5:\n        df[\"SibSp\"][i]= 2\n    elif df[\"SibSp\"][i]== 6:\n        df[\"SibSp\"][i]= 2\n    elif df[\"SibSp\"][i]== 8:\n        df[\"SibSp\"][i]= 2\n        \nprint(df[\"SibSp\"].value_counts())","fa431265":"for i in range(len(df)):\n    if df[\"Parch\"][i]== 3:\n        df[\"Parch\"][i]= 2\n    elif df[\"Parch\"][i]== 4:\n        df[\"Parch\"][i]= 2\n    elif df[\"Parch\"][i]== 5:\n        df[\"Parch\"][i]= 2\n    elif df[\"Parch\"][i]== 6:\n        df[\"Parch\"][i]= 2\n        \nprint(df[\"Parch\"].value_counts())\n        ","d1a69ddc":"# \nfig, ax=plt.subplots(1,2)\n\n\nsns.countplot(x='SibSp', hue='Survived',  data=df, ax=ax[0])\nax[0].set_title(\"SibSp\")\n\nsns.countplot(x='Parch', hue='Survived',  data=df, ax=ax[1])\nax[1].set_title(\"Parch\");","93a10137":"fig, ax = plt.subplots(figsize=(10,6))\n\n\nsns.kdeplot(df[df[\"Survived\"]==1][\"Age\"], shade=True, \n            color=\"blue\", label=\"Survived\", alpha=0.5, ax=ax)\n\nsns.kdeplot(df[df[\"Survived\"]==0][\"Age\"],  shade=True, \n            color=\"green\", label=\"Notsurvived\", alpha=0.5, ax=ax);\n","e817ee77":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(df[df[\"Survived\"]==1][\"Fare\"], shade=True, \n            color=\"blue\", label=\"Survived\", alpha=0.5, ax=ax)\n\nsns.kdeplot(df[df[\"Survived\"]==0][\"Fare\"],  shade=True, \n            color=\"green\", label=\"Notsurvived\", alpha=0.5, ax=ax)\n\n\n####\nax2 = plt.axes([0.35, 0.35, .4, .4], facecolor='w')\n\nsns.kdeplot(df[df[\"Survived\"]==1][\"Fare\"], shade=True, \n            color=\"blue\", label=\"Survived\", alpha=0.5, ax=ax2)\n\nsns.kdeplot(df[df[\"Survived\"]==0][\"Fare\"],  shade=True, \n            color=\"green\", label=\"Notsurvived\", alpha=0.5, ax=ax2)\nax2.set_xlim([-50, 200])\nax2.set_title('Zoom');","73989404":"fig, ax=plt.subplots(figsize=(10,6))\nsns.scatterplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", data=df);","b2f0a64a":"# Check for Multicolinearity\ncorr_matrix = df.loc[:,['Age','Fare', 'SibSp', 'Parch']].corr()\n#print(corr_matrix)\nsns.heatmap(corr_matrix, annot=True);","61fabcbf":"df.loc[:,['Age', 'Fare']].hist(bins=50, figsize=(15,7));","d7fa0715":"df.head()","70a030fa":"df_train=df.drop(['Name', 'Ticket', 'Cabin',], axis=1)","d6700452":"df_train.head()","64328008":"df_train.head()","a67bf24f":"df_train.info()","106cd976":"df_train['Pclass']= df_train['Pclass'].astype('object')\ndf_train['SibSp']= df_train['SibSp'].astype('object')\ndf_train['Parch']= df_train['Parch'].astype('object')","185249ed":"# filling the missings with median\nmedian= df_train['Age'].median()\ndf_train['Age'].fillna(median, inplace=True)\n\n# Check if it worked\ndf_train['Age'].describe()","95bce7e3":"df_train['Age']= df_train['Age'].astype('int64')\ndf_train['Fare']= df_train['Fare'].astype('int64')","f5323d06":"mode= df_train['Embarked'].mode()\ndf_train['Embarked'].fillna(value=mode[0], inplace=True)\n# Check\ndf_train['Embarked'].describe()","0c521c5b":"X= df_train.drop(['PassengerId', 'Survived'], axis=1)\ny_train= df_train[\"Survived\"]","2171745f":"from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import OneHotEncoder","e9e3451c":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self,X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","a3c40de7":"num_X=list(X.select_dtypes(include= ['int64']))\ncat_X=list(X.select_dtypes(include='object'))\nprint(\"\",\"numerical:  \", num_X, len(num_X), \"\\n\",\n      \"categorical:\", cat_X, len(cat_X),)","aff0d165":"#num_pip \nnum_pip = Pipeline([\n    ('selector', DataFrameSelector(num_X)),\n    ('pt', PowerTransformer(method= 'yeo-johnson', standardize=True))\n])\n\n# cat_pip \ncat_pip = Pipeline([\n    ('selector', DataFrameSelector(cat_X)),\n    ('ohe', OneHotEncoder(sparse=False))\n])\n\n# voll_pip \nvoll_pip= FeatureUnion(transformer_list=[\n    ('num_pip_train', num_pip),\n    ('cat_pip_train', cat_pip)\n])","6bfdfa18":"# \nX_train= voll_pip.fit_transform(X)\nX_train.shape","6dae87bd":"test= pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","ebee15a0":"test=test.drop(['Name', 'Ticket', 'Cabin',], axis=1)","76e05705":"test.info()","6cabff76":"for i in range(len(test)):\n    if test[\"SibSp\"][i]== 3:\n        test[\"SibSp\"][i]= 2\n    elif test[\"SibSp\"][i]== 4:\n        test[\"SibSp\"][i]= 2         \n    elif test[\"SibSp\"][i]== 5:\n        test[\"SibSp\"][i]= 2\n    elif test[\"SibSp\"][i]== 6:\n        test[\"SibSp\"][i]= 2\n    elif test[\"SibSp\"][i]== 8:\n        test[\"SibSp\"][i]= 2\n        \nprint(test[\"SibSp\"].value_counts())","ea8d9dc6":"for i in range(len(test)):\n    if test[\"Parch\"][i]== 3:\n        test[\"Parch\"][i]= 2\n    elif test[\"Parch\"][i]== 4:\n        test[\"Parch\"][i]= 2\n    elif test[\"Parch\"][i]== 5:\n        test[\"Parch\"][i]= 2\n    elif test[\"Parch\"][i]== 6:\n        test[\"Parch\"][i]= 2\n    elif test[\"Parch\"][i]== 9:\n        test[\"Parch\"][i]= 2\n        \nprint(test[\"Parch\"].value_counts())","cf0a4d09":"test['Pclass']= test['Pclass'].astype('object')\ntest['SibSp']= test['SibSp'].astype('object')\ntest['Parch']= test['Parch'].astype('object')","acf02701":"# filling the missings with median\nmedian= test['Age'].median()\ntest['Age'].fillna(median, inplace=True)\n\n# Check if it worked\ntest['Age'].describe()","36fcf254":"# filling the missings with median\nmedian= test['Fare'].median()\ntest['Fare'].fillna(median, inplace=True)\n\n# Check if it worked\ntest['Fare'].describe()","21c9b0c5":"test['Age']= test['Age'].astype('int64')\ntest['Fare']= test['Fare'].astype('int64')","589a0ae4":"X= test.drop('PassengerId', axis=1)","024c48bd":"X.head()","bcf24720":"num_X=list(X.select_dtypes(include= ['int64']))\ncat_X=list(X.select_dtypes(include='object'))\nprint(\"\",\"numerical:  \", num_X, len(num_X), \"\\n\",\n      \"categorical:\", cat_X, len(cat_X),)","638f5711":"#num_pip \nnum_pip = Pipeline([\n    ('selector', DataFrameSelector(num_X)),\n    ('pt', PowerTransformer(method= 'yeo-johnson', standardize=True))\n])\n\n# cat_pip \ncat_pip = Pipeline([\n    ('selector', DataFrameSelector(cat_X)),\n    ('ohe', OneHotEncoder(sparse=False))\n])\n\n# voll_pip \nvoll_pip= FeatureUnion(transformer_list=[\n    ('num_pip_train', num_pip),\n    ('cat_pip_train', cat_pip)\n])","93e69cb8":"# \nX_test= voll_pip.fit_transform(X)\nX_test.shape","44872bf0":"# Einbinden notwendiger Bibliotheken.\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom statistics import mean ","5d689828":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\n\ny_pred_=cross_val_predict(gaussian, X_train, y_train, cv=5)\nacc_gaussian=cross_val_score(gaussian, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_gaussian))\nacc_gaussian=mean(acc_gaussian)","56cd77a1":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlg = LogisticRegression(random_state = 0, solver= 'lbfgs')\n\ny_pred_=cross_val_predict(lg, X_train, y_train, cv=5)\nacc_lg=cross_val_score(lg, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_lg))\n\nacc_logreg=mean(acc_lg)","ef594c71":"lg = LogisticRegression(random_state = 0, solver= 'newton-cg')\n\ny_pred_=cross_val_predict(lg, X_train, y_train, cv=5)\nacc_lg=cross_val_score(lg, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_lg))\n\nacc_logreg=mean(acc_lg)","bc0dc073":"lg = LogisticRegression(random_state = 0, solver= 'saga')\n\ny_pred_=cross_val_predict(lg, X_train, y_train, cv=5)\nacc_lg=cross_val_score(lg, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_lg))\n\nacc_logreg=mean(acc_lg)","2e64dc8d":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1], [0,1], '--', color= 'red', label= \"Null-Modell\")\n    plt.axis([0,1, 0,1])\n    plt.xlabel('Anteil falsche Positiven')\n    plt.ylabel('Anteil richtige Positiven')","eaf29a65":"# Vergleich der Logit-Modelle\nplt.plot(fpr_newton, tpr_newton, \"-\", color=\"green\", label= \"Solver= newton-cg\")\nplt.plot(fpr_saga, tpr_saga, \"-\", color=\"orange\", label= \"Solver= saga\")\nplot_roc_curve(fpr_lbfgs, tpr_lbfgs, \"Solver= lbfgs\")\nplt.title(\"Logistische Regressionen mit diversen Solvern\")\nplt.legend(loc=\"lower right\")\nplt.show();","245517de":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\n\ny_pred_=cross_val_predict(svc, X_train, y_train, cv=5)\nacc_svc=cross_val_score(svc, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_svc))\n\nacc_svc=mean(acc_svc)","935a7056":"#Decision Trees\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state = 0)\n\ny_pred_=cross_val_predict(dt, X_train, y_train, cv=5)\nacc_dt=cross_val_score(dt, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_dt))\n\nacc_decisiontree=mean(acc_dt)","5767db79":"dt = DecisionTreeClassifier(random_state = 0, criterion=\"gini\", max_depth= 6)\n\ny_pred_=cross_val_predict(dt, X_train, y_train, cv=5)\nacc_dt=cross_val_score(dt, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_dt))\n\nacc_decisiontree=mean(acc_dt)","8dae9e12":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(random_state = 0)\n\ny_pred_=cross_val_predict(randomforest, X_train, y_train, cv=5)\nacc_randomforest=cross_val_score(randomforest, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_randomforest))\n\nacc_randomforest=mean(acc_randomforest)","33db06d8":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\n\ny_pred_=cross_val_predict(gbk, X_train, y_train, cv=5)\nacc_gbk=cross_val_score(gbk, X_train, y_train, cv=5, scoring='roc_auc',)\nprint(\"Mean of Scores\",\n     mean(acc_gbk))\n\nacc_gbk=mean(acc_gbk)","84d9f6fe":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              'Decision Tree',  'Gradient Boosting Classifier'],\n    'Score': [acc_svc,  acc_logreg, \n              acc_randomforest, acc_gaussian,  acc_decisiontree,\n               acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","dfd5a129":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = decisiontree.predict(test.drop('PassengerId', axis=1))","f808b48b":"#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","22e4ed3e":"#Check\nprint(predictions, predictions.size)","35d12244":"- There is a strong difference of the percentage in the sex groups. 74.20% of females survived and from males on the other survived only 18.89%.\n- Passengers classes show also similar diffrence. The first class passengers had the most chance to survive, than the second and the third.\n- Age shows also a slight indication that the people older than 60 years of age had much less chance to survive","e84a17ab":"# Support Vector Machines","f0d8a72c":"## The Quastion to answer here is: \n# \u201cWhat sorts of people were more likely to survive?\u201d","d6f5762c":"# Logistic Regression","c8ae2c1d":"## 4. Preprossesing of data","3b0436fc":"### Prepairing test data acordingly for fiting model","9c389a41":"So, now we have the presumably improtant features for predicting Survived people on Titanic. In the next step we gain some insights throu visualization of this features\n***","8bd4ca81":" **Obviously the *model decision* tree has the best scoring and we go with that model further**\n***","2290485a":"# 1. Import Necessary Libraries\u00b6\n***\nFirst off, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","a3ddde68":"- Inbalanced distribution, which means that the classifier will be inclined to classify nonsurvivors better","889ed1c0":"- The features **Parch** and **SibSp** have in categories 2 and above only a few people in it. Therefore I'll join all the categories above 2 in categorie 2. The new features will have following categories: 0, 1, 2","6979ff2f":"# Decision Trees","77f5c116":"- As we can see the passengers with none siblings or spouse had less chance than the one with one or two siblings or spouse but more chance than the one with three or four siblings\n- Further the passengers with none children or parents had less chance to survivel than the one with one, two or three children or parents. Least chance had the one with 5 children\n- Port embarkation shows also diffrence for the chances of survival. The passengers who embarked in Cherbourg had most chance for survival, followed by Queenstown. Least chance had the passengers who embarked in Southampton","a78c065e":"## First insight about dataset\n\n- There are **891 rows** and **12 columns** in this dataset.\n- Three features (**age, cabin** and **embarked**) have missing values. **Age** could be important feature for the project purposes and w'll have to fill missing value. **Embarked** hasn't many missings and they can be replaced easily. **Cabin** has the most missing values but this feature with **Ticket** and **Name** I consider as not relevant and will be droped.\n- There are following datatypes in it: **float64(2), int64(5), object(5)**. This schould be certainly taken into account during EDA.\n\n\n","d1f85949":"# 2. Read In and Explore the Data","af7590a0":"# End of Notebook","b801f34e":"# 3. Explorative Data Analysis **(EDA)**","376a8458":"### Testing Different Models\n\n- Gaussian Naive Bayes\n- Logistic Regression\n- Support Vector Machines\n- Decision Tree Classifier\n- Random Forest Classifier\n- Gradient Boosting Classifier","4d9f4f4a":"# 7. Saving predections of the best model","f2875e4d":"# Project Steps\n\n# 1. Import Necessary Libraries\n# 2. Read In and Explore the Data\n# 3. Explorative Data Analysis (EDA)\n# 4. Preprossesing of selected data\n# 5. Which model is the best\n# 6. Saving predections of the best m# odel","9e7f8c5b":"# 6. Which model is the best"}}