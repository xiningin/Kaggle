{"cell_type":{"40245b9d":"code","d3d45893":"code","f7cac072":"code","92dc18ac":"code","5c2c0259":"code","00996ecf":"code","eb6a7923":"code","db28ffee":"code","c553d14f":"code","8d2c2209":"code","20af86af":"code","1f93698f":"code","fa6ac36f":"code","b0b42fca":"code","244ae4d6":"code","a683bb6b":"code","bd0e1857":"code","cb01635d":"code","66320096":"code","5200a780":"code","31adacf2":"code","671ed6f9":"code","cbb63da9":"code","72bac6d8":"markdown","d3f528e1":"markdown","77494565":"markdown","8eced9ea":"markdown","255fe523":"markdown","b9211fba":"markdown","f4c33dc2":"markdown","aa7586cf":"markdown","57b8c86a":"markdown","724d903d":"markdown","b1a2a2b6":"markdown","82c905d5":"markdown","ff105100":"markdown","2088445f":"markdown","393fd8e0":"markdown","8e7e80a6":"markdown","de38588b":"markdown","0d0c867b":"markdown","b04d0d9b":"markdown","1b8bd6af":"markdown","061efa0b":"markdown","2fdfdec4":"markdown","dbf5c2d6":"markdown","d7b3d162":"markdown","9d35d878":"markdown","cffd8160":"markdown","b5ae995b":"markdown","c0be66a4":"markdown","ee315cb5":"markdown","ffdf90e2":"markdown","6b39eed3":"markdown","d14c964a":"markdown","669038f3":"markdown","771812c3":"markdown","db22032e":"markdown","52fc6be7":"markdown","56d264fd":"markdown","a76aee36":"markdown","4c2f55fe":"markdown","e02f456c":"markdown","265df087":"markdown","10afd313":"markdown","a503d453":"markdown","adec5d9c":"markdown","8b2f4abc":"markdown","d9e20644":"markdown","646314a7":"markdown","29b90115":"markdown","b705a08c":"markdown","26792b2a":"markdown","e406bae4":"markdown","f45350c4":"markdown","6904bdc8":"markdown","c16f2f0b":"markdown","2956ce82":"markdown","8fe7cc80":"markdown","17b2e46b":"markdown","157f061f":"markdown","012b96cf":"markdown","f2aad4bb":"markdown","8ea3ed1e":"markdown","ced01c94":"markdown","01e173ef":"markdown","c773d1f5":"markdown","2ecabf74":"markdown","c5855e5a":"markdown","2b412984":"markdown"},"source":{"40245b9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3d45893":"retail = cv2.imread('..\/input\/cv-images\/retail-min.png', cv2.IMREAD_COLOR)\nhealthcare = cv2.imread('..\/input\/cv-images\/healthcare-min.jpg', cv2.IMREAD_COLOR)\nautomotive = cv2.imread('..\/input\/cv-images\/automotive-min.jpg', cv2.IMREAD_COLOR)\nsecurity = cv2.imread('..\/input\/cv-images\/security-min.png', cv2.IMREAD_COLOR)\nindustry = cv2.imread('..\/input\/cv-images\/industry-min.png', cv2.IMREAD_COLOR)\nfinance = cv2.imread('..\/input\/cv-images\/finance-min.jpg', cv2.IMREAD_COLOR)\n\nplt.figure(figsize=(60, 50))\nplt.subplot(3, 2, 1).set_title('Retail', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(retail, cv2.COLOR_BGR2RGB))\nplt.subplot(3, 2, 2).set_title('Health Care', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(healthcare, cv2.COLOR_BGR2RGB))\nplt.subplot(3, 2, 3).set_title('Automotive', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(automotive, cv2.COLOR_BGR2RGB))\nplt.subplot(3, 2, 4).set_title('Security', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(security, cv2.COLOR_BGR2RGB))\nplt.subplot(3, 2, 5).set_title('Industry', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(industry, cv2.COLOR_BGR2RGB))\nplt.subplot(3, 2, 6).set_title('Finance', fontsize = 65); plt.axis('off')   \nplt.imshow(cv2.cvtColor(finance, cv2.COLOR_BGR2RGB))\nplt.suptitle('Applications of Computer Vision', fontsize = 80)\nplt.show()    ","f7cac072":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline","92dc18ac":"height = 224\nwidth = 224\nfont_size = 20\npaths = ['..\/input\/cv-images\/Dog.jpg', '..\/input\/cv-images\/Cat.jpg']","5c2c0259":"plt.figure(figsize=(15, 8))\nfor i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    resized_img = cv2.resize(img, (height, width))\n    \n    plt.subplot(1, 2, i+1).set_title(name[ : -4], fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\nplt.show()","00996ecf":"plt.figure(figsize=(15, 8))\nfor i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, 0)\n    resized_img = cv2.resize(img, (height, width))\n    \n    plt.subplot(1, 2, i + 1).set_title(f'Grayscale {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(resized_img, cmap='gray')\nplt.show()","eb6a7923":"for i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    resized_img = cv2.resize(img, (height, width))\n    \n    denoised_img = cv2.medianBlur(resized_img, 5)\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Original {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n    plt.subplot(1, 2, 2).set_title(f'After Median Filtering of {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(denoised_img, cv2.COLOR_BGR2RGB))\n    plt.show()","db28ffee":"for i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, 0)\n    resized_img = cv2.resize(img, (height, width))\n    denoised_img = cv2.medianBlur(resized_img, 5)\n    \n    th = cv2.adaptiveThreshold(denoised_img, maxValue = 255, adaptiveMethod = cv2.ADAPTIVE_THRESH_GAUSSIAN_C, thresholdType = cv2.THRESH_BINARY, blockSize = 11, C = 2)\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Grayscale {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(resized_img, cmap = 'gray')\n    plt.subplot(1, 2, 2).set_title(f'After Adapative Thresholding of {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(th, cv2.COLOR_BGR2RGB))\n    plt.show()","c553d14f":"for i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, 0)\n    resized_img = cv2.resize(img, (height, width))\n    laplacian = cv2.Laplacian(resized_img, cv2.CV_64F)\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Grayscale {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(resized_img, cmap = 'gray')\n    plt.subplot(1, 2, 2).set_title(f'After finding Laplacian Derivatives of {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(laplacian.astype('float32'), cv2.COLOR_BGR2RGB))\n    plt.show()","8d2c2209":"for i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, 0)\n    resized_img = cv2.resize(img, (height, width))\n    edges = cv2.Canny(resized_img, threshold1 = 100, threshold2 = 200)\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Grayscale {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(resized_img, cmap = 'gray')\n    plt.subplot(1, 2, 2).set_title(f'After Canny Edge Detection of {name[ : -4]} Image', fontsize = font_size); plt.axis('off')\n    plt.imshow(cv2.cvtColor(edges, cv2.COLOR_BGR2RGB))\n    plt.show()","20af86af":"for i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, 0)\n    resized_img = cv2.resize(img, (height, width))\n    \n    freq = np.fft.fft2(resized_img)\n    \n    freq_shift = np.fft.fftshift(freq)\n    \n    magnitude_spectrum = 20 * np.log(np.abs(freq_shift))\n\n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Grayscale {name[ : -4]} Image', fontsize = font_size); plt.axis('off')   \n    plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n    plt.subplot(1, 2, 2).set_title(f'Magnitude Spectrum of {name[ : -4]} Image', fontsize = font_size); plt.axis('off')   \n    plt.imshow(magnitude_spectrum, cmap = 'gray')\n    plt.show()","1f93698f":"min_line_length = 100\nmax_line_gap = 10\n\nimg = cv2.imread('..\/input\/cv-images\/hough-min.png')\nresized_img = cv2.resize(img, (height, width))\nimg_copy = resized_img.copy()\n\nedges = cv2.Canny(resized_img, threshold1 = 50, threshold2 = 150)\n\nlines = cv2.HoughLinesP(edges, rho = 1, theta = np.pi \/ 180, threshold = 100, minLineLength = min_line_length, maxLineGap = max_line_gap)\n\nfor line in lines:\n    for x1, y1, x2, y2 in line:\n        hough_lines_img = cv2.line(resized_img ,(x1,y1),(x2,y2),color = (0,255,0), thickness = 2)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1).set_title('Original Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(1, 2, 2).set_title('After Hough Line Transformation', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(hough_lines_img, cv2.COLOR_BGR2RGB))\nplt.show()","fa6ac36f":"img = cv2.imread('..\/input\/cv-images\/corners-min.jpg')\nresized_img = cv2.resize(img, (height, width))\nimg_copy = resized_img.copy()\ngray = cv2.cvtColor(resized_img,cv2.COLOR_BGR2GRAY)\n\ngray = np.float32(gray)\ncorners = cv2.cornerHarris(gray, blockSize = 2, ksize = 3, k = 0.04)\n\ncorners = cv2.dilate(corners, None)\nresized_img[corners > 0.0001 * corners.max()] = [0, 0, 255]\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1).set_title('Original Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(1, 2, 2).set_title('After Harris Corner Detection', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\nplt.show()","b0b42fca":"kernel = np.ones((5,5), np.uint8)\n\nplt.figure(figsize=(15, 8))\n\nimg = cv2.imread('..\/input\/cv-images\/morph-min.jpg', cv2.IMREAD_COLOR)\nresized_img = cv2.resize(img, (height, width))\n\nmorph_open = cv2.morphologyEx(resized_img, cv2.MORPH_OPEN, kernel)\nmorph_close = cv2.morphologyEx(morph_open, cv2.MORPH_CLOSE, kernel)\n    \nplt.subplot(1,2,1).set_title('Original Digit - 7 Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\nplt.subplot(1,2,2).set_title('After Morphological Opening and Closing of Digit - 7 Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(morph_close, cv2.COLOR_BGR2RGB))\nplt.show()","244ae4d6":"pts1 = np.float32([[1550, 1170],[2850, 1370],[50, 2600],[1850, 3450]])\npts2 = np.float32([[0,0],[4160,0],[0,3120],[4160,3120]])\n\nimg = cv2.imread('..\/input\/cv-images\/book-min.jpg', cv2.IMREAD_COLOR)\n\ntransformation_matrix = cv2.getPerspectiveTransform(pts1, pts2)\n\nfinal_img = cv2.warpPerspective(img, M = transformation_matrix, dsize = (4160, 3120))\n\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, (256, 256))\nfinal_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\nfinal_img = cv2.resize(final_img, (256, 256))\n \nplt.figure(figsize=(15, 8))    \nplt.subplot(1,2,1).set_title('Original Book Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(img)\nplt.subplot(1,2,2).set_title('After Perspective Transformation of Book Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(final_img)\nplt.show()","a683bb6b":"plt.figure(figsize=(15, 8))\n\nimg = cv2.imread('..\/input\/cv-images\/contours-min.jpg', cv2.IMREAD_COLOR)\nresized_img = cv2.resize(img, (height, width))\ncontours_img = resized_img.copy()\nimg_gray = cv2.cvtColor(resized_img,cv2.COLOR_BGR2GRAY)\n\nret,thresh = cv2.threshold(img_gray, thresh = 127, maxval = 255, type = cv2.THRESH_BINARY)\n\ncontours, hierarchy = cv2.findContours(thresh, mode = cv2.RETR_TREE, method = cv2.CHAIN_APPROX_NONE)\n\ncv2.drawContours(contours_img, contours, contourIdx = -1, color = (0, 255, 0), thickness = 2)\n\nplt.subplot(1,2,1).set_title('Original Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(resized_img)\nplt.subplot(1,2,2).set_title('After Finding Contours', fontsize = font_size); plt.axis('off')   \nplt.imshow(contours_img)\nplt.show()","bd0e1857":"\nR = cv2.imread('..\/input\/cv-images\/GR-min.jpg', cv2.IMREAD_COLOR)\nR = cv2.resize(R, (224, 224))\nH = cv2.imread('..\/input\/cv-images\/H-min.jpg', cv2.IMREAD_COLOR)\nH = cv2.resize(H, (224, 224))\n\nG = R.copy()\nguassian_pyramid_c = [G]\nfor i in range(6):\n    G = cv2.pyrDown(G)\n    guassian_pyramid_c.append(G)\n \n     \nG = H.copy()\nguassian_pyramid_d = [G]\nfor i in range(6):\n    G = cv2.pyrDown(G)\n    guassian_pyramid_d.append(G)\n\nlaplacian_pyramid_c = [guassian_pyramid_c[5]]\nfor i in range(5, 0, -1):\n    GE = cv2.pyrUp(guassian_pyramid_c[i])\n    L = cv2.subtract(guassian_pyramid_c[i-1], GE)\n    laplacian_pyramid_c.append(L)\n\nlaplacian_pyramid_d = [guassian_pyramid_d[5]]\nfor i in range(5,0,-1):\n    guassian_expanded = cv2.pyrUp(guassian_pyramid_d[i])\n    L = cv2.subtract(guassian_pyramid_d[i-1], guassian_expanded)\n    laplacian_pyramid_d.append(L)\n\nlaplacian_joined = []\nfor lc,ld in zip(laplacian_pyramid_c, laplacian_pyramid_d):\n    r, c, d = lc.shape\n    lj = np.hstack((lc[:, 0 : int(c \/ 2)], ld[:, int(c \/ 2) :]))\n    laplacian_joined.append(lj)\n\nlaplacian_reconstructed = laplacian_joined[0]\nfor i in range(1,6):\n    laplacian_reconstructed = cv2.pyrUp(laplacian_reconstructed)\n    laplacian_reconstructed = cv2.add(laplacian_reconstructed, laplacian_joined[i])\n\ndirect = np.hstack((R[ : , : int(c \/ 2)], H[ : , int(c \/ 2) : ]))\n\nplt.figure(figsize=(30, 20))\nplt.subplot(2,2,1).set_title('Golden Retriever', fontsize = 35); plt.axis('off')   \nplt.imshow(cv2.cvtColor(R, cv2.COLOR_BGR2RGB))\nplt.subplot(2,2,2).set_title('Husky', fontsize = 35); plt.axis('off')   \nplt.imshow(cv2.cvtColor(H, cv2.COLOR_BGR2RGB))\nplt.subplot(2,2,3).set_title('Direct Joining', fontsize = 35); plt.axis('off')   \nplt.imshow(cv2.cvtColor(direct, cv2.COLOR_BGR2RGB))\nplt.subplot(2,2,4).set_title('Pyramid Blending', fontsize = 35); plt.axis('off')   \nplt.imshow(cv2.cvtColor(laplacian_reconstructed, cv2.COLOR_BGR2RGB))\nplt.show()","cb01635d":"lower_white = np.array([0, 0, 150])\nupper_white = np.array([255, 255, 255])\n\nimg = cv2.imread('..\/input\/cv-images\/color_space_cat.jpg', cv2.IMREAD_COLOR)\nimg = cv2.resize(img, (height, width))\n\nbackground = cv2.imread(\"..\/input\/cv-images\/galaxy.jpg\", cv2.IMREAD_COLOR)\nbackground = cv2.resize(background, (height, width))\n\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nhsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\nmask = cv2.inRange(hsv_img, lowerb = lower_white, upperb = upper_white)\n\nfinal_img = cv2.bitwise_and(img, img, mask = mask)\nfinal_img = np.where(final_img == 0, background, final_img)\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1,2,1).set_title('Original Cat Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(img)\nplt.subplot(1,2,2).set_title('After Object Tracking using Color-space Conversion of Cat Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(final_img)\nplt.show()","66320096":"img = cv2.imread('..\/input\/cv-images\/Cat.jpg', cv2.IMREAD_COLOR)\nimg = cv2.resize(img, (height, width))\nimg_copy = img.copy()\nmask = np.zeros(img.shape[ : 2], np.uint8)\n\nbackground_model = np.zeros((1,65),np.float64)\nforeground_model = np.zeros((1,65),np.float64)\n\nrect = (10, 10, 224, 224)\n\ncv2.grabCut(img, mask = mask, rect = rect, bgdModel = background_model, fgdModel = foreground_model, iterCount = 5, mode = cv2.GC_INIT_WITH_RECT)\n\nnew_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\nimg = img * new_mask[:, :, np.newaxis]\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1,2,1).set_title('Original Cat Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(1,2,2).set_title('After Interactive Foreground Extraction of Cat Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()","5200a780":"kernel = np.ones((3 , 3), np.uint8)\n\nimg = cv2.imread('..\/input\/cv-images\/lymphocytes-min.jpg', cv2.IMREAD_COLOR)\nresized_img = cv2.resize(img, (height, width))\nimg_copy = resized_img.copy()\n\ngray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray, thresh = 0, maxval = 255, type = cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\nopening = cv2.morphologyEx(thresh, op = cv2.MORPH_OPEN, kernel = kernel, iterations = 2)\nbackground = cv2.dilate(opening, kernel = kernel, iterations = 5)\n\ndist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\nret, foreground = cv2.threshold(dist_transform, thresh = 0.2  * dist_transform.max(), maxval = 255, type = cv2.THRESH_BINARY)\n\nforeground = np.uint8(foreground)\nunknown = cv2.subtract(background, foreground)\n\nret, markers = cv2.connectedComponents(foreground)\n\nmarkers = markers + 1\nmarkers[unknown == 255] = 0\n\nmarkers = cv2.watershed(resized_img, markers)\nresized_img[markers == -1] = [0, 0, 255]\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1).set_title('Lymphocytes Image', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(1, 2, 2).set_title('After Watershed Algorithm', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\nplt.show()","31adacf2":"mask = cv2.imread('..\/input\/cv-images\/mask.png',0)\nmask = cv2.resize(mask, (height, width))\n\nfor i, path in enumerate(paths):\n    \n    name = os.path.split(path)[-1]\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    resized_img = cv2.resize(img, (height, width))\n    \n    ret, th = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n    inverted_mask = cv2.bitwise_not(th)\n    damaged_img = cv2.bitwise_and(resized_img, resized_img, mask = inverted_mask)\n    \n    result = cv2.inpaint(resized_img, mask, inpaintRadius = 3, flags = cv2.INPAINT_TELEA)\n\n\n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1).set_title(f'Damaged Image of {name[ : -4]}', fontsize = font_size); plt.axis('off')   \n    plt.imshow(cv2.cvtColor(damaged_img, cv2.COLOR_BGR2RGB))\n    plt.subplot(1, 2, 2).set_title(f'After Image Inpainting of {name[ : -4]}', fontsize = font_size); plt.axis('off')   \n    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n    plt.show()","671ed6f9":"\nimg = cv2.imread('..\/input\/cv-images\/gomez_swift-min.jpg', cv2.IMREAD_COLOR)\nimg_copy = img.copy()\ntemplate = cv2.imread('..\/input\/cv-images\/gomez-min.jpg', cv2.IMREAD_COLOR)\nw, h, c = template.shape\n\nmethod = eval('cv2.TM_CCOEFF')\n\nresult = cv2.matchTemplate(img, templ = template, method = method)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\ntop_left = max_loc\n\nbottom_right = (top_left[0] + w, top_left[1] + h)\n\ncv2.rectangle(img, top_left, bottom_right, color = (255, 0, 0), thickness = 3)\n\nplt.figure(figsize=(30, 20))\nplt.subplot(2, 2, 1).set_title('Image of Selena Gomez and Taylor Swift', fontsize = 35); plt.axis('off')\nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(2, 2, 2).set_title('Face Template of Selena Gomez', fontsize = 35); plt.axis('off')\nplt.imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))\nplt.subplot(2, 2, 3).set_title('Matching Result', fontsize = 35); plt.axis('off')\nplt.imshow(result, cmap = 'gray')\nplt.subplot(2, 2, 4).set_title('Detected Face', fontsize = 35); plt.axis('off')\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()","cbb63da9":"face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n\nimg = cv2.imread('..\/input\/cv-images\/elon-min.jpg')\nimg = cv2.resize(img, (height, width))\nimg_copy = img.copy()\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nfaces = face_cascade.detectMultiScale(gray, scaleFactor = 1.3, minNeighbors = 5)\n\nfor (fx, fy, fw, fh) in faces:\n    img = cv2.rectangle(img, (fx, fy), (fx + fw, fy + fh), (255, 0, 0), 2)\n\n    roi_gray = gray[fy:fy+fh, fx:fx+fw]\n    roi_color = img[fy:fy+fh, fx:fx+fw]\n    \n    eyes = eye_cascade.detectMultiScale(roi_gray)\n    \n    for (ex, ey, ew, eh) in eyes:\n        cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n        \nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1).set_title('Elon Musk', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB))\nplt.subplot(1, 2, 2).set_title('Elon Musk - After Face and Eyes Detections', fontsize = font_size); plt.axis('off')   \nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()       ","72bac6d8":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Template Matching matches the template provided to the image in which the template must be found. The template is compared to each patch of the input image. This is similar to a 2D convolution operation. It results in a grayscale image where each pixel denotes the similarity of the neighborhood pixels to that of the template. From this output, the maximum\/minimum value is determined. This can be regarded as the top-left corner coordinates of the rectangle. By also considering the width and height of the template, the resultant rectangle is the region of the template in the image. <\/p>","d3f528e1":"<h3 style = 'font-family: Comic Sans MS;background-color:#FFD3B5;color:black;'> PLEASE UPVOTE AND SHARE IF YOU FOUND ANYTHING HELPFUL. THANK YOU IN ADVANCE! <\/h3>\n\n![1200px-Facebook_Thumb_icon.svg-min.png](attachment:6ac4ae71-b26c-403b-a2b9-3ed83d8d8416.png)","77494565":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Geometric Transformation of images is achieved by two transformation functions namely cv2.warpAffine and cv2.warpPerspective that receive a 2x3  and 3x3 transformation matrix respectively. The types of geometrics transformations are as follows. <br><br> \ud83d\udccc Scaling - Resizing the image <br> \ud83d\udccc Translation - Shifting object's location in the image <br> \ud83d\udccc Rotation - Rotation of an image by an angle theta <br> \ud83d\udccc Affine Transformation - All parallel lines remain parallel. Three points in the input image and their corresponding points in the output image are required to find the transformation matrix. <br> \ud83d\udccc Perspective Transformation - It is similar to Affine Transformation but it is four points instead of three points and three of those four points must be collinear. <br><br> Perspective Transformation is implemented below. After finding the 3x3 transformation matrix using cv2.getPerspectiveTransform, cv2.warpPerspective is applied to it to get the final transformed image. <\/p>","8eced9ea":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> This notebook intends to explore Computer Vision using OpenCV. OpenCV is a popular, free, and open-source library for Computer Vision developed by Intel. This notebook will discuss fundamental Computer Vision concepts and their implementations by playing around with the library and harnessing its functionalities. Please upvote and share the notebook if you found it helpful in any way. Thank you in advance! <br><br> Note: <br> \ud83d\udccc All images are sourced from the internet. <br> \ud83d\udccc Reference: <a href = \"https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/index.html\"> OpenCV Documentation <\/a> <\/p>\n\n![opencv-min.png](attachment:c88caf45-c74f-4ad4-939f-9d85a8774787.png)","255fe523":"## **COLORSPACE CONVERSION AND OBJECT TRACKING**","b9211fba":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Grayscale images are images that are shades of gray. It represents the degree of luminosity and carries the intensity information of pixels in the image. Black is the weakest intensity and white is the strongest intensity. Grayscale images are efficient as they are simpler and faster than color images during image processing. Some of these methods will be covered further in the notebook. <\/p>","f4c33dc2":"<h1 id = 'sec13' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \u2b55 CONTOURS \u2b55 <\/center> <\/h1>","aa7586cf":"<h1 id = 'sec14' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udd06 IMAGE PYRAMIDS \ud83d\udd06 <\/center> <\/h1>","57b8c86a":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Computer Vision is a field of Computer Science that helps computers understand and interpret the visual world around us. It makes use of the images and videos captured on cameras to develop models and techniques to process and analyze the underlying features. Computer Vision is a strong pillar of Artificial Intelligence that has wide-ranging applications in diverse domains.<\/p>","724d903d":"<h1 id = 'sec8' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83c\udf1f FOURIER TRANSFORM ON IMAGE \ud83c\udf1f <\/center> <\/h1>","b1a2a2b6":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Fourier Transform analyzes the frequency characteristics of an image. Discrete Fourier Transform is used to find the frequency domain. Fast Fourier Transform (FFT) calculates the Discrete Fourier Transform. Frequency is higher usually at the edges or wherever noise is present. When FFT is applied to the image, the high frequency is mostly in the corners of the image. To bring that to the center of the image, it is shifted by N\/2 in both horizontal and vertical directions. Finally, the magnitude spectrum of the outcome is achieved. Fourier Transform is helpful in object detection as each object has a distinct magnitude spectrum. <\/p>","82c905d5":"<h1 style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udc41 COMPUTER VISION \ud83d\udc41 <\/center> <\/h1>","ff105100":"## **IMAGE THRESHOLDING**","2088445f":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Gradients are the slope of the tangent of the graph of the function. Image gradients find the edges of a grayscale image in the x and y direction. This can be done by calculating derivates in both directions using Sobel x and Sobel y operations. These two derivatives put together give the laplacian derivatives by the following relation. <\/p>\n\n![laplacian derivative.png](attachment:ac8a8ede-07f5-483a-9d03-5ff30ad174ed.png)\n\n<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'>White to black transition is known as 'Positive Slope' and black to white transition is known as 'Negative Slope'. Some edge detections are missed when the output datatype is int8. Hence having the output datatype as float64 is a better option for all edge detections. <\/p> ","393fd8e0":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Object Detection is done using Haar Cascades. It uses a Machine Learning approach that trains on positive and negative examples. Then it extracts features like convolutional kernels. The best features of those large number of features are filtered that may be irrelevant which is achieved by Adaboost. There is a threshold for each feature for classification. After each classification, weights of misclassified images are increased and new error rates are determined. This process is continued until accuracy or error rate or the number of features is achieved. The final classifier is the weighted sum of weak classifiers that are not competent enough to function on their own. Using Cascade of Classifiers, the features are grouped into different stages. Hence, if the window fails a particular stage, it is not considered for further stages. Once the face is found, that rectangular portion is only passed to the eye detector instead of the whole image. ","8e7e80a6":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Hough Transform can detect any shape even if it is distorted when presented in mathematical form. A line in the cartesian coordinate system y = mx + c can be put in its polar coordinate system as rho = xcos\u03b8 + ysin\u03b8. rho is the perpendicular distance from the origin to the line and \u03b8 is the angle formed by the horizontal axis and the perpendicular line in the clockwise direction. So, the line is represented in these two terms (rho, \u03b8). An array is created for these two terms where rho forms the rows and \u03b8 forms the columns. This is called the accumulator. rho is the distance resolution of the accumulator in pixels and \u03b8 is the angle resolution of the accumulator in radians. For every line, its (x, y) values can be put into (rho, \u03b8) values. For every (rho, \u03b8) pair, the accumulator is incremented. This is repeated for every point on the line. A particular (rho, \u03b8) cell is voted for the presence of a line. This way the cell with the maximum votes implies a presence of a line at rho distance from the origin and at angle \u03b8 degrees. <\/p>","de38588b":"## **RGB IMAGE AND RESIZING**","0d0c867b":"## **CONTOURS**","b04d0d9b":"## **IMAGE PYRAMIDS**","1b8bd6af":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Harris Corner finds the difference in intensity for a displacement in all directions to detect a corner. This mathematical equation is given below. <\/p>\n\n![1-min.png](attachment:8d00da39-62f6-41ff-a00d-4dbd174cf8b5.png)\n\n<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Window function may be a rectangular window or Gaussian window. E(u, v) has to be maximized for corner detection which means the second term has to be maximized. By applying Taylor's Expansion to the above equation, we get the following equation. Ix and Iy are derivatives in the horizontal and vertical directions that can be obtained by Sobel operations. <\/p> \n\n![2-min.png](attachment:87bf2aaa-601c-4f7d-a392-459d070d8460.png)\n\n<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Finally, the equation below decides whether a corner is detected or not. <\/p> \n\n![3-min.png](attachment:b7c4b8d7-410e-4d9c-b692-5d3e5ede2e02.png)\n\n![harris_corner-min.jpg](attachment:281f06aa-7d44-45b6-9fb9-8f1e4385d35f.jpg)","061efa0b":"<h1 id = 'sec9' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \u270f LINE TRANSFORM \u270f <\/center> <\/h1>","2fdfdec4":"<h1 id = 'sec7' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udd0d EDGE DETECTION \ud83d\udd0d <\/center> <\/h1>","dbf5c2d6":"## **IMPORT LIBRARIES**","d7b3d162":"<h1 style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udd17 CONTENT \ud83d\udd17 <\/center> <\/h1>","9d35d878":"## **MORPHOLOGICAL TRANSFORMATION OF IMAGE**","cffd8160":"## **INTERACTIVE FOREGROUND EXTRACTION**","b5ae995b":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Images have a resolution which is the measure of the information in the image. In certain scenarios of image processing like Image Blending, working with images of different resolutions is necessary to make the blend look more realistic. In OpenCV, images of high resolution can be converted to low resolution and vice-versa. By converting a higher-level image to a lower-level image, the lower-level image becomes 1\/4th the area of the higher-level image. When this is done for a number of iterations and the resultant images are placed next to each other in order, it looks like it is forming a pyramid and hence its name 'Image Pyramid'. There are two types of Pyramids as follows. <br><br> \ud83d\udccc Gradient Pyramids <br> \ud83d\udccc Laplacian Pyramids <br><br> A level in the Laplacian pyramids is formed by the difference between that level in the Gaussian Pyramid and the expanded version of its upper level in the Gaussian Pyramid. <br><br> Steps for Image Blending are as follows. <br> \ud83d\udccc Load both images <br> \ud83d\udccc Find Gaussian Pyramids for both images <br> \ud83d\udccc Find Laplacian Pyramids for both images <br> \ud83d\udccc Join the halves of both images on either side in each level of Laplacian Pyramids <br> \ud83d\udccc Reconstruct image <\/p>","c0be66a4":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> The foreground of the image is extracted using user input and Gaussian Mixture Model (GMM). Steps to extract the foreground of an image are as follows. <br><br> \ud83d\udccc User Input - User provides a rectangular region where all pixels outside of it are sure background and everything inside is unknown <br> \ud83d\udccc Initial Labelling - Computer performs initial labeling based on the given labels <br> \ud83d\udccc GMM - Models the foreground and the background <br> \ud83d\udccc GMM creates new pixel distribution - Based on the data, the model predicts a probable foreground or background for the unknown pixels (Similar to Clustering) <br> \ud83d\udccc Build Graph - A graph is built for all the pixels where all the foreground pixels are connected to Source Node and all background pixels are connected to Sink Node. The weights between the pixels represent the similarity between the connected nodes. The weight of the edges connected to Source or Sink Node is defined by the probability of the pixel being a foreground or background pixel <br> \ud83d\udccc Mincut Algorithm - It separates the graph into two segments each containing the Source and the Sink Node with Minimum Cost Function. The cost is the sum of all the weights of the edges in the segment. <br> \ud83d\udccc Repeat - Repeat a number of iterations until the classification converges <br><br> GrabCut modifies the mask. Pixels are marked with four flags where 0 and 2 mean background and 1 and 3 mean foreground. <\/p>","ee315cb5":"<h1 id = 'sec18' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83c\udfa8 IMAGE INPAINTING \ud83c\udfa8 <\/center> <\/h1>","ffdf90e2":"<h1 id = 'sec20' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\ude03 FACE AND EYE DETECTION \ud83d\udc40 <\/center> <\/h1>","6b39eed3":"## **GRAYSCALE IMAGE**","d14c964a":"<h1 id = 'sec6' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udc9a IMAGE GRADIENTS \ud83d\udc9c <\/center> <\/h1>","669038f3":"<h1 id = 'sec19' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udc69 TEMPLATE MATCHING \ud83d\udc69 <\/center> <\/h1>","771812c3":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Contours are outlines representing the shape or form of objects in an image. They are useful in object detection and recognition. Binary images produce better contours. There are separate functions for finding and drawing contours. <\/p>","db22032e":"<h1 id = 'sec15' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83e\udde1 COLORSPACE CONVERSION AND OBJECT TRACKING \ud83d\udc9b <\/center> <\/h1>","52fc6be7":"<h1 id = 'sec3' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \u26ab GRAYSCALE IMAGE \u26aa <\/center> <\/h1>","56d264fd":"## **EDGE DETECTION**","a76aee36":"<h1 id = 'sec11' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83e\udd8b MORPHOLOGICAL TRANSFORMATION OF IMAGE  \ud83e\udd8b <\/center> <\/h1>","4c2f55fe":"## **FOURIER TRANSFORM ON IMAGE**","e02f456c":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> In Colorspace Conversion, BGR\u2194Gray, BGR\u2194HSV conversions are possible. The BGR\u2194Gray conversion was previously seen. HSV stands for Hue, Saturation, and Value respectively. Since HSV describes images in terms of their hue, saturation, and value instead of RGB where R, G, B are all co-related to color luminance, object discrimination is much easier with HSV images than RGB images. <br><br>\nSteps for Object Tracking are listed below: <br> \ud83d\udccc Convert Image from BGR to HSV <br> \ud83d\udccc Threshold the HSV image to extract the pixels that are in a range of one's color of choice (Color White is chosen to extract the cat in the implementation below) <br> \ud83d\udccc The remaining unextracted pixels are filled by pixels from another image (maybe background image) of one's choice \/ Anything can be done with the extracted object [Optional] <\/p>","265df087":"## **IMAGE SEGMENTATION**","10afd313":"## **LINE TRANSFORM**","a503d453":"<h1 id = 'sec10' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udd32 CORNER DETECTION \ud83d\udd33 <\/center> <\/h1>","adec5d9c":"<h1 id = 'sec16' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\ude3a INTERACTIVE FOREGROUND EXTRACTION \ud83d\ude3a <\/center> <\/h1>","8b2f4abc":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Image Thresholding is self-explanatory. If the pixel value in an image is above a certain threshold, a particular value is assigned and if it is below the threshold, another particular value is assigned. There are many thresholding styles as follows. <\/p>\n\n![threshold-min.png](attachment:bd8e0868-b1ee-4663-a47a-aefd61c5fe79.png)\n\n<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Adaptive Thresholding does not have global threshold values. Instead, a threshold is set for a small region of the image. Hence, there are different thresholds for the entire image and they produce greater outcomes for dissimilar illumination. There are different Adaptive Thresholding methods as follows <br><br> \ud83d\udccc cv2.ADAPTIVE_THRESH_MEAN_C - Threshold is the mean of the neighborhood area values <br> \ud83d\udccc cv2.ADAPTIVE_THRESH_GAUSSIAN_C - Threshold is the weighted sum of the neighborhood area values <br><br> Adaptive Thresholding is implemented below.<\/p>","d9e20644":"## **FACE AND EYE DETECTION**","646314a7":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Morphological Transformation is usually applied on binary images where it takes an image and a kernel which is a structuring element as inputs. Binary images may contain imperfections like texture and noise. These transformations help in correcting these imperfections by accounting for the form of the image. The types of morphological transformation are as follows. <br><br> \ud83d\udccc Erosion - Erodes boundaries of the foreground object <br> \ud83d\udccc Dilation - Opposite of Erosion <br> \ud83d\udccc Opening - Erosion followed by Dilation <br> \ud83d\udccc Closing - Dilation followed by Erosion <br> \ud83d\udccc Morphological Gradient - Difference between Dilation and Erosion <br> \ud83d\udccc Top Hat - Difference between the input image and Opening of the image <br> \ud83d\udccc Black Hat - Difference between Closing of the image and input image <br><br> Morphological Opening and Closing are implemented below. <\/p>\n    ","29b90115":"<h1 id = 'sec12' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83c\udfb2 GEOMETRIC TRANSFORMATION OF IMAGE  \ud83c\udfb2 <\/center> <\/h1>","b705a08c":"<h1 id = 'sec2' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83c\udf08 RGB IMAGE AND RESIZING \ud83c\udf08 <\/center> <\/h1>","26792b2a":"<h1 id = 'sec4' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udd07 IMAGE DENOISING \ud83d\udd07 <\/center> <\/h1>","e406bae4":"## **CORNER DETECTION**","f45350c4":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Image denoising removes noise from the image. It is also known as 'Image Smoothing'. The image is convolved with a low pass filter kernel which gets rid of high-frequency content like edges of an image. There are many image blurring techniques as follows.\n<br><br> \ud83d\udccc Averaging - Image blurring where central element is replaced by the average of all pixels in kernel area <br> \ud83d\udccc Gaussian Filtering - Image blurring using Gaussian function <br> \ud83d\udccc Median Filtering - Image blurring where central element is replaced by the median of all pixels in kernel area <br> \ud83d\udccc Bilateral Filtering - Image blurring where the intensity of each pixel is replaced by the weighted average of intensity values from nearby pixels <br><br> Median Filtering is employed in the implementation below. Since all the central elements are replaced by pixel values that were already present in the image, it effectively reduces salt and pepper noise. \n<\/p>","6904bdc8":"## **GEOMETRIC TRANSFORMATION OF IMAGE**","c16f2f0b":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Edge Detection is performed using Canny Edge Detection which is a multi-stage algorithm. The stages to achieve edge detection are as follows. <br><br> \ud83d\udccc Noise Reduction - Smoothen image using Gaussian filter <br> \ud83d\udccc Find Intensity Gradient - Using the Sobel kernel, find the first derivative in the horizontal (Gx) and vertical (Gy) directions. The edge gradient is determined using the relation below. <\/p>\n\n![edge gradient.png](attachment:07ce532a-a222-4d3f-8668-5c1c6f57a248.png)\n\n<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> \ud83d\udccc Non-maximum Suppression - Remove unwanted pixels that may not be edges by checking if it is a local maximum in its neighborhood in the direction of the gradient <br> \ud83d\udccc Hysteresis Thresholding - Decide whether all edges are really edges. There are two thresholds minVal and maxVal. Edges above the maxVal are sure edges and those below the minVal are not edges. The edges in between are edges if they are connected to a sure edge and not edges otherwise. <\/p>","2956ce82":"## **IMAGE GRADIENTS**","8fe7cc80":"<h1 id = 'sec17' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83c\udf70 IMAGE SEGMENTATION \ud83c\udf70 <\/center> <\/h1>","17b2e46b":"<h1 id = 'sec1' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udcc3 IMPORT LIBRARIES \ud83d\udcc3 <\/center> <\/h1>","157f061f":"## **IMAGE INPAINTING**","012b96cf":"## **TEMPLATE MATCHING**","f2aad4bb":"## **IMAGE DENOISING**","8ea3ed1e":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> An RGB image where RGB indicates Red, Green, and Blue respectively can be considered as three images stacked on top of each other. It also has a nickname called 'True Color Image' as it represents a real-life image as close as possible and is based on human perception of colors. The RGB color model is used to display images on cameras, televisions, and computers. <br><br> Resizing all images to a particular height and width will ensure uniformity and thus makes processing them easier since images are naturally available in different sizes. If the size is reduced, though the processing is faster, data might be lost in the image. If the size is increased, the image may appear fuzzy or pixelated. Additional information is usually filled using interpolation. <\/p>","ced01c94":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Images may be damaged and require fixing. For example, an image may have no pixel information in certain portions. Image Inpainting will fill all the missing information with the help of the surrounding pixels. There are two algorithms for performing Image Inpainting <br><br> \ud83d\udccc An Image Inpainting Technique Based on the Fast Marching Method - Fast Marching Method (Explained below) <br> \ud83d\udccc Navier-Stokes, Fluid Dynamics, and Image and Video Inpainting - Inspired by partial differential equations <br> <br> \nIt starts by filling the portion near the boundaries and then gradually fills the inside. It takes a small neighborhood of the pixel that is to be inpainted and calculates the weighted sum of all those neighborhood pixels as the fill value. Obviously, the pixels that are nearer to the target pixel are given a higher weightage than those far away. Once a pixel is filled, it repeats this process for all remaining pixels using the Fast Marching Method (FMM). FMM paints the pixels near to the known pixels first like a manual heuristic. FFM is implemented below. <\/p>","01e173ef":"1. [IMPORT LIBRARIES](#sec1) <br>\n2. [RGB IMAGE AND RESIZING](#sec2) <br>\n3. [GRAYSCALE IMAGE](#sec3) <br>\n4. [IMAGE DENOISING](#sec4) <br>\n5. [IMAGE THRESHOLDING](#sec5) <br>\n6. [IMAGE GRADIENTS](#sec6) <br>\n7. [EDGE DETECTION](#sec7) <br>\n8. [FOURIER TRANSFORM ON IMAGE](#sec8) <br>\n9. [LINE TRANSFORM](#sec9) <br>\n10. [CORNER DETECTION](#sec10) <br>\n11. [MORPHOLOGICAL TRANSFORMATION OF IMAGE](#sec11) <br>\n12. [GEOMETRIC TRANSFORMATION OF IMAGE](#sec12) <br>\n13. [CONTOURS](#sec13) <br>\n14. [IMAGE PYRAMIDS](#sec14) <br>\n15. [COLORSPACE CONVERSION AND OBJECT TRACKING](#sec15) <br>\n16. [INTERACTIVE FOREGROUND EXTRACTION](#sec16) <br>\n17. [IMAGE SEGMENTATION](#sec17) <br>\n18. [IMAGE INPAINTING](#sec18) <br>\n19. [TEMPLATE MATCHING](#sec19) <br>\n20. [FACE AND EYE DETECTION](#sec20) <br>","c773d1f5":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> ","2ecabf74":"<p style = 'background-color:black;color:white;font-family:Georgia;font-size:18px'> Image Segmentation is done using the Watershed Algorithm. This algorithm treats the grayscale image as hills and valleys representing high and low-intensity regions respectively. If these valleys are filled with colored water and as the water rises, depending on the peaks, different valleys with different colored water will start to merge. To avoid this, barriers can be built which gives the segmentation result. This is the concept of the Watershed algorithm. This is an interactive algorithm as one can specify which pixels belong to an object or background. The pixels that one is unsure about can be marked as 0. Then the watershed algorithm is applied on this where it updates the labels given and all the boundaries are marked as -1. The steps for Image Segmentation using the Watershed algorithm are as follows. <br><br> \ud83d\udccc Thresholding - To differentiate individual objects from the background <br> \ud83d\udccc Morphological Opening - Remove small white noises in the image <br> \ud83d\udccc Dilation - To clearly identify the background of the image <br> \ud83d\udccc Distance Transform - To clearly identify the objects of the image <br> \ud83d\udccc Thresholding <br> \ud83d\udccc Finding boundaries - Subtract foreground from background <br> \ud83d\udccc Use Connected Components - It labels all the background pixels as 0 and other objects as any integer starting from 1 <br> \ud83d\udccc Label region correctly - Since marking the background as 0 will be perceived as unknown by the Watershed algorithm. The background is marked as 1 and the unknown region is marked as  0. <br><br> Note: Distance Transform calculates the distance of a pixel from the nearest obstacle pixel. <\/p>","c5855e5a":"<h1 style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \ud83d\udcdd DESCRIPTION \ud83d\udcdd <\/center> <\/h1>","2b412984":"<h1 id = 'sec5' style = 'background-color:purple;color:white;font-family:COMIC SANS MS'> <center> \u23f2 IMAGE THRESHOLDING \u23f2 <\/center> <\/h1>"}}