{"cell_type":{"8888465c":"code","2619c548":"code","2e4c0c00":"code","167b8b30":"code","8f62df7c":"code","a30479fd":"code","b63ce939":"code","e8bb1340":"code","65d2b732":"code","f2a03991":"code","8e8fd56a":"code","141b2930":"code","c1943d74":"code","a5c776fc":"code","56670166":"code","3f48b920":"code","63935532":"code","13c3fbb9":"code","0d00854b":"code","036776d0":"code","769cb900":"code","6238719c":"code","97b913a8":"code","1a07d587":"code","e08906dd":"code","ce199c2e":"code","1e105e7d":"code","7e4c98a1":"code","c7f20c02":"code","fed347a4":"code","53dc18c9":"code","369774f7":"code","265d1c27":"code","7f35689c":"code","d50d1bcb":"code","f9678a6f":"code","5aec9e35":"code","a9aeac4e":"code","d2c1ec9f":"code","1faa5e63":"code","409883be":"code","0e18af7f":"code","4db444bd":"code","63b212e4":"code","885f0ac2":"code","c79d1f09":"code","ab1c0212":"code","0499d12c":"code","9df601f5":"code","96e78bc8":"code","8f89c559":"code","ea0f0d8e":"code","88011277":"code","794f4888":"code","da447635":"code","6a13c4bb":"code","55d2fb79":"code","e6a11edb":"code","0e66c3ed":"code","b4e2629b":"code","b25945f4":"code","aa42e4d7":"code","accad631":"code","3d9b0f6d":"code","761664c0":"code","72f14203":"code","ee8357ec":"code","d12554ec":"code","ca2c7ac4":"code","4658b7a0":"code","5f757153":"code","da705edb":"code","ba74c13f":"code","1470219b":"code","3479fcf7":"code","cefacfc4":"code","96c07952":"code","42a7bdd0":"code","cc6feed5":"code","ed85f566":"code","d64570e6":"code","741b5ba4":"code","ae00eae1":"code","0fa0abe7":"code","19bbf2e1":"code","5656839a":"markdown","d7f61e2c":"markdown","e02c0c42":"markdown","e1ae0d8e":"markdown","72b41e58":"markdown","192741f1":"markdown","7dffdd0e":"markdown","9a3dc706":"markdown"},"source":{"8888465c":"#About RMS Titanic\n# RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean \n# in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to \n# New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of \n# modern history's deadliest peacetime commercial marine disasters. \n# Wikipedia link: https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic","2619c548":"# Import Packages\nimport pandas as pd                                                       # Pandas package for reading csv files\nimport numpy as np                                                        # Numpy package for computing\nimport matplotlib.pyplot as plt                                           # Visualization package\n%matplotlib inline\nimport seaborn as sns                                                     # Visualization package","2e4c0c00":"import os\n#Reading Titanic Test given Data Set.\nimport os\nfor dirname, _, filenames in os.walk('kaggle\/input\/titanic_test.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","167b8b30":"import os\n#Reading Titanic Train given Data Set.\nimport os\nfor dirname, _, filenames in os.walk('kaggle\/input\/titanic_train.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8f62df7c":"# Load given titanic train and test data set.\ntitanic_train = pd.read_csv('\/kaggle\/input\/titanic_train.csv')                             # Reading data using simple Pandas\ntitanic_test = pd.read_csv('\/kaggle\/input\/titanic_test.csv')                             # Reading data using simple Pandas","a30479fd":"#Checking the first 5 rows\ntitanic_train.head(2)","b63ce939":"titanic_test.head(2)","e8bb1340":" # Checking the shape of the train dataframe\ntitanic_train.shape","65d2b732":" # Checking the shape of the test dataframe\ntitanic_test.shape","f2a03991":"# Describe method is used to view some basic statistical details like percentile, mean, std etc. of a data frame of numeric values.\ntitanic_train.describe()","8e8fd56a":"titanic_test.describe()","141b2930":"# Info method is used to get a concise summary of the dataframe.\ntitanic_train.info()","c1943d74":"# this information shows null value percentage\ntitanic_train.isnull().sum()\/1309*100","a5c776fc":"# we have 418 null values for survived column and those are related to test data set which can ignore it.\n# Rest of other columns Age,Cabin,Embarked and Fare column we need to fill those values.\ntitanic_train.isnull().sum()","56670166":"# Verify 'Embarked' column data count\ntitanic_train['Embarked'].value_counts()","3f48b920":"# Verify 'Embarked' column data count\ntitanic_test['Embarked'].value_counts()","63935532":"# Picking the mode value of Embarked column\ntitanic_train['Embarked'].mode()","13c3fbb9":"titanic_test['Embarked'].mode()","0d00854b":"# Filling with mode value to the missing value of Embarked column\ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna('S')\ntitanic_train['Embarked'].value_counts()","036776d0":"# Filling with mode value to the missing value of Embarked column\ntitanic_test['Embarked'] = titanic_test['Embarked'].fillna('S')\ntitanic_test['Embarked'].value_counts()","769cb900":"# Verifying the mean Age from the given dataset who were traveling by Pclass.\ntitanic_train.groupby('Pclass').mean()['Age']","6238719c":"# Verifying the mean Age from the given dataset who were traveling by Pclass.\ntitanic_test.groupby('Pclass').mean()['Age']","97b913a8":"# Verifying the mean Fare from the given dataset who were traveling by Pclass.\ntitanic_train.groupby('Pclass').mean()['Fare']","1a07d587":"# Verifying the mean Fare from the given dataset who were traveling by Pclass.\ntitanic_test.groupby('Pclass').mean()['Fare']","e08906dd":"# Verify 'Sex' column data count\ntitanic_train['Sex'].value_counts()","ce199c2e":"# Verify 'Sex' column data count\ntitanic_test['Sex'].value_counts()","1e105e7d":"# Verifying the mean value of Sex column\ntitanic_train.groupby('Sex').mean()","7e4c98a1":"# Verifying the mean value of Sex column\ntitanic_test.groupby('Sex').mean()","c7f20c02":"# Verify the title assinged to each and every passenger.\ntitanic_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False).unique()","fed347a4":"# Verify the title assinged to each and every passenger.\ntitanic_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False).unique()","53dc18c9":"# Introduce new column named as 'Title' and perform one-hot encoding by filling with nominal values.\ndata = [titanic_train]\ntitles = {\"Mr\": 1, \"Mrs\": 2, \"Miss\": 3, \"Master\": 4}\n\nfor dataset in data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Dr', 'Mme',\\\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Jonkheer'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].map(titles)\n    dataset['Title'] = dataset['Title'].fillna(5)  # Unknown persons\n\ntitanic_train = titanic_train.drop(['Name'], axis=1)\ntitanic_train.head(2)","369774f7":"# Introduce new column named as 'Title' and perform one-hot encoding by filling with nominal values.\ndata = [titanic_test]\ntitles = {\"Mr\": 1, \"Mrs\": 2, \"Miss\": 3, \"Master\": 4}\n\nfor dataset in data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Dr', 'Mme',\\\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Jonkheer'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].map(titles)\n    dataset['Title'] = dataset['Title'].fillna(5)  # Unknown persons\n\ntitanic_test = titanic_test.drop(['Name'], axis=1)\ntitanic_test.head(2)","265d1c27":"# Convert Title datatype from float to int.\ntitanic_train['Title'] = titanic_train['Title'].apply(np.int64)\ntitanic_train.head(2)","7f35689c":"# Convert Title datatype from float to int.\ntitanic_test['Title'] = titanic_test['Title'].apply(np.int64)\ntitanic_test.head(2)","d50d1bcb":"# Verify Sex (male and female) count based on Title.\ntitanic_train.groupby(by=['Title','Sex']).size()","f9678a6f":"# Verify Sex (male and female) count based on Title.\ntitanic_test.groupby(by=['Title','Sex']).size()","5aec9e35":"# Verify Age count based on Title.\ntitanic_train.groupby('Title').count()['Age']","a9aeac4e":"# Verify Age count based on Title.\ntitanic_test.groupby('Title').count()['Age']","d2c1ec9f":"# Verify mean Age value based on Title and Sex.\ntitanic_train.groupby(by=['Title', 'Sex'])['Age'].mean()","1faa5e63":"# total number of missing Age value count is 263.\ntitanic_train['Age'].isnull().sum()","409883be":"# Below apply_age function is used to assign missing age value calculate w.r.t to mean age value based on Title and Sex.\n# To maintain accuracy of missing Age value. In real time it is very difficult to calculate missing Age value in case of\n# DateOfBirth value column details were missing in the given dataset. To maintain consistency written below function.\ndef apply_age(title,sex):\n    if(title==1 and sex=='male'):\n        age=32\n    elif (title==2 and sex=='female'):\n        age=36\n    elif (title==3 and sex=='female'):\n        age=22\n    elif (title==4 and sex=='male'):\n        age=5\n    elif (title==5 and sex=='male'):\n        age=46\n    elif (title==5 and sex=='female'):\n        age=34\n    else:\n        age=30 # mean age considered from describe()\n    return age\n\n#print(apply_age(1,'male'))\n        \n","0e18af7f":"# Filling missing values of age column.\nage_nulldata=titanic_train[titanic_train['Age'].isnull()]\nage_nulldata['Age'] = age_nulldata.apply(lambda row : apply_age(row['Title'],row['Sex']), axis = 1) \n#age_nulldata['Age']\ntitanic_train['Age'].fillna(value=age_nulldata['Age'],inplace=True)","4db444bd":"#Verify whether is their any null values exists in Age column or not.\ntitanic_train['Age'].isnull().sum()","63b212e4":"# Verify Age count based on Title.\ntitanic_train.groupby('Title').count()['Age']","885f0ac2":"#Checking the first 5 rows\ntitanic_train.head(2)","c79d1f09":"# Verify mean Age value based on Title and Sex.\ntitanic_test.groupby(by=['Title', 'Sex'])['Age'].mean()","ab1c0212":"# total number of missing Age value count is 263.\ntitanic_test['Age'].isnull().sum()","0499d12c":"# Below apply_age function is used to assign missing age value calculate w.r.t to mean age value based on Title and Sex.\n# To maintain accuracy of missing Age value. In real time it is very difficult to calculate missing Age value in case of\n# DateOfBirth value column details were missing in the given dataset. To maintain consistency written below function.\ndef apply_age_test(title,sex):\n    if(title==1 and sex=='male'):\n        age=32\n    elif (title==2 and sex=='female'):\n        age=39\n    elif (title==3 and sex=='female'):\n        age=22\n    elif (title==4 and sex=='male'):\n        age=7\n    elif (title==5 and sex=='male'):\n        age=45\n    elif (title==5 and sex=='female'):\n        age=39\n    else:\n        age=30 # mean age considered from describe()\n    return age\n\n#print(apply_age(1,'male'))\n        ","9df601f5":"# Filling missing values of age column.\nage_nulldata1=titanic_test[titanic_test['Age'].isnull()]\nage_nulldata1['Age'] = age_nulldata1.apply(lambda row : apply_age_test(row['Title'],row['Sex']), axis = 1) \n#age_nulldata['Age']\ntitanic_test['Age'].fillna(value=age_nulldata1['Age'],inplace=True)","96e78bc8":"#Verify whether is their any null values exists in Age column or not.\ntitanic_test['Age'].isnull().sum()","8f89c559":"titanic_train['Embarked'].value_counts()","ea0f0d8e":"# perform one-hot encoding to Embarked column by filling with nominal values.\ndata = [titanic_train]\nembark_titles = {\"Q\": 1, \"C\": 2, \"S\": 3}\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(embark_titles)\n\ntitanic_train.head(2)","88011277":"# perform one-hot encoding to Embarked column by filling with nominal values.\ndata = [titanic_test]\nembark_titles = {\"Q\": 1, \"C\": 2, \"S\": 3}\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(embark_titles)\n\ntitanic_test.head(2)","794f4888":"# Drop unused columns 'Ticket' and 'Cabin' which were not required to trian the model.\ntitanic_train = titanic_train.drop(['Ticket'], axis=1)\ntitanic_train = titanic_train.drop(['Cabin'], axis=1)","da447635":"# Drop unused columns 'Ticket' and 'Cabin' which were not required to trian the model.\ntitanic_test = titanic_test.drop(['Ticket'], axis=1)\ntitanic_test = titanic_test.drop(['Cabin'], axis=1)","6a13c4bb":"# Identified one missing value in 'Fare' column and filled with taking median value based on PClass to calculate accurately.\ntitanic_train['Fare'] = titanic_train.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","55d2fb79":"# Identified one missing value in 'Fare' column and filled with taking median value based on PClass to calculate accurately.\ntitanic_test['Fare'] = titanic_test.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","e6a11edb":"# Convert Fare and Age Column datatype from float to int datattype.\ntitanic_train['Fare'] = titanic_train['Fare'].astype(int)\ntitanic_train['Age'] = titanic_train['Age'].astype(int)\n","0e66c3ed":"# Convert Fare and Age Column datatype from float to int datattype.\ntitanic_test['Fare'] = titanic_test['Fare'].astype(int)\ntitanic_test['Age'] = titanic_test['Age'].astype(int)","b4e2629b":"# perform one-hot encoding to Sex column by filling with nominal values.\ndata = [titanic_train]\nsex_titles = {\"male\": 0, \"female\": 1}\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(sex_titles)\n\ntitanic_train.head(2)","b25945f4":"# perform one-hot encoding to Sex column by filling with nominal values.\ndata = [titanic_test]\nsex_titles = {\"male\": 0, \"female\": 1}\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(sex_titles)\n\ntitanic_test.head(2)","aa42e4d7":"#family size\ntitanic_train['Family_Size'] = titanic_train['SibSp'] + titanic_train['Parch'] +1\ntitanic_train.head(2)","accad631":"#family size\ntitanic_test['Family_Size'] = titanic_test['SibSp'] + titanic_test['Parch'] +1\ntitanic_test.head(2)","3d9b0f6d":"for dataset in [titanic_train]:\n    dataset.loc[dataset['Age']<=16,'Age']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32),'Age']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48),'Age']=2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<=64),'Age']=3\n    dataset.loc[(dataset['Age']>64) & (dataset['Age']<=80),'Age']=4","761664c0":"# Drop unused columns 'Ticket' and 'Cabin' which were not required to trian the model.\ntitanic_train = titanic_train.drop(['PassengerId'], axis=1)\ntitanic_train = titanic_train.drop(['SibSp'], axis=1)\ntitanic_train = titanic_train.drop(['Parch'], axis=1)","72f14203":"titanic_train.head(5)","ee8357ec":"titanic_train.shape","d12554ec":"for dataset in [titanic_test]:\n    dataset.loc[dataset['Age']<=16,'Age']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32),'Age']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48),'Age']=2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<=64),'Age']=3\n    dataset.loc[(dataset['Age']>64) & (dataset['Age']<=80),'Age']=4","ca2c7ac4":"# Drop unused columns 'Ticket' and 'Cabin' which were not required to trian the model.\ntitanic_test = titanic_test.drop(['PassengerId'], axis=1)\ntitanic_test = titanic_test.drop(['SibSp'], axis=1)\ntitanic_test = titanic_test.drop(['Parch'], axis=1)","4658b7a0":"# Checking the shape of the final test dataframe\ntitanic_test.shape","5f757153":"# Checking the shape of the final test dataframe\ntitanic_train.shape","da705edb":"#Import basic packages\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, log_loss\n%matplotlib inline\n\n# Importing Models\nfrom sklearn import model_selection, metrics                                    \nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n#from xgboost import XGBClassifier\n#from lightgbm import LGBMClassifier\n\n# Importing other tools\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix, classification_report, make_scorer\nfrom sklearn.metrics import accuracy_score, recall_score, precision_recall_curve\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\nfrom sklearn.preprocessing import StandardScaler\n","ba74c13f":"# Seperate target variable and prepare X and y data to train your model on training dataset.\nX = titanic_train.drop(['Survived'],axis=1)\nY = titanic_train['Survived']","1470219b":"\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report","3479fcf7":"\nclf_xgb = XGBClassifier()\n\nparams = {\"n_estimators\" : [500, 700, 1000],\n          \"learning_rate\" : [0.005, 0.1],\n          \"max_depth\" : [5, 7],\n          \"max_features\" : [3, 5], \n          \"gamma\" : [0.5, 0.6, 0.7]}\n\ngsc_xgb = GridSearchCV(clf_xgb, params, cv = StratifiedShuffleSplit(n_splits = 5, \n                                                                    test_size = 0.3, \n                                                                    random_state = 15)) \ngsc_xgb = gsc_xgb.fit(X, Y)\n\nprint(gsc_xgb.best_estimator_)\nclf_xgb = gsc_xgb.best_estimator_\nclf_xgb.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_xgb.score(X, Y), 4)))\n\nY_predicted_xgb = clf_xgb.predict_proba(X)[:, 1]","cefacfc4":"\nclf_boost = GradientBoostingClassifier()\n\nparams = {\"n_estimators\" : [300, 500, 700],\n          \"learning_rate\" : [0.002, 0.01, 0.05],\n          \"max_depth\" : [3, 5],\n          \"max_features\" : [7, 9]}\n\ngsc_boost = GridSearchCV(clf_boost, params, cv = StratifiedShuffleSplit(n_splits = 5, \n                                                                        test_size = 0.3, \n                                                                        random_state = 15)) \ngsc_boost = gsc_boost.fit(X, Y)\n\nprint(gsc_boost.best_estimator_)\nclf_boost = gsc_boost.best_estimator_\nclf_boost.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_boost.score(X, Y), 4)))\n\nY_predicted_boost = clf_boost.predict_proba(X)[:, 1]","96c07952":"clf_log = LogisticRegression()\n\nparams = {\"C\" : [0.001, 0.01, 0.1, 1, 1.1, 10],\n          \"max_iter\" : [10000],\n          \"solver\" : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]}\n\ngsc_log = GridSearchCV(clf_log, params, cv = StratifiedShuffleSplit(n_splits = 10, \n                                                                    test_size = 0.3, \n                                                                    random_state = 15)) \ngsc_log = gsc_log.fit(X, Y)\n\nprint(gsc_log.best_estimator_)\nclf_log = gsc_log.best_estimator_\nclf_log.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_log.score(X, Y), 4)))\n\nY_predicted_log = clf_log.predict_proba(X)[:, 1]","42a7bdd0":"clf_rf = RandomForestClassifier(criterion = \"gini\", \n                                max_features = \"auto\")\n\nparams = {\"max_depth\" : [1, 3, 5, 7, 9],\n          \"n_estimators\" : [150, 200, 250, 300]}\n\ngsc_rf = GridSearchCV(clf_rf, params, cv = StratifiedShuffleSplit(n_splits = 5, \n                                                                  test_size = 0.3, \n                                                                  random_state = 15))\ngsc_rf = gsc_rf.fit(X, Y)\n\nprint(gsc_rf.best_estimator_)\nclf_rf = gsc_rf.best_estimator_\nclf_rf.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_rf.score(X, Y), 4)))\n\nY_predicted_rf = clf_rf.predict_proba(X)[:, 1]","cc6feed5":"\npipeline = Pipeline([(\"scaler\", StandardScaler()), (\"svm\", SVC(probability = True, kernel = \"rbf\"))])\n\nparams = {\"svm__C\" : [0.01, 0.1, 1],\n          \"svm__gamma\" : [0.01, 0.1, 1]}\n\ngsc_svm = GridSearchCV(pipeline, param_grid = params, cv = StratifiedShuffleSplit(n_splits = 10, \n                                                                                  test_size = 0.3, \n                                                                                  random_state = 15)) \ngsc_svm = gsc_svm.fit(X, Y)\n\nprint(gsc_svm.best_estimator_)\nclf_svm = gsc_svm.best_estimator_\nclf_svm.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_svm.score(X, Y), 4)))\n\nY_predicted_svm = clf_svm.predict_proba(X)[:, 1]","ed85f566":"\nclf_bag = BaggingClassifier()\n\nparams = {\"n_estimators\" : [30, 50, 70, 100],\n          \"max_features\" : [3, 5, 7, 9],\n          \"max_samples\" : [3, 5, 7, 9]}\n\ngsc_bag = GridSearchCV(clf_bag, \n                       params, \n                       cv = StratifiedShuffleSplit(n_splits = 5, \n                                                   test_size = 0.3, \n                                                   random_state = 15))\n\ngsc_bag = gsc_bag.fit(X, Y)\n\nprint(gsc_bag.best_estimator_)\nclf_bag = gsc_bag.best_estimator_\nclf_bag.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_bag.score(X, Y), 4)))\n\nY_predicted_bag = clf_bag.predict_proba(X)[:, 1]","d64570e6":"\nclf_xt = ExtraTreesClassifier(criterion = \"gini\", \n                              max_features = \"auto\")\n\nparams = {\"max_depth\" : [5, 7, 9],\n          \"n_estimators\" : [300, 500, 700]}\n\ngsc_xt = GridSearchCV(clf_xt, params, cv = StratifiedShuffleSplit(n_splits = 10, \n                                                                  test_size = 0.3, \n                                                                  random_state = 15))\ngsc_xt = gsc_xt.fit(X, Y)\n\nprint(gsc_xt.best_estimator_)\nclf_xt = gsc_xt.best_estimator_\nclf_xt.fit(X, Y)\n\nprint(\"\")\nprint(\"Accuracy Score: \" + str(round(clf_xt.score(X, Y), 4)))\n\nY_predicted_xt = clf_xt.predict_proba(X)[:, 1]","741b5ba4":"from sklearn.metrics import roc_curve, roc_auc_score","ae00eae1":"log_fpr, log_tpr, log_treshholds = roc_curve(Y, Y_predicted_log)\nboost_fpr, boost_tpr, boost_treshholds = roc_curve(Y, Y_predicted_boost) \nsvm_fpr, svm_tpr, svm_treshholds = roc_curve(Y, Y_predicted_svm)\nrf_fpr, rf_tpr, rf_treshholds = roc_curve(Y, Y_predicted_rf)\nxgb_fpr, xgb_tpr, xgb_treshholds = roc_curve(Y, Y_predicted_xgb)\nbag_fpr, bag_tpr, bag_treshholds = roc_curve(Y, Y_predicted_bag)\nxt_fpr, xt_tpr, xt_treshholds = roc_curve(Y, Y_predicted_xt)\n\nauc_score_log = roc_auc_score(Y, Y_predicted_log)\nauc_score_boost = roc_auc_score(Y, Y_predicted_boost)\nauc_score_svm = roc_auc_score(Y, Y_predicted_svm)\nauc_score_rf = roc_auc_score(Y, Y_predicted_rf)\nauc_score_xgb = roc_auc_score(Y, Y_predicted_xgb)\nauc_score_bag = roc_auc_score(Y, Y_predicted_bag)\nauc_score_xt = roc_auc_score(Y, Y_predicted_xt)\n\nplt.figure(figsize = (12,6))\nplt.plot([0,1], [0,1])\nplt.plot(log_fpr, log_tpr, label = \"Logistic Regression (AUC-Score: \" + str(round(auc_score_log, 2)) + \")\")\nplt.plot(boost_fpr, boost_tpr, label = \"Gradient Boosting (AUC-Score: \" + str(round(auc_score_boost, 2)) + \")\")\nplt.plot(svm_fpr, svm_tpr, label = \"SVM (AUC-Score: \" + str(round(auc_score_svm, 2)) + \")\")\nplt.plot(rf_fpr, rf_tpr, label = \"Random Forest (AUC-Score: \" + str(round(auc_score_rf, 2)) + \")\")\nplt.plot(xgb_fpr, xgb_tpr, label = \"XGBoost (AUC-Score: \" + str(round(auc_score_xgb, 2)) + \")\")\nplt.plot(bag_fpr, bag_tpr, label = \"Bagging Classifier (AUC-Score: \" + str(round(auc_score_bag, 2)) + \")\")\nplt.plot(xt_fpr, xt_tpr, label = \"Extra Trees Clasifier (AUC-Score: \" + str(round(auc_score_xt, 2)) + \")\")\nplt.title(\"ROC-Curve\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.legend()\nplt.show()","0fa0abe7":"titanic_test.head(1)","19bbf2e1":"\nvote = VotingClassifier(estimators = [(\"XGBoost\", clf_xgb), \n                                      (\"GradientBoosting\", clf_boost),\n                                      (\"RandomForest\", clf_rf), \n                                      (\"Logistic Regression\", clf_log), \n                                      (\"SVM\", clf_svm),  \n                                      (\"Extra Trees Classifier\", clf_xt)], \n                                      voting = \"soft\")\n\nvote.fit(X, Y)\nprint(round(vote.score(X, Y), 4))\n\nY_predicted_stack = vote.predict(titanic_test)","5656839a":"#Feature engineering: One-hot encoded for Embarked Column","d7f61e2c":"# Feature Engineering","e02c0c42":"# Dealing with missing Values in the data set","e1ae0d8e":"# Data Loading","72b41e58":"# Import Packages","192741f1":"# Project Name: Titanic: Machine Learning from Disaster","7dffdd0e":"# Initial EDA (Exploratory Data Analysis)","9a3dc706":"# Model Building"}}