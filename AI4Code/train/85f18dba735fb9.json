{"cell_type":{"cd15ad56":"code","588cb5ea":"code","ef69a5fb":"code","f733566b":"code","a83b64f6":"code","08122394":"code","29d4082b":"code","4b75afa4":"code","6cdb4b96":"code","0570de53":"code","b0745029":"code","9931aebd":"code","d69a3416":"code","e15c168e":"code","bc2adc86":"code","a3731a16":"code","fb358d62":"code","c832c287":"code","e0e57322":"code","60a71a2d":"code","a737b39a":"code","29cf076a":"markdown"},"source":{"cd15ad56":"# Libraries\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nfrom IPython.display import display","588cb5ea":"\ntrain = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col=0)\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\nsubmission.head()","ef69a5fb":"# Predictors & target\npredictors = train.columns[:-1]\ntarget = train.columns[-1]\npredictors","f733566b":"def label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature","a83b64f6":"cat_cols = [col for col in predictors if 'cat' in col]\ncont_cols = [col for col in predictors if 'cont' in col]","08122394":"le_cols = []\nfor feature in cat_cols:\n    le_cols.append(label_encode(train, test, feature))","29d4082b":"train.head()","4b75afa4":"cols = le_cols + cont_cols","6cdb4b96":"len(cols)","0570de53":"len(predictors)","b0745029":"# Functions for KFold evaluation\ndef create(hyperparams):\n    \"\"\"Create LGBM Classifier for a given set of hyper-parameters.\"\"\"\n    model = XGBClassifier(**hyperparams)\n    return model\n\ndef fit(model, X, y):\n    \"\"\"Simple training of a given model.\"\"\"\n    model.fit(X, y)\n    return model\n\ndef fit_with_stop(model, X, y, X_val, y_val):\n    \"\"\"Advanced training with early stopping.\"\"\"\n    model.fit(X, y,\n              eval_set=[(X_val, y_val)],\n              early_stopping_rounds=200, # ! Hard-coded value\n              verbose=300)\n    return model\n\ndef evaluate(model, X, y):\n    \"\"\"Compute AUC for a given model.\"\"\"\n    yp = model.predict_proba(X)[:, 1]\n    auc_score = roc_auc_score(y, yp)\n    return auc_score\n\ndef kfold_evaluation(X, y, k, hyperparams):\n    \"\"\"Run a KFlod evaluation.\"\"\"\n    scores = []\n    \n    print(f\"\\n------ {k}-fold evaluation -----\")\n    print(hyperparams)\n    \n    kf = KFold(k)\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"----- FOLD {i} -----\")\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val)\n        train_score = evaluate(model, X_train, y_train)\n        val_score = evaluate(model, X_val, y_val)\n        scores.append((train_score, val_score))\n        \n        print(f\"Eval AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns=['train score', 'validation score'])\n    \n    return scores\n\ndef kfold_prediction(X, y, X_test, k, hyperparams):\n    \"\"\"Make predictions with a bagged model based on KFold.\"\"\"\n    yp = np.zeros(len(X_test))\n    \n    kf = KFold(k)\n    for train_idx, test_idx in kf.split(X):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model = create(hyperparams)\n        model = fit_with_stop(model, X_train, y_train, X_val, y_val)\n        yp += model.predict_proba(X_test)[:, 1] \/ k\n    \n    return yp","9931aebd":"# Constant\nK = 5\nX = train[cols]\nY = train[target]\nX_TEST = test[cols]\nBEST_PARAMS = {'learning_rate': 0.03, \n               'eval_metric': 'auc',\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',}","d69a3416":"X","e15c168e":"# Objective function\ndef objective(trial):\n    # Search spaces\n    hyperparams = {\n        'seed': 137,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'use_label_encoder': False,\n        'max_bin': trial.suggest_int('max_bin', 2, 1000),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'alpha': trial.suggest_float('alpha', 1E-16, 12),\n        'gamma': trial.suggest_float('gamma', 1E-16, 12),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1E-16, 12),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 1E-16, 1.0),\n        'subsample': trial.suggest_float('subsample', 1E-16, 1.0), \n        'min_child_weight': trial.suggest_float('min_child_weight', 1E-16, 12),\n    }\n    \n    # Add BEST_PARAMS\n    hyperparams.update(BEST_PARAMS)\n    \n    # Evaluation\n    scores = kfold_evaluation(X, Y, K, hyperparams)\n    \n    return scores['validation score'].mean()","bc2adc86":"# Optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, timeout=3600*2)","a3731a16":"# Best score\nstudy.best_value","fb358d62":"# Historic\nplot_optimization_history(study)\n","c832c287":"# Importance\nplot_param_importances(study)","e0e57322":"# Best parameters\nBEST_PARAMS.update(study.best_params)\nBEST_PARAMS","60a71a2d":"model = XGBClassifier(**BEST_PARAMS, use_label_encoder=False)","a737b39a":"%%time\n# Predictions on test set and submission\nsubmission['target'] = kfold_prediction(X, Y, X_TEST, K, BEST_PARAMS)\nsubmission.to_csv('submission.csv', index=False)","29cf076a":"This is an XGB version of the following notebook: https:\/\/www.kaggle.com\/rmiperrier\/lgb-optuna\n\nIt uses Label Encoding (LE) and GPU acceleration."}}