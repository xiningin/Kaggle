{"cell_type":{"85133c59":"code","a54a97c6":"code","055d56d6":"code","748fc102":"code","2ff26700":"code","31d28421":"code","c8fda3f3":"code","96969a3e":"code","40b7248a":"code","6ac220fc":"code","81ad2513":"code","cb041e44":"code","01545376":"markdown","cde666cf":"markdown","4765bfca":"markdown","be9aad5b":"markdown","bbce862b":"markdown","3a78a0e0":"markdown","c7eaf980":"markdown","e76fc8bd":"markdown","a29d2309":"markdown","18bed6b1":"markdown","44fa5f9d":"markdown","6faeab53":"markdown"},"source":{"85133c59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization \n#Keras imports for machine learning\nfrom keras.layers import Input, Dense, BatchNormalization, Add, GaussianNoise, Dropout\nfrom keras.models import Model\nfrom keras.layers import Wrapper\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils import to_categorical\nfrom keras import regularizers, Sequential\nfrom keras.wrappers.scikit_learn import KerasClassifier\n#sklearn imports for metrics and data division\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler","a54a97c6":"precisiones_globales=[]\nepochs = 15\ndef graf_model(train_history):\n    f = plt.figure(figsize=(15,10))\n    ax = f.add_subplot(121)\n    ax2 = f.add_subplot(122)\n    # summarize history for accuracy\n    ax.plot(train_history.history['binary_accuracy'])\n    ax.plot(train_history.history['val_binary_accuracy'])\n    ax.set_title('model accuracy')\n    ax.set_ylabel('accuracy')\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(train_history.history['loss'])\n    ax2.plot(train_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper left')\n    plt.show()\ndef precision(model, registrar=False):\n    y_pred = model.predict(train_dfX)\n    train_auc = roc_auc_score(train_dfY, y_pred)\n    y_pred = model.predict(val_dfX)\n    val_auc = roc_auc_score(val_dfY, y_pred)\n    print('Train AUC: ', train_auc)\n    print('Vali AUC: ', val_auc)\n    if registrar:\n        precisiones_globales.append([train_auc,val_auc])","055d56d6":"train_set = pd.read_csv(\"..\/input\/train.csv\")\ntrain_dfY = train_set['Survived']\ntest_set = pd.read_csv(\"..\/input\/test.csv\")\nsubmission = test_set['PassengerId'].copy()\nprint(train_set.shape)\nprint(test_set.shape)\nprint(train_dfY.shape)\n\n\n","748fc102":"train_set['Cabin'].value_counts()\n","2ff26700":"datasets = [train_set, test_set]\noriginalData = train_set\nfor dataset in datasets:\n    dataset['Age'].fillna(dataset[\"Age\"].median(), inplace=True)\n    dataset['Embarked'].fillna(dataset[\"Embarked\"].mode()[0], inplace=True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    dataset.drop(['PassengerId','Cabin', 'Ticket'], axis=1, inplace=True)\n    #+1 to count himself\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1 \n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\nprint('Ahora columnas null en el trainset')\nprint(train_set.isnull().sum())\nprint('Ahora columnas null en el test set')\nprint(test_set.isnull().sum())\n","31d28421":"for dataset in datasets:   \n    #This creates an array with 0 beign the last name and 1 beign title + first and second name\n    StringArray = dataset['Name'].str.split(\", \", expand=True)\n    #We grab the title and names and further split it into title and name.0 Contains the title, 1 contains the name\n    StringArray = StringArray[1].str.split(\".\", expand=True)\n    dataset['Title'] = StringArray[0]\n    title_names = dataset['Title'].value_counts()\n    print('===Prior to grouping===')\n    print(title_names)\n    #Podemos observar que hay una gran diferencia entre el n\u00famero de Dr. y Master, agruparemos todo lo que sea menor o igual a 7 en titulos misc\n    #Tambi\u00e9n se podr\u00eda hacer que cada t\u00edtulo tenga su \n    title_names = (dataset['Title'].value_counts() > 7)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == False else x)\n    print('===Post grouping===')\n    print(dataset['Title'].value_counts())\n    ","c8fda3f3":"label = LabelEncoder()\nenc = OneHotEncoder(handle_unknown='ignore')\ntrain_set = datasets[0]\ntest_set = datasets[1]\ntrain_set = train_set.drop(['Survived'], axis=1)\nEncodedDataFrames = []\nprint(train_set.shape, test_set.shape)\nwholeData = pd.concat([train_set, test_set], ignore_index=True)\n#from 0 to 890: train_set. From 890 to 1308: Test_set\n\nwholeData['Sex_Code'] = label.fit_transform(wholeData['Sex'])\nwholeData['Embarked_Code'] = label.fit_transform(wholeData['Embarked'])\nwholeData['Title_Code'] = label.fit_transform(wholeData['Title'])\n#Column order: Female, Male\nsex_encoded = to_categorical(wholeData['Sex_Code'])\nSexOneHot = pd.DataFrame(sex_encoded, columns = [\"Female\", \"Male\"])\n#Column order: C, Q ,S\nembarked_encoded = to_categorical(wholeData['Embarked_Code'])\nEmbarkedOneHot = pd.DataFrame(embarked_encoded, columns = [\"C\", \"Q\", \"S\"])\n#Column order: Master, Misc, Miss, Mr, Mrs\ntitle_encoded = to_categorical(wholeData['Title_Code']) \nTitleOneHot = pd.DataFrame(title_encoded, columns = [\"Master\", \"Misc\", \"Miss\", \"Mr\", \"Mrs\"])\n#Column order: 0, 1, 2, 3\npclass_encoded = to_categorical(wholeData['Pclass'])\nPClassOneHot = EmbarkedOneHot = pd.DataFrame(pclass_encoded, columns = [\"0th Class\", \"1th Class\", \"2nd Class\", \"3rd Class\"])\nPClassOneHot = PClassOneHot.drop(\"0th Class\", axis=1)\nwholeData = pd.concat([wholeData, SexOneHot, EmbarkedOneHot, TitleOneHot, PClassOneHot], axis=1)\nEncodedDataFrames.append(wholeData)\n\nprint(wholeData.shape)\n\n#train_set = EncodedDataFrames[0]\n#test_set = EncodedDataFrames[1]","96969a3e":"\nwholeData = wholeData.drop(['Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'Pclass'], axis= 1)\n\n[train_set, test_set] = np.split(wholeData, [891], axis= 0)\ntrain_dfX = train_set\nprint(train_dfX.shape, train_dfY.shape)\n\n#train_dfX = train_set.drop(['Sex_Code', 'Embarked_Code', 'Title_Code', 'Survived'], axis = 1)\ntrain_dfX,val_dfX,train_dfY, val_dfY = train_test_split(train_dfX,train_dfY , test_size=0.1, stratify=train_dfY)\n\nprint(\"Entrenamiento: \",train_dfX.shape, train_dfY.shape)\nprint(\"Validacion : \",val_dfX.shape, val_dfY.shape)\nprint(\"Test: \", test_set.shape)\n","40b7248a":"'''\ndef func_model():   \n    inp = Input(shape=(20,))\n    x=Dense(1028, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(inp)\n    x=Dense(1028, activation=\"relu\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x) \n    x=Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_normal', bias_initializer='zeros')(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n    return model\nmodel = func_model()\nprint(model.summary())\n'''\n\n#Probando otro modelo\n\nmodel = Sequential()\nmodel.add(Dense(40, kernel_initializer = 'glorot_normal', bias_initializer='zeros', activation = 'relu', kernel_regularizer=regularizers.l2(0.01), input_dim = 19))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(20, kernel_initializer = 'glorot_normal', bias_initializer='zeros', activation = 'relu', kernel_regularizer=regularizers.l2(0.01) ))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, kernel_initializer = 'glorot_normal', bias_initializer='zeros', activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\nmodel.summary()\n\n","6ac220fc":"#train_history = model.fit(train_dfX, train_dfY, batch_size= 25, epochs=20, validation_data=(val_dfX, val_dfY))\n\n#Probando otro modelo \ntrain_history = model.fit(train_dfX, train_dfY, batch_size=24, epochs= 20, validation_data=(val_dfX, val_dfY))","81ad2513":"graf_model(train_history)\n","cb041e44":"\ny_test = model.predict(test_set)\ny_formatted = np.where(y_test > 0.5, 1, 0)\ny_dataFrame = pd.DataFrame(np.ravel(y_formatted), columns=['Survived'])\nsubmission = pd.concat([submission, y_dataFrame], axis=1)\nsubmission.to_csv('submission.csv', index=False)","01545376":"1) Importar las librerias.","cde666cf":"7) Graficas","4765bfca":"1.1) Funciones auxiliares","be9aad5b":"4.2) One hot encoding","bbce862b":"5) Definici\u00f3n del modelo ","3a78a0e0":"8) Submission","c7eaf980":"4.3) Separacion entre X y Y y drop a cosas que ya no se utilizan  ","e76fc8bd":"2) Importar los datos ","a29d2309":"4.1 Separaci\u00f3n del t\u00edtulo","18bed6b1":"6) Prueba del modelo ","44fa5f9d":"3) Revisi\u00f3n previa de los datos ","6faeab53":"4) Limpieza de los datos: Eliminacion de nulls y eliminaci\u00f3n de propiedades irrelevantes (Valores al azar, como el n\u00famero del boleto). "}}