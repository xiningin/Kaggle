{"cell_type":{"b7ad60ab":"code","3acaa4ee":"code","a2bf2f03":"code","aff91959":"code","4beb03d2":"code","c9ee1041":"code","d7779c74":"code","84d03cea":"code","13c6022d":"code","cc6b3c84":"code","a4aa9f7e":"code","12597092":"code","8b9dcda1":"code","6c431e67":"code","52b053a3":"code","e5e97234":"code","8061731f":"code","f302771a":"code","9ba4dd37":"code","cbe1698b":"code","574fadf2":"code","31272351":"markdown","63f15fb9":"markdown","121a39c6":"markdown","0f0a8713":"markdown","85daf34f":"markdown","348a4f93":"markdown","0a88da60":"markdown"},"source":{"b7ad60ab":"import numpy as np\nfrom numpy.fft import *\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport scipy.fftpack\nfrom scipy import signal\nfrom pykalman import KalmanFilter\nfrom sklearn import tree\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport gc\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\nDATA_PATH = \"..\/input\/liverpool-ion-switching\"\n\nx = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n#test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n#submission_df = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))","3acaa4ee":"x = x.rename(columns = {'open_channels':'y'})\ndt = 0.0001\nfs = 10000\nf0 = 100\n\nK = np.int(fs\/f0)\na1 = -0.99 \nx['x0'] = 0.\nx['x0'] = x['signal'] + a1 * x['signal'].shift(K)\nx.loc[0:K-1,'x0'] = x.loc[0:K-1,'signal']\n\nx.loc[:,'signal_energy'] = x['x0']**2 * dt\nx.loc[:,'original_energy'] = x['signal']**2 * dt\n\nfilter_gain =  np.sqrt( x['signal_energy'].sum()\/x['original_energy'].sum()) \nprint(f'filter gain is {filter_gain:.3f}')\nprint(f'filter loss is {1.\/filter_gain:.3f}')\n","a2bf2f03":"r = 350. # units are Ohms\nr1 = 3. # current source resistance (est)\na1 = 1\/filter_gain  # loss in estimate of injection current)\n\nx.loc[:,'signal_energy'] = a1 * (r+r1)\/(r*r1) *  x['x0']**2 * dt","aff91959":"# Energy of a signal is r i^2 * time\n\n\ndt = 0.0001\nfs = 1. \/ dt\n\n\n# energy of our signal = energy of message + injection energy\n# the injection current is DC, so we must use original siggnal before filtering \n# to find the DC offset (aka injection current). Assume it is a square wave as described in literature.\n\nx['injected_current'] = a1 * x['x0'].rolling(window=7500,min_periods=5).min()\nx.loc[0:4,'injected_current'] = 0\nx['injected_energy'] = r1 * x['injected_current']**2 * dt\/4 # I think injected current is square wave\nx.loc[0:4,'injected_energy']  = 0.\n\n# energy of message = energy of signal - injection energy\n# message energy\n\nx= x.fillna(0)\nx['message_current'] = x['x0'] - x['injected_current']\nx['message_energy'] = r * x['message_current']**2 * dt\n\n# calculate the mean of open channels as a moving averge\nx['y_mean'] = x['y'].rolling(window=100,min_periods=5).mean()\nx.loc[0:4,'y_mean'] = 0","4beb03d2":"\nexamples = ['signal','y','y_mean','signal_energy','injected_energy','message_energy','x0','injected_current','message_current']\n\nfig, ax = plt.subplots(nrows=len(examples), ncols=1, figsize=(25, 3.5*len(examples)))\nfig.subplots_adjust(hspace = .5)\nax = ax.ravel()\ncolors = plt.rcParams[\"axes.prop_cycle\"]()\n\nfor i in range(len(examples)):\n    \n    c = next(colors)[\"color\"]\n    ax[i].grid()\n    if examples[i] in ['x0','signal','message_current','injected_current']:\n        ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 2)\n        ax[i].set_ylabel('current (pA)', fontsize=14)\n        \n    if examples[i] in ['y','y_mean']:\n        ax[i].scatter(x['time'][::10], x[examples[i]][::10],marker ='.', color=c)\n        ax[i].set_ylabel('Open Channels', fontsize=14)\n    if examples[i] in ['signal_energy','energy_inj','message_energy','injected_energy']:\n        ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 1)\n        ax[i].set_ylabel('Energy 10^-24 Joules', fontsize=14)                     \n    ax[i].plot(x['time'], x[examples[i]],color=c, linewidth=.5)\n    ax[i].set_title(examples[i], fontsize=24)\n    ax[i].set_xlabel('Time (seconds)', fontsize=14)\n    #ax[i].set_ylabel('current (pA)', fontsize=24)\n    #ax[i].set_ylim(0,5)","c9ee1041":"plt.close()\n# calculate the average energy per change in open channels\n\nx['y_var'] = x['y'] - x['y'].shift(1)\nx.loc[0,'y_var'] = 0\n\n# when a change in message energy occurs\n\nx['dE'] = (x['message_energy'] - x['message_energy'].shift(1))\nx.loc[0,'dE'] = 0\nde = x.loc[x.dE != 0.,['dE']].values\ndy = x.loc[x.dE != 0.,['y_var']].values\n\n# measure k = (change in y) \/ (energy change)\nk = np.mean(dy\/de)\nk2 = np.mean(np.abs(dy\/de))\nprint(f'{k:.3e} ion transitions per 10^-24 J')\nprint(f'{k2:.3e} ion transitions per J --- k2')\n\n# The message current is the flow of ions in the probe. It should be a direct measure of open channels\nx['y_est'] = r * x['message_current']**2 * k * dt * 5\n# the constant 6 makes better solution, but why 6?\nx['y_est'] = x['y_est'].clip(0,10)\n# calculate the mean of open channels as a moving averge\nx['y_mean_est'] = x['y_est'].rolling(window=100,min_periods=5).mean()\nx.loc[0:4,'y_mean'] = 0\n\nx['y_var_est'] = x['y_est']- x['y_est'].shift(1)\nx.loc[0,'y_var'] = 0\n\n\n","d7779c74":"examples = ['signal','y','y_est', 'y_var','y_var_est','y_mean','y_mean_est']\n\nfig, ax = plt.subplots(nrows=len(examples), ncols=1, figsize=(25, 3.5*len(examples)))\nfig.subplots_adjust(hspace = .5)\nax = ax.ravel()\ncolors = plt.rcParams[\"axes.prop_cycle\"]()\n\nfor i in range(len(examples)):\n    \n    c = next(colors)[\"color\"]\n    ax[i].grid()\n    if examples[i] in ['x0','signal','message_current']:\n        ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 2)\n        ax[i].set_ylabel('current (pA)', fontsize=14)\n        \n    if examples[i] in ['y','y_var','y_mean','y_var']:\n        ax[i].scatter(x['time'][::10], x[examples[i]][::10],marker ='.', color=c, linewidth=0)\n        ax[i].set_ylabel('Open Channels', fontsize=14)\n    if examples[i] in ['y_mean_est','y_pred']:\n        ax[i].scatter(x['time'][::10], (x[examples[i]][::10]).round(0),marker ='.', color=c, linewidth=0)\n        ax[i].set_ylabel('Open Channels', fontsize=14)\n        #ax[i].set_ylim(0,10)\n    if examples[i] in ['y_est']:\n        ax[i].scatter(x['time'][::10], (x[examples[i]][::10]).round(0),marker ='.', color=c, linewidth=0)\n        ax[i].set_ylabel('Open Channels', fontsize=14)\n        #ax[i].set_ylim(0,10)\n    if examples[i] in ['y_var_est']:\n        ax[i].scatter(x['time'][::10], (x[examples[i]][::10]).round(0),marker ='.', color=c, linewidth=0)\n        ax[i].set_ylabel('Open Channels', fontsize=14)\n    if examples[i] in ['signal_energy','injected_energy','message_energy']:\n        ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 1)\n        ax[i].set_ylabel('Energy 10^-24 Joules', fontsize=14)                     \n    ax[i].plot(x['time'], x[examples[i]],color=c, linewidth=.5)\n    ax[i].set_title(examples[i], fontsize=24)\n    ax[i].set_xlabel('Time (seconds)', fontsize=14)\n    #ax[i].set_ylabel('current (pA)', fontsize=24)\n    #ax[i].set_ylim(0,5)","84d03cea":"plt.close()\n","13c6022d":"# Calculate the f1_score for the estimate\n# first descretize the estimate by rounding to ones digit.\n\nx['y_est'] = x['y_est'].round(0)\n\nf1 = f1_score(x.y, x.y_est, average='macro')\nprint(f'f1_score is {f1:.3f}')","cc6b3c84":"# Reference 3\nfrom collections import namedtuple\ngaussian = namedtuple('Gaussian', ['mean', 'var'])\ngaussian.__repr__ = lambda s: '\ud835\udca9(\u03bc={:.3f}, \ud835\udf0e\u00b2={:.3f})'.format(s[0], s[1])\n\ndef update(prior, measurement):\n    x, P = prior        # mean and variance of prior of x (system)\n    z, R = measurement  # mean and variance of measurement (open_channels) with ion probe\n    \n    J = z-x          #1 - f1_score(z,x)        # residual - This is error we want to minumize\n    K = P \/ (P + R)              # Kalman gain\n\n    x = x + K*J      # posterior\n    P = (1 - K) * P  # posterior variance\n    return gaussian(x, P)\n\ndef predict(posterior, movement):\n    x, P = posterior # mean and variance of posterior\n    dx, Q = movement # mean and variance of movement\n    x = x + dx\n    P = P + Q\n    return gaussian(x, P)\n","a4aa9f7e":"R = 1\nQ = 2.\nP = gaussian(0,R)\n\nfor i in range(1000):\n    measurement = gaussian(x.loc[i,'y_est'],R)        \n    est, P = update(P,measurement)\n    \n    movement = gaussian(x.loc[i,'y'],Q)\n    est, P = predict(P,movement)\n    x.loc[i,'y_pred_k'] = est\n    ","12597092":"from itertools import islice\n\ndef window(seq, n=2):\n    \"Sliding window width n from seq.  From old itertools recipes.\"\"\"\n    it = iter(seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n        \npairs = pd.DataFrame(window(x.loc[:,'y']), columns=['state1', 'state2'])\ncounts = pairs.groupby('state1')['state2'].value_counts()\nalpha = 1 # Laplacian smoothing is when alpha=1\ncounts = counts + 1\n#counts = counts.fillna(0)\nP = ((counts + alpha )\/(counts.sum()+alpha)).unstack()\nP","8b9dcda1":"\npairs = pd.DataFrame(window(x.loc[:,'signal_energy']), columns=['state1', 'state2'])\nmeans = pairs.groupby('state1')['state2'].mean()\nalpha = 1 # Laplacian smoothing is when alpha=1\nmeans = means.unstack()\nmeans","6c431e67":"print('Occurence Table of State Transitions')\not = counts.unstack().fillna(0)\not","52b053a3":"P = (ot)\/(ot.sum())\nCal = - P * np.log(P)\nCal","e5e97234":"Caliber = Cal.sum().sum()\nCaliber","8061731f":"# reference https:\/\/www.kaggle.com\/friedchips\/on-markov-chains-and-the-competition-data\ndef create_axes_grid(numplots_x, numplots_y, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    return fig, axes\n\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    P,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","f302771a":"eig_values, eig_vectors = np.linalg.eig(np.transpose(P))\nprint(\"Eigenvalues :\", eig_values)","9ba4dd37":"# reference: http:\/\/kitchingroup.cheme.cmu.edu\/blog\/2013\/02\/03\/Using-Lagrange-multipliers-in-optimization\/\ndef func(X):\n    x = X[0]\n    y = X[1]\n    L = X[2] \n    return x + y + L * (x**2 + k * y)\n\ndef dfunc(X):\n    dL = np.zeros(len(X))\n    d = 1e-4 \n    for i in range(len(X)):\n        dX = np.zeros(len(X))\n        dX[i] = d\n        dL[i] = (func(X+dX)-func(X-dX))\/(2*d);\n    return dL","cbe1698b":"from scipy.optimize import fsolve\n\n# this is the max\nX1 = fsolve(dfunc, [1, 1, 0])\nprint(X1, func(X1))\n\n# this is the min\nX2 = fsolve(dfunc, [-1, -1, 0])\nprint(X2, func(X2))","574fadf2":"del x","31272351":"## Calculate Energies","63f15fb9":"## Injection energy is converted into open channel Potential Energy y'(t) * k\n\nx2 above, the injection energy (IE) of current IC, frees ion channels to open with more vigor. When IE is lowest (0.1 x10^-24 Joules), it is rare for more than 1 channel to be open. As IE increase, so does ion channel freedom to open. At IE = 6.0 x10^-24 J, we see up to 10 open channels.\n\nOther notebooks have shown that given the mode {0,1,2,3,4}, the state of open channels are gaussian distributions. We also make the assumption that after initial comb filtering (Drift removal) that our signal also has a gaussian distribution. I assume that the injection current is DC and has variance=0. Gaussians have continuous first and second derivatives (a HJB requirement). This also inspires me to try to imagine the \"True\" reciever's transfer function H(z) as a gaussian and as the sum of two gaussians:\n\n     gaussian(y (mean,var)) = gaussian(injection_energy(mean,var=0)) + gaussian(message_energy(mean,var))\n     y(mean) = injection_energy_mean + message_energy_mean\n     y(var) = message_energy_variance","121a39c6":"## Reduce known non-gaussian noise.\n\nTo eliminate low frequency waveform noise, the filter should block DC and VLF. In train data from 365s to 385s exists obvious 100Hz and harmonics of 100Hz (n x 100Hz).\n\nTho remove both of these noises I'll first try a Comb filter using it's difference equation:\n\nx<sub>1<\/sub>(t) = a0 x<sub>0<\/sub>(t) + a1 x<sub>0<\/sub>(t-K)<br>\nK = fs\/f0<br>\na0 = 1<br>\na1 = -0.99","0f0a8713":"Above injected energy prompts the mode of operation. This injected energy inreases system energy which is converted into the entropy of the open channel distribution. As I understand the science, the distribution of Y will change, given energy, to the distribution with greatest entropy. *(maybe not a Gaussian?)*\n\n## Plot Energies","85daf34f":"## Calculate constants r, k, and a1\nThese constants arise from our equations for energy\n\nr = probe resistance in ohms (my initial guess is 350 Ohms)<br>\nk = energy to open a channel (a.k.a. move ion through probe)<br>\na1 = is the gain factor for our estimate of the injection energy<br>\n\nI need to work on derivation of these constants. But for now I have found intuitive values for r and a1. k is calculated below.","348a4f93":"## Derive the equations of motion y(t) and y'(t)\n\nThis problem might be solved using optimal control mathematics. We want to control a detector of the system's output states (how many open channels) so that the f1 score of the detector is maximized. The Kalman filter is based on this approach yet it has a slightly different measure of performance **J(x)**.\n\nThe Hamiltonian equation offers a solution. The solution of the minumized partial derivative of the Hamiltonian is the solution for this problem. It tries to provide an estimate of \"open_channels\" at the highest possible f1 score. This requires that our performance score is diffenciable on both time and x.\n\n![A System Model](https:\/\/www.elmtreegarden.com\/wp-content\/uploads\/2020\/04\/ion-channel-system-model.jpg)\n\n## The HJB equation, now called the Hamiltonian\n\nIn this case the Hamiltonian is the energy equation of the system. **H = T - V** = kinetic energy - potential energy. And we know energy is conserved.\n\nBy applying Kirkoff's law, on conservation of charge, that no charge may accumulate at a node ==> all current entering a node, must exit the node gives this:\n\nSignal Current = Injection current + Message current    where message current = probe current<br>\nMessage Current = Signal Current - Injected Current.\n\n![The Hamiltonian](https:\/\/www.elmtreegarden.com\/wp-content\/uploads\/2020\/05\/pontryagin.jpg)\n\nLet,\n\nx0(t) = signal(t) which has been comb filtered to remove noise<br>\nx1(t) = energy of the x0 current = r x0(t)^2 * dt<br>\nx2(t) = injected energy to probe = a1 * x1(t-Ta:t).min() * dt<br>\ny(t) = open_channels<br>\n\nThe energy of open channels is k * open_channels * dt. This energy is the energy required to transport the ion through the electric field in the probe.\n\n### to conserve enery we know that total energy is zero, so\n\n    0 = Signal Energy + Injected Energy - energy of transition\n    \n    0 = x1(t) dt + x2(t) dt - k y'(t)\n    y'(t) = 1\/k * (x1(t) dt + x2(t) dt) \n    y'(t) = 1\/k * ( x1(t) + a1 * x1(t-Ta:t).min)) * dt\n    y'(t) = 1\/k * ( r x0(t)^2 + a1 r x0(t-Ta:t)^2.min()) * dt\n\nSimplify x0(t) = x(t)\n    \n    y'(t) = dy(t)\/dt = r\/k ( x(t)^2 + a1 x(t-Ta:t)^2.min() ) * dt\n    \nfinally we get   \n \n### g(x,u,t) = y'(t) = r\/k (x(t)^2 + a1 x(t-Ta:t)^2.min() + u(t)) * dt\n\n\n\n### Measure of performance\n\nThe performance function, **J(t)** , is the f_1 score, f_1(y, y_true). It is a measure of energy. Unfortunately the f1 score only measures an ensemble's score. There is no microscopic f1_score. That is to say the f1_score needs more than a single pair(y_est,y_true) It is not differentiable with time: there is no function f = d(f1_score)\/dt.\n\nThe f1_score is really two measures. Precision and Recall.\n\nA solution is, maybe, to 'soften this function' into a measurement score that is differentiable. Here is [a link to the code](https:\/\/towardsdatascience.com\/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d). <br>\n\n  **J**(y(t), t) = soft_f1_score(y(t),y_true(t))\n\n\n\n### Initial conditions\nInitial conditions will be assumed are<br>\nX0(0) = X0_dot(0) = 0<br>\ny'(0) = 0\n\n### The old HJB Equation (as done 40 years ago)\n\nThe HJB equation it's partial derivatives of d**J**\/dt = **J<sub>t<\/sub>(t)**, and d**J**\/dx = **J<sub>x<\/sub>(t)** is:\n\nPer Donald Kirk's book, the text I learned 40 years ago:<br>\n0 = **J<sub>t<\/sub>(t)**(**y**, t) + **H**{**x**, **u'**{**x**, **J<sub>x<\/sub>(x)**(**y**,t), t},**J<sub>x<\/sub>(x)**(**y**, t), t}<br>\nThe **u'** that solves this diffential equation is the optimal control for the History.\n\nOuch! I really like the new math. It has such a simple formulation.\n\nReferences:\n1. Kirk, \"Optimal Control Theory\", 1970 Chapters 1-3<br>\n2. [E.T. Jaynes: Minimum Entropy Principle](https:\/\/pdfs.semanticscholar.org\/b326\/6b25cb2ff34634aff48434652bacb3fede9c.pdf)\n3. [Toward Data Science Blog](https:\/\/towardsdatascience.com\/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)\n4. Hamill, \"A Student's Guide to Lagrangians and Hamiltonians\", Cambridge Press (Nice concise and modern!)\n5. This community of programmers, Kaggle, for much of this code. \n6. KitchinGroup, Using Lagrangian Multipliers [source](http:\/\/kitchingroup.cheme.cmu.edu\/blog\/2013\/02\/03\/Using-Lagrange-multipliers-in-optimization\/)","0a88da60":"Lagrangian analysis seeks to find minimums and maxima for prediction purposes. First find the Lagrangian L such that:\n\nf(x,y) = L g(x,y)\n"}}