{"cell_type":{"b8978e99":"code","9f96ba33":"code","1e40b750":"code","e70d449e":"code","767e52b3":"code","0870d517":"code","708955e1":"code","5873c555":"code","3824ecd3":"code","80ee65eb":"code","c68b78bc":"code","2aab01eb":"code","66c8fa82":"code","f135bd90":"code","1823d943":"code","9a00098a":"code","01f992c6":"code","909747b8":"code","d497f74f":"code","2313115f":"code","5e08b3c5":"code","b90d6b06":"code","e7a40e57":"code","526c46bd":"code","4da5c6d0":"code","6a3c8c57":"code","0b839e28":"code","24fdfdd9":"markdown","970e3de1":"markdown","e50324f1":"markdown","ae6e0b72":"markdown","b8faffff":"markdown","b2368b23":"markdown","805aeea4":"markdown","949a37eb":"markdown","f1b37b93":"markdown","1da290a7":"markdown","213c7e56":"markdown","c15e086c":"markdown","ed22d286":"markdown","6e81bda6":"markdown","d74b04bc":"markdown","a3914db2":"markdown","5b7fd256":"markdown","db5455e7":"markdown","832f2ea4":"markdown","1119a259":"markdown","f0a6f135":"markdown","0fe1bb48":"markdown","9f05ebef":"markdown","cebfcf83":"markdown","a0f34716":"markdown","4bc0940d":"markdown","f90408e3":"markdown","f26a6de4":"markdown","95cb99f7":"markdown","3c5decd7":"markdown","e0af7a49":"markdown","3b139305":"markdown","ce537826":"markdown","4937fc02":"markdown","5c3650de":"markdown","c153256f":"markdown","1ca4ab2b":"markdown","600a05e6":"markdown","64920393":"markdown","21262caf":"markdown","0f8118fc":"markdown","1fed85aa":"markdown","27ba83ea":"markdown","3b4474bc":"markdown"},"source":{"b8978e99":"import torch\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\n\nimport torch.nn as nn                     # neural networks\nimport torch.nn.functional as F           # layers, activations and more\nimport torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\nfrom torch.optim import lr_scheduler\n\nfrom tqdm.autonotebook import tqdm\n\n#nltk\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nimport string\n\n#!pip install git+https:\/\/github.com\/gmihaila\/ml_things\n#from ml_things import plot_dict\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tokenizers import ByteLevelBPETokenizer\nfrom transformers import RobertaModel, RobertaConfig\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import logging\nlogging.set_verbosity_warning()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","9f96ba33":"train_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df.drop(314, inplace=True) # This row was found to have 'nan' values, so dropping it\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text']\n\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').reset_index(drop=True)\ntest_df['text'] = test_df['text'].astype(str)","1e40b750":"pd.isnull(train_df.selected_text).sum()\ntrain_df.loc[pd.isnull(train_df.selected_text),:]","e70d449e":"import seaborn as sns\nax = sns.countplot(train_df.sentiment)\nplt.xlabel('Sentiment balance in the data')","767e52b3":"PATH = \"..\/input\/roberta-base\"\n\ntokenizer = ByteLevelBPETokenizer(\n            vocab = f'{PATH}\/vocab.json', \n            merges= f'{PATH}\/merges.txt', \n            add_prefix_space = True,\n            lowercase=True)\n\ntokenizer.enable_truncation(max_length=512)    # since length cannot be set, use enable_truncation() instead","0870d517":"MAX_LENGTH = 100\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 16\nEPOCHS = 12","708955e1":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","5873c555":"val1 = tokenizer.encode('negative').ids\nval2 = tokenizer.encode('positive').ids\nval3 = tokenizer.encode('neutral').ids\nval1, val2, val3","3824ecd3":"class TextDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n        #data loading\n        self.df = df\n        self.selected_text = \"selected_text\" in df\n        self.tokenizer = tokenizer\n        self.max_length = MAX_LENGTH\n        \n    def __len__(self):\n        #len(dataset) i.e., the total number of samples\n        return len(self.df)\n    \n    '''\n    def cleanText(self, text):\n        #clean the text\n        text = str(text).lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = ' '.join([word for word in text.split(' ') if word not in stop])\n        text = text.encode('ascii', 'ignore').decode()\n        text = re.sub(r'https*\\S+', ' ', text)\n        text = re.sub(r'@\\S+', ' ', text)\n        text = re.sub(r'#\\S+', ' ', text)\n        text = re.sub(r'\\'\\w+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n        text = re.sub(r'\\w*\\d+\\w*', '', text)\n        text = re.sub(r'\\s{2,}', ' ', text)\n        text = re.sub(r\"^\\s+|\\s+$\", \"\", text) \n        #text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n        #text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n        return text\n    '''\n\n    \n    def get_data(self, row):\n        #processing the data\n        text = \" \"+\" \".join(row.text.lower().split()) # clean the text\n        encoded_input = self.tokenizer.encode(text) # the sentence to be encoded\n        \n        sentiment_id = {\n                'positive': 1313,\n                'negative': 2430,\n                'neutral': 7974\n            }  # stating the ids of the sentiment values \n        \n        #print ([list((i, encoded_input[i])) for i in range(len(encoded_input))])\n        '''\n        # The input_ids are the sentence or sentences represented as tokens. \n        # There are a few BERT special tokens that one needs to take note of:\n\n        # [CLS] - Classifier token, value: [101] \n        # [SEP] - Separator token, value: [102]\n        # [PAD] - Padding token, value: 0\n\n        # Bert expects every row in the input_ids to have the special tokens included as follows:\n\n        # For one sentence as input:\n        # [CLS] ...word tokens... [SEP]\n\n        # For two sentences as input:\n        # [CLS] ...sentence1 tokens... [SEP]..sentence2 tokens... [SEP]\n        '''\n        \n        input_ids = [101] + [sentiment_id[row.sentiment]] + [102] + encoded_input.ids + [102]\n            \n        '''\n        id: unique identifier for each token\n        offset: starting and ending point in a sentence\n        '''        \n                 \n        #ID offsets       \n        offsets = [(0, 0)] * 3 + encoded_input.offsets + [(0, 0)]     # since first 3 are [CLS] ...sentiment tokens... [SEP]\n        \n        \n        pad_len = self.max_length - len(input_ids)    \n        if pad_len > 0:\n            input_ids += ([0] * pad_len)\n            offsets += ([(0, 0)] * pad_len)\n                       \n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        \n        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n        masks = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0))  \n        '''\n        # The attention mask has the same length as the input_ids(or token_type_ids). \n        # It tells the model which tokens in the input_ids are words and which are padding. \n        # 1 indicates a word (or special token) and 0 indicates padding.\n\n        # For example:\n        # Tokens: [101, 7592, 2045, 1012,  102,    0,    0,    0,    0,    0]\n        # Attention mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n        '''\n        \n        masks = torch.tensor(masks, dtype=torch.long)\n        offsets = torch.tensor(offsets, dtype=torch.long)\n                \n        return input_ids, masks, text, offsets\n    \n    def get_target_ids(self, row, text, offsets):\n        # preparing data only for the training\n        selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n\n        string_len = len(selected_text) - 1\n        \n        idx0 = None\n        idx1 = None\n            \n        for ind in (position for position, line in enumerate(text) if line == selected_text[1]):\n            if \" \" + text[ind: ind+string_len] == selected_text:\n                idx0 = ind\n                idx1 = ind + string_len - 1\n                break\n                \n        char_targets = [0] * len(text)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n        '''\n        char_targets only give 1 to the part of the selected_text within a text\n        e.g: [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n        Here, for the length of the text, [1] is only at the index position of the selected text.\n        This helps us get the start and end indices of the selected text in the next stage.\n        '''               \n        # Start and end tokens\n        target_idx = []\n        for k, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                try:\n                    target_idx.append(k)\n                except:\n                    continue\n\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n                \n        return selected_text, targets_start, targets_end\n    \n   \n    \n    def __getitem__(self, index): # addressing each row by its index\n        #dataset[index] i.e., generates one sample of data\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, text, offsets = self.get_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['text'] = text\n        data['offsets'] = offsets\n        data['sentiment'] = row.sentiment\n        \n        if self.selected_text:   #checking if selected text exists\n            # This part only exists in the training\n            selected_text,start_index, end_index = self.get_target_ids(row, text, offsets)\n            data['start_index'] = start_index\n            data['end_index'] = end_index   \n            data['selected_text'] = selected_text\n                \n        return data\n","80ee65eb":"e = TextDataset(train_df, tokenizer, MAX_LENGTH)\ne[1]","c68b78bc":"class TextModel(nn.Module):\n     \n    def __init__(self):\n        super(TextModel, self).__init__()\n        # RoBERTa encoder \n        config = RobertaConfig.from_pretrained(\n            '..\/input\/roberta-base\/config.json', output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            '..\/input\/roberta-base\/pytorch_model.bin', config=config)\n\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n    \n        self.drop0 = nn.Dropout(0.15)\n        self.l0 = nn.Linear(config.hidden_size * 2,config.hidden_size) \n        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        self.drop1 = nn.Dropout(config.hidden_dropout_prob)\n        self.l1 = nn.Linear(config.hidden_size, 2)  # The output will have two dimensions- start and end logits\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        torch.nn.init.normal_(self.l0.bias, 0)\n        \n    def forward(self, ids, masks): #, token_type_ids\n        # Return the hidden states from the RoBERTa backbone\n        # Type: torch tensor\n        last_hidden_state, pooled_output, hidden_states = self.roberta(input_ids=ids, attention_mask=masks, return_dict=False)\n        # input_ids.shape and attention_mask.shape both will be of the size (batch size x seq length)\n        #print(last_hidden_state.shape) : torch.Size([24, 100, 768])\n        # But why 768? \n        #This is the number of hidden units in the feedforward-networks. We can verify that by checking the config.\n        '''\n        About the parameters:\n        \n        input_ids (torch.LongTensor of shape (batch_size, sequence_length)) \u2013\n        Indices of input sequence tokens in the vocabulary.\n        Indices can be obtained using Tokenizer. See transformers.PreTrainedTokenizer.encode() and transformers.PreTrainedTokenizer.__call__() for details.\n\n        attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) \u2013\n        Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n        1 for tokens that are not masked,\n        0 for tokens that are masked.\n        \n        '''\n        \n        # Concatenate the last two hidden states\n        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)   \n        #out = torch.mean(out, 0) # take the mean along axis 0\n        \n        # adding dropouts and linear layers\n        out = self.drop0(out)\n        out = F.relu(self.l0(out))\n        out = self.drop1(out) \n        out = self.l1(out) \n        \n        #splitting the tensor into two logits \n        start_logits, end_logits = out.split(1, dim=-1) # dimension along which to split the tensor.\n        # Return a tensor with all the dimensions of input of size 1 removed, for both the logits.\n        start_logits = start_logits.squeeze()  #Squeezing a tensor removes the dimensions or axes that have a length of one\n        end_logits = end_logits.squeeze() \n        \n        return start_logits, end_logits        ","2aab01eb":"def train_val_dataloaders(df, train_idx, val_idx, batch_size):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TextDataset(train_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=0,   # to avoid multi-process, keep it at 0\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TextDataset(val_df, tokenizer, MAX_LENGTH),\n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict","66c8fa82":"def test_loader(df, batch_size=TEST_BATCH_SIZE):\n    loader = torch.utils.data.DataLoader(\n        TextDataset(test_df, tokenizer, MAX_LENGTH), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=0)    \n    return loader","f135bd90":"def loss_function(start_logits, end_logits, start_positions, end_positions):\n    # calculating cross entropy losses for both the start and end logits\n    loss = nn.CrossEntropyLoss(reduction='mean') # for a multi-class classification problem\n    start_loss = loss(start_logits, start_positions)\n    end_loss = loss(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","1823d943":"def get_selected_text(text, start_idx, end_idx, offsets):\n    if end_idx < start_idx:\n        end_idx = start_idx\n    select_text = \"\"\n    for idx in range(start_idx, end_idx + 1):\n        select_text += text[offsets[idx][0]: offsets[idx][1]]\n        if (idx + 1) < len(offsets) and offsets[idx][1] < offsets[idx + 1][0]:\n            select_text += \" \"\n    return select_text","9a00098a":"# evaluation metric \ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    d = (len(a) + len(b) - len(c))\n    if d != 0:\n         return float(len(c) \/ d)\n    else:\n         return 0.0","01f992c6":"def find_jaccard_score(text, selected_text, sentiment, offsets, start_logits, end_logits): #start_idx, end_idx\n    start_pred = np.argmax(start_logits) # Predicted start index using argmax\n    end_pred = np.argmax(end_logits) # Predicted end index using argmax\n    if (end_pred <= start_pred) or sentiment == 'neutral' or len(text.split()) < 2:\n        enc = tokenizer.encode(text)\n        prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])   \n    else:\n        prediction = get_selected_text(text, start_pred, end_pred, offsets)\n    true = selected_text\n    #true = get_selected_text(text, start_idx, end_idx, offsets)\n    return jaccard(true, prediction), prediction","909747b8":"train_loss = []\nval_loss = []\njac_train = []\njac_val = []","d497f74f":"def train(model, dataloaders_dict, optimizer, num_epochs, scheduler, device, filename): \n    '''\n    Train pytorch model on a single pass through the data loader.\n \n        This function is built with reusability in mind: it can be used as is as long\n        as the `dataloader` outputs a batch in dictionary format that can be passed \n        straight into the model - `model(**batch)`.\n\n      Some of the arguments:\n\n      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n          Parsed data into batches of tensors.\n \n      optimizer_ (:obj:`transformers.optimization.AdamW`):\n          Optimizer used for training.\n \n      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n          PyTorch scheduler.\n \n      device_ (:obj:`torch.device`):\n          Device used to load tensors before feeding to model.\n    '''\n    # Set device as `cuda` (GPU)\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        for key in ['train', 'val']:\n            if key == 'train':\n                model.train()\n                dataloaders = dataloaders_dict['train']\n            else:\n                model.eval()\n                dataloaders = dataloaders_dict['val']\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            # Set tqdm to add loading screen and set the length\n            loader = tqdm(dataloaders, total=len(dataloaders))\n            #print(len(dataloaders))\n                        \n            # loop over the data iterator, and feed the inputs to the network\n            # Train the model on each batch\n            for (idx, data) in enumerate(loader):\n                ids = data['ids']\n                masks = data['masks']\n                text = data['text']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_index']\n                end_idx = data['end_index']\n                sentiment = data['sentiment']\n\n                model.zero_grad()\n                optimizer.zero_grad()\n                \n                ids = ids.to(device, dtype=torch.long)\n                masks = masks.to(device, dtype=torch.long)\n                start_idx = start_idx.to(device, dtype=torch.long)\n                end_idx = end_idx.to(device, dtype=torch.long)\n\n                with torch.set_grad_enabled(key == 'train'): \n\n                    start_logits, end_logits = model(ids, masks) \n                    \n                    loss = loss_function(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if key == 'train':\n                        if idx != 0: \n                            loss.backward() # Perform a backward pass to calculate the gradients\n                        optimizer.step() # Update parameters and take a step using the computed gradient\n                        scheduler.step() # Update learning rate schedule                        \n                        \n                        # Clip the norm of the gradients to 1.0.\n                        # This is to help prevent the \"exploding gradients\" problem.\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n                        \n                    epoch_loss += loss.item() * len(ids)\n                    \n                    # Move logits to CPU\n                    # detaching these outputs so that the backward passes stop at this point\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    selected_text = data['selected_text']\n                    \n                    filtered_sentences = []\n                    for i, t_data in enumerate(text):\n                    #for i in range(len(ids)):                        \n                        jaccard_score, filtered_output = find_jaccard_score(\n                            t_data,\n                            selected_text[i],\n                            sentiment[i],\n                            offsets[i],\n                            start_logits[i], \n                            end_logits[i])\n                        epoch_jaccard += jaccard_score\n                        filtered_sentences.append(filtered_output)\n            \n            # Calculate the average loss over the training data\n            epoch_loss = epoch_loss \/ len(dataloaders.dataset)\n            # Calculate the average jaccard score over the training data\n            epoch_jaccard = epoch_jaccard \/ len(dataloaders.dataset)\n            \n            print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, key, epoch_loss, epoch_jaccard))\n            \n            # Store the loss value for plotting the learning curve.\n            if key == 'train':\n                train_loss.append(epoch_loss)\n                jac_train.append(epoch_jaccard)\n                \n            else:\n                val_loss.append(epoch_loss)\n                jac_val.append(epoch_jaccard)\n    \n    torch.save(model.state_dict(), filename)","2313115f":"def set_seed(seed):\n    # Set random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n        \nseed = 777\nset_seed(seed)","5e08b3c5":"skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)","b90d6b06":"%%time\n\nfor fold, (idxTrain, idxVal) in enumerate(skf.split(train_df, train_df.sentiment), start=1):\n    \n    print('#'*10)\n    print('### FOLD %i'%(fold))\n    print('#'*10)\n    \n    model = TextModel()\n    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01, correct_bias=False)\n    dataloaders_dict = train_val_dataloaders(train_df, idxTrain, idxVal, batch_size=TRAIN_BATCH_SIZE)\n    num_training_steps = int(len(train_df) \/ EPOCHS * TRAIN_BATCH_SIZE)\n    # warmup_proportion = float(num_warmup_steps) \/ float(num_training_steps)\n    scheduler = get_linear_schedule_with_warmup(\n                  optimizer,\n                  num_warmup_steps=0, #default #use a linear scheduler with no warmup steps\n                  num_training_steps=num_training_steps\n                )    \n    train(\n        model, \n        dataloaders_dict, \n        optimizer, \n        EPOCHS,\n        scheduler,\n        device,\n        f'bert_fold{fold}.pth')","e7a40e57":"plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\nplt.plot(range(len(val_loss)), val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.show()","526c46bd":"plt.plot(range(len(jac_train)), jac_train, 'b', label='Training jaccard score')\nplt.plot(range(len(jac_val)), jac_val, 'r', label='Validation jaccard score')\nplt.title('Training and validation jaccard score')\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Jaccard Score',fontsize=16)\nplt.show()","4da5c6d0":"%%time\n\nt_loader = test_loader(test_df)\npredictions = []\nmodels = []\nfor fold in range(skf.n_splits):\n    model = TextModel()\n    model.to(device)\n    model.load_state_dict(torch.load(f'.\/bert_fold{fold+1}.pth'))\n    model.eval()\n    models.append(model)\n\nloader = tqdm(t_loader, total=len(t_loader))\nfor (idx, data) in enumerate(loader):\n    ids = data['ids'].to(device)\n    masks = data['masks'].to(device)\n    text = data['text']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    \n    for i, t_data in enumerate(text):\n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred >= end_pred:\n            enc = tokenizer.encode(t_data)\n            prediction = tokenizer.decode(enc.ids[start_pred-1:end_pred])\n        else:\n            prediction = get_selected_text(t_data, start_pred, end_pred, offsets[i])\n        predictions.append(prediction)","6a3c8c57":"sub_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nsub_df['selected_text'] = predictions\n# post-processing trick\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)","0b839e28":"sub_df.sample(25)","24fdfdd9":"In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. For preserving the imbalanced class distribution in each fold, it is used here.","970e3de1":"A random seed is used to ensure that results of our model are reproducible. In other words, the use of this parameter ensures that anyone who runs your code will get the exact same outputs. In data science reproducibility is extremely important.","e50324f1":"### The special tokens which are used in the sentences","ae6e0b72":"### Checking for the sentiment ids in the tokenizer","b8faffff":"Observe the data dictionary generated through the TextDataset class","b2368b23":"## Dataloaders","805aeea4":"The DataLoader class, in addition to the Dataset class (as discussed earlier), takes in the following important arguments:\n\n- batch_size, which denotes the number of samples contained in each generated batch.\n\n- shuffle. If set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n\n- num_workers, which denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).","949a37eb":"The classifier delegates most of the heavy lifting to the RobertaModel. The dropout layers are used for some regularization and fully-connected layers for our output. Note that the raw output of the last layer is returned (or used in this case) since that is required for the cross-entropy loss function in PyTorch to work.","f1b37b93":"### What makes RoBERTa different?\n\nRoBERTa is an extension of BERT with changes to the pretraining procedure. The modifications include:\n\n- training the model longer, with bigger batches, over more data\n- removing the next sentence prediction objective\n- training on longer sequences\n- dynamically changing the masking pattern applied to the training data\n\n\n<img src=\"https:\/\/paperswithcode.com\/media\/methods\/Screen_Shot_2020-05-31_at_1.41.28_PM.png\" width=\"500px\">\n","1da290a7":"Check to see if there are any null values in the dataset. ","213c7e56":"### Test dataloader","c15e086c":"### Calculating jaccard similarity score\nUsing the above functions, find the similarity score between the predicted selected text and the actual text.","ed22d286":"### Loss Function\nCross-entropy is the default loss function to use for multi-class classification problems. In here, it will return the sum of the cross entropy losses for both the start and end logits. It is the preferred loss function under the inference framework of maximum likelihood, and is usually evaluated first.  ","6e81bda6":"### Plot jaccard curves","d74b04bc":"## Submission","a3914db2":"DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset. Using the DataLoader class improves both the accuracy and the loss for the model.","5b7fd256":"**Want to know more about BPE?** <br>\nBPE is a frequency-based character concatenating algorithm: it starts with two-byte characters as tokens and based on the frequency of n-gram token-pairs, it includes additional, longer tokens. For example, if the letter `e` and `r` are frequently together in the language, a new token, `er` is added to the vocabulary. Next, if `h` and `er` are frequently together, `her` is added to the vocabulary. The algorithm goes on until it reaches the wanted size.\nThe word _tokenization_ tokenized with the model RoBERTa:\n`[\u2018token\u2019, \u2018ization\u2019]`\n\n**NOTE:** If we compare it with 'bert-based-cased' model, the word _tokenization_ will be tokenized as `[\u2018token\u2019, \u2018##ization\u2019]`","db5455e7":"Similarly, for the test loader:","832f2ea4":"### Importing libraries","1119a259":"- ('[SEP]', 102) <br>   `[SEP]` - marker for ending of a sentence\n- ('[CLS]', 101) <br>   `[CLS]` - we must add this token to the start of each sentence, so BERT knows we're doing classification\n- ('[PAD]', 0) <br>   special token for padding","f0a6f135":"### Training and validation dataloaders","0fe1bb48":"There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task, one might want to use BertForSequenceClassification, BertForQuestionAnswering or something else. Instead, for this model, the basic RobertaModel is used to build the classifer on top of that.","9f05ebef":"## Acknowledgements\n\nThis notebook could not have been possible without the help of my mentor [Anshul Sharma](https:\/\/www.kaggle.com\/anshuls235) who guided me throughout the course of my learning journey, and the [Kaggle team](https:\/\/www.kaggle.com\/kaggleteam) for providing me with an opportunity to explore this platform further and utilizing it fully for developing my skills.\n\nFew notebooks which helped me in my work:\n- [TensorFlow roBERTa - [0.705]](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705)\n- [Tweet Sentiment RoBERTa PyTorch](https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch)\n- [roberta inference 5 folds](https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds)\n- [BERT Base Uncased using PyTorch](https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch)","cebfcf83":"### Function to predict the selected text using the logit values we get from the model","a0f34716":"### Loading data","4bc0940d":"### What is BERT?\n\nBERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It is a multi-layer bidirectional Transformer encoder. And it has been pre-trained on Wikipedia and BooksCorpus and requires task-specific fine-tuning.\n\n\n### What is the flow of information of a word in BERT?\nA word starts with its embedding representation from the embedding layer. Every layer does some multi-headed attention computation on the word representation of the previous layer to create a new intermediate representation. All these intermediate representations are of the same size. In the figure above, E1 is the embedding representation, T1 is the final output and Trm are the intermediate representations of the same token. In a 12-layers BERT model a token will have 12 intermediate representations.\n\n<img src=\"https:\/\/yashuseth.files.wordpress.com\/2019\/06\/fig9.png\" width=\"500px\">\n\n### What are the tasks BERT has been pre-trained on?\nMasked Language Modeling and Next Sentence Prediction.\n\n### What is Masked Language Modeling and why use it over standard language modeling?\nLanguage Modeling is the task of predicting the next word given a sequence of words. In masked language modeling instead of predicting every next token, a percentage of input tokens is masked at random and only those masked tokens are predicted. Bi-directional models are more powerful than uni-directional language models. But in a multi-layered model bi-directional models do not work because the lower layers leak information and allow a token to see itself in later layers.\n\n### What is Next Sentence Prediction?\nNext sentence prediction task is a binary classification task in which, given a pair of sentences, it is predicted if the second sentence is the actual next sentence of the first sentence.\n\n\n\nBERT\u2019s key technical innovation makes it a popular attention model to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.","f90408e3":"PyTorch allows us to create custom datasets and implement data loaders upon then. This makes programming in PyTorch very flexible.\n\nDataset \u2013 It is mandatory for a DataLoader class to be constructed with a dataset first. PyTorch Dataloaders support two kinds of datasets:\n- Map-style datasets \u2013 These datasets map keys to data samples. Each item is retrieved by a __ __get_item__()__ method implementation.\n- Iterable-style datasets \u2013 These datasets implement the __ __iter__()__ protocol. Such datasets retrieve data in a stream sequence rather than doing random reads as in the case of map datasets.\n\nHere, the map-style datasets is used.","f26a6de4":"## Functions for evaluation","95cb99f7":"## Importing libraries, loading the data and the tokenizer\n\nFor this notebook, [HuggingFace](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html) transformers are used.","3c5decd7":"### Loading tokenizer\nThe transformers library provides a wide variety of transformer models (including roBERta), which work on PyTorch (or TensorFlow). And it even includes prebuild tokenizers which do the heavy lifting for us.\n\nRoBERTa uses a variant called \" byte-level BPE \", the main benefit it gives is that it results in a smaller vocabulary while maintaining the quality of splits. The tokenizer used here is byte-level BPE (BBPE), since it is compacter than character vocabulary and has no out-of-vocabulary tokens, and also more efficient than using pure bytes only. Studies show that BBPE has comparable performance to BPE while its size is only 1\/8 of that for BPE. With the same vocabulary size, BBPE segments sentences into shorter sequences than character-based methods do, leading to faster training and inference. Also it has some additional convenience features like `lowercase=True` and `addprefixspace=True`.","e0af7a49":"## Overview\n\n\n\nThis notebook is a PyTorch template for solving  Kaggle's Tweet Sentiment Extraction competition, using RoBERTa. All the steps are shown followed up with exaplanations and reasons for their usage. Starting with the question, `why is RoBERTa used?`\n\n\nBut before that, let me explain what BERT does....","3b139305":"## Build the RoBERTa model architecture\n\nA pretrained roBERTa base model is used with a custom question answer head, where the tweet text takes the role of the 'question', and the selected text takes the role of an 'answer'. It follows the pattern:\n<br>text, selected_text = \"what interview! leave me alone\", \"leave me alone\"\n<br>input_text = \"[CLS] \" + text + \" [SEP] \" + selected_text + \" [SEP]\"","ce537826":"To define the custom dataset, we need to override two major functions of the torch.util.data.Dataset class \u2013 __ __len__ __ and __ __getitem__ __ \u2013 which are used to retrieve the size of the dataset and get a sample item from a particular index respectively.","4937fc02":"As you can see from the above, the function for clean text is commented out. This is because BERT uses the BPE based technique for tokenization. And in this approach an out of vocabulary word is progressively split into subwords and the word is then represented by a group of subwords. Since the subwords are part of the vocabulary, we have learned representations an context for these subwords and the context of the word is simply the combination of the context of the subwords.\n\n\nAlthough stop words lack lexical content, they are not entirely devoid of content. Since stop words contribute meaning, and because BERT is sensitive to such meanings, stop words need not be eliminated.","5c3650de":"### Evaluation metric\nJaccard Similarity is also known as the Jaccard index and Intersection over Union. Jaccard Similarity matric used to determine the similarity between two text document means how the two text documents close to each other in terms of their context that is how many common words are exist over total words. \n\n\nFor this dataset, it is essential to use this metric for getting a similarity score between the text and the selected text of the train dataset, which we can further use for making predictions of the selected text for the test text data.","c153256f":"<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTWUcN4geWf3-WTk1abNbOiBNMCugcc35QMfA&usqp=CAU\" width=\"500px\">","1ca4ab2b":"# Extracting support phrases for sentiment labels from Tweets","600a05e6":"Checking the class (or sentiment) imbalance in the data","64920393":"## Token ids for a few common tokens","21262caf":"### Plot loss curves","0f8118fc":"## Training the model\n\n","1fed85aa":"## Preprocessing\nSince machine learning models do not work with raw text, these texts are needed to be converted to numbers (of some sort). BERT (or roBERTa) requires more attention:\n- Add special tokens to separate sequences and do the classification ([CLS], [SEP])\n- Passing sequences of constant length (by introducing padding)\n- Creating array of 0s (pad token) and 1s (real token) called _attention mask_\n\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked with [CLS] and the end of one by [SEP].\n\nThe details of the masking procedure for each sentence are the following:\n\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by <mask>.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n    \n Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n    \nBoth the train and text are tokenized in the same way, just creating some additional conditions for the training set.","27ba83ea":"The following is a Custom dataset which consists of 3 main classes:\n\n- __ __init__()__: Constructor function. Deals with class instance initalization, variable definition, etc\n- __ __getitem__()__: This function deals with getting the elements when the dataset is called in iteration\n- __ __len__()__: Length function overload. This function just returns the length of the dataset.","3b4474bc":"Setting some configurations for the model:"}}