{"cell_type":{"f7970cac":"code","9284fa54":"code","c86ce75d":"code","8a9065e3":"code","205a4d60":"code","c57c6af4":"code","3416d21d":"code","b0c654f9":"code","75b9cae2":"code","98d8e5c8":"code","6aa8f17e":"code","85002522":"code","13448492":"code","5f963609":"code","4614a9f2":"code","21a56f75":"code","55f826e5":"code","8996a744":"code","bc8a13f6":"code","337077e2":"code","906f6565":"code","00e8c829":"code","af2ec131":"code","4c664168":"code","aab960fd":"code","08e19100":"code","3045ce08":"code","6e6ce322":"code","35ab1956":"code","a0d1d246":"code","9f3a3917":"code","c50fb0ea":"code","0b5a20c8":"code","17d9acbe":"code","7bf3bb66":"code","0554698a":"code","0d659998":"code","677daf7d":"code","9926a12e":"code","eb93b523":"code","89f5fca1":"code","06f484c3":"code","4d3460c3":"code","f6f01b9d":"code","6ec1715d":"code","da8502a6":"code","6ab236d9":"code","ec1fa325":"code","c0178369":"code","025cc117":"code","6e49b424":"code","3ba54b53":"code","1d3c9e50":"code","bb4d1bcb":"code","c34c2d45":"code","6770511c":"code","05eb9137":"code","34403149":"code","ee9fe3df":"code","c15a7bac":"code","5928bedd":"code","cbb59a08":"code","f77e7719":"code","d20c6c2c":"code","73739a8c":"code","a3a76b6e":"code","1f6ceda1":"code","dac5f0f6":"code","9f69aa64":"code","f15d368e":"code","a67aa895":"code","7bbd35f8":"code","fbec28c7":"code","c570dff4":"code","e5137e83":"code","1a96ad6c":"code","3d32ae12":"code","ad18f811":"code","a96081a5":"code","84adfbae":"code","d318717f":"code","a2e437cb":"code","25fed98a":"code","c31dc4f8":"code","8d4b1846":"code","7fe918df":"code","7d3469fa":"code","9e173d38":"code","c56a227e":"code","d84ff756":"code","792a4653":"code","3b174874":"code","efef6477":"code","c9d9f2c5":"code","4b66f814":"code","6dbdbf94":"code","80d6b6d5":"code","68e60a43":"code","52fdf120":"code","0d0084c4":"code","a4c12342":"markdown","d4216bab":"markdown","1b129323":"markdown","560d2cd5":"markdown","1d28a86e":"markdown","6651f58e":"markdown","e584a1c9":"markdown","ca9fbb06":"markdown","74a44555":"markdown","f1cd0720":"markdown","ff2dd287":"markdown","cfeb192d":"markdown","a1fda7f8":"markdown","f3105c56":"markdown","b0180db1":"markdown","c3cc643a":"markdown","9d7d02fb":"markdown","c16a16ee":"markdown","bbbada90":"markdown","73f0b01a":"markdown","c586c65c":"markdown","66974626":"markdown","02906b39":"markdown","f8b9ef0d":"markdown","d20f729e":"markdown","544a96a9":"markdown","8cba687d":"markdown","14817c28":"markdown","a084aa9f":"markdown"},"source":{"f7970cac":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport nltk\nimport string","9284fa54":"# Create a SQL connection to our SQLite database\n#con = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite')","c86ce75d":"#type(con)","8a9065e3":"#pd.read_sql_query(\"SELECT * FROM Reviews\", con)","205a4d60":"#df=pd.read_sql_query(\"SELECT * FROM Reviews\", con)","c57c6af4":"#pd.read_sql_query(\"SELECT * FROM Reviews LIMIT 3\", con)","3416d21d":"df = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\n\nprint(df.shape)\ndf.head()","b0c654f9":"df.shape","75b9cae2":"from textblob import TextBlob","98d8e5c8":"TextBlob(df['Summary'][0]).sentiment.polarity","6aa8f17e":"## takes 3 mins \npolarity=[] # list which will contain the polarity of the comments\n\nfor i in df['Summary']:\n    try:\n        polarity.append(TextBlob(i).sentiment.polarity)   \n    except:\n        polarity.append(0)","85002522":"len(polarity)","13448492":"data=df.copy()","5f963609":"data['polarity']=polarity","4614a9f2":"data.head()","21a56f75":"data['polarity'].nunique()","55f826e5":"data_positive = data[data['polarity']>0]\ndata_positive.shape","8996a744":"from wordcloud import WordCloud, STOPWORDS","bc8a13f6":"stopwords=set(STOPWORDS)","337077e2":"positive=data_positive[0:200000]","906f6565":"\ntotal_text= (' '.join(data_positive['Summary']))","00e8c829":"len(total_text)","af2ec131":"total_text[0:10000]","4c664168":"import re\ntotal_text=re.sub('[^a-zA-Z]',' ',total_text)","aab960fd":"total_text[1:500]","08e19100":"## remove extra spaces\ntotal_text=re.sub(' +',' ',total_text)","3045ce08":"total_text[0:10000]","6e6ce322":"len(total_text)","35ab1956":"wordcloud = WordCloud(width = 1000, height = 500,stopwords=stopwords).generate(total_text)\nplt.figure(figsize=(15,5))\nplt.imshow(wordcloud)\nplt.axis('off')","a0d1d246":"data_negative = data[data['polarity']<0]\ndata_negative.shape","9f3a3917":"data_negative.head()","c50fb0ea":"total_negative= (' '.join(data_negative['Summary']))","0b5a20c8":"total_negative[1:10000]","17d9acbe":"import re\ntotal_negative=re.sub('[^a-zA-Z]',' ',total_negative)","7bf3bb66":"len(total_negative)","0554698a":"total_negative[1:1000]","0d659998":"total_negative=re.sub(' +',' ',total_negative)","677daf7d":"len(total_negative)","9926a12e":"\nwordcloud = WordCloud(width = 1000, height = 500,stopwords=stopwords).generate(total_negative)\nplt.figure(figsize=(15,5))\nplt.imshow(wordcloud)\nplt.axis('off')","eb93b523":"df['UserId'].shape","89f5fca1":"df['UserId'].nunique()","06f484c3":"df.head()","4d3460c3":"raw=df.groupby(['UserId']).agg({'Summary':'count', 'Text':'count','Score':'mean','ProductId':'count'}).sort_values(by='Text',ascending=False)\nraw","f6f01b9d":"raw.columns=['Number_of_summaries','num_text','Avg_score','Number_of_products_purchased']\nraw","6ec1715d":"user_10=raw.index[0:10]\nnumber_10=raw['Number_of_products_purchased'][0:10]\n\nplt.bar(user_10, number_10, label='java developer')\nplt.xlabel('User_Id')\nplt.ylabel('Number of Products Purchased')\nplt.xticks(rotation='vertical')","da8502a6":"df.head()","6ab236d9":"## picking a random sample\nfinal=df.sample(n=2000)","ec1fa325":"final=df[0:2000]","c0178369":"final.isna().sum()","025cc117":"final.duplicated().sum()","6e49b424":"final.head()","3ba54b53":"len(final['Text'][0].split(' '))","1d3c9e50":"final['Text'][0]","bb4d1bcb":"\ndef calc_len(text):\n    return (len(text.split(' ')))","c34c2d45":"final['Text_length']=final['Text'].apply(calc_len)","6770511c":"import plotly.express as px\npx.box(final, y=\"Text_length\")","05eb9137":"sns.countplot(final['Score'], palette=\"plasma\")","34403149":"final['Text'] =final['Text'].str.lower()\nfinal.head(10)","ee9fe3df":"final['Text'][164]","c15a7bac":"import re\nre.sub('[^a-zA-Z]',' ',final['Text'][164])","5928bedd":"# define punctuation\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n\ndata= final['Text'][164]\n\n# remove punctuation from the string\nno_punct = \"\"\nfor char in data:\n    if char not in punctuations:\n        no_punct = no_punct + char\n\n# display the unpunctuated string\nno_punct","cbb59a08":"def remove_punc(review):\n    import string\n    punctuations =string.punctuation\n    # remove punctuation from the string\n    no_punct = \"\"\n    for char in review:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    return no_punct","f77e7719":"final['Text'] =final['Text'].apply(remove_punc)","d20c6c2c":"final.head()","73739a8c":"final['Text'][164]","a3a76b6e":"import re\nfrom nltk.corpus import stopwords","1f6ceda1":"review='seriously this product was as tasteless as they come there are much better tasting products out there but at 100 calories its better than a special k bar or cookie snack pack you just have to season it or combine it with something else to share the flavor'","dac5f0f6":"re=[word for word in review.split(' ') if word not in set(stopwords.words('english'))]\nstr=''\nfor wd in re:\n    str=str+wd\n    str=str+' '\nstr","9f69aa64":"re=[word for word in review.split(' ') if word not in set(stopwords.words('english'))]\n' '.join(re)","f15d368e":"def remove_stopwords(review):\n    return ' '.join([word for word in review.split(' ') if word not in set(stopwords.words('english'))])","a67aa895":"remove_stopwords(review)","7bbd35f8":"final.shape","fbec28c7":"final.columns","c570dff4":"final['Text'] = final['Text'].apply(remove_stopwords)","e5137e83":"final.head()","1a96ad6c":"final['Text'].str.contains('http?').sum()","3d32ae12":"final['Text'].str.contains('http').sum()","ad18f811":"pd.set_option('display.max_rows',50) #better to use max 2000 to looking for True \nfinal['Text'].str.contains('http',regex=True)","a96081a5":"final['Text'][21]","84adfbae":"final['Text'][21]","d318717f":"review=final['Text'][21]\nreview","a2e437cb":"import re","25fed98a":"url_pattern = re.compile(r'href|http.\\w+')\nurl_pattern.sub(r'', review)","c31dc4f8":"import re\ndef remove_urls(review):\n    url_pattern = re.compile(r'href|http.\\w+')\n    return url_pattern.sub(r'', review)","8d4b1846":"final['Text'] = final['Text'].apply(remove_urls)","7fe918df":"final.head()","7d3469fa":"final['Text'].str.contains('http').sum()","9e173d38":"final['Text'][34]","c56a227e":"final['Text'][34].replace('br','')","d84ff756":"for i in range(len(final['Text'])):\n    final['Text'][i]=final['Text'][i].replace('br','')","792a4653":"data2=final.copy()","3b174874":"data2['Text'][34]","efef6477":"data2.shape","c9d9f2c5":"data2.dtypes","4b66f814":"from wordcloud import WordCloud, STOPWORDS","6dbdbf94":"stopwords = set(STOPWORDS)","80d6b6d5":"data2.head()","68e60a43":"comment_words = '' \nfor val in data2['Text']:\n    # typecaste each val to string\n    \n    # split the value \n    tokens = val.split() \n    \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n    comment_words=comment_words+ \" \".join(tokens)+\" \"","52fdf120":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)","0d0084c4":"# plot the WordCloud image                        \nplt.figure(figsize = (8, 8)) \nplt.imshow(wordcloud) \nplt.axis(\"off\")","a4c12342":"#### Removing the Duplicates if any","d4216bab":"##### as we will see we have lots of br in my data, let me remove wherever i have br","1b129323":"#### These are the Top 10 Users so we can recommend more & more Prodcuts to these Usser Id as there will be a high probability that these person are going to be buy more","560d2cd5":"The column or features in the dataset:\nId\nProductId \u2014 unique identifier for the product\nUserId \u2014 unqiue identifier for the user\nProfileName\nHelpfulnessNumerator \u2014 number of users who found the review helpful\nHelpfulnessDenominator \u2014 number of users who indicated whether they found the review helpful or not\nScore \u2014 rating between 1 and 5\nTime \u2014 timestamp for the review\nSummary \u2014 brief summary of the review\nText \u2014 text of the review","1d28a86e":"### Text Pre-Processsing","6651f58e":"#### Analyze Score","e584a1c9":"#### Conclusion-->>\n    Seems to have Almost 50 percent users are going to give their Feedback limited to 50 words whereas there are only few users who are going give Lengthy Feedbacks","ca9fbb06":"#### we will observe we have some kind of URLs over here in my data that is definitely a kind of Dirtines in data, so we have to clean this data & make ready data for the analysis purpose","74a44555":"#### check missing values in dataset","f1cd0720":"#### reading some n number of rows, use LIMIT over ther","ff2dd287":"#### using join to convert list into string","cfeb192d":"#### logic to remove punctuations or all the special characters","a1fda7f8":"#### or we can also Load the dataset using pandas","f3105c56":"#### check if urls is present in Text column or not","b0180db1":"## Lets perform EDA for the Neagtive sentences","c3cc643a":"### Lets perform EDA for the Positve sentences\u00b6","9d7d02fb":"#### Removal of Stopwords","c16a16ee":"Advantages of Word Clouds :\nAnalyzing customer and employee feedback.\nIdentifying new SEO keywords to target.","bbbada90":"#### drawback of this re.sub in this use-case is, it will remove some numerical data too & may be that numerical values matters alot\n#### thats way, I am going to create my own logic over here,that will remove all the special character","73f0b01a":"## Analyse to what User Amazon Can recommend more product","c586c65c":"####  Removal of urls","66974626":"#### reading data from Sqlite database","02906b39":"### What is sentiment analysis?\n    Sentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text\n    Some examples of applications for sentiment analysis include:\n\n    1.Analyzing the social media discussion around a certain topic\n    2.Evaluating survey responses\n    3.Determining whether product reviews are positive or negative\n\n    Sentiment analysis is not perfect.It also cannot tell you why a writer is feeling a certain way. However, it can be useful to quickly summarize some qualities of text, especially if you have so much text that a human reader cannot analyze it.For this project,the goal is to to classify Food reviews based on customers' text.","f8b9ef0d":"### Pre-process your Data in a Depth","d20f729e":"### Analyse Length of Comments whether Customers are going to give Lengthy comments or short one","544a96a9":"#### Create function to remove punctuations in your review","8cba687d":"#### as data is so huge,so if your system takes a lot for the execution , u can considered some sample of data from entire data,\n    as may be some of you have not that much good specifications in terms of processor ,RAM & HArd Disk..\n    so according to system specifications,u can considered some sample of data,if u have not issue with your specifications,\n    u can go ahead with this bulky data","14817c28":"#### Amazon can recommend more products to only those who are going to buy more or to one who has a better conversion rate,so lets ready data according to this problem statement","a084aa9f":"#### perform this task using function as I have to apply this logic on my entire column"}}