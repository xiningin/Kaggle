{"cell_type":{"02cef6cc":"code","a67b3e96":"code","dc9815c2":"code","89a125be":"code","8b88aece":"code","5f6cf687":"code","70991303":"code","b88e7cf8":"code","cacd2840":"code","9f5c9560":"code","daa8315b":"code","31271aad":"code","c620004d":"code","9035fcd4":"code","156ac3a1":"code","5c58ed7d":"code","e68e9dcb":"code","8266f0c8":"code","26b9bde9":"code","9a0855b9":"code","d0f59acc":"code","a2956967":"code","e5c70d65":"code","685bc10f":"code","4d7e0285":"code","e3a0a4ac":"code","bce6f51e":"code","49df2cd6":"code","e51edeea":"code","797c5207":"code","7a305bb8":"code","356b7555":"code","21db71e3":"code","7cd91046":"code","8565ad29":"code","9146945a":"code","c22e46fd":"code","8a8891ad":"code","dce6d47c":"code","ff7542d3":"code","f4a30cba":"code","8f37cd03":"code","34fa1738":"code","0bcf647d":"code","c7a95362":"code","bc8d2250":"code","6fb60d23":"code","2dea3f5e":"code","3aeeab59":"code","9bc8457b":"code","0bea2240":"code","b2dc2cab":"code","d94fd5dd":"code","a2f1ba69":"code","36d8a5e5":"code","faa9fd1e":"code","7ac8e95f":"code","ea9b4e5d":"code","525e7659":"code","a1b9ddf0":"code","342c3161":"code","b92c4532":"code","bf037aeb":"code","4afc1cf5":"code","f1523404":"code","5ea494f9":"code","93c8ec4d":"code","e274503a":"code","379d67d5":"code","d89cb051":"code","393a4c30":"code","14b3d8ff":"code","6e273897":"code","de990985":"code","62c14aef":"code","0d3dc881":"code","a53adf42":"code","c0e43335":"code","70c5afa9":"code","b86ab7cf":"code","bac55830":"code","54049fd7":"code","edf1b2eb":"code","f27d35ea":"code","59ffd36a":"code","34f24844":"code","33e5c46d":"code","c1bb4649":"code","805cf435":"code","330d1c61":"code","3052737b":"code","86c36a60":"code","3fbb7493":"code","8beafc7f":"code","f93b5a6d":"code","6036ce34":"code","727cd3e1":"code","8b742881":"code","b90702ea":"code","749f2c90":"code","fb5a51dd":"code","2adba664":"code","723e9498":"code","eebfc061":"code","3a8f1fb2":"code","dcd12fb7":"code","66fc4c38":"code","aed6cf96":"code","8f197ef6":"code","45caab59":"code","1bee8d18":"markdown","7a88e9ad":"markdown","3af0061f":"markdown","21ed44fb":"markdown","0249f88c":"markdown","dbd5ed4a":"markdown","00bcac07":"markdown","8c8c1031":"markdown","54b9dbe3":"markdown","2e916e1a":"markdown","7e08a257":"markdown","fdaffcfb":"markdown","17a55da8":"markdown","88a6c8ec":"markdown","8f0636d0":"markdown","f35a5193":"markdown","f2e82823":"markdown","36b1fd47":"markdown","39ff8179":"markdown","e21138c0":"markdown","12ad12d1":"markdown","1ccfaccc":"markdown","44a73d1e":"markdown","7b9b71dc":"markdown","e102caab":"markdown","8ee58946":"markdown","eb3b2405":"markdown","ab96acfc":"markdown","cf265c92":"markdown","fcf5560f":"markdown","aa6c2187":"markdown","f9ae49b0":"markdown","d7103aef":"markdown","e1d34ed4":"markdown","c14bb84d":"markdown","08e21ed5":"markdown","21424e63":"markdown","4152b48b":"markdown","7a89fc1c":"markdown","53f8bb35":"markdown","7909536d":"markdown","a93c0205":"markdown","8659f9f2":"markdown","c18e6113":"markdown","7b1d6f53":"markdown","b41d1574":"markdown","554d39dc":"markdown","c3b72026":"markdown","a910ab8d":"markdown","183f14c1":"markdown","0de8f86f":"markdown","62761bcf":"markdown","afda9342":"markdown","4a974c13":"markdown","41169273":"markdown","30a65156":"markdown","e2f2fffb":"markdown","049f89cc":"markdown","c844ea69":"markdown","6288ca5a":"markdown"},"source":{"02cef6cc":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","a67b3e96":"#https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df\n","dc9815c2":"train = import_data(\"train.csv\")\ntest = import_data(\"test.csv\")\n\nprint(\"\\n\\nTrain Size : \\t{}\\nTest Size : \\t{}\".format(train.shape, test.shape))","89a125be":"train.head(2)","8b88aece":"sns.countplot(train['target'])","5f6cf687":"train.target.value_counts()","70991303":"mylst = list(df_train[\"target\"].value_counts())\nzero = round(float((mylst[0]\/sum(mylst))*100),2)\none = round(float((mylst[1]\/sum(mylst))*100),2)\nprint('The dataset has {zero} % of target 0 and {one} % of target 1'.format(zero=zero, one=one))","b88e7cf8":"t0=train[train['target']==0]\nt1=train[train['target']==1]","cacd2840":"print('Distributions of 1st 100 features')\nplt.figure(figsize=(20,16))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7,4,i + 1)\n    plt.hist(t0[col],label='target 0',color='Red')\n    plt.hist(t1[col],label='target 1',color='Blue')\n    plt.title(col)\n    plt.grid()\n    plt.legend(loc='upper right')\n    plt.tight_layout()","9f5c9560":"train.drop(['ID_code'],axis=1,inplace=True)\nlabels=train['target']\ntrain.drop(['target'],axis=1,inplace=True)","daa8315b":"train.select_dtypes(include='float16')","31271aad":"train.astype(np.float64).describe()","c620004d":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","9035fcd4":"missing_data(train)","156ac3a1":"missing_data(test)","5c58ed7d":"features = train.columns.tolist()","e68e9dcb":"plt.figure(figsize=(16,6))\nplt.title(\"Mean in train and test set\")\nsns.distplot(train[features].mean(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].mean(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","8266f0c8":"plt.figure(figsize=(16,6))\nplt.title(\"Standard Deviation in train and test set\")\nsns.distplot(train[features].std(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].std(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","26b9bde9":"plt.figure(figsize=(16,6))\nplt.title(\"Skewness in train and test set\")\nsns.distplot(train[features].skew(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].skew(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","9a0855b9":"plt.figure(figsize=(16,6))\nplt.title(\"Min in train and test set\")\nsns.distplot(train[features].min(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].min(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","d0f59acc":"plt.figure(figsize=(16,6))\nplt.title(\"Min in train and test set\")\nsns.distplot(train[features].max(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].max(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","a2956967":"features = train.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","e5c70d65":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","685bc10f":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","4d7e0285":"idx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","e3a0a4ac":"train.head(2)","bce6f51e":"train.drop(['kurt'],axis=1,inplace=True)","49df2cd6":"train.head()","e51edeea":"test.head(2)","797c5207":"test.drop(['kurt','ID_code'],axis=1,inplace=True)","7a305bb8":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_data = scaler.fit_transform(train)","356b7555":"train_data.shape","21db71e3":"# choose a threshold to spot correlation above its abs()\n# try 0.08 or 0.05 to have some results, even though is not a relevant correlation \nthreshold = 0.3\ndfcorr = df_train.corr()\ndfcorr1 = dfcorr.copy()\ndfcorr1[abs(dfcorr1) < threshold] = None\ndfcorr1[abs(dfcorr1) >= threshold] = 1","7cd91046":"# all the variables have at least corr = 1 with itself so we want to know\n# which variables have more than 1 record above the threshold\ncor = dfcorr1.sum(axis=1) > 1","8565ad29":"# Listing the variables that is worth investigating on\nvar_to_check = list(cor[cor.values == True].index)","9146945a":"if len(var_to_check) > 0:\n    print('These are the variables with correlations >= {}:'.format(threshold))\n    print(str(var_to_check) + '\\n')\n    for i in var_to_check:\n        print(str(dfcorr[(abs(dfcorr[i]) >= threshold) & (abs(dfcorr[i]) != 1)][i]) + '\\n')\nelse:\n    print('There are no significant correlations to look!')","c22e46fd":"dfcorr[(dfcorr!=1) & (abs(dfcorr)>0.1)].count()","8a8891ad":"!pip install lightgbm","dce6d47c":"import lightgbm as lgb","ff7542d3":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}\n\n#njobs = -1","f4a30cba":"#https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.train.html#lightgbm.train\n#https:\/\/www.kaggle.com\/ashishpatel26\/kfold-lightgbm\/code \n#(learned from here how to use stratified k-fold with model)\n#https:\/\/github.com\/KazukiOnodera\/Santander-Customer-Transaction-Prediction\/blob\/master\/final_solution\/onodera\/py\/907_predict_0410-2.py\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfolds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, labels.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=labels.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=labels.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(labels, oof)))","8f37cd03":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","34fa1738":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom random import randrange, uniform\nfrom scipy.stats import chi2_contingency\n%matplotlib inline","0bcf647d":"trans = pd.read_csv(\"C:\/Users\/shali\/Desktop\/INFO 7390\/Assignments\/Assignment2\/Data\/test.csv\")","c7a95362":"for i in range(2,202):\n        #print(i)\n        q75, q25 = np.percentile(trans.iloc[:,i], [75 ,25])\n        iqr = q75 - q25\n\n        min = q25 - (iqr*1.5)\n        max = q75 + (iqr*1.5)\n        #print(min)\n        #print(max)\n       \n        trans = trans.drop(trans[trans.iloc[:,i] < min].index)\n        trans = trans.drop(trans[trans.iloc[:,i] > max].index)","bc8d2250":"trans.shape","6fb60d23":"plt.boxplot(trans['var_0'] ,vert=True,patch_artist=True)","2dea3f5e":"trans = trans.drop(trans.columns[0], axis = 1)","3aeeab59":"from  sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(trans.drop('target',axis=1), \n                                                    trans['target'], test_size=0.30, \n                                                    random_state=101)","9bc8457b":"print(x_train.shape)\nprint(x_test.shape)","0bea2240":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom math import log","b2dc2cab":"from sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nC = LogisticRegression()\n\nimport math\n\nparameter_data = [0.0001,0.001,0.01,0.1,1,5,10,20,30,40]\n\nlog_my_data = [math.log10(x) for x in parameter_data]\n\n#print(log_my_data)\nprint(\"Printing parameter Data and Corresponding Log value\")\ndata={'Parameter value':parameter_data,'Corresponding Log Value':log_my_data}\nparam=pd.DataFrame(data)\nprint(\"=\"*100)\nprint(param)\nparameters = {'C':parameter_data}\nclf = RandomizedSearchCV(C, parameters, cv=3, scoring='roc_auc', return_train_score=True, n_jobs=-1)\nclf.fit(x_train, y_train)\n\n#data={'Parameter value':[0.0001,0.001,0.01,0.1,1,5,10,20,30,40],'Corresponding Log Value':[log_my_data]}\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']\n\nplt.plot(log_my_data, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_my_data,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(log_my_data, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_my_data,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(log_my_data, train_auc, label='Train AUC points')","d94fd5dd":"def model_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    y_data_pred.extend(clf.predict_proba(data[:])[:,1])\n  \n    return y_data_pred","a2f1ba69":"from sklearn.metrics import roc_curve, auc\n\n\nneigh = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\nneigh.fit(x_train, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = model_predict(neigh, x_train)    \ny_test_pred = model_predict(neigh, x_test)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","36d8a5e5":"from sklearn.metrics import confusion_matrix\nCM = confusion_matrix(y_test, y_test_pred)\nCM = pd.crosstab(y_test, y_test_pred)\n\n#let us save TP, TN, FP, FN\nTN = CM.iloc[0,0]\nFN = CM.iloc[1,0]\nTP = CM.iloc[1,1]\nFP = CM.iloc[0,1]","faa9fd1e":"sns.heatmap(CM, annot=True, fmt=\"d\" )\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title(\"Confusion Matrix\")","7ac8e95f":"test =pd.read_csv(\"test.csv\")","ea9b4e5d":"id_code = test.iloc[:,0]","525e7659":"test = test.drop(\"ID_code\" ,axis=1)\npredictions_test = neigh.predict(test)\ndf = pd.DataFrame({\"ID_code\" :id_code ,\"target\": predictions_test})\ndf.head()","a1b9ddf0":"test_logistic = df.join(test)\ntest_logistic.to_csv('logisticmodelpred.csv')\ntest_logistic.head()","342c3161":"sns.set_style('whitegrid')\nsns.countplot(x='target',data=test_logistic,palette='RdBu_r')\ntest_logistic['target'].value_counts()","b92c4532":"from sklearn.naive_bayes import GaussianNB","bf037aeb":"neigh = GaussianNB()\nneigh.fit(x_train, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = model_predict(neigh, x_train)    \ny_test_pred = model_predict(neigh, x_test)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","4afc1cf5":"CM = confusion_matrix(y_test, y_test_pred)\nCM = pd.crosstab(y_test, y_test_pred)\n\n#let us save TP, TN, FP, FN\nTN = CM.iloc[0,0]\nFN = CM.iloc[1,0]\nTP = CM.iloc[1,1]\nFP = CM.iloc[0,1]\nsns.heatmap(CM, annot=True, fmt=\"d\" )\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title(\"Confusion Matrix\")","f1523404":"predictions_test = neigh.predict(test)","5ea494f9":"df = pd.DataFrame({\"ID_code\" :id_code ,\"target\": predictions_test})\ndf.head()","93c8ec4d":"test_nb = df.join(test)","e274503a":"test_nb.head(2)","379d67d5":"sns.set_style('whitegrid')\nsns.countplot(x='target',data=test_nb,palette='RdBu_r')\ntest_nb['target'].value_counts()","d89cb051":"test_nb.to_csv('Naive Bayes Prediction.csv')","393a4c30":"from sklearn.ensemble import RandomForestClassifier\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\nfrom sklearn.model_selection import GridSearchCV\n\nC = RandomForestClassifier()\n\nn_estimators=[10,50,100,200]\nmax_depth=[1, 5, 10, 50]\n\nimport math\n\nlog_max_depth = [math.log10(x) for x in max_depth]\nlog_n_estimators=[math.log10(x) for x in n_estimators]\n\nprint(\"Printing parameter Data and Corresponding Log value for Max Depth\")\ndata={'Parameter value':max_depth,'Corresponding Log Value':log_max_depth}\nparam=pd.DataFrame(data)\nprint(\"=\"*100)\nprint(param)\n\nprint(\"Printing parameter Data and Corresponding Log value for Estimators\")\ndata={'Parameter value':n_estimators,'Corresponding Log Value':log_n_estimators}\nparam=pd.DataFrame(data)\nprint(\"=\"*100)\nprint(param)\n\nparameters = {'n_estimators':n_estimators, 'max_depth':max_depth}\nclf = GridSearchCV(C, parameters, cv=3, scoring='roc_auc', return_train_score=True,n_jobs=-1)\nclf.fit(x_train, y_train)\n\n#data={'Parameter value':[0.0001,0.001,0.01,0.1,1,5,10,20,30,40],'Corresponding Log Value':[log_my_data]}\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']","14b3d8ff":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n#from sklearn.calibration import CalibratedClassifierCV\n\n\nneigh = RandomForestClassifier(n_estimators=100,max_depth=10,class_weight='balanced')\nneigh.fit(x_train, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = model_predict(neigh, x_train)   \ny_test_pred = model_predict(neigh, x_test)\n\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","6e273897":"CM = confusion_matrix(y_test, y_test_pred)\nCM = pd.crosstab(y_test, y_test_pred)\n\n#let us save TP, TN, FP, FN\nTN = CM.iloc[0,0]\nFN = CM.iloc[1,0]\nTP = CM.iloc[1,1]\nFP = CM.iloc[0,1]\nsns.heatmap(CM, annot=True, fmt=\"d\" )\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title(\"Confusion Matrix\")","de990985":"predictions_rfc = neigh.predict(test)","62c14aef":"df = pd.DataFrame({\"ID_code\" :id_code ,\"target\": predictions_rfc})\ndf.head()","0d3dc881":"test_rfc = df.join(test)","a53adf42":"sns.set_style('whitegrid')\nsns.countplot(x='target',data=test_rfc,palette='RdBu_r')\ntest_rfc['target'].value_counts()","c0e43335":"test_rfc.to_csv('RandomForestPrediction.csv')","70c5afa9":"df_train = train.copy()\ndf_test = test.copy()","b86ab7cf":"print(\"Train shape: \" + str(df_train.shape))\nprint(\"Test shape: \" + str(df_test.shape))","bac55830":"# Splitting the target variable and the features\nX_train = df_train.loc[:,'var_0':]\ny_train = df_train.loc[:,'target']","54049fd7":"print(X_train.shape)\nprint(y_train.shape)","edf1b2eb":"import pandas as pd\nsynthetic_samples_indexes = pd.read_csv('C:\/Users\/shali\/Desktop\/INFO 7390\/Assignments\/Assignment2\/Data\/synthetic_samples_indexes.csv')","f27d35ea":"df_test_real = test.copy()\ndf_test_real = df_test_real[~df_test_real.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))]\nX_test = df_test_real.loc[:,'var_0':]\nX_test.shape","59ffd36a":"fig = plt.figure(figsize=(5,5))\nsns.distplot(df_train[df_train['target']>0]['var_0'], hist=False,label='0', color='red')\nsns.distplot(df_train[df_train['target']==0]['var_0'], hist=False,label='1', color='green')\nplt.xlabel('var_0', fontsize=12)\nlocs, labels = plt.xticks()\nplt.tick_params(axis='x', which='major', labelsize=10)\nplt.tick_params(axis='y', which='major', labelsize=10)\n\n#balanced output","34f24844":"def get_count(df):\n    '''\n    Function that adds one column for each variable (excluding 'ID_code', 'target')\n    populated with the value frequencies\n    '''\n    for var in [i for i in df.columns if i not in ['ID_code','target']]:\n        df[var+'_count'] = df.groupby(var)[var].transform('count')\n    return df","33e5c46d":"X_tot = pd.concat([X_train, X_test])\nprint(X_tot.shape)","c1bb4649":"import time\nstart = time.time()\nX_tot = get_count(X_tot)\nend = time.time()\nprint('It took %.2f seconds\\nShape: ' %(end - start))\nprint(X_tot.shape)","805cf435":"X_train_count = X_tot.iloc[0:200000]\nX_test_count = X_tot.iloc[200000:]","330d1c61":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1, color='red')\n        sns.distplot(df2[feature], hist=False,label=label2, color='green')\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show()","3052737b":"t0 = df_train.loc[df_train['target'] == 0]\nt1 = df_train.loc[df_train['target'] == 1]\nfeatures = df_train.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","86c36a60":"features = df_train.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","3fbb7493":"# Libraries\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, train_test_split\nimport lightgbm as lgb","8beafc7f":"# 0.8 train, 0.2 dev\nX_train,X_valid,y_train,y_valid = train_test_split(X_train_count, y_train, test_size=0.2, random_state=42, stratify=y_train)","f93b5a6d":"print('X_train shape: {}\\n'.format(X_train.shape))\nprint('y_train shape: {}\\n'.format(y_train.shape))\nprint('X_valid shape: {}\\n'.format(X_valid.shape))\nprint('y_valid shape: {}'.format(y_valid.shape))","6036ce34":"# Data Augmentation 2x if y = 1 , 1x if y = 0\n#Reference\n#https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment\n\ndef augment(x,y,t=2):\n    '''\n    Data Augmentation 2x if y = 1 , 1x if y = 0\n    '''\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(int(x1.shape[1]\/2)):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(int(x1.shape[1]\/2)):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","727cd3e1":"start = time.time()\n# Trying Augmentation Only for training set!\nX_tr, y_tr = augment(X_train.values, y_train.values)\nprint('X_tr Augm shape: {}'.format(X_tr.shape))\nprint('y_tr Augm shape: {}'.format(y_tr.shape))\n\nend = time.time()\nprint('It took %.2f seconds' %(end - start))","8b742881":"X_tr = pd.DataFrame(data=X_tr,columns=X_train.columns)\ny_tr = pd.DataFrame(data=y_tr)","b90702ea":"y_tr.columns = ['target']","749f2c90":"# List of all the features\nfeatures = [c for c in X_train.columns if c not in ['ID_code', 'target']]","fb5a51dd":"# The parameters for Light Gradient Boost\nlgb_params = {\n        'bagging_fraction': 0.77,\n        'bagging_freq': 2,\n        'lambda_l1': 0.7,\n        'lambda_l2': 2,\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'min_data_in_leaf': 22,\n        'min_gain_to_split': 0.07,\n        'min_sum_hessian_in_leaf': 19,\n        'num_leaves': 20,\n        'feature_fraction': 1,\n        'save_binary': True,\n        'seed': 42,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'drop_seed': 42,\n        'data_random_seed': 42,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': 'false',\n        'num_threads': 6\n}","2adba664":"val_pred = (y_hat).sum(axis=1)\/200\npredictions = (test_hat).sum(axis=1)\/200\nscore = roc_auc_score(y_valid, val_pred)\nprint('>>> Your CV score is: ', score)","723e9498":"start = time.time()\n\niteration = 120\ny_hat = np.zeros([int(200000*0.2), 200])\ntest_hat = np.zeros([100000, 200])\ni = 0\nfor feature in ['var_' + str(x) for x in range(200)]: # loop over all the raw features\n    feat_choices = [feature, feature + '_count']\n    trn_data = lgb.Dataset(X_tr[feat_choices], y_tr) # Augmentation\n    val_data = lgb.Dataset(X_valid[feat_choices], y_valid)\n    clf = lgb.train(lgb_params, trn_data, iteration, valid_sets=[val_data], verbose_eval=-1)\n    y_hat[:, i] = clf.predict(X_valid[feat_choices])\n    test_hat[:, i] = clf.predict(X_test_count[feat_choices])\n    i += 1\n    \nend = time.time()\nprint('It took %.2f seconds' %(end - start))","eebfc061":"val_pred = (y_hat).sum(axis=1)\/200\npredictions = (test_hat).sum(axis=1)\/200\nscore = roc_auc_score(y_valid, val_pred)\nprint('>>> Your CV score is: ', score)","3a8f1fb2":"folds = KFold(n_splits=4, random_state=42)\ntarget = df_train['target']\ny_hat = np.zeros([200000, 200])\ntest_hat = np.zeros([100000, 200])\ni = 0\nstart = time.time()\nfor feature in ['var_' + str(x) for x in range(200)]: # loop over all features \n    feat_choices = [feature, feature + '_count']\n    print('Model using: ' + str(feat_choices))\n    oof = np.zeros(len(X_train_count))\n    predictions = np.zeros(len(X_test_count))\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_count[feat_choices].values, target.values)):\n        trn_data = lgb.Dataset(X_train_count.iloc[trn_idx][feat_choices], label=target.iloc[trn_idx])\n        val_data = lgb.Dataset(X_train_count.iloc[val_idx][feat_choices], label=target.iloc[val_idx])\n        clf = lgb.train(lgb_params, trn_data, 130, valid_sets = [val_data], verbose_eval=-1)\n        oof[val_idx] = clf.predict(X_train_count.iloc[val_idx][feat_choices])\n        predictions += clf.predict(X_test_count[feat_choices]) \/ folds.n_splits\n    print(\">>> CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n    \n    y_hat[:, i] = oof\n    test_hat[:, i] = predictions\n    i += 1\n\n    \nend = time.time()\nprint('It took %.2f seconds' %(end - start))","dcd12fb7":"valid_pred = (y_hat).sum(axis=1)\/200\npredictions = (test_hat).sum(axis=1)\/200\nprint('>>> Your CV score is:', roc_auc_score(target, valid_pred))","66fc4c38":"subm = pd.DataFrame({\"ID_codes\":df_test[~df_test.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))].loc[:,'ID_code']})\nsubm['pred'] = predictions\nsubm.head()","aed6cf96":"ID_codes = df_test[~df_test.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))].loc[:,'ID_code']\nsubmission = pd.DataFrame({\"ID_code\": df_test.ID_code.values})\nsubmission['target'] = 0\nsubmission.loc[submission['ID_code'].isin(ID_codes), 'target'] = subm['pred'].values","8f197ef6":"submission.head()","45caab59":"submission.to_csv(r'submission.csv', index = None, header=True)","1bee8d18":"# <a id='2'>Let's see if there are some correlations between our variables<\/a> \n","7a88e9ad":"# <a id='1'>Content<\/a>  ","3af0061f":"#### Missing Values:","21ed44fb":"# <a id='2'>Now let's plot density graphs<\/a>","0249f88c":"All the correlations are < |0.1| ... They are extremely uncorrelated.\n\nMaybe Santander team had preprocessed the data!","dbd5ed4a":"### Duplicate Values","00bcac07":"# Let's build Model\nBut first let's split the train set into training and validation","8c8c1031":"# <a id='2'>Feature Engineering<\/a>  ","54b9dbe3":"We can notice that there is no missing values in both the Train and the Test Dataset","2e916e1a":"# <a id='2'> Load Data and Reducing Memory Usage<\/a>  ","7e08a257":" Well, almost 36 min from my pc. It's quite good!\n\nPlease note that when I tried KFold CV with all the raw features it took several hours!","fdaffcfb":"#### Shape of Training and Test Data","17a55da8":"We can see from the above that nearly 90% of the Target value is 0(we assume that 0 stands for Customer didnot do transaction) and only 10% is 1(we assume 1 stands for Customer did a Transaction).\n\nThis makes the data significantly imbalanced","88a6c8ec":"I tried both with and without Augmentation. These are the results:\n\nNo Augm: CV: 0.8974\nAugm: CV: 0.8963","8f0636d0":"# <a id='2'> Basic Stats<\/a>","f35a5193":"The data obtained is entirely masked so with even domain knowledge we will not be able to find out any significant features. We can try with basic features like mean, standard deviation, counts, median, etc. We will do feature engineering later.","f2e82823":"#### Mean","36b1fd47":"# <a id='2'>Exploratory Data Analysis<\/a>  \n\n","39ff8179":"#### Comparing Distribution of Feature","e21138c0":"# <a id='1'>Abstract<\/a>  \n\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n\nIn the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n\n\nAt Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\n","12ad12d1":"#### Standard Deviation","1ccfaccc":"\nAugmentation is a method to increase the amount of training data by randomly shuffle\/transform the features in a certain way. It improves accuracy by letting the model see more cases of both \"1\" and \"0\" samples in training so the model can generalize better to new data.\n\nI will tend to summarise that this technique works, when:\n\na. Number of features are large\n\nb. No feature is strongly correlated with target (which is the case in my example)\n\nc. Correlations among features is also low\n\nI augmented both 1 and 0 training samples and tune the amount of augmentation. Currently, my best parameter is to add 2x \"1\"s and 1x \"0\"s.","44a73d1e":"We can see from above that all the variables have nearly same distribution with the same scales","7b9b71dc":"#### Min","e102caab":"# <a i> Predictive Analysis & Model Tuning <\/a>","8ee58946":"## Frequency Encoding\n","eb3b2405":"- Introduction\n- Exploratory Data Analysis\n- Load Data and Reducing Memory Usage\n- Basic Statistics Plot\n- Feauture Engineering\n- Regression Model (Light GBM)\n- Classification Model (Logistic Regression, Naive Bayes , Random Forest)\n- Feature Engineering (Sorting Fake Test Data, Frequency Encoding, K Fold CV)\n- Conclusion\n- Submission","ab96acfc":"### Naive Bayes","cf265c92":"Less than 6 min with my pc! That's fast :)","fcf5560f":"## Data Augmentation","aa6c2187":"We can make few observations here:   \n\u200b\n* standard deviation is relatively large for both train and test variable data;  \n* min, max, mean, sdt values for train and test data looks quite close;  ","f9ae49b0":"After applying multiple models (Logistic, Decision Tree, random Forest, Naive Bayes and LGB) based on sampling using Sorting Fake test data, Frequency Encoding and KFold CV.LGB given maximum score from these models.","d7103aef":"### Performing EDA","e1d34ed4":"# Submission","c14bb84d":"As discussed in the EDA notebook frequency encoding may help our tree based model to learn also the values occurrences for each variable.\nhttps:\/\/www.kaggle.com\/cdeotte\/200-magical-models-santander-0-920\n\nI tried both considering only the training set and concatenating train + test.\n\nThe second path takes significant advantages in terms of performance!\n\n\n**MAKING MY JOB EASIER using Frequency Encoding**\n\nAt this point, I had a problem to solve: Each new feature about values frequency only mattered for one other specific feature. My model, however, was checking all possible interactions between my 400 features and taking a long time to run.\n\n\nSo I decided train my model with 2 features at a time: The original one and an extra column with the unique values count.","08e21ed5":"# <a id='2'>Modelling<\/a>  ","21424e63":"We can see that the train Dataset has 202 columns while the test Dataset has 201 Columns. The extra column in the Train Dataset is the target data set which is not present in the Test Dataset","4152b48b":"### Random Forest","7a89fc1c":"\n**Well, not very balanced... we'll keep that into account!**","53f8bb35":"#### Skewness","7909536d":"Preparing the submission file!\n\nWe only need 'ID_codes' and 'pred' columns.\n\nNote that I predicted values only for the real test data and I setted the fake ones at 0.","a93c0205":"<h1><center><font size=\"6\">Santander Customer Transaction Prediction<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Dataset used: Santander Customer Transaction Prediction<\/font><\/center><\/h2>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\" width=\"500\"><\/img>\n\n<br>","8659f9f2":"# Kaggle Score Evaluation\n\nPublic Score: 0.92223\n\nPrivate Score: 0.92057\n\nKaggle Rank Public leaderboard is 62 out of 8820 i.e 0.007","c18e6113":"At this point, what did I know?\n\n1) Regular transformations wouldn't work (many topics shared failed experiments).\n\n2) The test dataset had fake data, and that was relevant.\n\n3) That unique values were, somehow, important.\n\nSo I decided to experiment with both of these ideas: I merged the train and test datataset without fake values, created new columns for each variable with the number of unique values in it, and trained my model.\n\n \nSETTING FEATURE FRACTION TO 1\n\nFeature fraction is a parameter of the LGB model. It goes from 0 to 1 and represents the percentage of the data that you will use on each iteration of the traning. My feature fraction was 0.3. When I set it to 1, the model was able to look at all the variables at once, and voila: CV 0.899\n\nTIP: Setting feature fraction to 1 is a great way to understand the impact of a new feature in your model.\n\n\n\n\n","7b1d6f53":"#### Max","b41d1574":"Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.","554d39dc":"# Conclusion","c3b72026":"This is the best result I achieved  0.89, but I am grateful for this experience.\n\n\n4 KFold Results:\n\nCV: 0.8959","a910ab8d":"Let's try to split  Training set with KFold cross validation. This should help us to increase a bit our performances and to have more reliable results!\n\nI choose 4 Fold, but this could be changed!","183f14c1":"### Target Distribution","0de8f86f":"### Regression Model","62761bcf":"## Sorting Fake Test Data\n\nAfter saw a discussion on Kaggle. It seems that the test dataset was created with half real data (used for LB scores) and synthetic data (maybe to increase the diffuculty of the competition). Note that this was one of the most important kernel of the competition, so it is worth looking it :)\n\nReference https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split","afda9342":"### 1. Logistic Regression","4a974c13":"### Is the Dataset balanced?","41169273":"### Classification Model","30a65156":"# KFold CV","e2f2fffb":"## Light GBM","049f89cc":"###  With Augmentation","c844ea69":"# MY FINAL RUN WITH 4 FOLD","6288ca5a":"### Detect and delete outliers from data"}}