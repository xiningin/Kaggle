{"cell_type":{"7171276e":"code","5179dec2":"code","52935595":"code","c96a56a5":"code","67210d7e":"code","e8d569d8":"code","cff3b3df":"code","c82f26d9":"code","09eede7e":"code","eefee9d7":"code","68fbfab4":"code","d2998db3":"code","e502eeb0":"code","b84925c7":"code","743c5744":"code","0dcf08fc":"code","1b37714f":"code","1e3dcccc":"code","4cc46496":"code","9ca525b8":"code","de4d5cf8":"code","11c99f67":"code","40773583":"code","ba2675bb":"code","70749fd7":"code","f1b09d69":"code","fa53c779":"code","c4d47fe4":"markdown","f71d2179":"markdown","dc1168ac":"markdown","5711c475":"markdown","0b27d018":"markdown","826c7a79":"markdown","db3816ce":"markdown","9c983f7e":"markdown","0f763496":"markdown","6420603a":"markdown","3473d630":"markdown","80a3a3c7":"markdown","27542626":"markdown","6894d9b2":"markdown","e49fd911":"markdown","07ed542a":"markdown","380c749d":"markdown","d419331c":"markdown","67fb092d":"markdown","c66f7d53":"markdown","1d6ad0a6":"markdown","e5f60177":"markdown","63e63a34":"markdown","18b37151":"markdown","49360f19":"markdown","282d7aa3":"markdown"},"source":{"7171276e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report,f1_score,accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')","5179dec2":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","52935595":"df.head()\n","c96a56a5":"\ndf.info()","67210d7e":"print(df.describe())\nprint(df.isna().sum())","e8d569d8":"def Date_process(dataframe):\n  # Process Date\n  dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n  dataframe['Year'] = dataframe['Date'].dt.year\n  dataframe['Month'] = dataframe['Date'].dt.month\n  dataframe['Day'] = dataframe['Date'].dt.day\n  dataframe.drop(['Date'], axis=1, inplace=True)\n  dataframe['RISK_MM'] = dataframe.Rainfall.shift(-1)\n  \n  return dataframe\n\ndef Rain_process(dataframe):\n  # Process rain\n  dataframe['RainToday'].replace({'No': 0, 'Yes': 1}, inplace = True)\n  dataframe['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace = True)\n\n  return dataframe","cff3b3df":"def model_1(dataframe):\n  fake_df = Date_process(dataframe.copy())\n  fake_df = Rain_process(fake_df)\n\n  X = fake_df\n\n  numerical_columns = [col for col in X.columns if X[col].dtypes != 'O']\n  categorical_columns = [col for col in X.columns if X[col].dtypes == 'O']\n\n\n  for col in numerical_columns:\n      col_median = X[col].median()\n      X[col].fillna(col_median, inplace=True)\n  X['WindGustDir'].fillna(X['WindGustDir'].mode()[0], inplace=True)\n  X['WindDir9am'].fillna(X['WindDir9am'].mode()[0], inplace=True)\n  X['WindDir3pm'].fillna(X['WindDir3pm'].mode()[0], inplace=True)\n  \n  X = pd.get_dummies(X, columns= categorical_columns)\n  X = X.dropna()\n  target = X['RainTomorrow'].copy()\n  X = X.drop(['RainTomorrow'], axis=1).copy()\n\n\n  return X, target","c82f26d9":"X,y = model_1(df.copy())\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n","09eede7e":"X_train_full","eefee9d7":"X_valid, X_train = X_train_full[:53322] , X_train_full[53322:] \ny_valid, y_train = y_train_full[:53322], y_train_full[53322:]","68fbfab4":"model = keras.models.Sequential()\nmodel.add(Dense(118, input_dim=118, activation='relu'))\nmodel.add(Dense(59, input_dim=118, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","d2998db3":"keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)","e502eeb0":"early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)","b84925c7":"def last_time_step_bce(Y_true, Y_pred):\n    return tf.keras.metrics.binary_crossentropy(y_true, y_pred)","743c5744":"model.compile(loss=\"binary_crossentropy\",\n              optimizer=\"Adam\",\n              metrics=[\"accuracy\"])","0dcf08fc":"train = model.fit(X_train, y_train, epochs=25,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[early_stopping_cb])","1b37714f":"pd.DataFrame(train.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","1e3dcccc":"y_pred = model.predict(X_test)","4cc46496":"model.evaluate(X_test, y_test)","9ca525b8":"rounded = [round(x[0]) for x in y_pred]\n\ny_pred = rounded","de4d5cf8":"cm_matrix = pd.DataFrame(data=confusion_matrix(y_test, y_pred),\n                          columns=['No Rain', 'Rain'],\n                          index=['No Rain', 'Rain'])\ncm_matrix_coef =cm_matrix\/cm_matrix.sum(axis=1)","11c99f67":"f1_score(y_test, y_pred)","40773583":"cm_matrix","ba2675bb":"cm_matrix_coef.style.background_gradient(cmap=\"Blues\")","70749fd7":"from sklearn.model_selection import StratifiedKFold\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\ncvscores = []\n\nX,Y = model_1(df.copy())\n\nfor train, test in kfold.split(X, Y):\n  \n  training_data = X.iloc[train]\n  training_validation_data = Y.iloc[train]\n\n  test_data = X.iloc[test]\n  test_validation_data = Y.iloc[test]\n\n  model = keras.models.Sequential()\n  model.add(Dense(118, input_dim=118, activation='relu'))\n  model.add(Dense(59, input_dim=118, activation='relu'))\n  model.add(Dense(1, activation='sigmoid'))\n\n  model.compile(loss=\"binary_crossentropy\",\n                optimizer=\"Adam\",\n                metrics=[\"accuracy\"])\n\n  model.fit(training_data, training_validation_data, epochs=25,\n            callbacks=[early_stopping_cb])\n\n  scores = model.evaluate(test_data, test_validation_data, verbose=0)\n  cvscores.append(scores[1] * 100)\n\n","f1b09d69":"i=1\nfor index in cvscores:\n  print(f\"Result of {i} test : %.2f%%\" % (index))\n  i+=1\n\n","fa53c779":"print(\"Cross validation score : %.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n","c4d47fe4":"#neural network","f71d2179":"Even if the accuracy gives us important information, it is not enough on its own to validate the model used. For this we will analyze the F1 Score and the confusion matrix.\n","dc1168ac":"#Import ","5711c475":"We will compile our neural network with a binary crossentropy as loss function :\n\n$ \\mbox{Loss} = -\\frac{1}{N}\\sum_{i=1}^N {( {y}_i\\log(\\hat{y}_i) + (1 - {y}_i)\\log(1 - \\hat{y}_i))} $\n\nand we will use an Adam optimization: \nAdam optimization is an extension to Stochastic gradient decent and can be used in place of classical stochastic gradient descent to update network weights more efficiently. \n","0b27d018":"In this graph, we have the evolution of the metric accuracy and loss as a function of the epoche. The aim is to maximise accuracy and minimise loss. ","826c7a79":"We are going to use here a neural network to make weather prediction with a database found on kaggle :\nhttps:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package\n\nThis dataset contains about 10 years of daily weather observations from many locations across Australia.\n","db3816ce":"##Cross validation with KFold","9c983f7e":"What we want to predict here is the RainTomorrow column. \n\nTogether with the information on the other columns, we want to know if it will rain the day after a given day.  ","0f763496":"We can now make a prediction on the set of unknown data of the neural network. ","6420603a":"#First Model","3473d630":"Cross-Validation is a method for testing the performance of a Machine Learning predictive model. We will re-train our model several times, in this case 10 times, and for each training we will change the data used for training and the test data. \n\n ","80a3a3c7":"#Procecing","27542626":"#Prediction and result","6894d9b2":"We also used a callback function called \"Early Stopping\" which stops the training when a monitored metric has stopped improving.","e49fd911":"Before any processing, we must first obtain information about the data we are going to use. \nWe need to know how the data is distributed, what type of data it is, if we have any missing values. Understand what information these data give us. ","07ed542a":"Once our processing model has been applied to our dataframe, we will separate the data into two sets of data. \nOne that will be used to train the machine learning model (X_train, y_train).\n\nThe other (X_test,y_test) that will be used to get results on a set of data that is not known by the algorithm, which will help us to tune the algorithm. ","380c749d":"Here we need to make the information understandable for a leanring machine model. We need to create 2 first functions  \n\nData_precess: This function processes the \"Date\" column. It will simply create 3 new columns, one for the day, one for the month and one for the year, then it will delete the Date column because it will no longer provide any information. \n\nRain_precess: This function treats the \"RainToday\" and \"RainTomorrow\" columns. These two columns have boolean data. It will simply replace the 'Yes' with 1 and 'No' with 0","d419331c":"First we will use the 2 functions we defined earlier. \n\nWe will then define a target, which will have the information from the \"RainTomorrow\" column. This is the information we want to predict so we copy the column on an array and deleted this column from the\n dataframe.\n\nWe have missing values that we need to process: \n\nFor numerical columns: We replace all missing values by the median value of the column. \n\nFor categorical columns: We apply the get_dummies function which convert categorical variable into dummy\/indicator variables. \n","67fb092d":"We can now train our eurone network with 25 epoche ","c66f7d53":"We have here a first model. This model will do all the necessary operations for the data to be understood by a machine learning algorithm. ","1d6ad0a6":"\nIf we observe the other models used on kaggle, we can notice that the use of simpler machine learning algorithms such as Ramdom Forest or XGBoost also gives us very good results (more than 90% of good prediction). We can therefore question the relevance of using a neural network, which is more computationally and resource intensive. \n\n\nBe careful, here the prediction is very limited. We predict if it will rain the day after a given day in a given region. We have no information about the exact location in the region, the duration and the amount of rain. \nThe information predicted above has very little relevance on its own.\n","e5f60177":"\n#DataFrame importation and information","63e63a34":"We can see with the above diagram the composition of the neural network we will use. \n\nWe have here 3 layers: \n\nThe first layer is composed of 118 artificial neurons. This layer has the activation function Rectified Linear Unit (ReLU) which is defined by this fonction : $f(x)=max(0,x)$ \n\nThe second layer is composed of 59 artificial neurons. we also use a ReLU activation function  : $f(x)=max(0,x)$ \n\nThe last layer is composed of only 1 articial neuron. For this layer we use the sigmoid activation function because we have to make a binary prediction, this is defined by this fonction :\n$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n\n","18b37151":"#Conclusion on model and possible evolution ","49360f19":"## Analysis of results","282d7aa3":"# Weather prediction in Australia\n"}}