{"cell_type":{"e1c5bdd6":"code","a788a70e":"code","a797434e":"code","2d5e5759":"code","e28ef10c":"code","90bc6fea":"code","db63620f":"code","bb398b9f":"code","1e011c22":"code","e7f061cb":"code","2bf45a01":"code","7e0eb981":"code","a24f0eab":"code","b8c64de6":"code","ce40ab62":"code","4fa0bced":"markdown","7fbf63fb":"markdown","b7067bc5":"markdown","4a95a4a9":"markdown","e9d0b9e6":"markdown","52cc142c":"markdown","39c7c643":"markdown","17c21548":"markdown","ead29169":"markdown","f4c7dcc6":"markdown","38d183f8":"markdown"},"source":{"e1c5bdd6":"import numpy as np \nimport pandas as pd\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import cross_val_score\n\nimport os\nprint(os.listdir(\"..\/input\"))","a788a70e":"train_data = pd.read_csv('..\/input\/train.csv',header = None)\ntrain_labels = pd.read_csv('..\/input\/trainLabels.csv',header = None)\ntest_data =  pd.read_csv('..\/input\/test.csv',header = None)","a797434e":"train_data.head()","2d5e5759":"train_data.shape,test_data.shape,train_labels.shape","e28ef10c":"train_data.describe()","90bc6fea":"datasetHasNan = False\nif train_data.count().min() == train_data.shape[0] and test_data.count().min() == test_data.shape[0] :\n    print('There are no missing values.') \nelse:\n    datasetHasNan = True\n    print('Yes, we have missing values')\n\n# now list items    \nif datasetHasNan == True:\n    nas = pd.concat([train_data.isnull().sum(), test_data.isnull().sum()], axis=1, keys=['Train Dataset', 'Test Dataset']) \n    print('--'*40)\n    print('Nan in the data sets')\n    print(nas[nas.sum(axis=1) > 0])","db63620f":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(train_data,train_labels, test_size = 0.30, random_state = 101)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","bb398b9f":"# NAIVE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(x_train,y_train.values.ravel())\npredicted= model.predict(x_test)\nprint('Naive Bayes',accuracy_score(y_test, predicted))\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model.fit(x_train,y_train.values.ravel())\npredicted= knn_model.predict(x_test)\nprint('KNN',accuracy_score(y_test, predicted))\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 22)\nrfc_model.fit(x_train,y_train.values.ravel())\npredicted = rfc_model.predict(x_test)\nprint('Random Forest',accuracy_score(y_test,predicted))\n\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(random_state=0, solver='sag')\nlr_model.fit(x_train,y_train.values.ravel())\nlr_predicted = lr_model.predict(x_test)\nprint('Logistic Regression',accuracy_score(y_test, lr_predicted))\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\nsvc_model.fit(x_train,y_train.values.ravel())\nsvc_predicted = svc_model.predict(x_test)\nprint('SVM',accuracy_score(y_test, svc_predicted))\n\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\ndtree_model.fit(x_train,y_train.values.ravel())\ndtree_predicted = dtree_model.predict(x_test)\nprint('Decision Tree',accuracy_score(y_test, dtree_predicted))\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train.values.ravel())\nxgb_predicted = xgb.predict(x_test)\nprint('XGBoost',accuracy_score(y_test, xgb_predicted))\n","1e011c22":"from sklearn.preprocessing import StandardScaler, Normalizer\n\nstd = StandardScaler()\nstd_train_data = std.fit_transform(train_data)\n\nnorm = Normalizer()\nnorm_train_data = norm.fit_transform(train_data)","e7f061cb":"# NAIVE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\n#nb_model.fit(x_norm_train,y_train.values.ravel())\n#nb_predicted= nb_model.predict(x_norm_test)\n#print('Naive Bayes',accuracy_score(y_test, nb_predicted))\nprint('Naive Bayes',cross_val_score(nb_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 5)\n#knn_model.fit(x_norm_train,y_train.values.ravel())\n#knn_predicted= knn_model.predict(x_norm_test)\n#print('KNN',accuracy_score(y_test, knn_predicted))\nprint('KNN',cross_val_score(knn_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n#rfc_model.fit(x_norm_train,y_train.values.ravel())\n#rfc_predicted = rfc_model.predict(x_norm_test)\n#print('Random Forest',accuracy_score(y_test,rfc_predicted))\nprint('Random Forest',cross_val_score(rfc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\n#svc_model.fit(x_norm_train,y_train.values.ravel())\n#svc_predicted = svc_model.predict(x_norm_test)\n#print('SVM',accuracy_score(y_test, svc_predicted))\nprint('SVM',cross_val_score(svc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#DECISION TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\n#dtree_model.fit(x_norm_train,y_train.values.ravel())\n#dtree_predicted = dtree_model.predict(x_norm_test)\n#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\nprint('Decision Tree',cross_val_score(dtree_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n#xgb.fit(x_norm_train,y_train.values.ravel())\n#xgb_predicted = xgb.predict(x_norm_test)\n#print('XGBoost',accuracy_score(y_test, xgb_predicted))\nprint('XGBoost',cross_val_score(xgb,norm_train_data, train_labels.values.ravel(), cv=10).mean())","2bf45a01":"from sklearn.decomposition import PCA\n\npca = PCA(0.85, whiten=True)\npca_train_data = pca.fit_transform(train_data)\nprint(pca_train_data.shape,'\\n')\n\nexplained_variance = pca.explained_variance_ratio_ \nprint(explained_variance)","7e0eb981":"# NAIVE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\nprint('Naive Bayes',cross_val_score(nb_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 5)\n#knn_model.fit(pca_train_data,y_train.values.ravel())\n#knn_predicted= knn_model.predict(x_norm_test)\n#print('KNN',accuracy_score(y_test, knn_predicted))\nprint('KNN',cross_val_score(knn_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n#rfc_model.fit(pca_train_data,y_train.values.ravel())\n#rfc_predicted = rfc_model.predict(x_norm_test)\n#print('Random Forest',accuracy_score(y_test,rfc_predicted))\nprint('Random Forest',cross_val_score(rfc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\n#svc_model.fit(x_norm_train,y_train.values.ravel())\n#svc_predicted = svc_model.predict(x_norm_test)\n#print('SVM',accuracy_score(y_test, svc_predicted))\nprint('SVM',cross_val_score(svc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#DECISION TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\n#dtree_model.fit(x_norm_train,y_train.values.ravel())\n#dtree_predicted = dtree_model.predict(x_norm_test)\n#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\nprint('Decision Tree',cross_val_score(dtree_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n#xgb.fit(x_norm_train,y_train.values.ravel())\n#xgb_predicted = xgb.predict(x_norm_test)\n#print('XGBoost',accuracy_score(y_test, xgb_predicted))\nprint('XGBoost',cross_val_score(xgb,pca_train_data, train_labels.values.ravel(), cv=10).mean())","a24f0eab":"# Importing libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.svm import SVC\n\nX = np.r_[train_data,test_data]\nprint('X shape :',X.shape)\nprint('\\n')\n\n# USING THE GAUSSIAN MIXTURE MODEL \n\n#The Bayesian information criterion (BIC) can be used to select the number of components in a Gaussian Mixture in an efficient way. \n#In theory, it recovers the true number of components only in the asymptotic regime\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\n\n#The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: \n# spherical, diagonal, tied or full covariance.\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.aic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(train_data)\ngmm_test = best_gmm.predict_proba(test_data)","b8c64de6":"##### ------------------------ Random Forest Classifier -------------------------- #####\nrfc = RandomForestClassifier(random_state=99)\n\n#USING GRID SEARCH\n#The first step you need to perform is to create a dictionary of all the parameters and their corresponding set of values that you want to test for best performance. \nn_estimators = [10, 50, 100, 200,400]\nmax_depth = [3, 10, 20, 40]\nparam_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n#in the above script we want to find which value (out of 10, 50, 100, 200,400) provides the highest accuracy.\n#The Grid Search algorithm basically tries all possible combinations of parameter values and returns the combination with the highest accuracy.\n\n#create an instance of the GridSearchCV class\ngrid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(gmm_train, train_labels.values.ravel())\n#Using 10-folds CV whereas a value of -1 for n_jobs parameter means that use all available computing power.\n\nrfc_best = grid_search_rfc.best_estimator_\nprint('Random Forest Best Score',grid_search_rfc.best_score_)\nprint('Random Forest Best Parmas',grid_search_rfc.best_params_)\nprint('Random Forest Accuracy',cross_val_score(rfc_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\nprint('--'*40,'\\n')\n\n##### -------------------------- KNN -------------------------- ##### \nknn = KNeighborsClassifier()\n\n#USING GRID SEARCH\n#First off, the n_neighbors should always be an odd number. You can choose an even number, but in the case of a tie vote, \n#the decision on which class to assign will be done randomly when weights is set to uniform. By choosing an odd number, there are no ties.\nn_neighbors=[3,5,6,7,8,9,10]\nparam_grid = dict(n_neighbors=n_neighbors)\n# Another important thing to note is that when you do a GridSearch, you\u2019re running many more models than when you simply fit and score. It\u2019s important to set \n# verbose so you\u2019ll get feedback on the model and know how long it may take to finish. \n# kNN can take a long time to complete as it measures the individual distances for each point in the test set.\n\n#create an instance of the GridSearchCV class\ngrid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train,train_labels.values.ravel())\nknn_best = grid_search_knn.best_estimator_\nprint('KNN Best Score', grid_search_knn.best_score_)\nprint('KNN Best Params',grid_search_knn.best_params_)\nprint('KNN Accuracy',cross_val_score(knn_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\nprint('--'*40,'\\n')\n\n##### -------------------------- SVM -------------------------- ##### \nsvc = SVC()\n\n#USING GRID SEARCH\n#setup a parameter grid (using multiples of 10\u2019s is a good place to start) and then pass the algorithm, parameter grid and number of cross validations to the GridSearchCV method.\nparameters = [{'kernel':['linear'],'C':[1,10,100]},\n              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n\n#create an instance of the GridSearchCV class\ngrid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train, train_labels.values.ravel())\nsvm_best = grid_search_svm.best_estimator_\nprint('SVM Best Score',grid_search_svm.best_score_)\nprint('SVM Best Params',grid_search_svm.best_params_)\nprint('SVM Accuracy',cross_val_score(svm_best,gmm_train, train_labels.values.ravel(), cv=10).mean())","ce40ab62":"# Fitting our model\nrfc_best.fit(gmm_train,train_labels.values.ravel())\npred  = rfc_best.predict(gmm_test)\nrfc_best_pred = pd.DataFrame(pred)\n\nrfc_best_pred.index += 1\n\n# FRAMING OUR SOLUTION\nrfc_best_pred.columns = ['Solution']\nrfc_best_pred['Id'] = np.arange(1,rfc_best_pred.shape[0]+1)\nrfc_best_pred = rfc_best_pred[['Id', 'Solution']]\n\nrfc_best_pred.to_csv('Submission.csv',index=False)","4fa0bced":"## **Check for missing data & list them in train and test set**","7fbf63fb":"See effect of feature scaling by classifying each of the models again!","b7067bc5":"The **predict_proba** method will take in new data points and predict the responsibilities for each Gaussian. In other words, the probability that this data point came from each distribution.\n\n\n\n**Now Applying Grid Search Algorithm:** \n\nTo identify the best algorithm and best parameters","4a95a4a9":"## **Applying Gaussian Mixture and Grid Search to improve the accuracy**\n\nWe select the above three algorithms **(KNN, Random Forest and SVM)** which  gave maximum accuracy for further analysis","e9d0b9e6":"## **Introduction:**\nThis tutorial is for beginners learning the concept of `Scikit-Learn library` which is a high level framework designed for supervised and unsupervised machine learning algorithms, built on top of NumPy and SciPy libraries, each responsible for lower-level data science tasks.\n\nThis tutorial seeks inspiration from [https:\/\/www.kaggle.com\/chahat1\/data-science-london-classification](http:\/\/)\n\n**The sequence of steps are as follows:**\n\n* Check for missing values in the dataset\n* Pre-process data by splitting into Train-Test sets\n* Models Classification\n* Feature Scaling by standardizing and normalizing your data\n    > * Learn its effect by improved accuracy\n* Reduce the dimension of your data using PCA\n    > * Learn its effect by improved accuracy\n* Applying Gaussian Mixture and Grid Search\n    > * Learn its effect by improved accuracy\n* Fit our best model \n\n","52cc142c":"Introducing another concept now i.e. **K-Fold Cross-validation**, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n\nCross-Validation can be used to evaluate performance of a model by handling the variance problem of the result set.\n\nIn this approach, the data used for training and testing are non-overlapping. To implement, first separate your data set into two subsets. One subset you use for training and other for testing. Now, do the exercise again by swapping the data sets. Report the average test result. This is call 2-fold cross validation. \n\nSimilarly if you divide your entire data set in to five sub sets and perform the exercise ten times and report the average test result then that would be 10-fold cross validation (which is what we'll be doing now).","39c7c643":"**KNN** gives the maximum accuracy after Feature Scaling.\n\nNote: Before moving forward just know that it is necessary to normalize data before performing PCA. So that all variables have the same standard deviation, thus all variables have the same weight and your PCA calculates relevant axis.","17c21548":"## **Feature Scaling**\n\nTwo approaches are shown below:\n1. The **StandardScaler** assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\n2. The **normalizer** scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.","ead29169":"## **Principal Component Analysis**\n\nPCA helps us to identify patterns in data based on the correlation between features. Used to reduce number of variables in your data by extracting important one from a large pool. Thus, it reduces the dimension of your data with the aim of retaining as much information as possible.\n\nHere we will use a straightforward PCA, asking it to preserve 85% of the variance in the projected data.","f4c7dcc6":"## **MODELS CLASSIFICATION**\n\nSee how sklearn library is used here","38d183f8":"## **PRE-PROCESSING**\n**Train-Test Split**\n\nSplit data into train=70% and val=30%"}}