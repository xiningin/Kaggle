{"cell_type":{"7eedc559":"code","c1046174":"code","481bb757":"code","60fa7ed0":"code","08372a95":"code","86f5f51f":"code","16a92bf5":"code","f7ac490c":"code","429b5d9a":"code","d0e0f418":"code","19d3192f":"code","ba603c12":"code","3828f8d4":"markdown","70bbdc10":"markdown","c50a22d7":"markdown","dde022ef":"markdown"},"source":{"7eedc559":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy import signal\nimport pywt\nimport numba\n\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\nimport gc","c1046174":"@numba.jit(parallel=True)\ndef wavelet_coeffs(x, wavelet='db9', level=9):\n    coeffs = pywt.wavedec(x, wavelet=wavelet, level=level)\n    return coeffs\n\nclass FeatureGenerator(object):\n    def __init__(self, dtype, n_jobs=1, chunk_size=None, fs=4e6, wavelet='db9', level=9):\n        self.chunk_size = chunk_size\n        self.dtype = dtype\n        self.filename = None\n        self.n_jobs = n_jobs\n        self.fs = fs\n        self.wavelet = wavelet\n        self.level = level\n        self.test_files = []\n        if self.dtype == 'train':\n            self.filename = '..\/input\/train.csv'\n            self.total_data = int(629145481 \/ self.chunk_size)\n        else:\n            submission = pd.read_csv('..\/input\/sample_submission.csv')\n            for seg_id in submission.seg_id.values:\n                self.test_files.append((seg_id, '..\/input\/test\/' + seg_id + '.csv'))\n            self.total_data = int(len(submission))\n\n    def read_chunks(self):\n        if self.dtype == 'train':\n            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n            for counter, df in enumerate(iter_df):\n                if df.time_to_failure.values[0] > df.time_to_failure.values[-1]:\n                    x = df.acoustic_data.values\n                    y = df.time_to_failure.values[-1]\n                    seg_id = 'train_' + str(counter)\n                    yield seg_id, x, y\n        else:\n            for seg_id, f in self.test_files:\n                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n                x = df.acoustic_data.values\n                yield seg_id, x, -999\n                \n    @numba.jit(parallel=True)\n    def features(self, x, y, seg_id, fs=4e6):\n        feature_dict = dict()\n        feature_dict['target'] = y\n        feature_dict['seg_id'] = seg_id\n        coeffs = wavelet_coeffs(x, wavelet=self.wavelet, level=self.level)\n        coeffs_diff = wavelet_coeffs(np.diff(x), wavelet=self.wavelet, level=self.level)\n        percentiles_ranges = [99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 80, 75, \n                              50, 25, 20, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n        \n        signals_list = {\n            'regular': coeffs, \n            'diff': coeffs_diff,\n        }\n        for signal_name, signal_i in signals_list.items():\n            for i, x_i in enumerate(signal_i):\n                if i == 0:\n                    name = '{}_cA'.format(signal_name)\n                else:\n                    name = '{}_cD{}'.format(signal_name, self.level - (i - 1))\n                # statistics and centered moments\n                feature_dict['rms_{}'.format(name)] = np.sqrt(np.mean(np.sum(x_i ** 2)))\n                feature_dict['mean_{}'.format(name)] = np.mean(x_i)\n                feature_dict['median_{}'.format(name)] = np.median(x_i)\n                feature_dict['var{}'.format(name)] = np.var(x_i)\n                feature_dict['skewnes_{}'.format(name)] = stats.skew(x_i)\n                feature_dict['kurtosis_{}'.format(name)] = stats.kurtosis(x_i)\n                # non-centered moments\n                for m in range(2, 5):\n                    feature_dict['moment_{}_{}'.format(m, name)] = np.mean(np.sum(x_i ** m))\n                # percentile ranges\n                for pct in percentiles_ranges:\n                    feature_dict['percentile{}_{}'.format(str(pct), name)] = np.percentile(x_i, pct)\n                # sum of energy of coefficients within bands\n                chunks = 20\n                step = len(x_i) \/\/ chunks\n                for chunk_no, band in enumerate(range(0, len(x_i), step)):\n                    feature_dict['energy_chunk{}_{}'.format(chunk_no, name)] = np.sum(x_i[band:band+step] ** 2)\n                    feature_dict['energy_chunk_rms{}_{}'.format(chunk_no, name)] = np.sqrt(\n                        np.mean(\n                            np.sum(\n                                feature_dict['energy_chunk{}_{}'.format(chunk_no, name)] ** 2)\n                        )\n                    )\n                \n        return feature_dict\n\n    def generate(self):\n        feature_list = []\n        res = Parallel(n_jobs=self.n_jobs)(delayed(self.features)(x, y, s, fs=self.fs)\n                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n        for r in res:\n            feature_list.append(r)\n        return pd.DataFrame(feature_list)","481bb757":"wavelet = 'db4'\nlevel = 9\ntraining_fg = FeatureGenerator(dtype='train', n_jobs=1, chunk_size=150000, wavelet=wavelet, level=level)\ntraining_data = training_fg.generate()\n\ntest_fg = FeatureGenerator(dtype='test', n_jobs=1, chunk_size=None, wavelet=wavelet, level=level)\ntest_data = test_fg.generate()\n\ntraining_data.to_csv(\"train_features.csv\", index=False)\ntest_data.to_csv(\"test_features.csv\", index=False)","60fa7ed0":"training_data.sample(5)","08372a95":"training_data.dropna(axis=1, inplace=True)\ntest_data.dropna(axis=1, inplace=True)\nprint(training_data.shape, test_data.shape)","86f5f51f":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","16a92bf5":"features_cols = [c for c in training_data.columns if (('target' not in c) and ('seg_id' not in c))]\n\nX = training_data[features_cols]\ny = training_data['target']\nX_test = test_data[features_cols]","f7ac490c":"xgb = XGBRegressor(random_state=11)\nsearch_space = {\n    'n_estimators': (100, 1000),\n    'learning_rate': (1e-6, 3e-1, 'log-uniform'),\n    'min_child_weight': (4, 10),\n    'reg_alpha': (1e-6, 0.5, 'log-uniform'),\n    'reg_lambda': (1e-6, 1.0, 'log-uniform'),\n    'colsample_bytree': (0.2, 0.8, 'log-uniform'),\n}","429b5d9a":"from skopt import BayesSearchCV","d0e0f418":"folds = TimeSeriesSplit(n_splits=20, max_train_size=3000)\ncv = folds.split(X, y)\n\nopt = BayesSearchCV(\n    xgb,\n    search_space,\n    scoring='neg_mean_absolute_error',\n    n_iter=32,\n    cv=cv,\n    n_jobs=-1,\n)\n\nopt.fit(X, y)\n\nprint('val. score: {:.3f}'.format(opt.best_score_))","19d3192f":"print(opt.best_params_)","ba603c12":"y_pred = opt.predict(X_test)\n\n# submission\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['time_to_failure'] = y_pred\nsub.to_csv('submission.csv', index=False)","3828f8d4":"# Data yielding","70bbdc10":"# Modeling","c50a22d7":"# Submitting","dde022ef":"# Parse data and preprocess"}}