{"cell_type":{"9815ee91":"code","7712907f":"code","8707bd21":"code","ac015359":"code","b357aefc":"code","55cf5bc9":"code","3917e2a8":"code","8db4596c":"code","c1db7922":"code","5da18bb0":"code","6b11158d":"markdown","d3c2e4a6":"markdown","37896ab4":"markdown","cbff0eab":"markdown"},"source":{"9815ee91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7712907f":"class LinearRegression():\n    def __init__(self,epochs=100,learning_rate=.01):\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.log = {'train_loss':[],'epoch':[]}\n    \n    def predict(self,X,w,b):\n        y_hat = np.dot(w.T,X) + b\n        return y_hat\n    \n    def cost(self,y_hat,y):\n        cost = (1\/self.m)*(np.sum(np.power(y_hat-y,2)))\n        return cost\n    \n    def gradient_descent(self,w,b,X,y,y_hat):\n        dCdw = (-2\/self.m)*np.dot(X,(y-y_hat).T)\n        dCdb = (-2\/self.m)*np.sum(y-y_hat)\n        w = w - self.learning_rate * dCdw\n        b = b - self.learning_rate * dCdb\n        return w,b\n    \n    def plot_loss(self):\n        loss = self.log['train_loss']\n        epoch = self.log['epoch']\n        x=epoch\n        y=loss\n        trace = go.Scatter(x=x,y=y,marker=dict(color='#ffdc51'))\n        return trace\n        \n        \n    def fit(self,X,y):\n        self.m = X.shape[1]\n        self.n = X.shape[0]\n        \n        w = np.zeros((self.n, 1))\n        b = np.ones((1,self.m))\n        \n        for epoch in range(self.epochs+1):\n            y_hat = self.predict(X, w, b)\n            cost = self.cost(y_hat, y)\n            self.log['train_loss'].append(cost)\n            self.log['epoch'].append(epoch)\n            if epoch % 100 == 0:\n                print(f\"Loss at epoch: {epoch} is {cost}\")\n                \n            w,b = self.gradient_descent(w,b, X, y, y_hat)\n        \n        return w,b\n        ","8707bd21":"X = np.random.rand(1, 400)\ny = 5 * X + 7 + np.random.randn(1, 400) * 0.1\nregression = LinearRegression(epochs=100)\nw,b = regression.fit(X, y)","ac015359":"data = regression.plot_loss()\nfig1 = go.Figure(data = [data])\niplot(fig1)","b357aefc":"class LassoRegression():\n    def __init__(self,epochs=100,learning_rate=.01,alpha=5):\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.log = {'train_loss':[],'epoch':[]}\n        self.alpha = alpha\n    \n    def predict(self,X,w,b):\n        y_hat = np.dot(w.T,X) + b\n        return y_hat\n    \n    def cost(self,y_hat,y):\n        cost = (1\/self.m)*(np.sum(np.power(y_hat-y,2))  +self.alpha*np.linalg.norm(w))\n        return cost\n    \n    def gradient_descent(self,w,b,X,y,y_hat):\n        dCdw = (-2\/self.m)*(np.dot(X,(y-y_hat).T) + self.alpha*np.sign(w))\n        dCdb = (-2\/self.m)*np.sum(y-y_hat)\n        w = w - self.learning_rate * dCdw\n        b = b - self.learning_rate * dCdb\n        return w,b\n    \n    def plot_loss(self):\n        loss = self.log['train_loss']\n        epoch = self.log['epoch']\n        x=epoch\n        y=loss\n        trace = go.Scatter(x=x,y=y,marker=dict(color='#ffdc51'))\n        return trace\n        \n        \n    def fit(self,X,y):\n        self.m = X.shape[1]\n        self.n = X.shape[0]\n        \n        w = np.zeros((self.n, 1))\n        b = np.ones((1,self.m))\n        \n        for epoch in range(self.epochs+1):\n            y_hat = self.predict(X, w, b)\n            cost = self.cost(y_hat, y)\n            self.log['train_loss'].append(cost)\n            self.log['epoch'].append(epoch)\n            if epoch % 100 == 0:\n                print(f\"Loss at epoch: {epoch} is {cost}\")\n                \n            w,b = self.gradient_descent(w,b, X, y, y_hat)\n        \n        return w,b\n        ","55cf5bc9":"X = np.random.rand(1, 400)\ny = 5 * X + 7 + np.random.randn(1, 400) * 0.1\nregression = LassoRegression(epochs=100)\nw,b = regression.fit(X, y)","3917e2a8":"data = regression.plot_loss()\nfig1 = go.Figure(data = [data])\niplot(fig1)","8db4596c":"class RidgeRegression():\n    def __init__(self,epochs=100,learning_rate=.01,alpha=1):\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.log = {'train_loss':[],'epoch':[]}\n        self.alpha = alpha\n    \n    def predict(self,X,w,b):\n        y_hat = np.dot(w.T,X) + b\n        return y_hat\n    \n    def cost(self,y_hat,y):\n        cost = (1\/self.m)*(np.sum(np.power(y_hat-y,2)) +self.alpha*0.5*np.sum(np.power(w,2)))\n        return cost\n    \n    def gradient_descent(self,w,b,X,y,y_hat):\n        dCdw = (-2\/self.m)*(np.dot(X,(y-y_hat).T) + np.sum(self.alpha*w))\n        dCdb = (-2\/self.m)*np.sum(y-y_hat)\n        w = w - self.learning_rate * dCdw\n        b = b - self.learning_rate * dCdb\n        return w,b\n    \n    def plot_loss(self):\n        loss = self.log['train_loss']\n        epoch = self.log['epoch']\n        x=epoch\n        y=loss\n        trace = go.Scatter(x=x,y=y,marker=dict(color='#ffdc51'))\n        return trace\n        \n        \n    def fit(self,X,y):\n        self.m = X.shape[1]\n        self.n = X.shape[0]\n        \n        w = np.zeros((self.n, 1))\n        b = np.ones((1,self.m))\n        \n        for epoch in range(self.epochs+1):\n            y_hat = self.predict(X, w, b)\n            cost = self.cost(y_hat, y)\n            self.log['train_loss'].append(cost)\n            self.log['epoch'].append(epoch)\n            if epoch % 100 == 0:\n                print(f\"Loss at epoch: {epoch} is {cost}\")\n                \n            w,b = self.gradient_descent(w,b, X, y, y_hat)\n        \n        return w,b\n        ","c1db7922":"X = np.random.rand(1, 400)\ny = 5 * X + 7 + np.random.randn(1, 400) * 0.1\nregression = RidgeRegression(epochs=100)\nw,b = regression.fit(X, y)","5da18bb0":"data = regression.plot_loss()\nfig1 = go.Figure(data = [data])\niplot(fig1)","6b11158d":"## Part 1: Linear Regresssion with gradient descent","d3c2e4a6":"# 100 Days of ML\n## Day 1: Linear Regression, Lasso Regression, Ridge Regression","37896ab4":"## Part 3: Ridge Regresssion with gradient descent","cbff0eab":"## Part 2: Lasso Regresssion with gradient descent"}}