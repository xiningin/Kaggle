{"cell_type":{"f4f089de":"code","c7334913":"code","56e0bb8c":"code","30922961":"code","92ac66e9":"code","18aba582":"code","274bff5a":"code","e3fa4088":"code","93f323f9":"code","e7b5aacd":"code","fa7903a8":"code","a48e7813":"code","b27fd27a":"code","1b97ad07":"code","f7bc5a33":"markdown","f7fb2e17":"markdown","b6b95ad1":"markdown","04f30ebc":"markdown","67182440":"markdown","cfb558bb":"markdown","fa09af4c":"markdown","d352681e":"markdown","2f05ceec":"markdown","c3626362":"markdown","b47fc3ab":"markdown","c5c73fb0":"markdown"},"source":{"f4f089de":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","c7334913":"train_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntrain_data","56e0bb8c":"test_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest_data","30922961":"missing_cols = [\"keyword\", \"location\"]\ntrain_missing_vals = [train_data[col].isna().sum() \/ train_data.shape[0] * 100 for col in missing_cols]\ntest_missing_vals = [test_data[col].isna().sum() \/ test_data.shape[0] * 100 for col in missing_cols]\n\nplt.figure(figsize = (12, 4))\n\nplt.subplot(121)\nplt.bar(missing_cols, train_missing_vals)\nplt.title(\"% missing values in training set\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Missing values %\")\n\nplt.subplot(122)\nplt.bar(missing_cols, test_missing_vals)\nplt.title(\"% missing values in test set\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Missing values %\")","92ac66e9":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom transformers import TFAutoModel, AutoTokenizer","18aba582":"def tokenize_dataset(tokenizer):\n    \"\"\"\n    Returns a dict of the train and test datasets based on the given tokenizer.\n    \"\"\"\n    return {\n        \"train\": {\n            \"data\": tokenizer(list(train_data[\"text\"].values), padding = \"max_length\", max_length = 84, truncation = True, return_tensors = \"tf\").data,\n            \"labels\": train_data[\"target\"].values,\n        },\n        \"test\": {\n            \"data\": tokenizer(list(test_data[\"text\"].values), padding = \"max_length\", max_length = 84, truncation = True, return_tensors = \"tf\").data\n        }\n    }","274bff5a":"class disasterClassificationModel(tf.keras.Model):\n    \"\"\"\n    Adds a classification head to the transformers base model.\n    \"\"\"\n    def __init__(self, checkpoint):\n        super(disasterClassificationModel, self).__init__()\n        self.dropout_rate = 0.7\n        \n        self.base_model = TFAutoModel.from_pretrained(checkpoint)\n        self.flatten = layers.Flatten()\n        \n        self.dropout1 = layers.Dropout(rate = self.dropout_rate)\n        self.dense1 = layers.Dense(units = 768, kernel_regularizer = \"l1_l2\")\n        self.batchNorm1 = layers.BatchNormalization()\n        self.activation1 = layers.Activation(\"relu\")\n        \n        self.dropout2 = layers.Dropout(rate = self.dropout_rate)\n        self.dense2 = layers.Dense(units = 32, kernel_regularizer = \"l1_l2\")\n        self.batchNorm2 = layers.BatchNormalization()\n        self.activation2 = layers.Activation(\"relu\")\n        \n        self.dropout3 = layers.Dropout(rate = self.dropout_rate)\n        self.dense3 = layers.Dense(units = 1, activation = \"sigmoid\")\n\n    def call(self, inputs, training = False):\n        x = self.base_model(inputs).last_hidden_state\n        x = self.flatten(x)\n        \n        x = self.dropout1(x) if training else x\n        x = self.dense1(x)\n        x = self.batchNorm1(x)\n        x = self.activation1(x)\n        \n        x = self.dropout2(x) if training else x\n        x = self.dense2(x)\n        x = self.batchNorm2(x)\n        x = self.activation2(x) \n        \n        x = self.dropout3(x) if training else x\n        x = self.dense3(x)\n        return x","e3fa4088":"class F1_score(tf.keras.metrics.Metric):\n    \"\"\"\n    F1 score metric based on Keras Precision and Recall metrics.\n    \"\"\"\n    def __init__(self, name = \"f1_score\", **kwargs):\n        super(F1_score, self).__init__(name = name, **kwargs)\n        \n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n        \n    def update_state(self, y_true, y_pred, sample_weight = None):\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n        \n    def reset_states(self):\n        self.precision.reset_states()\n        self.recall.reset_states()\n        \n    def result(self):\n        return 2 \/ ((1 \/ self.precision.result()) + (1 \/ self.recall.result()))","93f323f9":"def compile_model(model, batch_size, epochs, tokenized_dataset):\n    \"\"\"\n    Compiles the given model by adding a learning rate scheduler to the Adam optimizer.\n    \"\"\"\n    train_steps = (len(tokenized_dataset[\"train\"][\"data\"][\"input_ids\"]) \/\/ batch_size) * epochs\n\n    # Learning rate scheduler to linearly reduce the learning rate from an initial value to an end value\n    lr_scheduler = PolynomialDecay(\n        initial_learning_rate = 5e-5,\n        end_learning_rate = 0,\n        decay_steps = train_steps,\n    )\n\n    optimizer = Adam(learning_rate = lr_scheduler)\n    loss = BinaryCrossentropy()\n    \n    model.compile(loss = loss, optimizer = optimizer, metrics = [\"accuracy\", F1_score()])","e7b5aacd":"checkpoint = \"cardiffnlp\/twitter-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = disasterClassificationModel(checkpoint)\ntokenized_dataset = tokenize_dataset(tokenizer)\ncompile_model(model, batch_size = 10, epochs = 10, tokenized_dataset = tokenized_dataset)","fa7903a8":"history = model.fit(\n    x = tokenized_dataset[\"train\"][\"data\"],\n    y = tokenized_dataset[\"train\"][\"labels\"],\n    batch_size = 10,\n    epochs = 10,\n    validation_split = 0.1,\n)","a48e7813":"plt.figure(figsize = (14, 4))\n\nplt.subplot(131)\nplt.plot(history.history[\"loss\"], label = \"loss\")\nplt.plot(history.history[\"val_loss\"], label = \"val_loss\")\nplt.title(\"Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")\n\nplt.subplot(132)\nplt.plot(history.history[\"accuracy\"], label = \"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label = \"val_accuracy\")\nplt.title(\"Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")\n\nplt.subplot(133)\nplt.plot(history.history[\"f1_score\"], label = \"f1_score\")\nplt.plot(history.history[\"val_f1_score\"], label = \"val_f1_score\")\nplt.title(\"F1 Score\")\nplt.ylabel(\"F1 Score\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")","b27fd27a":"predictions = model.predict(tokenized_dataset[\"test\"][\"data\"], verbose = True)\npredictions = np.where(predictions >= 0.5, 1, 0)","1b97ad07":"submissions = test_data.drop(labels = [\"keyword\", \"location\", \"text\"], axis = 1)\nsubmissions[\"target\"] = predictions\nsubmissions.to_csv(\"submissions.csv\", index = False)","f7bc5a33":"*Plotting % missing values*","f7fb2e17":"*This particular model uses a Roberta base and has been finetuned for sentiment-analysis using 58M tweets.*","b6b95ad1":"# **Data Analysis**","04f30ebc":"*Test Data*","67182440":"*Train Data*","cfb558bb":"*Helper Functions and Classes*","fa09af4c":"**ROBERTa base model**","d352681e":"*Plotting model history*","2f05ceec":"# **Transformer Model**","c3626362":"### Please upvote if you find this notebook useful.","b47fc3ab":"# **Submission**","c5c73fb0":"*Custom F1 Score Metric*"}}