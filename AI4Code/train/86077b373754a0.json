{"cell_type":{"f9270e16":"code","ff607d2a":"code","6247a109":"code","a9cef651":"code","b694ab1f":"code","3cba57ea":"code","ec13f868":"code","39137462":"code","d69e9159":"code","c6ed65c1":"code","3d95c640":"code","dc45318c":"code","12df2bd2":"code","db879bec":"code","8f91f5e9":"code","9e9014f3":"code","8e09f830":"code","1b8dd4c5":"code","d86c372a":"code","16641b74":"code","07382355":"code","17a968e4":"code","0d3f76a5":"code","97841055":"code","57154686":"code","4f9514e7":"code","896dc16c":"code","36d6b6ef":"code","18db8cc0":"code","0dd39d15":"code","760e1b3d":"code","af0cb9f8":"code","0d9eee39":"code","ba9f54e4":"markdown","5e8a89e8":"markdown","e40d4317":"markdown","c808874d":"markdown","4077a9bc":"markdown","0f2b5b7f":"markdown","2da044b0":"markdown","cf5f2325":"markdown","a13f6da1":"markdown","ac4c9509":"markdown","2bf05523":"markdown","f3f963cb":"markdown","6f563b24":"markdown"},"source":{"f9270e16":"import matplotlib.pyplot as plt #this is for visualization\nimport numpy as np #this is for matrix calculation and maths\nimport pandas as pd #this is for data manipulation\nimport seaborn as sns #this is also to help in visualization \n# import featuretools as ft #used for auto feature generation where we have many datasets\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split #to help in spliting the data\nfrom sklearn.metrics import mean_squared_error #this is to test the accuracy of the model\nfrom sklearn import cross_validation #to help in feature selection\n\n# I need to install xgboost as my ML method and other parameters I may have forgotten\n# I am going to need feature tools since I have many dataframes so that I can create more features\n\n# Suppress warnings \nimport warnings \nwarnings.filterwarnings('ignore')\n","ff607d2a":"df_test = pd.read_csv('..\/input\/application_test.csv')\ndf_train = pd.read_csv('..\/input\/application_train.csv')\ndf_bureau = pd.read_csv('..\/input\/bureau_balance.csv')\ncredit_card_bal = pd.read_csv('..\/input\/credit_card_balance.csv')\ninstalments = pd.read_csv('..\/input\/installments_payments.csv')\nPOS_cash = pd.read_csv('..\/input\/POS_CASH_balance.csv')","6247a109":"#how big are our data sets?\ndf_bureau.shape,df_test.shape,df_train.shape,credit_card_bal.shape,instalments.shape,POS_cash.shape","a9cef651":"#I want to look at the variables in the train dataset because it is the most important dataset\ndf_train.describe()","b694ab1f":"#how many empty values do we have\ndf_train.isnull().any().sum()","3cba57ea":"df_train['TARGET'].value_counts()","ec13f868":"#let us look at the distribution of the target\ndf_train['TARGET'].plot.hist()\n#so we see here that there are more defaults than those that have paid their loans back","39137462":"#check for missing values:\ndef miss_val(df):\n    #the number of missing values\n    val_miss = df.isnull().sum()\n    \n    #percentage of the missing values\n    perc_miss = 100* df.isnull().sum()\/len(df)\n    \n    #put the two together to form a table\n    miss_table = pd.concat([val_miss,perc_miss], axis= 1)\n    \n    #rename the columns\n    fin_mis_table = miss_table.rename(columns = {0:'Missing Values', 1: 'Percentage'})\n    \n    #sort the values in descending order\n    final_table = fin_mis_table[fin_mis_table.iloc[:,1]!=0].sort_values('Percentage', ascending = False ).round(2)\n    \n    return final_table","d69e9159":"miss_val(df_train).head(20)","c6ed65c1":"#how many data types do we have\ndf_train.dtypes.value_counts()","3d95c640":"#we have 16 data types that are non numeric\n#lets see them\ndf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","dc45318c":"#since we have many values that are categorical we need to encode them because they are not easily handled\n\n#create the encoder object\n# labE = LabelEncoder()\n# le_count = 0 #to keep track of the encoded\n\n# for col in df_train:\n#     if df_train[col].dtype == 'object':\n#         #we want to encode the labels with fewer labels\n#         if len(list(df_train[col].unique())) <= 2:\n#             #train on the data\n#             labE.fit(df_train[col])\n#             #transform all the dataframes.\n#             df_test[col] = labE.transform(df_test[col])\n#             df_train[col]= labE.transform(df_train[col])\n            \n#             le_count += 1\n            \n# print('Were transformed', le_count)\n","12df2bd2":"#one hot encoding\ndf_test = pd.get_dummies(df_test)\ndf_train = pd.get_dummies(df_train)\n\nprint('Shape', df_test.shape)\nprint('Shape', df_train.shape)","db879bec":"#one hot encoding creates many more columns and so the dataframes are not aligned \n#let's align the datasets\n\n#first we take out the target column\ntarget_label = df_train['TARGET']\n\ndf_train,df_test = df_train.align(df_test,axis=1,join='inner')\n\n#replace the target column\ndf_train['TARGET'] = target_label\n\nprint('Test Shape',df_test.shape)\nprint('Train Train', df_train.shape)","8f91f5e9":"#there is an anomaly in the number of days worked, the days are so many so we will replace them\n#CREATE A FLAG COLUMN FOR THE ANOMALY DAYS\ndf_train['DAYS_EMPLOYED_ANOMALY'] = df_train['DAYS_EMPLOYED']== 365243\n\n#replace the days which are abnormal\ndf_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\ndf_train['DAYS_EMPLOYED'].plot.hist()","9e9014f3":"#from numpy we have a correlation function, using pearson's correlation\ncorrelation = df_train.corr()['TARGET'].sort_values()\n\nprint('Least correlated', correlation.tail(10))\nprint('Most correlated', correlation.head(10))","8e09f830":"#there is a high correlation between target and the days birth\ndf_train['DAYS_BIRTH'] = abs(df_train['DAYS_BIRTH'])\ndf_train['DAYS_BIRTH'].corr(df_train['TARGET'])","1b8dd4c5":"# let's plot the fig and see how it looks like\nplt.style.use('fivethirtyeight')\n\nplt.hist(df_train['DAYS_BIRTH']\/365, bins=10, color= 'blue', edgecolor = 'k')","d86c372a":"#first we create a list of the variables\npoly_train = df_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_test = df_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n#to handle missing values we use the imputer to fill them in based on the mean\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\n#we take the target column out because we don't want to add anything\npoly_target = poly_train['TARGET']\n\n#lets drop it from the poly_train df\npoly_train = poly_train.drop(columns = ['TARGET'])\n\n#now lets impute the values into the dataframes\npoly_train = imputer.fit_transform(poly_train)\npoly_test = imputer.transform(poly_test)\n\n#lets now bring in the polynomial feature creator\n#we will create the features to the 4 degree to prevent over fitting\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#we create the polynomial feature object\npoly_creator = PolynomialFeatures(degree=4)\n","16641b74":"#we fit the poly features\/ train the features on the training data\npoly_creator.fit(poly_train)\n\n\n#now we need to  in put the data frames created\npoly_train_ft = poly_creator.transform(poly_train)\npoly_test_ft = poly_creator.transform(poly_test)\n\nprint('Poly Features Shape:', poly_train_ft.shape)\nprint('Poly Features Shape:', poly_test_ft.shape)","07382355":"poly_creator.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:20]","17a968e4":"poly_df = pd.DataFrame(poly_train_ft)","0d3f76a5":"#return the target column into the data created\npoly_df['TARGET']= poly_target\n\n#lets now look at the features correlation\npoly_corr = poly_df.corr()['TARGET'].sort_values()\n\n#display the top 10 and bottom 10\nprint(poly_corr.head(10))\nprint('-'*20)\nprint(poly_corr.tail(10))","97841055":"#we want a data frame from the info above\npoly_df_ft = pd.DataFrame(poly_test_ft, columns = poly_creator.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))","57154686":"'SK_ID_CURR' in df_train.columns","4f9514e7":"type(poly_train)","896dc16c":"\n#lets merge the training data frame with the features\npoly_df_ft['SK_ID_CURR'] = df_train['SK_ID_CURR']\ntrain_poly = df_train.merge(poly_df_ft, on = 'SK_ID_CURR', how = 'left')\n\n#lets join the testing data witht he poly features\npoly_df_ft['SK_ID_CURR']= df_test['SK_ID_CURR']\ntest_poly = df_test.merge(poly_df_ft, on = 'SK_ID_CURR', how = 'left')\n\ntrain_poly,test_poly = train_poly.align(test_poly, join = 'inner', axis = 1)\n\nprint('Shape of the training data:',train_poly.shape)\nprint('Shape of the testing data:',test_poly.shape)","36d6b6ef":"#lets get the polynomial feature names\npoly_feature_names = list(train_poly.columns)\n\n#impute the polynomial features\ntrain_poly = imputer.fit_transform(train_poly)\ntest_poly = imputer.transform(test_poly)\n\n#scale the features so they can be in the range of 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range= (0,1))\n\ntrain_poly = scaler.fit_transform(train_poly)\ntest_poly = scaler.transform(test_poly)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\npoly_classiffier = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n","18db8cc0":"poly_classiffier.fit(train_poly,target_label)","0dd39d15":"predictions = poly_classiffier.predict_proba(test_poly)[:,1]","760e1b3d":"submit = df_test[['SK_ID_CURR']]\nsubmit['TARGET']= predictions","af0cb9f8":"submit.shape","0d9eee39":"# submit.to_csv('Random_Forest_HandIn.csv',index= False)","ba9f54e4":"## The next issue we need to look at is correlations\n\nVery high correlation means we maybe inputing data of the same type into the model hence overfitting. So this kind of data should be removed from the equation such that, the model will have some real data to work with","5e8a89e8":"### Importing the necesary Libraries","e40d4317":"By Atwine Mugume Twinamatsiko","c808874d":"### Explore the Target Column","4077a9bc":"I have studied some good examples and I hope to make my predictions better. In this notebook I am going to apply some of the techniques I have learnt over the past weeks of reading.\n\nI will clearly note and demontrate what i am doing for easy following:","0f2b5b7f":"# Home Credit Default Risk","2da044b0":"### Polinomial Features:\n\nThese are features with powers, for example if we have x, the x^2\n- we use the variables with the highest negative correlation to create the features, the reason we do this is because, the lesser the correlation, the better the variable is to help separate the differences and there by helping the model.","cf5f2325":"### Injest the data into data frames\n","a13f6da1":"# Initial Data Exploration\n### Look into the training data set to get some exploration going on;\n#### What are we looking for?\n> Data anomalies\n\n> Missing data\n\n> Multicollinearity\n\n> Standardizing the data after which we will do feature engineering","ac4c9509":"The number of features created have been increased and it could cause over fitting, in order to see the features created we use the code below.","2bf05523":"# Feature Engineering:\nFor feature engineering,  I want to use feature tools to create new features \n- There are some ways to create features one of which is polynomial features\n- But first we will use random forest to look at the most important features in the dataframe","f3f963cb":"Right now we need to transform these df into the right data frames so we can test them","6f563b24":"## Random Forest Classifier Using the polynomial Features:\n\n"}}