{"cell_type":{"50a9af30":"code","fceaf483":"code","92a2981f":"code","145bd9e7":"code","01c95052":"code","e3ecbd90":"code","f991f2e0":"code","a86480ea":"code","6b562597":"code","111ddb7c":"code","9a6dc074":"code","7aef3473":"code","e0f9707a":"code","5e446d12":"code","3e392db8":"code","fe5e34da":"code","07fd9222":"markdown","527d1361":"markdown","aca4e3a2":"markdown","52ab5100":"markdown"},"source":{"50a9af30":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fceaf483":"# Read train and test set\ntrain = pd.read_csv(\"\/kaggle\/input\/bri-data-hackathon-pa\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/bri-data-hackathon-pa\/test.csv\")","92a2981f":"# Split train set into dependent variables and independent variable\ny = train['Best Performance']\nX = train.drop('Best Performance', axis=1)","145bd9e7":"# Convert to dummy variables\nX = pd.get_dummies(X)\ntest = pd.get_dummies(test)","01c95052":"# Extract the common features between train and test set and use it to filter the train and test set\ncommon = list(set(X.columns).intersection(set(test.columns)))\nX = X[common]\ntest = test[common]","e3ecbd90":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\n# fit model to training data\nmodel = xgb.XGBRFClassifier(n_estimators=1000, random_state=1245)\n# cross validation score\nscore = cross_val_score(model, X, y, cv=5, scoring=\"roc_auc\", n_jobs=-1)\nprint(\"XGB ROC-AUC Mean Score: \", np.mean(score))","f991f2e0":"from bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings('ignore')","a86480ea":"# Convert to special data format\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html\ndtrain = xgb.DMatrix(X, y, feature_names=X.columns.values)\n\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'binary:logistic',\n    'eval_metric':'auc',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=5)\n    return scores['test-auc-mean'].iloc[-1]","6b562597":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 10),\n  'subsample':(0.5, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (2, 15),\n  'learning_rate': (0.01, 0.5)\n}","111ddb7c":"optimizer = BayesianOptimization(hyp_xgb, pds, random_state=1)\noptimizer.maximize(init_points=4, n_iter=25)","9a6dc074":"optimizer.max['params']","7aef3473":"# Copied from above\n# Some params need to be an integer\nparams = {\n    'colsample_bytree': 0.8485245380480577,\n    'gamma': 0.07362378227392719,\n    'learning_rate': 0.02919903835311307,\n    'max_depth': 13,\n    'min_child_weight': 5,\n    'subsample': 0.8782155860634835,\n    'objective': 'binary:logistic',\n    'eval_metric':'auc',\n    'n_jobs':-1\n}","e0f9707a":"xgbr =  xgb.XGBClassifier(**params, random_state=12345, nthread=-1)\nxgbr.fit(X, y)","5e446d12":"# Predict the probability using predict_proba\ny_pred = xgbr.predict_proba(test)[:,1]","3e392db8":"submission = pd.read_csv(\"..\/input\/bri-data-hackathon-pa\/sample_submission.csv\")\nsubmission.head()","fe5e34da":"submission['Best Performance'] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","07fd9222":"### Instantiate with new hyperparameters","527d1361":"### Tuning with Bayesian Optimization\n\nNow we will use Bayesian Optimization to tune the hyperparameter. Our goal is to maximize AUC.\n\nYou can also adjust what parameter you want to tune and the range of hyperparameter.","aca4e3a2":"Hello everyone!\n\n**This notebook presents a straightforward code to tune hyperparameter of LGBM, CAT, and XGB with Bayesian Optimization. It is like GridSearchCV and RandomizedSearchCV.**\n\nGridSearchCV searches for all combinations of parameters, and it could take a very long time. Not very efficient. RandomizedSearchCV searches the combination randomly. Somehow the algorithm can skip the optimal parameter, especially if the search grid is enormous. Bayesian Optimization is a smarter method to tune the hyperparameter. I won't discuss the theory behind it in this notebook as it is straightforward.\n\nIf you have any questions regarding the code, please comment below. I will update the notebook accordingly.\n\n**Please do upvote the notebook if this notebook helps you as it will be a benchmark for me to do more work in the future. Thank you :)**\n\n**Note: I do not do the feature engineering here, so the result may sub-optimal**","52ab5100":"### XGBoost - Cross Validation Score"}}