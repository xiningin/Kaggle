{"cell_type":{"d0609787":"code","f967226a":"code","1774679a":"code","40aa29b3":"code","d757cda5":"code","f63b8adc":"code","62cae429":"code","7758e51a":"code","fcce3162":"code","a0d2521f":"code","1edfefbf":"code","45a2750e":"code","62f5e993":"code","11066115":"code","9abc0013":"markdown","81bfbf0a":"markdown","806b008d":"markdown","78030b80":"markdown","bfa41ade":"markdown","6fd543e4":"markdown"},"source":{"d0609787":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f967226a":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\nx = np.array(df.loc[:,\"fixed acidity\"]).reshape(-1,1)\ny = np.array(df.loc[:,\"pH\"]).reshape(-1,1)\nplt.figure(figsize=[25,10])\nplt.scatter(x,y)\nplt.xlabel(\"Fixed Acidity\",fontsize=28)\nplt.ylabel(\"pH\",fontsize=28)\nplt.show()\n","1774679a":"#%% prediction\nfrom sklearn.linear_model import LinearRegression\n\nLreg = LinearRegression()\n\nLreg.fit(x,y)\ny_head=Lreg.predict(x).reshape(-1,1)\n\nb0=Lreg.predict([[0]])\nb1=Lreg.coef_\nprint(\"b0 : \" , b0)\nprint(\"b1 : \", b1)\n# predict ph when fixed acidity equal 16\nprint(\"Predict  : \" , (b0+(b1*16)))","40aa29b3":"#%% R^2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))\n\n","d757cda5":"#%% # Plot regression line and scatter\nplt.figure(figsize=[25,10])\nplt.scatter(x,y)\nplt.xlabel(\"Fixed Acidity\",fontsize=28)\nplt.ylabel(\"pH\",fontsize=28)\nplt.plot(x,y_head,color=\"red\",linewidth=4)\nplt.show()\n","f63b8adc":"\nfrom sklearn.linear_model import LinearRegression\n\nx = df.iloc[:,[0,1,2,3,4,5,6,7,9]].values\ny = df.iloc[:, 0].values.reshape(-1,1)\nmultiple_Lreg=LinearRegression()\n\nmultiple_Lreg.fit(x,y)\nb0 = multiple_Lreg.intercept_\nbs = multiple_Lreg.coef_\ny_head=multiple_Lreg.predict(x)\nprint(\"b0 : \" ,b0)\ndict_of_b={}\nz=1\nfor i in bs:\n    for j in i:\n        if z==8:\n            dict_of_b[\"b\"+str(z+1)]=j\n            continue\n        dict_of_b[\"b\"+str(z)]=j\n        \n        z+=1\n\nprint(\"Weight of all x : \",dict_of_b)\n\n","62cae429":"#%% r2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))\n\n","7758e51a":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 3)\nx_polynomial = polynomial_regression.fit_transform(x)","fcce3162":"from sklearn.linear_model import LinearRegression\nplt.figure(figsize=[25,10])\n\npoly_reg=LinearRegression()\npoly_reg.fit(x_polynomial,y)\ny_head=poly_reg.predict(x_polynomial)\nplt.plot(x,y_head,color=\"purple\")\nplt.show()\n","a0d2521f":"#%% R^2 score\nfrom sklearn.metrics import r2_score\nprint(\"R^2 : \",r2_score(y,y_head))","1edfefbf":"from sklearn.tree import DecisionTreeRegressor\n\nx = df.loc[:,\"fixed acidity\"].values.reshape(-1,1)\ny = df.loc[:,\"pH\"].values.reshape(-1,1)\n\ndTree = DecisionTreeRegressor()\ndTree.fit(x,y)\nx_ =  np.arange(min(x),max(x),0.001).reshape(-1,1)\ny_head = dTree.predict(x_)\n","45a2750e":"plt.figure(figsize=[25,10])\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,y_head,color=\"red\" ,linewidth=6)\nplt.xlabel(\"Fixed Acidity\" ,fontsize=25)\nplt.ylabel(\"pH\" ,fontsize=25)\nplt.show()","62f5e993":"\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf=RandomForestRegressor(n_estimators=200,random_state=(42))\n\nrf.fit(x,y)\nx_=np.arange(x.min(),x.max(),0.001).reshape(-1,1)\n\ny_head=rf.predict(x_).reshape(-1,1)\n","11066115":"plt.figure(figsize=[25,10])\nplt.scatter(x,y,color=\"brown\")\nplt.plot(x_,y_head,color=\"green\",linewidth=3)\nplt.xlabel(\"Fixed Acidity\")\nplt.ylabel(\"pH\")\nplt.show()","9abc0013":"# 2)Multiple Linear Regression\n\nmathematical formula :\n\ny=b0+b1x1+b2x2+...+bn*xn\n\nIn multiple linear regression we have more features. I used all the features in the csv file \n    ","81bfbf0a":"# 5)Random Forest Regression\n\n","806b008d":"# 3)Polynomial Regression\n\nmathematical formula :\n\ny=b0+b1x+b2x^2+...+bn*x^n\n","78030b80":"# 1)Linear Regression\nmathematical formula :\n\ny = b0 + b1*x\n\ny = Dependent variable \n\nx = Independent variable\n\nb0 = Costant variable(bias)\n\nb1 = Weight of x(slope)\n\nA feature (x) is required for linear regression so I used fixed acidity column for this regression. And target is pH column.","bfa41ade":"# 4)Decision Tree Regression\n","6fd543e4":"\n# Introduction\n## Content \n### 1.Linear Regression\n### 2.Multiple Linear Regression\n### 3.Polynomial Linear Regression\n### 4.Decision Tree Regression\n### 5.Random Forest Regression\n\n"}}