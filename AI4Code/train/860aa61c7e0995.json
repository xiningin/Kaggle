{"cell_type":{"1e7120d8":"code","c0028bfb":"code","00ec0b61":"code","d0daa1c1":"code","66f6198d":"code","efac8303":"code","e32448f8":"code","96ece9d0":"code","3d67e42d":"code","1fd48788":"code","c40357a3":"code","099d06e3":"code","047fb597":"code","ff44e609":"code","aa049b51":"code","be8d9055":"code","ff3a62f5":"code","4de6f839":"code","8c2b7958":"code","419cc796":"code","4d0150aa":"code","de973f4f":"code","3538af77":"code","2c294269":"code","56868ac6":"code","37da4731":"code","243c45e1":"code","3d568b7c":"code","20991a76":"code","73f89cd6":"code","ec3f4461":"code","5699e574":"code","644405b9":"code","e70eb366":"code","41ac5623":"code","c4b4901d":"code","316437fb":"code","e6fdedb8":"code","53ca960e":"code","e94d23e4":"code","94d0350d":"code","509542cb":"code","64bc22b8":"code","850e8865":"code","5741680f":"code","a8c4a4d1":"code","7b2e12b7":"code","1169639f":"code","b2bc0f76":"code","031154bd":"markdown","243068e3":"markdown","2ed4d8f7":"markdown","3abd79b8":"markdown","ccad463a":"markdown","22b29bf0":"markdown","39a988df":"markdown","0555d241":"markdown","f16c95e6":"markdown","49087120":"markdown","dc439aba":"markdown","4357ad10":"markdown","d7e92d93":"markdown","0ced7a15":"markdown","0776e315":"markdown","50994539":"markdown","b66febb8":"markdown","f0b14c32":"markdown","d5b46f30":"markdown","da27f9ca":"markdown","4c862f1e":"markdown","379466da":"markdown","cd3a054a":"markdown","a898cae7":"markdown","1c6af174":"markdown","dfcd7e43":"markdown"},"source":{"1e7120d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0028bfb":"!pip install chart_studio","00ec0b61":"import numpy as np\nimport chart_studio.plotly as py\nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport regex as re","d0daa1c1":"sar_acc = pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)\nsar_acc['source'] = sar_acc['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\nsar_acc.head()","66f6198d":"sar_acc_tar = sar_acc['is_sarcastic'].value_counts()\nlabels = ['Acclaim', 'Sarcastic']\nsizes = (np.array((sar_acc_tar \/ sar_acc_tar.sum())*100))\ncolors = ['#58D68D', '#9B59B6']\n\ntrace = go.Pie(labels=labels, values=sizes, opacity = 0.8, hoverinfo='label+percent',\n               marker=dict(colors=colors, line=dict(color='#FFFFFF', width=2)))\nlayout = go.Layout(\n    title='Sarcastic Vs Acclaim'\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"Sa_Ac\")","efac8303":"all_words = sar_acc['headline'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Viridis',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Frequent Occuring word (unclean) in Headlines'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='basic-bar')","e32448f8":"sar_det = sar_acc[sar_acc.is_sarcastic==1]\nsar_det.reset_index(drop=True, inplace=True)\nacc_det = sar_acc[sar_acc.is_sarcastic==0]\nacc_det.reset_index(drop=True, inplace=True)\n\n# Tokenizing the Headlines of Sarcasm\nsar_news = []\nfor rows in range(0, sar_det.shape[0]):\n    head_txt = sar_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    sar_news.append(head_txt)\n\n#Converting into single list for Sarcasm\nimport itertools\nsar_list = list(itertools.chain(*sar_news))\n\n# Tokenizing the Headlines of Acclaim\nacc_news = []\nfor rows in range(0, acc_det.shape[0]):\n    head_txt = acc_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    acc_news.append(head_txt)\n    \n#Converting into single list for Acclaim\nacc_list = list(itertools.chain(*acc_news))","96ece9d0":"# removing stopwords\nimport nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\nsar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\nacc_list_restp = [word for word in acc_list if word.lower() not in stopwords]\n\nprint(\"Length of original Sarcasm list: {0} words\\n\"\n      \"Length of Sarcasm list after stopwords removal: {1} words\"\n      .format(len(sar_list), len(sar_list_restp)))\n\nprint(\"==\"*46)\n\nprint(\"Length of original Acclaim list: {0} words\\n\"\n      \"Length of Acclaim list after stopwords removal: {1} words\"\n      .format(len(acc_list), len(acc_list_restp)))","3d67e42d":"#Data cleaning for getting top 30\nfrom collections import Counter\nsar_cnt = Counter(sar_list_restp)\nacc_cnt = Counter(acc_list_restp)\n\n#Dictonary to Dataframe\nsar_cnt_df = pd.DataFrame(list(sar_cnt.items()), columns = ['Words', 'Freq'])\nsar_cnt_df = sar_cnt_df.sort_values(by=['Freq'], ascending=False)\nacc_cnt_df = pd.DataFrame(list(acc_cnt.items()), columns = ['Words', 'Freq'])\nacc_cnt_df = acc_cnt_df.sort_values(by=['Freq'], ascending=False)\n\n#Top 30\nsar_cnt_df_30 = sar_cnt_df.head(30)\nacc_cnt_df_30 = acc_cnt_df.head(30)","1fd48788":"#Plotting the top 30 Sarcasm Vs Acclaim\nfrom plotly import tools\nsar_tr  = go.Bar(\n    x=sar_cnt_df_30['Freq'],\n    y=sar_cnt_df_30['Words'],\n    name='Sarcasm',\n    marker=dict(\n        color='rgba(155, 89, 182, 0.6)',\n        line=dict(\n            color='rgba(155, 89, 182, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nacc_tr  = go.Bar(\n    x=acc_cnt_df_30['Freq'],\n    y=acc_cnt_df_30['Words'],\n    name='Acclaim',\n    marker=dict(\n        color='rgba(88, 214, 141, 0.6)',\n        line=dict(\n            color='rgba(88, 214, 141, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in Sarcasm Headlines',\n                                                          'Top 30 Most occuring words in Acclaim Headlines'))\n\nfig.append_trace(sar_tr, 1, 1)\nfig.append_trace(acc_tr, 2, 1)\n\n\nfig['layout'].update(height=1200, width=800)\n\niplot(fig, filename='sar_vs_acc')","c40357a3":"# stemming\nstemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n\nprint(\"The stemmed form of learning is: {}\".format(stemmer.stem(\"learning\")))\nprint(\"The stemmed form of learns is: {}\".format(stemmer.stem(\"learns\")))\nprint(\"The stemmed form of learn is: {}\".format(stemmer.stem(\"learn\")))\nprint(\"==\"*46)\nprint(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))\nprint(\"==\"*46)","099d06e3":"# lemattization\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","047fb597":"#Sarcasm headline after Lemmatization\nsar_wost_lem = []\nfor batch in sar_news:\n    sar_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    sar_list_lemm =  [lemm.lemmatize(word) for word in sar_list_restp]\n    sar_wost_lem.append(sar_list_lemm)\n\n#Acclaim headline after Lemmatization\nacc_wost_lem = []\nfor batch in acc_news:\n    acc_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    acc_list_lemm =  [lemm.lemmatize(word) for word in acc_list_restp]\n    acc_wost_lem.append(sar_list_lemm)","ff44e609":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = []\nfor block in sar_wost_lem:\n    vectorizer = CountVectorizer(min_df=0)\n    sentence_transform = vectorizer.fit_transform(block)\n    vec.append(sentence_transform)\n    \nprint(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\nprint(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","aa049b51":"# Converting all sarcasm keywords to single list after lemmatization\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsar_list_wd = list(itertools.chain(*sar_wost_lem))\nfrom wordcloud import WordCloud\nsar_cloud = WordCloud(background_color='black',max_words=1000, width = 1800, height = 1000).\\\n                generate(\" \".join(sar_list_wd))\nplt.figure(figsize=(15,15))\nplt.imshow(sar_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","be8d9055":"acc_list_wd = list(itertools.chain(*acc_wost_lem))\nacc_cloud = WordCloud(background_color='black',max_words=1000, width = 1800, height = 1000).\\\n                generate(\" \".join(acc_list_wd))\nplt.figure(figsize=(15,15))\nplt.imshow(acc_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","ff3a62f5":"# bigram\n\nsar_wost_lem_df = pd.DataFrame({'sarcasm':sar_wost_lem})\nacc_wost_lem_df = pd.DataFrame({'acclaim':acc_wost_lem})\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    ngrams = zip(*[text[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n#Plotting the Bigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Sarcasm Headlines\", \n                                          \"Frequent bigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","4de6f839":"#Plotting the Trigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent Trigrams of Sarcasm Headlines\", \n                                          \"Frequent Trigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","8c2b7958":"from wordcloud import STOPWORDS\nimport string\n## Number of words in the text ##\nsar_acc[\"num_words\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\nsar_acc[\"num_unique_words\"] = sar_acc[\"headline\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\nsar_acc[\"num_chars\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\nsar_acc[\"num_stopwords\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\nsar_acc[\"num_punctuations\"] =sar_acc['headline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_upper\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_title\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\nsar_acc[\"mean_word_len\"] = sar_acc[\"headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","419cc796":"## Truncate some extreme values for better visuals ##\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='is_sarcastic', y='num_words', data=sar_acc, ax=axes[0])\naxes[0].set_xlabel('is_sarcastic', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_chars', data=sar_acc, ax=axes[1])\naxes[1].set_xlabel('is_sarcastic', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_punctuations', data=sar_acc, ax=axes[2])\naxes[2].set_xlabel('is_sarcastic', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()\n","4d0150aa":"#Getting X and Y ready\nfrom sklearn.preprocessing import LabelEncoder\nX = sar_acc.headline\nY = sar_acc.is_sarcastic\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","de973f4f":"# 2. Split into Training and Test data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)","3538af77":"# 3. Processing the data for the model\n\n# Tokenize the data and convert the text to sequences.\n# Add padding to ensure that all the sequences have the same shape.\n# There are many ways of taking the max_len and here an arbitrary length of 150 is chosen\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nmax_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","2c294269":"# importing the functions from keras library \nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\n\n# defining the RNN function \ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n#     adding words to the layer of NN\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n#     adding LTSM to layer\n    layer = LSTM(64)(layer)\n#     adding dense layer\n    layer = Dense(256,name='FC1')(layer)\n#     adding the relu activation function\n    layer = Activation('relu')(layer)\n#     adding dropout pf 20%\n    layer = Dropout(0.2)(layer)\n#     adding the dense layer\n    layer = Dense(1,name='out_layer')(layer)\n#     adding sigmoid activation function\n    layer = Activation('sigmoid')(layer)\n#     initializing the model of RNN based on inputs and layers\n    model = Model(inputs=inputs,outputs=layer)\n    return model","56868ac6":"# calling the model of RNN\nmodel = RNN()\n# generating the summary of the model formed\nmodel.summary()\n# compiling the model and assigning loss function and optimizer\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","37da4731":"from keras.callbacks import EarlyStopping\n# fitting the model\nmodel.fit(sequences_matrix,Y_train,batch_size=100,epochs=5,\n          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","243c45e1":"# creating test sequence from text\ntest_sequences = tok.texts_to_sequences(X_test)\n# creating test sequence matrix using above created test sequence\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","3d568b7c":"# initializing accuracy matrix to store accuracies of all the models and compare them\naccuracy = {}","20991a76":"# evaluation of RNN model\naccr = model.evaluate(test_sequences_matrix,Y_test)\n# printing the loss and accuracy of our model\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n# storing the model name and accuracy in accuracy dictionary\naccuracy.update({\"RNN\":accr[1]})","73f89cd6":"# min max scaling for features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n# feature scaling on training matrix\nprint(scaler.fit(sequences_matrix))\nscaler.transform(sequences_matrix)","ec3f4461":"# min max scaling on testing matrix\nprint(scaler.fit(test_sequences_matrix))\nscaler.transform(test_sequences_matrix)","5699e574":"# importing libraries for random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","644405b9":"# reshaping the ytrain and ytest \nY_train = np.ravel(Y_train) \nY_test = np.ravel(Y_test)","e70eb366":"# initializing randomforestclassifier model with 500 estimators\nRF = RandomForestClassifier(n_estimators=500)\n# fitting the model\nRF.fit(sequences_matrix,Y_train)\n# predicting the values from RFC trained model\nRF_pred = RF.predict(test_sequences_matrix)\n# checking the accuracy of model\naccr = metrics.accuracy_score(Y_test, RF_pred)\nprint(f\"Accuracy of Random Forest Classifier is: {accr}\")\naccuracy.update({\"Random Forest Classifier\":accr})","41ac5623":"print(classification_report(Y_test, RF_pred))","c4b4901d":"# importing DecisionTreeClassifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","316437fb":"# initializing the DecisionTreeClassifier model with gini criterion\nDT = DecisionTreeClassifier(criterion = 'gini')\n# fitting the model\nDT = DT.fit(sequences_matrix,Y_train)\n# predicting values from testing dataset\nDT_pred = DT.predict(test_sequences_matrix)\n# calculating the accuracy of model\naccr = metrics.accuracy_score(Y_test, DT_pred)\nprint(f\"Accuracy of Decision Tree Classifier is: {accr}\")\naccuracy.update({\"Decision Tree Classifier\":accr})","e6fdedb8":"print(classification_report(Y_test, DT_pred))","53ca960e":"# importing support vector classifier model from sklearn\nfrom sklearn.svm import SVC","e94d23e4":"# initializing the model\nsvc = SVC(C =  0.9, kernel = 'rbf', coef0 = 4)\n# fitting the model\nsvc.fit(sequences_matrix,Y_train)\n# predicting values from test dataset\nsvc_pred = svc.predict(test_sequences_matrix)\n# calculating the accuracy\naccr = metrics.accuracy_score(Y_test, svc_pred)\nprint(f\"Accuracy of Support Vector Classifier is: {accr}\")\naccuracy.update({\"Support Vector Classifier\":accr})","94d0350d":"print(classification_report(Y_test, svc_pred))","509542cb":"# sorting the accuracy dicrionary in descending order based on accuracy\naccuracy_sorted = {k:accuracy[k] for k in sorted(accuracy, key=accuracy.get, reverse = True)}\n# printing the accuracy of each model\nprint(\"Accuracy of the above four models are:\\n\")\nfor key,value in accuracy_sorted.items():\n    print(f\"accuracy of {key} model is {value}\")","64bc22b8":"print(f\"The highest accuracy we got is from {list(accuracy_sorted.keys())[0]} with accuracy {list(accuracy_sorted.values())[0]}\")","850e8865":"from sklearn.decomposition import PCA\npca = PCA(2)\nX_projected = pca.fit_transform(sequences_matrix)\nx1 = X_projected[:,0]\nx2 = X_projected[:,1]","5741680f":"# plotting the principle component graph\nfig = plt.figure\nplt.scatter(x1,x2,c=Y_train,alpha=0.8,cmap='cividis')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.colorbar()\nplt.show()","a8c4a4d1":"def user_text_processing(user_text):\n    user_text = user_text.split()\n    user_text = [word.lower() for word in user_text if word not in stopwords]\n    user_text = [lemm.lemmatize(word) for word in user_text]\n    user_text\n\n    # converting user text to sequence\n    user_seq = np.array(user_text)\n    user_seq = tok.texts_to_sequences(user_seq)\n    user_seq = sequence.pad_sequences(user_seq,maxlen=max_len)\n    \n    # min max scaling\n    scaler.fit(user_seq)\n    scaler.transform(user_seq)\n    return user_seq","7b2e12b7":"def predict_sarcasm(user_seq):\n#     prediction\n    prob = model.predict(user_seq)\n    probability = np.mean(prob, axis=0)\n#     print(prob)\n    if probability > 0.5:\n        return(\"Sarcastic\")\n    elif probability < 0.5:\n        return(\"Not Sarcastic\")\n    elif probability == 0.5:\n        return(\"Neutral\")","1169639f":"# user input and pre-processing\nuser_text = 'state population to double by 2040, babies to blame'\nuser_seq = user_text_processing(user_text)\nuser_seq\nprediction = predict_sarcasm(user_seq)\nprint(f\"Sentence '{user_text}' is of '{prediction}' nature\")","b2bc0f76":"# Saving Model\nimport joblib\njoblib.dump(RF, \".\/RF_model.joblib\")\n\nfrom keras.models import load_model\nmodel.save('RNN_model.h5')\n\nimport pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \nfrom keras.models import model_from_yaml\nmodel_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n# serialize weights to HDF5\nmodel.save_weights(\"RNN_model.h5\")\nprint(\"Saved model to disk\")","031154bd":"# Decision Tree Classifier","243068e3":"**Pie Chart to show number of sarcastic and acclaim headlines**","2ed4d8f7":"# Support Vector Classifier","3abd79b8":"# Saving the models","ccad463a":"## Feature Scaling","22b29bf0":"**Plotting the top 30 Sarcasm Vs Acclaim**","39a988df":"**Tokeninizing**","0555d241":"# Accuracy Comparision","f16c95e6":"**Data cleaning to get top 30 words**","49087120":"**Stop words removal**","dc439aba":"**Formation of bag of words**","4357ad10":"# RNN","d7e92d93":"**Loading Libraries**","0ced7a15":"**Lemattization of tokenized words**","0776e315":"**Sarcasm and acclaim headlines after Lemmatization**","50994539":"**creating bigrams for the words and plotting a bigram graph**","b66febb8":"**Stemming the tokenized words**","f0b14c32":"**Converting all acclaim keywords to single list after lemmatization**","d5b46f30":"# **Classification of User Text into Sarcastic or Non-Sarcastic**","da27f9ca":"**Bar graph to show the frequent occuring words**","4c862f1e":"**Converting all sarcasm keywords to single list after lemmatization**","379466da":"**Making trigrams of the words and plotting the graph**","cd3a054a":"# Random Forest Classifier","a898cae7":"**Reading the dataset**","1c6af174":"# Principal Component Plot ","dfcd7e43":"# Data Preperation"}}