{"cell_type":{"3aa92195":"code","6ed01eb3":"code","68198653":"code","76e7e101":"code","c1f87b56":"code","dbcb5a65":"code","14f6c75b":"code","ab8200f1":"code","ecc56221":"code","e0b6dfa7":"code","fe30507f":"code","848f7323":"code","ab734e43":"code","f3c956d2":"code","71e3da2d":"code","f67e4e64":"code","638e1e6e":"code","e65cadd8":"code","bbc5f945":"code","efaf276b":"code","c55a157f":"code","8cf16dca":"code","44638205":"code","6fba6ffe":"code","99e1fb03":"code","b4ef9e82":"code","17c8df20":"code","e4a916c4":"code","42951247":"code","dc6cc6ee":"code","630ef866":"code","b01a066d":"code","8c636c1e":"code","b02eeee4":"code","3abde665":"code","648da94b":"code","3677aee8":"code","3dee57df":"code","f0ad9eb2":"code","b1f82ffe":"markdown","74d3ce06":"markdown","d751d41a":"markdown","a6e07eb4":"markdown","d4cb7099":"markdown","48cd8e6e":"markdown","9d5a51c3":"markdown","de299b77":"markdown","31e9d6ca":"markdown","569d4e19":"markdown","baebb126":"markdown","24c84e46":"markdown","1c69f3cf":"markdown","8cde9cba":"markdown","f99ee225":"markdown","8dde0b84":"markdown","928e89e4":"markdown","d72b0435":"markdown","c5432c98":"markdown","05d0d35a":"markdown","50e8f123":"markdown","4924861f":"markdown","67128deb":"markdown","b0e8f045":"markdown","d46d1907":"markdown","567f93cb":"markdown","691cfdb0":"markdown","5d78fb78":"markdown","57ead37f":"markdown","9ffbbcdc":"markdown","b6e6f122":"markdown","fe01e509":"markdown","9d8f716e":"markdown"},"source":{"3aa92195":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport seaborn as sns","6ed01eb3":"import os\nif not os.path.exists('figures'):\n    os.makedirs('figures')","68198653":"import os\nif not os.path.exists('figures'):\n    os.makedirs('figures')","76e7e101":"def draw_dataframe(df, loc=None, width=None, ax=None, linestyle=None,\n                   textstyle=None):\n    loc = loc or [0, 0]\n    width = width or 1\n\n    x, y = loc\n\n    if ax is None:\n        ax = plt.gca()\n\n    ncols = len(df.columns) + 1\n    nrows = len(df.index) + 1\n\n    dx = dy = width \/ ncols\n\n    if linestyle is None:\n        linestyle = {'color':'black'}\n\n    if textstyle is None:\n        textstyle = {'size': 12}\n\n    textstyle.update({'ha':'center', 'va':'center'})\n\n    # draw vertical lines\n    for i in range(ncols + 1):\n        plt.plot(2 * [x + i * dx], [y, y + dy * nrows], **linestyle)\n\n    # draw horizontal lines\n    for i in range(nrows + 1):\n        plt.plot([x, x + dx * ncols], 2 * [y + i * dy], **linestyle)\n\n    # Create index labels\n    for i in range(nrows - 1):\n        plt.text(x + 0.5 * dx, y + (i + 0.5) * dy,\n                 str(df.index[::-1][i]), **textstyle)\n\n    # Create column labels\n    for i in range(ncols - 1):\n        plt.text(x + (i + 1.5) * dx, y + (nrows - 0.5) * dy,\n                 str(df.columns[i]), style='italic', **textstyle)\n        \n    # Add index label\n    if df.index.name:\n        plt.text(x + 0.5 * dx, y + (nrows - 0.5) * dy,\n                 str(df.index.name), style='italic', **textstyle)\n\n    # Insert data\n    for i in range(nrows - 1):\n        for j in range(ncols - 1):\n            plt.text(x + (j + 1.5) * dx,\n                     y + (i + 0.5) * dy,\n                     str(df.values[::-1][i, j]), **textstyle)\n\n\n#----------------------------------------------------------\n# Draw figure\n\nimport pandas as pd\ndf = pd.DataFrame({'data': [1, 2, 3, 4, 5, 6]},\n                   index=['A', 'B', 'C', 'A', 'B', 'C'])\ndf.index.name = 'key'\n\n\nfig = plt.figure(figsize=(8, 6), facecolor='white')\nax = plt.axes([0, 0, 1, 1])\n\nax.axis('off')\n\ndraw_dataframe(df, [0, 0])\n\nfor y, ind in zip([3, 1, -1], 'ABC'):\n    split = df[df.index == ind]\n    draw_dataframe(split, [2, y])\n\n    sum = pd.DataFrame(split.sum()).T\n    sum.index = [ind]\n    sum.index.name = 'key'\n    sum.columns = ['data']\n    draw_dataframe(sum, [4, y + 0.25])\n    \nresult = df.groupby(df.index).sum()\ndraw_dataframe(result, [6, 0.75])\n\nstyle = dict(fontsize=14, ha='center', weight='bold')\nplt.text(0.5, 3.6, \"Input\", **style)\nplt.text(2.5, 4.6, \"Split\", **style)\nplt.text(4.5, 4.35, \"Apply (sum)\", **style)\nplt.text(6.5, 2.85, \"Combine\", **style)\n\narrowprops = dict(facecolor='black', width=1, headwidth=6)\nplt.annotate('', (1.8, 3.6), (1.2, 2.8), arrowprops=arrowprops)\nplt.annotate('', (1.8, 1.75), (1.2, 1.75), arrowprops=arrowprops)\nplt.annotate('', (1.8, -0.1), (1.2, 0.7), arrowprops=arrowprops)\n\nplt.annotate('', (3.8, 3.8), (3.2, 3.8), arrowprops=arrowprops)\nplt.annotate('', (3.8, 1.75), (3.2, 1.75), arrowprops=arrowprops)\nplt.annotate('', (3.8, -0.3), (3.2, -0.3), arrowprops=arrowprops)\n\nplt.annotate('', (5.8, 2.8), (5.2, 3.6), arrowprops=arrowprops)\nplt.annotate('', (5.8, 1.75), (5.2, 1.75), arrowprops=arrowprops)\nplt.annotate('', (5.8, 0.7), (5.2, -0.1), arrowprops=arrowprops)\n    \nplt.axis('equal')\nplt.ylim(-1.5, 5);\n\nfig.savefig('figures\/03.08-split-apply-combine.png')","c1f87b56":"# common plot formatting for below\ndef format_plot(ax, title):\n    ax.xaxis.set_major_formatter(plt.NullFormatter())\n    ax.yaxis.set_major_formatter(plt.NullFormatter())\n    ax.set_xlabel('feature 1', color='gray')\n    ax.set_ylabel('feature 2', color='gray')\n    ax.set_title(title, color='gray')","dbcb5a65":"from sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.svm import SVC\n\n# create 50 separable points\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state=0, cluster_std=0.60)\n\n# fit the support vector classifier model\nclf = SVC(kernel='linear')\nclf.fit(X, y)\n\n# create some new points to predict\nX2, _ = make_blobs(n_samples=80, centers=2,\n                   random_state=0, cluster_std=0.80)\nX2 = X2[50:]\n\n# predict the labels\ny2 = clf.predict(X2)","14f6c75b":"# plot the data\nfig, ax = plt.subplots(figsize=(8, 6))\npoint_style = dict(cmap='Paired', s=50)\nax.scatter(X[:, 0], X[:, 1], c=y, **point_style)\n\n# format plot\nformat_plot(ax, 'Input Data')\nax.axis([-1, 4, -2, 7])\n\nfig.savefig('figures\/05.01-classification-1.png')","ab8200f1":"# Get contours describing the model\nxx = np.linspace(-1, 4, 10)\nyy = np.linspace(-2, 7, 10)\nxy1, xy2 = np.meshgrid(xx, yy)\nZ = np.array([clf.decision_function([t])\n              for t in zip(xy1.flat, xy2.flat)]).reshape(xy1.shape)\n\n# plot points and model\nfig, ax = plt.subplots(figsize=(8, 6))\nline_style = dict(levels = [-1.0, 0.0, 1.0],\n                  linestyles = ['dashed', 'solid', 'dashed'],\n                  colors = 'gray', linewidths=1)\nax.scatter(X[:, 0], X[:, 1], c=y, **point_style)\nax.contour(xy1, xy2, Z, **line_style)\n\n# format plot\nformat_plot(ax, 'Model Learned from Input Data')\nax.axis([-1, 4, -2, 7])\n\nfig.savefig('figures\/05.01-classification-2.png')","ecc56221":"# plot the results\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nax[0].scatter(X2[:, 0], X2[:, 1], c='gray', **point_style)\nax[0].axis([-1, 4, -2, 7])\n\nax[1].scatter(X2[:, 0], X2[:, 1], c=y2, **point_style)\nax[1].contour(xy1, xy2, Z, **line_style)\nax[1].axis([-1, 4, -2, 7])\n\nformat_plot(ax[0], 'Unknown Data')\nformat_plot(ax[1], 'Predicted Labels')\n\nfig.savefig('figures\/05.01-classification-3.png')","e0b6dfa7":"from sklearn.linear_model import LinearRegression\n\n# Create some data for the regression\nrng = np.random.RandomState(1)\n\nX = rng.randn(200, 2)\ny = np.dot(X, [-2, 1]) + 0.1 * rng.randn(X.shape[0])\n\n# fit the regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# create some new points to predict\nX2 = rng.randn(100, 2)\n\n# predict the labels\ny2 = model.predict(X2)","fe30507f":"# plot data points\nfig, ax = plt.subplots()\npoints = ax.scatter(X[:, 0], X[:, 1], c=y, s=50,\n                    cmap='viridis')\n\n# format plot\nformat_plot(ax, 'Input Data')\nax.axis([-4, 4, -3, 3])\n\nfig.savefig('figures\/05.01-regression-1.png')","848f7323":"from mpl_toolkits.mplot3d.art3d import Line3DCollection\n\npoints = np.hstack([X, y[:, None]]).reshape(-1, 1, 3)\nsegments = np.hstack([points, points])\nsegments[:, 0, 2] = -8\n\n# plot points in 3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[:, 0], X[:, 1], y, c=y, s=35,\n           cmap='viridis')\nax.add_collection3d(Line3DCollection(segments, colors='gray', alpha=0.2))\nax.scatter(X[:, 0], X[:, 1], -8 + np.zeros(X.shape[0]), c=y, s=10,\n           cmap='viridis')\n\n# format plot\nax.patch.set_facecolor('white')\nax.view_init(elev=20, azim=-70)\nax.set_zlim3d(-8, 8)\nax.xaxis.set_major_formatter(plt.NullFormatter())\nax.yaxis.set_major_formatter(plt.NullFormatter())\nax.zaxis.set_major_formatter(plt.NullFormatter())\nax.set(xlabel='feature 1', ylabel='feature 2', zlabel='label')\n\n# Hide axes (is there a better way?)\nax.w_xaxis.line.set_visible(False)\nax.w_yaxis.line.set_visible(False)\nax.w_zaxis.line.set_visible(False)\nfor tick in ax.w_xaxis.get_ticklines():\n    tick.set_visible(False)\nfor tick in ax.w_yaxis.get_ticklines():\n    tick.set_visible(False)\nfor tick in ax.w_zaxis.get_ticklines():\n    tick.set_visible(False)\n\nfig.savefig('figures\/05.01-regression-2.png')","ab734e43":"from matplotlib.collections import LineCollection\n\n# plot data points\nfig, ax = plt.subplots()\npts = ax.scatter(X[:, 0], X[:, 1], c=y, s=50,\n                 cmap='viridis', zorder=2)\n\n# compute and plot model color mesh\nxx, yy = np.meshgrid(np.linspace(-4, 4),\n                     np.linspace(-3, 3))\nXfit = np.vstack([xx.ravel(), yy.ravel()]).T\nyfit = model.predict(Xfit)\nzz = yfit.reshape(xx.shape)\nax.pcolorfast([-4, 4], [-3, 3], zz, alpha=0.5,\n              cmap='viridis', norm=pts.norm, zorder=1)\n\n# format plot\nformat_plot(ax, 'Input Data with Linear Fit')\nax.axis([-4, 4, -3, 3])\n\nfig.savefig('figures\/05.01-regression-3.png')","f3c956d2":"# plot the model fit\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nax[0].scatter(X2[:, 0], X2[:, 1], c='gray', s=50)\nax[0].axis([-4, 4, -3, 3])\n\nax[1].scatter(X2[:, 0], X2[:, 1], c=y2, s=50,\n              cmap='viridis', norm=pts.norm)\nax[1].axis([-4, 4, -3, 3])\n\n# format plots\nformat_plot(ax[0], 'Unknown Data')\nformat_plot(ax[1], 'Predicted Labels')\n\nfig.savefig('figures\/05.01-regression-4.png')","71e3da2d":"from sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.cluster import KMeans\n\n# create 50 separable points\nX, y = make_blobs(n_samples=100, centers=4,\n                  random_state=42, cluster_std=1.5)\n\n# Fit the K Means model\nmodel = KMeans(4, random_state=0)\ny = model.fit_predict(X)","f67e4e64":"# plot the input data\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(X[:, 0], X[:, 1], s=50, color='gray')\n\n# format the plot\nformat_plot(ax, 'Input Data')\n\nfig.savefig('figures\/05.01-clustering-1.png')","638e1e6e":"# plot the data with cluster labels\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n\n# format the plot\nformat_plot(ax, 'Learned Cluster Labels')\n\nfig.savefig('figures\/05.01-clustering-2.png')","e65cadd8":"from sklearn.datasets import make_swiss_roll\n\n# make data\nX, y = make_swiss_roll(200, noise=0.5, random_state=42)\nX = X[:, [0, 2]]\n\n# visualize data\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], color='gray', s=30)\n\n# format the plot\nformat_plot(ax, 'Input Data')\n\nfig.savefig('figures\/05.01-dimesionality-1.png')","bbc5f945":"from sklearn.manifold import Isomap\n\nmodel = Isomap(n_neighbors=8, n_components=1)\ny_fit = model.fit_transform(X).ravel()\n\n# visualize data\nfig, ax = plt.subplots()\npts = ax.scatter(X[:, 0], X[:, 1], c=y_fit, cmap='viridis', s=30)\ncb = fig.colorbar(pts, ax=ax)\n\n# format the plot\nformat_plot(ax, 'Learned Latent Parameter')\ncb.set_ticks([])\ncb.set_label('Latent Variable', color='gray')\n\nfig.savefig('figures\/05.01-dimesionality-2.png')","efaf276b":"fig = plt.figure(figsize=(6, 4))\nax = fig.add_axes([0, 0, 1, 1])\nax.axis('off')\nax.axis('equal')\n\n# Draw features matrix\nax.vlines(range(6), ymin=0, ymax=9, lw=1)\nax.hlines(range(10), xmin=0, xmax=5, lw=1)\nfont_prop = dict(size=12, family='monospace')\nax.text(-1, -1, \"Feature Matrix ($X$)\", size=14)\nax.text(0.1, -0.3, r'n_features $\\longrightarrow$', **font_prop)\nax.text(-0.1, 0.1, r'$\\longleftarrow$ n_samples', rotation=90,\n        va='top', ha='right', **font_prop)\n\n# Draw labels vector\nax.vlines(range(8, 10), ymin=0, ymax=9, lw=1)\nax.hlines(range(10), xmin=8, xmax=9, lw=1)\nax.text(7, -1, \"Target Vector ($y$)\", size=14)\nax.text(7.9, 0.1, r'$\\longleftarrow$ n_samples', rotation=90,\n        va='top', ha='right', **font_prop)\n\nax.set_ylim(10, -2)\n\nfig.savefig('figures\/05.02-samples-features.png')","c55a157f":"def draw_rects(N, ax, textprop={}):\n    for i in range(N):\n        ax.add_patch(plt.Rectangle((0, i), 5, 0.7, fc='white'))\n        ax.add_patch(plt.Rectangle((5. * i \/ N, i), 5. \/ N, 0.7, fc='lightgray'))\n        ax.text(5. * (i + 0.5) \/ N, i + 0.35,\n                \"validation\\nset\", ha='center', va='center', **textprop)\n        ax.text(0, i + 0.35, \"trial {0}\".format(N - i),\n                ha='right', va='center', rotation=90, **textprop)\n    ax.set_xlim(-1, 6)\n    ax.set_ylim(-0.2, N + 0.2)","8cf16dca":"fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nax.axis('off')\ndraw_rects(2, ax, textprop=dict(size=14))\n\nfig.savefig('figures\/05.03-2-fold-CV.png')","44638205":"fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nax.axis('off')\ndraw_rects(5, ax, textprop=dict(size=10))\n\nfig.savefig('figures\/05.03-5-fold-CV.png')","6fba6ffe":"import numpy as np\n\ndef make_data(N=30, err=0.8, rseed=1):\n    # randomly sample the data\n    rng = np.random.RandomState(rseed)\n    X = rng.rand(N, 1) ** 2\n    y = 10 - 1. \/ (X.ravel() + 0.1)\n    if err > 0:\n        y += err * rng.randn(N)\n    return X, y","99e1fb03":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\ndef PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree),\n                         LinearRegression(**kwargs))","b4ef9e82":"X, y = make_data()\nxfit = np.linspace(-0.1, 1.0, 1000)[:, None]\nmodel1 = PolynomialRegression(1).fit(X, y)\nmodel20 = PolynomialRegression(20).fit(X, y)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nax[0].scatter(X.ravel(), y, s=40)\nax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\nax[0].axis([-0.1, 1.0, -2, 14])\nax[0].set_title('High-bias model: Underfits the data', size=14)\n\nax[1].scatter(X.ravel(), y, s=40)\nax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\nax[1].axis([-0.1, 1.0, -2, 14])\nax[1].set_title('High-variance model: Overfits the data', size=14)\n\nfig.savefig('figures\/05.03-bias-variance.png')","17c8df20":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nX2, y2 = make_data(10, rseed=42)\n\nax[0].scatter(X.ravel(), y, s=40, c='blue')\nax[0].plot(xfit.ravel(), model1.predict(xfit), color='gray')\nax[0].axis([-0.1, 1.0, -2, 14])\nax[0].set_title('High-bias model: Underfits the data', size=14)\nax[0].scatter(X2.ravel(), y2, s=40, c='red')\nax[0].text(0.02, 0.98, \"training score: $R^2$ = {0:.2f}\".format(model1.score(X, y)),\n           ha='left', va='top', transform=ax[0].transAxes, size=14, color='blue')\nax[0].text(0.02, 0.91, \"validation score: $R^2$ = {0:.2f}\".format(model1.score(X2, y2)),\n           ha='left', va='top', transform=ax[0].transAxes, size=14, color='red')\n\nax[1].scatter(X.ravel(), y, s=40, c='blue')\nax[1].plot(xfit.ravel(), model20.predict(xfit), color='gray')\nax[1].axis([-0.1, 1.0, -2, 14])\nax[1].set_title('High-variance model: Overfits the data', size=14)\nax[1].scatter(X2.ravel(), y2, s=40, c='red')\nax[1].text(0.02, 0.98, \"training score: $R^2$ = {0:.2g}\".format(model20.score(X, y)),\n           ha='left', va='top', transform=ax[1].transAxes, size=14, color='blue')\nax[1].text(0.02, 0.91, \"validation score: $R^2$ = {0:.2g}\".format(model20.score(X2, y2)),\n           ha='left', va='top', transform=ax[1].transAxes, size=14, color='red')\n\nfig.savefig('figures\/05.03-bias-variance-2.png')","e4a916c4":"x = np.linspace(0, 1, 1000)\ny1 = -(x - 0.5) ** 2\ny2 = y1 - 0.33 + np.exp(x - 1)\n\nfig, ax = plt.subplots()\nax.plot(x, y2, lw=10, alpha=0.5, color='blue')\nax.plot(x, y1, lw=10, alpha=0.5, color='red')\n\nax.text(0.15, 0.2, \"training score\", rotation=45, size=16, color='blue')\nax.text(0.2, -0.05, \"validation score\", rotation=20, size=16, color='red')\n\nax.text(0.02, 0.1, r'$\\longleftarrow$ High Bias', size=18, rotation=90, va='center')\nax.text(0.98, 0.1, r'$\\longleftarrow$ High Variance $\\longrightarrow$', size=18, rotation=90, ha='right', va='center')\nax.text(0.48, -0.12, 'Best$\\\\longrightarrow$\\nModel', size=18, rotation=90, va='center')\n\nax.set_xlim(0, 1)\nax.set_ylim(-0.3, 0.5)\n\nax.set_xlabel(r'model complexity $\\longrightarrow$', size=14)\nax.set_ylabel(r'model score $\\longrightarrow$', size=14)\n\nax.xaxis.set_major_formatter(plt.NullFormatter())\nax.yaxis.set_major_formatter(plt.NullFormatter())\n\nax.set_title(\"Validation Curve Schematic\", size=16)\n\nfig.savefig('figures\/05.03-validation-curve.png')","42951247":"N = np.linspace(0, 1, 1000)\ny1 = 0.75 + 0.2 * np.exp(-4 * N)\ny2 = 0.7 - 0.6 * np.exp(-4 * N)\n\nfig, ax = plt.subplots()\nax.plot(x, y1, lw=10, alpha=0.5, color='blue')\nax.plot(x, y2, lw=10, alpha=0.5, color='red')\n\nax.text(0.2, 0.88, \"training score\", rotation=-10, size=16, color='blue')\nax.text(0.2, 0.5, \"validation score\", rotation=30, size=16, color='red')\n\nax.text(0.98, 0.45, r'Good Fit $\\longrightarrow$', size=18, rotation=90, ha='right', va='center')\nax.text(0.02, 0.57, r'$\\longleftarrow$ High Variance $\\longrightarrow$', size=18, rotation=90, va='center')\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\nax.set_xlabel(r'training set size $\\longrightarrow$', size=14)\nax.set_ylabel(r'model score $\\longrightarrow$', size=14)\n\nax.xaxis.set_major_formatter(plt.NullFormatter())\nax.yaxis.set_major_formatter(plt.NullFormatter())\n\nax.set_title(\"Learning Curve Schematic\", size=16)\n\nfig.savefig('figures\/05.03-learning-curve.png')","dc6cc6ee":"from sklearn.datasets import make_blobs\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n\nfig, ax = plt.subplots()\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\nax.set_title('Naive Bayes Model', size=14)\n\nxlim = (-8, 8)\nylim = (-15, 5)\n\nxg = np.linspace(xlim[0], xlim[1], 60)\nyg = np.linspace(ylim[0], ylim[1], 40)\nxx, yy = np.meshgrid(xg, yg)\nXgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n\nfor label, color in enumerate(['red', 'blue']):\n    mask = (y == label)\n    mu, std = X[mask].mean(0), X[mask].std(0)\n    P = np.exp(-0.5 * (Xgrid - mu) ** 2 \/ std ** 2).prod(1)\n    Pm = np.ma.masked_array(P, P < 0.03)\n    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n                  cmap=color.title() + 's')\n    ax.contour(xx, yy, P.reshape(xx.shape),\n               levels=[0.01, 0.1, 0.5, 0.9],\n               colors=color, alpha=0.2)\n    \nax.set(xlim=xlim, ylim=ylim)\n\nfig.savefig('figures\/05.05-gaussian-NB.png')","630ef866":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"Uniformly-spaced Gaussian Features for 1D input\"\"\"\n    \n    def __init__(self, N, width_factor=2.0):\n        self.N = N\n        self.width_factor = width_factor\n    \n    @staticmethod\n    def _gauss_basis(x, y, width, axis=None):\n        arg = (x - y) \/ width\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n        \n    def fit(self, X, y=None):\n        # create N centers spread along the data range\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n        return self\n        \n    def transform(self, X):\n        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n                                 self.width_, axis=1)\n\nrng = np.random.RandomState(1)\nx = 10 * rng.rand(50)\ny = np.sin(x) + 0.1 * rng.randn(50)\nxfit = np.linspace(0, 10, 1000)\n\ngauss_model = make_pipeline(GaussianFeatures(10, 1.0),\n                            LinearRegression())\ngauss_model.fit(x[:, np.newaxis], y)\nyfit = gauss_model.predict(xfit[:, np.newaxis])\n\ngf = gauss_model.named_steps['gaussianfeatures']\nlm = gauss_model.named_steps['linearregression']\n\nfig, ax = plt.subplots()\n\nfor i in range(10):\n    selector = np.zeros(10)\n    selector[i] = 1\n    Xfit = gf.transform(xfit[:, None]) * selector\n    yfit = lm.predict(Xfit)\n    ax.fill_between(xfit, yfit.min(), yfit, color='gray', alpha=0.2)\n\nax.scatter(x, y)\nax.plot(xfit, gauss_model.predict(xfit[:, np.newaxis]))\nax.set_xlim(0, 10)\nax.set_ylim(yfit.min(), 1.5)\n\nfig.savefig('figures\/05.06-gaussian-basis.png')","b01a066d":"%%file helpers_05_08.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ipywidgets import interact\n\n\ndef visualize_tree(estimator, X, y, boundaries=True,\n                   xlim=None, ylim=None, ax=None):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    if xlim is None:\n        xlim = ax.get_xlim()\n    if ylim is None:\n        ylim = ax.get_ylim()\n    \n    # fit the estimator\n    estimator.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    n_classes = len(np.unique(y))\n    Z = Z.reshape(xx.shape)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap='viridis', clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    \n    # Plot the decision boundaries\n    def plot_boundaries(i, xlim, ylim):\n        if i >= 0:\n            tree = estimator.tree_\n        \n            if tree.feature[i] == 0:\n                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k', zorder=2)\n                plot_boundaries(tree.children_left[i],\n                                [xlim[0], tree.threshold[i]], ylim)\n                plot_boundaries(tree.children_right[i],\n                                [tree.threshold[i], xlim[1]], ylim)\n        \n            elif tree.feature[i] == 1:\n                ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)\n                plot_boundaries(tree.children_left[i], xlim,\n                                [ylim[0], tree.threshold[i]])\n                plot_boundaries(tree.children_right[i], xlim,\n                                [tree.threshold[i], ylim[1]])\n            \n    if boundaries:\n        plot_boundaries(0, xlim, ylim)\n\n\ndef plot_tree_interactive(X, y):\n    def interactive_tree(depth=5):\n        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n        visualize_tree(clf, X, y)\n\n    return interact(interactive_tree, depth=[1, 5])\n\n\ndef randomized_tree_interactive(X, y):\n    N = int(0.75 * X.shape[0])\n    \n    xlim = (X[:, 0].min(), X[:, 0].max())\n    ylim = (X[:, 1].min(), X[:, 1].max())\n    \n    def fit_randomized_tree(random_state=0):\n        clf = DecisionTreeClassifier(max_depth=15)\n        i = np.arange(len(y))\n        rng = np.random.RandomState(random_state)\n        rng.shuffle(i)\n        visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n                       xlim=xlim, ylim=ylim)\n    \n    interact(fit_randomized_tree, random_state=[0, 100]);","8c636c1e":"fig = plt.figure(figsize=(10, 4))\nax = fig.add_axes([0, 0, 0.8, 1], frameon=False, xticks=[], yticks=[])\nax.set_title('Example Decision Tree: Animal Classification', size=24)\n\ndef text(ax, x, y, t, size=20, **kwargs):\n    ax.text(x, y, t,\n            ha='center', va='center', size=size,\n            bbox=dict(boxstyle='round', ec='k', fc='w'), **kwargs)\n\ntext(ax, 0.5, 0.9, \"How big is\\nthe animal?\", 20)\ntext(ax, 0.3, 0.6, \"Does the animal\\nhave horns?\", 18)\ntext(ax, 0.7, 0.6, \"Does the animal\\nhave two legs?\", 18)\ntext(ax, 0.12, 0.3, \"Are the horns\\nlonger than 10cm?\", 14)\ntext(ax, 0.38, 0.3, \"Is the animal\\nwearing a collar?\", 14)\ntext(ax, 0.62, 0.3, \"Does the animal\\nhave wings?\", 14)\ntext(ax, 0.88, 0.3, \"Does the animal\\nhave a tail?\", 14)\n\ntext(ax, 0.4, 0.75, \"> 1m\", 12, alpha=0.4)\ntext(ax, 0.6, 0.75, \"< 1m\", 12, alpha=0.4)\n\ntext(ax, 0.21, 0.45, \"yes\", 12, alpha=0.4)\ntext(ax, 0.34, 0.45, \"no\", 12, alpha=0.4)\n\ntext(ax, 0.66, 0.45, \"yes\", 12, alpha=0.4)\ntext(ax, 0.79, 0.45, \"no\", 12, alpha=0.4)\n\nax.plot([0.3, 0.5, 0.7], [0.6, 0.9, 0.6], '-k')\nax.plot([0.12, 0.3, 0.38], [0.3, 0.6, 0.3], '-k')\nax.plot([0.62, 0.7, 0.88], [0.3, 0.6, 0.3], '-k')\nax.plot([0.0, 0.12, 0.20], [0.0, 0.3, 0.0], '--k')\nax.plot([0.28, 0.38, 0.48], [0.0, 0.3, 0.0], '--k')\nax.plot([0.52, 0.62, 0.72], [0.0, 0.3, 0.0], '--k')\nax.plot([0.8, 0.88, 1.0], [0.0, 0.3, 0.0], '--k')\nax.axis([0, 1, 0, 1])\n\nfig.savefig('figures\/05.08-decision-tree.png')","b02eeee4":"from helpers_05_08 import visualize_tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_blobs\n\n        \nfig, ax = plt.subplots(1, 4, figsize=(16, 3))\nfig.subplots_adjust(left=0.02, right=0.98, wspace=0.1)\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.0)\n\nfor axi, depth in zip(ax, range(1, 5)):\n    model = DecisionTreeClassifier(max_depth=depth)\n    visualize_tree(model, X, y, ax=axi)\n    axi.set_title('depth = {0}'.format(depth))\n\nfig.savefig('figures\/05.08-decision-tree-levels.png')","3abde665":"model = DecisionTreeClassifier()\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\nvisualize_tree(model, X[::2], y[::2], boundaries=False, ax=ax[0])\nvisualize_tree(model, X[1::2], y[1::2], boundaries=False, ax=ax[1])\n\nfig.savefig('figures\/05.08-decision-tree-overfitting.png')","648da94b":"\n\nfrom sklearn.decomposition import PCA\n\ndef draw_vector(v0, v1, ax=None):\n    ax = ax or plt.gca()\n    arrowprops=dict(arrowstyle='->',\n                    linewidth=2,\n                    shrinkA=0, shrinkB=0)\n    ax.annotate('', v1, v0, arrowprops=arrowprops)\n\nrng = np.random.RandomState(1)\nX = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\npca = PCA(n_components=2, whiten=True)\npca.fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\n# plot data\nax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\nax[0].axis('equal');\nax[0].set(xlabel='x', ylabel='y', title='input')","3677aee8":"\nX_pca = pca.transform(X)\nax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\ndraw_vector([0, 0], [0, 3], ax=ax[1])\ndraw_vector([0, 0], [3, 0], ax=ax[1])\nax[1].axis('equal')\nax[1].set(xlabel='component 1', ylabel='component 2',\n          title='principal components',\n          xlim=(-5, 5), ylim=(-3, 3.1))\n\nfig.savefig('figures\/05.09-PCA-rotation.png')","3dee57df":"def plot_pca_components(x, coefficients=None, mean=0, components=None,\n                        imshape=(8, 8), n_components=8, fontsize=12,\n                        show_mean=True):\n    if coefficients is None:\n        coefficients = x\n        \n    if components is None:\n        components = np.eye(len(coefficients), len(x))\n        \n    mean = np.zeros_like(x) + mean\n        \n\n    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n    g = plt.GridSpec(2, 4 + bool(show_mean) + n_components, hspace=0.3)\n\n    def show(i, j, x, title=None):\n        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n        ax.imshow(x.reshape(imshape), interpolation='nearest')\n        if title:\n            ax.set_title(title, fontsize=fontsize)\n\n    show(slice(2), slice(2), x, \"True\")\n    \n    approx = mean.copy()\n    \n    counter = 2\n    if show_mean:\n        show(0, 2, np.zeros_like(x) + mean, r'$\\mu$')\n        show(1, 2, approx, r'$1 \\cdot \\mu$')\n        counter += 1\n\n    for i in range(n_components):\n        approx = approx + coefficients[i] * components[i]\n        show(0, i + counter, components[i], r'$c_{0}$'.format(i + 1))\n        show(1, i + counter, approx,\n             r\"${0:.2f} \\cdot c_{1}$\".format(coefficients[i], i + 1))\n        if show_mean or i > 0:\n            plt.gca().text(0, 1.05, '$+$', ha='right', va='bottom',\n                           transform=plt.gca().transAxes, fontsize=fontsize)\n\n    show(slice(2), slice(-2, None), approx, \"Approx\")\n    return fig","f0ad9eb2":"from sklearn.datasets import load_digits\n\ndigits = load_digits()\nsns.set_style('white')\n\nfig = plot_pca_components(digits.data[10],\n                          show_mean=False)\n\nfig.savefig('figures\/05.09-digits-pixel-components.png')","b1f82ffe":"# Regression Example Figure 2","74d3ce06":"# Learning Curve","d751d41a":"# 2-Fold Cross-Validation","a6e07eb4":"# Gaussian Naive Bayes\n\n### Gaussian Naive Bayes Example","d4cb7099":"# Clustering Example Figures 1","48cd8e6e":"# Decision Tree Overfitting","9d5a51c3":"# Decision Tree Levels","de299b77":"# Principal Components Rotation","31e9d6ca":"# 5-Fold Cross-Validation","569d4e19":"# Classification Example Figure 1","baebb126":"# Bias-Variance Tradeoff","24c84e46":"# Decision Tree Example","1c69f3cf":"# Regression Example Figures","8cde9cba":"# Hyperparameters and Model Validation","f99ee225":"# Random Forests","8dde0b84":"# Dimensionality Reduction Example Figure 1","928e89e4":"# Classification Example Figure 3","d72b0435":"# Overfitting and Underfitting","c5432c98":"# Regression Example Figure 3","05d0d35a":"# Bias-Variance Tradeoff Metrics","50e8f123":"# Validation Curve","4924861f":"# What Is Machine Learning?","67128deb":"# Dimensionality Reduction Example Figure 2","b0e8f045":"# Linear Regression\n\n### Gaussian Basis Functions","d46d1907":"# Introducing Scikit-Learn","567f93cb":"# Classification Example Figure 2","691cfdb0":"# Regression Example Figure 4","5d78fb78":"# Clustering Example Figure 2","57ead37f":"# Aggregation and Grouping","9ffbbcdc":"# Clustering Example Figures","b6e6f122":"# Digits Pixel Components","fe01e509":"# Classification Example Figures","9d8f716e":"# plot principal components"}}