{"cell_type":{"a8168e82":"code","b7d7dfda":"code","4f28c88f":"code","f3a0f663":"code","1e857c92":"code","5d97ec09":"code","8bc85adc":"code","79497107":"code","8d6de879":"code","40b138cf":"code","d0b6d96b":"code","0f2aecc8":"code","31455d26":"code","080820d0":"code","fa23eb0d":"code","7acb0197":"code","66cc37c0":"code","76399920":"code","cc9924fd":"code","2836901e":"code","6c72d758":"code","337ffe2f":"code","b0584693":"code","48d0d0c7":"code","5f005d3f":"markdown","8ab5e153":"markdown","259b77e6":"markdown","9aca01ea":"markdown","4046d2ea":"markdown","4cb23834":"markdown","eec961a1":"markdown","6cc85eb0":"markdown","c1b625ab":"markdown","6fa20731":"markdown","0e625f2f":"markdown","67e5507c":"markdown","36913528":"markdown","c7d6ca41":"markdown","2ba4b4f5":"markdown","bd532794":"markdown","0e6f500c":"markdown"},"source":{"a8168e82":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","b7d7dfda":"df = pd.read_csv('\/kaggle\/input\/co2-mm\/co2_mm_mlo.csv')\ndf['date'] = pd.to_datetime({'year':df['year'], 'month':df['month'], 'day':1}, errors='coerce')\ndf.head()","4f28c88f":"df.set_index('date', inplace=True)\ndf.index.freq = 'MS'\ndf.head()","f3a0f663":"df[['interpolated']].plot(figsize=(12,6));","1e857c92":"import numpy as np\n\nls = ['2016-03-15', '2017-05-24', '2018-08-09']\n\nnp.array(ls, dtype='datetime64')","5d97ec09":"print(np.array(ls, dtype='datetime64[Y]'))\nprint(np.array(ls, dtype='datetime64[h]'))","8bc85adc":"print(np.arange('2018-06-01', '2018-06-23', 7, dtype='datetime64[D]'))\nprint(np.arange('1968', '1976', dtype='datetime64[Y]'))","79497107":"import pytz\npytz.all_timezones[:10]","8d6de879":"print(pd.date_range('1\/1\/2018', periods=7, freq='D'))\nprint(pd.date_range('1 Jan, 2018', '7 Jan, 2018', freq='D'))\nprint(pd.date_range('1 Jan, 2018', '7 Jan, 2018', freq='D', tz='Asia\/Bangkok'))","40b138cf":"pd.to_datetime(['2x1x2019'], format='%dx%mx%Y', errors='raise')","d0b6d96b":"pd.to_datetime(['Jan 01, 2018', '1\/2\/18', '03-Jan-2018', None])","0f2aecc8":"dates = np.arange('2020-07-01', '2020-08-01', dtype = 'datetime64[D]')\nidx = pd.DatetimeIndex(dates)\n\ndf = pd.DataFrame({'A':1, 'B':np.arange(len(dates))}, index=idx)\n\ndf.head(10)","31455d26":"df.resample('3D').sum().head(10)","080820d0":"def last(x):\n    return x[-1]\n\ndf.resample('3D').apply(last).head(10)","fa23eb0d":"df.shift(1).head() # We will lose the first piece of data","7acb0197":"df.shift(-1).tail() # We will lose the last piece of data","66cc37c0":"df.head()","76399920":"df.shift(periods=2, freq='D').head()","cc9924fd":"df = pd.read_csv('\/kaggle\/input\/starbucks\/starbucks.csv', index_col='Date', parse_dates=True)\ndf.head(3)","2836901e":"df['Close'].plot(figsize=(12,6)).autoscale(axis='x', tight=True);","6c72d758":"print('Mean first 3 closes',df.iloc[:3]['Close'].mean())\nprint('Mean first 3 volumns',df.iloc[:3]['Volume'].mean())\nprint()\nprint('Mean next 3 closes',df.iloc[1:4]['Close'].mean())\nprint('Mean next 3 volumns',df.iloc[1:4]['Volume'].mean())","337ffe2f":"df.rolling(window=3).mean().head(4)","b0584693":"df['Close'].plot(figsize=(12,6), label='original').autoscale(axis='x', tight=True);\ndf['Close'].rolling(window=7).mean().plot(label='window=7')\ndf['Close'].rolling(window=60).mean().plot(label='window=60')\n\nplt.legend();","48d0d0c7":"df['Close'].plot(figsize=(12,5), label='').autoscale(axis='x',tight=True)\ndf['Close'].expanding(min_periods=30).mean().plot(figsize=(12,5), label='')\nplt.axhline(df['Close'].mean(), color='k', label='Global avg.')\n\nplt.legend();","5f005d3f":"# 1) Tools\nLet's see tools to represent date, time in Python.","8ab5e153":"### Shifting date index based on Time Series Frequency Code\nWe can choose to shift ***index values*** up or down *without realigning the data* by passing in a freq argument.","259b77e6":"The above plot clearly illustrates some of the fundamentals of time series analysis:\n* **trend** - It's clearly non-linear upward trend \n* **seasonality** - Within a year, there are cyclical patterns of rising and falling \n* **noise** - We also see random, non-systemic fluctuations in the data","9aca01ea":"## 3) Rolling\nThe idea is to divide the data into \"windows\" of time, and then calculate an aggregate function for each window. In this way we can obtain a simple moving average that is able to smooth the data.","4046d2ea":"***Note :*** When we used`pd.date_range()`, we always have to pass `freq` param.","4cb23834":"## endog & exog\nThe data seen in a time series is described as either <u><em>endogenous<\/em><\/u>, that is, caused by factors within the system, or <u><em>exogenous<\/em><\/u>, caused by factors outside the system.","eec961a1":"## 1) Resampling\nA common operation with time series data is resampling based on the time series index. When calling `.resample()` you first need to pass in a **rule** parameter, then you need to call some sort of aggregation function.\n\nThe **rule** parameter describes the frequency with which to apply the aggregation function (daily, monthly, yearly, etc.)<br>\nIt is passed in using an \"offset alias\" - refer to the table below.\n\n<table style=\"display: inline-block\">\n    <caption style=\"text-align: center\"><strong>TIME SERIES OFFSET ALIASES<\/strong><\/caption>\n<tr><th>ALIAS<\/th><th>DESCRIPTION<\/th><\/tr>\n<tr><td>B<\/td><td>business day frequency<\/td><\/tr>\n<tr><td>C<\/td><td>custom business day frequency (experimental)<\/td><\/tr>\n<tr><td>D<\/td><td>calendar day frequency<\/td><\/tr>\n<tr><td>W<\/td><td>weekly frequency<\/td><\/tr>\n<tr><td>M<\/td><td>month end frequency<\/td><\/tr>\n<tr><td>SM<\/td><td>semi-month end frequency (15th and end of month)<\/td><\/tr>\n<tr><td>BM<\/td><td>business month end frequency<\/td><\/tr>\n<tr><td>CBM<\/td><td>custom business month end frequency<\/td><\/tr>\n<tr><td>MS<\/td><td>month start frequency<\/td><\/tr>\n<tr><td>SMS<\/td><td>semi-month start frequency (1st and 15th)<\/td><\/tr>\n<tr><td>BMS<\/td><td>business month start frequency<\/td><\/tr>\n<tr><td>CBMS<\/td><td>custom business month start frequency<\/td><\/tr>\n<tr><td>Q<\/td><td>quarter end frequency<\/td><\/tr>\n<tr><td><\/td><td><font color=white>intentionally left blank<\/font><\/td><\/tr><\/table>\n\n<table style=\"display: inline-block; margin-left: 40px\">\n<caption style=\"text-align: center\"><\/caption>\n<tr><th>ALIAS<\/th><th>DESCRIPTION<\/th><\/tr>\n<tr><td>BQ<\/td><td>business quarter endfrequency<\/td><\/tr>\n<tr><td>QS<\/td><td>quarter start frequency<\/td><\/tr>\n<tr><td>BQS<\/td><td>business quarter start frequency<\/td><\/tr>\n<tr><td>A<\/td><td>year end frequency<\/td><\/tr>\n<tr><td>BA<\/td><td>business year end frequency<\/td><\/tr>\n<tr><td>AS<\/td><td>year start frequency<\/td><\/tr>\n<tr><td>BAS<\/td><td>business year start frequency<\/td><\/tr>\n<tr><td>BH<\/td><td>business hour frequency<\/td><\/tr>\n<tr><td>H<\/td><td>hourly frequency<\/td><\/tr>\n<tr><td>T, min<\/td><td>minutely frequency<\/td><\/tr>\n<tr><td>S<\/td><td>secondly frequency<\/td><\/tr>\n<tr><td>L, ms<\/td><td>milliseconds<\/td><\/tr>\n<tr><td>U, us<\/td><td>microseconds<\/td><\/tr>\n<tr><td>N<\/td><td>nanoseconds<\/td><\/tr><\/table>","6cc85eb0":"errors{\u2018ignore\u2019, \u2018raise\u2019, \u2018coerce\u2019}, default \u2018raise\u2019\n- If `raise`, then invalid parsing will raise an exception.\n- If `coerce`, then invalid parsing will be set as NaT.\n- If `ignore`, then invalid parsing will return the input.","c1b625ab":"Pandas has `pd.to_datetime` that can do everything like Python's datetime calss.","6fa20731":"We can also create our own function. The function will recieve each row of dataframe as an input.","0e625f2f":"We can also use `np.arange` to create date range.","67e5507c":"We can also use Pandas as well.  In Pandas, We can specify precision level as well as *time zone*. We can check all timezone code in Python by using `pytz` module.","36913528":"# 2) Time Series Operations","c7d6ca41":"## 2) Shifting\nshifts the entire date index a given number of rows, without regard for time periods (months & years).","2ba4b4f5":"type='datetime64[D]' -> Numpy applied date in day-level precision.","bd532794":"# Time Series Analysis\n\nIn this first part of TSA series, we'll be looking into tools that are useful to analyze time-series data.","0e6f500c":"## 4) Expanding\nTake into account everything from the start of the time series up to each point in time (not just the moving window). For example, instead of considering the average over the last 7 days, we would consider all prior data."}}