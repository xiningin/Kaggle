{"cell_type":{"9310ce3c":"code","aafa467b":"code","1dcca05d":"code","66909b6a":"code","21f9fd95":"code","f58637d1":"code","331d8db6":"code","86b190e2":"code","ffb422b7":"code","23fea7c7":"code","15b277c5":"code","d325f06a":"code","3f55e7b4":"code","04c93d02":"code","43d0a34b":"code","1a5604cc":"code","2c52ac38":"code","5e80c3b2":"code","79772ccc":"code","9532efe2":"code","847c26ee":"code","2fe9239f":"code","ab582e51":"code","516c08eb":"code","437c2769":"code","5c03585b":"code","73bbc9fa":"code","d56d7fc5":"code","43c641a2":"code","b0ade65c":"code","cb817f41":"code","e7c50be7":"code","03606e9e":"code","f2172d54":"code","4bafb8f3":"code","1e0e61de":"code","50328b1f":"code","d4562e6f":"code","432d5629":"code","d0d6d893":"code","7267f721":"code","84ed8c57":"code","f7c798f7":"code","8484595d":"code","d50998a0":"code","70ed1476":"code","ea1d1efb":"code","a582c317":"code","8a62d55a":"code","1a4384a0":"code","b0640a58":"code","d2f16a65":"code","1c91540b":"code","d6dbfd83":"code","a70c4bbc":"code","62837b40":"code","09e3aceb":"code","45316533":"code","4ad518d4":"code","c9080787":"code","6a48788c":"code","d12d5e50":"code","1d80d7d0":"code","27ad61f4":"code","a8ed676d":"markdown","52d56e7c":"markdown","e407d885":"markdown","178e5c00":"markdown","2a481026":"markdown","dfdd7640":"markdown","58bf9cd8":"markdown","f3c366ba":"markdown","65ac0aa4":"markdown","170b734e":"markdown","db03ee38":"markdown","85ebf7db":"markdown"},"source":{"9310ce3c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport datetime\n\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\n\nimport lightgbm as lgb\nimport gc\n\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score","aafa467b":"def feature_importance(forest, X_train):\n    ranked_list = []\n    \n    importances = forest.feature_importances_\n\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(X_train.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n        ranked_list.append(X_train.columns[indices[f]])\n    \n    return ranked_list\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) ","1dcca05d":"data_path = \"..\/input\"\ntrain = pd.read_csv(os.path.join(data_path, \"train.csv\"))\ntest = pd.read_csv(os.path.join(data_path, \"test.csv\"))","66909b6a":"# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# change education to a number instead of a string, combine the two fields and drop the originals\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 1\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\n\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 1\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\n\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\ntest['edjefe'] = test['edjefe'].astype(\"int\")\n\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# let's keep these features for now\n# train.drop([\"edjefe\", \"edjefa\"], axis=1, inplace=True)\n# test.drop([\"edjefe\", \"edjefa\"], axis=1, inplace=True)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# some rows indicate both that the household does and does not have a toilet, if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0","21f9fd95":"# this came from another kernel, some households have different targets for the same household,\n# we should make the target for each household be the target for the head of that household\nd={}\nweird=[]\nfor row in train.iterrows():\n    idhogar=row[1]['idhogar']\n    target=row[1]['Target']\n    if idhogar in d:\n        if d[idhogar]!=target:\n            weird.append(idhogar)\n    else:\n        d[idhogar]=target\n        \nfor i in set(weird):\n    hhold=train[train['idhogar']==i][['idhogar', 'parentesco1', 'Target']]\n    target=hhold[hhold['parentesco1']==1]['Target'].tolist()[0]\n    for row in hhold.iterrows():\n        idx=row[0]\n        if row[1]['parentesco1']!=1:\n            train.at[idx, 'Target']=target        ","f58637d1":"print(\"Unique Households:\", train.idhogar.nunique())\nprint(\"Total Rows:\", len(train))","331d8db6":"train[['Id','idhogar', 'v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig',\n       'v18q', 'v18q1', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3',\n       'r4t1', 'r4t2', 'r4t3', 'tamhog', 'tamviv', 'escolari', 'rez_esc',\n       'hhsize', 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes',\n       'paredmad', 'paredzinc', 'paredfibras', 'paredother', 'pisomoscer',\n       'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene',\n       'pisomadera', 'techozinc', 'techoentrepiso', 'techocane',\n       'techootro', 'cielorazo', 'abastaguadentro', 'abastaguafuera',\n       'abastaguano', 'public', 'planpri', 'noelec', 'coopele',\n       'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5',\n       'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3',\n       'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3',\n       'elimbasu4', 'elimbasu5', 'elimbasu6', 'epared1', 'epared2',\n       'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2',\n       'eviv3', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2',\n       'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6',\n       'estadocivil7', 'parentesco1', 'parentesco2', 'parentesco3',\n       'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7',\n       'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11',\n       'parentesco12', 'hogar_nin', 'hogar_adul',\n       'hogar_mayor', 'hogar_total', 'dependency', \n       'meaneduc', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4',\n       'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8',\n       'instlevel9', 'bedrooms', 'overcrowding', 'tipovivi1', 'tipovivi2',\n       'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television',\n       'mobilephone', 'qmobilephone', 'lugar1', 'lugar2', 'lugar3',\n       'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'age',\n       'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe',\n       'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned',\n       'agesq', 'Target']].sort_values(\"idhogar\").head(20)","86b190e2":"# min\/max education by household\ntrain['max_esc'] = train.groupby('idhogar')['escolari'].transform('max')\ntrain['min_esc'] = train.groupby('idhogar')['escolari'].transform('min')\n\ntest['max_esc'] = test.groupby('idhogar')['escolari'].transform('max')\ntest['min_esc'] = test.groupby('idhogar')['escolari'].transform('min')","ffb422b7":"# min\/max\/mean behind in education by household\ntrain['max_rez'] = train.groupby('idhogar')['rez_esc'].transform('max')\ntrain['min_rez'] = train.groupby('idhogar')['rez_esc'].transform('min')\ntrain['mean_rez'] = train.groupby('idhogar')['rez_esc'].transform('mean')\n\ntest['max_rez'] = test.groupby('idhogar')['rez_esc'].transform('max')\ntest['min_rez'] = test.groupby('idhogar')['rez_esc'].transform('min')\ntest['mean_rez'] = test.groupby('idhogar')['rez_esc'].transform('mean')\n\n# these features are added by a function, so there is no need to add them twice\n# train['max_age'] = train.groupby('idhogar')['age'].transform('max')\n# train['min_age'] = train.groupby('idhogar')['age'].transform('min')\n\n# test['max_age'] = test.groupby('idhogar')['age'].transform('max')\n# test['min_age'] = test.groupby('idhogar')['age'].transform('min')\n\n# we'll init the feature to 0, then count the people over 18 in the household and use the max of that\ntrain['num_over_18'] = 0\ntrain['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain['num_over_18'] = train['num_over_18'].fillna(0)\n\ntest['num_over_18'] = 0\ntest['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest['num_over_18'] = test['num_over_18'].fillna(0)","23fea7c7":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']\/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']\/df['num_over_18']\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    df['dependency_yes'] = df['dependency'].apply(lambda x: 1 if x == 'yes' else 0)\n    df['dependency_no'] = df['dependency'].apply(lambda x: 1 if x == 'no' else 0)\n    \ndef do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    return df    \n\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\n\ndef process_df(df_):\n    # fix categorical features\n#     encode_data(df_)\n    #fill in missing values based on https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # do feature engineering and drop useless columns\n    return do_features(df_)","15b277c5":"extract_features(train)\nextract_features(test)\n\ntrain = process_df(train)\ntest = process_df(test)","d325f06a":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n    #print(pd.get_dummies(train[cols_2_ohe], \n    #                                   columns=cols_2_ohe).head())\n    #print(tmp_df.head())\n    #print(tmp_df.groupby(['lugar_LE','idhogar']).mean().head())\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    #print(gb.T)\n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n#train, test = train_test_apply_func(train, test, convert_geo2aggs)","3f55e7b4":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","04c93d02":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","43d0a34b":"print(\"train:\", train.shape)\nprint(\"test:\", test.shape)","1a5604cc":"# only train on head of household records since apparently only they are scored?\n# train2 = train.query('parentesco1==1')\n\ntrain2 = train.copy()\ntarget = train2.Target\ntrain2 = train2.drop(\"Target\", axis=1)\n\ntrain_hhs = train2.idhogar\n\nhouseholds = train2.idhogar.unique()\ncv_hhs = np.random.choice(households, size=int(len(households) * 0.25), replace=False)\n\ncv_idx = np.isin(train2.idhogar, cv_hhs)\n\n# train2 = train2.dropna(axis=1, how='any')\n# test = test.dropna(axis=1, how='any')\n\nX_cv = train2[cv_idx]\ny_cv = target[cv_idx]\n\nX_tr = train2[~cv_idx]\ny_tr = target[~cv_idx]\n\n# train on entire dataset\nX_tr = train2\ny_tr = target","2c52ac38":"drop_cols = train.columns[train.isnull().any()]\n\nX_tr.drop(drop_cols, axis=1, inplace=True)\nX_cv.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)","5e80c3b2":"print(\"X_tr:\", X_tr.shape)\nprint(\"X_cv:\", X_cv.shape)\nprint(\"test:\", test.shape)","79772ccc":"extra_drop_cols = []","9532efe2":"# we got this list of columns by training on all columns and looking at the importances\nextra_drop_cols = ['fe_people_weird_stat',\n 'min_rez',\n 'dependency_no',\n 'rez_esc',\n 'parentesco1',\n 'parentesco_LE',\n 'dependency_yes']","847c26ee":"et_drop_cols = ['Id', 'idhogar'] + extra_drop_cols\n\net = ExtraTreesClassifier(n_estimators=250, max_depth=24, min_impurity_decrease=1e-10, min_samples_leaf=1, min_samples_split=2, \n            min_weight_fraction_leaf=0.0, n_jobs=-1, random_state=1, verbose=1)\net.fit(X_tr.drop(et_drop_cols, axis=1), y_tr)","2fe9239f":"et_cv_preds = et.predict(X_cv.drop(et_drop_cols, axis=1))\net_cv_probs = et.predict_proba(X_cv.drop(et_drop_cols, axis=1)) * 2\n\nprint(\"Accuracy:\", et.score(X_cv.drop(et_drop_cols, axis=1), y_cv))\nprint(\"F1:\", f1_score(y_cv, et_cv_preds, average=\"micro\"))","ab582e51":"# depth 24\nprint(\"Accuracy:\", et.score(X_cv.drop(et_drop_cols, axis=1), y_cv))\nprint(\"F1:\", f1_score(y_cv, et_cv_preds, average=\"micro\"))","516c08eb":"et_preds = et.predict(test.drop(et_drop_cols, axis=1))\net_probs = et.predict_proba(test.drop(et_drop_cols, axis=1)) * 2\nmodel_count = 2","437c2769":"cv_submission = pd.DataFrame()\ncv_submission['Id'] = X_cv.Id\ncv_submission['Target_et'] = et_cv_preds","5c03585b":"submission = pd.DataFrame()\nsubmission['Id'] = test.Id\nsubmission['Target_et'] = et_preds","73bbc9fa":"submission[[\"Id\", \"Target_et\"]].to_csv(\"20180725_etc_1.csv\", header=[\"Id\", \"Target\"], index=False)","d56d7fc5":"ranked_features = feature_importance(et, X_tr.drop(et_drop_cols, axis=1))","43c641a2":"extra_drop_cols = ranked_features[95:]\nextra_drop_cols","b0ade65c":"extra_lgb_drop_cols = ['techo_LE',\n 'abastagua_LE',\n 'hogar_total',\n 'v18q',\n 'mobilephone',\n 'hacdor',\n 'tamhog',\n 'hacapo',\n 'parentesco1'\n]","cb817f41":"lgb_drop_cols = ['Id', 'idhogar'] + extra_lgb_drop_cols\n# use the full training set for our cv, and then train only on training set so we can validate\n# train_all = lgb.Dataset(train[feature_names],signal)\ntrain_data = lgb.Dataset(X_tr.drop(lgb_drop_cols, axis=1), y_tr-1)\n\nmax_depth = 15\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': max_depth,\n    'num_leaves': 2**max_depth-1,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 4,\n    'lambda_l2': 0.85,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y_tr)),\n}\nmodel = lgb.train(params, train_data, num_boost_round=500)","e7c50be7":"lgb_cv_probs = model.predict(X_cv.drop(lgb_drop_cols, axis=1))\nlgb_cv_preds = np.argmax(lgb_cv_probs, axis=1) + 1\n\nprint(\"Accuracy:\", accuracy_score(y_cv, lgb_cv_preds))\nprint(\"F1:\", f1_score(y_cv, lgb_cv_preds, average=\"micro\"))","03606e9e":"# lambda 0.85\n# lr 0.05\n# ff 0.75\n# bf 0.85\n# depth 15\nprint(\"Accuracy:\", accuracy_score(y_cv, lgb_cv_preds))\nprint(\"F1:\", f1_score(y_cv, lgb_cv_preds, average=\"micro\"))","f2172d54":"lgb_probs = model.predict(test.drop(lgb_drop_cols, axis=1))\nlgb_preds = np.argmax(lgb_probs, axis=1) + 1","4bafb8f3":"# give this model a lot of weight\ncv_probs = et_cv_probs + lgb_cv_probs + lgb_cv_probs + lgb_cv_probs\ntest_probs = et_probs + lgb_probs + lgb_probs + lgb_probs\nmodel_count += 3","1e0e61de":"# since this is the best model we'll give it more weight in the vote by adding the predictions twice\nsubmission['Target_lgb'] = lgb_preds\nsubmission['Target_lgb4'] = lgb_preds\nsubmission[[\"Id\", \"Target_lgb\"]].to_csv(\"20180725_lgb_1.csv\", header=[\"Id\", \"Target\"], index=False)","50328b1f":"# since this is the best model we'll give it more weight in the vote by adding the predictions twice\ncv_submission['Target_lgb'] = lgb_cv_preds\ncv_submission['Target_lgb4'] = lgb_cv_preds","d4562e6f":"importance = model.feature_importance()\nmodel_fnames = model.feature_name()\ntuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\ntuples = [x for x in tuples if x[1] > 0]\nranked_features = []\n\nprint('Important features:')\nfor i in range(len(model_fnames)):\n    if i < len(tuples):\n        print(i, tuples[i])\n        ranked_features.append(tuples[i][0])\n    else:\n        break\n\ndel importance, model_fnames, tuples","432d5629":"extra_lgb_drop_cols = ranked_features[85:]\nextra_lgb_drop_cols","d0d6d893":"extra_rf_drop_cols = []","7267f721":"extra_rf_drop_cols = ['r4t3_to_tamhog', 'min_rez', 'dependency_no', 'dependency_yes']","84ed8c57":"rf_drop_cols = ['Id', 'idhogar', 'parentesco1'] + extra_rf_drop_cols\n\nrf = RandomForestClassifier(n_estimators=350, max_depth=31, verbose=1, min_impurity_decrease=1e-6, n_jobs=-1)\nrf.fit(X_tr.drop(rf_drop_cols, axis=1), y_tr)","f7c798f7":"rf_cv_preds = rf.predict(X_cv.drop(rf_drop_cols, axis=1))\nrf_cv_probs = rf.predict_proba(X_cv.drop(rf_drop_cols, axis=1))\n\nprint(\"Accuracy:\", accuracy_score(y_cv, rf_cv_preds))\nprint(\"F1:\", f1_score(y_cv, rf_cv_preds, average=\"micro\"))","8484595d":"# 95 cols\nprint(\"Accuracy:\", accuracy_score(y_cv, rf_cv_preds))\nprint(\"F1:\", f1_score(y_cv, rf_cv_preds, average=\"micro\"))","d50998a0":"rf_preds = rf.predict(test.drop(rf_drop_cols, axis=1))\nrf_probs = rf.predict_proba(test.drop(rf_drop_cols, axis=1))","70ed1476":"# weight this model more\ncv_probs = cv_probs + rf_cv_probs\ntest_probs = test_probs + rf_probs\nmodel_count += 1","ea1d1efb":"submission['Target_rf'] = rf_preds\nsubmission[[\"Id\", \"Target_rf\"]].to_csv(\"20180725_rf_1.csv\", header=[\"Id\", \"Target\"], index=False)","a582c317":"cv_submission['Target_rf'] = rf_cv_preds","8a62d55a":"rf_features = feature_importance(rf, X_tr.drop(rf_drop_cols, axis=1))","1a4384a0":"extra_rf_drop_cols = rf_features[96:]\nextra_rf_drop_cols","b0640a58":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\ndf_all = pd.concat([train, test], axis=0)\ncols = [f_ for f_ in df_all.columns if df_all[f_].dtype == 'object' and f_ != id_name]\n\nfor c in cols:\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n\ngc.collect()\nprint(\"Done.\")\n\n# Build the model\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=0)\nerr_buf = []   \n\ncols_to_drop = [\n    id_name, \n    target_name,\n    'idhogar',\n]\n\nX = train2.drop(cols_to_drop, axis=1, errors='ignore')\nfeature_names = list(X.columns)\n\nX = X.fillna(0)\nX = X.values\ny = target\n\nclasses = np.unique(y)\ndprint('Number of classes: {}'.format(len(classes)))\nc2i = {}\ni2c = {}\nfor i, c in enumerate(classes):\n    c2i[c] = i\n    i2c[i] = c\n\ny_le = np.array([c2i[c] for c in y])\n\nX_test = test.drop(cols_to_drop, axis=1, errors='ignore')\nX_test = X_test.fillna(0)\nX_test = X_test.values\nid_test = test[id_name].values\n\ndprint(X.shape, y.shape)\ndprint(X_test.shape)\n\nn_features = X.shape[1]\n\nmax_depth = 12\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': max_depth,\n    'num_leaves': (2**max_depth)-1,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 4,\n    'lambda_l2': 0.85,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y)),\n}\n\nfor i in range(5):\n    cv_hhs = np.random.choice(households, size=int(len(households) * 0.25), replace=False)\n\n    valid_index = np.isin(train_hhs, cv_hhs)\n    train_index = ~np.isin(train_hhs, cv_hhs)\n    \n    print('Fold {}\/{}*{}'.format(cnt + 1, n_splits, n_repeats))\n    params = lgb_params.copy() \n\n    sampler = ADASYN(random_state=0)\n    X_train, y_train = sampler.fit_sample(X[train_index], y_le[train_index])\n\n    lgb_train = lgb.Dataset(\n        X_train, \n        y_train, \n        feature_name=feature_names,\n        )\n    lgb_train.raw_data = None\n\n    lgb_valid = lgb.Dataset(\n        X[valid_index], \n        y_le[valid_index],\n        feature_name=feature_names,\n        )\n    lgb_valid.raw_data = None\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=99999,\n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=200, \n        verbose_eval=100, \n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        for i in range(10):\n            if i < len(tuples):\n                print(i, tuples[i])\n            else:\n                break\n\n        del importance, model_fnames, tuples\n\n    p = model.predict(X[valid_index], num_iteration=model.best_iteration)\n    \n    err = f1_score(y_le[valid_index], np.argmax(p, axis=1), average='micro')\n\n    dprint('{} F1: {}'.format(cnt + 1, err))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    if len(p_buf) == 0:\n        p_buf = np.array(p, dtype=np.float16)\n    else:\n        p_buf += np.array(p, dtype=np.float16)\n    err_buf.append(err)\n\n    cnt += 1\n\n    del model, lgb_train, lgb_valid, p\n    gc.collect()\n\n\nerr_mean = np.mean(err_buf)\nerr_std = np.std(err_buf)\nprint('F1 = {:.6f} +\/- {:.6f}'.format(err_mean, err_std))\n\npreds_lgb2 = p_buf\/cnt\n\n# Prepare probas\nsubm = pd.DataFrame()\nsubm[id_name] = id_test\nfor i in range(preds_lgb2.shape[1]):\n    subm[i2c[i]] = preds_lgb2[:, i]\n\n# Prepare submission\nsubmission[\"Target_lgb2\"] = [i2c[np.argmax(p)] for p in preds_lgb2]\nsubmission[[\"Id\", \"Target_lgb2\"]].to_csv(\"20180725_lgb2_preds.csv\", header=[\"Id\", \"Target\"], index=False)","d2f16a65":"test_probs = test_probs + preds_lgb2\nmodel_count += 1","1c91540b":"lgb_drop_cols = ['Id', 'idhogar']\nopt_parameters = {'colsample_bytree': 0.93, 'min_child_samples': 56, 'num_leaves': 19, 'subsample': 0.84}","d6dbfd83":"def evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nfit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_tr.drop(lgb_drop_cols, axis=1),y_tr-1), (X_cv.drop(lgb_drop_cols, axis=1),y_cv-1)],\n            'eval_names': ['train', 'valid'],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['verbose'] = 200\nfit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]","a70c4bbc":"from sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier\n\ndef _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    \"\"\"Private function used to fit an estimator within a job.\"\"\"\n    if sample_weight is not None:\n        estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X, y, **fit_params)\n    return estimator\n\nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","62837b40":"clfs = []\nfor i in range(10):\n    clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=314+i, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced')\n    clf.set_params(**opt_parameters)\n    clfs.append(('lgbm{}'.format(i), clf))\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\ndel clfs\n#Train the final model with learning rate decay\n_ = vc.fit(X_tr.drop(lgb_drop_cols, axis=1), y_tr-1, **fit_params)\n\nclf_final = vc.estimators_[0]","09e3aceb":"global_score = f1_score(y_cv-1, clf_final.predict(X_cv.drop(lgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_cv-1, vc.predict(X_cv.drop(lgb_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))","45316533":"lgb_cv_probs2 = clf_final.predict_proba(X_cv.drop(lgb_drop_cols, axis=1))\n\nlgb_probs2 = clf_final.predict_proba(test.drop(lgb_drop_cols, axis=1))\nlgb_preds2 = np.argmax(lgb_probs2, axis=1) + 1\nsubmission['Target_lgb3'] = lgb_preds2","4ad518d4":"# weight this model more\ncv_probs = cv_probs + lgb_cv_probs2 + lgb_cv_probs2\ntest_probs = test_probs + lgb_probs2 + lgb_probs2\nmodel_count += 3","c9080787":"submission[[\"Id\", \"Target_lgb3\"]].to_csv(\"20180725_lgb3_preds.csv\", header=[\"Id\", \"Target\"], index=False)","6a48788c":"cv_probs = cv_probs \/ model_count\ntest_probs = test_probs \/ model_count","d12d5e50":"cv_predictions = np.argmax(cv_probs, axis=1) + 1\ntest_predictions = np.argmax(test_probs, axis=1) + 1","1d80d7d0":"print(\"Accuracy:\", accuracy_score(y_cv, cv_predictions))\nprint(\"F1:\", f1_score(y_cv, cv_predictions, average=\"micro\"))","27ad61f4":"submission['Avg_probs'] = test_predictions\nsubmission[[\"Id\", \"Avg_probs\"]].to_csv(\"20180725_avg_preds.csv\", header=[\"Id\", \"Target\"], index=False)","a8ed676d":"The only features which vary within a household are related to the individuals, we will consolidate these and add some household level features.","52d56e7c":"## Some EDA","e407d885":"### Random Forest","178e5c00":"### LGB","2a481026":"Some features taken from: https:\/\/www.kaggle.com\/mlisovyi\/feature-engineering-lighgbm-with-f1-macro by Misha Lisovyi\n\nSecond LGB model taken from: https:\/\/www.kaggle.com\/opanichev\/lgb-as-always by Oleg Panichev","dfdd7640":"### First Model with ExtraTrees","58bf9cd8":"### Split Data\n\nWe'll split the data by household to avoid leakage.","f3c366ba":"### Add Some Features","65ac0aa4":"### One More LGB\n\nTaken from https:\/\/www.kaggle.com\/mlisovyi\/feature-engineering-lighgbm-with-f1-macro by Misha Lisovyi, with some slight modifications.","170b734e":"## Clean Up Data","db03ee38":"### LGB 2\n\nOriginally from https:\/\/www.kaggle.com\/opanichev\/lgb-as-always by Oleg Panichev","85ebf7db":"### Average Probabilities and Make Predictions"}}