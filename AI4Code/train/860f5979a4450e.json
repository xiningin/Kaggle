{"cell_type":{"f9a233d5":"code","a99652e4":"code","6bf20690":"code","3393968f":"code","45ff5a46":"code","ca8c3e9f":"code","27264ff2":"code","fccbd2c7":"code","71c13399":"code","d1d7866e":"code","31ce1293":"code","a338bad2":"code","c70ed989":"code","0c682152":"code","f8da5c2f":"code","d47daa6f":"code","10e445fe":"code","2c0f4f81":"code","94c03e20":"code","3e4f9971":"code","852ecb91":"code","89d686ca":"code","6e7f935a":"code","f2fa651f":"code","26484f23":"code","490e1d7d":"code","315ef0da":"code","180a1537":"code","60b40d30":"code","b8bc7bf7":"code","611b1cd5":"code","f6923c9a":"code","b4857f0c":"code","e9535707":"code","db1db874":"code","54241489":"code","76ecb6e7":"code","da4288e8":"code","a63e5093":"code","b31248bb":"code","e14c9c8c":"code","3ff9865e":"code","0b27e481":"code","584f0605":"code","bc2464ea":"code","82276b2b":"code","2600ccbc":"code","85888f7e":"code","2634305f":"code","14ca2f47":"code","55f794c9":"code","96762217":"markdown","e64e7da2":"markdown","9dd208bb":"markdown","01d901ea":"markdown","2b5656d1":"markdown","82b8ee61":"markdown","b93830e6":"markdown","09d2d09b":"markdown","e0acf963":"markdown","969b212a":"markdown","b6eb6fd4":"markdown","8b8f274d":"markdown","25f547d8":"markdown","c272de67":"markdown","8a9ee98a":"markdown","595ae432":"markdown","28a852f7":"markdown","fcfb8b0a":"markdown","463f67eb":"markdown","b4dbd226":"markdown","3e7d64f9":"markdown","cbc0fc94":"markdown","bf5358f2":"markdown","f3abb95c":"markdown","7ec63ffe":"markdown","7b52ce22":"markdown","4ed2baf3":"markdown","0b3ada5d":"markdown","0de83f1e":"markdown","06eec99c":"markdown","25858c98":"markdown","07931f26":"markdown","b26437e9":"markdown","9a8aeacc":"markdown","dad472d7":"markdown","1b0ec470":"markdown","affc7143":"markdown","d49ba251":"markdown","95afbb83":"markdown"},"source":{"f9a233d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a99652e4":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n\n# Set option \u0111\u1ec3 hi\u1ec7n t\u1ea5t c\u1ea3 c\u00e1c c\u1ed9t, d\u00f2ng\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","6bf20690":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint('Training data shape: ', df_train.shape)\ndf_train.head()","3393968f":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint('Test data shape: ', df_test.shape)\ndf_test.head()","45ff5a46":"# G\u1ed9p 2 t\u1eadp l\u1ea1i\ndf_all = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\nprint('All data shape: ', df_all.shape)\ndf_all.head()","ca8c3e9f":"df_train['Survived'].value_counts()","27264ff2":"df_train['Survived'].astype(int).plot.hist()","fccbd2c7":"# H\u00e0m t\u00ednh missing value\ndef missing_values_table(df):\n        # t\u1ed5ng missing value c\u1ee7a t\u1eebng col\n        miss_val = df.isnull().sum()\n        \n        # % d\u1eef li\u1ec7u thi\u1ebfu\n        miss_val_percent = round(100 * miss_val\/len(df), 2)\n        \n        # t\u1ea1o dataFrame\n        miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n        \n        # Rename\n        miss_val_table.rename(columns = {0 : 'Missing Values', 1 : 'T\u1ec9 l\u1ec7 %'}, inplace=True)\n        \n        # s\u1eafp x\u1ebfp gi\u1ea3m d\u1ea7n t\u1ec9 l\u1ec7 missing\n        miss_val_table.sort_values(by='T\u1ec9 l\u1ec7 %', ascending=False, inplace=True)\n        \n        # ch\u1ec9 l\u1ea5y nh\u1eefng col c\u00f3 missing value\n        miss_val_table = miss_val_table[miss_val_table.iloc[:,1] != 0]\n        \n        # in s\u1ed1 c\u1ed9t thi\u1ebfu\n        print (\"Dataframe c\u00f3: \" + str(df.shape[1]) + \" c\u1ed9t.\\n\"      \n            \"Trong \u0111\u00f3 c\u00f3 \" + str(miss_val_table.shape[0]) + \" c\u1ed9t c\u00f3 missing values.\")\n        \n        # return\n        return miss_val_table","71c13399":"# D\u1eef li\u1ec7u thi\u1ebfu tr\u00ean train.csv\nmissing_values_train = missing_values_table(df_train)\nmissing_values_train","d1d7866e":"# D\u1eef li\u1ec7u thi\u1ebfu tr\u00ean test.csv\nmissing_values_test = missing_values_table(df_test)\nmissing_values_test","31ce1293":"# T\u1ea1o dataframe m\u1edbi \u0111\u1ec3 label encode v\u00e0 t\u00ednh to\u00e1n ph\u1ee5c v\u1ee5 EDA m\u00e0 kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn d\u1eef li\u1ec7u g\u1ed1c\ndf_all_temp = df_all.copy()","a338bad2":"# Convert sang str \u0111\u1ec3 label encode v\u00e0 t\u00ednh correlation\ndf_all_temp['Embarked'] = df_all_temp['Embarked'].astype('str')","c70ed989":"# Label encode cho c\u1ed9t 'Embarked' categorical \u0111\u1ec3 t\u00ednh correlation\ndf_all_temp['Embarked'] = LabelEncoder().fit_transform(df_all_temp['Embarked'])","0c682152":"# T\u00ednh correlation\ndf_all_corr = df_all_temp.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']","f8da5c2f":"df_all_corr = df_all_temp.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Pclass']","d47daa6f":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('\u0110\u1ed9 tu\u1ed5i trung b\u00ecnh theo Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('\u0110\u1ed9 tu\u1ed5i trung b\u00ecnh c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c kh\u00e1ch h\u00e0ng: {}'.format(df_all['Age'].median()))\n\n# \u0110i\u1ec1n missing values \u1edf Age b\u1eb1ng medians c\u1ee7a group Sex v\u00e0 Pclass\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","10e445fe":"df_all[df_all['Embarked'].isnull()]","2c0f4f81":"# \u0110i\u1ec1n theo d\u1eef li\u1ec7u thu th\u1eadp \u0111\u01b0\u1ee3c\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')","94c03e20":"df_all[df_all['Fare'].isnull()]","3e4f9971":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# \u0110i\u1ec1n missing value theo th\u00f4ng tin t\u00ecm \u0111\u01b0\u1ee3c \u1edf tr\u00ean\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","852ecb91":"# Th\u00eam c\u1ed9t Deck l\u01b0u c\u00e1c boong t\u00e0u (ch\u1eef c\u00e1i \u0111\u1ea7u c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh Cabin)\n# v\u1edbi c\u00e1c d\u00f2ng b\u1ecb missing th\u00ec ta cho n\u00f3 gi\u00e1 tr\u1ecb l\u00e0 'M'\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\n# Xem th\u1eed c\u00f3 bao nhi\u00eau boong\ndf_all['Deck'].unique()","89d686ca":"# Group by Deck v\u00e0 Pclass \u0111\u1ec3 xem ph\u00e2n b\u1ed1 c\u1ee7a c\u00e1c ticket class trong c\u00e1c boong\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # T\u1ea1o dictionary cho t\u1eebng ticket class \u1edf m\u1ed7i boong, t\u00ednh count\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # T\u1ea1o dictionary cho t\u1eebng ticket class \u1edf m\u1ed7i boong, t\u00ednh ph\u1ea7n tr\u0103m\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Ticket Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Ticket Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Ticket Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('T\u1ec9 l\u1ec7 ph\u1ea7n tr\u0103m Ticket Class', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Ph\u00e2n b\u1ed1 c\u1ee7a Ticket Class trong t\u1eebng boong', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)","6e7f935a":"# Ph\u00e2n b\u1ed1 c\u1ee5 th\u1ec3 theo s\u1ed1 ng\u01b0\u1eddi tr\u00ean t\u1eebng Deck(boong)\ndf_all_decks","f2fa651f":"idx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","26484f23":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=\n['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns=\n{'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # T\u1ea1o dictionary \u0111\u1ec3 \u0111\u1ebfm s\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t \u1edf t\u1eebng boong\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Kh\u00f4ng qua kh\u1ecfi\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"S\u1ed1ng s\u00f3t\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('T\u1ec9 l\u1ec7 s\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('T\u1ec9 l\u1ec7 s\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t \u1edf c\u00e1c Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","490e1d7d":"df_all_decks","315ef0da":"# B\u1ecf c\u1ed9t Cabin v\u00ec \u0111\u00e3 c\u00f3 c\u1ed9t Deck thay th\u1ebf\ndf_all.drop(['Cabin'], inplace=True, axis=1)","180a1537":"# Ph\u00e2n b\u1ed1 l\u1ea1i t\u1eadp train v\u00e0 test (v\u00ec n\u00e3y gi\u1edd ch\u00fang ta ch\u1ec9 x\u1eed l\u00ed tr\u00ean t\u1eadp g\u1ed9p)\ndf_train, df_test = df_all.loc[:890], df_all.loc[891:].drop(['Survived'], axis=1)","60b40d30":"# Ki\u1ec3m tra l\u1ea1i missing value\nmissing_values_table(df_train)","b8bc7bf7":"# Ki\u1ec3m tra l\u1ea1i missing value\nmissing_values_table(df_test)","611b1cd5":"# V\u1ebd ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u c\u1ee7a 'Age' v\u00e0 'Fare' \u0111\u1ec3 ki\u1ec3m tra th\u1eed\ncont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","f6923c9a":"# T\u1ea1o thu\u1ed9c t\u00ednh Family_Size v\u00e0 visualize xem ph\u00e2n b\u1ed1 s\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t nh\u01b0 th\u1ebf n\u00e0o\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=1)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[1])\n\naxs[0].set_title('Ph\u00e2n b\u1ed1 (theo count) c\u1ee7a thu\u1ed9c t\u00ednh Family Size', size=20, y=1.05)\naxs[1].set_title('S\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t t\u00ednh theo Family Size', size=20, y=1.05)\n\naxs[1].legend(['Kh\u00f4ng qua kh\u1ecfi', 'S\u1ed1ng s\u00f3t'], loc='upper right', prop={'size': 20})\nfor i in range(2):\n    axs[i].tick_params(axis='x', labelsize=20)\n    axs[i].tick_params(axis='y', labelsize=20)\n    axs[i].set_xlabel('')\n    axs[i].set_ylabel('')\n\nplt.show()","b4857f0c":"# Group l\u1ea1i v\u00e0 visualize l\u1ea7n n\u1eefa\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=1)\nplt.subplots_adjust(right=1.5)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Medium', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1])\n\naxs[0].set_title('Ph\u00e2n b\u1ed1 (theo count) c\u1ee7a thu\u1ed9c t\u00ednh Family Size sau khi g\u1ed9p', size=20, y=1.05)\naxs[1].set_title('S\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t t\u00ednh theo Family Size sau khi g\u1ed9p', size=20, y=1.05)\n\naxs[1].legend(['Kh\u00f4ng qua kh\u1ecfi', 'S\u1ed1ng s\u00f3t'], loc='upper right', prop={'size': 20})\nfor i in range(2):\n    axs[i].tick_params(axis='x', labelsize=20)\n    axs[i].tick_params(axis='y', labelsize=20)\n    axs[i].set_xlabel('')\n    axs[i].set_ylabel('S\u1ed1 kh\u00e1ch h\u00e0ng', size=15, labelpad=20)\n\nplt.show()","e9535707":"# Th\u00eam thu\u1ed9c t\u00ednh Ticket_Frequency \u0111\u1ec3 \u0111\u1ebfm t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c ticket\ndf_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\n# Xem t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t c\u1ee7a c\u00e1c group ticket\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('S\u1ed1 kh\u00e1ch h\u00e0ng', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Kh\u00f4ng qua kh\u1ecfi', 'S\u1ed1ng s\u00f3t'], loc='upper right', prop={'size': 15})\nplt.title('S\u1ed1 ng\u01b0\u1eddi s\u1ed1ng s\u00f3t theo {}'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","db1db874":"# D\u00f9ng log + 1 transformation cho Age v\u00e0 Fare\ndf_all['Age'] = np.log1p(df_all['Age'])\ndf_all['Fare'] = np.log1p(df_all['Fare'])","54241489":"# Th\u00eam Is_Married\ndf_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1","76ecb6e7":"# Chuy\u1ec3n danh x\u01b0ng\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')","da4288e8":"df_all.head()","a63e5093":"# T\u00e1ch h\u1ecd gia \u0111\u00ecnh ra (T\u1eeb \u0111\u1ea7u ti\u00ean tr\u01b0\u1edbc d\u1ea5u ph\u1ea9y c\u1ee7a thu\u1ed9c t\u00ednh Name)\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","b31248bb":"# C\u00e1c h\u1ecd gia \u0111\u00ecnh c\u00f3 trong c\u1ea3 2 t\u1eadp train v\u00e0 t\u1eadp test\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Ki\u1ec3m tra xem Family \u0111\u00f3 c\u00f3 trong c\u1ea3 t\u1eadp train v\u00e0 t\u1eadp test, v\u00e0 s\u1ed1 th\u00e0nh vi\u00ean ph\u1ea3i > 1,\n    # v\u00ec b\u1eb1ng 1 th\u00ec kh\u00f4ng t\u00ednh l\u00e0 Family m\u00e0 t\u00ednh l\u00e0 Alone\n    # df_family_survival_rate.iloc[i, 1] = family_size\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # T\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\u00ean nh\u01b0ng cho Ticket\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","e14c9c8c":"mean_survival_rate = np.mean(df_train['Survived'])\n\n# Family\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\n# X\u00e9t \u1edf t\u1eadp train\nfor i in range(len(df_train)):\n    # N\u1ebfu xu\u1ea5t hi\u1ec7n \u1edf t\u1eadp train\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        # Family_survival_rate_NA = 1\n        train_family_survival_rate_NA.append(1)\n        \n    # Ng\u01b0\u1ee3c l\u1ea1i n\u1ebfu kh\u00f4ng xu\u1ea5t hi\u1ec7n\n    else:\n        # \u0110i\u1ec1n gi\u00e1 tr\u1ecb mean\n        train_family_survival_rate.append(mean_survival_rate)\n        # Family_survival_rate_NA = 0\n        train_family_survival_rate_NA.append(0)\n        \n# X\u00e9t \u1edf t\u1eadp test, t\u01b0\u01a1ng t\u1ef1 nh\u01b0 t\u1eadp train\nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\n# Ticket\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","3ff9865e":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2    ","0b27e481":"# Ki\u1ec3m tra ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ec3 encode\ndf_train.info()","584f0605":"df_train.head()","bc2464ea":"# X\u00f3a c\u00e1c c\u1ed9t c\u0169 \u0111i \u0111\u1ec3 tr\u00e1nh ovetfit v\u00e0 t\u00e1ch ID, Survived\ny_train = df_train['Survived']\npred_id = df_test['PassengerId']\ndf_train.drop(['Name', 'Ticket', 'Survived', 'Family', 'PassengerId', 'Parch', 'SibSp', 'Family_Survival_Rate', \n               'Family_Survival_Rate_NA', 'Ticket_Survival_Rate', 'Ticket_Survival_Rate_NA'], axis=1, inplace=True)\ndf_test.drop(['Name', 'Ticket', 'Survived', 'Family', 'PassengerId', 'Parch', 'SibSp', 'Family_Survival_Rate', \n               'Family_Survival_Rate_NA', 'Ticket_Survival_Rate', 'Ticket_Survival_Rate_NA'], axis=1, inplace=True)\ndf_train.head()","82276b2b":"# Label encode cho t\u1ea5t c\u1ea3 c\u00e1c c\u1ed9t lu\u00f4n v\u00ec s\u1ed1 gi\u00e1 tr\u1ecb unique c\u0169ng kh\u00f4ng qu\u00e1 nhi\u1ec1u\nlabel_cols = ['Age', 'Embarked', 'Fare', 'Sex', 'Deck', 'Family_Size_Grouped', 'Title']\nfor col in label_cols:\n    df_train[col] = LabelEncoder().fit_transform(df_train[col])\n    df_test[col] = LabelEncoder().fit_transform(df_test[col])\n\ndf_train.head()","2600ccbc":"df_test.head()","85888f7e":"# Chia t\u1eadp train, test\nX_train = df_train\nY_train = y_train\nX_test  = df_test\nX_train.shape, Y_train.shape, X_test.shape","2634305f":"# D\u00f9ng model RandomForestClassifier\nrandom_forest = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=5,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nrandom_forest.fit(X_train, Y_train)\nY_pred_1 = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","14ca2f47":"# D\u00f9ng model HistGradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\ngradient = HistGradientBoostingClassifier(max_iter= 500,\n                                n_iter_no_change= 10,\n                                verbose= 1,\n                                scoring= 'roc_auc',\n                                validation_fraction= 0.2,\n                                early_stopping= True,\n                                tol= 0,\n                                learning_rate= 0.1,\n                                max_depth= 5,\n                                random_state= 5)\ngradient.fit(X_train, Y_train)\nY_pred_2 = gradient.predict(X_test)\nacc_gradient = round(gradient.score(X_train, Y_train) * 100, 2)\nacc_gradient","55f794c9":"submission = pd.DataFrame({'PassengerId': pred_id, 'Survived': Y_pred_1.astype(int)})\nsubmission.to_csv('submis_titanic.csv', index = False)","96762217":"###### Gia \u0111\u00ecnh","e64e7da2":"- Deck B, D, E c\u00f3 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t cao nh\u1ea5t, c\u1ea3 3 boong n\u00e0y \u0111\u1ec1u c\u00f3 ph\u1ea7n l\u1edbn kh\u00e1ch h\u00e0ng thu\u1ed9c Ticket Class 1\n- Deck A c\u0169ng c\u00f3 to\u00e0n b\u1ed9 kh\u00e1ch h\u00e0ng l\u00e0 Ticket Class 1, v\u1eady t\u1ea1i sao l\u1ea1i c\u00f3 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t th\u1ea5p h\u01a1n h\u1eb3n?\n> Em ngh\u0129 l\u00e0 v\u00ec Deck A n\u1eb1m ngay \u0111\u1ea7u m\u0169i t\u00e0u, nh\u01b0 v\u1eady khi \u0111\u00e2m v\u00e0o b\u0103ng th\u00ec ph\u1ea7n n\u00e0y thi\u1ec7t h\u1ea1i nhi\u1ec1u nh\u1ea5t n\u00ean kh\u00f3 c\u00f3 th\u1ec3 s\u1ed1ng s\u00f3t \u0111\u01b0\u1ee3c\n- \u1edf ph\u1ea7n n\u00e0y th\u00ec t\u00e1c gi\u1ea3 g\u1ed9p c\u00e1c Deck c\u00f3 chung t\u1ec9 l\u1ec7 ph\u00e2n b\u1ed1 c\u1ee7a Ticket Class l\u1ea1i v\u1edbi nhau, tuy nhi\u00ean th\u00ec em th\u1ea5y kh\u00f4ng c\u00f3 m\u1ed9t l\u00ed do x\u00e1c \u0111\u00e1ng n\u00e0o \u0111\u1ec3 l\u00e0m nh\u01b0 v\u1eady c\u1ea3, n\u00ean em kh\u00f4ng g\u1ed9p theo t\u00e1c gi\u1ea3","9dd208bb":"- **Family_Survival_Rate:** t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t trung b\u00ecnh c\u1ee7a h\u1ecd gia \u0111\u00ecnh c\u00f3 nhi\u1ec1u h\u01a1n 1 ng\u01b0\u1eddi, xu\u1ea5t hi\u1ec7n trong c\u1ea3 t\u1eadp *train* v\u00e0 t\u1eadp *test*, tuy nhi\u00ean s\u1ebd ch\u1ec9 \u0111\u01b0\u1ee3c t\u00ednh t\u1eeb t\u1eadp train v\u00ec t\u1eadp test kh\u00f4ng c\u00f3 thu\u1ed9c t\u00ednh **Survived**\n- **Family_Survival_Rate_NA:** thu\u1ed9c t\u00ednh binary \u0111\u00e1nh d\u1ea5u c\u00e1c h\u1ecd gia \u0111\u00ecnh ch\u1ec9 xu\u1ea5t hi\u1ec7n trong t\u1eadp *test*\n\n- 2 thu\u1ed9c t\u00ednh **Ticket_Survival_Rate** v\u00e0 **Ticket_Survival_Rate_NA** c\u0169ng \u0111\u01b0\u1ee3c t\u1ea1o ra t\u01b0\u01a1ng t\u1ef1 t\u1eeb thu\u1ed9c t\u00ednh **Ticket**","01d901ea":"##### Title - Danh x\u01b0ng & Is Married - Ph\u1ee5 n\u1eef \u0111\u00e3 c\u01b0\u1edbi\n\n- Chuy\u1ec3n **Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona** th\u00e0nh **Miss\/Mrs\/Ms**, v\u00ec \u0111\u00e2y \u0111\u1ec1u l\u00e0 ch\u1ec9 ph\u1ee5 n\u1eef\n- Chuy\u1ec3n **Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev** th\u00e0nh **Dr\/Military\/Noble\/Clergy**, v\u00ec \u0111\u00e2y \u0111\u1ec1u l\u00e0 danh x\u01b0ng ch\u1ec9 t\u1ea7ng l\u1edbp cao qu\u00fd\n- Th\u00eam m\u1ed9t c\u1ed9t *Is_Married* \u0111\u1ec3 \u0111\u00e1nh d\u1ea5u nh\u1eefng ng\u01b0\u1eddi c\u00f3 danh x\u01b0ng l\u00e0 **Mrs**, v\u00ec \u0111\u00e2y l\u00e0 nh\u1eefng ng\u01b0\u1eddi c\u00f3 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t cao nh\u1ea5t","2b5656d1":"**Nh\u1eadn x\u00e9t:** D\u1eef li\u1ec7u c\u00f3 v\u1ebb kh\u00f4ng b\u1ecb l\u1ec7ch qu\u00e1 nhi\u1ec1u -> \u1ed5n","82b8ee61":"### Import th\u01b0 vi\u1ec7n","b93830e6":"##### Li\u1ec7u nh\u1eefng ng\u01b0\u1eddi \u0111i m\u1ed9t m\u00ecnh c\u00f3 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t cao h\u01a1n gia \u0111\u00ecnh ho\u1eb7c m\u1ed9t nh\u00f3m ng\u01b0\u1eddi kh\u00f4ng?\n\n- Ta s\u1ebd t\u1ea1o th\u00eam m\u1ed9t thu\u1ed9c t\u00ednh **Family_Size** th\u1ec3 hi\u1ec7n s\u1ed1 ng\u01b0\u1eddi trong m\u1ed9t gia \u0111\u00ecnh, \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng **SibSp** + **Parch** + **1** (**1** l\u00e0 b\u1ea3n th\u00e2n kh\u00e1ch h\u00e0ng \u0111\u00f3)\n- \u0110\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00f4ng \u0111i v\u1edbi gia \u0111\u00ecnh m\u00e0 \u0111i v\u1edbi b\u1ea1n b\u00e8 (nh\u00f3m ng\u01b0\u1eddi) ta s\u1ebd d\u00f9ng thu\u1ed9c t\u00ednh **Ticket**, t\u1ee9c l\u00e0 nh\u1eefng ng\u01b0\u1eddi \u0111i c\u0169ng nhau s\u1ebd c\u00f3 chung m\u1ed9t ticket number - thu\u1ed9c t\u00ednh **Ticket**","09d2d09b":"##### Thu\u1ed9c t\u00ednh *Age*","e0acf963":"- \u1ede \u0111\u00e2y th\u00ec t\u00e1c gi\u1ea3 t\u1ed5ng h\u1ee3p l\u1ea1i th\u00e0nh 1 thu\u1ed9c t\u00ednh **Survival_Rate** = (**Ticket_Survival_Rate** + **Family_Survival_Rate**) \/ 2, nh\u01b0ng m\u00e0 em ngh\u0129 l\u00e0 m\u00ecnh n\u00ean \u0111\u1ec3 nguy\u00ean nh\u01b0 v\u1eady \u0111\u1ec3 model c\u00f3 th\u1ec3 h\u1ecdc t\u1ed1t h\u01a1n, tuy nhi\u00ean c\u0169ng c\u00f3 kh\u1ea3 n\u0103ng s\u1ebd b\u1ecb overfit, em s\u1ebd ki\u1ec3m tra sau\n>Sau khi th\u1eed v\u00e0i l\u1ea7n th\u00ec em th\u1ea5y c\u00f3 b\u1ecb overfit n\u00ean em \u0111\u00e3 s\u1eeda l\u1ea1i theo h\u01b0\u1edbng c\u1ee7a t\u00e1c gi\u1ea3","969b212a":"V\u1edbi 2 tr\u01b0\u1eddng h\u1ee3p n\u00e0y th\u00ec t\u00e1c gi\u1ea3 Gunes Evitan \u0111\u00e3 search T\u00ean c\u1ee7a h\u1ecd tr\u00ean google v\u00e0 t\u00ecm \u0111\u01b0\u1ee3c th\u00f4ng tin c\u1ee5 th\u1ec3 l\u00e0 2 ng\u01b0\u1eddi n\u00e0y l\u00e0 b\u00e0 ch\u1ee7 v\u00e0 h\u1ea7u g\u00e1i, l\u00ean t\u00e0u \u1edf c\u1ea3ng Southampton, t\u1ee9c l\u00e0 **Embarked** = S","b6eb6fd4":"![Titanic Side Plan](https:\/\/vignette.wikia.nocookie.net\/titanic\/images\/f\/f9\/Titanic_side_plan.png\/revision\/latest?cb=20180322183733)","8b8f274d":"### Model","25f547d8":"##### Thu\u1ed9c t\u00ednh *Fare*\n\n- **Fare** b\u1ecb thi\u1ebfu \u1edf 1 d\u00f2ng, th\u1eed ki\u1ec3m tra","c272de67":"##### X\u1eed l\u00ed l\u1ec7ch d\u1eef li\u1ec7u","8a9ee98a":"##### Thu\u1ed9c t\u00ednh *Embarked*\n\n- **Embarked** b\u1ecb thi\u1ebfu \u1edf 2 d\u00f2ng, th\u1eed ki\u1ec3m tra","595ae432":"## M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n\n**B\u00e0i to\u00e1n:** [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic\/overview)","28a852f7":"- **Pclass** c\u0169ng c\u00f3 t\u01b0\u01a1ng quan kh\u00e1 cao v\u1edbi **Survived** - thu\u1ed9c t\u00ednh c\u1ea7n d\u1ef1 \u0111o\u00e1n, v\u1eady n\u00ean \u0111i\u1ec1n theo group **Pclass** l\u00e0 m\u1ed9t l\u1ef1a ch\u1ecdn h\u1ee3p l\u00ed, tuy nhi\u00ean li\u1ec7u c\u00f3 b\u1ecb Overfit hay kh\u00f4ng?\n> Theo em ngh\u0129 th\u00ec corr = 0.338481 c\u0169ng kh\u00f4ng ph\u1ea3i l\u00e0 qu\u00e1 cao, h\u01a1n n\u1eefa ta c\u0169ng c\u00f3 th\u1ec3 group by th\u00eam m\u1ed9t l\u1ea7n n\u1eefa theo **Sex** - gi\u1edbi t\u00ednh, thu\u1ed9c t\u00ednh m\u00e0 ta bi\u1ebft \u1edf ngo\u00e0i \u0111\u1eddi th\u1ef1c l\u00e0 c\u00f3 \u1ea3nh h\u01b0\u1edfng kh\u00e1 nhi\u1ec1u","fcfb8b0a":"### Import Data","463f67eb":"##### Encoding\n\n- T\u1eeb ph\u1ea7n n\u00e0y tr\u1edf \u0111i em l\u00e0m kh\u00e1c t\u00e1c gi\u1ea3 ho\u00e0n to\u00e0n v\u00ec em feature engineering kh\u00e1c v\u00e0i ch\u1ed7, v\u00e0 s\u1eed d\u1ee5ng model kh\u00e1c","b4dbd226":"#### C\u1ed9t c\u1ea7n d\u1ef1 \u0111o\u00e1n - Survived","3e7d64f9":"V\u1eady c\u00f2n nh\u1eefng gia \u0111\u00ecnh, ticket ch\u1ec9 c\u00f3 trong t\u1eadp train kh\u00f4ng c\u00f3 trong t\u1eadp test, v\u00e0 ng\u01b0\u1ee3c l\u1ea1i ch\u1ec9 c\u00f3 trong t\u1eadp test kh\u00f4ng c\u00f3 trong t\u1eadp train th\u00ec sao?\n- Ta s\u1ebd \u0111i\u1ec1n gi\u00e1 tr\u1ecb **mean c\u1ee7a thu\u1ed9c t\u00ednh Survived \u1edf t\u1eadp train** cho c\u00e1c tr\u01b0\u1eddng h\u1ee3p n\u00e0y\n- V\u00e0 n\u1ebfu c\u00e1c Family ho\u1eb7c Ticket n\u00e0y xu\u1ea5t \u1edf t\u1eadp train th\u00ec **Family_survival_rate_NA = 1**, kh\u00f4ng xu\u1ea5t hi\u1ec7n th\u00ec **= 0**; v\u00e0 t\u01b0\u01a1ng t\u1ef1 cho t\u1eadp test","cbc0fc94":"### Fearture Engineering","bf5358f2":"![submit_final.png](attachment:62d9597d-ab97-43a0-8d2c-d97b677d98d0.png)","f3abb95c":"- \u1ede \u0111\u00e2y th\u00ec ph\u00e2n b\u1ed1 c\u1ee7a c\u1ea3 **Age** v\u00e0 **Fare** \u0111\u1ec1u b\u1ecb l\u1ec7nh kh\u00e1 n\u1eb7ng, t\u00e1c gi\u1ea3 ti\u1ec1n x\u1eed l\u00ed b\u1eb1ng c\u00e1ch **chia bin**, tuy nhi\u00ean th\u00ec em ngh\u0129 n\u00f3 s\u1ebd kh\u00f4ng c\u00f3 t\u00e1c d\u1ee5ng m\u1ea5y n\u00ean em s\u1ebd d\u00f9ng **log transform** \u0111\u1ec3 x\u1eed l\u00ed v\u1ea5n \u0111\u1ec1 l\u1ec7ch d\u1eef li\u1ec7u n\u00e0y","7ec63ffe":"##### Thu\u1ed9c t\u00ednh *Cabin*","7b52ce22":"##### T\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t theo Family - h\u1ecd gia \u0111\u00ecnh","4ed2baf3":"Ch\u1eef c\u00e1i \u0111\u1ea7u ti\u00ean c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh **Cabin** ch\u00ednh l\u00e0 c\u00e1c deck (boong t\u00e0u) m\u00e0 cabin \u0111\u00f3 thu\u1ed9c v\u1ec1\n\nC\u00f3 kh\u1ea3 n\u0103ng m\u1ed7i boong t\u00e0u v\u1edbi v\u1ecb tr\u00ed kh\u00e1c nhau s\u1ebd cho t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t kh\u00e1c nhau, h\u01a1n n\u1eefa c\u00e1c boong kh\u00e1c nhau s\u1ebd ph\u1ee5c v\u1ee5 cho c\u00e1c ticket class kh\u00e1c nhau n\u00ean ch\u00fang ta s\u1ebd ki\u1ec3m tra th\u1eed","0b3ada5d":"Em \u0111\u00e3 th\u1eed 2 model:\n- **RandomForestClassifier:** cho validation score ch\u1ec9 0.862, nh\u01b0ng khi n\u1ed9p l\u00ean Kaggle th\u00ec \u0111\u01b0\u1ee3c **0.79425**, l\u00e0 \u0111i\u1ec3m cao nh\u1ea5t c\u1ee7a em\n- **HistGradientBoostingClassifier:** validation score t\u1eadn 0.8878, nh\u01b0ng khi n\u1ed9p ch\u1ec9 \u0111\u1ea1t **0.74162** n\u1ebfu em nh\u1edb kh\u00f4ng nh\u1ea7m, t\u1ee9c l\u00e0 th\u1ea5p h\u01a1n c\u1ea3 baseline; h\u01a1n n\u1eefa l\u00fac \u0111\u1ea7u em feature engineering theo ki\u1ec3u kh\u00e1c: b\u1ecf lu\u00f4n ph\u1ea7n **T\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t theo Family - h\u1ecd gia \u0111\u00ecnh** (t\u1ee9c ph\u1ea7n thu\u1ed9c t\u00ednh **Family_Survival_Rate**, **Family_Survival_Rate_NA**, ...) v\u00e0 gi\u1eef l\u1ea1i **SibSp**, **Parch** th\u00ec m\u00f4 h\u00ecnh n\u00e0y l\u1ea1i cho k\u1ebft qu\u1ea3 l\u00e0 **0.76555**\n> Em ngh\u0129 c\u00f3 c\u00f3 2 tr\u01b0\u1eddng h\u1ee3p l\u00e0: do feature engineering g\u00e2y ra ovetfit ho\u1eb7c do em ch\u01b0a tuning hyperparameters, tuy nhi\u00ean em kh\u00f4ng c\u00f3 \u0111\u1ee7 th\u1eddi gian \u0111\u1ec3 ki\u1ec3m tra th\u00eam n\u1eefa @@\n\n-- Anyway, c\u0169ng \u0111\u00e3 g\u1ea7n T\u1ebft r\u1ed3i, em ch\u00fac th\u1ea7y v\u00e0 gia \u0111\u00ecnh m\u1ed9t c\u00e1i T\u1ebft \u0111o\u00e0n vi\u00ean v\u00e0 h\u1ea1nh ph\u00fac \u1ea1!","0de83f1e":"#### Ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u","06eec99c":"- Ta th\u1ea5y l\u00e0 th\u01b0\u1eddng th\u00ec ng\u01b0\u1eddi ta s\u1ebd \u0111i chung v\u1edbi gia \u0111\u00ecnh, v\u00e0 c\u1ea3 gia \u0111\u00ecnh s\u1ebd c\u00f3 chung m\u1ed9t ticket class (thu\u1ed9c t\u00ednh **Pclass**), nh\u01b0 v\u1eady th\u00ec s\u1ebd c\u00f3 chung gi\u00e1 v\u00e9 v\u1edbi nhau (thu\u1ed9c t\u00ednh **Fare**)\n> V\u1eady n\u00ean ta s\u1ebd group by **Pclass**, **Parch** v\u00e0 **SibSp** v\u00e0 \u0111i\u1ec1n gi\u00e1 tr\u1ecb thi\u1ebfu b\u1eb1ng median","25858c98":"###### Nh\u00f3m ng\u01b0\u1eddi","07931f26":"Ch\u00fang ta s\u1ebd ti\u1ebfn h\u00e0nh \u0111i\u1ec1n d\u1eef li\u1ec7u cho **Age**, v\u00ec c\u1ed9t n\u00e0y thi\u1ebfu ch\u1ec9 kho\u1ea3ng 20%. Nh\u01b0ng m\u00e0 s\u1ebd \u0111i\u1ec1n nh\u01b0 th\u1ebf n\u00e0o? Median, hay mode?\n\nTheo nh\u01b0 em tham kh\u1ea3o \u1edf [ngu\u1ed3n n\u00e0y](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) th\u00ec m\u00ecnh s\u1ebd group by theo **Pclass** r\u1ed3i \u0111i\u1ec1n median\n\n(T\u00e1c gi\u1ea3 \u0111\u01b0a ra \u00fd t\u01b0\u1edfng v\u00e0 code, em d\u1ef1a v\u00e0o \u0111\u00f3 \u0111\u1ec3 t\u1ef1 gi\u1ea3i th\u00edch v\u00e0 code th\u00eam \u0111\u1ec3 b\u1ed5 sung cho \u00fd ki\u1ebfn c\u1ee7a m\u00ecnh)","b26437e9":"## Gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n","9a8aeacc":"- Ta th\u1ea5y ph\u00e2n b\u1ed1 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t c\u1ee7a c\u00e1c nh\u00f3m: **2, 3, 4**; **5, 6, 7**; **8, 11** gi\u1ed1ng nhau n\u00ean ta s\u1ebd ti\u1ebfn h\u00e0nh group c\u00e1c nh\u00f3m n\u00e0y l\u1ea1i v\u1edbi nhau nh\u01b0 sau:\n    - **Family_Size** = 1 \u0111\u01b0\u1ee3c label th\u00e0nh **Alone**\n    - **Family_Size** = 2, 3, 4 \u0111\u01b0\u1ee3c label th\u00e0nh **Small**\n    - **Family_Size** = 5, 6, 7 \u0111\u01b0\u1ee3c label th\u00e0nh **Medium** (\u1edf \u0111\u00e2y t\u00e1c gi\u1ea3 ch\u1ec9 g\u1ed9p 5 v\u00e0 6, kh\u00f4ng c\u00f3 7 nh\u01b0ng em th\u1ea5y 7 c\u0169ng c\u00f3 t\u1ec9 l\u1ec7 s\u1ed1ng s\u00f3t kh\u00e1 gi\u1ed1ng n\u00ean t\u1ef1 th\u00eam v\u00e0o)\n    - **Family_Size** = 8, 11 \u0111\u01b0\u1ee3c label th\u00e0nh **Large**","dad472d7":"- Ta th\u1ea5y Deck T ch\u1ec9 c\u00f3 duy nh\u1ea5t 1 kh\u00e1ch h\u00e0ng v\u00e0 thu\u1ed9c class 1 -> g\u1ed9p chung v\u00e0o Deck A lu\u00f4n","1b0ec470":"#### Missing Value","affc7143":"- N\u1ebfu nh\u01b0 ch\u1ec9 \u0111\u01a1n gi\u1ea3n \u0111i\u1ec1n d\u1eef li\u1ec7u thi\u1ebfu theo median c\u1ee7a to\u00e0n b\u1ed9 d\u1eef li\u1ec7u th\u00ec s\u1ebd \u0111em l\u1ea1i k\u1ebft qu\u1ea3 kh\u00f4ng t\u1ed1t l\u1eafm -> v\u1eady n\u00ean ch\u00fang ta n\u00ean \u0111i\u1ec1n d\u1eef li\u1ec7u theo m\u1ed9t ti\u00eau ch\u00ed c\u1ee5 th\u1ec3 h\u01a1n\n- \u1ede \u0111\u00e2y **Age** c\u00f3 \u0111\u1ed9 t\u01b0\u01a1ng quan kh\u00e1 cao v\u1edbi **Pclass**, v\u1eady ch\u00fang ta c\u00f3 th\u1ec3 group by **Age** theo **Pclass** v\u00e0 \u0111i\u1ec1n gi\u00e1 tr\u1ecb thi\u1ebfu b\u1eb1ng median c\u1ee7a group. Ki\u1ec3m tra c\u1ee5 th\u1ec3 h\u01a1n **Pcalss** xem sao","d49ba251":"### T\u00ecm hi\u1ec3u d\u1eef li\u1ec7u (Exploratory Data Analysis)","95afbb83":"# B\u00e1o c\u00e1o b\u00e0i t\u1eadp c\u1ed9ng \u0111i\u1ec3m m\u00f4n \"Khoa h\u1ecdc d\u1eef li\u1ec7u \u1ee9ng d\u1ee5ng\" - CQ 18_21\n### Gi\u1ea3ng vi\u00ean h\u01b0\u1edbng d\u1eabn: Th\u1ea7y Tr\u1ea7n Trung Ki\u00ean\n\n### Th\u00f4ng tin nh\u00f3m\n1. 18120555 - \u0110\u1eb7ng H\u1eefu Th\u1eafng - [Github](https:\/\/github.com\/huuthang2509)"}}