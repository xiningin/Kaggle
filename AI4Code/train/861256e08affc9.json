{"cell_type":{"da64d29e":"code","ad3782d3":"code","5602932b":"code","c3d365b6":"code","b8e1625c":"code","b0e00a00":"code","c0b9de26":"code","b716d4ec":"code","5cfb7477":"code","c8dd1805":"code","69dabf31":"code","5455fa73":"code","da748add":"code","2e118e3c":"code","b5b5aec8":"code","e3f26582":"code","efe6ec81":"code","4d5a6841":"code","854d7369":"code","d5c8d4cf":"code","60db9cda":"code","11b27588":"code","c01ae6d8":"code","f831dea0":"code","aab6b4aa":"code","1dbf2eb2":"code","dad2e4f4":"code","145ccee4":"code","8e819e51":"code","f89cacfe":"markdown"},"source":{"da64d29e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad3782d3":"import matplotlib.pyplot as plt\nimport seaborn as sns","5602932b":"true=pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ntrue.head()","c3d365b6":"fake=pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake.head()","b8e1625c":"len(true),len(fake)","b0e00a00":"true['category']=1\nfake['category']=0","c0b9de26":"df=pd.concat([true,fake])\nlen(df)","b716d4ec":"pd.crosstab(df['subject'],df['category'])","5cfb7477":"df.info()","c8dd1805":"import string\nfrom nltk.corpus import stopwords\n\nstop=set(stopwords.words('english'))\n","69dabf31":"#cleaning the data\nfrom bs4 import BeautifulSoup\n\n#removing html content\ndef strip_markup(text):\n    soup=BeautifulSoup(text,'html.parser')\n    return soup.get_text()\n\n","5455fa73":"#removing square brackets from the text\nimport re\n\ndef remove_punc(text):\n    text=re.sub(\"[^\\w\\s']\",' ',text)\n    return text","da748add":"def remove_stopwords(text):\n    text=' '.join([word.lower() for word in text.split() if word.lower() not in stop])\n    return text","2e118e3c":"def clean_text(text):\n    text=strip_markup(text)\n    text=remove_punc(text)\n    text=remove_stopwords(text)\n    return text","b5b5aec8":"df['text']=df['text'].apply(lambda x: clean_text(x))","e3f26582":"df.head()","efe6ec81":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df['text'],df['category'],test_size=0.3,random_state=123)","4d5a6841":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_features=10000\nmax_len=300\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntrain_sequence=tokenizer.texts_to_sequences(x_train)\ntest_sequence=tokenizer.texts_to_sequences(x_test)","854d7369":"train_sequence=pad_sequences(train_sequence,maxlen=max_len,padding='post')\ntest_sequence=pad_sequences(test_sequence,maxlen=max_len,padding='post')","d5c8d4cf":"\nEMBEDDING_FILE='..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.100d.txt'\n","60db9cda":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","11b27588":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()","c01ae6d8":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words,100))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n","f831dea0":"embedding_matrix.shape","aab6b4aa":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","1dbf2eb2":"#Defining Neural Network\nmodel = Sequential()\nmodel.add(Embedding(max_features, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=False))\nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n","dad2e4f4":"model.summary()","145ccee4":"from tensorflow.keras import optimizers\nmodel.compile(optimizer=optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","8e819e51":"history = model.fit(train_sequence, y_train, batch_size = 256 , validation_data = (test_sequence,y_test) , epochs =5 , callbacks = [learning_rate_reduction])","f89cacfe":"Topics in the subject column does not have texts divided between two categories,so we can exclude the category. "}}