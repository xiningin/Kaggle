{"cell_type":{"39d562f5":"code","356946cb":"code","551e4024":"code","d2a19c63":"code","a84e270f":"code","85fa439c":"code","79d648cc":"code","0546d034":"code","d7273255":"code","371b06c1":"code","d8c90d0d":"code","57752ff9":"code","53551546":"code","dcdd628e":"code","dc8f9d34":"code","da54ce5d":"code","48984edc":"code","f5d19cd0":"code","12d896f6":"code","04cea3c5":"code","41a74488":"code","fec67bed":"code","b2a24c4e":"code","77e163e9":"code","72b5c814":"code","cc7cb5da":"code","ae140fba":"markdown","23ab227f":"markdown","3fe9d8ed":"markdown","5419603c":"markdown","f37f6af2":"markdown","6421b805":"markdown","38e9d878":"markdown","30ce2305":"markdown"},"source":{"39d562f5":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n%matplotlib inline","356946cb":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","551e4024":"train_x = train.drop(['id', 'target'], axis = 1)\ntrain_y = train['target']\ntest_x = test.drop([\"id\"], axis = 1)","d2a19c63":"def baseline_model(train_x, train_y, run_num = 10, fold = 5):\n    train_result, test_result = [], []\n    for i in range(run_num):\n        # result list\n        train_fold, test_fold = [], []\n        # split dataset\n        skf = StratifiedKFold(n_splits = fold, shuffle = True)\n        fold_num = 1\n        for train_index, valid_index in skf.split(train_x, train_y):\n            # dataset\n            X_train, X_valid = train_x.iloc[train_index], train_x.iloc[valid_index]\n            y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n            # model\n            reg = LogisticRegression(solver = \"liblinear\", penalty = \"l2\")\n            reg.fit(X_train, y_train)\n            y_train_pred = reg.predict(X_train)\n            y_valid_pred = reg.predict(X_valid)\n            # result AUC\n            train_auc = roc_auc_score(y_train, y_train_pred)\n            test_auc = roc_auc_score(y_valid, y_valid_pred)\n            if i == 1:\n                print(\"TRAIN Fold {0}, AUC score: {1}\".format(fold_num, round(train_auc, 4)))\n                print(\"TEST Fold {0}, AUC score: {1}\".format(fold_num, round(test_auc, 4)))\n            fold_num += 1\n            train_fold.append(train_auc)\n            test_fold.append(test_auc)\n        train_result.append(train_fold)\n        test_result.append(test_fold)\n    return train_result, test_result","a84e270f":"train_result, test_result = baseline_model(train_x = train_x, train_y = train_y, run_num = 10, fold = 5)","85fa439c":"def model_result(train_result, test_result):\n    base_test_re = pd.DataFrame(test_result).T\n    base_test_re.index = ['fold {0}'.format(i) for i in range(5)]\n    base_test_re.columns = ['run {0}'.format(i) for i in range(10)]\n    base_train_re = pd.DataFrame(train_result).T\n    base_train_re.index = ['fold {0}'.format(i) for i in range(5)]\n    base_train_re.columns = ['run {0}'.format(i) for i in range(10)]\n    return base_train_re, base_test_re\nbase_train_re, base_test_re = model_result(train_result, test_result)","79d648cc":"base_train_re","0546d034":"base_test_re.round(3)","d7273255":"def binning(data, feature, n_bins):\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n    est.fit(data[feature].values)\n    Xt = est.transform(data[feature].values)\n    data[feature] = pd.DataFrame(Xt)\n    return data","371b06c1":"train_x_bin = binning(train_x, train_x.columns, n_bins = 15)\ntest_x_bin = binning(test_x, test_x.columns, n_bins = 15)","d8c90d0d":"train_result_bin, test_result_bin = baseline_model(train_x_bin, train_y, run_num = 10, fold = 5)","57752ff9":"base_train_re_bin, base_test_re_bin = model_result(train_result, test_result)","53551546":"base_train_re_bin","dcdd628e":"base_test_re_bin.round(3)","dc8f9d34":"sig_features = []\nfor each_feature in train.columns[2:]:\n    X = train[each_feature]\n    X = sm.add_constant(X)\n    y = train.iloc[:,1]\n    model = sm.OLS(y, X)\n    result = model.fit()\n    pvalue = result.pvalues[1]\n    # using 90% significance level\n    if pvalue <= 0.1:\n        print(\"Feature {0}, p value is {1}\".format(each_feature, round(pvalue, 3)))\n        sig_features.append(each_feature)","da54ce5d":"train_x = train.drop(['id', 'target'], axis = 1)\ntrain_y = train['target']","48984edc":"train_select_x = train_x[sig_features]\ntrain_select_bin_x = train_x_bin[sig_features]","f5d19cd0":"train_result_select, test_result_select = baseline_model(train_select_x, train_y, run_num = 10, fold = 5)\nbase_train_re_select, base_test_re_select = model_result(train_result_select, test_result_select)","12d896f6":"base_train_re_select","04cea3c5":"base_test_re_select","41a74488":"train_result_bin_select, test_result_bin_elect = baseline_model(train_select_bin_x, train_y, run_num = 10, fold = 5)\nbase_train_re_bin_select, base_test_re_bin_select = model_result(train_result_bin_select, test_result_bin_elect)","fec67bed":"base_train_re_bin_select","b2a24c4e":"base_test_re_bin_select","77e163e9":"train_select_bin_x = train_x_bin[sig_features]\ntest_select_bin_x = test_x_bin[sig_features]","72b5c814":"# split dataset\nskf = StratifiedKFold(n_splits = 5, shuffle = True)\nfold_num = 1\ny_test = np.zeros(len(test_select_bin_x))\nfor train_index, valid_index in skf.split(train_select_bin_x, train_y):\n    # dataset\n    X_train, X_valid = train_select_bin_x.iloc[train_index], train_select_bin_x.iloc[valid_index]\n    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n    # model\n    reg = LogisticRegression(solver = \"liblinear\", penalty = \"l2\")\n    reg.fit(X_train, y_train)\n    y_train_pred = reg.predict(X_train)\n    y_valid_pred = reg.predict(X_valid)\n    # result AUC\n    train_auc = roc_auc_score(y_train, y_train_pred)\n    test_auc = roc_auc_score(y_valid, y_valid_pred)\n    fold_num += 1\n    # predict test set\n    y_test_fold = reg.predict_proba(test_select_bin_x)[:, 1]\n    y_test += y_test_fold\ny_test = y_test\/5","cc7cb5da":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['target'] = y_test\nsub.to_csv(\"submission_logit.csv\", index = False)","ae140fba":"### 2. Baseline model","23ab227f":"### 3. feature bining\n```\nif x < quantile(0.1):\n     x = 0\nif quantile(0.1) < x < quantile(0.9)\n     x = 1-9\nif x > quantile(0.9):\n     x = 10\n```","3fe9d8ed":"### 4. submission","5419603c":"### 1. import packages and data","f37f6af2":"**Conclusion:** I got better results but still overfitting...","6421b805":"**Conclusion:** Overfitting..., I also tried to change logistic regression parameters, but it doesn't improve the score and cannot handle overfitting problem.","38e9d878":"This kernel shows some experiments I have done.\n\n1. basedline model logistic regression (result: overfitting)\n2. feature binning (result: overfitting, but slightly improve the result)\n3. feature selection (result: better, but still overfitting)","30ce2305":"### 3. feature selection by statistics test"}}