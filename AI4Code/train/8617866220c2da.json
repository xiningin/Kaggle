{"cell_type":{"76c03bc1":"code","55715090":"code","fa55c524":"code","09508ca9":"code","145ffa3f":"code","f4f088f5":"code","272ca184":"code","6e9f09e0":"code","b8fffd11":"code","4b860c15":"code","1950731b":"code","9cf01680":"code","ad3345be":"code","65194707":"code","a3102929":"code","d902d7fa":"code","29f79dec":"code","f4292f4f":"code","42983192":"code","01a1b7a3":"code","5fc2168b":"code","f15788c8":"code","fb31888f":"code","3d447651":"markdown","116b7a2d":"markdown","0f445449":"markdown","5f8d1083":"markdown","35d283fe":"markdown","2e88599b":"markdown","6f54cb2e":"markdown","ec7b3be9":"markdown","7b51c4dd":"markdown","f8a8a1f7":"markdown","7a4f1b9f":"markdown","d9c48c15":"markdown","2d8e12ef":"markdown"},"source":{"76c03bc1":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.structured import *\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.core.debugger import set_trace\nfrom sklearn.model_selection import KFold\n\nimport os\nprint(os.listdir(\"..\/input\"))\nPATH = \"..\/input\/\"","55715090":"df_train = pd.read_csv(f'{PATH}train.csv', parse_dates=['Open Date'])\ndf_test = pd.read_csv(f'{PATH}test.csv', parse_dates=['Open Date'])\n\ndf_joined = pd.concat([df_train.drop('revenue', axis=1), df_test], axis=0)","fa55c524":"def score(model, X_train, y_train, X_valid = [], y_valid = []):\n    #set_trace()\n    rms = sqrt(mean_squared_error(np.square(np.exp(y_train)), np.square(np.exp(model.predict(X_train)))))\n    score = [rms, model.score(X_train, y_train)]\n    \n    if len(X_valid) != 0 and len(y_valid) != 0:\n        score.append(sqrt(mean_squared_error(np.square(np.exp(y_valid)), np.square(np.exp(model.predict(X_valid))))))\n        \n    if model.oob_score:\n        score.append(model.oob_score_)\n    \n    return score\n\n\nn_train = df_train.shape[0]\n\ndef prcs(df, fe=[]):\n    add_datepart(df, 'Open Date')\n    # Eliminamos columnas que no aportan nada\n    drop_cols = ['Open Year', 'Open Month', 'Open Week', 'Open Day', 'Open Dayofweek',\n       'Open Dayofyear', 'Open Is_month_end', 'Open Is_month_start',\n       'Open Is_quarter_end', 'Open Is_quarter_start', 'Open Is_year_end',\n       'Open Is_year_start']\n    \n    df = df.drop(drop_cols, axis=1)\n    \n    #########################################\n    if 'city' in fe:\n        df = df.drop('City', axis=1)\n        \n    #########################################\n\n    if 'MB' in fe:\n        # No hay apenas tipo 'MB'\n        df['Type'] = df['Type'].replace('MB', 'DT')\n    \n    #########################################\n    if 'city_group' in fe:\n        df = df.drop('City Group', axis=1)\n    \n    #########################################\n    # La columna Id no aporta nada\n    if 'id' in fe:\n        df = df.drop('Id', axis=1)\n    #########################################\n    \n    if 'dummies' in fe:\n        #Get dummies\n        p_cols = [ f'P{n}' for n in range(1,38)]\n            \n        df = pd.get_dummies(df, columns=p_cols)\n        if 'city_group' not in fe:\n            df = pd.get_dummies(df, columns=['City Group'], drop_first=True)\n        df = pd.get_dummies(df, columns=['Type'])\n    \n    #########################################\n    # Quitamos el outlier (16)\n    if 'outlier' in fe:\n        df = df.drop(index=16, axis=0)\n    \n    #########################################\n    #Train cats\n    train_cats(df)\n\n    X, _, _ = proc_df(df, None)\n\n\n    \n    if 'scale_open' in fe:\n        X['Open Elapsed'] = (X['Open Elapsed']\/1000).apply(np.log)\n    \n    X_train = X[:n_train]\n    X_test = X[n_train:]\n    \n    return X_train, X_test","09508ca9":"X_train, X_test = prcs(df_joined.copy())\ny_train = df_train['revenue'].copy().apply(np.log)","145ffa3f":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","f4f088f5":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission0.csv', index=True, index_label='Id')\ndf_preds.head()","272ca184":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","6e9f09e0":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission1', index=True, index_label='Id')\ndf_preds.head()","b8fffd11":"X_train, X_test = prcs(df_joined.copy(), fe=['id'])\n\n# Doble transformaci\u00f3n para que la distribuci\u00f3n sea Normal\ny_train = df_train['revenue'].copy().apply(np.sqrt).apply(np.log)","4b860c15":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","1950731b":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission2.csv', index=True, index_label='Id')\ndf_preds.head()","9cf01680":"X_train, X_test = prcs(df_joined.copy(), fe=['id', 'dummies'])\n\n# Doble transformaci\u00f3n para que la distribuci\u00f3n sea Normal\ny_train = df_train['revenue'].copy().apply(np.sqrt).apply(np.log)","ad3345be":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","65194707":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission3.csv', index=True, index_label='Id')\ndf_preds.head()","a3102929":"X_train, X_test = prcs(df_joined.copy(), fe=['id', 'dummies', 'city'])\n\n# Doble transformaci\u00f3n para que la distribuci\u00f3n sea Normal\ny_train = df_train['revenue'].copy().apply(np.sqrt).apply(np.log)","d902d7fa":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","29f79dec":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission4.csv', index=True, index_label='Id')\ndf_preds.head()","f4292f4f":"X_train, X_test = prcs(df_joined.copy(), fe=['id', 'dummies', 'city', 'city_group'])\n\n# Doble transformaci\u00f3n para que la distribuci\u00f3n sea Normal\ny_train = df_train['revenue'].copy().apply(np.sqrt).apply(np.log)","42983192":"m = RandomForestRegressor(n_jobs=-1, n_estimators=400, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","01a1b7a3":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission5.csv', index=True, index_label='Id')\ndf_preds.head()","5fc2168b":"X_train, X_test = prcs(df_joined.copy(), fe=['id', 'dummies', 'city', 'city_group'])\n\n# Doble transformaci\u00f3n para que la distribuci\u00f3n sea Normal\ny_train = df_train['revenue'].copy().apply(np.sqrt).apply(np.log)","f15788c8":"m = RandomForestRegressor(n_jobs=-1, n_estimators=4000, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nscore(m,X_train, y_train)","fb31888f":"df_preds = pd.DataFrame(columns=['Prediction'],index=X_test.index, data=np.square(np.exp(m.predict(X_test))))\ndf_preds.to_csv('submission6.csv', index=True, index_label='Id')\ndf_preds.head()","3d447651":"## A\u00f1adiendo dummies en las variables categoricas ","116b7a2d":"# n_estitamors = 400 ","0f445449":"## Quitando columna 'City' ","5f8d1083":"### RMSE 1.82 M (Rank 600)","35d283fe":"### RMSE 1.784 M (Rank 75)","2e88599b":"# Entrenando m\u00e1s arboles","6f54cb2e":"### RMSE 1.70 M (Rank 1)","ec7b3be9":"# Quitando columna \"Id\"","7b51c4dd":"### RMSE 1.83 M (Rank 600)","f8a8a1f7":"### RMSE 1.71 M (Rank 1)","7a4f1b9f":"# Feature engineering","d9c48c15":"## Quitando columna 'City Group' ","2d8e12ef":"### RMSE 1.92 M (Rank 1600)"}}