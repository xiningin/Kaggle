{"cell_type":{"78f7b908":"code","9a178a42":"code","2f368665":"code","432794e2":"code","16c9445b":"code","01d2b194":"code","d9a38b9a":"code","488aeb76":"code","1a90865d":"code","dde7dc19":"code","9eea8098":"code","bfb94d49":"code","38291bc2":"code","a59733f3":"code","363a24ea":"code","58f9e93f":"code","954a735c":"code","a26c15c5":"code","a3a8e429":"code","16d4c258":"code","bca29264":"code","fae35fad":"markdown","e95e90be":"markdown","8817396a":"markdown","21aefe51":"markdown","3d3f15ff":"markdown","c490d808":"markdown","68f55b87":"markdown","8fc13761":"markdown","e1299364":"markdown","8d5f106f":"markdown","3f3e0d75":"markdown","f5a7f9cb":"markdown","32c483a7":"markdown","10ae662d":"markdown","6f46af59":"markdown","02308479":"markdown","b027be13":"markdown","50fae46f":"markdown","2a247205":"markdown","45111972":"markdown","1c249adb":"markdown","e884308b":"markdown","9d6df5de":"markdown","49682ca5":"markdown"},"source":{"78f7b908":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","9a178a42":"data = pd.read_csv(\"..\/input\/housing.csv\")\ndata.head()","2f368665":"data.shape","432794e2":"data.info()","16c9445b":"# Pearson correlation\nplt.subplots(figsize=(15, 9))\ncor = data.corr()\nsns.heatmap(cor, annot=True, linewidths=.5)\nplt.show()","01d2b194":"# taking two variables\ndata = data.drop([\"housing_median_age\",\"households\",\"total_bedrooms\",\"longitude\",\"latitude\",\"total_rooms\",\"population\",\"ocean_proximity\"], axis=1)\ndata.head()","d9a38b9a":"X = data.drop(\"median_house_value\", axis=1)\ny = data[\"median_house_value\"]","488aeb76":"plt.scatter(X, y, alpha=0.5)\nplt.title('Scatter plot')\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\nplt.show()","1a90865d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","dde7dc19":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Model initialization\nregression_model = LinearRegression(normalize = True)\n\n# Fit the data(train the model)\nregression_model.fit(X_train, y_train)","9eea8098":"# Predict\ny_predicted = regression_model.predict(X_test)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)","bfb94d49":"# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)","38291bc2":"# data points\nplt.scatter(X_train, y_train, s=10)\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n\n# predicted values\nplt.plot(X_test, y_predicted, color='r')\nplt.show()","a59733f3":"residual = y_test - y_predicted\nsns.residplot(residual,y_predicted, lowess=True,scatter_kws={'alpha': 0.5},line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()","363a24ea":"tf = np.sqrt(X_train) \ntf1 = np.sqrt(X_test)\n\nplt.scatter(tf, y_train)\nplt.show()","58f9e93f":"regression_model.fit(tf, y_train)\n# Predict\ny_predicted = regression_model.predict(tf1)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)","954a735c":"res = y_test - y_predicted\nsns.residplot(res,y_predicted, lowess=True,scatter_kws={'alpha': 0.5},line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()","a26c15c5":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression(normalize = True)\npol_reg.fit(X_poly, y_train)","a3a8e429":"def viz_polymonial():\n    plt.scatter(X_train, y_train, color=\"red\")\n    plt.plot(X_train, pol_reg.predict(poly_reg.fit_transform(X_train)))\n    plt.xlabel('median_income')\n    plt.ylabel('median_house_value')\n    plt.show()\n    return\nviz_polymonial()","16d4c258":"# Predict\nX_p = poly_reg.fit_transform(X_test)\ny_predicted = pol_reg.predict(X_p)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)","bca29264":"res = y_test - y_predicted\nsns.residplot(res,y_predicted, lowess=True,scatter_kws={'alpha': 0.5},line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()","fae35fad":"If we have to select a single variable for the regression analysis then\nhigher possibility is to pick the most correlated feature with the target variable(**median_house_value**).\n* In our case it is the **median_income** with correlation coefficent of **0.69**","e95e90be":"# Model 2:","8817396a":"## Fitting a model","21aefe51":"**The residuals exhibit a clear non straight line, which provides a strong indication of non-linearity in the data**\n\nThis makes us to do somethhing more to find better fit of the model.\n> These are the possible approaches:\n\n     First we will try some transformations to best fit the data.\n     Next could be try ploynomial regression of some higher degree to increase the R2 score.","3d3f15ff":"# Model 1:","c490d808":"## Fitting polynomial Regression model","68f55b87":"## Load the data","8fc13761":"The above info function provide the information about the dataset .\nFor example:\n* Missing values(no missing values in our dataset)\n* datatype(9 of them are floats and 1 is categorical)","e1299364":"There are 20640 observations and 9 features + 1 target variable(median_house_value).","8d5f106f":">  **Interpretation**:\n\nThis simple linear regression with single variable (y = mx+b) has \n* Slope of the line(m) : [42032.17769894]\n* Intercept (b) : 44320.63\n* R2 score:  0.4466 (For R2 score more is better in the range [0,1])\n* Root mean squared error:  84941.0515 (Lower is better)\n","3f3e0d75":"# Model 3:","f5a7f9cb":"## Applying transformation","32c483a7":"## Split the data","10ae662d":"## Residual plot for transformed","6f46af59":"## Residual plot from linear regression","02308479":">  **Interpretation**:\n\nThis transformed linear regression with single variable (y = mx+b) has \n* Slope of the line(m) : 175550.81\n* Intercept (b) : -129097.46\n* R2 score:  0.4498 (For R2 score more is better in the range [0,1])\n\nFound R2 score is the best so far. This means that we will keep this ploynomial model with degree 2 as our final and best model(but there is one other thing to consider i.e. simple is better than complex)\n* Root mean squared error:  84699.9 (Lower is better)","b027be13":"**The plot of simple linear regression :**","50fae46f":"## Linear regression model","2a247205":"Residual plot for the transformed linear regression is more zigzag than the simple linear regression.\nThis residual plot suggest that transformation makes the relationship more non- linear in nature.","45111972":"Using this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house.\nThere is somewhat positive linear relationship between them.","1c249adb":"## Import packages","e884308b":"## Comparing the model","9d6df5de":">  **Interpretation**:\n\nThis transformed linear regression with single variable (y = mx+b) has \n* Slope of the line(m) : 175550.81\n* Intercept (b) : -129097.46\n* R2 score:  0.4385 (For R2 score more is better in the range [0,1])\n\nFound R2 score is worse than the simple linear regression. This suggest that we do not need  transformation in this case, we have try polynomial regression to improve the predication.\n* Root mean squared error:  85566.36 (Lower is better)\n","49682ca5":"* Model 1 has R2 score:  0.4466\n* model 2 has R2 score:  0.4385\n* model 3 has R2 score:  0.44982\n\n**After analyzing the R2 score , My final model will be Model 1 as it is simple and has not worse R2 score as compared to the model 3.**"}}