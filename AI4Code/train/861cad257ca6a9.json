{"cell_type":{"8a1a6207":"code","1f170469":"code","6e97836b":"code","18f7c5d8":"code","25efb681":"code","f8b54afe":"code","7fc69bca":"code","2b5c4ef5":"code","d1be026e":"code","c93125a2":"code","3c128f60":"code","54048123":"markdown","89046987":"markdown","8a09eec3":"markdown","e7f8fd9f":"markdown","363a9db5":"markdown","bd120215":"markdown","55eac45a":"markdown","a588e318":"markdown","65081f39":"markdown","5bdc1fdb":"markdown","ee606ebf":"markdown"},"source":{"8a1a6207":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import r2_score","1f170469":"def AlgoFun (X_train, Y_train, X_test, Y_test, Algo, Result):\n    \n    if (Algo == 'LOG'):\n        log_reg=LogisticRegression(C=1000,max_iter=50000)\n        log_reg.fit(X_train, Y_train)\n        Y_pred = log_reg.predict(X_test)\n        \n    elif (Algo == 'LIN'):\n        model = linear_model.LinearRegression()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'KNN'):\n        KNN=KNeighborsClassifier(n_neighbors=20)\n        KNN.fit(X_train, Y_train)\n        Y_pred=KNN.predict(X_test)\n\n    elif (Algo == 'RFC'):\n        Clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        Clf.fit(X_train, Y_train)\n        Y_pred=Clf.predict(X_test) \n        compare1 = pd.DataFrame()\n        compare1[0] = Clf.feature_importances_\n        compare1[1] = X_test.columns\n        print('Feature importance: ')\n        print(compare1.sort_values(by=0,ascending= False))\n        \n    elif (Algo == 'NN'):\n        NN = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\n        NN.fit(X_train, Y_train)\n        Y_pred = NN.predict(X_test)\n        \n    elif (Algo == 'DTR'):\n        DTR = tree.DecisionTreeClassifier()\n        DTR.fit(X_train, Y_train)\n        Y_pred=DTR.predict(X_test)\n        \n    elif (Algo == 'GSCV'):\n        estimator = RandomForestRegressor(random_state = 42,criterion='mse')\n        para_grids = {\n                    \"n_estimators\" : [10,50,100],\n                    \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n                    'max_depth' : [4,5,6,7,8,9,15],\n                    \"bootstrap\"    : [True, False]\n                }\n        Grid = GridSearchCV(estimator, para_grids,cv= 5)\n        Grid.fit(X_train, Y_train)\n        best_param = Grid.best_estimator_\n        print(best_param)\n        Y_pred = best_param.predict(X_test)\n\n        \n    elif (Algo == 'RFR'):\n        # Using the best model from Grid Serach CV\n        model = RandomForestRegressor(max_depth=15, random_state=42) \n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n      \n    elif (Algo == 'GBR'):   \n        GBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n        GBR.fit(X_train, Y_train)\n        Y_pred = GBR.predict(X_test)\n        \n    elif (Algo == 'GNB'):   \n        GNB = GaussianNB()\n        GNB.fit(X_train, Y_train)\n        Y_pred = GNB.predict(X_test)\n      \n    elif (Algo == 'ADA'):\n        model = AdaBoostRegressor(random_state=0, n_estimators=100)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'XGB'):\n        model = XGBRegressor()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'LGB'):\n        model = LGBMClassifier(objective='multiclass', random_state=5)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)        \n   \n    elif (Algo == 'CAT'):\n        Cat = CatBoostClassifier(silent = True)\n        details = Cat.fit(X_train, Y_train)\n        Y_pred = Cat.predict(X_test)\n        \n    elif (Algo == 'SVM'):\n        model = svm.SVC(kernel='linear') # Linear Kernel\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)       \n            \n    else:\n        print(\"Wrong Algo\")\n        \n    Y_test=pd.DataFrame(Y_test).iloc[:, [0]].to_numpy()\n    Y_pred=pd.DataFrame(Y_pred).iloc[:, [0]].to_numpy()\n    \n    ActVPred = pd.DataFrame({'Actual': Y_test[:,0], 'Predicted': Y_pred[:,0]})\n    print(ActVPred)\n\n    #Checking the accuracy\n    print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n    print('Model R2 scores: ', r2_score(Y_test, Y_pred))\n\n    if (Result == 'Binary'):\n        Count_row = []\n        Visual_rep = []\n\n        index = 0\n        for i, row in ActVPred.iterrows():\n            if (row['Predicted'] < 0.5):\n                Visual_rep.append(0)\n            else:\n                Visual_rep.append(1)\n\n            if (row['Actual'] < 1):\n                if (row['Predicted'] < 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            else:\n                if (row['Predicted'] >= 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            index = index + 1\n\n        print('--------------------------------------------------------------------------')\n        print(Algo)\n        print('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n   \n        ax = plt.subplots(figsize=(10, 10))\n        ax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\n        ax.set_title('Prediction vs Original Data (Confusion Matrix)',fontsize=18)\n        ax.set_xticklabels(['Actual 0','Actual 1'],fontsize=18)\n        ax.set_yticklabels(['Predicted 0','Predicted 1'],fontsize=18)\n\n        filename = 'ConfusionMatrix.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    elif (Result == 'Analog'):\n        fig, ax = plt.subplots()\n        minimum = min (Y_test.min(), Y_pred.min())\n        maximum = max (Y_test.max(), Y_pred.max())\n        ax.scatter(Y_test, Y_pred)\n        ax.plot([minimum, maximum], [minimum, maximum], 'k--', lw=4)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        filename = 'Result Plot.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    else:\n        print(\"Wrong parameter\")\n\n    return (ActVPred)\n\ndef Data_clean_func (Data, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    X_train, X_test, Y_train, Y_test  = train_test_split(Result[X_column], Result[Y_column], test_size=0.3,random_state=42)\n    \n    return X_train, X_test, Y_train, Y_test\n\ndef Data_clean_func1 (Data, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    \n    X_data = Result[X_column]\n    Y_data = Result[Y_column]\n    \n    return X_data, Y_data","6e97836b":"Data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')","18f7c5d8":"Data.head()","25efb681":"Drop_col = ['enrollee_id']\nDate_col = []\nY_column = ['target']\n\nX_train, X_test, Y_train, Y_test = Data_clean_func(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')","f8b54afe":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'RFR', 'Binary')","7fc69bca":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'RFC', 'Binary')","2b5c4ef5":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'XGB', 'Binary')","d1be026e":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'LIN', 'Binary')","c93125a2":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'CAT', 'Binary')","3c128f60":"Drop_col = ['enrollee_id']\nDate_col = []\nY_column = ['target']\n\nData = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\nX_train, Y_train = Data_clean_func1(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')\n\nY_column = []\nData = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nX_test = Data_clean_func1(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')\n  \nCat = CatBoostClassifier(silent = True)\ndetails = Cat.fit(X_train, Y_train)\nY_pred = Cat.predict(X_test)","54048123":"<h1><center>HR ANALYTICS<\/center><\/h1>","89046987":"<h1><center>Random Forest Regression <\/center><\/h1>","8a09eec3":"<h1><center>Training Data Load<\/center><\/h1>","e7f8fd9f":"<h1><center>XG Boost<\/center><\/h1>","363a9db5":"<h1><center>Random Forest Classifier<\/center><\/h1>","bd120215":"<h1><center>Clean Data<\/center><\/h1>","55eac45a":"<h1><center>Mother of Algorithms<\/center><\/h1>","a588e318":"<h1><center>Data Check<\/center><\/h1>","65081f39":"<h1><center>CAT Boost<\/center><\/h1>","5bdc1fdb":"<center>\n    \n![](https:\/\/kritikalsolutions.com\/wp-content\/uploads\/2019\/10\/ml-making-iot-banner-.jpg)\n\n\n<\/center>","ee606ebf":"<h1><center>Linear Regression<\/center><\/h1>"}}