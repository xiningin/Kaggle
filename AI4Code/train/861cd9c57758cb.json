{"cell_type":{"b99f0c82":"code","a9a8561b":"code","97c8d9ad":"code","77d54a4e":"code","36528263":"code","08a7816e":"code","2db70a8f":"code","744f148b":"code","261f1269":"code","e26cb3a2":"code","f8aa24aa":"code","cd6ff7f4":"code","b70500ac":"code","94d249c3":"code","29b5b123":"code","49017ef6":"code","ae704a15":"code","f3c12468":"code","32bae9f4":"code","3e49c426":"code","cb89fb58":"code","c94c7aa7":"code","2466a7a6":"code","305bbffc":"code","ab3918dc":"code","b52663c9":"code","8d900418":"code","ab52b99a":"code","1cd06418":"code","9fa51a8b":"code","668aef46":"code","1f592fc9":"code","cd935920":"code","1e80ce32":"code","c7df3ed1":"code","ec3aebf2":"code","407b45ab":"code","be99c167":"code","df017eed":"code","1b0e9e1b":"code","0ab1f61a":"code","afc6bbef":"code","007f8924":"markdown","6df6066b":"markdown","e546b211":"markdown","3bab8f71":"markdown","7b002a28":"markdown","4720a9af":"markdown","187041c7":"markdown","7a6db87f":"markdown","3c5a4a87":"markdown","9adaf6de":"markdown","8f5d9fe5":"markdown","423225f9":"markdown","55b28e49":"markdown","ce072ddb":"markdown","101d953c":"markdown","622b69a0":"markdown","7ee1f022":"markdown","2762914e":"markdown","5efac615":"markdown","93c87172":"markdown","bbac34d1":"markdown","f628e2fa":"markdown","1d9a9307":"markdown","20f3d039":"markdown"},"source":{"b99f0c82":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pycountry\nimport warnings\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.dates as mdates\nimport re\nimport string\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.utils import shuffle\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.regularizers import l2\n\nnp.random_seed = 0\n\n\nwarnings.filterwarnings('ignore')","a9a8561b":"df = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","97c8d9ad":"df.head()","77d54a4e":"print(\"Total number of records in data: \",len(df))","36528263":"df.info()","08a7816e":"df['user_location'].fillna('unknown', inplace=True)","2db70a8f":"df['user_location'].value_counts()","744f148b":"c = list(pycountry.countries)","261f1269":"def correct_location(x):\n    for i in c:\n        if str(i.name).lower() in x or str(i.alpha_2).lower() in x.split() or str(i.alpha_3).lower() in x.split():\n            return str(i.name)\n    return x","e26cb3a2":"df['user_location'] = df['user_location'].apply(lambda x: correct_location(x.lower()))","f8aa24aa":"df['user_location'].value_counts()","cd6ff7f4":"def plot_bar(x,y,title,x_label,y_label):\n    fig = go.Figure(data=[go.Bar(\n                x=x,\n                y=y,\n                text=y,\n                textposition='auto',\n            )])\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=x_label,\n        yaxis_title=y_label\n    )\n\n    fig.show()","b70500ac":"plot_bar(df['user_location'].value_counts().index[0:20],\n         df['user_location'].value_counts().values[0:20],\n         \"Top 20 Locations by the number of tweets\",\n         \"Location\",\n         \"Tweet Count\")","94d249c3":"def plot_time_series(dates, counts, title):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Configure x-ticks\n    ax.set_xticks(dates) # Tickmark + label at every plotted point\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d\/%m\/%Y'))\n   \n    ax.plot_date(dates, counts, ls='-', marker='o')\n    ax.set_title(title)\n    ax.set_ylabel('Tweet Count)')\n    ax.grid(True)\n\n    # Format the x-axis for dates (label formatting, rotation)\n    fig.autofmt_xdate(rotation=45)\n    fig.tight_layout()\n\n    fig.show()","29b5b123":"df['just_date'] = pd.to_datetime(df['date']).dt.normalize()\n\ntop = list(df['user_location'].value_counts().index[1:5])\ndf_top = df[df['user_location'].isin(top)]","49017ef6":"grp_df = df_top.groupby('user_location')\nfor i, grp in grp_df:\n    dates = grp['just_date'].value_counts().sort_index().index\n    counts = grp['just_date'].value_counts().sort_index().values\n        \n    plot_time_series(dates,counts,'Tweet count trend for '+i)","ae704a15":"plot_bar(df['user_name'].value_counts().index[0:20],\n         df['user_name'].value_counts().values[0:20],\n         \"Top 20 Users by the number of tweets\",\n         \"User\",\n         \"Tweet Count\")","f3c12468":"plot_bar(df['source'].value_counts().index[0:20],\n         df['source'].value_counts().values[0:20],\n         \"Top 20 Tweeter Sources by the number of tweets\",\n         \"Source\",\n         \"Tweet Count\")","32bae9f4":"stopwords_ = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        collocations=False,\n        background_color='white',\n        stopwords=stopwords_,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","3e49c426":"show_wordcloud(df['text'], \"WordCloud for tweets\")","cb89fb58":"hashtags = list(df['hashtags'].dropna())\n\nhashtags = [x.replace(\"'\", '') for x in hashtags]\nhashtags = [(re.sub(r'[^\\w\\s]','',x)).lower() for x in hashtags]\n","c94c7aa7":"show_wordcloud(' '.join(hashtags),\"WordCloud for hashtags\")","2466a7a6":"data = pd.read_csv('..\/input\/twitterdata\/finalSentimentdata2.csv', encoding='\"ISO-8859-1\"')\ndata.drop(columns=['Unnamed: 0'], inplace=True)","305bbffc":"data.head()","ab3918dc":"data['sentiment'].value_counts()","b52663c9":"def clean_text(text):\n    \n    #remove urls\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n    \n    #remove html\n    html_pattern = re.compile(r'<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    #remove emojis\n    emoji_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'',text)\n    \n    #remove punctuations\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    text = text.translate(table)\n\n    return text\n#     #remove stopwords\n#     stop = set(stopwords.words('english'))\n#     text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n#     return ' '.join(text)","8d900418":"data['sentiment'] = data['sentiment'].apply(lambda x: int(x == 'joy'))\ndata['text'] = data['text'].apply(lambda x: clean_text(x))\ndf['text'] = df['text'].apply(lambda x: clean_text(x))","ab52b99a":"def word_counter(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count  \n\ncounter = word_counter(data['text'].append(df['text']))\nnum_of_words = len(counter)\nmax_len = 20\n\nt = Tokenizer(num_words = num_of_words)\nt.fit_on_texts(data['text'].append(df['text']))","1cd06418":"train_x, test_x, train_y, test_y = train_test_split(data['text'], data['sentiment'], test_size=0.1, random_state=30)","9fa51a8b":"train_tweets = t.texts_to_sequences(train_x)\ntrain_tweets_padded = pad_sequences(train_tweets, maxlen=max_len, padding='post', truncating='post')\n\ntest_tweets = t.texts_to_sequences(test_x)\ntest_tweets_padded = pad_sequences(test_tweets, maxlen=max_len, padding='post', truncating='post')","668aef46":"model = Sequential()\nmodel.add(Embedding(num_of_words, 30, input_length = max_len))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])","1f592fc9":"model.summary()","cd935920":"data['sentiment'].value_counts()","1e80ce32":"weights = {0:1, 1:2}\nhistory = model.fit(train_tweets_padded, train_y, epochs=5, validation_data = (test_tweets_padded,test_y),class_weight=weights,batch_size=40)","c7df3ed1":"pred_y = model.predict_classes(test_tweets_padded)\nprint(\"F1 Score : \", f1_score(test_y,pred_y))","ec3aebf2":"original_tweet = t.texts_to_sequences([df['text'][8]])\ntweet_padded = pad_sequences(original_tweet, maxlen=max_len, padding='post', truncating='post')\nres = model.predict_classes(tweet_padded)","407b45ab":"print(\"Tweet :\",df['text'][8])\nprint(\"Sentiment :\",res[0][0])","be99c167":"original_tweet = t.texts_to_sequences([df['text'][644]])\ntweet_padded = pad_sequences(original_tweet, maxlen=max_len, padding='post', truncating='post')\nres = model.predict_classes(tweet_padded)","df017eed":"print(\"Tweet :\",df['text'][644])\nprint(\"Sentiment :\",res[0][0])","1b0e9e1b":"original_tweets = t.texts_to_sequences(df['text'])\ntweets_padded = pad_sequences(original_tweets, maxlen=max_len, padding='post', truncating='post')\nres = model.predict_classes(tweets_padded)","0ab1f61a":"res = [i[0] for i in res]","afc6bbef":"plot_bar(list(Counter(res).keys()),\n         list(Counter(res).values()),\n         \"Count of Positive(1) and Negative(0) sentiments\",\n         \"Sentiment\",\n         \"Tweet Count\")","007f8924":"### The model:\n\nlet's define our sequential model with following layers:\n\n* Embedding Layer\n* LSTM layer\n* Dropout layer\n* Dense layer","6df6066b":"There are hashtags used in tweets, let's find out popular covid hashtags and most buzzing words in tweets using wordcloud.","e546b211":"Now we see the results got better. Observe the count for India, previously it was 2690 now it has become 11535.\n\nLet's plot the results.","3bab8f71":"Let's plot top 20 Tweeter sources by the number of tweets.","7b002a28":"Accuracy is not a good measure for imbalanced class dataset, let's find f1 score to see how our model is performing on test data.","4720a9af":"As we have merged classes anger, fear and san into one, there is a class imbalance problem here, count of class 0 is higher than class 1. Let's handle it by assigning class_weights in the model.","187041c7":"Load the data.","7a6db87f":"Let's plot top 20 users by the number of tweets.","3c5a4a87":"## EDA","9adaf6de":"We can observe from result that location information needs some processing. Location \"New Delhi, India\" can be written as India. Let's use pycountry to get country names and their aplha-2 and alpha_3 codes and search for them in user_location,if found replace long addresses with their country names.","8f5d9fe5":"Some processing of hashtags to remove commas and square brackets.","423225f9":"# Sentimental Analysis","55b28e49":"Let's start with loading the data.","ce072ddb":"We cannot directly use textual data as input to our sequence model. Let's map each word in the tweet to an integer based on their count using Tokenizer from keras.\n\nWe need to have a fixed sized input for the model, here I am using maximum length as 20. Try with different values to find the best one. Usually a smaller value is recommended since it makes the input less sparse when padded with zeros.","101d953c":"Tokenize and pad the train and test tweets.","622b69a0":"Split the data into train and test sets.","7ee1f022":"# COVID-19 Tweets EDA and Sentimental Analysis\n\nIn this notebook I have performed the following:\n\n### EDA:\n\n* Processing of user locations to get only country names.\n* Trend of tweet count over the period of time for top 4 countries.\n* Plotting of top countries, users, source of tweet based on the number of tweets.\n* Word cloud presentation of hashtags and tweets.\n\n\n### Sentimental Analysis:\n\nSince this dataset does not have information regarding the sentiment of tweets, I have used the [Covid 19 Indian Sentiments on covid19 and lockdown](https:\/\/www.kaggle.com\/surajkum1198\/twitterdata) data for training the model and have applied the trained model on this dataset for sentimental analysis. Got to know about this dataset from Purva Singh's kernel [Covid19 Tweets EDA and Sentiment Analysis](https:\/\/www.kaggle.com\/purvasingh\/covid19-tweets-eda-and-sentiment-analysis)\n\nSteps involved in sentimental analysis are:\n* Preprocessing of tweets to remove urls, emojies, html tags, punctuations.\n* Tokenization of tweets.\n* Building and compiling the model.\n* Training and evaluating the model.\n\nI have not removed stopwords from the text, since stopwords like \"not\" have importance in sentimental analysis. \n\nThis kernel also how to deal with class imbalance problem.\n\nLet's start with importing all necessary packages.\n","2762914e":"f1 score of 0.71 is good enough to go ahead. let's apply this model on some of the tweets from original dataset and see how it performs.","5efac615":"Let's replace label 'joy' with 1 and others with 0 to make it a binary classification task. \n\nClean the text using the function clean_text defined above.","93c87172":"Let's see the sentiment labels and their counts in data.","bbac34d1":"Let's define a function to remove the urls, html tags, punctuations and emojies from the text.","f628e2fa":"Let's plot the trend of tweet count against the dates for top 4 countries.","1d9a9307":"Let's find the count of tweets location wise.","20f3d039":"Our model is able to correctly identify the sentiments of tweets from original dataset.\n\nLet's predict sentiment of all tweets from original dataset and plot positive and negative sentiment counts."}}