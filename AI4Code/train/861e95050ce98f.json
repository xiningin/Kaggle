{"cell_type":{"2323e58a":"code","b1d06e39":"code","fb3d7267":"code","f649a843":"code","4c0b8471":"code","8156fb41":"code","148c34b1":"code","a73c3e57":"code","c169307f":"code","37993010":"code","3cadaf5c":"code","2dd917b3":"code","d63ab103":"code","f5eeca47":"code","41a30575":"code","43516044":"code","22040f13":"code","889e05ab":"code","6a4d208a":"code","80afb9ca":"code","c28afbc7":"code","3cf7aeca":"code","88247f05":"code","79cc6d97":"code","5441c739":"code","9e8bef78":"code","b3209a7b":"code","ee9bf3ce":"code","63337b6a":"code","de5348bc":"code","036f878f":"code","b0b4836b":"code","f25af549":"code","368d2c7e":"code","21f40dab":"code","1889a54d":"code","95dfdfc8":"code","8f9c2a08":"code","d506eac9":"code","1b3f4292":"code","e5b9e1d5":"code","ee28887c":"code","bccde2f5":"code","1ada1a71":"code","a0fdc89e":"code","3ba06fbe":"code","63a313b3":"code","182d1b21":"code","0f778b49":"code","4ea43e52":"code","0500778b":"code","1cb1c781":"code","a27047cd":"code","ad349556":"code","87abac5d":"code","c5dc0e7e":"code","5cf6aed4":"code","1b14ab6a":"code","7cfebf61":"code","a2ab6ee9":"code","6c2fd020":"code","ae9725cd":"markdown","a89cd415":"markdown","929b64c6":"markdown","0c25ad41":"markdown","961ac393":"markdown","ada28aae":"markdown","896fef09":"markdown","e3448776":"markdown"},"source":{"2323e58a":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","b1d06e39":"df = pd.read_csv('..\/input\/datasets_4123_6408_framingham.csv')\ndf.head()","fb3d7267":"\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","f649a843":"df.describe()","4c0b8471":"print (df.shape)\nprint (df.info())","8156fb41":"round((df.isnull().sum()\/len(df.index))*100,2)","148c34b1":"print (df['glucose'].describe())\ndf['glucose'].median()","a73c3e57":"plt.figure(figsize = (10,5))\n\nsns.boxplot(x=df['glucose'])","c169307f":"df = df.dropna()","37993010":"print (df.shape)\nprint (df.info())","3cadaf5c":"df.head()","2dd917b3":"def new(dataframe, features, rows, col):\n    fig = plt.figure(figsize = (20,20))\n    for i,feature in enumerate(features):\n        ax = fig.add_subplot(rows,col, i+1)\n        dataframe[feature].hist(bins=20,ax=ax)\n        ax.set_title(feature+\" Distribution\")\n\n    fig.tight_layout()  \n    plt.show()\n        \nnew(df,df.columns,6,3)\n        ","d63ab103":"\nfor i in df.columns:\n     print ('Unique values in ', i , 'is: ' , df[i].unique())","f5eeca47":"df['BPMeds'] = df['BPMeds'].map({1.:1,0.:0}) \ndf['BPMeds'].unique()","41a30575":"X = df.drop('TenYearCHD',axis = 1)\ny = df['TenYearCHD']","43516044":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train, y_test = train_test_split (X,y, train_size=0.7 , test_size =0.3 , random_state = 100)\n","22040f13":"print (X_train.shape)\n#y_train = y_train.values.reshape(-1,1)\nprint (y_train.shape)\nprint (X_test.shape)\n#y_test = y_test.values.reshape(-1,1)\nprint (y_test.shape)","889e05ab":"from sklearn.preprocessing import StandardScaler","6a4d208a":"import numpy as np\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\n\nclass MyScaler(TransformerMixin, BaseEstimator):\n\n    def fit(self, X, y=None):\n        self.means_ = X.mean(axis=0)\n        self.std_dev_ = X.std(axis=0)\n        return self\n\n    def transform(self, X, y=None):\n        return (X - self.means_[:X.shape[1]]) \/ self.std_dev_[:X.shape[1]]\n","80afb9ca":"varlist = ['age','education','cigsPerDay',\n          'totChol','sysBP','diaBP','BMI','heartRate',\n          'glucose']\nX_train.head()","c28afbc7":"scaler = StandardScaler()\n\nX_train[varlist] = scaler.fit_transform(X_train[varlist])\nX_test[varlist] = scaler.transform(X_test[varlist])","3cf7aeca":"#print (X_train.head())\nX_test.head()","88247f05":"plt.figure(figsize = (16,10))\nax = sns.heatmap(X_train.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n","79cc6d97":"import statsmodels.api as sm","5441c739":"X_train_lr = sm.add_constant(X_train)","9e8bef78":"log1 = sm.GLM(y_train, X_train_lr, family= sm.families.Binomial())\nlog1 = log1.fit()\nlog1.summary()","b3209a7b":"from sklearn.linear_model import LogisticRegression","ee9bf3ce":"logreg = LogisticRegression()","63337b6a":"from sklearn.feature_selection import RFE\nrfe = RFE (logreg, 10)\nrfe = rfe.fit(X_train, y_train)","de5348bc":"print (list(zip(X_train.columns, rfe.support_, rfe.ranking_)))","036f878f":"col = X_train.columns[rfe.support_]\nprint (col)","b0b4836b":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nlogm2 = logm2.fit()\nlogm2.summary()","f25af549":"y_train_pred = logm2.predict(X_train_sm)\ny_train_pred","368d2c7e":"y_train_pred_final = pd.DataFrame({'TenYearCHD':y_train,'Prob':y_train_pred})\ny_train_pred_final.head()\n","21f40dab":"y_train_pred_final['predicted'] = y_train_pred_final['Prob'].map(lambda x: 1 if x>0.2 else 0)\ny_train_pred_final.head()\n","1889a54d":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final['TenYearCHD'], y_train_pred_final['predicted'] )\nprint(confusion)","95dfdfc8":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final['TenYearCHD'], y_train_pred_final['predicted']))\n","8f9c2a08":"def back_feature_elem (df,dependent_var,col_list):\n\n    while len(col_list)>0 :\n        df1 = sm.add_constant(df[col_list])\n        model=sm.GLM(dependent_var,df1,family=sm.families.Binomial())\n        result=model.fit()\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(X_train,y_train,col)\n","d506eac9":"result.summary()","1b3f4292":"\nX_train_sm = sm.add_constant(X_train[['male','age','cigsPerDay','prevalentHyp','diabetes']])\nmodel=sm.GLM(y_train,X_train_sm,family=sm.families.Binomial())\nres=model.fit()\nres.summary()","e5b9e1d5":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:5]","ee28887c":"\ny_train_pred_final = pd.DataFrame({'TenYearCHD':y_train.values,'prob':y_train_pred })\ny_train_pred_final.head()","bccde2f5":"y_train_pred_final['prediction'] = y_train_pred_final['prob'].map(lambda x: 1 if x>0.2 else 0)\ny_train_pred_final.head()","1ada1a71":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final['TenYearCHD'], y_train_pred_final['prediction'] )\nprint(confusion)","a0fdc89e":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final['TenYearCHD'], y_train_pred_final['prediction']))\n","3ba06fbe":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","63a313b3":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm[['male','age','cigsPerDay','prevalentHyp','diabetes']].columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm[['male','age','cigsPerDay','prevalentHyp','diabetes']].values, i) for i in range(X_train_sm[['male','age','cigsPerDay','prevalentHyp','diabetes']].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","182d1b21":"TP = int(confusion[1,1]) # true positive \nTN = int(confusion[0,0]) # true negatives\nFP = int(confusion[0,1]) # false positives\nFN = int(confusion[1,0]) # false negatives\n","0f778b49":"from sklearn.metrics import classification_report\n\nclassification_report(y_train_pred_final['TenYearCHD'], y_train_pred_final['prediction'])","4ea43e52":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_train_pred_final['TenYearCHD'], y_train_pred_final['prob'])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Heart disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)\n","0500778b":"num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    y_train_pred_final[i] = y_train_pred_final['prob'].map(lambda x: 1 if x > i else 0)\n    \ny_train_pred_final.head()","1cb1c781":"cutoff_df = pd.DataFrame(columns = ['prob', 'accuracy', 'sensi', 'speci'])\n\nfrom sklearn.metrics import confusion_matrix\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    confusion = metrics.confusion_matrix(y_train_pred_final['TenYearCHD'], y_train_pred_final[i] )\n    TP = int(confusion[1,1]) # true positive \n    TN = int(confusion[0,0]) # true negatives\n    FP = int(confusion[0,1]) # false positives\n    FN = int(confusion[1,0]) # false negatives\n\n    total1=TP+FP+TN+FN\n    accuracy = (TP+TN)\/total1\n    speci = TN\/(TN+FP)\n    sensi = TP\/(TP+FN)\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint (cutoff_df)","a27047cd":"ax = cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.grid()\n\nplt.show()","ad349556":"y_train_pred_final['final_predicted'] = y_train_pred_final['prob'].map( lambda x: 1 if x > 0.165 else 0)\n\ny_train_pred_final.head()","87abac5d":"metrics.accuracy_score(y_train_pred_final['TenYearCHD'], y_train_pred_final.final_predicted)\n","c5dc0e7e":"\n\n\nX_test_1 = X_test[['male','age','cigsPerDay','prevalentHyp','diabetes']]\nX_test_1.head()\n","5cf6aed4":"X_test_sm = sm.add_constant(X_test_1)\nlogtest = sm.GLM(y_test, X_test_sm, family= sm.families.Binomial())\nlogtest = logtest.fit()\nlogtest.summary()","1b14ab6a":"y_test_pred = logtest.predict(X_test_sm)\ny_test_pred[:5]","7cfebf61":"y_test_pred_final = pd.DataFrame({'TenYearCHD':y_test,'prob':y_test_pred })\ny_test_pred_final.head()\n","a2ab6ee9":"y_test_pred_final['prediction'] = y_test_pred_final['prob'].map(lambda x: 1 if x > 0.165 else 0)\ny_test_pred_final.head()                   \n                  ","6c2fd020":"metrics.accuracy_score(y_test_pred_final.TenYearCHD, y_test_pred_final.prediction)\n\n","ae9725cd":"# Feature Selection: Backward elimination (p-value)","a89cd415":"# Feature Selection","929b64c6":"# Making prediction on test dataset","0c25ad41":"# Test-Train Split","961ac393":"# Feature Scaling","ada28aae":" Here we have to predict if the patient have the risk of Coronary Heart Risk in the span of 10 years. \n \n \n \n Variables :\nEach attribute is a potential risk factor. There are both demographic, behavioural and medical risk factors.\n\n### Demographic:\nsex: male or female;(Nominal)\n\n### age: \nage of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\nBehavioural\n\n### currentSmoker: \nwhether or not the patient is a current smoker (Nominal)\n\n### cigsPerDay: \nthe number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarretts, even half a cigarette.)\n\nMedical( history):\n\n### BPMeds:\nwhether or not the patient was on blood pressure medication (Nominal)\n\n### prevalentStroke: \nwhether or not the patient had previously had a stroke (Nominal)\n\n### prevalentHyp: \nwhether or not the patient was hypertensive (Nominal)\n\n### diabetes: \nwhether or not the patient had diabetes (Nominal)\n\nMedical(current):\n\n### totChol:\ntotal cholesterol level (Continuous)\n\n### sysBP:\nsystolic blood pressure (Continuous)\n\n### diaBP: \ndiastolic blood pressure (Continuous)\n\n### BMI: \nBody Mass Index (Continuous)\n\n### heartRate:\nheart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n### glucose: \nglucose level (Continuous)\n\nPredict variable (desired target):\n\n## 10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n","896fef09":"Framingham Heart Study","e3448776":"# Model Building\n\n"}}