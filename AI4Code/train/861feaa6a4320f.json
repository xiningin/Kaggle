{"cell_type":{"995c21bc":"code","dda4b609":"code","ffe1351a":"code","dca459cb":"code","aed9ecbb":"code","ccb9486f":"code","6ccef15b":"code","ede9c801":"code","79f300b7":"code","669df2c9":"code","530fdcf9":"code","252ac171":"code","60bed7bf":"code","f1d9e0e8":"code","a7894f0f":"code","a7323796":"code","7001184e":"code","4fa15511":"code","bedd7d1b":"code","c67dbc8c":"code","79120be0":"code","74b7b54b":"code","eb260c98":"code","d723527d":"code","1803a876":"code","0c41c1ad":"code","743134ad":"code","7447c71c":"code","12aee6a7":"code","68ac8e4e":"code","3649b1ad":"code","73e52d2c":"code","9da9b2c7":"code","11851961":"code","7abe7b04":"code","c7b27a03":"code","bf3b096c":"code","e30ede8e":"code","fb4089be":"code","07c291ef":"code","341d51dd":"code","01c9e5a4":"code","d329dcff":"code","f8dbfa00":"code","72e9820e":"code","ad51883c":"code","8df627b0":"code","d8019c81":"code","637cc78b":"code","704e42a8":"code","b19b6144":"code","8af95c53":"code","beb7be23":"code","df265907":"code","31d80e7c":"markdown","7c466ecf":"markdown","78d94215":"markdown","f8591e22":"markdown","f37115e1":"markdown","44569cba":"markdown","dfa6f712":"markdown","aec4d80b":"markdown","b0e1bc77":"markdown","fe152f93":"markdown","3332092f":"markdown","01c22152":"markdown","baaaf1c8":"markdown","a5d223a9":"markdown","4fdaae0f":"markdown","491e7977":"markdown","8b9879f8":"markdown","566df730":"markdown","9cd52965":"markdown","6e78c672":"markdown","6ed2d6f7":"markdown","55c80d70":"markdown","f80490d8":"markdown","a9793ef1":"markdown"},"source":{"995c21bc":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n%matplotlib inline","dda4b609":"# Read files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","ffe1351a":"#importing the liberaries\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","dca459cb":"#display the top 5 values\ntrain.head()","aed9ecbb":"#shape of train data\ntrain.shape","ccb9486f":"#you can also check the data set information using the info() command.\ntrain.info()","6ccef15b":"#Analysis for numerical variable\n\ntrain['SalePrice'].describe()\nsns.distplot(train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","ede9c801":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Plot histogram and probability\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('Before transformation')\n\n# Apply transformation\ntrain.SalePrice = np.log1p(train.SalePrice )\n# New prediction\ny_train = train.SalePrice.values\ny_train_orig = train.SalePrice\n\n\n# Plot histogram and probability after transformation\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('After transformation')","79f300b7":"# y_train_orig = train.SalePrice\n# train.drop(\"SalePrice\", axis = 1, inplace = True)\ndata_features = pd.concat((train, test), sort=False).reset_index(drop=True)\nprint(data_features.shape)\n\n# print(train.SalePrice)","669df2c9":"#Let's check if the data set has any missing values. \ndata_features.columns[train.isnull().any()]","530fdcf9":"#plot of missing value attributes\nplt.figure(figsize=(12, 6))\nsns.heatmap(train.isnull())\nplt.show()","252ac171":"#missing value counts in each of these columns\nIsnull = train.isnull().sum()\/len(data_features)*100\nIsnull = Isnull[Isnull>0]\nIsnull.sort_values(inplace=True, ascending=False)\nIsnull","60bed7bf":"#Convert into dataframe\nIsnull = Isnull.to_frame()\nIsnull.columns = ['count']\nIsnull.index.names = ['Name']\nIsnull['Name'] = Isnull.index\n#plot Missing values\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\nsns.barplot(x='Name', y='count', data=Isnull)\nplt.xticks(rotation = 90)\nplt.show()","f1d9e0e8":"#missing data percent plot, basically percent plot is for categorical columns\n\ntotal = data_features.isnull().sum().sort_values(ascending=False)\npercent = (data_features.isnull().sum()\/data_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","a7894f0f":"#Corralation between train And test attributes","a7323796":"#Separate variable into new dataframe from original dataframe which has only numerical values\n#there is 38 numerical attribute from 81 attributes\ntrain_corr = data_features.select_dtypes(include=[np.number])","7001184e":"train_corr.shape\n","4fa15511":"#Delete Id because that is not need for corralation plot\n#del train_corr['Id']","bedd7d1b":"#Coralation plot\ncorr = train_corr.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corr, annot=True)","c67dbc8c":"#0.5 is nothing but a threshold value.\n#It is good to take it 0.5 because your feautres which are fitting under this threshold will give good accuracy.\n\ntop_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = data_features[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()\n","79120be0":"col = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.set(style='ticks')\nsns.pairplot(data_features[col], height=2, kind='reg')","74b7b54b":"#unique value of OverallQual\ndata_features.OverallQual.unique()","eb260c98":"sns.barplot(data_features.OverallQual, data_features.SalePrice)","d723527d":"#boxplot\nplt.figure(figsize=(18, 8))\nsns.boxplot(x=data_features.OverallQual, y=data_features.SalePrice)","1803a876":"col = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.set(style='ticks')\nsns.pairplot(data_features[col], height=2, kind='reg')","0c41c1ad":"print(\"Find most important features relative to target\")\ncorr = data_features.corr()\ncorr.sort_values(['SalePrice'], ascending=False, inplace=True)\ncorr.SalePrice","743134ad":"# PoolQC has missing value ratio is 99%+. So, there is fill by None\ndata_features['PoolQC'] = data_features['PoolQC'].fillna('None')","7447c71c":"#Arround 50% missing values attributes have been fill by None\ndata_features['MiscFeature'] = data_features['MiscFeature'].fillna('None')\ndata_features['Alley'] = data_features['Alley'].fillna('None')\ndata_features['Fence'] = data_features['Fence'].fillna('None')\ndata_features['FireplaceQu'] = data_features['FireplaceQu'].fillna('None')\ndata_features['SaleCondition'] = data_features['SaleCondition'].fillna('None')","12aee6a7":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata_features['LotFrontage'] = data_features.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","68ac8e4e":"#GarageType, GarageFinish, GarageQual and GarageCond these are replacing with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    data_features[col] = data_features[col].fillna('None')","3649b1ad":"#GarageYrBlt, GarageArea and GarageCars these are replacing with zero\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    data_features[col] = data_features[col].fillna(int(0))","73e52d2c":"#BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual these are replacing with None\nfor col in ('BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual'):\n    data_features[col] = data_features[col].fillna('None')","9da9b2c7":"#MasVnrArea : replace with zero\ndata_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(int(0))","11851961":"#MasVnrType : replace with None\ndata_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')","7abe7b04":"#There is put mode value \ndata_features['Electrical'] = data_features['Electrical'].fillna(data_features['Electrical']).mode()[0]","c7b27a03":"#There is no need of Utilities\ndata_features = data_features.drop(['Utilities'], axis=1)","bf3b096c":"#Checking there is any null value or not\nplt.figure(figsize=(10, 5))\nsns.heatmap(data_features.isnull())","e30ede8e":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood',\n        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n        'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation', 'GarageType', 'MiscFeature', \n        'SaleType', 'SaleCondition', 'Electrical', 'Heating')\n","fb4089be":"from sklearn.preprocessing import LabelEncoder\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(data_features[c].values)) \n    data_features[c] = lbl.transform(list(data_features[c].values))","07c291ef":"train = data_features.iloc[:len(y_train), :]\ntest = data_features.iloc[len(y_train):, :]\nprint(['Train data shpe: ',train.shape,'Prediction on (Sales price) shape: ', y_train.shape,'Test shape: ', test.shape])","341d51dd":"#Take targate variable into y\ny = train['SalePrice']","01c9e5a4":"#Delete the saleprice\ndel train['SalePrice']","d329dcff":"#Take their values in X and y\nX = train.values\ny = y.values","f8dbfa00":"# Split data into train and test formate\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)","72e9820e":"#Train the model\nfrom sklearn import linear_model\nmodel = linear_model.LinearRegression()","ad51883c":"#Fit the model\nmodel.fit(X_train, y_train)","8df627b0":"#Prediction\nprint(\"Predict value \" + str(model.predict([X_test[142]])))\nprint(\"Real value \" + str(y_test[142]))","d8019c81":"#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X_test, y_test)*100)","637cc78b":"#Train the model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=1000)","704e42a8":"#Fit\nmodel.fit(X_train, y_train)","b19b6144":"#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X_test, y_test)*100)","8af95c53":"#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)","beb7be23":"#Fit\nGBR.fit(X_train, y_train)","df265907":"print(\"Accuracy --> \", GBR.score(X_test, y_test)*100)","31d80e7c":"## Encoding str to int","7c466ecf":"# Concatenate  train and test\nThis is good practice to concatenate the test and train data before start working on missing values.","78d94215":"# Top 50% Corralation concatenate attributes with sale-price\n","f8591e22":"Here we can to know Predictor Variables , Target Variable , their types and type of Data Set\n","f37115e1":"At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n**Continuous Variables:-** In case of continuous variables, we need to understand the central tendency and spread of the variable. Note: Univariate analysis is also used to highlight missing and outlier values. In the upcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/#one","44569cba":"There Object, Float and Int DataTypes.","dfa6f712":"In this Data Set , Target Variable is SALE PRICE and Rest Are Predictor\n","aec4d80b":"# RandomForestRegression","b0e1bc77":"**Above Graph is showing Positive Skewness, Now we will get rid of the skewness by using log transformation. **","fe152f93":"### 1.2 Univariate Analysis","3332092f":"#Variable Identification\n  ### 1.1 First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.","01c22152":"# GradientBoostingRegressor","baaaf1c8":"Type of Data \n1. Categorical 2. Ordinal (mix of int and string)","a5d223a9":"# Linear Regression","4fdaae0f":"## Missing data","491e7977":"# Bi-variate Analysis\nBi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.","8b9879f8":"## Visualising missing values","566df730":"## Here OverallQual is highly correlated with target feature of saleprice by 82%","9cd52965":"### **Transforming **","6e78c672":"The very First Step in preparing the good model is perform EDA (Explotary Data Analysis.) Let's Do it. \n1. EDA - Steps of EDA \n","6ed2d6f7":"# Imputting missing values","55c80d70":"### Splitting the data back to train and test","f80490d8":"# Prepraring data for prediction","a9793ef1":"**Problem Description :** Predict the Final Price of the House based on few features. For Example, some people want Two Bedroom or other want Three Bedroom House. So the House Sale Price Depend on the Living Area of the House and Depend upon how many people want small or Big one. We can predict which living area is high in demand and what cost of house."}}