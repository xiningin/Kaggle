{"cell_type":{"089c51c1":"code","09571342":"code","10af25b0":"code","1639b578":"code","9a76626c":"code","83340ca3":"code","4dd7f69f":"code","1bb93e12":"code","954afb67":"code","d6bb2333":"code","9ac91d5b":"code","6499cee3":"code","e447c35b":"code","3cb8a8c3":"code","eb5fdf31":"code","b1b03763":"code","eefba5b9":"code","d351ea53":"code","78023f73":"code","7b8c126e":"code","757ae79d":"code","af798114":"code","1151cb7c":"code","94a29159":"code","d0eec8ac":"code","9d955873":"code","713840de":"markdown","f400bbd7":"markdown","6500def2":"markdown","6765e6a3":"markdown","a8cb1f25":"markdown","38089635":"markdown","cd11d9d0":"markdown","a4c14ab9":"markdown","1f254fe1":"markdown","372be5f8":"markdown","691e44ac":"markdown","7131d6f6":"markdown","ef7385fb":"markdown","0e72d066":"markdown","c67c222d":"markdown","891986ae":"markdown","c2c8b070":"markdown","d52ad362":"markdown","d8603939":"markdown","1ff9e2a7":"markdown","7a4d8fdf":"markdown","e691a05a":"markdown","8dbc815a":"markdown","0fa1b036":"markdown"},"source":{"089c51c1":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","09571342":"# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"\/kaggle\/input\/data-path\/Data_path.csv\")\nref.head()","10af25b0":"# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1\n    print(counter, mfccs)\n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","1639b578":"# save it\ndf.to_pickle('my4EmotionDatabases-mfccs')","9a76626c":"# load it\ndf = pd.read_pickle('my4EmotionDatabases-mfccs')\ndf.head()","83340ca3":"# Now extract the mean bands to its own feature columns\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","4dd7f69f":"# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]","1bb93e12":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","954afb67":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Check the dataset now \nX_train[150:160]","d6bb2333":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","9ac91d5b":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","6499cee3":"# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()","e447c35b":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))","3cb8a8c3":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","eb5fdf31":"# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","b1b03763":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","eefba5b9":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","d351ea53":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","78023f73":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","7b8c126e":"# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'\n","757ae79d":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","af798114":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","1151cb7c":"modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","94a29159":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","d0eec8ac":"modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","9d955873":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","713840de":"<a id=\"data\"><\/a>\n## 1. Data preparation and processing\nWe saw in [Part 1](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-recognition-part-1-explore-data) and [Part 2](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-recognition-part-2-feature-extra) the way we process the audio file into data and the MFCC features we extracted. We're going to do the same thing here except we process the entirity of the audio files. First up we need the reference file that contains the path to the raw audio files for training.","f400bbd7":"# <center>Audio Emotion Recognition<\/center>\n## <center>Part 3 - Baseline model<\/center>\n#### <center> 24th August 2019 <\/center> \n#####  <center> Eu Jin Lok <\/center> ","6500def2":"<a id=\"validation\"><\/a>\n## 4. Model validation\nNow predicting emotions on the test data. After serialising the model above, i'm going to just reload it into disk. Essentially to re-use the model without having to retrain by re-running the code, we just need to run this section of the code and apply the model to a new dataset. Since we used the same test set in the keras model, the result is essentially the same as the last epoch of 100 which is 43.80%","6765e6a3":"50%, not too shabby indeed. The precision and recall for 'Surprise' and 'Angry' is pretty good in particular ","a8cb1f25":"<a id=\"serialise\"><\/a>\n## 3. Model serialisation\nSo its time to serialise the model for re-usability. Serialisation and saving mean the same thing. We need to serialise the model architecture and the weights, thats all. ","38089635":"The prediction is in the form of numbers, we'll need to append the labels to it before we run the accuracy measure...","cd11d9d0":"For the sake of documentation, I'll just mention here that there's another method for normalisation but it hasn't worked out well, at least not when I implemented it. So I swapped in for something more simple, which is what i've implemented above. Perhaps someone else could give it a try below\n\n```python\nmax_data = np.max(X_train)\nmin_data = np.min(X_train)\nX_train = (X_train-min_data)\/(max_data-min_data+1e-6)\nX_train =  X_train-0.5\n\nmax_data = np.max(X_test)\nmin_data = np.min(X_test)\nX_test = (X_test-min_data)\/(max_data-min_data+1e-6)\nX_test =  X_test-0.5\n\nX_train[150:160]\n```\n\nNext part we'll need to convert the data format to a numpy array, because we are using keras. Initially I had plans to use XGboost or LightGBM for this task. But since I've potential plans to move to a 2D CNN, it may make sense to continue on the Deep Learning path way and implement a ID CNN.","a4c14ab9":"-------------------\nNow because we are using a CNN, we need to specify the 3rd dimension, which for us is 1. Its 1 because we're doing a 1D CNN and not a 2D CNN. If we use the MFCC data in its entirity, we could feed that through as the input data, thus making the network a 2D CNN.  ","1f254fe1":"#### Emotion accuracy\nWe'll now ignore the gender part and just super group them into the 7 core emotions. Lets see what we get...","372be5f8":"Now because we are mixing up a few different data sources, it would be wise to normalise the data. This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, the problem used to be know as \"exploding gradients\". ","691e44ac":"<a id=\"processing\"><\/a>\n### Data processing\n\nLike any good standard data science workflow, data processing is the most important step. Cause garbage in grabage out. So lets start munging the data into a workable format and pad out any issues we find. ","7131d6f6":"Now that looks alot better. Next step we will split the data into 2 parts, one for training and one for validation. This ensures we measure the model's performance at its true accuracy. ","ef7385fb":"<a id=\"final\"><\/a>\n## 5. Final thoughts \nThe gender seperation turns out to be a curcial implementation in order to accurately classify emotions. Upon closer inspection of the confusion matrix, it seems that female tends to express emotions in a more, obvious manner, for the lack of a better word. Whilst males tend to be very placid or subtle. This is probably why we see the error rate amongst males are really high. For example, male happy and angry gets mixed up quite often. \n\nIn our next section we will be checking for generalisability of this initial baseline solution before  before implementing further enhancements, followed by an audio streamer that will give us the capability of predicting the emotions of a segment of the audio call.  \n\nThis section of the notebook borrowed heavily from this [repository](https:\/\/github.com\/MITESHPUTHRANNEU\/Speech-Emotion-Analyzer). The original author may have overstated the accuracy as I wasn't able to replicate the accuracy results but, by in large the approach is pretty sound and I've taken his work as a blueprint to setup my own here. ","0e72d066":"---------------\n#### Gender accuracy result \nif you notice, that the gender classification is more accurate. So lets group them up and measure the accuracy again?","c67c222d":"Absolute accuracy for the gender by emotions is 43%. Whilst that may not seem high at first but remember, a random guess correct is 1 out of 14 which is 7%. So 43% is huge! The heat map plot below will do justice in illustrating how good the results is. And note we have only just scratched the surface","891986ae":"#### Emotion by gender accuracy  \nSo lets visualise how well we have done for the Emotion by Gender model","c2c8b070":"With just gender we get a 80% accuracy. The model is especially precise in capturing female voices. However, male voices tends to be harder and it does make higher mistakes thinking its female. ","d52ad362":"Lets write the predictions out into a file for re-use","d8603939":"So we can see that the loss starts to plateau now at around 50 epochs. Regardless we'll keep it at 100 as the final model.","1ff9e2a7":"So, we've made our predictions, so how well have we done? We're going to use the most simplest form of accuracy measure which is absolute accuracy, which is really just the % of records where Actual = Predicted, over the total number of records predicted. We'll also produce the F1, recall and precision scores. \n\nThe most common way to visualise this output is via a confusion matrix. I found an excellent heat map plot to visualise the accuracy of the confusion matrix [here](https:\/\/gist.github.com\/shaypal5\/94c53d765083101efc0240d776a23823) which i've borrowed for this notebook","7a4d8fdf":"## Introduction \nContinuing where we left off in [Part 1](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-recognition-part-1-explore-data) and [Part 2](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-recognition-part-2-feature-extra), here we'll build a baseline model for an emotion classifier. When I say baseline, I mean its the simplest most parsimonious model I can think of. And view points will vary from one data scientist to another, but essentially its a model __NOT__ meant to achieve full accuracy potential. It's just to qucikly test the framework and setup the blueprint for how we go about creating a workable emotion classifier, cause at the moment, we don't know what works and what doesn't. This is a long notebook so this is the agenda below: \n\n1. [Data preparation and processing](#data)\n    * [Data preparation](#preparation)\n    * [Data processing](#processing)\n2. [Modelling](#modelling)\n3. [Model serialisation](#serialise)\n4. [Model validation](#validation)\n5. [Final thoughts](#final)\n\nUpvote this notebook if you like, and be sure to check out the other parts which are now available:\n* [Part 1 | Explore data](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-1-explore-data)\n* [Part 2 | Feature Extract](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-2-feature-extract)\n* [Part 3 | Baseline model](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-3-baseline-model)\n* [Part 4 | Apply to new audio data](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-4-apply-to-new-audio-data)\n* [Part 5 | Data augmentation](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-5-data-augmentation)\n* [Part 6 | 2D CNN Implementation](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-6-2d-cnn-66-accuracy)\n\nMost importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n\n- [TESS](https:\/\/tspace.library.utoronto.ca\/handle\/1807\/24487)\n- [CREMA-D](https:\/\/github.com\/CheyneyComputerScience\/CREMA-D)\n- [SAVEE](http:\/\/kahlan.eps.surrey.ac.uk\/savee\/Database.html)\n- [RAVDESS](https:\/\/zenodo.org\/record\/1188976#.XYP8CSgzaUk)\n- [RAVDESS_Kaggle](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)","e691a05a":"So we've already seen the shape of an MFCC output for each file, and it's a 2D matrix of the number of bands by time. In order to optimise space and memory, we're going to read each audio file, extract its mean across all MFCC bands by time, and  just keep the extracted features, dropping the entire audio file data. ","8dbc815a":"<a id=\"preparation\"><\/a>\n### Data preparation\nLets pick up the meta-data file which we save in [part 1](\"https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-recognition-part-1-explore-data\"), we're going to need it here to run a loop over it to read all the audio files spread across the 4 directories. ","0fa1b036":"> > <a id=\"modelling\"><\/a>\n## 2. Modelling\nThe architecture of the model below is based on a few sources that I've seen before such as Kaggle and Stackoverflow. I'm unable to find the source but safe to say this particular format works quite well and is fast, although I've used GPU. "}}