{"cell_type":{"9ac08bc7":"code","69b9fcb4":"code","e5d5e07a":"code","37a63910":"code","4357420f":"code","936d85f4":"code","e1dfbd28":"code","23e1bc64":"code","fc1bcfae":"code","383a88fb":"code","4902bcff":"code","71d45ced":"code","46c02fd4":"code","e3228a84":"code","6cf3470f":"code","1a9097d6":"code","a3c50140":"code","e00888ba":"code","27703b30":"code","067999e7":"code","fd285522":"code","8ae79660":"code","c348f6d6":"code","74b07b1e":"code","89631ddb":"code","ca2b3458":"code","ac772ccd":"code","df41f177":"code","0cbc8102":"code","f2bb3a97":"code","cc9f4e24":"code","00c503d8":"code","75284c9d":"code","bfcef275":"code","618fbf75":"code","930de283":"code","1afce6b8":"code","6db7762c":"code","a6ed1734":"code","669bbd2e":"code","ef9ccedb":"code","8a4e15bc":"code","d38c7b5d":"code","63375c7b":"code","04a102f3":"code","fa5e69ee":"code","1aa6246f":"code","64d82c99":"code","aa4bdf42":"code","94a7a8e8":"code","f3581b30":"code","52c6fcba":"code","419ce216":"code","4a22d654":"code","47199636":"code","13e5b6ed":"code","6bf7d7b1":"code","7135a3f1":"code","21d43b04":"code","a1688c43":"code","9adbe99c":"code","f91a2d4b":"code","18d8f7eb":"code","e5970c90":"code","52a72db9":"code","3fef82a8":"code","6f7112fe":"code","e274eb5a":"code","dfa1e399":"code","77b383a9":"code","01284ff6":"code","aa0bb29a":"code","e39ac37c":"code","a83ca119":"code","77902854":"code","610f61ff":"code","b5d35b4c":"code","29527c52":"code","066154f8":"markdown","835ffeb4":"markdown","15d88fd3":"markdown","546376c3":"markdown","0c06c3b2":"markdown","f9ff345d":"markdown"},"source":{"9ac08bc7":"import numpy as np\nimport pandas as pd\n\n\ndata = pd.read_csv('..\/input\/teaching-assistant-evaluation-dataset\/tae.data', index_col=False, names = ['speaker', 'course_instructor', 'course', 'semester', 'class_size', 'class'], )  \n ","69b9fcb4":"data","e5d5e07a":"data.describe()","37a63910":"data.isna().sum()","4357420f":"data = data.drop_duplicates()","936d85f4":"data.info()","e1dfbd28":"for col in data.columns:\n    print(data[col].value_counts())","23e1bc64":"import plotly.express as px\n\nfig = px.box(data, y = ['speaker', 'course_instructor', 'course', 'semester', 'class_size'] )\nfig.show()","fc1bcfae":"#outliers calculation\n\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\n((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()","383a88fb":"#uncomment if you want to delete outliers\n\n#data = data[(data >= (Q1 - 1.5*IQR)) & (data <= (Q1 + 1.5*IQR))]\n\n#data = data.dropna()\n\n#data.describe()","4902bcff":"X = data.drop(['class'], axis=1)\ny = data['class']","71d45ced":"#feature importance with ExtraTreesClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nforest = ExtraTreesClassifier(n_estimators=250, max_depth=5, random_state=1)\nforest.fit(X , y)\n\nimportances = forest.feature_importances_\nstd = np.std(\n    [tree.feature_importances_ for tree in forest.estimators_],\n    axis=0\n)\nindices = np.argsort(importances)[::-1]\nindices = indices[:]\n\nprint('Top features:')\nfor f in range(1,5):\n    print('%d. feature %d (%f)' % (f , indices[f], importances[indices[f]]))","46c02fd4":"#feature importance with RFECV\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\n\nestimator = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state = 42)\nselector = RFECV(estimator, cv=5, step=1)\nselector = selector.fit(X, y)\nselector.support_\n\n\nselector.ranking_","e3228a84":"#X = X.drop(['speaker'], axis=1)","6cf3470f":"X = pd.DataFrame(X)\nX.describe()","1a9097d6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","a3c50140":"print(y_train.value_counts())","e00888ba":"from sklearn.ensemble import IsolationForest\n\nisf = IsolationForest(n_jobs=-1, random_state=1)\nisf.fit(X_train, y_train)\n\nprint(isf.score_samples(X_train))","27703b30":"isf.predict(X_train).sum()","067999e7":"# Get names of indexes for which score samples have value -1\nindexNames = X_train[ isf.score_samples(X_train) == -1 ].index\n# Delete these row indexes from dataFrame\nX_train.drop(indexNames , inplace=True)\n\n","fd285522":"#uncomment if you want to oversample using SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 42)\n           \nX_train, y_train = sm.fit_sample(X_train, y_train) ","8ae79660":"print(y_train.value_counts())","c348f6d6":"#scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nstandardized_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nstandardized_X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","74b07b1e":"np.unique(y_test)","89631ddb":"from sklearn.metrics import log_loss\n\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=i, random_state = 42)\n    \n    \n    clf_gini.fit(standardized_X_train, y_train)\n    y_pred_gini = clf_gini.predict_proba(standardized_X_test)\n    y_pred_train_gini = clf_gini.predict_proba(standardized_X_train)\n     \n    #print('Max Depth:' ,i)    \n    #print('Training set score: {:.4f}'.format(clf_gini.score(standardized_X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_gini))\n\n    #print('Test set score: {:.4f}'.format(clf_gini.score(standardized_X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_gini))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","ca2b3458":"import matplotlib.pyplot as plt\n\ny = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('max depth')\n# Set the y axis label of the current axis.\nplt.ylabel('log-loss')\n# Set a title of the current axes.\nplt.title('Overfitting check ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","ac772ccd":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'max_depth':range(1,20)}\nclf = GridSearchCV(DecisionTreeClassifier(criterion='gini', random_state = 42),  parameters, n_jobs=4)\nclf.fit(X=standardized_X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) ","df41f177":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nkfold = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state = 42, min_samples_split=20, min_samples_leaf=9)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gini, standardized_X_train, y_train, cv=kfold)))","0cbc8102":"clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state = 42,  min_samples_leaf=9)\n\nclf_gini.fit(standardized_X_train, y_train)","f2bb3a97":"y_pred_gini = clf_gini.predict(standardized_X_test)","cc9f4e24":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred_gini))","00c503d8":"import seaborn as sns\n%matplotlib inline\nimport graphviz\nfrom sklearn import tree\n\n\n\ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=standardized_X_train.columns,  \n                              class_names=y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","75284c9d":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_gini)\n\nprint('Confusion matrix\\n\\n', cm)\n\nplt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(np.unique(y_test))\nax.yaxis.set_ticklabels(np.unique(y_test))","bfcef275":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_gini))","618fbf75":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state = 42)\n    \n    \n    clf_en.fit(standardized_X_train, y_train)\n    y_pred_en = clf_en.predict_proba(standardized_X_test)\n    y_pred_train_en = clf_en.predict_proba(standardized_X_train)\n    \n     \n    #print('Max Depth:' ,i)    \n    #print('Training set score: {:.4f}'.format(clf_en.score(standardized_X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_en))\n\n    #print('Test set score: {:.4f}'.format(clf_en.score(standardized_X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_en))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","930de283":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","1afce6b8":"parameters = {'max_depth':range(1,10)}\nclf = GridSearchCV(DecisionTreeClassifier(criterion='entropy', random_state = 42),  parameters, n_jobs=4)\nclf.fit(X=standardized_X_train, y=y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) ","6db7762c":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state = 42, min_samples_leaf = 5, max_leaf_nodes = 9)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_en, standardized_X_train, y_train, cv=kfold)))","a6ed1734":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state = 42)\n\nclf_en.fit(standardized_X_train, y_train)","669bbd2e":"y_pred_en = clf_en.predict(standardized_X_test)","ef9ccedb":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred_en))","8a4e15bc":"\n\ndot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=standardized_X_train.columns,  \n                              class_names=y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","d38c7b5d":"cm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)","63375c7b":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(np.unique(y_test))\nax.yaxis.set_ticklabels(np.unique(y_test))","04a102f3":"print(classification_report(y_test, y_pred_en))","fa5e69ee":"from sklearn.ensemble import GradientBoostingClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gb = GradientBoostingClassifier( max_depth=i, random_state = 42)\n    \n    \n    clf_gb.fit(standardized_X_train, y_train.values.ravel())\n    y_pred_gb = clf_gb.predict_proba(standardized_X_test)\n    y_pred_train_gb = clf_gb.predict_proba(standardized_X_train)\n    \n     \n    #print('Max Depth:' ,i)    \n    #print('Training set score: {:.4f}'.format(clf_gb.score(standardized_X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_gb))\n\n    #print('Test set score: {:.4f}'.format(clf_gb.score(standardized_X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_gb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","1aa6246f":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","64d82c99":"parameters = {'max_depth':range(1,11)}\nclf = GridSearchCV(GradientBoostingClassifier( max_depth=i, random_state = 42),  parameters, n_jobs=4)\nclf.fit(X=standardized_X_train, y=y_train.values.ravel())\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) ","aa4bdf42":"clf_gb = GradientBoostingClassifier( max_depth=2, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gb, standardized_X_train, y_train.values.ravel(), cv=kfold)))","94a7a8e8":"clf_gb = GradientBoostingClassifier( max_depth=2, random_state = 42)\n\nclf_gb.fit(standardized_X_train, y_train.values.ravel())","f3581b30":"y_pred_gb = clf_gb.predict(standardized_X_test)","52c6fcba":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gb.score(standardized_X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gb.score(standardized_X_test, y_test)))","419ce216":"dot_data = tree.export_graphviz(clf_gb.estimators_[0, 0], out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","4a22d654":"cm = confusion_matrix(y_test, y_pred_gb)\n\nprint('Confusion matrix\\n\\n', cm)","47199636":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","13e5b6ed":"print(classification_report(y_test, y_pred_gb))","6bf7d7b1":"import xgboost as xgb","7135a3f1":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_xgb = xgb.XGBClassifier( max_depth=i, random_state = 42, n_jobs = 4)\n    \n    \n    clf_xgb.fit(standardized_X_train, y_train.values.ravel())\n    y_pred_xgb = clf_xgb.predict_proba(standardized_X_test)\n    y_pred_train_xgb = clf_xgb.predict_proba(standardized_X_train)\n    \n     \n    #print('Max Depth:' ,i)    \n    #print('Training set score: {:.4f}'.format(clf_xgb.score(standardized_X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_xgb))\n\n    #print('Test set score: {:.4f}'.format(clf_xgb.score(standardized_X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_xgb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","21d43b04":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","a1688c43":"parameters = {'max_depth':range(1,11)}\nclf = GridSearchCV(xgb.XGBClassifier( max_depth=i, random_state = 42),  parameters, n_jobs=4)\nclf.fit(X=standardized_X_train, y=y_train.values.ravel())\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) ","9adbe99c":"clf_xgb = xgb.XGBClassifier( max_depth=2, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_xgb, standardized_X_train, y_train.values.ravel(), cv=kfold)))","f91a2d4b":"clf_xgb = xgb.XGBClassifier( \n    max_depth=2,\n    gamma=2,\n    eta=0.8,\n    reg_alpha=0.5,\n    reg_lambda=0.5\n)\n\nclf_xgb.fit(standardized_X_train, y_train.values.ravel())\n","18d8f7eb":"y_pred_xgb = clf_xgb.predict(standardized_X_test)","e5970c90":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_xgb.score(standardized_X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_xgb.score(standardized_X_test, y_test)))","52a72db9":"fig, ax = plt.subplots(figsize=(30, 30))\nxgb.plot_tree(clf_xgb, num_trees=2, ax=ax)\nplt.show()","3fef82a8":"cm = confusion_matrix(y_test, y_pred_xgb)\n\nprint('Confusion matrix\\n\\n', cm)","6f7112fe":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","e274eb5a":"print(classification_report(y_test, y_pred_xgb))","dfa1e399":"from sklearn.ensemble import RandomForestClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n    clf_rf = RandomForestClassifier( max_depth=i, random_state = 42, n_jobs = 4, n_estimators = 300, criterion = 'entropy')\n    \n    \n    clf_rf.fit(X_train, y_train.values.ravel())\n    y_pred_rf = clf_rf.predict_proba(X_test)\n    y_pred_train_rf = clf_rf.predict_proba(X_train)\n    \n     \n    #print('Max Depth:' ,i)    \n    #print('Training set score: {:.4f}'.format(clf_rf.score(X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_rf))\n\n    #print('Test set score: {:.4f}'.format(clf_rf.score(X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_rf))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","77b383a9":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","01284ff6":"parameters = [{'max_depth':range(1,6), 'n_estimators' : [10,100,200,300,400]}]\nclf = GridSearchCV(RandomForestClassifier(criterion = 'entropy',  random_state = 42),  parameters, n_jobs=-1)\nclf.fit(X=standardized_X_train, y=y_train.values.ravel())\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_) ","aa0bb29a":"clf_rf = RandomForestClassifier( max_depth=2, random_state = 42, n_estimators = 300, criterion = 'entropy', max_leaf_nodes = 2)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_rf, standardized_X_train, y_train.values.ravel(), cv=kfold)))","e39ac37c":"clf_rf = RandomForestClassifier(max_depth=2, random_state = 42, n_estimators = 300, criterion = 'entropy', max_leaf_nodes=2)\n\nclf_rf.fit(standardized_X_train, y_train.values.ravel())","a83ca119":"y_pred_rf = clf_rf.predict(standardized_X_test)","77902854":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_rf.score(standardized_X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_rf.score(standardized_X_test, y_test)))","610f61ff":"cm = confusion_matrix(y_test, y_pred_rf)\n\nprint('Confusion matrix\\n\\n', cm)","b5d35b4c":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","29527c52":"print(classification_report(y_test, y_pred_rf))","066154f8":"<h1 id=\"Gradient_Boosting\">\n4. Gradient Boosting\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Gradient_Boosting\">\u00b6<\/a>\n<\/h1>","835ffeb4":"<h1 id=\"Exploratory_Data_Analysis\">\n1. Exploratory Data Analysis\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Exploratory_Data_Analysis\">\u00b6<\/a>\n<\/h1>","15d88fd3":"<h1 id=\"XGB\">\n5. XGBOOST\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#XGB\">\u00b6<\/a>\n<\/h1","546376c3":"<h1 id=\"Decision_Tree:_Entropy_Criterion\">\n3. Decision Tree: Entropy Criterion\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Decision_Tree:_Entropy_Criterion\">\u00b6<\/a>\n<\/h1>","0c06c3b2":"<h1 id=\"Decision_Tree:_Gini_Criterion\">\n2. Decision Tree: Gini Criterion\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Decision_Tree:_Gini_Criterion\">\u00b6<\/a>\n<\/h1>","f9ff345d":"<h1 id=\"Random_Forest\">\n6. Random Forest\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Random_Forest\">\u00b6<\/a>\n<\/h1>"}}