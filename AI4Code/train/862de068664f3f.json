{"cell_type":{"cf99d24a":"code","3eac6a4e":"code","b74f7801":"code","8b7431a4":"code","1ad7dda6":"code","06c4977c":"code","a6eb4fe2":"code","7b62e90e":"code","d3666328":"markdown"},"source":{"cf99d24a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \npd.set_option('display.max_columns',200)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3eac6a4e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder,StandardScaler,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_selection import SelectKBest,mutual_info_classif,f_classif,f_regression\nfrom sklearn.feature_selection import VarianceThreshold\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","b74f7801":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')  #train column\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')    #test column\n\nsubmit=pd.DataFrame(test['Id'])\n\n\n\ny=np.log(train['SalePrice'])\ntrain=train.drop('SalePrice',axis=1)","8b7431a4":"#We are dividing the dataset on basis of categorical\n\ncat=train.select_dtypes(include='object')   # \n\n\n#-------------Missing values------------------\n\nnull_cat=cat.loc[:,cat.isnull().sum()>500]\ncat=cat.drop(null_cat,axis=1)\n\n#-------------------------------------------\n\nencode=LabelEncoder()\n\nfor i in cat.columns:\n    cat[i]=encode.fit_transform(cat[i])\n\npipeline=Pipeline(steps=[('impute',KNNImputer(n_neighbors=23)),('mo',SelectKBest(score_func=f_classif,k=32))])\npipeline.fit(cat,y)\n\ntrain_cat=pd.DataFrame(pipeline.transform(cat))\n\n\n\n#--------------------------------------------------------------------------------\n\n\ncat_test=test.select_dtypes(include='object')   # test column\n\n\n#-------------Missing values------------------\n\nnull_cat=cat_test.loc[:,cat_test.isnull().sum()>500]\ncat_test=cat_test.drop(null_cat,axis=1)\n\n#-------------------------------------------\n\nencode=LabelEncoder()\n\nfor i in cat_test.columns:\n    cat_test[i]=encode.fit_transform(cat_test[i])\n\n#----------------------------------------------------------\n\ntest_cat=pd.DataFrame(pipeline.transform(cat_test))\n","1ad7dda6":"reg=train.select_dtypes(exclude='object')\nreg_test=test.select_dtypes(exclude='object')\n\n#--------------------------\n\nnull=reg.loc[:,reg.isnull().sum()>500]\nreg=reg.drop(null,axis=1)\n\n\nnull=reg_test.loc[:,reg_test.isnull().sum()>500]\nreg_test=reg_test.drop(null,axis=1)\n\n\n#-----------------------------------------\n\npipeline2=Pipeline(steps=[('impute',KNNImputer(n_neighbors=23)),('scale',StandardScaler()),('ss',SelectKBest(score_func=f_regression,k=31))])\n\npipeline2.fit(reg,y)\n\n\ntrain_reg=pd.DataFrame(pipeline2.transform(reg))\ntest_reg=pd.DataFrame(pipeline2.transform(reg_test))","06c4977c":"train_cat['Id']=train['Id']\ntrain_reg['Id']=train['Id']\noriginal_train=pd.merge(train_reg,train_cat,on='Id',how='outer')\n\n\n\ntest_cat['Id']=test['Id']\ntest_reg['Id']=test['Id']\noriginal_test=pd.merge(test_reg,test_cat,on='Id',how='outer')\n","a6eb4fe2":"model=model=XGBRegressor(base_score=0.9, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.4603, gamma=0.05,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.05, max_delta_step=0, max_depth=3,\n             min_child_weight=1.7817, monotone_constraints='()',\n             n_estimators=2200, n_jobs=4, nthread=-1, num_parallel_tree=1,\n             random_state=7, reg_alpha=0.464, reg_lambda=0.8571,\n             scale_pos_weight=1, subsample=0.5213,silent = True,tree_method='exact',\n             validate_parameters=1, verbosity=0)\n\npipeline=Pipeline(steps=[('model',model)])\n\n\npipeline.fit(original_train,y)\n\nyhat=np.exp(pipeline.predict(original_test))\nsubmit['SalePrice']=yhat\nsubmit.to_csv('ver2.csv',index=False)","7b62e90e":"original_train","d3666328":"# Regression columns"}}