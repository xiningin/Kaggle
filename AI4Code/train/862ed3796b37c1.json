{"cell_type":{"d1911e25":"code","fd974770":"code","24b133fd":"code","b7f0fc7c":"code","8f9e68b2":"code","ec34d4f9":"code","be617477":"code","aa4efd87":"code","3b6ff276":"markdown","114a7914":"markdown","b4c8aefe":"markdown","f658009e":"markdown","d4c5837b":"markdown","79fa5a00":"markdown","ce09ff52":"markdown","5566a4bd":"markdown","92da7cc9":"markdown"},"source":{"d1911e25":"# import libraries\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\n%matplotlib inline\n#for matrix math\nimport numpy as np\n#for normalization + probability density function computation\nfrom scipy import stats\n#for data preprocessing\nimport pandas as pd\nfrom math import sqrt, log, exp, pi\nfrom random import uniform\nprint(\"import done\")","fd974770":"random_seed=36788765\nnp.random.seed(random_seed)\n\nMean1 = 2.0  # Input parameter, mean of first normal probability distribution\nStandard_dev1 = 4.0 #@param {type:\"number\"}\nMean2 = 9.0 # Input parameter, mean of second normal  probability distribution\nStandard_dev2 = 2.0 #@param {type:\"number\"}\n\n# generate data\ny1 = np.random.normal(Mean1, Standard_dev1, 1000)\ny2 = np.random.normal(Mean2, Standard_dev2, 500)\ndata=np.append(y1,y2)\n\n# For data visiualisation calculate left and right of the graph\nMin_graph = min(data)\nMax_graph = max(data)\nx = np.linspace(Min_graph, Max_graph, 2000) # to plot the data\n\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nsns.distplot(data, bins=20, kde=False);","24b133fd":"class Gaussian:\n    \"Model univariate Gaussian\"\n    def __init__(self, mu, sigma):\n        #mean and standard deviation\n        self.mu = mu\n        self.sigma = sigma\n\n    #probability density function\n    def pdf(self, datum):\n        \"Probability of a data point given the current parameters\"\n        u = (datum - self.mu) \/ abs(self.sigma)\n        y = (1 \/ (sqrt(2 * pi) * abs(self.sigma))) * exp(-u * u \/ 2)\n        return y\n    \n    def __repr__(self):\n        return 'Gaussian({0:4.6}, {1:4.6})'.format(self.mu, self.sigma)\nprint(\"done\")","b7f0fc7c":"#gaussian of best fit\nbest_single = Gaussian(np.mean(data), np.std(data))\nprint('Best single Gaussian: \u03bc = {:.2}, \u03c3 = {:.2}'.format(best_single.mu, best_single.sigma))\n#fit a single gaussian curve to the data\ng_single = stats.norm(best_single.mu, best_single.sigma).pdf(x)\nsns.distplot(data, bins=20, kde=False, norm_hist=True);\nplt.plot(x, g_single, label='single gaussian');\nplt.legend();","8f9e68b2":"class GaussianMixture_self:\n    \"Model mixture of two univariate Gaussians and their EM estimation\"\n\n    def __init__(self, data, mu_min=min(data), mu_max=max(data), sigma_min=1, sigma_max=1, mix=.5):\n        self.data = data\n        #todo the Algorithm would be numerical enhanced by normalizing the data first, next do all the EM steps and do the de-normalising at the end\n        \n        #init with multiple gaussians\n        self.one = Gaussian(uniform(mu_min, mu_max), \n                            uniform(sigma_min, sigma_max))\n        self.two = Gaussian(uniform(mu_min, mu_max), \n                            uniform(sigma_min, sigma_max))\n        \n        #as well as how much to mix them\n        self.mix = mix\n\n    def Estep(self):\n        \"Perform an E(stimation)-step, assign each point to gaussian 1 or 2 with a percentage\"\n        # compute weights\n        self.loglike = 0. # = log(p = 1)\n        for datum in self.data:  \n            # unnormalized weights\n            wp1 = self.one.pdf(datum) * self.mix\n            wp2 = self.two.pdf(datum) * (1. - self.mix)\n            # compute denominator\n            den = wp1 + wp2\n            # normalize\n            wp1 \/= den   \n            wp2 \/= den     # wp1+wp2= 1, it either belongs to gaussian 1 or gaussion 2\n            # add into loglike\n            self.loglike += log(den) #freshening up self.loglike in the process\n            # yield weight tuple\n            yield (wp1, wp2)\n\n    def Mstep(self, weights):\n        \"Perform an M(aximization)-step\"\n        # compute denominators\n        (left, rigt) = zip(*weights) \n        one_den = sum(left)\n        two_den = sum(rigt)\n\n        # compute new means\n        self.one.mu = sum(w * d  for (w, d) in zip(left, data)) \/ one_den\n        self.two.mu = sum(w * d  for (w, d) in zip(rigt, data)) \/ two_den\n        \n        # compute new sigmas\n        self.one.sigma = sqrt(sum(w * ((d - self.one.mu) ** 2)\n                                  for (w, d) in zip(left, data)) \/ one_den)\n        self.two.sigma = sqrt(sum(w * ((d - self.two.mu) ** 2)\n                                  for (w, d) in zip(rigt, data)) \/ two_den)\n        # compute new mix\n        self.mix = one_den \/ len(data)\n\n        \n    def iterate(self, N=1, verbose=False):\n        \"Perform N iterations, then compute log-likelihood\"\n        for i in range(1, N+1):\n            self.Mstep(self.Estep()) #The heart of the algorith, perform E-stepand next M-step\n            if verbose:\n                print('{0:2} {1}'.format(i, self))\n        self.Estep() # to freshen up self.loglike\n\n    def pdf(self, x):\n        return (self.mix)*self.one.pdf(x) + (1-self.mix)*self.two.pdf(x)\n        \n    def __repr__(self):\n        return 'GaussianMixture({0}, {1}, mix={2.03})'.format(self.one, \n                                                              self.two, \n                                                              self.mix)\n\n    def __str__(self):\n        return 'Mixture: {0}, {1}, mix={2:.03})'.format(self.one, \n                                                        self.two, \n                                                        self.mix)\nprint(\"done\")","ec34d4f9":"# See the algorithem in action\nn_iterations = 20\nbest_mix = None\nbest_loglike = float('-inf')\nmix = GaussianMixture_self(data)\nfor _ in range(n_iterations):\n    try:\n        #train!\n        mix.iterate(verbose=True)\n        if mix.loglike > best_loglike:\n            best_loglike = mix.loglike\n            best_mix = mix\n        \n    except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n        print(\"one less\")\n        pass\n","be617477":"# Find best Mixture Gaussian model\nn_iterations = 300\nn_random_restarts = 4\nbest_mix = None\nbest_loglike = float('-inf')\nprint('Computing best model with random restarts...\\n')\nfor _ in range(n_random_restarts):\n    mix = GaussianMixture_self(data)\n    for _ in range(n_iterations):\n        try:\n            mix.iterate()\n            if mix.loglike > best_loglike:\n                best_loglike = mix.loglike\n                best_mix = mix\n        except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n            pass\n#print('Best Gaussian Mixture : \u03bc = {:.2}, \u03c3 = {:.2} with \u03bc = {:.2}, \u03c3 = {:.2}'.format(best_mix.one.mu, best_mix.one.sigma, best_mix.two.mu, best_mix.two.sigma))\n\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nprint('Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}, weight = {:.2}'.format(\"1\", best_mix.one.mu, best_mix.one.sigma, best_mix.mix))\nprint('Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}, weight = {:.2}'.format(\"2\", best_mix.two.mu, best_mix.two.sigma, (1-best_mix.mix)))\n#Show mixture\nsns.distplot(data, bins=20, kde=False, norm_hist=True);\ng_both = [best_mix.pdf(e) for e in x]\nplt.plot(x, g_both, label='gaussian mixture');\ng_left = [best_mix.one.pdf(e) * best_mix.mix for e in x]\nplt.plot(x, g_left, label='gaussian one');\ng_right = [best_mix.two.pdf(e) * (1-best_mix.mix) for e in x]\nplt.plot(x, g_right, label='gaussian two');\nplt.legend();\n","aa4efd87":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components = 2, tol=0.000001)\ngmm.fit(np.expand_dims(data, 1)) # Parameters: array-like, shape (n_samples, n_features), 1 dimension dataset so 1 feature\nGaussian_nr = 1\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nfor mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n    print('Gaussian {:}: \u03bc = {:.2}, \u03c3 = {:.2}, weight = {:.2}'.format(Gaussian_nr, mu, sd, p))\n    g_s = stats.norm(mu, sd).pdf(x) * p\n    plt.plot(x, g_s, label='gaussian sklearn');\n    Gaussian_nr += 1\nsns.distplot(data, bins=20, kde=False, norm_hist=True)\ngmm_sum = np.exp([gmm.score_samples(e.reshape(-1, 1)) for e in x]) #gmm gives log probability, hence the exp() function\nplt.plot(x, gmm_sum, label='gaussian mixture');\nplt.legend();","3b6ff276":"Daha sonra t\u00fcm veriler i\u015flemlerde kullanmak i\u00e7in yeni de\u011fi\u015fkenlere atan\u0131r. Hatal\u0131 verilerin de\u011fi\u015fkenleri en az iki tane olmal\u0131d\u0131r, \u00e7\u00fcnk\u00fc e\u011fer tek de\u011fi\u015fkenden ibaret olursa zaten ona e\u015fit olurdu. Hatal\u0131 veriler bu sebepten en az iki de\u011fi\u015fkene atanmal\u0131d\u0131r.\n\n\u00c7i: x1 N: x2 H: x3 + x4 \u00c7H: x5 (2)\n\nYukar\u0131da g\u00f6r\u00fclen \u00f6rnekte g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi hatal\u0131 veriye sahip \u2018H\u2019 verisi iki tane de\u011fi\u015fken bi\u00e7iminde yer alm\u0131\u015ft\u0131r.\n\nDaha sonra \u00f6rnek k\u00fcme \u00fczerindeki veriler incelenir ve bu verilere ait olas\u0131l\u0131k da\u011f\u0131l\u0131mlar\u0131 belirlenir. Bu da\u011f\u0131l\u0131mlar hatal\u0131 veya eksik veri olas\u0131l\u0131\u011f\u0131 olan \u201cp\u201d ile yaz\u0131l\u0131rlar. Yani burada p\u2019 yi kullanarak beklentinin makzime edilmesinin yolu a\u00e7\u0131lm\u0131\u015f olur. \u00d6rnek olarak da\u011f\u0131l\u0131m:\n\nP(\u00c7\u0130)= 0.2,  P(N)= 0.6-0.5p,  P(H)= 0.2 + 0.25p,  P(\u00c7H) = 0.25p (3)\n\nYukar\u0131daki gibi olas\u0131l\u0131k da\u011f\u0131l\u0131mlar\u0131 \u00e7\u0131kart\u0131l\u0131r ve daha sonra olas\u0131l\u0131k yo\u011funluk fonksiyonunun kurulmas\u0131na ge\u00e7ilir. Bu kurulum yukar\u0131da ki f(x,p) fonksiyonudur. Bu fonksiyonda ilk k\u0131s\u0131m yani,  faktoriyel i\u015fleminin hesaplanmas\u0131na gerek yoktur. \u00c7\u00fcnk\u00fc bu i\u015flem ilerde g\u00f6r\u00fclece\u011fi gibi t\u00fcrev alma i\u015fleminden sonra \u201c0\u201d a e\u015fit olacakt\u0131r. Bunun yerine i\u015flem kolayl\u0131\u011f\u0131 a\u00e7\u0131s\u0131ndan \u201cA\u201d demek yeterli olacakt\u0131r.\n\nOlas\u0131l\u0131k da\u011f\u0131l\u0131m\u0131nda dikkat edilirse baz\u0131 parametreler ayn\u0131 \u00fcst alt\u0131nda yaz\u0131lm\u0131\u015ft\u0131r. Bunun sebebi de\u011fi\u015fken atamad\u0131r. De\u011fi\u015fken atamalardan sonra ayn\u0131 k\u00f6ke sahip k\u00f6kler \u00e7\u0131kacakt\u0131r. Yani:\n\nP(x1) = 0.2 , P(x2) = 0.6-0.5p , P(x3) = 0.2 , P(x4) = 0.25 , P(x5) = 0.25p \u2018dir.\n\nBurada g\u00f6r\u00fclece\u011fi gibi x1 ile x2 , x4 ile x5 ayn\u0131 k\u00f6klere sahiptir. Bu da olas\u0131l\u0131k yo\u011funluk fonksiyonu \u00fczerinde g\u00f6r\u00fclebilir.\n\nOlas\u0131l\u0131k yo\u011funluk fonksiyonunun bu \u015fekliyle i\u015flemlerde kullan\u0131lmas\u0131 olduk\u00e7a zordur. \u0130\u015flemleri kolayla\u015ft\u0131rma i\u00e7in bu fonksiyonun do\u011fal logaritmas\u0131 al\u0131n\u0131r.\n\nln(f(x,p)) =ln A+ (x1+x3) ln() + x2 ln(0.6-0.5p) + (x4+x5) ln(0.25p)\n\nDaha sonra literat\u00fcrde E-Step denilen beklenti ad\u0131m\u0131na ge\u00e7ilir. Bu ad\u0131mda olas\u0131l\u0131k yo\u011funluk fonksiyonun beklentisi al\u0131n\u0131r. Fonksiyon \u00fczerinde hatal\u0131 veriye ait de\u011fi\u015fkenler hari\u00e7 di\u011ferlerinin hepsi bilinir. Yani beklenti ad\u0131m\u0131ndan sonra:\n\nE(ln(f(x,p))) = ln A+ (x1+x3) ln() + x2 ln(0.6-0.5p) + (+x5) ln(0.25p)\n\nFonksiyon bu hale gelecektir. Burada x3 ve x4 haricindeki b\u00fct\u00fcn x\u2019ler bilinmektedir. Bu x\u2019ler problemin tan\u0131m\u0131nda bilinene de\u011fi\u015fkenlerdir. Yine burada k\u2019l\u0131 yaz\u0131m\u0131n sebebi k. iterasyonu vurgulamakt\u0131r.\n\nDaha sonra bu fonksiyonun  t\u00fcrevi al\u0131n\u0131r ve sonu\u00e7 s\u0131f\u0131ra e\u015fitlenir. \u0130\u015flem i\u00e7indeki p\u2019ler yaln\u0131z b\u0131rak\u0131l\u0131r. Bu ad\u0131ma literat\u00fcrde M-Step yani makzimizasyon ad\u0131m\u0131 denir. Ama\u00e7 yak\u0131nsayan \u201cp\u201d de\u011ferleri i\u00e7in bilinmeyen parametrelerin bulunmas\u0131d\u0131r.","114a7914":"# The code for EM with 2 Gaussian mixture model","b4c8aefe":"# Expectation Maximization\n\nSource 1: [Can Erol](https:\/\/echo108.wordpress.com\/2009\/06\/28\/beklenti-makzimizasyonu-algoritmasi\/)\nSource 2: [Charel van Hoof](https:\/\/www.kaggle.com\/charel\/learn-by-example-expectation-maximization)\n\n\n\nBeklenti makzimizasyonu algoritmas\u0131, literat\u00fcrde Expectation Maksimation (EM) diye ge\u00e7mektedir. Bu algoritman\u0131n amac\u0131, \u00f6rnek k\u00fcme \u00fczerinde baz\u0131 tespitler yap\u0131l\u0131r ancak bunlardan bir k\u0131sm\u0131 eksik veya hatal\u0131 parametrelerle temsil edildi\u011fi durumlarda kullan\u0131labilir. \u00d6zellikle istatistiksel uygulamalar ile istatistiksel \u00f6r\u00fcnt\u00fc tan\u0131ma sistemlerinde olduk\u00e7a kullan\u0131labilen bir y\u00f6ntemdir.\n\nTan\u0131m yaparken \u00fczerini vurgulad\u0131\u011f\u0131m gibi bu algoritma istatistiksel y\u00f6ntemlere dayanmaktad\u0131r. Bu algoritmay\u0131 uygulamak i\u00e7in ilk\u00f6nce hatal\u0131 veya eksik olan verilen tespit edilir. Daha sonra ise tespit edilen parametrelerin \u00f6rnek k\u00fcme \u00fczerindeki b\u00fcy\u00fckl\u00fckleri yani tespit edilen parametreler ka\u00e7 elemana ait oldu\u011fu \u00e7\u0131kart\u0131l\u0131r. \u00c7\u0131kart\u0131lan bu say\u0131lar\u0131n kullan\u0131m amac\u0131 \u00f6rnek k\u00fcmeye ait olas\u0131l\u0131k yo\u011funluk fonksiyonun i\u00e7inde kullan\u0131laca\u011f\u0131ndand\u0131r. Bu t\u00fcr da\u011f\u0131l\u0131mlar tekrar eden da\u011f\u0131l\u0131mlard\u0131r.  Benzer bir uygulama olarak:\n\n\n![](https:\/\/echo108.files.wordpress.com\/2009\/06\/fx.jpg?w=307&h=53&zoom=2)\n\n","f658009e":"![](https:\/\/echo108.files.wordpress.com\/2009\/06\/11.jpg?w=307&h=111&zoom=2)\n\nBurada i\u015flem i\u00e7erisinde bulunan x4 de\u011feri problem \u00fczerinde ki olas\u0131l\u0131k da\u011f\u0131l\u0131mlar\u0131ndan yola \u00e7\u0131k\u0131larak bulunur. Bu ama\u00e7la ilk\u00f6nce birden k\u00fc\u00e7\u00fck olan bir p de\u011feri verilir. Daha sonra:","d4c5837b":"![X(k+1)'in Bulunusu](https:\/\/echo108.files.wordpress.com\/2009\/06\/2.jpg?w=200&h=83&zoom=2)\n\nBurada -2.6 de\u011feri hatal\u0131 verinin ba\u015flang\u0131\u00e7 de\u011feridir. 0.25p de\u011feri ise olas\u0131l\u0131k oran\u0131ndan kaynaklanmaktad\u0131r. X4 \u00fcn olas\u0131l\u0131k oran\u0131na 0.25p d\u00fc\u015fmektedir. Bunun yan\u0131 s\u0131ra oran i\u00e7in hatal\u0131 veriye d\u00fc\u015fen t\u00fcm olas\u0131l\u0131\u011fa b\u00f6l\u00fcnm\u00fc\u015flerdir.    Burada bulunan  de\u011feri , Pk+1 de\u011ferinin bulunmas\u0131 i\u00e7in kullan\u0131l\u0131r. Bu i\u015flemler belirli bir noktadan sonra sabit de\u011ferler almaya ba\u015flar. \u0130stenilen de\u011ferler o sabit de\u011ferler olacakt\u0131r. Algoritman\u0131n genel \u015fekli:\n\n![](https:\/\/echo108.files.wordpress.com\/2009\/06\/3.jpg?w=310&zoom=2)\n","79fa5a00":"Source 1: [Can Erol](https:\/\/echo108.wordpress.com\/2009\/06\/28\/beklenti-makzimizasyonu-algoritmasi\/)\nSource 2: [Charel van Hoof](https:\/\/www.kaggle.com\/charel\/learn-by-example-expectation-maximization)\n\n\n","ce09ff52":"# A single Gaussion will not fit the data well\nCalculating the mean and standard deviation of the dataset shows it does not fit well","5566a4bd":"# Results\nThe models nicely estimates our own mean entered \u03bc and \u03c3. With this understanding it is not hard to imagine to extend this to n-Gaussians or m-dimensions. You don't need to write complax code since these algorithms have been implemented in some excellent libraries.\n\n### Further enhancements to the code\nThe Algorithm would be numerical enhanced by normalizing the data first, next do all the EM steps and do the de-normalising at the end. For me (and guess others) I needed to get to base-camp first and get the EM steps understood. \n\n## sklearn GaussianMixture \nOr we could make use of a library that already has the functionality implemented. The sklearn GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.  A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from train data. Now you can try yourself with n-Gaussians or m-dimensions.","92da7cc9":"![](http:\/\/)# Generate the data yourself \nSelect $\\mu_1, \\sigma_1$ and $\\mu_2, \\sigma_2$ to generate the data\n"}}