{"cell_type":{"aeb00174":"code","852c35fc":"code","0f7a0cbf":"code","75239023":"code","bd5f259b":"code","108cc94d":"code","210ea254":"code","c4a954a8":"code","c90f70f7":"code","87d89992":"code","1a09f928":"code","444c6ade":"code","f4c0c20e":"code","046110d9":"code","49395129":"code","e66467cf":"code","f0ab59a0":"code","1a39654d":"code","fc02a2a5":"code","5d00a78c":"code","c8049856":"code","0f3ad789":"code","c11e5cad":"code","bc97f4c7":"code","53f189d0":"code","f88da7bd":"code","8b333a1a":"code","78e02dad":"code","64da1c87":"markdown","e9f88208":"markdown","997599c0":"markdown","4a9247d2":"markdown","c5ecf2dc":"markdown","d0d6c5dc":"markdown","3a0ae523":"markdown","384fa6c5":"markdown","3327876f":"markdown"},"source":{"aeb00174":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","852c35fc":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\nfrom scipy.optimize import minimize\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\nfrom tqdm import tqdm_notebook\n\nfrom itertools import product\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","0f7a0cbf":"datapath = '..\/input\/tesla-stock-data-from-2010-to-2020\/TSLA.csv'\n\ndata = pd.read_csv(datapath)\ndata.head(10)","75239023":"data.describe()","bd5f259b":"data.isnull().values.sum()","108cc94d":"# Convert the Date column to DateTime object\ndata['Date'] = pd.to_datetime(data['Date'])","210ea254":"# Disable the scientific notation to understand figures better\npd.set_option('display.float_format', lambda x: '%.2f' % x)","c4a954a8":"# A glimpse of how the market shares varied over the given time\n\n# Create a list for numerical columns that are to be visualized\nColumn_List = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n\n\n# Plot to view the same\ndata.plot(x = 'Date', y = Column_List, subplots = True, layout = (3, 3), figsize = (15, 15), sharex = False, title = \"Stock Value Trend from 2010 - 2012\", rot = 90)","c90f70f7":"# Visualize the spread and skweness through the distribution plot\n\n# Use the Column_List : list initialized above in the following steps\nfig, ax = plt.subplots(len(Column_List), figsize = (15, 10))\n\nfor i, col_list in enumerate(Column_List):\n    sns.distplot(data[col_list], hist = True, ax = ax[i])\n    ax[i].set_title (\"Frequency Distribution of\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_xlabel (col_list, fontsize = 8)\n    ax[i].set_ylabel ('Distribution Value', fontsize = 8)\n    fig.tight_layout (pad = 1.1) # To provide space between plots\n    ax[i].grid('on') # Enabled to view and make markings","87d89992":"# Check for factors responsible in overall volume trade\nfig, ax = plt.subplots (figsize = (10, 10))\ncorr_matrix = data.corr() # Perform default correlation using Pearson Method \n\n# Plot the correlation matrix in a heatmap to understand better\nsns.heatmap(corr_matrix, xticklabels = corr_matrix.columns.values, yticklabels = corr_matrix.columns.values)","1a09f928":"# View the matrix in a table to identify the numerical values of strengths\ncorr_matrix","444c6ade":"# Plot closing price\n\nplt.figure(figsize=(17, 8))\nplt.plot(data.Date, data.Close)\nplt.title('Closing price of Tesla')\nplt.ylabel('Closing price ($)')\nplt.xlabel('Trading day')\nplt.grid(True)\nplt.show()","f4c0c20e":"# Generate whisker plots to detect the presence of any outliers\nfig, ax = plt.subplots (len(Column_List), figsize = (10, 20))\n\nfor i, col_list in enumerate(Column_List):\n    sns.boxplot(data[col_list], ax = ax[i], palette = \"winter\", orient = 'h')\n    ax[i].set_title(\"Whisker Plot for Outlier Detection on\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_ylabel(col_list, fontsize = 8)\n    fig.tight_layout(pad = 1.1)","046110d9":"# it is clear from the whisker plots that there are some outliers in all the variables\nfrom scipy import stats\n\n# Remove the variables \nDescriptive_Statistics = data.describe()\nDescriptive_Statistics = Descriptive_Statistics.T # Convert into a dataframe\n\n# Extract the IQR values \nDescriptive_Statistics['IQR'] = Descriptive_Statistics['75%'] - Descriptive_Statistics['25%']\n\n# In this scenario, the outliers are removed using Z-Score due to the variability in historical data\ndata = data[(np.abs(stats.zscore(data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])) < 3).all(axis = 1)] # abs for (+\/-) 3-sigma\ndata = data.reset_index() # Due to elimination of rows, index has to be reset\n\n# Now compare the new dimension with the old one (The one mentioned during the reading of the file)\ndata # 2359 * 8","49395129":"# Since the data is a time series data, we should be able to predict the future through forecasting techniques\n\n# Delete the index column due to reset\n# del data['index']\n\n# Decompose the time series year-wise and month-wise to analyse further\ndata['Year'] = data['Date'].dt.year\ndata['Month'] = data['Date'].dt.month\ndata['WeekDay'] = data['Date'].dt.weekday\n\n# Firstly, plot the data year-wise to see the duration of when it hiked and dipped\nfig, ax = plt.subplots(len(Column_List), figsize = (10, 20))\n\n# Group the data by year and plot\nfor i, col_list in enumerate(Column_List):\n    data.groupby('Year')[col_list].plot(ax = ax[i], legend = True)\n    ax[i].set_title(\"Stock Price Movement Grouped by Year on\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_ylabel(col_list + \" \" + \"Price\", fontsize = 8)\n    ax[i].set_xlabel('Date')\n    fig.tight_layout(pad = 1.1)\n    ax[i].yaxis.grid(True) # To enable grid only on the Y-axis","e66467cf":"# Visualzing only the total volume of stocks traded grouped year-wise\ncheck = data.groupby('Year')['Volume'].sum()\nplt.figure(figsize = (30, 4))\nax1 = plt.subplot(121)\ncheck.plot(y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Total Volume of Stocks Traded Year-wise from 2010 - 2020\", rot = 90, color = \"green\")\nax1.ticklabel_format(useOffset = False, style = 'plain')\nax1.set_ylabel(\"Total Stock Volumes\")\nax1.yaxis.grid(True)\n\n# Visualzing only the total volume of stocks traded grouped month-wise\ncheck = data.groupby('Month')['Volume'].sum()\nplt.figure(figsize = (30, 4))\nax1 = plt.subplot(121)\ncheck.plot(y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Total Volume of Stocks Traded Month-wise from 2010 - 2020\", rot = 90, color = \"blue\")\nax1.ticklabel_format(useOffset = False, style = 'plain')\nax1.set_ylabel(\"Total Stock Volumes\")\nax1.yaxis.grid(True)\n\n# Visualzing only the total volume of stocks traded grouped weekday-wise\ncheck = data.groupby('WeekDay')['Volume'].sum()\nplt.figure(figsize = (30, 4))\nax1 = plt.subplot(121)\ncheck.plot(y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Total Volume of Stocks Traded WeekDay-wise from 2010 - 2020\", rot = 90, color = \"red\")\nax1.ticklabel_format(useOffset = False, style = 'plain')\nax1.set_ylabel(\"Total Stock Volumes\")\nax1.yaxis.grid(True)","f0ab59a0":"from pandas.plotting import table\n\n# Analyse based on Year\nfor i, col_list in enumerate(Column_List):\n    var = data.groupby('Year')[col_list].sum()\n    \n# Convert the variable into a pandas dataframe\nvar = pd.DataFrame(var)\n\n# Plot to understand the trend\nplt.figure(figsize = (16, 7))\nax1 = plt.subplot(121)\nvar.plot(kind = \"pie\", y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Time Series Influence on Total Volume Trade by Year\", ax = ax1)\n\n# Plot the table to identify numbers\nax2 = plt.subplot(122)\nplt.axis('off') # Since we are plotting the table\ntbl = table(ax2, var, loc = 'center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(12)\nplt.show()","1a39654d":"# Analyse based on Month\nfor i, col_list in enumerate(Column_List):\n    var = data.groupby('Month')[col_list].sum()\n    \n# Convert the variable into a pandas dataframe\nvar = pd.DataFrame(var)\n\n# Plot to understand the trend\nplt.figure(figsize = (16, 7))\nax1 = plt.subplot(121)\nvar.plot(kind = \"pie\", y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Time Series Influence on Total Volume Trade by Month\", ax = ax1)\n\n# Plot the table to identify numbers\nax2 = plt.subplot(122)\nplt.axis('off') # Since we are plotting the table\ntbl = table(ax2, var, loc = 'center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(12)\nplt.show()","fc02a2a5":"# Analyse based on WeekDay\nfor i, col_list in enumerate(Column_List):\n    var = data.groupby('WeekDay')[col_list].sum()\n    \n# Convert the variable into a pandas dataframe\nvar = pd.DataFrame(var)\n\n# Plot to understand the trend\nplt.figure(figsize = (16, 7))\nax1 = plt.subplot(121)\nvar.plot(kind = \"pie\", y = \"Volume\", legend = False, fontsize = 12, sharex = False, title = \"Time Series Influence on Total Volume Trade by WeekDay\", ax = ax1)\n\n# Plot the table to identify numbers\nax2 = plt.subplot(122)\nplt.axis('off') # Since we are plotting the table\ntbl = table(ax2, var, loc = 'center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(12)\nplt.show()","5d00a78c":"def plot_moving_average(series, window, plot_intervals=False, scale=1.95):\n    rolling_mean = series.rolling(window=window).mean()\n    \n    plt.figure(figsize=(17,8))\n    plt.title('Moving average\\n window size = {}'.format(window))\n    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n    \n    #plot confidence intervals for smoothning\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bound = rolling_mean - (mae + scale * deviation)\n        upper_bound = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bound, 'r--', label='Upper bound \/ Lower bound')\n        plt.plot(lower_bound, 'r--')\n        \n    plt.plot(series[window:], label='Actual values')\n    plt.legend(loc='best')\n    plt.grid(True)\n    \n#smooth by the previous 7 days(a week)\nplot_moving_average(data.Close, 7)\n\n#by previous month\nplot_moving_average(data.Close, 30)\n\n#by quater year\nplot_moving_average(data.Close, 90, plot_intervals=True)","c8049856":"\ndef exponential_smoothing(series, alpha):\n\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n  \ndef plot_exponential_smoothing(series, alphas):\n \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n    plt.plot(series.values, \"c\", label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Exponential Smoothing\")\n    plt.grid(True);\n\nplot_exponential_smoothing(data.Close, [0.0091, 0.03])","0f3ad789":"def double_exponential_smoothing(series, alpha, beta):\n\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n        trend = beta * (level - last_level) + (1 - beta) * trend\n        result.append(level + trend)\n    return result\n\ndef plot_double_exponential_smoothing(series, alphas, betas):\n     \n    plt.figure(figsize=(17, 8))\n    for alpha in alphas:\n        for beta in betas:\n            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n    plt.plot(series.values, label = \"Actual\")\n    plt.legend(loc=\"best\")\n    plt.axis('tight')\n    plt.title(\"Double Exponential Smoothing\")\n    plt.grid(True)\n    \nplot_double_exponential_smoothing(data.Close, alphas=[0.09, 0.0079], betas=[0.9, 0.02])","c11e5cad":"def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n    \n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style='bmh'):\n        fig = plt.figure(figsize=figsize)\n        layout = (2,2)\n        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1,0))\n        pacf_ax = plt.subplot2grid(layout, (1,1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\n        \ntsplot(data.Close, lags=30)\n\n# Take the first difference to remove to make the process stationary\ndata_diff = data.Close - data.Close.shift(1)\n\ntsplot(data_diff[1:], lags=30)","bc97f4c7":"#Set initial values and some bounds\nps = range(0, 5)\nd = 1\nqs = range(0, 5)\nPs = range(0, 5)\nD = 1\nQs = range(0, 5)\ns = 5\n\n#Create a list with all possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Train many SARIMA models to find the best set of parameters\ndef optimize_SARIMA(parameters_list, d, D, s):\n    \"\"\"\n        Return dataframe with parameters and corresponding AIC\n        \n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order\n        D - seasonal integration order\n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float('inf')\n    \n    for param in tqdm_notebook(parameters_list):\n        try: model = sm.tsa.statespace.SARIMAX(data.Close, order=(param[0], d, param[1]),\n                                               seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n            \n        aic = model.aic\n        \n        #Save best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n        \n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    #Sort in ascending order, lower AIC is better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table\n\nresult_table = optimize_SARIMA(parameters_list, d, D, s)\n\n#Set parameters that give the lowest AIC (Akaike Information Criteria)\np, q, P, Q = result_table.parameters[0]\n\nbest_model = sm.tsa.statespace.SARIMAX(data.Close, order=(p, d, q),\n                                       seasonal_order=(P, D, Q, s)).fit(disp=-1)\n\nprint(best_model.summary())","53f189d0":"import pickle\nfilename = '_model.sav'\npickle.dump(best_model, open(filename, 'wb'))","f88da7bd":"best_model.plot_diagnostics(figsize=(15,12))","8b333a1a":"import pickle\nfilename = '..\/input\/arima-model\/_model.sav'\nwith open(filename, 'rb') as file:  \n    Model = pickle.load(file)\n\n","78e02dad":"data['arima_model'] = Model.fittedvalues\ndata['arima_model'][:4+1] = np.NaN\n\nforecast = Model.predict(start=data.shape[0], end=data.shape[0] + 8)\nforecast = data['arima_model'].append(forecast)\n\nplt.figure(figsize=(15, 7.5))\nplt.plot(forecast, color='r', label='model')\nplt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\nplt.plot(data['Close'], label='actual')\nplt.legend()\n\nplt.show()","64da1c87":"Analysis on Historical data to find any info","e9f88208":"INFO GAINED:--\n\n1. It is seen that the stock volume trade spiked from 2013\n2. Data for 2020 can be considered invalid as it is very less data\n3. The histogram distribution shows that the data is skewed to the left (Indication of values range between 0 - 400)\n4. It is very evident that Open, Close, High, Low, Adj Close stock values are highly collinear and hence have a very strong relationship\n5. The whisker plot indicates the presence of outliers. the outliers are removed based on standard techniques \n6. When viewed by Year, the spike increased from 2012 through to 2020 thereby undergoing a dip in 2016\n7. When viewed by Month, (January, June, July, August and September) showed extensive dip in stock trades whereas the other months showed substantial increase in trade\n8. When viewed by Day,start of the week(Mon & Tue) seems to have recorded maximum trades rather than the rest\n","997599c0":"Series is now stationary\n\nApply SARIMA","4a9247d2":"Exponential smoothing","c5ecf2dc":"Double exponential smoothing","d0d6c5dc":"Moving average","3a0ae523":"Outlier detection (& removal)","384fa6c5":"PieCharts!!","3327876f":"Modelling\n\nwe must turn our series into a stationary process in order to model it. To see if its a stationary process we'll apply Dickey-Fuller test"}}