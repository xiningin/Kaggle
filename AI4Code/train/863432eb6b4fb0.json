{"cell_type":{"cbfa9ce2":"code","d47de0d4":"code","e6011377":"code","6e7cb579":"code","02d31905":"code","56cc3a1e":"code","218e2cbf":"code","4f661ed0":"code","db94daa1":"code","44c83b31":"code","a2a5a22d":"code","a8173509":"code","bd24dc29":"code","fd77567a":"code","eac2f960":"code","f441b31f":"code","9f60d078":"code","8c33a77f":"code","de822b3f":"code","d64ea1d5":"markdown","13848e31":"markdown","5680ec2a":"markdown","0b1fa2a8":"markdown","52571a12":"markdown","5988bb4c":"markdown"},"source":{"cbfa9ce2":"# importing the stuff\nfrom pathlib import Path\nimport numpy as np \nimport pandas as pd \nimport lightgbm as lgbm\nimport optuna\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nsns.set_theme()","d47de0d4":"# reading the data from the directory \npath = Path('\/kaggle\/input\/house-prices-advanced-regression-techniques\/')\ntrain_ = pd.read_csv(path.joinpath('train.csv'))\ntest_ = pd.read_csv(path.joinpath('test.csv'))","e6011377":"# there are some other categorical feature those have the data type of int64, but here i consider only those have datatype of object\ncategorical_feature = []\nfor col in train_.columns.tolist():\n    if train_[col].dtype==object: \n        categorical_feature.append(col)","6e7cb579":"nan_train_cols = train_.columns[1:-1][(train_.iloc[:,1:-1].isnull().sum() > 0)].tolist()\nfor col in nan_train_cols:\n    if col in categorical_feature:\n        train_.loc[train_[col].isna(),col] = train_.loc[~train_[col].isna(),col].mode().values[0]\n    else:\n        train_.loc[train_[col].isna(),col] = train_.loc[~train_[col].isna(),col].mean()","02d31905":"nan_test_cols = test_.columns[1:-1][(test_.iloc[:,1:-1].isnull().sum() > 0)].tolist()\nfor col in nan_test_cols:\n    if col in categorical_feature:\n        test_.loc[test_[col].isna(),col] = test_.loc[~test_[col].isna(),col].mode().values[0]\n    else:\n        test_.loc[test_[col].isna(),col] = test_.loc[~test_[col].isna(),col].mean()","56cc3a1e":"columns_drop = ['GarageCars','GarageYrBlt','GrLivArea','Id', 'YearRemodAdd']\ntrain_.drop(columns=columns_drop,inplace=True)\ntest_.drop(columns=columns_drop,inplace=True)","218e2cbf":"all_ = pd.concat([train_,test_])\ndumy = pd.get_dummies(all_[categorical_feature])\nall_ = pd.concat([all_.loc[:,~all_.columns.isin(categorical_feature)], dumy],axis=1)\n\n# selecting the training and testing data from full dataset\ntrain_data = all_.iloc[0:1460,:] \ntest_data = all_.iloc[1460:,:]","4f661ed0":"# freeing up some memory\ndel train_, test_, dumy, all_\ngc.collect()","db94daa1":"test_data.drop(columns=['SalePrice'],inplace=True) # initially test dataset did not have the SalePrice column it comes when we concatenated both train_ and test_dataset\nfeature_cols = train_data.columns.tolist() # converting pd.Index to python list\nfeature_cols.remove('SalePrice') # removing the SalePrice column from the feature list on which our LightGBM model shall be trained","44c83b31":"train_data['SalePrice'] = np.log(train_data['SalePrice'])  # as our model will be evaluated at the RMSE of the logarithm of the predicted and true values, lets convert them to log\ntrain_data, validation_data = train_test_split(train_data,test_size=0.2,random_state=42) # spliting the train and test, 20% for the validation purposes\ntrain_data.reset_index(drop=True, inplace=True)\nvalidation_data.reset_index(drop=True, inplace=True)","a2a5a22d":"baseline = lgbm.LGBMRegressor()\nbaseline.fit(train_data[feature_cols], train_data['SalePrice'])\nbaseline_val_y = baseline.predict(validation_data[feature_cols])\nbase_line_score = np.sqrt(mean_squared_error(baseline_val_y, validation_data['SalePrice'].values))\nprint(f'The base line score is : {base_line_score}')","a8173509":"def objective(trial,x_train,y_train,x_valid, y_valid):\n    train_d = lgbm.Dataset(x_train,y_train)\n    val_d = lgbm.Dataset(x_valid,y_valid)\n    param = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': trial.suggest_categorical('boosting_type',['gbdt','rf','dart']),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 10_000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 500),\n    }\n\n    gbm = lgbm.train(param, train_d,valid_sets = val_d,verbose_eval=100)\n    off = gbm.predict(x_valid)\n    error = mean_squared_error(y_valid,off)\n    return np.sqrt(error)\nx_train, x_val = train_data[feature_cols].values, validation_data[feature_cols].values\ny_train, y_val = train_data['SalePrice'].values, validation_data['SalePrice'].values\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: objective(trial, x_train, y_train, x_val, y_val), n_trials=1_000)","bd24dc29":"param = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': 'gbdt'}\nparam.update(study.best_trial.params) # getting the hyperparameters of the best trial which has the minimum validation loss\nprint(param)","fd77567a":"baseline_best_pram = lgbm.LGBMRegressor(**param)\nbaseline_best_pram.fit(train_data[feature_cols],train_data['SalePrice'])\nval_predict = baseline_best_pram.predict(validation_data[feature_cols]);\nbest_val_score = np.sqrt(mean_squared_error(val_predict,validation_data['SalePrice'].values))\nprint(f'Optimized parameter validation score: {best_val_score}')\nprint(f'Improvement from baseline: {base_line_score - best_val_score}')","eac2f960":"n_folds = 5;\nfolds_ = KFold(n_splits=n_folds)\nfor fold_idx, (tr_idx,val_idx) in enumerate(folds_.split(train_data)):\n    train_data.loc[val_idx,'fold'] = int(fold_idx)","f441b31f":"X_test = test_data[feature_cols].values\npreds = np.zeros((X_test.shape[0]))\nbest_val_loss = 1e9;\nbest_model = 0;\navg_val_loss = 0;\nn_folds = 5;\nval_estimate = np.zeros(y_val.shape)\nfor fold_idx in range(n_folds):\n    cur_train, cur_val = train_data[train_data['fold']!=int(fold_idx)],train_data[train_data['fold']==int(fold_idx)]\n    x_train,x_valid = cur_train[feature_cols].values,cur_val[feature_cols].values\n    y_train,y_valid = cur_train['SalePrice'].values, cur_val['SalePrice'].values\n    xdata = lgbm.Dataset(x_train,y_train)\n    xvl = lgbm.Dataset(x_valid, y_valid)\n    gbm = lgbm.train(param, xdata, valid_sets = xvl,verbose_eval=100)\n    off = gbm.predict(x_valid)\n    \n    val_loss = np.sqrt(mean_squared_error(off,y_valid))\n    avg_val_loss += val_loss\/n_folds\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss;\n        best_model = gbm\n        #print(f'Best loss: {best_val_loss}, and Saving best_model')\n    preds += gbm.predict(X_test)\/n_folds \n    val_estimate+=gbm.predict(x_val)\/n_folds","9f60d078":"print(f'5-fold validation score: {np.sqrt(mean_squared_error(y_val, val_estimate))}')","8c33a77f":"val_data = pd.DataFrame(np.asarray([y_val, val_estimate]).T, columns = ['ground_truth', 'estimate'])\nsns.displot(val_data, multiple='stack',kde=True)","de822b3f":"subm = pd.read_csv(path.joinpath('sample_submission.csv'))\nsubm['SalePrice'] = np.exp(preds)\nsubm.to_csv('submission.csv',index=False)","d64ea1d5":"# Selecting only one feature among highly correlated features\nIn [notebook](https:\/\/www.kaggle.com\/aeyeee\/eda-for-the-beginners) we made the following observations and based on this we shall drop some of the highly correlated features\n<html>\n    <body>\n             <table style=\"width:100%\">\n          <tr>\n            <th>first<\/th>\n            <th>second<\/th>\n            <th>coef<\/th>\n          <\/tr>\n          <tr>\n            <td>GarageArea<\/td>\n            <td>GarageCars<\/td>\n            <td>0.882475<\/td>\n          <\/tr>\n          <tr>\n            <td>GarageYrBlt<\/td>\n            <td>YearBuilt<\/td>\n            <td>0.825667<\/td>\n          <\/tr>\n               <tr>\n                   <td>TotRmsAbvGrd<\/td>\n                   <td>GrLivArea<\/td>\n                   <td>0.825489<\/td>\n                 <\/tr>\n        <\/table> \n            <ul>\n                <li>In the first row <b>coef<\/b> has the value of <b>0.882475<\/b> between <b>GarageArea<\/b> and <b>GarageCars<\/b>, these two features are correlated as the higher the garage area more cars can you park in.\n        <\/li>\n                <li>In the second row <b>coef<\/b> has the value of <b>0.825667<\/b> between <b>GarageYrBlt<\/b> and <b>YearBuilt<\/b>, these two features are highly correlated because it is highly likely that one build a garage at the same time when the house was built.<\/li>\n                <li>Same is true for the third row in which we have <b>0.825489<\/b> value of the <b>coef<\/b> which can be explained in the same way. <\/li>\n        <\/ul>\n        While feeding these features to ML model one can only select the one of among two highly correlated features, because they do not give any extra information to the model.\n    <\/body>\n<\/html>","13848e31":"# Strategy for handling the missing values in the training data\n    - selecting the columns in the training dataset those having some missing values\n    - if the column is a categorical we shall place missing values by the mode of the remaining values\n    - if the column is not a categorical we shall place missing values by the mean of the remaining values\n### Explaining the code\n  pd.DataFrame() which are train_ and test_ in our case has a function called isna()  which can also be operated on single column can be used to access the indices of the columns having NaNs\n  \n  For example there is a column called **Alley** in the train_ dataframe with >90% of the entries being NaNs, and we want to replace those NaNs by the most frequent (mode) of the rest of the training rows, our code goes as follows,\n  \n      - train_[train_['Alley'].isna(),'Alley'] = train_[~train_['Alley'].isna(),'Alley'].mode().values[0]\n In the above one-line command we first select those rows in **Alley** which are not NaNs (isna() gives true for the locations that are NaNs by placing ~ (tilde) sign complements the decision and let us select the rows that are not NaNs), and mode() calculates the most occuring element and values[0] convert the pandas series to numpy array and access the element at the index 0.","5680ec2a":"# Optimizing hyperparameters of the LightGBM model using Optuna","0b1fa2a8":"# Visualizing the true distribution and the estimated distribution of the validation data","52571a12":"# K-fold cross-validation (CV)\nLets split the training data into 5-folds to see if we can improve any performance.\nThe following figure from **sklearn** official website explains k-fold CV graphically,\n<html>\n    <img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_002.png\" alt=\"kfold_image\">\n    <\/html>","5988bb4c":"#### This notebook is created to show the optimization of the hyperparameters of the LightGBM model\n    - One can further improve the performance by doing some feature engineering."}}