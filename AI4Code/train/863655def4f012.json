{"cell_type":{"a7a9c25c":"code","bc1748a3":"code","3b00819d":"code","e8cda889":"code","9a12f6f4":"code","9b9e2aca":"code","49177a23":"code","217d22c0":"code","9f9dc3c5":"code","65ad4809":"code","76cb7472":"code","c42005a4":"code","ce9f5dcd":"code","fd8771c1":"code","01618bdb":"code","058e78b6":"code","047b41e9":"code","76ed3647":"code","ee862132":"code","0d706fd5":"code","2db6d748":"code","de062ef6":"code","06284bc6":"code","051573ee":"code","af4180ee":"code","2946377f":"code","3ac06344":"code","1f4aa86c":"code","6a450f23":"code","64c7dac9":"code","fffcc384":"markdown","6f8f201d":"markdown","275113a6":"markdown","af4c206a":"markdown","12dc8018":"markdown","c35ba311":"markdown","4b7c7e34":"markdown","7b0f5d72":"markdown","21e7f156":"markdown","91526af4":"markdown","3042c023":"markdown","df25a475":"markdown","faaa54a4":"markdown","6183320e":"markdown","f56e0699":"markdown","13e0e440":"markdown","a8585424":"markdown","811aa0ca":"markdown"},"source":{"a7a9c25c":"# <!-- collapse=True -->\n# importando modulos necesarios\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats \nimport seaborn as sns \n\nnp.random.seed(2016) # replicar random\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (8, 4)})","bc1748a3":"# Graficando histograma\nmu, sigma = 0, 0.2 # media y desvio estandar\ndatos = np.random.normal(mu, sigma, 1000) #creando muestra de datos\n\n# histograma de distribuci\u00f3n normal.\ncuenta, cajas, ignorar = plt.hist(datos, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma')\nplt.show()","3b00819d":"# Graficando Poisson\nmu =  3.6 # parametro de forma \npoisson = stats.poisson(mu) # Distribuci\u00f3n\nx = np.arange(poisson.ppf(0.01),\n              poisson.ppf(0.99))\nfmp = poisson.pmf(x) # Funci\u00f3n de Masa de Probabilidad\nplt.plot(x, fmp, '--')\nplt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\nplt.title('Distribuci\u00f3n Poisson')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","e8cda889":"# histograma\naleatorios = poisson.rvs(1000)  # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Poisson')\nplt.show()","9a12f6f4":"# Graficando Binomial\nN, p = 30, 0.4 # parametros de forma \nbinomial = stats.binom(N, p) # Distribuci\u00f3n\nx = np.arange(binomial.ppf(0.01),\n              binomial.ppf(0.99))\nfmp = binomial.pmf(x) # Funci\u00f3n de Masa de Probabilidad\nplt.plot(x, fmp, '--')\nplt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\nplt.title('Distribuci\u00f3n Binomial')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","9b9e2aca":"# histograma\naleatorios = binomial.rvs(1000)  # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Binomial')\nplt.show()","49177a23":"# Graficando Geom\u00e9trica\np =  0.3 # parametro de forma \ngeometrica = stats.geom(p) # Distribuci\u00f3n\nx = np.arange(geometrica.ppf(0.01),\n              geometrica.ppf(0.99))\nfmp = geometrica.pmf(x) # Funci\u00f3n de Masa de Probabilidad\nplt.plot(x, fmp, '--')\nplt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\nplt.title('Distribuci\u00f3n Geom\u00e9trica')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","217d22c0":"# histograma\naleatorios = geometrica.rvs(1000)  # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Geom\u00e9trica')\nplt.show()","9f9dc3c5":"# Graficando Hipergeom\u00e9trica\nM, n, N = 30, 10, 12 # parametros de forma \nhipergeometrica = stats.hypergeom(M, n, N) # Distribuci\u00f3n\nx = np.arange(0, n+1)\nfmp = hipergeometrica.pmf(x) # Funci\u00f3n de Masa de Probabilidad\nplt.plot(x, fmp, '--')\nplt.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\nplt.title('Distribuci\u00f3n Hipergeom\u00e9trica')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","65ad4809":"# histograma\naleatorios = hipergeometrica.rvs(1000)  # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Hipergeom\u00e9trica')\nplt.show()","76cb7472":"# Graficando Bernoulli\np =  0.5 # parametro de forma \nbernoulli = stats.bernoulli(p)\nx = np.arange(-1, 3)\nfmp = bernoulli.pmf(x) # Funci\u00f3n de Masa de Probabilidad\nfig, ax = plt.subplots()\nax.plot(x, fmp, 'bo')\nax.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)\nax.set_yticks([0., 0.2, 0.4, 0.6])\nplt.title('Distribuci\u00f3n Bernoulli')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","c42005a4":"# histograma\naleatorios = bernoulli.rvs(1000)  # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Bernoulli')\nplt.show()","ce9f5dcd":"# Graficando Normal\nmu, sigma = 0, 0.2 # media y desvio estandar\nnormal = stats.norm(mu, sigma)\nx = np.linspace(normal.ppf(0.01),\n                normal.ppf(0.99), 100)\nfp = normal.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Normal')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","fd8771c1":"# histograma\naleatorios = normal.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Normal')\nplt.show()","01618bdb":"# Graficando Uniforme\nuniforme = stats.uniform()\nx = np.linspace(uniforme.ppf(0.01),\n                uniforme.ppf(0.99), 100)\nfp = uniforme.pdf(x) # Funci\u00f3n de Probabilidad\nfig, ax = plt.subplots()\nax.plot(x, fp, '--')\nax.vlines(x, 0, fp, colors='b', lw=5, alpha=0.5)\nax.set_yticks([0., 0.2, 0.4, 0.6, 0.8, 1., 1.2])\nplt.title('Distribuci\u00f3n Uniforme')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","058e78b6":"# histograma\naleatorios = uniforme.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Uniforme')\nplt.show()","047b41e9":"# Graficando Log-Normal\nsigma = 0.6 # parametro\nlognormal = stats.lognorm(sigma)\nx = np.linspace(lognormal.ppf(0.01),\n                lognormal.ppf(0.99), 100)\nfp = lognormal.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Log-normal')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","76ed3647":"# histograma\naleatorios = lognormal.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Log-normal')\nplt.show()","ee862132":"# Graficando Exponencial\nexponencial = stats.expon()\nx = np.linspace(exponencial.ppf(0.01),\n                exponencial.ppf(0.99), 100)\nfp = exponencial.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Exponencial')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","0d706fd5":"# histograma\naleatorios = exponencial.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Exponencial')\nplt.show()","2db6d748":"# Graficando Gamma\na = 2.6 # parametro de forma.\ngamma = stats.gamma(a)\nx = np.linspace(gamma.ppf(0.01),\n                gamma.ppf(0.99), 100)\nfp = gamma.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Gamma')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","de062ef6":"# histograma\naleatorios = gamma.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Gamma')\nplt.show()","06284bc6":"# Graficando Beta\na, b = 2.3, 0.6 # parametros de forma.\nbeta = stats.beta(a, b)\nx = np.linspace(beta.ppf(0.01),\n                beta.ppf(0.99), 100)\nfp = beta.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Beta')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","051573ee":"# histograma\naleatorios = beta.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Beta')\nplt.show()","af4180ee":"# Graficando Chi cuadrado\ndf = 34 # parametro de forma.\nchi2 = stats.chi2(df)\nx = np.linspace(chi2.ppf(0.01),\n                chi2.ppf(0.99), 100)\nfp = chi2.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n Chi cuadrado')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","2946377f":"# histograma\naleatorios = chi2.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma Chi cuadrado')\nplt.show()","3ac06344":"# Graficando t de Student\ndf = 50 # parametro de forma.\nt = stats.t(df)\nx = np.linspace(t.ppf(0.01),\n                t.ppf(0.99), 100)\nfp = t.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n t de Student')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","1f4aa86c":"# histograma\naleatorios = t.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma t de Student')\nplt.show()","6a450f23":"# Graficando Pareto\nk = 2.3 # parametro de forma.\npareto = stats.pareto(k)\nx = np.linspace(pareto.ppf(0.01),\n                pareto.ppf(0.99), 100)\nfp = pareto.pdf(x) # Funci\u00f3n de Probabilidad\nplt.plot(x, fp)\nplt.title('Distribuci\u00f3n de Pareto')\nplt.ylabel('probabilidad')\nplt.xlabel('valores')\nplt.show()","64c7dac9":"# histograma\naleatorios = pareto.rvs(1000) # genera aleatorios\ncuenta, cajas, ignorar = plt.hist(aleatorios, 20)\nplt.ylabel('frequencia')\nplt.xlabel('valores')\nplt.title('Histograma de Pareto')\nplt.show()","fffcc384":"### Distribuci\u00f3n de Log-normal\n\nLa [Distribuci\u00f3n Log-normal](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_log-normal) esta dada por la formula:\n\n$$p(x;\\mu, \\sigma) = \\frac{1}{ x \\sigma \\sqrt{2 \\pi}} e^{\\frac{-1}{2}\\left(\\frac{\\ln x - \\mu}{\\sigma} \\right)^2}\n$$\n\nEn d\u00f3nde la variable $x > 0$ y los par\u00e1metros $\\mu$ y $\\sigma > 0$ son todos [n\u00fameros reales](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real). La [Distribuci\u00f3n Log-normal](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_log-normal) es aplicable a [variables aleatorias](https:\/\/es.wikipedia.org\/wiki\/Variable_aleatoria) que est\u00e1n limitadas por cero, pero tienen pocos valores grandes. Es una [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) con [asimetr\u00eda positiva](https:\/\/es.wikipedia.org\/wiki\/Asimetr%C3%ADa_estad%C3%ADstica). Algunos de los ejemplos en que la solemos encontrar son:\n* El peso de los adultos.\n* La concentraci\u00f3n de los minerales en dep\u00f3sitos.\n* Duraci\u00f3n de licencia por enfermedad.\n* Distribuci\u00f3n de riqueza\n* Tiempos muertos de maquinarias.","6f8f201d":"### Distribuci\u00f3n de Pareto\n\nLa [Distribuci\u00f3n de Pareto](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Pareto) esta dada por la funci\u00f3n:\n\n$$p(x; \\alpha, k) = \\frac{\\alpha k^{\\alpha}}{x^{\\alpha + 1}} \n$$\n\nEn d\u00f3nde la variable $x \\ge k$ y el par\u00e1metro $\\alpha > 0$ son [n\u00fameros reales](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real). Esta [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) fue introducida por su inventor, [Vilfredo Pareto](https:\/\/es.wikipedia.org\/wiki\/Vilfredo_Pareto), con el fin de explicar la distribuci\u00f3n de los salarios en la sociedad. La [Distribuci\u00f3n de Pareto](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Pareto) se describe a menudo como la base de la [regla 80\/20](https:\/\/es.wikipedia.org\/wiki\/Principio_de_Pareto). Por ejemplo, el 80% de las quejas de los clientes con respecto al funcionamiento de su veh\u00edculo por lo general surgen del 20% de los componentes.","275113a6":"## Distribuciones continuas\n\nAhora que ya conocemos las principales [distribuciones discretas](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad#Distribuciones_de_variable_discreta), podemos pasar a describir a las [distribuciones continuas](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad_continua); en ellas a diferencia de lo que ve\u00edamos antes, la variable puede tomar cualquier valor dentro de un intervalo espec\u00edfico. Dentro de este grupo vamos a encontrar a las siguientes: \n\n### Distribuci\u00f3n de Normal\n\nLa [Distribuci\u00f3n Normal](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_normal), o tambi\u00e9n llamada [Distribuci\u00f3n de Gauss](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_normal), es aplicable a un amplio rango de problemas, lo que la convierte en la [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) m\u00e1s utilizada en [estad\u00edstica](https:\/\/relopezbriega.github.io\/tag\/estadistica.html); esta dada por la formula:\n\n$$p(x;\\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{\\frac{-1}{2}\\left(\\frac{x - \\mu}{\\sigma} \\right)^2}\n$$\n\nEn d\u00f3nde $\\mu$ es el par\u00e1metro de ubicaci\u00f3n, y va a ser igual a la [media aritm\u00e9tica](https:\/\/es.wikipedia.org\/wiki\/Media_aritm%C3%A9tica) y $\\sigma^2$ es el [desv\u00edo est\u00e1ndar](https:\/\/es.wikipedia.org\/wiki\/Desviaci%C3%B3n_t%C3%ADpica). Algunos ejemplos de variables asociadas a fen\u00f3menos naturales que siguen el modelo de la [Distribuci\u00f3n Normal](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_normal) son:\n* caracter\u00edsticas morfol\u00f3gicas de individuos, como la estatura;\n* caracter\u00edsticas sociol\u00f3gicas, como el consumo de cierto producto por un mismo grupo de individuos;\n* caracter\u00edsticas psicol\u00f3gicas, como el cociente intelectual;\n* nivel de ruido en telecomunicaciones;\n* errores cometidos al medir ciertas magnitudes;\n* etc.","af4c206a":"### Distribuci\u00f3n de Exponencial\n\nLa [Distribuci\u00f3n Exponencial](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_exponencial) esta dada por la formula:\n\n$$p(x;\\alpha) = \\frac{1}{ \\alpha} e^{\\frac{-x}{\\alpha}}\n$$\n\nEn d\u00f3nde tanto la variable $x$ como el par\u00e1metro $\\alpha$ son [n\u00fameros reales](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real) positivos. La [Distribuci\u00f3n Exponencial](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_exponencial) tiene bastantes aplicaciones, tales como la desintegraci\u00f3n de un \u00e1tomo radioactivo o el tiempo entre eventos en un proceso de [Poisson](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Poisson) donde los acontecimientos suceden a una velocidad constante.","12dc8018":"### Distribuci\u00f3n de Bernoulli\n\nUn proceso de Bernoulli es la repetici\u00f3n de un ensayo de Bernoulli. Por ejemplo de una moneda estaremos estudiando cu\u00e1ntas veces sale \"cara\" o cu\u00e1ntas veces sale \"cruz\", o la probabilidad de que salga \"cara\", al menos una vez, de un n\u00famero n de intentos.\n\nLa [Distribuci\u00f3n de Bernoulli](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Bernoulli) esta dada por la formula:\n\n$$p(r;p) = \\left\\{\n\t\\begin{array}{ll}\n            1 - p = q  & \\mbox{si } r = 0  \\ \\mbox{(fracaso)}\\\\\n            p & \\mbox{si } r = 1 \\ \\mbox{(\u00e9xito)}\n\t\\end{array}\n\\right.$$\n\nEn d\u00f3nde el par\u00e1metro $p$ es la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de \u00e9xito en un solo ensayo, la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de fracaso por lo tanto va a ser $1 - p$ (muchas veces expresada como $q$). Tanto $p$ como $q$ van a estar limitados al intervalo de cero a uno. La [Distribuci\u00f3n de Bernoulli](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Bernoulli) describe un experimento probabil\u00edstico en donde el ensayo tiene dos posibles resultados, \u00e9xito o fracaso. Desde esta [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) se pueden deducir varias [Funciones de Densidad de Probabilidad](https:\/\/es.wikipedia.org\/wiki\/Funci%C3%B3n_de_densidad_de_probabilidad) de otras [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) que se basen en una serie de ensayos independientes.\nUtilice la distribuci\u00f3n de Bernoulli cuando un proceso aleatorio tenga exactamente dos resultados: evento o no evento. Por ejemplo, en el campo de la calidad, un producto se puede clasificar como bueno o malo.","c35ba311":"### Distribuci\u00f3n Gamma\n\nLa [Distribuci\u00f3n Gamma](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_gamma) esta dada por la formula:\n\n$$p(x;a, b) = \\frac{a(a x)^{b -1} e^{-ax}}{\\Gamma(b)}\n$$\n\nEn d\u00f3nde los par\u00e1metros $a$ y $b$ y la variable $x$ son [n\u00fameros reales](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real) positivos y $\\Gamma(b)$ es la [funci\u00f3n gamma](https:\/\/es.wikipedia.org\/wiki\/Funci%C3%B3n_gamma). La [Distribuci\u00f3n Gamma](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_gamma) comienza en el *origen* de coordenadas y tiene una forma bastante flexible. Otras [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) son casos especiales de ella.","4b7c7e34":"## Distribuciones\n\nAhora que ya conocemos como podemos hacer para representar a las [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad); pasemos a analizar cada una de ellas en m\u00e1s detalle para conocer su forma, sus principales aplicaciones y sus propiedades. Comencemos por las [distribuciones discretas](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad#Distribuciones_de_variable_discreta).\n\n## Distribuciones Discretas\n\nLas [distribuciones discretas](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad#Distribuciones_de_variable_discreta) son aquellas en las que la variable puede tomar solo algunos valores determinados. Los principales exponentes de este grupo son las siguientes: \n\n### Distribuci\u00f3n Poisson\n\nLa [Distribuci\u00f3n Poisson](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Poisson) esta dada por la formula:\n\n$$p(r; \\mu) = \\frac{\\mu^r e^{-\\mu}}{r!}$$\n\nEn d\u00f3nde $r$ es un [entero](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_entero) ($r \\ge 0$) y $\\mu$ es un [n\u00famero real](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real) positivo. La [Distribuci\u00f3n Poisson](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_Poisson) describe la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de encontrar exactamente $r$ eventos en un lapso de tiempo si los acontecimientos se producen de forma independiente a una velocidad constante $\\mu$. Es una de las [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) m\u00e1s utilizadas en [estad\u00edstica](https:\/\/relopezbriega.github.io\/tag\/estadistica.html) con varias aplicaciones; como por ejemplo describir el n\u00famero de fallos en un lote de materiales o la cantidad de llegadas por hora a un centro de servicios. \n\nEn [Python](https:\/\/python.org\/) la podemos generar f\u00e1cilmente con la ayuda de [scipy.stats](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/stats.html), paquete que utilizaremos para representar a todas las restantes [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) a lo largo de todo el art\u00edculo.\n\nAqu\u00ed algunos ejemplos t\u00edpicos de variables aleatorias que siguen una distribuci\u00f3n de Poisson:\n\n* El n\u00famero de clientes que ingresan a un supermercado en un d\u00eda.\n* El n\u00famero de accidentes registrados en una f\u00e1brica durante una semana. \n* El n\u00famero de llamadas que recibe una central telef\u00f3nica en el per\u00edodo de un minuto.\n* El n\u00famero de bacterias en un volumen de un litro de agua.\n* El n\u00famero de veh\u00edculos que llegan a una gasolinera en una hora.\n* El n\u00famero de fallas en la superficie de una pieza de cer\u00e1mica rectangular.\n* El n\u00famero de toxinas en partes por mill\u00f3n encontradas en un litro de agua de un r\u00edo.","7b0f5d72":"# Distribuciones de probabilidad con Python","21e7f156":"## \u00bfC\u00f3mo elegir la distribuci\u00f3n que mejor se ajusta a mis datos?\n\nAhora ya tenemos un conocimiento general de las principales [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) con que nos podemos encontrar; pero \u00bfc\u00f3mo determinamos que [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) debemos utilizar?\n\nUn modelo que podemos seguir cuando nos encontramos con datos que necesitamos ajustar a una [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad), es comenzar con los datos sin procesar y responder a cuatro preguntas b\u00e1sicas acerca de los mismos, que nos pueden ayudar a caracterizarlos. La **primer pregunta** se refiere a si los datos **pueden tomar valores [discretos](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad#Distribuciones_de_variable_discreta) o [continuos](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad_continua)**. **La segunda pregunta** que nos debemos hacer, hace referencia a la **[simetr\u00eda](https:\/\/es.wikipedia.org\/wiki\/Asimetr%C3%ADa_estad%C3%ADstica) de los datos** y si hay asimetr\u00eda, en qu\u00e9 direcci\u00f3n se encuentra; en otras palabras, son los [valores at\u00edpicos](https:\/\/es.wikipedia.org\/wiki\/Valor_at%C3%ADpico) positivos y negativos igualmente probables o es uno m\u00e1s probable que el otro. **La tercer pregunta** abarca los **l\u00edmites superiores e inferiores en los datos**; hay algunos datos, como los ingresos, que no pueden ser inferiores a cero, mientras que hay otros, como los m\u00e1rgenes de operaci\u00f3n que no puede exceder de un valor (100%). **La \u00faltima pregunta** se refiere a la **posibilidad de observar valores extremos** en la [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad); en algunos casos, los valores extremos ocurren con muy poca frecuencia, mientras que en otros, se producen con mayor frecuencia.\nEste proceso, lo podemos resumir en el siguiente gr\u00e1fico:\n\n<img alt=\"Distribuciones estad\u00edsticas\" title=\"Distribuciones estad\u00edsticas\" src=\"https:\/\/relopezbriega.github.io\/images\/distributions_choice.png\" >\n\nCon la ayuda de estas preguntas fundamentales, m\u00e1s el conocimiento de las distintas [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) deber\u00edamos estar en condiciones de poder caracterizar cualquier [conjunto de datos](https:\/\/es.wikipedia.org\/wiki\/Conjunto_de_datos).","91526af4":"### Distribuci\u00f3n Uniforme\n\nLa [Distribuci\u00f3n Uniforme](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_uniforme_discreta) es un caso muy simple expresada por la funci\u00f3n:\n\n$$f(x; a, b) = \\frac{1}{b -a} \\ \\mbox{para} \\ a \\le x \\le b\n$$\n\nSu [funci\u00f3n de distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Funci%C3%B3n_de_distribuci%C3%B3n) esta entonces dada por:\n\n$$\np(x;a, b) = \\left\\{\n\t\\begin{array}{ll}\n            0  & \\mbox{si } x \\le a \\\\\n            \\frac{x-a}{b-a} & \\mbox{si } a \\le x \\le b \\\\\n            1 & \\mbox{si } b \\le x\n\t\\end{array}\n\\right.\n$$\n\nTodos los valore tienen pr\u00e1cticamente la misma probabilidad.","3042c023":"### Distribuci\u00f3n Chi cuadrado\n\nLa [Distribuci\u00f3n Chi cuadrado](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_%CF%87%C2%B2) esta dada por la funci\u00f3n:\n\n$$p(x; n) = \\frac{\\left(\\frac{x}{2}\\right)^{\\frac{n}{2}-1} e^{\\frac{-x}{2}}}{2\\Gamma \\left(\\frac{n}{2}\\right)}\n$$\n\nEn d\u00f3nde la variable $x \\ge 0$ y el par\u00e1metro $n$, el n\u00famero de grados de libertad, es un [n\u00famero entero](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_entero) positivo. Una importante aplicaci\u00f3n de la [Distribuci\u00f3n Chi cuadrado](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_%CF%87%C2%B2) es que cuando un [conjunto de datos](https:\/\/es.wikipedia.org\/wiki\/Conjunto_de_datos) es representado por un modelo te\u00f3rico, esta [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) puede ser utilizada para controlar cuan bien se ajustan los valores predichos por el modelo, y los datos realmente observados.","df25a475":"### Distribuci\u00f3n Binomial\n\nLa [Distribuci\u00f3n Binomial](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_binomial) esta dada por la formula:\n\n$$p(r; N, p) = \\left(\\begin{array}{c} N \\\\ r \\end{array}\\right) p^r(1 - p)^{N - r}\n$$\n\nEn d\u00f3nde $r$ con la condici\u00f3n $0 \\le r \\le N$ y el par\u00e1metro $N$ ($N > 0$) son [enteros](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_entero); y el par\u00e1metro $p$ ($0 \\le p \\le 1$) es un [n\u00famero real](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real). La [Distribuci\u00f3n Binomial](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_binomial) describe la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de exactamente $r$ \u00e9xitos en $N$ pruebas si la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de \u00e9xito en una sola prueba es $p$.\n\nUtilizamos la distribuci\u00f3n binomial en todos los eventos donde solamente hay dos resultados, por ejemplo, la definici\u00f3n del sexo de un beb\u00e9; el que nuestro equipo favorito gane o pierda alg\u00fan partido; el que pase o repruebe un examen. Ah\u00ed, sin darnos cuenta, estamos haciendo uso de este concepto.","faaa54a4":"### Distribuci\u00f3n Hipergeom\u00e9trica\n\nLa distribuci\u00f3n hipergeom\u00e9trica es especialmente \u00fatil en todos aquellos casos en los que se extraigan muestras o se realizan experiencias repetidas sin devoluci\u00f3n del elemento extra\u00eddo o sin retornar a la situaci\u00f3n experimental inicial.\n\n\n\nLa [Distribuci\u00f3n Hipergeom\u00e9trica](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_hipergeom%C3%A9trica) esta dada por la formula:\n\n$$p(r; n, N, M) = \\frac{\\left(\\begin{array}{c} M \\\\ r \\end{array}\\right)\\left(\\begin{array}{c} N - M\\\\ n -r \\end{array}\\right)}{\\left(\\begin{array}{c} N \\\\ n \\end{array}\\right)}\n$$\n\nEn d\u00f3nde el valor de $r$ esta limitado por $\\max(0, n - N + M)$ y $\\min(n, M)$ inclusive; y los par\u00e1metros $n$ ($1 \\le n \\le N$), $N$ ($N \\ge 1$) y $M$ ($M \\ge 1$) son todos [n\u00fameros enteros](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_entero). La [Distribuci\u00f3n Hipergeom\u00e9trica](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_hipergeom%C3%A9trica) describe experimentos en donde se seleccionan los elementos al azar *sin reemplazo* (se evita seleccionar el mismo elemento m\u00e1s de una vez). M\u00e1s precisamente, supongamos que tenemos $N$ elementos de los cuales $M$ tienen un cierto atributo (y $N - M$ no tiene). Si escogemos $n$ elementos al azar *sin reemplazo*, $p(r)$ es la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de que exactamente $r$ de los elementos seleccionados provienen del grupo con el atributo.  ","6183320e":"Con esto concluyo este *tour* por las principales [distribuciones](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) utilizadas en [estad\u00edstica](https:\/\/relopezbriega.github.io\/tag\/estadistica.html). Para m\u00e1s informaci\u00f3n tambi\u00e9n pueden visitar mi art\u00edculo [Probabilidad y Estad\u00edstica con Python](https:\/\/relopezbriega.github.io\/blog\/2015\/06\/27\/probabilidad-y-estadistica-con-python\/) o la categor\u00eda [estad\u00edstica](https:\/\/relopezbriega.github.io\/tag\/estadistica.html) del blog. Espero les resulte \u00fatil.\n\nSaludos!\n\n*Este post fue escrito utilizando Jupyter notebook. Pueden descargar este [notebook](https:\/\/github.com\/relopezbriega\/relopezbriega.github.io\/blob\/master\/downloads\/DistStatsPy.ipynb) o ver su version est\u00e1tica en [nbviewer](https:\/\/nbviewer.ipython.org\/github\/relopezbriega\/relopezbriega.github.io\/blob\/master\/downloads\/DistStatsPy.ipynb).*","f56e0699":"### Distribuci\u00f3n Geom\u00e9trica\n\nLa distribuci\u00f3n geom\u00e9trica se utiliza en la distribuci\u00f3n de tiempos de espera, de manera que si los ensayos se realizan a intervalos regulares de tiempo, esta variable aleatoria proporciona el tiempo transcurrido hasta el primer \u00e9xito.\n\nLa [Distribuci\u00f3n Geom\u00e9trica](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_geom%C3%A9trica) esta dada por la formula:\n\n$$p(r; p) = p(1- p)^{r-1}\n$$\n\nEn d\u00f3nde $r \\ge 1$  y el par\u00e1metro $p$ ($0 \\le p \\le 1$) es un [n\u00famero real](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real). La [Distribuci\u00f3n Geom\u00e9trica](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_geom%C3%A9trica) expresa la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de tener que esperar exactamente $r$ pruebas hasta encontrar el primer \u00e9xito si la *[probabilidad](https:\/\/es.wikipedia.org\/wiki\/Probabilidad)* de \u00e9xito en una sola prueba es $p$. \n\nPor ejemplo, en un proceso de selecci\u00f3n, podr\u00eda definir el n\u00famero de entrevistas que deber\u00edamos realizar antes de encontrar al primer candidato aceptable.","13e0e440":"*Esta notebook fue creada originalmente como un blog post por [Ra\u00fal E. L\u00f3pez Briega](https:\/\/relopezbriega.com.ar\/).*","a8585424":"### Distribuci\u00f3n Beta\n\nLa [Distribuci\u00f3n Beta](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_beta) esta dada por la formula:\n\n$$p(x;p, q) = \\frac{1}{B(p, q)} x^{p-1}(1 - x)^{q-1}\n$$\n\nEn d\u00f3nde los par\u00e1metros $p$ y $q$ son [n\u00fameros reales](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real) positivos, la variable $x$ satisface la condici\u00f3n $0 \\le x \\le 1$ y $B(p, q)$ es la [funci\u00f3n beta](https:\/\/es.wikipedia.org\/wiki\/Funci%C3%B3n_beta). Las aplicaciones de la [Distribuci\u00f3n Beta](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_beta) incluyen el modelado de [variables aleatorias](https:\/\/es.wikipedia.org\/wiki\/Variable_aleatoria) que tienen un rango finito de $a$ hasta $b$. Un\nejemplo de ello es la distribuci\u00f3n de los tiempos de actividad en las redes de proyectos. La [Distribuci\u00f3n Beta](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_beta) se utiliza tambi\u00e9n con frecuencia como una [probabilidad a priori](https:\/\/es.wikipedia.org\/wiki\/Probabilidad_a_priori) para proporciones [binomiales]((https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_binomial) en el [an\u00e1lisis bayesiano](https:\/\/es.wikipedia.org\/wiki\/Inferencia_bayesiana).","811aa0ca":"### Distribuci\u00f3n T de Student\n\nLa [Distribuci\u00f3n t de Student](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_t_de_Student) esta dada por la funci\u00f3n:\n\n$$p(t; n) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n\\pi}\\Gamma(\\frac{n}{2})} \\left( 1 + \\frac{t^2}{2} \\right)^{-\\frac{n+1}{2}}\n$$\n\nEn d\u00f3nde la variable $t$ es un [n\u00famero real](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_real) y el par\u00e1metro $n$ es un [n\u00famero entero](https:\/\/es.wikipedia.org\/wiki\/N%C3%BAmero_entero) positivo. La [Distribuci\u00f3n t de Student](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_t_de_Student) es utilizada para probar si la diferencia entre las *medias* de dos muestras de observaciones es estad\u00edsticamente significativa. Por ejemplo, las alturas de una muestra aleatoria de los jugadores de baloncesto podr\u00eda compararse con las alturas de una muestra aleatoria de jugadores de f\u00fatbol; esta [distribuci\u00f3n](https:\/\/es.wikipedia.org\/wiki\/Distribuci%C3%B3n_de_probabilidad) nos podr\u00eda ayudar a determinar si un grupo es significativamente m\u00e1s alto que el otro."}}