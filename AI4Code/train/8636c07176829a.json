{"cell_type":{"9931d02f":"code","5c5ef531":"code","696c222c":"code","207842e8":"code","8820fdfa":"code","0bf2d33c":"code","b41f6227":"code","66961db5":"code","211e4e86":"code","ea520436":"code","0d8ceedc":"code","6b578429":"code","9e09d0ba":"code","31854f7f":"code","5042666a":"code","8974d1bf":"code","b4e38af6":"code","cd334818":"code","06a21110":"code","da381ec2":"code","8286fe62":"code","01270c57":"code","8f57b62f":"code","fc4b0085":"code","c80be552":"code","ec48c9e3":"code","aa22870d":"code","5c36529a":"code","dd1a9e30":"code","7d19d5da":"markdown","e797c58e":"markdown","26b714fd":"markdown","0cc009ee":"markdown","03ae7a78":"markdown"},"source":{"9931d02f":"import pandas as pd","5c5ef531":"train_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_df  = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nsample_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')","696c222c":"train_df","207842e8":"train_df.isna().sum()","8820fdfa":"train_df['language'].value_counts()","0bf2d33c":"train_df['label'].value_counts()","b41f6227":"test_df","66961db5":"test_df['language'].value_counts()","211e4e86":"sample_df","ea520436":"import tensorflow as tf","0d8ceedc":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","6b578429":"from transformers import TFAutoModel, AutoTokenizer","9e09d0ba":"from tensorflow.keras.layers import Dense, Input","31854f7f":"from tensorflow.keras.models import Model","5042666a":"from tensorflow.keras.optimizers import Adam","8974d1bf":"def model_watson(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFAutoModel.from_pretrained(transformer)\n        \n        input_layer = Input(shape=(100,), dtype=tf.int32, name=\"input_layer\")\n        sequence_output = transformer_encoder(input_layer)[0]\n        \n        cls_token = sequence_output[:, 0, :]\n        \n        output_layer = Dense(3, activation='softmax')(cls_token)\n        \n        model = Model(inputs=input_layer, outputs=output_layer)\n        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        \n        return model","b4e38af6":"model = model_watson(strategy,\"jplu\/tf-xlm-r-ner-40-lang\")","cd334818":"tokenizer = AutoTokenizer.from_pretrained(\"jplu\/tf-xlm-r-ner-40-lang\")","06a21110":"train_data = train_df[['premise', 'hypothesis']].values.tolist()\ntest_data = test_df[['premise', 'hypothesis']].values.tolist()","da381ec2":"train_encoded=tokenizer.batch_encode_plus(train_data,pad_to_max_length=True,max_length=100)\ntest_encoded=tokenizer.batch_encode_plus(test_data,pad_to_max_length=True,max_length=100)","8286fe62":"from sklearn.model_selection import train_test_split","01270c57":"x_train, x_valid, y_train, y_valid = train_test_split(train_encoded['input_ids'], train_df.label.values, test_size=0.2)\nx_test = test_encoded['input_ids']","8f57b62f":"train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(20 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(20 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(20 * strategy.num_replicas_in_sync))","fc4b0085":"model.summary()","c80be552":"history = model.fit(train_dataset,steps_per_epoch=len(train_df) \/\/ 20 * strategy.num_replicas_in_sync,validation_data=valid_dataset,epochs= 5)","ec48c9e3":"predictions = model.predict(test_dataset, verbose=1)\nsample_df['prediction'] = predictions.argmax(axis=1)","aa22870d":"import os\nos.chdir(r'\/kaggle\/working')","5c36529a":"sample_df.to_csv(r'submission.csv',index= False)","dd1a9e30":"sample_df.head(10)","7d19d5da":"Let's add the libraries where they are really needed, not all of them at the first line","e797c58e":"### Our prediction output ","26b714fd":"### Initiative knowledge about our data","0cc009ee":"### Modeling","03ae7a78":"### our data frames"}}