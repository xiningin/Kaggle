{"cell_type":{"7e602ba5":"code","3561a456":"code","4493860a":"code","ba6a4668":"code","d735d75f":"code","8678c1f7":"code","2f8827ab":"code","1b2de98e":"code","e5669b1d":"code","4a6e6962":"code","911f08c6":"markdown","b86649c0":"markdown","f27258c0":"markdown","5fa4f57c":"markdown","21988a2c":"markdown","44707686":"markdown","312b4de5":"markdown","a8cc2680":"markdown","8f1e65c7":"markdown","1fbadd15":"markdown"},"source":{"7e602ba5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3561a456":"!pip install networkx\n!pip install gensim\n!pip install torch\n!pip install tensorflow","4493860a":"import networkx as nx\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\ntrain_df=pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\nprint(train_df.columns)\ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\nedgelist=[]\ned_c=g.number_of_edges()\nfor e in g.edges():\n    node1=e[0]\n    node2=e[1]\n    n_c=0\n    n_c+=len([j for j in g.neighbors(node1)])\n    n_c+=len([j for j in g.neighbors(node2)])\n    normalized_count=n_c\/ed_c\n    g[e[0]][e[1]]['weight']=normalized_count\n\nnx.draw(g,pos=nx.spring_layout(g))\nclass laplacian_maps():\n    def __init__(self,graph,is_weighted,d):\n        self.graph=graph\n        self.is_weighted=is_weighted\n        self.graph=graph.to_undirected()\n        self.d=d\n    def create_embedding(self):\n        laplace_matrix=nx.normalized_laplacian_matrix(self.graph)\n        if(self.graph.number_of_nodes()<self.d):\n            einsum,embedding_vectors=scipy.sparse.linalg.eigsh(laplace_matrix,k=self.graph.number_of_nodes()-1,which='LM',ncv=10*self.d, return_eigenvectors=True)\n            diff=self.d-embedding_vectors.shape[0]-1\n            einsum= np.pad(embedding_vectors, (1, diff), 'constant', constant_values=0)\n        else:\n            einsum,embedding_vectors=scipy.sparse.linalg.eigsh(laplace_matrix,k=self.d,which='LM',ncv=10*self.d, return_eigenvectors=True)\n        self.embedding=embedding_vectors\n        return embedding_vectors\n    \n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions\n        \ndimension=500\nlaplace=laplacian_maps(g,True,dimension) \nembeddings=laplace.create_embedding()\nlaplace.plot_embedding(embeddings)\nnode_num=24\nnode,distances,questions=laplace.node_level_embedding(node_num,embeddings)\nlaplace_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\nlaplace_df['Question']=[node]*len(distances)\nlaplace_df['Sample_Question']=questions\nlaplace_df['Chebyshev_Distance']=distances\nlaplace_df.head()\n\nlaplace_df.to_csv('..\/Laplacian_Embeddings.csv',index=False)","ba6a4668":"from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport plotly\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nlaplacian_g=nx.from_pandas_edgelist(laplace_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\nG=laplacian_g\ndef plotter(G,title):\n    pos = nx.spring_layout(G, k=0.5, iterations=50)\n    for n, p in pos.items():\n        G.nodes[n]['pos'] = p\n    edge_trace = go.Scatter(\n        x=[],\n        y=[],\n        line=dict(width=0.5,color='#888'),\n        hoverinfo='none',\n        mode='lines')\n\n    for edge in G.edges():\n        x0, y0 = G.nodes[edge[0]]['pos']\n        x1, y1 = G.nodes[edge[1]]['pos']\n        edge_trace['x'] += tuple([x0, x1, None])\n        edge_trace['y'] += tuple([y0, y1, None])\n    node_trace = go.Scatter(\n        x=[],\n        y=[],\n        text=[],\n        mode='markers',\n        hoverinfo='text',\n        marker=dict(\n            showscale=True,\n            colorscale='RdBu',\n            reversescale=True,\n            color=[],\n            size=15,\n            colorbar=dict(\n                thickness=10,\n                title='Node Connections',\n                xanchor='left',\n                titleside='right'\n            ),\n            line=dict(width=0)))\n\n    for node in G.nodes():\n        x, y = G.nodes[node]['pos']\n        node_trace['x'] += tuple([x])\n        node_trace['y'] += tuple([y])\n    for node, adjacencies in enumerate(G.adjacency()):\n        node_trace['marker']['color']+=tuple([len(adjacencies[1])])\n        node_info = adjacencies[0] +' # of connections: '+str(len(adjacencies[1]))\n        node_trace['text']+=tuple([node_info])\n    fig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title=title,\n                titlefont=dict(size=16),\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    text=\"No. of connections\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\") ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n\n    iplot(fig)\nplotter(G,'Laplace Eigenmap Distance')","d735d75f":"class HOPE():\n    def __init__(self,graph,d):\n        self.graph=graph\n        self.dimension=d\n    def create_embeddings(self):\n        nodes=self.graph.number_of_nodes()\n        adj_matrix=nx.adjacency_matrix(self.graph,nodelist=range(nodes))\n        S_matrix=scipy.sparse.coo_matrix(adj_matrix.dot(adj_matrix))\n        U,sigma,vt=scipy.sparse.linalg.svds(S_matrix,k=self.dimension\/\/2)\n        sigma_norm=np.diag(np.sqrt(sigma))\n        self.left_embedding = np.dot(U, sigma_norm)\n        self.right_embedding = np.dot(vt.T, sigma_norm)\n        return np.concatenate([self.left_embedding, self.right_embedding], axis=1)\n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions\ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\ndimension=200\nhope=HOPE(g,dimension)\nhope_embeddings=hope.create_embeddings()\nhope.plot_embedding(hope_embeddings)\nnode_num=24\nnode,distances,questions=laplace.node_level_embedding(node_num,hope_embeddings)\nhope_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\nhope_df['Question']=[node]*len(distances)\nhope_df['Sample_Question']=questions\nhope_df['Chebyshev_Distance']=distances\nhope_df.head()\nhope_df.to_csv('..\/HOPE_Embeddings.csv',index=False)\nhope_g=nx.from_pandas_edgelist(hope_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\nG=hope_g\nplotter(G,'HOPE Embeddings')","8678c1f7":"import math\n# from sklearn.decomposition import TruncatedSVD\nclass GraRep():\n    def __init__(self,graph,d,order):\n        self.graph=graph\n        self.dimension=d\n        self.order=order\n        self._embeddings=[]\n    def create_matrix(self):\n        adj_matrix=nx.adjacency_matrix(self.graph,nodelist=range(self.graph.number_of_nodes()))\n        idx=np.arange(self.graph.number_of_nodes())\n        degree_vals=np.array([(1.0\/self.graph.degree[node]) for node in list(self.graph.nodes())])\n        n=self.graph.number_of_nodes()\n        #(data,row,columns)\n        D=scipy.sparse.coo_matrix((degree_vals,(idx,idx)),shape=(n,n))\n        adj_matrix=D.dot(adj_matrix)\n        return adj_matrix\n    \n    def create_loss(self):\n        adj_matrix=self.create_matrix()\n        #Apply log loss\n        alpha=1e-5\n        adj_tilde=scipy.sparse.coo_matrix(adj_matrix.dot(adj_matrix))\n        loss_scores=np.log(adj_tilde.data)-math.log(adj_tilde.shape[0])- alpha\n        rows=adj_tilde.row[loss_scores<0]\n        cols=adj_tilde.col[loss_scores<0]\n        loss_scores=loss_scores[loss_scores<0]\n        loss_matrix=scipy.sparse.coo_matrix((loss_scores,(rows,cols)),shape=adj_tilde.shape)\n        return loss_matrix\n    \n    def single_embedding(self):\n        S_matrix=self.create_loss()\n        \n        U,sigma,vt=scipy.sparse.linalg.svds(S_matrix,k=self.dimension\/\/2)\n        sigma_norm=np.diag(np.sqrt(sigma))\n        self.left_embedding = np.dot(U, sigma_norm)\n        self.right_embedding = np.dot(vt.T, sigma_norm)\n        embedding=np.concatenate([self.left_embedding, self.right_embedding], axis=1)\n        '''\n        svd = TruncatedSVD(n_components=self.dimension,\n                           n_iter=6,\n                           random_state=42)\n        svd.fit(S_matrix)\n        embedding = svd.transform(S_matrix)\n        '''\n        self._embeddings.append(embedding)\n        \n    def create_embeddings(self):\n        \n        loss_matrix=self.create_loss()\n        single_emb=self.single_embedding()\n        for i in range(self.order-1):\n            loss_matrix=self.create_loss()\n            self.single_embedding()\n        return np.concatenate(self._embeddings,axis=1)\n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions    \n    \n        \n\ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\ndimension=200\norder=5\ngrarep=GraRep(g,dimension,order)\ngrarep_embeddings=grarep.create_embeddings()\nprint(grarep_embeddings.shape)\ngrarep.plot_embedding(grarep_embeddings)\nnode_num=24\nnode,distances,questions=grarep.node_level_embedding(node_num,grarep_embeddings)\ngrarep_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\ngrarep_df['Question']=[node]*len(distances)\ngrarep_df['Sample_Question']=questions\ngrarep_df['Chebyshev_Distance']=distances\ngrarep_df.head()\ngrarep_df.to_csv('..\/Grarep_Embeddings.csv',index=False)\ngrarep_g=nx.from_pandas_edgelist(hope_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\nG=grarep_g\nplotter(G,'Grarep Embeddings')        \n        \n        \n        ","2f8827ab":"class NEMF():\n    def __init__(self,graph,d,order,negative_samples):\n        self.graph=graph\n        self.dimension=d\n        self.order=order\n        self.negative_samples=negative_samples\n        self._embeddings=[]\n    def create_matrix(self):\n        adj_matrix=nx.adjacency_matrix(self.graph,nodelist=range(self.graph.number_of_nodes()))\n        idx=np.arange(self.graph.number_of_nodes())\n        degree_vals=np.array([(1.0\/self.graph.degree[node]) for node in list(self.graph.nodes())])\n        n=self.graph.number_of_nodes()\n        #(data,row,columns)\n        D=scipy.sparse.coo_matrix((degree_vals,(idx,idx)),shape=(n,n))\n        adj_matrix=D.dot(adj_matrix)\n        return adj_matrix,D\n    \n    def create_loss(self):\n        adj_matrix,D=self.create_matrix()\n        #Apply log loss\n        alpha=1e-5\n        a_pooled=adj_matrix\n        for i in range(self.order-1):\n            adj_tilde=scipy.sparse.coo_matrix(adj_matrix.dot(adj_matrix))\n            a_pooled+=adj_tilde\n        a_pooled = (self.graph.number_of_edges()*a_pooled)\/(self.order*self.negative_samples)\n        a_pooled=scipy.sparse.coo_matrix(a_pooled.dot(D))\n        loss_scores=np.log(a_pooled.data)-math.log(a_pooled.shape[0])- alpha\n        rows=a_pooled.row[loss_scores<0]\n        cols=a_pooled.col[loss_scores<0]\n        loss_scores=loss_scores[loss_scores<0]\n        loss_matrix=scipy.sparse.coo_matrix((loss_scores,(rows,cols)),shape=a_pooled.shape)\n        return loss_matrix\n    \n    def single_embedding(self):\n        S_matrix=self.create_loss()\n        \n        U,sigma,vt=scipy.sparse.linalg.svds(S_matrix,k=self.dimension\/\/2)\n        sigma_norm=np.diag(np.sqrt(sigma))\n        self.left_embedding = np.dot(U, sigma_norm)\n        self.right_embedding = np.dot(vt.T, sigma_norm)\n        embedding=np.concatenate([self.left_embedding, self.right_embedding], axis=1)\n        '''\n        svd = TruncatedSVD(n_components=self.dimension,\n                           n_iter=6,\n                           random_state=42)\n        svd.fit(S_matrix)\n        embedding = svd.transform(S_matrix)\n        '''\n        self._embeddings.append(embedding)\n        \n    def create_embeddings(self):\n        \n        loss_matrix=self.create_loss()\n        single_emb=self.single_embedding()\n        for i in range(self.order-1):\n            loss_matrix=self.create_loss()\n            self.single_embedding()\n        return np.concatenate(self._embeddings,axis=1)\n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions    \ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\ndimension=200\norder=5\nnegative_samples=1\nnemt=NEMF(g,dimension,order,negative_samples)\nnemt_embeddings=nemt.create_embeddings()\nprint(nemt_embeddings.shape)\nnemt.plot_embedding(nemt_embeddings)\nnode_num=24\nnode,distances,questions=nemt.node_level_embedding(node_num,nemt_embeddings)\nnemt_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\nnemt_df['Question']=[node]*len(distances)\nnemt_df['Sample_Question']=questions\nnemt_df['Chebyshev_Distance']=distances\nnemt_df.head()\nnemt_df.to_csv('..\/NEMT_Embeddings.csv',index=False)\nnemt_g=nx.from_pandas_edgelist(hope_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\nG=nemt_g\nplotter(G,'NEMT Embeddings')        \n        \n        \n        ","1b2de98e":"import random\nfrom gensim.models.word2vec import Word2Vec\nclass RandomWalkerTemplate:\n    def __init__(self, walk_length, walk_number):\n        self.walk_length = walk_length\n        self.walk_number = walk_number\n\n    def do_walk(self, node):\n        walk = [node]\n        for _ in range(self.walk_length-1):\n            nebs = [node for node in self.graph.neighbors(walk[-1])]\n            if len(nebs) > 0:\n                walk = walk + random.sample(nebs, 1)\n        walk = [str(w) for w in walk]\n        return walk\n\n    def do_walks(self, graph):\n        self.walks = []\n        self.graph = graph\n        for node in self.graph.nodes():\n            for _ in range(self.walk_number):\n                walk_from_node = self.do_walk(node)\n                self.walks.append(walk_from_node)\nclass Walklet():\n    def __init__(self,graph, walk_number=10, walk_length=80, dimensions=32, workers=4,\n                 window_size=4, epochs=1, learning_rate=0.05, min_count=1, seed=42):\n        self.graph=graph\n        self.walk_number = walk_number\n        self.walk_length = walk_length\n        self.dimensions = dimensions\n        self.workers = workers\n        self.window_size = window_size\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.min_count = min_count\n        self.seed = seed\n        \n    def _select_walklets(self, walks, power):\n        walklets = []\n        for walk in walks:\n            for step in range(power+1):\n                neighbors = [n for i, n in enumerate(walk[step:]) if i % power == 0]\n                walklets.append(neighbors)\n        return walklets\n\n    def create_embeddings(self):\n        \n        walker = RandomWalkerTemplate(self.walk_length, self.walk_number)\n        walker.do_walks(self.graph)\n        num_of_nodes = self.graph.number_of_nodes()\n\n        self._embedding = []\n        for power in range(1, self.window_size+1):\n            walklets = self._select_walklets(walker.walks, power)\n            model = Word2Vec(walklets,\n                             hs=0,\n                             alpha=self.learning_rate,\n                             \n                             \n                             window=1,\n                             min_count=self.min_count,\n                             workers=self.workers,seed=42)\n            model.build_vocab(walklets, progress_per=2)\n            model.train(\n               walklets, total_examples=model.corpus_count, epochs=20, \n               report_delay=1\n                )\n            l=list(self.graph.nodes)\n            embedding = np.array([model.wv[str(n)] for n in l])\n            self._embedding.append(embedding)\n        return np.concatenate(self._embedding, axis=1)\n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions    \n    \ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\n\nwalklet=Walklet(g,walk_number=10, walk_length=80, dimensions=32, workers=4,\n                 window_size=4, epochs=1, learning_rate=0.05, min_count=1, seed=42)\nwalklet_embeddings=walklet.create_embeddings()\nprint(walklet_embeddings.shape)\nwalklet.plot_embedding(walklet_embeddings)\nnode_num=24\nnode,distances,questions=walklet.node_level_embedding(node_num,walklet_embeddings)\nwalklet_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\nwalklet_df['Question']=[node]*len(distances)\nwalklet_df['Sample_Question']=questions\nwalklet_df['Chebyshev_Distance']=distances\nwalklet_df.head()\nwalklet_df.to_csv('..\/Walklet_Embeddings.csv',index=False)\nwalklet_g=nx.from_pandas_edgelist(hope_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\nG=walklet_g\nplotter(G,'Walklet Embeddings')        ","e5669b1d":"import hashlib\nclass Wesfeiler_Lehman_IsoMap():\n    def __init__(self,graph,wl_iterations):\n        self.wl_iterations = wl_iterations\n        self.graph = graph\n        self.features = {node: self.graph.degree(node) for node in self.graph.nodes()}\n        self._do_recursions()\n    def _do_a_recursion(self):\n        self.extracted_features = {k: [str(v)] for k, v in self.features.items()}\n        new_features = {}\n        for node in self.graph.nodes():\n            neighbor = self.graph.neighbors(node)\n            degs = [self.features[nb] for nb in neighbor]\n            features = [str(self.features[node])]+sorted([str(deg) for deg in degs])\n            features = \"_\".join(features)\n            #Hasing the compressed values\n            hash_object = hashlib.md5(features.encode())\n            hashing = hash_object.hexdigest()\n            new_features[node] = hashing\n        self.extracted_features = {k: self.extracted_features[k] + [v] for k, v in new_features.items()}\n        return new_features\n    def _do_recursions(self):\n        for _ in range(self.wl_iterations):\n            self.features = self._do_a_recursion()\n\n    def get_node_features(self):\n        return self.extracted_features\n\n    def get_graph_features(self):\n        return [feature for node, features in self.extracted_features.items() for feature in features]\n    \ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\nwl_isomap=Wesfeiler_Lehman_IsoMap(g,20)\nnode_features=wl_isomap.get_node_features()\ngraph_features=wl_isomap.get_graph_features()","4a6e6962":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nclass GL2Vec():\n    def __init__(self,graph, wl_iterations=2, dimensions=128, workers=4, down_sampling=0.0001,\n                 epochs=10, learning_rate=0.025, min_count=5, seed=42):\n\n        self.wl_iterations = wl_iterations\n        self.dimensions = dimensions\n        self.workers = workers\n        self.down_sampling = down_sampling\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.min_count = min_count\n        self.seed = seed\n        self.graph=graph\n        self._embedding=[]\n\n    def _create_line_graph(self, graph):\n        graph = nx.line_graph(self.graph)\n        node_mapper = {node: i for i, node in enumerate(self.graph.nodes())}\n        edges = [[node_mapper[edge[0]], node_mapper[edge[1]]] for edge in self.graph.edges()]\n        line_graph = nx.from_edgelist(edges)\n        return line_graph\n\n    def create_embeddings(self, graphs):\n        \n        graphs = [self._create_line_graph(graph) for graph in graphs]\n        docs = [Wesfeiler_Lehman_IsoMap(graph, self.wl_iterations) for graph in graphs]\n        docs = [TaggedDocument(words=doc.get_graph_features(), tags=[str(i)]) for i, doc in enumerate(docs)]\n\n        model = Doc2Vec(docs,\n                        vector_size=self.dimensions,\n                        window=0,\n                        min_count=self.min_count,\n                        dm=0,\n                        sample=self.down_sampling,\n                        workers=self.workers,\n                        \n                        alpha=self.learning_rate,\n                        seed=self.seed)\n        model.build_vocab(docs, progress_per=2)\n        model.train(\n               docs, total_examples=model.corpus_count, epochs=20, \n               report_delay=1\n                )\n        print('complete')\n        l=list(self.graph.nodes)\n        \n        embedding=np.array([model.docvecs[str(n)] for n,_ in enumerate(docs)])\n        self._embedding.append(embedding)\n        return self._embedding\n    def plot_embedding(self,node_pos,node_colors=None, di_graph=None, labels=None):\n        node_num,embedding_dimension = node_pos.shape\n        if(embedding_dimension > 3):\n            print(\"Embedding dimension greater than 3, use tSNE to reduce it to 3\")\n            model = TSNE(n_components=3)\n            node_pos = model.fit_transform(node_pos)\n\n        if di_graph is None:\n            \n            plt.scatter(node_pos[:, 0], node_pos[:, 1], c=node_colors)\n        else:\n            pos = {}\n            for i in range(node_num):\n                pos[i] = node_pos[i, :]\n            if node_colors is not None:\n                nx.draw_networkx_nodes(di_graph, pos,\n                                   node_color=node_colors,\n                                   width=0.1, node_size=100,\n                                   arrows=False, alpha=0.8,\n                                   font_size=5, labels=labels)\n            else:\n                nx.draw_networkx(di_graph, pos, node_color=node_colors,\n                             width=0.1, node_size=300, arrows=False,\n                             alpha=0.8, font_size=12, labels=labels)\n\n    def node_level_embedding(self,node,embed):\n        embed_node=embed[node]\n        vals=list(self.graph.nodes())\n        def chebyshev_distance(node1,node2):\n            return scipy.spatial.distance.chebyshev(node1,node2)\n        distances=[]\n        questions=[]\n        for i in range(self.graph.number_of_nodes()):\n            if i!=node:\n                distances.append(chebyshev_distance(embed_node,embed[i]))\n                questions.append(vals[i])\n        return vals[node],distances,questions    \n        \n\ng=nx.from_pandas_edgelist(train_df[:500],source='question_body',target='category')\nnx.draw(g)\nlist_graph=[g]*20\ngl2vec=GL2Vec(g,wl_iterations=2, dimensions=128, workers=4, down_sampling=0.0001,\n                 epochs=10, learning_rate=0.025, min_count=5, seed=42)\ngl2vec_embeddings=gl2vec.create_embeddings(list_graph)\nprint(gl2vec_embeddings)\ngl2vec.plot_embedding(gl2vec_embeddings[0])\n# node_num=24\n# node,distances,questions=gl2vec.node_level_embedding(node_num,gl2vec_embeddings)\n# gl2vec_df=pd.DataFrame(columns=['Question','Sample_Question','Chebyshev_Distance'])\n# gl2vec_df['Question']=[node]*len(distances)\n# gl2vec_df['Sample_Question']=questions\n# gl2vec_df['Chebyshev_Distance']=distances\n# gl2vec_df.head()\n# gl2vec_df.to_csv('..\/Gl2Vec_Embeddings.csv',index=False)\n# gl2vec_g=nx.from_pandas_edgelist(gl2vec_df,source='Question',target='Sample_Question',edge_attr='Chebyshev_Distance')\n# G=gl2vec_g\n# plotter(G,'Gl2Vec Embeddings')        ","911f08c6":"## Wesfeiler Lehman Test\n\nThe WL Test produces for each graph a canonical form. If the canonical forms of two graphs are not equivalent, then the graphs are definitively not isomorphic. However, it is possible for two non-isomorphic graphs to share a canonical form, so this test alone cannot provide conclusive evidence that two graphs are isomorphic.\n\nThe Algorithm:\n\nFor iteration  of the algorithm we will be assigning to each node a tuple  containing the node\u2019s old compressed label and a multiset of the node\u2019s neighbors' compressed labels. A multiset is a set (a collection of elements where order is not important) where elements may appear multiple times.\nAt each iteration we will additionally be assigning to each node  a new \u201ccompressed\u201d label  for that node\u2019s set of labels. Any two nodes with the same  will get the same compressed label.\n\n### Finding the Correspondance Between Isomorphic Graphs\n\nThe core idea of the Weisfeiler-Lehman isomorphism test is to find for each node in each graph a signature based on the neighborhood around the node. These signatures can then be used to find the correspondance between nodes in the two graphs, which can be used to check for isomorphism.\n\nIn the algorithm descibed above, the \u201ccompressed labels\u201d serve as the signatures. Since multiple nodes may have the same compressed label, there are multiple possible correspondances suggested by a Weisfeiler-Lehman labeling. The Weisfeiler-Lehman isomorphism test itself does not provide a way of narrowing down the possible correspondances further.\n\n<img src=\"https:\/\/davidbieber.com\/post\/2019-05-10-weisfeiler-lehman-isomorphism-test\/graph-isomorphism-000.png\">\n\n[Blog](https:\/\/davidbieber.com\/post\/2019-05-10-weisfeiler-lehman-isomorphism-test\/)\n[Journal](https:\/\/www.jmlr.org\/papers\/volume12\/shervashidze11a\/shervashidze11a.pdf)","b86649c0":"## Graph Level Embeddings\n\nWe have seen some of the popular node based embeddings for capturing information related to neighbourhoods and degrees. In this case we will be looking into one of the famous algorithms for graph based embeddings (subgraph embeddings) which uses the Wesfeiler Lehman Isomorphic Test, which is used to determine if 2 graphs are similar \/isomorphic based on iterative traversals and hashing of the nodes. The test is then used to create Subgraph embeddings which in this case is the GL2Vec algorithm","f27258c0":"## NEMF\n\nAn implementation of `\"NetMF\" <https:\/\/keg.cs.tsinghua.edu.cn\/jietang\/publications\/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf>`_ from the WSDM '18 paper \"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec\". The procedure uses sparse truncated SVD to learn embeddings for the pooled powers of the PMI matrix computed from powers of the normalized adjacency matrix.\n\nIn this case, we are creating the adjacency matrix as in grarep. We are also creating a pooled adjacency matrix by multiplying the edge weights with the number of edges in the graph and then dividing by the number of dimensions required for in the embedding space. Then we are decomposing the PMI matrix with SVD to learn embedding vectors.\n<img src=\"https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1007%2Fs00439-020-02226-3\/MediaObjects\/439_2020_2226_Fig3_HTML.png\">\n\n","5fa4f57c":"## HOPE\n\nAn implementation of `\"HOPE\" <https:\/\/www.kdd.org\/kdd2016\/papers\/files\/rfp0184-ouA.pdf>`_ from the KDD '16 paper \"Asymmetric Transitivity Preserving Graph Embedding\". The procedure uses sparse SVD on the neighbourhood overlap matrix. The singular value rescaled left and right singular vectors are used as the node embeddings after concatenation.\nA diagram depicting the operation of HOPE is shown here:\n\n<img src=\"https:\/\/d3i71xaburhd42.cloudfront.net\/07627bf7eb649220ffbcdf6bf233e3a4a76e8590\/2-Figure2-1.png\">\n\nIn this case, we are creating an adjacency matrix and then converting it to a sparse matrix which is in triplet format (according to scipy). Then a decomposition method is applied on the sparse matrix (ideally SVD) such that the matrix can be siubdivided into nomralizes\/rescaled left and right submatrix (subvector). The final embedding vector is attained by combining both the left and right counterparts. HOPE is used for preserving the higher order degree based adjacency matrix similarities by decomposing through a standard reduction technique. A walkthrough of the same is present in the video:\n\n[Video](\"https:\/\/www.semanticscholar.org\/paper\/Asymmetric-Transitivity-Preserving-Graph-Embedding-Ou-Cui\/07627bf7eb649220ffbcdf6bf233e3a4a76e8590\/video\/4d967a6b\")","21988a2c":"## Graph Embeddings\n\nThese are different Graph embeddings which bear important resemblance to the node level representations in a particular context. Graph embeddings are  use to capture dofferent aspects of representations arising from node \/subgraph level heuristics. These include relations between the different nodes in a graph, different representations amongst different sub graphs, and projection of the embedding vectors in lower dimensional space. Graph embeddings work on knowledge graph data structures and capture representations based on different factors. These representations are encoded in a lower dimensional space where each input representation is mapped to. The metrics can be based on edge weights, neighbourhood count, degrees ,centralities, heat kernels and many other factors. The most simplistic idea of generating a graph embedding is to create a representation where 2 nodes ```x``` and ```y``` can be represented as ```grad(|x-y|w)``` where w is the edge weight connecting the nodes. There are different types of graph embeddings such as node embeddings, structural embeddings, graph embeddings, community based embeddings, geometric embeddings and others. In this notebook, we will be looking at some of the most popular node embeddings which can be used to create representations of a graph along with the associated research papers. The following shows a representation of how subgraph level instructions are encoded in a  lower dimensional vector.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1212\/1*qYGg0y_0hRdITlMIT5TadA.png\">\n","44707686":"## Laplacian Eigenvalues\n\nSolve the generalized eigenvalue problem:\n\n                                                   Lv=\u03bbDv\n\nThe Laplacian Eigenmap uses the smallest eigenvectors. But not the very smallest eigenvector, v1, which is constant (we can scale it to be a vector of 1s), and corresponds to an eigenvalue of zero. So if you want to reduce to two dimensions, use the second-smallest and third-smallest eigenvectors.\n\nLet\u2019s do a brief bit of rearranging:\n\n                                  Lv=\u03bbDv(Dexp(\u22121))Lv=\u03bbv(D\u22121D\u2212D\u22121W)v=\u03bbv(I\u2212P)v=\u03bbvLrwv=\u03bbv\n\nSo it turns out that the standard eigenvalue problem with Lrw will produce the same results as the generalized eigenvalue problem with L and D. A non-generalized eigenvalue problem is preferable to the generalized problem, at least in R, because generalized problems require installing the CRAN package geigen. You could even use the eigenvectors of P, although you have to bear in mind that the eigenvalues of P differ from Lrw although in that case the order of the eigenvectors are reversed, i.e. you want those associated with the largest eigenvalues, ignoring the top eigenvector (which is the constant eigenvector). P might even be preferable because it\u2019s ever-so-slightly less work to calculate than Lrw. We\u2019ll revisit the relationship between Lrw and P when we talk about diffusion maps.\n\n### Output\n\nNow that you have k eigenvectors, stack them columnwise to form an N x k matrix (let\u2019s call it Y):\n\n                                             Y=[v2|v3|\u2026|vk]\n\nwhere, as noted above, we are not using the uninformative smallest eigenvector, v1. The rows of that matrix are the coordinates of the graph vertices in the reduced dimension, i.e. the ith row of the 2D Laplacian Eigenmap representing vertex i would be:\n\nyi=(vi,2,vi,3)\n\nThe Connection with Locally Linear Embedding\nThe Laplacian Eigenmap paper demonstrates a connection between LE and LLE, in that LLE is approximately computing the eigenvectors of L2, which has the same eigenvectors as L (and the square of the eigenvalues).\n\nThe paper on [Laplacian Eigen maps](https:\/\/papers.nips.cc\/paper\/2001\/file\/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf)\n[Blog](http:\/\/www.dakotamurray.me\/post\/graph_laplacian_eigenmap\/)\n\n<img src=\"https:\/\/ww2.mathworks.cn\/matlabcentral\/mlc-downloads\/downloads\/submissions\/36141\/versions\/2\/screenshot.png\">\n\nIn many cases the Eigen map can be used to represent node level representations in a social network:\n\n<img src=\"http:\/\/www.dakotamurray.me\/post_images\/graph_laplacian_eigenmaps\/karate_graph.png\">","312b4de5":"## A Driver for Plotting the Graphs\n\nThe below code sample is used for drawing the graphs created with any kind of node level representational embeddings.","a8cc2680":"## Walklets \n\nWalklets are based on randomized deep walks in a neighbour of a particular node to create embeddings. These operate in a manner similar to node2vec\/deepwalk but skip some internal nodes in the walk stage. This is greatly used for capturing multi hop based representation patterns corresponding to a particular node embedding. The [paper](https:\/\/arxiv.org\/abs\/1605.02115) provides this idea. \n\nThe abstract reads as follows:\n\n```We present Walklets, a novel approach for learning multiscale representations of vertices in a network. In contrast to previous works, these representations explicitly encode multiscale vertex relationships in a way that is analytically derivable. Walklets generates these multiscale relationships by subsampling short random walks on the vertices of a graph. By `skipping' over steps in each random walk, our method generates a corpus of vertex pairs which are reachable via paths of a fixed length. This corpus can then be used to learn a series of latent representations, each of which captures successively higher order relationships from the adjacency matrix. We demonstrate the efficacy of Walklets's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, DBLP, Flickr, and YouTube. Our results show that Walklets outperforms new methods based on neural matrix factorization. Specifically, we outperform DeepWalk by up to 10% and LINE by 58% Micro-F1 on challenging multi-label classification tasks. Finally, Walklets is an online algorithm, and can easily scale to graphs with millions of vertices and edges.```\n\n\n<img src=\"https:\/\/d3i71xaburhd42.cloudfront.net\/37cf46e45777e67676f80c9110bed675a9840590\/7-Figure4-1.png\">\n\nIn this implementation, we have created a Random Walker Template from the previous node2vec approach.\n```python\nfor step in range(power+1):\n                neighbors = [n for i, n in enumerate(walk[step:]) if i % power == 0]\n                walklets.append(neighbors)\n```\nWhere skipping happens based in the count of the neighbours of a particular node. A Word2Vec model is then trained based on negative skipgram sampling to create the new embedding vectors for the walked paths.\n","8f1e65c7":"## GraRep\n\nAn implementation of `\"GraRep\" <https:\/\/dl.acm.org\/citation.cfm?id=2806512>`_from the CIKM '15 paper \"GraRep: Learning Graph Representations with Global Structural Information\". The procedure uses sparse truncated SVD to learn embeddings for the powers of the PMI matrix computed from powers of the normalized adjacency matrix.\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQ0i5YeMIs2BkTNd4PVQleBaJMqC70Ag5SbsQ&usqp=CAU\">\n\nThe abstract of the paper reads as follows:\n\n```In this paper, we present GraRep, a novel model for learning vertex representations of weighted graphs. This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. as well as the skip-gram model with negative sampling of Mikolov et al. We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization. Empirical results demonstrate that our representation significantly outperforms other state-of-the-art methods in such tasks.```\n\nGrarep uses multihop similarity by running on k nearest neighbours.In this implementation, we create the adjacency matrix and normalize the weights with the degree values ```(1\/degree(node))```. Then we use the degree matrix created and multiply it with the adjacency matrix. The loss function followed here is similar to the one shown in the below image:\n\n<img src=\"https:\/\/i.imgur.com\/Ia2wvcZ.png\">\n\nSince we are concerned with reducing the effective distance between the vectors, we take the lower scores,which is better known as the Pointwise Mutual Information PMI matrix. The remaining part is the same as decomposing the vector space with truncated svd followed in HOPE. \n[Resource](http:\/\/snap.stanford.edu\/proj\/embeddings-www\/files\/nrltutorial-part1-embeddings.pdf)","1fbadd15":"## GL2Vec\n\nThe GL2Vec algorithm creates the line graph of each graph in the graph dataset.\nThe procedure creates Weisfeiler-Lehman tree features for nodes in graphs. \nUsing these features a document (graph) - feature co-occurence matrix is decomposed in order to generate representations for the graphs.\nThe procedure assumes that nodes have no string feature present and the WL-hashing defaults to the degree centrality. \n\n[Paper](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-030-36718-3_1)\n\n<img src=\"https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-36718-3_1\/MediaObjects\/493010_1_En_1_Fig1_HTML.png\">\n\nThis implementation uses a document vectorization technique (Doc2Vec) for creating a higher order representation of deocument vectors. These document vectors are created by performing the WL test on the nodes of the graph. The PMI matrix at a subgraph level is then used to represent the embedding space. "}}