{"cell_type":{"23a29017":"code","94ea1f25":"code","6dd54059":"code","8c44dd6c":"code","4e608d76":"code","720d6677":"code","26033de3":"code","6f421838":"code","fc6bbbc3":"code","5bb4df07":"code","b624d794":"markdown","65817add":"markdown","2a5b8327":"markdown","909ff599":"markdown"},"source":{"23a29017":"# import libraries\nimport gc\nimport os\nimport sys\nimport PIL\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\nfrom keras.layers import Conv2D, Dense, Flatten, GlobalAveragePooling2D, GlobalMaxPool2D, Dropout, MaxPooling2D\nfrom keras.models import Model\n!pip install efficientnet==0.0.4\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.xception import Xception\nfrom efficientnet import EfficientNetB3","94ea1f25":"def build_model(model_name=None, include_top=False, input_shape=(256,256,3), fine_tuning=True, layer_to_freeze=None, load_pretrained : str = None, summary=False):\n    pmodel_name = model_name.strip().lower()\n    print(pmodel_name)\n\n    if pmodel_name == 'resnet50': base_model = ResNet50(include_top=include_top, input_shape=input_shape)\n    elif pmodel_name == 'inception_v3' : base_model = InceptionV3(include_top=include_top, input_shape=input_shape)\n    elif pmodel_name == 'xception' : base_model = Xception(include_top=include_top, input_shape=input_shape)\n    elif pmodel_name == 'efficient_net' : base_model = EfficientNetB3(include_top=include_top, input_shape=input_shape)\n    else : raise ValueError\n\n    if fine_tuning:\n        # Freese layers\n        assert layer_to_freeze != None, 'You must define layer\\'s name to freese.'\n        fr_layer_name = layer_to_freeze\n        set_trainable = False\n\n        for layer in base_model.layers:\n            if not layer.name == fr_layer_name:\n                set_trainable = True\n\n            layer.trainable = set_trainable\n\n    # change last layers\n    last_1dconv_1 = Conv2D(1024, 1, activation='relu')(base_model.output)\n    last_pool_1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(last_1dconv_1)\n    global_avg_pool = GlobalAveragePooling2D()(last_pool_1)\n#     last_Dense_1 = Dense(512, activation='relu')(global_avg_pool)\n    last_Dense_2 = Dense(196, activation='softmax')(global_avg_pool)\n\n    # compile\n    model = Model(base_model.input, last_Dense_2)\n\n    # summary\n    if summary:\n        model.summary()\n\n    # load pretrained weights\n    if load_pretrained:\n        model.load_weights(load_pretrained)\n\n    return model\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2 * ((precision * recall) \/ (precision + recall + K.epsilon()))\n\n# Define steps per epoch\ndef get_steps(num_samples, batch_size):\n    if (num_samples \/\/ batch_size) > 0:\n        return (num_samples \/\/ batch_size) + 1\n    else:\n        return num_samples \/\/ batch_size\n\n# https:\/\/www.kaggle.com\/seriousran\/cutout-augmentation-on-keras-efficientnet\ndef get_random_erazer(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1\/0.3, v_l=0, v_h=255, pixel_level=False):\n    def erazer(input_img):\n        img_h, img_w, img_c = input_img.shape\n        p_1 = np.random.rand()\n\n        if p_1 > p :\n            return input_img\n\n        while True:\n            s = np.random.uniform(s_l, s_h) * img_h * img_w\n            r = np.random.uniform(r_1, r_2)\n            w = int(np.sqrt(s\/r))\n            h = int(np.sqrt(s*r))\n            left = np.random.randint(0, img_w)\n            top = np.random.randint(0, img_h)\n\n            if left + w <= img_w and top + h <= img_h:\n                break\n\n        if pixel_level:\n            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n        else:\n            c = np.random.uniform(v_l, v_h)\n\n        input_img[top:top + h, left:left + w, :] = c\n\n        return input_img\n\n    return erazer\n\n\n# For prediction\ndef predict_class(model, num_samples, batch_size, train_generator, test_generator, output_name, DATA_PATH = '\/kaggle\/input\/2019-3rd-ml-month-with-kakr', OUTPUT_PATH = '.\/output'):\n\n    # Prediction\n    prediction = model.predict_generator(\n        generator=test_generator,\n        steps=get_steps(num_samples, batch_size),\n        verbose=1\n    )\n\n    predicted_indices = np.argmax(prediction, axis=1)\n    labels = (train_generator.class_indices)\n    labels = dict((v, k) for k, v in labels.items())\n    predictions = [labels[k] for k in predicted_indices]\n    \n    # Load submission form\n    submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n    submission['class'] = predictions\n    submission.to_csv(os.path.join(OUTPUT_PATH, '{}.csv'.format(output_name)), index=False)\n\ndef predict_class_ensemble(models, weights, num_samples, batch_size, train_generator, test_generator, DATA_PATH = '\/kaggle\/input\/2019-3rd-ml-month-with-kakr', OUTPUT_PATH = '.\/output'):\n    predictions = []\n\n\n    if not models == None:\n        num_models = len(models)\n\n        if not weights == None:\n            num_weights = len(weights)\n            for model_name in models:\n                for i, weight_name in enumerate(weights):\n                    print('=== predict {0} model - {1}\\'s split ==='.format(model_name, i))\n                    model = build_model(model_name=model_name, input_shape=(299,299,3), fine_tuning=False, summary=False)\n                    model.load_weights(os.path.join('\/kaggle\/input\/3rd-ml-month-efficeintnet-5-folds', weight_name))\n\n                    test_generator.reset()\n\n                    prediction = model.predict_generator(\n                        generator=test_generator,\n                        steps=get_steps(num_samples, batch_size),\n                        verbose=1\n                    )\n\n                    predictions.append(prediction)\n\n        else:\n            for model_name in models:\n                print('=== predict {} model ==='.format(model_name))\n                model = build_model(model_name=model_name, input_shape=(299, 299, 3), fine_tuning=False, summary=False)\n                model.load_weights(os.path.join('\/kaggle\/input\/3rd-ml-month-efficeintnet-5-folds', '{}_model_50_epochs.h5'.format(model_name)))\n\n                # \ud55c\ubc88 \uc608\uce21 \ud6c4\uc5d0\ub294 \ubc18\ub4dc\uc2dc \ub9ac\uc14b \ud544\uc218!\n                test_generator.reset()\n\n                prediction = model.predict_generator(\n                    generator=test_generator,\n                    steps=get_steps(num_samples, batch_size),\n                    verbose=1\n                )\n\n                print(np.argmax(prediction, axis=-1)[:10])\n\n                predictions.append(prediction)\n\n        print('Complete!')\n        predictions = np.array(predictions)\n        print(predictions.shape)\n\n        predictions = np.mean(predictions, axis=0)\n\n        # \uc81c\ucd9c \ud615\uc2dd\uc73c\ub85c \ubcc0\ud658\n        predict_indices = np.argmax(predictions, axis=-1)\n        labels = (train_generator.class_indices)\n        labels = dict((v,k) for k,v in labels.items())\n        prediction_ensemble = [labels[k] for k in predict_indices]\n\n        # \uc81c\ucd9c\n        output_name = 'ensemble_' + ','.join(models) + '_{}_splits.csv'.format(num_weights)\n        submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n        submission['class'] = prediction_ensemble\n        submission.to_csv(os.path.join(OUTPUT_PATH, output_name), index=False)\n        \ndef crop_boxing_img(img, pos, margin=16):\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(width, pos[2] + margin)\n    y2 = min(height, pos[3] + margin)\n\n    cropped_img = img.crop((x1,y1,x2,y2))\n    # plt.imshow(cropped_img)\n    # plt.show()\n    return cropped_img","6dd54059":"# set random seed\nRANDOM_SEED = 40\n\n# \ub370\uc774\ud130 \uacbd\ub85c \uc124\uc815\nDATA_PATH = '\/kaggle\/input\/2019-3rd-ml-month-with-kakr'\nOUTPUT_PATH = '.\/cropped_images'\nos.listdir(DATA_PATH)\n\n# \uc774\ubbf8\uc9c0 \uacbd\ub85c \uc124\uc815\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')","8c44dd6c":"# generate cropped images\nif not os.path.exists('.\/output'):\n    os.mkdir('.\/output')\nif not os.path.exists('.\/cropped_images'):\n    os.mkdir('.\/cropped_images')\n    \ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n\nif not os.path.exists('.\/cropped_images\/train_crop'):\n    os.mkdir('.\/cropped_images\/train_crop')\n\nif not os.path.exists('.\/cropped_images\/test_crop'):\n    os.mkdir('.\/cropped_images\/test_crop')\n\n# \ud6c8\ub828 \uc774\ubbf8\uc9c0 \uc790\ub974\uae30\nfor i, img_name in tqdm(enumerate(df_train['img_file'])):\n    img = PIL.Image.open(os.path.join(TRAIN_IMG_PATH, img_name))\n    pos = df_train.iloc[i][['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n    cropped_img = crop_boxing_img(img, pos)\n    cropped_img.save(os.path.join(OUTPUT_PATH, 'train_crop\/'+img_name))\n\n# \uc2dc\ud5d8 \uc774\ubbf8\uc9c0 \uc790\ub974\uae30\nfor i, img_name in tqdm(enumerate(df_test['img_file'])):\n    img = PIL.Image.open(os.path.join(TEST_IMG_PATH, img_name))\n    pos = df_test.iloc[i][['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n    cropped_img = crop_boxing_img(img, pos)\n    cropped_img.save(os.path.join(OUTPUT_PATH, 'test_crop\/' + img_name))","4e608d76":"# \ub370\uc774\ud130 \uacbd\ub85c \uc124\uc815\nDATA_PATH = '\/kaggle\/input\/2019-3rd-ml-month-with-kakr'\nOUTPUT_PATH = '.\/output'\nos.listdir(DATA_PATH)\n\n# \uc774\ubbf8\uc9c0 \uacbd\ub85c \uc124\uc815\nTRAIN_IMG_PATH = os.path.join('.\/cropped_images', 'train_crop')\nTEST_IMG_PATH = os.path.join('.\/cropped_images', 'test_crop')\n\n# \ub370\uc774\ud130 \uc77d\uc5b4\uc624\uae30\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n\n# \ubaa8\ub378\ub9c1\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44\ndf_train['class'] = df_train['class'].astype('str')\n\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]","720d6677":"# Parameters\nimg_size = (299, 299)\nepochs = 50\nbatch_size = 16\nlearning_rate = 0.0002\nbase_model = 'efficient_net'\nload_pretrained = None\npatience = 5\nn_splits=1","26033de3":"# Define Generator config\ntrain_datagen = ImageDataGenerator(\n    horizontal_flip = True,\n    vertical_flip = False,\n    zoom_range = 0.10,\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range=0.5,\n    brightness_range=[0.5, 1.5],\n    fill_mode='nearest',\n    rescale=1.\/255,\n    preprocessing_function=get_random_erazer(v_l=0, v_h=255)\n)\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","6f421838":"# Train, Test spllit\nsplitter = StratifiedShuffleSplit(n_splits=n_splits, train_size=0.9, test_size=0.1, random_state=RANDOM_SEED)\n\nscores = []","fc6bbbc3":"# \ud559\uc2b5\nfor i, (trn_idx, val_idx) in enumerate(splitter.split(df_train['img_file'], df_train['class'])):\n\n    print('============', '{}\\'th Split'.format(i), '============\\n')\n\n    X_train = df_train.iloc[trn_idx].copy()\n    X_val = df_train.iloc[val_idx].copy()\n    print('Train : {0} \/ Test : {1}'.format(X_train.shape, X_val.shape))\n\n    train_size = len(X_train)\n    val_size = len(X_val)\n\n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe = X_train,\n        directory = TRAIN_IMG_PATH,\n        x_col = 'img_file',\n        y_col = 'class',\n        target_size = img_size,\n        color_mode = 'rgb',\n        class_mode = 'categorical',\n        batch_size = batch_size,\n        seed=RANDOM_SEED,\n        shuffle=True,\n        interploation = 'bicubic'\n    )\n\n    val_generator = val_datagen.flow_from_dataframe(\n        dataframe = X_val,\n        directory = TRAIN_IMG_PATH,\n        x_col = 'img_file',\n        y_col = 'class',\n        target_size = img_size,\n        color_mode = 'rgb',\n        class_mode = 'categorical',\n        batch_size = batch_size,\n        seed=RANDOM_SEED,\n        shuffle=True,\n        interploation = 'bicubic'\n    )\n\n    test_generator = test_datagen.flow_from_dataframe(\n        dataframe = df_test,\n        directory = TEST_IMG_PATH,\n        x_col = 'img_file',\n        y_col = None,\n        target_size = img_size,\n        color_mode = 'rgb',\n        class_mode = None,\n        batch_size = batch_size,\n        shuffle=False\n    )\n\n    # \ud6c8\ub828 \ub8e8\ud2f4\n    \"\"\"\n    model = build_model(model_name=base_model, input_shape=(299, 299, 3), fine_tuning=False, summary=False)\n\n    # Load pretrained\n    if load_pretrained is not None:\n        print('Loading Pretrained networks {}'.format(load_pretrained))\n        model.load_weights(os.path.join(os.getcwd(), '.\/weights\/' + load_pretrained))\n\n\n    optimizer = optimizers.adam(lr=learning_rate)\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', f1_m])\n\n    if not os.path.exists('.\/weights'):\n        os.mkdir('.\/weights')\n\n    # checkpoint_filename = 'inception_v3_model_{0}_epochs.h5'.format(epochs)\n    if n_splits > 1:\n        checkpoint_filename = '{0}_model_{1}_epochs_split_{2}.h5'.format(base_model,epochs, i)\n    else:\n        checkpoint_filename = '{0}_model_{1}_epochs.h5'.format(base_model, epochs)\n\n\n    # Define callbacks\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=int(patience \/ 2),\n        verbose=1,\n        mode='min',\n        min_lr=0.0000001\n    )\n\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        min_delta=0.000001,\n        patience=patience,\n        verbose=1,\n        mode='min'\n    )\n\n    model_checkpoint = ModelCheckpoint(\n        filepath=os.path.join('.\/weights', checkpoint_filename),\n        monitor='val_loss',\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode='min',\n        period=5\n    )\n\n    callbacks = [reduce_lr, early_stopping, model_checkpoint]\n\n    # \ud6c8\ub828\n    history = model.fit_generator(\n        generator = train_generator,\n        steps_per_epoch = get_steps(train_size, batch_size),\n        epochs = epochs,\n        callbacks = callbacks,\n        validation_data = val_generator,\n        validation_steps = get_steps(val_size, batch_size)\n    )\n    \"\"\"\n    ## \ub2e8\uc77c\ubaa8\ub378 \uc608\uce21\n    # Predict class\n    # predict_class(model, df_test.shape[0], batch_size, train_generator, test_generator, checkpoint_filename[:-3])\n\n    # Predict ensemble\n    # models = ['inception_v3', 'resnet50', 'xception', 'efficient_net']\n    weights = ['efficient_net_model_50_epochs_split_0.h5',\n               'efficient_net_model_50_epochs_split_1.h5',\n               'efficient_net_model_50_epochs_split_2.h5',\n               'efficient_net_model_50_epochs_split_3.h5',\n               'efficient_net_model_50_epochs_split_4.h5']\n    predict_class_ensemble(['efficient_net'], weights, df_test.shape[0], batch_size, train_generator, test_generator)\n\n    gc.collect()","5bb4df07":"# output file\uc774 \uc9c0\ub098\uc9c0\uac8c \ub9ce\uc544\uc9c0\uae30\ub54c\ubb38\uc5d0 \uc804\ucc98\ub9ac\ud55c \uc0ac\uc9c4\uc740 \uc0ad\uc81c\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.\n!rm -r cropped_images","b624d794":"# 3rd ML Month : Car Class Classification\n\uc548\ub155\ud558\uc138\uc694! \ub450 \ubc88\uc9f8 \ub300\ud68c\uc5d0 \uc774\uc5b4\uc11c \ucc98\uc74c\uc73c\ub85c \uc774\ubbf8\uc9c0 \ub300\ud68c\uc5d0 \ucc38\uac00\ud574\ubcf4\ub294 4\ud559\ub144 \uc7ac\ud559\uc911\uc778 \ucc3d\uc6d0\uc758 \ud559\ubd80\uc0dd\uc785\ub2c8\ub2e4!<br>\ub4e4\uc5b4\uac00\uae30\uc5d0 \uc55e\uc11c \ub300\ud68c\ub97c \uac1c\ucd5c\ud558\uace0 \uad00\ub9ac\ud574\uc8fc\uc2e0 \uad00\ub9ac\uc790 \ubd84\ub4e4\uacfc \uc9c0\uc2dd\uc744 \uacf5\uc720\ud574\uc8fc\uc2e0 \ubaa8\ub4e0 \ubd84\ub4e4\uc5d0\uac8c \uae4a\uc740 \uac10\uc0ac\ub97c \ub4dc\ub9bd\ub2c8\ub2e4!","65817add":"* \uc774\ubc88 \ub300\ud68c\uc5d0\uc11c 25\uc704\ub97c \uae30\ub85d\ud55c \ucee4\ub110\uc785\ub2c8\ub2e4.\n* \ud30c\uc774\uc36c \uc2a4\ud06c\ub9bd\ud2b8\ub85c \uc791\uc131\ud55c \uac83\uc744 \uc2dc\uac04 \uad00\uacc4\uc0c1 \ud6c8\ub828 \uacfc\uc815\uc740 \ube7c\uace0\n* \ubbf8\ub9ac \ud6c8\ub828\uc2dc\ucf1c\ub193\uc740 \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud574 \ud074\ub798\uc2a4\ub97c \uc608\uce21\ud558\ub294 \uacfc\uc815\ub9cc \ub123\uc5c8\uc2b5\ub2c8\ub2e4.\n* \ucc38\uace0\ud55c \ucf54\ub4dc\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.(\ub9ce\uc740 \uacf5\ubd80\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ubaa8\ub450\uc5d0\uac8c \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4!)\n    * [3rd ML Month baseline code : TaeJin Kim\ub2d8](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)\n    * [Car Image Cropping : Taemyung Heo\ub2d8](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping-updated-7-10)\n    * [Cutout Augmentation (on Keras EfficientNet) : Chanran Kim\ub2d8](https:\/\/www.kaggle.com\/seriousran\/cutout-augmentation-on-keras-efficientnet)","2a5b8327":"### Modeling\n* VGG19, Inception_v3, Resnet50, Xception, EfficientNetB3 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n* \ubaa8\ub378 \ubaa8\ub450 Imagenet\uc73c\ub85c pretrained\ub41c \uac00\uc911\uce58\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.\n* \ud074\ub798\uc2a4\ubcc4\ub85c \uade0\ub4f1\ud55c \uc591\uc744 \ucd94\ucd9c\ud574 \ud559\uc2b5\ud558\uae30\uc704\ud574\uc11c Stratified \ubd84\ub9ac\ub97c \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4.\n* StratifiedKfold \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud574 5\uac1c\uc758 fold\ub85c \ubd84\ub9ac\ud574 \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\ub824\uace0\ud558\ub2c8 \uac80\uc99d\uc6a9 \ub370\uc774\ud130\uc758 \uc591\uc774 \ubd88\ud544\uc694\ud558\uac8c \ub9ce\uc544\uc9c4\ub2e4\uace0 \uc0dd\uac01\ud574 StratifiedShuffleSplit\uc744 \uc0ac\uc6a9\ud558\uc5ec train \ub370\uc774\ud130 90%, val \ub370\uc774\ud130 10% \ube44\uc728\ub85c \ub098\ub204\ub3c4\ub85d \ud588\uc2b5\ub2c8\ub2e4.\n* \uac8c\uc73c\ub984\uc744 \ud53c\uc6b4 \ud0d3\uc5d0 \ub9ce\uc740 \uc2e4\ud5d8\uc744 \ud574\ubcf4\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4..\u3160 \uadf8 \uc911 \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc558\ub358 efficient_ent_B3 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c 5folds \uc559\uc0c1\ube14\ub9c1\uc744 \ud574 \ucd5c\uc885\uc81c\ucd9c\uc744 \ud558\uc600\uc2b5\ub2c8\ub2e4.","909ff599":"## \ud6c8\ub828 \uacfc\uc815\n### Feature Engineering\n* bounding box \uc815\ubcf4\uac00 \ud6c8\ub828, \uc2dc\ud5d8 \ub370\uc774\ud130\uc5d0 \ub2e4\ud589\ud788 \ub2e4 \uc788\uc5b4\uc11c \uc0ac\uc9c4\ub4e4\uc744 \uc804\ubd80 \ucc28 \ud06c\uae30\uc815\ubcf4\ub9cc \ub0a8\ub3c4\ub85d Taemyung Heo\ub2d8\uc758 Cropping\uc744 \uc801\uc6a9 \ud588\uc2b5\ub2c8\ub2e4.\n* Keras\uc758 ImageDataGenerator\ub97c \uc0ac\uc6a9\ud574 data augmentation\uc744 \ud588\uc2b5\ub2c8\ub2e4.\n* Chanran Kim \ub2d8\uc758 Cutout Augmentation\uc744 \uc801\uc6a9\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4."}}