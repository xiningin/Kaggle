{"cell_type":{"23c11eb9":"code","8024b62c":"code","9181a0ad":"code","599a943d":"code","9d266f30":"code","eb993c5a":"code","1de340b9":"code","de8e3312":"code","98b149ed":"code","bbec69b0":"code","47c60791":"code","47a9193e":"code","0496065d":"code","5ca60c86":"code","686ac95c":"code","9df5b69a":"code","0b622303":"code","1b981e2a":"code","be2ba9ed":"code","189a0a6d":"code","5b266e6c":"code","b1568ab9":"code","90514cdf":"code","56b22992":"code","899fea69":"code","0ec015bf":"code","8985b1ad":"code","e658b498":"code","3e6d8ee6":"code","00baf0cd":"code","b9317b18":"code","a9e96bba":"code","337ddc57":"code","89a8fdf2":"code","fbfd5e2f":"code","c7a9c523":"code","31d1de03":"code","498144be":"code","55e3c1cb":"code","ee5b083c":"code","aefc0bc5":"code","bf5d9d9a":"code","6ff78190":"code","f89d9e3b":"code","3334ad9c":"code","be30db11":"code","5682391c":"code","dc7b79fc":"code","a6005162":"code","966a72ac":"code","ccd781e7":"code","fe54e42e":"code","b1384268":"code","160e188f":"code","529ca63e":"code","79a7becf":"code","eb32dd2b":"code","c76aff61":"code","523eca02":"code","9180e797":"code","48a87e9c":"code","5e068114":"code","b25972d2":"code","605a1fe5":"code","7d8ed88f":"code","bd0edd76":"code","a2756ab0":"code","bd87382e":"code","f7f856a3":"markdown","28f5e222":"markdown","7f96b0f8":"markdown","ba064ff0":"markdown","deee3267":"markdown","2ac0bac3":"markdown","be77f3d6":"markdown","5a06099d":"markdown","64a34f0b":"markdown","98ebddd4":"markdown","da383296":"markdown","eba506d6":"markdown","b6a0bc10":"markdown","0c7498e1":"markdown","dcc37a1c":"markdown","4b991c6b":"markdown","c840b1a6":"markdown","1e0a9a60":"markdown","fae26207":"markdown","9e4aedf8":"markdown","16564ec9":"markdown","548237ec":"markdown","0170d857":"markdown","bdfa2fd6":"markdown","5a1b47b1":"markdown","e1e95bfb":"markdown","1872a5a0":"markdown","e5481949":"markdown","2f7a4234":"markdown","cd967d98":"markdown","dcd671aa":"markdown","a5cfb20d":"markdown","c8eee708":"markdown","6e7d53c7":"markdown","7c7b0dfe":"markdown","af7e83d3":"markdown","57f2277b":"markdown","308dfc47":"markdown","39852c00":"markdown","c078b2c5":"markdown","d0a1e200":"markdown","bc0a0c25":"markdown","40347692":"markdown","b90361d7":"markdown","c106508d":"markdown","9df4579a":"markdown","b6d19f9e":"markdown","ae5d6e5d":"markdown","f55421a1":"markdown","c3591ebf":"markdown","d80ab9c1":"markdown","e2152105":"markdown"},"source":{"23c11eb9":"# Put these at the top of every notebook, to get automatic reloading and inline plotting\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","8024b62c":"# This file contains all the main external libs we'll use\nfrom fastai.imports import *","9181a0ad":"from fastai.transforms import *\nfrom fastai.conv_learner import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.sgdr import *\nfrom fastai.plots import *","599a943d":"PATH = \"..\/input\/\"\nTMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\/\"\nsz=224","9d266f30":"torch.cuda.is_available()","eb993c5a":"torch.backends.cudnn.enabled","1de340b9":"PATH","de8e3312":"os.listdir(PATH)","98b149ed":"fnames = np.array([f'train\/{f}' for f in sorted(os.listdir(f'{PATH}train'))])\nlabels = np.array([(0 if 'cat' in fname else 1) for fname in fnames])","bbec69b0":"len(os.listdir(f'{PATH}train'))","47c60791":"len(os.listdir(f'{PATH}test'))","47a9193e":"os.listdir(f'{PATH}test')[:5]","0496065d":"img = plt.imread(f'{PATH}{fnames[1]}')\nplt.imshow(img);\n#files = os.listdir(f'{PATH}test\/cats')[:5]\n#files","5ca60c86":"# img = plt.imread(f'{PATH}valid\/cats\/{files[1]}')\n# plt.imshow(img);","686ac95c":"img.shape","9df5b69a":"img[:4,:4]","0b622303":"# Uncomment the below if you need to reset your precomputed activations\n# shutil.rmtree(f'{PATH}tmp', ignore_errors=True)","1b981e2a":"arch=resnet34\n# data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))\n# learn = ConvLearner.pretrained(arch, data, precompute=True)\ndata = ImageClassifierData.from_names_and_array(\n    path=PATH, \n    fnames=fnames, \n    y=labels, \n    classes=['dogs', 'cats'], \n    test_name='test', \n    tfms=tfms_from_model(arch, sz)\n)\nlearn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)\nlearn.fit(0.01, 2)","be2ba9ed":"# tfms = tfms_from_model(resnet34, sz)","189a0a6d":"# data = ImageClassifierData.from_names_and_array(\n#     path=PATH, \n#     fnames=fnames, \n#     y=labels, \n#     classes=['dogs', 'cats'], \n#     test_name='test', \n#     tfms=tfms_from_model(arch, sz)\n# )","5b266e6c":"# learn = ConvLearner.pretrained(resnet34, data, precompute=True)","b1568ab9":"# learn.fit(1e-2, 1)","90514cdf":"# This is the label for a val data\ndata.val_y","56b22992":"# from here we know that 'cats' is label 0 and 'dogs' is label 1.\ndata.classes","899fea69":"# this gives prediction for validation set. Predictions are in log scale\nlog_preds = learn.predict()\nlog_preds.shape","0ec015bf":"log_preds[:10]","8985b1ad":"preds = np.argmax(log_preds, axis=1)  # from log probabilities to 0 or 1\nprobs = np.exp(log_preds[:,1])        # pr(dog)","e658b498":"def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\ndef rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)","3e6d8ee6":"def plots(ims, figsize=(12,6), rows=1, titles=None):\n    f = plt.figure(figsize=figsize)\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, len(ims)\/\/rows, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i])","00baf0cd":"def load_img_id(ds, idx): return np.array(PIL.Image.open(PATH+ds.fnames[idx]))\n\ndef plot_val_with_title(idxs, title):\n    imgs = [load_img_id(data.val_ds,x) for x in idxs]\n    title_probs = [probs[x] for x in idxs]\n    print(title)\n    return plots(imgs, rows=1, titles=title_probs, figsize=(16,8))","b9317b18":"# 1. A few correct labels at random\nplot_val_with_title(rand_by_correct(True), \"Classificados corretamente\")","a9e96bba":"# 2. A few incorrect labels at random\nplot_val_with_title(rand_by_correct(False), \"Classificados incorretamente\")","337ddc57":"def most_by_mask(mask, mult):\n    idxs = np.where(mask)[0]\n    return idxs[np.argsort(mult * probs[idxs])[:4]]\n\ndef most_by_correct(y, is_correct): \n    mult = -1 if (y==1)==is_correct else 1\n    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), mult)","89a8fdf2":"plot_val_with_title(most_by_correct(0, True), \"Gatos melhor classificados\")","fbfd5e2f":"plot_val_with_title(most_by_correct(1, True), \"C\u00e3es melhor classificados\")","c7a9c523":"plot_val_with_title(most_by_correct(0, False), \"Gatos pior classificados\")","31d1de03":"plot_val_with_title(most_by_correct(1, False), \"C\u00e3es pior classificados\")","498144be":"most_uncertain = np.argsort(np.abs(probs -0.5))[:4]\nplot_val_with_title(most_uncertain, \"Predi\u00e7\u00f5es mais incertas\")","55e3c1cb":"learn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","ee5b083c":"lrf=learn.lr_find()","aefc0bc5":"learn.sched.plot_lr()","bf5d9d9a":"learn.sched.plot()","6ff78190":"tfms = tfms_from_model(resnet34, sz, aug_tfms=transforms_side_on, max_zoom=1.1)","f89d9e3b":"def get_augs():\n    data = ImageClassifierData.from_names_and_array(\n        path=PATH, \n        fnames=fnames, \n        y=labels, \n        classes=['dogs', 'cats'], \n        test_name='test', \n        tfms=tfms,\n        num_workers=1,\n        bs=2\n    )\n    x,_ = next(iter(data.aug_dl))\n    return data.trn_ds.denorm(x)[1]","3334ad9c":"ims = np.stack([get_augs() for i in range(6)])","be30db11":"plots(ims, rows=2)","5682391c":"data = ImageClassifierData.from_names_and_array(\n    path=PATH, \n    fnames=fnames, \n    y=labels, \n    classes=['dogs', 'cats'], \n    test_name='test', \n    tfms=tfms\n)\nlearn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","dc7b79fc":"learn.fit(1e-2, 1)","a6005162":"learn.precompute=False","966a72ac":"learn.fit(1e-2, 3, cycle_len=1)","ccd781e7":"learn.sched.plot_lr()","fe54e42e":"learn.save('224_lastlayer')","b1384268":"learn.load('224_lastlayer')","160e188f":"learn.unfreeze()","529ca63e":"lr=np.array([1e-4,1e-3,1e-2])","79a7becf":"learn.fit(lr, 3, cycle_len=1, cycle_mult=2)","eb32dd2b":"learn.sched.plot_lr()","c76aff61":"learn.save('224_all')","523eca02":"learn.load('224_all')","9180e797":"log_preds,y = learn.TTA()\nprobs = np.mean(np.exp(log_preds),0)","48a87e9c":"accuracy_np(probs, y)","5e068114":"preds = np.argmax(probs, axis=1)\nprobs = probs[:,1]","b25972d2":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, preds)","605a1fe5":"plot_confusion_matrix(cm, data.classes)","7d8ed88f":"plot_val_with_title(most_by_correct(0, False), \"Gatos mais incorretos\")","bd0edd76":"plot_val_with_title(most_by_correct(1, False), \"C\u00e3es mais incorretos\")","a2756ab0":"def binary_loss(y, p):\n    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))","bd87382e":"acts = np.array([1, 0, 0, 1])\npreds = np.array([0.9, 0.1, 0.2, 0.8])\nbinary_loss(acts, preds)","f7f856a3":"Agora que temos uma boa camada final treinada, podemos fazer um ajuste fino das outras camadas. Para dizer ao 'learner' que queremos descongelar(unfreeze) as camadas restantes, basta chamar  `unfreeze()`.\n","28f5e222":"A perda ainda est\u00e1 claramente melhorando em lr = 1e-2 (0,01), e \u00e9 isso que usamos. Observe que a taxa de aprendizado ideal pode mudar conforme treinamos o modelo, portanto, voc\u00ea pode querer executar novamente essa fun\u00e7\u00e3o de tempos em tempos.\n","7f96b0f8":"## Nosso primeiro modelo - in\u00edcio r\u00e1pido.","ba064ff0":"Observe que as outras camadas j\u00e1 foram treinadas para reconhecer fotos da imagenet (enquanto que nossas camadas finais foram inicializadas aleatoriamente), portanto, queremos ter cuidado para n\u00e3o destruir os pesos, cuidadosamente ajustados, que j\u00e1 est\u00e3o l\u00e1.\n\nDe um modo geral, as camadas anteriores (como vimos) t\u00eam mais recursos de prop\u00f3sito geral. Portanto, esperamos que elas precisem de menos ajuste fino para novos conjuntos de dados. Por essa raz\u00e3o, usaremos diferentes taxas de aprendizado para diferentes camadas: as primeiras camadas ter\u00e3o 1e-4, as camadas intermedi\u00e1rias em 1e-3 e nossas camadas FC ficar\u00e3o em 1e-2 como antes. Referimo-nos a isso como *differential learning rates*, embora n\u00e3o haja um nome padr\u00e3o para essa t\u00e9cnica na literatura, at\u00e9 onde  estamos cientes.\n","deee3267":"A taxa de aprendizado determina qu\u00e3o r\u00e1pido ou qu\u00e3o lento voc\u00ea deseja atualizar os pesos (ou par\u00e2metros). A taxa de aprendizado \u00e9 um dos par\u00e2metros mais dif\u00edceis de definir, porque afeta significativamente o desempenho do modelo.\n\nO m\u00e9todo learn.lr_find () ajuda voc\u00ea a encontrar uma taxa de aprendizado ideal. Ele usa a t\u00e9cnica desenvolvida no paper 2015 Cyclical Learning Rates for Training de Redes Neurais, onde n\u00f3s simplesmente continuamos aumentando a taxa de aprendizado de um valor muito pequeno, at\u00e9 que a perda pare de diminuir. Podemos tra\u00e7ar a taxa de aprendizado entre os lotes(batches) para ver como isso se parece.\n\nPrimeiro criamos um novo learner, pois queremos saber como definir a taxa de aprendizado para um novo modelo (n\u00e3o treinado).\n","2ac0bac3":"Precisamos de um <b>path<\/b> que aponte para o conjunto de dados. Nesse caminho, tamb\u00e9m armazenaremos dados tempor\u00e1rios e resultados finais. `ImageClassifierData.from_paths` l\u00ea dados de um caminho fornecido e cria um conjunto de dados pronto para treinamento.\n","be77f3d6":"### Vendo as fotos novamente","5a06099d":"Here is how the raw data looks like","64a34f0b":"Nosso objeto `learn` cont\u00e9m um atributo `sched` que cont\u00e9m nosso scheduler de taxa de aprendizado e possui alguma funcionalidade gr\u00e1fica interessante, incluindo esta:\n","98ebddd4":"Note que no nosso exemplo acima, nossa precis\u00e3o \u00e9 de 100% e nossa perda \u00e9 de 0,16. Compare isso com uma perda de 0,03 que estamos conseguindo enquanto prevemos c\u00e3es e gatos. Exerc\u00edcio: jogue com as predi\u00e7\u00f5es para obter uma perda menor para este exemplo.\n\nExemplo: Aqui est\u00e1 um exemplo de como calcular a perda(loss) para um exemplo de problema de classifica\u00e7\u00e3o bin\u00e1ria. Suponha que para uma imagem x com r\u00f3tulo 1 e seu modelo forne\u00e7a uma previs\u00e3o de 0,9. Para este caso, a perda(loss) deve ser pequena porque nosso modelo est\u00e1 prevendo um r\u00f3tulo 1 com alta probabilidade.\n\n`loss = -log(0.9) = 0.10`\n\nAgora, suponha que x tenha r\u00f3tulo 0, mas nosso modelo est\u00e1 prevendo 0,9. Neste caso, nossa perda deve ser muito maior.\n\n`loss = -log(1-0,9) = 2.30`\n\nExerc\u00edcio: observe os outros casos e se conven\u00e7a de que isso faz sentido.\nExerc\u00edcio: como voc\u00ea iria reescrever binary_loss usando 'if' em vez de '*' e '+'?\nPor que n\u00e3o apenas maximizar a precis\u00e3o? A perda da classifica\u00e7\u00e3o bin\u00e1ria \u00e9 uma fun\u00e7\u00e3o mais f\u00e1cil de otimizar.\n","da383296":"Vamos criar novos objetos de dados que incluam esta augmentation dentro do transforms.\n","eba506d6":"Bem-vindos \u00e0 oficina de Deep Learning - Vis\u00e3o Computacional.     \nVamos usar redes neurais por convolu\u00e7\u00e3o (CNNs) para ensinar o computador reconhecer imagens.   \nAlgo poss\u00edvel gra\u00e7as ao aprendizado profundo(deep learning).","b6a0bc10":"### Dados aumentados (data augmentation)","0c7498e1":"J\u00e1 que temos um modelo muito bom neste momento, podemos querer salv\u00e1-lo para que possamos carreg\u00e1-lo novamente mais tarde sem termos de trein\u00e1-lo do zero novamente.\n","dcc37a1c":"Outro truque que usamos aqui, \u00e9 adicionar o par\u00e2metro cycle_mult. D\u00ea uma olhada no gr\u00e1fico a seguir e veja se voc\u00ea pode descobrir o que o par\u00e2metro est\u00e1 fazendo:\n","4b991c6b":"Vamos ver o c\u00f3digo Dogs v Cats linha por linha.\n\n**tfms** significa *transformations*. `tfms_from_model` cuida do redimensionamento, recorte de imagem, normaliza\u00e7\u00e3o inicial (cria\u00e7\u00e3o de dados com (mean,stdev) of (0,1)), e outras coisas mais.\n","c840b1a6":"Al\u00e9m disso, a NVidia oferece fun\u00e7\u00f5es  especialmente aceleradas para aprendizado profundo em um pacote chamado CuDNN. Embora n\u00e3o seja estritamente necess\u00e1rio, ele melhorar\u00e1 significativamente o desempenho do treinamento e ser\u00e1 inclu\u00eddo por padr\u00e3o em todas as configura\u00e7\u00f5es fastai suportadas. Portanto, se o seguinte n\u00e3o retornar True, voc\u00ea pode querer investigar o motivo.\n","1e0a9a60":"## Melhorando o modelo","fae26207":"### Matrix de confus\u00e3o","9e4aedf8":"## Analisando os resultados","16564ec9":"## Entendendo o c\u00f3digo do nosso primeiro modelo","548237ec":"## Revis\u00e3o : passos f\u00e1ceis para treinar um classificador de imagem de n\u00edvel mundial\n","0170d857":"Vamos importar as bibliotecas, com c\u00f3digo aberto, que vamos precisar.","bdfa2fd6":"## Escolhendo a taxa de aprendizagem","5a1b47b1":"Podemos apenas imprimir a matriz de confus\u00e3o, ou podemos mostrar um gr\u00e1fico (que \u00e9 principalmente \u00fatil para dependentes com um n\u00famero maior de categorias).\n","e1e95bfb":"O que \u00e9 o par\u00e2metro cycle_len? O que fizemos aqui foi usar uma t\u00e9cnica chamada descida de gradiente estoc\u00e1stica com reinicializa\u00e7\u00f5es - *stochastic gradient descent with restarts (SGDR)*, uma variante de *learning rate annealing*, que diminui gradualmente a taxa de aprendizado \u00e0 medida que o treinamento avan\u00e7a. Isso \u00e9 \u00fatil porque, \u00e0 medida que nos aproximamos dos pesos ideais, queremos dar passos menores.\n\nNo entanto, podemos nos encontrar em uma parte do espa\u00e7o de peso que n\u00e3o \u00e9 muito resiliente - isto \u00e9, pequenas mudan\u00e7as nos pesos podem resultar em grandes mudan\u00e7as na fun\u00e7\u00e3o de perda. Queremos incentivar nosso modelo a encontrar partes do espa\u00e7o de peso que sejam precisas e est\u00e1veis. Portanto, de tempos em tempos, aumentamos a taxa de aprendizado (isso \u00e9 o 'restarts' em 'SGDR'), o que for\u00e7ar\u00e1 o modelo a saltar para uma parte diferente do espa\u00e7o de peso se a \u00e1rea atual for \"spikey\". Aqui est\u00e1 uma imagem de como isso pode parecer se redefinirmos as taxas de aprendizado 3 vezes (neste documento eles chamam de \"cyclic LR schedule\"):  \n<img src=\"images\/sgdr.png\" width=\"80%\">\n(From the paper [Snapshot Ensembles](https:\/\/arxiv.org\/abs\/1704.00109)).\n\nO n\u00famero de \u00e9pocas entre a redefini\u00e7\u00e3o da taxa de aprendizado \u00e9 definido por `cycle_len`, e o n\u00famero de vezes que isso acontece \u00e9 chamado de *number of cycles*, e \u00e9 o que estamos passando como o segundo par\u00e2metro para` fit( )`. Ent\u00e3o, eis como eram, realmente, as taxas de aprendizado:\n","1872a5a0":"A biblioteca Fastai assumir\u00e1 que voc\u00ea tem diret\u00f3rios de treino e teste. Ele tamb\u00e9m assume que cada diret\u00f3rio ter\u00e1 subdiret\u00f3rios para cada classe que voc\u00ea deseja reconhecer (neste caso, 'cats' e 'dogs').\n","e5481949":"1. precompute=True\n1. Use `lr_find()` to find highest learning rate where loss is still clearly improving\n1. Train last layer from precomputed activations for 1-2 epochs\n1. Train last layer with data augmentation (i.e. precompute=False) for 2-3 epochs with cycle_len=1\n1. Unfreeze all layers\n1. Set earlier layers to 3x-10x lower learning rate than next higher layer\n1. Use `lr_find()` again\n1. Train full network with cycle_mult=2 until over-fitting","2f7a4234":"Ser\u00e1 que nosso modelo \u00e9 bom ?  Bem, como mencionamos, antes desta competi\u00e7\u00e3o, o estado da arte tinha 80% de precis\u00e3o. Mas a competi\u00e7\u00e3o resultou em um enorme salto para 99% de precis\u00e3o, com o autor de uma popular biblioteca de aprendizagem profunda vencendo a competi\u00e7\u00e3o. Extraordinariamente, menos de 4 anos depois, podemos agora bater esse resultado em segundos! \n","cd967d98":"`PATH` \u00e9 o caminho para seus dados - se voc\u00ea usar as abordagens de configura\u00e7\u00e3o recomendadas da li\u00e7\u00e3o, n\u00e3o precisar\u00e1 alterar isso. `sz` \u00e9 o tamanho que as imagens ser\u00e3o redimensionadas para garantir que o treinamento seja executado rapidamente. N\u00f3s estaremos falando muito sobre este par\u00e2metro durante o curso. Deixe-o em `224` por enquanto.\n","dcd671aa":"\nVamos usar um modelo pr\u00e9-treinado, ou seja, um modelo criado por algu\u00e9m para resolver um problema diferente. Em vez de construir um modelo a partir do zero para resolver um problema semelhante, usaremos um modelo treinado no ImageNet (1,2 milh\u00f5es de imagens e 1000 classes) como ponto de partida. O modelo \u00e9 uma rede neural por convolu\u00e7\u00e3o (CNN), um tipo de rede neural que constr\u00f3i modelos de \u00faltima gera\u00e7\u00e3o para vis\u00e3o computacional.   \n\nVamos usar o modelo resnet34. O resnet34 \u00e9 uma vers\u00e3o do modelo que ganhou a competi\u00e7\u00e3o 2015 ImageNet.  \n\nVeja como treinar e avaliar um modelo de c\u00e3es vs gatos em 3 linhas de c\u00f3digo e em menos de 2 minutos (no laptop):","a5cfb20d":"Observe que na itera\u00e7\u00e3o de plotagem anterior h\u00e1 uma itera\u00e7\u00e3o (ou minibatch) de SGD(Stochastic gradient descent). Em uma \u00e9poca existem (num_train_samples \/ num_iterations) de SGD.\n\nPodemos ver o gr\u00e1fico da perda versus taxa de aprendizado para ver onde nossa fun\u00e7\u00e3o de perda para de diminuir:\n","c8eee708":"Quando executamos `learn.fit`, imprimimos 3 valores de desempenho (ver acima). Aqui, 0,03 \u00e9 o valor da perda no conjunto de treinamento, 0,0226 \u00e9 o valor da perda(**loss**) no conjunto de valida\u00e7\u00e3o e 0,9927 \u00e9 a precis\u00e3o na valida\u00e7\u00e3o. Qual \u00e9 a perda? O que \u00e9 precis\u00e3o? Por que n\u00e3o apenas mostrar precis\u00e3o?\n\nPrecis\u00e3o(**Accuracy**) \u00e9 a raz\u00e3o entre a previs\u00e3o correta e o n\u00famero total de previs\u00f5es.\n\nNo aprendizado de m\u00e1quina, a fun\u00e7\u00e3o de perda(**loss**) ou fun\u00e7\u00e3o de custo, representa o pre\u00e7o pago pela imprecis\u00e3o das previs\u00f5es.\n\nA perda associada a um exemplo na classifica\u00e7\u00e3o bin\u00e1ria \u00e9 dada por: `-(y * log(p) + (1-y) * log (1-p))` onde y \u00e9 o verdadeiro r\u00f3tulo de x e p \u00e9 a probabilidade estimada por nosso modelo, no caso em que o r\u00f3tulo \u00e9 1.\n","6e7d53c7":"H\u00e1 algo a mais que podemos fazer com o aumento de dados(data augmentation): use-o no *inference* time (tamb\u00e9m conhecido como *test* time). N\u00e3o \u00e9 de surpreender que isso seja conhecido como * aumento do tempo de teste * ou apenas * TTA *.\n\nO TTA simplesmente faz previs\u00f5es n\u00e3o apenas sobre as imagens em seu conjunto de valida\u00e7\u00e3o, mas tamb\u00e9m faz previs\u00f5es em um n\u00famero de vers\u00f5es aumentadas aleatoriamente(randomly augmented versions) delas tamb\u00e9m (por padr\u00e3o, ele usa a imagem original junto com 4 vers\u00f5es aumentadas aleatoriamente). Em seguida, faz a previs\u00e3o m\u00e9dia dessas imagens e usa isso. Para usar o TTA no conjunto de valida\u00e7\u00e3o, podemos usar o m\u00e9todo `TTA()` method.\n","7c7b0dfe":"## Introdu\u00e7\u00e3o ao Deep Learning: 'C\u00e3es e Gatos'","af7e83d3":"### Ajuste fino e 'differential learning rate annealing'","57f2277b":"## Vamos conhecer as fotos dos gatos","308dfc47":"Al\u00e9m de observar as m\u00e9tricas em geral, tamb\u00e9m \u00e9 uma boa ideia ver exemplos de cada uma delas:\n\n1. Algumas classifica\u00e7\u00f5es corretas aleatoriamente\n2. Algumas classifica\u00e7\u00f5es incorretas aleatoriamente\n3. Os r\u00f3tulos mais corretos de cada classe (ou seja, aqueles com maior probabilidade de estarem corretos)\n4. Os r\u00f3tulos mais incorretos de cada classe (ou seja, aqueles com maior probabilidade de estarem incorretos)\n5. Os r\u00f3tulos mais incertos (isto \u00e9, aqueles com probabilidade mais pr\u00f3xima de 0,5).\n","39852c00":"## Analisando os resultados: perda(loss) e acur\u00e1cia(accuracy)","c078b2c5":"Os par\u00e2metros s\u00e3o aprendidos ajustando um modelo aos dados. Os hiperpar\u00e2metros s\u00e3o outro tipo de par\u00e2metro, que n\u00e3o podem ser aprendidos diretamente do processo de treinamento regular. Esses par\u00e2metros expressam propriedades de \u201calto n\u00edvel\u201d do modelo, como sua complexidade ou qu\u00e3o r\u00e1pido ele deve aprender. Dois exemplos de hiperpar\u00e2metros s\u00e3o a taxa de aprendizado(*learning rate*) e o n\u00famero de \u00e9pocas(*number of epochs*).\n\nDurante o treinamento iterativo de uma rede neural, um lote(*batch*) ou mini-lote(*mini-batch*) \u00e9 um subconjunto de amostras de treinamento usado em uma itera\u00e7\u00e3o do Stochastic Gradient Descent (SGD). Uma \u00e9poca \u00e9 uma passagem \u00fanica por todo o conjunto de treinamento, que consiste em v\u00e1rias itera\u00e7\u00f5es de SGD.\n\nAgora podemos treinar(*fit*) o modelo; isto \u00e9, use a descida de gradiente(*gradient descent*) para encontrar os melhores par\u00e2metros para a camada totalmente conectada(fully connected layer) que adicionamos, que pode separar as imagens de gatos das fotos de c\u00e3es. Precisamos passar dois hyper\u00e2metros: a taxa de aprendizado - *learning rate* (geralmente 1e-2 ou 1e-3 \u00e9 um bom ponto de partida, veremos mais a seguir) e o n\u00famero de \u00e9pocas - *number of epochs* (voc\u00ea pode passar em um n\u00famero maior e simplesmente parar de treinar quando voc\u00ea v\u00ea que n\u00e3o est\u00e1 mais melhorando, ent\u00e3o execute-o novamente com o n\u00famero de \u00e9pocas que voc\u00ea achou que funciona bem.)\n\n","d0a1e200":"Uma maneira comum de analisar o resultado de um modelo de classifica\u00e7\u00e3o \u00e9 usar uma [matriz de confus\u00e3o](http:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/).   \nO Scikit-learn possui uma fun\u00e7\u00e3o interessante que podemos usar para esse prop\u00f3sito:\n","bc0a0c25":"## Analisando resultados: olhando as fotos","40347692":"\u00c9 importante que voc\u00ea tenha uma GPU NVidia em funcionamento. A estrutura de programa\u00e7\u00e3o usada nos bastidores para trabalhar com GPUs NVidia \u00e9 chamada de CUDA. Portanto, voc\u00ea precisa garantir que a linha a seguir retorne `True` antes de prosseguir. Se voc\u00ea tiver problemas com isso, verifique o FAQ e pe\u00e7a ajuda nos [f\u00f3runs] (http:\/\/forums.fast.ai).\n","b90361d7":"## Classifica\u00e7\u00e3o de imagens com redes neurais convolucionais","c106508d":"Geralmente verifica-se uma redu\u00e7\u00e3o de 10-20% no erro neste conjunto de dados ao usar o TTA neste momento, o que \u00e9 um resultado incr\u00edvel para uma t\u00e9cnica t\u00e3o r\u00e1pida e f\u00e1cil!\n","9df4579a":"Vamos criar um modelo para entrar na competi\u00e7\u00e3o Dogs vs Cats no Kaggle.   \nTemos 25.000 fotos de c\u00e3es e gatos rotuladas dispon\u00edveis para treinamento e   \n12.500 no conjunto de testes que devemos tentar rotular para esta competi\u00e7\u00e3o.   \nDe acordo com o site da Kaggle, quando esta competi\u00e7\u00e3o foi lan\u00e7ada (final de 2013): \"Estado da arte: A literatura atual sugere que classificadores de m\u00e1quinas podem pontuar acima de 80% de precis\u00e3o nesta tarefa\".  \n  \nEnt\u00e3o, se conseguirmos bater 80%, estaremos na vanguarda a partir de 2013!\n","b6d19f9e":"`ConvLearner.pretrained` constr\u00f3i um *learner* que cont\u00e9m um modelo pr\u00e9-treinado. A \u00faltima camada do modelo precisa ser substitu\u00edda com a camada de dimens\u00f5es corretas. O modelo pre-treinado foi treinado para 1000 classes, portanto, a camada final prev\u00ea um vetor de 1000 probabilidades. O modelo para gatos e c\u00e3es precisa produzir um vetor bidimensional. O diagrama abaixo mostra em um exemplo como isso foi feito em uma das primeiras CNNs bem-sucedidas. A camada \"FC8\" aqui seria substitu\u00edda por uma nova camada com 2 sa\u00eddas.\n\n<img src=\"images\/pretrained.png\" width=\"500\">\n[original image](https:\/\/image.slidesharecdn.com\/practicaldeeplearning-160329181459\/95\/practical-deep-learning-16-638.jpg)","ae5d6e5d":"Nossa perda(loss) na valida\u00e7\u00e3o n\u00e3o est\u00e1 melhorando muito, ent\u00e3o provavelmente n\u00e3o h\u00e1 necessidade de treinar mais a \u00faltima camada.\n","f55421a1":"Por padr\u00e3o, quando criamos um `learner`, ele define todos, exceto a \u00faltima camada, para *frozen*. Isso significa que est\u00e1 apenas atualizando os pesos na \u00faltima camada, quando chamamos o `fit`.\n","c3591ebf":"## FIM","d80ab9c1":"Se voc\u00ea tentar treinar por mais \u00e9pocas, perceber\u00e1 que come\u00e7amos a ter *overfit*, o que significa que nosso modelo est\u00e1 aprendendo a reconhecer as imagens espec\u00edficas no conjunto de treinamento, em vez de generalizar, de tal forma que possamos obter bons resultados no conjunto de valida\u00e7\u00e3o. Uma maneira de corrigir isso \u00e9 criar, literalmente, mais dados, por meio do *data augmentation*. Isso significa alterar aleatoriamente as imagens de maneira que n\u00e3o venha a afetar sua interpreta\u00e7\u00e3o, tal como inverter horizontalmente, aplicar zoom e girar.\n\nPodemos fazer isso, passando `aug_tfms` (transforma\u00e7\u00f5es de aumento-*augmentation transforms*) para `tfms_from_model`, com uma lista de fun\u00e7\u00f5es a serem aplicadas que alteram aleatoriamente a imagem como desejarmos. Para fotos tiradas em grande parte do lado (por exemplo, a maioria das fotos de c\u00e3es e gatos, em oposi\u00e7\u00e3o a fotos tiradas de cima para baixo, como imagens de sat\u00e9lite), podemos usar a lista predefinida de fun\u00e7\u00f5es `transforms_side_on`. Tamb\u00e9m podemos especificar o zoom aleat\u00f3rio de imagens at\u00e9 a escala especificada, adicionando o par\u00e2metro `max_zoom`.\n","e2152105":"Note que o que est\u00e1 sendo plotado acima \u00e9 a taxa de aprendizado das camadas finais. As taxas de aprendizado das camadas anteriores s\u00e3o fixadas nos mesmos m\u00faltiplos das taxas de camada final como solicitamos inicialmente (ou seja, as primeiras camadas t\u00eam 100x menores e as camadas intermedi\u00e1rias 10x menores taxas de aprendizado, pois definimos `lr=np.array([1e-4,1e-3,1e-2]).\n"}}