{"cell_type":{"f89525c8":"code","f1c41c5f":"code","6719c87e":"code","ebcaf8c5":"code","ec15a510":"code","38cb7726":"code","9cc88f04":"code","11fc2091":"code","9d38018f":"code","f3efb3dc":"code","a98c788e":"code","afa4bcdc":"code","f48382c6":"code","fe32d24c":"code","d8d90a43":"code","82b07da3":"code","db412f5a":"code","1a2a167f":"code","bb3c1b66":"code","41a35d3b":"code","d6a3f35d":"code","dfcb99d4":"code","9adfd529":"code","3470159d":"code","9b2edafd":"code","a1c6ee6b":"code","f4690ead":"code","dd4cbfe4":"code","31d3d65b":"code","1075ccd4":"code","ddc0162f":"code","323eced8":"code","9c480e1d":"code","41590641":"code","e8a93567":"code","f5759156":"code","8a66f2a9":"code","add919be":"code","2f3b72db":"code","5468fc94":"code","4e51c50b":"code","a9b5f591":"code","a255e343":"code","ea4315be":"code","f5a62af6":"code","c0825191":"code","213c3955":"code","bb753fe8":"code","201a0eab":"code","caf3163c":"code","86088252":"markdown","b3248db8":"markdown","28e92df8":"markdown","d0dc7dfc":"markdown","1f509ff9":"markdown","93674c6d":"markdown","d45e5b88":"markdown","2003da9e":"markdown","82ceebac":"markdown","0d079c18":"markdown","0323157d":"markdown","fdf1a790":"markdown","37e3c686":"markdown","6dc878e7":"markdown","2675eb0c":"markdown","af69789d":"markdown","11015db4":"markdown","0b050626":"markdown","0e7cee95":"markdown","b60c8967":"markdown","5b5646ff":"markdown","bf31ca16":"markdown","ad6bab93":"markdown","d8a25541":"markdown","e2421533":"markdown","c3be3c64":"markdown","f00b3afa":"markdown","1c51c8b0":"markdown","f8f19ebb":"markdown","af9e8b98":"markdown","d47eb308":"markdown","1ac8dbb1":"markdown","71345326":"markdown","4faecea8":"markdown","10008831":"markdown","def64f3d":"markdown","55e770a4":"markdown","d676f4aa":"markdown","215c8bb8":"markdown","1af082fa":"markdown","1e872638":"markdown","4b916418":"markdown","46a97f64":"markdown","f56df516":"markdown"},"source":{"f89525c8":"from IPython.display import Image\nImage(\"..\/input\/pictures\/airbnb.png\")","f1c41c5f":"import pandas as pds\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score,precision_score, f1_score, roc_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold, GridSearchCV, StratifiedKFold\nimport warnings\nwarnings.filterwarnings('ignore')","6719c87e":"# =============================================================================#\n# Etude du fichier age_gender.csv                                              #\n# =============================================================================#\ndf_age = pds.read_csv(\"\/kaggle\/input\/airbnb-data\/age_gender_bkts.csv\")\nnRow, nCol = df_age.shape\nprint(f'Il y a {nRow} lignes and {nCol} colonnes dans le fichier age_gender_bkts.csv')\ndf_age.head(5)\n","ebcaf8c5":"#Regarder s'il y a des valeurs nulles dans le fichier\ndf_age.isnull().values.any() \n#Convertir le bucket 100+ en r\u00e9el bucket : les buckets sont tous de taille 5\ndf_age['age_bucket'] = df_age['age_bucket'].apply(lambda x : '100-104' if x == '100+' else x)\ndf_age.head(5)\n#Regarder quelles sont les differentes destinations, les differents genre et les differentes ann\u00e9es\nprint(df_age['country_destination'].value_counts())\nprint(df_age['gender'].value_counts())\nprint(df_age['year'].value_counts())   #Il n'y a qu'une seule ann\u00e9e : 2015 --> cette colonne ne sert \u00e0 rien\ndf_age = df_age.drop('year', axis=1)     \ndf_age.head(5)","ec15a510":"# =============================================================================#\n# Etude du fichier countries.csv                                               #\n# =============================================================================#\ndf_countries = pds.read_csv(\"\/kaggle\/input\/airbnb-data\/countries.csv\")  \nnRow, nCol = df_countries.shape\nprint(f'Il y a {nRow} lignes and {nCol} colonnes dans le fichier countries.csv')\ndf_countries.head(10)","38cb7726":"#Regarder s'il y a des valeurs nulles dans le fichier\ndf_countries.isnull().values.any() ","9cc88f04":"# =============================================================================#\n# Etude du fichier sessions.csv                                                #\n# =============================================================================#\ndf_sessions = pds.read_csv(\"\/kaggle\/input\/airbnb-data\/sessions.csv\")  \nnRow, nCol = df_sessions.shape\nprint(f'Il y a {nRow} lignes and {nCol} colonnes dans le fichier countries.csv')\ndf_sessions.head(20)","11fc2091":"sns.distplot(df_sessions[df_sessions['secs_elapsed'].notnull()]['secs_elapsed'])","9d38018f":"df_sessions['secs_elapsed'].describe()","f3efb3dc":"print(len(df_sessions[df_sessions['secs_elapsed'].isnull()]))\nmedian_secs = df_sessions['secs_elapsed'].median()\ndf_sessions['secs_elapsed'] = df_sessions['secs_elapsed'].fillna(median_secs)\nprint(len(df_sessions[df_sessions['secs_elapsed'].isnull()]))\nprint(df_sessions['secs_elapsed'].describe())\ndf_sessions.head(5)","a98c788e":"print(df_sessions['device_type'].value_counts())\ndf_countries.isnull().values.any()","afa4bcdc":"# =============================================================================#\n# Etude du fichier train_users_2.csv                                           #\n# =============================================================================#\ndf_app = pds.read_csv(\"\/kaggle\/input\/airbnb-data\/train_users_2.csv\") \ndf_app['type_dataset'] = \"apprentissage\"\nnRow, nCol = df_app.shape\nprint(f'Il y a {nRow} lignes and {nCol} colonnes dans le fichier train_users_2.csv')\ndf_test = pds.read_csv(\"\/kaggle\/input\/airbnb-data\/test_users.csv\") \ndf_test['type_dataset'] = \"test\"\ndf_test['country_destination'] =\"predict\"\ndf_all = pds.concat([df_app,df_test],ignore_index=True,sort=False)\ndf_app.head(5)","f48382c6":"# =============================================================================#\n# Nettoyage des donn\u00e9es                                                        #\n# =============================================================================#\n#Transformation des date_first_booking\ndf_all['date_first_booking'].fillna('1800-01-01',inplace=True)\ndf_all['month_first_booking'] = df_all['date_first_booking'].apply(lambda x : int(x.split('-')[1]))\ndf_all['day_first_booking'] = df_all['date_first_booking'].apply(lambda x : int(x.split('-')[2]))\ndf_all['year_first_booking'] = df_all['date_first_booking'].apply(lambda x : int(x.split('-')[0]))\ndf_all.drop(columns=['date_first_booking'],axis=1, inplace=True)\n\n#Transformation des date_account_created\ndf_all['date_account_created'].fillna('1800-01-01',inplace=True)\ndf_all['month_account_created'] = df_all['date_account_created'].apply(lambda x : int(x.split('-')[1]))\ndf_all['day_account_created'] = df_all['date_account_created'].apply(lambda x : int(x.split('-')[2]))\ndf_all['year_account_created'] = df_all['date_account_created'].apply(lambda x : int(x.split('-')[0]))\ndf_all.drop(columns=['date_account_created'],axis=1, inplace=True)\n\n#Transformation des timestamp\ndf_all['timestamp_first_active'] = df_all['timestamp_first_active'].apply(lambda x : int(str(x)[0:8]))\ndf_all['year_first_active'] = df_all['timestamp_first_active'].apply(lambda x : int(str(x)[0:4]))\ndf_all['month_first_active'] = df_all['timestamp_first_active'].apply(lambda x : int(str(x)[4:6]))\ndf_all['day_first_active'] = df_all['timestamp_first_active'].apply(lambda x : int(str(x)[6:8]))\n\n#Remplacer les nan dans 'first_affiliate_tracked' par 'None'\ndf_all['first_affiliate_tracked'].replace(np.nan,'none', inplace=True)\ndf_all['country_destination'].replace(np.nan,'other', inplace=True)\n\n#Joindre avec countries pour obtenir la colonne language_levenshtein_distance\ndf_countries['language'] = df_countries['destination_language'].apply(lambda x : x[0:2])\njoin_countries = df_countries[['language','language_levenshtein_distance']].drop_duplicates()\ndf_all = df_all.merge(join_countries,on='language',how='left')\n\n#df_all = df_all.merge(df_sessions, left_on=\"id\", right_on=\"user_id\", how='outer')\n\n#Nettoyage des colonnes rajout\u00e9es \u00e0 partir de sessions\n#df_all['action_type'].replace(np.nan,'unknown', inplace=True)\n#df_all['action_type'].replace('-unknown-','unknown', inplace=True)\n#df_all['device_type'].replace(np.nan,'unknown', inplace=True)\n#df_all['device_type'].replace('-unknown-','unknown', inplace=True)\ndf_all['gender'].replace('-unknown-',np.nan, inplace=True)\ndf_all['first_browser'] = df_all['first_browser'].replace('-unknown-', np.nan)\ndf_all.head(5)","fe32d24c":"#print(df_all['age'].value_counts())\ndf_all[df_all['age'] > 100].head()","d8d90a43":"from datetime import date\ndf_all['age'] = df_all['age'].apply(lambda age : date.today().year - age if (age > 100 and age < 2005) else (np.nan if age >= 2005 else age))\ndf_all[df_all['age'] > 100].head()\nprint(df_all['age'].describe())","82b07da3":"print(df_all['country_destination'].value_counts())","db412f5a":"df_all = df_all[(df_all['country_destination'] != 'NDF') & (df_all['country_destination'] != 'other')]\nprint(df_all['country_destination'].value_counts())                       ","1a2a167f":"df_app = df_all[df_all['type_dataset']==\"apprentissage\"]\ndf_test = df_all[df_all['type_dataset']==\"test\"]\ndf_app = df_app.drop('type_dataset', axis=1)\nclasses = ['US','FR','IT','GB','ES','CA','DE','NL','AU','PT']\n\ndef stacked_bar(feature,size=(6, 6)):\n    ctab = pds.crosstab([df_app[feature].fillna('Unknown')], df_app.country_destination, dropna=False).apply(lambda x: x\/x.sum(), axis=1)\n    ctab[classes].plot(kind='bar', stacked=True, colormap='gist_ncar', legend=True,figsize=size)\nstacked_bar('gender')","bb3c1b66":"sns.distplot(df_app['age'].dropna())","41a35d3b":"def age_group(x) : \n    if x <= 20 :\n        return 'Teenager'\n    elif 20 <= x and x < 40 :\n        return 'Young'\n    elif 40 <= x and x < 60 :\n        return 'Adult'\n    elif 60 <= x and x < 100: \n        return 'Old '\n    else : \n        return 'Unknown'\n    \ndf_app['age_group'] = df_app['age'].apply(lambda age : age_group(age))   \nstacked_bar('age_group')","d6a3f35d":"df_app = df_app.drop('age_group', axis=1)","dfcb99d4":"fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(15, 8))\nsns.boxplot(x='country_destination', y='age', data=df_app, palette=\"muted\", ax =ax)\nax.set_ylim([10, 75])","9adfd529":"stacked_bar('language', size=(15,6))\nstacked_bar('signup_app')\nstacked_bar('first_device_type')\nstacked_bar('first_browser', size=(15,6))","3470159d":"stacked_bar('signup_method')","9b2edafd":"stacked_bar('affiliate_channel')\nstacked_bar('affiliate_provider')\nstacked_bar('first_affiliate_tracked')","a1c6ee6b":"class_dict = {\n    'US': 0,\n    'FR': 1,\n    'CA': 2,\n    'GB': 3,\n    'ES': 4,\n    'IT': 5,\n    'PT': 6,\n    'NL': 7,\n    'DE': 8,\n    'AU': 9\n}\nX, y = df_app.drop('country_destination', axis=1), df_app['country_destination'].apply(lambda x: class_dict[x])\nprint(\"ok\")","f4690ead":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntolabelize_columns = ['gender','signup_method','language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser'] \ndef labelize(df) : \n  for col in tolabelize_columns :\n    df[col]=le.fit_transform(df[col].astype(str))\n  return df\n\nX = labelize(X)\nX = X.drop(columns=['id'])\nX.fillna(-1, inplace=True)\nX.isnull().values.any()","dd4cbfe4":"train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.7, stratify=y)\nprint(len(train_X))\nprint(len(test_X))\nprint(len(train_y))\nprint(len(test_y))","31d3d65b":"print(train_y.value_counts())","1075ccd4":"print(test_y.value_counts())","ddc0162f":"Image(\"..\/input\/pictures\/matrice_confusion.png\")","323eced8":"# =============================================================================#\n# Application des forets al\u00e9atoires                                            #\n# =============================================================================#\nfrom sklearn.metrics import confusion_matrix\nrfc1 = RandomForestClassifier()\n    #Apprendre aux mod\u00e8les nos donn\u00e9es\nrfc1.fit(train_X, train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = rfc1.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = rfc1.predict(test_X)\n#print(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score sur les donn\u00e9es de test par random forest classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_micro = recall_score(test_y, predictions, average='micro')\nrecall_score_macro = recall_score(test_y, predictions, average='macro')\nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_micro : \", recall_score_micro*100, \"%\")\nprint(\"recall_score_macro : \", recall_score_macro*100, \"%\")\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_micro = precision_score(test_y, predictions, average='micro')\nprecision_score_macro = precision_score(test_y, predictions, average='macro')\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_micro : \", precision_score_micro*100, \"%\")\nprint(\"precision_score_macro : \", precision_score_macro*100, \"%\")\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_micro = f1_score(test_y, predictions, average='micro')\nfmesure_score_macro = f1_score(test_y, predictions, average='macro')\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_micro : \", fmesure_score_micro*100, \"%\")\nprint(\"fmesure_score_macro : \", fmesure_score_macro*100, \"%\")\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n            #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","9c480e1d":"# =============================================================================#\n# Application des forets al\u00e9atoires avec max_depth = 15                        #\n# =============================================================================#\nrfc2 = RandomForestClassifier(max_depth=15)\n    #Apprendre aux mod\u00e8les nos donn\u00e9es\nrfc2.fit(train_X, train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = rfc2.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = rfc2.predict(test_X)\nprint(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score sur les donn\u00e9es de test par random forest classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n    #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","41590641":"# =============================================================================#\n# Application des forets al\u00e9atoires avec max_depth=15, n_estimators=20         #\n# =============================================================================#\nrfc3 = RandomForestClassifier(max_depth=15, n_estimators=20)\n    #Apprendre aux mod\u00e8les nos donn\u00e9es\nrfc3.fit(train_X, train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = rfc3.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = rfc3.predict(test_X)\nprint(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score par random forest classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n    #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","e8a93567":"Image(\"..\/input\/pictures\/foret.png\")","f5759156":"# =============================================================================#\n# Cr\u00e9ation d'une fonction pour les arbres de d\u00e9cisions                         #\n# Nous avons mis les valeurs qui sont cod\u00e9es par defaut                        #\n# =============================================================================#\nfrom sklearn import tree\ndef decision_tree(max_depth=None, criteron='gini', min_samples_split=2) : \n    if max_depth is not None : \n        clf = tree.DecisionTreeClassifier(max_depth = max_depth)\n    elif criteron != 'gini' :\n        clf = tree.DecisionTreeClassifier(criterion = criteron)\n    elif min_samples_split != 2 :\n        clf = tree.DecisionTreeClassifier(min_samples_split = min_samples_split)\n    elif max_depth is not None and criteron != 'gini' :\n        clf = tree.DecisionTreeClassifier(max_depth, criterion)\n    elif max_depth is not None and min_samples_split != 2 :\n        clf = tree.DecisionTreeClassifier(max_depth, min_samples_split)\n    else :\n        clf = tree.DecisionTreeClassifier()\n        #Apprendre aux mod\u00e8les nos donn\u00e9es\n    training = clf.fit(train_X, train_y)\n        #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n            #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\n    predictions_app = clf.predict(train_X)\n        #2) Calculer son score de prediction\n    score_app = accuracy_score(train_y, predictions_app)\n    print(\"Score sur les donn\u00e9es d'apprentissage : \", score_app*100 , \"%\")\n    #tree.plot_tree(training)\n\n        #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\n    predictions = clf.predict(test_X)\n    #print(np.unique(predictions))\n        #Score\n    score = accuracy_score(test_y, predictions)\n    print(\"Score sur les donn\u00e9es pr\u00e9dites par arbre : \", score*100 , \"%\")\n        #Recall \n    recall_score_weighted = recall_score(test_y, predictions, average='weighted')\n    print(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n        #Precision\n    precision_score_weighted = precision_score(test_y, predictions, average='weighted')\n    print(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n        #FMesure\n    fmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\n    print(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n                #Matrice de confusion \n    df_int = df_app[(df_app['country_destination'] != 'predict')]\n    conf = confusion_matrix(test_y, predictions)\n    cf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\n    cf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\n    print(cf)\n    ","8a66f2a9":"#sans parametre\nclf1 = decision_tree()","add919be":"#max_depth = 5\nclf2 = decision_tree(max_depth=5)","2f3b72db":"#max_depth = 20\nclf3 = decision_tree(max_depth=20)","5468fc94":"#max_depth = 10\nclf4 = decision_tree(max_depth=10)","4e51c50b":"#max_depth = 8\nclf5 = decision_tree(max_depth=8)","a9b5f591":"#max_depth = 5 + crit\u00e8re = entropy\nclf6 = decision_tree(max_depth=5, criteron='entropy')","a255e343":"# =============================================================================#\n# Application d'arbre de d\u00e9cision avec un \u00e9chantillon minimal \u00e9gal \u00e0 100       #\n# =============================================================================#\nclf7 = decision_tree(min_samples_split=100)","ea4315be":"# =============================================================================#\n# Application d'arbre de d\u00e9cision avec max_depth=5,min_samples_split=100       #\n# =============================================================================#\nclf8 = decision_tree(max_depth=5,min_samples_split=100)","f5a62af6":"# =============================================================================#\n# R\u00e9capitulatif des r\u00e9sultats obtenus                                          #\n# =============================================================================#\nImage(\"..\/input\/pictures\/arbre.png\")","c0825191":"# =============================================================================#\n# Application des bayesien naif                                                #\n# =============================================================================#\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n    #Apprendre aux mod\u00e8les nos donn\u00e9es\ngnb.fit(train_X, train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = gnb.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = gnb.predict(test_X)\n#print(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score par bayesien naif classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n        #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","213c3955":"# =============================================================================#\n# Gradient Boosting                                                            #\n# =============================================================================#\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier()\nclf.fit(train_X,train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = clf.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = clf.predict(test_X)\n#print(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score par gradient boosting classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n        #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","bb753fe8":"# =============================================================================#\n# Gradient Boosting                                                            #\n# =============================================================================#\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=20)\nclf.fit(train_X,train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = clf.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = clf.predict(test_X)\n#print(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score par gradient boosting classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n        #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","201a0eab":"# =============================================================================#\n# XG boost                                                                     #\n# =============================================================================#\nxgboost = XGBClassifier()\nxgboost.fit(train_X,train_y)\n    #V\u00e9rifier que notre score sur nos donn\u00e9es de validation n'est ni trop faible (underfitting) ni trop fort (overfitting)\n        #1) Pr\u00e9dire nos donn\u00e9es d'apprentissage (sur lesquelles nous avons appris \u00e0 notre mod\u00e8le)\npredictions_app = xgboost.predict(train_X)\n        #2) Calculer son score de prediction\nscore = accuracy_score(train_y, predictions_app)\nprint(\"Score sur les donn\u00e9es d'apprentissage : \", score*100 , \"%\")\n    #Faire des pr\u00e9dictions en utilisant nos donn\u00e9es de validation\npredictions = xgboost.predict(test_X)\n#print(np.unique(predictions))\n    #Score\nscore = accuracy_score(test_y, predictions)\nprint(\"Score sur les donn\u00e9es de validation par gradient boosting classifier : \", score*100 , \"%\")\n\n    #Recall \nrecall_score_weighted = recall_score(test_y, predictions, average='weighted')\nprint(\"recall_score_weighted : \", recall_score_weighted*100, \"%\")\n    #Precision\nprecision_score_weighted = precision_score(test_y, predictions, average='weighted')\nprint(\"precision_score_weighted : \", precision_score_weighted*100, \"%\")\n    #FMesure\nfmesure_score_weighted = f1_score(test_y, predictions, average='weighted')\nprint(\"fmesure_score_weighted : \", fmesure_score_weighted*100, \"%\")\n    #Matrice de confusion \ndf_int = df_app[(df_app['country_destination'] != 'predict')]\nconf = confusion_matrix(test_y, predictions)\ncf = pds.DataFrame(conf, columns=['pr\u00e9dict ' + cl for cl in df_int['country_destination'].drop_duplicates()])\ncf.index = ['actual ' + cl for cl in df_int['country_destination'].drop_duplicates()]\nprint(cf)","caf3163c":"df_test = df_all[df_all['type_dataset']==\"test\"]\nid = df_test['id']\ndf_test.head(5)\ndf_test = df_test.drop('country_destination', axis=1)\ndf_test = df_test.drop('type_dataset', axis=1)\ndf_test = df_test.drop('id', axis=1)\ntolabelize_columns = ['gender','signup_method','language','affiliate_channel','affiliate_provider','first_affiliate_tracked','signup_app','first_device_type','first_browser'] \ndef get_original_country(x) :\n    for c in class_dict : \n        if class_dict[c] == x : \n            return c\n        \ndef labelize(df) : \n  for col in tolabelize_columns :\n    df[col]=le.fit_transform(df[col].astype(str))\n  return df\n\ndf_test = labelize(df_test)\npredictions = xgboost.predict(df_test)\nresult = pds.DataFrame(data={'country' : predictions, 'id' : id})\nresult['country'] = result['country'].apply(lambda x : get_original_country(x))\nprint(result.head(10))\nprint(np.unique(result['country']))\nresult.to_csv(\"soumission_projet_xgboost.csv\", index=False)","86088252":"Le choix du crit\u00e8re ne change pas grand chose aux r\u00e9sultats. Il n'est donc pas n\u00e9cessaire de le prendre en compte dans la suite de l'\u00e9tude.\n\nEn revanche, nous allons essayer un autre param\u00e8tre qui est le min_samples_split = le nombre d'\u00e9chantillon minimal pour faire un split. On lui affecte une valeur arbitraire de 100.","b3248db8":"**Conclusion : **\n\nLe meilleur mod\u00e8le par arbre de d\u00e9cision est celui comprenant : Max_depth = 5 ","28e92df8":"On va regarder s'il y a des ages sup\u00e9rieurs \u00e0 100 ans. En effet, on peut s'imaginer que les personnes de plus de 100 ans (dans les donn\u00e9es) ont pu se tromper lors de leur saisie dans le formulaire","d0dc7dfc":"Les m\u00e9triques sont ensuite calcul\u00e9es:\n   1. **Accuracy** : (TP+TN)\/(TP+TN+FP+FN)\n   2. **Recall** : TP\/(TP+FN)\n   3. **Pr\u00e9cision** : TP\/(TP+FP)\n   4. **Fmesure** : 2TP\/(2TP+FP+FN) <br>\n    \nA noter que pour chacune des m\u00e9triques, nous pouvons choisir dans les param\u00e8tres des m\u00e9thodes la moyenne choisie entre \"**binary**\"; \"**micro**\"; \"**macro**\" et \"**weighted**\". <br>\n* **Binary** : est utilisable uniquement lorsque l'on pr\u00e9dit des classes binaires (0 ou 1) <br>\n* **Micro** : calcule les moyennes globalement en comptant le nombre de vrai positifs, faux positifs, faux n\u00e9gatifs et faux positifs <br>\n* **Macro** : calcule les moyennes pour chaque classe et prends ensuite la moyenne non pond\u00e9r\u00e9e donc ne tient pas compte du d\u00e9s\u00e9quilibre entre les classes. <br>\n* **Weighted** : calcule les moyennes pour chaque classe et prends ensuite la moyenne pond\u00e9r\u00e9e par classe. <br>\n\nDans notre code, pour le premier mod\u00e8le, nous avons calcul\u00e9 les m\u00e9triques dans chacun des cas (sauf le binaire). Cependant, pour notre \u00e9tude il est pr\u00e9f\u00e9rable de retenir les r\u00e9sultats de la m\u00e9thode \"weighted\" car elle tient compte de la pond\u00e9ration. C'est pourquoi nous avons calcul\u00e9 uniquement celui-ci pour les autres mod\u00e8les ","1f509ff9":"Le fichier est compos\u00e9 de 10 lignes et 7 colonnes sans aucune valeur nulle. <br>\nNous pouvons donc passer au fichier suivant : sessions.csv","93674c6d":"La profondeur de l'arbre a l'air d'\u00eatre concluante. Le score nous parait coh\u00e9rent. Nous allons essayer d'am\u00e9liorer notre mod\u00e8le en y rajoutant des param\u00e8tres.","d45e5b88":"NB : On reprend df_app car on veut apprendre \u00e0 connaitre nos donn\u00e9es d'app\n<!-- -->","2003da9e":"Le meilleur mod\u00e8le est XGBOOST. Nous allons donc pr\u00e9dire nos donn\u00e9es de test avec ce mod\u00e8le.","82ceebac":"Ayant une valeur statistique : secs_elapsed, nous allons tenter de dessiner un graphique en courbe afin d'avoir de plus amples informations.","0d079c18":"# Data Mining\n\nDans cette partie, nous allons appliquer plusieurs mod\u00e8les afin de pr\u00e9dire nos donn\u00e9es de test. Pour cela, nous allons proc\u00e9der de la mani\u00e8re suivante : \n   1. **D\u00e9couper** notre fichier df_train en deux parties : les X toutes nos donn\u00e9es sauf la colonne que nous devons pr\u00e9dire : country_destination. Les y : la colonne que nous devons pr\u00e9dire. \n   2. **Encoder** les donn\u00e9es de X afin de pouvoir lui appliquer des mod\u00e8les.\n   3. **S\u00e9parer** notre jeu de donn\u00e9es en jeu d'apprentissage et jeu de test. Afin de connaitre la proportion de donn\u00e9es d'apprentissage et de test nous avons fait vari\u00e9 le param\u00e8tre train_size de la fonction trai_test_split. En effet, en cas de sur-apprentissage (over-fitting) nous aurons un bon score de pr\u00e9diction pour le training_set mais il sera difficile d'obtenir un bon score lors de la pr\u00e9diction de donn\u00e9es inconnues. A l'inverse, en cas de sous-apprentissage, le score de pr\u00e9diction pour le training_set sera mauvais car notre mod\u00e8le est mauvais, et donc par voix de cons\u00e9quence, il sera impossible de g\u00e9n\u00e9raliser le mod\u00e8le pr\u00e9dictif. Notre score sur nos donn\u00e9es inconnues sera donc tr\u00e8s mauvais.\n   4. Appliquer des **arbres de d\u00e9cision** : L'apprentissage par arbre de d\u00e9cision d\u00e9signe une m\u00e9thode bas\u00e9e sur l'utilisation d'un arbre de d\u00e9cision comme mod\u00e8le pr\u00e9dictif. Un arbre de d\u00e9cision d\u00e9crit les donn\u00e9es mais pas les d\u00e9cisions elles-m\u00eames, l'arbre est utilis\u00e9 comme point de d\u00e9part au processus de d\u00e9cision.\n   5. Appliquer des **for\u00eats al\u00e9atoires** : L'apprentissage est bas\u00e9 sur de multiples arbres de d\u00e9cision entrain\u00e9s sur des ensembles de donn\u00e9es l\u00e9g\u00e8rement diff\u00e9rents.\n   6. Appliquer un classifieur **bay\u00e9sien naif** : Il s'agit d'un classifieur bas\u00e9 sur le th\u00e9or\u00e8me de Bayes avec une forte ind\u00e9pendance des hypoth\u00e8ses. Le mod\u00e8le s'appuie sur l'estimation des param\u00e8tres moyenne et variance.\n   7. Appliquer un **XGBoost** : Cette algorithme fait partie de la cat\u00e9gorie des algorithmes de boosting \u00e0 la diff\u00e9rence qu'il ne prend uniquement en entr\u00e9 des arbres de d\u00e9cision. Le boosting consiste \u00e0 cr\u00e9er des mod\u00e8les successivement bas\u00e9s sur les erreurs des pr\u00e9c\u00e9dents en augmenant le poids des points mals class\u00e9s et en diminuant les poids des biens class\u00e9s.","0323157d":"Avec cette valeur de max_depth , nous avons des scores proche du depth 5 mais moins int\u00e9ressant.\nOn essaye maitenant d'entrainer notre mod\u00e8le avec le gradient Boosting. <br>\n\n**Gradient boosting**\n\nNous allons ici appliquer le classifieur de type gradiant boosting en faisant varier le param\u00e8tre n_estimators.","fdf1a790":"Le r\u00e9sultat sur les donn\u00e9es d'apprentissage est quasiment le m\u00eame. En revanche, nous sommes moins bons sur les donn\u00e9es de validation.\nNous allons essayer cette fois d'utiliser ce param\u00e8tre avec le param\u00e8tre concluant pr\u00e9c\u00e9dent : max_depth.","37e3c686":"Avec cette valeur de max_depth , nous avons des scores proche du depth 5 mais moins int\u00e9ressants.\n\nDans les mod\u00e8les suivants, nous d\u00e9cidons de garder la profondeur maximale \u00e0 5 et de jouer sur d'autres param\u00e8tres tels que crit\u00e8re qui par d\u00e9faut est \u00e0 gini. Nous d\u00e9cidons donc de choisir comme crit\u00e8re l'entrepie.","6dc878e7":"Le r\u00e9sultat est un tout petit peu meilleur en affectant n_estimator=20","2675eb0c":"**Bayesien naif**","af69789d":"        B. Countries","11015db4":"On remarque que la majorit\u00e9 des donn\u00e9es sont comprises entre 0 et 250 000 secondes mais que certaines ont un temps nettement sup\u00e9rieur (1 750 000 secondes). <br>\nDonc, pour avoir de plus amples informations, on va afficher les m\u00e9triques moyenne, minimum, maximum, m\u00e9diane.. \u00e0 l'aide de la fonction describe().","0b050626":"En affichant les lignes de l'age est > 100 ans, on s'aper\u00e7oit que certaines personnes ont rentr\u00e9 des ann\u00e9es \u00e0 la place de leur age. <br>\nOn d\u00e9cide donc que pour les ages compris entre 100 et 2005 on va calculer l'ann\u00e9e actuelle - l'age rentr\u00e9 et pour les ages sup\u00e9rieur \u00e0 2005 on saisi np.nan. En effet, on ne peut pas avoir de personne ayant 2014 ans ou 2019 - 2014 = 5 ans.","0e7cee95":"> # D\u00e9couverte des donn\u00e9es\n\nDans cette section, nous allons commencer par d\u00e9couvrir \/ appr\u00e9hender les donn\u00e9es afin d'avoir une id\u00e9e sur ce que nous allons devoir traiter.\n\nNous commencons par le fichier age_gender.csv","b60c8967":"**Les mod\u00e8les de donn\u00e9es** <br>\nAfin d'\u00e9valuer nos mod\u00e8les, nous allons calculer 4 m\u00e9triques. Ces derni\u00e8res sont calcul\u00e9es \u00e0 partir de 4 valeurs : \n   1. Les Trues Positives (TP) : pr\u00e9diction classe + vs r\u00e9alit\u00e9 classe +\n   2. Les Trues Negatives (TN) : pr\u00e9diction classe - vs r\u00e9alit\u00e9 classe -\n   3. Les Falses Positives (FP) : pr\u00e9diction classe + vs r\u00e9alit\u00e9 classe -\n   4. Les Falses Nagtives (FN) : pr\u00e9diction classe - vs r\u00e9alit\u00e9 classe + <br>","5b5646ff":"**Arbre de d\u00e9cision**\n\nNous allons ici appliquer le classifieur de type arbre de d\u00e9cision en faisant varier plusieurs param\u00e8tres :\n   1. **max_depth** : profondeur max de l'arbre\n   2. **criteron** : La fonction pour mesurer la qualit\u00e9 d'une scission. Les crit\u00e8res pris en charge sont \"gini\" pour l'impuret\u00e9 de Gini et \"entropie\" pour le gain d'information. Remarque: ce param\u00e8tre est sp\u00e9cifique \u00e0 l'arbre \n   3. **min_samples_split** : Le nombre minimum d'\u00e9chantillons requis pour fractionner un n\u0153ud interne ","bf31ca16":"Le score sur les donn\u00e9es d'apprentissage est de 97% ce qui veut dire que notre mod\u00e8le a \u00e9t\u00e9 sur-appris. Nous allons donc tenter de modifier les param\u00e8tres","ad6bab93":"                *Age\n\nNous allons dans cette partie, repr\u00e9senter sous trois formes les destinations choisies en fonction de l'\u00e2ge.\n   1. Distribution : Il s'agit d'une loi de probabilit\u00e9 qui dans notre cas permettra d'approximer les donn\u00e9es au plus juste.\n   2. Graphique \u00e0 barres empil\u00e9es : en fonction de quatre cat\u00e9gories : teenager, young, adult, old\n   3. Boite \u00e0 moustache : ayant des donn\u00e9es num\u00e9riques, nous pouvons repr\u00e9senter une boite \u00e0 moustache ce qui permettra d'avoir des m\u00e9triques math\u00e9matiques : moyenne, m\u00e9diane, minimum, maximum...","d8a25541":"**Conclusion** <br>\nLe classifieur avec les param\u00e8tres max_depth=15, n_estimators=20 a le meilleur score de pr\u00e9diction.","e2421533":"Pour des raisons de fiabilit\u00e9 des donn\u00e9es, nous prenons comme hypoth\u00e8se que la destination doit \u00eatre connue --> nous supprimons les lignes dont la destination est \"NDF\" ou \"other\". En effet, le but de ce projet est de pr\u00e9dire une destination de voyage. Or, pr\u00e9dire \"NDF\" ou \"other\" n'est pas concluant.","c3be3c64":"Avec un max_depth = 5, nous obtenons un score de classification sur les donn\u00e9es pr\u00e9dites de 79,14% ce qui est interessant.\nNous allons donc faire varier ce param\u00e8tre afin d'obtenir - ou non - un meilleur score.\n\nNous commencons par essayer avec 20.","f00b3afa":"# Visualisation des donn\u00e9es\n\nDans cette partie nous allons visualiser nos donn\u00e9es d'apprentissage afin d'avoir une vision g\u00e9n\u00e9rale. En effet, visualiser les donn\u00e9es nous permettra d'avoir des m\u00e9triques globales sur nos donn\u00e9es.\n\nEn representant des ** graphiques \u00e0 barres empill\u00e9es** nous allons tenter de voir si les valeurs de certains attributs influent sur la destination du voyage.","1c51c8b0":"On remarque alors que la session d'un utilisateur dure en moyenne 19 450 secondes alors que la m\u00e9diane est de 1 147 secondes.\nCela sugg\u00e8re un ensemble de donn\u00e9es fortement asym\u00e9trique avec des observations aberrantes influancant la moyenne de la distribution. <br>\nPour les valeurs qui sont nulles ont decide donc de les remplacer par la m\u00e9diane. \n","f8f19ebb":"**Forets al\u00e9atoires** <br>\nNous allons ici appliquer le classifieur de type for\u00eat al\u00e9atoire en faisant varier plusieurs param\u00e8tres : <br>\n   1. **max_depth** : profondeur max de l'arbre \n   2. **n_estimators** : nombre d'arbres dans le mod\u00e8le","af9e8b98":"Il y a dix pays de destination possible. Par ailleurs on remarque que la colonne \"year\" est compos\u00e9e uniquement d'une seule valeur : 2015. Elle n'est donc pas utile dans notre \u00e9tude.\n<!-- -->","d47eb308":"Avec max_depth = 10, le score est \u00e9galement satisfaisant mais moins bon qu'avec 5. \nPar dichotomie, on va essayer avec 8.","1ac8dbb1":"On peut remarquer ici que la plupart des personnes ont entre 25 et 50 ans. Il semble donc que ce soit des jeunes et des adultes qui font le plus de reservations sur le site airbnb.\n\n<!-- -->","71345326":"On va donc cr\u00e9er des classes afin de connaitre les destinations choisies en fonction du groupe d'age des individus.","4faecea8":"# Pr\u00e9paration des donn\u00e9es\n\nLes donn\u00e9es sur lesquelles nos mod\u00e8les vont apprendre sont les donn\u00e9es issues du fichier train_users_2.csv. <br>\nNous allons donc en m\u00eame temps qu'\u00e9tudier nos donne\u00e9es, effectuer une pr\u00e9paration afin de corriger les valeurs ab\u00e9rrantes.","10008831":"# Bibliographie\n\nhttps:\/\/www.kaggle.com\/residentmario\/images-in-kernels <br>\nhttps:\/\/scikit-plot.readthedocs.io\/en\/stable\/Quickstart.html <br>\nhttps:\/\/mrmint.fr\/overfitting-et-underfitting-quand-vos-algorithmes-de-machine-learning-derapent <br>\nhttps:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html <br>\nhttps:\/\/www.kaggle.com\/rounakbanik\/airbnb-new-user-bookings <br>\nhttps:\/\/www.kaggle.com\/roydatascience\/airbnb-new-user-booking-random-forest <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html <br>\nhttps:\/\/machinelearningmastery.com\/evaluate-gradient-boosting-models-xgboost-python\/ <br>\n","def64f3d":"Le score sur les donn\u00e9es d'apprentissage est de 100% ce qui veut dire que notre mod\u00e8le a \u00e9t\u00e9 sur-appris. Cela est du au fait que l'arbre a \u00e9t\u00e9 d\u00e9v\u00e9lopp\u00e9 jusqu'\u00e0 la fin. On peut d'ailleurs le voir avec notre score sur les donn\u00e9es pr\u00e9dites qui est de 60% ce qui est tr\u00e8s faible\n\nNous allons donc mettre une limite \u00e0 sa profondeur afin d'avoir un r\u00e9sultat un peu plus r\u00e9aliste. Nous choisissons une profondeur maximale de 5 = Nombre de caract\u00e9ristiques \/ colonnes divis\u00e9 par 3.","55e770a4":"# Pr\u00e9sentation du projet\n\nLe but de ce projet est de d\u00e9crire la premi\u00e8re destination o\u00f9 un utilisateur d'AirBnb ira. <br>\nPour cela, nous disposons de 6 fichiers : <br>\n   *     train_users.csv : l'ensemble des donn\u00e9es d'apprentissage <br>\n   *     test_users.csv : l'ensemble des donn\u00e9es \u00e0 pr\u00e9dire <br>\n   *     sessions.csv : le log des sessions internet des utilisateurs <br>\n   *     countries.csv : des statistiques sur le pays de destination <br>\n   *     age_gender_bkts.csv : des statistiques sur les ages, les groupes d'ages, le sexe <br>\n   *     sample_submission.csv : le format pour soumettre les predictions <br>\n\nNotre d\u00e9marche a \u00e9t\u00e9 compos\u00e9e de 4 grandes parties : \n   1. **D\u00e9couverte des donn\u00e9es** : on va commencer \u00e0 prendre nos marques avec les fichiers en d\u00e9couvrant les colonnes, les lignes ... de chacun des fichiers <br>\n   2. **Visualisation des donn\u00e9es** : pour le fichier train_users_2, on va r\u00e9aliser des visualisations des donn\u00e9es afin de mieux les appr\u00e9hender. Le fichier train_users_2 correspondant \u00e0 nos donn\u00e9es d'apprentissage. <br>\n   3. **Pr\u00e9paration des donn\u00e9es** : sur les fichiers train_users_2 et test_users on va r\u00e9aliser la normalisation de nos donn\u00e9es (remplacement des NaN par unknow par exemple) <br>\n   4. **Data Mining** : on va r\u00e9aliser plusieurs mod\u00e8les afin de pr\u00e9dire la destination des utilisateurs fournis dans le fichier test_users <br>\n","d676f4aa":"Le r\u00e9sultat est moins int\u00e9ressant car le score sur les donn\u00e9es de validation a baiss\u00e9.\nPar dichotomie, on essaie avec 10. ","215c8bb8":"Comme vous pouvez le constater, l'Espagne et les Pays-Bas sont ceux qui attirent le plus les jeunes. A l'oppos\u00e9, la Grande-Bretagne a tendance \u00e0 attirer des personnes plus ag\u00e9es.","1af082fa":"Une autre mani\u00e8re de repr\u00e9senter ceci est d'effectuer un diagramme \u00e0 moustache.","1e872638":"Les r\u00e9sultats sont int\u00e9ressants m\u00eame sans aucun param\u00e8trage. Nous allons toute fois essayer d'am\u00e9liorer ces performances.","4b916418":"<!-- -->\nOn effectue tout d'abord une concat\u00e9nation entre les fichiers train_users_2 et test_users de mani\u00e8re \u00e0 ce que chaque modification se fasse sur les deux fichiers. <br>\nLa visualisation des donn\u00e9es s'effectuera quand \u00e0 elle uniquement sur le fichier train_users_2","46a97f64":"On d\u00e9finit ici une fonction pour cr\u00e9er nos graphiques en barres empil\u00e9es","f56df516":"# Conclusion\n\nPour conclure ce projet nous a familiaris\u00e9es avec les algorithmes et techniques de machine learning vus en cours. <br>\nNous avons pu constater que la phase de pr\u00e9paration des donn\u00e9es peut s'av\u00e9rer complexe car il faut au pr\u00e9alable bien comprendre les donn\u00e9es en question pour pouvoir les exploiter. <br> \nNous avons rencontr\u00e9 certaines difficult\u00e9s \u00e0 ce sujet lors de la r\u00e9alisation du projet notamment concernant les valeurs ab\u00e9rantes que nous avons du \u00e9carter. C'est la cas de l'\u00e2ge pour laquelle nous avions plus de 28000 lignes ab\u00e9rantes comme par exemple \"2014\". Nous avions alors d\u00e9cid\u00e9 de remplacer ces valeurs ab\u00e9rantes par des np.nan. Les r\u00e9sultats des mod\u00e8les \u00e9taient fauss\u00e9es et lorsque nous avons fait notre soumission \u00e0 kaggle, une seule classe \u00e9tait pr\u00e9dite : US et le score \u00e9tait de 23%. Nous avons donc d\u00e9cid\u00e9 de faire un compromis en supprimant ces lignes. Le score a \u00e9t\u00e9 encore plus mauvais puisque nous avons obtenu un score de 12 %. Nous avons ensuite essayer de supprimer la colonne age. Le r\u00e9sultat n'a pas non plus \u00e9t\u00e9 concluant. On est donc revenu \u00e0 des np.Nan. Notre score final est de 23,5 %. <br> <br>\n\nUne autre difficult\u00e9 a \u00e9t\u00e9 le choix des mod\u00e8les mais plus encore de leur param\u00e8tre. Nous avons donc proc\u00e9d\u00e9 \u00e0 une s\u00e9rie de tests pour augmenter nos scores.\n\nCe projet a donc \u00e9t\u00e9 tr\u00e8s instructif pour nous et nous esp\u00e9rons pour vous aussi lors de votre lecture."}}