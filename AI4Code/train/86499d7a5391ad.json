{"cell_type":{"848cc50f":"code","c8f75a8d":"code","95a396b1":"code","67205251":"code","932482f8":"code","f86c1601":"code","3d408fc0":"code","5bc307e8":"code","da168c5b":"code","e320d0f8":"code","e1ef3b6f":"code","98768109":"code","5e37b137":"markdown","4b8b64fb":"markdown","6bcf236f":"markdown","f3b3cf19":"markdown","0eea67c3":"markdown","7f0e2b0a":"markdown","31f4441d":"markdown","af5e08aa":"markdown","21b02ca5":"markdown","c558585d":"markdown","fe740421":"markdown","405332d5":"markdown","cc2dd891":"markdown"},"source":{"848cc50f":"import numpy as np \nimport pandas as pd \n\nespn = pd.read_csv(\"..\/input\/youtube-video-statistics\/ESPN.csv\")\nespn.head()","c8f75a8d":"from nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\nclass LemmaTokenizer: # Keeping terms composed of only alphabets\n  def __init__(self):\n   self.wnl = WordNetLemmatizer()\n  def __call__(self, doc):\n   return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if re.match(r'(?u)\\b[A-Za-z]+\\b',t)] \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef warn(*args, **kwargs): # Turning off the warnings\n    pass\nimport warnings\nwarnings.warn = warn\n\nvectorizer = TfidfVectorizer(lowercase = False, tokenizer=LemmaTokenizer(), stop_words='english', min_df=0.001, max_df=0.99)\ncorpus = vectorizer.fit_transform(espn['title'])","95a396b1":"from sklearn.decomposition import NMF \nmodel = NMF(n_components=20, init='random', random_state=0)\ncorpus_by_topics = model.fit_transform(corpus) ","67205251":"max_index_in_row = np.argmax(corpus_by_topics, 1)\nmax_value_in_row = np.amax(corpus_by_topics, 1)\n\nprint(max_index_in_row)\nprint(max_value_in_row)","932482f8":"from scipy import stats\nstats.describe(max_value_in_row)","f86c1601":"topic = pd.Series(max_index_in_row)\nrelevance = pd.Series(max_value_in_row)\nframe = {'topic':topic, 'relevance':relevance}\ndf = pd.DataFrame(frame)\n\nespn_derived = pd.concat([df, espn[['likes', 'dislikes', 'comments']]], axis=1, sort=False)\nespn_derived.head()","3d408fc0":"#getting rid of negative values\nstats.describe(espn_derived['comments'])\nespn_derived['comments'] = np.where(espn_derived['comments'] < 0, 0, espn_derived['comments'] )\n\n#removing outliers\nz_scores = stats.zscore(espn_derived)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nespn_derived = espn_derived[filtered_entries]","5bc307e8":"import seaborn as sns\nsns.set(style=\"white\")\nimport matplotlib.pyplot as plt\n\n#used https:\/\/mokole.com\/palette.html to generate 20 visually distinct colors\ncolors=[ \"#696969\", \"#ffe4c4\", \"#2e8b57\", \"#8b0000\", \"#808000\", \"#000080\", \"#ff0000\", \"#ff8c00\", \"#ffd700\", \"#ba55d3\", \"#00ff7f\", \"#00bfff\", \"#0000ff\", \"#adff2f\", \"#ff00ff\", \"#f0e68c\", \"#fa8072\", \"#dda0dd\", \"#ff1493\", \"#7fffd4\"]\n\nplt.figure(figsize=(15,8))\n\n# it is not possible to parametrize alpha in Seaborn; so I define three levels of alphas and draw them seperately.\nespn_derived[\"alpha\"] = np.where(espn_derived['relevance'] < 0.22, 0.1, np.where(espn_derived['relevance'] < 0.4, 0.4, 0.5))\n\nax = sns.scatterplot(x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\", sizes=(1,500), size_norm=(100,4000), alpha=0.1, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.1])\n\nsns.scatterplot(legend=False, ax=ax, x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\",sizes=(1,500), size_norm=(100,4000), alpha=0.4, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.4])\n\nsns.scatterplot(legend=False, ax=ax, x=\"likes\", y=\"dislikes\", hue=\"topic\", size=\"comments\", sizes=(1,500), size_norm=(100,4000),alpha=0.5, palette=sns.color_palette(colors), data=espn_derived[espn_derived['alpha']==0.5])\n\nplt.plot([0,3000], [0,3000], color='r')\n\nplt.plot([0,14000], [0,1800], color='b')\n\nplt.legend(bbox_to_anchor=(1, 1), loc=2)\n\nax.legend(ncol=2)\n","da168c5b":"\ncorpus_by_topics_model = model.fit(corpus) # the wieght of each term in every topic       \nweight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[4])) # associating the actual terms with their weight\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","e320d0f8":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[17]))\n \nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","e1ef3b6f":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[3]))\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","98768109":"weight_dict = dict(zip(vectorizer.get_feature_names(), corpus_by_topics_model.components_[0]))\n\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=1600, height=800)\nwc.generate_from_frequencies(weight_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","5e37b137":"### <span style=\"color:red\">Topic analysis<\/span>\n#### Visualizing five dimensions of the dataframe: likes (x-axis), dislikes (y-axis), topic (color), number of comments (bubble size), the relevance of the topic (transparency of the bubble):","4b8b64fb":"#### Topic #17 mostly centers around the basketball personalities with James Leborn on the top (also including Michael Jordan, Dwyane Wade, Tyronn Lue, etc). On the other side of the spectrum, topic #3 seems to be quite favored by most of the viewers:","6bcf236f":"#### Now that we have the \"corpus\" (in the form of a sparse matrix), we can dig out topics using a suitable matrix factorization.    \n\n#### The classic method here would be SVD (Singular Value Decomposition). SVD in particular,\u00a0generates a byproduct called sigma matrix which is useful in some applications . SVD is also an exact factorization method. There is also NMF (Nonnegative Matrix Factorization), which may not deliver the mentioned perks, yet is otherwise fast and straightforward. In particular, NMF unlike SVD, does not produce negative values, which turn out to be tricky when it comes to interpretation (for more information on how NMF compares to SVD you can check out [here](https:\/\/medium.com\/@nixalo\/comp-linalg-l2-topic-modeling-with-nmf-svd-78c94330d45f) and [here](https:\/\/discuss.analyticsvidhya.com\/t\/how-is-svd-different-from-other-matrix-factorization-techniques-like-non-negative-matrix-factorization\/67519\/4)). So long story short, let's choose NMF over SVD to produce 20 topics (which sounds enough for sports category):","f3b3cf19":"#### Cleaning up the invalid \/ poor values which happen to exist in the dataframe:","0eea67c3":"#### Topic #3 is about Stephan A Smith (an ESON host) with a significant NFL undertone. Topic #0 is also among the favorites,\u00a0albeit less than\u00a0topic #0:\u00a0","7f0e2b0a":"#### Basically the same ESPN host, but here with a heavy NBA undertone.","31f4441d":"### <span style=\"color:red\">Assembling a table for analysis<\/span>\n#### One way of managing\u00a0the complexity of so many videos, is clustering them into several topics. To do so, first we need to vectorize the \"title\" column:","af5e08aa":"#### Rather than just particular sports\u00a0or technical aspects of them, topic #4 centers\u00a0around some strong words which foment controversy. We may also take a look\u00a0at topic #17 (associated with some of the most controversial videos):","21b02ca5":"#### The red line delineates where the number of likes equals the number of dislikes. So in a way, one can call the points on this line, controversial. But the situation seems to be more nuanced. For example we usually call the points close to the vertical axis also controversial. Or considering the fact that videos normally seem\u00a0to get more likes than dislikes, something like the blue delineator might have some metirts to it as well.\u00a0 At any rate, the plot right-off-the-bat, reveals curious materials for inference:   \u00a0\n* #### Viewers\u00a0do not seem to comment on videos when they haven't hit like or unlike (as the number of comments are considerably lower around the origin).\n* #### Some of the most liked and unliked videos (bubbles close to the tip of the margins), are not discussed as much.\n* #### The less nuanced topics (videos with lower relevance score), have lesser numbers of likes, dislikes, and comments.\n* #### Some of the most overwhelmingly liked or disliked videos are nuanced topics.   \n\n#### Now regarding the topics, topic #4 seems to be amongst the most contentious. A quick way to get a sense of the topic would be the wordcloud visualization:\u00a0","c558585d":"#### Next I'll assign the most relevant topic to each document in the corpus (corresponding to each title in the main table).","fe740421":"#### An average of almost 0.2 (with a rather small variance), doesn't seem to be terrible for 20 topics. Next I'll assemble a new table to work on:\u00a0","405332d5":"### <span style=\"color:red\">Introduction<\/span>\n#### A glance over the data shows that it allows for various studies into different aspects of the popularity of some YouTube videos (or lack-there-of). In particular perhaps, the types of videos which happen to be more contentious. So let's look at the ESPN data for example:","cc2dd891":"#### We can statistically look at how accurately the assigned topic describes\u00a0each document:\u00a0"}}