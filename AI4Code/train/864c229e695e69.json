{"cell_type":{"728feace":"code","02715050":"code","29b5971d":"code","5cd61824":"code","822b022b":"code","a21a5197":"code","67cf2247":"code","8aa97108":"code","949fea95":"code","4603479d":"code","0cd34855":"code","2e164f90":"code","b9066482":"code","a03db87d":"code","c6b6be4b":"code","b44e62b1":"code","d501ee7d":"code","ddb79867":"code","ae0f5472":"code","62224f50":"code","d9b28668":"code","5e0e65ca":"code","e02506fe":"code","011538a3":"code","945abe2a":"code","0fd50b2f":"code","75428775":"code","65207635":"code","6ed3f3c6":"code","b9378a0a":"code","c8dc0404":"code","4b06fb17":"code","ca0b8b50":"code","a936d6dc":"code","cc386eca":"code","5b3244ce":"code","ab6dea2c":"code","1f56a110":"code","7de39404":"code","283e02ad":"code","f52e1d41":"code","9fc65e74":"code","8b436cd4":"code","f086e760":"code","66c42146":"code","d40c4bae":"code","ad984256":"code","a69a6d2d":"code","6187b39e":"code","d8515554":"code","02c752e1":"code","b045baa0":"code","fdca12a8":"code","1ea5aa71":"code","18a7c56c":"code","d1878e52":"code","dd428df3":"code","d8f5bf8a":"code","8e90a834":"code","cd99f966":"code","bdcaefa2":"code","4b11dc08":"code","32ac08fd":"code","53a7cc8a":"code","fd7b45c7":"code","a05e96ad":"code","d72bf0ca":"code","a987dce4":"code","9d0ea273":"code","22714b69":"code","7d443119":"code","6cc733ae":"code","bb12a178":"code","e9a4da02":"code","4b416412":"code","71ace8ee":"code","a075efdd":"code","2edc29bc":"code","2bb5e642":"code","afa87a8f":"code","176005e3":"code","d1d24c8a":"code","bd728264":"code","c84116f2":"code","0ee1c6e4":"code","855f37aa":"code","3a071a8f":"code","418c6c96":"code","fc8e88de":"code","9ad39a1c":"code","ae6007ff":"code","dbff2974":"code","e8b53aea":"code","d001f8c2":"code","436a1b01":"code","13890611":"code","479dc578":"code","eefcda8b":"code","173105a2":"code","29c398b5":"code","0d8aa876":"code","21e50950":"code","ce124b1e":"code","4372d604":"code","7c810f15":"code","ab51c219":"code","bbeb408e":"code","9ff6d548":"code","1331d3ae":"code","307bea9b":"code","fd4f4eb5":"code","0b833a1c":"code","eb7a7a26":"code","c18c3d46":"code","5f2828a0":"code","13114de2":"code","9ba5e995":"code","2f2311ff":"code","29d382d3":"code","33ca6654":"code","5922b2da":"code","6115baf5":"code","02778785":"code","40b0f552":"code","2d595619":"code","9f0a5426":"code","07cfd750":"code","e1c332bd":"code","268ae0a7":"markdown","d9ff4c97":"markdown","ed769fc1":"markdown","ef8cefe1":"markdown","5d525866":"markdown","231acafe":"markdown","8e44934a":"markdown","f08e6b05":"markdown","6080c846":"markdown","058e8879":"markdown","0557d4d0":"markdown","59f9e482":"markdown","893917e4":"markdown","fffdf495":"markdown","529a282c":"markdown","51927779":"markdown","159ec220":"markdown","e089a21a":"markdown","11b52680":"markdown","2066a186":"markdown","5d047cef":"markdown","364c7889":"markdown","8f1409f7":"markdown","402f26dc":"markdown","15be654d":"markdown","95448055":"markdown","6b7c11cb":"markdown","32bc25d5":"markdown","c6811ca1":"markdown","58c8f053":"markdown","2996fcf0":"markdown","70be4f0c":"markdown","307bb04b":"markdown","18cd9c76":"markdown","315a34d4":"markdown","c09423b0":"markdown","6393b6e5":"markdown","2b9db76a":"markdown","b058ddfc":"markdown"},"source":{"728feace":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02715050":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D","29b5971d":"import plotly.offline as pyoff\nimport plotly.graph_objs as go","5cd61824":"import plotly.graph_objs as go","822b022b":"fake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')","a21a5197":"fake['Target']=1","67cf2247":"real['Target']=0","8aa97108":"frames = [fake, real]\n\ndf = pd.concat(frames)","949fea95":"df","4603479d":"patternDel = \"http\"\nfilter1 = df['date'].str.contains(patternDel)","0cd34855":"df = df[~filter1]","2e164f90":"pattern = \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\nfilter2 = df['date'].str.contains(pattern)","b9066482":"df=df[filter2]","a03db87d":"df['date'] = pd.to_datetime(df['date'])","c6b6be4b":"df_sub=df.groupby(['subject', 'Target'])['text'].count()","b44e62b1":"df_sub","d501ee7d":"df_sub = df_sub.unstack().fillna(0)\ndf_sub","ddb79867":"# Visualize this data in bar plot\nax = (df_sub).plot(\nkind='bar',\nfigsize=(10, 7),\ngrid=True\n)\nax.set_ylabel('Count')\nplt.show()","ae0f5472":"df_sub['Count']=df_sub[0]+df_sub[1]","62224f50":"import plotly.graph_objects as go\n\nlabels = df_sub.index\nvalues = df_sub['Count']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.show()","d9b28668":"df_=df.copy()","5e0e65ca":"df_=df_.sort_values(by=['date'])","e02506fe":"df_=df_.reset_index(drop=True)","011538a3":"df_","945abe2a":"df_1=df_[df_['Target']==1]","0fd50b2f":"df_1=df_1.groupby(['date'])['Target'].count()","75428775":"df_1=pd.DataFrame(df_1)","65207635":"df_1['Target']","6ed3f3c6":"df_0=df_[df_['Target']==0]","b9378a0a":"df_0=df_0.groupby(['date'])['Target'].count()","c8dc0404":"df_0=pd.DataFrame(df_0)","4b06fb17":"plot_data = [\n    go.Scatter(\n        x=df_0.index,\n        y=df_0['Target'],\n        name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n    ),\n    go.Scatter(\n        x=df_1.index,\n        y=df_1['Target'],\n        name='Fake'\n    )\n    \n]\nplot_layout = go.Layout(\n        title='Day-wise',\n        yaxis_title='Count',\n        xaxis_title='Time',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","ca0b8b50":"from wordcloud import WordCloud, STOPWORDS ","a936d6dc":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df[df['Target']==1]['text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n\n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","cc386eca":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df[df['Target']==0]['text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n\n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","5b3244ce":"df_['news']=df_['subject']+' '+df_['title']+' '+df_['text']","ab6dea2c":"df_['news'] = df_.apply(lambda x: x['news'].lower(),axis=1)","1f56a110":"df_[\"news\"] = df_['news'].str.replace('[^\\w\\s]','')","7de39404":"all_news=pd.DataFrame(pd.Series(' '.join(df_['news']).split()).value_counts())","283e02ad":"allnews1=all_news.head(30)","f52e1d41":"plot_data = [\n    go.Bar(\n        x=allnews1.index,\n        y=allnews1[0],\n        name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = allnews1[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","9fc65e74":"fake_news=pd.DataFrame(pd.Series(' '.join(df_[df_['Target']==1]['news']).split()).value_counts())","8b436cd4":"fake_news30=fake_news.head(30)","f086e760":"plot_data = [\n    go.Bar(\n        x=fake_news30.index,\n        y=fake_news30[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = fake_news30[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words from Fake news',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","66c42146":"real_news=pd.DataFrame(pd.Series(' '.join(df_[df_['Target']==0]['news']).split()).value_counts())","d40c4bae":"real_news30=real_news.head(30)","ad984256":"plot_data = [\n    go.Bar(\n        x=real_news30.index,\n        y=real_news30[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = real_news30[0]\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 words from Real news',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","a69a6d2d":"import nltk, re, string, collections\nfrom nltk.util import ngrams","6187b39e":"import re\nimport unicodedata\nimport nltk\nfrom nltk.corpus import stopwords","d8515554":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","02c752e1":"words = basic_clean(''.join(str(df_['news'].tolist())))","b045baa0":"bigram_all=(pd.Series(nltk.ngrams(words, 2)).value_counts())[:30]","fdca12a8":"bigram_all=pd.DataFrame(bigram_all)\nbigram_all","1ea5aa71":"bg_a=bigram_all.copy()","18a7c56c":"bg_a['in']=bg_a.index","d1878e52":"bg_a['in'] = bg_a.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","dd428df3":"plot_data = [\n    go.Bar(\n        x=bg_a['in'],\n        y=bg_a[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bigrams from News',\n        yaxis_title='Count',\n        xaxis_title='Word',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","d8f5bf8a":"trigram_all=(pd.Series(nltk.ngrams(words, 3)).value_counts())[:30]","8e90a834":"trigram_all=pd.DataFrame(trigram_all)","cd99f966":"trigram_all","bdcaefa2":"trigram_all['in']=trigram_all.index","4b11dc08":"trigram_all['in'] = trigram_all.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","32ac08fd":"trigram_all","53a7cc8a":"plot_data = [\n    go.Bar(\n        x=trigram_all['in'],\n        y=trigram_all[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 trigrams from News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","fd7b45c7":"fake_words = basic_clean(''.join(str(df_[df_['Target']==1]['news'].tolist())))","a05e96ad":"bigram_fake=(pd.Series(nltk.ngrams(fake_words, 2)).value_counts())[:30]","d72bf0ca":"bigram_fake=pd.DataFrame(bigram_fake)","a987dce4":"bigram_fake","9d0ea273":"bigram_fake['in']=bigram_fake.index","22714b69":"bigram_fake['in'] = bigram_fake.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","7d443119":"plot_data = [\n    go.Bar(\n        x=bigram_fake['in'],\n        y=bigram_fake[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from Fake News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","6cc733ae":"trigram_fake=(pd.Series(nltk.ngrams(fake_words, 3)).value_counts())[:30]","bb12a178":"trigram_fake=pd.DataFrame(trigram_fake)\ntrigram_fake","e9a4da02":"trigram_fake['in']=trigram_fake.index","4b416412":"trigram_fake['in'] = trigram_fake.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","71ace8ee":"plot_data = [\n    go.Bar(\n        x=trigram_fake['in'],\n        y=trigram_fake[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from Fake News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","a075efdd":"true_words = basic_clean(''.join(str(df_[df_['Target']==0]['news'].tolist())))","2edc29bc":"bigram_true=(pd.Series(nltk.ngrams(true_words, 2)).value_counts())[:30]","2bb5e642":"bigram_true=pd.DataFrame(bigram_true)","afa87a8f":"bigram_true","176005e3":"bigram_true['in']=bigram_true.index","d1d24c8a":"bigram_true['in'] = bigram_true.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+')',axis=1)","bd728264":"plot_data = [\n    go.Bar(\n        x=bigram_true['in'],\n        y=bigram_true[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 bi-grams from True News',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","c84116f2":"trigram_true=(pd.Series(nltk.ngrams(true_words, 3)).value_counts())[:30]","0ee1c6e4":"trigram_true=pd.DataFrame(trigram_true)","855f37aa":"trigram_true","3a071a8f":"trigram_true['in']=trigram_true.index","418c6c96":"trigram_true['in'] = trigram_true.apply(lambda x: '('+x['in'][0]+', '+x['in'][1]+', '+x['in'][2]+')',axis=1)","fc8e88de":"plot_data = [\n    go.Bar(\n        x=trigram_true['in'],\n        y=trigram_true[0],\n        #name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 30 tri-grams from True News',\n        yaxis_title='Count',\n        xaxis_title='tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","9ad39a1c":"y = df_['Target'].values\n#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\nX = []\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in df_[\"news\"].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)\n\n#del data","ae6007ff":"import gensim","dbff2974":"#Dimension of vectors we are generating\nEMBEDDING_DIM = 100\n\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)","e8b53aea":"len(w2v_model.wv.vocab)","d001f8c2":"w2v_model[\"trump\"]","436a1b01":"w2v_model.wv.most_similar(\"trump\")","13890611":"w2v_model.wv.most_similar(\"obama\")","479dc578":"w2v_model.wv.most_similar(\"news\")","eefcda8b":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_['news'], \n                                                    df_['Target'], \n                                                    random_state=0)","173105a2":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)","29c398b5":"X_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","0d8aa876":"from sklearn.linear_model import LogisticRegression\n\n# Train the model\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","21e50950":"from sklearn.metrics import roc_auc_score\n\n# Predict the transformed test documents\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","ce124b1e":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","4372d604":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, predictions , labels=[0, 1])","7c810f15":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","ab51c219":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","bbeb408e":"from sklearn.naive_bayes import MultinomialNB \nvectorizer = CountVectorizer() \nX_train_transformed = vectorizer.fit_transform(X_train) \nX_test_transformed = vectorizer.transform(X_test)\n\nclf = MultinomialNB(alpha=0.1) \nclf.fit(X_train_transformed, y_train)\n\ny_predicted = clf.predict(X_test_transformed)","9ff6d548":"print('AUC: ', roc_auc_score(y_test, y_predicted))","1331d3ae":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, y_predicted, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, y_predicted, labels=[0, 1]))","307bea9b":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","fd4f4eb5":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","0b833a1c":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","eb7a7a26":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","c18c3d46":"cm = confusion_matrix(y_test, predictions , labels=[0, 1])","5f2828a0":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","13114de2":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","9ba5e995":"\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","2f2311ff":"vectorizer = TfidfVectorizer(min_df=3)\nX_train_transformed = vectorizer.fit_transform(X_train)\nX_test_transformed = vectorizer.transform(X_test)\n\nclf = MultinomialNB(alpha=0.1)\nclf.fit(X_train_transformed, y_train)\n\n# y_predicted_prob = clf.predict_proba(X_test_transformed)[:, 1]\ny_predicted = clf.predict(X_test_transformed)","29d382d3":"print('AUC: ', roc_auc_score(y_test, y_predicted))","33ca6654":"print(metrics.confusion_matrix(y_test, y_predicted, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, y_predicted, labels=[0, 1]))","5922b2da":"cm = confusion_matrix(y_test, y_predicted , labels=[0, 1])","6115baf5":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","02778785":"#ngrams\n# Fit the CountVectorizer to the training data specifiying a minimum \n# document frequency of 5 and extracting 1-grams and 2-grams\nvect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","40b0f552":"\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","2d595619":"print(metrics.confusion_matrix(y_test, predictions, labels=[0, 1]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_test, predictions, labels=[0, 1]))","9f0a5426":"cm = confusion_matrix(y_test, predictions , labels=[0, 1])","07cfd750":"ax= plt.subplot()\nsns.heatmap(cm, annot=True, ax= ax)\nax.set_ylabel('y_test')\nax.set_xlabel('predictions')","e1c332bd":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","268ae0a7":"Finding top 30 most frequent Tri-grams from Fake News","d9ff4c97":"Logistic Regression","ed769fc1":"Finding top 30 most frequent Tri-grams from Fake News","ef8cefe1":"Using CountVectorizer","5d525866":"Finding top 30 most frequent Bi-grams from Fake News","231acafe":"Pie chart for Subjects","8e44934a":"Top 30 most Frequently occuring words from True news","f08e6b05":"Multinomial Naive Bayes","6080c846":" tag cloud (word cloud or wordle or weighted list in visual design) is a novelty visual representation of text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color.","058e8879":"In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.","0557d4d0":"Word Cloud for Real News","59f9e482":"removing urls","893917e4":"word2vec:\n1. Word2vec is a two-layer neural net that processes text by \u201cvectorizing\u201d words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.","fffdf495":"Top 30 most Frequently occuring words from Fake news","529a282c":"Confusion Matrix","51927779":"Finding top 30 most frequent Bi-grams","159ec220":"Finding top 30 most frequent Bi-grams from True News","e089a21a":"Finding top 30 most frequent Tri-grams","11b52680":"Now it can be said that true news is from subjects like politicsNews and worldnews.","2066a186":"Reading Files","5d047cef":"Using TfidfVectorizer","364c7889":"Confusion Matrix","8f1409f7":"Multinomial Naive Bayes","402f26dc":"Finding N-grams:","15be654d":"Day-wise count of Fake and Real News","95448055":"Finding most similar words using word2vec","6b7c11cb":"Modeling (To Detect Fake news)","32bc25d5":"Top 30 most frequently occuring words","c6811ca1":"Importing required libraries","58c8f053":"Data Cleaning","2996fcf0":"Confsion Matrix","70be4f0c":"Bi-grams for news","307bb04b":"Now the winner is Logistic Regression with n-gram range 1, 2 for the detection of fake news.","18cd9c76":"AUC is 0.9984 which means this model is a good fit.","315a34d4":"N-gram Analysis","c09423b0":"Logistic Regression","6393b6e5":"Logistic Regression","2b9db76a":"Combining 'Subject', 'title' and 'text' into one column.","b058ddfc":"Word Cloud for Fake news"}}