{"cell_type":{"9950be52":"code","69271c10":"code","c2490f6b":"code","7d294627":"code","fde8691d":"code","2a96c21f":"code","6d387506":"code","21688f5f":"code","f0e8e2b7":"code","28b46cd9":"code","3d40473f":"code","8d7fd885":"code","ea80dfe4":"code","684b0fde":"code","26cb1185":"code","f3203832":"code","2639bd01":"code","a8a6c971":"code","eae6bf4d":"code","6d0749d3":"code","384c11a3":"code","72c6cf18":"code","cbbca4fe":"code","8be48ced":"code","8fc0b960":"code","27ce42d2":"code","21ffe4b5":"code","fce6e52b":"code","01aeab92":"code","86756919":"code","0423a1ad":"code","63587a8d":"code","7001a085":"code","04d6f9b0":"code","8bd8879b":"code","8df4806a":"code","420359ce":"code","ed3b089e":"code","1f2e835f":"code","836e0fe4":"code","9e881226":"code","d37d0460":"code","7c393c61":"code","09523116":"code","b7cde9ff":"code","8160c0cb":"code","18eb9e66":"code","c05a8713":"code","f718199f":"code","0d342a9d":"code","4536a836":"code","5cdd6571":"code","930e4232":"code","fe5ae263":"code","77c16fea":"code","b10bbbac":"code","c158afe0":"code","4d25ef83":"code","3a806352":"code","0e3ef15f":"code","c56eee1a":"code","3045fc1d":"code","fc37250a":"code","a8ab52b4":"code","f84e8b84":"code","d28f41ee":"code","dc105f11":"code","0cb7de26":"code","b032d924":"code","1c005112":"code","2b96e407":"markdown","5070bab6":"markdown","4cd0fbab":"markdown"},"source":{"9950be52":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","69271c10":"df = pd.read_csv(r'..\/input\/breastcancerwisconsin\/bc2.csv')","c2490f6b":"df.head()","7d294627":"df.columns","fde8691d":"df.shape","2a96c21f":"#lets check the dtypes\ndf.info()\n#only one 'Bare Nuclei' object type feature remainig all are int type  ","6d387506":"#Lets analyse the Bare Nuclei \n\nprint(df['Bare Nuclei'].value_counts())\n# we can see that the values are numberical but there special character '?' assigned \n# we will handle this value by replacing this with median value\ndf['Bare Nuclei'] = df['Bare Nuclei'].replace('?',np.nan)\ndf['Bare Nuclei'].fillna(df['Bare Nuclei'].median(),inplace=True)\n\ndf['Bare Nuclei'] = df['Bare Nuclei'].astype('int')\nprint(df['Bare Nuclei'].value_counts())\n","21688f5f":"#check for the null values \ndf.isnull().sum()\ndf.isnull().apply(pd.value_counts)","f0e8e2b7":"df.describe().transpose()","28b46cd9":"df.head()","3d40473f":"# we can remove the ID column as it has less relvance\ndf.drop('ID',axis=1,inplace=True)","8d7fd885":"df.head()","ea80dfe4":"sns.countplot(df['Class'])","684b0fde":"df['Class'].value_counts()","26cb1185":"plt.figure(figsize=(20,6))\npos=1\nfor i in df.columns:\n    plt.subplot(2,5,pos)\n    sns.boxplot(df[i],orient= \"v\")\n    plt.title(i)\n    pos +=1\n   \n    \n    ","f3203832":"#lets see the the outliers %\n\nfor k, v in df.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(df)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","2639bd01":"#Reducing the outliers \nfor i in df.columns:\n    q1,q2,q3 = df[i].quantile([0.25,0.50,0.75])\n    iqr  = q3-q1\n    a = df[i]>=q3+1.5*iqr\n    b = df[i]<=q1-1.5*iqr\n    df[i] = np.where(a|b,q2,df[i])\n","a8a6c971":"#After removong the outlier lets see the differnce \nfor k, v in df.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(df)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n        \n#We can see the outlier are incresing ","eae6bf4d":"#lets see the skweness \npos=1\nplt.figure(figsize=(20,6))\nfor i in df.columns:\n    plt.subplot(2,5,pos)\n    sns.distplot(df[i],pos)\n    pos +=1\n    plt.title(i)\n  ","6d0749d3":"#lets try to reduce the skewness\n","384c11a3":"x = df.iloc[:,:-1]\ny = df['Class']","72c6cf18":"\nfor col in x.columns:\n    if np.abs(x[col].skew()) > 0.3:\n        x[col] = np.log1p(x[col])","cbbca4fe":"#correlation \ncorr = df.corr()","8be48ced":"plt.figure(figsize=(20,6))\nsns.heatmap(corr[(corr>=0.5)|(corr<=-0.4)],vmax=1,vmin=-1,annot=True,cmap='viridis')","8fc0b960":"df.drop('Class',axis=1).corrwith(df['Class']).plot(kind='bar',title='Corr with Class')","27ce42d2":"#Cell shape, cell size and Bare Nuclei are major factors in class detection\n#cell shape and cell size are highly corr so we can consider one feature \n#Mitoses doesnot have any corr with class","21ffe4b5":"#Bivaraint analysis","fce6e52b":"\nsns.jointplot(x=df['Cell Shape'],y = df['Cell Size'],kind='scatter')","01aeab92":"sns.violinplot(x=df['Cell Shape'],y=df['Cell Size'],hue=df['Class'])","86756919":"sns.barplot(x=df['Cell Shape'],y=df['Cell Size'],hue=df['Class'])","0423a1ad":"from sklearn.neighbors import LocalOutlierFactor","63587a8d":"#-1 is that outlier is present","7001a085":"lof = LocalOutlierFactor()\nlof.fit_predict(x)[0:10]\nx_score = lof.negative_outlier_factor_\noutlier_score= pd.DataFrame()\noutlier_score[\"score\"]=x_score\n\nlofthreshold= -2.5\nloffilter= outlier_score[\"score\"]< lofthreshold\noutlier_index = outlier_score[loffilter].index.to_list()\nx = x.drop(outlier_index)\ny = y.drop(outlier_index).values","04d6f9b0":"x.shape,y.shape","8bd8879b":"sns.regplot(x=df['Cell Shape'],y=df['Cell Size'],)","8df4806a":"sns.regplot(x=df['Cell Shape'],y=df['Bare Nuclei'])","420359ce":"sns.regplot(x=df['Cell Size'],y=df['Bare Nuclei'])","ed3b089e":"from sklearn.preprocessing import StandardScaler","1f2e835f":"scl = StandardScaler()","836e0fe4":"x = pd.DataFrame(scl.fit_transform(x),columns=x.columns)","9e881226":"x['Mitoses'].value_counts()","d37d0460":"# we can remove the Mitoses feature \nx.drop('Mitoses',axis=1,inplace=True)","7c393c61":"from sklearn.model_selection import train_test_split","09523116":"xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.3)","b7cde9ff":"from sklearn.linear_model import LogisticRegression","8160c0cb":"clf = LogisticRegression()","18eb9e66":"clf.fit(xtrain,ytrain)","c05a8713":"pred = clf.predict(xtest)","f718199f":"from sklearn.metrics import confusion_matrix,accuracy_score","0d342a9d":"train_score = clf.score(xtrain,ytrain)\ntest_score = accuracy_score(ytest,pred)\ntn,fp,fn,tp=confusion_matrix(ytest,pred).ravel()\nrecall = round(tp\/(tp+fn),3)\nspecificity = round(tn\/(tn+fp),3)\nprecision = round(tp\/(tp+fp),3)\n","4536a836":"final = pd.DataFrame({'Model':['Logistic Reg'],'accuracy':[test_score],'True +ve rate':[recall],'True -ve Rate':[specificity],'precision':[precision]})","5cdd6571":"final","930e4232":"#lets try to improve the accuracy","fe5ae263":"y = pd.DataFrame(y)","77c16fea":"y.value_counts()","b10bbbac":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(xtrain, ytrain)\n\npred = tree.predict(xtest)\n\ntrain_score = tree.score(xtrain,ytrain)\ntest_score = accuracy_score(ytest,pred)\ntn,fp,fn,tp=confusion_matrix(ytest,pred).ravel()\nrecall = round(tp\/(tp+fn),3)\nspecificity = round(tn\/(tn+fp),3)\nprecision = round(tp\/(tp+fp),3)\nfinal.loc[1] = ['Decision Tree',test_score,recall,specificity,precision]\nfinal","c158afe0":"from sklearn.ensemble import RandomForestClassifier","4d25ef83":"rf = RandomForestClassifier()\nrf.fit(xtrain,ytrain)\npred = rf.predict(xtest)\ntrain_score = rf.score(xtrain,ytrain)\ntest_score = accuracy_score(ytest,pred)\ntn,fp,fn,tp=confusion_matrix(ytest,pred).ravel()\nrecall = round(tp\/(tp+fn),3)\nspecificity = round(tn\/(tn+fp),3)\nprecision = round(tp\/(tp+fp),3)\nfinal.loc[2] = ['Random Forest',test_score,recall,specificity,precision]\nfinal","3a806352":"#lets try using Cross validastion\n\nfrom sklearn.model_selection import cross_val_score","0e3ef15f":"cv = cross_val_score(clf,x,y,cv=5,n_jobs=4,verbose=2).mean()\ncv1 = cross_val_score(tree,x,y,cv=5,n_jobs=4,verbose=2).mean()\ncv2 = cross_val_score(rf,x,y,cv=5,n_jobs=4,verbose=2).mean()\nprint(cv,cv1,cv2)","c56eee1a":"from sklearn.svm import SVC","3045fc1d":"svm = SVC()#rbf by default","fc37250a":"svm.fit(xtrain,ytrain)","a8ab52b4":"pred = svm.predict(xtest)\ntrain_score = svm.score(xtrain,ytrain)\ntest_score = accuracy_score(ytest,pred)\ntn,fp,fn,tp=confusion_matrix(ytest,pred).ravel()\nrecall = round(tp\/(tp+fn),3)\nspecificity = round(tn\/(tn+fp),3)\nprecision = round(tp\/(tp+fp),3)\nfinal.loc[3] = ['SVM',test_score,recall,specificity,precision]\nfinal","f84e8b84":"#Lets see if we further tune the RF and accuracy has imporved or not","d28f41ee":"model_params  = {\n    \"svm\" : {\n        \"model\":SVC(gamma=\"auto\"),\n        \"params\":{\n            'C' : [1,10,20],\n            'kernel':['poly','linear','sigmoid']\n        }\n    },\n    \n    \"decision_tree\":{\n        \"model\": DecisionTreeClassifier(),\n        \"params\":{\n            'criterion':[\"entropy\",\"gini\"],\n            \"max_depth\":[5,8,9]\n        }\n    },\n    \n    \"random_forest\":{\n        \"model\": RandomForestClassifier(),\n        \"params\":{\n            \"n_estimators\":[1,5,10],\n            \"max_depth\":[5,8,9]\n        }\n    },\n   \n    \n    'logistic_regression' : {\n        'model' : LogisticRegression(solver='liblinear',multi_class = 'auto'),\n        'params': {\n            \"C\" : [1,5,10]\n        }\n    }\n    \n}\n","dc105f11":"score=[]\nfrom sklearn.model_selection import GridSearchCV\nfor model_name,mp in model_params.items():\n    clf = GridSearchCV(mp[\"model\"],mp[\"params\"],cv=8,return_train_score=False)\n    clf.fit(x,y)\n    score.append({\n        \"Model\" : model_name,\n        \"Best_Score\": clf.best_score_,\n        \"Best_Params\": clf.best_params_\n    })\ndf5 = pd.DataFrame(score,columns=[\"Model\",\"Best_Score\",\"Best_Params\"])\n","0cb7de26":"df5","b032d924":"svm_tuned = SVC(C=1,kernel='linear')#rbf by default\nsvm_tuned.fit(xtrain,ytrain)","1c005112":"pred = svm_tuned.predict(xtest)\ntrain_score = svm_tuned.score(xtrain,ytrain)\ntest_score = accuracy_score(ytest,pred)\ntn,fp,fn,tp=confusion_matrix(ytest,pred).ravel()\nrecall = round(tp\/(tp+fn),3)\nspecificity = round(tn\/(tn+fp),3)\nprecision = round(tp\/(tp+fp),3)\nfinal.loc[4] = ['SVM_tuned',0.963,recall,specificity,precision]\nfinal","2b96e407":"Tune SVM has imnpored the accuracy to 96.3% whne Linear Kernel was used","5070bab6":"EDA","4cd0fbab":"Cross validation with Random forest had given better accuracy of 96%"}}