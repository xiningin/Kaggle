{"cell_type":{"5482b497":"code","84b241ad":"code","f130d4f0":"code","465dd7e2":"code","cc67bd21":"code","74f031d3":"code","fa64aa87":"code","211379d4":"markdown","d9764308":"markdown","4196411f":"markdown","bceb0999":"markdown"},"source":{"5482b497":"# Essential libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_circles\nfrom sklearn.datasets import make_moons\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n\n# IPython libraries\n\nfrom ipywidgets import interactive\nfrom IPython.display import display\nimport ipywidgets as widgets","84b241ad":"data = make_circles(n_samples=100, shuffle=True, noise=0, random_state=2020)\nX,Y = data","f130d4f0":"# Lets plot our circles\n\nplt.figure(figsize=(8,5))\nplt.scatter(X[:,0],X[:,1],c=Y,s=200,edgecolors='k')\nplt.xlabel('x1',fontsize=14)\nplt.ylabel('x2',fontsize=14)\nplt.grid(True)\nplt.show()","465dd7e2":"def decision_tree(max_depth):\n    \n    classifier = DecisionTreeClassifier(max_depth = max_depth)\n    classifier.fit(X,Y)\n    # Plotting the decision boundary\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    Z = classifier.predict(np.c_[xx.ravel(),yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx,yy,Z,alpha=0.4)\n    plt.scatter(X[:,0], X[:,1], c = Y, s = 20, edgecolor = 'k')\n    plt.xlabel(\"Sepal length (cm)\")\n    plt.ylabel(\"Petal length (cm)\")\n    plt.show()","cc67bd21":"\nstyle = {'description_width': 'initial'}\nm = interactive(decision_tree,max_depth=widgets.IntSlider(min=1,max=8,step=1,description= 'Max Depth',\n                                       stye=style,continuous_update=False))\n\n# Set the height of the control.children[-1] so that the output does not jump and flicker\noutput = m.children[-1]\noutput.layout.height = '350px'\n\n# Display the control\ndisplay(m)","74f031d3":"data = make_moons(n_samples=100, shuffle=True, noise=0.15, random_state=2020)\nX,Y = data\n\n\n# Lets plot our moons\n\nplt.figure(figsize=(8,5))\nplt.scatter(X[:,0],X[:,1],c=Y,s=200,edgecolors='k')\nplt.xlabel('x1',fontsize=14)\nplt.ylabel('x2',fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n","fa64aa87":"style = {'description_width': 'initial'}\nm = interactive(decision_tree,max_depth=widgets.IntSlider(min=1,max=8,step=1,description= 'Max Depth',\n                                       stye=style,continuous_update=False))\n\n# Set the height of the control.children[-1] so that the output does not jump and flicker\noutput = m.children[-1]\noutput.layout.height = '350px'\n\n# Display the control\ndisplay(m)","211379d4":"## In the both plots, we see that Decision Tree classifier is able to classify both circles and moons but not very accurately. This is because the Decision trees love orthogonal decision boundaries i.e. all splits are perpendicular to an axis.","d9764308":"### Lets now load the moon dataset into objects and visualize it ","4196411f":"## In this notebook, we will see how Decision tree classifier performs on a data which is not linearly separable. ","bceb0999":"### Now lets first load the circles dataset into objects and visualize it "}}