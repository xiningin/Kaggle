{"cell_type":{"c628b329":"code","f5b306e3":"code","8d95ae7e":"code","c7ea086d":"code","f4e7d751":"code","753b5dab":"code","9c548b89":"code","ccd9d86d":"code","b77d8548":"code","2d7292b9":"code","188a7b07":"code","e12d0f7b":"code","2ae2bdf3":"code","434c9a33":"code","1acd63f1":"markdown","1003ada8":"markdown","b57839de":"markdown","6a5d3bda":"markdown","607be7bf":"markdown","573c21a1":"markdown","d8f4b96e":"markdown"},"source":{"c628b329":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5b306e3":"#load data set\n\nx= np.load('\/kaggle\/input\/sign-language-digits-dataset\/X.npy')\ny = np.load('\/kaggle\/input\/sign-language-digits-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x[260].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x[900].reshape(img_size,img_size))\nplt.axis('off')\n","8d95ae7e":"X = np.concatenate((x[204:409],x[822:1027]), axis=0)\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o),axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \",X.shape)\nprint(\"Y shape: \",Y.shape)","c7ea086d":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","f4e7d751":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test.reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten \",X_train_flatten.shape)\nprint(\"X test flatten \",X_test_flatten.shape)","753b5dab":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)\n","9c548b89":"def dummy(parameter):\n    dummy_parameter = parameter + 5\n    return dummy_parameter\nresult = dummy(3)\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b= 0.0\n    return w,b","ccd9d86d":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","b77d8548":"y_head = sigmoid(0)\ny_head","2d7292b9":"# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\n\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x.train.shape[1]\n    return cost","188a7b07":"def forward_backward_propagation(w,b,x_train,y_train):\n    #forward\n    z= np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    #backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]   # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","e12d0f7b":"# Updating(learning) parameters\ndef update(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list =[]\n    cost_list2=[]\n    index =[]\n\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n    \n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        if i%10==0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of number_of_iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list   \n    \n    ","2ae2bdf3":" # prediction\ndef predict(w,b,x_test):\n    z= sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction= np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n            \n    return Y_prediction\n","434c9a33":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","1acd63f1":"# Forward Propagation","1003ada8":"* Mathematical expression of log loss(error) function is that: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>","b57839de":"# Logistic Regression","6a5d3bda":"* The all steps from pixels to cost is called forward propagation\n","607be7bf":"#  Optimization Algorithm with Gradient Descent\n\n","573c21a1":"* Lets flatten X array(images array).","d8f4b96e":"![image.png](attachment:image.png)\n\n* Parameters are weight and bias\n* Weights: Coefficients of each pixels\n* Bias: intercept\n* z = (w.T)x + b => z equals to (transpose of weights times input x) + bias \n* In an other saying => z = b + px1*w1 + px2*w2 + ... + px4096*w4096\n* y_heaad = sigmoid(z)\n"}}