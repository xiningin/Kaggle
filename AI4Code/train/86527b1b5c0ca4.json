{"cell_type":{"2c2c5f9a":"code","74b439a9":"code","e72678d0":"code","9658494d":"code","d0527361":"code","d4030422":"code","84273bd9":"code","8205a02d":"code","b5236276":"code","a17c7b52":"code","a7482ca5":"code","06029835":"code","b9e76ff3":"code","02a5b532":"code","9ef5163e":"code","401523b1":"code","7554be69":"code","730606e7":"code","b4c64529":"code","2dbfc075":"code","b5953556":"code","62583801":"code","b0b2b839":"code","d4dd5d40":"code","b2587fc3":"code","587ab008":"code","bb8dd4eb":"code","7545e6cb":"code","bf171e79":"code","f9f3a9e5":"code","4ad8cc3d":"code","998ea2ee":"code","2ed80cb7":"code","79e92765":"code","ee6c8558":"code","2e4596e7":"code","7a5a6dcc":"code","44709ed9":"code","7fb5891f":"code","d3bbb7c1":"code","24c66bef":"code","20474eaa":"code","881aae68":"code","1566cef7":"code","25535ffa":"code","df023937":"code","a1d1845d":"code","0ae7a3c2":"code","647ec75e":"code","064c7f89":"code","cf82a3c4":"code","9f0f6d72":"code","3501630a":"code","efc37c13":"code","929cb8f3":"code","b1bf1613":"code","c2123f0e":"code","1429faf8":"code","230052e3":"code","bee82d26":"code","dd5083a4":"code","f5649d03":"code","aadf45c0":"code","321d3754":"code","6d1a7e5c":"code","756526fa":"code","869b2709":"code","6b220612":"code","78baa66f":"code","e88d9146":"code","3345c6b4":"code","85248333":"markdown","2d6be5ed":"markdown","01027f2d":"markdown","40be8a2a":"markdown","7274a528":"markdown","730139ab":"markdown","0ee4c07f":"markdown","931dc562":"markdown","bd0d0f3e":"markdown","534a768f":"markdown","b77fec95":"markdown","2b1adbb5":"markdown","3c2801c8":"markdown","c876f92b":"markdown","5495be61":"markdown","ad4057c5":"markdown","c2b73f47":"markdown","b413a2d3":"markdown","3d578b4d":"markdown","b40b722b":"markdown","0e4b6daa":"markdown","9ee15d7e":"markdown","c9343a10":"markdown","62c0e917":"markdown","88a93751":"markdown","72d4af31":"markdown","3d7074c6":"markdown","d7643074":"markdown","38f1c1a2":"markdown"},"source":{"2c2c5f9a":"#importing library\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","74b439a9":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","e72678d0":"train.head()","9658494d":"test.head()","d0527361":"train.info()","d4030422":"test.info()","84273bd9":"#First check out the survival rate\ntrain['Survived'].value_counts(normalize=True)","8205a02d":"sns.countplot(train['Survived'])","b5236276":"train['Survived'].groupby(train['Pclass']).mean()","a17c7b52":"sns.countplot(train['Pclass'], hue=train['Survived'])","a7482ca5":"train['Sex'].value_counts(normalize=True)","06029835":"#Survival rate w.r.t. sex\ntrain['Survived'].groupby(train['Sex']).mean()","b9e76ff3":"sns.countplot(train['Sex'], hue=train['Survived'])","02a5b532":"train['Survived'].groupby(pd.qcut(train['Age'],5)).mean()","9ef5163e":"pd.qcut(train['Age'],5).value_counts()","401523b1":"sns.countplot(train['Age'], hue=train['Survived'])","7554be69":"train.info()","730606e7":"train['Survived'].groupby(train['SibSp']).mean()","b4c64529":"train['SibSp'].value_counts()","2dbfc075":"sns.countplot(train['SibSp'], hue=train['Survived'])","b5953556":"train['Survived'].groupby(train['Parch']).mean()","62583801":"train['Parch'].value_counts()","b0b2b839":"sns.countplot(train['Parch'], hue=train['Survived'])","d4dd5d40":"train['Ticket'].head()","b2587fc3":"train['Ticket_Length'] = train['Ticket'].apply(lambda x: len(x))","587ab008":"train['Ticket_Length'].value_counts()","bb8dd4eb":"#Another important feature is of using ticket first letter as room number\ntrain['Ticket_Letter'] = train['Ticket'].apply(lambda x: str(x)[0])\ntrain['Ticket_Letter'].value_counts()","7545e6cb":"train.groupby(['Ticket_Letter'])['Survived'].mean()","bf171e79":"sns.countplot(train['Ticket_Letter'], hue=train['Survived'])","f9f3a9e5":"pd.qcut(train['Fare'], 5).value_counts()","4ad8cc3d":"train['Survived'].groupby(pd.qcut(train['Fare'], 5)).mean()","998ea2ee":"pd.crosstab(pd.qcut(train['Fare'],10), columns=train['Pclass'])","2ed80cb7":"sns.countplot(train['Fare'], hue=train['Survived'])","79e92765":"train['Embarked'].value_counts()","ee6c8558":"train['Embarked'].value_counts(normalize=True)","2e4596e7":"train['Embarked'].value_counts(normalize=True)","7a5a6dcc":"sns.countplot(train['Embarked'], hue=train['Survived'])","44709ed9":"sns.countplot(train['Embarked'], hue=train['Pclass'])","7fb5891f":"train['Name'].head()","d3bbb7c1":"train['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\ntrain['Name_Title'].value_counts()","24c66bef":"train['Survived'].groupby(train['Name_Title']).mean()","20474eaa":"#let's see any correlation between name lenth and survival\ntrain['Name_Length'] = train['Name'].apply(lambda x: len(x))\ntrain['Survived'].groupby(pd.qcut(train['Name_Length'],5)).mean()","881aae68":"pd.qcut(train['Name_Length'],5).value_counts()","1566cef7":"sns.countplot(train['Name_Length'], hue=train['Survived'])","25535ffa":"sns.countplot(train['Name_Title'], hue=train['Survived'])","df023937":"g = sns.pairplot(data=train, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","a1d1845d":"#first handel the Age \ndef age_impute(train, test):\n    for i in [train, test]:\n        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n    return train, test","0ae7a3c2":"def family_size(train, test):\n    for i in [train, test]:\n        i['Family_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n        del i['SibSp']\n        del i['Parch']\n    return train, test\n","647ec75e":"def ticket_grouped(train, test):\n    for i in [train, test]:\n        i['Ticket_Letter'] = i['Ticket'].apply(lambda x: str(x)[0])\n        i['Ticket_Letter'] = i['Ticket_Letter'].apply(lambda x: str(x))\n        i['Ticket_Letter'] = np.where((i['Ticket_Letter']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Letter'],\n                                   np.where((i['Ticket_Letter']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n                                            'Low_ticket', 'Other_ticket'))\n        i['Ticket_Length'] = i['Ticket'].apply(lambda x: len(x))\n        del i['Ticket']\n    return train, test\n","064c7f89":"def names(train, test):\n    for i in [train, test]:\n        i['Name_Length'] = i['Name'].apply(lambda x: len(x))\n        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n        del i['Name']\n    return train, test\n","cf82a3c4":"def embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test","9f0f6d72":"test['Fare'].fillna(train['Fare'].mean(), inplace = True)","3501630a":"def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letter', 'Name_Title', 'Family_Size']):\n    for column in columns:\n        train[column] = train[column].apply(lambda x: str(x))\n        test[column] = test[column].apply(lambda x: str(x))\n        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n        del train[column]\n        del test[column]\n    return train, test","efc37c13":"def drop(train, test, bye = ['PassengerId']):\n    for i in [train, test]:\n        for z in bye:\n            del i[z]\n    return train, test","929cb8f3":"train, test = names(train, test)\ntrain, test = age_impute(train, test)\ntrain, test = embarked_impute(train, test)\ntrain, test = family_size(train, test)\ntrain, test = ticket_grouped(train, test)\ntrain, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letter','Name_Title', 'Family_Size'])\ntrain, test = drop(train, test)","b1bf1613":"train.head()","c2123f0e":"test.head()","1429faf8":"#checking the correlation between features and survival\nsns.heatmap(train.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","230052e3":"drop_column = ['Cabin']\ntrain.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)","bee82d26":"#Final Check the train dataset\ntrain.info()","dd5083a4":"#Final Check for Test dataset\ntest.info()","f5649d03":"from sklearn.svm import SVC\nclassifier_svc = SVC(kernel = 'rbf',random_state = 0)\nclassifier_svc.fit(train.iloc[:, 1:], train.iloc[:, 0])","aadf45c0":"print(classifier_svc.score(train.iloc[:, 1:], train.iloc[:, 0]))","321d3754":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\nclassifier.fit(train.iloc[:, 1:], train.iloc[:, 0])\n\nprint(\"%.4f\" % classifier.oob_score_)","6d1a7e5c":"print(classifier.score(train.iloc[:, 1:], train.iloc[:, 0]))","756526fa":"from sklearn import model_selection\nscores = model_selection.cross_val_score(classifier, train.iloc[:, 1:], train.iloc[:, 0], scoring = 'accuracy', cv = 10)","869b2709":"print(scores)","6b220612":"prediction_forest = classifier.predict(test)","78baa66f":"predictions = pd.DataFrame(prediction_forest, columns=['Survived'])\ntest = pd.read_csv(os.path.join('..\/input', 'test.csv'))\npredictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\npredictions.to_csv('output.csv', sep=\",\", index = False)","e88d9146":"predictions.head()","3345c6b4":"predictions['Survived'].hist()","85248333":"# End notes...\n\n## <font color='blue'>I hope you find this kernel useful and enjoyable.Please upvote it.<\/font>\n\n### Your comments and feedback are most welcome.","2d6be5ed":"## Fare...\n\nNow looking closely you can figure out that Fare is closely related to Ticket, room, class and survival.","01027f2d":"### Pclass\n\nClass played a critical role in survival, as the survival rate decreased drastically for the lowest class. This variable is both useful and clean, and I will be treating it as a categorical variable.","40be8a2a":"**Droping few column which doesn't useful for prediction**","7274a528":"This first function creates two separate columns: a numeric column indicating the length of a passenger's Name field, and a categorical column that extracts the passenger's title.","730139ab":"## Ticket..\n\nNot all factor directly contribute in deciding factor of survival.Just like Ticket column,but it may help in deciding other factor which may effect the survival like ticket column help in decid type of ticket each survivar has which help to decide the location of people in it. It can be done with the help of number of characters in the Ticket column.","0ee4c07f":"We can put the cabin column in dataset but there is lot of missing value in it which may reduce the model performance.It give accuracy of 82%.so we drop the Cabin column.","931dc562":"**Name**\n\nI am not very clear about the contribution of name in survival rate. let's try to extract some information from Name.","bd0d0f3e":"# About the dataset..\u00b6\nThe data has been split into two groups:\n\ntraining set (train.csv) test set (test.csv) The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.","534a768f":"We fill the null values in the Embarked column with the most commonly occuring value, which is 'S.'","b77fec95":"Interpreting The Heatmap The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","2b1adbb5":"**SEX**\n\nAnother Important Feature is ** SEX**.Sex matter lot for deciding the survival rate. Women and childern has greater chance of survial than men.let compare the survival rate with sex.","3c2801c8":"**One-Hot-Encoding**\n\nCreating the dummies value for the character value.","c876f92b":"**Embarked**\n\nLooks like the Cherbourg people had a 20% higher survival rate than the other embarking locations. This is very likely due to the high presence of upper-class passengers from that location.","5495be61":"**Age**\n\nAs sex affect the survival rate. Age also matter a lot for deciding the survival rate.Before comparing the survival with Age, we have to handle the missing value of age columns.","ad4057c5":"## Parch...\n\npassengers with zero parents or children had a lower likelihood of survival than otherwise, but that survival rate was only slightly less than the overall population survival rate.","c2b73f47":"## First understand the Problem statment...\n\nIn this challenge, we have to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","b413a2d3":"**Sibsp** ","3d578b4d":"## Applying the ML Model","b40b722b":"## Feature Engineering\n\n*Now wwe have clear idea of dependencies of different fetarue on survival of a person. Let's apply **Feature Engineering** to extract some more features.*\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist.","0e4b6daa":"# About this notebook...\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\nYou can check out my previous work related to it in some other aproach.[click here](https:\/\/www.kaggle.com\/saife245\/titanic-deep-learning-model-with-80-accuracy)\n\n\n## <font color='red'>If You Like the notebook and think that it helped you PLEASE UPVOTE.<\/font>;)","9ee15d7e":"## Analyze the dataset and see the feature effect the Survival","c9343a10":"# Introduction...\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n","62c0e917":"**Data Defination**\n\n*Variable Definition Key*\n\n* survival Survival 0 = No, 1 = Yes\n\n* pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\n* sex Male or Female\n\n* Age Age in years\n\n* sibsp # of siblings \/ spouses aboard the Titanic\n\n* parch # of parents \/ children aboard the Titanic\n\n* ticket Ticket number\n\n* fare Passenger fare\n\n* cabin Cabin number\n\n* embarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n* Variable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n\n* Sibling = brother, sister, stepbrother, stepsister\n\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n\n* Parent = mother, father\n\n* Child = daughter, son, stepdaughter, stepson\n\n* Some children travelled only with a nanny, therefore parch=0 for them","88a93751":"As we see above **sibsp** and **Parch** are weak feature to decide the survival but with the help of this we can generate the new feature which can effect the survival more.","72d4af31":"## Topic will be convered in build the Machine Learning Model..\n\nA) Introduction\n\nB) Load the data\n\nC) Analyze the dataset\n\nD) Feature engineering \n\nE) Filling missing Values\n\nF) Modeling\n\nG) Prediction\n\n","3d7074c6":"**Handling Missing Value**","d7643074":"Handling the missing value of fare columns.","38f1c1a2":"The Ticket column is used to create two new columns: Ticket_Letter, which indicates the first letter of each ticket (with the smaller-n values being grouped based on survival rate); and Ticket_Length, which indicates the length of the Ticket field."}}