{"cell_type":{"60e78630":"code","92e91a48":"code","cf050e11":"code","5c40a3a9":"code","9766137c":"code","e69d011d":"code","06759b1e":"code","38accca1":"code","8bdfb4c4":"code","00ab9f9a":"code","4a108bbf":"code","4ae10e0f":"code","81acaf11":"code","b5b197a4":"code","80caf19e":"code","adf1e090":"code","37e97719":"code","96696091":"code","33bc097c":"code","2ec0aafd":"code","22eddfdc":"code","e2a116c0":"code","7e32e74e":"code","dd9920ed":"code","dbfedb5b":"code","b9a31c73":"code","46c9c008":"code","f30a176f":"code","44677d9b":"code","b3aeb379":"code","aa8ebeb0":"markdown","5d37401e":"markdown","6c18deee":"markdown","a0b78957":"markdown"},"source":{"60e78630":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","92e91a48":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","cf050e11":"#change the dataset location\ndf1 = pd.read_csv('\/kaggle\/input\/gpu-runtime\/sgemm_product.csv')\ndf = df1.sample(frac=0.4) #reducing data size for faster computation\ndf.shape","5c40a3a9":"#creating Runtime, target variable by taking average of Run1, Run2, Run3, Run4\ndf['Runtime']=df[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)","9766137c":"#viewing data\ndf.head()","e69d011d":"#drop other Run time variables\ndf1=df.drop(columns =['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'], axis = 1)\ndf1.info()","06759b1e":"#checking descriptive stats\ndf1.describe().T","38accca1":"#checking for NULL values\ndf1.isnull().sum() #no NULL values","8bdfb4c4":"#checking for outliers\nplt.figure(figsize=(10,6))\nsns.boxplot(df1['Runtime']);","00ab9f9a":"#removing outliers\nQ1=df1['Runtime'].quantile(0.25)\nQ2=df1['Runtime'].quantile(0.75)\nIQR = Q2 - Q1\nLL=Q1-1.5*IQR\nUL=Q2+1.5*IQR\ndf2 = df1[(df1.Runtime>LL) & (df1.Runtime<UL)]\ndf2.describe().T","4a108bbf":"plt.figure(figsize=(10,6))\nsns.boxplot(df2['Runtime']);","4ae10e0f":"#checking variable distribution\nfor index in range(10):\n   df2.iloc[:,index] = (df2.iloc[:,index]-df2.iloc[:,index].mean()) \/ df2.iloc[:,index].std();\ndf2.hist(figsize= (14,16));","81acaf11":"#plotting the distribution of Runtime\nsns.distplot(df2['Runtime'])","b5b197a4":"df2['target']=np.log(df2.Runtime)\nsns.distplot(df2['target'])","80caf19e":"plt.figure(figsize=(14,14))\nsns.set(font_scale=1)\nsns.heatmap(df2.corr(),cmap='GnBu_r',annot=True, square = True ,linewidths=.5);\nplt.title('Variable Correlation')","adf1e090":"#Creating binary classification target variable\nmean = df2['target'].mean()\ndf2.loc[df2['target'] <= mean, 'target'] = 0\ndf2.loc[df2['target'] > mean, 'target'] = 1\ndf_target=df2[['target']].values\ndf_features=df2.drop(columns=['target','Runtime'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)","37e97719":"sc = StandardScaler()\nx1_train = sc.fit_transform(x1_train)\nx1_test = sc.transform(x1_test)","96696091":"#Linear SVM\nprint('Linear Model',end='\\n')\nlsvclassifier = SVC(kernel='linear')\nlsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = lsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_linear=accuracies.mean()\nstd_svm_linear=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_linear*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_linear*100,end='\\n')\n\n#Predict SVM\ny_predl = lsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predl))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predl))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predl, normalize=True, sample_weight=None))","33bc097c":"#Polynomial SVM\nprint('Polynomial Model',end='\\n')\npsvclassifier = SVC(kernel='poly')\npsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = psvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_poly=accuracies.mean()\nstd_svm_poly=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_poly*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_poly*100,end='\\n')\n\n#Predict SVM\ny_predp = psvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predp))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predp))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predp, normalize=True, sample_weight=None))","2ec0aafd":"#RBF SVM\nprint('RBF Model',end='\\n')\nrsvclassifier = SVC(kernel='rbf')\nrsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = rsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_rbf=accuracies.mean()\nstd_svm_rbf=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_rbf*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_rbf*100,end='\\n')\n\n#Predict SVM\ny_predr = rsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predr))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predr))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predr, normalize=True, sample_weight=None))","22eddfdc":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n  \n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs,scoring='accuracy', train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\n\ntitle = r\"Learning Curves (SVM, RBF kernel, $\\gamma=auto$)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n#estimator = SVC(kernel = 'rbf', random_state = 0,gamma='auto')\nplot_learning_curve(rsvclassifier, title, df_features, df_target, (0.8, 1.1), cv=cv)\nplt.show()","e2a116c0":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(rsvclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","7e32e74e":"#Entropy Model\neclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\neclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = eclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_e=accuracies.mean()\nstd_dt_e=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_e*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_e*100,end='\\n')\n\n#predict y\ny_pred = eclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","dd9920ed":"#Gini Model\ngclassifier = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ngclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = gclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_g=accuracies.mean()\nstd_dt_g=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_g*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_g*100,end='\\n')\n\n#predict y\ny_pred = gclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","dbfedb5b":"#Pruning the better tree - Gini Tree\nparameters = [{'criterion': ['gini'],'min_samples_leaf':[5,10,20,30,50,100],'max_depth':[1,5,10,20,50,100]}] \ngrid_search = GridSearchCV(estimator = gclassifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","b9a31c73":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(gclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","46c9c008":"# Boosting via Gradient Boost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifiergb = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nclassifiergb.fit(x1_train, y1_train)\n\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifiergb, X = x1_train, y = y1_train, cv = 10,n_jobs=-1)\nmean_boosting=accuracies.mean()\nstd_boosting=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_boosting*100,end='\\n')\nprint('Standard deviation of Accuracies',std_boosting*100,end='\\n')\n\n# Predicting the Test set results\ny_predgb = classifiergb.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_predgb))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_predgb))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_predgb))\n","f30a176f":"#playing around with the pruning to get the best boosting tree\n# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier_AdaBoost = AdaBoostClassifier(random_state=1)\nclassifier_AdaBoost.fit(x1_train, y1_train)\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [50,100,200,300,500,1000,1500]}] \ngrid_search = GridSearchCV(estimator = classifier_AdaBoost,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","44677d9b":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, df_features, df_target,cv=3,n_jobs=-1)\ntrain_sizes \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\n\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\nplt.legend(loc=\"best\")\nplt.show","b3aeb379":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","aa8ebeb0":"#**Data** **Preprocessing**","5d37401e":"#Decision Trees\n","6c18deee":"#Boosting","a0b78957":"#Run SVM\n"}}