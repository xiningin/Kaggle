{"cell_type":{"7d874ab9":"code","1bfc213f":"code","c403fe93":"code","b67382ed":"code","eca03eac":"code","2376d0fa":"code","b89327c0":"code","52c7b04f":"code","4bcc800a":"code","2d5e47ee":"code","75d0e01a":"code","3c98686c":"code","713c9acd":"code","b8d6a80d":"code","873e176d":"markdown","99d30de8":"markdown","77f0c51d":"markdown","88ac0af7":"markdown","7729b98f":"markdown","25a4d05d":"markdown","775ad7a6":"markdown","3aac5b15":"markdown","7032cc07":"markdown","c6062b59":"markdown","44c0f648":"markdown","a49e085c":"markdown","a868de83":"markdown","5f6c3cb1":"markdown"},"source":{"7d874ab9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nprint(os.listdir(\"..\/input\/tiny-imagenet\/tiny-imagenet-200\"))","1bfc213f":"import torch\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor, Compose, Normalize, RandomHorizontalFlip, RandomErasing, RandomPerspective, RandomRotation\n\ncomposed_transformers = Compose([\n    ToTensor(),\n    # \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f\n    #RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0),\n    #RandomRotation(15, resample=False, expand=False, center=None, fill=None)\n    #RandomHorizontalFlip(p=0.5),\n    #RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n    #Normalize((0.5, 0.5, 0.5), (0.225, 0.225, 0.225)),    \n])\ndataset = ImageFolder('..\/input\/tiny-imagenet\/tiny-imagenet-200\/train', composed_transformers)\n\n# \u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train, validation (cv_data) \u0438 test (val_data)\ntorch.manual_seed(42)\ntrain_data, cv_data, val_data = torch.utils.data.random_split(dataset, [80000, 10000, 10000])\n#val_data = ImageFolder('..\/input\/tiny-imagenet\/tiny-imagenet-200\/test', composed_transformers)\n#test_data = ImageFolder('..\/input\/tiny-imagenet\/tiny-imagenet-200\/val', composed_transformers)\nlen(train_data), len(cv_data), len(val_data)","c403fe93":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0438 \u0432\u044b\u0431\u043e\u0440\u043e\u043a\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nbatch_size = 256\n\n# train \u0432\u044b\u0431\u043e\u0440\u043a\u0430\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# validation \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 train\ncv_loader = torch.utils.data.DataLoader(cv_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n# \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e accuracy\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)","b67382ed":"# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0431\u0430\u0442\u0447 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 (16 \u0448\u0442.) \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train\nfrom torchvision.utils import make_grid\n\nfor images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0)))\n    break","eca03eac":"# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0431\u0430\u0442\u0447 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 (16 \u0448\u0442.) \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 (cv_data)\nfrom torchvision.utils import make_grid\n\nfor images, _ in cv_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0)))\n    break","2376d0fa":"# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0431\u0430\u0442\u0447 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438 (16 \u0448\u0442.) \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0422\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 (val_data)\nfrom torchvision.utils import make_grid\n\nfor images, _ in val_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0)))\n    break","b89327c0":"# \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c GPU\n# \u0435\u0441\u043b\u0438 \u043c\u044b \u043d\u0430 GPU, \u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0432\u0435\u0434\u0435\u043d\u043e \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435, \u0438\u043d\u0430\u0447\u0435 - \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u0442\u0441\u044f CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif(not torch.cuda.is_available()):\n    print('\u0422\u0435\u043a\u0443\u0449\u0438\u0439 device - CPU. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c GPU (\u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430 \u043d\u0435 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0430)')\nelse:\n    print(device)","52c7b04f":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)","4bcc800a":"class CNNClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #self.device = device\n        #self.conv0 = nn.Conv2d(3, 32, kernel_size=3)\n        #self.maxpool0 = nn.MaxPool2d((2,2))\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n        #self.bnorm1 = nn.BatchNorm2d(64)\n        #self.drop1 = nn.Dropout(p=0.3)\n        self.maxpool1 = nn.MaxPool2d((4,4))\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n        self.maxpool2 = nn.MaxPool2d((2,2))\n        #self.conv3 = nn.Conv2d(64, 3, kernel_size=3)\n        #self.bnorm2 = nn.BatchNorm2d(3)\n        #self.drop2 = nn.Dropout(p=0.5)\n        self.flatten = Flatten()\n        # kernel = 3,  conv1-64, maxpool 4x4\n        self.fc1 = nn.Linear(4608, 1024)\n        self.fc2 = nn.Linear(1024, 200)\n        \n        #self.fc1 = nn.Linear(512, 256)\n        #self.fc2 = nn.Linear(256, 200)\n        # kernel = 3,  conv1-64, maxpool 4x4\n        #self.fc1 = nn.Linear(14400, 4096)\n        #self.fc2 = nn.Linear(4096, 200)\n        # kernel = 3, conv-16 x 3 - maxpool 4x4\n        #self.fc = n.Linear(3600, 200)\n        # kernel = 3, conv-16 x 3 - maxpool 2x2\n        #self.fc = nn.Linear(15376, 200)\n        # maxpooling, kernel = 3, conv-64, maxpool 4x4\n        #self.fc = nn.Linear(14400, 200)\n        # \u0431\u0435\u0437 maxpooling, kernel = 3, conv-64 x 3\n        #self.fc = nn.Linear(10092, 200)\n        # \u0431\u0435\u0437 maxpooling, kernel = 3\n        # self.fc = nn.Linear(10800, 200)\n        # \u0434\u043b\u044f maxpooling, kernel = 3\n        #self.fc = nn.Linear(972, 200)\n        # \u0431\u0435\u0437 maxpooling, kernel = 2\n        #self.fc = nn.Linear(11532, 200)\n        \n    \n    def forward(self, x):\n        # forward pass \u0441\u0435\u0442\u0438\n        # \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0435\u0441\u043e\u0432 1 \u0441\u043b\u043e\u044f \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n        #x = F.relu(self.conv0(x))\n        #x = self.maxpool0(x)\n        x = F.relu(self.conv1(x))\n        #x = self.bnorm1(x)\n        #x = self.drop1(x)\n        x = self.maxpool1(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool2(x)\n        #x = F.relu(self.conv3(x))\n        #x = self.bnorm2(x)\n        #x = self.drop2(x)\n        x = self.flatten(x)\n        # \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0435\u0441\u043e\u0432 2 \u0441\u043b\u043e\u044f \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n        #print(x.shape)\n        #x = F.softmax(self.fc(x))\n        #x = F.relu(self.fc(x))\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ncnn_model = CNNClassifier()\n\n# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u0441\u0435\u0442\u044c \u043d\u0430 GPU\ncnn_model.to(device)","2d5e47ee":"# Loss-\u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\n\nfrom torch import optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nLEARNING_RATE = 7e-4\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, cnn_model.parameters()),\n    lr=LEARNING_RATE,\n)\n\nscheduler = CosineAnnealingLR(optimizer, 1)","75d0e01a":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0448\u0430\u0433\u0438 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\n\ndef train_epoch(model, optimizer, train_loader):\n    model.train()\n    total_loss, total = 0, 0\n    for i, (inputs, target) in enumerate(train_loader):\n        # \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u043d\u0430 GPU\n        inputs = inputs.to(device)\n        target = target.to(device)\n        \n        # Reset gradient\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(inputs)\n        \n        # Compute loss\n        loss = criterion(output, target)\n        \n        # Perform gradient descent, backwards pass\n        loss.backward()\n\n        # Take a step in the right direction\n        optimizer.step()\n        scheduler.step()\n\n        # Record metrics\n        total_loss += loss.item()\n        total += len(target)\n\n    return total_loss \/ total\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0438 \u0448\u0430\u0433\u0438 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ndef validate_epoch(model, cv_loader):\n    model.eval()\n    total_loss, total = 0, 0\n    with torch.no_grad():\n        for inputs, target in cv_loader:           \n            # \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u043d\u0430 GPU\n            inputs = inputs.to(device)\n            target = target.to(device)\n            \n            # Forward pass\n            output = model(inputs)\n\n            # Calculate how wrong the model is\n            loss = criterion(output, target)\n\n            # Record metrics\n            total_loss += loss.item()\n            total += len(target)\n\n    return total_loss \/ total","3c98686c":"# \u0442\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c\nfrom tqdm import tqdm\n\nmax_epochs = 45\nn_epochs = 0\ntrain_losses = []\nvalid_losses = []\n\nfor epoch_num in range(max_epochs):\n\n    train_loss = train_epoch(cnn_model, optimizer, train_loader)\n    valid_loss = validate_epoch(cnn_model, val_loader)\n    \n    tqdm.write(\n        f'\u044d\u043f\u043e\u0445\u0430 #{n_epochs + 1:3d}\\ttrain_loss: {train_loss:.2e}\\tvalid_loss: {valid_loss:.2e}\\n',\n    )\n    \n    # Early stopping (\u0440\u0430\u043d\u043d\u044f\u044f \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430) \u0435\u0441\u043b\u0438 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 valid_loss (\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438) \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c 10 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 valid losses\n    if len(valid_losses) > 4 and all(valid_loss >= loss for loss in valid_losses[-5:]):\n        print('Stopping early')\n        break\n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    n_epochs += 1\n\nprint('\u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043e\u043a\u043e\u043d\u0447\u0435\u043d\u0430')","713c9acd":"# \u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a loss-\u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\nepoch_ticks = range(1, n_epochs + 1)\nplt.plot(epoch_ticks, train_losses)\nplt.plot(epoch_ticks, valid_losses)\nplt.legend(['Train Loss', 'Valid Loss'])\n#plt.legend(['Train Loss'])\nplt.title('Losses') \nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.xticks(epoch_ticks)\nplt.show()","b8d6a80d":"# \u042d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c CNN-\u043c\u043e\u0434\u0435\u043b\u0438\nfrom sklearn.metrics import accuracy_score\n\ncnn_model.eval()\ntest_accuracy, n_examples = 0, 0\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for inputs, target in val_loader:\n       \n        inputs = inputs.to(device)\n        target = target.to(device)\n        \n        probs = cnn_model(inputs)\n        \n        probs = probs.detach().cpu().numpy()\n        predictions = np.argmax(probs, axis=1)\n        target = target.cpu().numpy()\n        \n        y_true.extend(predictions)\n        y_pred.extend(target)\n        \ntest_accuracy = accuracy_score(y_true, y_pred)\n\nprint(\"Final results:\")\nprint(\"  test accuracy:\\t\\t{:.2f} %\".format(\n    test_accuracy * 100))\n\nif test_accuracy * 100 > 40:\n    print(\"Achievement unlocked: 110lvl Warlock!\")\nelif test_accuracy * 100 > 35:\n    print(\"Achievement unlocked: 80lvl Warlock!\")\nelif test_accuracy * 100 > 30:\n    print(\"Achievement unlocked: 70lvl Warlock!\")\nelif test_accuracy * 100 > 25:\n    print(\"Achievement unlocked: 60lvl Warlock!\")\nelse:\n    print(\"We need more magic! Follow instructons below\")\n","873e176d":"### \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0441\u0435\u0442\u0438","99d30de8":"### \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c Loss function","77f0c51d":"<u>\u0418\u0442\u043e\u0433\u043e\u0432\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u0435\u0442\u0438:<\/u>\n\n    (conv1): Conv2d(3, 64, kernel_size=3)\n    (maxpool1): MaxPool2d((4,4))\n    \n    (conv2): Conv2d(64, 128, kernel_size=3)\n    (maxpool2): MaxPool2d((2,2))\n\n    (flatten): Flatten()\n\n    (fc1): Linear(4608, 1024)\n    (fc2): Linear(1024, 200)\n\n<u>\u0424\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u0438\u043f\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:<\/u>\n* batch_size: 256\n* learning rate: 7e-4\n* \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 45\n* loss function: CrossEntropyLoss\n* optimizer: Adam\n* early stopping: 5\n\n<u>Accuracy:<\/u> **25.34%** \n\n\u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u0441\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u0430 13 \u044d\u043f\u043e\u0445 \u0431\u0435\u0437 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.","88ac0af7":"### \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","7729b98f":"### \u0424\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0441\u0435\u0442\u0438","25a4d05d":"<u>\u0418\u0441\u0445\u043e\u0434\u0430\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u0435\u0442\u0438:<\/u>\n\n(\u041d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0437\u0430\u043d\u044f\u0442\u0438\u044f \u0441 \u043c\u0435\u043d\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0440\u0442 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0439 \u0434\u043b\u044f \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438)\n\n    (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n    (conv2): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1))\n  \n    (flatten): Flatten()\n  \n    (fc): Linear(in_features=10800, out_features=200, bias=True)\n\n<u>\u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0433\u0438\u043f\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:<\/u>\n* batch_size: 16\n* learning rate: 3e-3\n* \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 2\n* loss function: nn.CrossEntropyLoss\n* optimizer: Adam\n* early stopping: none\n\n<u>\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043e\u0446\u0435\u043d\u043a\u0438<\/u>\n* Accuracy score\n\n* \u0421\u0442\u0430\u0440\u0442\u043e\u0432\u044b\u0439 accuracy: 0.85%\n","775ad7a6":"1. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u0445: **0.85%**.\n\n2. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 10\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.36%**, train_loss: **3.31e-01**. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u0441\u0435\u0442\u044c \u0441 \u0442\u0430\u043a\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u0438 \u043d\u0435 \u0434\u0430\u0451\u0442 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 - \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u043a\u0430\u0440\u0442 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0439 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u0430\u043b\u043e \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0441\u043b\u043e\u0435 \u0438 \u043d\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u043b\u0438\u0431\u043e \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 (\u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 64\u044564 \u043f\u0438\u043a\u0441\u0435\u043b\u044f), \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0443\u0436\u043d\u043e \u0432\u043d\u0435\u0441\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u0435\u0440\u044c\u0451\u0437\u043d\u044b\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432 \u0441\u0430\u043c\u0443 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 \u0441\u0435\u0442\u0438.\n\n3. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 5, batch size = 256, early stopping = 3, **learning rate = 3e-2**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.36%**, train_loss: **3.31e-01**. \u0422.\u0435. \u0431\u0435\u0437 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439. \n\n4. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: (conv1): Conv2d(3, **15**, kernel_size=(3, 3), stride=(1, 1)), (conv2): Conv2d(**15**, 3, kernel_size=(3, 3), stride=(1, 1))\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.56%**, train_loss: **3.31e-01** (\u043f\u043e\u0447\u0442\u0438 \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f). \u0423\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043d\u0435 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435.\n\n5. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: (conv1): Conv2d(3, **128**, kernel_size=(3, 3), stride=(1, 1)), (conv2): Conv2d(**128**, 3, kernel_size=(3, 3), stride=(1, 1))\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.56%**, train_loss: **3.31e-01** (\u043f\u043e\u0447\u0442\u0438 \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f). \u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 = 128 \u0434\u043b\u044f cnn \u0438\u0441\u0445\u043e\u0434\u044f \u0438\u0437 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 [Leon Yao, John Miller \"Tiny ImageNet Classification with Convolutional Neural Networks\" (Stanford University)](http:\/\/cs231n.stanford.edu\/reports\/2015\/pdfs\/leonyao_final.pdf), \u0433\u0434\u0435 \u0430\u0432\u0442\u043e\u0440 \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c n = 32, 64, 128, 256 \u0438 512 \u0438  \u0443\u043a\u0430\u0437\u0430\u043b \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044f\u043c \u0441 \u043c\u0435\u043d\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432 \u043d\u0435 \u0443\u0434\u0430\u0451\u0442\u0441\u044f \u0432 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e\u0439 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c\u0441\u044f, \u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u043c\u0438 256 \u0438 512 \u043d\u0435 \u043e\u0431\u043b\u0430\u0434\u0430\u044e\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u043c\u0438 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430\u043c\u0438 \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.36%**, train_loss: **3.31e-01**. \u0422.\u0435. \u0431\u0435\u0437 \u043e\u0441\u043e\u0431\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439. \n\n6. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 20, **learning rate = 1e-1**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.46%**, train_loss: **3.31e-01**. \u0424\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445: 4 \u043f\u0440\u0438 early stopping = 3. \u0422.\u0435. \u0431\u0435\u0437 \u043e\u0441\u043e\u0431\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439. \n\n7. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"**learning rate = 3e-4**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **2.84%** - \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0435.\n\n8. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 30 **early stopping = 10**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **2.76%**, train_loss: **3.26e-01**. Accuracy \u0445\u0443\u0436\u0435, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 \u0431\u044b\u043b\u043e 11, Loss-function \u043e\u043a\u0430\u0437\u0430\u043b\u0430\u0441\u044c \u043b\u0443\u0447\u0448\u0435.\n\n9. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 5, early stopping = 3, **learning rate = 3e-5**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **3.31%**, train_loss: **3.29e-01**. Accuracy \u043b\u0443\u0447\u0448\u0435, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 \u0431\u044b\u043b\u043e 4.\n\n10. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: **\u0421onv2d-128 \u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 Convd2d-64**\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **3.45%**, train_loss: **3.29e-01**. Accuracy \u0441\u0442\u0430\u043b\u043e \u043b\u0443\u0448\u0447\u0435 \u043f\u0440\u0438 \u043c\u0435\u043d\u044c\u0448\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 \u0431\u044b\u043b\u043e 4.\n\n11. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445: 20, early stopping = 5\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **3.75%**, train_loss: **3.29e-01**. Accuracy \u0441\u0442\u0430\u043b\u043e \u0435\u0449\u0451 \u043b\u0443\u0448\u0447\u0435, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 \u0431\u044b\u043b\u043e 6.\n\n12. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 MaxPool2d \u043c\u0435\u0436\u0434\u0443 Conv1 \u0438 Conv2\"<\/u>. MaxPool2d \u043f\u043e\u0441\u043b\u0435 Conv \u0438 ReLU \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u043e\u0441\u044c \u0432 \u0441\u0435\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0430 \u043a\u0430\u043a Baseline \u0432 \u0440\u0430\u0431\u043e\u0442\u0435 [Hujia Yu \"Deep Convolutional Neural Networks for Tiny ImageNet Classification\" (Stanford University)](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf) \u0438 \u0434\u0430\u0432\u0430\u043b\u0430 \u0430\u0432\u0442\u043e\u0440\u0443 \u0447\u0443\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 33% \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0441\u0435\u0442\u0438: **3.23%**, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 - 6. \u0422.\u0435. MaxPooling \u0443\u0445\u0443\u0434\u0448\u0438\u043b \u0441\u0435\u0442\u044c.\n\n13. \u041e\u043f\u0438\u0440\u0430\u044f\u0441\u044c \u043d\u0430 [\u0442\u0443 \u0436\u0435 \u0440\u0430\u0431\u043e\u0442\u0443](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf), **\u0443\u0431\u0435\u0440\u0451\u043c Softmax** \u0441\u043b\u043e\u0439 \u0438 \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0432\u043c\u0435\u0441\u0442\u043e \u043d\u0435\u0433\u043e fc.linear. \u0422.\u0435. \u0441\u0435\u0442\u044c \u0431\u0443\u0435\u0442 \u0442\u0430\u043a\u043e\u0439: Conv2d-64 - ReLU - Pool - Conv2d-64 - ReLU - **Linear**. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **6.21%**, train_loss: **2.96e-01**. Accuracy \u0441\u0442\u0430\u043b\u043e \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0443\u0448\u0447\u0435, \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u044d\u043f\u043e\u0445 \u0431\u044b\u043b\u043e 6.\n\n14. \u041e\u043f\u0438\u0440\u0430\u044f\u0441\u044c \u043d\u0430 [\u0442\u0443 \u0436\u0435 \u0440\u0430\u0431\u043e\u0442\u0443](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf), \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043c \u043f\u043e\u0441\u043b\u0435 fc.linear \u0444\u0443\u043d\u043a\u0446\u0438\u044e ReLU. \u0422.\u0435. \u0441\u0435\u0442\u044c \u0431\u0443\u0435\u0442 \u0442\u0430\u043a\u043e\u0439: Conv2d-64 - ReLU - Pool - Conv2d-64 - ReLU- Linear - **ReLU**. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **2.34%**, \u0442.\u0435. ReLU \u0432 \u043a\u043e\u043d\u0446\u0435 \u043f\u043e\u0441\u043b\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u0445\u0443\u0434\u0448\u0430\u0435\u0442 \u0435\u0451, \u0432 \u0441\u0432\u044f\u0437\u0438 \u0441 \u0447\u0435\u043c \u043c\u044b \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u044d\u0442\u0443 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u044e\u043e\u043d\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0441 \u044d\u0442\u043e\u0433\u043e \u043c\u0435\u0441\u0442\u0430.\n\n15. <span style=\"color: green\">\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: \u0443\u0431\u0440\u0430\u043b\u0438 MaxPool2d \u043c\u0435\u0436\u0434\u0443 Conv1 \u0438 Conv2, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0443\u0434\u0430\u043b\u0438\u043b\u0438 ReLU \u0432 \u043a\u043e\u043d\u0446\u0435 \u0441\u0435\u0442\u0438\"<\/u>.<\/span> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**7.51%**<\/span>, train_loss: **2.82e-01**. \u0422.\u0435. Maxpooling \u0438 ReLU \u0432 \u043a\u043e\u043d\u0446\u0435 \u0441\u0435\u0442\u0438 \u044f\u0432\u043d\u043e \u0443\u0445\u0443\u0434\u0448\u0430\u043b\u0438 \u0435\u0451.\n \n16. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: kernel = 2 \u0434\u043b\u044f Conv1 \u0438 Conv2; 30 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **4.25%**, \u0442.\u0435. kernel = 3 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435.\n\n17. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: kernel = 2, \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 dropout(0.3) \u043f\u043e\u0441\u043b\u0435 Conv1 \u0438 dropout(0.5) \u043f\u043e\u0441\u043b\u0435 Conv2, \u0447\u0442\u043e\u0431\u044b \u0441\u0435\u0442\u044c \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c; 30 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **7.12%**, \u0442.\u0435. \u043d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u0448\u0430\u0433\u0435 \u0441\u0435\u0442\u044c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c.\n\n18. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: **kernel = 3**, \u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438 dropout(0.3) \u043f\u043e\u0441\u043b\u0435 Conv1 \u0438 dropout(0.5) \u043f\u043e\u0441\u043b\u0435 Conv2, \u0447\u0442\u043e\u0431\u044b \u0441\u0435\u0442\u044c \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c; 30 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **7.17%**, \u0442.\u0435. \u043d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u0448\u0430\u0433\u0435 \u0441\u0435\u0442\u044c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c.\n\n19. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: \u043e\u043f\u0438\u0440\u0430\u044f\u0441\u044c \u043d\u0430 [\u0442\u0443 \u0436\u0435 \u0440\u0430\u0431\u043e\u0442\u0443](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf) \u0438\u0437\u043c\u0435\u043d\u0438\u043c learning rate <u>\"learning rate = 1e-3; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **4.36%**\n\n20. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 (\u043e\u0442\u043a\u0430\u0442\u0438\u043b\u0438 \u0434\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 16, \u0433\u0434\u0435 \u0431\u044b\u043b\u043e 7.51% + \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0438 learning rate): <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: kernel = 3, \u0443\u0431\u0440\u0430\u043b\u0438 dropout, **learning rate = 1e-3**; 6 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **0.39%** \u0421\u0438\u043b\u044c\u043d\u043e\u0435 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0435\n\n21. <span style=\"color: green\">\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 (\u043e\u0442\u043a\u0430\u0442\u0438\u043b\u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0434\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 15, \u0433\u0434\u0435 \u0431\u044b\u043b\u043e 7.51%): <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: kernel = 3, \u0443\u0431\u0440\u0430\u043b\u0438 dropout, **learning rate = 3e-5**, early stopping = 3; 10 \u044d\u043f\u043e\u0445\"<\/u>.<\/span> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**7.66%**<\/span> (\u0432\u0442\u043e\u0440\u043e\u0439 \u0440\u0430\u0437 \u043f\u043e\u043a\u0430\u0437\u0430\u043b 7.32%). \u041f\u043e \u0444\u0430\u043a\u0442\u0443: 10 \u044d\u043f\u043e\u0445, early stopping \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u044d\u043f\u043e\u0445 \u043c\u043e\u0436\u0435\u0442 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c accuracy.\n\n22. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 Batch Normalization \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e Conv-\u0441\u043b\u043e\u044f; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **6.29%**. \u041c\u043e\u0434\u0435\u043b\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u043b\u0430\u0441\u044c, \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043d\u0430 \u0432\u0430\u043b\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u043e\u0448\u0451\u043b \u0432\u0432\u0435\u0440\u0445.\n\n23. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u041e\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d Batch Normalization \u043f\u043e\u0441\u043b\u0435 Conv1; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **6.38%**. \u041c\u043e\u0434\u0435\u043b\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u043b\u0430\u0441\u044c, \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043d\u0430 \u0432\u0430\u043b\u0434\u0438\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u043e\u0448\u0451\u043b \u0432\u0432\u0435\u0440\u0445. \u0411\u0435\u0437 \u043e\u0441\u043e\u0431\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043a \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0439 \u0432\u0440\u0430\u0438\u0430\u0446\u0438\u0438.\n\n24. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 (\u043e\u0442\u043a\u0430\u0442\u0438\u043b\u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0434\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 16(22), \u0433\u0434\u0435 \u0431\u044b\u043b\u043e 7.51%): <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u043d\u043e\u0432\u044b\u0439 Conv-64 \u0441\u043b\u043e\u0439 \u0441\u0435\u0442\u0438; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **7.36%**. Early stopping \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b. \u0422.\u0435. \u0434\u043e\u043f. \u0441\u043b\u043e\u0439 Conv \u043d\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u043b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u0430 \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0435\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043d\u0430 \u043f\u0440\u0435\u0436\u043d\u0435\u043c \u0443\u0440\u043e\u0432\u043d\u0435.\n\n25. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,128) - Conv2(128,64) - Conv3(64,3) - Linear; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **4.97%**, \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b Early stopping \u043d\u0430 7-\u043c\u043e\u043c \u0440\u0430\u0443\u043d\u0434\u0435 (\u0441\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 3 \u044d\u043f\u043e\u0445\u0438). \u0422.\u0435. \u0441 \u0442\u0430\u043a\u043e\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043e\u0439 \u0441\u0435\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0445\u0443\u0436\u0435.\n\n26. <span style=\"color: green\">\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: \u0443\u0431\u0440\u0430\u043b\u0438 \u0432\u0441\u0435 Conv \u0441\u043b\u043e\u0438, \u043a\u0440\u043e\u043c\u0435 \u043e\u0434\u043d\u043e\u0433\u043e Conv1(3,64) \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 maxpooling(4,4) \u043f\u043e\u0441\u043b\u0435 \u043d\u0435\u0433\u043e, \u0447\u0442\u043e \u0431\u044b \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u0447\u0438\u0441\u043b\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432; 10 \u044d\u043f\u043e\u0445\"<\/u>.<\/span> \u0412\u0434\u043e\u0445\u043d\u043e\u0432\u0435\u043d\u0438\u0435: [Hujia Yu \"Deep Convolutional Neural Networks for Tiny ImageNet Classification\" (Stanford University)](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf). \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**13.08%!**<\/span>, \u0432\u0442\u043e\u0440\u043e\u0439 \u0440\u0430\u0437 - 13.73% - \u044d\u0442\u043e \u043f\u0440\u043e\u0440\u044b\u0432, Early stopping \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u0435\u0442 \u0434\u0430\u0442\u044c \u0435\u0449\u0451 \u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u0422.\u0435. \u0441 \u0442\u0430\u043a\u043e\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043e\u0439 \u0441\u0435\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0443\u0447\u0448\u0435!\n\n27. <span style=\"color: green\">\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0441\u0435\u0442\u044c \u211626 \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u044d\u043f\u043e\u0445 - 30 \u0441 early stopping = 5\"<\/u>.<\/span> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**16.43%!**<\/span> - \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u0440\u044b\u0432! Early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 24 \u044d\u043f\u043e\u0445\u0435.\n\n28. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 \u0441\u0435\u0442\u0438, \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u043d\u0443\u044e \u043a \u0441\u0442\u0430\u0442\u044c\u0435: [Hujia Yu \"Deep Convolutional Neural Networks for Tiny ImageNet Classification\" (Stanford University)](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/931.pdf):\n\n|Layer  |  Dimension |\n|-------|:-----------:|\n|Input  |  64x64x3|\n|CONV1-16 | 56x56x16|\n|ReLU ||\n|Pool-2 |   28x28x16|\n|FC1-1024 | 1x1x1024|\n|ReLU ||\n|Output |   1x1x200|\n\n\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,16) - ReLU - Maxpool(2,2) - fc.Linear - ReLU; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **1.58%** - \u043e\u0447 \u043f\u043b\u043e\u0445\u043e\u0439\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u0445\u0443\u0434\u0448\u0438\u043b\u0430\u0441\u044c.\n\n29. \u0423\u0431\u0435\u0440\u0451\u043c \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 28 \u0432 \u043a\u043e\u043d\u0446\u0435 ReLU. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,16) - ReLU - Maxpool(2,2) - fc.Linear; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **11.88%** - \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441\u0438\u043b\u044c\u043d\u043e \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 ReLU, \u043d\u043e \u0432\u0441\u0451 \u0440\u0430\u0432\u043d\u043e \u0435\u0449\u0451 \u0445\u0443\u0436\u0435, \u0447\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 27\/28 (Conv1(3,64) - ReLU - Maxpool(4,4) - fc.Linear). \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e, \u0447\u0442\u043e \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u043b\u0438\u044f\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u043d\u0438\u0439 \u0441\u043b\u043e\u0439 Linear (\u0432\u043e \u0432\u0441\u0435\u0445 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u044f\u0445 \u043e\u043d \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e \u0431\u044b\u043b \u0431\u043e\u043b\u0435\u0435 10 000), \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0435\u0433\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0435:\n\n30. \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0437\u0435\u0440\u043d\u043e maxpooling, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u0441\u043b\u043e\u0439: \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,16) - ReLU - **Maxpool(3,3)** - fc.Linear; 10 \u044d\u043f\u043e\u0445\"<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **8.73%** - \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 \u043d\u0435 \u043e\u043f\u0440\u0430\u0432\u0434\u0430\u043b\u0430\u0441\u044c, \u0442.\u043a.\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0445\u0443\u0436\u0435.\n\n31. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u0441\u044f \u043a \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c - 26 \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0430\u0443\u0433\u043c\u0435\u0442\u043d\u0430\u0446\u0438\u044e. \u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435: <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,64) - ReLU - Maxpool(4,4) - fc.Linear; 10 \u044d\u043f\u043e\u0445\" + \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f: RandomHorizontalFlip, RandomErasing<\/u>. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **12.17%** - \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0445\u0443\u0436\u0435, \u0447\u0435\u043c \u0431\u0435\u0437 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e, \u044d\u0442\u0438 \u0442\u0438\u043f\u044b \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442.\n\n32. \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430, \u0447\u0442\u043e\u0431\u044b \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u0448\u043b\u0430 \u0431\u044b\u0441\u0442\u0440\u0435\u0435: \u0441 16 \u0434\u043e 256, \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u211631 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u0434\u043e 45 \u0438 early stopping \u0434\u043e 5. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **12.46%**, \u0442.\u0435. \u043f\u043e\u0447\u0442\u0438 \u0431\u0435\u0437 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u0445\u043e\u0442\u044c \u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043b\u043e\u0441\u044c, \u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0430 \u044d\u0442\u043e \u043d\u0435 \u0434\u0430\u043b\u043e. \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0442\u0438\u043f\u0430\u043c\u0438 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043f\u043b\u043e\u0445\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442.\n\n33. \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043f\u0435\u0440\u0435\u0434 \u0432\u0445\u043e\u0434\u043e\u043c \u0432 \u0441\u0435\u0442\u044c. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **12.6%** - \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u043e\u0447\u0442\u0438\u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0441\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430 - \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0443\u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u044b.\n\n34. <span style=\"color: green\">\u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u0441\u044f \u043a \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c - \u211626<\/span> \u0438 \u043f\u043e\u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u0435\u043c \u0441 Learning rate:\n    * LR = 3e-3, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 14.67%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 6 \u044d\u043f\u043e\u0445\u0435. \u041c\u043e\u0434\u0435\u043b\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f\n    * LR = 1e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 17.16%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 43 \u044d\u043f\u043e\u0445\u0435.\n    * LR = 2e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 17.42%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 23 \u044d\u043f\u043e\u0445\u0435.\n    * LR = 3e-4. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 16.93% \u0437\u0430 10 \u044d\u043f\u043e\u0445 \u0431\u0435\u0437 early stopping\n    * LR = 3e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 17.15%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 16 \u044d\u043f\u043e\u0445\u0435\n    * LR = 4e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 16.95%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 13 \u044d\u043f\u043e\u0445\u0435\n    * LR = 5e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 17.24%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 11 \u044d\u043f\u043e\u0445\u0435.\n    * <span style=\"color: green\">LR = **7e-4**<\/span>, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">17.41%<\/span>, \u0432\u0442\u043e\u0440\u043e\u0439 \u0440\u0430\u0437 - 17.4% (\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u044b\u0439), early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 9 \u044d\u043f\u043e\u0445\u0435.\n    * LR = 8e-4, 45 \u044d\u043f\u043e\u0445, early stopping = 5: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 17.2%, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 9 \u044d\u043f\u043e\u0445\u0435.\n    \n    \u041e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c\u0441\u044f \u043d\u0430 Learning rate = 7e-4\n\n35. <span style=\"color: green\">\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u043c train-\u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0434\u043e 80 000, \u043d\u0430 validation (cv_data) \u043e\u0441\u0442\u0430\u0432\u0438\u043c 10 000 \u0438 \u043d\u0430 test (val_data) \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0430\u043a\u0436\u0435 10 000 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a.<\/span> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**18.27%!**<\/span>, early stopping \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043d\u0430 9 \u044d\u043f\u043e\u0445\u0435.\n\n36. <span style=\"color: green\">\u041f\u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0432 \u043a\u043e\u043d\u0446\u0435 \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u0441\u043b\u043e\u0439. <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,64) - ReLU - Maxpool(4,4) - fc.Linear(14400, 4096) - ReLU - fc.Linear(4096, 200)\"<\/u> \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e, \u0447\u0442\u043e \u043f\u043e\u0441\u043b\u0435 \u0441\u0432\u0451\u0440\u0442\u043a\u0438 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u043c \u0441\u043b\u043e\u0435 \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0441\u043b\u043e\u0439 \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043a\u0430\u043a \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438<\/span> (\u0442.\u043a. \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438\u043c\u0435\u044e\u0442 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c 64\u044564, \u0442\u043e 4096 \u043d\u0430 \u0432\u0445\u043e\u0434 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u0441\u043b\u043e\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u0443\u043c\u043d\u043e\u0439 \u0438\u0434\u0435\u0435\u0439). \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**22.19%!**<\/span> - \u044d\u0442\u043e \u043f\u0440\u043e\u0440\u044b\u0432!\n\n37. \u041e\u043f\u0438\u0440\u0430\u044f\u0441\u044c \u043d\u0430 \u0441\u0442\u0430\u0442\u044c\u044e [\"Jason Ting (Stanford University) \"Using Convolutional Neural Network for the Tiny ImageNet Challenge\"](http:\/\/cs231n.stanford.edu\/reports\/2016\/pdfs\/425_Report.pdf), \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440 **\u043f\u0440\u0435\u0434\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f - 256** \u0432\u043c\u0435\u0441\u0442\u043e 4096. <u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,64) - ReLU - Maxpool(4,4) - fc1.Linear(14400, 256) - ReLU - fc2.Linear(256, 200)\"<\/u>. \u041f\u043e \u0441\u0443\u0442\u0438, \u044d\u0442\u0430 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u043f\u043e\u0445\u043e\u0436\u0430 \u043d\u0430 \u0442\u0443, \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0430 \u0432 \u0440\u0430\u0431\u043e\u0442\u0435 (IMG\u2192 ConvReLU(F5-16)\u2192 MaxPool\u2192 ConvReLU(F3-16)\u2192 MaxPool\u2192ConvReLU(F3-32)\u2192 MaxPool \u2192 FCReLU(256) \u2192 FC(200)), \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u043c\u0435\u0441\u0442\u043e 3\u0445 \u0441\u043b\u043e\u0435\u0432 Conv-16 - Conv-16 - Conv-32 \u0443 \u043d\u0430\u0441 \u043e\u0434\u0438\u043d \u0441\u043b\u043e\u0439 Conv-64 \u043f\u0440\u0438 \u043f\u043e\u0447\u0442\u0438 \u0442\u043e\u0439 \u0436\u0435 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: 19.67% \u0437\u0430 31 \u044d\u043f\u043e\u0445\u0443, \u0447\u0442\u043e \u0445\u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 36 \u0441\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u043c \u0441\u043b\u043e\u0435\u043c \u043d\u0430 4096 \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432. \u041d\u043e \u0432\u043e\u043e\u0431\u0449\u0435, \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e, \u0447\u0442\u043e \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u0430 \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 \u044d\u0442\u043e\u043c \u0441\u043b\u043e\u0435 \u0441\u0442\u043e\u0438\u043b\u043e \u043d\u0430\u043c ~2.5%\n\n38. \u041e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u0443\u044f\u0441\u044c \u043d\u0430 [\u0442\u0443 \u0436\u0435 \u0441\u0442\u0430\u0442\u044c\u044e](http:\/\/cs231n.stanford.edu\/reports\/2016\/pdfs\/425_Report.pdf) <span style=\"color: green\">\u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0435\u0449\u0451 \u043e\u0434\u0438\u043d Conv2(128) \u0441\u043b\u043e\u0439 \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 (128) + maxpooling \u0434\u043b\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a, \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0441\u043b\u043e\u0435 Conv1(64).<u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,64) - ReLU - Maxpool(4,4) - Conv2(64,128) - ReLU - Maxpool(2,2) - fc1.Linear(4608, 1024) - ReLU - fc2.Linear(1024, 200)\"<\/u><\/span> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: <span style=\"color: red\">**25.19%!**, \u043f\u0440\u0438 \u0432\u0442\u043e\u0440\u043e\u043c \u0437\u0430\u043f\u0443\u0441\u043a\u0435 - 25.34%<\/span>(\u0442.\u0435. \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e \u0438 \u0431\u043e\u043b\u044c\u0448\u0435 25%) \u042d\u0442\u043e \u043f\u0440\u043e\u0440\u044b\u0432!\n\n39. \u041e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u0443\u044f\u0441\u044c \u043d\u0430 [\u0442\u0443 \u0436\u0435 \u0441\u0442\u0430\u0442\u044c\u044e](http:\/\/cs231n.stanford.edu\/reports\/2016\/pdfs\/425_Report.pdf) \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0435\u0449\u0451 \u043e\u0434\u0438\u043d Conv0(32) \u0441\u043b\u043e\u0439 \u0441 \u043c\u0435\u043d\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 (32) + maxpooling \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u043e\u043b\u0435\u0435 \u043c\u0435\u043b\u043a\u0438\u0445 \u0434\u0435\u0442\u0430\u043b\u0435\u0439.<u>\"\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430: Conv1(3,32) - ReLU - Maxpool(2,2) - Conv1(32,64) - ReLU - Maxpool(4,4) - Conv2(64,128) - ReLU - Maxpool(2,2) - fc1.Linear(512, 256) - ReLU - fc2.Linear(256, 200)\"<\/u> \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442: **22.84%** \u0437\u0430 35 \u044d\u043f\u043e\u0445 - \u044d\u0442\u043e \u044f\u0432\u043d\u043e\u0435 \u0443\u0445\u0443\u0434\u0448\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0435\u0442\u044c\u044e \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 \u211638.","3aac5b15":"\u0423\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f **25.34% accuracy \u043f\u0440\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439 \u0441\u0435\u0442\u0438 \u0432 2 Conv-\u0441\u043b\u043e\u044f**. \u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u044b\u0439 \u0432\u043a\u043b\u0430\u0434 \u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u043e:\n* \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 Conv-\u0441\u043b\u043e\u044f\u0445 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0451\u0432. \u0412 \u043d\u0438\u0436\u043d\u0438\u0445 \u0441\u043b\u043e\u044f\u0445 \u043b\u0443\u0447\u0448\u0435 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0433\u0430\u0442\u044c \u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u0434\u0446\u0435\u043f\u043b\u044f\u043b\u0438\u0441\u044c \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438, \u0432 \u0432\u0435\u0440\u0445\u043d\u0438\u0445 \u0441\u043b\u043e\u044f\u0445 \u043b\u0443\u0447\u0448\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0435\u0439 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0433\u0435\u043d\u0435\u0440\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e \u0441\u043b\u043e\u044f.\n* \u0417\u0430\u043c\u0435\u043d\u0430 Softmax-\u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043d\u0430 \u043e\u0431\u044b\u0447\u043d\u044b\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u0441\u043b\u043e\u0439.\n* \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u0432 \u043a\u043e\u043d\u0446\u0435. \u041e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u0441\u043b\u043e\u0439 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 - \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\u0438 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u044f \u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u043c. \u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0435\u0441\u043b\u0438 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043e\u0434\u0438\u043d \u0441\u043b\u043e\u0439, \u0442\u043e \u043c\u044b \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u043c \u0437\u0430 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438, \u0447\u0442\u043e \u043d\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e.\n* \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0441\u043b\u043e\u044f\u0445. \u041e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 4096 \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c \u0441\u043b\u043e\u0435, \u0447\u0435\u043c 256, \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u044d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u043c\u0443 \u0441\u043b\u043e\u044e \u043f\u0440\u043e\u0449\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c 14400 \u0434\u043e 4096 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u0447\u0435\u043c \u0434\u043e 256 (\u0441\u043b\u043e\u0436\u043d\u0435\u0435 \u043e\u0431\u043e\u0431\u0449\u0430\u0442\u044c), \u043b\u0438\u0431\u043e \u044d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043a\u0430\u043a-\u0442\u043e \u0441 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0442.\u043a. 64 \u0445 64 = 4096.\n* Learning rate \u0442\u0430\u043a\u0436\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u0432\u043b\u0438\u044f\u043b, \u0438\u0434\u0435\u0430\u043b\u044c\u043d\u044b\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u043c \u043e\u043a\u0430\u0437\u0430\u043b\u0441\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 7e-4.\n\n\u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f, dropout, batch normalization \u043d\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u0438 \u043a\u0430\u043a-\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c.","7032cc07":"# Pytorch CNN \u043d\u0430 GPU","c6062b59":"### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","44c0f648":"### \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432\n\n(\u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u044d\u0442\u0430\u043f\u044b \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u044b <span style=\"color: green\">\u0437\u0435\u043b\u0451\u043d\u044b\u043c<\/span> \u0438 <span style=\"color: red\">\u043a\u0440\u0430\u0441\u043d\u044b\u043c<\/span>)","a49e085c":"### \u0412\u044b\u0432\u043e\u0434\u044b","a868de83":"# \u041e\u0442\u0447\u0451\u0442","5f6c3cb1":"### \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043d\u0430 train(train_data), validation (cv_data) \u0438 test(val_data)"}}