{"cell_type":{"644d87aa":"code","03040e60":"code","5a888ec2":"code","958e63c5":"code","00f1ddc2":"code","9fbba045":"code","616f130c":"code","107d51ca":"code","e5e5c5d6":"code","0752009d":"code","e2e285e1":"code","fb5c7f48":"code","0dac3264":"code","4e121380":"code","8efe9fb0":"code","d0c22463":"code","61d67578":"code","8c52741e":"code","13199e35":"code","f3429f5e":"code","03055943":"code","2669519e":"code","aa94cdbb":"code","0051d6cd":"code","6969b994":"code","a9134572":"code","c785276f":"code","c8374477":"code","187d3b3b":"markdown","9006c4d8":"markdown","f041d420":"markdown","0ffc60fe":"markdown","dcab90dc":"markdown","3ecaef01":"markdown","7be3e2ea":"markdown","45f05690":"markdown"},"source":{"644d87aa":"!pip install pytorch-lightning","03040e60":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport string\nimport re\nimport bs4\nfrom bs4 import BeautifulSoup\n\nimport transformers\nfrom transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n\nSEED = 1234\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nprint(\"PyTorch version: \", torch.__version__)\nprint('Huggingface version: ', transformers.__version__)\nprint('PyTorch Lightning version: ', pl.__version__)","5a888ec2":"PATH = '..\/input\/tweet-sentiment-extraction\/'\n\ntrain_df = pd.read_csv(PATH + 'train.csv')\ntrain_df.head()","958e63c5":"test_df = pd.read_csv(PATH + 'test.csv')\ntest_df.head()","00f1ddc2":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    return BeautifulSoup(text, \"lxml\").text","9fbba045":"train_df['text'] = train_df['text'].apply(lambda x: str(x))\ntest_df['text'] = test_df['text'].apply(lambda x: str(x))\n\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_urls(text))\ntest_df['text'] = test_df['text'].apply(lambda text: remove_urls(text))\n\ntrain_df['text'] = train_df['text'].apply(lambda text: remove_html(text))\ntest_df['text'] = test_df['text'].apply(lambda text: remove_html(text))\n\ntrain_df['text_lower'] = train_df['text'].apply(lambda x: x.lower())\ntest_df['text_lower'] = test_df['text'].apply(lambda x: x.lower())\n\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x: str(x).lower())","616f130c":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n\ntrain_df['text_tokenized'] = train_df['text_lower'].apply(tokenizer.tokenize)\ntest_df['text_tokenized'] = test_df['text_lower'].apply(tokenizer.tokenize)\n\ntrain_df['selected_text_tokenized'] = train_df['selected_text'].apply(tokenizer.tokenize)","107d51ca":"# Start and end positions\n\nstart_positions = []\nend_positions = []\n\ntrain_df['select_length'] = train_df['selected_text_tokenized'].map(len)\n\nfor i in range(len(train_df)):\n    start_position = [j for j, token in enumerate(train_df['text_tokenized'].iloc[i]) if token == train_df['selected_text_tokenized'].iloc[i][0]]\n    end_position = [j for j, token in enumerate(train_df['text_tokenized'].iloc[i]) if token == train_df['selected_text_tokenized'].iloc[i][-1]]\n    \n    start_position = [idx for idx in start_position if idx + train_df['select_length'].iloc[i] - 1 in end_position]\n    end_position = [idx for idx in end_position if idx - train_df['select_length'].iloc[i] + 1 in start_position]\n    \n    start_positions.append(start_position)\n    end_positions.append(end_position)","e5e5c5d6":"start_positions = [l[0] if len(l) > 0 else -1 for l in start_positions]\nend_positions = [l[0] if len(l) > 0 else -1 for l in end_positions]","0752009d":"train_df['start_position'] = start_positions\ntrain_df['end_position'] = end_positions\n\ntest_df['start_position'] = -1\ntest_df['end_position'] = -1\n\ntrain_df = train_df.query('start_position!=-1')","e2e285e1":"train_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = SEED)","fb5c7f48":"pos_train = train_df.query('sentiment==\"positive\"')\nneg_train = train_df.query('sentiment==\"negative\"')\nneu_train = train_df.query('sentiment==\"neutral\"')\n\npos_val = val_df.query('sentiment==\"positive\"')\nneg_val = val_df.query('sentiment==\"negative\"')\nneu_val = val_df.query('sentiment==\"neutral\"')\n\npos_test = test_df.query('sentiment==\"positive\"')\nneg_test = test_df.query('sentiment==\"negative\"')\nneu_test = test_df.query('sentiment==\"neutral\"')","0dac3264":"pos_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\nneg_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nMAX_LENGTH = 128\nBATCH_SIZE = 32","4e121380":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['text_lower'].values\n        self.start_ids = df['start_position'].values\n        self.end_ids = df['end_position'].values\n        self.hash_index = df['textID'].values\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        returns = {'text': self.texts[idx],\n                   'start': self.start_ids[idx],\n                   'end': self.end_ids[idx],\n                   'idx': idx}\n        return returns\n    \nclass TestDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['text_lower'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {'text' : self.texts[idx],\n                   'idx' : idx}\n        return returns","8efe9fb0":"ds_pos_train = TrainDataset(pos_train)\nds_neg_train = TrainDataset(neg_train)\n\nds_pos_val = TrainDataset(pos_val)\nds_neg_val = TrainDataset(neg_val)\n\nds_pos_test = TestDataset(pos_test)\nds_neg_test = TestDataset(neg_test)\n\n\n\ndl_pos_train = DataLoader(ds_pos_train, \n                          batch_size = BATCH_SIZE, \n                          shuffle = True,\n                          num_workers = 8)\ndl_neg_train = DataLoader(ds_neg_train, \n                          batch_size = BATCH_SIZE, \n                          shuffle = True,\n                          num_workers = 8)\n\ndl_pos_val = DataLoader(ds_pos_val, \n                        batch_size = BATCH_SIZE, \n                        shuffle = False,\n                        num_workers = 8)\ndl_neg_val = DataLoader(ds_neg_val, \n                        batch_size = BATCH_SIZE, \n                        shuffle = False,\n                        num_workers = 8)\n\ndl_pos_test = DataLoader(ds_pos_test, \n                         batch_size = BATCH_SIZE, \n                         shuffle = False,\n                         num_workers = 8)\ndl_neg_test = DataLoader(ds_neg_test, \n                         batch_size = BATCH_SIZE, \n                         shuffle = False,\n                         num_workers = 8)","d0c22463":"class DistilBERTModule(pl.LightningModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__()\n        self.distilbertmodel = distilbertmodel\n        self.tokenizer = tokenizer\n        self.prediction_save_path = prediction_save_path\n        \n    def get_device(self):\n        return self.distilbertmodel.state_dict()['distilbert.embeddings.word_embeddings.weight'].device\n    \n    def save_predictions(self, start_posit, end_posit):\n        df = pd.DataFrame({'start_position': start_posit,\n                           'end_position': end_posit})\n        df.to_csv(self.prediction_save_path, index = False)\n        \n    def forward(self, batch):\n        encoded_batch = tokenizer.batch_encode_plus(batch['text'],\n                                                    max_length = MAX_LENGTH,\n                                                    pad_to_max_length = True)\n        input_ids = torch.tensor(encoded_batch['input_ids']).to(self.get_device())\n        attention_mask = torch.tensor(encoded_batch['attention_mask']).to(self.get_device())\n        start_posit = batch['start'].to(self.get_device()) + 1 if 'start' in batch.keys() else None\n        end_posit = batch['end'].to(self.get_device()) + 1 if 'end' in batch.keys() else None\n        \n        model_inputs = {'input_ids': input_ids,\n                        'attention_mask': attention_mask,\n                        'start_positions': start_posit,\n                        'end_positions': end_posit}\n        \n        return self.distilbertmodel(**model_inputs)\n    \n    def training_step(self, batch, batch_nb):\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss': loss, 'idx': idx}\n    \n    def validation_step(self, batch, batch_nb):\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss': loss, 'idx': idx}\n    \n    def test_step(self, batch, batch_nb):\n        idx = batch['idx']\n        start_scores = self.forward(batch)[0]\n        end_scores = self.forward(batch)[1]\n        return {'start_scores':start_scores, 'end_scores':end_scores, 'idx':idx}\n    \n    def training_end(self, outputs):\n        return {'loss':outputs['loss']}\n    \n    def validation_end(self, outputs):\n        return {'loss':torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n    \n    def test_end(self, outputs):\n        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n        start_positions = np.argmax(start_scores, axis=1) - 1\n\n        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n        end_positions = np.argmax(end_scores, axis=1) - 1\n        self.save_predictions(start_positions, end_positions)\n        return {}\n    \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr = 2e-5)\n    \n    @pl.data_loader\n    def train_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def val_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def test_dataloader(self):\n        pass","61d67578":"class PositiveModule(DistilBERTModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__(distilbertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_pos_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_pos_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_pos_test\n    \n    \n\nclass NegativeModule(DistilBERTModule):\n    def __init__(self, distilbertmodel, tokenizer, prediction_save_path):\n        super().__init__(distilbertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_neg_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_neg_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_neg_test","8c52741e":"pos_module = PositiveModule(pos_model, tokenizer, 'pos_pred.csv')\nneg_module = NegativeModule(neg_model, tokenizer, 'neg_pred.csv')\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n\npos_module.to(device)\nneg_module.to(device)","13199e35":"pos_trainer = pl.Trainer(max_nb_epochs = 5, fast_dev_run = False)\nneg_trainer = pl.Trainer(max_nb_epochs = 5, fast_dev_run = False)\n\npos_trainer.fit(pos_module)\nneg_trainer.fit(neg_module)","f3429f5e":"pos_trainer.test()\nneg_trainer.test()","03055943":"pos_pred = pd.read_csv('pos_pred.csv')\nneg_pred = pd.read_csv('neg_pred.csv')\npos_pred.head()","2669519e":"neg_pred.head()","aa94cdbb":"test_df.index = test_df['textID']\ntest_df['selected_text'] = ''\n\ntest_df.loc[ds_pos_test.hash_index[:BATCH_SIZE if False else len(test_df)], 'start_position':'end_position'] = pos_pred.values\ntest_df.loc[ds_neg_test.hash_index[:BATCH_SIZE if False else len(test_df)], 'start_position':'end_position'] = neg_pred.values\ntest_df.head()","0051d6cd":"test_df.tail()","6969b994":"for i in range(len(test_df)):\n    if test_df['sentiment'].iloc[i] in ('positive', 'negative'):\n        tokenized_text = test_df['text_tokenized'].iloc[i]\n        start_position = max(test_df['start_position'].iloc[i], 0)\n        end_position = min(test_df['end_position'].iloc[i], len(tokenized_text) - 1)\n        \n        selected_text = tokenizer.convert_tokens_to_string(tokenized_text[start_position:end_position + 1])\n        for original_token in test_df['text'].iloc[i].split():\n            tokenized_form = tokenizer.convert_tokens_to_string(tokenizer.tokenize(original_token))\n            selected_text = selected_text.replace(tokenized_form, original_token, 1)\n            \n        test_df['selected_text'].iloc[i] = selected_text","a9134572":"for i in range(len(test_df)):\n    if test_df['sentiment'].iloc[i] == 'neutral':\n        test_df['selected_text'].iloc[i] = test_df['text'].iloc[i]\n    else:\n        pass","c785276f":"test_df.loc[:,['textID', 'selected_text']]","c8374477":"test_df.loc[:, ['textID', 'selected_text']].to_csv('submission.csv', index = False)","187d3b3b":"Helper functions for text preprocessing","9006c4d8":"[Reference](https:\/\/www.kaggle.com\/yutanakamura\/dear-pytorch-lovers-bert-transformers-lightning#3.-Postprocessing)","f041d420":"## Postprocessing","0ffc60fe":"### Positive\/Negative\/Neutral Split","dcab90dc":"## DistilBERT","3ecaef01":"# Data Preprocessing\n\nRead in the data","7be3e2ea":"## Tokenize","45f05690":"## Split the data"}}