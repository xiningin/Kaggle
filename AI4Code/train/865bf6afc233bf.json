{"cell_type":{"2e382f99":"code","4e0e9929":"code","93d70b71":"code","de27931d":"code","f4efe019":"code","d3378ae7":"code","e55ffdc6":"code","25a9c9fe":"code","d9418a3e":"code","e9f9499d":"code","8504a0ee":"code","3f3fb084":"code","b4fff0a5":"code","9f570d9b":"code","8ff73e69":"code","ed43c44f":"code","de21968c":"code","561d00d6":"code","16e73b09":"code","6c5b61af":"code","f628cb86":"code","e7be1b8d":"code","6dd15b5b":"code","c989f1f9":"code","487bfe20":"code","eb168536":"code","0d7695cd":"code","f891d0ad":"code","b7b0cdd9":"code","e102d4d0":"code","825e766f":"code","57484a3f":"code","ccabadc2":"code","33fec5fe":"code","37531166":"code","2b90349b":"code","516bf1a7":"code","57a78bc4":"code","edc67298":"code","5acaccf9":"code","52724bfe":"code","98074b4c":"code","86d5ddec":"code","0b9c0a53":"code","b6ffc675":"code","363c92d0":"code","135f1e35":"code","7b3fcb2e":"code","ca369359":"code","5ed00712":"code","d9b968db":"code","cec8e98f":"code","add69223":"code","b35eb8f6":"code","11d39739":"code","70d9c436":"code","a080d4fe":"code","7533f7a1":"code","c3c1839b":"markdown","5af7c952":"markdown","21af144e":"markdown","1f7cee43":"markdown","2111fc68":"markdown","468d1114":"markdown","e5363f90":"markdown","caed5451":"markdown","09d0555c":"markdown","9a0487d2":"markdown","68a1a68e":"markdown","0093fe3c":"markdown","fc99873d":"markdown","ca51d158":"markdown","fb44315c":"markdown","0388a1e3":"markdown","19f70b43":"markdown","f7e7fe95":"markdown","7e09cf96":"markdown","e8e386d2":"markdown","4a5bf5a7":"markdown","d5cd1c5a":"markdown","9d963848":"markdown","0f0b9db8":"markdown","f6d5d372":"markdown","e9c39472":"markdown","2c56894b":"markdown","c2fc39f6":"markdown","c502e201":"markdown","f9062839":"markdown","9dfcd441":"markdown","aa17e161":"markdown","179228ca":"markdown","676228ea":"markdown","1d1f6a75":"markdown","69f33dcf":"markdown","a32fb726":"markdown","5df42a39":"markdown","16d8d4b6":"markdown","c09185e9":"markdown","0281a51c":"markdown","0600b11b":"markdown","7cc4ef06":"markdown","50e014ce":"markdown","92c63bdb":"markdown","b79d6e76":"markdown","6f1d056e":"markdown","a9ea9b7c":"markdown","a890d784":"markdown","14a67cf6":"markdown","ca4d5e8a":"markdown","4ea14adb":"markdown","a924153b":"markdown","2129c04f":"markdown","e39d9064":"markdown","18047e3b":"markdown","46bcdff6":"markdown","b8e893f5":"markdown","bfa13863":"markdown","5cfebc27":"markdown","c7acfbbe":"markdown","8d3d9c2e":"markdown","4c8f9525":"markdown","6a3a9091":"markdown","45b9b6f1":"markdown","10d889a5":"markdown","f898ed45":"markdown","68abde05":"markdown","b1d582e5":"markdown","a3c04ca9":"markdown","ac15a63f":"markdown","e7f65680":"markdown","2501d960":"markdown","ace6ea94":"markdown","e337695e":"markdown","94177651":"markdown","84e2dca7":"markdown","31bcde94":"markdown","eeeb1706":"markdown","2e12634a":"markdown","1ad17b59":"markdown","fea37a15":"markdown","0ee61dcf":"markdown","826f24d9":"markdown","6d587090":"markdown","0f07d633":"markdown","90254d6c":"markdown","231ccdba":"markdown","9d5ea87d":"markdown","0eb786ef":"markdown","0e451828":"markdown","cea8892c":"markdown","c7de136f":"markdown","6ddd60b6":"markdown","a6d8451a":"markdown","108df74c":"markdown","c6fb6b7f":"markdown","ccc669c2":"markdown"},"source":{"2e382f99":"import tensorflow as tf\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import *\nfrom keras.layers import *\nfrom keras.models import Sequential,Model\nimport kerastuner as kt\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom transformers import AutoConfig, AutoModel\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport nltk\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer,LancasterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n!pip install chart_studio\nfrom IPython.display import HTML\nimport plotly\nimport cufflinks\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\n\nplotly.offline.init_notebook_mode(connected=True)\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nfrom string import punctuation\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')","4e0e9929":"data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","93d70b71":"data.head(10)","de27931d":"data.info()","f4efe019":"data = data[['text','target']]\ndata.head()","d3378ae7":"fig = px.bar(x=[\"0\",\"1\"], y=data[\"target\"].value_counts(),\n             color=[\"red\", \"goldenrod\"])\n\n#Change this value for bar widths\nfor dt in fig.data:\n    dt[\"width\"] = 0.4 \n\nfig.update_layout(\n    title_text = \"Counts for Disaster and Non-Disaster Tweets\",\n    title_x=0.5,\n    width=800,\n    height=550,\n    xaxis_title=\"Targets\",\n    yaxis_title=\"Count\",\n    showlegend=False\n).show()\n\n# py.plot(fig,filename='Counts for Disaster and Non-Disaster Tweets',auto_open=False,show_link=False)","e55ffdc6":"word_len_dis = data[data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nword_len_non_dis = data[data['target']==0]['text'].str.split().map(lambda x : len(x))\n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n    \n    go.Histogram(x=word_len_dis,marker_line=dict(color='black'),\n                 marker_line_width=1.2),row=1, col=1\n    \n).add_trace(\n    \n    go.Histogram(x=word_len_non_dis,marker_line=dict(color='black'),\n                 marker_line_width=1.2),row=1, col=2\n    \n).update_layout(title_text=\"Length of words in Tweets\",\n                title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Length of words in Tweets',auto_open=False,show_link=False)","25a9c9fe":"def avgwordlen(strlist):\n    sum=[]\n    for i in strlist:\n        sum.append(len(i))\n    return sum\n\nnon_dis_data = data[data['target']==0]['text'].str.split()\ndis_data = data[data['target']==1]['text'].str.split()\n\navgword_len_dis = dis_data.apply(avgwordlen).map(lambda x: np.mean(x))\navgword_len_non_dis = non_dis_data.apply(avgwordlen).map(lambda x: np.mean(x))\n\ngroup_labels = ['Disaster', 'Non-Disaster']\ncolors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n\nfig = ff.create_distplot([avgword_len_dis, avgword_len_non_dis], \n                         group_labels, bin_size=.2, colors=colors,)\n\nfig.update_layout(title_text=\"Average word length in tweets\",title_x=0.5,\n                  xaxis_title=\"Text\",yaxis_title=\"Density\").show()\n\n# py.plot(fig,filename='Average word length in tweets',auto_open=False,show_link=False)","d9418a3e":"def create_corpus(target):\n    corpus = []\n    for i in data[data['target']==target]['text'].str.split():\n        for x in i:\n            corpus.append(x)\n    return corpus","e9f9499d":"values_list = []\n\ndef analyze_stopwords(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        dic[word] += 1\n    \n    top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n    x_items,y_values = zip(*top)\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing stopwords for 0 and 1 target labels\nanalyze_stopwords(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n\nfig.add_trace(\n      go.Bar(x=values_list[1],y=values_list[0],orientation='h',marker=dict(color= 'rgba(152, 255, 74,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[3],y=values_list[2],orientation='h',marker=dict(color= 'rgba(255, 143, 92,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top stop words in the text\",title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top stop words in the text',auto_open=False,show_link=False)","8504a0ee":"values_list = []\n\ndef analyze_punctuations(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in punctuation:\n            dic[word] += 1 \n    \n    x_items, y_values = zip(*dic.items())\n    \n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing punctuations for 0 and 1 target labels\nanalyze_punctuations(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n  \nfig.add_trace(\n    \n    go.Bar(x=values_list[0],y=values_list[1],\n           marker=dict(color= 'rgba(196, 94, 255,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=1, col=1\n    \n).add_trace(\n    \n    go.Bar(x=values_list[2],y=values_list[3],\n          marker=dict(color= 'rgba(255, 163, 102,0.8)'),\n          marker_line=dict(color='black'),marker_line_width=1.2),\n          row=1, col=2\n    \n).update_layout(title_text=\"Top Punctuations in the text\",\n                title_x=0.5,showlegend=False).show()\n\n# py.plot(fig,filename='Top Punctuations in the text',auto_open=False,show_link=False)","3f3fb084":"lemmatizer = WordNetLemmatizer()\n# stemmer = LancasterStemmer()\n\ndef preprocess_data(data):\n    \n    #removal of url\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+|http?:\/\/\\S+',' ',data) \n    \n    #decontraction\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    \n    #removal of html tags\n    text = re.sub(r'<.*?>',' ',text) \n    \n    # Match all digits in the string and replace them by empty string\n    text = re.sub(r'[0-9]', '', text)\n    text = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',text)\n    \n    # filtering out miscellaneous text.\n    text = re.sub('[^a-zA-Z]',' ',text) \n    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n    \n    # remove mentions\n    text = re.sub('@\\S+', '', text)  \n    \n    # remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~\"\"\"), '', text)  \n    \n\n    # Lowering all the words in text\n    text = text.lower()\n    text = text.split()\n    \n    text = [lemmatizer.lemmatize(words) for words in text if words not in stopwords.words('english')]\n    \n    # Removal of words with length<2\n    text = [i for i in text if len(i)>=2] \n    text = ' '.join(text)\n    return text\n\ndata[\"Cleaned_text\"] = data[\"text\"].apply(preprocess_data)","b4fff0a5":"data.head()","9f570d9b":"def wordcloud(data,title):\n    words = ' '.join(data['Cleaned_text'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords,width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n\ndata_disaster = data[data['target'] == 1]\ndata_non_disaster = data[data['target'] == 0]","8ff73e69":"wordcloud(data_disaster,\"Disaster Tweets\")","ed43c44f":"wordcloud(data_non_disaster,\"Non-Disaster Tweets\")","de21968c":"common_words = ['via','like','build','get','would','one','two','feel',\n                'lol','fuck','take','way','may','first','latest','want',\n                'make','back','see','know','let','look','come','got',\n                'still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ndata[\"Cleaned_text\"] = data[\"Cleaned_text\"].apply(text_cleaning)","561d00d6":"def top_ngrams(data,n,grams):\n    count_vec = CountVectorizer(ngram_range=(grams,grams)).fit(data)\n    bow = count_vec.transform(data)\n    add_words = bow.sum(axis=0)\n    word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True) \n    return word_freq[:n]","16e73b09":"common_uni = top_ngrams(data[\"Cleaned_text\"],10,1)\ncommon_bi = top_ngrams(data[\"Cleaned_text\"],10,2)\ncommon_tri = top_ngrams(data[\"Cleaned_text\"],10,3)\ncommon_uni_df = pd.DataFrame(common_uni,columns=['word','freq'])\ncommon_bi_df = pd.DataFrame(common_bi,columns=['word','freq'])\ncommon_tri_df = pd.DataFrame(common_tri,columns=['word','freq'])","6c5b61af":"fig = make_subplots(rows=3, cols=1,subplot_titles=(\"Top 20 Unigrams in Text\",\n                    \"Top 20 Bigrams in Text\",\"Top 20 Trigrams in Text\"))\n  \nfig.add_trace(\n    \n    go.Bar(x=common_uni_df[\"word\"],y=common_uni_df[\"freq\"],\n           marker=dict(color= 'rgba(255, 170, 59,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=1, col=1\n\n).add_trace(\n    \n    go.Bar(x=common_bi_df[\"word\"],y=common_bi_df[\"freq\"],\n           marker=dict(color= 'rgba(89, 255, 147,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=2, col=1\n\n).add_trace(\n    \n    go.Bar(x=common_tri_df[\"word\"],y=common_tri_df[\"freq\"],\n           marker=dict(color= 'rgba(89, 153, 255,0.8)'),\n           marker_line=dict(color='black'),marker_line_width=1.2),\n           row=3, col=1\n    \n).update_layout(title_text=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",\n                title_x=0.5,showlegend=False,\n                width=800,height=1600,).update_xaxes(tickangle=-90).show()\n\n# py.plot(fig,filename='Visualization of Top 20 Unigrams, Bigrams and Trigrams',auto_open=False,show_link=False)","f628cb86":"#original data after cleaning \nX_inp_clean = data['Cleaned_text']\nX_inp_original = data['text']\ny_inp = data['target']","e7be1b8d":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(X_inp_clean.values)\nvocab_length = len(word_tokenizer.word_index) + 1","6dd15b5b":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(X_inp_clean.values, key=lambda sentence: len(word_tokenize(sentence)))\n\nlength_long_sentence = len(word_tokenize(longest_train))\n\npadded_sentences = pad_sequences(embed(X_inp_clean.values), \n                                 length_long_sentence, padding='post')","c989f1f9":"embeddings_dictionary = dict()\n\nembedding_dim = 100\n\nglove_file = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n\nglove_file.close()","487bfe20":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    \n    embedding_vector = embeddings_dictionary.get(word)\n    \n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","eb168536":"X_train, X_val, y_train, y_val = train_test_split(padded_sentences,\n                                                  y_inp.values,test_size=0.2,random_state=1)","0d7695cd":"def model_history(model_history):\n    fig,(ax1,ax2) =  plt.subplots(1,2,figsize=(12,5))\n    \n    # summarize history for accuracy\n    ax1.plot(model_history.history['accuracy'])\n    ax1.plot(model_history.history['val_accuracy'])\n    ax1.set_title('model accuracy')\n    ax1.set_ylabel('accuracy')\n    ax1.set_xlabel('epoch')\n    ax1.legend(['train', 'test'], loc='upper right')\n\n    # summarize history for loss\n    ax2.plot(model_history.history['loss'])\n    ax2.plot(model_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper right')\n    \n    fig.suptitle(\"Model History\")\n    ","f891d0ad":"def CNN(hp):\n    \n    model = keras.Sequential()\n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5])\n    \n    model.add(Embedding(vocab_length, 100, weights=[embedding_matrix],\n                                     input_length=length_long_sentence,trainable=False))\n    \n    model.add(Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=200,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5]),\n                                activation='relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(MaxPooling1D(pool_size=2))\n  \n    model.add(Flatten())\n\n    model.add(Dense(units = hp.Int('dense_1',min_value=21,max_value=150,step=14),\n                                   activation='relu'))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1,activation='sigmoid'))\n  \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","b7b0cdd9":"tuner_CNN = kt.Hyperband(CNN,objective='val_accuracy',\n                         max_epochs=15,factor=5,\n                         directory='my_dir',\n                         project_name='DisasterTweets_kt',\n                         overwrite=True)","e102d4d0":"stop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=10)\n\ntuner_CNN.search(X_train, y_train, epochs=15,\n                 validation_data=(X_val,y_val),callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_CNN=tuner_CNN.get_best_hyperparameters(num_trials=1)[0]","825e766f":"model_CNN = tuner_CNN.hypermodel.build(best_hps_CNN)\n\ncheckpoint = ModelCheckpoint(\n    'model_CNN.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_CNN = model_CNN.fit(X_train, y_train,epochs=50,\n                            validation_data=(X_val,y_val),\n                            callbacks=[checkpoint,stop_early])","57484a3f":"model_CNN.summary()","ccabadc2":"model_history(history_CNN)","33fec5fe":"def MultichannelCNN(hp): \n    \n    inputs1 = Input(shape=(length_long_sentence,))\n    \n    embedding1 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_sentence, trainable=False)(inputs1) \n    \n    conv1 = Conv1D(filters=hp.Int('conv_1_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_1_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding1) \n    \n    drop1 = Dropout(0.3)(conv1) \n    \n    pool1 = MaxPooling1D()(drop1) \n    \n    flat1 = Flatten()(pool1)\n    \n    inputs2 = Input(shape=(length_long_sentence,)) \n    \n    embedding2 = Embedding(vocab_length, 100, weights=[embedding_matrix],\n                           input_length=length_long_sentence,trainable=False)(inputs2) \n    \n    conv2 = Conv1D(filters=hp.Int('conv_2_filter',min_value=21,max_value=150,step=14),\n                                kernel_size=hp.Choice('conv_2_kernel',values=[3,4,5,6,7,8]),\n                                activation='relu')(embedding2) \n    \n    drop2 = Dropout(0.3)(conv2) \n    \n    pool2 = MaxPooling1D()(drop2) \n    \n    flat2 = Flatten()(pool2) \n    \n    # merge \n    merged = concatenate([flat1, flat2]) \n    \n    dense1 = Dense(units = hp.Int('dense_1',min_value=21,max_value=120,step=14),\n                               activation='relu')(merged)\n    \n    drop4 = Dropout(0.5)(dense1)\n    \n    outputs = Dense(1, activation='sigmoid')(drop4) \n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs) \n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[3e-2, 3e-3, 3e-4, 3e-5]) \n    \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","37531166":"tuner_MCNN = kt.Hyperband(MultichannelCNN,objective='val_accuracy',\n                          max_epochs=15,factor=5,\n                          directory='my_dir',\n                          project_name='DisasterTweetsMCNN_kt',\n                          overwrite=True)\n\nstop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=10)\n\ntuner_MCNN.search([X_train,X_train], y_train, epochs=15,\n                  validation_data=([X_val,X_val], y_val),\n                  callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_MCNN=tuner_MCNN.get_best_hyperparameters(num_trials=1)[0]","2b90349b":"model_MCNN = tuner_MCNN.hypermodel.build(best_hps_MCNN)\n\ncheckpoint = ModelCheckpoint(\n    'model_MCNN.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_MCNN = model_MCNN.fit([X_train,X_train], y_train,epochs=50,\n                              validation_data=([X_val,X_val], y_val),\n                              callbacks=[checkpoint,stop_early])","516bf1a7":"model_MCNN.summary()","57a78bc4":"model_history(history_MCNN)","edc67298":"def BiLSTM(hp):\n    model = Sequential()\n    \n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=length_long_sentence,trainable = False))\n    \n    model.add(Bidirectional(CuDNNLSTM(units = hp.Int('dense_1',\n                                      min_value=21,max_value=120,step=14)\n                                      ,return_sequences = True)))\n    \n    model.add(GlobalMaxPool1D())\n   \n    model.add(BatchNormalization())\n    \n    model.add(Dropout(0.2))\n    \n    model.add(Dense(units = hp.Int('dense_1',min_value=21,\n                                   max_value=120,step=14),\n                    activation = \"relu\"))\n    \n    model.add(Dropout(0.3))\n    \n    model.add(Dense(units = hp.Int('dense_1',min_value=21,\n                                   max_value=100,step=14),\n                    activation = \"relu\"))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    hp_learning_rate = hp.Choice('learning_rate', \n                                 values=[3e-2, 3e-3, 3e-4, 3e-5]) \n    \n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n  \n    return model","5acaccf9":"tuner_BiLSTM = kt.Hyperband(BiLSTM,objective='val_accuracy',\n                     max_epochs=15,factor=5,\n                     directory='my_dir',\n                     project_name='DisasterTweetsBiLSTM_kt',\n                     overwrite=True)\n\nstop_early = EarlyStopping(monitor='val_loss', mode='min',\n                           verbose=1, patience=12)\n\ntuner_BiLSTM.search(X_train, y_train, epochs=15,\n                    validation_data=(X_val, y_val),\n                    callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps_BiLSTM=tuner_BiLSTM.get_best_hyperparameters(num_trials=1)[0]","52724bfe":"model_BiLSTM = tuner_BiLSTM.hypermodel.build(best_hps_BiLSTM)\n\ncheckpoint = ModelCheckpoint(\n    'model_BiLSTM.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nhistory_BiLSTM = model_BiLSTM.fit(X_train, y_train, epochs=50,\n                                  validation_data=(X_val, y_val),\n                                  callbacks=[checkpoint,stop_early])","98074b4c":"model_BiLSTM.summary()","86d5ddec":"model_history(history_BiLSTM)","0b9c0a53":"onehot_encoder = OneHotEncoder(sparse=False)\n\ny = (np.asarray(y_inp)).reshape(-1,1)\n\nY = onehot_encoder.fit_transform(y)\n\nX_train, X_val, y_train, y_val = train_test_split(X_inp_clean,Y,\n                                                  test_size=0.2, random_state=1)","b6ffc675":"model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","363c92d0":"tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")","135f1e35":"def regular_encode(texts, tokenizer, maxlen=512):\n    \n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        add_special_tokens = True,\n        truncation=True\n    )\n    \n    return np.array(enc_di['input_ids'])\n\nX_train_t = regular_encode(list(X_train), tokenizer, maxlen=512)\n\nX_val_t = regular_encode(list(X_val), tokenizer, maxlen=512)","7b3fcb2e":"AUTO = tf.data.experimental.AUTOTUNE\n\nbatch_size = 16\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_t, y_val))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","ca369359":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32,\n                           name=\"input_word_ids\")\n    \n    sequence_output = transformer(input_word_ids)[0]\n    \n    cls_token = sequence_output[:, 0, :]\n    \n    out = Dense(2, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=1e-5),\n                  loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","5ed00712":"transformer_layer = TFAutoModel.from_pretrained(model_checkpoint)\n\nmodel_DistilBert = build_model(transformer_layer)","d9b968db":"model_DistilBert.summary()","cec8e98f":"n_steps = X_train.shape[0] \/\/ batch_size\n\nhistory_DistilBert = model_DistilBert.fit(train_dataset,\n                                          steps_per_epoch=n_steps,\n                                          validation_data=valid_dataset,\n                                          epochs=3)","add69223":"model_history(history_DistilBert)","b35eb8f6":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)\n\ntest[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)\n\ntest_sentences = pad_sequences(embed(test.Cleaned_text.values),\n                               length_long_sentence, padding='post')","11d39739":"predsCNN = model_CNN.predict_classes(test_sentences)\n\npredictions_test = pd.DataFrame(predsCNN)\n\ntest_id = pd.DataFrame(test[\"id\"])\n\nsubmissionCNN = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionCNN.columns = [\"id\",\"target\"]\n\nsubmissionCNN.to_csv(\"submissionCNN.csv\",index=False)","70d9c436":"predsMCNN = model_MCNN.predict([test_sentences,test_sentences])\n\npredsMCNN = (predsMCNN[:,0] > 0.5).astype(np.int)\n\npredictions_test = pd.DataFrame(predsMCNN)\n\nsubmissionMCNN = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionMCNN.columns = [\"id\",\"target\"]\n\nsubmissionMCNN.to_csv(\"submissionMCNN.csv\",index=False)","a080d4fe":"predsBiLSTM = model_BiLSTM.predict(test_sentences)\n\npredsBiLSTM = (predsBiLSTM[:,0] > 0.5).astype(np.int)\n\npredictions_test = pd.DataFrame(predsBiLSTM)\n\nsubmissionBiLSTM = pd.concat([test_id,predictions_test],axis=1)\n\nsubmissionBiLSTM.columns = [\"id\",\"target\"]\n\nsubmissionBiLSTM.to_csv(\"submissionBiLSTM.csv\",index=False)","7533f7a1":"X_test = regular_encode(list(test.Cleaned_text), tokenizer, maxlen=512)\n\ntest1 = (tf.data.Dataset.from_tensor_slices(X_test).batch(batch_size))\n\npred = model_DistilBert.predict(test1,verbose = 0)\n\npred = np.argmax(pred,axis=-1)\n\npred = pred.astype('int32')\n\nres=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv',index_col=None)  \n\nres['target'] = pred\n\nres.to_csv('submissionDistilBert.csv',index=False)","c3c1839b":"Function that will return our model","5af7c952":"### What do you mean by N-grams?  \nN-grams of texts are extensively used in text mining and natural language processing tasks.     They are basically a set of co-occurring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).  \n\nFor example, for the sentence \u201cThe cow jumps over the moon\u201d.    \nIf N=2 (known as bigrams), then the ngrams would be:  \n* the cow \n* cow jumps \n* jumps over \n* over the \n* the moon\n\nBelow we perform [N-grams](https:\/\/en.wikipedia.org\/wiki\/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.) analysis on cleaned data","21af144e":"As observed from the plots the most occuring punctuations in both disaster\/non-disaster tweets is '-' (350+) and '|'    \nwhile the least occuring for non-disaster are '%' , '\/:' , '_' and for disaster tweets is '=>' , ')'.","1f7cee43":"Let's plot the counts of values under the target column","2111fc68":"ploting training and validation history of the model","468d1114":"## 2.4 Visualising most common stop words in the text data","e5363f90":"An example of how tokenizer works","caed5451":"## 5.3 Spliting data into training and validation dataset","09d0555c":"Creating a model for best found hyperparameters and training it","9a0487d2":"ploting model's training and validation history","68a1a68e":"## 6.1 Training and Tuning Convolutional Neural Network","0093fe3c":"### Why Mulichannel CNN?   \n\nA multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels.   \nThis allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n\nFunction for creating [Multichannel CNN](https:\/\/machinelearningmastery.com\/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis\/)","fc99873d":"The plot shows that our data is quite balanced, you can also click on the plot to explore more about [interactive plots](https:\/\/plotly.com\/)","ca51d158":"Reading [data](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data) and choosing important columns using [pandas](https:\/\/pandas.pydata.org\/)","fb44315c":"Ploting model's training and validation history","0388a1e3":"Training the model","19f70b43":"# 4. Extra Data Exploration and Analysis with Cleaned Text","f7e7fe95":"# 1. Dependancies and Dataset","7e09cf96":"# Project Description\n\n[Twitter](https:\/\/twitter.com\/?lang=en) has become an important communication channel in times of emergency.   \nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time.    \nBecause of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\n# Objective     \n\n\n[Sentiment analysis](https:\/\/en.wikipedia.org\/wiki\/Sentiment_analysis) is a common use case of [NLP](https:\/\/machinelearningmastery.com\/natural-language-processing\/) where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet.     \nThis problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\nIn this project [Deep Learning](https:\/\/en.wikipedia.org\/wiki\/Deep_learning) models are implemented for predicting that a tweet regarding a disaster is real or fake    \nWhole code below is in [Python](https:\/\/www.python.org\/) using various libraries. Open source library [Tensorflow](https:\/\/www.tensorflow.org\/) and [Transformers](https:\/\/huggingface.co\/transformers\/model_doc\/auto.html) are used for creating the models and tuning them with [Keras Tuner](https:\/\/www.tensorflow.org\/tutorials\/keras\/keras_tuner).\n<p align=\"center\">\n    <br clear=\"right\"\/>\n    <img src=\"https:\/\/cdn-images-1.medium.com\/max\/1018\/1*I5O6NX_DIKYI1VBuLfX77Q.jpeg\" alt=\"Tweets\" width=\"800\" height=\"1000\" \/>\n<\/p>","e8e386d2":"# 3. Data Cleaning","4a5bf5a7":"## 2.5 Visualising most common punctuations in the text data","d5cd1c5a":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/5\/?share_key=tfNQPMyUblqOh7JL1sEiqW\" target=\"_blank\" title=\"Average word length in tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/5.png?share_key=tfNQPMyUblqOh7JL1sEiqW\" alt=\"Average word length in tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:5\" sharekey-plotly=\"tfNQPMyUblqOh7JL1sEiqW\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","9d963848":"### What are stopwords?\n\nIn computing, stop words are words that are filtered out before or after the natural language data (text) are processed.       \nWhile \u201cstop words\u201d typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.  \n\nAnalysing most occuring [stop words](https:\/\/en.wikipedia.org\/wiki\/Stop_word) in the text using corpus creating function(create_corpus)","0f0b9db8":"From the plot we can say that the number of words in the tweets ranges from 2 to 30 in both cases","f6d5d372":"Displaying first 10 rows of our data using [DataFrame.head()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.head.html)","e9c39472":"Creating data of top 10 n-grams for n = 1, 2, 3","2c56894b":"### What is Tokenization  \n\nTokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units,  \nsuch as individual words or terms. Each of these smaller units are called tokens.  \n\nThe tokens could be words, numbers or punctuation marks. In tokenization, smaller units are created by  \nlocating word boundaries, which are the ending point of a word and the beginning of the next word. \n\nNow we use [keras](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras) text preprocessing [Tokenizer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer) to fit on text on Cleaned data","c2fc39f6":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/3\/?share_key=c65IIAyuBQBfgU1Rfovdfb\" target=\"_blank\" title=\"Length of words in Tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/3.png?share_key=c65IIAyuBQBfgU1Rfovdfb\" alt=\"Length of words in Tweets\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:3\" sharekey-plotly=\"c65IIAyuBQBfgU1Rfovdfb\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","c502e201":"### What is DistilBert?\nDistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased ,    \nruns 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nCreating model checkpoint for [DistilBert](https:\/\/huggingface.co\/distilbert-base-uncased) model and importing it's pretrained AutoTokenizer","f9062839":"## 2.3 Visualising average word lengths of tweets","9dfcd441":"## 4.2 Visualising words inside Real Disaster Tweets","aa17e161":"The average word countss for real disaster tweets are found to be in the range(5-7.5)                 \nwhile for fake disaster tweets are in the range of (4-6).","179228ca":"Analyzing lengths of words in a tweets according to it being real or fake target value by ploting [histograms](https:\/\/www.investopedia.com\/terms\/h\/histogram.asp#:~:text=A%20histogram%20is%20a%20bar,used%20to%20visualize%20data%20distributions.)","676228ea":"printing model's summary","1d1f6a75":"we can see that most common words in disaster tweets are fire,storm,flood , police etc. ","69f33dcf":"## 4.1  Creating function and data for visualising words","a32fb726":"printing model's summary","5df42a39":"Creating a model for best found hyperparameters and training it","16d8d4b6":"\n<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/1\/?share_key=hgjA8Zkl35RjZtywNHe0jm\" target=\"_blank\" title=\"Counts for Disaster and Non-Disaster Tweets\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/1.png?share_key=hgjA8Zkl35RjZtywNHe0jm\" alt=\"Counts for Disaster and Non-Disaster Tweets\" style=\"max-width: 100%;width: 1000px;\"  width=\"1000\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:1\" sharekey-plotly=\"hgjA8Zkl35RjZtywNHe0jm\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>","c09185e9":"## 4.5 Analysing top 10 N-grams where N is 1,2,3","0281a51c":"Creating padded sentences using [pad_sequences](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences) function","0600b11b":"Predictions of CNN model","7cc4ef06":"predictions of DistilBert model","50e014ce":"### What is lemmatizing?  \nLemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.   \nLemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n  \nText preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same.    \nActually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n\nFunction for cleaning the data, we use [RegEx](https:\/\/docs.python.org\/3\/library\/re.html) i.e. re python library and [WordNetLemmatizer()](https:\/\/www.nltk.org\/_modules\/nltk\/stem\/wordnet.html).","92c63bdb":"ploting model's summary","b79d6e76":"### What are Bidirectional-LSTM networks?  \n\nBidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together.   \nThis structure allows the networks to have both backward and forward information about the sequence at every time step\n   \nUsing bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs \nthis approach   \nfrom unidirectional is that in the LSTM that runs backward you preserve information from the future and using the two hidden states combined  \nyou are able in any point in time to preserve information from both past and future.\n\nFunction for creating [Bidirectional Long Short-Term Memory](https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/#:~:text=Bidirectional%20LSTMs%20are%20an%20extension,LSTMs%20on%20the%20input%20sequence.) (LSTM) networks","6f1d056e":"Creating a tuner using Keras-Tuner(kt) for above function, searching for best hyperparameters","a9ea9b7c":"### What is a CNN?  \nCNNs are basically just several layers of convolutions with nonlinear activation functions like ReLU or tanh applied to the results.     \nIn a traditional feedforward neural network we connect each input neuron to each output neuron in the next layer.    \nThat\u2019s also called a fully connected layer, or affine layer. In CNNs we don\u2019t do that. Instead, we use convolutions over the input layer to compute the output.   \nThis results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters,   \ntypically hundreds or thousands like the ones showed above, and combines their results.\n\nFunction for creating [Convolutional Neural Network](https:\/\/www.tensorflow.org\/tutorials\/images\/cnn)","a890d784":"Predictions of Bidirectional LSTM model","14a67cf6":"Using [scikit-learn's train_test_split](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) to split the data into training and validation dataset","ca4d5e8a":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/15\/?share_key=9JgPThmm677jJmNjJTc0BZ\" target=\"_blank\" title=\"Top Punctuations in the text\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/15.png?share_key=9JgPThmm677jJmNjJTc0BZ\" alt=\"Top Punctuations in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:15\" sharekey-plotly=\"9JgPThmm677jJmNjJTc0BZ\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","4ea14adb":"Our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model,    \nresulting in false prediction. Now, we will further remove some words with high frequency from text based on above charts.","a924153b":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/13\/?share_key=icoxxtajqMGbKIizrTLUX0\" target=\"_blank\" title=\"Top stop words in the text\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/13.png?share_key=icoxxtajqMGbKIizrTLUX0\" alt=\"Top stop words in the text\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:13\" sharekey-plotly=\"icoxxtajqMGbKIizrTLUX0\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>","2129c04f":"## 5.2 Data preprocessing and creating padded sentences","e39d9064":"printing model's [summary](https:\/\/www.ibm.com\/docs\/SSLVMB_24.0.0\/spss\/tutorials\/curveest_modelsummary_virus.html#:~:text=The%20model%20summary%20table%20reports,values%20of%20the%20dependent%20variable.)","18047e3b":"Concise summarization of total information provided by the data using [DataFrame.info()](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23\/generated\/pandas.DataFrame.info.html)","46bcdff6":"<div>\n    <a href=\"https:\/\/plotly.com\/~raklugrin01\/17\/?share_key=rHBUmASeWITErHR7rEdZqJ\" target=\"_blank\" title=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"display: block; text-align: center;\"><img src=\"https:\/\/plotly.com\/~raklugrin01\/17.png?share_key=rHBUmASeWITErHR7rEdZqJ\" alt=\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\" style=\"max-width: 100%;width: 1400px;\"  width=\"1400\" onerror=\"this.onerror=null;this.src='https:\/\/plotly.com\/404.png';\" \/><\/a>\n    <script data-plotly=\"raklugrin01:17\" sharekey-plotly=\"rHBUmASeWITErHR7rEdZqJ\" src=\"https:\/\/plotly.com\/embed.js\" async><\/script>\n<\/div>\n","b8e893f5":"## 4.6 Visualising top 10 N-grams for N = 1, 2, 3","bfa13863":"#  Table of Contents\n\n1. Dependancies and Dataset\n\n2. Data Exploration\n\n3. Data Cleaning\n\n4. Extra Data Exploration and Analysis with Cleaned Text\n\n5. Data Preprocessing and Creating Word Embedding Matrix\n\n6. Training and Tuning Deep Learning Models\n\n7. Conclusion","5cfebc27":"## 4.4 Removing unwanted words with high frequency","c7acfbbe":"ploting model training and validating history","8d3d9c2e":"Using scikit-learn's [OneHotEncoder](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) to [one hot encode](https:\/\/www.sciencedirect.com\/topics\/computer-science\/one-hot-encoding) the target column   \nand spliting the data into training and validation set.","4c8f9525":"## 6.4 Training DistilBert transformer","6a3a9091":"Creating a model using the best hyperparameters and training it using [callbacks](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/Callback) to save  the model at appropriate [epoch](https:\/\/radiopaedia.org\/articles\/epoch-machine-learning)","45b9b6f1":"# 6. Training and Tuning Deep Learning Models","10d889a5":"## 2. Data Exploration","f898ed45":"[Tokenizing](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html) training and validation data","68abde05":"Displaying Cleaned Data ","b1d582e5":"## 1.1 Importing dependancies","a3c04ca9":"## 6.3 Training and Tuning Bidirectinal Long Short-Term Memory (LSTM) networks","ac15a63f":"Searching for a model with highest accuracy and saving it's [hyperparameters](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/hyperparameter)","e7f65680":"### What is a corpus?\n\nIn linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts.   \nSuch collections may be formed of a single language of texts, or can span multiple languages\n   \nFunction for creating sample [corpus](https:\/\/21centurytext.wordpress.com\/home-2\/special-section-window-to-corpus\/what-is-corpus\/) for further analysis.    ","2501d960":"love,new,time etc are the most common words in wordcloud of Non-disaster tweets","ace6ea94":"## 5.1 Spliting original data after cleaning ","e337695e":"[The Bar Charts](https:\/\/plotly.com\/python\/bar-charts\/) displays the top 10 stop words in tweets where **'the'** is most frequent in both groups","94177651":"Function to summarise history of train our model","84e2dca7":"Building the Model","31bcde94":"### Why Glove?\n\nThe advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words),   \nbut incorporates global statistics (word co-occurrence) to obtain word vectors\n\nCreating Embedding dictionary using [Glove Twitter data](https:\/\/www.kaggle.com\/danielwillgeorge\/glove6b100dtxt)","eeeb1706":"## 2.2 Visualising lengths of tweets","2e12634a":"## 2.1 Visualising counts of real and fake tweets","1ad17b59":"# **Deep Learning approach to predict real or fake tweets about disaster**\n---","fea37a15":"Creating train and validation dataset with [batch size](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset) 16","0ee61dcf":"Importing and preprocessing test data","826f24d9":"## 3.1 Removing unwanted text using regular expressions","6d587090":"Creating a tuner using Keras-Tuner(kt) for above function","0f07d633":"## 1.2 Reading and preparation of data","90254d6c":"## 4.3 Visualising words inside Fake Disaster Tweets","231ccdba":"Now let's have a look at the punctuations inside our data","9d5ea87d":"## 5.2 Using Glove word embeddings for creating Embedding Matrix","0eb786ef":"Predictions of Multichannel CNN","0e451828":"We only use text and target column of dataset for rest of our work as there lot's of null values inside other columns","cea8892c":" \nnow let's have a look on Non-Disaster tweets","c7de136f":"Creating Word Embedding matrix which is a list of all words and their corresponding embeddings.","6ddd60b6":"# 5. Data Preprocessing and Creating Word Embedding Matrix","a6d8451a":"Checking average word length for both type of tweets","108df74c":"Using the popular [WordCloud](https:\/\/www.python-graph-gallery.com\/wordcloud\/) python library for visulaising the cleaned data","c6fb6b7f":"## 6.2 Training and Tuning Multi-Channel Convolutional Neural Network","ccc669c2":"Creating a tuner using Keras-Tuner(kt) for above function, searching for best hyperparameters"}}