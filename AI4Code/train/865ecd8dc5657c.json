{"cell_type":{"633933e9":"code","7121f790":"code","0162bee1":"code","0ced8303":"code","686fac59":"code","df3b9b00":"code","a79cb2ca":"code","d14655ec":"code","f2be1260":"code","53081df6":"code","21cad174":"code","f8a020ac":"code","10b38e55":"code","8de32294":"code","0eb3406b":"code","49c425a7":"code","879f8c08":"code","53ce8047":"code","c4236597":"code","47518aef":"code","722d36aa":"code","7814a67b":"markdown","696d2682":"markdown","47965a2e":"markdown","06a17d6f":"markdown","9a47025d":"markdown","a12117fe":"markdown","7cde30d0":"markdown","258d7a39":"markdown","891f7932":"markdown","99829d8c":"markdown","e6961e71":"markdown","5b8b0eb2":"markdown","e059109e":"markdown","e7926043":"markdown","b52a7ada":"markdown","61e3e4eb":"markdown","128648c4":"markdown","fdfd7abe":"markdown"},"source":{"633933e9":"import tensorflow as tf\nimport pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import roc_curve\nimport seaborn as sns\nsns.set(palette=\"Set2\")\n","7121f790":"df=pd.read_csv('..\/input\/deep-learning-az-ann\/Churn_Modelling.csv')\ndf.info()","0162bee1":"df.describe().T\n# Avoid using common describe bcs if there was more columns you wouldnt be able to see all of them","0ced8303":"df.info()","686fac59":"# There is a few useless columns such as RowNumber, Surname, CustomerId, basically they are sure to not have any like correlation to the prediction so lets just drop them\ndf.drop(['RowNumber','Surname','CustomerId'],axis=1, inplace= True) \ndf.head()","df3b9b00":"corrmat=df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,vmax=0.9,square=True,annot=True,cmap=\"RdYlBu\")","a79cb2ca":"df.drop(labels='Tenure',axis=1,inplace=True)","d14655ec":"bplot = df.boxplot(patch_artist=True)\nplt.xticks(rotation=90)       \nplt.show()\n# Seens like there is none, this actually already displays wether you have one with a little weird dot","f2be1260":"df=pd.get_dummies(df,columns=['Gender','Geography','NumOfProducts'])\n# Be carefull to not run this line of code twice or an error will appear\ndf.info()","53081df6":"# print('Max Age', df['Age'].max())\n# print('Min Age',df['Age'].min())\n# print('Mean Age',df['Age'].mean())\nage_cate=[0 if i < 38.92  else  1 if i < 50  else 2 if i < 60 else 4 for i in df['Age']]\ndf['AgeCate']=age_cate\ndf.drop(labels='Age', axis=1,inplace=True)\n#Drop useless age column\ndf.head(n=10)","21cad174":"df['AgeCate']=df['AgeCate'].astype('category')\n# #We need to definy as category or eles pandas is going to cry out\ndf=pd.get_dummies(df,columns=['AgeCate'])\ndf.head()","f8a020ac":"print('Max Age', df['CreditScore'].max())\nprint('Min Age',df['CreditScore'].min())\nprint('Mean Age',df['CreditScore'].mean())\ncredit_cate=[0 if i < 450  else  1 if i < 600  else 2 if i < 700 else 4 for i in df['CreditScore']]\ndf['CreditScoreCate']=credit_cate","10b38e55":"df['CreditScoreCate']=df['CreditScoreCate'].astype('category')\n# #We need to definy as category or eles pandas is going to cry out\ndf=pd.get_dummies(df,columns=['CreditScoreCate'])\ndf.drop(labels='CreditScore', axis=1,inplace=True)\ndf.info()","8de32294":"X = df.drop([\"Exited\"],axis=1)\ny = df[\"Exited\"].values","0eb3406b":"#This function is going to help split our data into training data and testing data\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.75)","49c425a7":"from sklearn.preprocessing import StandardScaler\n# Here we are making sure the  training data that is inputed  has a standard distribution format,\n# Something that the neural networks likes a lot, because STATISTICS\n\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns.values)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns.values)","879f8c08":"print(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","53ce8047":"model=keras.Sequential([\nlayers.Dense(10,activation='relu',input_dim=X_train.shape[1]),\nlayers.Dense(10,activation='relu'),\nlayers.Dense(10,activation='relu'),\nlayers.Dense(10,activation='relu'),\nlayers.Dense(10,activation='relu'),\nlayers.Dense(1,activation='sigmoid')\n])\n#Dense, is just a layer. And yes i know in sequential is bad practice to name the input function but i dont give a fuck\n\nmodel.compile(\n\tloss=keras.losses.BinaryCrossentropy(from_logits=False),\n\toptimizer=keras.optimizers.Adam(learning_rate=0.001),\n\tmetrics=['accuracy']\n\t)\n\nmodel.fit(X_train, y_train,batch_size=128 ,epochs=600,verbose=1)\n# Set verbose =1 to get accuracy\n# Defining a batch size is good practice\n# I am not sorry kaggle","c4236597":"model.evaluate(X_test,y_test,batch_size=128,verbose=1)","47518aef":"y_pred = model.predict(X_train)\nscore, acc = model.evaluate(X_train, y_train,batch_size=128)\nprint('Train score:', score)\nprint('Train accuracy:', acc*100)","722d36aa":"y_pred = model.predict(X_test)\nscore, acc = model.evaluate(X_test, y_test,batch_size=10)\nprint('Test score:', score)\nprint('Test accuracy:', acc*100)","7814a67b":"### Checking correlation from the remaining columns\n* Checking for correlation among the features columns(all of them besides exited) and exited(prediction column)\n* If one column has a bad correlation we need to check out why, to see if there is a reason behind it like too many outliers\n* But the thing is neural networks are actually capable of utilizing uncorrelated columns and those columns can even improve the model\n* So lets just check for outliers\n* Now if you are going to drop that column or keep it is up to you\n* I am going to keep a few of the uncorrelated and see if we get better precision","696d2682":"* Making Age and Credit Score into categorical data\n### Age","47965a2e":"# Step 2 - Lets get the data going and lets standardize it (making sure that everything on the correct shape)\n*  So lets start by getting our y and X variables, whichs is basically the column that we want to find out and the  columns that are going to use to predict\n","06a17d6f":"### Handling categorical data so we can put it into the neural network(better know as feature engineering )\n* Since this dataset has a lot of categorical columns, we need to convert them into something more easily readable therefore we use get dummies pandas function to make sure every category has a integer value, and not a specific string. And they only vary beetween 1  and 0.\n* Also there are a few columns, which  are worth more in a categorical status which is a nono in my book so lets fix them\n* On the first cell we will convert the easy ones like Gender Geography NumOfPruducts, which are already on a integer mode\n* The second cell is going to massively convert columns which should be transfered into a categorical status","9a47025d":"### Check your datatypes\n* Kind of an important thing you know","a12117fe":"### It is always a good idea to check the shapes of the data, so when u input it. U know exactly the shapes the arrays are in\n* Which is very important, specifically for me bcs i fucked up on the train test split part","7cde30d0":"* And that is basically it there is way more, ways of improving this and so on. And i am gonna teach you guy wait it out for my new projects. We going to learng a few ways of handling overfitting, with regularizers and also build up your neural on a functional api","258d7a39":"* Decided to drop tenure, because i didnt want the bother of transitioning into categorical and sice it is uncorrelated it is fine","891f7932":"### Now we just need to evaluate our model :)","99829d8c":"# Introduction\n\n* Hello my name is yappy yo.\n* And right now we going to learn on how to build up simple neural networks\n* And we going to begin with data handling\n* The purpose of this dataset is to classify correct wether a client is going to stay  with the bank or wether he is going to leave\n* This style of notebook cant be used to formulate every single neural network possible but it displays a good few step before doing a simple ANN\n* So be carefull \n* There is going be a lot of steps, but everything is going to be okay :)","e6961e71":"# Step 1 -  Imports and data handling","5b8b0eb2":"# Step 3- Build up the model\n* On my next notebook we are going to buid up a functional api","e059109e":"### Standardizing","e7926043":"### And start looking for weird discrepancies","b52a7ada":"### Checking for outliers","61e3e4eb":"### Read dataset and check out if there are null values","128648c4":"### Renember to drop uncorrelated columns like id and so on","fdfd7abe":"### CreditScore"}}