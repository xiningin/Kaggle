{"cell_type":{"6e7fc7c6":"code","e986b106":"code","9cd75ca6":"code","8670d552":"code","82425950":"code","e1c2c2a0":"code","e0ed20b4":"code","2145d882":"code","8d07bf48":"code","5b7aabc7":"code","f35b1b21":"code","fbf4032a":"code","6b991b9d":"code","c2a5588c":"code","f673f3e8":"code","1b137367":"code","9dc5a7ce":"code","4bd2beb0":"code","25a48093":"code","796d2143":"code","d41b0159":"code","cedc807d":"code","fb81ba39":"code","4eff206c":"code","b0090955":"code","4730077a":"code","9d48f97b":"code","6232e159":"code","a5d441fb":"code","671043a5":"code","cf9317e8":"code","c0d2f73c":"code","bf9adabd":"code","6986f5a1":"code","a5a5a7f5":"code","a3bb1e4b":"code","f2d7db3e":"code","17f0c842":"code","ea671bc6":"code","906c5e53":"code","2dcafb1e":"markdown","526cb7d4":"markdown","9f4f5efd":"markdown","dd58a950":"markdown","b791da8b":"markdown","3b13866d":"markdown","76447a16":"markdown","6e54edb8":"markdown","022837fa":"markdown","fc9c6e0a":"markdown","445aa1cd":"markdown","20843a08":"markdown","59647764":"markdown","e9f7f7a0":"markdown","1c911c67":"markdown","c9f54eeb":"markdown","533965a3":"markdown","5a66990c":"markdown","835e975b":"markdown","e269dc9d":"markdown","d50b5ac8":"markdown","9e336737":"markdown","11f172fc":"markdown","6760e979":"markdown","ee586e1f":"markdown","a4bc58a3":"markdown","89a3eea5":"markdown","215a16f1":"markdown","bec9ea4a":"markdown","fc92a9d7":"markdown","58bc1c9e":"markdown","876a5eb3":"markdown","a515043b":"markdown","ccdfce7b":"markdown","44ddc0fa":"markdown","6e9d60ab":"markdown","11530b24":"markdown","66d37805":"markdown","03795656":"markdown","b0b6f109":"markdown","fbe26a34":"markdown","49d9d4eb":"markdown","b01b2cef":"markdown","d7d807c5":"markdown","a6d5ac01":"markdown","0b74a911":"markdown","66926d12":"markdown"},"source":{"6e7fc7c6":"import os\nimport warnings\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","e986b106":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import models\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import models\nfrom PIL import Image \nfrom skimage.io import imread\nimport cv2\n\nK.clear_session()","9cd75ca6":"img = plt.imread('..\/input\/indian-food-classification\/dataset\/Dataset\/train\/pizza\/033.jpg')\ndims = np.shape(img)\nmatrix = np.reshape(img, (dims[0] * dims[1], dims[2]))\nprint(np.shape(matrix))","8670d552":"plt.imshow(img)\nprint(\"Image shape -> \",dims[:2])\nprint(\"Color channels -> \",dims[2])\nprint(\"Min color depth : {}, Max color depth {}\".format(np.min(img),np.max(img)))","82425950":"sns.distplot(matrix[:,0], bins=20,color=\"red\",hist_kws=dict(alpha=0.3))\nsns.distplot(matrix[:,1], bins=20,color=\"green\",hist_kws=dict(alpha=0.35))\nsns.distplot(matrix[:,2], bins=20,color=\"blue\",hist_kws=dict(alpha=0.2))\nplt.show()","e1c2c2a0":"_ = plt.hist2d(matrix[:,1], matrix[:,2], bins=(50,50))\nplt.xlabel('Green channel')\nplt.ylabel('Blue channel')\nplt.show()","e0ed20b4":"from sklearn import cluster\nn_vals=[2,4,6,8]\nplt.figure(1, figsize=(12, 8))\n\nfor subplot,n in enumerate(n_vals):\n    kmeans=cluster.KMeans(n)\n    clustered = kmeans.fit_predict(matrix)\n    dims = np.shape(img)\n    clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n    plt.subplot(2,2, subplot+1)\n    plt.title(\"n = {}\".format(n), pad = 10,size=18)\n    plt.imshow(clustered_img)\n    \nplt.tight_layout()","2145d882":"from mpl_toolkits.mplot3d import Axes3D\n\nfig=plt.figure(figsize=(14,10))\n\nax = [fig.add_subplot(221, projection='3d'),\n      fig.add_subplot(222, projection='3d'),\n      fig.add_subplot(223, projection='3d'),\n      fig.add_subplot(224, projection='3d')]\n\nfor plot_number,n in enumerate(n_vals):\n    \n    kmeans=cluster.KMeans(n)\n    clustered = kmeans.fit_predict(matrix)\n    x1, y1, z1 = [np.where(clustered == x)[0] for x in [0, 1, 2]]\n\n    plot_vals = [('r', x1),\n                 ('b', y1),\n                 ('g', z1),\n                 ]\n    \n    for c, channel in plot_vals:\n        x = matrix[channel, 0]\n        y = matrix[channel, 1]\n        z = matrix[channel, 2]\n        ax[plot_number].scatter(x, y, z, c=c,s=10)\n    \n    ax[plot_number].set_xlabel('Blue channel')\n    ax[plot_number].set_ylabel('Green channel')\n    ax[plot_number].set_zlabel('Red channel')\n\nplt.tight_layout()","8d07bf48":"bnorm = np.zeros_like(matrix, dtype=np.float32)\nmax_range = np.max(matrix, axis=1)\nbnorm = matrix \/ np.vstack((max_range, max_range, max_range)).T\nbnorm_img = np.reshape(bnorm, (dims[0],dims[1],dims[2]))\nplt.figure(figsize=(8,10))\nplt.imshow(bnorm_img)\nplt.show()","5b7aabc7":"import skimage\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.filters import sobel\nfrom skimage.filters import sobel_h\n\nplt.figure(1,figsize=(20,15))\ncmap=\"YlGnBu\"\nplt.subplot(3,1,1)\nplt.imshow(img)\n\nplt.subplot(3,1,2)\nplt.imshow(sobel(img[:,:,2]),cmap=cmap)\n\nplt.subplot(3,1,3)\nplt.imshow(sobel_h(img[:,:,1]), cmap=cmap)\n\nplt.tight_layout()","f35b1b21":"from sklearn.decomposition import PCA\n\npca = PCA(3)\npca.fit(matrix)\nimg_pca = pca.transform(matrix)\nimg_pca = np.reshape(img_pca, (dims[0], dims[1], dims[2]))\n\nfig = plt.figure(figsize=(8, 8))\nplt.imshow(img_pca[:,:,1], cmap=cmap)","fbf4032a":"main='..\/input\/indian-food-classification\/dataset\/Dataset\/train\/'\n\ndata=dict()\n\nfor i in os.listdir(main):\n    sub_dir=os.path.join(main,i)\n    count=len(os.listdir(sub_dir))\n    data[i]=count\n    \n  \nkeys = data.keys()\nvalues = data.values()\n\ncolors=[\"red\" if x<= 150 else \"green\" for x in values]\n\nfig, ax = plt.subplots(figsize=(12,8))\ny_pos=np.arange(len(values))\nplt.barh(y_pos,values,align='center',color=colors)\nfor i, v in enumerate(values):\n    ax.text(v+1.4, i-0.25, str(v), color=colors[i])\nax.set_yticks(y_pos)\nax.set_yticklabels(keys)\nax.set_xlabel('Images',fontsize=16)\nplt.xticks(color='black',fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()","6b991b9d":"import random\n\ntrain_folder = \"\/kaggle\/input\/indian-food-classification\/dataset\/Dataset\/train\"\nimages = []\n\nfor food_folder in sorted(os.listdir(train_folder)):\n    food_items = os.listdir(train_folder + '\/' + food_folder)\n    food_selected = np.random.choice(food_items)\n    images.append(os.path.join(train_folder,food_folder,food_selected))\n                                     \nfig=plt.figure(1, figsize=(25, 25))\n\nfor subplot,image_ in enumerate(images):\n    category=image_.split('\/')[-2]\n    imgs = plt.imread(image_)\n    a,b,c=imgs.shape\n    fig=plt.subplot(5, 4, subplot+1)\n    fig.set_title(category, pad = 10,size=18)\n    plt.imshow(imgs)\n    \nplt.tight_layout()","c2a5588c":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nn_classes = 20\nbatch_size = 32\nimg_width, img_height = 299, 299\n\ntrain_data_dir = '\/kaggle\/input\/indian-food-classification\/dataset\/Dataset\/train'\n\n# Data Augmentation with ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nval_data_dir = '\/kaggle\/input\/indian-food-classification\/dataset\/Dataset\/val'\n\nval_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\nval_generator = val_datagen.flow_from_directory(\n    val_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","f673f3e8":"class_map = train_generator.class_indices\nclass_map","1b137367":"# print(tf.__version__)\n# print(tf.test.gpu_device_name())","9dc5a7ce":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dense, Dropout\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.regularizers import l2\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n\nnb_train_samples = 3583 \nnb_validation_samples = 1089\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='v1_inceptionV3', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_v1_inceptionV3.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=val_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=20,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])","4bd2beb0":"model.save('model_v1_inceptionV3.h5')","25a48093":"def plot_accuracy(history):\n    \n    plt.plot(history.history['accuracy'],label='train accuracy')\n    plt.plot(history.history['val_accuracy'],label='validation accuracy')\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(loc='best')\n    plt.savefig('Accuracy_v1_InceptionV3')\n    plt.show()\n    \ndef plot_loss(history):\n    \n    plt.plot(history.history['loss'],label=\"train loss\")\n    plt.plot(history.history['val_loss'],label=\"validation loss\")\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(loc='best')\n    plt.savefig('Loss_v1_InceptionV3')\n    plt.show()\n    \nplot_accuracy(history)\nplot_loss(history)","796d2143":"K.clear_session()\npath_to_model='.\/model_v1_inceptionV3.h5'\nprint(\"Loading the model..\")\nmodel = load_model(path_to_model)\nprint(\"Done!\")","d41b0159":"test_data_dir = '..\/input\/indian-food-classification\/test'\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","cedc807d":"scores = model.evaluate_generator(test_generator)\n\nprint(\"Test Accuracy: {:.3f}\".format(scores[1]))","fb81ba39":"category={\n    0: ['burger','Burger'], 1: ['butter_naan','Butter Naan'], 2: ['chai','Chai'],\n    3: ['chapati','Chapati'], 4: ['chole_bhature','Chole Bhature'], 5: ['dal_makhani','Dal Makhani'],\n    6: ['dhokla','Dhokla'], 7: ['fried_rice','Fried Rice'], 8: ['idli','Idli'], 9: ['jalegi','Jalebi'],\n    10: ['kathi_rolls','Kaathi Rolls'], 11: ['kadai_paneer','Kadai Paneer'], 12: ['kulfi','Kulfi'],\n    13: ['masala_dosa','Masala Dosa'], 14: ['momos','Momos'], 15: ['paani_puri','Paani Puri'],\n    16: ['pakode','Pakode'], 17: ['pav_bhaji','Pav Bhaji'], 18: ['pizza','Pizza'], 19: ['samosa','Samosa']\n}\n\ndef predict_image(filename,model):\n    img_ = image.load_img(filename, target_size=(299, 299))\n    img_array = image.img_to_array(img_)\n    img_processed = np.expand_dims(img_array, axis=0) \n    img_processed \/= 255.   \n    \n    prediction = model.predict(img_processed)\n    \n    index = np.argmax(prediction)\n    \n    plt.title(\"Prediction - {}\".format(category[index][1]))\n    plt.imshow(img_array)\n    \ndef predict_dir(filedir,model):\n    cols=5\n    pos=0\n    images=[]\n    total_images=len(os.listdir(filedir))\n    rows=total_images\/\/cols + 1\n    \n    true=filedir.split('\/')[-1]\n    \n    fig=plt.figure(1, figsize=(25, 25))\n    \n    for i in sorted(os.listdir(filedir)):\n        images.append(os.path.join(filedir,i))\n        \n    for subplot,imggg in enumerate(images):\n        img_ = image.load_img(imggg, target_size=(299, 299))\n        img_array = image.img_to_array(img_)\n        \n        img_processed = np.expand_dims(img_array, axis=0) \n\n        img_processed \/= 255.\n        prediction = model.predict(img_processed)\n        index = np.argmax(prediction)\n        \n        pred=category.get(index)[0]\n        if pred==true:\n            pos+=1\n        \n        fig=plt.subplot(rows, cols, subplot+1)\n        fig.set_title(category.get(index)[1], pad = 10,size=18)\n        plt.imshow(img_array)\n\n    acc=pos\/total_images\n    print(\"Accuracy of Test : {:.2f} ({pos}\/{total})\".format(acc,pos=pos,total=total_images))\n    plt.tight_layout()","4eff206c":"predict_image('..\/input\/indian-food-classification\/test\/burger\/images (16).jpg',model)","b0090955":"predict_dir(\"..\/input\/indian-food-classification\/test\/masala_dosa\",model)","4730077a":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nimg_width, img_height = 299, 299\n\ndef labels_confusion_matrix():\n    folder_path=\"..\/input\/indian-food-classification\/test\"\n    \n    mapping={}\n    for i,j in enumerate(sorted(os.listdir(folder_path))):\n        mapping[j]=i\n    \n    files=[]\n    real=[]\n    predicted=[]\n\n    for i in os.listdir(folder_path):\n        \n        true=os.path.join(folder_path,i)\n        true=true.split('\/')[-1]\n        true=mapping[true]\n        \n        for j in os.listdir(os.path.join(folder_path,i)):\n            \n            img_ = image.load_img(os.path.join(folder_path,i,j), target_size=(img_height, img_width))\n            img_array = image.img_to_array(img_)\n            img_processed = np.expand_dims(img_array, axis=0) \n            img_processed \/= 255.\n            prediction = model.predict(img_processed)\n            index = np.argmax(prediction)\n\n            predicted.append(index)\n            real.append(true)\n            \n    return (real,predicted)\n\ndef print_confusion_matrix(real,predicted):\n\n    cmap=\"viridis\"\n    cm_plot_labels = [i for i in range(20)]\n\n    cm = confusion_matrix(y_true=real, y_pred=predicted)\n    df_cm = pd.DataFrame(cm,cm_plot_labels,cm_plot_labels)\n    sns.set(font_scale=1.1) # for label size\n    plt.figure(figsize = (15,10))\n    s=sns.heatmap(df_cm, annot=True,cmap=cmap) # font size\n#     bottom,top=s.get_ylim()\n#     s.set_ylim(bottom+0.6,top-0.6)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig('confusion_matrix.png')\n    plt.show()","9d48f97b":"y_true,y_pred=labels_confusion_matrix()\nprint_confusion_matrix(y_true,y_pred)","6232e159":"def get_activations(img, model_activations):\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255. \n    plt.imshow(img[0])\n    plt.show()\n    return model_activations.predict(img)\n\ndef show_activations(activations, layer_names):\n    \n    images_per_row = 16\n\n    # Now let's display our feature maps\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # This is the number of features in the feature map\n        n_features = layer_activation.shape[-1]\n\n        # The feature map has shape (1, size, size, n_features)\n        size = layer_activation.shape[1]\n\n        # We will tile the activation channels in this matrix\n        n_cols = n_features \/\/ images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # We'll tile each filter into this big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,:, :,col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,row * size : (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    plt.show()\n    \ndef activation_conv():\n    first_convlayer_activation = activations[0]\n    second_convlayer_activation = activations[3]\n    third_convlayer_activation = activations[6]\n    f,ax = plt.subplots(1,3, figsize=(10,10))\n    ax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\n    ax[0].axis('OFF')\n    ax[0].set_title('Conv2d_1')\n    ax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\n    ax[1].axis('OFF')\n    ax[1].set_title('Conv2d_2')\n    ax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\n    ax[2].axis('OFF')\n    ax[2].set_title('Conv2d_3')\n    \n    \ndef get_attribution(food):\n    \n    tf.compat.v1.disable_eager_execution()\n    \n    img = image.load_img(food, target_size=(299, 299))\n    img = image.img_to_array(img) \n    img \/= 255. \n    f,ax = plt.subplots(1,3, figsize=(15,15))\n    ax[0].imshow(img)\n    \n    img = np.expand_dims(img, axis=0)\n    model = load_model('.\/model_v1_inceptionV3.h5')\n        \n    preds = model.predict(img)\n    class_id = np.argmax(preds[0])\n    ax[0].set_title(\"Input Image\")\n    class_output = model.output[:, class_id]\n    last_conv_layer = model.get_layer(\"mixed10\")\n    \n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    \n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n    ax[1].imshow(heatmap)\n    ax[1].set_title(\"Heat map\")\n    \n    \n    act_img = cv2.imread(food)\n    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n    cv2.imwrite('classactivation.png', superimposed)\n    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n    ax[2].imshow(img_act)\n    ax[2].set_title(\"Class Activation\")\n    plt.show()\n    return preds","a5d441fb":"print(\"Total layers in the model : \",len(model.layers),\"\\n\")\n\n# We start with index 1 instead of 0, as input layer is at index 0\nlayers = [layer.output for layer in model.layers[1:11]]\n# We now initialize a model which takes an input and outputs the above chosen layers\nactivations_output = models.Model(inputs=model.input, outputs=layers)\n# print(layers)\n\nlayer_names = []\nfor layer in model.layers[1:11]: \n    layer_names.append(layer.name)\n    \nprint(\"First 10 layers which we can visualize are -> \", layer_names)\n","671043a5":"food = '..\/input\/indian-food-classification\/dataset\/Dataset\/val\/pizza\/155.jpg'\nactivations = get_activations(food,activations_output)\nshow_activations(activations, layer_names)","cf9317e8":"activation_conv()","c0d2f73c":"food = '..\/input\/indian-food-classification\/dataset\/Dataset\/val\/idli\/065.jpg'\nactivations = get_activations(food,activations_output)\nshow_activations(activations, layer_names)","bf9adabd":"activation_conv()","6986f5a1":"pred = get_attribution('..\/input\/indian-food-classification\/dataset\/Dataset\/val\/idli\/065.jpg')","a5a5a7f5":"pred2=get_attribution('..\/input\/indian-food-classification\/test\/fried_rice\/images (5).jpg')","a3bb1e4b":"pred3=get_attribution('..\/input\/indian-food-classification\/test\/chai\/images (3).jpg')","f2d7db3e":"pred4=get_attribution('..\/input\/indian-food-classification\/test\/jalebi\/images (4).jpg')","17f0c842":"pred5=get_attribution('..\/input\/indian-food-classification\/test\/chole_bhature\/images (10).jpg')","ea671bc6":"!wget -O download.jpg https:\/\/www.cookwithmanali.com\/wp-content\/uploads\/2015\/01\/Restaurant-Style-Dal-Makhani-Recipe.jpg\n    \nmodel_load = load_model('.\/model_v1_inceptionV3.h5')","906c5e53":"pred = get_attribution('download.jpg')","2dcafb1e":"## This time let's visualize some other food item's layer.","526cb7d4":"<hr>\n<h2><div style=\"color:purple;text-align: center;font-weight:bold;\">THANK YOU FOR BEARING WITH ME!<\/div><\/h2>\n<hr>\n<h2><center>If you liked this kernel or the yummy dataset, please consider upvoting.<br>Happy Kaggling.<\/center><\/h2>\n<hr>\n","9f4f5efd":"- <h3> Image classification is the task of identifying images and categorizing them in one of several predefined distinct classes.<\/h3><br>\n- <h3> Being one of the computer vision (CV) tasks, Image classification serves as the foundation for solving different CV problems such as object detection which further gets divided into semantic and instance segmentation.<\/h3><br>\n- <h3> The leading architecture used for image recognition and detection tasks is Convolutional Neural Networks (CNNs). Convolutional neural networks consist of several layers with small neuron collections, each of them perceiving small parts of an image. <\/h3>","dd58a950":"<span style=\"font-size:16px;\"><b>This image was given to confuse the model as both classes <u>Butter naan<\/u> and <u>Dal makhni<\/u> are present. But it predicted dal makhni and why was that?<br><br>Because the output layer inside the model computed high activations for dal makhni object. It does happen sometimes due to centralized nature of model's focus area. <\/b><\/span>","b791da8b":"## Let's visualize number of training examples for each food item","3b13866d":"## Let's visualize our dataset by randomly picking an image from every class","76447a16":"- <h3> Single image prediction <\/h3>","6e54edb8":"<h2><span style=\"color:blue\">NOW THIS IS WHERE THINGS WILL GET INTERESTING!<\/span><\/h2>\n\n- <span style=\"font-size:17px;\"> So far we were doing activation maps visualization which helped us understand how the input is transformed from one layer to another as it goes through several operations. <\/span>\n\n- <span style=\"font-size:17px;\"> At the end of training, we want the model to classify or detect objects based on features which are specific to the class.<\/span>\n\n- <span style=\"font-size:17px;\"> To validate how model attributes the features to class output, we can generate heat maps using gradients to find out which regions in the input images were instrumental in determining the class. <\/span>","022837fa":"## Downloading random image from net to predict and generate heatmap","fc9c6e0a":"<h2>Plot for visualizing histogram between 2 color channel<\/h2>","445aa1cd":"<h3>We notice that 2 classes (Pani puri and Kulfi) lacks behind with training data.<\/h3>\n\n> - <span style=\"font-size:18px;color:blue\">Data augmentation helps with classes not having enough training examples by increasing the amount of relevant data in the dataset.<\/span>\n<br><br>\n> - <span style=\"font-size:18px;color:blue\"> We would be doing what is known as **offline augmentation**. It works on relatively smaller datasets, by increasing the size of the dataset by a factor equal to the number of transformations you perform. (For example, by flipping all my images, it would increase the size of the dataset by a factor of 2).<\/span>\n<br><br>\n> - <span style=\"font-size:18px;color:blue\"><u>You will get more clarity in the coming section<\/u><\/span>","20843a08":"## MODEL LAYERS ","59647764":"## Testing model on test set ","e9f7f7a0":"# <center>Food Classification using TensorFlow (CNN Transfer learning)<\/center>\n<center><img src= \"https:\/\/timesofindia.indiatimes.com\/thumb\/msid-77563051,width-1200,height-900,resizemode-4\/.jpg\" alt =\"Titanic\" style='width:500px;'><\/center><br>","1c911c67":"## Let's plot a confusion matrix for all the food items","c9f54eeb":"## Load the model","533965a3":"# <b> MODEL TRAINING <\/b>","5a66990c":"<h2>Reshaping dimensions so we can start processing arrays. <\/h2>","835e975b":"## Saving the model","e269dc9d":"## Accuracy and Loss curves","d50b5ac8":"## Training the model","9e336737":"<h1> <b> MODEL LEARNING VISUALIZATIONS <\/b><\/h1>","11f172fc":"<h2> <span style=\"color:red\"><b> CHALLENGES <\/b><\/span><\/h2><hr> <h3>1. Cleaning the dataset due to following :<\/h3>\n    \n- A lot of misspelled labels.<br>\n- Overwhelming number of raw images not related to any class.<br>\n\n<h3> 2. Training model without bias : <\/h3>\n\n- Similarly looking items with common attributes (Fried samosa and pakode looks familiar.)<br>\n- Multiple labels clashing together (Butter Naan and Dal Makhni in one picture, Samosa and Pakode, etc.)<br>\n- Unwanted feature selection because of big architectures in transfer learning (Ketchup and green chutney is most common thing alongside pakode, samosa, kathi rolls and what not.)<br>\n\n<hr>\n\n<h2> <span style=\"color:green\"> Finally after 4-5 hours of data cleaning and correcting labels, it was time for fun! <\/span> <\/h2>","6760e979":"<center><span style=\"font-size:18px;color:blue\"><b>It is evident from above graph, intensity is reduced in the color space as the number of clusters are increased.<\/b><\/span><\/center>","ee586e1f":"# <b>Exploratory Data Analysis (EDA)<\/b>","a4bc58a3":"<span style=\"font-size:15px;\"><b>Image data consists of variations due to resolution differences between scenes, pixel intensities of an image and the environment around which the image was taken. This area of image processing is critical in today's time with the rise of Artificial intelligence.From motion detection to complex circuits in self driving car, the research requires tremendous amount of work and can be seen as widely growing areas of computer vision.<\/b><span>","89a3eea5":"<hr>","215a16f1":"<span style=\"font-size:21px;color:blue\"><b>So far, So good!<\/b><\/span>\n<hr>","bec9ea4a":"# <b>GENERATING HEATMAPS<\/b>","fc92a9d7":"## We discussed Data Augmentation before. Let's see how it works:\n<span style=\"font-size:16px\"> 1. Accepting a batch of images used for training.<\/span>\n\n<span style=\"font-size:16px\"> 2. Taking this batch and applying a series of random transformations to each image in the batch. (including random rotation, resizing, shearing, etc.)<\/span>\n\n<span style=\"font-size:16px\"> 3. Replacing the original batch with the new, randomly transformed batch.<\/span>\n\n<span style=\"font-size:16px\"> 4. Training the CNN on this randomly transformed batch. (i.e, the original data itself is not used for training)<\/span>\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/1700\/1*ae1tW5ngf1zhPRyh7aaM1Q.png\" width=500><\/center>","58bc1c9e":"<h2> Let's visualize the channel intensity for every cluster we just generated.<\/h2>","876a5eb3":"## Function to predict single image or predict all images from a directory","a515043b":"<h2> Brightness normalization is a process that changes the range of pixel intensity values. Applications include photographs with poor contrast due to glare, for example. Normalization is sometimes called contrast stretching or histogram stretching. <\/h2>","ccdfce7b":"<span style=\"font-size:18px;color:blue\"><b>Our model has 315 layers with InceptionV3 architecture!<\/b><\/span>\n<br><br>\n<span style=\"font-size:18px;color:blue\"><b>Whoa! That's a lot to process at first, so let's just stick with 10 layers for now and visualize these first to see how neural networks classify.<\/b><\/span>\n<hr>","44ddc0fa":"# <b>LAYER WISE ACTIVATIONS<\/b>","6e9d60ab":"- <span style=\"font-size:18px;color:blue\"><b>The pixels between green and blue bands are correlated (as evident from overlapping on above graph), and typically has visible imagery.<\/b><\/span>\n\n- <span style=\"font-size:18px;color:blue\"><b>Raw band differences will need to be scaled or thresholded <\/b><\/span>","11530b24":" <h2> Sobel filter is a basic way to get an edge magnitude\/gradient image. <\/h2>\n    \n**It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction.**","66d37805":"<h2><span style=\"color:green\">WOW! WE DID IT!<\/span><\/h2>\n<hr><\/hr>\n<ol>\n    <h3>\n        <li>In the above plot, we see on left the input image passed to the model, heat map in the middle and the class activation map on right<\/li><br>\n        <li>Heat map gives a visual of what regions in the image were used in determining the class of the image<\/li><br>\n        <li>Now it's clearly visible what a model looks for in an image if it has to be classified as an idli!<\/li>\n    <\/h3>\n<\/ol>","03795656":"- <h3> Predicting category <\/h3>","b0b6f109":"<h2> Let's apply Principal Component Analysis. It is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space.<\/h2>","fbe26a34":"### SOME HELPER FUNCTIONS WHICH WILL ENABLE US TO VISUALIZE HOW NEURAL NETWORK WORKS AND PERFORMS!","49d9d4eb":"<center><span style=\"font-size:18px;color:blue\"><b>Clearly the results are fascinating. We are able to isolate object and detect the edge. <\/b><\/span><\/center>","b01b2cef":"# <b> PREDICTIONS <\/b>","d7d807c5":"<hr>","a6d5ac01":"<h2>Plot for visualizing pixel intensities for RGB in color space<\/h2>","0b74a911":"## Let's show the activation outputs of Conv2D layer (we have three of them in first 10 layers) to compare how layers get abstract with depth.","66926d12":"# <b>IMAGE PROCESSSING<\/b>"}}