{"cell_type":{"0fbda372":"code","86843af1":"code","f37b458c":"code","36ce6c3c":"code","4c36a57c":"code","2761c474":"code","0ff6bfb2":"code","625977b2":"code","eaf53f88":"code","e6ed3ef4":"code","7145219e":"code","b176356d":"code","6262354a":"code","ed83c9a8":"code","87e81958":"code","7970f0cf":"code","18d6998f":"code","0a2f5e59":"code","09f80f10":"code","db8f44f4":"code","2eccc4a5":"code","4aff5f2d":"code","7723146c":"code","dbb95400":"markdown","b1f632b7":"markdown","0c9b4d63":"markdown","d8191f2d":"markdown","3dd05601":"markdown","ea0ddd41":"markdown"},"source":{"0fbda372":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder #Encode Categorical Features\nimport lightgbm as lgb #Gradient Boosting Machine\nimport matplotlib.pyplot as plt #Visualization\nimport seaborn as sns #Visualization\nfrom sklearn.model_selection import KFold #N-Fold Validation\nfrom sklearn.metrics import mean_squared_error #Evaluation Metric\nimport optuna #hyperparams Tuning","86843af1":"trainSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntestSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')","f37b458c":"trainSet.head()","36ce6c3c":"#plot the Target Distribution\nsns.displot(data=trainSet, x=\"target\", kde=True)","4c36a57c":"#encode categorical feats\ncat_feat = [f\"cat{val}\" for val in range(0,10)]\n\nlabelEnc = [LabelEncoder() for _ in range(len(cat_feat))]\n\nfor i in range(len(cat_feat)):\n    trainSet[cat_feat[i]] = labelEnc[i].fit_transform(trainSet[cat_feat[i]])","2761c474":"for i in range(len(cat_feat)):\n    testSet[cat_feat[i]] = labelEnc[i].transform(testSet[cat_feat[i]])","0ff6bfb2":"cont_var = [f\"cont{val}\" for val in range(14)]\nfor i in cont_var:\n    trainSet[i] = np.log(trainSet[i])\n    testSet[i] = np.log(testSet[i])","625977b2":"#Seperate features and its target\ny = trainSet.target\nX = trainSet.drop(['target', 'id'], axis=1)\nX_test = testSet.drop('id', axis=1)","eaf53f88":"X.head()","e6ed3ef4":"X_test.head()","7145219e":"#For time sake, I will not rerun the hyperparam tunning, here is the best Hyperparams I got from optuna tunner\nbest_params = {\n    'num_iterations': 979, \n    'learning_rate': 0.04867910597290001, \n    'min_data_in_leaf': 109, \n    'num_leaves': 15, \n    'lambda_l1': 19.336354545776132, \n    'lambda_l2': 22.70600360390991, \n    'bagging_freq': 1, \n    'cat_smooth': 18.499097172037967,\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'verbose': -1\n}","b176356d":"N_FOLDS = 5\nrmse_score = 0\nlgbm_models = []\neval_results = [{} for _ in range (N_FOLDS)]\n\nkf = KFold(n_splits = N_FOLDS)","6262354a":"#Train our LGBM using the best parameter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ny_test = pd.DataFrame()\n\nfor folds, (train_idx,val_idx) in enumerate(kf.split(X, y)):\n    print(f\"folds: {folds}\")\n    trainSet = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n    valSet = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n    \n    model = lgb.train(best_params, trainSet, valid_sets=[trainSet, valSet], evals_result=eval_results[folds], verbose_eval= 100)\n    lgbm_models.append(model)\n    y_pred = model.predict(X.iloc[val_idx])\n    \n    rmse_score += mean_squared_error(y.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n    \n    print(mean_squared_error(y.iloc[val_idx], y_pred, squared=False))\n    \n    y_test = pd.concat([y_test, pd.Series(model.predict(X_test))], axis=1)","ed83c9a8":"y_test = y_test.mean(axis=1)","87e81958":"X_concat = pd.concat([X, X_test])\ny_concat = pd.concat([y, y_test])","7970f0cf":"N_FOLDS = 5\nrmse_score = 0\nlgbm_models = []\neval_results = [{} for _ in range (N_FOLDS)]\n\n\nkf2 = KFold(n_splits=10, shuffle=True, random_state=5473)","18d6998f":"#Train our LGBM using the best parameter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ny_test = pd.DataFrame()\n\nfor folds, (train_idx,val_idx) in enumerate(kf.split(X_concat, y_concat)):\n    print(f\"folds: {folds}\")\n    trainSet = lgb.Dataset(X_concat.iloc[train_idx], y_concat.iloc[train_idx])\n    valSet = lgb.Dataset(X_concat.iloc[val_idx], y_concat.iloc[val_idx])\n    \n    model = lgb.train(best_params, trainSet, valid_sets=[trainSet, valSet], evals_result=eval_results[folds], verbose_eval= 100)\n    lgbm_models.append(model)\n    y_pred = model.predict(X_concat.iloc[val_idx])\n    \n    rmse_score += mean_squared_error(y_concat.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n    \n    print(mean_squared_error(y_concat.iloc[val_idx], y_pred, squared=False))\n    \n    y_test = pd.concat([y_test, pd.Series(model.predict(X_test))], axis=1)","0a2f5e59":"y_test = y_test.mean(axis=1)","09f80f10":"#plot the rmse score for each iteration in 5th fold model\nlgb.plot_metric(eval_results[4])","db8f44f4":"lgb.plot_importance(lgbm_models[4])","2eccc4a5":"id = testSet.id\ntestSet.drop('id', axis=1, inplace=True)","4aff5f2d":"submFile = pd.concat([id, y_test],axis=1)\nsubmFile.columns = ['id', 'target']","7723146c":"submFile.to_csv('submFile.csv', index=False)","dbb95400":"# Predict the Test Set","b1f632b7":"From the correlation matrix, I could say that there is no single feature that is highly correlated to the target. So for this notebook, I will use all those features.","0c9b4d63":"# Create Submission File as in sample_submission.csv","d8191f2d":"# Data Preprocessing","3dd05601":"# Create pseudo Label","ea0ddd41":"# Train using Pseudo Label"}}