{"cell_type":{"99d1c099":"code","2e7ee148":"code","d4b3d9c9":"code","2109682a":"code","793d31e2":"code","61ba814e":"code","76c75212":"code","b3959aa5":"code","ce20ed91":"code","28e0a5ec":"code","11b432cd":"code","2a8105c7":"code","01a87623":"code","fc60646c":"code","e9281d64":"code","a384875d":"code","8c2fbf71":"code","b550bbbd":"code","ed525bc5":"code","41393daa":"code","3ff57b51":"code","eaea8780":"code","64669e81":"code","f462e7c2":"code","8249b7c4":"code","cc9319df":"code","850065ff":"code","39141d9d":"code","5a1c4e8c":"code","b294b3cd":"code","164bbdc6":"code","c7d90595":"code","40cd7a37":"code","63eec2d6":"code","5169c55c":"code","a2f0e813":"code","d5695f97":"code","29ef6ef3":"code","dcb0cf54":"code","b61f8a32":"code","fb368273":"code","fd38e3f9":"code","08838d9a":"code","19026c49":"code","ec304b58":"code","f258ba9d":"code","2a233f4b":"markdown","a0c4d37b":"markdown","8e91568d":"markdown","293ea067":"markdown","045b8e49":"markdown","cb9672d8":"markdown","1d1cda19":"markdown","c212cc7b":"markdown","7f172c6a":"markdown","3a7118a0":"markdown","d4240c9b":"markdown","4b20d90f":"markdown","ff469a3b":"markdown"},"source":{"99d1c099":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e7ee148":"train = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/test.csv\")","d4b3d9c9":"train","2109682a":"def log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)","793d31e2":"train_Y = train['Cost']","61ba814e":"train_Y","76c75212":"dev1 = np.std(train_Y)\ndev1","b3959aa5":"train_Y = log1p(train_Y)\ntrain_X = train.drop(['Cost'], axis=1)\ntest_X = test\ntrain_X","ce20ed91":"train_Y","28e0a5ec":"dev2 = np.std(train_Y)\ndev2","11b432cd":"import pandas_profiling ","2a8105c7":"train.profile_report()","01a87623":"train_X = train_X.drop(['Customer Id', 'Artist Name', 'Artist Reputation', 'Transport'], axis = 1)\ntest_X = test.drop(['Customer Id', 'Artist Name', 'Artist Reputation', 'Transport'], axis = 1)","fc60646c":"train_X['State'] = train_X['Customer Location'].map(lambda x:x.split()[-2])\ntrain_X.drop(['Customer Location'], inplace = True, axis = 1)\ntest_X['State'] = test_X['Customer Location'].map(lambda x:x.split()[-2])\ntest_X.drop(['Customer Location'], inplace = True, axis = 1)","e9281d64":"train_X['Scheduled Date'] = pd.to_datetime(train_X['Scheduled Date'])\ntrain_X['Delivery Date'] = pd.to_datetime(train_X['Delivery Date'])\ntrain_X['scheduleDiff'] = (train_X['Scheduled Date'] - train_X['Delivery Date']).map(lambda x:str(x).split()[0])\ntrain_X['scheduleDiff'] = pd.to_numeric(train_X['scheduleDiff'])\ntest_X['Scheduled Date'] = pd.to_datetime(test_X['Scheduled Date'])\ntest_X['Delivery Date'] = pd.to_datetime(test_X['Delivery Date'])\ntest_X['scheduleDiff'] = (test_X['Scheduled Date'] - test_X['Delivery Date']).map(lambda x:str(x).split()[0])\ntest_X['scheduleDiff'] = pd.to_numeric(test_X['scheduleDiff'])","a384875d":"train_X['dday'] = train_X['Delivery Date'].dt.day\ntrain_X['dmonth'] = train_X['Delivery Date'].dt.month\ntrain_X['dyear'] = train_X['Delivery Date'].dt.year\ntrain_X['ddayofweek'] = train_X['Delivery Date'].dt.dayofweek\n\ntest_X['dday'] = test_X['Delivery Date'].dt.day\ntest_X['dmonth'] = test_X['Delivery Date'].dt.month\ntest_X['dyear'] = test_X['Delivery Date'].dt.year\ntest_X['ddayofweek'] = test_X['Delivery Date'].dt.dayofweek\n\ntest_X.head()","8c2fbf71":"train_X.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ntest_X.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ntrain_X.head()","b550bbbd":"#1. Function to replace NAN values with mode value\ndef impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['Material', 'Remote Location']:\n    impute_nan_most_frequent_category(train_X,Columns)\n    impute_nan_most_frequent_category(test_X,Columns)\n    \n# Display imputed result\ntrain_X[['Material','Material_Imputed','Remote Location','Remote Location_Imputed']].head(10)\n#3. Drop actual columns\ntrain_X = train_X.drop(['Material', 'Remote Location'], axis = 1)\ntest_X = test_X.drop(['Material', 'Remote Location'], axis = 1)","ed525bc5":"#test_X.profile_report()","41393daa":"train_X","3ff57b51":"# Get list of categorical variables\ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","eaea8780":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nonehot_X_train = one_hot_encoder.fit_transform(train_X[object_cols])\nonehot_X_test = one_hot_encoder.fit_transform(test_X[object_cols])\nonehot_X_train = pd.DataFrame(onehot_X_train)\nonehot_X_test = pd.DataFrame(onehot_X_test)\n\nnum_X_train = train_X.drop(object_cols, axis=1)\nnum_X_valid = test_X.drop(object_cols, axis=1)\n\nonehot_X_train = pd.concat([num_X_train, onehot_X_train], axis=1)\nonehot_X_test = pd.concat([num_X_valid, onehot_X_test], axis=1)\n","64669e81":"onehot_X_train","f462e7c2":"from fancyimpute import KNN \nfrom fancyimpute import IterativeImputer\n#imputer = KNN(k=5)\nimputer = IterativeImputer()\n#train_ = train[columns]\nfullset = pd.concat([onehot_X_train, onehot_X_test]) \n#fancy impute removes column names\nfullset = pd.DataFrame(imputer.fit_transform(fullset))\n#fullset.columns = columns\nfullset","8249b7c4":"train_df = fullset[:onehot_X_train.shape[0]]\ntest_df = fullset[onehot_X_train.shape[0]:]","cc9319df":"import sklearn\nscaler = sklearn.preprocessing.StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\nX_test = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n","850065ff":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, train_Y)","39141d9d":"y_prediction_rf = rf.predict(X_test)","5a1c4e8c":"y_prediction_rf","b294b3cd":"#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","164bbdc6":"param_grid = [\n{'n_estimators': [50,100,250,500], \n 'max_depth': [10, 50, 100], 'bootstrap': [True, False]}\n]\n","c7d90595":"random_search_forest = RandomizedSearchCV(rf, param_grid, cv=10)\nrandom_search_forest.fit(X_train, train_Y)","40cd7a37":"tuned_rf_best_random = random_search_forest.best_estimator_","63eec2d6":"tuned_rf_best_random","5169c55c":"tuned_rf_best_random.fit(X_train, train_Y)","a2f0e813":"tuned_rf_random_pred = tuned_rf_best_random.predict(X_test)","d5695f97":"tuned_rf_random_pred","29ef6ef3":"from lightgbm import LGBMRegressor","dcb0cf54":"lgbm = LGBMRegressor()\nlgbm.fit(X_train, train_Y)\ny_test_pred_lgbm = lgbm.predict(X_test)","b61f8a32":"y_test_pred_lgbm","fb368273":"y_test_pred_lgbm2 = expm1(y_test_pred_lgbm)","fd38e3f9":"y_test_pred_lgbm2","08838d9a":"df = pd.DataFrame()","19026c49":"df['Customer Id'] = test['Customer Id']\ndf['Cost'] = y_test_pred_lgbm2\ndf['Cost'] = df['Cost'].abs()","ec304b58":"df","f258ba9d":"df.to_csv('prediction1.csv', index = False)","2a233f4b":"## # Suggestions and views are most welcomed","a0c4d37b":"**By problem statement I observed that score is calculated using MEAN-SQUARE-LOG-ERROR. It is so because the target has very large standard deviation(std)(shown below). So for prediction purpose I wanted to reduce std. This is done by taking[numpy.log1p] of the target variable. **","8e91568d":"**LGBM**","293ea067":"Earlier I tried this model and procedure","045b8e49":"# Highlights \n## Used Pandas profiling for the first time and just amazed with it's utility, used np.log1p()[Explained in detail below], also explored fancyimpute","cb9672d8":"Nextly used this model","1d1cda19":"# Using Pandas Profiling for EDA ","c212cc7b":"**RandomForest Regressor**","7f172c6a":"**One-Hot Encoding Categorical Data**","3a7118a0":"**Tuning RF**","d4240c9b":"**Dealing with categorical columns having missing values**","4b20d90f":"**Imputing missing values using fancyimpute**","ff469a3b":"**Data Normalisation**"}}