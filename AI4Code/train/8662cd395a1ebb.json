{"cell_type":{"60ae7712":"code","02fa3e49":"code","959f52b5":"code","7103e5aa":"code","495bb591":"code","be3974cf":"code","803a006b":"code","7faf8599":"code","6008ea44":"code","fc629c27":"code","b8a7f329":"code","92e6b9e8":"code","f5e2dac8":"code","46a07d1f":"code","6aa06656":"code","f512084e":"code","33d54ae3":"code","7117a432":"code","8217c567":"code","f77617de":"code","a5259c41":"code","8537c0eb":"code","f0cd18f7":"code","4936276c":"code","da0182dc":"code","6c03ebda":"code","9acf5690":"code","d41a23c5":"code","58ce5850":"code","d8f2b9a6":"code","01bc0672":"code","d0ac2598":"code","2be957d8":"code","11193533":"code","54cb2fe1":"code","3719f060":"code","7d2d175f":"code","4e785a03":"code","81034505":"code","6d0aea67":"code","3e03d9dd":"code","2d90a357":"code","e2a95bce":"code","d3359eb1":"markdown","709674bb":"markdown","3c641b85":"markdown","5adea4d9":"markdown","2501b901":"markdown","394a912a":"markdown","05a69a28":"markdown","54d3ef34":"markdown","18fd0982":"markdown","907a51b3":"markdown"},"source":{"60ae7712":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02fa3e49":"df_djia = pd.read_csv(\"\/kaggle\/input\/stocknews\/upload_DJIA_table.csv\")\ndf_combined = pd.read_csv(\"\/kaggle\/input\/stocknews\/Combined_News_DJIA.csv\")\ndf_reddit = pd.read_csv(\"\/kaggle\/input\/stocknews\/RedditNews.csv\")","959f52b5":"print(df_djia.head())\nprint(df_combined.head())\nprint(df_reddit.head())","7103e5aa":"import nltk\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords \nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize","495bb591":"stop_words = set(stopwords.words('english'))\npunctuation = [\"'\", \":\", \"b'\", \".\", \"\\\"\", \"]\", \"[\", \".'\", \".\\\"\", \"?\"]\n\nstemmer = SnowballStemmer(\"english\")\nfor p in punctuation:\n    stop_words.add(p)\ntokenizer = WordPunctTokenizer()","be3974cf":"def custom_tokenize(text):\n    tokens = tokenizer.tokenize(str(text))\n    # filtered_tokens = [stemmer.stem(t.lower()) for t in tokens if not t in stop_words and len(t) > 1]\n    filtered_tokens = [t.lower() for t in tokens if not t in stop_words and len(t) > 1]\n    return \" \".join(filtered_tokens)\n    ","803a006b":"print(custom_tokenize(\"let's go to mall!! Watch Jim's match\"))","7faf8599":"cols= df_combined.columns\nfor col in cols:\n    if \"Top\" in col:\n        df_combined[str(col)+\"_\"]=df_combined[col].apply(custom_tokenize)\n        df_combined.drop([col], axis=1, inplace=True)","6008ea44":"df_combined.head()","fc629c27":"all_headlines = []\nfor row in range(0,len(df_combined.index)):\n    all_headlines.append(' '.join(str(x) for x in df_combined.iloc[row,2:27]))\n    ","b8a7f329":"all_headlines[0:4]","92e6b9e8":"len(df_combined),len(all_headlines)","f5e2dac8":"df_combined[\"combined\"]=all_headlines","46a07d1f":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import roc_auc_score","6aa06656":"train=df_combined[0:int(len(df_combined)*.80)]\ntest=df_combined[int(len(df_combined)*.80):]","f512084e":"train_x, test_x, y_train, y_test=train[\"combined\"], test[\"combined\"], train[\"Label\"], test[\"Label\"]\nprint(train_x.shape, y_train.shape)","33d54ae3":"tfvectorizer= TfidfVectorizer(max_features = 50000, ngram_range=(2,3))\nX_train=tfvectorizer.fit_transform(train_x)\nX_test=tfvectorizer.transform(test_x)","7117a432":"print(tfvectorizer.get_feature_names())\nprint(X_train.shape)","8217c567":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","f77617de":"print(y_test.shape, X_test.shape)","a5259c41":"def all_classifier():\n    clf = LogisticRegressionCV(Cs=[0.1,1.0,10.0], cv=5, solver='liblinear').fit(X_train, y_train)\n    print(\"Logistic Classifier\", clf.score(X_test, y_test))\n    y_preds=clf.predict_proba(X_test)\n    y_preds = y_preds[:, 1]\n    print(\"Logistic ROC Curve\", roc_auc_score(y_test, y_preds))\n    \n    rclf = RandomForestClassifier(max_depth=10)\n    rclf.fit(X_train, y_train)\n    print(\"RandomForest Classifier\",rclf.score(X_test, y_test))\n    \n    mnb = MultinomialNB()\n    mnb.fit(X_train, y_train)\n    print(\"MultinomialNB Classifier\", mnb.score(X_test, y_test))\n    \n    boost = XGBClassifier()\n    boost.fit(X_train, y_train)\n    y_pred = boost.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"XGBClassifier Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n    return clf, rclf, mnb, boost\n    ","8537c0eb":"def model_words(model_obj, vectorizer):\n    coeffs_list = model_obj.coef_.tolist()[0]\n    features = vectorizer.get_feature_names()\n    print(len(features), len(coeffs_list))\n\n    \n    coeff_df = pd.DataFrame({'Words' : features, \n                        'Coefficient' : coeffs_list})\n    coeff_df = coeff_df.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n    print(\"Positive\", coeff_df.head(10))\n    print(\"Negative\", coeff_df.tail(10))","f0cd18f7":"a,b,c,d = all_classifier()","4936276c":"model_words(a, tfvectorizer)","da0182dc":"c_vectorizer= CountVectorizer(max_features = 50000, ngram_range=(2,3))\nX_train=c_vectorizer.fit_transform(train_x)\nX_test=c_vectorizer.transform(test_x)","6c03ebda":"a,b,c,d = all_classifier()","9acf5690":"model_words(a, c_vectorizer)","d41a23c5":"c_vectorizer= CountVectorizer(max_features = 50000, ngram_range=(2,3))\nX_train=c_vectorizer.fit_transform(train_x)\nX_test=c_vectorizer.transform(test_x)\na,b,c,d = all_classifier()\nmodel_words(a, c_vectorizer)","58ce5850":"from keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.models import Sequential","d8f2b9a6":"t=Tokenizer()\nt.fit_on_texts(df_combined[\"combined\"])\nmax_length = max([len(d.split(\" \")) for d in df_combined[\"combined\"]])\nmax_length","01bc0672":"print(t.document_count)\nprint(len(t.word_index))","d0ac2598":"doc_encoded=t.texts_to_sequences(df_combined['combined'].values)\nprint(doc_encoded[0])","2be957d8":"print(t.word_index[\"world\"])","11193533":"doc_encoded=pad_sequences(doc_encoded, maxlen=400)\nprint(doc_encoded[0])","54cb2fe1":"vocab_size=len(t.word_index)+1\nEMB_OUTPUT_DIMS = 100","3719f060":"e = Embedding(input_dim=vocab_size, output_dim=EMB_OUTPUT_DIMS, input_length=400)","7d2d175f":"model=Sequential()\nmodel.add(e)\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","4e785a03":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","81034505":"train_len = int(len(doc_encoded)*0.7)","6d0aea67":"X_train, y_train, X_test, y_test = doc_encoded[0:train_len], df_combined[0:train_len][\"Label\"], doc_encoded[train_len:], df_combined[train_len:][\"Label\"]\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","3e03d9dd":"model.summary()","2d90a357":"model.fit(X_train, y_train, batch_size=16, epochs=3)","e2a95bce":"results = model.evaluate(X_test, y_test)\nprint('test loss, test acc:', results)","d3359eb1":"Apply count vectorizer transformed train to different classifier","709674bb":"Create an embedding layer","3c641b85":"Create a sequential Model with dropout","5adea4d9":"Apply Stopwords and Punctuation","2501b901":"Lets apply classifier","394a912a":"Looks like positive and negative words are making some sense","05a69a28":"LSTM Modelling","54d3ef34":"Now, lets check with CountVectorizer","18fd0982":"Single token is not making much sense, its better to take larger token","907a51b3":"Let's apply TFIDF Vectorizer"}}