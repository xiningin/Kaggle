{"cell_type":{"2f3de77f":"code","60e92c0a":"code","166a558c":"code","be301821":"code","ac1b2fed":"code","099c2652":"code","532b1032":"code","170b46f5":"code","7e1418d5":"code","bd8f61d5":"code","b93664cd":"code","4a1813c1":"code","54338ad4":"code","70d4826c":"code","f98abc96":"code","bdf7114b":"code","baaebbd6":"code","79165012":"code","d995d9ff":"code","cc81483f":"code","403a37db":"markdown","6850eb05":"markdown","826d7fa8":"markdown","140c3626":"markdown","0d713e88":"markdown","ba54ebb7":"markdown","e4006a6d":"markdown","5f7e0ac7":"markdown","9e8bb62a":"markdown","6d2fa402":"markdown"},"source":{"2f3de77f":"import re\nimport os\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","60e92c0a":"# install transformers\n!pip install transformers","166a558c":"# import the model and tokenizer\nfrom transformers import (DistilBertTokenizerFast, \n                         TFDistilBertForSequenceClassification)    ","be301821":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of replicas in sync: ', strategy.num_replicas_in_sync)","ac1b2fed":"# dataframe display settings\npd.set_option('display.max_colwidth', None)","099c2652":"train_data = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', sep = '\\t')\ntest_data = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip', sep = '\\t')\n\ntrain_data.head()","532b1032":"# check the shape of the train data\ntrain_data.shape","170b46f5":"# display the head of test data\ntest_data.head()","7e1418d5":"# check the shape of the test data\ntest_data.shape","bd8f61d5":"# check the number of examples in each class\ntrain_data.Sentiment.value_counts(normalize = True).plot(kind = 'bar', figsize = (10, 6), xlabel = 'Sentiments');","b93664cd":"# set of stop words in english\nstop_words = set(stopwords.words('english'))\n\nneg = [\"aren't\", \"didn't\", \"doesn't\", \"hadn't\",  \"haven't\", \"isn't\", 'no', 'not', \"shouldn't\", \"wasn't\", \"weren't\", \"wouldn't\"]\nstop_words.difference_update(neg)","4a1813c1":"# this function will clean the text\ndef text_cleaning(text):\n    if text:\n        text = ' '.join(text.split('.'))\n        text = re.sub('\\\/', ' ', text)\n        text = re.sub(r'\\\\', ' ', text)\n        text = re.sub(r'((http)\\S+)', '', text)\n        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n        text = [word for word in text.split() if word not in stop_words]\n        return text\n    return []","54338ad4":"# clean train and test dataframes\ntrain_data['Phrase'] = train_data['Phrase'].apply(lambda x: ' '.join(text_cleaning(x)))\ntest_data['Phrase'] = test_data['Phrase'].apply(lambda x: ' '.join(text_cleaning(x)))","70d4826c":"# drop duplicates from train data\ntrain_data.drop_duplicates(subset = ['Phrase'], inplace = True)\ntrain_data.head(8)","f98abc96":"# add length column to train data\ntrain_data['length'] = train_data['Phrase'].apply(lambda x: len(x.split()))\n\n# add length column to test data\ntest_data['length'] = test_data['Phrase'].apply(lambda x: len(x.split()))\n\n# filter the phrases from the test data with zero length\nlen_zero_data = test_data[test_data['length'] == 0]\nlen_zero_data.shape","bdf7114b":"# select phrases with length > 1 and split data into train and validation set\nx_train, x_val, y_train, y_val = train_test_split(train_data[train_data['length'] > 1]['Phrase'], \n                                                  train_data[train_data['length'] > 1]['Sentiment'], \n                                                  test_size = 0.2, \n                                                  stratify = train_data[train_data['length'] > 1]['Sentiment'],\n                                                  random_state = 42)\n\nprint(f'Shape of x_train: {x_train.shape}\\nShape of y_train: {y_train.shape}')\nprint(f'Shape of x_val: {x_val.shape}\\nShape of y_val: {y_val.shape}')","baaebbd6":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# encode the training and validation data\ntrain_encodings = tokenizer(x_train.tolist(), truncation = True, padding = True)\nval_encodings = tokenizer(x_val.tolist(), truncation = True, padding = True)","79165012":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings), y_train.values)).shuffle(10000).batch(32).repeat()\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings), y_val.values)).shuffle(10000).batch(32)","d995d9ff":"with strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n      'distilbert-base-uncased', num_labels = 5)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)\n    model.compile(optimizer = optimizer, loss = model.compute_loss, metrics = \n                ['accuracy'])","cc81483f":"# fit the model\nmodel.fit(train_dataset, epochs = 3, batch_size = 32, steps_per_epoch = len(x_train) \/\/ 32,\n          validation_data = val_dataset, validation_steps = len(x_val) \/\/ 32)","403a37db":"## Calculate length of the phrase","6850eb05":"## Read train and test data into pandas dataframe","826d7fa8":"## Train and validation split","140c3626":"- Dataset is highly imbalanced","0d713e88":"## Import and installation section","ba54ebb7":"## Create training and validation datasets for training","e4006a6d":"## Detect and initialze tpu","5f7e0ac7":"## Create tokenizer","9e8bb62a":"## Create model","6d2fa402":"## Cleaning of text"}}