{"cell_type":{"d235f9c2":"code","dbb55457":"code","d5273d8c":"code","7e27541f":"code","1547457b":"code","4c69f6ec":"code","a8bf34b1":"code","c6378dcf":"code","53667f40":"code","b6d7e6cd":"code","70e89a95":"code","327877dc":"code","442e2674":"code","a59669e5":"code","a6afafb5":"code","609e26d3":"code","fd9c6b0a":"code","7a62bd50":"code","c643e088":"code","323dd3cc":"code","04433c2a":"code","c9182bff":"code","03ec4e9b":"code","dae712c1":"code","09aef3d0":"code","3a8fec1f":"code","2f7b9893":"code","6199d00b":"code","0b69b79a":"code","a29c2c92":"code","0e5991ea":"code","12f3358c":"code","84c8e437":"code","5cf86249":"markdown","7222f472":"markdown","e65e5775":"markdown","e53d8ff4":"markdown","8b4ef28f":"markdown","6a98626b":"markdown","9fdb8e3c":"markdown","930c618f":"markdown","cd2bd688":"markdown","83fece3a":"markdown","042b609c":"markdown","f125f608":"markdown","d8ccf0d4":"markdown","b4060aa7":"markdown","21bc0d7c":"markdown","349ff2bd":"markdown","9af8bc0e":"markdown","c35fd522":"markdown","27275148":"markdown","9119d163":"markdown","f31a0569":"markdown","e1d1e4a3":"markdown","4128b5a8":"markdown","a73a3b60":"markdown","f52c9eca":"markdown","ca0ce704":"markdown","7c233371":"markdown","8cb2d020":"markdown","efe2eae9":"markdown","1a4aa90a":"markdown","4ebdf5c3":"markdown","c3f098db":"markdown","550e7df5":"markdown","33610906":"markdown","652d0812":"markdown","2f1df2d1":"markdown","3f9cebd7":"markdown","073fd987":"markdown","5f9f9cab":"markdown","969736d3":"markdown"},"source":{"d235f9c2":"import json\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2","dbb55457":"f, axarr = plt.subplots(2,2)\nimg1 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_0.jpg')\nimg2 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_1.jpg')\nimg3 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_2.jpg')\nimg4 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_3.jpg')\naxarr[0,0].imshow(img1)\naxarr[0,1].imshow(img2)\naxarr[1,0].imshow(img3)\naxarr[1,1].imshow(img4)","d5273d8c":"f, axarr = plt.subplots(2,2)\nimg1 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_10.jpg')\nimg2 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_1005.jpg')\nimg3 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_101.jpg')\nimg4 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_1.jpg')\naxarr[0,0].imshow(img1)\naxarr[0,1].imshow(img2)\naxarr[1,0].imshow(img3)\naxarr[1,1].imshow(img4)","7e27541f":"data_train = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv\")","1547457b":"data_train.head()","4c69f6ec":"data_train_new = pd.DataFrame(columns = ['image_id', 'Category']) ","a8bf34b1":"data_train_new.head()","c6378dcf":"for index,row in data_train.iterrows():\n    #print(row)\n    #print(index)\n    pathname = str(row['image_id'])+'.jpg'\n    data_train_new.loc[index,'image_id']=pathname\n    if(row['healthy']==1):\n        cat = 'healthy'\n    elif(row['multiple_diseases']==1):\n        cat = 'multiple_diseases'\n    elif(row['rust']==1):\n        cat = 'rust'\n    else:\n        cat = 'scab'\n    \n    data_train_new.loc[index,'Category']=cat","53667f40":"data_train_new.head()","b6d7e6cd":"data_train_new.to_csv(\"trainWithext.csv\", index=False)","70e89a95":"!git clone https:\/\/github.com\/Tessellate-Imaging\/monk_v1.git","327877dc":"!cd monk_v1\/installation\/Misc && pip install -r requirements_kaggle.txt","442e2674":"# Monk\nimport os\nimport sys\nsys.path.append(\"monk_v1\/monk\/\");","a59669e5":"#Using pytorch backend \nfrom pytorch_prototype import prototype","a6afafb5":"gtf = prototype(verbose=1);\ngtf.Prototype(\"PlantPathology2020\", \"Using_Pytorch_Backend\");","609e26d3":"gtf.Default(dataset_path=\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/\",\n            path_to_csv=\"trainWithext.csv\", # updated csv file \n            model_name=\"resnet18\", \n            freeze_base_network=False,\n            num_epochs=20); ","fd9c6b0a":"gtf.EDA(check_corrupt=True)","7a62bd50":"gtf.List_Models();","c643e088":"#Start Training\ngtf.Train();\n#Read the training summary generated once you run the cell and training is completed","323dd3cc":"gtf = prototype(verbose=0);\ngtf.Prototype(\"PlantPathology2020\", \"Using_Pytorch_Backend\", eval_infer=True);","04433c2a":"from IPython.display import Image\nImage(filename=\"workspace\/PlantPathology2020\/Using_Pytorch_Backend\/output\/logs\/train_val_accuracy.png\") ","c9182bff":"from IPython.display import Image\nImage(filename=\"workspace\/PlantPathology2020\/Using_Pytorch_Backend\/output\/logs\/train_val_loss.png\") ","03ec4e9b":"img_name = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_0.jpg\";\npredictions = gtf.Infer(img_name=img_name);\n\n#Display \nfrom IPython.display import Image\nImage(filename=img_name)","dae712c1":"img_name = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_1004.jpg\";\npredictions = gtf.Infer(img_name=img_name);\n\n#Display \nfrom IPython.display import Image\nImage(filename=img_name)","09aef3d0":"img_name = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_10.jpg\";\npredictions = gtf.Infer(img_name=img_name);\n\n#Display \nfrom IPython.display import Image\nImage(filename=img_name)","3a8fec1f":"import pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.special import softmax\n#np.set_printoptions(precision=2)\ndf = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\")","2f7b9893":"img_name = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Test_10.jpg\";\npredictions = gtf.Infer(img_name=img_name,return_raw=True);\n\ntype(predictions)\n\npredictions.keys()\n\nprint(predictions[\"raw\"])\n\nprint(\" Predictions in terms of probabilities\")\nprint(softmax(predictions[\"raw\"]))\n\n#Display \nfrom IPython.display import Image\nImage(filename=img_name)","6199d00b":"for i in tqdm(range(len(df))):\n    img_name = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/\" + df[\"image_id\"][i] + \".jpg\";\n    \n    #Invoking Monk's nferencing engine inside a loop\n    predictions = gtf.Infer(img_name=img_name, return_raw=True);\n    x = predictions[\"raw\"]\n    out = softmax(x)\n    df[\"healthy\"][i] = out[0];\n    df[\"multiple_diseases\"][i] = out[1];\n    df[\"rust\"][i] = out[2];\n    df[\"scab\"][i] = out[3];","0b69b79a":"df.head()","a29c2c92":"df.to_csv(\"submission.csv\", index=False);","0e5991ea":"! rm -r monk_v1","12f3358c":"! rm -r workspace","84c8e437":"! rm pylg.log trainWithext.csv","5cf86249":"* [MONK](#MONK)\n* [Exploratory Data Analysis\/ Data Visualization](#dv)\n* [Installing Monk](#installingmonk)\n* [Importing Pytorch Backend](#pyb)\n* [Creating and Managing experiments](#cme)\n* [Quick Mode Training - Load the data and the model](#train)\n* [EDA Using Monk](#edaM)\n* [See what other models Monk's backend supports](#mod)\n* [Train the classifier](#tc)\n* [Running inference on test images](#inf)","7222f472":"Loss Curve","e65e5775":"Viewing 4 Test images","e53d8ff4":"# **To contribute to Monk AI or Pytorch RoadMap repository raise an issue in the git-repo or DM us on linkedin** ","8b4ef28f":"Viewing 4 Train images","6a98626b":"Load the experiment in inference mode\n- Set flag eval_infer as True","9fdb8e3c":"* low-code\n* unified wrapper over major deep learning framework - keras, pytorch, gluoncv\n* syntax invariant wrapper\n","930c618f":"Accuracy Curve","cd2bd688":"# Load the data and the model","83fece3a":"Docs on  quick mode loading of data and model: https:\/\/github.com\/Tessellate-Imaging\/monk_v1#4\n\nTutorials on Monk: https:\/\/github.com\/Tessellate-Imaging\/monk_v1\/tree\/master\/study_roadmaps\/1_getting_started_roadmap","042b609c":"*Monk is a low code Deep Learning tool and a unified wrapper for Computer Vision.*","f125f608":"# <div id=\"cme\"> Creating and managing experiments <\/div>\n\n- Provide project name\n- Provide experiment name\n- For a specific data create a single project\n- Inside each project multiple experiments can be created\n- Every experiment can be have diferent hyper-parameters attached to it","d8ccf0d4":"<div id=\"edaM\"> EDA in MONK <\/div>","b4060aa7":"# Check out \n\n# [Monk_Object_Detection](https:\/\/github.com\/Tessellate-Imaging\/Monk_Object_Detection)\n\nA one-stop repository for low-code easily-installable object detection pipelines.\n\nand\n\n# [Monk_Gui](https:\/\/github.com\/Tessellate-Imaging\/Monk_Gui)\n\nA Graphical user Interface for deep learning and computer vision over Monk Libraries\n\nalso\n\n# [Pytorch_Tutorial](https:\/\/github.com\/Tessellate-Imaging\/Pytorch_Tutorial)\n\nA set of jupyter notebooks on pytorch functions with examples","21bc0d7c":"1. To create, manage and version control deep learning experiments.\n2. To compare experiments across training metrics.\n3. To quickly find best hyper-parameters.\n","349ff2bd":"* To use mxnet backend\n\nfrom gluon_prototype import prototype\n\n* To use keras backend\n\nfrom keras_prototype import prototype","9af8bc0e":"Goals","c35fd522":"Running Inference on all test images","27275148":"<div id=\"mod\"> See what other models Monk's backend supports <\/div>","9119d163":"Adding extension .jpg to image_id and adding corresponding label to Category","f31a0569":"*Imports*","e1d1e4a3":"<div id=\"pyb\"> *Using Pytorch backend* <\/div>","4128b5a8":"**Monk Features**","a73a3b60":"# <div id=\"dv\"> ** Exploratory Data Analysis ** <\/div>","f52c9eca":"* If using Colab install using the commands below\n\n!cd monk_v1\/installation\/Misc && pip install -r requirements_colab.txt\n\n* If using Kaggle uncomment the following command\n\n#!cd monk_v1\/installation\/Misc && pip install -r requirements_kaggle.txt\n\n* Select the requirements file as per OS and CUDA version when using a local system or cloud\n\n#!cd monk_v1\/installation\/Linux && pip install -r requirements_cu9.txt","ca0ce704":"* https:\/\/www.tessellateimaging.com\/\n* Abhishek - https:\/\/www.linkedin.com\/in\/abhishek-kumar-annamraju\/\n* Akash - https:\/\/www.linkedin.com\/in\/akashdeepsingh01\/","7c233371":"* git clone https:\/\/github.com\/Tessellate-Imaging\/monk_v1.git\n\n* cd monk_v1\/installation\/Linux && pip install -r requirements_cu9.txt\n\n* (Select the requirements file as per OS and CUDA version)","8cb2d020":"If there are multiple classes , the format that Monk accepts currently is\n\nFirst column should contain image_id with extension .jpg or .png and the \nsecond column must contain the label corresponding to its category.\n\nMonk will internally convert to the above one hot encoded format","efe2eae9":"- To experiment with Models\n- Understand how easy is it to use Monk","1a4aa90a":"# <div id=\"installingmonk\"> **[Installing Monk](https:\/\/github.com\/Tessellate-Imaging\/monk_v1\/tree\/master\/installation)** <\/div>","4ebdf5c3":"<div id=\"train\"> Quick mode training <\/div>\n- Using Default Function\n    - dataset_path\n    - model_name\n    - num_epochs","c3f098db":"Data Visualization","550e7df5":"So creating the required format","33610906":"# Select image and Run inference","652d0812":"# <div id=\"tc\"> Train the classifier <\/div>","2f1df2d1":"#  <div id=\"MONK\">  ** [MONK](https:\/\/github.com\/Tessellate-Imaging\/monk_v1)** <\/div>","3f9cebd7":"# <div id=\"inf\"> **Running inference on test images** <\/div>","073fd987":"# This creates files and directories as per the following structure\n\n\nworkspace\n\n\n    |\n    |--------iWildCam2020 (Project name can be different)\n                    |\n                    |\n                    |-----Using_Pytorch_Backend (Experiment name can be different)\n                                |\n                                |-----experiment-state.json\n                                |\n                                |-----output\n                                        |\n                                        |------logs (All training logs and graphs saved here)\n                                        |\n                                        |------models (all trained models saved here)","5f9f9cab":"**Monk Enables**","969736d3":"# **Table of Contents**"}}