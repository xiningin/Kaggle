{"cell_type":{"346ec188":"code","d387f137":"code","d6c705ef":"code","441f0ce4":"code","d6f393db":"code","8c832cc9":"code","1fddabea":"code","25cd00a9":"code","5cb11331":"code","96989231":"code","434dfe86":"code","48e6de96":"code","651d56b9":"code","7956acc7":"code","53a31b83":"code","2e5294b1":"code","54f2d555":"code","eef53101":"code","96e59080":"code","101fd457":"code","07a04e5f":"code","6d888291":"code","e0b94e17":"code","cb0220d0":"code","b339bbaf":"code","7706c618":"code","958c6cab":"code","c18affbd":"code","cb863142":"code","96e2529f":"code","b60f9707":"code","aa12caad":"code","dbf26a99":"code","19730380":"code","c5cf14a2":"code","365ea77e":"code","1d916697":"code","58f1f213":"code","568a2118":"code","e4b4446e":"code","c231674a":"code","4ebaaefb":"code","db7f1710":"code","a1d4da72":"code","b7edfb12":"code","6630fc4f":"code","75ba2e91":"code","c2f77fcf":"code","5a399d17":"code","ee7bd2ed":"code","a454f7c5":"code","467b5963":"code","41822582":"code","5fe2d607":"code","fd89b8ae":"code","28f78feb":"code","93faef40":"code","3e52e9a8":"code","6f20364f":"code","867280d6":"code","96f7d54c":"code","a82af38a":"code","4f60c75a":"code","daa7d3ae":"code","5cb40fa8":"code","0a205845":"code","379728cc":"code","fcf22de5":"code","02896b13":"code","8786913e":"code","1f1f6f37":"code","08fba34f":"code","53f2186e":"code","8ac6b403":"code","6568658c":"code","94680606":"code","a0a6ad7e":"code","08dfe7f8":"code","31d604cb":"code","d70ea21f":"code","3e1a4ffd":"code","f31cba52":"code","02f41528":"code","c8409771":"code","b729c45d":"code","fd92b3cd":"code","3a9edd4e":"code","d6414f49":"code","7babffd9":"code","69f5c640":"code","149d5eef":"code","56929d68":"code","a6ff34bd":"code","3f2d5b99":"code","ce44fc70":"code","8faacfb2":"code","df3ea986":"code","6396712d":"code","c7d253d3":"code","d829b0c9":"code","50a02ae1":"code","056896a2":"code","d033da46":"code","ef942151":"code","18a81173":"code","c9e630ea":"code","c397cca1":"code","cce2d349":"code","879df787":"code","f8225fd4":"code","f397df43":"code","c45e3a3d":"code","7b08bcc3":"code","88a85762":"code","ea5c10b1":"code","608edd73":"code","d3febe5b":"code","5475f034":"code","648ce013":"code","6db32493":"code","a483324d":"code","3b76224d":"code","fb45f4df":"code","379a0400":"code","ef114bfd":"code","08987728":"code","da6d76b0":"code","ee98a8cd":"code","7279af22":"code","e691042c":"code","751fdd2f":"code","c0901782":"code","6de3e364":"code","65056ec7":"code","c472df97":"code","459556c0":"markdown","a89bc469":"markdown","d166f8d1":"markdown","a09d5ed5":"markdown","08dfd5ff":"markdown","5d302519":"markdown","c6379e03":"markdown","eef9be42":"markdown","c1d2c90d":"markdown","195b9d38":"markdown","7f754b3d":"markdown","4a856fc8":"markdown","62f6cf80":"markdown","fa7e7d82":"markdown","97eb9296":"markdown","ee05b212":"markdown","818f52c5":"markdown","8b0809c5":"markdown","788f4f03":"markdown","e7e7e8a3":"markdown","9ae2880d":"markdown","e8f56723":"markdown","e7b12698":"markdown","413a5553":"markdown","d1f0d6ca":"markdown","2965d8f4":"markdown","b7248c46":"markdown","10c8b48f":"markdown","27617487":"markdown","733ae57e":"markdown","ac2c3151":"markdown","b8b40efe":"markdown","7377490b":"markdown","ae2784aa":"markdown","e9d54562":"markdown","879b91f5":"markdown","47ccfc0f":"markdown","0800a4ba":"markdown","8d99d3cd":"markdown","208ef5b0":"markdown","934e73c7":"markdown","e5b61445":"markdown","0ccabbec":"markdown","db03a18e":"markdown","50565758":"markdown","fbc76d53":"markdown","8e8415b8":"markdown","3f5be0ab":"markdown","646a88f6":"markdown","b9dcdc9c":"markdown","b6de661d":"markdown","e1059116":"markdown","e804cf28":"markdown","c369546b":"markdown","28009a1a":"markdown","5a5b1dc0":"markdown","ebfde9d7":"markdown","cba7e7f5":"markdown","4adac4f1":"markdown","57803423":"markdown","f7cb915b":"markdown","7797ecfa":"markdown","879bd2dd":"markdown","691e3ddd":"markdown","d91a2c83":"markdown","3c081c75":"markdown","dbc313b6":"markdown","0a191e29":"markdown","66cfa8ea":"markdown","6be16f3d":"markdown","24befd88":"markdown","e6b1de8b":"markdown","467d4940":"markdown","0c6462a0":"markdown","b7f2fe73":"markdown","1afe5196":"markdown","4bc57d35":"markdown","dd53294f":"markdown","34209319":"markdown","850d0e48":"markdown"},"source":{"346ec188":"import numpy as np \n# large, multi-dimensional arrays and matrices, \n# along with a large collection of high-level mathematical functions to operate on these arrays.\nimport pandas as pd\n# data structures and operations for manipulating numerical tables and time series\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nfrom matplotlib.ticker import PercentFormatter\n# plotting\nimport plotly.express as px\n# graph\nimport plotly.graph_objects as go\n# graph\nimport seaborn as sns\n# t-test\nfrom scipy import stats\n# regression\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n# Word Cloud\nfrom wordcloud import WordCloud","d387f137":"data=pd.read_csv('..\/input\/data-engineer-jobs\/DataEngineer.csv')","d6c705ef":"data.head(2)","441f0ce4":"data.describe(include='all')","d6f393db":"# Check for missing values\ndef missing_values_table(df):\n    # number of missing values\n    mis_val = df.isnull().sum()\n    # % of missing values\n    mis_val_percent = 100 * mis_val \/ len(df)\n    # make table # axis '0' concat along index, '1' column\n    mis_val_table = pd.concat([mis_val,mis_val_percent],axis=1) \n    # rename columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0:'Missing Values',1:'% of Total Values'})\n    # sort by column\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n        '% of Total Values',ascending=False).round(1) #Review\n    print(\"Your selected datset has \"+str(df.shape[1])+\" columns and \"+str(len(df))+\" observations.\\n\"\n         \"There are \"+str(mis_val_table_ren_columns.shape[0])+\" columns that have missing values.\")\n    # return the dataframe with missing info\n    return mis_val_table_ren_columns\n\nmissing_values_table(data)","8c832cc9":"data['Easy Apply'].value_counts()","1fddabea":"data['Competitors'].value_counts()","25cd00a9":"# Replace -1 or -1.0 or '-1' to NaN\ndata=data.replace(-1,np.nan)\ndata=data.replace(-1.0,np.nan)\ndata=data.replace('-1',np.nan)","5cb11331":"missing_values_table(data)","96989231":"#Remove '\\n' from Company Name. \ndata['Company Name'],_=data['Company Name'].str.split('\\n', 1).str\n# 1st column after split, 2nd column after split (delete when '_')\n# string.split(separator, maxsplit) maxsplit default -1, which means all occurrances","434dfe86":"# Split salary into two columns min salary and max salary.\ndata['Salary Estimate'],_=data['Salary Estimate'].str.split('(', 1).str","48e6de96":"# Split salary into two columns min salary and max salary.\ndata['Min_Salary'],data['Max_Salary']=data['Salary Estimate'].str.split('-').str\ndata['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\ndata['Max_Salary']=data['Max_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n# lstrip is for removing leading characters\n# rstrip is for removing rear characters","651d56b9":"#Drop the original Salary Estimate column\ndata.drop(['Salary Estimate'],axis=1,inplace=True)","7956acc7":"# To estimate the salary with regression and other analysis, better come up with one number: Est_Salary = (Min_Salary+Max_Salary)\/2\ndata['Est_Salary']=(data['Min_Salary']+data['Max_Salary'])\/2","53a31b83":"# Create a variable for how many years a firm has been founded\ndata['Years_Founded'] = 2020 - data['Founded']","2e5294b1":"# A final look at the data before analysis\ndata.head(2)","54f2d555":"plt.figure(figsize=(13,5))\nsns.set() #style==background\nsns.distplot(data['Min_Salary'], color=\"b\")\nsns.distplot(data['Max_Salary'], color=\"r\")\n\nplt.xlabel(\"Salary ($'000)\")\nplt.legend({'Min_Salary':data['Min_Salary'],'Max_Salary':data['Max_Salary']})\nplt.title(\"Distribution of Min & Max Salary\",fontsize=19)\nplt.xlim(0,210)\nplt.xticks(np.arange(0, 210, step=10))\nplt.tight_layout()\nplt.show()","eef53101":"min_max_view = data.sort_values(['Min_Salary','Max_Salary'],ascending=True).reset_index(drop=True).reset_index()","96e59080":"f, (ax_box, ax_line) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\": (0.05,1)},figsize=(13,5))\nmean=min_max_view['Est_Salary'].mean()\nmedian=min_max_view['Est_Salary'].median()\n\nbpv = sns.boxplot(y='Est_Salary',data=min_max_view, ax=ax_box).set(ylabel=\"Est. Salary ($'000)\")\nax_box.axhline(mean, color='k', linestyle='--')\nax_box.axhline(median, color='y', linestyle='-')\n\nlp1 = sns.lineplot(x='index',y='Min_Salary',data=min_max_view, color='b')\nlp2 = sns.lineplot(x='index',y='Max_Salary',ax=ax_line,data=min_max_view, color='r')\nax_line.axhline(mean, color='k', linestyle='--')\nax_line.axhline(median, color='y', linestyle='-')\n\nplt.legend({'Min_Salary':data['Min_Salary'],'Max_Salary':data['Max_Salary'],'Mean':mean,'Median':median})\nplt.title(\"Salary Estimates of Each Engineer\",fontsize=19)\nplt.xlabel(\"Observations\")\nplt.tight_layout()\nplt.show()","101fd457":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Est_Salary'].mean()\nmedian=data['Est_Salary'].median()\n\nbph = sns.boxplot(data['Est_Salary'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Est_Salary'],ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,210)\nplt.xticks(np.arange(0,210,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","07a04e5f":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Years_Founded'].mean()\nmedian=data['Years_Founded'].median()\n\nbph = sns.boxplot(data['Years_Founded'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Years_Founded'],ax=ax_hist, color=\"g\").set(xlabel=\"Years_Founded\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,240)\nplt.xticks(np.arange(0,240,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","6d888291":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Rating'].mean()\nmedian=data['Rating'].median()\n\nbph = sns.boxplot(data['Rating'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp = sns.distplot(data['Rating'],ax=ax_hist, color=\"g\").set(xlabel=\"Ratings\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\nplt.xlim(0,6)\nplt.xticks(np.arange(0,6,step=1))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","e0b94e17":"# First I count the positions opened by the companies.\ndf_by_firm=data.groupby('Company Name')['Job Title'].count().reset_index().sort_values(\n    'Job Title',ascending=False).head(20).rename(columns={'Job Title':'Hires'})\n# When we reset the index, the old index is added as a column, and a new sequential index is used","cb0220d0":"# Merge with original data to get salary estimates.\nSal_by_firm = df_by_firm.merge(data,on='Company Name',how='left')","b339bbaf":"sns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Company Name',data=Sal_by_firm,ax=ax_bar, palette='Set2').set(ylabel=\"\")\nsns.pointplot(x='Est_Salary',y='Company Name',data=Sal_by_firm, join=False,ax=ax_point).set(\n    ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","7706c618":"df_by_city=data.groupby('Location')['Job Title'].count().reset_index().sort_values(\n    'Job Title',ascending=False).head(20).rename(columns={'Job Title':'Hires'})\nSal_by_city = df_by_city.merge(data,on='Location',how='left')","958c6cab":"sns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Location',data=Sal_by_city,ax=ax_bar, palette='Set2').set(ylabel=\"\")\nsns.pointplot(x='Est_Salary',y='Location',data=Sal_by_city, join=False,ax=ax_point).set(\n    ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","c18affbd":"data['City'],data['State'] = data['Location'].str.split(', ',1).str","cb863142":"data['State']=data['State'].replace('Arapahoe, CO','CO')","96e2529f":"stateCount = data.groupby('State')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nstateCount = stateCount.merge(data, on='State',how='left')","b60f9707":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='State',data=stateCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='State',data=stateCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","aa12caad":"data['HQCity'],data['HQState'] = data['Headquarters'].str.split(', ',1).str","dbf26a99":"data['HQState']=data['HQState'].replace('NY (US), NY','NY')","19730380":"HQCount = data.groupby('HQState')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).head(20).reset_index(drop=True)\nHQCount = HQCount.merge(data, on='HQState',how='left')","c5cf14a2":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='HQState',data=HQCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='HQState',data=HQCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","365ea77e":"RevCount = data.groupby('Revenue')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)","1d916697":"#Make the Revenue column clean\nRevCount[\"Revenue_USD\"]=['Unknown','10+ billion','100-500 million','50-100 million','2-5 billion','10-25 million','25-50 million','1-5 million','5-10 billion','<1 million','1-2 billion','0.5-1 billion','5-10 million']\n#Merge the new Revenue back to data\nRevCount2 = RevCount[['Revenue','Revenue_USD']]\nRevCount = RevCount.merge(data, on='Revenue',how='left')","58f1f213":"data=data.merge(RevCount2,on='Revenue',how='left')","568a2118":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Revenue_USD',data=RevCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Revenue_USD',data=RevCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","e4b4446e":"SizeCount = data.groupby('Size')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nSizeCount = SizeCount.merge(data, on='Size',how='left')","c231674a":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Size',data=SizeCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Size',data=SizeCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","4ebaaefb":"SecCount = data.groupby('Sector')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nSecCount = SecCount.merge(data, on='Sector',how='left')\nSecCount = SecCount[SecCount['Hires']>29]","db7f1710":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Sector',data=SecCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Sector',data=SecCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","a1d4da72":"OwnCount = data.groupby('Type of ownership')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\nOwnCount = OwnCount.merge(data, on='Type of ownership',how='left')","b7edfb12":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Type of ownership',data=OwnCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Type of ownership',data=OwnCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","6630fc4f":"# create a new dataset from original data\ntext_Analysis = data[['Job Title','Job Description','Est_Salary','Max_Salary','Min_Salary','City','State','Easy Apply','Revenue_USD','Rating','Size','Industry','Sector','Type of ownership','Years_Founded','Company Name','HQState']]\n# remove special characters and unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job Title'].str.upper().replace('[^A-Za-z0-9]+', ' ',regex=True)\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['\u00c2','AND ','WITH ','SYSTEMS','OPERATIONS','ANALYTICS','SERVICES','ENGINEERS','NETWORKS','GAMES','MUSICS','INSIGHTS','SOLUTIONS','JR ','MARKETS','STANDARDS','FINANCE','PRODUCTS','DEVELOPERS','SR '],\n    ['','','','SYSTEM','OPERATION','ANALYTIC','SERVICE','ENGINEER','NETWORK','GAME','MUSIC','INSIGHT','SOLUTION','JUNIOR ','MARKET','STANDARD','FINANCIAL','PRODUCT','DEVELOPER','SENIOR '],regex=True)","75ba2e91":"# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['BUSINESS INTELLIGENCE','INFORMATION TECHNOLOGY','QUALITY ASSURANCE','USER EXPERIENCE','USER INTERFACE','DATA WAREHOUSE','DATA ANALYST','DATA BASE','DATA QUALITY','DATA GOVERNANCE','BUSINESS ANALYST','DATA MANAGEMENT','REPORTING ANALYST','BUSINESS DATA','SYSTEM ANALYST','DATA REPORTING','QUALITY ANALYST','DATA ENGINEER','BIG DATA','SOFTWARE ENGINEER','MACHINE LEARNING','FULL STACK','DATA SCIENTIST','DATA SCIENCE','DATA CENTER','ENTRY LEVEL','NEURAL NETWORK','SYSTEM ENGINEER'],\n    ['BI','IT','QA','UX','UI','DATA_WAREHOUSE','DATA_ANALYST','DATABASE','DATA_QUALITY','DATA_GOVERNANCE','BUSINESS_ANALYST','DATA_MANAGEMENT','REPORTING_ANALYST','BUSINESS_DATA','SYSTEM_ANALYST','DATA_REPORTING','QUALITY_ANALYST','DATA_ENGINEER','BIG_DATA','SOFTWARE_ENGINEER','MACHINE_LEARNING','FULL_STACK','DATA_SCIENTIST','DATA_SCIENCE','DATA_CENTER','ENTRY_LEVEL','NEURAL_NETWORK','SYSTEM_ENGINEER'],regex=True)","c2f77fcf":"# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['DATA_ENGINEER JUNIOR','DATA_ENGINEER SENIOR','DATA  REPORTING_ANALYST'],\n    ['JUNIOR DATA_ENGINEER','SENIOR DATA_ENGINEER','DATA_REPORTING_ANALYST'],regex=True)","5a399d17":"jobCount=text_Analysis.groupby('Job_title_2')[['Job Title']].count().reset_index().rename(\n    columns={'Job Title':'Count'}).sort_values('Count',ascending=False)\njobSalary = text_Analysis.groupby('Job_title_2')[['Max_Salary','Est_Salary','Min_Salary']].mean().sort_values(\n    ['Max_Salary','Est_Salary','Min_Salary'],ascending=False)\njobSalary['Spread']=jobSalary['Max_Salary']-jobSalary['Est_Salary']\njobSalary=jobSalary.merge(jobCount,on='Job_title_2',how='left').sort_values('Count',ascending=False).head(20)","ee7bd2ed":"f, axs = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\":(1,0.5)},figsize=(13,8))\n\nax = axs[0]\nax.errorbar(x='Job_title_2',y='Est_Salary',data=jobSalary,yerr=jobSalary['Spread'],fmt='o')\nax.set_ylabel('Est. Salary ($\\'000)')\n\nax = axs[1]\nsns.barplot(x=jobSalary['Job_title_2'],y=jobSalary['Count']).set(xlabel=\"\")\n\nplt.xticks(rotation=65,horizontalalignment='right')\nplt.tight_layout()","a454f7c5":"# get top keywords\ns = text_Analysis['Job_title_2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS = s[s['Count']>29]\nS","467b5963":"# write get_keyword method\ndef get_keyword(x):\n   x_ = x.split(\" \")\n   keywords = []\n   try:\n      for word in x_:\n         if word in np.asarray(S['KW']):\n            keywords.append(word)\n   except:\n      return -1\n\n   return keywords","41822582":"# get keywords from each row\ntext_Analysis['KW'] = text_Analysis['Job_title_2'].apply(lambda x: get_keyword(x))","5fe2d607":"# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['KW'].apply(pd.Series).stack()).sum(level=0).replace(2,1)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True).replace(np.nan,0)","fd89b8ae":"# run t-test for top keywords to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests=[]\nfor word in text_columns:\n    if word in set(S['KW']):\n        ttest = stats.ttest_ind(text_Analysis[text_Analysis[word]==1]['Est_Salary'],\n                                     text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests.append([word,ttest])\n        \nttests = pd.DataFrame(ttests,columns=['KW','R'])\nttests['R']=ttests['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests['Statistic'],ttests['P-value']=ttests['R'].str.split(', ',1).str\nttests=ttests.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests","28f78feb":"# Selecting keywords with p-value <0.1 into multiple regression model.\nttest_pass = list(ttests[ttests['P-value'].astype(float)<0.1]['KW'])\nprint(*ttest_pass,sep=' + ')","93faef40":"TitleBar=ttests[ttests['P-value'].astype(float)<0.05]\nTitleBar['Statistic']=(TitleBar['Statistic'].astype(float)\/101)*100\nTitleBar=TitleBar.sort_values('Statistic',ascending=False).replace(\n    'ENGINEER','*OTHER*_ENGINEER').replace('_',' ',regex=True)\nTitleBar['KW']='\"' + TitleBar['KW'] + '\"'","3e52e9a8":"fig = plt.figure(figsize=(13, 5))\nsns.barplot(x='KW',y='Statistic',data=TitleBar).set(xlabel=\"\",ylabel=\"Salary Performance (%) \\n Against Average\")\n\nplt.xticks(rotation=45,horizontalalignment='right')","6f20364f":"# run regression\n# Remove variables with p-value >0.05 one by one until all <0.05\ntitleMod_final = ols(\"Est_Salary ~ SOFTWARE_ENGINEER + NETWORK + SECURITY + SYSTEM_ENGINEER + SYSTEM + SOFTWARE + MACHINE_LEARNING + ENGINEER + DATA_ENGINEER\",\n               data=text_Analysis).fit()\nprint(titleMod_final.summary())","867280d6":"# Plot with scatterplots\nfig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(titleMod_final,fig=fig)\nfig.tight_layout(pad=1.0)\n# Sorry somebody tell me how to remove that \"Partial Regression Plot\"","96f7d54c":"text_Analysis['Job_Desc2'] = text_Analysis['Job Description'].replace('[^A-Za-z0-9]+', ' ',regex=True)","a82af38a":"text_Analysis['Job_Desc2'] = text_Analysis['Job_Desc2'].str.upper().replace(\n    ['COMPUTER SCIENCE','ENGINEERING DEGREE',' MS ','BUSINESS ANALYTICS','SCRUM MASTER','MACHINE LEARNING',' ML ','POWER BI','ARTIFICIAL INTELLIGENCE',' AI ','ALGORITHMS','DEEP LEARNING','NEURAL NETWORK','NATURAL LANGUAGE PROCESSING','DECISION TREE','CLUSTERING','PL SQL'],\n    ['COMPUTER_SCIENCE','ENGINEERING_DEGREE',' MASTER ','BUSINESS_ANALYTICS','SCRUM_MASTER','MACHINE_LEARNING',' MACHINE_LEARNING ','POWER_BI','ARTIFICIAL_INTELLIGENCE',' ARTIFICIAL_INTELLIGENCE ','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','CLUSTER','PLSQL'],regex=True)","4f60c75a":"# Create a list of big data buzzwords to see if those words in JD would influence the salary\nbuzzwords = ['COMPUTER_SCIENCE','MASTER','MBA','SQL','PYTHON','R','PHD','BUSINESS_ANALYTICS','SAS','PMP','SCRUM_MASTER','STATISTICS','MATHEMATICS','MACHINE_LEARNING','ARTIFICIAL_INTELLIGENCE','ECONOMICS','TABEAU','AWS','AZURE','POWER_BI','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','REGRESSION','CLUSTER','ORACLE','EXCEL','TENSORFLOW','HADOOP','SPARK','NOSQL','SAP','ETL','API','PLSQL','MONGODB','POSTGRESQL','ELASTICSEARCH','REDIS','MYSQL','FIREBASE','SQLITE','CASSANDRA','DYNAMODB','OLTP','OLAP','DEVOPS','PLATFORM','NETWORK','APACHE','SECURITY']","daa7d3ae":"# Count the JD keywords.\nS2 = text_Analysis['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS2 = S2[S2['KW'].isin(buzzwords)].reset_index(drop=True)\n# .sort_values('Count',ascending=False)\nS2_TOP = S2[S2['Count']>29]\nS2_TOP_JD = S2_TOP\nS2_TOP_JD['KW'] = S2_TOP_JD['KW'] +'_JD'\nS2_TOP_JD","5cb40fa8":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(S2['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Keywords in Data Engineer Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","0a205845":"# write get_keyword method\ndef get_keyword(x):\n   x_ = x.split(\" \")\n   keywords = []\n   try:\n      for word in x_:\n         if word + '_JD' in np.asarray(S2_TOP_JD['KW']):\n            keywords.append(word + '_JD')\n   except:\n      return -1\n\n   return keywords","379728cc":"# get keywords from each row\ntext_Analysis['JDKW'] = text_Analysis['Job_Desc2'].apply(lambda x: get_keyword(x))","fcf22de5":"# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['JDKW'].apply(pd.Series).stack()).sum(level=0)\n# Since a JD sometimes repeat a keyword, the value may >1\n# But what we want to know is whether the appearance of the keyword impact the salary, not frequency\n# So values >1 have to be replaced by 1, but there must be a better way than coding like this \u2193\nkwdummy = kwdummy.replace([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,35,39],\n                         [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])","02896b13":"# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","8786913e":"# let's see if number of buzzwords contained or how wordy the JD is would have impact.\ntext_Analysis['JDKWlen']=text_Analysis['JDKW'].str.len()\ntext_Analysis['JDlen']=text_Analysis['Job Description'].str.len()","1f1f6f37":"# run t-test for top keywords to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests_JD=[]\nfor word in text_columns:\n    if word in set(S2_TOP_JD['KW']):\n        ttest2 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_JD.append([word,ttest2])\n\nttests_JD = pd.DataFrame(ttests_JD,columns=['KW','R'])\nttests_JD['R']=ttests_JD['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_JD['Statistic'],ttests_JD['P-value']=ttests_JD['R'].str.split(', ',1).str\nttests_JD=ttests_JD.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_JD","08fba34f":"JDBar=ttests_JD[(ttests_JD['P-value'].astype(float)<0.05)&(ttests_JD['Statistic'].astype(float)>0)]\nJDBar['Statistic']=(JDBar['Statistic'].astype(float)\/101)*100\nJDBar=JDBar.sort_values('Statistic',ascending=False).replace('_JD','',regex=True).replace('_',' ',regex=True)\nJDBar['KW']='\"' + JDBar['KW'] + '\"'","53f2186e":"fig = plt.figure(figsize=(13, 5))\nsns.barplot(x='KW',y='Statistic',data=JDBar).set(xlabel=\"\",ylabel=\"Salary Markup %\")\n\nplt.xticks(rotation=45,horizontalalignment='right')","8ac6b403":"#Selecting keywords with p-value <0.1 into multiple regression model.\nttest_JD_pass1 = list(ttests_JD[ttests_JD['P-value'].astype(float)<0.05]['KW'])\nprint(*ttest_JD_pass1,sep=' + ')","6568658c":"#Run regression and remove variables with p-value >0.05 one by one until all <0.05\nJDMod = ols(\"Est_Salary ~ CLUSTER_JD + COMPUTER_SCIENCE_JD + ALGORITHM_JD + PLATFORM_JD + PYTHON_JD + CASSANDRA_JD\",\n               data=text_Analysis).fit()\nprint(JDMod.summary())","94680606":"fig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(JDMod,fig=fig)\nfig.tight_layout(pad=1.0)","a0a6ad7e":"# create dummy columns by State\nkwdummy = pd.get_dummies(text_Analysis['State'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","08dfe7f8":"S3 = text_Analysis['State'].value_counts().reset_index().rename(\n    columns={'index':'State','State':'Count'})\nS3_Top = S3[S3['Count']>29]\nS3_Top","31d604cb":"#run t-test for top states hiring engineers to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests_state=[]\nfor word in text_columns:\n    if word in set(S3_Top['State']):\n        ttest3 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_state.append([word,ttest3])\n\nttests_state = pd.DataFrame(ttests_state,columns=['State','R'])\nttests_state['R']=ttests_state['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_state['Statistic'],ttests_state['P-value']=ttests_state['R'].str.split(', ',1).str\nttests_state=ttests_state.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_state","d70ea21f":"#Selecting states with p-value <0.1 into multiple regression model.\nttest_state_pass = list(ttests_state[ttests_state['P-value'].astype(float)<0.1]['State'])\nprint(*ttest_state_pass,sep=' + ')","3e1a4ffd":"StateMod = ols(\"Est_Salary ~ FL + CA + TX\",\n               data=text_Analysis).fit()\nprint(StateMod.summary())","f31cba52":"fig = plt.figure(figsize=(13, 13))\nfig = sm.graphics.plot_partregress_grid(StateMod,fig=fig)\nfig.tight_layout(pad=1.0)","02f41528":"text_Analysis['City']=text_Analysis['City'].str.replace(' ','_',regex=True)","c8409771":"S35 = text_Analysis['City'].value_counts().reset_index().rename(\n    columns={'index':'City','City':'Count'})\nS35_Top = S35[S35['Count']>29]","b729c45d":"# create dummy columns by City\nkwdummy = pd.get_dummies(text_Analysis[text_Analysis['City'].isin(np.asarray(S35_Top['City']))]['City'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","fd92b3cd":"#run t-test for top cities hring data engineers to see their correlation with salaries\ntext_columns = list(text_Analysis.columns)\nttests_city=[]\nfor word in text_columns:\n    if word in set(S35_Top['City']):\n        ttest35 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_city.append([word,ttest35])\n\nttests_city = pd.DataFrame(ttests_city,columns=['City','R'])\nttests_city['R']=ttests_city['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_city['Statistic'],ttests_city['P-value']=ttests_city['R'].str.split(', ',1).str\nttests_city=ttests_city.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_city","3a9edd4e":"#Selecting cities with p-value <0.1 into multiple regression model.\nttest_city_pass = list(ttests_city[ttests_city['P-value'].astype(float)<0.1]['City'])\nprint(*ttest_city_pass,sep=' + ')","d6414f49":"CityMod = ols(\"Est_Salary ~ Irving + Jacksonville + Houston + Fort_Worth + San_Jose + San_Diego + Los_Angeles + San_Antonio + Sunnyvale\",\n               data=text_Analysis).fit()\nprint(CityMod.summary())","7babffd9":"S31 = text_Analysis['HQState'].value_counts().reset_index().rename(\n    columns={'index':'HQState','HQState':'Count'}).replace(0,'Unknown_State')\nS31_Top = S31[S31['Count']>29]\nS31_Top['HQState_HQ'] = [s + '_HQ' for s in S31_Top['HQState']]","69f5c640":"# create dummy columns by HQ State\nkwdummy = pd.get_dummies(S31_Top['HQState_HQ'].apply(pd.Series).stack()).sum(level=0)\nS31_Top2 = S31_Top.merge(kwdummy,left_index=True,right_index=True,how='left').drop(['Count'],axis=1)\ntext_Analysis = text_Analysis.merge(S31_Top2,on='HQState',how='left').replace(np.nan,0)","149d5eef":"text_columns = list(text_Analysis.columns)\nttests_HQstate=[]\nfor word in text_columns:\n    if word in set(S31_Top['HQState_HQ']):\n        ttest31 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_HQstate.append([word,ttest31])\n\nttests_HQstate = pd.DataFrame(ttests_HQstate,columns=['HQState_HQ','R'])\nttests_HQstate['R']=ttests_HQstate['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_HQstate['Statistic'],ttests_HQstate['P-value']=ttests_HQstate['R'].str.split(', ',1).str\nttests_HQstate=ttests_HQstate.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_HQstate","56929d68":"ttest_HQstate_pass = list(ttests_HQstate[ttests_HQstate['P-value'].astype(float)<0.1]['HQState_HQ'])\nprint(*ttest_HQstate_pass,sep=' + ')","a6ff34bd":"HQStateMod = ols(\"Est_Salary ~ FL_HQ + TX_HQ + CA_HQ\",\n               data=text_Analysis).fit()\nprint(HQStateMod.summary())","3f2d5b99":"#Remove special characters.\ntext_Analysis['Revenue_USD'] = text_Analysis['Revenue_USD'].replace('[^A-Za-z0-9]+', '_',regex=True).replace(['_1_million','Unknown','5_10_billion'],['Small_Business','RevUnknown','Large_Corp'])\ntext_Analysis['Size'] = text_Analysis['Size'].replace('[^A-Za-z0-9]+', '_',regex=True).replace(['51_to_200_employees','10000_employees'],['SMB','Giant']).replace('Unknown','SizeUnknown')\ntext_Analysis['Sector'] = text_Analysis['Sector'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','SectorUnknown').replace(['Government','Unknown'],['GovSec','SectorUnknown'])\ntext_Analysis['Industry'] = text_Analysis['Industry'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','IndUnknown')\ntext_Analysis['Type of ownership'] = text_Analysis['Type of ownership'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','OwnUnknown')","ce44fc70":"#Rename column name for running regression later.\ntext_Analysis = text_Analysis.rename(columns={\"Easy Apply\":\"Easy_Apply\"})","8faacfb2":"# create dummy columns by Revenue\nkwdummy = pd.get_dummies(text_Analysis['Revenue_USD'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","df3ea986":"S4 = text_Analysis['Revenue_USD'].value_counts().reset_index().rename(\n    columns={'index':'Revenue_USD','Revenue_USD':'Count'})\nS4_Top = S4[S4['Count']>29]\nS4_Top","6396712d":"#run t-test to see the salary differences by companies' revenue.\ntext_columns = list(text_Analysis.columns)\nttests_rev=[]\nfor word in text_columns:\n    if word in set(S4_Top['Revenue_USD']):\n        ttest4 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_rev.append([word,ttest4])\n\nttests_rev = pd.DataFrame(ttests_rev,columns=['Revenue_USD','R'])\nttests_rev['R']=ttests_rev['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_rev['Statistic'],ttests_rev['P-value']=ttests_rev['R'].str.split(', ',1).str\nttests_rev=ttests_rev.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_rev","c7d253d3":"#Selecting revenues with p-value <0.1 into multiple regression model.\nttest_rev_pass = list(ttests_rev[ttests_rev['P-value'].astype(float)<0.1]['Revenue_USD'])\nprint(*ttest_rev_pass,sep=' + ')","d829b0c9":"kwdummy = pd.get_dummies(text_Analysis['Size'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","50a02ae1":"S5 = text_Analysis['Size'].value_counts().reset_index().rename(\n    columns={'index':'Size','Size':'Count'})\nS5_Top = S5[S5['Count']>29]\nS5_Top","056896a2":"text_columns = list(text_Analysis.columns)\nttests_size=[]\nfor word in text_columns:\n    if word in set(S5_Top['Size']):\n        ttest5 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_size.append([word,ttest5])\n\nttests_size = pd.DataFrame(ttests_size,columns=['Size','R'])\nttests_size['R']=ttests_size['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_size['Statistic'],ttests_size['P-value']=ttests_size['R'].str.split(', ',1).str\nttests_size=ttests_size.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_size","d033da46":"ttest_size_pass = list(ttests_size[ttests_size['P-value'].astype(float)<0.1]['Size'])\nprint(*ttest_size_pass,sep=' + ')","ef942151":"kwdummy = pd.get_dummies(text_Analysis['Sector'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","18a81173":"S6 = text_Analysis['Sector'].value_counts().reset_index().rename(\n    columns={'index':'Sector','Sector':'Count'})\nS6_Top = S6[S6['Count']>29]\nS6_Top","c9e630ea":"text_columns = list(text_Analysis.columns)\nttests_sec=[]\nfor word in text_columns:\n    if word in set(S6_Top['Sector']):\n        ttest6 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_sec.append([word,ttest6])\n\nttests_sec = pd.DataFrame(ttests_sec,columns=['Sector','R'])\nttests_sec['R']=ttests_sec['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_sec['Statistic'],ttests_sec['P-value']=ttests_sec['R'].str.split(', ',1).str\nttests_sec=ttests_sec.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_sec","c397cca1":"ttest_sec_pass = list(ttests_sec[ttests_sec['P-value'].astype(float)<0.1]['Sector'])\nprint(*ttest_sec_pass,sep=' + ')","cce2d349":"kwdummy = pd.get_dummies(text_Analysis['Type of ownership'].apply(pd.Series).stack()).sum(level=0)\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","879df787":"S8 = text_Analysis['Type of ownership'].value_counts().reset_index().rename(\n    columns={'index':'Type_of_ownership','Type of ownership':'Count'})\nS8_Top = S8[S8['Count']>29]\nS8_Top","f8225fd4":"text_columns = list(text_Analysis.columns)\nttests_own=[]\nfor word in text_columns:\n    if word in set(S8_Top['Type_of_ownership']):\n        ttest8 = stats.ttest_ind(text_Analysis[text_Analysis[word]>0]['Est_Salary'],\n                                 text_Analysis[text_Analysis[word]==0]['Est_Salary'])\n        ttests_own.append([word,ttest8])\n\nttests_own = pd.DataFrame(ttests_own,columns=['Type_of_ownership','R'])\nttests_own['R']=ttests_own['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\nttests_own['Statistic'],ttests_own['P-value']=ttests_own['R'].str.split(', ',1).str\nttests_own=ttests_own.drop(['R'],axis=1).sort_values('P-value',ascending=True)\nttests_own","f397df43":"ttest_own_pass = list(ttests_own[ttests_own['P-value'].astype(float)<0.1]['Type_of_ownership'])\nprint(*ttest_own_pass,sep=' + ')","c45e3a3d":"ModC = ols(\"Est_Salary ~ FL + CA + TX + CASSANDRA_JD + ENGINEER + Irving + Houston + Fort_Worth + San_Diego + San_Antonio\",\n               data=text_Analysis).fit()\n# Rating, Years_Founded, Easy_Apply, PHD\/Master, Sector, Size, Type_of_ownership not significant\nprint(ModC.summary())","7b08bcc3":"# Trying different interaction terms.\ntext_Analysis['CA_CA_HQ']=text_Analysis['CA']*text_Analysis['CA_HQ']\ntext_Analysis['PYTHON_CASSANDRA']=text_Analysis['PYTHON_JD']*text_Analysis['CASSANDRA_JD']\ntext_Analysis['CA_PYTHON']=text_Analysis['CA']*text_Analysis['PYTHON_JD']\ntext_Analysis['CA_CASSANDRA']=text_Analysis['CA']*text_Analysis['CASSANDRA_JD']\ntext_Analysis['ENGINEER_FL']=text_Analysis['ENGINEER']*text_Analysis['FL']\ntext_Analysis['ENGINEER_CA']=text_Analysis['ENGINEER']*text_Analysis['CA']\ntext_Analysis['ENGINEER_TX']=text_Analysis['ENGINEER']*text_Analysis['TX']\ntext_Analysis['ENGINEER_CA_HQ']=text_Analysis['ENGINEER']*text_Analysis['CA_HQ']\ntext_Analysis['ENGINEER_PYTHON']=text_Analysis['ENGINEER']*text_Analysis['PYTHON_JD']\ntext_Analysis['ENGINEER_CASSANDRA']=text_Analysis['ENGINEER']*text_Analysis['CASSANDRA_JD']\ntext_Analysis['ENGINEER_Irving']=text_Analysis['ENGINEER']*text_Analysis['Irving']\ntext_Analysis['ENGINEER_Houston']=text_Analysis['ENGINEER']*text_Analysis['Houston']\ntext_Analysis['ENGINEER_Fort_Worth']=text_Analysis['ENGINEER']*text_Analysis['Fort_Worth']\ntext_Analysis['ENGINEER_San_Antonio']=text_Analysis['ENGINEER']*text_Analysis['San_Antonio']","88a85762":"# Final model considering interaction terms.\nModC = ols(\"Est_Salary ~ FL + CA + TX + CASSANDRA_JD + ENGINEER + Irving + Houston + Fort_Worth + San_Diego + San_Antonio\",\n               data=text_Analysis).fit()\n# Rating, Years_Founded, Easy_Apply, PHD, Sector, Size, Type_of_ownership not significant\nprint(ModC.summary())","ea5c10b1":"fig = plt.figure(figsize=(13, 26))\nfig = sm.graphics.plot_partregress_grid(ModC,fig=fig)\nfig.tight_layout(pad=1.0)","608edd73":"# create a separate dataset for CA\ndata_CA = data[data['State']=='CA']","d3febe5b":"pd.set_option('display.max_columns', None)\ndata_CA.describe(include='all')","5475f034":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,5))\nmean=data['Est_Salary'].mean()\nmedian=data['Est_Salary'].median()\n\nbph = sns.boxplot(data['Est_Salary'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndp1 = sns.distplot(data_CA['Est_Salary'],ax=ax_hist, color=\"r\").set(xlabel=\"Est. Salary ($'000)\")\ndp2 = sns.distplot(data['Est_Salary'],ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (All)':mean,'Median (All)':median,'California':data_CA['Est_Salary'],'All':data['Est_Salary']})\nplt.xlim(0,210)\nplt.xticks(np.arange(0,210,step=10))\nplt.tight_layout() #Adjust the padding between and around subplots\nplt.show()","648ce013":"# Create a table for heatmap of number of companies with different sizes and revenues\nFirm_Size = data.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create a table for heatmap of number of companies with different sizes and revenues in CA\nFirm_Size_CA = data_CA.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size_CA = Firm_Size_CA[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_CA = Firm_Size_CA.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_CA = Firm_Size_CA.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create table for heatmap of salaries by companies with different sizes and revenues\nFirm_Size_Sal = data.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\n# Create table for heatmap of salaries by companies with different sizes and revenues in CA\nFirm_Size_CA_Sal = data_CA.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_CA_Sal = Firm_Size_CA_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_CA_Sal = Firm_Size_CA_Sal.reindex([11,2,9,4,7,10,5,0,1,6,8,3,12])\nFirm_Size_CA_Sal = Firm_Size_CA_Sal.set_index('Revenue_USD').replace(np.nan,0)","6db32493":"f, axs = plt.subplots(nrows=2,ncols=2, sharey=True,sharex=True, figsize=(13,9))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0,0]).set(title=\"Number of Companies in the US\",xlabel=\"\")\nfsc = sns.heatmap(Firm_Size_CA,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0,1]).set(title=\"Number of Companies in CA\",xlabel=\"\",ylabel=\"\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1,0]).set(title=\"Avg. Salaries in the US\")\nfscs = sns.heatmap(Firm_Size_CA_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1,1]).set(title=\"Avg. Salaries in CA\",ylabel=\"\")\n\nplt.setp([a.get_xticklabels() for a in axs[1,:]],rotation=45,ha='right')\nplt.tight_layout()\nplt.show()","a483324d":"ca_sal_by_firm = data_CA.groupby('Company Name')[['Est_Salary']].mean().reset_index()","3b76224d":"SmallHighPay = data_CA[((data_CA['Revenue_USD']=='5-10 million')|(data_CA['Revenue_USD']=='10-25 million')|(data_CA['Revenue_USD']=='25-50 million'))&(\n    data_CA['Size']=='51 to 200 employees')]['Company Name'].value_counts().reset_index().rename(\n    columns={'index':'Company Name','Company Name':'Hires'})","fb45f4df":"SmallHighPay = SmallHighPay.merge(ca_sal_by_firm, on='Company Name',how='left')\nSmallHighPay = SmallHighPay.merge(data_CA[['Company Name','Rating','Headquarters','Type of ownership','Industry','Sector','Years_Founded','Competitors']], on='Company Name',how='left')\nSmallHighPay = SmallHighPay.drop_duplicates().reset_index(drop=True)\nSmallHighPay","379a0400":"SmallHighPay.describe(include='all')","ef114bfd":"MLHighPay = data_CA[((data_CA['Revenue_USD']=='50-100 million')|(\n    data_CA['Revenue_USD']=='100-500 million')|(\n    data_CA['Revenue_USD']=='0.5-1 billion'))&(\n    data_CA['Size']=='1001 to 5000 employees')]['Company Name'].value_counts().reset_index().rename(\n    columns={'index':'Company Name','Company Name':'Hires'})","08987728":"MLHighPay = MLHighPay.merge(ca_sal_by_firm, on='Company Name',how='left')\nMLHighPay = MLHighPay.merge(data_CA[['Company Name','Rating','Headquarters','Type of ownership','Industry','Sector','Years_Founded','Competitors']], on='Company Name',how='left')\nMLHighPay = MLHighPay.drop_duplicates().reset_index(drop=True)\nMLHighPay","da6d76b0":"MLHighPay.describe(include='all')","ee98a8cd":"RevCountCA = data_CA.groupby('Revenue_USD')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nRevCountCA = RevCountCA.merge(data_CA, on='Revenue_USD',how='left')","7279af22":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Revenue_USD',data=RevCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Revenue_USD',data=RevCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","e691042c":"SizeCountCA = data_CA.groupby('Size')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nSizeCountCA = SizeCountCA.merge(data_CA, on='Size',how='left')","751fdd2f":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Size',data=SizeCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Size',data=SizeCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","c0901782":"SecCountCA = data_CA.groupby('Sector')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).head(12).reset_index(drop=True)\nSecCountCA = SecCountCA.merge(data_CA, on='Sector',how='left')","6de3e364":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Sector',data=SecCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Sector', join=False,data=SecCountCA,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","65056ec7":"OwnCountCA = data_CA.groupby('Type of ownership')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Count'}).sort_values(\n    'Count', ascending=False).reset_index(drop=True)\nOwnCountCA = OwnCountCA.merge(data_CA, on='Type of ownership',how='left')","c472df97":"sns.set(style=\"whitegrid\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Count',y='Type of ownership',data=OwnCountCA,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='Type of ownership',data=OwnCountCA, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\n\nplt.tight_layout()","459556c0":"## Hires and Salary Estimates by Job Location States","a89bc469":"## Who are those high-paying small firms in CA?","d166f8d1":"Another view the see the distribution of salary min, max, mean and median: \n* X-axis: the id(index) of all observations sorted by ascending order of min salaries.\n* There's 50% chance a data engineer's salary would be **at least** (minimum) higher than 75K.\n* There's 95% chance a data engineer's salary is within 40K-160K.","a09d5ed5":"### [Create Revenue Variables for Multiple Regression]","08dfd5ff":"# Upvote if you like my work!","5d302519":"# Key Insights\n### Job location is the top deciding factor of Data Engineers' salary variation in this dataset (R-Square 30%+)\nOverall average estimated salary of a data engineer is about USD 100K, and median is USD 97K, whereas a data engineer in California earns USD 128K, for both mean and median.\n![image.png](attachment:image.png)","c6379e03":"## Hires and Salary Estimates by Job Location Cities (Top 20)","eef9be42":"## Hires and Salary Estimates by Sector (Top 12)","c1d2c90d":"# [Heatmap] Number, Size and Salary of Hiring Companies (CA vs All)","195b9d38":"### Plenty of high-paid opportunities for Data Engineer are in San Diego. \nOne-third of California's Data Engineer jobs come from San Diego. Data Engineers are paid 33K higher than the average. Meanwhile Data Engineers in many Texas and Florida cities are highly undervalued.\n![image.png](attachment:image.png)","7f754b3d":"In CA, a dominant portion of data engineers positions are largely released by companies with revenues information unknown.","4a856fc8":"* By the modes of distribution, we can say Data Engineer's minimum salary is 55K and maximum 105K.\n* The Salaries distributions of Data Engineers are quite spread.","62f6cf80":"## Who are those high-paying medium-large businesses in CA?","fa7e7d82":"Again created a bar chart & error bar combo intended to see the salaries and counts, but not very effective (most sample sizes are under 30) as there are too many ways in presenting a position's name even some wordings are standardized. \n\n### Regression model may be a better approach: some certain keywords in job title\/desc may be correlated with salary pay.","97eb9296":"# More on California Data Engineer Salary (Sector & Type of Ownership)","ee05b212":"Now you can see there are lots of missing values in the dataset. Most positions don't support the Easy Apply function. Competitors are not identified for majority of the companies.","818f52c5":"### On Data Engineers' job titles, functional keywords matter much more than seniorities. These functional keywords on titles may be a clue in identifying high-paying Data Engineer positions.\n![image.png](attachment:image.png)","8b0809c5":"## Hires and Salary Estimates by Firms (Top 20)","788f4f03":"### 70% of Data Engineer jobs do not require Master\/PHD degrees. Experience\/skill\/knowledge in SQL, PYTHON, SPARK, AWS and Data Security are more required.\n![image.png](attachment:image.png)","e7e7e8a3":"## Correlation: Job Description vs Salary","9ae2880d":"## Distribution of Company Ages","e8f56723":"* The lines in the Salary chart represent 95% confidence interval, whereas points are point estimates.\n* Amazon was hiring most data engineers at by USD100K+.\n* Apple and Management Decisions, Inc. are with the highest est. salaries.\n* All sample sizes for those companies are lower than 30, so we'd better be conservative about the est. salaries by firms.","e7b12698":"## Data Cleaning","413a5553":"The regional difference (job location) is the most crucial factor to the salary variation.","d1f0d6ca":"Giant companies hire the most Data Engineers but they don't necessarily pay more.","2965d8f4":"* The seniorities are not relevant, but functional words (SOFTWARE, NETWORK...) are.\n* This model can explain less than 5% of variations in salaries.","b7248c46":"## More Variables: Revenue, Size, Sector, Industry and Type of Ownership","10c8b48f":"It's statistically significant that giants tend to pay 2K less and SMBs pay 2K more than average companies.","27617487":"It is unclear why 'Cluster' or 'Clustering' in JD would lead to lower salaries.","733ae57e":"Compared with that of the US as a whole, the salary distribution in CA shifts to the right, indicating overall higher salary payments.","ac2c3151":"* A big portion of source of data engineer hiring are from giants (10K+ employees & USD10B+ revenues).\n* There's high demand among financially unpublic firms (Revenue 'Unknown'). And they pay similar or even slightly higher salaries to data engineers than giants.\n* In California, Small firms (especially 51-200 employees & USD5-50M revenues) and medium-large businesses (1001-5000 employees & 50M-1B revenues unknown) pay more.\n* Companies in CA obviously pay more (27K+).","b8b40efe":"Data includes job title , salary estimation , job description , rating ,company name , location and many more ...\n\"Easy Apply\" should be the function that applicants can directly apply a job directly through 3rd party jobboard (e.g. Glassdoor, LinkedIn...) without logging into the hiring company's career site.","7377490b":"## Hires and Salary Estimates by Type of Ownership","ae2784aa":"### Characters of those high-paying medium-large businesses in CA \n* Half are private IT companies (1001-5000 employees & 50M-1B revenues unknown). \n* Avg. hires: 1.3; Avg. rating 4 (higher than ttl. avg. 3.5); Avg. company age: 29.5 (ttl. avg. 37); Avg. salary: 132K (ttl. avg. 100)","e9d54562":"### [Create Type of Ownership Variables for Multiple Regression]","879b91f5":"## Salary Distribution of All Data Engineers","47ccfc0f":"# About\n## Dataset\nThis dataset was created by [picklesueat](https:\/\/github.com\/picklesueat\/data_jobs_data) and contains more than 2000 job listing for data engineer positions (all assumed to be open positions at the time the dataset was published in July 2020), with features such as:\n\n* Salary Estimate\n* Location\n* Company Rating\n* Job Description\n  and more.\n\n## Objectives\n* What kind of Data Engineer jobs get higher salaries? (Job Title, Job Description, EasyApply)\n* What kind of companies pay more? (Rating, Company, Size, *Years established (now - Founded)*, Type of ownership, Industry & Sector, Revenue)\n* Does job\/headquarters location matter to salaries?\n\n## Methodologies\n\n1. Exploratory Data Analysis (distribution, boxplot, barcharts, errorbars, heatmaps, scatterplots...etc.)\n2. T-test\n3. Multiple Regression\n\n## Limitations and Assumptions\n\n* The results only reflet the outcome at the time the dataset was published, which is pressumed to be July 2020. Seasonal variation is disregarded (not a time-series data).\n* Somehow remote positions are not found in this dataset, so the impact of pandemic (more jobs becoming remote) on salary cannot be measured.\n* The salary estimates come from Glassdoor, which may not reflect the actual salaries.\n* The dataset is assumed to reflect the traits of the actual job market.\n* The salaries are nominal, not adjusted by living costs or consumer price index.","0800a4ba":"## Correlation: Job Location (City) vs Salary","8d99d3cd":"### Hires and Salary Estimates by Types of Ownership (CA)","208ef5b0":"## Correlation: Job Title Keywords vs Salary","934e73c7":"# Data Preparation","e5b61445":"* Regional difference (job location) is still the most deciding factor to salary variations.\n* Citywise, San Diego, CA is the best. In TX, only some cites' salaries are significantly lower, but some others such as Austin or Dallas are not.\n* Data Engineer with Apache Cassandra experience\/knowledge get higer pay.\n* If the position name does not directly contain 'Data Engineer', the salaries tend to be 4K lower.","0ccabbec":"I want to know the companies actively hiring Data Engineers and the estimated salaries they offer.","db03a18e":"### [Create Sector Variables for Multiple Regression]","50565758":"Private companies' demand are higher, and the salary offers are comparable to public firms.","fbc76d53":"## Hires and Salary Estimates by Revenues (CA)","8e8415b8":"## Correlation: Job Location (State) vs Salary","3f5be0ab":"To have an overview on the number, size and salary of those hiring companies and compare the outcomes between CA and all US, I want to create a heatmap.","646a88f6":"## Import Libraries and Dataset","b9dcdc9c":"Revenue '0' are those NaN values replaced for making dummy columns and are to be ignored.","b6de661d":"\u2193The combined regression model before considering interaction terms.","e1059116":"### [Create Size Variables for Multiple Regression]","e804cf28":"## Hires and Salary Estimates by Headquarters Location (Top 20)","c369546b":"Focus only on Est. Salary(Avg. of Min & Max). Both mean and median are around 100K.","28009a1a":"Biotech and Pharmaceuticals sectors pay more.","5a5b1dc0":"* IT companies hire the most Data Engineers, followed by Business Services and Finance companies.\n* Healthcare-related Data Engineer jobs seem to have better pay.","ebfde9d7":"## Hires and Salary Estimates by Size","cba7e7f5":"Companies headquartered in CA pay more.","4adac4f1":"As some of the columns contains -1 or '-1.0' or '-1' etc . We need to clean this(This is kind of null values)","57803423":"### These experience\/skills\/knowledge may help boost your earnings as a Data Engineer.\n![image.png](attachment:image.png)","f7cb915b":"# Exploratory Analysis","7797ecfa":"### Hires and Salary Estimates by Sectors (CA)","879bd2dd":"* Cities in CA and TX hire the most Data Engineers. Citywise, New York and Chicago are also on top of the chart.\n* Salaries are higher in cities of CA.","691e3ddd":"### Larger firms do not necessarily pay more to Data Engineers.\nInstead, high demands with slightly higher salaries are found in clusters such as follows. Meanwhile, a big portion of data engineer positions are released from financially unpublic firms.\n* **California Small firms (especially 51-200 employees & USD5-50M revenues)** \n* **California medium-large businesses (1001-5000 employees & 50M-1B revenues unknown)**\n![image.png](attachment:image.png)","d91a2c83":"## Hires and Salary Estimates by Sizes (CA)","3c081c75":"\u2193Here's a series operations to create and clean a dataset to dissect texts in Job Titles and Descriptions","dbc313b6":"Size '0' are those NaN values replaced for making dummy columns and are to be ignored.","0a191e29":"# Deeper Look at California - Salary Distribution","66cfa8ea":"Not many states are hiring Data engineers. Though there are more opportunities in TX, CA offers much higher pay.","6be16f3d":"## Explore the Data","24befd88":"\u2193Preparing for visualisation","e6b1de8b":"## Hires and Salary Estimates by Job Titles","467d4940":"1,000+ (40%+) of Data Engineer jobs are released from financially unpublic or medium businesses.","0c6462a0":"# Regression Analysis","b7f2fe73":"## Distribution of Company Ratings","1afe5196":"## Hires and Salary Estimates by Revenue","4bc57d35":"## Correlation: HQ Location (State) vs Salary","dd53294f":"# Final Regression Model (California rocks!)","34209319":"### Characters of those high-paying small firms in CA\n* Most are private companies (51-200 employees & USD5-50M revenues). \n* More than half are IT companies.\n* Avg. hires: 1.4; Avg. rating 4 (higher than ttl. avg. 3.8); Avg. company age: 18.7 (ttl. avg. 37); Avg. salary: 138K (ttl. avg. 100)","850d0e48":"* Big portion of companies hiring Data Engineers are headquartered in California. \n* With such high level of demand, Data engineers' salaries are obviously undervalued.\n* We can also see foreign companies like India, Japan and UK."}}