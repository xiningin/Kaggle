{"cell_type":{"af6789ad":"code","2a6b34fd":"code","9e878ed2":"code","73547962":"code","3048fb12":"code","cbca3699":"code","572e06fb":"code","cbe22938":"code","8180c937":"code","46106c3f":"code","03be0cee":"code","dbb2368a":"code","18b38c64":"code","e7c11cf8":"markdown","5b1cc2e3":"markdown","88e768ff":"markdown","8a9752ae":"markdown","1144a50d":"markdown","311026bc":"markdown","d57f0a07":"markdown","2d7548e8":"markdown","b0f37d3f":"markdown","84030d89":"markdown","ad848284":"markdown","2c0cfa23":"markdown"},"source":{"af6789ad":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nlabels = ['t-shirt','trouser','pullover','dress','coat','sandal','shirt','sneaker','bag','ankle boot']\n\n# LOAD THE DATA\ntrain = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\n\n# PREPARE THE DATA FOR THE NEURAL NETWORK\nY_data = train[\"label\"]\nX_data = train.drop(labels = [\"label\"], axis = 1)\nX_data = X_data \/ 255.\nX_data = X_data.values.reshape(-1,28,28,1)\nY_data = to_categorical(Y_data, num_classes = 10)\n\nY_test = test[\"label\"]\nX_test = test.drop(labels = [\"label\"], axis = 1)\nX_test = X_test \/ 255.\nX_test = X_test.values.reshape(-1,28,28,1)\nY_test = to_categorical(Y_test, num_classes = 10)","2a6b34fd":"# SHOW SOME IMAGES\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,7.5))\nfor i in range(60):\n    plt.subplot(5, 12, i+1)\n    plt.imshow(X_data[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\nplt.subplots_adjust(wspace=-0.05, hspace=-0.05)\nplt.show()","9e878ed2":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    horizontal_flip=True,\n    zoom_range=0.10,\n    width_shift_range=0.1,\n    height_shift_range=0.1)","73547962":"image = X_data[0].reshape((1,28,28,1))\nlabel = Y_data[0].reshape((1,10))\nplt.figure(figsize=(15,5))\nfor i in range(30):\n    plt.subplot(3, 10, i+1)\n    image_augmented, label_augmented = datagen.flow(image,label).next()\n    plt.imshow(image_augmented[0].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\n    if i==9: image = X_data[1].reshape((1,28,28,1))\n    if i==19: image = X_data[2].reshape((1,28,28,1))\nplt.subplots_adjust(wspace=-0.05, hspace=-0.05)\nplt.show()","3048fb12":"from tensorflow.keras import layers\n\n# build convolutional neural networks\nnets = 10\nmodel = [0] *nets\n\nfor i in range(nets):\n    model[i] = keras.Sequential([\n        layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n\n        layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n        \n        layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, kernel_size=5, strides=2, padding='same', activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n        \n        layers.Flatten(),\n        layers.Dense(256, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4),\n        layers.Dense(10, activation='softmax')\n    ])\n\n    # COMPILE MODEL\n    model[i].compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\nmodel[0].summary()","cbca3699":"# TRAIN NETWORKS\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\nearlystopping = EarlyStopping(\n    monitor='val_loss',\n    patience=30,\n    restore_best_weights=True\n)\n\n#annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\nreduce_LR_plateau = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.5, \n    patience = 4)\n\nhistory = [0] *nets\nbatch_size = 64\n\nfor i in range(nets):\n    #do validation split here to change it for every net\n    X_train, X_validate, Y_train, Y_validate = train_test_split(X_data, Y_data, test_size=0.1)\n    history[i] = model[i].fit_generator(\n        datagen.flow(X_train,Y_train, batch_size=batch_size),\n        epochs=200,\n        steps_per_epoch=X_train.shape[0]\/\/batch_size,\n        validation_data=(X_validate, Y_validate),\n        callbacks=[earlystopping, reduce_LR_plateau],\n        verbose=0)\n    epochs = np.argmax(history[i].history['val_accuracy'])\n    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\"\n          .format(i+1, epochs+1, history[i].history['accuracy'][epochs], history[i].history['val_accuracy'][epochs]))","572e06fb":"# ENSEMBLE PREDICTIONS\nresults = np.zeros( (X_test.shape[0],10) )\nfor i in range(nets):\n    results = results + model[i].predict(X_test)\npredictions = results\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")","cbe22938":"# PREVIEW PREDICTIONS\nplt.figure(figsize=(15,6))\nfor i in range(40):\n    true_value=np.argmax(Y_test[i])\n    predicted_value=results[i]\n    \n    plt.subplot(4, 10, i+1)\n    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n    if predicted_value==true_value:\n        title = plt.title(\"p={0:d}\".format(predicted_value),y=1.0)\n        plt.setp(title, color='g')\n    else:\n        title = plt.title(\"p={0:d} t={1:d}\".format(predicted_value,true_value),y=1.0)\n        plt.setp(title, color='r')\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=0.5)\nplt.show()","8180c937":"# SHOW ENSEMBLE SCORE FOR WRONG PREDICTIONS (Random)\nfrom random import randrange\ndf_predictions = pd.DataFrame(data=predictions, index=range(int(predictions.size\/10)), columns=labels)\ndf_predictions = df_predictions[results.to_numpy()!=test[\"label\"]]\n\nplt.figure(figsize=(15,35))\nfor j in range(1, 5):\n    sample = df_predictions.sample()\n    i=sample.index.values[0]\n    plt.subplot(6, 2, 2*j-1)\n    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n    title = plt.title(\"predicted label= {0} \\nactual label= {1}\".format(labels[results[i]],labels[np.argmax(Y_test[i])]),y=1.0)\n    plt.setp(title, color='r')\n    plt.axis('off')\n    plt.subplot(6, 2, 2*j)\n    plt.bar(labels, predictions[i])\n    plt.xticks(rotation=45)\n\nplt.show()","46106c3f":"# SHOW TOP(HIGHEST CONFIDENCE FOR ANOTHER LABEL) WRONG PREDICTED\nplt.figure(figsize=(15,35))\nj=1\nfor i in df_predictions.nlargest(5,columns=labels).index.values:\n    plt.subplot(6, 2, 2*j-1)\n    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n    title = plt.title(\"predicted label= {0} \\nactual label= {1}\".format(labels[results[i]],labels[np.argmax(Y_test[i])]),y=1.0)\n    plt.setp(title, color='r')\n    plt.axis('off')\n    plt.subplot(6, 2, 2*j)\n    plt.bar(labels, predictions[i])\n    plt.xticks(rotation=45)\n    j+=1\n\nplt.show()","03be0cee":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nplt.figure(figsize=(14,8))\nsns.heatmap(data=confusion_matrix(test[\"label\"], results), xticklabels=labels, yticklabels=labels, annot=True, cmap='rocket_r', fmt='d')\nplt.yticks(rotation=0) ","dbb2368a":"from sklearn.metrics import classification_report\nprint(classification_report(test[\"label\"], results, target_names = labels))","18b38c64":"# SCORE ON TEST SET\nprint('Test accuracy:', (results.to_numpy()==test[\"label\"]).value_counts().get(True)\/results.size)","e7c11cf8":"# Ensemble score for some wrong predictions (choosen randomly)","5b1cc2e3":"# Fashion MNIST\n\nThis ensemble of CNNs predicts types of cothing items on the Fashion-MNIST dataset. The dataset contains the following categories:\n\n* 0 T-shirt\/top\n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot","88e768ff":"# Predictions on the test set","8a9752ae":"# Confusion Matrix\n\nThe confusion matrix shows the network has the most problems with predicting shirts(wrongly classifing them as t-shirt, pullover dress or coat) or one of those classes being predicted as a shirt. Intrestingly though the mistakes between those classes is clearly lower. The most often wronly predicted class is tshirts as shirts this can be explained that for some of these small images the difference between those classes can be impossible to distinguish.","1144a50d":"# Architectural Remarks\n\n* During the limited testing I did Adam seems to perform better then RMSprop (other optimizers were not tried).\n* Switching to padding same for every convolutional layer(and thereby being able to add another conv-conv-conv block) increased performance by >1-2%, even though total capacity is lower now before it was 128-128-128-256-256-256\n* The use of the ensemble learning increased the prediction by >0.5%.\n* There seems to be not much of a diffrence between using pooling layers and a convolution with stride 2 prediction wise, but convolution should be faster since we get rid of one calculation and its another trainable layer.\n\nTo further increase the performance more and better documented architecure expirements should be done.","311026bc":"# Build 12 Convolutional Neural Networks","d57f0a07":"# Ensemble Predictions","2d7548e8":"# Load and prepare Images","b0f37d3f":"# Train the Networks\n\nFor training EarlyStopping is used that is why Epoch is intentionally to high at 200. For the latest version of the notebook only 10% of validation data is used. Maybe it would be even better to train on the whole training data without validation, but then it would be no longer possible to use earlystopping and the learning rate would need to be reduced on a different metric or not at all.","84030d89":"# Accuracy score on the test set","ad848284":"# Ensemble Score for the images with the highes confidence for another label\n\nSome of these seem to be wrongly labeld \/ impossible to predict.","2c0cfa23":"# Generate more Images via Data Augmentiation"}}