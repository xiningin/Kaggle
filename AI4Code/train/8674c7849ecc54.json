{"cell_type":{"e1ab5597":"code","61e959f4":"code","edf85f1e":"code","60d3bec2":"code","621e3224":"code","05ebbbdf":"code","599b5ef7":"code","d5af6691":"code","54a8982b":"code","61c41c11":"code","094a61e6":"code","ca9a350f":"markdown","049a8c38":"markdown","07227f3d":"markdown","2042fa6a":"markdown","389681fa":"markdown"},"source":{"e1ab5597":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\nimport string\nimport re","61e959f4":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'","edf85f1e":"def read_append_return(filename, train_files_path = train_files_path, output = 'text'):\n    json_path = os.path.join(train_files_path, (filename + '.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\n\n\ndef text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","60d3bec2":"train_df['text'] = train_df['Id'].apply(read_append_return)","621e3224":"train_df.head()","05ebbbdf":"train_df.shape","599b5ef7":"import spacy\n\nnlp = spacy.load('en_core_web_sm')\n#nlp.max_length = 5000000\n\n# Console execution\n# low_ = int(input('low: ')) # Low end of the chunk\n# high_ = int(input('high: ')) # High end of the chunk\n\n#Kaggle kernel\nlow_ = 0\nhigh_ = 5\n\n\ndata = []\nfor ix in train_df.iloc[low_:high_,:].index:\n    df = train_df.loc[ix]\n    text = df.text\n    true_label = df.cleaned_label\n    if len(text) < nlp.max_length:\n        data.append((ix, text, true_label))","d5af6691":"texts = [x[1] for x in data]\nindexes = [x[0] for x in data]\nlabs = [x[2] for x in data]","54a8982b":"print('spacy pipeline')\n\nidx = 0\norgs = []\nfor doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    orgs.extend([[indexes[idx], ent.text, ent.start_char, ent.end_char, ent.label_, labs[idx]]for ent in doc.ents])\n    idx += 1\n    print(idx, end = '\\r')\n\nprint(len(orgs),'entities')","61c41c11":"labeled = []\nfor org in orgs:\n    if org[-1] in org[1].lower():\n        labeled.append(org + [1])\n    else:\n        labeled.append(org + [0])","094a61e6":"df = pd.DataFrame(labeled, columns = ['ix','phrase','start_char',\n                                           'end_char','ner_label','dataset_label',\n                                           'label_in_phrase'])\n\nprint('Entities dataframe shape:', df.shape)\n\noutput_file = 'ents_%s_%s.csv' % (low_,high_)\nprint('Output filename:', output_file)\ndf.to_csv(output_file)","ca9a350f":"We export the data into a csv files using the delimitators (variables *high_* and *low_*) for naming each file.","049a8c38":"Credits for the *read_append_return* function to [prashansdixit](https:\/\/www.kaggle.com\/prashansdixit)  at https:\/\/www.kaggle.com\/prashansdixit\/coleridge-initiative-eda-baseline-model.","07227f3d":"Hi everyone!\n\nThe following code allows you to extract all entities from the training data texts using Spacy's Named Entity Recognition (NER) tool. This process was designed with the purpose of producing inputs for a classification model that would break every text as a set of candidate entities that possibly refer to a dataset. \n\nWe label each entity using the 'clean_label' column provided by the competition: if clean_label is in the entity.text.lower() string, the entity is labeled with a 1 (dataset), else a 0 (not dataset).\n\nThe outputs of the classification model would be entities with their respect probability of actually being a dataset which, given a threshold for the predictions, could be postprocessed into something like *dataset_1|dataset_2|dataset_3.*\n\n\nSince the NER task is quite memory-intensive to be executed locally using all the data, this process was coded as a script that would work on chunks of the training set saving entities into different csv files corresponding to each chunk. Once run the script in the console, delimitators for the chunks come from user inputs prompts (*input()* function) in execution.","2042fa6a":"Using a NER spacy pipeline, we extract all entities from the selected texts. Starting and ending position of the entity's","389681fa":"Once the entities are extracted, we use the actual target variable (clean_label) to label each entity as a dataset (1) or not dataset (0)."}}