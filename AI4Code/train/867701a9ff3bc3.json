{"cell_type":{"3e7ee20d":"code","c6354807":"code","dfb47268":"code","e752db39":"code","77c8460a":"code","dd621830":"code","04b5e0c8":"code","58fe9799":"code","2084f848":"code","324c12ad":"code","94715d85":"code","b282909a":"code","5d611590":"code","a04a5198":"code","f71bf055":"code","e0c59219":"code","cc01a290":"code","c8c60d42":"code","5a6c2e75":"code","04f92990":"markdown","cb983cec":"markdown","dddf65c6":"markdown","690cff3f":"markdown","e47d5944":"markdown","898f52fc":"markdown","35c0da56":"markdown","33dc1667":"markdown","0c1de450":"markdown","42095d6f":"markdown","9c50f16b":"markdown","ebd4a904":"markdown","efc0f8c3":"markdown"},"source":{"3e7ee20d":"import pandas as pd\ntrain=pd.read_csv(\"\/kaggle\/input\/av-comp\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/av-comp\/test.csv\")\nsample=pd. read_csv (\"\/kaggle\/input\/av-comp\/sample.csv\")\ntrain.head() ","c6354807":"train=train.drop([\"id\"],axis=1)\ntest=test.drop([\"id\"],axis=1)\n\n \ntrain=train.drop(['age'],axis=1)\ntest=test.drop(['age'],axis=1)\n\ntrain=train.replace({'difficulty_level': {'easy' :0,'intermediate' :1,'hard':2,'vary hard' :3},\n                     'education' :{'No Qualification' :0,'Matriculation' :1,'High School Diploma' :2,'Bachelors' :3,'Masters':4}, \n                    'program_type':{'S':0,'T':1,'U':2,'V':3,'W':4,'X':5,'Y':6,'Z':7},\n                    'test_type':{'offline':0,'online':1},\n                    'gender':{'M':1,'F':0}, \n                    'is_handicapped':{'N':0,'Y':1} \n                    })\n\ntest=test.replace({'difficulty_level': {'easy' :0,'intermediate' :1,'hard':2,'vary hard' :3},\n                     'education' :{'No Qualification' :0,'Matriculation' :1,'High School Diploma' :2,'Bachelors' :3,'Masters':4}, \n                    'program_type':{'S':0,'T':1,'U':2,'V':3,'W':4,'X':5,'Y':6,'Z':7},\n                    'test_type':{'offline':0,'online':1},\n                    'gender':{'M':1,'F':0}, \n                    'is_handicapped':{'N':0,'Y':1} \n                    })\n\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\ncol=['trainee_id','gender','education','city_tier', 'trainee_engagement_rating'] \nX_tr=train[col]\nX_test=test[col] \nX_tr=imputer.fit_transform(X_tr)\nX_test=imputer.transform(X_test) \ntrain=train.join(pd.DataFrame({'imp':X_tr[:,- 1]}))\ntest=test.join(pd.DataFrame({'imp':X_test[:,- 1]}))\n\ntrain=train.drop(['trainee_engagement_rating'],axis=1)\ntest=test.drop(['trainee_engagement_rating'],axis=1)\n\nId=train.program_id.unique()\nfor i in range(len(Id)):\n    train=train.replace({Id[i]:i})\n    test=test.replace({Id[i]:i})\n","dfb47268":"train.head() ","e752db39":"\nfrom catboost import CatBoostClassifier,Pool\nfrom sklearn.model_selection import train_test_split\n","77c8460a":"train_y=train.is_pass\ntrain=train.drop([\"is_pass\"],axis=1)\nx_train,x_valid,y_train,y_valid=train_test_split(train,train_y,train_size =0.8)","dd621830":"cat_features=[0,1,3,6] \n#these are the indices of \"program_id\", \"program_type\",\n#\"test_id\", \"trainee_id\" ","04b5e0c8":"train_pool=Pool(train, train_y, cat_features=cat_features)\ntest_pool=Pool(test, cat_features =cat_features) ","58fe9799":"weight=[1, 0.43]","2084f848":"model = CatBoostClassifier(\n        iterations=5000,logging_level=\"Silent\",early_stopping_rounds=500,\n    use_best_model=True,custom_loss=[\"AUC\"], class_weights=weight, \n        eval_metric=\"AUC\")","324c12ad":"model.fit(  x_train,y_train, \n        cat_features=cat_features,\n        eval_set=(x_valid,y_valid),\n        plot=True \n        )","94715d85":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid, model.predict_proba(x_valid) [:,1]) ","b282909a":"feature_importances=model.get_feature_importance(train_pool) \nfeature_names=train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","5d611590":"import numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix \nCm=confusion_matrix(y_valid,model.predict(x_valid))\nplot_confusion_matrix(model,x_valid, y_valid, cmap=plt.cm.Blues ) ","a04a5198":"Cm","f71bf055":"final_model = CatBoostClassifier(\n        iterations=1500,logging_level=\"Silent\",class_weights=weight, \n        eval_metric=\"AUC\")\nfinal_model.fit(train, train_y, cat_features=cat_features )","e0c59219":"final_pred=final_model.predict_proba(test)[:,1]","cc01a290":"sample.is_pass=final_pred\nsample.to_csv(\"best_model.csv\",index=None)\n","c8c60d42":"#from imblearn.under_sampling import RandomUnderSampler\n#import warnings\n","5a6c2e75":"#rus = RandomUnderSampler(random_state=0, sampling_strategy=0.8)\n#train_,train_y_ = rus.fit_resample(train,train_y)","04f92990":"Since target has class imbalance, we will assign mOre priority\nTo minority class.","cb983cec":"This is my first kernel on kaggle. This is of a competition of Analyticsvidhya \nMachine learning competition, where I got 1st rank out of 1700.\nUsed simple catboost for this.\n\nPlease upvote if you like and Do point out mistakes or give suggestions.  ","dddf65c6":"Below, I tried stacking.\nBut didn't find too much improvement.\nI have  markdown thIs snippet so that I can figure out the right\nWay of stacking.\nBut didn't find too much improvement. I have markdown thIs snippet so \nthat I can figure out later the right Way of stacking.","690cff3f":"After some exploration I came up to some conclusion.\n1.Dataset split between train and test set was perfectly consistent.\nLike average and value counts of columns were same on test and train sets.\n\nIn the above code, first I dropped \"id\" column,which was actually string combined of trainee_id+test_id.\n\n\"Age\" column was having lots of missing values, so I decided to drop that.\n\nThen other categorical I replaced with integer values.\n\"education_level\", \"city_tier\", were ordinal. So did label them accordingly.\n\n\"trainee_engagement_rating\" was also having some missing values but very few. So did knn imputing\n\nLastly program_id was in string, so changed to arbitrary integers.\nBut for modelling I won't use these integers,instead I would pass feature imdices of this variable\nAlong with \"train_id\", \"test_id\". I  using catboost model so, catboost will\nDo encode them accordingly(catboost encoder, they use some statistics Internally). \n\n                           ","e47d5944":"drop=[\"is_handicapped\",\"test_type\", \"gender\"]\nx_train1=x_train.copy()\nx_valid1=x_valid.copy()\ntest1=test.copy()\n\nx_train1=x_train1.drop(drop,axis=1)\nx_valid1=x_valid1.drop(drop, axis=1)\ntest1=test1.drop(drop,axis=1)\n\nmodel1 = CatBoostClassifier(\n       iterations=3000,logging_level=\"Silent\",early_stopping_rounds=500,\n    task_type=\"GPU\", \n     \n       eval_metric=\"AUC\")\n\nmodel2 = CatBoostClassifier(\n       iterations=3000,logging_level=\"Silent\",early_stopping_rounds=500,\n    class_weights=[1, 0.43], \n    task_type=\"GPU\", \n       eval_metric=\"AUC\")\n\nmodel3= CatBoostClassifier(\n        iterations=3000,logging_level=\"Silent\",early_stopping_rounds=500,\n    task_type=\"GPU\", \n     \n        eval_metric=\"AUC\")\n\nmodel1.fit(\n        x_train,y_train, \n        cat_features=[0,1,3,6],\n   verbose=False )\n\nmodel2.fit(\n        x_train,y_train, \n        cat_features=[0,1,3,6] ,\n        verbose=False,)\n\nmodel3.fit(\n        x_train1,y_train, \n        cat_features=[0,1,3,5] ,\n    verbose=False,)\n\nx_val1=model1.predict_proba(x_valid)[:,1]\nx_test1=model1.predict_proba(test)[:,1]\n\nx_val2=model2.predict_proba(x_valid)[:, 1]\nx_test2=model2.predict_proba(test)[:, 1]\n\nx_val3=model3.predict_proba(x_valid1)[:, 1]\nx_test3=model3.predict_proba(test1)[:, 1]\n\nbase_x=pd.DataFrame({\"model1\":x_val1,\"model2\":x_val2,\"model3\":x_val3})\nbase_y=y_valid\n\ntest_x=pd.DataFrame({\"model1\":x_test1,\n                    \"model2\":x_test2,\n                     \"model3\":x_test3})\n\n        \n\nmeta= CatBoostClassifier(\n        iterations=3000,logging_level=\"Silent\",early_stopping_rounds=500,\n   task_type=\"GPU\", \n    \n        eval_metric=\"AUC\")\n                      \nmeta.fit(\n        base_x,base_y,\n    verbose=False )\n\nfinal_pred=meta.predict_proba(test_x)[:,1]","898f52fc":"From the above plot, in 1376 iterations We reached optima.\nAnd our \"AUC\" score is also quite high.Good enough to reach Top10. ","35c0da56":"Did some basic EDA to come up with some conclusions for modeling. ","33dc1667":"Finally,with above number of optimal iterations, I will train\nOn whole dataset","0c1de450":"Withhis submission, I reached 3rd place on public leaderboard. ","42095d6f":"[https:\/\/datahack.analyticsvidhya.com\/contest\/machine-learning-starter-program-hackathon\/#LeaderBoard]","9c50f16b":"As there dataset split was consistent across train and test set.\nI didn't tried cross validation scores.\nAs on submission there wasn't any subtle differences in leaderboard\nScore and validation score. ","ebd4a904":"I also tried hyperparameter Tuning for catboost but with no avail.\n","efc0f8c3":"For final submission,To get consistent score I took weighted\nAverages of some predictions. \nThose predictions were based on same catboost model and\nParameters but with some variations Which were-\nI did undersampling of majority class,\ndropped some column according to feature importance\nDidn't used class weights.\n\nIndividually they were good around 0.82 AUC. "}}