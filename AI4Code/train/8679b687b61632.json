{"cell_type":{"bb3d14f9":"code","a5550e5a":"code","4fa69de4":"code","0915437a":"code","59f03d38":"code","79eb0d62":"code","ee4eb072":"code","085b02ae":"code","ad0f5dd0":"code","b7555aa5":"code","408c5f7c":"code","2686aec0":"code","77ac9642":"code","5523fa9b":"code","48f98d02":"code","98e65786":"code","7220e15f":"code","9a4b790a":"code","c924cc87":"code","1a377ba7":"code","27f41753":"code","81211a49":"code","137640ca":"code","bcd85f8c":"code","d83be221":"code","aef80071":"code","b67286a6":"code","84c0d622":"code","088a479a":"code","2e47c153":"code","6c5d8fe2":"code","3df32bd3":"markdown"},"source":{"bb3d14f9":"#import neccesary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","a5550e5a":"#load data\nwine_data = pd.read_csv(\"..\/input\/redwinequality\/datasets_4458_8204_winequality-red.csv\")","4fa69de4":"#checking the datas\nwine_data.head()","0915437a":"#counting the frequency of each class\nwine_data.quality.value_counts()","59f03d38":"#check for any null values\nwine_data.isnull().sum()","79eb0d62":"wine_data.info()","ee4eb072":"#visualizing the data\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine_data)","085b02ae":"#grouping the data with new features\nbins = (2, 6, 8)\ngroup_names = ['bad', 'good']\nwine_data['quality_name'] = pd.cut(wine_data['quality'], bins = bins, labels = group_names)","ad0f5dd0":"wine_data.head()","b7555aa5":"#non numeric to numeric\nle = LabelEncoder()\nwine_data['quality_mark'] = le.fit_transform(wine_data.quality_name)","408c5f7c":"#check the value counts\nwine_data.quality_name.value_counts()","2686aec0":"#value count after encoding\nwine_data.quality_mark.value_counts()","77ac9642":"#checking for any outliers\nfor ingredients in wine_data.columns[:11]:\n    plt.title(ingredients)\n    sns.boxplot(x='quality_mark', y=ingredients, data=wine_data)\n    plt.show()","5523fa9b":"pd.plotting.scatter_matrix(wine_data.drop(['quality','quality_name','quality_mark'], axis=1), c=wine_data.quality_mark)","48f98d02":"#correlation between the datas\nsns.heatmap(wine_data.drop(['quality','quality_name'], axis=1),annot=True, cmap='YlGnBu')","98e65786":"#importing libraries for model building\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LinearRegression, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nX = wine_data.drop(['quality','quality_name','quality_mark'], axis=1)\ny = wine_data.quality_mark","7220e15f":"#splitting the data in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)","9a4b790a":"#considering the apis in one list\nclassifiers = [LinearRegression(), SVC(), LinearSVC(), SGDClassifier(penalty=None), RandomForestClassifier(n_estimators=100, max_depth=6)]","c924cc87":"#normalizing the datas\nst_scl = StandardScaler()\nX_train = st_scl.fit_transform(X_train)\nX_test = st_scl.fit_transform(X_test)","1a377ba7":"#fitting and testing the different models\ntrain_model = []\n\nfor classes in classifiers:\n    classes.fit(X_train, y_train)\n    train_score = classes.score(X_train, y_train)\n    test_score = classes.score(X_test, y_test)\n    train_model.append([classes, train_score, test_score])","27f41753":"#removing models with less trainning score\nfor train_score in train_model:\n    if train_score[1]<0.88:\n        train_model.remove(train_score)","81211a49":"#qualified models\ntrain_model","137640ca":"#For SVC\ntrain_model[0][0].fit(X_train, y_train)\ny_predict_svc = train_model[0][0].predict(X_test)\nprint(accuracy_score(y_test, y_predict_svc))\nprint(classification_report(y_test, y_predict_svc))","bcd85f8c":"#For LinearSVC\ntrain_model[1][0].fit(X_train, y_train)\ny_predict_lsvc = train_model[1][0].predict(X_test)\nprint(accuracy_score(y_test, y_predict_lsvc))\nprint(classification_report(y_test, y_predict_lsvc))","d83be221":"#For RandomForestClassifier\ntrain_model[2][0].fit(X_train, y_train)\ny_predict_rfc = train_model[2][0].predict(X_test)\nprint(accuracy_score(y_test, y_predict_rfc))\nprint(classification_report(y_test, y_predict_rfc))","aef80071":"#as SVC as close train and test accuracy so loss of data is minimum hence tunning hyperparameter in SVC\nparams = {\n    'C': list(np.linspace(0.1,2,20)),\n    'kernel':['linear', 'rbf'],\n    'gamma' :list(np.linspace(0.1,2,20))\n}\nsvc=SVC()\ngrid_svc = GridSearchCV(estimator=svc, param_grid=params, scoring='roc_auc', cv=5, refit=True, return_train_score=True)","b67286a6":"grid_svc.fit(X_train, y_train)","84c0d622":"grid_svc.best_params_","088a479a":"#using new parameters\nsvc_new = SVC(C=0.8999999999999999, gamma=1.0999999999999999, kernel='rbf')\nsvc_new.fit(X_train, y_train)\ny_predict = svc_new.predict(X_test)","2e47c153":"accuracy_score(y_test, y_predict)","6c5d8fe2":"print(classification_report(y_test, y_predict))","3df32bd3":"***Here we can see that the accuracy has increased by a slight margin***"}}