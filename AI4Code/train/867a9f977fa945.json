{"cell_type":{"8eab338a":"code","05b7057b":"code","9e2cef53":"code","068b326e":"code","80451624":"code","fdefd597":"code","65bededd":"code","d6da7d3d":"code","f8d26d2c":"code","be589a8c":"code","ee0938fd":"code","b81fc2c2":"code","28c9e41f":"code","c4d4772c":"code","77033492":"code","b1a6017d":"code","6cdf8130":"code","05485caf":"code","066b8459":"code","db1b53ee":"code","26e5a8c6":"code","9805e031":"code","e31ecb46":"code","978b4a0a":"code","c250d760":"code","e2bf94dc":"code","8a60bbc7":"markdown","2cb292fc":"markdown","70a5683f":"markdown","7d858c29":"markdown","65cd90f2":"markdown","39affdb3":"markdown","7a896e6d":"markdown","3e3c5dde":"markdown","efc4627e":"markdown"},"source":{"8eab338a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Any results you write to the current directory are saved as output.","05b7057b":"str = 'Bank is a financial institution.My friend works in a bank.He opened me a bank account.He even gave me a lift in his a car.'\nsentences = nltk.sent_tokenize(str.lower())\nfor sentence in sentences:\n    print(sentence)","9e2cef53":"sentences","068b326e":"\n\ndef removePunctuation(sentences):\n    \n    tt=\"\"\n    sentences2=[]\n    for x in sentences:\n        tokenizer = RegexpTokenizer('\\w+')\n        text2=tokenizer.tokenize(x)\n       \n        cnt=1\n        for x2 in text2:\n            if cnt==1:\n                tt+=x2\n                cnt=0\n            else:\n                tt+=\" \"+x2    \n        sentences2.append(tt)\n        tt=\"\"\n    return sentences2\n    \n","80451624":"sentences2=removePunctuation(sentences)\nsentences2","fdefd597":"\n\ndef stopWords(sentences2):\n    stop_words = set(stopwords.words(\"english\"))\n    context_tab=[]\n    for sentence in sentences2:\n        words = nltk.word_tokenize(sentence)\n        without_stop_words = [word for word in words if not word in stop_words]\n        context_tab.append(without_stop_words)\n    return context_tab \n    \n   ","65bededd":"context_tab =stopWords(sentences2)\ncontext_tab","d6da7d3d":"\n\ndef lemmatization(context_tab):\n    lemma=[]\n    wl=WordNetLemmatizer()\n    for x in context_tab:\n        m2=[]\n        for x2 in x:\n            x3=wl.lemmatize(x2,wn.VERB)\n            x3=wl.lemmatize(x3,wn.NOUN)\n            x3=wl.lemmatize(x3,wn.ADJ)\n            x3=wl.lemmatize(x3,wn.ADV)\n            m2.append(x3)\n        lemma.append(m2)\n    return lemma\n    \n    ","f8d26d2c":"lemma=lemmatization(context_tab)\nlemma\n","be589a8c":"def tagPos(lemma):\n    pos=[]\n    for n in lemma:\n        pos.append(nltk.pos_tag(n))\n    return pos    \nwordPos=tagPos(lemma)\nwordPos","ee0938fd":"\ndef wordOfInterest(pos):\n    wn_pos=['NN','VB','JJ','JJR','JJS','NNP','VBG','RB','VBD','VBP']\n\n    woi1=[]\n\n    for x in pos:\n        arr=[]\n        for y in x:\n            if y[1] in wn_pos:\n                arr.append(y)\n        woi1.append(arr) \n    woi=[]\n\n\n    for i in woi1:\n        arr2=[]\n        for j in i:\n\n            if j[1]=='VBD' or j[1]=='VB' or j[1]=='VBP':\n                tup=(j[0],'v')\n                arr2.append(tup)\n            elif j[1]=='VBG':\n                tup=(j[0],'n')\n                arr2.append(tup)\n            elif j[1]=='NN' or j[1]=='NNP':\n                tup=(j[0],'n')\n                arr2.append(tup)\n            elif j[1]== 'JJ' or j[1]=='JJR' or j[1]=='JJS':\n                tup=(j[0],'a')\n                arr2.append(tup)\n            elif j[1]=='RB':\n                tup=(j[0],'r')\n                arr2.append(tup)\n        woi.append(arr2)       \n            \n    return woi","b81fc2c2":"woi=wordOfInterest(wordPos)\nwoi","28c9e41f":"w=woi[0][1][0]\ntag=woi[0][1][1]\n\nsynsB=wn.synsets(w,pos=tag)\nprint(synsB)\n\nw=woi[0][0][0]\ntag=woi[0][0][1]\n\nfor i in wn.synsets(w,pos=tag):\n    print(i,i.definition(),i.examples())","c4d4772c":"\ndef sentence_similarity(sentence1WOI,exampleSen):\n    exampleSentence=[exampleSen]\n    examples=removePunctuation(exampleSentence)\n    examples=stopWords(examples)\n    examples=lemmatization(examples)\n    examples=tagPos(examples)\n    examples=wordOfInterest(examples)\n    \n    exsynset=[]\n\n    for i in examples:\n        for j in i:\n            if len(wn.synsets(j[0], j[1])) != 0:\n                exsynset.append(wn.synsets(j[0], j[1])[0])\n    \n   \n    score, count = 0.0, 0\n    score1, count1 = 0.0, 0\n    # For each word in the first sentence\n    for synset in sentence1WOI:\n        # Get the similarity value of the most similar word in the other sentence\n        best_score = max([synset.path_similarity(ss)!=None for ss in exsynset])\n \n        # Check that the similarity could have been computed\n        if best_score is not None:\n            score += best_score\n            count += 1\n \n    # Average the values\n    score \/= count\n    \n    for synset in exsynset:\n        # Get the similarity value of the most similar word in the other sentence\n        best_score = max([synset.path_similarity(ss)!=None for ss in sentence1WOI])\n \n        # Check that the similarity could have been computed\n        if best_score is not None:\n            score1 += best_score\n            count1 += 1\n \n    # Average the values\n    score1 \/= count1\n    return (score+score1)\/2\n    \n\n    \n    \n","77033492":"woi","b1a6017d":"hyponymB=[]\nhyponymW=[]","6cdf8130":"def WSD1(woi):\n    for aword in woi:\n        print(aword)\n        arr=[]\n        for j in aword:\n            arr.append(wn.synsets(j[0], j[1]))\n        from itertools import product\n        list1=arr[0]\n        for a in range (1,len(arr)):\n            print (a)\n            list1 = [(a,b) for a, b in product(arr[a], list1) ]        \n        print('Total combenation ',len(list1))\n        c=0\n        outcome=[]\n        for i in list1:\n            word_c=0\n            probability=0.0\n            for j in i:\n                val=[]\n                for e in j.examples():\n                    ar=[]\n                    for a in i:\n                        ar.append(a)\n                    \n                    val.append(sentence_similarity(ar,e))\n                    \n                if len(val)!=0:\n                    probability=probability+max(val)\n                    word_c=word_c+1\n                else:\n                    word_c=word_c+1\n            probability=probability\/word_c\n            #print(probability)\n            outcome.append((i,probability))\n        outcome = sorted(outcome, key=lambda tup: tup[1],reverse=True)\n        #print(outcome)\n        c=0\n        for i in outcome:\n            if i[1]==1:\n                for p in i[0]:\n                    print(p,p.definition())\n                print('#########################')\n                c=c+1\n        print('**************************************************************************************',c,'**********')\n        for i in outcome:\n            if i[1]==1:\n                hyponymB.append(i[0][0])\n                hyponymW.append(i[0][1])\n                \n\n\n    ","05485caf":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n  \n# X = input(\"Enter first string: \").lower()\n# Y = input(\"Enter second string: \").lower()\ndef cosineSimilarity(X,Y):\n    # tokenization\n    X_list = word_tokenize(X) \n    Y_list = word_tokenize(Y)\n\n    # sw contains the list of stopwords\n    sw = stopwords.words('english') \n    l1 =[];l2 =[]\n\n    # remove stop words from string\n    X_set = {w for w in X_list if not w in sw} \n    Y_set = {w for w in Y_list if not w in sw}\n\n    # form a set containing keywords of both strings \n    rvector = X_set.union(Y_set) \n    for w in rvector:\n        if w in X_set: l1.append(1) # create a vector\n        else: l1.append(0)\n        if w in Y_set: l2.append(1)\n        else: l2.append(0)\n    c = 0\n\n    # cosine formula \n    for i in range(len(rvector)):\n            c+= l1[i]*l2[i]\n    cosine = c \/ float((sum(l1)*sum(l2))**0.5)\n    return cosine","066b8459":"outcome=[]\ndef WSD(woi):\n    lineNo=0;\n    for line in woi:\n        wordSenseAr=[]\n        for word in line:\n            senseAr=[]\n            temp=[]\n            senseAr = wn.synsets(word[0], word[1])\n            for i in senseAr:\n                score=0\n                count=0\n                for j in i.examples():\n                    score=score+cosineSimilarity(j,sentences[lineNo])\n                    count=count+1\n                if count ==0:\n                    score=0\n                else:\n                    score = score\/count\n                temp.append((i,score))\n            temp = sorted(temp, key=lambda tup: tup[1],reverse=True)\n            wordSenseAr.append(temp[0:int(len(temp)\/2+1)])\n        outcome.append(wordSenseAr)\n    print(outcome)\n    print('\\n')\n\n\n\n                \n","db1b53ee":"WSD(woi)","26e5a8c6":"outcome","9805e031":"#each NODE\nclass Node:\n    endmark = False\n    context = None\n    def __init__(self):\n        self.nxt = []\n        self.endmark = False\n        for i in range(26):\n            self.nxt.append(None)\n\n##initiate tree root\n\n\n##inserting each word in tree\ndef insert(root,string,length,context):\n    curr = root\n    ##print('\\n')\n    for i in range(length):\n       ## print(string[i])\n        index = ord(string[i]) - ord('a')\n        if curr.nxt[index] is None:\n            curr.nxt[index] = Node()\n        curr = curr.nxt[index]\n    curr.endmark = True\n    curr.context = context\n    ##print(context)\n\n#insert('slope',5,'sloping land')\n\n#search for a word\ndef search(root,string,length):\n    curr = root\n    ##print('root')\n    for i in range(length):\n        index = index = ord(string[i]) - ord('a')\n        #print('index: ',index)\n        if curr.context is not None:\n            print(curr.context)\n        if curr.nxt[index] is None:\n            return False\n        curr = curr.nxt[index]\n       ## print(string[i])\n    ##print(curr.context)    \n    return curr.context","e31ecb46":"##input to context table:\ncontextTable = [\n['financial_institution.n.01',['deposite','money','lend','credit','finance',\n                          'institute','capital','market','reserve','payment','account','institution']],\n['vehicle.n.01',['car','engine','road','drive','wheel','lift']]\n]","978b4a0a":"root = Node()\n\nfor context in contextTable:\n    for word in context[1]:\n        insert(root,word,len(word),context[0])\n        \n\n\n    ","c250d760":"lineCount=0\nfor line in woi:\n    wordCount=0\n    for word in line:\n        ok=0\n        contextWord=[]\n        for line1 in range(0,len(woi)):\n            for word1 in range(0,len(woi[line1])):\n                if word==woi[line1][word1]:\n                    if(word1-1>=0):\n                        contextWord.append(woi[line1][word1-1])\n                    if (word1+1<len(woi[line1])):\n                        contextWord.append(woi[line1][word1+1])\n                    \n        print(word,'\\n',\"Context words ==>>\",contextWord)\n        for w in contextWord:\n            result=search(root,w[0],len(w[0]))\n            if result!= False:\n                if ok==0:\n                    ok=1\n                    maxx=-1\n                    j=None\n                    for sense in outcome [lineCount][wordCount]:\n                        #print(sense[0].definition(),sense[1])\n                        s=sense[0]\n                        gs=wn.synset(result)\n                        a=s.path_similarity(gs)\n                        b=gs.path_similarity(s)\n                        if a is None:\n                            a=0\n                        if b is None:\n                            b=0\n                        p=(a+b)\/2\n                       # \n                        p=p+sense[1]\n                        p=p\/2\n                        print(\"***\",sense[0],p)\n                        if(p>maxx):\n                            maxx=p\n                            j=s\n                    if j is not None:\n                        print(\">\",j,j.definition()) \n                        print('\\n')\n        if(ok==0):\n            if len(outcome[lineCount][wordCount])!= 0:\n                #outcome[lineCount][wordCount][0][0].definition()\n                print(\"==>\",wn.synsets(woi[lineCount][wordCount][0],woi[lineCount][wordCount][1])[0].definition())\n                print('\\n')\n                    \n        wordCount=wordCount+1\n    lineCount=lineCount+1\n                \n            \n            ","e2bf94dc":"from nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\nclass SimplifiedLesk:\n    \"\"\"Implement Lesk algorithm.\"\"\"\n\n    def __init__(self):\n        self.stopwords = set(stopwords.words('english'))\n\n    def disambiguate(self, word, sentence):\n        \"\"\"Return the best sense from wordnet for the word in given sentence.\n        Args:\n            word (string)       The word for which sense is to be found\n            sentence (string)   The sentence containing the word\n        \"\"\"\n        word_senses = wordnet.synsets(word)\n        best_sense = word_senses[0]  # Assume that first sense is most freq.\n        max_overlap = 0\n        context = set(word_tokenize(sentence))\n        for sense in word_senses:\n            signature = self.tokenized_gloss(sense)\n            overlap = self.compute_overlap(signature, context)\n            if overlap > max_overlap:\n                max_overlap = overlap\n                best_sense = sense\n        return best_sense\n\n    def tokenized_gloss(self, sense):\n        \"\"\"Return set of token in gloss and examples\"\"\"\n        tokens = set(word_tokenize(sense.definition()))\n        for example in sense.examples():\n            tokens.union(set(word_tokenize(example)))\n        return tokens\n\n    def compute_overlap(self, signature, context):\n        \"\"\"Returns the number of words in common between two sets.\n        This overlap ignores function words or other words on a stop word list\n        \"\"\"\n        gloss = signature.difference(self.stopwords)\n        return len(gloss.intersection(context))\n\n\n# Sample Driver code:\n\nsentence = (\"bank is a finantial institution\"\n            \"my friend works in a bank\"\n            \"he opened me a bank account\"\n            \"he even gave me a lift in his a car\"\n           )\nword = \"bank\"\nlesk = SimplifiedLesk()\narray=['bank','financial','institution','friend','work','open','account','even','give','lift','car']\n\nfor i in range(0,11):\n    print (\"Word :\", array[i])\n    print (\"Best sense: \", lesk.disambiguate(array[i], sentence).definition())\n    print ('')","8a60bbc7":"# **Lemmatization**","2cb292fc":"# Similerity","70a5683f":"# ** Removing punctuation**","7d858c29":"> ****Simplified Lesk****","65cd90f2":" # Sentence tokenization\n ","39affdb3":"# Parts of speech tagging","7a896e6d":"CONTEXT TABLE BY TREE","3e3c5dde":"# **Removing unnecessary words**","efc4627e":"# wordnet is divided into four total subnets such as\n\n                 1 Noun\n                 2 Verb\n                 3 Adjective\n                 4  Adverb\n                \n# woi= word of interest"}}