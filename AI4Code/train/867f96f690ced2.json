{"cell_type":{"2feb9c79":"code","1de171ba":"code","5c4f9256":"code","05d5d360":"code","d20a2f42":"code","48d53858":"code","6e281c36":"code","b09f4ede":"code","92fdd71f":"code","130aec07":"code","c54df134":"code","e00777c8":"code","74235bf3":"code","47e75bde":"code","5a89e125":"code","065e5b48":"code","48058671":"code","5d781eb6":"code","fca9278e":"code","be6f86fe":"code","32338eb6":"code","b3d79bee":"code","f51fd523":"code","30cecb53":"code","31265efb":"code","c505682c":"code","b98a2127":"code","40440e5a":"code","fb9dd186":"code","0ed8e291":"code","0a90de38":"code","21637bea":"code","7326c525":"code","1108d4c5":"code","7d12a0da":"code","81783d68":"code","f1d0ae36":"code","b0bf2dbc":"code","8affc21e":"code","dbe2f932":"code","1d77250d":"code","d380229e":"code","bc1649ba":"code","ca383a66":"code","b366c8a0":"code","36e3d026":"code","caa380c3":"code","209eff77":"code","b0c9e092":"code","76b81f59":"code","95219a50":"code","43235744":"code","c0d29943":"code","7c934477":"code","94f0810b":"code","fd86b376":"code","d06604ef":"code","e63cf3f4":"code","f3707c9f":"code","72c700a2":"code","2c4d9dd3":"code","13112686":"code","abf6dc7f":"code","bba79d1d":"code","db0beb47":"code","edb13fb9":"code","846ba7ab":"code","834d0ca9":"code","2faf1ca4":"code","e293fbb4":"code","af6edbc6":"code","e1a437d7":"code","2b5046d7":"code","7fd32d88":"code","f0ea7406":"code","edebbf34":"code","740cb6d3":"code","73675eb8":"code","53fefd01":"code","a38e11ef":"code","47ec9683":"code","de47d9bb":"code","0acb0ce2":"code","7249d14d":"code","8e803499":"code","53d6a72f":"code","0d7f95d5":"code","319e91ab":"code","d8351c79":"code","d554fcf8":"code","15dff8b0":"code","aa2f2e6c":"code","b8dd5977":"markdown","300b3ddc":"markdown","06f3d014":"markdown","4ca6f792":"markdown","c1f85c6d":"markdown","019cddd8":"markdown","f78833b2":"markdown","a0407ddd":"markdown","ad1c7a75":"markdown","70a03405":"markdown","9f2f74cb":"markdown","7eac9106":"markdown","c87143ef":"markdown","377c9181":"markdown","859da855":"markdown","ee8aa348":"markdown","777865e0":"markdown","6b20011e":"markdown","b16ca359":"markdown","83083af1":"markdown","2353d7db":"markdown","d7da354c":"markdown","7b30c652":"markdown","18f084e3":"markdown","589a0775":"markdown","d8a11917":"markdown","88773d1b":"markdown","fe495241":"markdown","f01a5d0b":"markdown","09dfbf25":"markdown","9a18f58e":"markdown","6da51781":"markdown","fd46f7ac":"markdown","7ad09f39":"markdown","8b9fce76":"markdown","aa3642b0":"markdown","b7d04a1f":"markdown","277ba527":"markdown","9f1fd5b7":"markdown","cfd438cb":"markdown","12c5dac0":"markdown","f0bb8bf7":"markdown","ff5299cb":"markdown","a25c8888":"markdown","6ab39d42":"markdown","f9c274ac":"markdown","838a561c":"markdown"},"source":{"2feb9c79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt #visualization\nplt.style.use('seaborn-whitegrid') #to change the style of the pyplot visualization \n#plt.style.available ====> to loook the all pyplot styles \n\nimport seaborn as sns #for visualization \nfrom collections import Counter # Look at it I do not know\n\nimport warnings \nwarnings.filterwarnings(\"ignore\") #do not show python warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1de171ba":"a=[1,2,3,4]\nplt.plot(a)\nplt.show()","5c4f9256":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","05d5d360":"train_df.columns","d20a2f42":"train_df.head()","48d53858":"train_df.describe()","6e281c36":"train_df.info()","b09f4ede":"\ndef bar_plot(variable):\n    \"\"\"\n    input: variable ==> example: \"sex\"\n    output: bar plot & value count \n    \n    \"\"\"\n    #get feature\n    var = train_df[variable]\n    varValue = var.value_counts() # value_count() is a pandas method to count uniqe features amount\n    \n    #visualization \n    plt.figure(figsize =(9,3))\n    plt.bar(varValue.index, varValue) # bar plot for categorical (x=> male , female and y=> amount of male,female)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency \")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","92fdd71f":"category1 = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\",\"Parch\"]\nfor c in category1:\n    bar_plot(c)","130aec07":"category2  = [\"Cabin\",\"Name\",\"Ticket\"] # spesific features are more complex to visualize\nfor c in category2:\n    bar_plot(c)","c54df134":"def plot_histogram(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins =50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequncy\")\n    plt.title(\"{} distrubition with histogram\".format(variable))\n    plt.show()","e00777c8":"numericVar = [\"Fare\",\"Age\",\"PassengerId\"]\nfor i in numericVar:\n    plot_histogram(i)","74235bf3":"# Pclass vs Survived\ntrain_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"],as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","47e75bde":"#Sex - Survived\ntrain_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"],as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","5a89e125":"train_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","065e5b48":"train_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],as_index = False).mean().sort_values(by = \"Survived\",ascending = False)","48058671":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1 \n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c]< Q1-outlier_step) | (df[c] > Q1+outlier_step)].index\n        # store indeces \n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","5d781eb6":"train_df.loc[detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","fca9278e":"# drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","be6f86fe":"# first of all we need to concat train_df and test_df to find and fill missing values\ntrain_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop =True)","32338eb6":"train_df.head()","b3d79bee":"train_df.columns[train_df.isnull().any()]","f51fd523":"train_df.isnull().sum()","30cecb53":"train_df[train_df[\"Embarked\"].isnull()]","31265efb":"# let's look at Fare feature to fill Embark\ntrain_df.boxplot(column=\"Fare\",by=\"Embarked\")\nplt.show()","c505682c":"# C's median is closer 80 Fare, so I will fill NaN values with C\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()] # we fill the Embarked NaN valeues with C","b98a2127":"train_df[train_df[\"Fare\"].isnull()]","40440e5a":"np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"]) # mean of the fare of the passanger whose pclass is 3","fb9dd186":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"]== 3][\"Fare\"]))","0ed8e291":"train_df[train_df[\"Fare\"].isnull()]","0a90de38":"list1= [\"SibSp\",\"Parch\",\"Age\",\"Fare\",\"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot = True,cmap=\"YlGnBu\", fmt = '.2f')\nplt.show()","21637bea":"g = sns.factorplot(x =\"SibSp\",y = \"Survived\",data = train_df,kind = \"bar\",size = 6)\ng.set_ylabels(\"Survived Probabailty\")\nplt.show()","7326c525":"g = sns.factorplot(x = \"Parch\",y =\"Survived\",kind = \"bar\",data =train_df,size = 6)\ng.set_ylabels(\"Survived probability\")\nplt.show()","1108d4c5":"g = sns.factorplot(x=\"Pclass\",y = \"Survived\",kind = \"bar\",data =train_df,size = 6)\ng.set_ylabels(\"Survival probability\")\nplt.show()","7d12a0da":"g = sns.FacetGrid(train_df,col = \"Survived\")\ng.map(sns.distplot,\"Age\",bins = 25)\nplt.show()","81783d68":"g = sns.FacetGrid(train_df,col = \"Survived\",row=\"Pclass\",size = 3)\ng.map(plt.hist,\"Age\",bins = 25)\ng.add_legend()\nplt.show()","f1d0ae36":"g = sns.FacetGrid(train_df,row = \"Embarked\",size=2)\ng.map(sns.pointplot,\"Pclass\",\"Survived\",\"Sex\")\ng.add_legend()\nplt.show()","b0bf2dbc":"g =sns.FacetGrid(train_df,row=\"Embarked\",col=\"Survived\",size =2.3)\ng.map(sns.barplot,\"Sex\",\"Fare\")\ng.add_legend()\nplt.show()","8affc21e":"train_df[train_df[\"Age\"].isnull()]","dbe2f932":"# we start with sex feature\nsns.factorplot(x = \"Sex\",y =\"Age\",data =train_df,kind = \"box\")\nplt.show()","1d77250d":"sns.factorplot(x=\"Sex\",y=\"Age\",hue = \"Pclass\",data =train_df,kind = \"box\")\nplt.show()","d380229e":"sns.factorplot(x=\"Parch\",y =\"Age\",data=train_df,kind =\"box\")\nsns.factorplot(x=\"SibSp\",y =\"Age\",data=train_df,kind =\"box\")\n\nplt.show()","bc1649ba":"# is there a corelation among these features? \nsns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),annot = True)\nplt.show()","ca383a66":"# we can not see sex feature because of it is object(male and female).\n# that's why we need to conver it into numerical \n# we need to do list comprehension\ntrain_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]]","b366c8a0":"# now let's correlate features \nsns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),annot = True)\nplt.show()","36e3d026":"# we fill NaN values in the Age\nindex_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\n# index_nan_age\nfor i in index_nan_age:\n    age_prediciton = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"]) & (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_median = train_df[\"Age\"].median()\n    if not np.isnan(age_prediciton):\n        train_df[\"Age\"].iloc[i] = age_prediciton\n    else:\n        train_df[\"Age\"].iloc[i] = age_median","caa380c3":"# we finally filled NaN values of the Age Feature\ntrain_df[train_df[\"Age\"].isnull()]","209eff77":"train_df[\"Name\"].head","b0c9e092":"# split(\".\")[number] gives us string \nn = \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\"\nn.split(\".\")[0].split(\",\")[-1].strip() # it returns a list which is consisted strings\n#strip delete the space in the striig","76b81f59":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","95219a50":"train_df[\"Title\"].head(10)","43235744":"# how many different Titles it has\nsns.countplot(x=\"Title\",data = train_df)\nplt.xticks(rotation =60)\nplt.show()","c0d29943":"# conver to categorical all features above \n# we are going to replace rare titles\ntrain_df[\"Title\"]=train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Johnkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i ==\"Master\" else 1 if i == \"Miss\" or i ==\"Ms\" or i ==\"Mlle\" or i == \"Mrs\" else 2 if i ==\"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(5)","7c934477":"# how many different Titles it has AFTER ABOVE\nsns.countplot(x=\"Title\",data = train_df)\nplt.xticks(rotation =60)\nplt.show()","94f0810b":"# THERE IS AN ERROR ?\n# THERE MUST BE 4 FEATURES BUT NOT\ng = sns.factorplot(x =\"Title\", y = \"Survived\", data =train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","fd86b376":"train_df.drop(labels = [\"Name\"],axis = 1,inplace = True)","d06604ef":"train_df.head()","e63cf3f4":"train_df = pd.get_dummies(train_df,columns=[\"Title\"])\ntrain_df.head()","f3707c9f":"#we have sibsp and parch\ntrain_df.head()","72c700a2":"train_df[\"FamilySize\"] = train_df[\"SibSp\"]+train_df[\"Parch\"] + 1","2c4d9dd3":"train_df.head()","13112686":"#let's look at is there a correlance between family size and survival ratio\ng = sns.factorplot(x=\"FamilySize\",y=\"Survived\",data = train_df , kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","abf6dc7f":"# we need to choose a threshold(5) and we need to categorize them\ntrain_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"FamilySize\"]]","bba79d1d":"train_df.head(10)","db0beb47":"sns.countplot(x = \"family_size\",data = train_df)\nplt.show()","edb13fb9":"# let's look at their survival rates\ng = sns.factorplot(x = \"family_size\",y = \"Survived\",data = train_df,kind =\"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","846ba7ab":"# we allocate family_size as family_size_0 and family_size_1\ntrain_df = pd.get_dummies(train_df,columns = [\"family_size\"])\ntrain_df.head()","834d0ca9":"train_df[\"Embarked\"].head(6)","2faf1ca4":"sns.countplot(x = \"Embarked\",data = train_df)\nplt.show()","e293fbb4":"#we allocate them embarked_S , embarked_c, embarked_q by using pandas\ntrain_df = pd.get_dummies(train_df,columns = [\"Embarked\"])\ntrain_df.head()","af6edbc6":"train_df[\"Ticket\"].head(20)","e1a437d7":"a = \"A\/5. 2151\"\na.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")","2b5046d7":"# wee need to remove everything before uniqe numbers\ntickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ntrain_df[\"Ticket\"] = tickets","7fd32d88":"train_df[\"Ticket\"].head(20)","f0ea7406":"train_df.head()","edebbf34":"# AGAIN we categorize Tickets\ntrain_df = pd.get_dummies(train_df,columns = [\"Ticket\"],prefix = \"T\")\ntrain_df.head()","740cb6d3":"sns.countplot(x = \"Pclass\",data = train_df)\nplt.show()","73675eb8":"# we categorize them again \ntrain_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df , columns =[\"Pclass\"])\ntrain_df.head()","53fefd01":"# we categorize them \ntrain_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df,columns = [\"Sex\"])\ntrain_df.head()","a38e11ef":"train_df.drop(labels = [\"PassengerId\",\"Cabin\"],axis = 1,inplace = True)","47ec9683":"train_df.columns","de47d9bb":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom  sklearn.metrics import accuracy_score","0acb0ce2":"train_df_len","7249d14d":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1,inplace = True)","8e803499":"test.head()","53d6a72f":"train = train_df[:train_df_len]\nx_train = train.drop(labels = \"Survived\",axis = 1)\ny_train = train[\"Survived\"]\nx_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size = 0.33,random_state = 42)\nprint(\"x_train\",len(x_train))\nprint(\"x_test\",len(x_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test))","0d7f95d5":"# xtrain = our features, y_train = Survived Values\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\naccuracy_log_train = round(logreg.score(x_train, y_train)*100,2)\naccuracy_log_test = round(logreg.score(x_test, y_test)*100,2)\nprint(\"Training Accuracy : %{}\".format(accuracy_log_train))\nprint(\"Testing Accuracy : %{}\".format(accuracy_log_test))","319e91ab":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n              SVC(random_state = random_state),\n              RandomForestClassifier(random_state = random_state),\n              LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]\n\n\ndecisiontree_parameter_grid = {\"min_samples_split\":range(10,500,20),\n                              \"max_depth\":range(1,20,2)}\n\n\nsvc_parameter_grid = {\"kernel\": [\"rbf\"],\n                     \"gamma\":[0.001,0.01,0.1,1],\n                     \"C\":[1,10,50,100,200,300,1000]}\n\n\nrandomforestparameter_grid  = {\"max_features\": [1,3,10],\n                              \"min_samples_split\":[2,3,10],\n                              \"min_samples_leaf\":[1,3,10],\n                              \"bootstrap\": [False],\n                              \"n_estimators\":[100,300],\n                              \"criterion\":[\"gini\"]}\n\n\n\nlogreg_parameter_grid ={\"C\":np.logspace(-3,3,7),\n                       \"penalty\":[\"l1\",\"l2\"]}\n\n\nknn_parameter_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\n\nclassifier_parameter_grid = [decisiontree_parameter_grid,\n                             svc_parameter_grid,\n                             randomforestparameter_grid,\n                             logreg_parameter_grid,\n                             knn_parameter_grid]","d8351c79":"cv_result = []\nbest_estimators = []\n\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_parameter_grid[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","d554fcf8":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\",\"SVC\",\"RandomForestClassifier\",\"LogisticRegression\",\"KNeighborsClassifier\"]})\n\ng=sns.barplot(\"Cross Validation Means\",\"ML Models\",data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")                           ","15dff8b0":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                         (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs=-1)\n\nvotingC = votingC.fit(x_train,y_train)\nprint(accuracy_score(votingC.predict(x_test),y_test))","aa2f2e6c":"test_survived = pd.Series(votingC.predict(test),name=\"Survived\").astype(int)\nresults = pd.concat([test_PassengerId,test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\",index = False)","b8dd5977":"<a id ='16'><\/a>\n## Corelation Between Age -- Survived","300b3ddc":"That means,Passanger who has a big family (>5) has lower survival rate than passangers who have family under 5","06f3d014":"Sex is not informative for age prediction because age distiribution for women and man seems same.","4ca6f792":"<a id = '15'><\/a>\n## Corelation Between Pclass -- Survived","c1f85c6d":"<a id = '1'><\/a><br>\n# Load and check data","019cddd8":"* Female passanges have mush better survival rate than males\n* Males in embarked C have more survival rates \n* embarked and sex will be used in training","f78833b2":"<a id ='22'><\/a>\n## Name -- Title","a0407ddd":"# Introduction\nThe sinking of Titanic was one of the most notorious  accident in 1912 in the history.During her voyage the Titanic sank after colliding with an iceberg.1502 people died out of 2224.\n\n<font color = 'purple'>\nContent:\n    \n1. [Load and check data](#1)\n2. [Variable Description](#2)\n    * [Univariate Varible Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n5. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Corelation Between Sibsp -- Parch-- Age--Fare--Survive](#12)\n    * [Corelation Between Sibsp -- Survived](#13)\n    * [Corelation Between Parch -- Survived](#14)\n    * [Corelation Between Pclass -- Survived](#15)\n    * [Corelation Between Age -- Survived](#16)\n    * [Pclass -- Survived -- age](#17)\n    * [Embarked -- Sex -- Pclass-- Survived](#18)\n    * [Embarked -- Sex -- Fare-- Survived](#19)\n    * [Fill missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop Passanger ID and Cabin](#28)\n1. [Modeling](#29)\n    * [Train - Test Split](#30)\n    * [Simple Logistic Regression](#31)\n    * [Hyperparameter Tuning -- Grids Search -- Cross Validation](#32)\n    * [Ensemble Modeling](#33)\n    * [Prediction and Submission](#34)","ad1c7a75":"<a id ='19'><\/a>\n## Embarked -- Sex -- Fare-- Survived","70a03405":"<a id = '34'><\/a>\n## Prediction and Submission","9f2f74cb":"Fare feature seems to have coreelation with Survived feature about 0,26.","7eac9106":"<a id = '8'><\/a>\n# Missing Value\n   * Find Missing Value\n   * Fill Missing Value","c87143ef":"* Pclass is significant feature for model training","377c9181":"<a id = '29'><\/a>\n# Modeling","859da855":"<a id ='26'><\/a>\n# Pclass","ee8aa348":"<a id = '10'><\/a>\n## Fill Missing Value\n* Embarked has 2 missing values\n* Fare has 1 missing value","777865e0":"<a id = '30'><\/a>\n# Train - Test Split","6b20011e":"### Info() of tran_df\n* float64(2):Age, Fare\n* int64(5):PassengerId, Survived, Pclass, SibSp, Parch,\n* object(5):Name, Sex, ticket, Cabin, Embarked","b16ca359":"<a id ='21'><\/a>\n# Feature Engineering\n","83083af1":"<a id =\"14\"><\/a>\n## Corelation Between Parch -- Survived","2353d7db":"<a id = '24'><\/a>\n# Embarked","d7da354c":"<a id = '18'><\/a>\n## Embarked -- Sex -- Pclass-- Survived","7b30c652":"<a id ='32'><\/a>\n# Hyperparameter Tuning -- Grids Search -- Cross Validation\n* We will compare 5 ML[Machine learninig model] classifer and evaluate mean accuracy of each of them by stratified cross validation\n\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression","18f084e3":"<a id ='25'><\/a>\n# Ticket","589a0775":"<a id = '4'><\/a>\n## Categorical Variable\nwe are going to use bar plot to visualize categorical variables\n","d8a11917":"<a id ='23'><\/a>\n# Family Size","88773d1b":"<a id ='7'><\/a>\n# Outlier Detection","fe495241":"* Having lots of SibSp have less change to survive.\n* If Sibsp = 0 or 1,2, passanger has more chance to survive\n* We can consider a new feature describing these categories","f01a5d0b":"<a id = '3'><\/a>\n# Univariate Varible Analysis\n* Categorical Variable: Survived, Sex, Pclass, Embarked, Cabin , Name, Ticket, SibSp, ParCh (features which have 2 or more options)\n* Numerical Variable: PassangerId, Age, Fare","09dfbf25":"Age is not correlated with sex but it is correlated with parch,sibsp and pclass.","9a18f58e":"<a id = '31'><\/a>\n# Simple Logistic Regression","6da51781":"<a id = '2' ><\/a>\n# Variable Description\n1. PassengerId: unique id number for each passanger\n1. Survived : 1 = alive , 0 = died\n1. Pclass: class of the passanger(class 1, class 2, class 3)\n1. Name: name of the passanger\n1. Sex: male , female\n1. Age: age of passangers\n1. SibSp : sib=siblings , sp= spouses\n1. Parch: par=parent , ch=child\n1. Ticket: information of the ticket\n1.  Fare: price of the ticket\n1.  Cabin: names of the cabins\n1.  Embarked: the first cost of the passangers (C = Cherbourg, Q = Queenstown, S = Southampton)","fd46f7ac":"<a id ='5'><\/a>\n## Numerical Variable\nwe use histogram plot for numerical Varible","7ad09f39":"* Passangers who paid more,have a little better survival rate.\n* Fare can be used as categorical for training","8b9fce76":"<a id ='11'><\/a>\n# Visualization","aa3642b0":"* sibsp and pach can be used for new feature extraction with threshold th =3\n* small families have more chance to survive\n* thre is a std in survival of passanger with parch = 3","b7d04a1f":"<a id = '6'><\/a>\n# Basic Data Analysis\n\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parcha - Survived","277ba527":"<a id = '20'><\/a>\n# Fill missing: Age Feature","9f1fd5b7":"<a id='12'><\/a>\n## Corelation Between Sibsp -- Parch-- Age--Fare--Survive\nwe use seaborn","cfd438cb":"<a id = '28'><\/a>\n# Drop Passanger ID and Cabin\n\nwe do not want to use passanger \u0131d and cabin for our ML","12c5dac0":"<a id ='9'><\/a>\n## Find Missing Value","f0bb8bf7":"<a id ='13'><\/a>\n## Corelation Between Sibsp -- Survived","ff5299cb":"1st class passangers are older than 2nd, and 2nd is older than 3rd class.","a25c8888":"<a id ='17'><\/a>\n## Pclass -- Survived -- age","6ab39d42":"<a id = '33'><\/a>\n## Ensemble Modeling","f9c274ac":"* age <= 10 has a high survival rate\n* oldest passangers (age of 80) survived,\n* the pig age of the survived = 0 among people is 25 and 30\n* the pig age of the survived = 1 among people is 35\n* most passangers in 15-35 age range\n* use age feature in training\n* use age distribution for missing value of age ","838a561c":"<a id = '27'><\/a>\n# Sex"}}