{"cell_type":{"9cdef244":"code","7c2b25cf":"code","5c155465":"code","82d83fa9":"code","9ba71a17":"code","4792cfd0":"code","ac59b909":"code","67b52ac9":"code","4502df88":"code","203f13f6":"code","3068980f":"code","5b612c3a":"code","b2da9a20":"code","cbab601d":"code","b1df23c6":"code","66687400":"code","f26652e1":"code","f71c51c7":"code","1cad3ac7":"code","fff12e52":"markdown"},"source":{"9cdef244":"import numpy as np \nimport pandas as pd \n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))","7c2b25cf":"import os\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport glob\nfrom glob import glob\nimport datetime\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5c155465":"import keras\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D , Conv2DTranspose\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam","82d83fa9":"import warnings\nwarnings.filterwarnings(\"ignore\")","9ba71a17":"# Hyperparameters\nIMAGE_SIZE = 64\nNOISE_SIZE = 100\nLR_D = 0.00004\nLR_G = 0.0004\nBATCH_SIZE = 6\nEPOCHS = 50 # For better results increase this value\nBETA1 = 0.5\nWEIGHT_INIT_STDDEV = 0.02\nEPSILON = 0.00005\nSAMPLES_TO_SHOW = 8","4792cfd0":"# Create Discriminator\n\ndef get_discriminator(input_shape=(IMAGE_SIZE, IMAGE_SIZE,3)):\n    # 64x64x3 -> 32x32x32\n    input_layer = Input(input_shape)\n    hid = Conv2D(filters=32,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv1\")(input_layer)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm1\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv1_out\")(hid)    \n    \n    # 32x32x32-> 16x16x64 \n    hid = Conv2D(filters=64,\n                        kernel_size=[5,5],\n                        strides=[2,2],\n                        padding=\"same\",\n                        kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                        name = \"conv2\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm2\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv2_out\")(hid) \n    \n    # 16x16x64  -> 8x8x128  \n    hid = Conv2D(filters=128,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm3\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv3_out\")(hid)\n    \n    # 8x8x128 -> 8x8x256\n    hid = Conv2D(filters=256,\n                    kernel_size=[5,5],\n                    strides=[1,1],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv4\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm4\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv4_out\")(hid)\n    \n\n    # 8x8x256 -> 4x4x512\n    hid = Conv2D(filters=512,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv5\")(hid)  \n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm5\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv5_out\")(hid)\n    \n    hid = Flatten(name = \"flatten\")(hid)\n\n    out = Dense(1, activation='sigmoid', name = \"ligit\")(hid)\n    model = Model(inputs= input_layer, outputs=out)\n\n    model.summary()\n\n    return model","ac59b909":"def show_samples(sample_images, name, epoch):\n    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = sample_images[index]\n        axis.imshow(image_array)\n        image = Image.fromarray(image_array)\n        #image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n    #plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n    plt.show()\n    plt.close()","67b52ac9":"def test(input_z, epoch):\n    samples = generator.predict(input_z[:SAMPLES_TO_SHOW])\n    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)","4502df88":"def summarize_epoch(d_losses, g_losses , data_shape, epoch, duration, input_z):\n    minibatch_size = int(data_shape[0]\/\/BATCH_SIZE)\n    print(\"Epoch {}\/{}\".format(epoch, EPOCHS),\n          \"\\nDuration: {:.5f}\".format(duration),\n          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n    fig, ax = plt.subplots()\n    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n    plt.plot(g_losses, label='Generator', alpha=0.6)\n    plt.title(\"Losses\")\n    plt.legend()\n    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n    plt.show()\n    plt.close()\n    test(input_z, epoch)","203f13f6":"def get_batches(data):\n    batches = []\n    for i in range(int(data.shape[0]\/\/BATCH_SIZE)):\n        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n        augmented_images = []\n        for img in batch:\n            image = Image.fromarray(img)\n            if random.choice([True, False]):\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            augmented_images.append(np.asarray(image))\n        batch = np.asarray(augmented_images)\n        normalized_batch = (batch \/ 127.5) - 1.0\n        batches.append(normalized_batch)\n    return np.array(batches)","3068980f":"# Create Generator\ndef get_generator_2layer(z=(NOISE_SIZE,)):\n    # 4 x 4 x 512\n    input_layer = Input(z)\n    hid = Dense(4*4*512, activation='relu', name =\"Dense\")(input_layer)    \n    hid = LeakyReLU(alpha=0.2)(hid)\n    hid = Reshape((4, 4, 512))(hid)\n\n    # 4x4x512 -> 8x8x512\n    hid = Conv2DTranspose(256, kernel_size=[5,5],\n                               strides=[4,4],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv1\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv1\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv1_out\")(hid)\n    \n\n    # 8x8x512 -> 16x16x256\n#     hid = Conv2DTranspose(256, kernel_size=[5,5],\n#                                strides=[2,2],\n#                                padding=\"same\",\n#                                kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv2\")(hid)\n#     hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv2\")(hid)\n#     hid = LeakyReLU(alpha=0.2,name =\"trans_conv2_out\")(hid)\n    \n    \n    # 16x16x256 -> 32x32x128\n     # 16x16x256 ->-> 64x64x64\n    hid = Conv2DTranspose(64, kernel_size=[5,5],\n                               strides=[4,4],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv3\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv3_out\")(hid)    \n    \n\n#     # 32x32x128 -> 64x64x64\n#     hid = Conv2DTranspose(64, kernel_size=[5,5],\n#                                strides=[2,2],\n#                                padding=\"same\",\n#                                kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv4\")(hid)\n#     hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv4\")(hid)\n#     hid = LeakyReLU(alpha=0.2,name =\"trans_conv4_out\")(hid)        \n    \n\n    # 64x64x64 -> 64x64x3\n    hid = Conv2DTranspose(3, kernel_size=[5,5],\n                               strides=[1,1],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"logits\")(hid)\n  \n    out = Activation(\"tanh\", name =\"out\")(hid)\n    \n    model = Model(inputs=input_layer, outputs=out)\n    model.summary()\n  \n    return model","5b612c3a":"# Create Generator\ndef get_generator_3layer(z=(NOISE_SIZE,)):\n    # 4 x 4 x 512\n    input_layer = Input(z)\n    hid = Dense(4*4*512, activation='relu', name =\"Dense\")(input_layer)    \n    hid = LeakyReLU(alpha=0.2)(hid)\n    hid = Reshape((4, 4, 512))(hid)\n\n    # 4x4x512 -> 8x8x512\n    hid = Conv2DTranspose(512, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv1\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv1\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv1_out\")(hid)\n    \n\n    # 8x8x512 -> 16x16x256\n    hid = Conv2DTranspose(256, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv2\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv2\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv2_out\")(hid)\n    \n    \n    # 16x16x256 -> 32x32x128\n     # 16x16x256 ->-> 64x64x64\n    hid = Conv2DTranspose(64, kernel_size=[5,5],\n                               strides=[4,4],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv3\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv3_out\")(hid)    \n    \n\n#     # 32x32x128 -> 64x64x64\n#     hid = Conv2DTranspose(64, kernel_size=[5,5],\n#                                strides=[2,2],\n#                                padding=\"same\",\n#                                kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv4\")(hid)\n#     hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv4\")(hid)\n#     hid = LeakyReLU(alpha=0.2,name =\"trans_conv4_out\")(hid)        \n    \n\n    # 64x64x64 -> 64x64x3\n    hid = Conv2DTranspose(3, kernel_size=[5,5],\n                               strides=[1,1],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"logits\")(hid)\n  \n    out = Activation(\"tanh\", name =\"out\")(hid)\n    \n    model = Model(inputs=input_layer, outputs=out)\n    model.summary()\n  \n    return model","b2da9a20":"# Create Generator\ndef get_generator_4layer(z=(NOISE_SIZE,)):\n    # 4 x 4 x 512\n    input_layer = Input(z)\n    hid = Dense(4*4*512, activation='relu', name =\"Dense\")(input_layer)    \n    hid = LeakyReLU(alpha=0.2)(hid)\n    hid = Reshape((4, 4, 512))(hid)\n\n    # 4x4x512 -> 8x8x512\n    hid = Conv2DTranspose(512, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv1\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv1\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv1_out\")(hid)\n    \n\n    # 8x8x512 -> 16x16x256\n    hid = Conv2DTranspose(256, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv2\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv2\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv2_out\")(hid)\n    \n    \n    # 16x16x256 -> 32x32x128\n    hid = Conv2DTranspose(128, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv3\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv3_out\")(hid)    \n    \n\n    # 32x32x128 -> 64x64x64\n    hid = Conv2DTranspose(64, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv4\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv4\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv4_out\")(hid)        \n    \n\n    # 64x64x64 -> 64x64x3\n    hid = Conv2DTranspose(3, kernel_size=[5,5],\n                               strides=[1,1],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"logits\")(hid)\n  \n    out = Activation(\"tanh\", name =\"out\")(hid)\n    \n    model = Model(inputs=input_layer, outputs=out)\n    model.summary()\n  \n    return model\n","cbab601d":"# Paths\nINPUT_DATA_DIR = \"..\/input\/cropped\/\" # Path to the folder with input images. For more info check simspons_dataset.txt\n#OUTPUT_DIR = '.\/{date:%Y-%m-%d_%H:%M:%S}\/'.format(date=datetime.datetime.now())\n#if not os.path.exists(OUTPUT_DIR):\n #   os.makedirs(OUTPUT_DIR)\nOUTPUT_DIR =\"\"\n    ","b1df23c6":"# Import Data\nimport re\n\nexclude_img = [\"9746\",\"9731\",\"9717\",\"9684\",\"9637\",\"9641\",\"9642\",\"9584\",\"9541\",\"9535\",\n\"9250\",\"9251\",\"9252\",\"9043\",\"8593\",\"8584\",\"8052\",\"8051\",\"8008\",\"7957\",\n\"7958\"\"7761\",\"7762\",\"9510\",\"9307\",\"4848\",\"4791\",\"4785\",\"4465\",\"2709\",\n\"7724\",\"7715\",\"7309\",\"7064\",\"7011\",\"6961\",\"6962\",\"6963\",\"6960\",\"6949\",\n\"6662\",\"6496\",\"6409\",\"6411\",\"6406\",\"6407\",\"6170\",\"6171\",\"6172\",\"5617\",\n\"4363\",\"4232\",\"4086\",\"4047\",\"3894\",\"3889\",\"3493\",\"3393\",\"3362\",\"2780\",\n\"2710\",\"2707\",\"2708\",\"2711\",\"2712\",\"2309\",\"2056\",\"1943\",\"1760\",\"1743\",\n\"1702\",\"1281\",\"1272\",\"772\",\"736\",\"737\",\"691\",\"684\",\"314\",\"242\",\"191\"]\n\nexclude_img = [s + \".png\" for s in exclude_img]\n\nprint(\"Image Samples\")\n#input_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*')])\ninput_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*') if file not in exclude_img])\n\nprint (\"Input: \" + str(input_images.shape))\n\nnp.random.shuffle(input_images)\n\nsample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)\nshow_samples(sample_images, OUTPUT_DIR + \"inputs\", 0)","66687400":"#Discriminator\ndiscriminator = get_discriminator((IMAGE_SIZE, IMAGE_SIZE,3))\ndiscriminator.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_D, beta_1=BETA1),metrics=['accuracy'])\n\n# For the combined model we will only train the generator\ndiscriminator.trainable = False\n\n# #Generator\n# generator = get_generator_3layer((NOISE_SIZE,))\n\n# #GAN\n# # The discriminator takes generated images as input and determines validity\n# gan_input = Input(shape=(NOISE_SIZE,))\n# x = generator(gan_input)\n# gan_out = discriminator(x)\n# gan = Model(gan_input, gan_out)\n# gan.summary()\n\n# gan.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_G, beta_1=BETA1))","f26652e1":"# Hyperparameters\n# IMAGE_SIZE = 64\n# NOISE_SIZE = 100\n# LR_D = 0.00004\n# LR_G = 0.0004\nBATCH_SIZE = 16\nEPOCHS = 100 # For better results increase this value\nBETA1 = 0.5\nWEIGHT_INIT_STDDEV = 0.02\nEPSILON = 0.00005\nSAMPLES_TO_SHOW = 8","f71c51c7":"# Training\nprint(\"Training Starts!\")\n\nwarnings.filterwarnings(\"ignore\")\n\n\nlayers_functions = [None, None, None, get_generator_2layer, get_generator_3layer, get_generator_4layer ]\n\n#EPOCHS = 50\nfor BATCH_SIZE in [64, 16, 32, 128]:\n    for layer_number in [3,4,5]:\n        d_losses = []\n        g_losses = []\n        cum_d_loss = 0\n        cum_g_loss = 0\n        #Generator\n        generator = layers_functions[layer_number]((NOISE_SIZE,))\n\n        #GAN\n        # The discriminator takes generated images as input and determines validity\n        gan_input = Input(shape=(NOISE_SIZE,))\n        x = generator(gan_input)\n        gan_out = discriminator(x)\n        gan = Model(gan_input, gan_out)\n        gan.summary()\n\n        gan.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_G, beta_1=BETA1))\n        for epoch in range(EPOCHS):\n            epoch += 1\n            start_time = time.time()\n\n            for batch_images in get_batches(input_images):\n\n                noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n                # We use same labels for generated images as in the real training batch\n                generated_images = generator.predict(noise_data)\n\n                noise_prop = 0.05 # Randomly flip 5% of targets\n                real_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n                flipped_idx = np.random.choice(np.arange(len(real_labels)), size=int(noise_prop*len(real_labels)))\n                real_labels[flipped_idx] = 1 - real_labels[flipped_idx]\n\n                # Train discriminator on real data\n                d_loss_real = discriminator.train_on_batch(batch_images, real_labels)\n\n\n                # Prepare labels for generated data\n                fake_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n                flipped_idx = np.random.choice(np.arange(len(fake_labels)), size=int(noise_prop*len(fake_labels)))\n                fake_labels[flipped_idx] = 1 - fake_labels[flipped_idx]\n\n                # Train discriminator on generated data\n                d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n                cum_d_loss += d_loss\n                d_losses.append(d_loss[0])\n\n\n                # Train generator\n                noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n                g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n                cum_g_loss += g_loss\n                g_losses.append(g_loss)\n\n            if epoch > 0 and epoch % 20 == 0 :\n                path = \".\/models\/gen_batchs\"+str(BATCH_SIZE) +\"_layers\"+str(layer_number)\n                if not os.path.exists(os.path.dirname(path)):\n                    try:\n                        os.makedirs(os.path.dirname(path))\n                    except OSError as exc: # Guard against race condition\n                        if exc.errno != errno.EEXIST:\n                            raise\n                print(\"saving model\")\n                discriminator.save_weights(path+\"desc-simposon-model.h5-\" + str(epoch))\n                gan.save_weights(path+\"gen-simposon-model.h5-\" + str(epoch))\n\n            # Plot the progress\n            summarize_epoch(d_losses, g_losses, input_images.shape, epoch, time.time()-start_time, noise_data)\n","1cad3ac7":"# # Load Weights if crash\n# print(discriminator.load_weights(\"desc-simposon-model.h5-40\"))\n# gan.load_weights(\"gan-simposon-model.h5-40\")","fff12e52":"[](http:\/\/)In this kernal, I am generating sample images of Simpsons with deep convolutional generative adversarial networks (DCGANs). \nThe code is implemented using Keras.\nThe approach is inspired from:\nhttps:\/\/www.kaggle.com\/greg115\/image-generator-dcgan-the-simpsons-dataset (The code is implemented using Tensorflow and I change it to fully uses Keras)"}}