{"cell_type":{"275897a0":"code","1f508938":"code","cfe8c8e9":"code","076b0a60":"code","aed27ae9":"code","1bab2ae2":"code","ca3c3bcd":"code","b5736b37":"code","16cba880":"code","9d17397d":"code","04d0e86c":"code","827b054e":"code","3b8070a6":"code","888f04f1":"code","17d91f44":"code","d770be9d":"code","aa3d4de9":"code","e2eb26c1":"code","ac918ee1":"code","63a37904":"code","6e8dfec2":"code","4ff3ccfc":"code","21182fe5":"code","8fb86a42":"code","6c1a7089":"code","ec697ab3":"code","a53e7fd6":"code","832a7cd6":"code","3d376852":"code","5cdb6b7f":"code","63b160a5":"code","80d5dd13":"code","0d37b7b9":"code","ea11b66c":"code","5a692d50":"code","09244dd9":"code","939eed0b":"code","956ddcdf":"code","e7f87a8e":"code","6388f0c8":"code","3a65a517":"code","fd2969c9":"code","cbb6fb0c":"markdown","cc0c1487":"markdown","05059f4d":"markdown","f24abd2f":"markdown","b35422a0":"markdown","ebca7c61":"markdown","54424ecc":"markdown","ad3b9c71":"markdown","1af94c3a":"markdown","bed53719":"markdown","45a1f37a":"markdown","0034ef4f":"markdown","6bafbbbc":"markdown","992d48d9":"markdown","97728651":"markdown","0a92d9cd":"markdown","f190f1bf":"markdown","fede1d79":"markdown","81be1971":"markdown","d0e6be1b":"markdown","17f73286":"markdown","5381bdc6":"markdown"},"source":{"275897a0":"import sys\n\npackage_path = '..\/input\/visiontransformerpytorch121\/VisionTransformer-Pytorch'\nsys.path.append(package_path)\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nimport cv2\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom vision_transformer_pytorch import VisionTransformer","1f508938":"transform = transforms.Compose([\n    transforms.Resize((384, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n])","cfe8c8e9":"# ====================================================\n# Helper functions\n# ====================================================\ndef load_state(model_path):\n    state_dict = torch.load(model_path)['model']\n    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n    state_dict = {k[6:] if k.startswith('model.') else k: state_dict[k] for k in state_dict.keys()}\n\n    return state_dict","076b0a60":"def get_attention_map(img, get_mask=False):\n    x = transform(img)\n    x.size()\n\n    logits, att_mat = model(x.unsqueeze(0))\n\n    att_mat = torch.stack(att_mat).squeeze(1)\n\n    # Average the attention weights across all heads.\n    att_mat = torch.mean(att_mat, dim=1)\n\n    # To account for residual connections, we add an identity matrix to the\n    # attention matrix and re-normalize the weights.\n    residual_att = torch.eye(att_mat.size(1))\n    aug_att_mat = att_mat + residual_att\n    aug_att_mat = aug_att_mat \/ aug_att_mat.sum(dim=-1).unsqueeze(-1)\n\n    # Recursively multiply the weight matrices\n    joint_attentions = torch.zeros(aug_att_mat.size())\n    joint_attentions[0] = aug_att_mat[0]\n\n    for n in range(1, aug_att_mat.size(0)):\n        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n\n    v = joint_attentions[-1]\n    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n    if get_mask:\n        result = cv2.resize(mask \/ mask.max(), img.size)\n    else:        \n        mask = cv2.resize(mask \/ mask.max(), img.size)[..., np.newaxis]\n        result = (mask * img).astype(\"uint8\")\n    \n    return result\n\ndef plot_attention_map(original_img, att_map):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n    ax1.set_title('Original')\n    ax2.set_title('Attention Map Last Layer')\n    _ = ax1.imshow(original_img)\n    _ = ax2.imshow(att_map)","aed27ae9":"model = VisionTransformer.from_name('ViT-B_16', num_classes=5)\nstate = load_state('..\/input\/cassava-vit-b-16\/ViT-B_16_fold0.pth')\nmodel.load_state_dict(state)","1bab2ae2":"label_map = pd.read_json('..\/input\/cassava-leaf-disease-classification\/label_num_to_disease_map.json', \n                         orient='index')\n\ndisplay(label_map)","ca3c3bcd":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbb-114.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbb-44.jpg\")\n\nresult1 = get_attention_map(img1)\nresult2 = get_attention_map(img2)","b5736b37":"plot_attention_map(img1, result1)","16cba880":"plot_attention_map(img2, result2)","9d17397d":"result1 = get_attention_map(img1, True)\nresult2 = get_attention_map(img2, True)","04d0e86c":"plot_attention_map(img1, result1)","827b054e":"plot_attention_map(img2, result2)","3b8070a6":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-154.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-821.jpg\")","888f04f1":"result1 = get_attention_map(img1)\nresult2 = get_attention_map(img2)","17d91f44":"plot_attention_map(img1, result1)","d770be9d":"plot_attention_map(img2, result2)","aa3d4de9":"result1 = get_attention_map(img1, True)\nresult2 = get_attention_map(img2, True)","e2eb26c1":"plot_attention_map(img1, result1)","ac918ee1":"plot_attention_map(img2, result2)","63a37904":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cgm-498.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cgm-6.jpg\")\n\nresult1 = get_attention_map(img1)\nresult2 = get_attention_map(img2)","6e8dfec2":"plot_attention_map(img1, result1)\nplot_attention_map(img2, result2)","4ff3ccfc":"result1 = get_attention_map(img1, True)\nresult2 = get_attention_map(img2, True)","21182fe5":"plot_attention_map(img1, result1)","8fb86a42":"plot_attention_map(img2, result2)","6c1a7089":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-154.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-821.jpg\")","ec697ab3":"result1 = get_attention_map(img1)\nresult2 = get_attention_map(img2)","a53e7fd6":"plot_attention_map(img1, result1)","832a7cd6":"plot_attention_map(img2, result2)","3d376852":"result1 = get_attention_map(img1, True)\nresult2 = get_attention_map(img2, True)","5cdb6b7f":"plot_attention_map(img1, result1)","63b160a5":"plot_attention_map(img2, result2)","80d5dd13":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-healthy-105.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-healthy-236.jpg\")\n\nresult1 = get_attention_map(img1)\nresult2 = get_attention_map(img2)","0d37b7b9":"plot_attention_map(img1, result1)","ea11b66c":"plot_attention_map(img2, result2)","5a692d50":"result1 = get_attention_map(img1, True)\nresult2 = get_attention_map(img2, True)","09244dd9":"plot_attention_map(img1, result1)","939eed0b":"plot_attention_map(img2, result2)","956ddcdf":"def get_attention_info(img):\n    x = transform(img)\n    x.size()\n\n    logits, att_mat = model(x.unsqueeze(0))\n\n    att_mat = torch.stack(att_mat).squeeze(1)\n\n    # Average the attention weights across all heads.\n    att_mat = torch.mean(att_mat, dim=1)\n\n    # To account for residual connections, we add an identity matrix to the\n    # attention matrix and re-normalize the weights.\n    residual_att = torch.eye(att_mat.size(1))\n    aug_att_mat = att_mat + residual_att\n    aug_att_mat = aug_att_mat \/ aug_att_mat.sum(dim=-1).unsqueeze(-1)\n\n    # Recursively multiply the weight matrices\n    joint_attentions = torch.zeros(aug_att_mat.size())\n    joint_attentions[0] = aug_att_mat[0]\n\n    for n in range(1, aug_att_mat.size(0)):\n        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n\n    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n    \n    return joint_attentions, grid_size\n\n# def plot_attention_map(original_img, att_map):\n#     fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n#     ax1.set_title('Original')\n#     ax2.set_title('Attention Map Last Layer')\n#     _ = ax1.imshow(original_img)\n#     _ = ax2.imshow(att_map)","e7f87a8e":"img1 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-154.jpg\")\nimg2 = Image.open(\"..\/input\/cassava-vit-b-16\/train-cbsd-821.jpg\")","6388f0c8":"joint_att1, grid_size1 = get_attention_info(img1)\njoint_att2, grid_size2 = get_attention_info(img2)","3a65a517":"for i, v in enumerate(joint_att1):\n    v = joint_att1[-1]\n    mask = v[0, 1:].reshape(grid_size1, grid_size1).detach().numpy()\n    mask = cv2.resize(mask \/ mask.max(), img1.size)[..., np.newaxis]\n    result = (mask * img1).astype(\"uint8\")\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n    ax1.set_title('Original')\n    ax2.set_title('Attention Map_%d Layer' % (i+1))\n    _ = ax1.imshow(img1)\n    _ = ax2.imshow(result)","fd2969c9":"for i, v in enumerate(joint_att2):\n    v = joint_att2[-1]\n    mask = v[0, 1:].reshape(grid_size2, grid_size2).detach().numpy()\n    mask = cv2.resize(mask \/ mask.max(), img1.size)[..., np.newaxis]\n    result = (mask * img2).astype(\"uint8\")\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n    ax1.set_title('Original')\n    ax2.set_title('Attention Map_%d Layer' % (i+1))\n    _ = ax1.imshow(img2)\n    _ = ax2.imshow(result)","cbb6fb0c":"# Load ViT Model","cc0c1487":"## Healthy - Class4","05059f4d":"This is the Attention Map example.\n- Reference is [here](https:\/\/github.com\/jeonsworld\/ViT-pytorch\/blob\/main\/visualize_attention_map.ipynb).","f24abd2f":"## CMD - Class3","b35422a0":"## CGM - Class2","ebca7c61":"For example, I will use cbsd images.","54424ecc":"### Check mask for Attention Map","ad3b9c71":"# Visualize Attention Maps","1af94c3a":"### Check mask for Attention Map","bed53719":"### Check mask for Attention Map","45a1f37a":"## Helper function","0034ef4f":"Next, we will see the attention map for cassava leaf!","6bafbbbc":"## If this kernel is useful, <font color='orange'>please upvote<\/font>!","992d48d9":"## CBB - Class0","97728651":"# Vision Transformer (ViT) : Attention Map","0a92d9cd":"# About this notebook  \n- Visualize ViT Attention Map\n- ViT github is [here](https:\/\/github.com\/tczhangzhi\/VisionTransformer-Pytorch).\n(I modified a little for attention map. please see this [issue](https:\/\/github.com\/tczhangzhi\/VisionTransformer-Pytorch\/issues\/1#issuecomment-739138519).)\n\n\nI want to show that Attention Map for cassava.\n- I just show a few sample in 2019 train dataset.\n\nYou can check my pretrained ViT weight in [here](https:\/\/www.kaggle.com\/piantic\/cassava-vit-b-16).\n\n### If this kernel is useful, feel free to upvote:)","f190f1bf":"# Visualize Attention Map","fede1d79":"## CBSD - Class1","81be1971":"<img src='https:\/\/user-images.githubusercontent.com\/6073256\/101206904-2a338f00-36b3-11eb-8920-f617abab1604.png'>","d0e6be1b":"# Import libraries","17f73286":"### Check mask for Attention Map","5381bdc6":"### Check mask for Attention Map"}}