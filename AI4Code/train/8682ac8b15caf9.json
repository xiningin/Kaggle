{"cell_type":{"c70e9e4e":"code","dd2a8427":"code","9be6195d":"code","defbcc8a":"code","8f657dcc":"code","9788ad32":"code","fe42e412":"code","94918715":"code","2bc39d48":"code","f4da5500":"code","a68e7bb4":"code","ec697265":"code","00e80cd3":"code","d39bf946":"code","37cf03cf":"code","2b1cdef6":"code","515f3d15":"code","be290d33":"code","bbf46799":"code","4b7dd440":"code","7d5a9606":"code","52149b23":"code","fd726638":"code","210ffc44":"code","aacf0db0":"code","c1b39691":"code","39f18b6c":"code","c89aefee":"code","1e0bc8bd":"code","0b465cb4":"code","530e97d9":"code","301954be":"code","2df1f5e3":"code","0cb81cfd":"code","6777ca87":"code","c65a33b8":"markdown","9c639143":"markdown","05548f6d":"markdown","cf83d30f":"markdown","bc9317ca":"markdown","ed951276":"markdown","656b93b2":"markdown"},"source":{"c70e9e4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd2a8427":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntitanic_train_data = train_data.copy()\ntitanic_test_data = test_data.copy()","9be6195d":"#!pip install facets-overview\nfrom IPython.core.display import display, HTML\n\njsonstr = titanic_train_data.to_json(orient='records')\nHTML_TEMPLATE = \"\"\"\n        <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/webcomponentsjs\/1.3.3\/webcomponents-lite.js\"><\/script>\n        <link rel=\"import\" href=\"https:\/\/raw.githubusercontent.com\/PAIR-code\/facets\/1.0.0\/facets-dist\/facets-jupyter.html\">\n        <facets-dive id=\"elem\" height=\"600\"><\/facets-dive>\n        <script>\n          var data = {jsonstr};\n          document.querySelector(\"#elem\").data = data;\n        <\/script>\"\"\"\nhtml = HTML_TEMPLATE.format(jsonstr=jsonstr)\ndisplay(HTML(html))","defbcc8a":"# Basic data describe for numeric data\ntitanic_train_data.describe()","8f657dcc":"titanic_test_data.describe()","9788ad32":"# Retrieve dataframe information\ntitanic_train_data.info()","fe42e412":"titanic_test_data.info()","94918715":"print (\"Rows     : \" ,titanic_train_data.shape[0])\nprint (\"Columns  : \" ,titanic_train_data.shape[1])\nprint (\"\\nFeatures : \\n\" ,titanic_train_data.columns.tolist())\nprint (\"\\nMissing values :  \", titanic_train_data.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",titanic_train_data\n       .nunique())","2bc39d48":"# Explore pattern: woman and man survival rate\ntitanic_women = titanic_train_data[titanic_train_data.Sex == 'female']['Survived']\nwomen_survival_rate = sum(titanic_women) \/ len(titanic_women)\n\ntitanic_men = titanic_train_data[titanic_train_data.Sex == 'male']['Survived']\nmen_survival_rate = sum(titanic_men) \/ len(titanic_men)\n\nprint(\"% of women who survived:\", women_survival_rate)\nprint(\"% of men who survived:\", men_survival_rate)\n\n# A good heuristic for model is based on gender and predict all women will suvive\n# The heuristic model will give us 74%+ accuracy based on testing data","f4da5500":"corr = titanic_train_data.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps","a68e7bb4":"# For simplicity of data processing, we will combine both testing and training data first\ncombine = [titanic_train_data, titanic_test_data]","ec697265":"# MISSING SOURCES (refer to one of other notebooks)\n# Assign 'S' to missing port. According to Titianic Encylopedia, both passengers boarded in Southampton\n# https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html\n\ntitanic_train_data[titanic_train_data['Embarked'].isnull()]\n\ntitanic_train_data['Embarked'] = titanic_train_data.Embarked.fillna('S')\n\n# Assign numeric value to Embarkation\n#for dataset in combine:\n#    dataset['Embarked'] = dataset.Embarked.map({'S': -1, 'C': 0, 'Q': 1} ).astype(int)\n\ntitanic_train_data[['Embarked', 'Survived']].groupby(by=['Embarked'],as_index=False).mean()","00e80cd3":"# https:\/\/www.kaggle.com\/nikhilkmr300\/titanic-detailed-eda-and-feature-engineering#Dealing-with-nulls\n# Step 1 - Create new feature Title \ndef group_titles(title): \n    if(title in {'Lady.', 'the', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'Dona.'}):\n        return 'Rare.'\n    elif(title in {'Mlle.', 'Miss.', 'Ms.'}):\n        return 'Miss.'\n    elif(title == 'Mme.'):\n        return 'Mrs.'\n    else:\n        return title\n    \nfor dataset in combine:\n    dataset['Title'] = [name.split(',')[1].strip().split(' ')[0] for name in dataset['Name']]\n    dataset['Title'] = dataset['Title'].apply(group_titles)\n\n# Step 2 - Fill NA based on Title and Pclass across entire dataset\ndataset_title_class = pd.concat([combine[0][['Title', 'Pclass', 'Age']], combine[1][['Title', 'Pclass', 'Age']]])\nage_per_group = dataset_title_class.groupby(by=['Title', 'Pclass']).median().astype(int)\n\nfor dataset in combine:\n    null_ids = dataset[dataset['Age'].isnull()].index.tolist()\n    \n    for index in null_ids:\n        title = dataset.loc[index, 'Title']\n        pclass = dataset.loc[index, 'Pclass']\n        dataset.loc[index, 'Age'] = age_per_group.loc[title].loc[pclass]['Age']\ntitanic_train_data[['Title', 'Survived']].groupby(by=['Title'],as_index=False).mean()","d39bf946":"# There are some data with 0 fare. This is very strange\n# I would rather think this 0-fare passengers are noise to our training set, and decide to drop those from training examples\n# Inspired by - https:\/\/www.kaggle.com\/sunaysawant\/theory-behind-titanic-disaster-top-4\n\ntitanic_train_data.drop(labels=titanic_train_data[titanic_train_data['Fare']==0].index.tolist(), inplace=True)","37cf03cf":"# Fare for #152 is Nan, he is a 3rd class passenger\n# Assign average fare for 3rd class passenger to #152\n\ndataset_class_title_fare = pd.concat([combine[0][['Pclass', 'Title', 'Fare']], combine[1][['Title', 'Pclass', 'Fare']]])\nfare_per_group = dataset_class_title_fare.groupby(by=['Pclass', 'Title']).median()\n\nfor dataset in combine:\n    null_ids = dataset[dataset['Fare'].isnull()].index.tolist()\n    for index in null_ids:\n        title = dataset.loc[index, 'Title']\n        pclass = dataset.loc[index, 'Pclass']\n        dataset.loc[index, 'Fare'] = age_per_group.loc[title].loc[pclass]['Age']","2b1cdef6":"for dataset in combine: \n    print(dataset.info())","515f3d15":"# Creating FamilySize column\n# Creating isAlone column\ntitanic_train_data['FamilySize'] = 0\ntitanic_test_data['FamilySize'] = 0\ntitanic_train_data['isAlone'] = 0\ntitanic_test_data['isAlone'] = 0\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset.loc[dataset['FamilySize']==1,'isAlone'] = 1\n\n    print(dataset['isAlone'].value_counts())","be290d33":"# Adding FamilySurvival Feature as suggested by:\n# https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever\n\ndata_df = titanic_train_data.append(titanic_test_data)\ndata_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n\nDEFAULT_SURVIVAL_VALUE = 0.5\ndata_df['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in data_df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',]].groupby(['Last_Name', 'Fare']):\n    if (len(grp_df) != 1):\n        # A Family group is found (in training set)\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data_df.loc[data_df['PassengerId'] == passID, 'FamilySurvival'] = 1\n            elif (smin==0.0):\n                data_df.loc[data_df['PassengerId'] == passID, 'FamilySurvival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      data_df.loc[data_df['FamilySurvival']!=0.5].shape[0])\n\nfor _, grp_df in data_df.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data_df.loc[data_df['PassengerId'] == passID, 'FamilySurvival'] = 1\n                elif (smin==0.0):\n                    data_df.loc[data_df['PassengerId'] == passID, 'FamilySurvival'] = 0\n                        \nprint(\"Number of passenger with family\/group survival information: \" \n      +str(data_df[data_df['FamilySurvival']!=0.5].shape[0]))\n\n# FamilySurvival in TRAIN_DF and TEST_DF:\ntitanic_train_data['FamilySurvival'] = data_df['FamilySurvival'][:len(titanic_train_data)]\ntitanic_test_data['FamilySurvival'] = data_df['FamilySurvival'][len(titanic_train_data):]","bbf46799":"# Creating Deck column\ndef split_deck(cabin):\n    return list(cabin)[0]\n\ntitanic_train_data['Deck'] = 'U'\ntitanic_test_data['Deck'] = 'U'\n\nfor dataset in combine:\n    dataset['Cabin'].fillna('U', inplace=True)\n    dataset['Deck'] = dataset['Cabin'].apply(split_deck)","4b7dd440":"titanic_train_data[['Deck', 'Survived']].groupby(by=['Deck'],as_index=False).mean()","7d5a9606":"for dataset in combine: \n    print(dataset.info())","52149b23":"import tensorflow as tf\nprint(tf.__version__)\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n","fd726638":"# Drop un-used columns.\n# titanic_train_data = titanic_train_data.drop(columns=['Cabin', 'PassengerId'])\n# titanic_test_data = titanic_test_data.drop(columns=['Cabin'])","210ffc44":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):    \n    dataframe = dataframe.copy()\n    labels = dataframe.pop('Survived')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n        ds = ds.batch(batch_size)\n    return ds","aacf0db0":"batch_size = 15 # A small batch sized is used for demonstration purposes\ntrain, test = train_test_split(titanic_train_data, test_size=0.15)\ntrain_ds = df_to_dataset(train, batch_size=batch_size)","c1b39691":"for feature_batch, label_batch in train_ds.take(1):\n    print('Every feature:', list(feature_batch.keys()))\n    print('A batch of ages:', feature_batch['Age'])\n    print('A batch of genders:', feature_batch['Sex'])\n    print('A batch of targets:', label_batch )","39f18b6c":"example_batch = next(iter(train_ds))[0]\n\ndef demo(feature_column):\n    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n    print(feature_layer(example_batch).numpy())","c89aefee":"# Create Feature Columns \nfeature_columns = []\n# Best so far: ['Age', 'Fare', 'FamilySurvival', 'Sex', 'Pclass', 'Embarked', 'isAlone', 'FamilySize']\n# Numeric Columns\nnum_cols = ['Age', 'Fare']\nfor num_col in num_cols:\n    feature_columns.append(tf.feature_column.numeric_column(\n        num_col, dtype=tf.dtypes.as_dtype(titanic_train_data[num_col].dtype), \n        normalizer_fn=lambda x: (x - tf.reduce_mean(x)) \/ tf.math.reduce_std(x)))\n    \nfeature_columns.append(tf.feature_column.numeric_column('FamilySurvival'))\n\n\n# Categorical Columns\ncat_cols = ['Sex', 'Pclass', 'Embarked', 'isAlone']\nfor cat_col in cat_cols:\n    cat = tf.feature_column.categorical_column_with_vocabulary_list(\n        cat_col, titanic_train_data[cat_col].unique())\n    feature_columns.append(tf.feature_column.indicator_column(cat))\n\n# Embedding Columns \n# emb_cols = ['Name']\n# for emb_col in emb_cols:\n#     cat_emb = tf.feature_column.categorical_column_with_vocabulary_list(emb_col, titanic_train_data[emb_col].unique())\n    #feature_columns.append(tf.feature_column.embedding_column(cat_emb, dimension=10)) ## to refine dimensions\n\n# Bucketized columns\nfamilySize = tf.feature_column.numeric_column('FamilySize')\nfamilySize_buckets = tf.feature_column.bucketized_column(familySize, boundaries=[2,5,8])\nfeature_columns.append(familySize_buckets)\n\n\n# Crossed feature columns\nage_nc = tf.feature_column.numeric_column('Age')\nage_bin = tf.feature_column.bucketized_column(age_nc, boundaries=[12, 18, 26, 32, 40])\nfare_nc = tf.feature_column.numeric_column('Fare')\nfare_bin = tf.feature_column.bucketized_column(fare_nc, boundaries=[10, 20, 40, 80])\nfeature_columns.append(tf.feature_column.crossed_column([age_bin, fare_bin], 32))\n\n# feature_columns.append(tf.feature_column.indicator_column(age_class))\n\n# The output of this step is to create the DenseFeature layer\n# feature_layer = tf.keras.layers.DenseFeatures(tf.feature_columns)\n\nfeature_columns","1e0bc8bd":"X_train, X_val = train_test_split(titanic_train_data, test_size=0.2)\n\ny_train = X_train.pop('Survived')\ny_val = X_val.pop('Survived')\n\nprint(len(y_train), 'train examples')\nprint(len(y_val), 'validation examples')","0b465cb4":"# Use entire batch since this is such a small dataset.\nNUM_EXAMPLES = len(X_train)\n\n# (TF_FEATURE_CREATION) Define Input Functions \ndef make_input_fn(X, y=None, n_epochs=None, shuffle=True):\n    def input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n            dataset = dataset.shuffle(NUM_EXAMPLES)\n        # For training, cycle thru dataset as many times as need (n_epochs=None).    \n        dataset = dataset.repeat(n_epochs)\n        # In memory training doesn't use batching.\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn\n\n# Training and evaluation input functions.\n\n# (TF_FEATURE_CREATION) Call Input Functions - for trainining input\ntrain_input_fn = make_input_fn(X_train, y_train)\n# (TF_FEATURE_CREATION) Call Input Functions - for evaluation input\neval_input_fn = make_input_fn(X_val, y_val, shuffle=False, n_epochs=1)","530e97d9":"%%time\n\nbt_estor = tf.estimator.BoostedTreesClassifier(feature_columns, n_batches_per_layer=1, \n                                               n_trees=150, max_depth=5, \n                                               center_bias=True, l2_regularization=0.1)\n\n# The model will stop training once the specified number of trees is built, not \n# based on the number of steps.\nbt_estor.train(train_input_fn, max_steps=200)\n\n# Eval.\nresult = bt_estor.evaluate(eval_input_fn)\nprint(pd.Series(result))","301954be":"pred_dicts = list(bt_estor.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nprobs.plot(kind='hist', bins=20, title='predicted probabilities')\nplt.show()","2df1f5e3":"# (TF_FEATURE_CREATION) Call Input Functions - for serving input\npred_input_fn = make_input_fn(titanic_test_data, shuffle=False, n_epochs=1)\n\npred_dicts = list(bt_estor.predict(pred_input_fn))\npred_outputs = pd.Series([pred['class_ids'][0] for pred in pred_dicts])","0cb81cfd":"# Save output\nfrom datetime import datetime\ntimestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n\noutput = pd.DataFrame({'PassengerId': titanic_test_data.PassengerId, 'Survived': pred_outputs})\noutput.to_csv('submission_tf_btc' +  '.csv', index=False)\nprint(\"Your submission was successfully saved!\")","6777ca87":"# # Visualize Forest\n# from sklearn.tree import export_graphviz\n# # Export as dot file\n# export_graphviz(titanicRandomForestModel.estimators_[1], \n#                 out_file='tree.dot', \n#                 feature_names = X_competition.columns,\n#                 class_names = ['Survived', 'Dead'],\n#                 rounded = True, proportion = False, \n#                 precision = 2, filled = True)\n\n# # Convert to png using system command (requires Graphviz)\n# from subprocess import call\n# call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# # Display in jupyter notebook\n# from IPython.display import Image\n# Image(filename = 'tree.png')","c65a33b8":"## Step 3 - Feature Engineering\nWe will add following new features:\n* Family Size: Sum of SibSp and ParCh\n* isAlone: Indicator show if passenger is travelling alone\n* Age\\*Class: Product of age and class \n* Family Survival Indicator: Useful, but this is a tricky value because in real life scenarios, this feature is not obtainable\n","9c639143":"## Step 2 - Clean Data\nNow, let's do some data cleaning. In previous steps, we noted 'Embarked', 'Age', 'Fare' and 'Cabin' contain missing values. Here is what we are going to do:\n* For Embarked: assigning 'S' to missing embarked examples\n* For Age: allocate age group based on Title and Class\n* For Fare: Assign average fare based on Class\n* For Cabin: too many missing ... we will drop the feature for the time being. Hopefully, we don't lose much information \ud83d\ude4f\n* NOTE: some data has fare in 0 value, we will drop to avoid it confuse model","05548f6d":"### 4.2 - Create, compile, and train the model\nWe will use Tensorflow BoostedTreesClassifer, TensorFlow version of random forest.","cf83d30f":"## Step 4 - Use Tensorflow API to Build Model","bc9317ca":"In Boosted Trees Classifier, we must define train and valuation input fuction. This is to let model know how to read the data in training and validation.","ed951276":"### 4.1 - Convert Pandas dataframe to feature columns\nand select which columns for feature","656b93b2":"## Step 1 - Explore the data\nWe are using [Facets](https:\/\/pair-code.github.io\/facets\/) to visualise data. \nIn Step 2, we found some null values for Age, Cabin and Emarked features. At this step, we can use Facets tool to visualize relationships and decide how to handle those NULL values. Below are some findings:\n1. Comparing Age against Surivial, it looks like higher suvivial rate for younger people. Hence, an accurate allocation of missing age is important. (But intuitively, this will introduce higher false positives and false negatives)\n2. Comparing Cabin against Surivial, seems like people assigned a Cabin (especially B, E and F) has higher survial rate, however, this doesn't represent anything.\n3. Only 2 missing values for embarkation, we can either assign a most frequent value, or just try to find the embarkation point of these two persons, from a different channel (maybe Google).\n"}}