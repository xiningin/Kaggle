{"cell_type":{"8f6f8ff4":"code","6f687f7d":"code","1bd916ed":"code","614ae863":"code","11a01d04":"code","c6e8bb7a":"code","bb279b3d":"code","da1d6fc0":"code","b4fa0c88":"code","18b6dc67":"code","72c7edde":"code","2881d11b":"code","54906ebb":"code","048de20e":"code","7c780c11":"code","2478e77b":"code","dfd5f1bc":"code","359b6dc0":"code","e22433cb":"code","5ec2ce86":"code","db00b52d":"code","65d314d4":"code","04c6bb70":"code","75a5fc9c":"code","a6089da7":"code","14052c18":"code","13c3f438":"code","c3e6b857":"code","0cadedbf":"code","46293830":"code","18f91d4a":"code","ffe71634":"code","64199d0d":"code","4f03d5d3":"code","a60b8301":"code","352fd029":"code","e36efe5c":"code","af398b08":"code","79e563c1":"code","7d0e0e0c":"code","bb400f13":"code","0f401edd":"code","e382b8c9":"code","1dc1ed6e":"code","6d19496e":"code","8987ac9f":"code","7bf3d002":"code","3b6546ec":"markdown","40ffda68":"markdown","3e1633f6":"markdown","3eb1f52c":"markdown","9e741503":"markdown","1bd9caa9":"markdown","9b774df2":"markdown","b9600b69":"markdown","123a79a3":"markdown","9f09dc6d":"markdown","2e7fbd33":"markdown","b52fab8f":"markdown","49d45938":"markdown","ccfdf18f":"markdown","ddd040df":"markdown","00b157e5":"markdown","b3c249af":"markdown"},"source":{"8f6f8ff4":"# Set warning messages\nimport warnings\n# Show all warnings in IPython\nwarnings.filterwarnings('always')\n# Ignore specific numpy warnings (as per <https:\/\/github.com\/numpy\/numpy\/issues\/11788#issuecomment-422846396>)\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n# Other warnings that sometimes come up\nwarnings.filterwarnings(\"ignore\", message=\"unclosed file <_io.TextIOWrapper\")","6f687f7d":"# Import built-in modules\nimport sys\nimport platform\nimport os\nfrom pathlib import Path\n\n# Import external modules\nfrom IPython import __version__ as IPy_version\nimport IPython.display as ipyd\nimport numpy as np\nimport pandas as pd\nfrom sklearn import __version__ as skl_version\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bokeh import __version__ as bk_version\nfrom scipy import __version__ as scipy_version\nfrom statsmodels import __version__ as sm_version\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport xgboost as xgb\nimport scipy.stats as sps\n\n# Import project modules\nfrom bucketplot import __version__ as bplt_version\nimport bucketplot as bplt\n\n# Check they have loaded and the versions are as expected\nassert platform.python_version_tuple() == ('3', '6', '6')\nprint(f\"Python version:\\t\\t{sys.version}\")\nassert IPy_version == '7.13.0'\nprint(f'IPython version:\\t{IPy_version}')\nassert np.__version__ == '1.18.2'\nprint(f'numpy version:\\t\\t{np.__version__}')\nassert pd.__version__ == '0.25.3'\nprint(f'pandas version:\\t\\t{pd.__version__}')\nassert skl_version == '0.22.2.post1'\nprint(f'sklearn version:\\t{skl_version}')\nassert mpl.__version__ == '3.2.1'\nprint(f'matplotlib version:\\t{mpl.__version__}')\nassert sns.__version__ == '0.10.0'\nprint(f'seaborn version:\\t{sns.__version__}')\nassert bk_version == '2.0.1'\nprint(f'bokeh version:\\t\\t{bk_version}')\nassert scipy_version == '1.4.1'\nprint(f'scipy version:\\t\\t{scipy_version}')\nassert sm_version == '0.11.0'\nprint(f'statsmodels version:\\t{sm_version}')\nassert bplt_version == '0.0.2'\nprint(f'bucketplot version:\\t{bplt_version}')","1bd916ed":"# Bokeh imports\nfrom bokeh.layouts import gridplot\nfrom bokeh.plotting import figure, output_file, show, output_notebook\nfrom bokeh.models.ranges import Range1d\nfrom bokeh.models.axes import LinearAxis\n\n# Load Bokeh for use in a notebook\nfrom bokeh.io import output_notebook\noutput_notebook()","614ae863":"# Output exact environment specification, in case it is needed later\nprint(\"Capturing full package environment spec\")\nprint(\"(But note that not all these packages are required)\")\n!pip freeze > requirements_Kaggle.txt\n!jupyter --version > jupyter_versions.txt","11a01d04":"input_folder_path = Path('\/kaggle\/input')\nclaims_data_filepath = (\n    input_folder_path \/ 'french-motor-claims-datasets-fremtpl2freq' \/ 'freMTPL2freq.csv'\n)\n\nGLM_folder_path = input_folder_path \/ 'models-of-french-motor-claims'\nassert GLM_folder_path.is_dir()\nGLM_data_filepath = GLM_folder_path \/ 'df_validation_GLM_preds.gzip'\nassert GLM_data_filepath.is_file()\n\nRF_folder_path = input_folder_path \/ 'alex-f-french-motor-claims-analysis'\nassert RF_folder_path.is_dir()\nRF_data_filepath = RF_folder_path \/ 'Alex_Farquharson_rf_dataframe.gzip'\nassert RF_data_filepath.is_file()\n\nXGB_folder_path = input_folder_path \/ 'chuns-french-motor-claims-project'\nassert XGB_folder_path.is_dir()\nXGB_data_filepath = XGB_folder_path \/ 'xgb_filtered_pred_valid_set_new.gzip'\nassert XGB_data_filepath.is_file()\n\nprint(\"Correct: All locations are available as expected\")","c6e8bb7a":"# Load full modelling data set\nexpected_dtypes = {\n    **{col: np.dtype('int64') for col in [\n        'IDpol', 'ClaimNb', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']},\n    **{col: np.dtype('float64') for col in ['Exposure']},\n    **{col: np.dtype('O') for col in ['Area', 'VehBrand', 'VehGas', 'Region']},\n}\ndf_raw = pd.read_csv(claims_data_filepath, delimiter=',', dtype=expected_dtypes)","bb279b3d":"# Check it has loaded OK\nnRows, nCols = (678013, 12)\nassert df_raw.shape == (nRows, nCols)\nprint(f\"Correct: Shape of DataFrame is as expected: {nRows} rows, {nCols} cols\")\nassert df_raw.dtypes.equals(pd.Series(expected_dtypes)[df_raw.columns])\nprint(\"Correct: Data types are as expected\")\nassert df_raw.isna().sum().sum() == 0\nprint(\"Correct: There are no missing values in the raw dataset\")","da1d6fc0":"# Get index sorted with ascending IDpol, just in case it is out or order\ndf_all = df_raw.sort_values('IDpol').reset_index(drop=True)\n\n# Proportions we want to split in (must sum to 1)\nsplit_props = pd.Series({\n    'train': 0.7,\n    'validation': 0.15,\n    'holdout': 0.15\n})\n\n# Split out training data\ndf_train, df_not_train = train_test_split(\n    df_all, test_size=(1 - split_props['train']), random_state=51, shuffle=True\n)\n# Split remaining data between validation and holdout\ndf_validation, df_holdout = train_test_split(\n    df_not_train, test_size=split_props['holdout'] \/ (1 - split_props['train']), random_state=13, shuffle=True\n)","b4fa0c88":"# Check all rows have been accounted for\npd.concat([df_train, df_validation, df_holdout]).sort_index().equals(df_all)","18b6dc67":"# Sort to make it easier to compare\n# We know that IDpol is unique\ndf_validation = df_validation.sort_values('IDpol')\nact_ClaimNb_validation = df_validation.ClaimNb.sum()\n\n# Print number of rows and fields\ndf_validation.shape","72c7edde":"expl_var_names = [\n    col_name for col_name in df_validation.columns.to_list() \n     if col_name not in ['IDpol', 'ClaimNb', 'Exposure', 'Frequency']\n]\nprint(\"Explanatory variables\\n\" + '\\t'.join(expl_var_names))","2881d11b":"# From GLM\ndf_GLM_preds = pd.read_pickle(\n    GLM_data_filepath\n).sort_values('IDpol')","54906ebb":"# Reasonableness checks\nassert df_GLM_preds.shape[0] == df_validation.shape[0]\nassert (df_validation.IDpol == df_GLM_preds.IDpol).all()\nassert (df_validation.ClaimNb == df_GLM_preds.ClaimNb).all()\nassert df_GLM_preds.iloc[:,:12].equals(df_validation)\nprint(\"Correct: Reasonableness checks have passed for the GLM data\")","048de20e":"pred_ClaimNb_GLM = (df_GLM_preds.pred_freq * df_validation.Exposure).sum()\nprint(f\"GLM predicted total number of claims:\\t{pred_ClaimNb_GLM:.1f}\")\nprint(f\"Actual total number of claims:\\t\\t{act_ClaimNb_validation:.1f}\")\nprint(f\"Difference:\\t\\t\\t\\t{pred_ClaimNb_GLM - act_ClaimNb_validation:.1f}\")","7c780c11":"# From GBM (i.e. xgboost)\ndf_XGB_preds = pd.read_pickle(\n    XGB_data_filepath\n).sort_values('IDpol').reset_index(drop=True)\n# Cast IDpol to integer to match modelling data\ndf_XGB_preds.IDpol = df_XGB_preds.IDpol.astype(np.dtype('int64'))","2478e77b":"# Reasonableness checks\nassert df_XGB_preds.shape[0] == df_validation.shape[0]\nassert (df_validation.reset_index(\n    drop=True).IDpol == df_XGB_preds.IDpol).all()\nassert (df_validation.reset_index(\n    drop=True).ClaimNb == df_XGB_preds.ClaimNb).all()\nassert df_validation.reset_index(drop=True)[\n    ['IDpol', 'ClaimNb', 'Exposure']].equals(df_XGB_preds.iloc[:,:3])\nprint(\"Correct: Reasonableness checks have passed for the XGB data\")\nprint(\n    \"Note that, for the XGB data:\\n\"\n    \"\\t-The index has been reset, but we can match to the validation data by IDpol\"\n)","dfd5f1bc":"pred_ClaimNb_XGB = (df_XGB_preds.pred_ClaimNb * df_validation.reset_index().Exposure).sum()\nprint(f\"XGB predicted total number of claims:\\t{pred_ClaimNb_XGB:.1f}\")\nprint(f\"Actual total number of claims:\\t\\t{act_ClaimNb_validation:.1f}\")\nprint(f\"Difference:\\t\\t\\t\\t{pred_ClaimNb_XGB - act_ClaimNb_validation:.1f}\")","359b6dc0":"# From RF\ndf_RF_preds = pd.read_pickle(RF_data_filepath)","e22433cb":"assert df_RF_preds.shape[0] == df_validation.shape[0]\nassert (df_RF_preds.index == df_validation.index).all()\nassert (df_RF_preds.ClaimNb == df_validation.ClaimNb).all()\nassert np.max(np.abs(\n    df_RF_preds.Exposure - df_validation.assign(\n        Exp_4dps=lambda x: np.round(x.Exposure, 4)\n    ).Exp_4dps\n)) < 1e-14\nprint(\"Correct: Reasonableness checks have passed for the RF data\")\nprint(\n    \"Note that, for the RF data:\\n\"\n    \"\\t-IDpol is not included but we can match to the validation data by index\\n\"\n    \"\\t-The Exposure field on the RF is rounded to 4dps\"\n)","5ec2ce86":"pred_ClaimNb_RF = df_RF_preds['Random Forest Predictions'].sum()\nprint(f\"RF predicted total number of claims:\\t{pred_ClaimNb_RF:.1f}\")\nprint(f\"Actual total number of claims:\\t\\t{act_ClaimNb_validation:.1f}\")\nprint(f\"Difference:\\t\\t\\t\\t{pred_ClaimNb_RF - act_ClaimNb_validation:.1f}\")","db00b52d":"df_validation_all = df_validation.assign(\n    act_freq=lambda x: x.ClaimNb \/ x.Exposure\n).merge(\n    df_RF_preds.assign(\n        RF_pred_freq=lambda x: x['Random Forest Predictions'] \/ x.Exposure\n    )[['RF_pred_freq']],\n    how='inner', left_index=True, right_index=True\n).merge(\n    df_GLM_preds.rename(columns={\n        'pred_freq': 'GLM_pred_freq'\n    })[['IDpol', 'GLM_pred_freq']],\n    how='inner', left_on='IDpol', right_on='IDpol'\n).merge(\n    df_XGB_preds.assign(\n        XGB_pred_freq=lambda x: x.pred_ClaimNb \/ x.Exposure\n    )[['IDpol', 'XGB_pred_freq']],\n    how='inner', left_on='IDpol', right_on='IDpol'\n)\n# Reasonableness checks on the result\nassert df_validation_all.shape[0] == df_validation.shape[0]","65d314d4":"# Look at result (excluding explanatory variables)\ndf_validation_all.loc[:, ~df_validation_all.columns.isin(expl_var_names)].head()","04c6bb70":"stat_cols = ['GLM_pred_freq', 'act_freq']\nlift_plt_data_df = bplt.get_agg_plot_data(\n    df_validation_all,\n    stat_cols=stat_cols,\n    stat_wgt='Exposure',\n    bucket_wgt='Exposure',\n    set_config=\"lift\",\n    n_bins=10\n)\nlift = {\n    stat_col: lift_plt_data_df[stat_col + \"_wgt_av\"].agg(\n        lambda x: x.iloc[-1] \/ x.iloc[0])\n    for stat_col in stat_cols\n}\nprint(f'Lift on predicted:\\t{lift[stat_cols[0]]:.3f}')\nprint(f'Lift on actuals:\\t{lift[stat_cols[1]]:.3f}')\nlift_plt = bplt.create_plot(lift_plt_data_df, stat_cols=stat_cols)\nshow(lift_plt)","75a5fc9c":"stat_cols = ['RF_pred_freq', 'act_freq']\nlift_plt_data_df = bplt.get_agg_plot_data(\n    df_validation_all,\n    stat_cols=stat_cols,\n    stat_wgt='Exposure',\n    bucket_wgt='Exposure',\n    set_config=\"lift\",\n    n_bins=10\n)\nlift = {\n    stat_col: lift_plt_data_df[stat_col + \"_wgt_av\"].agg(\n        lambda x: x.iloc[-1] \/ x.iloc[0])\n    for stat_col in stat_cols\n}\nprint(f'Lift on predicted:\\t{lift[stat_cols[0]]:.3f}')\nprint(f'Lift on actuals:\\t{lift[stat_cols[1]]:.3f}')\nlift_plt = bplt.create_plot(lift_plt_data_df, stat_cols=stat_cols)\nshow(lift_plt)","a6089da7":"stat_cols = ['XGB_pred_freq', 'act_freq']\nlift_plt_data_df = bplt.get_agg_plot_data(\n    df_validation_all,\n    stat_cols=stat_cols,\n    stat_wgt='Exposure',\n    bucket_wgt='Exposure',\n    set_config=\"lift\",\n    n_bins=10\n)\nlift = {\n    stat_col: lift_plt_data_df[stat_col + \"_wgt_av\"].agg(\n        lambda x: x.iloc[-1] \/ x.iloc[0])\n    for stat_col in stat_cols\n}\nprint(f'Lift on predicted:\\t{lift[stat_cols[0]]:.3f}')\nprint(f'Lift on actuals:\\t{lift[stat_cols[1]]:.3f}')\nlift_plt = bplt.create_plot(lift_plt_data_df, stat_cols=stat_cols)\nshow(lift_plt)","14052c18":"stat_cols = ['RF_pred_Nb', 'ClaimNb']\nlift_plt_data_df = bplt.get_agg_plot_data(\n    df_validation_all.assign(\n        GLM_pred_Nb=lambda x: x.GLM_pred_freq * x.Exposure,\n        RF_pred_Nb=lambda x: x.RF_pred_freq * x.Exposure,\n        XGB_pred_Nb=lambda x: x.XGB_pred_freq * x.Exposure,\n    ),\n    stat_cols=stat_cols,\n    stat_wgt=None,\n    bucket_wgt='Exposure',\n    order_by=stat_cols[0],\n    cut_by='cum_wgt',\n    #x_axis_var=stat_cols[1],\n    n_bins=10\n)\n\n# Plot actual average against predicted average\nlift_plt = bplt.create_plot(lift_plt_data_df.assign(\n    x_left=lambda x: x[stat_cols[1] + '_wgt_av'],\n    x_right=lambda x: x[stat_cols[1] + '_wgt_av'],\n    x_mid=lambda x: x[stat_cols[1] + '_wgt_av'],\n), stat_cols=stat_cols)\nshow(lift_plt)\n\n# Plot lines of actual average and predicted average\nlift_plt = bplt.create_plot(lift_plt_data_df, stat_cols=stat_cols)\nshow(lift_plt)\n\n# Calculate lift\nlift = {\n    stat_col: lift_plt_data_df[stat_col + \"_wgt_av\"].agg(\n        lambda x: x.iloc[-1] \/ x.iloc[0])\n    for stat_col in stat_cols\n}\nprint(f'Lift on predicted:\\t{lift[stat_cols[0]]:.3f}')\nprint(f'Lift on actuals:\\t{lift[stat_cols[1]]:.3f}')","13c3f438":"predictions_df = df_validation_all.assign(\n    GLM_pred_Nb=lambda x: x.GLM_pred_freq * x.Exposure,\n    RF_pred_Nb=lambda x: x.RF_pred_freq * x.Exposure,\n    XGB_pred_Nb=lambda x: x.XGB_pred_freq * x.Exposure,\n)\nweights_colm = 'Exposure'\npredicted_colm, actual_colm = stat_cols\nq = 10\n\n# Get weighted quantiles and add grouping to the DataFrame\norder = predictions_df[\n    weights_colm  # bucket_wgt\n].iloc[predictions_df[\n    predicted_colm  # order_by\n].argsort()].cumsum()\nquantiles = np.linspace(0, 1, q + 1)\nbins = pd.cut(order \/ order.iloc[-1], quantiles, labels=False).sort_index()\npredictions_df['weighted_cut'] = bins\npredictions_df.head()\n\npredicted_mean = []\nactual_mean = []\nfor x in np.arange(10):\n    pred = predictions_df[predictions_df['weighted_cut'] == x][predicted_colm].mean()\n    predicted_mean.append(pred)\n    actual = predictions_df[predictions_df['weighted_cut'] == x][actual_colm].mean()\n    actual_mean.append(actual)\n\n# Check on the above\nassert (predictions_df.groupby('weighted_cut').agg(\n    pred=(predicted_colm, 'mean'),\n    actual=(actual_colm, 'mean'),\n    n_rows=('IDpol', 'size'),\n    pred_sum=(predicted_colm, 'sum'),\n    actual_sum=(actual_colm, 'sum'),\n).assign(\n    pred_mean=lambda x: x.pred_sum \/ x.n_rows,\n    actual_mean=lambda x: x.actual_sum \/ x.n_rows,\n    diff_pred=lambda x: x.pred_mean - predicted_mean,\n    diff_actual=lambda x: x.actual_mean - actual_mean,\n)[['diff_pred', 'diff_actual']].sum() == [0,0]).all()\n\nmeans = pd.DataFrame(data = list(zip(predicted_mean,actual_mean)), columns = ['predicted','actual'])\n\n# Plot actual average against predicted average\nsns.scatterplot(data=means,x='actual',y='actual')\nsns.scatterplot(data=means, x='actual',y='predicted')\n\n# Calculate lift\na = means.iloc[9]['actual'] \/ means.iloc[0]['actual']\nb = means.iloc[9]['predicted'] \/ means.iloc[0]['predicted']\nprint(predicted_colm[:-12], 'actual differentiation', a)\nprint(predicted_colm[:-12], 'model differentiation', b)\nprint(predicted_colm[:-12], 'factor', b\/a)","c3e6b857":"stat_cols = ['RF_pred_Nb', 'ClaimNb']\nlift_plt_data_df = bplt.get_agg_plot_data(\n    df_validation_all.assign(\n        GLM_pred_Nb=lambda x: x.GLM_pred_freq * x.Exposure,\n        RF_pred_Nb=lambda x: x.RF_pred_freq * x.Exposure,\n        XGB_pred_Nb=lambda x: x.XGB_pred_freq * x.Exposure,\n    ),\n    stat_cols=stat_cols,\n    stat_wgt=None,\n    bucket_wgt='Exposure',\n    order_by=stat_cols[0],\n    cut_by='cum_wgt',\n    #x_axis_var=stat_cols[1],\n    n_bins=10\n)\n\n# Plot actual average against predicted average\nlift_plt = bplt.create_plot(lift_plt_data_df.assign(\n    x_left=lambda x: x[stat_cols[1] + '_wgt_av'],\n    x_right=lambda x: x[stat_cols[1] + '_wgt_av'],\n    x_mid=lambda x: x[stat_cols[1] + '_wgt_av'],\n), stat_cols=stat_cols)\nshow(lift_plt)\n\n# Plot lines of actual average and predicted average\nlift_plt = bplt.create_plot(lift_plt_data_df, stat_cols=stat_cols)\nshow(lift_plt)\n\n# Calculate lift\nlift = {\n    stat_col: lift_plt_data_df[stat_col + \"_wgt_av\"].agg(\n        lambda x: x.iloc[-1] \/ x.iloc[0])\n    for stat_col in stat_cols\n}\nprint(f'Lift on predicted:\\t{lift[stat_cols[0]]:.3f}')\nprint(f'Lift on actuals:\\t{lift[stat_cols[1]]:.3f}')","0cadedbf":"# Packages needed for this section\nimport xgboost as xgb\nimport scipy.stats as sps","46293830":"# Simulate data\nsize = 10000\n\ndf = pd.DataFrame({\n    'x1': sps.randint(low=0, high=2).rvs(size=size, random_state=67),\n    'x2': sps.randint(low=0, high=2).rvs(size=size, random_state=92),\n    'exposure': sps.uniform(loc=1, scale=9).rvs(size=size, random_state=67) * 0.3,\n}).assign(\n    frequency=lambda x: np.where((x.x1 == 1) & (x.x2 == 1), 2, 1),\n    claims=lambda x: sps.poisson(mu=x.frequency * x.exposure).rvs(size=size, random_state=14),\n)","18f91d4a":"# xgboost: set up\nparam0 = {\n    \"objective\": \"count:poisson\",\n    \"eval_metric\": \"poisson-nloglik\",\n    \"eta\": 1,\n    \"subsample\": 1,\n    \"colsample_bytree\": 1,\n    \"min_child_weight\": 1,\n    \"max_depth\": 2,\n    \"lambda\": 0,\n}\n\n# It is a simple pattern in the data, \n# so should be able to get close with few rounds\nnum_boost_round = 1","ffe71634":"# 1: Try to use the 'weight' argument\nxgtrain1 = xgb.DMatrix(\n    df[['x1', 'x2']],\n    label = df.claims,\n    weight = df.exposure\n)\nxgb_mod1 = xgb.train(\n    dtrain=xgtrain1, params=param0,\n    num_boost_round=num_boost_round,\n)\ndf = df.assign(\n    XGB_P1_Freq=xgb_mod1.predict(xgtrain1),\n)","64199d0d":"# 2: Try to set an offset in the DMatrix\nxgtrain2 = xgb.DMatrix(\n    df.assign(\n        offset=lambda x: np.log(x.exposure)\n    )[['x1', 'x2', 'offset']],\n    label = df.claims,\n)\nxgb_mod2 = xgb.train(\n    dtrain=xgtrain2, params=param0,\n    num_boost_round=num_boost_round,\n)\ndf = df.assign(\n    XGB_P2_Freq=xgb_mod2.predict(xgtrain2),\n)","4f03d5d3":"# 3: Try to set base margin as exposure\nxgtrain3 = xgb.DMatrix(\n    df[['x1', 'x2']],\n    label = df.claims,\n)\n\nxgtrain3_w_bm = xgb.DMatrix(\n    df[['x1', 'x2']],\n    label = df.claims,\n)\nxgtrain3_w_bm.set_base_margin(np.log(df.exposure))\n\nassert xgtrain3.get_base_margin().shape[0] == 0\nassert np.max(np.abs(xgtrain3_w_bm.get_base_margin() - np.log(df.exposure))) < 1e-6\n\nxgb_mod3 = xgb.train(\n    dtrain=xgtrain3_w_bm, params=param0,\n    num_boost_round=num_boost_round,\n)\n\ndf = df.assign(\n    # If you do *not* set base margin, the assumption is 0.5 *not* 1\n    XGB_P3_Freq_no_exp=lambda x: xgb_mod3.predict(xgtrain3) \/ 0.5,\n    XGB_P3_Freq_w_exp=lambda x: xgb_mod3.predict(xgtrain3_w_bm) \/ x.exposure,\n    \n    XGB_P3_Nb_no_exp=lambda x: x.XGB_P3_Freq_no_exp * x.exposure,\n    XGB_P3_Nb_w_exp=lambda x: x.XGB_P3_Freq_w_exp * x.exposure,\n)","a60b8301":"df.groupby(['x1', 'x2']).mean()","352fd029":"fig, ax = plt.subplots(figsize=(20, 10))\nxgb.plot_tree(xgb_mod3, num_trees=0, ax=ax)\nplt.show()","e36efe5c":"import inspect\nimport textwrap\nimport re\nimport functools","af398b08":"# Example data\nsize = 20\nexample_df = pd.DataFrame({\n    'cat': pd.Series(['A','B'])[sps.randint(low=0, high=2).rvs(size=size, random_state=67)],\n    'field1': np.linspace(1, 10, size),\n    'field2': np.linspace(10, -70, size),\n    'exp': sps.uniform(loc=1, scale=9).rvs(size=size, random_state=67) * 0.3,\n})","79e563c1":"# We want to put the following into a function with variables.\n# But we also want to be able to extract this query as code.\nexample_df.assign(\n    wgt=lambda x: x.exp,\n    field1_x_exp=lambda x: x.field1 * x.wgt\n).groupby('cat').agg(\n    wgt_sum=('wgt', 'sum'),\n    field1_wgt_sum=('field1_x_exp', 'sum'),\n).assign(\n    field1_wgt_av=lambda x: x.field1_wgt_sum \/ x.wgt_sum\n)","7d0e0e0c":"# Here is the parametrised query\ndf = example_df\nwgt_col = 'exp'\nstat_cols = ['field1']\ncut_by = 'cat'\n\ndf.assign(\n    wgt=lambda x: x[wgt_col],\n    **{\n        stat_col + '_x_exp': \n        lambda x, stat_col=stat_col: x[stat_col] * x.wgt \n        for stat_col in stat_cols\n    },\n).groupby(cut_by).agg(\n    wgt_sum=('wgt', 'sum'),\n    **{\n        stat_col + '_wgt_sum': (\n            stat_col + '_x_exp',\n            'sum'\n        ) for stat_col in stat_cols\n    },\n).assign(\n    **{\n        stat_col + '_wgt_av': \n        lambda x, stat_col=stat_col: x[stat_col + '_wgt_sum'] \/ x.wgt_sum \n        for stat_col in stat_cols\n    },\n)","bb400f13":"def get_assign_dict(assign_dict, dict_name, replacement_dict):\n    replace = \"**\" + dict_name\n    replace_with = ',\\n    '.join([\n        key + \"=\" + inspect.getsource(val).strip(\n        ).replace(\n            \"stat_col\", f\"'{key}'\"\n        ).replace(\n            f\", '{key}'='{key}'\", \"\"\n        )\n        for key, val in assign_dict.items()\n    ])\n    replacement_dict[replace] = replace_with\n    return(assign_dict)","0f401edd":"def get_inner_code(\n    func,\n    from_after_re=r'#\\s*<Query begin>.*\\n',\n    to_before_re=r'\\n\\s*#\\s*<Query end>',\n):\n    code_raw_str = inspect.getsource(get_wgt_av)\n    \n    # Remove the first row, and de-indent the remainder\n    code_body_str = textwrap.dedent(\n        re.sub(r'.+:\\n', r'', code_raw_str)\n    )\n    \n    # Find the specified start and end patterns\n    from_idx = re.search(from_after_re, code_body_str).end()\n    to_idx = re.search(to_before_re, code_body_str).start()\n    \n    return(code_body_str[from_idx:to_idx])","e382b8c9":"# Put it in a function\ndef get_wgt_av(df, wgt_col, stat_cols, cut_by=None, return_code=False):\n    # Capture the arg names. Must do this first\n    arg_names = list(locals().keys())\n    \n    # Set default parameter values\n    if cut_by is None:\n        cut_by = 'cat'\n    \n    # Create dictionary to map argument names to string values\n    replacement_dict = dict()\n    for var_name in arg_names:\n        var_val = locals()[var_name]\n        if var_name in ['df']:\n            continue\n        if isinstance(var_val, str):\n            replacement_dict[var_name] = f\"'{var_val}'\"\n            continue\n        replacement_dict[var_name] = var_val\n    \n    # Unpack iterable arguments\n    extra_cols_on_input_df = {\n        stat_col + '_x_exp': \n        lambda x, stat_col=stat_col: x[stat_col] * x.wgt \n        for stat_col in stat_cols\n    }\n    replacement_dict['**extra_cols_on_input_df'] = ',\\n    '.join([\n        f\"{stat_col}_x_exp=\"\n        f\"lambda x: x['{stat_col}'] * x.wgt\"\n        for stat_col in stat_cols\n    ])\n    \n    agg_cols = {\n        stat_col + '_wgt_sum': (\n            stat_col + '_x_exp', 'sum'\n        ) for stat_col in stat_cols\n    }\n    replacement_dict['**agg_cols'] = ',\\n    '.join([\n        f\"{stat_col}_wgt_sum=\"\n        f\"('{stat_col}_x_exp', 'sum')\"\n        for stat_col in stat_cols\n    ])\n    \n    extra_cols_on_result = {\n        stat_col + '_wgt_av': \n        lambda x, stat_col=stat_col: x[stat_col + '_wgt_sum'] \/ x.wgt_sum \n        for stat_col in stat_cols\n    }\n    replacement_dict['**extra_cols_on_result'] = ',\\n    '.join([\n        f\"{stat_col}_wgt_av=\"\n        f\"lambda x: x['{stat_col}_wgt_sum'] \/ x.wgt_sum\"\n        for stat_col in stat_cols\n    ])\n    \n    # If we're just getting the code, no need to run the query below\n    if return_code:\n        query_code = get_inner_code(get_wgt_av)\n        code_w_vals = functools.reduce(\n            lambda code_str, arg_item: code_str.replace(*arg_item),\n            {key: str(val) for key, val in replacement_dict.items()}.items(),\n            query_code\n        )\n        return(code_w_vals)\n    \n    # <Query begin> # (This is a special command, do not modify)\n    res = df.assign(\n        wgt=lambda x: x[wgt_col],\n        **extra_cols_on_input_df,\n    ).groupby(cut_by).agg(\n        wgt_sum=('wgt', 'sum'),\n        **agg_cols,\n    ).assign(\n        **extra_cols_on_result,\n    )\n    # <Query end> # (This is a special command, do not modify)\n    return(res)","1dc1ed6e":"# Test it works\ncode_example = get_wgt_av(\n    df = example_df,\n    wgt_col = 'exp',\n    stat_cols = ['field1'],\n    cut_by = 'cat',\n    return_code=True\n)\nprint(code_example)","6d19496e":"run_this_chunk = True\nif run_this_chunk:\n    exec(code_example)\n    assert res.equals(get_wgt_av(\n        df = example_df,\n        wgt_col = 'exp',\n        stat_cols = ['field1'],\n        cut_by = 'cat',\n    ))\n    print(\"Correct: Evaluated string equals function result\")\n    display(res)","8987ac9f":"# Another test\nparams = {\n    'wgt_col': 'exp',\n    'stat_cols': ['field1', 'field2'],\n    'cut_by': 'cat'\n}\ncode_example2 = get_wgt_av(\n    df = example_df,\n    **params,\n    return_code=True\n)\nprint(code_example2)","7bf3d002":"run_this_chunk = True\nif run_this_chunk:\n    exec(code_example2)\n    assert res.equals(get_wgt_av(\n        df = example_df,\n        **params,\n    ))\n    print(\"Correct: Evaluated string equals function result\")\n    display(res)","3b6546ec":"# French car insurance claims: Model comparisons\nComparing models built to predict car insurance claim frequencies. The models are from:\n- Poisson Generalised Linear Model (GLM): <https:\/\/www.kaggle.com\/btw78jt\/models-of-french-motor-claims>\n- Gradient Boosting Maching (GBM) using `xgboost`: <https:\/\/www.kaggle.com\/chun88\/chuns-french-motor-claims-project>\n- Random Forest: <https:\/\/www.kaggle.com\/alexanderfarquharson\/alex-f-french-motor-claims-analysis>\n\nThe modelling data is from: <https:\/\/www.kaggle.com\/floser\/french-motor-claims-datasets-fremtpl2freq>","40ffda68":"## Modelling data","3e1633f6":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>","3eb1f52c":"## Lift","9e741503":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Load data","1bd9caa9":"## Join data","9b774df2":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Setup","b9600b69":"## Model predictions","123a79a3":"## Specifying exposure for `xgboost`\nExamples adapted from here: <https:\/\/stackoverflow.com\/questions\/35660588\/xgboost-poisson-distribution-with-varying-exposure-offset>","9f09dc6d":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n# Visualise fit","2e7fbd33":"### Conclusions\n- The first two methods (using `weight` or putting the offset in the `DMatrix`) are not the correct way of specifying that the model should use exposure.\n- `base_margin` is the correct way of doing it.\n- With this very simple pattern, `xgboost` would need multiple boosting rounds to replicate it (even though we've set the parameters to try to use the least possible number of rounds).\n- The `base_margin` that is associated with the `DMatrix` *is* considered when passing it to the `xgb.predict()` method. That is, the result is:\n    $$\n    \\exp(\\ln(e_i) + \\eta_i) = \\hat{y}_i\n    $$\n    where $\\ln(e_i)$ is the `base_margin`. This is the number of claims predicted on that obseration. So get the predicted frequency, we therefore need to divide by $e_i$.\n- If no `base_margin` is specified, then the default is **0.5**, not 1 as you might expect.","b52fab8f":"<div align=\"right\" style=\"text-align: right\"><a href=\"#Contents\">Back to Contents<\/a><\/div>\n\n## Rough work only","49d45938":"## Function that can also return its code","ccfdf18f":"### Recreate Alex F's plots to debug","ddd040df":"## Config variables","00b157e5":"## Split for modelling\nFor this kernel, we want the validation data.","b3c249af":"<!-- This table of contents is updated *manually* -->\n# Contents\n1. [Setup](#Setup): Import packages, Config variables\n1. [Load data](#Load-data): Modelling data, Split for modelling, Model predictions, Join data\n1. [Visualise fit](#Visualise-fit): Lift\n1. [Rough work only](#Rough-work-only): Specifying exposure for `xgboost`, Function that can also return its code"}}