{"cell_type":{"44ec656d":"code","1703791d":"code","87a1f2e5":"code","742edc84":"code","24d6c60d":"code","535d5fa8":"code","46317d3d":"code","67a8690f":"code","9929e7a8":"code","fa30bf32":"code","d8dce985":"code","ef95bce1":"code","57a3ef79":"code","5f8e98e3":"code","3b413747":"code","786a4a9e":"code","740f6bfb":"code","e9805d9d":"code","f607f3bb":"code","1b215a75":"code","db557180":"code","7c9fae5a":"code","a081cf3a":"code","ebf538f7":"code","0ba10db2":"code","f3899449":"code","fb48efda":"markdown","3dd0f7e2":"markdown","e461a96f":"markdown","3e590315":"markdown","d5e87f4a":"markdown","266b228f":"markdown","af9bed7d":"markdown","602418ce":"markdown","2382d2f6":"markdown","41b0cb2f":"markdown","cd996e94":"markdown","c84bf0bb":"markdown","0be13f90":"markdown","c6c3ea8e":"markdown","be6deb79":"markdown","b36bd8eb":"markdown","9226e115":"markdown","86134ecf":"markdown","38e957d1":"markdown","92e46bbe":"markdown","76fb7a54":"markdown","c1c4e35d":"markdown","126cc792":"markdown","d6981638":"markdown","c211b57b":"markdown","cecc97f1":"markdown","05a0f0a2":"markdown","a47b95f6":"markdown","c3a0b1f7":"markdown"},"source":{"44ec656d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=2)\nprint(os.listdir(\"..\/input\"))","1703791d":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","87a1f2e5":"#reference : https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\n# def reduce_mem_usage(props):\n#     start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n#     print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n#     NAlist = [] # Keeps track of columns that have missing values filled in. \n#     for col in props.columns:\n#         if props[col].dtype != object:  # Exclude strings\n#             IsInt = False\n#             mx = props[col].max()\n#             mn = props[col].min()\n#             if not np.isfinite(props[col]).all(): \n#                 NAlist.append(col)\n#                 props[col].fillna(mn-1,inplace=True)                     \n#             asint = props[col].fillna(0).astype(np.int64)\n#             result = (props[col] - asint)\n#             result = result.sum()\n#             if result > -0.01 and result < 0.01:\n#                 IsInt = True\n#             if IsInt:\n#                 if mn >= 0:\n#                     if mx < 255:\n#                         props[col] = props[col].astype(np.uint8)\n#                     elif mx < 65535:\n#                         props[col] = props[col].astype(np.uint16)\n#                     elif mx < 4294967295:\n#                         props[col] = props[col].astype(np.uint32)\n#                     else:\n#                         props[col] = props[col].astype(np.uint64)\n#                 else:\n#                     if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n#                         props[col] = props[col].astype(np.int8)\n#                     elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n#                         props[col] = props[col].astype(np.int16)\n#                     elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n#                         props[col] = props[col].astype(np.int32)\n#                     elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n#                         props[col] = props[col].astype(np.int64) \n#             else:\n#                 props[col] = props[col].astype(np.float32)            \n\n#     print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n#     mem_usg = props.memory_usage().sum() \/ 1024**2 \n#     print(\"Memory usage is: \",mem_usg,\" MB\")\n#     print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n#     return props, NAlist","742edc84":"print(train_transaction.shape, test_transaction.shape)\nprint(train_identity.shape, test_identity.shape)","24d6c60d":"train_identity.head()","535d5fa8":"train_transaction.head()","46317d3d":"train_transaction.describe()","67a8690f":"print(round(train_transaction['isFraud'].value_counts(normalize=True) * 100,2))\ntrain_transaction['isFraud'].astype(int).plot.hist();","9929e7a8":"# Reference : https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction\ndef missing_values_table(df):    \n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n\n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    return mis_val_table_ren_columns","fa30bf32":"missing_values_iden = missing_values_table(train_identity)\nmissing_values_iden.head(10)","d8dce985":"missing_values_trans = missing_values_table(train_transaction)\nmissing_values_trans.head(10)","ef95bce1":"print('\\n', 'Number of each type of column')\nprint(train_identity.dtypes.value_counts())\nprint('\\n', 'Number of unique classes in each object column')\nprint(train_identity.select_dtypes('object').apply(pd.Series.nunique, axis = 0).sort_values(ascending = False))","57a3ef79":"print('\\n', 'Number of each type of column')\nprint(train_transaction.dtypes.value_counts())\nprint('\\n', 'Number of unique classes in each int column')\nprint(train_transaction.select_dtypes('int').apply(pd.Series.nunique, axis = 0).sort_values(ascending = False))\nprint('\\n', 'Number of unique classes in each object column')\nprint(train_transaction.select_dtypes('object').apply(pd.Series.nunique, axis = 0).sort_values(ascending = False))","5f8e98e3":"#Visualize except float type features.\ndef bar_plot(col, data, hue=None):\n    f, ax = plt.subplots(figsize = (30, 5))\n    sns.countplot(x=col, hue=hue, data=data, alpha=0.5)\n\n#Visualize float type features.\ndef dist_plot(col, data):\n    f, ax = plt.subplots(figsize = (30, 5))\n    sns.distplot(data[col].dropna(), kde=False, bins=10)","3b413747":"fig, ax = plt.subplots(1, 2, figsize=(30,15))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain')\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction)\nax[1].set_title('R_emaildomain')","786a4a9e":"bar_plot('ProductCD', train_transaction, hue='isFraud')","740f6bfb":"card_float = ['card1', 'card2', 'card3', 'card5']\nfor col in card_float:\n    print(col, train_transaction[col].nunique())","e9805d9d":"card_n_float = ['card4', 'card6']    \n\nfor col in card_n_float:\n    bar_plot(col, train_transaction, hue='isFraud')","f607f3bb":"card_float = ['addr1', 'addr2']\nfor col in card_float:\n    dist_plot(col, train_transaction)","1b215a75":"C_col = [c for c in train_transaction if c[0] == 'C']\ncorr = train_transaction[C_col].corr()\n\ncmap = sns.color_palette(\"Blues\")\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr, cmap=cmap)","db557180":"D_col = [c for c in train_transaction if c[0] == 'D']\ncorr = train_transaction[D_col].corr()\n\ncmap = sns.color_palette(\"Blues\")\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr, cmap=cmap)","7c9fae5a":"M_col = [c for c in train_transaction if c[0] == 'M']\nfor col in M_col:\n    bar_plot(col, train_transaction, hue='isFraud')","a081cf3a":"V_col = [c for c in train_transaction if c[0] == 'V']\ntrain_transaction[V_col].describe()","ebf538f7":"train_transaction['TransactionDT'].plot(kind='hist', figsize=(15, 5), label='train_transaction', bins=100)\ntest_transaction['TransactionDT'].plot(kind='hist', label='test_transaction', bins=100)\nplt.legend()\nplt.show()","0ba10db2":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionAmt'].values\n\nsns.distplot(train_transaction['TransactionAmt'].values, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[0].set_xlim([min(train_transaction['TransactionAmt'].values), max(train_transaction['TransactionAmt'].values)])\n\nsns.distplot(np.log(train_transaction['TransactionAmt'].values), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(train_transaction['TransactionAmt'].values)), max(np.log(train_transaction['TransactionAmt'].values))])\n\nplt.show()","f3899449":"print('\\n', 'Number of each type of column')\nprint(train_identity.dtypes.value_counts())\nprint('\\n', 'Number of unique classes in each object column')\nprint(train_identity.select_dtypes('object').apply(pd.Series.nunique, axis = 0).sort_values(ascending = False))","fb48efda":"# features - transaction\n\n* emaildomain\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).","3dd0f7e2":"### emaildomain","e461a96f":"# Column Types","3e590315":"Some categorical variables have a relatively large number of unique entries. \nWe will need to find a way to deal with these categorical variables.\nBecause of the large number of columns, I think it would be better to do frequency encoding or mean encoding than one-hot encoding.","d5e87f4a":"### TransactionAMT","266b228f":"### Product","af9bed7d":"### V1 - V339","602418ce":"- D1, D2\n- D4, D12, D6\n- D5, D7","2382d2f6":"# Load data","41b0cb2f":"- imbalanced data set (more than 75% of isFraud is filled with 0).\n- a lot of NaN values.\n","cd996e94":"### Purpose : Predicting the probability that an online transaction is fraudulent\n### Target : isFraud(binary target)**\n\nThe data is broken into two files \"identity\" and \"transaction\", which are joined by \"TransactionID\". \n\nNot all transactions have corresponding identity information.","c84bf0bb":"The following hist shows that train_transaction and test_transaction were split by TransactionDT. So it would be prudent to use time-based split for validation.","0be13f90":"There are many columns with over 90% missing values.\nIt is important to deal with missing values.","c6c3ea8e":"## Categorical features - identity\n\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n","be6deb79":"# Examine the Distribution of the Target Colum","b36bd8eb":"Most of the columns are filled with zero","9226e115":"# Exploratoy Data Analysis","86134ecf":"### M1 - M9","38e957d1":"### D1 - D9","92e46bbe":"### addr1, addr2\n","76fb7a54":"The results show that the number of columns in the train_identity and test_identity is the same.","c1c4e35d":"### C1 - C14","126cc792":"To be continue...","d6981638":"To save memory, you can use reduce_mem_usage function. ","c211b57b":"### TransactionDT","cecc97f1":"There are many variables with a correlation of 1. Therefore, it is necessary to process the correlated variables.","05a0f0a2":"From this information, we see this is an imbalanced class problem (Only have 3.5% of positive values). So, we can weight the classes by their representation in the data to reflect this imbalance.","a47b95f6":"### card1 - card6","c3a0b1f7":"# Examine Missing Values"}}