{"cell_type":{"1e4a8dcf":"code","fbfb6d97":"code","f4915c7f":"code","bf6e679d":"code","dd4907d0":"code","04876408":"code","c665b9e3":"code","a96ac99f":"code","2cecc7ab":"code","027516d7":"code","2868d265":"code","df66268a":"code","2920a93e":"code","5daae6af":"code","ab1d22a1":"code","981982aa":"markdown"},"source":{"1e4a8dcf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(rc={'figure.figsize': [7, 7]}, font_scale=1.2)","fbfb6d97":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf","f4915c7f":"df.info()","bf6e679d":"df.describe()","dd4907d0":"df.isnull().sum()","04876408":"df.info()","c665b9e3":"plt.figure(figsize=(20,10))  #This is used to change the size of the figure\/ heatmap\nsns.heatmap(df.corr(), annot=True, fmt='.0%')","a96ac99f":"df.shape","2cecc7ab":"df","027516d7":"X = df.drop(['Outcome'],axis=1)\ny = df['Outcome']","2868d265":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","df66268a":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","2920a93e":"def models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n    \n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n \n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n  forest.fit(X_train, Y_train)\n  \n    \n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n    \n    \n  \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train)*100)\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train)*100)\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train)*100)\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train)*100)\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train)*100)\n  print('[5]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train)*100)\n  \n  \n  return log, knn, svc_lin, svc_rbf, gauss,forest\n\nmodel = models(X_train,Y_train)","5daae6af":"#Classification accuracy is the ratio of correct predictions to total predictions made.\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line\n\n#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line","ab1d22a1":"#Print Prediction of Random Forest Classifier model\npred = model[5].predict(X_test)\nprint(pred)\n#Print a space\nprint()\n#Print the actual values\nprint(Y_test)","981982aa":"### Pima Indians Diabetes Database\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n\n[![image.png](attachment:image.png)](http:\/\/)\n\n\n#### We use 6 algorithms\n\n* 1-Logistic Regression. \n* 2-K Nearest Neighbor.\n* 3-Support Vector Machine.\n* 4-Support Vector Machine.\n* 5-Gaussian Naive Bayes. \n* 6-Random Forest Classifier.\n\n#### Dataset\n\n[link](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)\n\n"}}