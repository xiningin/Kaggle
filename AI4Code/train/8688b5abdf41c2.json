{"cell_type":{"babb8582":"code","b515023b":"code","a29123ec":"code","24835527":"code","4d605346":"code","078e1627":"code","942cee7f":"code","29a7bfc7":"code","ea564764":"code","4f80856b":"code","b39f3c83":"code","0d8c3e56":"code","8e316e88":"markdown","0d6a8d21":"markdown"},"source":{"babb8582":"import tensorflow as tf\nimport keras \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport sys\nimport random","b515023b":"image_size = 1024\ninput_size = 331","a29123ec":"def img_transf(img):\n    img -= img.mean()\n    img \/= np.maximum(img.std(), 1\/image_size**2) #prevent \/0\n    return img","24835527":"from skimage.transform import rotate\ndef rand_crop(img):\n    img \/= 255\n    theta = 5*random.randint(0,9)\n    img = rotate(img,theta, resize=False)\n    max_height = np.floor(image_size\/(np.cos(np.pi*theta\/180)+np.sin(np.pi*theta\/180)))\n    min_border = np.ceil((image_size-max_height)\/2)\n    h = random.randint(input_size, max_height) \n    cx = random.randint(min_border, min_border+max_height-h)\n    cy = random.randint(min_border, min_border+max_height-h)\n    cropped_img = img[cx:cx+h,cy:cy+h,...]\n    return cv2.resize(cropped_img, (input_size,input_size))","4d605346":"from keras.preprocessing.image import ImageDataGenerator\ndata_dir = '..\/input\/neuron cy5 full\/Neuron Cy5 Full'\n\ndata_gen = ImageDataGenerator(horizontal_flip=True,\n                              vertical_flip=True,\n                              validation_split=0.2,\n                              preprocessing_function = img_transf)\ntrain_gen = data_gen.flow_from_directory(data_dir, \n                                         target_size=(image_size,image_size),\n                                         color_mode='grayscale',\n                                         class_mode='categorical',\n                                         batch_size=16, \n                                         subset='training',\n                                         shuffle=True)\ntest_gen = data_gen.flow_from_directory(data_dir, \n                                        target_size=(image_size, image_size),\n                                        color_mode='grayscale',\n                                        class_mode='categorical',\n                                        batch_size=16, \n                                        subset='validation',\n                                        shuffle=True)\n\nclasses = dict((v, k) for k, v in train_gen.class_indices.items())\nnum_classes = len(classes)","078e1627":"def crop_gen(batches):\n    while True:\n        batch_x, batch_y = next(batches)\n        batch_crops = np.zeros((batch_x.shape[0], input_size, input_size, 1))\n        for i in range(batch_x.shape[0]):\n            batch_crops[i,...,0] = rand_crop(batch_x[i])\n        yield (batch_crops, batch_y)","942cee7f":"from tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.layers import Dense, GlobalMaxPooling2D\n\n\npretrained_model = ResNet50(include_top=False,\n                         pooling='none',\n                         input_shape=(input_size, input_size, 3),\n                        weights='imagenet')\ncfg = pretrained_model.get_config()\ncfg['layers'][0]['config']['batch_input_shape'] = (None, input_size, input_size, 1)\nresnet_model = Model.from_config(cfg)\nfor i, layer in enumerate(resnet_model.layers):\n    if i == 1:\n        new_weights = pretrained_model.layers[i].get_weights()[0].sum(axis=2, keepdims=True)\n        resnet_model.set_weights([new_weights])\n        layer.trainable = False\n    else: \n        layer.set_weights(pretrained_model.layers[i].get_weights())\n        layer.trainable = False\n\nx = GlobalMaxPooling2D()(resnet_model.output)\nx = Dense(2048, activation='relu')(x)\nx = Dense(2048, activation='relu')(x)\noutp = Dense(num_classes, activation='softmax')(x)\nmodel = Model(resnet_model.input, outp)        \n    \nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","29a7bfc7":"history = model.fit_generator(crop_gen(train_gen),\n                              epochs=5,\n                              steps_per_epoch=4*len(train_gen), #effectively 1 run through every possibility of reflected data\n                              validation_data=crop_gen(test_gen),\n                              validation_steps=len(test_gen), \n                              verbose=1)","ea564764":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss','val_loss'], loc='upper right');\nplt.title('Learning curve for the training of Dense Layers')\nplt.show()\nprint('Final val_acc: '+history.history['val_acc'][-1].astype(str))","4f80856b":"from tensorflow.python.keras.optimizers import Adam\n\nfor layer in model.layers:\n    layer.trainable = True\nadam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #20x smaller than standard\nmodel.compile(optimizer=adam_fine, loss='binary_crossentropy', metrics=['accuracy'])","b39f3c83":"history2 = model.fit_generator(crop_gen(train_gen),\n                              epochs=20,\n                              steps_per_epoch=4*len(train_gen), #effectively 1 run through every possibility of reflected data\n                              validation_data=crop_gen(test_gen),\n                              validation_steps=len(test_gen), \n                              verbose=1)","0d8c3e56":"full_history = dict()\nfor key in history.history.keys():\n    full_history[key] = history.history[key]+history2.history[key][1:] #first epoch is wasted due to initialisation of momentum\n    \nplt.plot(full_history['loss'])\nplt.plot(full_history['val_loss'])\nplt.legend(['loss','val_loss'], loc='upper right')\nplt.title('Full Learning curve for the training process')\nplt.show()\nprint('Final val_acc: '+full_history['val_acc'][-1].astype(str))","8e316e88":"Firstly some standard imports.","0d6a8d21":"In attempt to reduce the overfitting of the model, the images are randomly rotated to effectively increase the size of the training set. Guassian Noise is also added during training time."}}