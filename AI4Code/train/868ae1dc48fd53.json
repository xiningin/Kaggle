{"cell_type":{"63e6ae00":"code","115939b4":"code","0e6459bd":"code","8d93ebdd":"code","0a419630":"code","914ec86a":"code","be323995":"code","8fc9e16a":"code","96fcf420":"code","bda598f5":"code","3d0a1f7e":"code","3a68697d":"code","d2bb7fb2":"code","21cae884":"code","4e4c9d7c":"code","f0fab76b":"code","a9e0e4c4":"code","9a679f1c":"code","3a930243":"code","c6113eac":"code","11517a3d":"code","074dc8c9":"code","2bb1feb2":"code","38ffde6a":"code","c90bf465":"code","47b2a149":"code","7688f381":"code","a2287ef2":"code","0677089a":"code","7389e659":"code","b311d085":"code","ecf73206":"code","01bbaa09":"code","f1b5f966":"code","651f51fb":"code","e1dd00e0":"code","cee8b2f9":"code","c401ba08":"code","13d17bf6":"code","cb34b01c":"code","8438d681":"code","0066e5d3":"code","4957066b":"code","fbec560f":"code","94dde635":"code","4f6339f2":"code","1969aeca":"code","d4dae7c1":"code","08f2bc79":"code","0481ea17":"code","f98ac336":"markdown","2b3ecfeb":"markdown","5ceb4f1b":"markdown","c6a4c5cd":"markdown"},"source":{"63e6ae00":"specified_class_names = \"\"\"0. Nucleoplasm\n1. Nuclear membrane\n2. Nucleoli\n3. Nucleoli fibrillar center\n4. Nuclear speckles\n5. Nuclear bodies\n6. Endoplasmic reticulum\n7. Golgi apparatus\n8. Intermediate filaments\n9. Actin filaments \n10. Microtubules\n11. Mitotic spindle\n12. Centrosome\n13. Plasma membrane\n14. Mitochondria\n15. Aggresome\n16. Cytosol\n17. Vesicles and punctate cytosolic patterns\n18. Negative\"\"\"\n\nclass_names = [class_name.split('. ')[1] for class_name in specified_class_names.split('\\n')]\nclass_names","115939b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#import math, re, os\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#from kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\n#from functools import partial\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sn\nprint(\"Tensorflow version \" + tf.__version__)\nimport cv2\nfrom PIL import Image\n#from tensorflow_examples.models.pix2pix import pix2pix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e6459bd":"AUTOTUNE = tf.data.experimental.AUTOTUNE","8d93ebdd":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob('..\/input\/hpa-write-mask\/*.npz'),\n    test_size=0.35, random_state=28 ## was 0.35\n)","0a419630":"#!pip install  git+'https:\/\/github.com\/HasnainRaz\/SemSegPipeline.git'\n\n!pip install -q git+'https:\/\/github.com\/tensorflow\/examples.git'","914ec86a":"from tensorflow_examples.models.pix2pix import pix2pix","be323995":"#import hpacellseg.cellsegmentator as cellsegmentator\n#from hpacellseg.utils import label_cell,label_nuclei","8fc9e16a":"#AUTOTUNE = tf.data.experimental.AUTOTUNE\n#GCS_PATH = KaggleDatasets().get_gcs_path()\n#BATCH_SIZE = 1# 8 * strategy.num_replicas_in_sync\n#IMAGE_SIZE = [1024, 1024] # was 512\n#CLASSES = ['0', '1', '2', '3', '4']\n#EPOCHS = 25","96fcf420":"filename = '..\/input\/hpa-single-cell-image-classification\/train.csv'\n\ndf_train = pd.read_csv(filename)\n\n#df_train","bda598f5":"#df_train[~df_train[\"Label\"].str.contains('\\|', na = False)]","3d0a1f7e":"new_df = df_train[~df_train[\"Label\"].str.contains('\\|', na = False)].head(500)\nnew_df['images'] = '..\/input\/write-data-one-label\/' + new_df['ID'] + '_blend.png'\nnew_df['masks'] =  '..\/input\/hpa-write-mask\/' + new_df['ID'] + '.npz'\n","3a68697d":"pd.set_option('display.max_colwidth', None)\n#new_df","d2bb7fb2":"#filename = ['..\/input\/hpa-write-mask\/5e22a522-bb99-11e8-b2b9-ac1f6b6435d0.npz',\n#           '..\/input\/hpa-write-mask\/5c801c04-bb99-11e8-b2b9-ac1f6b6435d0.npz']","21cae884":"IMG_SIZE, BATCH_SIZE = [512,512], 4\nnum_classes = 20","4e4c9d7c":"def get_data_from_filename(filename):\n    npdata = np.load(filename)\n    label = npdata['label']\n    label[label >= 0.5] = 1\n    label[label < 0.5] = 0\n    label = cv2.resize(label, dsize=(512,512), interpolation=cv2.INTER_AREA)\n    image = npdata['image']\n    image = cv2.resize(image, dsize=(512,512), interpolation=cv2.INTER_AREA)\n    return tf.convert_to_tensor(image), tf.convert_to_tensor(label)\n    #return \n\n#def get_data_wrapper(filename):\n#   # Assuming here that both your data and label is float type.\n#    features,labels = tf.compat.v1.py_func(\n#       get_data_from_filename, [filename], (tf.float64,tf.float64)) \n#    return tf.data.Dataset.from_tensor_slices((features,labels))","f0fab76b":"def data_setshape(image, label):\n    image.set_shape([*IMG_SIZE, 3])\n    label.set_shape([*IMG_SIZE, 20])\n    return image,label","a9e0e4c4":"#a = get_data_from_filename('..\/input\/hpa-write-mask\/5e22a522-bb99-11e8-b2b9-ac1f6b6435d0.npz')","9a679f1c":"tf.config.run_functions_eagerly(True)","3a930243":"def get_training_dataset():\n    dataset_train = tf.data.Dataset.list_files(TRAINING_FILENAMES)\n    #dataset_train = dataset_train.shuffle(100)\n    dataset_train = dataset_train.repeat()\n    dataset_train = dataset_train.map(lambda name: tf.compat.v1.py_func(get_data_from_filename, [name],\n                                                        (tf.float32,\n                                                         tf.float64)))\n    dataset_train = dataset_train.map(data_setshape)\n    #\n    \n    dataset_train = dataset_train.batch(BATCH_SIZE)\n    dataset_train = dataset_train.prefetch(AUTOTUNE)\n    return dataset_train","c6113eac":"def get_validation_dataset():\n    dataset_val = tf.data.Dataset.list_files(VALID_FILENAMES)\n    dataset_val = dataset_val.map(lambda name: tf.compat.v1.py_func(get_data_from_filename, [name],\n                                                        (tf.float32,\n                                                         tf.float64)))\n    dataset_val = dataset_val.map(data_setshape)\n    dataset_val = dataset_val.batch(BATCH_SIZE)\n    dataset_val = dataset_val.cache()\n    dataset_val = dataset_val.prefetch(AUTOTUNE)\n    \n    return dataset_val","11517a3d":"traning_data = get_training_dataset()\nvalidation_data = get_validation_dataset()","074dc8c9":"for i in traning_data.take(1):\n    print(i[1][0])","2bb1feb2":"new_df1 = df_train[~df_train[\"Label\"].str.contains('\\|', na = False)].head(100)\nnew_df1['images'] = '..\/input\/write-data-one-label\/' + new_df1['ID'] + '_blend.png'\nnew_df1['masks'] =  '..\/input\/hpa-write-mask\/' + new_df1['ID'] + '.npz'\nnew_df1['masks']","38ffde6a":"new_df['Label'].value_counts()","c90bf465":"#datagen_im = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255)\n#datagen_mas = tf.keras.preprocessing.image.ImageDataGenerator()\n\n#new_df\n# seed, so that image_generator and mask_generator will rotate and shuffle equivalently\n#seed = 42\n#image_generator = datagen_im.flow_from_dataframe(new_df, \n#                                      directory='.', \n#                                      x_col='images', \n                                      #y_col='masks', \n #                                     batch_size=BATCH_SIZE, \n #                                     target_size = (IMG_SIZE, IMG_SIZE),\n #                                     class_mode=None,                                       \n #                                     seed=seed)\n#mask_generator = datagen_mas.flow_from_dataframe(new_df, \n#                                     directory='.', \n#                                     x_col='masks', \n                                     #y_col='images', # Or whatever \n#                                     batch_size=BATCH_SIZE,\n#                                     target_size = (IMG_SIZE, IMG_SIZE),\n#                                     class_mode=None, \n#                                     seed=seed)\n\n#train_generator = zip(image_generator, ds_mask)\n\n# same for validation data generator\n\n","47b2a149":"boolean_array = np.logical_and(i[1][0] > 0, i[1][0] < 1)\nin_range_indices = np.where(boolean_array)\n#boolean_array\nin_range_indices\n#print(in_range_indices)","7688f381":"fig = plt.figure(figsize=(30,30),constrained_layout = True)\nax = fig.add_subplot(1, 2, 1,)\nplt.imshow(i[0][0])\nax = fig.add_subplot(1, 2, 2,)\nplt.imshow(i[1][0][:,:,0])","a2287ef2":"my_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=3,verbose=0,monitor='val_categorical_accuracy'),\n    tf.keras.callbacks.ModelCheckpoint(filepath='HPA_model_eff3.h5',verbose=0,monitor='val_loss',save_best_only=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=2,\n        min_lr=1e-7,\n        mode='min',\n        verbose=0,\n    )\n    \n]","0677089a":"loss_func = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True)","7389e659":"base_model = tf.keras.applications.MobileNetV2(input_shape=[*IMG_SIZE, 3],weights='imagenet', include_top=False) #imagenet(input_shape=[1024, 1024, 3], include_top=False)\n\n# Use the activations of these layers\n\n#layer_names [\n#    'block6f_expand_activation',\n#    'block6f_se_expand',\n#    'block6e_expand_conv'\n#]\n\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n\ndown_stack.trainable = False\n#base_model.trainable = False\n","b311d085":"up_stack = [\n    pix2pix.upsample(1024, 3),\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]\n","ecf73206":"def unet_model(output_channels):\n    inputs = tf.keras.layers.Input(shape=[*IMG_SIZE, 3])\n    x = inputs\n\n  # Downsampling through the model\n    skips = down_stack(x)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip])\n\n  # This is the last layer of the model\n    last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n    x = last(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)\n","01bbaa09":"OUTPUT_CHANNELS = 20\n","f1b5f966":"model = unet_model(OUTPUT_CHANNELS)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['categorical_accuracy'])\n","651f51fb":"tf.keras.utils.plot_model(model, show_shapes=True)","e1dd00e0":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]\n","cee8b2f9":"#model.predict('..\/input\/write-data-one-label\/008a8630-bb9b-11e8-b2b9-ac1f6b6435d0_blend.png')\n","c401ba08":"#NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n#NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)","13d17bf6":"STEPS_PER_EPOCH = 650 \/\/ BATCH_SIZE\nVALID_STEPS = 350 \/\/ BATCH_SIZE","cb34b01c":"EPOCHS = 20\nVAL_SUBSPLITS = 5\n#VALIDATION_STEPS = 100\n\nmodel_history = model.fit(traning_data,\n                          epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          #validation_split=0.2,\n                          validation_steps=VALID_STEPS,\n                          validation_data=validation_data,\n                          callbacks=my_callbacks\n                         )\n","8438d681":"history_frame = pd.DataFrame(model_history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();","0066e5d3":"\n\ndef unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-18:]:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n             layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(\n        optimizer=optimizer, \n        #loss='sparse_categorical_crossentropy',  \n        loss=loss_func,\n        metrics=['categorical_accuracy']\n    )\n\n\nunfreeze_model(model)\n\nepochs = 10  # @param {type: \"slider\", min:8, max:50}\n\nhistory = model.fit(dataset_train, \n                    steps_per_epoch=100, \n                    epochs=15,\n                    validation_data=dataset_val,\n                    #validation_steps=VALID_STEPS,\n                   callbacks=my_callbacks)\n\n","4957066b":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();","fbec560f":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]\n","94dde635":"create_mask(a)","4f6339f2":"create_mask(a)\nplt.imshow(np.asarray(create_mask(a)))","1969aeca":"plt.imshow(np.asarray(b[0]))","d4dae7c1":"bild = cv2.imread('..\/input\/write-data-one-label\/5c801c04-bb99-11e8-b2b9-ac1f6b6435d0_blend.png', cv2.IMREAD_UNCHANGED)","08f2bc79":"bild = bild \/ 255","0481ea17":"filename = '..\/input\/hpa-single-cell-image-classification\/train.csv'\n\ndf_train = pd.read_csv(filename)\n\ndf_train","f98ac336":"# EDA","2b3ecfeb":"# Set up Variables","5ceb4f1b":"Image channels\n\nAll images have 4 channels: Red (Microtubules), Green (Protein of interest), Blue (Nucleus), Yellow (Endoplasmic reticulum) as followed:\n\n![](https:\/\/images.proteinatlas.org\/115\/663_E2_1_red_medium.jpg) ![](https:\/\/images.proteinatlas.org\/115\/663_E2_1_green_medium.jpg) ![](https:\/\/images.proteinatlas.org\/115\/663_E2_1_blue_medium.jpg) ![](https:\/\/images.proteinatlas.org\/115\/663_E2_1_yellow_medium.jpg)","c6a4c5cd":"# Helper Functions"}}