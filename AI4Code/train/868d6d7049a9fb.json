{"cell_type":{"09ea8c1f":"code","36048301":"code","6907e9f4":"code","b9017736":"code","2b85b7ef":"code","a34a99fa":"code","604d7d60":"code","087a32e4":"code","af779dcf":"code","db440f31":"code","21e8b274":"code","d01ecfc6":"code","de9c881f":"code","c4091ab4":"code","724907be":"code","f50a99a0":"code","7ce7ac62":"code","d098b6b5":"code","7eb45f27":"code","6e7ff3c6":"code","28d923f7":"code","3130791f":"code","1c646006":"code","96f0eeec":"code","0074f2b9":"code","75f312c0":"code","586ff3af":"code","153c83c6":"code","8d0ac132":"code","d5af6b89":"code","f71a8dfe":"code","38b174be":"code","e301b930":"code","5938a8cb":"code","eb3e5ca9":"code","8bbca7d4":"code","da7ef487":"code","fc947557":"code","db9a9b1e":"code","60e4d945":"code","df097c0f":"code","ded90032":"code","5e3f8cd3":"code","3be02881":"code","db34667b":"code","856a1108":"code","ef72e24a":"code","55e494b9":"code","b64396b2":"code","89abbb00":"code","1f6069f5":"code","4c33dcd0":"code","29e5d6ca":"code","35c5f4f2":"code","0d174a07":"code","0301df3e":"code","2b4fe3c6":"code","8e6e1540":"code","2c5c65f3":"code","766e5d87":"code","f513d560":"code","2d74ef65":"code","5607097b":"code","bb6292ce":"code","214b7763":"code","93d43cb3":"code","3362b7be":"code","d6140dc9":"code","ddf3462c":"code","7e447896":"code","2834aa8e":"code","cb02cea3":"code","f5bcc5cf":"code","3a8acb26":"code","232ab991":"code","c7571237":"code","c13fde58":"code","e5d79947":"markdown","5be6ef5f":"markdown","6f3c1c54":"markdown","6623545d":"markdown","7968e528":"markdown","0b8783d4":"markdown","07d0c204":"markdown","e02df9ff":"markdown","8096f8c8":"markdown","1dd900a7":"markdown","b94bedbb":"markdown","f4dd3f1f":"markdown","b961e5c7":"markdown","fd76fdf8":"markdown","46076e46":"markdown","39c49d58":"markdown","71e58f28":"markdown","456ef5bf":"markdown","5d52fc5c":"markdown","40c62e27":"markdown","b79fd329":"markdown","7f6291db":"markdown","748c9648":"markdown","1e06c34f":"markdown","2d612533":"markdown","cc454e3b":"markdown","04713a10":"markdown","fef2fb24":"markdown","ae1a0228":"markdown","8d0b5b7e":"markdown","fd5002fe":"markdown"},"source":{"09ea8c1f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport warnings\nfrom IPython.display import Markdown, display\n# Ignore warnings\nwarnings.filterwarnings('ignore')","36048301":"# Display markdown. This function is called when we have to display the observations programatically.\ndef printmd(string):\n    display(Markdown('**Observations:** ' + string))","6907e9f4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b9017736":"# Combining training and testing datasets since the data cleanup is same for both datasets.\n# This step avoids performing the same steps on test set again.\ndata = pd.concat([train, test], keys=('x', 'y'))","2b85b7ef":"# display all columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_info_columns', data.shape[1])","a34a99fa":"printmd('There are {} observations and {} features in the dataset.'.format(*data.shape))","604d7d60":"data.head()","087a32e4":"data.describe()","af779dcf":"data.info()","db440f31":"# Check number of unique values in all columns to identify which columns has to be converted to categorical type.\nunique_vals = []\ndtypes = []\nfor col in data.columns:\n    unique_vals.append(len(data[col].unique()))\n    dtypes.append(data[col].dtype)\ndf = pd.DataFrame({\n    'Column': data.columns,\n    'Unique_Count': unique_vals,\n    'Data Type': dtypes\n})\n# View all the observations of this dataframe.\npd.set_option('display.max_rows', data.shape[1])\nprint(df.sort_values('Unique_Count'))\npd.set_option('display.max_rows', 50)","21e8b274":"# If a column has finite list of unique values, it can be considered as categorical.\ncategory_cols = ['CentralAir', 'Street', 'Utilities', 'PavedDrive', 'Alley', 'BsmtHalfBath', 'HalfBath', 'LandSlope', \n                 'FullBath', 'GarageFinish', 'Fireplaces', 'LotShape', 'KitchenQual', 'KitchenAbvGr', 'PoolQC', 'LandContour',\n                 'ExterQual', 'BsmtFullBath', 'ExterCond', 'BsmtQual', 'HeatingQC', 'GarageCars', 'BsmtExposure', 'MasVnrType',\n                 'Fence', 'BldgType', 'MSZoning', 'LotConfig', 'RoofStyle', 'Foundation', 'FireplaceQu', 'SaleCondition', \n                 'GarageType', 'BsmtFinType1', 'BsmtFinType2', 'BedroomAbvGr', 'HouseStyle', 'OverallQual', 'OverallCond',\n                 'SaleType', 'Condition1', 'MiscFeature', 'BsmtCond', 'Heating', 'GarageQual', 'Electrical', 'GarageCond', \n                 'Functional', 'Condition2', 'RoofMatl', 'TotRmsAbvGrd', 'MoSold', 'MSSubClass', 'Exterior1st', 'Exterior2nd']\nfor col in category_cols:\n    data[col] = data[col].astype('category')","d01ecfc6":"data.info()","de9c881f":"# Check if ALL the column values in any of the rows are empty. If empty, those rows are removed.\ndata[data.isnull().all(axis=1)]","c4091ab4":"# Check whether ANY of the rows are having missing values > 40%. If found, those rows are removed.\ndata[100*(data.isnull().sum(axis=1)\/data.shape[1]) > 40]","724907be":"def get_missing_val_cols():\n    for col in data.columns:\n        missing_val_sum = data[col].isnull().sum()\n        missing_prcnt = round(100*(missing_val_sum\/len(data)), 2)\n        if missing_val_sum > 0:\n            print('There are {}% missing values in \"{}\"'.format(missing_prcnt, col))\nget_missing_val_cols()","f50a99a0":"# Remove columns having missing values > 90% (Alley, PoolQC, MiscFeature). According to data dictionary, the reason \n# for having null values is either because there is no pool or no fence depending on the column.\n# But, even if we define the missing values as a new category, it would aquire more than 90% of observations with this category.\n# Since one particular category withholds almost majority of the observations, this column might bias our predictions.\n\ncols_to_remove = ['Alley', 'PoolQC', 'MiscFeature']        \nprintmd(\"\"\"\n  - Number of columns having null values > {0}%: **{1}**\n  - List of columns having null values > {0}%: **{2}**\n  - Remove these columns since it doesn't contribute much for analysis\n\"\"\".format(90, len(cols_to_remove), ', '.join(cols_to_remove))\n)\ndata.drop(cols_to_remove, axis=1, inplace=True)","7ce7ac62":"# Imputing the missing values for the required columns with \"NotBuilt\". According to data dictionary, NA is is\n# nothing but the structure is not built. Ex: If Garage is NA, it means garage is not built.\n\ncols_to_impute_notbuilt = ['MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n                          'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'Fence']\n\nfor col in cols_to_impute_notbuilt:\n    if str(data[col].dtypes) == 'category':\n        # For categorical data type, the value should be imputed within the existing set of values. So, add \"NotBuilt\"\n        # to the list of categories.\n        data[col].cat.add_categories('NotBuilt', inplace=True)\n    data[col].fillna('NotBuilt', axis=0, inplace=True)","d098b6b5":"# Columns which have to be imputed with mode\nmode_impute_cols = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional']\nfor col in mode_impute_cols:\n    data[col].fillna(data[col].mode()[0], axis=0, inplace=True)","7eb45f27":"# When basement\/garage is not built, relevant columns which indicates the area of the basement should be imputed with 0.\nbsmt_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']\nfor col in bsmt_cols:\n    data[col].fillna(0, axis=0, inplace=True)","6e7ff3c6":"# For SaleType, there is a value called \"Other\". For missing values, we can impute with \"Oth\".\ndata.SaleType.fillna('Oth', axis=0, inplace=True)","28d923f7":"sns.displot(data['LotFrontage'], kind='kde')\nplt.show()\n# The frequency of distribution is high at 60 for LotFrontage. Hence, impute 60 for all missing values in this column.\ndata.LotFrontage.fillna(60, inplace=True)","3130791f":"data.MasVnrArea.plot(kind='kde')\nplt.show()\n# The frequency of distribution is high at 0 for MasVnrArea. Hence, impute 0 for all missing values in this column.\ndata.MasVnrArea.fillna(0, inplace=True)","1c646006":"# According to data dictionary, there's no value mentioned why the \"Electrical\" column has NA values.\n# Since there are only 0.07% of missing values for this column, impute this column with it's mode.\ndata.Electrical.fillna(data.Electrical.mode()[0], axis=0, inplace=True)","96f0eeec":"plt.scatter(data.YearBuilt, data.GarageYrBlt)\nplt.show()\nprint(\"Correlation between garage built year and house built year is:\", round(data.GarageYrBlt.corr(data.YearBuilt), 2))","0074f2b9":"# Missing values in garage built year column is due to the fact that, garage is not built and so the missing values.\n# Imputing 0 indicating not built, could make it as an outlier since the built date in data lies in 19th and 20th century.\n# Reasonable approach is to impute the garage built year with the house built year so that even if we consider the age of the\n# house, the selling price might not vary since both house and the garage is considered built at the same year.\n# From the scatter plot, it's clear that, the garage is built on or after the house is built which makes sense.\n\ndef fill_garage_yr(garageYr, builtYr):\n    if pd.isnull(garageYr):\n        return builtYr\n    else:\n        return garageYr\ndata['GarageYrBlt'] = data.apply(\n    lambda row: fill_garage_yr(row.GarageYrBlt, row.YearBuilt),\n    axis=1\n)","75f312c0":"# The missing values for all the independent variables are handled.\nplt.figure(figsize=(20,5))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.show()","586ff3af":"printmd('There are {} observations and {} features left after the missing value treatment.'.format(*data.shape))","153c83c6":"cat_cols = data.select_dtypes(include=['object', 'category']).columns\ncols_to_remove = []\nfor col in cat_cols:\n    unique_vals_per_col = len(data[col].unique())\n    most_repeated_prcnt = round(100*data[col].value_counts()[0]\/len(data))\n    print('There are \"{}\" unique values in \"{}\" of which the most repeated value is present in \"{}\"% of the total data.'\n         .format(unique_vals_per_col, col, most_repeated_prcnt))\n    if most_repeated_prcnt >= 90:\n        cols_to_remove.append(col)\n        \n# For few categorical columns, the most repeated value alone is higher when compared to all other category levels added together.\n# In other words, there is no variance for these features.\n# Henceforth, these columns won't add value to our predictions and are removed.\ndata.drop(cols_to_remove, axis=1, inplace=True)","8d0ac132":"printmd(\"There are {} features left after removing columns which doesn't add value for analysis.\".format(data.shape[1]))","d5af6b89":"data.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9, 1])","f71a8dfe":"# Define a function which creates box plots for all the numeric columns\ndef num_col_box_plot(df=data):\n    num_data = df.select_dtypes(include='number')\n    num_cols_per_row = 5\n    num_rows = int(np.ceil(len(num_data.columns)\/num_cols_per_row))\n    fig_num = 0\n    plt.figure(figsize=(15,15))\n    for col in num_data:\n        fig_num += 1\n        plt.subplot(num_rows, num_cols_per_row, fig_num)\n        num_data[col].plot.box()\n    plt.show()\n    \nnum_col_box_plot(data.drop(['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'SalePrice'], axis=1))","38b174be":"# Check the percentage of data present between 5th and 95th percentiles respectively.\nnum_data = data.select_dtypes(include='number')\nlower_prcntle = 5\nupper_prcntle = 100 - lower_prcntle\nfor num_col in num_data.columns:\n    obs_within_lmts = data[(data[num_col] > data[num_col].quantile(lower_prcntle\/100)) & \n                              (data[num_col] < data[num_col].quantile(upper_prcntle\/100))]\n    print('There are {}% of observations between {}th percentile and {}th percentile for the column {}.'\n          .format(round(100*obs_within_lmts.shape[0]\/data.shape[0], 2), lower_prcntle, upper_prcntle, num_col))","e301b930":"# There are few columns which could be futile for model building because if we are removing outliers and considering the\n# range of data between 5th and 95th percentile, there are hardly any number of observations left.\n# The outliers cannot be treated for these columns since the values lie beyond the range mentioned above.\n# Henceforth, drop these columns.\ndata.drop(['BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'], axis=1, inplace=True)","5938a8cb":"# Check outliers for garage year built.\nsns.boxplot(data.GarageYrBlt)\nplt.show()","eb3e5ca9":"# Having a value > current year is clearly an outlier. Use \"YearBuilt\" variable and impute the values for these outlier(s).\ndata.GarageYrBlt = data.apply(\n    lambda row: row.YearBuilt if row.GarageYrBlt > 2021 else row.GarageYrBlt,\n    axis=1\n)","8bbca7d4":"# By using flooring and capping method, for rest of the columns, when the value is less than 5th percentile floor the outliers\n# with 5th percentile and when the value is greater than 95th percentile, cap the outliers with 95th percentile respectively.\n\nnum_data = data.select_dtypes(include='number')\n# We are not treating outliers for date features and the target variable.\nnum_data.drop(['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'SalePrice'], axis=1, inplace=True)\ncols_to_treat_outliers = num_data.columns\ndef floor_cap_outliers(low, high, val):\n    if val < low:\n        return low\n    elif val > high:\n        return high\n    else:\n        return val\n\nfor num_col in cols_to_treat_outliers:\n    lower_prnctle_val = num_data[num_col].quantile(lower_prcntle\/100)\n    upper_prcntle_val = num_data[num_col].quantile(upper_prcntle\/100)\n    num_data[num_col] = num_data.apply(\n        lambda row: floor_cap_outliers(lower_prnctle_val, upper_prcntle_val, row[num_col]),\n        axis=1\n    )\n# Copy the values from numeric dataframe to our original dataframe\ndata[num_data.columns] = num_data\n# Check if all the outliers are handled\nnum_col_box_plot(num_data)","da7ef487":"data.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9, 1])","fc947557":"printmd('There are {} observations and {} features left after outlier treatment'.format(*data.shape))","db9a9b1e":"data.head()","60e4d945":"# Plot a heatmap for correlation\nplt.figure(figsize=(25,20))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","df097c0f":"# Visualizing the distribution of the target variable\nplt.figure(figsize=(5,3))\nsns.displot(data=data, x='SalePrice', kind='kde')\nplt.xlabel('SalePrice in linear scale')\nsns.displot(np.log(data['SalePrice']), kind='kde')\nplt.xlabel('SalePrice in log scale')\nplt.show()\nprintmd('The target variable `SalePrice` is right skewed. We can take log transformation of the target variable to handle skewness.')","ded90032":"# Taking log transformation for target variable.\ndata.SalePrice = np.log(data.SalePrice)","5e3f8cd3":"# Univariate analysis of categorical columns\ncat_cols = data.select_dtypes(include=['category']).columns\nnum_cols_per_row = 2\nnum_rows = int(np.ceil(len(cat_cols)\/num_cols_per_row))\nfig_num = 0\nplt.figure(figsize=(15,60))\nfor col in cat_cols:\n    fig_num += 1\n    plt.subplot(num_rows, num_cols_per_row, fig_num)\n    data[col].value_counts().sort_index().plot.bar()\n    plt.xlabel(col)\n    plt.xticks(rotation=0)\nplt.show()","3be02881":"# Analysing numerical columns with the target variable\nnum_data = data.select_dtypes(include='number').sample(500)\nnum_cols_per_row = 4\nnum_rows = int(np.ceil(len(num_data.columns)\/num_cols_per_row))\nfig_num = 0\n\nplt.figure(figsize=(15,15))\nfor col in num_data:\n    fig_num += 1\n    plt.subplot(num_rows, num_cols_per_row, fig_num)\n    plt.scatter(num_data[col], num_data['SalePrice'])\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\nplt.show()","db34667b":"data.select_dtypes(include=['number'])","856a1108":"# Age of a house, Garage age can be determined by subtracting year sold with respective year.\ndata['Age'] = data.YrSold - data.YearBuilt\ndata['AgeSinceRemod'] = data.YrSold - data.YearRemodAdd\ndata['GarageAge'] = data.YrSold - data.GarageYrBlt\ndata.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], axis=1, inplace=True)","ef72e24a":"plt.figure(figsize=(15, 5))\nplt.subplot(131)\nplt.scatter(data=data.sample(500), x='Age', y='SalePrice')\nplt.xlabel('Age')\nplt.ylabel('SalePrice')\nplt.subplot(132)\nplt.scatter(data=data.sample(500), x='AgeSinceRemod', y='SalePrice')\nplt.xlabel('AgeSinceRemod')\nplt.subplot(133)\nplt.scatter(data=data.sample(500), x='GarageAge', y='SalePrice')\nplt.xlabel('GarageAge')\nplt.show()\nprintmd(\"It is clear that as the age of the house increases, the sale price goes down!\")","55e494b9":"# Analysing categorical columns with the target variable\ncat_cols = data.select_dtypes(include=['category']).columns\nnum_cols_per_row = 2\nnum_rows = int(np.ceil(len(cat_cols)\/num_cols_per_row))\nfig_num = 0\nplt.figure(figsize=(15,60))\nfor col in cat_cols:\n    fig_num += 1\n    plt.subplot(num_rows, num_cols_per_row, fig_num)\n    sns.boxplot(data[col], data['SalePrice'])\n    plt.xlabel(col)\nplt.show()","b64396b2":"plt.figure(figsize=(15, 3))\nsns.boxplot(data['Neighborhood'], data['SalePrice'])\nplt.xticks(rotation=45)\nplt.show()","89abbb00":"df = pd.pivot_table(data[['SalePrice', 'Neighborhood']], index=['Neighborhood'])\ndf.sort_values('SalePrice').plot.kde()\nplt.show()\n# Under 5th and above 95th percentiles, the density is less. Grouping the neighborhood column based on these percentile values\n# and derive another column with 3 levels.\n# 0 - under 5th percentile, 1 - 5th to 95th percentile, 2 - above 95th percentile\ndf.describe(percentiles=[0.05, 0.95])","1f6069f5":"def cnvrt_to_levels(cat):\n    x = df.loc[cat].SalePrice\n    if x < df.quantile(0.05).SalePrice:\n        return 0\n    elif df.quantile(0.05).SalePrice < x < df.quantile(0.95).SalePrice:\n        return 1\n    else:\n        return 2\ndata['NeighborhoodGrp'] = data.Neighborhood.apply(cnvrt_to_levels).astype('category')\ndata.drop('Neighborhood', axis=1, inplace=True)","4c33dcd0":"# There are ideally 3 sub-categories for quality related columns. i.e., Poor, Average and Good.\n# The same can be encoded as 0, 1 and 2 respectively.\ndef quality_rating(x):\n    if x <= 3:\n        return 0\n    elif x <= 6:\n        return 1\n    else:\n        return 2\n\ndata['OverallQual'] = data.OverallQual.apply(quality_rating).astype('category')\ndata['OverallCond'] = data.OverallCond.apply(quality_rating).astype('category')\ndata['BedroomAbvGr'] = data.BedroomAbvGr.apply(quality_rating).astype('category')","29e5d6ca":"# There are few columns which indicates the quality of the structures of the building which could be the exterior quality or the\n# basement quality and so on. If a structure is not built, encode with 0 since the structure doesn't exist, quality cannot\n# be determined. Poor and fair is encoded with 1, average with 2, good and excellent with 3 respectively.\nquality_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual']\nfor col in quality_cols:\n    data[col] = data[col].map({'NotBuilt': 0, 'Po': 1, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 3}).astype('category')","35c5f4f2":"# Exposure can also be encoded as ordinal numbers. good exposure is preferred over min\/no exposure.\ndata['BsmtExposure'] = data['BsmtExposure'].map({'NotBuilt': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}).astype('category')","0d174a07":"# Number of rooms above grade ranges from 2-14. When a customer plans to purchase a house, s\/he thinks about their requirement\n# for rooms. If they have a small family, they tend to spend less and opt for lesser number of rooms. So, this variable looks \n# like a nomial variable and it has to be encoded as dummy variables.\ndef get_room_grade_lvl(x):\n    if x <= 5:\n        return 'less_rooms'\n    elif x <= 9:\n        return 'med_rooms'\n    else:\n        return 'more_rooms'\ndata['TotRmsAbvGrd'] = data.TotRmsAbvGrd.apply(get_room_grade_lvl)","0301df3e":"dummy_cols = ['MSZoning', 'LotShape', 'LotConfig', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st',\n             'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'TotRmsAbvGrd', 'GarageType',\n             'GarageFinish', 'Fence', 'SaleType', 'SaleCondition']\ndata = pd.get_dummies(data=data, columns=dummy_cols)","2b4fe3c6":"data.head()","8e6e1540":"# Earlier we had concatinated train and test sets so that the data cleaning process will be smoother. Now, lets split the data\n# back to train and test so that the scaling can be applied to the training dataset alone.\ntraining = data.loc['x']\ntesting = data.loc['y'].drop('SalePrice', axis=1)\n\nprintmd('There are {} observations in the training set and {} observations in the test set'.format(training.shape[0], testing.shape[0]))","2c5c65f3":"# Split training dataset into train and hold-out set. Actual testing set is used for final predictions.\nX = training.drop(['Id', 'SalePrice'], axis=1)\ny = training.SalePrice\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)","766e5d87":"scaler = MinMaxScaler()\ncols_to_scale = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'ExterQual',\n                 'ExterCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC', '1stFlrSF',\n                 '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenQual', 'Fireplaces',\n                 'FireplaceQu', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'MoSold', 'YrSold', 'Age',\n                 'AgeSinceRemod', 'GarageAge', 'NeighborhoodGrp']\nX_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n\n# For test set use transform only since the values are already fit using the training set.\nX_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\ntesting[cols_to_scale] = scaler.transform(testing[cols_to_scale])","f513d560":"# Check if all the variables lies within the range of 0 to 1 in training dataset.\nX_train.describe()","2d74ef65":"# Build a multiple linear regression model\nlr = LinearRegression()\nmodel = lr.fit(X_train, y_train)\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nprint('R2:Train =', r2_score(y_train, y_train_pred))\nprint('RMSE:Train =', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint('R2:Test =', r2_score(y_test, y_test_pred))\nprint('RMSE:Test =', np.sqrt(mean_squared_error(y_test, y_test_pred)))\nprintmd(\"Clear sign of overfitting since the R2 score for test set is negative and MSE is too high. Let's build ridge and lasso regression models to counter overfitting.\")","5607097b":"lambdas = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 50, 100, 500, 1000]\nr2_train = []\nr2_test = []\nrmse_train = []\nrmse_test = []\nfor lmda in lambdas:\n    ridge_model = Ridge(alpha=lmda)\n    ridge_model.fit(X_train, y_train)\n    y_train_pred = ridge_model.predict(X_train)\n    y_test_pred = ridge_model.predict(X_test)\n    r2_train.append(r2_score(y_train, y_train_pred))\n    rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n    r2_test.append(r2_score(y_test, y_test_pred))\n    rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))","bb6292ce":"# Creating a metric dataframe which will be useful for comparision.\nmetric_df = pd.DataFrame(columns=['alpha', 'ridge_r2_train', 'ridge_r2_test', 'ridge_rmse_train', 'ridge_rmse_test'])\nmetric_df['alpha'] = lambdas\nmetric_df['ridge_r2_train'] = r2_train\nmetric_df['ridge_r2_test'] = r2_test\nmetric_df['ridge_rmse_train'] = rmse_train\nmetric_df['ridge_rmse_test'] = rmse_test\nmetric_df","214b7763":"# Plot metrics (MSE and R2) vs alpha\nplt.figure(figsize=(13,3))\nplt.subplot(121)\nplt.plot(metric_df.alpha, metric_df.ridge_r2_train)\nplt.plot(metric_df.alpha, metric_df.ridge_r2_test)\nplt.title('Ridge regression: R2 vs alpha')\nplt.xlabel('alpha')\nplt.ylabel('R2 score')\nplt.legend(['r2_train', 'r2_test'], loc='upper right')\nplt.subplot(122)\nplt.plot(metric_df.alpha, metric_df.ridge_rmse_test)\nplt.plot(metric_df.alpha, metric_df.ridge_rmse_test)\nplt.title('Ridge regression: RMSE vs alpha')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.legend(['rmse_train', 'rmse_test'], loc='lower right')\nplt.show()","93d43cb3":"# Build a final ridge regression model with alpha 1\nridge_model = Ridge(alpha=1)\nridge_model.fit(X_train, y_train)\ny_train_pred = ridge_model.predict(X_train)\ny_test_pred = ridge_model.predict(X_test)\n\nprint('R2:Train =', r2_score(y_train, y_train_pred))\nprint('RMSE:Train =', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint('R2:Test =', r2_score(y_test, y_test_pred))\nprint('RMSE:Test =', np.sqrt(mean_squared_error(y_test, y_test_pred)))\nprintmd('For an alpha value of 1, the ridge regression model performs well enough for both train and test datasets')","3362b7be":"# Test for assumptions\ny_test_pred = ridge_model.predict(X_test)\nres = y_test - y_test_pred\nplt.scatter(res, y_test_pred)\nplt.xlabel('Error')\nplt.ylabel('Prediction')\nplt.show()","d6140dc9":"lambdas = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 50, 100, 500, 1000]\nr2_train = []\nr2_test = []\nrmse_train = []\nrmse_test = []\nfor lmda in lambdas:\n    lasso_model = Lasso(alpha=lmda)\n    lasso_model.fit(X_train, y_train)\n    y_train_pred = lasso_model.predict(X_train)\n    y_test_pred = lasso_model.predict(X_test)\n    r2_train.append(r2_score(y_train, y_train_pred))\n    rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n    r2_test.append(r2_score(y_test, y_test_pred))\n    rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))","ddf3462c":"# Adding MSE and R2 metrics for lasso to the metric dataframe.\nmetric_df['lasso_r2_train'] = r2_train\nmetric_df['lasso_r2_test'] = r2_test\nmetric_df['lasso_rmse_train'] = rmse_train\nmetric_df['lasso_rmse_test'] = rmse_test\nmetric_df","7e447896":"# Build a final lasso regression model with alpha 0.0001\nlasso_model = Lasso(alpha=0.0001)\nlasso_model.fit(X_train, y_train)\ny_train_pred = lasso_model.predict(X_train)\ny_test_pred = lasso_model.predict(X_test)\n\nprint('R2:Train =', r2_score(y_train, y_train_pred))\nprint('RMSE:Train =', np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint('R2:Test =', r2_score(y_test, y_test_pred))\nprint('RMSE:Test =', np.sqrt(mean_squared_error(y_test, y_test_pred)))\nprintmd('For an alpha value of 0.0001, the lasso regression model performs well enough for both train and test datasets')","2834aa8e":"# Test for assumptions\ny_test_pred = lasso_model.predict(X_test)\nplt.scatter(y_test - y_test_pred, y_test_pred)\nplt.xlabel('Error')\nplt.ylabel('Prediction')\nplt.show()","cb02cea3":"# Check the co-efficients for all 3 regression models\ncoeff_df = pd.DataFrame(columns=['Linear', 'Ridge', 'Lasso'], index=X_test.columns)\ncoeff_df['Linear'] = lr.coef_\ncoeff_df['Ridge'] = ridge_model.coef_\ncoeff_df['Lasso'] = lasso_model.coef_\npd.set_option('display.max_rows', None)\ncoeff_df","f5bcc5cf":"np.abs(coeff_df.Lasso).sort_values(ascending=False).head(10)","3a8acb26":"# Using lasso model for final predictions on test set. I'm using lasso over ridge because, on the test set lasso has \n# slightly better performance when compared to ridge and the model complexity is less in lasso model.\n\ny_final_test_pred = lasso_model.predict(testing.drop('Id', axis=1))\ntesting['SalePrice'] = np.exp(y_final_test_pred)","232ab991":"submission = testing[['Id', 'SalePrice']]","c7571237":"submission.head()","c13fde58":"# Create a submission file\nsubmission.to_csv('.\/submission.csv', index=False)","e5d79947":"#### Remove columns which doesn't add value to analysis and prediction","5be6ef5f":"**Observations:** Ridge regression has pushed the least important coefficients near to zero, whereas lasso regression has pushed the least important coefficients to zero.","6f3c1c54":"**Observations:** We don't have the rows whose missing values are >40%","6623545d":"### Data Cleanup\n- Removing unwanted features which doesn't contribute to analysis where,\n    - All values of a column are unique\n    - Column having only 1 unique value\n- Handle missing values in both column and row level\n- Outlier treatment","7968e528":"**Observations:**\n- `OverallQual` & `OverallCond`: When the overall quality is good, the sale price goes up! The same holds good for other features which indicates quality.\n- `BsmtExposure`: Sale price increases as the exposue of the basement increases.\n- `Fireplaces`: As Australia is a cold country, people give importance to fireplaces. More the number of fireplaces, greater the sale price is!\n- `MoSold`: Month sold doesn't have much impact on sale price.","0b8783d4":"## Hello Kagglers,\nThe aim of this dataset is to showcase how EDA is performed on the housing prediction dataset and how machine learning models can be built using a clean data. \nI have created 3 models (LinearRegression, Ridge, and Lasso) and explained using RMSE as a metric how well these models perform.  \n**Note: I have written my observations and the code is heavily commented so that, it would be easier for anybody who is willing to understand the logic.**\n\nThe steps involved in model building are as follows,\n- Understanding Data\n- Fixing datatypes\n- Data Cleanup\n    - Removing unwanted features which doesn't contribute to prediction\n    - Handle missing values in both column and row level\n    - Outlier treatment\n- Exploratory Data Analysis (EDA)\n- Scaling\n- Model Building\n- Testing the model on final test dataset (Public score obtained for Lasso model is 0.1454) ","07d0c204":"### Conclusion:\n- Regularized models (ridge and lasso) performs far better than the linear regression model with R2 scores 89% approx for both train and test datasets.\n- The hyperparameter (alpha) for ridge and lasso models are 1 and 0.0001 respectively.\n- Ridge and Lasso models are more generalizable and robust when compared to linear regression model since there is no much difference between the R2 scores of both train and the test data.\n- Lasso has reduced the complexity of the model by pushing the coefficients of few of the predictors to zero.\n- Since scaling is done on the data before building a model, the coefficients can be compared for it's importance.\n- Few of the important predictors include - `GrLivArea`, `TotalBsmtSF`, `OverallCond`, `Age`, `OverallQual` among others.\n- The target variable `SalePrice` is log transformed before building a model. To get the actual value from the model predictions, take an inverse log\/ exponential.","e02df9ff":"#### Handling missing values","8096f8c8":"### Understanding Data","1dd900a7":"**Observations:** When taken a sample of 500 observations and plotted a scatter plot, few features such as \"GrLivArea\", \"TotalBsmtSF\" among others are having a linear relationship with target variable.","b94bedbb":"#### Building a Ridge regression model","f4dd3f1f":"### Creating dummy variables for categorical columns\n- Here, drop_first is not being used because we need the variables for interpretability.","b961e5c7":"### Scaling features\n- Split the data into train and test sets before scaling the data","fd76fdf8":"**Observations:** No visible patterns in error terms. Error terms are independent of each other.","46076e46":"**Observations:** No visible patterns in error terms. Error terms are independent of each other.","39c49d58":"### Derive columns using the numerical columns","71e58f28":"#### Building a Lasso regression model","456ef5bf":"### Outlier Treatment","5d52fc5c":"### Fixing datatypes","40c62e27":"**Observations:** As the value of alpha increases,\n- R2 decreases\n- RMSE increases","b79fd329":"**Observations:** It's clear from the metric dataframe that for lasso regression, when the value of alpha is 0.0001, the model performs well enough for both train and test datasets.","7f6291db":"### Importing dataset","748c9648":"### Importing libraries","1e06c34f":"**Observations:** Few columns exhibit a pattern while moving along the axis\n- `MSZoning`: Residential Low Density (RL) is being sold more frequently.\n- `LotShape`: As expected, people will usually go for regular or slightly irregular shape houses. We don't usually buy irregular houses.\n- `OverallQual`: If the quality of the house is average or good, people are opting that. If the quality is extremely good, the plot can be really expensive. People are making a trade-off between cost and the quality of the house\n- `Foundation`: Cinder Block & Poured Concrete are commonly used type for foundation\n- In majority of the features, the frequency of \"average\" and \"good\" levels is more when compared to other levels.","2d612533":"**Observations:** Few of the columns have missing values which has to be imputed if required and few of the columns have to be removed since the amount of missing values is huge.","cc454e3b":"**Observations:**\n- We will look into the outliers while analysing data.\n- Scaling is required before building a model as many of the features lie on different scale.","04713a10":"### Encoding categorical variables","fef2fb24":"### Model Building & Evaluation","ae1a0228":"## EDA","8d0b5b7e":"### Bivariate Analysis","fd5002fe":"**Observations:** \n- There is multi-collinearity between independent features.\n- Couple of features have stronger correlation with the target variable which includes YearBuilt, YearRemodAdd, TotalBsmtSF, 1stFlrSF, GrLivArea, etc."}}