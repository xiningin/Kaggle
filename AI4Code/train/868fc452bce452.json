{"cell_type":{"f7aa00e2":"code","a24abbcd":"code","94214397":"code","f4ba4ced":"code","dfe96c67":"code","fd3ada5d":"code","b154b675":"code","61e0c8bb":"code","ee46c86a":"code","4a62085e":"code","a826fe65":"code","7a8d174a":"code","614f91f4":"code","ed21399c":"code","e69264b4":"code","d58921d4":"code","69ce7f7b":"code","9483b107":"code","f0b74e61":"code","65310e1e":"code","c5609454":"code","d6c95dc2":"code","02d0b431":"code","b5eb5627":"code","8923a2f6":"code","1f89d5e1":"code","25e56712":"code","9c39bd60":"code","725ee6b4":"code","f314384a":"code","4438cfa9":"code","a462bf83":"code","b45bb78c":"code","81ca83d4":"code","a9d8c6b4":"code","6889e219":"code","cab15324":"code","3adf74c2":"code","e0e0fca8":"code","a5009d51":"code","0215b9aa":"code","ff8836b3":"code","77a8160a":"code","32b2c36e":"code","d4ee18fa":"code","6e964fec":"code","5e16c154":"code","a4181395":"code","d79de44b":"code","95efb964":"code","5edc80f2":"code","e9bcf986":"code","374c8b33":"code","d7859e9e":"code","81f290ea":"code","ae549586":"code","4b0c53f3":"code","fb90644e":"code","8d1139fa":"code","940f948e":"code","7d37439c":"code","974b32dd":"code","255b9366":"code","e230851b":"code","90eb6f19":"code","c2f4f33c":"code","0c6f49db":"code","b525ad22":"code","feb39b8f":"code","d59e27f5":"code","f8cddfc4":"code","60cb80ea":"code","15c576fc":"code","efa83b42":"code","23d438a0":"code","adc2030e":"code","82fec3e7":"code","a1a83c73":"code","39ca798c":"code","c12b9c7b":"code","178f71fe":"code","20298c91":"code","33497e1e":"code","6777a616":"code","dd14c4cb":"code","e9e79027":"markdown","57c09546":"markdown","21e64907":"markdown","1ffc2679":"markdown","1092947c":"markdown","fac5c458":"markdown","ebd3b878":"markdown","e2ff6acc":"markdown","5d2a8d17":"markdown","85dcd458":"markdown","5d47e5eb":"markdown","77055248":"markdown","47882f8c":"markdown","00290b81":"markdown","58a47228":"markdown","a360c3e5":"markdown","58fb1b09":"markdown","d79c1159":"markdown","7e81d261":"markdown","d0ba9e47":"markdown","24581e21":"markdown","f38e2630":"markdown","d721deb4":"markdown","b60fe3aa":"markdown","47062190":"markdown","03fb99d1":"markdown","88b1ae93":"markdown","87f1284c":"markdown","d18caf50":"markdown","5c7b968f":"markdown","a6ec6f2e":"markdown","52f1e384":"markdown","955f14a8":"markdown","6bfa0fa2":"markdown"},"source":{"f7aa00e2":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno as msno\nimport numpy as np\n\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline","a24abbcd":"def jupyter_settings():\n    %matplotlib inline\n    %pylab inline\n    \n    sns.set(font_scale=1.6)\n    \n    plt.style.use(\"fivethirtyeight\")\n    # sns.set(style='whitegrid')\n    # plt.style.use('seaborn-darkgrid')\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['font.size'] = 16\n    \n    display( HTML('<style>.container {width:100% !important; }<\/style>'))\n    pd.options.display.max_columns = None\n    pd.options.display.max_rows = None\n    pd.set_option('display.expand_frame_repr', False)\n    \njupyter_settings()","94214397":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')","f4ba4ced":"train.head()","dfe96c67":"train.shape","fd3ada5d":"plt.figure(figsize=(8,6))\n\nsns.countplot(x=\"Response\", data = train, palette =\"husl\" ,edgecolor=\"black\")\nplt.ylabel('count', fontsize=15)\nplt.xlabel('gender', fontsize=15)\nplt.title('Balance of the output variable', fontsize=16)\nplt.show()","b154b675":"train['age_range'] = train['Age'].apply(lambda x: 'Adult 1' if 20<x<30 else('Adult 2' if 30 < x < 40 else('Adult 3' if 40<x<65 else 'Elderly')))","61e0c8bb":"train['monthly_premium'] = round(train['Annual_Premium']\/12, 2)","ee46c86a":"train['percentage_total_premium'] = train['Annual_Premium']\/train['Annual_Premium'].sum()","4a62085e":"df = pd.get_dummies(train['Vehicle_Damage'], prefix='Vehicle_Damage').rename(columns={'vehicle_damage_0':'vehicle_damage_no', 'vehicle_damage_1':'vehicle_damage_yes'})","a826fe65":"train  = pd.concat([train, df], axis=1)","7a8d174a":"train['insured_with_no_damage'] = train['Previously_Insured']*train['Vehicle_Damage_No']","614f91f4":"train[\"not_insured_with_damage\"] = train[\"Previously_Insured\"].apply(lambda x: 1 if x == 0 else 0) * train[\"Vehicle_Damage_Yes\"]","ed21399c":"train[\"vehicle_age_<_1_year\"] = train[\"Vehicle_Age\"].apply(lambda x: 1 if x=='< 1 Year' else 0)","e69264b4":"train[\"new_damage_no_insurance\"] = train[\"vehicle_age_<_1_year\"]*train[\"not_insured_with_damage\"]","d58921d4":"train.head()","69ce7f7b":"categorical_features = train.select_dtypes(exclude=[np.number])","9483b107":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","f0b74e61":"le = LabelEncoder()","65310e1e":"train['Gender'] = le.fit_transform(train['Gender'])\ntrain['Vehicle_Age'] = le.fit_transform(train['Vehicle_Age'])\ntrain['Vehicle_Damage'] = le.fit_transform(train['Vehicle_Damage'])\ntrain['age_range'] = le.fit_transform(train['age_range'])","c5609454":"y = train['Response'].copy()\nX = train.drop('Response', axis=1).copy()","d6c95dc2":"from imblearn.over_sampling import ADASYN","02d0b431":"adasyn = ADASYN()","b5eb5627":"X_adasyn, y_adasyn = adasyn.fit_resample(X,y)","8923a2f6":"print('The number of lines before oversampling : {}'.format(X.shape[0]))\nprint('The number of lines after oversampling : {}'.format(X_adasyn.shape[0]))","1f89d5e1":"import matplotlib.pyplot as plt","25e56712":"print(\"Now the training data is shorter but the classes are balanced\")\n\n# sets the plot size\nplt.figure(figsize=(8,6))\n\n# counts each class for the target var\nax = sns.countplot(x=y_adasyn, palette =\"husl\", edgecolor=\"black\")\n\n# sets plot features\nplt.title(\"Balancing of the output variable\")\nplt.xlabel(\"Response\")\nplt.ylabel(\"Count\")\nplt.xticks(ticks=[0,1], labels=['No','Yes'])\n\n# displays the plot\nplt.show()","9c39bd60":"x_train, x_val, y_train, y_val = train_test_split(X_adasyn, y_adasyn, test_size=0.3, random_state=72)","725ee6b4":"from sklearn.ensemble import RandomForestClassifier","f314384a":"rf = RandomForestClassifier(random_state=1)","4438cfa9":"rf.fit(X_adasyn, y_adasyn)","a462bf83":"importances = rf.feature_importances_","b45bb78c":"importance = list(importances)","81ca83d4":"colum = list(X_adasyn.columns)","a9d8c6b4":"feature_importance = pd.DataFrame(zip(colum, importance), columns=['Feature', 'Importance']).sort_values('Importance')","6889e219":"feature_importance = feature_importance.set_index('Feature')","cab15324":"feature_importance.plot(kind='barh', figsize=(12,10))\nplt.title('Feature Importance', fontsize=16)\nplt.legend(bbox_to_anchor=(0.95, 0.1), fontsize=14)\nplt.ylabel('Features', fontsize=14)\nplt.xlabel('Importance', fontsize=14)\nplt.show()","3adf74c2":"from sklearn.linear_model import Ridge","e0e0fca8":"model = Ridge(alpha=1e-2).fit(x_train, y_train)","a5009d51":"model.score(x_val, y_val)","0215b9aa":"feature_names = x_train.columns","ff8836b3":"from sklearn.inspection import permutation_importance\nr = permutation_importance(model, x_val, y_val, n_repeats=30,random_state=0)\n\npermutation_importance_name = []\npermutation_importance_mean = []\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n        print(f\"{feature_names[i]:<8}\"\n        f\"  {r.importances_mean[i]:.3f}\"\n        f\" +\/- {r.importances_std[i]:.3f}\")\n\n        permutation_importance_name.append(feature_names[i])\n        permutation_importance_mean.append(r.importances_mean[i]) ","77a8160a":"!pip install Boruta==0.3","32b2c36e":"#from boruta import BorutaPy\n\n###initialize Boruta\n#forest = RandomForestRegressor(\n#   n_jobs = -1, \n#   max_depth = 5\n#)\n\n#boruta = BorutaPy(\n#   estimator = rf, \n#   n_estimators = 'auto',\n#   max_iter = 20 # number of trials to perform\n#)\n### fit Boruta (it accepts np.array, not pd.DataFrame)\n#boruta.fit(np.array(X_adasyn), np.array(y_adasyn))\n### print results\n#green_area = X_adasyn.columns[boruta.support_].to_list()\n#blue_area = X_adasyn.columns[boruta.support_weak_].to_list()\n#print('features in the green area:', green_area)\n#print('features in the blue area:', blue_area)","d4ee18fa":"x_train.columns","6e964fec":"x_train_selected = x_train[['id', 'Age', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Vintage', 'age_range', 'Vehicle_Damage_No', 'Vehicle_Damage_Yes', 'insured_with_no_damage', 'not_insured_with_damage']]","5e16c154":"x_val_selected = x_val[['id', 'Age', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Vintage', 'age_range', 'Vehicle_Damage_No', 'Vehicle_Damage_Yes', 'insured_with_no_damage', 'not_insured_with_damage']]","a4181395":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import f1_score, recall_score, precision_score","d79de44b":"from sklearn.tree import DecisionTreeClassifier","95efb964":"dt = DecisionTreeClassifier()\ndt.fit(x_train_selected, y_train)\ny_pred_dt = dt.predict(x_val_selected)\nacc_dt = accuracy_score(y_val, y_pred_dt)\nf1_score_dt = f1_score(y_val, y_pred_dt)\nrecall_score_dt = recall_score(y_val, y_pred_dt)\nprecision_score_dt = precision_score(y_val, y_pred_dt)","5edc80f2":"print('The average accuracy is: {}'.format(acc_dt))","e9bcf986":"rf = RandomForestClassifier()\nrf.fit(x_train_selected, y_train)\ny_pred_rf = rf.predict(x_val_selected)\nacc_rf = accuracy_score(y_val, y_pred_rf)\nf1_score_rf = f1_score(y_val, y_pred_rf)\nrecall_score_rf = recall_score(y_val, y_pred_rf)\nprecision_score_rf = precision_score(y_val, y_pred_rf)","374c8b33":"print('The average accuracy is: {}'.format(acc_rf))","d7859e9e":"import xgboost as xgb","81f290ea":"xgb = xgb.XGBClassifier()\nxgb.fit(x_train_selected, y_train)\ny_pred_xgb = xgb.predict(x_val_selected)\nacc_xgb = accuracy_score(y_val, y_pred_xgb)\nf1_score_xgb = f1_score(y_val, y_pred_xgb)\nrecall_score_xgb = recall_score(y_val, y_pred_xgb)\nprecision_score_xgb = precision_score(y_val, y_pred_xgb)","ae549586":"print('The average accuracy is: {}'.format(round(acc_xgb,3)))","4b0c53f3":"from lightgbm import LGBMClassifier","fb90644e":"lgbm = LGBMClassifier()\nlgbm.fit(x_train_selected, y_train)\ny_pred_lgbm = lgbm.predict(x_val_selected)\nacc_lgbm = accuracy_score(y_val, y_pred_lgbm)\nf1_score_lgbm = f1_score(y_val, y_pred_lgbm)\nrecall_score_lgbm = recall_score(y_val, y_pred_lgbm)\nprecision_score_lgbm = precision_score(y_val, y_pred_lgbm)","8d1139fa":"print('The average accuracy is: {}'.format(round(acc_lgbm,3)))","940f948e":"from sklearn.neighbors import KNeighborsClassifier","7d37439c":"knn = KNeighborsClassifier() \nknn.fit(x_train_selected, y_train)  \ny_pred_knn = knn.predict(x_val_selected)  \nacc_knn = accuracy_score(y_val, y_pred_knn)\nf1_score_knn = f1_score(y_val, y_pred_knn)\nrecall_score_knn = recall_score(y_val, y_pred_knn)\nprecision_score_knn = precision_score(y_val, y_pred_knn)","974b32dd":"print('The average accuracy is: {}'.format(round(acc_knn,3)))","255b9366":"from sklearn.linear_model import LogisticRegression","e230851b":"log = LogisticRegression()\nlog.fit(x_train_selected, y_train)  \ny_pred_log = log.predict(x_val_selected)  \nacc_log = accuracy_score(y_val, y_pred_log)\nf1_score_log = f1_score(y_val, y_pred_log)\nrecall_score_log = recall_score(y_val, y_pred_log)\nprecision_score_log = precision_score(y_val, y_pred_log)","90eb6f19":"print('The average accuracy is: {}'.format(round(acc_log,3)))","c2f4f33c":"from sklearn.ensemble import BaggingClassifier","0c6f49db":"bag = BaggingClassifier()\nbag.fit(x_train_selected, y_train)  \ny_pred_bag = bag.predict(x_val_selected)  \nacc_bag = accuracy_score(y_val, y_pred_bag)\nf1_score_bag = f1_score(y_val, y_pred_bag)\nrecall_score_bag = recall_score(y_val, y_pred_bag)\nprecision_score_bag = precision_score(y_val, y_pred_bag)","b525ad22":"print('The average accuracy is: {}'.format(round(acc_bag,3)))","feb39b8f":"from sklearn.ensemble import GradientBoostingClassifier","d59e27f5":"gbst = GradientBoostingClassifier()\ngbst.fit(x_train_selected, y_train)  \ny_pred_gbst = gbst.predict(x_val_selected)  \nacc_gbst = accuracy_score(y_val, y_pred_gbst)\nf1_score_gbst = f1_score(y_val, y_pred_gbst)\nrecall_score_gbst = recall_score(y_val, y_pred_gbst)\nprecision_score_gbst = precision_score(y_val, y_pred_gbst)","f8cddfc4":"print('The average accuracy is: {}'.format(round(acc_gbst,3)))","60cb80ea":"results = pd.DataFrame({\n    'Model': ['Decision tree', 'Random Forest', 'XGBoost', 'LGBM', 'K Nearest Neighbor', 'Logistic Regression', 'Bagging Classifier', 'Gradient Boosting Classifier'],\n    'Accuracy': [acc_dt, acc_rf, acc_xgb, acc_lgbm, acc_knn, acc_log, acc_bag, acc_gbst],\n    'Recall': [recall_score_dt, recall_score_rf, recall_score_xgb, recall_score_lgbm, recall_score_knn, recall_score_log, recall_score_bag, recall_score_gbst],\n    'Precision': [precision_score_dt, precision_score_rf, precision_score_xgb, precision_score_lgbm, precision_score_knn, precision_score_log, precision_score_bag, precision_score_gbst],    \n    'F1-score': [f1_score_dt, f1_score_rf, f1_score_xgb, f1_score_lgbm, f1_score_knn, f1_score_log, f1_score_bag, f1_score_gbst]})\nresult = results.sort_values(by='F1-score', ascending=False)\nresult = result.set_index('Model')\ndisplay(result.head(10))","15c576fc":"param_grid = {\"n_estimators\": [200,300,400],\n              \"max_depth\": [4,5,6],\n             \"learning_rate\": [0.001, 0.01, 0.05]} \nxgb_grid_selected = GridSearchCV(xgb, cv=KFold(n_splits = 5, shuffle=True), param_grid=param_grid, scoring='accuracy')\neval_set = [(x_train_selected, y_train), (x_val_selected, y_val)]\nxgb_grid_selected.fit(x_train_selected, y_train , eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\nbest_xgb_selected = xgb_grid_selected.best_estimator_\n\nprint(best_xgb_selected)","efa83b42":"# retrieve performance metrics\nresults = xgb_grid_selected.best_estimator_.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","23d438a0":"y_pred_xgb_best_model = best_xgb_selected.predict(x_val_selected)","adc2030e":"print(classification_report(y_val, y_pred_xgb_best_model))","82fec3e7":"from sklearn.metrics import plot_confusion_matrix","a1a83c73":"plot_confusion_matrix(best_xgb_selected, x_val_selected, y_val) \nplt.title('Confusion matrix')\nplt.yticks(ticks=[0,1], labels=['No accepted','Accepted'])\nplt.xticks(ticks=[0,1], labels=['No accepted','Accepted'])\nplt.grid(False)\nplt.show()","39ca798c":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","c12b9c7b":"ns_probs = [0 for _ in range(len(y_val))]\n\n\n# fit a model\nSEED=1\n\ndt_clf = DecisionTreeClassifier(random_state=SEED)\nrf_clf = RandomForestClassifier(random_state=SEED)\nxgb_clf = XGBClassifier(random_state=SEED)\nlgbm_clf = LGBMClassifier(random_state=SEED)\nknn_clf = KNeighborsClassifier() \nlog_clf = LogisticRegression(random_state=SEED)\nbag_clf = BaggingClassifier(random_state=SEED)\ngbst_clf = GradientBoostingClassifier(random_state=SEED)\n\n\n# trains the classifiers\ndt_clf.fit(x_train_selected, y_train)\nrf_clf.fit(x_train_selected, y_train)\nxgb_clf.fit(x_train_selected, y_train)\nlgbm_clf.fit(x_train_selected, y_train)\nknn_clf.fit(x_train_selected, y_train)\nlog_clf.fit(x_train_selected, y_train)\nbag_clf.fit(x_train_selected, y_train)\ngbst_clf.fit(x_train_selected, y_train)\n\n\n# predict probabilities\n\ndt_probs = dt_clf.predict_proba(x_val_selected)\nrf_probs = rf_clf.predict_proba(x_val_selected)\nxgb_probs = xgb_clf.predict_proba(x_val_selected)\nlgbm_probs = lgbm_clf.predict_proba(x_val_selected)\nknn_probs = knn_clf.predict_proba(x_val_selected)\nlog_probs = log_clf.predict_proba(x_val_selected)\nbag_probs = bag_clf.predict_proba(x_val_selected)\ngbst_probs = gbst_clf.predict_proba(x_val_selected)\n\n# keep probabilities for the positive outcome only\n\ndt_probs = dt_probs[:, 1]\nrf_probs = rf_probs[:, 1]\nxgb_probs = xgb_probs[:, 1]\nlgbm_probs = lgbm_probs[:, 1]\nknn_probs = knn_probs[:, 1]\nlog_probs = log_probs[:, 1]\nbag_probs =  bag_probs[:, 1]\ngbst_probs =  gbst_probs[:, 1]\n\n# calculate scores\n\nns_auc = roc_auc_score(y_val, ns_probs)\ndt_auc = roc_auc_score(y_val, dt_probs)\nrf_auc = roc_auc_score(y_val, rf_probs)\nxgb_auc = roc_auc_score(y_val, xgb_probs)\nlgbm_auc = roc_auc_score(y_val, lgbm_probs)\nknn_auc = roc_auc_score(y_val, knn_probs)\nlog_auc = roc_auc_score(y_val, log_probs)\nbag_auc = roc_auc_score(y_val, bag_probs)\ngbst_auc = roc_auc_score(y_val, gbst_probs)\n\n\n# summarize scores\nprint('No Skill: ROC AUC=%.3f' % (ns_auc))\nprint('Decision Tree: ROC AUC=%.3f' % (dt_auc))\nprint('Random Forest: ROC AUC=%.3f' % (rf_auc))\nprint('XGBoost: ROC AUC=%.3f' % (xgb_auc))\nprint('LGBM: ROC AUC=%.3f' % (lgbm_auc))\nprint('KNN: ROC AUC=%.3f' % (knn_auc))\nprint('Logistic Regression: ROC AUC=%.3f' % (log_auc))\nprint('Bagging Classifier: ROC AUC=%.3f' % (bag_auc))\nprint('Gradient Boosting Classifier: ROC AUC=%.3f' % (gbst_auc))\n\n\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_val, ns_probs)\ndt_fpr, dt_tpr, _ = roc_curve(y_val, dt_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_val, rf_probs)\nxgb_fpr, xgb_tpr, _ = roc_curve(y_val, xgb_probs)\nlgbm_fpr, lgbm_tpr, _ = roc_curve(y_val, lgbm_probs)\nknn_fpr, knn_tpr, _ = roc_curve(y_val, knn_probs)\nlog_fpr, log_tpr, _ = roc_curve(y_val, log_probs)\nbag_fpr, bag_tpr, _ = roc_curve(y_val, bag_probs)\ngbst_fpr, gbst_tpr, _ = roc_curve(y_val, gbst_probs)\n\n\n\n# plot the roc curve for the model\nplt.figure(figsize=(16,8), dpi=100)\n\nplt.plot(ns_fpr, ns_tpr, linestyle='dashed', linewidth=2, color= 'black', label='No Skill (auc = %0.3f)' % ns_auc)\nplt.plot(dt_fpr, dt_tpr, linestyle='-', linewidth=2, color= 'red', label='Decision Tree (auc = %0.3f)' % dt_auc)\nplt.plot(rf_fpr, rf_tpr, linestyle='-', linewidth=2, color= 'blue', label='Random Forest (auc = %0.3f)' % rf_auc)\nplt.plot(xgb_fpr, xgb_tpr, marker='.', linewidth=2, color= 'green', label='XGBoost (auc = %0.3f)' % xgb_auc)\nplt.plot(lgbm_fpr, lgbm_tpr, linestyle='-', linewidth=2, color= 'yellow', label='LGBM (auc = %0.3f)' % lgbm_auc)\nplt.plot(knn_fpr, knn_tpr, linestyle='-', linewidth=2, color= 'orange', label='KNN (auc = %0.3f)' % knn_auc)\nplt.plot(log_fpr, log_tpr, linestyle='-', linewidth=2, color= 'magenta', label='Logistic Regression (auc = %0.3f)' % log_auc)\nplt.plot(bag_fpr, bag_tpr, linestyle='-', linewidth=2, color= 'gray', label='Bagging Classifier (auc = %0.3f)' % bag_auc)\nplt.plot(gbst_fpr, gbst_tpr, linestyle='-', linewidth=2, color= 'pink', label='Gradient Boosting Classifier (auc = %0.3f)' % gbst_auc)\n\n\n# axis labels\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\nplt.title(\"AUC-ROC Curve\")\nplt.legend()\n\nplt.show()","178f71fe":"import scikitplot as skplt","20298c91":"xgb_probs2 = best_xgb_selected.predict_proba(x_val_selected)\n\nskplt.metrics.plot_roc(y_val, xgb_probs2, figsize=(12,8))\nplt.xlim(-0.01,1.01)\nplt.ylim(-0.01,1.05)\nplt.show()","33497e1e":"# fit a model\nSEED=1\n\ndt_clf = DecisionTreeClassifier(random_state=SEED)\nrf_clf = RandomForestClassifier(random_state=SEED)\nxgb_clf = XGBClassifier(random_state=SEED)\nlgbm_clf = LGBMClassifier(random_state=SEED)\nknn_clf = KNeighborsClassifier() \nlog_clf = LogisticRegression(random_state=SEED)\nbag_clf = BaggingClassifier(random_state=SEED)\ngbst_clf = GradientBoostingClassifier(random_state=SEED)\n\n\n# trains the classifiers\ndt_clf.fit(x_train_selected, y_train)\nrf_clf.fit(x_train_selected, y_train)\nxgb_clf.fit(x_train_selected, y_train)\nlgbm_clf.fit(x_train_selected, y_train)\nknn_clf.fit(x_train_selected, y_train)\nlog_clf.fit(x_train_selected, y_train)\nbag_clf.fit(x_train_selected, y_train)\ngbst_clf.fit(x_train_selected, y_train)\n\n\n# predict probabilities\n\ndt_probs = dt_clf.predict_proba(x_val_selected)\nrf_probs = rf_clf.predict_proba(x_val_selected)\nxgb_probs = xgb_clf.predict_proba(x_val_selected)\nlgbm_probs = lgbm_clf.predict_proba(x_val_selected)\nknn_probs = knn_clf.predict_proba(x_val_selected)\nlog_probs = log_clf.predict_proba(x_val_selected)\nbag_probs = bag_clf.predict_proba(x_val_selected)\ngbst_probs = gbst_clf.predict_proba(x_val_selected)\n\n\nprobas_list = [dt_probs, rf_probs, xgb_probs, lgbm_probs, knn_probs, log_probs, bag_probs, gbst_probs]\n\nclf_names = ['Decision tree', 'Random Forest', 'XGBoost', 'LGBM', 'K Nearest Neighbor', 'Logistic Regression', 'Bagging Classifier', 'Gradient Boosting Classifier']\n\nskplt.metrics.plot_calibration_curve(y_val, probas_list, clf_names, figsize=(16,12))\nplt.show()","6777a616":"# get what the predicted probabilities are to use creating cumulative gains chart\nprobs = xgb_clf.predict_proba(x_val_selected)\n\nskplt.metrics.plot_cumulative_gain(\n    y_val, probs, figsize=(10, 8), title_fontsize=20, text_fontsize=18\n)\nplt.ylim(0,1.05)\nplt.show()","dd14c4cb":"skplt.metrics.plot_lift_curve(\n    y_val, probs, figsize=(10, 8), title_fontsize=20, text_fontsize=18\n)\nplt.legend(bbox_to_anchor=(1, 1), fontsize=14)\nplt.show()","e9e79027":"### Plotting the Loss and error to check the overfitting","57c09546":"### Age range","21e64907":"## Summary\n\n___\n- Feature Importance using Random Forest (Top 5 features)\n\npercentage_total_premium, Policy_Sales_Channel, Previously_Insured, Vehicle_Damage_Yes and Age.\n___\n\n- Permutation Importance (excluding features with high standard deviation)\n\nTop 5 features\n\nPreviously_Insured, percentage_total_premium, Vehicle_Damage_Yes, vehicle_age_<_1_year and insured_with_no_damage.\n___\n\n- Boruta\n\nFeatures selected\n\nid, Age, Region_Code, Previously_Insured, Policy_Sales_Channel, Vintage, age_range, Vehicle_Damage_No, Vehicle_Damage_Yes, insured_with_no_damage, not_insured_with_damage, vehicle_age_<_1_year\n___\n- Mutual info(Top 5 features)\n\nFeatures selected\n\nPolicy_Sales_Channel, Region_Code, Vehicle_Damage_No, Previously_Insured and not_insured_with_damage\n___\n**Conclusions**\n\n- age_range came from Age, features related to customer age were indicated in two analysis (Feature Importance and Boruta)\n- percentage_total_premium was indicated in two analysis (Feature Importance and Permutation Importance)\n- Features related to vehicle damage were indicated in all analysis(ANOVA, Feature Importance, Permutation Importance and Boruta)\n- Policy_Sales_Channel was indicated in two analysis(Feature Importance and Boruta)\n- Previous_Insured was indicated in three analysis(Feature Importance, Permutation Importance and Boruta)\n___\n","1ffc2679":"- Bagging Classifier","1092947c":"# This notebook is focused on to give a brief view about feature engineering and feature selection","fac5c458":"### Gains curve to check the quality of the model against the baseline(non-use of machine learning)","ebd3b878":"# Boruta\n\nBasically, you choose a model of convenience \u2014 capable of capturing non-linear relationships and interactions, e.g. a random forest \u2014 and you fit it on X and y. Then, you extract the importance of each feature from this model and keep only the features that are above a given threshold of importance.\n\nIn Boruta, features do not compete among themselves. Instead \u2014 and this is the first brilliant idea \u2014 they compete with a randomized version of them.\n\nbinomial distribution\nAs often happens in machine learning (in life?), the key is iteration. Not surprisingly, 20 trials are more reliable than 1 trial and 100 trials are more reliable than 20 trials.","e2ff6acc":"# Feature analysis and selection","5d2a8d17":"### Lift curve to check the quality of the model against the baseline(non-use of machine learning)","85dcd458":"- XGBoost","5d47e5eb":"# References\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nhttps:\/\/machinelearningmastery.com\/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost\/\nhttps:\/\/towardsdatascience.com\/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a","77055248":"# Feature Engineering","47882f8c":"### Confusion matrix","00290b81":"### Comparison of the different models","58a47228":"### Monthly premium","a360c3e5":"- Gradient Boosting Classifier","58fb1b09":"- Decision tree","d79c1159":"- K Nearest Neighbor","7e81d261":"Plotting the calibration curves of a classifier is useful for determining whether or not you can interpret \ntheir predicted probabilities directly as confidence level. For instance, a well-calibrated binary classifier \nshould classify the samples such that for samples to which it gave a score of 0.8, around 80% should actually \nbe from the positive class.","d0ba9e47":"### How Insurance Companies Work\n\n- Insurance companies assess the risk and charge premiums for various types of insurance coverage. If an insured event occurs and you suffer damages, the insurance company pays you up to the agreed amount of the insurance policy. The way insurance companies work, they can pay this and still make a profit.\n\nEvaluating Risk\n\n- Companies that buy insurance policies transfer their risk to the insurance company in return for paying their premiums. The insurance company has to define insurance risk it is taking on. It asks questions, each of which is designed to evaluate a particular risk. Depending on your answers to the questions, the insurance company quotes you a premium. If your risk is higher than usual \u2013 for example, if you are not near a fire hydrant, then your fire insurance will be higher. If you don't answer the questions honestly, the insurance company may refuse to pay if there are damages, according to the Insurance Institute of Michigan.\n\nShared Risk\n\n- Your premiums are much lower than the possible damages, but the insurance company can afford to pay them because it receives premiums from many customers. Insurance companies operate on the principle of shared risk. All the customers pay small amounts and share the risk that way. A fire or other covered event only happens rarely. The insurance company has to calculate the premiums so the total premiums it receives from its many customers cover the few damage claims, with some money left over for administration and profit.\n\n\nRe-Insurance\n\n- Insurance companies have to consider that, if they have a lot of policies in one area and there is a natural disaster, many customers will make a claim. The insurance company may not have collected enough premiums to cover so many claims. To prevent such a problem, insurance companies pass on some of the risk to other large financial firms that offer re-insurance, meaning they may be protected in a worst case scenario.\nThe large firms take over the extra risk from the insurance company that holds the policies, and it pays for this service. For major natural disasters, the re-insurance companies pay for some of the damages through the local insurance companies that sold the policies.\n\nInvestment Income\n\n- Over time, insurance companies receive lots of small amounts in premiums and have to occasionally pay out large amounts. Before paying out the damages, they may have large surpluses which they invest, according to Obrella. Because they don't want to take much additional risk, they typically place this money in safe investments, but it still generates a substantial income. This income increases the revenue of the insurance companies, and they can use it to reduce the premiums they charge or to increase their profits.\n\nSource: https:\/\/smallbusiness.chron.com\/insurance-companies-work-60269.html","24581e21":"# Encoder","f38e2630":"# Feature Importance (Random Forest)","d721deb4":"Percentage of total premium","b60fe3aa":"# Model Building","47062190":"ROC Curve","03fb99d1":"- Random Forest","88b1ae93":"### Feature selection using Boruta (In case you need to run Boruta again)","87f1284c":"# Oversampling (ADASYN)","d18caf50":"- LGBM","5c7b968f":"___","a6ec6f2e":"## Chosing the best hyperparameters using GridSearchCV - Fine tuning","52f1e384":"# Permutation Importance\n\nPermutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled 1. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.","955f14a8":"- Logistic Regression","6bfa0fa2":"Features selected by Boruta\n\nid, Age, Region_Code, Previously_Insured, Policy_Sales_Channel, Vintage, age_range, Vehicle_Damage_No, Vehicle_Damage_Yes, insured_with_no_damage, not_insured_with_damage, vehicle_age_<_1_year"}}