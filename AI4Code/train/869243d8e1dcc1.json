{"cell_type":{"05f201dc":"code","c64671b7":"code","d13cb5f7":"code","16882e5a":"code","9d75468f":"code","54dc5496":"code","6bd91119":"code","0adf71ab":"code","b86c6d1d":"code","05b18e87":"code","d1659811":"code","bba43abd":"code","b20a30a2":"code","3ffa8586":"code","6c1ae6d3":"code","4bcb9059":"code","915906d4":"code","b99a1d42":"code","f560cbcd":"code","e84ab82c":"code","fed48b65":"code","69412aa9":"code","e3114c0d":"code","e661a605":"code","f9506f7c":"code","bb9eab70":"code","e7e551f4":"code","162d59b1":"code","95529e38":"code","92c820cf":"code","ddbd0c9a":"code","2eda9e3e":"code","bfbcf47a":"code","5a7af988":"code","557aa7b0":"code","b091876c":"code","e5700ee6":"code","bb56c175":"markdown","b76292e7":"markdown","ad179c8e":"markdown","2d88aa38":"markdown","8289f607":"markdown","20359cf1":"markdown","397483af":"markdown","c99618d6":"markdown","e07147d5":"markdown","b29b27bd":"markdown","06068954":"markdown","74f94f90":"markdown","649670f4":"markdown","8dded07a":"markdown","3d0b387a":"markdown","33311f72":"markdown","3420deeb":"markdown","82aa4c90":"markdown"},"source":{"05f201dc":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom xgboost import XGBClassifier\npd.set_option('display.max_columns', 500)","c64671b7":"# reading the datasets\ndf_train = pd.read_csv('..\/input\/loan-delinquency-prediction\/train.csv').drop(['Unnamed: 0','loan_id'], axis = 1)\ndf_test = pd.read_csv('..\/input\/loan-delinquency-prediction\/test.csv').drop(['Unnamed: 0','loan_id'], axis = 1)\ndf_sub = pd.read_csv('..\/input\/loan-delinquency-prediction\/sample_submission.csv')\ndf_train.head()","d13cb5f7":"# Number of features in the Dataset\n\nprint(\"Number of features in the Training Dataset :\", len(df_train.columns))\nprint(\"Number of features in the Training Dataset :\", len(df_test.columns))","16882e5a":"#checking the features in the Dataset\nprint(\"\\nNo of features in the Training Dataset = \", len(df_train.columns))\nprint(\"\\nFeatures in the Training Dataset : \\n\\n\", list(df_train.columns))","9d75468f":"#checking the data types of features in training  data\nprint(\" Datatypes of features in the training dataset :\\n\",'-' * 45,\"\\n\", df_train.dtypes)","54dc5496":"#checking the number of rows\nprint(\" Number of observations in the Training Dataset :\",len(df_train))\nprint(\" Number of observations in the Test Dataset :\",len(df_test))","6bd91119":"# checking if there are any NaN values in the train and test dataset\nprint(\"Null Values in the training dataset fetures: \", df_train.isnull().values.any())\nprint(\"Null Values in the test dataset features: \", df_test.isnull().values.any())","0adf71ab":"# making a list of categorical and numerical columns\ncategory_columns=df_train.select_dtypes(include=['object']).columns.tolist()\nnumeric_columns=df_train.select_dtypes(exclude=['object']).columns.tolist()","b86c6d1d":"# No of unique values in the all the columns\nfor cols in df_train.columns:\n    print('Unique valeus in',cols,'column = ',df_train[cols].nunique())    ","05b18e87":"# categories in the categorical columns\nfor cols in df_train.columns:\n    print('categories in',cols,'column = ',df_train[cols].unique(),'\\n')    ","d1659811":"# Count plot of all the columns\nfig = plt.figure(figsize=(20,40))\nfor i, col in enumerate(df_train.columns):   \n    plt.subplot(7, 4, i+1)   \n    sns.countplot(x=col, data=df_train).set(xlabel=col)    \nplt.tight_layout()\nplt.show()\nplt.clf()\nplt.close('all')","bba43abd":"# Value count of all the columns\nfor column in df_train.columns:\n     print(\"\\n\"+'Value counts of ' + column+ \" column\\n\"+'-'*30)\n     print(df_train[column].value_counts())","b20a30a2":"df_train.describe()","3ffa8586":"# Class distribution of target column\nprint(df_train.m13.value_counts())\n\n# class percentage distribution of target column\ndf_train.m13.value_counts()\/len(df_train)*100","6c1ae6d3":"# Correlation values of independent columsn to the target column\ndf_train.corr()['m13']","4bcb9059":"for col in category_columns:\n    print(col ,'-->\\n',df_train[col].value_counts(),'\\n\\n')","915906d4":"df_train = pd.get_dummies(df_train, columns = category_columns)\ndf_test = pd.get_dummies(df_test, columns = category_columns)","b99a1d42":"# train test split\nX = df_train[numeric_columns]\ny = df_train['m13']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)","f560cbcd":"classifier = XGBClassifier(\n#     scale_pos_weight = (183),\n    eval_metric = 'error',use_label_encoder=False)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(f1_score(y_test, y_pred))","e84ab82c":"# train test split\nX = df_train.drop(columns = 'm13')\ny = df_train['m13']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)\n\nclassifier = XGBClassifier(\n#     scale_pos_weight = (183),\n    eval_metric = 'error',use_label_encoder=False)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(f1_score(y_test, y_pred))","fed48b65":"X = df_train.drop(columns = 'm13')\ny = df_train['m13']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)\n","69412aa9":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nover = SMOTE(sampling_strategy=.05)\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n# transform the dataset\nX, y = over.fit_resample(X, y)","e3114c0d":"y.value_counts()","e661a605":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)","f9506f7c":"classifier = XGBClassifier(\n#     scale_pos_weight = (183),\n    eval_metric = 'error',use_label_encoder=False)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nprint(f1_score(y_test, y_pred))","bb9eab70":"y_train.value_counts()","e7e551f4":"y_train.value_counts()","162d59b1":"pd.DataFrame(y_pred).value_counts()","95529e38":"from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","92c820cf":"# Step 3: Fit a Decision Tree model as comparison\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nf1_score(y_test, y_pred)","ddbd0c9a":"# Step 4: Fit a Random Forest model, \" compared to \"Decision Tree model, accuracy go up by 5%\nclf1 = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0)\nclf1.fit(X_train, y_train)\ny_pred1 = clf1.predict(X_test)\nf1_score(y_test, y_pred1)","2eda9e3e":"# Step 5: Fit a AdaBoost model, \" compared to \"Decision Tree model, accuracy go up by 10%\nclf2 = AdaBoostClassifier(n_estimators=100)\nclf2.fit(X_train, y_train)\ny_pred2 = clf2.predict(X_test)\nf1_score(y_test, y_pred2)","bfbcf47a":"# Step 6: Fit a Gradient Boosting model, \" compared to \"Decision Tree model, accuracy go up by 10%\nclf3 = GradientBoostingClassifier(n_estimators=100)\nclf3.fit(X_train, y_train)\ny_pred3 = clf3.predict(X_test)\nf1_score(y_test, y_pred3)","5a7af988":"y_sub_pred=classifier.predict(df_test)","557aa7b0":"len(y_sub_pred)","b091876c":"df_sub['m13'] = y_sub_pred","e5700ee6":"df_sub.to_csv('sub_xgboost_final.csv', index=False)","bb56c175":"AS XGBOOST IS GIVING  BETTER RESULT WE WILL USE THAT TO PRDICT THE TEST TARGET VALUES","b76292e7":"In all the columns the mean is not too far from min-max values so we dont need to do any kind of scaling on the data. ","ad179c8e":"class distribution on predicted values","2d88aa38":"Now we make a count plot of all the columns ","8289f607":"### With only numeric columns","20359cf1":"The target colum is highly imbalanced once we will so oversampling\/undersampling of the data points","397483af":"Now lets study the target column","c99618d6":"We can see that monthwise delinquency peformance columns are related to target columns more than any other column.","e07147d5":"### Correlation of independent variables to target column","b29b27bd":"Lets try few more models","06068954":"Let us do sampling of the data to balance target feature.\n## Sampling\nAs the data is highly imbalanced we will first oversample the minority class using SMOTE and then undersample the majority  class using RandonUnderSampler to balance the minority class.","74f94f90":"The f1 score of 1 is not a realistic score so we will take all the columns and again do the run the model over all the columns","649670f4":"Now the score seems to be very less, reason being the target columns highly imbalanced","8dded07a":"## Modelling","3d0b387a":"Submission csv file","33311f72":"We can see that all the columns are discrete and except four, all the columns have less than 20 value types.","3420deeb":"Class distriburion of the train dataset","82aa4c90":"## Encoding categorical columns"}}