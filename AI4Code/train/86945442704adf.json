{"cell_type":{"ece9dc88":"code","b6fa4905":"code","b91b83ad":"code","250e6b02":"code","ac045682":"code","adeac2ec":"code","b65fa894":"code","8897cecc":"code","b560632b":"code","07920ac0":"code","25f67be4":"code","6dd9e2f3":"code","31bb0aac":"code","4cc9e3b1":"code","971313d8":"code","5337a989":"code","12b97daf":"code","1c047022":"code","b0949e4e":"code","382b6dca":"code","56790adf":"code","8e484324":"code","08e9d86a":"code","60111123":"code","fd90504c":"code","fbc9ec62":"code","185b3095":"code","555b76c0":"code","137039b3":"code","3c3f26a2":"code","e65cf6a9":"code","75f6f4ed":"code","d7ccfc58":"code","c7297577":"code","327c6ad7":"code","756307bb":"code","7b917d0a":"code","3d82aec3":"code","348f0597":"code","d79c9e89":"code","e408eea4":"code","2c04a00d":"code","d7ca89bf":"code","cb5e0450":"code","40dc5d13":"code","c680b70b":"code","b074d105":"markdown","03a155ae":"markdown","ad77033f":"markdown","8bea04d7":"markdown","52dcb103":"markdown","e2defe70":"markdown","a98a32bf":"markdown","ab597d73":"markdown","b4a21717":"markdown","5f9e7ed6":"markdown","0a39a79a":"markdown","ccb37208":"markdown","d8205aed":"markdown","82f76228":"markdown","6ffec810":"markdown","64fc276b":"markdown","4b018445":"markdown","24dabd79":"markdown","159c19b8":"markdown"},"source":{"ece9dc88":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport re\n%matplotlib inline\n\n!pip install nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer\n\n!pip install wordcloud\nfrom wordcloud import WordCloud\n\n!pip install tweet-preprocessor\nimport preprocessor as p\n\nfrom gensim.models import KeyedVectors\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report","b6fa4905":"data_dir = \"..\/input\"","b91b83ad":"!ls {data_dir}","250e6b02":"encoding = 'ISO-8859-1'\ncol_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n\ndataset = pd.read_csv(os.path.join(data_dir, 'sentiment140\/training.1600000.processed.noemoticon.csv'), encoding=encoding, names=col_names)","ac045682":"dataset.head()","adeac2ec":"df = dataset.copy().sample(8000, random_state=42)\ndf[\"label\"] = 0\ndf = df[['text', 'label']]\ndf.dropna(inplace=True)\ndf.head()","b65fa894":"col_names = ['id', 'text']\ndf2 = pd.read_csv(os.path.join(data_dir, 'depressive-tweets-processed\/depressive_tweets_processed.csv'), sep = '|', header = None, usecols = [0,5], nrows = 3200, names=col_names)","8897cecc":"df2.info()","b560632b":"# add `label` colum with value 1's\ndf2['label'] = 1\ndf2 = df2[['text', 'label']]","07920ac0":"df = pd.concat([df,df2]) # merge the dataset on normal tweets and depressive tweets\ndf = df.sample(frac=1)  # shuffle the dataset","25f67be4":"df.info()","6dd9e2f3":"contractions = pd.read_json(os.path.join(data_dir, 'english-contractions\/contractions.json'), typ='series')\ncontractions = contractions.to_dict()","31bb0aac":"c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return contractions[match.group(0)]\n    return c_re.sub(replace, text)","4cc9e3b1":"BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n\ndef clean_tweets(tweets):\n    cleaned_tweets = []\n    for tweet in tweets:\n        tweet = str(tweet)\n        tweet = tweet.lower()\n        tweet = BAD_SYMBOLS_RE.sub(' ', tweet)\n        tweet = p.clean(tweet)\n        \n        #expand contraction\n        tweet = expandContractions(tweet)\n\n        #remove punctuation\n        tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n\n        #stop words\n        stop_words = set(stopwords.words('english'))\n        word_tokens = nltk.word_tokenize(tweet) \n        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n        tweet = ' '.join(filtered_sentence)\n        \n        cleaned_tweets.append(tweet)\n        \n    return cleaned_tweets","971313d8":"X = clean_tweets([tweet for tweet in df['text']])","5337a989":"depressive_tweets = [clean_tweets([t for t in df2['text']])]\ndepressive_words = ' '.join(list(map(str, depressive_tweets)))\ndepressive_wc = WordCloud(width = 512,height = 512, collocations=False, colormap=\"Blues\").generate(depressive_words)","12b97daf":"plt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(depressive_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","1c047022":"MAX_NUM_WORDS = 10000\ntokenizer= Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(X)","b0949e4e":"word_vector = tokenizer.texts_to_sequences(X)","382b6dca":"word_index = tokenizer.word_index","56790adf":"vocab_size = len(word_index)\nvocab_size   # num of unique tokens","8e484324":"MAX_SEQ_LENGTH = 140\ninput_tensor = pad_sequences(word_vector, maxlen=MAX_SEQ_LENGTH)","08e9d86a":"input_tensor.shape","60111123":"corpus = df['text'].values.astype('U')\ntfidf = TfidfVectorizer(max_features = MAX_NUM_WORDS) \ntdidf_tensor = tfidf.fit_transform(corpus)","fd90504c":"tdidf_tensor.shape","fbc9ec62":"# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(tdidf_tensor, df['label'].values, test_size=0.3)","185b3095":"baseline_model = SVC()\nbaseline_model.fit(x_train, y_train)","555b76c0":"predictions = baseline_model.predict(x_test)","137039b3":"accuracy_score(y_test, predictions)","3c3f26a2":"print(classification_report(y_test, predictions, digits=5))","e65cf6a9":"EMBEDDING_FILE = os.path.join(data_dir, 'googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin.gz')\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)","75f6f4ed":"EMBEDDING_DIM = 300\nembedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))","d7ccfc58":"for (word, idx) in word_index.items():\n    if word in word2vec.vocab and idx < MAX_NUM_WORDS:\n        embedding_matrix[idx] = word2vec.word_vec(word)","c7297577":"inp = Input(shape=(MAX_SEQ_LENGTH,))\nx = Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.25)(x)\nx = Dense(1, activation=\"sigmoid\")(x)","327c6ad7":"# Compile the model\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","756307bb":"# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(input_tensor, df['label'].values, test_size=0.3)","7b917d0a":"model.fit(x_train, y_train, batch_size=16, epochs=10)","3d82aec3":"preds = model.predict(x_test)","348f0597":"preds  = np.round(preds.flatten())\nprint(classification_report(y_test, preds, digits=5))","d79c9e89":"x_train, x_test, y_train, y_test = train_test_split(X, df.label, test_size=0.3, random_state = 42)","e408eea4":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\nnb.fit(x_train, y_train)","2c04a00d":"y_pred = nb.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","d7ca89bf":"from sklearn.linear_model import SGDClassifier\n\nsgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(x_train, y_train)","cb5e0450":"y_pred = sgd.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","40dc5d13":"from sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(x_train, y_train)","c680b70b":"y_pred = logreg.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","b074d105":"It's easy to spot words that are indicative of depression in these tweets: depression, treatment, suffering, crying, help, struggle, risk, hate, sad, anxiety, disorder, suicide, stress, therapy, mental health, emotional, bipolar.","03a155ae":"### Datasets\n\nFor this analysis, the [Sentiment140](https:\/\/www.kaggle.com\/kazanova\/sentiment140) dataset is used. ","ad77033f":"## LTSM model\n\nLet's improve our model with LTSM. ","8bea04d7":"### Training","52dcb103":"### Training","e2defe70":"### Word embedding","a98a32bf":"### TF-IDF classifier","ab597d73":"Since there is no readily available public dataset on depression, I found a dataset scraped by [Twint](https:\/\/github.com\/twintproject\/twint).  ","b4a21717":"# Introduction\n\nThe goal of this study is to investigate how Twitter, a popular social media platform, can be leveraged in detecting early risk of depression of its users. The study is inspired by [this article](https:\/\/time.com\/1915\/how-twitter-knows-when-youre-depressed\/). ","5f9e7ed6":"## Word analysis","0a39a79a":"## Classification of depressive and normal tweets","ccb37208":"### Tokenization","d8205aed":"## Playing with other models","82f76228":"### Naive Baye's","6ffec810":"## Baseline model","64fc276b":"## Linear Support Vector","4b018445":"### Preprocessing","24dabd79":"## Logistic Regression","159c19b8":"For this experiment, I took a random sample of 8000 tweets."}}