{"cell_type":{"8889d8d5":"code","f3503eb9":"code","9a4d511f":"code","0457d4b6":"code","eb729571":"code","b2899ec4":"code","52398302":"code","f7037ca5":"code","ce4af4bb":"code","cb15e8e8":"code","4cf475a0":"code","d81f1bb0":"code","aef06299":"code","32c42df0":"code","54dcd180":"code","2286b77d":"code","339c89d2":"code","e4603e88":"code","3fa8a6be":"code","202b2207":"code","406bed5f":"code","b0ffdc1d":"code","4bf8ba6b":"code","f323db1b":"code","aa4ca0ed":"code","6494e430":"code","20d03aa9":"markdown","56a505fa":"markdown","b975e231":"markdown","a837de0a":"markdown","927daa8a":"markdown","78fcf9c5":"markdown","b37080d5":"markdown","5b78b0c6":"markdown","eb753922":"markdown","7cd9a03f":"markdown","c1c90bfe":"markdown","f84618f1":"markdown","16094c78":"markdown","0c4f31e9":"markdown","22e04e4c":"markdown","d5db6deb":"markdown","2682fc08":"markdown","aaeb4331":"markdown","ed76c84d":"markdown","161a0515":"markdown","20d7b054":"markdown"},"source":{"8889d8d5":"# Common libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Utils\nfrom tqdm import tqdm\n\n# Image processing\nimport cv2\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\n\n# Tensorflow\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.optimizers import Adam","f3503eb9":"input_path = '..\/input\/horses-or-humans-dataset\/horse-or-human'\nlabels = ['horse', 'human']\n\ndef create_dataframe(type):\n    df = pd.DataFrame()\n    for label in labels:\n        label_path = os.path.join(input_path, type, label + 's')\n        file_paths = os.listdir(label_path)\n        file_paths = pd.Series([os.path.join(label_path, file_path) for file_path in file_paths], name='file_path')\n        label = pd.Series([label] * len(file_paths), name='label')\n        temp = pd.concat((file_paths, label), axis=1)\n        df = pd.concat((df, temp), axis=0)\n    return df\n\ntrain_df = create_dataframe('train')\nvalid_df = create_dataframe('validation')","9a4d511f":"# Shuffle data\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\nvalid_df = valid_df.sample(frac=1).reset_index(drop=True)","0457d4b6":"train_df.label.hist()\nvalid_df.label.hist()\nplt.title('Label counts')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()","eb729571":"def plot_first_six_image():\n    fig = plt.figure(figsize=(20, 20))\n    for i in range(6):\n        row = train_df.loc[i]\n        img_path = row.file_path\n        label= row.label\n        ax = plt.subplot(1, 6, i+1)      \n        img = image.load_img(img_path)\n        ax.set_title(label)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(np.squeeze(img))\n        \nplot_first_six_image()","b2899ec4":"# Convert file path to image\n\n#Dont use cv2.imread to load img because cv2 change channel from RGB to BGR\n\"\"\"\nX_train = np.array([cv2.imread(image_file_path) for image_file_path in train_df.file_path])\nX_valid = np.array([cv2.imread(image_file_path) for image_file_path in valid_df.file_path])\nX_test = np.array([cv2.imread(image_file_path) for image_file_path in test_df.file_path])\n\"\"\"\n\ndef path_to_tensor(img_path):\n    # Load image\n    img = image.load_img(img_path, target_size=(224, 224))\n    # Convert image to 3d tensor\n    img = image.img_to_array(img)\n    # Convert image to 4d tensor\n    img = np.expand_dims(img, axis=0)\n    return img\n    \ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)","52398302":"print('Converting train file paths to tensor...')\nX_train = paths_to_tensor(train_df.file_path)\nprint('Done')\nprint('Converting validation file paths to tensor...')\nX_valid = paths_to_tensor(valid_df.file_path)\nprint('Done')\n\nprint('X_train shape: ', X_train.shape)\nprint('X_valid shape: ', X_valid.shape)","f7037ca5":"mean = np.mean(X_train, axis=(0, 1, 2, 3))\nstd = np.std(X_train, axis=(0, 1, 2, 3))\nX_train = (X_train - mean) \/ (std + 1e-7)\nX_valid = (X_valid - mean) \/ (std + 1e-7)","ce4af4bb":"# Get y_train, y_valid, y_test\ny_train = train_df.label\ny_valid = valid_df.label","cb15e8e8":"label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)\ny_valid = label_encoder.fit_transform(y_valid)","4cf475a0":"print('First 5 labels after label encode: ',y_train[:5])\nprint('y_train shape: ',y_train.shape)","d81f1bb0":"onehot_encoder = OneHotEncoder(sparse=False)\n\n# len(y_train) = 827\n# Reshape y_train from 1d array (827, ) to 2d array (827, 1)\ny_train = y_train.reshape(len(y_train), 1)\ny_valid = y_valid.reshape(len(y_valid), 1)\n\nprint('y_train after reshape: ', y_train.shape)\n\ny_train = onehot_encoder.fit_transform(y_train)\ny_valid = onehot_encoder.fit_transform(y_valid)\n\nprint('First 5 labels after one-hot encode: ', y_train[:5])\nprint('y_train after one-hot encoding shape: ', y_train.shape)","aef06299":"X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)","32c42df0":"print('# samples of train set: ', len(X_train))\nprint('# samples of valid set: ', len(X_valid))\nprint('# samples of test set: ', len(X_test))","54dcd180":"datagen = ImageDataGenerator(rotation_range=15,\n                            width_shift_range=0.1,\n                            height_shift_range=0.1,\n                            brightness_range=(0.3, 0.7),\n                            horizontal_flip=True,\n                            vertical_flip=True,\n                            )\ndatagen.fit(X_train)","2286b77d":"# Download vgg15\nbase_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n# Freeze layers\nfor layer in base_model.layers:\n    layer.trainable = False\n","339c89d2":"last_layer = base_model.get_layer('block5_pool')\nlast_output = last_layer.output\n\nx = Flatten() (last_output)\n\nx = Dense(units=64, activation='relu') (x)\nx = BatchNormalization() (x)\nx = Dropout(rate=0.5) (x)\nx = Dense(units=2, activation='softmax', name='softmax') (x)\n\nnew_model = Model(inputs=base_model.input, outputs=x)","e4603e88":"if not os.path.exists('models'):\n    os.mkdir('models')","3fa8a6be":"history = None\n\ncheckpoint = ModelCheckpoint('models\/model-{epoch:0d}.h5',\n                            monitor='val_loss',\n                            verbose=1,\n                            save_best_only=True,\n                            mode='auto',\n                            save_weights_only=True)\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                              verbose=1,\n                              min_delta=0,\n                              patience=5,\n                              mode='auto',\n                              restore_best_weights=False)\n\nnew_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['categorical_accuracy'])\n\nif os.path.exists('models\/model-4.h5'):\n    new_model.load_weights('models\/model-4.h5')\nelse:\n    history = new_model.fit(datagen.flow(X_train, y_train, batch_size=128),\n             callbacks=[checkpoint, early_stopping],\n             epochs=100,\n             verbose=1,\n             batch_size=128,\n             validation_data=(X_valid, y_valid))","202b2207":"if history != None:\n    history_df = pd.DataFrame(history.history)\n    history_df.loc[:, ['loss', 'val_loss']].plot()","406bed5f":"score = new_model.evaluate(X_test, y_test, batch_size=128, verbose=1)","b0ffdc1d":"print('Test accuracy: %.3f\\nTest loss: %.3f' % (score[1]*100, score[0]))","4bf8ba6b":"y_hat = new_model.predict(X_test, batch_size=128)","f323db1b":"y_hat = np.argmax(y_hat, axis=1)","aa4ca0ed":"print(y_hat)\nground_truth = np.argmax(y_test, axis=1)\nconf_matrix = confusion_matrix(ground_truth, y_hat)\nprint(conf_matrix)","6494e430":"labels = ['horse', 'human']\n\nplt.imshow(conf_matrix, cmap=plt.cm.Blues)\nplt.colorbar()\nnum_classes = len(labels)\nfor i in range(num_classes):\n    for j in range(num_classes):\n        plt.text(j, i, conf_matrix[i, j],\n                horizontalalignment='center',\n                verticalalignment='center'),\n        plt.title('Confusion matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('Ground truth')\n        plt.xticks(range(num_classes), labels)\n        plt.yticks(range(num_classes), labels)\nplt.show()","20d03aa9":"## **2.4 Normalize data**","56a505fa":"## **4.2 Evaluate test set**","b975e231":"## **4. Evaluate**","a837de0a":"### **2.5.2 Label encoding**","927daa8a":"### **2.5.3 One-hot encoding**","78fcf9c5":"## **3.3 Compile model**","b37080d5":"## **2.5 One-hot encode labels**","5b78b0c6":"# **2. Preprocessing data**","eb753922":"## **4.1 Plot loss value**","7cd9a03f":"## **2.1 Create dataframe contains file path and label**","c1c90bfe":"### **2.5.1 Get y_train, y_valid, y_test**","f84618f1":"# **1. Import libs**","16094c78":"## **2.7 Data augmentation**","0c4f31e9":"## **3.2 Create new model**","22e04e4c":"## **2.2 Visualize images**","d5db6deb":"### **4.3.1 Generating y_hat**","2682fc08":"## **3.1 Get vgg16 as feature extractor**","aaeb4331":"# **3. Build model**","ed76c84d":"## **2.6 Split to train\/test dataset**\n","161a0515":"## **4.3 Confusion matrix**","20d7b054":"## **2.3 Load image**"}}