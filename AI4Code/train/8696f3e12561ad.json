{"cell_type":{"3f261f43":"code","5021e837":"code","bfd6debe":"code","ce6eeaae":"code","fa13c807":"code","e4e8eea2":"code","7bd4a1e9":"code","043c4886":"code","6af0cc9a":"code","b3304460":"code","c4555507":"code","2406ed78":"code","06925d09":"code","52c3c1d9":"code","8bc8023b":"code","1e86922f":"code","c5ff9ca7":"code","a24f4cfe":"code","61df7804":"code","9e1b4a79":"code","6d7559c1":"code","d85030ce":"code","808b9526":"code","3b9dd939":"code","40877a8a":"code","d297869b":"code","96aff9ff":"code","d50d73fe":"code","0cf42b1d":"code","f88c4b92":"code","fa10c949":"code","980ccc9f":"code","3a9f20e4":"code","adf9ccb3":"code","7f15cecb":"code","a52d4b1c":"code","52560fe9":"code","4aaf4569":"code","512a12ab":"code","78b4d144":"code","d12905a5":"code","fa2b2550":"code","91a66821":"code","dec7cbf8":"code","552ef7e8":"code","2ea8768b":"markdown","3805a83f":"markdown","50cd4572":"markdown","822e0181":"markdown","847def8d":"markdown","89f7887b":"markdown","a40c2a03":"markdown","0dc7925d":"markdown","021139be":"markdown","2455b890":"markdown","7cf3dd36":"markdown","83fa3324":"markdown","08fe66c2":"markdown","ed140f2e":"markdown","e61a56cd":"markdown"},"source":{"3f261f43":"# https:\/\/www.kaggle.com\/arunkumarramanan\/market-data-nn-baseline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\n# from plotly.tools import FigureFactory as FF \nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\n#=================================================\n#***********************************import keras\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\nimport keras\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD","5021e837":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","bfd6debe":"(market_train_df, news_train_df) = env.get_training_data()","ce6eeaae":"market_train, news_train = market_train_df.copy(), news_train_df.copy()\nmarket_train_df1, news_train_df1 = market_train_df.copy(), news_train_df.copy()","fa13c807":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nimport plotly.figure_factory as ff\n\n\n######### Function\ndef mis_value_graph(data):\n#     data.isnull().sum().plot(kind=\"bar\", figsize = (20,10), fontsize = 20)\n#     plt.xlabel(\"Columns\", fontsize = 20)\n#     plt.ylabel(\"Value Count\", fontsize = 20)\n#     plt.title(\"Total Missing Value By Column\", fontsize = 20)\n#     for i in range(len(data)):\n#          colors.append(generate_color())\n            \n    data = [\n    go.Bar(\n        x = data.columns,\n        y = data.isnull().sum(),\n        name = 'Unknown Assets',\n        textfont=dict(size=20),\n        marker=dict(\n#         color= colors,\n        line=dict(\n            color=generate_color(),\n            width=2,\n        ), opacity = 0.45\n    )\n    ),\n    ]\n    layout= go.Layout(\n        title= '\"Total Missing Value By Column\"',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n    fig= go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='skin')\n    \n\ndef mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\n\nimport random\n\ndef generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color","e4e8eea2":"mis_value_graph(market_train_df)\nmarket_train_df = mis_impute(market_train_df)\nmarket_train_df.isna().sum().to_frame()","7bd4a1e9":"# https:\/\/www.kaggle.com\/pestipeti\/simple-eda-two-sigma","043c4886":"best_asset_volume = market_train_df.groupby(\"assetCode\")[\"close\"].count().to_frame().sort_values(by=['close'],ascending= False)\nbest_asset_volume = best_asset_volume.sort_values(by=['close'])\nlargest_by_volume = list(best_asset_volume.nlargest(10, ['close']).index)\n# largest_by_volume","6af0cc9a":"for i in largest_by_volume:\n    asset1_df = market_train_df[(market_train_df['assetCode'] == i) & (market_train_df['time'] > '2015-01-01') & (market_train_df['time'] < '2017-01-01')]\n    # Create a trace\n    trace1 = go.Scatter(\n        x = asset1_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset1_df['close'].values,\n        line = dict(color = generate_color()),opacity = 0.8\n    )\n\n    layout = dict(title = \"Closing prices of {}\".format(i),\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  )\n\n    data = [trace1]\n    py.iplot(dict(data=data, layout=layout), filename='basic-line')","b3304460":"for i in largest_by_volume:\n\n    asset1_df['high'] = asset1_df['open']\n    asset1_df['low'] = asset1_df['close']\n\n    for ind, row in asset1_df.iterrows():\n        if row['close'] > row['open']:\n            \n            asset1_df.loc[ind, 'high'] = row['close']\n            asset1_df.loc[ind, 'low'] = row['open']\n\n    trace1 = go.Candlestick(\n        x = asset1_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        open = asset1_df['open'].values,\n        low = asset1_df['low'].values,\n        high = asset1_df['high'].values,\n        close = asset1_df['close'].values,\n        increasing=dict(line=dict(color= generate_color())),\n        decreasing=dict(line=dict(color= generate_color())))\n\n    layout = dict(title = \"Candlestick chart for {}\".format(i),\n                  xaxis = dict(\n                      title = 'Month',\n                      rangeslider = dict(visible = False)\n                  ),\n                  yaxis = dict(title = 'Price (USD)')\n                 )\n    data = [trace1]\n\n    py.iplot(dict(data=data, layout=layout), filename='basic-line')","c4555507":"assetsByTradingDay = market_train_df.groupby(market_train_df['time'].dt.date)['assetCode'].nunique()\n# Create a trace\ntrace1 = go.Bar(\n    x = assetsByTradingDay.index, # asset1_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = assetsByTradingDay.values, \n    marker=dict(\n        color= generate_color(),\n        line=dict(\n            color=generate_color(),\n            width=1.5,\n        ), opacity = 0.8\n    )\n)\n\nlayout = dict(title = \"Assets by trading days\",\n              xaxis = dict(title = 'Year'),\n              yaxis = dict(title = 'Assets'))\ndata = [trace1]\n\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","2406ed78":"for i in range(1,100,10):\n    volumeByAssets = market_train_df.groupby(market_train_df['assetCode'])['volume'].sum()\n    highestVolumes = volumeByAssets.sort_values(ascending=False)[i:i+9]\n    # Create a trace\n    colors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n    trace1 = go.Pie(\n        labels = highestVolumes.index,\n        values = highestVolumes.values,\n        textfont=dict(size=20),\n        marker=dict(colors=colors,line=dict(color='#000000', width=2)), hole = 0.45)\n    layout = dict(title = \"Highest trading volumes for range of {} to {}\".format(i, i+9))\n    data = [trace1]\n    py.iplot(dict(data=data, layout=layout), filename='basic-line')\n","06925d09":"assetNameGB = market_train_df[market_train_df['assetName'] == 'Unknown'].groupby('assetCode')\nunknownAssets = assetNameGB.size().reset_index('assetCode')\nunknownAssets.columns = ['assetCode',\"value\"]\nunknownAssets = unknownAssets.sort_values(\"value\", ascending= False)\nunknownAssets.head(5)\n\ncolors = []\nfor i in range(len(unknownAssets)):\n     colors.append(generate_color())\n\n        \ndata = [\n    go.Bar(\n        x = unknownAssets.assetCode.head(25),\n        y = unknownAssets.value.head(25),\n        name = 'Unknown Assets',\n        textfont=dict(size=20),\n        marker=dict(\n        color= colors,\n        line=dict(\n            color='#000000',\n            width=2,\n        ), opacity = 0.45\n    )\n    ),\n    ]\nlayout= go.Layout(\n    title= 'Unknown Assets by Asset code',\n    xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n    yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n    showlegend=True\n)\nfig= go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='skin')","52c3c1d9":"mis_value_graph(news_train_df)\nnews_train_df = mis_impute(news_train_df)\nnews_train_df.isna().sum().to_frame()","8bc8023b":"print(\"News data shape\",news_train_df.shape)\nnews_train_df.head()","1e86922f":"# news_train_df['urgency'].value_counts()\nnews_sentiment_count = news_train_df.groupby([\"urgency\",\"assetName\"])[[\"sentimentNegative\",\"sentimentNeutral\",\"sentimentPositive\"]].count()\nnews_sentiment_count = news_sentiment_count.reset_index()","c5ff9ca7":"trace = go.Table(\n    header=dict(values=list(news_sentiment_count.columns),\n                fill = dict(color='rgba(55, 128, 191, 0.7)'),\n                align = ['left'] * 5),\n    cells=dict(values=[news_sentiment_count.urgency,news_sentiment_count.assetName,news_sentiment_count[\"sentimentNegative\"], news_sentiment_count[\"sentimentPositive\"], news_sentiment_count[\"sentimentNeutral\"]],\n               fill = dict(color='rgba(245, 246, 249, 1)'),\n               align = ['left'] * 5))\n\ndata = [trace] \npy.iplot(data, filename = 'pandas_table')","a24f4cfe":"trace0 = go.Bar(\n    x= news_sentiment_count.assetName.head(30),\n    y=news_sentiment_count.sentimentNegative.values,\n    name='sentimentNegative',\n    textfont=dict(size=20),\n        marker=dict(\n        color= generate_color(),\n        opacity = 0.87\n    )\n)\ntrace1 = go.Bar(\n    x= news_sentiment_count.assetName.head(30),\n    y=news_sentiment_count.sentimentNeutral.values,\n    name='sentimentNeutral',\n    textfont=dict(size=20),\n        marker=dict(\n        color= generate_color(),\n        opacity = 0.87\n    )\n)\ntrace2 = go.Bar(\n    x= news_sentiment_count.assetName.head(30),\n    y=news_sentiment_count.sentimentPositive.values,\n    name='sentimentPositive',\n    textfont=dict(size=20),\n    marker=dict(\n        color= generate_color(),\n        opacity = 0.87\n    )\n)\n\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    xaxis=dict(tickangle=-45),\n    barmode='group',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='angled-text-bar')","61df7804":"news_sentiment_urgency = news_train_df.groupby([\"urgency\"])[[\"sentimentNegative\",\"sentimentNeutral\",\"sentimentPositive\"]].count()\nnews_sentiment_urgency = news_sentiment_urgency.reset_index()","9e1b4a79":"trace = go.Table(\n    header=dict(values=list(news_sentiment_urgency.columns),\n                fill = dict(color='rgba(55, 128, 191, 0.7)'),\n                align = ['left'] * 5),\n    cells=dict(values=[news_sentiment_urgency.urgency,news_sentiment_urgency[\"sentimentNegative\"], news_sentiment_urgency[\"sentimentPositive\"], news_sentiment_urgency[\"sentimentNeutral\"]],\n               fill = dict(color='rgba(245, 246, 249, 1)'),\n               align = ['left'] * 5))\n\ndata = [trace] \npy.iplot(data, filename = 'pandas_table')","6d7559c1":"trace0 = go.Bar(\n    x= news_sentiment_urgency.urgency.values,\n    y=news_sentiment_urgency.sentimentNegative.values,\n    name='sentimentNegative',\n    textfont=dict(size=20),\n        marker=dict(\n        color= generate_color(),\n            line=dict(\n            color='#000000',\n            width=2,\n        ),\n        opacity = 0.87\n    )\n)\ntrace1 = go.Bar(\n    x= news_sentiment_urgency.urgency.values,\n    y=news_sentiment_urgency.sentimentNegative.values,\n    name='sentimentNeutral',\n    textfont=dict(size=20),\n        marker=dict(\n        color= generate_color(),\n        line=dict(\n            color='#000000',\n            width=2,\n        ),\n        opacity = 0.87\n    )\n)\ntrace2 = go.Bar(\n    x= news_sentiment_urgency.urgency.values,\n    y=news_sentiment_urgency.sentimentNegative.values,\n    name='sentimentPositive',\n    textfont=dict(size=20),\n    marker=dict(\n        line=dict(\n            color='#000000',\n            width=2,\n        ),\n        color= generate_color(),\n        opacity = 0.87\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    xaxis=dict(tickangle=-45),\n    barmode='group',\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='angled-text-bar')","d85030ce":"cat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']","808b9526":"from sklearn.model_selection import train_test_split\ntrain_indices, val_indices = train_test_split(market_train_df1.index.values,test_size=0.25, random_state=23)","3b9dd939":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train_df1.loc[train_indices, cat].astype(str).unique())}\n    market_train_df1[cat] = market_train_df1[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","40877a8a":"from sklearn.preprocessing import StandardScaler\n \nmarket_train_df1[num_cols] = market_train_df1[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train_df1[num_cols] = scaler.fit_transform(market_train_df1[num_cols])","d297869b":"# %%time\n# def data_prep(market_train,news_train):\n#     market_train.time = market_train.time.dt.date\n#     news_train.time = news_train.time.dt.hour\n#     news_train.sourceTimestamp= news_train.sourceTimestamp.dt.hour\n#     news_train.firstCreated = news_train.firstCreated.dt.date\n#     news_train['assetCodesLen'] = news_train['assetCodes'].map(lambda x: len(eval(x)))\n#     news_train['assetCodes'] = news_train['assetCodes'].map(lambda x: list(eval(x))[0])\n#     kcol = ['firstCreated', 'assetCodes']\n#     news_train = news_train.groupby(kcol, as_index=False).mean()\n#     market_train = pd.merge(market_train, news_train, how='left', left_on=['time', 'assetCode'], \n#                             right_on=['firstCreated', 'assetCodes'])\n#     lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n#     market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    \n    \n#     market_train = market_train.dropna(axis=0)\n    \n#     return market_train\n\n# market_train = data_prep(market_train_df, news_train_df)\n# market_train.shape","96aff9ff":"# %%time\n# from datetime import datetime, date\n# # The target is binary\n# market_train = market_train.loc[market_train['time_x']>=date(2009, 1, 1)]\n# up = market_train.returnsOpenNextMktres10 >= 0\n# fcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n#                                              'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n#                                              'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]","d50d73fe":"# %%time\n# # We still need the returns for model tuning\n# X = market_train[fcol].values\n# up = up.values\n# r = market_train.returnsOpenNextMktres10.values\n\n# # Scaling of X values\n# # It is good to keep these scaling values for later\n# mins = np.min(X, axis=0)\n# maxs = np.max(X, axis=0)\n# rng = maxs - mins\n# X = 1 - ((maxs - X) \/ rng)\n\n# # Sanity check\n# assert X.shape[0] == up.shape[0] == r.shape[0]","0cf42b1d":"# %%time\n# # from xgboost import XGBClassifier\n# from sklearn import model_selection\n# from sklearn.metrics import accuracy_score\n# import time\n\n# X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.25, random_state=99)","f88c4b92":"# xgb_up = XGBClassifier(n_jobs=4,n_estimators=250,max_depth=8,eta=0.1)","fa10c949":"# t = time.time()\n# print('Fitting Up')\n# xgb_up.fit(X_train,up_train)\n# print(f'Done, time = {time.time() - t}')","980ccc9f":"# from sklearn.metrics import accuracy_score\n# accuracy_score(xgb_up.predict(X_test),up_test)","3a9f20e4":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\ncategorical_logits =Dropout(0.5)(categorical_logits)\ncategorical_logits =BatchNormalization()(categorical_logits)\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits=Dropout(0.5)(numerical_logits)\nnumerical_logits = BatchNormalization()(numerical_logits)\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)","adf9ccb3":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df1, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train_df1, val_indices)","7f15cecb":"# https:\/\/www.kaggle.com\/guowenrui\/market-nn-if-you-like-you-can-use-it-and-upvote\nclass SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])\/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')\n        \nclass SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"model.hdf5\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T \/\/ self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner \/= self.T \/\/ self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero \/ 2 * cos_out)","a52d4b1c":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=5,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","52560fe9":"from sklearn.metrics import accuracy_score\n# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\nconfidence_valid = model.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","4aaf4569":"# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","512a12ab":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# df = pd.DataFrame({'imp': xgb_up.feature_importances_, 'col':fcol})\n# df = df.sort_values(['imp','col'], ascending=[True, False])\n# # _ = df.plot(kind='barh', x='col', y='imp', figsize=(7,12))\n\n\n# #plt.savefig('lgb_gain.png')\n# trace = go.Table(\n#     header=dict(values=list(df.columns),\n#                 fill = dict(color='rgba(55, 128, 191, 0.7)'),\n#                 align = ['left'] * 5),\n#     cells=dict(values=[df.imp,df.col],\n#                fill = dict(color='rgba(245, 246, 249, 1)'),\n#                align = ['left'] * 5))\n\n# data = [trace] \n# py.iplot(data, filename = 'pandas_table')","78b4d144":"# data = [df]\n# for dd in data:  \n#     colors = []\n#     for i in range(len(dd)):\n#          colors.append(generate_color())\n\n#     data = [\n#         go.Bar(\n#         orientation = 'h',\n#         x=dd.imp,\n#         y=dd.col,\n#         name='Features',\n#         textfont=dict(size=20),\n#             marker=dict(\n#             color= colors,\n#             line=dict(\n#                 color='#000000',\n#                 width=0.5\n#             ),\n#             opacity = 0.87\n#         )\n#     )\n#     ]\n#     layout= go.Layout(\n#         title= 'Feature Importance of XGBOOST',\n#         xaxis= dict(title='Columns', ticklen=5, zeroline=True, gridwidth=2),\n#         yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n#         showlegend=True\n#     )\n\n#     py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')","d12905a5":"days = env.get_prediction_days()","fa2b2550":"import time\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","91a66821":"# sub  = pd.read_csv(\"submission.csv\")\n# sub.head()","dec7cbf8":"# import matplotlib.pyplot as plt\n# %matplotlib inline\n# from xgboost import plot_importance\n# plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n# plt.bar(range(len(xgb_up.feature_importances_)), xgb_up.feature_importances_)\n# plt.xticks(range(len(xgb_up.feature_importances_)), fcol, rotation='vertical');","552ef7e8":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","2ea8768b":"# 1.5 Unknown Value By Assets Code","3805a83f":"## 1.1 Top-10 Largest Assets code by Close value","50cd4572":"# 4.Model Training","822e0181":"## 1.4 Asset Code Analysis","847def8d":"## 1.3 Assets By Trading Days","89f7887b":"#### First Time Load environment can't load again","a40c2a03":"# Two Sigma: Using News to Predict Stock Movements\n\n![](https:\/\/media0.giphy.com\/media\/rM0wxzvwsv5g4\/giphy.gif?cid=3640f6095bab5cfd7030627455631fb5)","0dc7925d":"# 2.news_train_df Data Investigation","021139be":"# 5.Final Submission\n\n### Feature Gain & Split","2455b890":"## 2.1 Sentiment Count By Asset code or Urgency","7cf3dd36":"## Neural Network Baseline","83fa3324":"## 1.2 Open and Close value of Top 10 Asset Code","08fe66c2":"## Notebook Outline\n\n1. [**market_train_df Data Investigation**](#1.-market_train_df-Data-Investigation) - DataFrame with market training data  \n    1.1 [**Top-10 Largest Assets code by Close value**](#1.1-Top-10-Largest-Assets-code-by-Close-value)  \n    1.2 [**Open and Close value of Top 10 Asset Code**](#1.2-Open-and-Close-value-of-Top-10-Asset-Code)  \n    1.3 [**Assets By Trading Days**](#1.3-Assets-By-Trading-Days)  \n    1.4 [**Asset Code Analysis**](#1.4-Asset-Code-Analysis)  \n    1.5 [**Unknown Value By Assets Code**](#1.5-Unknown-Value-By-Assets-Code)\n2. [**news_train_df Data Investigation**](#2.news_train_df-Data-Investigation) - DataFrame with news training data  \n    2.1 [**Sentiment Count By Asset code or Urgency**](#2.1-Sentiment-Count-By-Asset-code-or-Urgency)\n3. [**Data Prepare**](#3.Data-Prepare)\n4. [**Model Training**](#4.Model-Training)\n5. [**Final Submission**](#5.Final-Submission)","ed140f2e":"# 3.Data Prepare","e61a56cd":"# 1. market_train_df Data Investigation"}}