{"cell_type":{"afd16620":"code","f4bd093a":"code","23fc1094":"code","7bfd9f0c":"code","90b729ef":"code","fede8132":"code","ce0cf152":"code","61038a20":"code","4fde9edd":"code","a1e77f43":"code","63dc9c76":"code","f4bd2dc2":"code","2629860d":"code","a088e94b":"code","d38bfafc":"code","843319d9":"code","9534f535":"code","78064ce7":"code","08a52f26":"code","9a718181":"code","6fc98dda":"code","aeb3654c":"code","1244afa1":"code","6204e7e9":"markdown","7dd8e2dd":"markdown","d0ba0815":"markdown","28cf3b8e":"markdown","8f61dd70":"markdown","602e0e35":"markdown","53393533":"markdown","44af28e0":"markdown","9eafc11c":"markdown","714d6ed8":"markdown","d405f856":"markdown","b556c379":"markdown","e0b3e518":"markdown","72b06b83":"markdown","21c6ee0d":"markdown","1d3f538e":"markdown","9dd2789f":"markdown","b1288e86":"markdown","462c3359":"markdown","9b37faa5":"markdown","1fcf4570":"markdown","107ad0bf":"markdown","23f7f8db":"markdown","d64c1503":"markdown","37d15cd7":"markdown","db1c50fe":"markdown","2e550330":"markdown","84a1c76e":"markdown","cb5bcc49":"markdown","3c1f0c30":"markdown","48a4b855":"markdown","169574d0":"markdown","7bf70377":"markdown","ad20785b":"markdown","a935d818":"markdown","37722fd2":"markdown","66ca97fe":"markdown","1155e88f":"markdown","0817f63c":"markdown","9292a3c1":"markdown","e193b86e":"markdown","ee45561d":"markdown","368a653f":"markdown"},"source":{"afd16620":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport scipy.stats as ss\n\nprint('Libraries are imported')","f4bd093a":"%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\nsns.set_palette('deep')\nsns.set_color_codes()\nsns.set_style('dark')\n\nprint('Parameters are set')","23fc1094":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\nprint('Data is Loaded')","7bfd9f0c":"print(f'Shape of the Dataset is: {df.shape}')","90b729ef":"df.head()","fede8132":"df.isna().apply(pd.value_counts, axis=0)","ce0cf152":"categorical = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'thall', 'caa', 'slp']\ncontinous = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nprint('Categorical Variables are:', ', '.join(categorical))\nprint('Continous Variables are:', ', '.join(continous))","61038a20":"df.describe()","4fde9edd":"chart_count = len(continous) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Violin plot for the\\n continous features\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = continous[i - 1]\n    ax = axes[i]\n    ax.grid(axis='y', linestyle=':')\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    color = sns.color_palette('deep')[i - 1]\n    sns.violinplot(data=df, y=var, ax=ax, color=color)\n    ax.set_xlabel('')\n    ax.set_ylabel('')","a1e77f43":"chart_count = len(categorical) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Count plot for the\\n categorical features\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = categorical[i - 1]\n    ax = axes[i]\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    sns.countplot(data=df, x=var, ax=ax)\n    ax.set_xlabel('')\n    ax.set_ylabel('')","63dc9c76":"chart_count = len(categorical) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Seperation of categorical \\nvariables w.r.t to output\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = categorical[i - 1]\n    ax = axes[i]\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    colorIndex = (2*(i - 1)) % 10\n    color1 = sns.color_palette('deep')[colorIndex]\n    color2 = sns.color_palette('deep')[colorIndex + 1]\n    sns.countplot(data=df, x=var, ax=ax, hue='output',\n                  palette=[color1, color2])\n    ax.set_xlabel('')\n    ax.set_ylabel('')","f4bd2dc2":"def cramers_corrected_stat(x, y):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    result = -1\n\n    conf_matrix = pd.crosstab(x, y)\n\n    if conf_matrix.shape[0] == 2:\n        correct = False\n    else:\n        correct = True\n\n    chi2, p = ss.chi2_contingency(conf_matrix, correction=correct)[0:2]\n\n    n = sum(conf_matrix.sum())\n    phi2 = chi2\/n\n    r, k = conf_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    result = np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))\n    return round(result, 6), round(p, 6)\n\n\nfor var in categorical:\n    x = df[var]\n    y = df['output']\n    cramersV, p = cramers_corrected_stat(x, y)\n    print(f'For variable {var}, Cramer\\'s V: {cramersV} and p value: {p}')","2629860d":"chart_count = len(continous) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Violin plot for the\\n continous features\\ndiffering by output\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = continous[i - 1]\n    ax = axes[i]\n    ax.grid(axis='y', linestyle=':')\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    colorIndex = (2*(i - 1)) % 10\n    color1 = sns.color_palette('deep')[colorIndex]\n    color2 = sns.color_palette('deep')[colorIndex + 1]\n    sns.violinplot(data=df, y=var, x='output', ax=ax, palette=[color1, color2])\n    ax.set_xlabel('')\n    ax.set_ylabel('')","a088e94b":"chart_count = len(continous) + 1\n\nfig = plt.figure(figsize=(20, 15))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Distribution of continous\\n variables by output\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = continous[i - 1]\n    ax = axes[i]\n    ax.grid(axis='y', linestyle=':')\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    colorIndex = (2*(i - 1)) % 10\n    color1 = sns.color_palette('deep')[colorIndex]\n    color2 = sns.color_palette('deep')[colorIndex + 1]\n    sns.kdeplot(data=df, x=var, hue='output', ax=ax, fill=True,\n                palette=[color1, color2])\n    ax.set_xlabel('')\n    ax.set_ylabel('')","d38bfafc":"for var in continous:\n    gp = df[[var, 'output']].groupby(['output'])\n    gp_array = [group[var].to_numpy() for name, group in gp]\n    kstat, p = ss.kruskal(*gp_array)\n    kstat, p = round(kstat, 6), round(p, 6)\n    print(f'For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}')","843319d9":"sns.set_style('white')\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\n\ncorr_matrix = df[continous].corr()\n\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, cmap='Blues', annot=True, mask=mask, ax=ax)\nfig.text(0.5, 1.05,\n         'Correlation of Continous variables (Pearson)',\n         horizontalalignment='center', verticalalignment='center',\n         fontsize=14, fontweight='bold', transform=ax.transAxes)\nsns.set_style('dark')","9534f535":"sns.set_style('white')\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\n\ncorr_matrix = df[continous].corr(method=lambda x, y:\n                                 cramers_corrected_stat(x, y)[0])\n\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, cmap='Blues', annot=True, mask=mask, ax=ax)\nfig.text(0.5, 1.05,\n         'Correlation of Categorical variables (Corrected Cramers\\' V)',\n         horizontalalignment='center', verticalalignment='center',\n         fontsize=14, fontweight='bold', transform=ax.transAxes)\nsns.set_style('dark')","78064ce7":"sns.pairplot(df, hue='output');","08a52f26":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nprint('Libraries are Loaded')","9a718181":"X = df[['sex', 'restecg', 'cp', 'exng', 'thall', 'caa', 'slp', 'age',\n        'trtbps', 'chol', 'thalachh', 'oldpeak']]\ny = df['output']\n\nscaler = StandardScaler()\nX[continous] = scaler.fit_transform(X[continous])\n\nencode_columns = categorical.copy()\nencode_columns.remove('fbs')\n\nX = pd.get_dummies(X, columns=encode_columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=65)\n\nprint('Done Pre-processing')\nprint('Final No. of features: ', X.shape[1])","6fc98dda":"models = {\n          'SVM': SVC(),\n          'Decision Tree': DecisionTreeClassifier(),\n          'Random Forest': RandomForestClassifier(),\n          'Logistic Regression': LogisticRegression(),\n          'K-Nearest Neighbors': KNeighborsClassifier(),\n          'Gradient Boosting': GradientBoostingClassifier(),\n          'AdaBoost Classifier': AdaBoostClassifier(learning_rate=0.15, n_estimators=25),\n         }\n\naccuracy_dict, precision_dict, recall_dict, f1_dict = dict(), dict(), dict(), dict()\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_hat = model.predict(X_test)\n    print('---------------------------------------------------\\n',\n          name,\n          '\\n---------------------------------------------------')\n\n    acc = accuracy_score(y_test, y_hat)\n    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_hat, average='binary')\n    acc, precision, recall, f1 = round(acc, 5), round(precision, 5), round(recall, 5), round(f1, 5)\n    \n    accuracy_dict[name] = acc\n    precision_dict[name] = precision\n    recall_dict[name] = recall\n    f1_dict[name] = f1\n\n    print(f'Accuracy: {acc}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')\n\n    cm = confusion_matrix(y_test, y_hat)\n    df_cm = pd.DataFrame(cm)\n    sns.heatmap(df_cm, annot=True, cmap='Blues', linewidths=2)\n    plt.title(f'Confusion Matrix for {name}', fontsize=15)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()","aeb3654c":"level0 = [(name, model) for name, model in models.items()]\nlevel1 = LogisticRegression()\nstacked = StackingClassifier(estimators=level0, final_estimator=level1, n_jobs=-1)\nstacked.fit(X_train, y_train)\ny_hat = stacked.predict(X_test)\n\nname = 'Stacked Classifier'\nprint('---------------------------------------------------\\n',\n      name,\n      '\\n---------------------------------------------------')\n\nacc = accuracy_score(y_test, y_hat)\nprecision, recall, f1, support = precision_recall_fscore_support(y_test, y_hat, average='binary')\nacc, precision, recall, f1 = round(acc, 5), round(precision, 5), round(recall, 5), round(f1, 5)\n\naccuracy_dict[name] = acc\nprecision_dict[name] = precision\nrecall_dict[name] = recall\nf1_dict[name] = f1\n\nprint(f'Accuracy: {acc}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')\n\ncm = confusion_matrix(y_test, y_hat)\ndf_cm = pd.DataFrame(cm)\nsns.heatmap(df_cm, annot=True, cmap='Blues', linewidths=2)\nplt.title(f'Confusion Matrix for {name}', fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","1244afa1":"scores_dicts = {\n                'Accuracy': accuracy_dict,\n                'Precision': precision_dict,\n                'Recall': recall_dict,\n                'F1 Score': f1_dict,\n              }\n\nfor name, scores_dict in scores_dicts.items():\n    index, values = zip(*scores_dict.items())\n    acc_df = pd.DataFrame(data=values, index=index, columns=[name])\n    plt.figure(figsize=(9, 10))\n    sns.barplot(y=acc_df.index, x=acc_df[name])\n    plt.title(f'Plot of {name} Score')","6204e7e9":"## Loading Libraries <span id='ll'\/>","7dd8e2dd":"#### Conclusion\n\nSuprisingly, all variables have correlation. Although `chol` and `trtpbs` cut very close to our alpha (which is 0.05). ","d0ba0815":"# Preparing Workspace <span id='pw'\/>","28cf3b8e":"`age` - Age of the patient\n\n`sex` - Sex of the patient\n\n`cp` - Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\n`trtbps` - Resting blood pressure (in mm Hg)\n\n`chol` - Cholestoral in mg\/dl fetched via BMI sensor\n\n`fbs` - (fasting blood sugar > 120 mg\/dl) ~ 1 = True, 0 = False\n\n`restecg` - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\n`thalachh` - Maximum heart rate achieved\n\n`oldpeak` - Previous peak\n\n`slp` - Slope\n\n`caa` - Number of major vessels\n\n`thall` - Thalium Stress Test result ~ (0,3)\n\n`exng` - Exercise induced angina ~ 1 = Yes, 0 = No\n\n`output` - Target variable","8f61dd70":"### Pair plot - One plot to rule them all <span id='pp'\/>","602e0e35":"#### Conclusion\n\nVariables do *not have strong correlation* and are *weakly correlated*","53393533":"#### Conclusion\n\n* `Restecg`, `Thall`, `Caa`, `Slp` are *likely to impact few models sensitive to data distribution* because value counts for some values is extremely low. \n\n* `Fbs` and `Cp` could *possibly affect models sensitive to data distribution* because their value counts is also not ideal. ","44af28e0":"## Shape of the dataset <span id='sotd'\/>","9eafc11c":"## General Stats <span id='gs'\/>","714d6ed8":"## Importing Libraries <span id='il'\/>","d405f856":"### Count plot for categorical features <span id='cpfcf'\/>","b556c379":"## Training Models and Getting Results <span id='tmagr'\/>","e0b3e518":"### Plots for continous variables w.r.t output <span id='pfcvwo'>","72b06b83":"## Univariate Analysis <span id='ua'\/>","21c6ee0d":"## Conclusion <span id='edac'\/>\n\nHere's the conclusion from the entire EDA:\n\n**Feature Insights**\n\n* `chol`, `trtbps`, and `oldpeak` have *decent amount of outliers.* This could affect certain models sensitive to them.\n\n* `oldpeak` and `chol` (moderately) are *not uniformly distributed*. This could affect models or analysis with uniform distribution as requirement. \n\n* `Restecg`, `Thall`, `Caa`, `Slp` are *likely to impact few models sensitive to data distribution* because value counts for some values is extremely low. \n\n* `Fbs` and `Cp` could *possibly affect models sensitive to data distribution* because their value counts is also not ideal.\n\n\n**Relation to target variable**\n\n* All categorical variables *except* `Fbs` are related to output, albeit to varying degrees. Especially, `Restecg` and `sex` have very weak relation.\n\n* All continous variables are related to output \n\n**Multi-collinearity**\n* Variables do *not have strong correlation* and are *weakly correlated*","1d3f538e":"## Loading Data <span id='ld'\/>","9dd2789f":"# Model Development <span id='md'\/>","b1288e86":"# Conclusion <span id='fcc'\/>\n\n* We started with tabular data and performed EDA and got crucial information\n* We did some feature engineering by performing Standardization and One-Hot Encoding\n* We created several models and got their results. The best ones were Ensemble Methods (Gradient Boost, AdaBoost) and Logistic Regression","462c3359":"### Corrected Cramer's V for categorical variables <span id='ccvfcv'\/>","9b37faa5":"# Heart Attack EDA and Prediction\n","1fcf4570":"## Setting Params <span id='sp'\/>","107ad0bf":"# Dataset Info <span id='di'\/>","23f7f8db":"## Categorical and Continous <span id='cac'\/>","d64c1503":"# Exploratory Data Analysis <span id='eda'\/>","37d15cd7":"## Feature Engineering <span id='fe'\/>","db1c50fe":"## Preview of the dataset <span id='pwotd'\/>","2e550330":"This notebook aims at exploring the dataset and making predictions on whether a person will have heart attack or not.","84a1c76e":"## Bivariate Analysis <span id='ba'\/>","cb5bcc49":"# Preliminary Analysis <span id='pa'\/>","3c1f0c30":"# Thank you! <span id='ty' \/>\n\nThanks for reading up to this far. \n\nIf you liked the notebook, please consider upvoting. \n\nAlso, thanks to my bro [Naman Manchanda](https:\/\/www.kaggle.com\/namanmanchanda) for his tips on visualizations. ","48a4b855":"### Kruskal-Wallis H-test <span id='kwht'\/>\n\nSince the distribution for some variables is non-Gaussian we would be using non-parametric test--specifically Kruskal-Wallis H Test ","169574d0":"# Table of Contents\n\n\n* <a href='#di'> Dataset Info <\/a>\n* <a href='#pw'> Preparing Workspace <\/a>\n\t- <a href='#il'> Importing Libraries <\/a>\n\t- <a href='#sp'> Setting Params <\/a>\n\t- <a href='#ld'> Loading Data <\/a>\n* <a href='#pa'> Preliminary Analysis <\/a>\n\t- <a href='#sotd'> Shape of the dataset <\/a>\n\t- <a href='#pwotd'> Preview of the dataset <\/a>\n\t- <a href='#mv'> Missing Values <\/a>\n\t- <a href='#cac'> Categorical and Continous <\/a>\n\t- <a href='#gs'> General Stats <\/a>\n* <a href='#eda'> Exploratory Data Analysis <\/a>\n\t- <a href='#ua'> Univariate Analysis <\/a>\n\t\t+ <a href='#vpfcv'> Voilin plot for continous variables <\/a>\n\t\t+ <a href='#cpfcf'> Count plot for categorical features <\/a>\n\t- <a href='#ba'> Bivariate Analysis <\/a>\n\t\t+ <a href='#socvwto'> Seperation of categorical variables w.r.t to output <\/a>\n\t\t+ <a href='#ccvfcv'> Corrected Cramer's V for categorical variables <\/a>\n\t\t+ <a href='#pfcvwo'> Plots for continous variables w.r.t output <a>\n\t\t+ <a href='#kwht'> Kruskal-Wallis H-test <\/a>\n\t\t+ <a href='#cfmc'> Check for Multi-Collinearity: Correlation of variables among themselves <\/a>\n\t\t+ <a href='#pp'> Pair plot - One plot to rule them all <a>\n\t- <a href='#edac'> Conclusion <\/a>\n* <a href='#md'> Model Development <\/a>\n\t- <a href='#ll'> Loading Libraries <\/a>\n\t- <a href='#fe'> Feature Engineering <\/a>\n\t- <a href='#tmagr'> Training Models and Getting Results <\/a>\n\t- <a href='#secm'> Stacking <\/a>\n\t- <a href='#pos'> Plot of Scores <\/a>\n* <a href='#fcc'> Conclusion <\/a>\n* <a href='#ty'> Thank you! <\/a>","7bf70377":"We would get out X as the values we determined to have an impact, Standardize them, and get One-hot encoding for categorical variables.","ad20785b":"### Seperation of categorical variables w.r.t to output <span id='socvwto'\/>","a935d818":"## Plot of Scores <span id='pos'\/>","37722fd2":"### Check for Multi-Collinearity: Correlation of variables among themselves <span id='cfmc'\/>","66ca97fe":"#### Conclusion\n\n* `Trtbps` and `Chol` are not likely to have correlation\n* `Age` and `Thalachh` might have weak correlation\n* `Oldpeak` is likely to be correlated. ","1155e88f":"#### Conclusion\n\n* `chol`, `trtbps`, and `oldpeak` have *decent amount of outliers* that could affect certain models sensitive to them.\n\n* `oldpeak` and `chol` (moderately) are *not uniformly distributed*. This could affect models or analysis with uniform distribution as requirement. ","0817f63c":"### Voilin plot for continous variables <span id='vpfcv'\/>","9292a3c1":"## Missing Values <span id='mv'\/>","e193b86e":"#### Conclusion\n\nThe stats seem to agree with the conclusions we drew previously from graph along with a measure of the correlation.\n\n* `Fbs` is *not related* at all\n\n* `Sex`, `Restecg` have *very weak correlation*\n\n* `Sp`, `Thall` seem to have a *moderately strong correlation*\n\n* `Exng`, `Caa`, `Slp` have *decent correlation*","ee45561d":"## Stacking <span id='secm'\/>","368a653f":"#### Conclusion\n\n* `Fbs` *doesn't seem to be related* at all to output\n\n* `Sex`, `Restecg` seem to be *somewhat correlated* to output\n\n* Others have a *fair correlation* w.r.t to output"}}