{"cell_type":{"67d5b736":"code","78c108e7":"code","34726ce6":"code","72ce39b1":"code","539d55d1":"code","7a4cf2f1":"code","42266f9d":"code","a9df2e61":"code","aa2d7878":"code","986e8ff5":"code","d40871ca":"code","af85ba01":"code","1b962320":"code","21f95cdf":"code","293e1d33":"code","868ff739":"code","a9e600b1":"code","3625826e":"code","15b2d852":"code","f3bfecdf":"code","e158f7e7":"code","41c9ab72":"code","1cb986f7":"code","def5a11d":"code","c050e5e4":"code","87036ae1":"code","33875f79":"code","924dd0f2":"code","bc039f0c":"code","d9e35919":"code","033aa02b":"code","8eee26c2":"code","1d4527f8":"code","e8eedfa5":"code","87b31144":"code","91bedf20":"code","805c5e6b":"code","cf8f675b":"code","555ea604":"code","6fd1251f":"code","4a9b6e68":"code","e67100a9":"code","5bc4665b":"code","fc6da4bc":"code","78e93e80":"code","163d920b":"code","f5b61f40":"code","6dd26da1":"code","9ee74670":"code","6378c322":"code","04259abd":"code","cd5cbffa":"code","c2b6e7bb":"code","0fc7b5a6":"code","cbd6354a":"markdown","dcbdd05c":"markdown","1b8f5f7b":"markdown","e5e9a919":"markdown","5df5a814":"markdown","31e79c96":"markdown","75b7d696":"markdown","e17285eb":"markdown","b716949c":"markdown","86b21bf7":"markdown","2290afb8":"markdown","4a9be5f6":"markdown","b21dca79":"markdown","4b44958a":"markdown","bb7d249e":"markdown","76634b13":"markdown","8ecefae3":"markdown","95c29f6f":"markdown","1db28023":"markdown"},"source":{"67d5b736":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78c108e7":"!pip install beautifulsoup4","34726ce6":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re,math\nfrom bs4 import BeautifulSoup\nimport unicodedata\nimport warnings\nwarnings.simplefilter('ignore')\n%matplotlib inline","72ce39b1":"df = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin1',header = None)","539d55d1":"df.head()","7a4cf2f1":"df = df[[5,0]]\ndf.columns = ['tweets','sentiments']\ndf.head()","42266f9d":"df['sentiments'].value_counts()","a9df2e61":"sent_map = {0 : 'Negative',4 : 'Positive'}","aa2d7878":"df['word_counts'] = df['tweets'].apply(lambda x : len(str(x).split()))","986e8ff5":"df.head()","d40871ca":"df['charachter_counts'] = df['tweets'].apply(lambda x : len(x))","af85ba01":"df.head()","1b962320":"def get_avg_words_length(x):\n    words = x.split()\n    cnt_word = 0\n    for word in words:\n        cnt_word += len(word)\n    return cnt_word\/len(words) ","21f95cdf":"df['Avg_words_length'] = df['tweets'].apply(lambda x : get_avg_words_length(x))","293e1d33":"df.head()","868ff739":"df['stop_words_count'] = df['tweets'].apply(lambda x : len([t for t in x.split() if t in STOP_WORDS]))","a9e600b1":"df.head()","3625826e":"df['Hastag_count'] = df['tweets'].apply(lambda x : len([t for t in x.split() if t.startswith('#')]))","15b2d852":"df['Mentions_counts'] = df['tweets'].apply(lambda x : len([t for t in x.split() if t.startswith('@')]))","f3bfecdf":"df.head()","e158f7e7":"df['numeric_count'] = df['tweets'].apply(lambda x : len([t for t in x.split() if t.isdigit()]))","41c9ab72":"df.head()","1cb986f7":"df['upper_case'] = df['tweets'].apply(lambda x : len([t for t in x.split() if t.isupper() and len(x) > 3]))","def5a11d":"df.head()","c050e5e4":"df['tweets'] =  df['tweets'].apply(lambda x : x.lower())","87036ae1":"df.head()","33875f79":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \"}","924dd0f2":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x","bc039f0c":"%%time\n\ndf['tweets'] = df['tweets'].apply(lambda x : cont_to_exp(x))","d9e35919":"df['Email'] = df['tweets'].apply(lambda x : re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x))","033aa02b":"df['Email_count'] = df['Email'].apply(lambda x : len(x))","8eee26c2":"df[df['Email_count'] > 0].head()","1d4527f8":"df['tweets'] = df['tweets'].apply(lambda x : re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)','', x))","e8eedfa5":"df.head(5)","87b31144":"df['Urls'] = df['tweets'].apply(lambda x : re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x))","91bedf20":"df.head()","805c5e6b":"df['Urls_count'] = df['tweets'].apply(lambda x : len(re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)))","cf8f675b":"df.head()","555ea604":"df['Urls'] = df['tweets'].apply(lambda x : re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x))","6fd1251f":"df.head()","4a9b6e68":"df['tweets'] = df['tweets'].apply(lambda x : re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?','', x))","e67100a9":"df.head()","5bc4665b":"df['tweets'] = df['tweets'].apply(lambda x : re.sub('RT','',x))","fc6da4bc":"df.shape","78e93e80":"df.head(5)","163d920b":"df['tweets'] = df['tweets'].apply(lambda x : \"\".join(x.split()))","f5b61f40":"df.head(5)","6dd26da1":"%%time\ndf['tweets'] = df['tweets'].apply(lambda x : BeautifulSoup(x,'lxml').get_text())","9ee74670":"df.head()","6378c322":"def remove_accented_text(x):\n    x = unicodedata.normalize('NFKD',x).encode('ascii','ignore').decode('utf-8','ignore')\n    return x","04259abd":"df['tweets'] = df['tweets'].apply(lambda x : remove_accented_text(x))","cd5cbffa":"df.head()","c2b6e7bb":"df['tweets'] = df['tweets'].apply(lambda x : \" \".join([t for t in x.split() if t not in STOP_WORDS]))","0fc7b5a6":"df.head()","cbd6354a":"## Remove Urls","dcbdd05c":"## Urls ","1b8f5f7b":"## Contractions to Expansions","e5e9a919":"## Lower Case","5df5a814":"## Remove Accented Text","31e79c96":"## Remove RT","75b7d696":"## Count #Hashtag & @Mentions","e17285eb":"# Preprocessing and cleaning of dataset","b716949c":"## If numeric data is present or not","86b21bf7":"## Stops words counts","2290afb8":"## Remove HTML tags ","4a9be5f6":"## Remove multiple spaces : `Hello  World   `","b21dca79":"## Avg_words_length","4b44958a":"## Upper case words count","bb7d249e":"# Counts and Remove Emails ","76634b13":"## Word Counts","8ecefae3":"## Charachter_counts","95c29f6f":"## Remove StopWords Using Spacy","1db28023":"## Remove Emails"}}