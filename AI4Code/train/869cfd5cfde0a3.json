{"cell_type":{"f2979eee":"code","71db9092":"code","61a05f3c":"code","646cea19":"code","dfc4a683":"code","ed15d3e3":"code","1484f692":"code","1cd49b6e":"code","1e372822":"code","1700f9dc":"code","0ad20eed":"code","e6e29d62":"code","399af255":"code","4f449bcc":"code","8fba51da":"code","cc6389aa":"code","7b25867a":"code","05c9653e":"code","93ecdfce":"code","1abdae27":"code","1d71e52a":"code","1693dac1":"code","b4522cdb":"code","1c2ab2c2":"code","5fb6686a":"code","3828ca26":"code","c8100e28":"code","80a5f789":"code","e86087d2":"code","b0a260a4":"code","4ec0ceb2":"code","ed37b668":"code","292b9a22":"code","094f24f6":"code","57d71cd7":"code","a8e87412":"code","cd04440e":"code","e83081b1":"code","212c9ce8":"code","cf9e506a":"code","3e72593a":"code","c61ec876":"code","d6723b5b":"code","f01b6427":"code","86adc931":"code","41a19b73":"code","75c64ff1":"code","917cc476":"code","eb29376d":"markdown","d47a2bcb":"markdown","4e4c58b3":"markdown","5e257437":"markdown","cce2dc00":"markdown","9abd1d68":"markdown","390caafb":"markdown","78596290":"markdown","bc14db10":"markdown","9c3e70d1":"markdown","1796aa25":"markdown","1c3b968d":"markdown","c0639211":"markdown","e6aa984b":"markdown","6269ba64":"markdown","915504cb":"markdown","36b6c590":"markdown","9cf204db":"markdown"},"source":{"f2979eee":"#!pip install transformers","71db9092":"#import torch library\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n\n\n# Get Huggingface BERT Model and Tokenizer\nimport transformers\nfrom transformers import BertModel,BertForSequenceClassification\n \n#importing fast \"BERT\" tokenizer\nfrom transformers import BertTokenizerFast , BertTokenizer\n# Training\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nimport time\nimport datetime\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","61a05f3c":"train_df = pd.read_csv('..\/input\/janata-data\/train.csv')\ntest_df  = pd.read_csv('..\/input\/janata-data\/test.csv')\ngame_df  = pd.read_csv('..\/input\/janata-data\/game_overview.csv')","646cea19":"train_df.head()","dfc4a683":"train_df.info()","ed15d3e3":"test_df.head()","1484f692":"test_df.info()","1cd49b6e":"game_df.head()","1e372822":"game_df.info()","1700f9dc":"# Check the distribution of labels\ntrain_df['user_suggestion'].value_counts()","0ad20eed":"# compute no. of words in each user review\nnum = [len(review.split()) for review in train_df.user_review]\n#plot\nplt.hist(num, bins = 50)\n#sns.distplot(num)\n#plt.xlim([0, 512]);\nplt.xlabel('Word count');\nplt.title(\"# Of Word in Review\")","e6e29d62":"# Split in to Train and Validation \ndf_train, df_val = train_test_split(train_df, test_size=0.2, random_state=RANDOM_SEED,stratify = train_df.user_suggestion.values)","399af255":"# Using the cased version - Intuitively, that makes sense, since \"BAD\" may convey more stronger sentiment than \"bad\".\nPRE_TRAINED_MODEL_NAME = 'bert-base-uncased'","4f449bcc":"#load a pre-trained BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","8fba51da":"MAX_LEN = 400","cc6389aa":"class SteamReviewDataset(Dataset):\n\n    def __init__(self, reviews, labels, tokenizer, max_len):\n        self.reviews = reviews\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n  \n    def __len__(self):\n        return len(self.reviews)\n  \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        review = \" \".join(review.split())\n        label  = self.labels[item]\n        # Tokenize and format input for BERT\n        inputs = self.tokenizer.encode_plus (\n                        review,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        return_token_type_ids=False,\n                        pad_to_max_length=True,\n                        return_attention_mask=True,\n                        return_tensors='pt'\n                       )\n        input_ids  = inputs[\"input_ids\"].flatten()\n        attn_mask = inputs[\"attention_mask\"].flatten()\n        #token_type_ids = inputs[\"token_type_ids\"].flatten()\n        \n        return {\n                \"input_ids\": input_ids,\n                \"mask\": attn_mask,\n                \"labels\": torch.tensor(label, dtype=torch.long),\n                #\"token_type_ids\": token_type_ids\n                \n        }","7b25867a":"df_train.user_suggestion.dtype","05c9653e":"train_dataset = SteamReviewDataset(reviews=df_train.user_review.to_numpy(), labels=df_train.user_suggestion.to_numpy(),tokenizer=tokenizer,max_len=MAX_LEN)\nvalid_dataset = SteamReviewDataset(reviews=df_val.user_review.to_numpy(), labels=df_val.user_suggestion.to_numpy(),tokenizer=tokenizer,max_len=MAX_LEN)","93ecdfce":"train_dataset.max_len","1abdae27":"# Let's also create an iterator for our dataset using the torch DataLoader class\n\n#define a batch size\nbatch_size = 16\n\ntrain_sampler = RandomSampler(train_dataset) # Select batches randomly\n#represents a iterator over a dataset. Supports batching, customized data loading order\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size,num_workers=4)\n\n#define a sequential sampler \n#This samples data in a sequential order\nvalidation_sampler = SequentialSampler(valid_dataset)\n\n#create a iterator over the dataset\nvalidation_dataloader = DataLoader(valid_dataset, sampler=validation_sampler, batch_size=batch_size)\n","1d71e52a":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)","1693dac1":"# check GPU availability\nif torch.cuda.is_available():    \n    # select GPU    \n    device = torch.device(\"cuda\")\nprint(device)","b4522cdb":"#create the model\n\n\n#push the model to GPU, if available\nmodel = model.to(device)","1c2ab2c2":"optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5) \n\nepochs = 2\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","5fb6686a":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","3828ca26":"#define a function for training the model\ndef train():\n    print(\"\\nTraining.....\")  \n    #record the current time\n    t0 = time.time()\n    #set the model on training phase - Dropout layers are activated\n    model.train()\n\n    #initialize loss and accuracy to 0\n    total_loss, total_accuracy = 0, 0\n    #Create a empty list to save the model predictions\n    total_preds=[]\n  \n    #for every batch\n    for step,batch in enumerate(train_dataloader):\n        # Progress update after every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n    \n        #Unpack the batch(dictionary contains 4 pytorch tensors - we are taking only 3) into separate variables\n        # ['ids']: input ids , ['mask']: attention masks, ['labels]: labels \n        input_ids, mask, labels = batch['input_ids'],batch['mask'],batch['labels']\n        # Move them to GPU before processing\n        input_ids   = input_ids.to(device,dtype=torch.long)\n        mask  = mask.to(device,dtype=torch.long)\n        labels = labels.to(device,dtype=torch.long)\n        \n        # Always clear any previously calculated gradients before performing a backward pass.\n        model.zero_grad()        \n\n        # Perform a forward pass. This returns the model predictions\n        #preds = model(sent_id, mask)\n        loss, logits = model(input_ids, \n                             token_type_ids=None, \n                             attention_mask=mask, \n                             labels=labels)\n\n\n        # Accumulate the training loss over all of the batches so that we can average loss at the end\n        total_loss = total_loss + loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        optimizer.step()\n        # Update the learning rate\n        scheduler.step()\n        \n\n    #compute the training loss of a epoch\n    avg_loss = total_loss\/len(train_dataloader)\n  \n    \n    #returns the loss and predictions per epoch\n    return avg_loss #, total_preds","c8100e28":"#define a function for evaluating the model\ndef evaluate():\n    print(\"\\nEvaluating.....\")\n    #record the current time\n    t0 = time.time()\n    #set the model on evaluation phase - Dropout layers are deactivated\n    model.eval()\n\n    #initialize the loss and accuracy to 0\n    total_loss, total_eval_accuracy = 0, 0\n  \n    #Create a empty list to save the model predictions\n    total_preds = []\n\n    #for each batch  \n    for step,batch in enumerate(validation_dataloader):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n\n        #unpack the batch(dictionary contains 4 pytorch tensors - we are taking only 3) into separate variables\n        # ['ids']: input ids (sentence ids), ['mask']: attention masks, ['labels]: labels \n        input_ids, mask, labels = batch['input_ids'],batch['mask'],batch['labels']\n        # Move them to GPU before processing\n        input_ids= input_ids.to(device,dtype=torch.long)\n        mask = mask.to(device,dtype=torch.long)\n        labels = labels.to(device,dtype=torch.long)\n\n        #deactivates autograd\n        with torch.no_grad():\n      \n            # Perform a forward pass. This returns the model predictions\n           \n            loss, logits = model(input_ids, \n                             token_type_ids=None, \n                             attention_mask=mask, \n                             labels=labels)\n\n            \n            # Accumulate the validation loss over all of the batches so that we can calculate the average loss at the end\n            total_loss = total_loss + loss.item()\n\n            #The model predictions are stored on GPU. So, push it to CPU\n            \n            # Move logits and labels to CPU\n            logits = logits.detach().cpu().numpy()\n            labels = labels.to('cpu').numpy()\n\n            # Calculate the accuracy for this batch of test sentences, and\n            # accumulate it over all batches.\n            total_eval_accuracy += flat_accuracy(logits, labels)\n\n    # Report the final accuracy for this validation run.\n    avg_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    #print(\"  Accuracy: {0:.2f}\".format(avg_accuracy))\n    \n    #compute the validation loss of a epoch\n    avg_loss = total_loss \/ len(validation_dataloader) \n\n    \n    return avg_loss, avg_accuracy","80a5f789":"# compute time in hh:mm:ss\ndef format_time(elapsed):\n    # round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # format as hh:mm:ss\n    return str(datetime.timedelta(seconds = elapsed_rounded))","e86087d2":"#Assign the initial loss to infinite\nbest_valid_loss = float('inf')\n\n#create a empty list to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#epochs = 10 # Change epoch from 5 to 15\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n....... epoch {:} \/ {:} .......'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss = train()\n    \n    #evaluate model\n    valid_loss, valid_accuracy = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'weights_Steam_BERTSEQCLASS_32_AdamW2e-5_2.pt')\n    \n    #accumulate training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'\\nValidation Loss: {valid_loss:.3f}')\n    print(f'Validation Accuracy: {valid_accuracy:.2f}')\n\n\nprint(\"\")\nprint(\"Training complete!\")","b0a260a4":"# load weights of best model\nPATH='.\/weights_Steam_BERTSEQCLASS_32_AdamW2e-5_2.pt'\ndevice = torch.device('cuda')\nmodel.load_state_dict(torch.load(PATH, map_location=device))","4ec0ceb2":"def Predict(data_loader,test= False):\n  # We can use this function for both validation and actual Test(where labels are not present)\n  #record the current time\n  t0 = time.time()\n  #Create a empty list to save the model predictions\n  all_preds = []\n  true_labels = []\n  \n\n  #for each batch  \n  for step,batch in enumerate(data_loader):\n    \n    # Progress update every 40 batches.\n    if step % 40 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n      # Report progress\n      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(data_loader), elapsed))\n\n    #unpack the batch(dictionary contains 4 pytorch tensors - we are taking only 3) into separate variables\n    # ['ids']: input ids (sentence ids), ['mask']: attention masks\n    if test == False : # Validation\n      input_ids, mask , labels = batch['input_ids'],batch['mask'],batch['labels']\n    else: # Test\n      input_ids, mask  = batch['input_ids'],batch['mask']\n    # Move them to GPU before processing\n    input_ids = input_ids.to(device,dtype=torch.long)\n    mask = mask.to(device,dtype=torch.long)\n   \n\n    #deactivates autograd\n    with torch.no_grad():\n      \n      # Perform a forward pass. This returns the model predictions\n      \n      outputs = model(input_ids, \n                             token_type_ids=None, \n                             attention_mask=mask \n                             )\n      logits = outputs[0]\n     \n      # Move logits and labels to CPU\n      \n      logits = logits.detach().cpu().numpy()\n      if test == False: # Validation\n        labels = labels.to('cpu').numpy()\n        # Store predictions and true labels\n        true_labels.append(labels)\n\n      #Accumulate the model predictions of each batch\n      all_preds.append(logits)\n      \n    \n  all_preds  = np.concatenate(all_preds, axis=0)\n  \n  if test == False: # Validation\n    #print(\"Test is false\")\n    return (all_preds,true_labels)\n  else:\n    #print(\"Test is True\")\n    return all_preds","ed37b668":"valid_preds, valid_labels = Predict(validation_dataloader,test=False)","292b9a22":"len(valid_preds)","094f24f6":"# Converting the log(probabities) into a classes\n# Choosing index of a maximum value as class\ny_pred = np.argmax(valid_preds,axis=1)\n\n# actual labels\ny_true = df_val.user_suggestion.values","57d71cd7":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","a8e87412":"#Create test dataset for the data loader\nclass TestDataset(Dataset):\n    def __init__(self, reviews, tokenizer, max_len):\n        self.reviews = reviews\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n        \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        review = \" \".join(review.split())\n\n        # Tokenize and format input for BERT\n        inputs = self.tokenizer.encode_plus (\n                        review,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        return_token_type_ids=False,\n                        pad_to_max_length=True,\n                        return_attention_mask=True,\n                        return_tensors='pt'\n                       )\n        input_ids  = inputs[\"input_ids\"].flatten()\n        attn_mask = inputs[\"attention_mask\"].flatten()\n\n        return {\n                  \"input_ids\": input_ids,\n                  \"mask\": attn_mask\n                }\n    ","cd04440e":"test_dataset = TestDataset(reviews=test_df.user_review.to_numpy(), tokenizer=tokenizer,max_len=MAX_LEN)","e83081b1":"#define a sequential sampler \n#This samples data in a sequential order\ntest_sampler = SequentialSampler(test_dataset)\n\n#create a iterator over the dataset\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)","212c9ce8":"# Get Test Predictions\ntest_preds = Predict(test_dataloader,test=True)\n","cf9e506a":"test_df.head()","3e72593a":"test_preds","c61ec876":"final_preds = np.argmax(test_preds, axis = 1)\nfinal_preds","d6723b5b":"print(len(test_df))","f01b6427":"print(len(final_preds))","86adc931":"test_df.head(5)","41a19b73":"test_df['user_suggestion'] = pd.DataFrame(final_preds)","75c64ff1":"test_df.head()","917cc476":"test_df[['review_id','user_suggestion']].to_csv(\"sub_Steam_BERT_2.csv\",index=False)","eb29376d":"## Data Pre-processing","d47a2bcb":"## Problem Statement\nSteam is a video game digital distribution service with a vast community of gamers globally. A lot of gamers write reviews at the game page and have an option of choosing whether they would recommend this game to others or not. However, determining this sentiment automatically from text can help Steam to automatically tag such reviews extracted from other forums across the internet and can help them better judge the popularity of games.\n\nGiven the review text with user recommendation and other information related to each game for 64 game titles, the task is to predict whether the reviewer recommended the game titles available in the test set on the basis of review text and other information.\n\nGame overview information for both train and test are available in single file game_overview.csv inside train.zip\n\n","4e4c58b3":"#### Download the pre-trained BertForSequenceClassification Model","5e257437":"### Train the Model","cce2dc00":"## 1. Load Libraries","9abd1d68":"#### Tokenize the Dataset","390caafb":"#### Evaluation Metric\nSubmissions are evaluated on binary F1 Score between the predicted and observed user suggestion for the reviews in the test set.","78596290":"## Submission to LeaderBoard","bc14db10":"Choosing Sequence Length","9c3e70d1":"We have all building blocks required to create a PyTorch dataset","1796aa25":"### Train the model","1c3b968d":"#### Define the optimizer, the scheduler and the loss function","c0639211":"### Prepare Datasets for BERT Pre-trained Model","e6aa984b":"There is a reasonable balance of distribution of label values","6269ba64":"#### Split dataset in to Train and Validation data","915504cb":"# Sentiment Analysis - Steam Reviews","36b6c590":"## 2. Explore Dataset","9cf204db":"**Training phase**\n\n1. Unpack our data inputs and labels\n\n2. Load data onto the GPU for acceleration\n\n3. Clear out the gradients calculated in the previous pass.\n\n4. Forward pass (feed input data through the network)\n\n5. Compute Loss\n\n6. Backward pass (backpropagation)\n\n7. Update parameters with optimizer.step()\n\n8. Track variables for monitoring progress"}}