{"cell_type":{"0126da5f":"code","0c797add":"code","6acecc1f":"code","08671882":"code","89f2e721":"code","98aa58a6":"code","80b46f76":"code","23290560":"code","a941d46c":"code","f8eb6f06":"code","21962557":"code","d570a526":"code","d09ffe04":"code","7d60a497":"markdown","c526ed48":"markdown","0ac3b07f":"markdown","55bf8d88":"markdown","8b57cf1c":"markdown","ab88ecf1":"markdown","a6fe8539":"markdown","e2a9111c":"markdown","1521c733":"markdown"},"source":{"0126da5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0c797add":"data = pd.read_csv(\"..\/input\/creditcard.csv\")\n\ndata.head(10)","6acecc1f":"data.describe()","08671882":"data.columns","89f2e721":"data.isnull().sum()","98aa58a6":"\"We will find the correlation among the various features and the class\"\n\ncorr = data.corr()\ncorr","80b46f76":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.heatmap(data=corr,vmax=1)\nplt.show()\n","23290560":"data = data.drop(['Time'],axis=1)","a941d46c":"#execution of this section will require some time since there are a lot of computations.\n#execute this code if you want to have a better insight into the distribution of data\n\"\"\"data['Class'].unique()\nfig, ax = plt.subplots(5,6,sharex=False, sharey=False, figsize=(20,24))\ni=1\nfor column in data.columns:\n    if(column != 'Class'):\n        data0 = data.loc[data['Class']==0,column]\n        data1 = data.loc[data['Class']==1,column]\n        plt.subplot(5,6,i)\n        i = i + 1\n        data0.plot(kind='density',label='class 0')\n        data1.plot(kind='density',label='class 1')\n        plt.ylabel(column)\n        plt.legend()\"\"\"","f8eb6f06":"target = data['Class']\nfeatures = data.drop(['Class'], axis = 1)","21962557":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(features, target, test_size = 0.25, random_state = 43)","d570a526":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression().fit(train_X,train_Y)\npred_class = model.predict(test_X)\npred_class","d09ffe04":"from sklearn.metrics import confusion_matrix\ncon_matrix = confusion_matrix(test_Y, pred_class)\ntot = test_Y.count()\ncon_matrix\/tot","7d60a497":"As we can see that the dataset is clean and the range of values is not too high, therefore we can directly work on the data.\n","c526ed48":"> The objective of this model is to understand how the various factors determine the chances of a fraudulent transaction.\n    We also intend to make our model efficient to detect all fraudulent transactions","0ac3b07f":"Plotting the correlation matrix using heatmap for better understanding and visualisation","55bf8d88":"checking for null or missing values in the dataset","8b57cf1c":"Let us understand the data distribution of features \n","ab88ecf1":"As the probability of FP and FN are neglible (of the order of e-04),our model can give accurate results with high probability.\n\nIf willing, one can try other models and find the confusion matrix and take a model more accurate than this.\nHope, that you understand the idea and like it. ","a6fe8539":"We create a confusion matrix to check the accuracy of our model .False Negative cases which implies that the transaction is fraudulent but the model predicts it to be not, is highly undesirable for our study.","e2a9111c":"Next we need to separate the features from the target variable and then divide the data for training and testing.\nWe shall be using logistic regression for our model since it works well for binary classification and is based on the independence of features. However,  other models could be selected based on the choice of the developer keeping in view the requirements of the model. ","1521c733":"As visualised from the heatmap the features are uncorrelated or weakly correlated. Thus all features independently effect the target variable.\n\nWe know that the feature time in this dataset have no effect on the target variable since it is only a reference with respect to the time of first transaction in the dataset. It can therefore be dropped."}}