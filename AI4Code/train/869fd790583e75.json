{"cell_type":{"96465c0f":"code","a6bef93b":"code","7f27a7d2":"code","674a3191":"code","d1e69de0":"code","ec76cc13":"code","890b83e3":"code","c968802a":"code","babbfd8d":"code","a855e273":"code","41a980ab":"code","6403ab88":"code","59af3690":"code","6357c865":"code","2445fcaf":"code","2404a935":"code","f324a389":"code","4ddf8cf3":"code","4c9f9848":"code","aedb0686":"code","993a63ea":"code","780306d1":"code","3d00391e":"code","f0fb63eb":"code","74b63a7b":"code","dfe6f066":"code","664b7ba6":"code","5eab5834":"code","d7562c91":"code","86554a9f":"code","7791a048":"code","ec8668ea":"code","351f275c":"code","1e77d5e2":"code","6f4f85ef":"markdown","7b4189de":"markdown","5e2ba55a":"markdown","89281de5":"markdown","3591a02a":"markdown","10fa2f8b":"markdown","4a0a5c69":"markdown","1a9ea22f":"markdown","e43a6ff2":"markdown","87e4f0dd":"markdown","f830aed0":"markdown","14aebff1":"markdown","de7aba7c":"markdown","3992c938":"markdown","9cfb6f57":"markdown","02394449":"markdown","d53e8c5d":"markdown","48485f0b":"markdown","c5fdb44d":"markdown","7aa44b1c":"markdown","06384cb5":"markdown","4843febb":"markdown","1899bea2":"markdown","a8c11cbf":"markdown","f020ba92":"markdown","b23e9d90":"markdown","59af41fe":"markdown","c4ab5510":"markdown","eb3e2653":"markdown","282c66f0":"markdown","a917cba4":"markdown","58c04015":"markdown","e0ca200b":"markdown","fc1dacfb":"markdown","4859abbd":"markdown","d44995bf":"markdown","4893cc7f":"markdown","81cf7f02":"markdown","0b7eedde":"markdown","91979c14":"markdown","a5d692bc":"markdown","7b491761":"markdown","0cfa2a6d":"markdown","31df9cba":"markdown","be02fca9":"markdown","534255be":"markdown","7b479406":"markdown","f6a8030d":"markdown","5c485e00":"markdown","f800d794":"markdown","e4e7ec32":"markdown","a9e2961c":"markdown"},"source":{"96465c0f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6bef93b":"# Importing neccesary packages\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 2000)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-bright')\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom datetime import datetime","7f27a7d2":"# Loading the data\n\ncalendar= pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/calendar.csv\")\nstv = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv\")\nsp = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/sell_prices.csv\")","674a3191":"# Checking sales training data\n\nstv.head()","d1e69de0":"# Checking the shape\n\nstv.shape, calendar.shape, sp.shape","ec76cc13":"# Checking unique values across categorical variables\n\ncols = [\"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\nprint([stv[i].nunique() for i in cols])\nprint([stv[i].unique() for i in cols])","890b83e3":"# Checking calendar data\n\ncalendar.head()","c968802a":"# Checking the sell price data\n\nsp.head()","babbfd8d":"# Checking missing values first\n\nstv.isnull().sum().sort_values(ascending=True)","a855e273":"# Collecting all days columns (d_*) in a list for data wrangling\n\nd_cols = [col for col in stv.columns if \"d_\" in col]","41a980ab":"# Short helper function for data transformation\n\ndef helper_func(df):\n    df = df.drop([\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"], axis=1)\n    df = df.melt(id_vars=\"id\",var_name='dates',value_name=\"sales\" )\n    df[\"id\"] = df[\"id\"].str.replace('_validation', '')\n    return df","6403ab88":"# Taking 50 random samples to see the sales distribution\nstv_= stv.sample(n=50, random_state=1)\n\nstv_random = helper_func(stv_)\n\nmerged = stv_random.merge(calendar, how=\"left\", left_on=\"dates\", right_on=\"d\")\n\nmerged_ = pd.DataFrame(merged.groupby(\"date\")[\"sales\"].sum()).reset_index()\n\nfig = px.line(merged_, x=\"date\", y=\"sales\", title='Sales Distribution of random 50 Samples', color_discrete_map= dict(color=\"red\"))\nfig.show()","59af3690":"# Aggregate sales distribution\n\nstv_agg = pd.DataFrame(stv[d_cols].sum()).rename(columns = {0:\"sales\"})\n\nagg = stv_agg.merge(calendar.set_index(\"d\"), how=\"left\", left_index=True,\n             right_index=True, validate=\"1:1\")\n\n\nfig = px.line(agg, x=\"date\", y=\"sales\")\nfig.show()","6357c865":"# sales distribution across states\n\nstate_agg = stv.groupby(\"state_id\")[d_cols].sum()\\\n                        .T\\\n                        .merge(calendar.set_index(\"d\"), how=\"left\", left_index=True,\n                         right_index=True, validate=\"1:1\")\\\n                        .loc[:, [\"CA\",\"WI\",\"TX\",\"date\"]]\n\nstate_agg[\"date\"] = pd.to_datetime(state_agg[\"date\"])\n\nstate_agg = state_agg.set_index(\"date\")\\\n                        .resample(\"1m\").sum()\n\n\n# Line Plot with plotly.express\n\nfig = px.line(state_agg, title='Sales across States')\nfig.show()","2445fcaf":"# sales distribution across stores\n\nstore_agg = stv.groupby(\"store_id\")[d_cols].sum()\\\n                        .T\\\n                        .merge(calendar.set_index(\"d\"), how=\"left\", left_index=True,\n                         right_index=True, validate=\"1:1\")\\\n                        .loc[:, \"CA_1\":\"date\"]\n\nstore_agg[\"date\"] = pd.to_datetime(store_agg[\"date\"])\n\nstore_agg = store_agg.set_index(\"date\")\\\n                        .resample(\"1m\").sum()\n\n\n# Line Plot with plotly.express\n\nfig = px.line(store_agg, title='Sales across Stores')\nfig.show()","2404a935":"# sales distribution by stores across states\n\nstore_agg1 = stv.groupby(\"store_id\")[d_cols].sum()\\\n                        .T\\\n                        .merge(calendar.set_index(\"d\"), how=\"left\", left_index=True,\n                         right_index=True, validate=\"1:1\")\\\n                        .loc[:, \"CA_1\":\"WI_3\"].reset_index()\\\n                        .melt(id_vars = \"index\", var_name= \"store_id\", value_name=\"sales\")\n\n# Box Plot with plotly.express\n\nfig = px.box(store_agg1, x=\"store_id\", y=\"sales\", color=\"store_id\", points=\"all\", title=\"Sales Distribution by Stores across States\")\n\nfig.show()","f324a389":"### Average Sales across Stores\n\nstore_agg1_ = pd.DataFrame(store_agg1.groupby(\"store_id\")[\"sales\"].mean()).reset_index()\n\n# Bar Plot with plotly.express\n\nfig = px.bar(store_agg1_, y=\"sales\", x=\"store_id\", color=\"store_id\", title=\"Mean sales across Stores\", text=\"sales\")\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')","4ddf8cf3":"### Sales Distribution across Category and Departments\n\ncat_agg = stv.groupby([\"cat_id\", \"dept_id\"])[d_cols].sum()\ncat_agg = pd.DataFrame(cat_agg.sum(axis=1)).reset_index().rename({0:\"sales\"}, axis=1)\n\n# Subplots and Pie charts with plotly\n\nfig = make_subplots(rows=1, cols=2,specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(values=cat_agg['sales'], labels=cat_agg['cat_id']), 1,1)\nfig.add_trace(go.Pie(values=cat_agg['sales'], labels=cat_agg['dept_id']), 1,2)\n\n# updating traces and layout\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Percentage Sales Distribution across Category and Departments\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Category', x=0.18, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Department', x=0.82, y=0.5, font_size=20, showarrow=False)])\n\nfig.show()","4c9f9848":"# Bar Plot with plotly.express\n\nfig = px.bar(cat_agg, y='sales', x='dept_id', text='sales', title=\"Total Sales across Departments\", color=\"dept_id\")\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","aedb0686":"### Total Sales in States across Departments\n\nstate_dept = stv.groupby([\"state_id\",\"dept_id\"])[d_cols].sum()\nstate_dept = state_dept.sum(axis=1)\nstate_dept = pd.DataFrame(state_dept).reset_index().rename({0:\"sales\"}, axis=1)\n\n# Bar Plot with plotly.express\n\nfig = px.bar(state_dept, x=\"state_id\", y=\"sales\", color=\"dept_id\", text=\"sales\", title=\"Sales per Department across States\")\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","993a63ea":"# yearly sales distribution\n\nstv1 = pd.DataFrame(stv[d_cols].T.sum(axis=1))\\\n                                        .rename({0:\"sales\"}, axis=1)\\\n                                        .merge(calendar.set_index(\"d\"),how=\"left\", left_index=True,\n                                         right_index=True, validate=\"1:1\")\n\nstv1 = stv1.groupby([\"year\",\"month\",\"weekday\"])[\"sales\"].sum()\nstv1 = pd.DataFrame(stv1)\n\n# Bar Plot with plotly.express\n\nfig = px.bar(stv1.reset_index(), x=\"year\", y=\"sales\", color=\"year\", text=\"sales\", title=\"Year-wise Contribution across each Month\")\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='auto')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","780306d1":"# monthly sales distribution\n\nstv1 = pd.DataFrame(stv[d_cols].T.sum(axis=1))\\\n                                        .rename({0:\"sales\"}, axis=1)\\\n                                        .merge(calendar.set_index(\"d\"),how=\"left\", left_index=True,\n                                         right_index=True, validate=\"1:1\")\n\nstv1 = stv1.groupby([\"year\",\"month\",\"weekday\"])[\"sales\"].sum()\nstv1 = pd.DataFrame(stv1)\n\n# Bar Plot with plotly.express\n\nfig = px.bar(stv1.reset_index(), x=\"month\", y=\"sales\", color=\"weekday\", text=\"sales\", title=\"Monthy Sales bifurcated by weekdays\")\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='auto')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","3d00391e":"# weekday sales distribution\n\n_stv1 = stv1.reset_index()\n\n# Pie charts with plotly\n\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen', \"red\", \"cyan\", \"blue\"] #setting colors for each weekday\n\nfig = go.Figure(data=[go.Pie(labels=_stv1[\"weekday\"],\n                             values=_stv1[\"sales\"], pull=[0, 0.2, 0.1, 0, 0, 0, 0])])\n\nfig.update_traces(hoverinfo='label+percent+name', textinfo='value', textfont_size=15,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Weekdays Contribution\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Weekday', x=0.5, y=0.5, font_size=20, showarrow=False)])\n\nfig.show()","f0fb63eb":"# Bar Plot with plotly.express\n\nfig = px.bar(stv1.reset_index(), x=\"month\", y=\"sales\", color=\"weekday\", \n            facet_row=\"year\", facet_col=\"weekday\", text=\"sales\", title=\"Sales across each Month on Weekdays across Years\")\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='auto')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","74b63a7b":"# Bar charts with matplotlib\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,5))\nfig.suptitle(\"Days with Event_1 Summary\", fontsize=20)\n\ncalendar[\"event_name_1\"].isnull().value_counts(normalize=True)\\\n                                                .plot(kind=\"barh\", xlabel=\"Days without Events (%)\", ax=ax1, title=\"Days without Events (%)\")\n\ncalendar[\"event_type_1\"].value_counts(normalize=True)\\\n                                                .plot(kind=\"barh\", xlabel=\"Days with Events (%)\", ax=ax2, color='y', title=\"Events Type (%)\")","dfe6f066":"# considering only event days for analysis \n\ncalendar_e1 = calendar[~calendar[\"event_name_1\"].isnull()]\n\nevent1 = pd.DataFrame(calendar_e1[[\"event_type_1\", \"event_name_1\"]].value_counts())\\\n                                                                   .reset_index()\\\n                                                                   .rename({0:\"events\"}, axis=1)\n\n# subplots and pie charts with plotly\n\nfig = make_subplots(rows=1, cols=2,specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(values=event1['events'], labels=event1['event_type_1']), 1,1)\n\nfig1 = px.sunburst(event1, path=['event_type_1', 'event_name_1'], values='events',\n                  color='events',hover_data=['events'], color_continuous_scale='RdBu', title = \"Events distribution per Event Types\")\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\nfig.update_layout(\n    title_text=\"Events Distribution by Event Type\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Event Type', x=0.18, y=0.5, font_size=20, showarrow=False)])\n\nfig.show()\nprint(event1)\nfig1.show()","664b7ba6":"# Bar charts with matplotlib\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24,6))\nfig.suptitle(\"Event_2 Summary\", fontsize=20)\n\ncalendar[\"event_name_2\"].isnull().value_counts()\\\n                                                .plot(kind=\"barh\", xlabel=\"Days without Events (%)\", ax=ax1, title=\"Count of Days without Events\")\n\ncalendar[\"event_type_2\"].value_counts(normalize=True)\\\n                                                .plot(kind=\"barh\", xlabel=\"Days with Events (%)\", ax=ax2, color='y', title=\"Events Type (%)\")\n\ncalendar[\"event_name_2\"].value_counts(normalize=True)\\\n                                                .plot(kind=\"barh\", xlabel=\"Event Name\", ax=ax3, color='g', title=\"Events Names (%)\")","5eab5834":"#snap days purchases\n\nsnap = ['snap_CA',\"snap_TX\",\"snap_WI\"]\n\npd.DataFrame([calendar[s].value_counts(normalize=True) for s in snap])\\\n                                    .plot(kind=\"barh\", figsize=(10,5), title=\"% of Days with SNAP purchases\")","d7562c91":"# Extracting categorty, department and state IDs from dataset\n\nsp[\"cat_id\"] = sp[\"item_id\"].apply(lambda x : str.rstrip(x[:-6]))\nsp[\"dept_id\"] = sp[\"item_id\"].apply(lambda x : x[-5])\nsp[\"state_id\"] = sp[\"store_id\"].apply(lambda x : x[:-2])","86554a9f":"# Plotting sell price density plots\n\nplt.figure(figsize=(25,6))\n\np1 = sns.kdeplot(sp[sp['cat_id']=='HOBBIES']['sell_price'], shade=True, color=\"b\")\n\np2 = sns.kdeplot(sp[sp['cat_id']=='FOODS']['sell_price'], shade=True, color=\"r\")\n\np3 = sns.kdeplot(sp[sp['cat_id']=='HOUSEHOLD']['sell_price'], shade=True, color=\"g\")\n\nplt.legend(labels=['HOBBIES','FOODS',\"HOUSEHOLD\"])\nplt.xscale(\"log\")\nplt.xlabel(\"Log of Prices\")\nplt.ylabel(\"Density\")\nplt.title(\"Density plot of log of prices accross Categories\")","7791a048":"# Plotting Selling Price Distribution across Categories, States and Departments\n\ndf = sp.groupby([\"cat_id\", \"state_id\",\"dept_id\"])[\"sell_price\"].agg([\"mean\",\"sum\"])\ndf.reset_index(inplace=True)\n\n# plots with plotly\n\nfig = px.sunburst(df, path=['cat_id', 'state_id',\"dept_id\"], values='sum',\n                  color='mean',hover_data=['mean'], color_continuous_scale='RdBu', title = \"Selling Price Distribution across Categories, States and Departments\")\n\nfig1 = px.treemap(df, path=['cat_id', 'state_id',\"dept_id\"], values='sum',\n                  color='sum',hover_data=['sum'])\n\nprint(df)\nfig.show()\nfig1.show()","ec8668ea":"# Median Sales during event vs non-event days\n\ndf2 = stv.groupby(\"cat_id\")[d_cols].sum()\\\n                             .T\\\n                             .merge(calendar[[\"date\",\"d\",\"event_type_1\"]].set_index(\"d\"), how=\"left\", left_index=True,\n                              right_index=True, validate=\"1:1\")\n\ndf2[\"event_type_1\"] = df2.event_type_1.map({'Sporting':\"Event\", 'Cultural':\"Event\", 'National':\"Event\", 'Religious':\"Event\"})\ndf2.event_type_1.fillna(\"No Event\", inplace=True)\n\ndf2_ = df2.melt(id_vars=[\"date\",\"event_type_1\"], var_name='category',value_name=\"sales\")\ndf2_ = df2_.groupby([\"category\",\"event_type_1\"])[\"sales\"].median().reset_index()\n\nfig = px.bar(df2_, x=\"category\", y=\"sales\", color=\"event_type_1\", title=\"Median Sales on Event vs Non-Event Days\", text=\"sales\")\nfig.show()","351f275c":"# Event vs Non event Sales distribution\n\nfig1 = px.line(df2, x=\"date\", y=\"FOODS\", color=\"event_type_1\", title = \"Event vs Non-Event Sales distribution across Categories\")\n\nfig2 = px.line(df2, x=\"date\", y=\"HOBBIES\", color=\"event_type_1\")\n\nfig3 = px.line(df2, x=\"date\", y=\"HOUSEHOLD\", color=\"event_type_1\")\n\nfig1.show()\nfig2.show()\nfig3.show()","1e77d5e2":"# Category-wise Sales Contribution on Event Days\n\ndf3 = stv.groupby(\"cat_id\")[d_cols].sum()\\\n                             .T\\\n                             .merge(calendar[[\"date\",\"d\",\"event_type_1\"]].set_index(\"d\"), how=\"left\", left_index=True,\n                              right_index=True, validate=\"1:1\")\n\ndf3_ = df3[~df3[\"event_type_1\"].isnull()]\\\n                            .groupby(\"event_type_1\")[[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]].sum()\\\n                            .reset_index()\n\n# Define color sets of paintings\nfood_colors = ['rgb(56, 75, 126)', 'rgb(18, 36, 37)', 'rgb(34, 53, 101)',\n                 'rgb(36, 55, 57)']\nhobb_colors = ['rgb(33, 75, 99)', 'rgb(79, 129, 102)', 'rgb(151, 179, 100)',\n                 'rgb(175, 49, 35)']\nhouse_colors =  ['rgb(146, 123, 21)', 'rgb(177, 180, 34)', 'rgb(206, 206, 40)',\n                'rgb(175, 51, 21)']\n\n\nfig = make_subplots(1, 3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]],\n                    subplot_titles=['FOODS', 'HOBBIES', \"HOUSEHOLD\"])\n\nfig.add_trace(go.Pie(labels=df3_[\"event_type_1\"], values=df3_[\"FOODS\"], textinfo='label+percent', marker_colors=food_colors), 1, 1)\nfig.add_trace(go.Pie(labels=df3_[\"event_type_1\"], values=df3_[\"HOBBIES\"], textinfo='label+percent', marker_colors=hobb_colors), 1, 2)\nfig.add_trace(go.Pie(labels=df3_[\"event_type_1\"], values=df3_[\"HOUSEHOLD\"], textinfo='label+percent', marker_colors=house_colors), 1, 3)\n\n\nfig.update_layout(title_text='Category-wise Sales Contribution on Event Days')\nfig.show()","6f4f85ef":"## 2.2.2 Calendar : Events Insights\n\n* There are other features which deals with events and food stamps.\n* There are two types of events in the dataset, Event_1 and Event_2. We will analyze each one of them.\n* Also, there are 3 SNAP columns, the acronym SNAP stands for [Supplemental Nutrition Assistance Program](http:\/\/www.benefits.gov\/benefit\/361). \n\n***What is this program?***\n\n*The Supplemental Nutrition Assistance Program (SNAP) is the largest federal nutrition assistance program. SNAP provides benefits to eligible low-income individuals and families via an Electronic Benefits Transfer card. This card can be used like a debit card to purchase eligible food in authorized retail food stores.*\n","7b4189de":"***Key Observations:***\n\n* Sales has an increasing trend over the years, which is good for Walmart.\n* Sales is the highest in March and lowest in January and May. The months of Nov and Dec show clear dips, while the summer months May, Jun, and Jul suggest a milder secondary dip.\n* Weekends (Sunday and Saturday) are the highest sales contributer, followed by Friday and Monday, seems like they get benefit from the weekend!\n* Wednesday has lowest sales which is understood, middle of the week!","5e2ba55a":"***Key Observations:***\n\n* Most of the prices for food products lie between 1 dollars and 10 dollars. As we can see the high peak between 10^0 and 10^1. \n* Hobbies show a pretty wide range of prices.\n* Households are costlier than Food.","89281de5":"### Random sample\nLet's take a random sample and see the distribution - ","3591a02a":" <a id=\"subsection-two\"><\/a>\n # 2.2 Explanatory Variables: Calendar\n \n ## 2.2.1 Calendar : Year\/Month\/Weekdays Insights\n \nLet's so some data wrangling to produce sales distribution insights across year, month and weekdays:","10fa2f8b":"***Key Observations:***\n\n* \u201cFOODS_3\u201d is clearly driving the majority of \u201cFOODS\u201d category sales. \n* Similarly, \u201cHOUSEHOLD_1\u201d is clearly outselling \u201cHOUSEHOLD_2\u201d. \u201cHOBBIES_1\u201d is on a higher total sales level than \u201cHOBBIES_2\u201d.","4a0a5c69":"### Sales Distribution across Category and Departments","1a9ea22f":"***Key Observations***:\n\n* Cleary California's stores outperforms other two states stores at mean aggregate sales. CA_3 is the clear winner among all stores, it consistly sells more items. While, CA_4 ranks the lowest.\n\n* CA_2 experienced a high jump in June-2015 till Sep-2015 and then catching up with rest of the stores goig forward. Maybe there were store developments or change in policy (just a thought!) ","e43a6ff2":"### Let's bring everything together!","87e4f0dd":"### Selling Price Distribution across Categories, States and Departments","f830aed0":"# **Introduction**\n\nWelcome to the comprehensive Exploratory Data Analysis for the 5th Makridakis forecasting competitions (M5). This notebook will grow over the coming days to include model section and model building to forecast the Walmart sales. Before we kickstart, let's take a brief background on the competetion: \n\n***Objective***: We have to predict Walmart sales for future 28 days based on heirarchical sales in the states of California, Texas, and Wisconsin. \u201cHierarchical\u201d means that data can be aggregated on different levels: item level, department level, product category level, and state level. In addition, we are also given corresponding data on prices, promotions, and holidays which can be merged with sales data to draw insights. \n\nIn this ***kernel***, I will provide comprehensive Exploratory Data Analysis on the entire dataset. I shall be using Plotly(mostly) and Matplotlib, so be ready to hover your mouse! :)\n\n*In the second stage(which is still in progress), I will work on model section and model building on this problem and will attempt to forecast sales.*\n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It will motivate me to contribute to the Data Science Community :)<\/font>","14aebff1":"### Aggregate Sales across the Stores","de7aba7c":"### Event vs Non Event Analysis","3992c938":"***Key Observations***\n\n* This is our calendar data. It has 1969 rows and 14 columns.\n* It includes features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0)","9cfb6f57":"### Aggregate Sales\n\nAfter peeking at a random 50 samples, we will now do some aggregation to get some decent statistics.\n\nFirstly, we plot the aggregate time series over all items, stores, categories, departments and sales. This is an interactive plot and you can use the usual plotly tools (upper right corner) to zoom, pan, and scale the view.","02394449":"This section will get updated in next few days. \n\nHappy Learning !","d53e8c5d":"# Comprehensive Data Visualization - Interactive M5 EDA","48485f0b":"***Key Observations:***\n\n* HOUSEHOLD has the highest average sales, followed by HOBBIES and FOODS.\n* Department 1 in HOBBIES has the highest average Sales, while department 3 in FOODS has the lowest average sales.\n* Department 2 in HOUSEHOLD has the highest Sales across all 3 states.\n* Department 2 in HOBBIES has the lowest Sales across all 3 states.\n* Department 3 in FOODS has the lowest Sales across all 3 states.","c5fdb44d":"### Year-wise Insights","7aa44b1c":"### Month-wise Insights","06384cb5":"###  Event_2 Analysis","4843febb":"***Key Observations***:\n\n* California (CA) sells more items continuously, while Wisconsin (WI) sales were lower till 2013 and then slowly catching up to Texas (TX) and eventually surpassed it in the last months of our training data.\n\n* All 3 states experienced dip in start of each year, and then moving up during the middle of each year and then again going down towards the end of the year. This pattern is consistent across all 3 states.","1899bea2":"### Category-wise Sales Contribution on Event Days","a8c11cbf":"# Table of Contents\n\n[1. Meeting the Data](#section-one)\n    \n[2. EDA - Interactive Time Series Plots](#section-two)\n   - [2.1 Explanatory Variables: Sales Train Validation](#subsection-one)\n   - [2.2 Explanatory Variables: Calendar](#subsection-two) \n   - [2.3 Explanatory Variables: Sell Price](#subsection-three)\n    \n[3. Models : Selection and Building](#section-three) (Work in Progress)","f020ba92":"### Average Sales across Stores","b23e9d90":" <a id=\"subsection-three\"><\/a>\n # 2.3 Explanatory Variables: Sell Price\n \nLet's do some data manipulation to extract some of the features from existing features:","59af41fe":"***Key Observations***\n\nFor FOODS the lines of event vs non-event sales are pretty similar, while for HOBBIES the red event line is consistently below the non-events and for HOUSEHOLD the same is true after 2013.\n\nThe FOODS sales are pretty comparable between events and non-events, while the event sales for HOUSEHOLD and especially HOBBIES are notably below the non-event level.","c4ab5510":"***Key Observations:***\n\nLooking at the percentage of days where purchases with SNAP food stamps are allowed in Walmart stores, we find that it is the exact same for each of the 3 states: 650 days or 33%. This is noteworthy.","eb3e2653":"### Aggregate Sales across the States","282c66f0":"***Key Observations***:\n\n* The above plot compares the sales distribution for each stores across states. The stores in California seem to have the highest variance in sales, which might indicate that some places in California grow significantly faster than others, i.e. there is income disparity. \n\n* On the other hand, the Wisconsin and Texas sales seem to be quite consistent among themselves, without much variance. This indicates that development might be more uniform in these states.\n\n* The variance is lowest in Wisconsin and highest in California.\n\n* Zero sales corrosponds to Christmas when stores are closed. Also, there are a few days where the daily sales is higher than the average sales among few stores (TX_2\/3, WI_1\/2).","a917cba4":"This concludes Exploratory Data Analysis on this amazing dataset. Many thanks to all of you for reading this kernel, please do provide your feedback in the shapes of upvotes and comments. \n\nI\u2019ll be glad if you find my work useful, and I\u2019m always open to questions and constructive criticism. I thoroughly appreciate your support! ","58c04015":"<a id=\"subsection-one\"><\/a>\n# 2.1 Explanatory Variables: Sales Train Validation","e0ca200b":"### Weekdays Insights","fc1dacfb":"### Density Plot","4859abbd":"***Key Observations***\n\n* Here, the sales price of the item is presented as a weekly average across the store and item IDs together. \n* We will combine Calendar and Sell price data with training data to generate insights in the later steps.","d44995bf":"***Key Observations***\n\n* This is our training data. It has 1 column (d_* ) for each of the 1913 days starting from 2011-01-29 whcih represents the number of units sold per day, plus a general ID that is a combination of the other IDs plus a flag for validation.\n* It also includes the IDs for item, department, category, store, and state. \n* The number of rows are 30490 acorss 3 States ('CA', 'TX', 'WI'), 3 categories ('HOBBIES', 'HOUSEHOLD', 'FOODS'), 3 departments and 10 stores ('CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1','WI_2', 'WI_3')","4893cc7f":"***Key Observations:***\n\n* \u201cFoods\u201d are the most common category (~69%), followed by \u201cHousehold\u201d (22%) and then \u201cHobbies\u201d (9.32%). Total Sales of Foods_3 (32M) is close to rest of the department's sales combined.\n* \"Food_3\" contributes 50% of Foods overall sales. It is also the highest sales contributer amongst all departments.\n* \"Household_1\" and \"Hobbies_1\" are the highest sales contributer within Household and Hobbies category respectively.","81cf7f02":"***Key Observations:***\n\n* In our calendar data, about 8% of days have a special event. Of these events, about 1\/3 are Religious and National Holidays. The remaining third is again split into 2\/3 Cultural and 1\/3 Sporting events.\n\n* Second pie charts depicts the event's count by event type. Father's day occured only 4 times, while many of them have a count of 5 & 6.","0b7eedde":"***Key Observations***\n\n* California stores have the highest variance and mean sales among all the stores. The store CA_3 has maximum sales while the store CA_4 has minimum sales, clearly indicate income disparity.\n \n* Texas stores ranks second in the overall mean sales and showing similar mean Sales across its 3 stores, TX_2 being the highest and TX_1 being the lowest. Clearly, no disparity.\n \n* Wisconsin's stores are no far off than Texas's, WI_2 and W1_3 have same mean Sales, W1_1 lowest among them.","91979c14":"***Key Observations:***\n\n* In our calendar data, about 0.002% of days (5 days in total) have a special event. Of these events, about 80% are Cultural and rest 20% are Religious. Father's day has the highest percentage (40%), rest of the events have equal percentages (20%).","a5d692bc":"<a id=\"section-one\"><\/a>\n# Meeting the Data","7b491761":"<a id=\"section-two\"><\/a>\n# EDA - Interactive Time Series Plots","0cfa2a6d":"###  Event_1 Analysis","31df9cba":"![image.png](attachment:image.png)","be02fca9":"## 2.2.3 SNAP Days Analysis","534255be":"***Key Observations***:\n\n* The sales has an upward trend. We can observe some yearly seasonality, and a dip at Christmas (zoom-in to see), which is the only day of the year when the stores are closed.\n\n* Further zooming in, we can see strong weekly seasonality (sales going up on weekends, lower in the middle of the week).","7b479406":"### Total Sales across Departments","f6a8030d":"No missing values, good news !","5c485e00":"### Total Sales in States across Departments","f800d794":"### Sales Distributon by Stores across the States","e4e7ec32":"***Key Observations***\n\n* Natioal and Religious events contributes for the maximum sales across all 3 categories.\n* Sporting events has lower contribution towards the net sales.","a9e2961c":"<a id=\"section-three\"><\/a>\n# 3. Models : Selection and Building"}}