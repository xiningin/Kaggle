{"cell_type":{"97b8172a":"code","2142d00e":"code","d5e4406f":"code","c51d7700":"code","c442bb17":"code","43dd0005":"code","bdac146a":"code","4bb95173":"code","2a9a0fe0":"code","99ad2f8f":"code","3b398103":"code","36c3a359":"code","a2c98b2d":"code","3801c633":"code","82223799":"code","01d7a2d7":"code","0d74d198":"code","80961f69":"code","fc0c8381":"code","e415f230":"code","42a74c16":"code","cbe1a322":"code","429f719e":"code","3f4e36bc":"code","859b3885":"code","47aa35ae":"code","d562c18e":"code","be685b20":"code","8ad633d5":"code","65cfe28b":"code","2d247320":"code","573011d0":"code","cb88a876":"code","0388f2be":"markdown","952397f9":"markdown","238072c7":"markdown","ad2752db":"markdown","d66a93bd":"markdown","25104299":"markdown","e120133f":"markdown","617c320b":"markdown","7f0c7c2a":"markdown","ae58bb56":"markdown","6a37378c":"markdown","0d2e9150":"markdown","fa3fa7ec":"markdown","e3940c25":"markdown","5699e8ba":"markdown","baa4135a":"markdown","9dc68996":"markdown","14a2e64f":"markdown","f1d8895d":"markdown","b0055d3b":"markdown","ebe42555":"markdown","e5aa2684":"markdown","d06d46c5":"markdown","a4097069":"markdown","7a56a9b7":"markdown","c5edeb78":"markdown","cd5cd3b5":"markdown","a60cd9cf":"markdown","575d1f4c":"markdown","e51e872e":"markdown","4cd76e5c":"markdown"},"source":{"97b8172a":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# stacking\uc5d0\uc11c 5\uac1c\uc758 base model \uc0ac\uc6a9\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\n# sklearn.cross_validation -> sklearn.model_selection\nfrom sklearn.model_selection import KFold","2142d00e":"# Load in train and test datasets\n# path \ubcc0\uacbd\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","d5e4406f":"train.shape, test.shape","c51d7700":"full_data = [train, test]\n\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test['Cabin'].apply(lambda x: 0 if type(x) == float else 1)","c442bb17":"# Create new feature FamilySize as a combination of SibSp and Parch\n# SibSp : \ud615\uc81c\uc790\ub9e4, Parch : \ubd80\ubaa8\uc790\uc2dd\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n# Create a new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Remove all NULLS in the Embarked column\n# embarked : \ucd9c\ubc1c\uc9c0\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n# Fare : \uc694\uae08\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\n# Create a New feature CategoricalAge\n# \uc774\ud6c4\uc5d0 \uc9c0\uc6b8 feature\uc784\nfor dataset in full_data:\n    for dataset in full_data:\n        age_avg = dataset['Age'].mean()\n        age_std = dataset['Age'].std()\n        age_null_count = dataset['Age'].isnull().sum()\n        age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n        dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n        dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)","43dd0005":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search('  ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","bdac146a":"# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping 'Rare'\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","4bb95173":"for dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map({'female' : 0, 'male':1}).astype(int)\n    \n    # Mapping titles\n    title_mapping = {'Mr' : 1, 'Miss' : 2, 'Mrs' : 3, 'Master' : 4, 'Rare' : 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map({'S' : 0, 'C' : 1, 'Q' : 2}).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4","2a9a0fe0":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest = test.drop(drop_elements, axis = 1)","99ad2f8f":"train.head(3)","3b398103":"colormap = plt.cm.RdBu\nplt.figure(figsize = (14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size = 15)\nsns.heatmap(train.astype(float).corr(), linewidths = 0.1, vmax = 1.0,\n           square = True, cmap = colormap, linecolor = 'white', annot=True)","36c3a359":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked', u'FamilySize', u'Title']], \n                 hue='Survived', palette = 'seismic', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\ng.set(xticklabels=[])","a2c98b2d":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits=NFOLDS, random_state=SEED) # \ubcc0\uacbd : n_folds -> n_splits, dataset_length(=ntrain) \uc785\ub825 X https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html","3801c633":"# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed = 0, params = None):\n        params['random_state'] = seed\n        self.clf = clf(**params) # unpacking\uc744 \ub450\ubc88 \uc77c\uc5b4\ub0a8. hyperparameter\ub97c dictionary\n        \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n        \n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self, x, y):\n        return self.clf.fit(x, y)\n    \n    def feature_importances(self, x, y):\n        print(self.clf.fit(x, y).feature_importances_)\n        \n# Class to extend XGboost classifier","82223799":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    # kf -> kf.split(dataset)\uc73c\ub85c \ubb38\ubc95 \ubcc0\uacbd\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","01d7a2d7":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'warm_start': True,\n    # 'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features': 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    # 'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n    # 'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters\nsvc_params = {\n    'kernel' : 'linear',\n    'C': 0.025\n}","0d74d198":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","80961f69":"# Create Numpy arrays of train, test and target(Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data (column\uba85\uc744 \uc81c\uc678\ud55c \uac12\ub4e4 \ubf51\uc544\ub0c4)\nx_test = test.values # Creates an array of the test data","fc0c8381":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees \nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","e415f230":"# tree based model\uc5d0\uc11c\ub294 feature importance\ub97c \ubf51\uc744 \uc218 \uc788\uc74c\nrf_feature = rf.feature_importances(x_train, y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train, y_train)","42a74c16":"rf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591, 0.02854371,\n               0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811, 0.028157,\n               0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028, 0.008, 0.012, 0.05866667, 0.032, 0.008, 0.04666667, 0. , 0.05733333,\n                0.73866667, 0.01066667]\ngb_features = [0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395, 0.04778854,\n               0.05965792, 0.02774745, 0.07462718, 0.4593142, 0.01340093]","cbe1a322":"# \uc2dc\uac01\ud654 \ud3b8\ud558\uac8c \ud558\uae30 \uc704\ud574 dataframe \uc0dd\uc131\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame({'features': cols, \n                                  'Random Forest feature importances': rf_features,\n                                  'Extra Trees feature importances': et_features,\n                                  'AdaBoost feature importances': ada_features,\n                                  'Gradient Boost feature importances': gb_features})","429f719e":"# Scatter plot\n# data\ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True),\n    text = feature_dataframe['features'].values)\ndata = [trace]\n\n# \uc81c\ubaa9, \ucd95 \ub4f1\uc758 layout\nlayout = go.Layout(\n    autosize = True,\n    title = 'Random Forest Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","3f4e36bc":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Extra Trees feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Extra Trees Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename = 'scatter2010')","859b3885":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'AdaBoost Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data=data, layout = layout)\npy.iplot(fig, filename = 'scatter2010')","47aa35ae":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Gradient Boosting Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend =  False\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'scatter2010')","d562c18e":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis = 1) # axis = 1 : computes the mean row-wise\nfeature_dataframe.head()","be685b20":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n    x = x,\n    y = y,\n    width = 0.5,\n    marker = dict(\n        color = feature_dataframe['mean'].values,\n        colorscale = 'Portland',\n        showscale = True,\n        reversescale = False\n    ),\n    opacity = 0.6\n)]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Barplot of Mean Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'bar-direct-labels')","8ad633d5":"base_predictions_train = pd.DataFrame({\n    'RandomForest': rf_oof_train.ravel(),\n    'ExtraTrees': et_oof_train.ravel(),\n    'AdaBoost': ada_oof_train.ravel(),\n    'GradientBoost': gb_oof_train.ravel()\n})\nbase_predictions_train.head()","65cfe28b":"data = [\n    go.Heatmap(\n        z = base_predictions_train.astype(float).corr().values,\n        x = base_predictions_train.columns.values,\n        y = base_predictions_train.columns.values,\n            colorscale = 'Viridis',\n            showscale = True,\n            reversescale = True\n    )\n]\n\npy.iplot(data, filename = 'labelled-heatmap')","2d247320":"x_train = np.concatenate((et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis = 1)\nx_test = np.concatenate((et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis = 1)","573011d0":"gbm = xgb.XGBClassifier(\n    n_estimators = 2000,\n    max_depth = 4,\n    min_child_weight = 2,\n    gamma = 0.9,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    objective = 'binary:logistic', \n    nthread = -1,\n    scale_pos_weight=1\n).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","cb88a876":"# Generate Submission File\nStackingSubmission = pd.DataFrame({'PassengerId': PassengerId,\n                                  'Survived': predictions})\nStackingSubmission.to_csv('StackingSubmission.csv', index=False)","0388f2be":"### Pairplots","952397f9":"### Helpers via Python Classes","238072c7":"## Visualization","ad2752db":"## Generating our Base First-Level Models\nsklearn\uc5d0\uc11c \ud574\ub2f9 classifier class\ub97c \uc81c\uacf5\ud568\n\n1. Random Forest classifier\n2. Extra Trees classifier\n3. AdaBoost classifier\n4. Gradient Boosting classifier\n5. Support Vector Machine\n\n1~4 : tree-based model","d66a93bd":"### Feature importances generated from the different classifiers","25104299":"\ub450 feature\uac04\uc758 \ubd84\ud3ec\ub97c \ubcf4\uae30 \uc704\ud568","e120133f":"**Table of Contents**\n- Feature Enginnering\n- Visualization\n- out-of-fold\n- \ucd08\uae30 \ubaa8\ub378 \uc124\uc815 (parameter)\n    - Random Forest Classifier\n    - Extra Trees Classifier\n    - Adaboost Classifier\n    - Gradient Boosting Classifier\n    - Support Vector Machine (Classifier)\n- Interactive feature importances via Plotly scatterplots,barplot\n- Stacking (2\ubc88\uc9f8 \ud559\uc2b5)\n  - XGBoost","617c320b":"### Base models\n\n**Tree-based model**\n- bagging\uacfc boosting \ub300\ubd80\ubd84 tree-based model\n- train\ud558\uae30 \uc27d\uace0, \ube60\ub984\n- **Random Forest**\n    - bagging\uc758 \ub300\ud45c\uc801\uc778 \uc54c\uace0\ub9ac\uc998\n        - bagging : \uac19\uc740 classifier \uc0ac\uc6a9, \ub370\uc774\ud130 \uc0d8\ud50c\ub9c1\uc744 \uc11c\ub85c \ub2e4\ub974\uac8c \uac00\uc838\uac00\uba74\uc11c train\ud574 voting\uc744 \uc218\ud589\n    - Decision Tree \uc5ec\ub7ec\uac1c\uc758 \uacb0\uacfc\ub97c \ud3c9\uade0\n    - \uc5ec\ub7ec\uac1c\uc758 DT\ub97c \uc804\uccb4 \ub370\uc774\ud130\uc5d0\uc11c bagging \ubc29\uc2dd\uc73c\ub85c \uac01\uc790\uc758 \ub370\uc774\ud130\ub97c \uc0d8\ud50c\ub9c1\ud574 \uac1c\ubcc4\uc801\uc73c\ub85c \ud559\uc2b5\uc744 \uc218\ud589\ud55c \ub4a4 \ucd5c\uc885\uc801\uc73c\ub85c \ubaa8\ub4e0 classifier\uac00 voting\uc744 \ud1b5\ud574 \uacb0\uacfc\n- **Extra Tree**\n    - Random Forest\uc640 \ube44\uc2b7\n    - split\uc744 \ud560 \ub54c \ubb34\uc791\uc704\ub85c feature \uc120\uc815\n    - \ube60\ub978 \uc18d\ub3c4, bias\/variance \ub0ae\uc74c\n\n\n**Boosting**\n- \uc5ec\ub7ec\uac1c\uc758 tree-based classifier\uac00 \uc21c\ucc28\uc801\uc73c\ub85c train - predict, \uc798\ubabb \uc608\uce21\ud55c \ub370\uc774\ud130\uc5d0 weight\uc744 \ubd80\uc5ec\ud558\uba74\uc11c \uc624\ub958\ub97c \uac1c\uc120\ud558\ub294 \ubc29\uc2dd\n- **AdaBoost** \n    - classifier\uac00 \uc21c\ucc28\uc801\uc73c\ub85c \uc624\ub958 \uac12\uc5d0 \ub300\ud574 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud55c \uc608\uce21 \uacb0\uc815 \uae30\uc900\uc744 \ubaa8\ub450 \uacb0\ud569\ud574 \uc608\uce21 \uc218\ud589\n- **Gradient Boosting**\n    - AdaBoost\uc640 \uc720\uc0ac, \uac00\uc911\uce58 \uc5c5\ub370\uc774\ud2b8\ub97c \uacbd\uc0ac \ud558\uac15\ubc95(gradient descent)\ub97c \uc774\uc6a9\n\n**Support Vector Machine** \n- decision boundary\ub97c \ub370\uc774\ud130\uc640 \uac00\uc7a5 \uba40\ub9ac \ub5a8\uc5b4\uc9c0\uac8c \uacb0\uc815\ud558\ub294 \ubc29\ubc95","7f0c7c2a":"## Feature Exploration, Engineering and Cleansing","ae58bb56":"Dataframe \ud615\ud0dc\uc758 data\ub97c numpy \ud615\ud0dc\ub85c \ucd94\ucd9c","6a37378c":"### Pearson Correlation Heatmap\ncorrelation(\uc0c1\uad00\ub3c4) \ubcf4\uae30 \uc704\ud574\n- +1\uacfc -1 \uc0ac\uc774\uc758 \uac12\uc744 \uac00\uc9d0\n    - +1\uc740 \uc644\ubcbd\ud55c \uc591\uc758 \uc120\ud615 \uc0c1\uad00 \uad00\uacc4, 0\uc740 \uc120\ud615 \uc0c1\uad00 \uad00\uacc4 \uc5c6\uc74c, -1\uc740 \uc644\ubcbd\ud55c \uc74c\uc758 \uc120\ud615 \uc0c1\uad00 \uad00\uacc4","0d2e9150":"tree-based model\uc740 feature importance\uae30\uc900\uc73c\ub85c tree\ub97c \ub098\ub204\ubbc0\ub85c feature_importance \ucd94\ucd9c \uac00\ub2a5","fa3fa7ec":"\ubaa8\ub378\uc5d0 \ub530\ub77c \uc11c\ub85c \ub2e4\ub978 feature importance\n- Random Forest, Extra Tree : Sex, FamilySize\n- AdaBoost, Gradient Descent : IsAlone","e3940c25":"\uac01 classifier\ub97c \uacf5\ud1b5\ub41c \ud615\uc2dd\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c class\ub85c \ub9cc\ub4e4\uc5b4 \ub193\uc74c (class \uad73\uc774 \ud544\uc694 \uc5c6\uc74c)","5699e8ba":"- 5\uac1c\uc758 \ub2e4\ub978 classifier\uc5d0 \uc911\ubcf5 \ucf54\ub4dc \uc791\uc131 \u2192 \uc880 \ub354 \ud6a8\uc728\uc801\uc73c\ub85c class\ub85c\n    - \uc5ec\uae30\uc5d0\uc11c\ub294 \ud06c\uac8c \ucc28\uc774 \uc5c6\uc74c, \ubaa8\ub378\uc774 100\uac1c \uc774\uc0c1\uc815\ub3c4\uc77c \ub54c \uc720\uc6a9\ud560 \ub4ef\n- Sklearn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \ud638\ucd9c \uac00\ub2a5\ud55c \ubaa8\ub378 5\uac1c (rf, et, ada, gb, svc)\n- sklearn classifier \ub0b4\uc5d0 \uc774\ubbf8 \uc874\uc7ac\ud558\ub294 \ud574\ub2f9 \uba54\uc18c\ub4dc\ub97c \ud638\ucd9c\ud558\ub294 \ud074\ub798\uc2a4\uc758 \uba54\uc18c\ub4dc","baa4135a":"## Second-Level Predictions from the First-level Output","9dc68996":"## Ensembling & Stacking models\n\n- Stacking \n    - base classifier\ub4e4\uc758 output(prediction)\uc744 second-level-model(meta \ubaa8\ub378)\uc758 input\uc73c\ub85c \uc0ac\uc6a9\n- \uc804\uccb4 \ub370\uc774\ud130\ub85c base classifier\ub97c train\ud558\uace0 \ud574\ub2f9 output\uc744 \uba54\ud0c0\ubaa8\ub378\uc5d0 \uc0ac\uc6a9\ud558\uba74 \uacfc\uc801\ud569(overfitting)\ub420 \uc218 \uc788\uc74c\n    - data\ub97c train, test\ub85c \ubd84\ub9ac\ud55c \ub4a4, train data\ub9cc cross validation\uc73c\ub85c \ud6c8\ub828\ud558\uace0 test data\ub85c \ud3c9\uac00","14a2e64f":"#### XGBoost \n- tree-based model \uc911 \uac00\uc7a5 \ub192\uc740 \uc131\ub2a5\n- GBM \uae30\ubc18, \ub2e8\uc810\uc778 \uc218\ud589 \uc2dc\uac04 \ubc0f regularization \ubb38\uc81c \ud574\uacb0, \ubcd1\ub82c CPU \ud658\uacbd\uc5d0\uc11c \ubcd1\ub82c \ud559\uc2b5 \ube60\ub984\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn","f1d8895d":"### Out-of-Fold Predictions\n\n- K-Fold Cross Validation \n    - train data\ub97c K\uac1c(\uc5ec\uae30\uc5d0\uc11c\ub294 5\uac1c)\ub85c \ucabc\uac1c\uc11c \uc0ac\uc6a9\n\n- Stacking \uc704\ud574 \ubaa8\ub378\ub9c8\ub2e4 \uacb0\uacfc\ubb3c \n    - \uacb0\uacfc \ub0b4\ub294 \ubc29\uc2dd : Cross Validation\uc744 \ud65c\uc6a9\ud55c OOF \ubc29\uc2dd \uc0ac\uc6a9\n        - CV : K-fold, fold = 5\n\n- get_oof\n    - train : fold\ubcc4\ub85c \uacb0\uacfc \ub0b4\uc11c concat\n    - train : \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ud3c9\uade0","b0055d3b":"### Correlation Heatmap of the Second Level Training set\n- \uac01 base model output\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc2dc\uac01\ud654\ub97c \ud1b5\ud574 \ud655\uc778\n    - Second Level \ubaa8\ub378\uc5d0\uc11c\uc758 input","ebe42555":"### Plotly Barplot of Average Feature Importances","e5aa2684":"### First-level output as new features\n- ravel() : \ub2e4\ucc28\uc6d0 \ubc30\uc5f4(array)\uc744 1\ucc28\uc6d0 \ubc30\uc5f4\ub85c \ud3c9\ud3c9\ud558\uac8c \ud3b4\uc8fc\ub294 NumPy \ud568\uc218\n- https:\/\/rfriend.tistory.com\/349","d06d46c5":"### Creating NumPy arrays out of our train and test sets","a4097069":"Ensemble \n- \uc11c\ub85c \ub2e4\ub978 feature\ub97c \uc798 \ubcf4\ub294 (\uc0c1\uad00\uad00\uacc4\uac00 \ub0ae\uc740) \ubaa8\ub378\ub07c\ub9ac \uc131\ub2a5\uc774 \uc88b\uc544\uc9c8 \uac83\uc73c\ub85c \uc608\uc0c1","7a56a9b7":"### Second level learning model via XGBoost\n- XGBoost : https:\/\/xgboost.readthedocs.io\/en\/latest\/","c5edeb78":"\uc6d0\ub798\ub294 \ube44\uad50\ub97c \uc704\ud574\uc11c\ub294 barplot\uc73c\ub85c \uc2dc\uac01\ud654\ud574\uc57c\ud568","cd5cd3b5":"### tree-based model parameters\n\n- `n_jobs` : training\uc5d0\uc11c \uc0ac\uc6a9\ub41c CPU \ucf54\uc5b4 \uc218 (-1 : \uc804\uccb4 \uc0ac\uc6a9)\n- `n_estimators` : classification tree\uc758 \uac1c\uc218 (default : 10)\n    - \uc5ec\uae30\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378\ub4e4\uc740 tree-based \ubaa8\ub378 (boosting model \ud3ec\ud568)\n- `max_depth` : tree\uc758 \ucd5c\ub300 \uae4a\uc774 (overfitting \uace0\ub824\ud574\uc11c \uc801\uc808\ud558\uac8c)\n- `verbose` : training \uacfc\uc815 \ucd9c\ub825\ud560\uc9c0 \uc5ec\ubd80 \n    - 0 : \ud14d\uc2a4\ud2b8 \uc228\uae40, 3 : \ubaa8\ub4e0 \ubc18\ubcf5\uc5d0\uc11c \ud504\ub85c\uc138\uc2a4 \ucd9c\ub825","a60cd9cf":"# Ensembling\/Stacking in Python (Korean ver.)\n\n- author : **Yewon Kang**\n- notebook : https:\/\/www.kaggle.com\/dolylupec\/ensembling-stacking-in-python-titanic-dataset\n- dataset : https:\/\/www.kaggle.com\/c\/titanic\n- original author : https:\/\/www.kaggle.com\/arthurtok \n\n**Ensemble** \uc774\ub780? \uc5ec\ub7ec\uac1c\uc758 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ud569\uccd0\uc11c \uc131\ub2a5\uc744 \ub192\uc774\ub294 \ubc29\ubc95.\n\n### \uc774 \ub178\ud2b8\ubd81\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uc559\uc0c1\ube14 \uae30\ubc95\n- Ensemble \ubaa8\ub378 (Random Forest, Extra Trees, Adaboost, Gradient Boosting)\n- Out-of-Fold\n- Stacking","575d1f4c":"out-of-fold\ub85c 5\uac1c\uc758 base classifier\ub85c \uc608\uce21 (output \uc0dd\uc131)","e51e872e":"### Interactive feature importances via Plotly scatterplots","4cd76e5c":"### Feature Engineering"}}