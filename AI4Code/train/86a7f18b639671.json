{"cell_type":{"cafe9e63":"code","4af2cdd1":"code","f5dbeb99":"code","787ec37d":"code","00da710f":"code","7360c46f":"code","99b428dc":"code","df6370cd":"code","580fe79f":"code","93bbd184":"code","a6b2b0d2":"code","9c218da4":"code","7c5c1bcc":"code","ecc01dac":"code","95b3a875":"code","452bb37b":"code","3e861123":"code","c2ab4106":"code","9ca2021f":"code","f6185141":"code","0eb1ad83":"code","e47fcdd7":"code","575b1b97":"markdown","4c628f0a":"markdown","138fdb60":"markdown","07bbc98e":"markdown","cb12835a":"markdown","b9e40ec1":"markdown","b014daa8":"markdown","d9b5a3a2":"markdown","cfc6b1f5":"markdown","6406fb29":"markdown","d2cfa8c9":"markdown","11fb9f4d":"markdown","f42626de":"markdown"},"source":{"cafe9e63":"import pandas as pd , numpy as np , matplotlib.pyplot as plt , seaborn as sns , warnings\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\ndf.head()","4af2cdd1":"df.tail()","f5dbeb99":"df.shape","787ec37d":"df.info()","00da710f":"df.Class.value_counts()","7360c46f":"df.describe()","99b428dc":"tdf = pd.DataFrame(df.isnull().sum(),columns = ['A'])\ntdf[tdf.A > 0]    # No null values Found","df6370cd":"plt.figure(figsize=(15,5))       # Expanding size\nsns.countplot(df['Class'])","580fe79f":"adf = df[df['Class'] == 0].iloc[:10000,:]\nbdf = df[df['Class'] == 1]\nadf = adf.reset_index().drop('index',axis=1)\nbdf = bdf.reset_index().drop('index',axis=1)\n\ndata = pd.concat([adf,bdf])\n\nprint(adf.shape,bdf.shape,data.shape)","93bbd184":"plt.figure(figsize=(15,5))       # Expanding size\nsns.countplot(data['Class'])     # Reduced but still unbalanced but can be handled","a6b2b0d2":"c= data.corr()    # Finding correlation\n\ni = 0\n\n# replacing diogonal corr() which is 1 to NaN for finding\n#i.e manipulating and getting the informative features.\n\nwhile True:    \n    try:\n        c.iloc[i,i] = np.nan\n        i += 1\n    except:\n        break","9c218da4":"# Getting high corr. values w.r.t output because it supports the output...\n\nfeatures = c[(c['Class'] > 0.1) | (c['Class'] < -0.1)].dropna(how = 'all')['Class']     \nfeatures_col = list(features.index)\nprint(features.shape, len(features_col))     # Exactly what i want....","7c5c1bcc":"features   # Most Informative features w.r.t correlation...:)","ecc01dac":"sns.pairplot(data[features_col])    ","95b3a875":"# Preparing  pipeline for all the models\n# Here RandomForestClassifier wins the race...\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import confusion_matrix,classification_report\n\n\nX = data[features_col]\ny = data.Class\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3)\n\n\ndef model(X,y):\n    models = [LogisticRegression(penalty='l2'),DecisionTreeClassifier()\n              ,RandomForestClassifier(),KNeighborsClassifier(),SVC(),IsolationForest()]\n    \n    for model in models:\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        print(accuracy_score(y_test,y_pred),print(confusion_matrix(y_test,y_pred)) \n             ,print(classification_report(y_test,y_pred)),type(model).__name__)\n\nmodel(X,y)","452bb37b":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.metrics import confusion_matrix,classification_report\n\n\nX = data[features_col]\ny = data.Class\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(accuracy_score(y_test,y_pred),type(model).__name__)","3e861123":"y_test.value_counts()","c2ab4106":"print(confusion_matrix(y_test,y_pred))","9ca2021f":"print(classification_report(y_test,y_pred))","f6185141":"len(y_pred)","0eb1ad83":"X.columns","e47fcdd7":"len(X.columns)","575b1b97":"# Building Final Model ( RandomForestClassifier )","4c628f0a":"## Exploratory Data Analysis ( EDA )","138fdb60":"## Exploratory Data Analysis of sample Dataset ( EDA )","07bbc98e":"## Comparing Models","cb12835a":"### Importing required Libraries & Dataset","b9e40ec1":"# Taking Sample of Dataset to reduce Execuation time & Handle Unbalanced Output","b014daa8":"# ...END... Happy Learning...:)","d9b5a3a2":"## Here RandomForestClassifier Clearly Wins the race with 100 percent Accuracy , Precision & Recall","cfc6b1f5":"## Feature Engineering & Feature Selection\n\n### 30 Features Reduced to 20 Features","6406fb29":"### Getting Deeper into Data","d2cfa8c9":"### Most Informative features for Predicting \"Credit Card Fraud Detection\" ( 20 features )","11fb9f4d":"# Credit Card Fraud Detection","f42626de":"### Dealing with Missing Values ( i.e Null Values )"}}