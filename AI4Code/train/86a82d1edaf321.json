{"cell_type":{"3c59a29c":"code","0e339a2f":"code","dccd09ab":"code","4fae0a5e":"code","66d29b54":"code","ddc0ac26":"code","749c1c8f":"code","358df25a":"code","0adb1c4c":"code","4a3e8c90":"code","e0a2f24c":"code","58230fc2":"code","cd7ea281":"code","b6bb8103":"code","bec6b613":"code","33814e54":"code","5cfbf4f6":"code","1ce4110f":"code","3e6f545a":"code","66dc3ece":"code","148b9e46":"code","d9ee5764":"code","24a4cd13":"code","1106985d":"code","f365d493":"code","fd37f698":"code","e9492a37":"code","fbc3b917":"code","9b97278e":"code","96e1a39b":"code","372c6c70":"code","e45f1750":"code","b053043b":"code","9d0ef18e":"code","3e30705f":"code","722b038b":"code","0ec86a6a":"code","c40ffbc3":"code","d6793e73":"code","8b9c9844":"code","6add1a31":"code","29b04628":"code","d484e05c":"code","afde4798":"code","64ed7446":"code","1714262a":"code","f7edbbe8":"markdown","804f236a":"markdown","2b07d663":"markdown","5dac7845":"markdown","8d7e9cb5":"markdown","8ee57014":"markdown","67e92f8a":"markdown","51645911":"markdown","1315d5dd":"markdown","8b7bae99":"markdown","2fe61c4f":"markdown","ee3c452c":"markdown","56eeb103":"markdown","80317b5a":"markdown","37398e15":"markdown","7bfeda9d":"markdown","7f5654fd":"markdown","bc77da16":"markdown","42215478":"markdown","8692379b":"markdown","9dca890c":"markdown","9879f6d9":"markdown","70b0634d":"markdown","173cbaad":"markdown","4124934f":"markdown","9dc92f93":"markdown","e05d28db":"markdown","46165b8c":"markdown","1d169bed":"markdown","30a26327":"markdown","9a90c975":"markdown","bee84c77":"markdown","96645b67":"markdown"},"source":{"3c59a29c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt","0e339a2f":"def plot_decision_boundaries(X, y, model_class, **model_params):\n    try:\n        X = np.array(X)\n        y = np.array(y).flatten()\n    except:\n        print(\"Coercing input data to NumPy arrays failed\")\n    # Reduces to the first two columns of data\n    reduced_data = np.hstack([X[:, 0:1],X[:, 1:2]])\n    # Instantiate the model object\n    model = model_class(**model_params)\n    # Fits the model with the reduced data\n    model.fit(reduced_data, y)\n    y[y=='Not Placed']=0\n    y[y=='Placed']=1\n    \n    # Step size of the mesh. Decrease to increase the quality of the VQ.\n    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \n\n    # Plot the decision boundary. For that, we will assign a color to each\n    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n    # Meshgrid creation\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Obtain labels for each point in mesh using the model.\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n\n    # Predictions to obtain the classification results\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    Z[Z=='Not Placed']=0\n    Z[Z=='Placed']=1\n    # Plotting\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1],c=y, alpha=0.8)\n    plt.xlabel(\"ssc_p\",fontsize=15)\n    plt.ylabel(\"hsc_p\",fontsize=15)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.legend(['Not Placed','Placed'])\n    plt.show()\n\n","dccd09ab":"data=pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\").drop('sl_no',axis=1)","4fae0a5e":"data.head()","66d29b54":"data.info()","ddc0ac26":"c1=data['status'].values.copy()\nc1[c1==\"Not Placed\"]=0\nc1[c1==\"Placed\"]=1\ndata.plot(kind='scatter',y='ssc_p',x='hsc_p',s='degree_p',c=c1,cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend([\"Not Placed\",\"Placed\"])\nplt.show()","749c1c8f":"data.describe()","358df25a":"data[\"status\"].value_counts()\/len(data)","0adb1c4c":"import matplotlib.pyplot as plt\nplt.imshow(plt.imread(\"..\/input\/confusion\/Confusion.png\"))\nplt.show()","4a3e8c90":"from sklearn.metrics import make_scorer,confusion_matrix","e0a2f24c":"def custom(x,y,beta=2):\n    r=confusion_matrix(x,y)[0,0]\/np.sum(confusion_matrix(x,y)[0,:])\n    p=confusion_matrix(x,y)[0,0]\/np.sum(confusion_matrix(x,y)[:,0])\n    fbeta =((1+beta**2)*(r*p))\/((beta**2)*p+r)\n    return r\n    \nTNR = make_scorer(custom,greater_is_better=True)","58230fc2":"from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold","cd7ea281":"strat=StratifiedShuffleSplit(n_splits=1,test_size=0.15,random_state=42)","b6bb8103":"for train_index, test_index in strat.split(data,data['status']):\n    strat_train=data.loc[train_index]\n    strat_test=data.loc[test_index]","bec6b613":"print(\"Tain_Placed \",strat_train[\"status\"].value_counts()[\"Placed\"]\/len(strat_train[\"status\"]))\nprint(\"Test_Placed \",strat_test[\"status\"].value_counts()[\"Placed\"]\/len(strat_test[\"status\"]))","33814e54":"train_X=strat_train.drop(['status','salary'],axis=1)\ntrain_Y=strat_train[\"status\"].values\ntest_X=strat_test.drop(['status','salary'],axis=1)\ntest_Y=strat_test[\"status\"].values","5cfbf4f6":"from sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.preprocessing import LabelBinarizer,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,FeatureUnion","1ce4110f":"numerical_data=['ssc_p','hsc_p','degree_p','etest_p','mba_p']\ncategroical_data=['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex','specialisation']\n","3e6f545a":"class DataFrame_selector(BaseEstimator,TransformerMixin):\n    def __init__(self,column_list):\n        self.column_list=column_list\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        return X[self.column_list].values\nclass CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n    def __init__(self, sparse_output=False):\n        self.sparse_output = sparse_output\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        enc = LabelBinarizer(sparse_output=self.sparse_output)\n        for i in range(len(X[0,:])):\n            if i ==0:\n                out=enc.fit_transform(X[:,i])\n            else:\n                out=np.hstack((out,enc.fit_transform(X[:,i])))\n        return out","66dc3ece":"Numerical_pipeline=Pipeline([\n    ('df_selector',DataFrame_selector(numerical_data)),\n    ('StandardScaler',StandardScaler())\n])\nNumerical_pipeline.fit_transform(train_X).shape","148b9e46":"Categorical_pipeline=Pipeline([\n    ('df_selector',DataFrame_selector(categroical_data)),\n    ('binary',CustomLabelBinarizer(sparse_output=False))\n])\nCategorical_pipeline.fit_transform(train_X).shape","d9ee5764":"final=FeatureUnion(transformer_list=[\n    ('numerical',Numerical_pipeline),\n    ('categorical',Categorical_pipeline)\n])\nfinal.fit_transform(train_X).shape","24a4cd13":"proccessed_train=final.fit_transform(train_X)","1106985d":"proccessed_test=final.fit_transform(test_X)","f365d493":"FINAL_X=final.fit_transform(data.drop(['status','salary'],axis=1))\nFINAL_Y=data[\"status\"].values","fd37f698":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n","e9492a37":"split=StratifiedKFold(n_splits=10,random_state=42)","fbc3b917":"lgr=LogisticRegression()\n\nparam_grid = [{}]\ngrid_search = GridSearchCV(lgr, param_grid, cv=split,scoring=TNR)\nlgr_model=grid_search.fit(X=proccessed_train,y=train_Y)","9b97278e":"DTC=DecisionTreeClassifier(random_state=1,class_weight=\"balanced\",splitter='random')\nparam_grid = [\n    {'max_depth':[1,2,3,4],'max_features':[\"auto\", \"sqrt\", \"log2\"],'criterion':[\"gini\", \"entropy\"]}\n  ]\ngrid = GridSearchCV(DTC, param_grid, cv=split,scoring=TNR)\nDTC_model=grid.fit(X=proccessed_train,y=train_Y)\nDTC_model.best_estimator_","96e1a39b":"import matplotlib.pyplot as plt\nfrom sklearn import tree \ncl=['ssc_p','hsc_p','degree_p','etest_p','mba_p']+['gender', 'ssc_b', 'hsc_b', 'Commerce','Science','Arts', 'Sci&Tech','Comm&Mgmt','Others','workex','specialisation']\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=100)\ntree.plot_tree(DTC_model.best_estimator_,class_names=['Placed','Not Placed'],feature_names=cl)\nfig.savefig('imagename.png')","372c6c70":"GNB=GaussianNB()\nparam_grid = [\n    {'var_smoothing':[5,4,3,2.15,2.1,2,1,1e-2,1e-6,1e-7,1e-8,1e-9,1e-10]}]\ngrid = GridSearchCV(GNB, param_grid, cv=10,scoring=TNR)\nGNB_model=grid.fit(X=proccessed_train,y=train_Y)\nGNB_model.best_estimator_","e45f1750":"plot_decision_boundaries(FINAL_X, FINAL_Y, GaussianNB,**GNB_model.best_params_)","b053043b":"RFC=RandomForestClassifier()\nparam_grid = [\n    {'max_depth':[1,2,3],'n_estimators':[6,7,8],'criterion' : [\"gini\", \"entropy\"],'max_features':[\"auto\", \"sqrt\", \"log2\"],'random_state':[1],'class_weight' : [\"balanced\", \"balanced_subsample\",None]}]\ngrid = GridSearchCV(RFC, param_grid, cv=split,scoring=TNR)\nRFC_model=grid.fit(X=proccessed_train,y=train_Y)\nRFC_model.best_estimator_","9d0ef18e":"plot_decision_boundaries(FINAL_X, FINAL_Y, RandomForestClassifier,**RFC_model.best_params_)","3e30705f":"KNC=KNeighborsClassifier()\nparam_grid = [{'n_neighbors':[2,3,4],'leaf_size':[1,10],'p':[1,2,3,4],'weights':['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}]\ngrid = GridSearchCV(KNC, param_grid, cv=split,scoring=TNR)\nKNC_model=grid.fit(X=proccessed_train,y=train_Y)\nKNC_model.best_estimator_","722b038b":"from sklearn.inspection import permutation_importance\nresults = permutation_importance(KNC_model, FINAL_X, FINAL_Y, scoring=TNR)\n# get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))","0ec86a6a":"plot_decision_boundaries(FINAL_X, FINAL_Y, KNeighborsClassifier,**KNC_model.best_params_)","c40ffbc3":"svc=SVC(probability=True)\nparam_grid = [{'C':[0.1,0.5,1,2,3,3.5,4],'kernel':['linear', 'rbf'],'degree':[1,2,3,4,5],'gamma' : ['scale', 'auto'],'class_weight' : [None,'balanced']}]\ngrid = GridSearchCV(svc, param_grid, cv=split,scoring=TNR)\nSVC_model=grid.fit(X=proccessed_train,y=train_Y)\nSVC_model.best_estimator_","d6793e73":"plot_decision_boundaries(FINAL_X, FINAL_Y, SVC,**SVC_model.best_params_)","8b9c9844":"from sklearn.ensemble import VotingClassifier","6add1a31":"voting_clf=VotingClassifier(\n    estimators=[('lr',lgr_model.best_estimator_),\n            ('GNB',GNB_model.best_estimator_),\n            ('DTC',DTC_model.best_estimator_),\n            ('RFC',RFC_model.best_estimator_),\n            ('KNN',KNC_model.best_estimator_),\n            ('SVC',SVC_model.best_estimator_)\n           ],\nvoting='soft'\n)\n\nvoting_model=voting_clf.fit(proccessed_train,train_Y)","29b04628":"print(\"LOGISTIC REGRESSION CLASSIFIER    \",lgr_model.score(proccessed_test,test_Y))\nprint(\"GAUSSIAN NAIVE BAYES CLASSIFIER   \",GNB_model.score(proccessed_test,test_Y))\nprint(\"DECISION TREE CLASSIFIER          \",DTC_model.score(proccessed_test,test_Y))\nprint(\"RANDOM FOREST CLASSIFIER          \",RFC_model.score(proccessed_test,test_Y))\nprint(\"K NEAREST NEIGHBORS CLASSIFIER    \",KNC_model.score(proccessed_test,test_Y))\nprint(\"SUPPORT VECTOR MACHINE CLASSIFIER \",SVC_model.score(proccessed_test,test_Y))\nprint(\"VOTING                            \",voting_model.score(proccessed_test,test_Y))","d484e05c":"y=[\n    lgr_model.score(proccessed_test,test_Y),\n    GNB_model.score(proccessed_test,test_Y),\n    DTC_model.score(proccessed_test,test_Y),\n    RFC_model.score(proccessed_test,test_Y),\n    KNC_model.score(proccessed_test,test_Y),\n    SVC_model.score(proccessed_test,test_Y),\n    voting_model.score(proccessed_test,test_Y)\n]\n\nx=[\n    \"LOGISTIC REGRESSION CLASSIFIER\",\"GAUSSIAN NAIVE BAYES CLASSIFIER\",\"DECISION TREE CLASSIFIER\",\n    \"RANDOM FOREST CLASSIFIER\",\n    \"K NEAREST NEIGHBORS CLASSIFIER\",\n    \"SUPPORT VECTOR MACHINE CLASSIFIER\",\n    \"VOTING CLASSIFIER (SOFT VOTING)\"\n]\n\nfig = plt.figure(figsize=[18,10])\nax = fig.add_axes([0,0,1,1])\nax.bar(x,y,width=0.5)\nplt.show()","afde4798":"print(\"LOR \",custom(FINAL_Y,lgr_model.predict(FINAL_X)))\nprint(\"GNB \",custom(FINAL_Y,GNB_model.predict(FINAL_X)))\nprint(\"DTC \",custom(FINAL_Y,DTC_model.predict(FINAL_X)))\nprint(\"RFC \",custom(FINAL_Y,RFC_model.predict(FINAL_X)))\nprint(\"KNC \",custom(FINAL_Y,KNC_model.predict(FINAL_X)))\nprint(\"SVC \",custom(FINAL_Y,SVC_model.predict(FINAL_X)))\nprint(\"VOTING \",custom(FINAL_Y,voting_model.predict(FINAL_X)))","64ed7446":"y=[ \n    custom(FINAL_Y,lgr_model.predict(FINAL_X)),\n    custom(FINAL_Y,GNB_model.predict(FINAL_X)),\n    custom(FINAL_Y,DTC_model.predict(FINAL_X)),\n    custom(FINAL_Y,RFC_model.predict(FINAL_X)),\n    custom(FINAL_Y,KNC_model.predict(FINAL_X)),\n    custom(FINAL_Y,SVC_model.predict(FINAL_X)),\n    custom(FINAL_Y,voting_model.predict(FINAL_X))\n]\nx=[\"LOGISTIC REGRESSION CLASSIFIER\",\n\"GAUSSIAN NAIVE BAYES CLASSIFIER\",\n\"DECISION TREE CLASSIFIER\",\n\"RANDOM FOREST CLASSIFIER\",\n\"K NEAREST NEIGHBORS CLASSIFIER\",\n\"SUPPORT VECTOR MACHINE CLASSIFIER\",\n  \"VOTING CLASSIFIER (SOFT VOTING)\"]\nfig = plt.figure(figsize=[18,10])\n\nax = fig.add_axes([0,0,1,1])\nax.bar(x,y,width=0.5)\nplt.show()","1714262a":"KNC_model.best_params_","f7edbbe8":"# Business metric\nOur aim is to segment the student list into \"Placed\" or \"Not Placed\" such that the \"Not Placed\" student do not get false categorised. ","804f236a":"## Pre-processing train and test data","2b07d663":"## Let's Check how well have the models genralised","5dac7845":"### Numerical Pipeline","8d7e9cb5":"## Stratified Train-Test Split\nStratified Train Test Split keeps the ratio of output classes same in train and test sets.","8ee57014":"# Data Imbalance \nThe ouptut class has imbalnce since there are only 31% Negative class and 69% positve. Since we negative class is important to us are metric should be chosen keeping \"Not Placed\" class in mind.","67e92f8a":"## Logistic Regression","51645911":"ATTRIBUTE LIST: ['sl_no', 'gender', 'ssc_p', 'ssc_b', 'hsc_p', 'hsc_b', 'hsc_s',\n       'degree_p', 'degree_t', 'workex', 'etest_p', 'specialisation', 'mba_p']","1315d5dd":"# Data Cleaning And Preparing for the model.","8b7bae99":"## Support Vector Machine Classifier","2fe61c4f":"## K Nearest Neighbors Classifier","ee3c452c":"### Train data transformed by our Pipeline","56eeb103":"# Modeling and Model Selection\n","80317b5a":"# Problem Statement\nThe college placement cell has decided to organize special training programming tied up with a placement training school. However due to funds limitation the college has decided to make it compulsory only for those who are likely to not get placed.\nOBJECTIVES: \nDetect a list of people who will not get placed.\nWhich factor influenced a candidate in getting placed?\nDoes percentage matters for one to get placed? ","37398e15":"# Data Overview","7bfeda9d":"## Scoring on the Test Set","7f5654fd":"Spliting status attribute from attribute data. Removing Salary attribute from the attribute data.","bc77da16":"# Global score","42215478":"## Preprocessing Pipeline\n### 1. Numerical Data Pipeline\n    1.1. Numerical data is extracted out of the data\n    1.2. Data is standard Scaled ((x-mean)\/std)\n### 2. Categorical Data Pipeline\n    2.1. Cateegorical data is extracted out of the data\n    2.2. Data is transformed into one hot encoding(Binarizing) \n### 3. Feature Union\n    Both the pipelines are merged such that numerical and categorical transformed data are horizontaly stacked.","8692379b":"## Decision Tree","9dca890c":"## We will train \n1. Random Forest Classifier\n2. Decision Tree Classifier\n3. Logistic Regression\n4. K Nearest Neighbors Classifier\n5. Gaussian Naive Bayes\n6. Support Vector Machine Classifier\n#### We will tune the Hyperparameters against 10 fold crossvalidation and choose the Hyperparameters that yeild the best validation score for each model. After getting the best models for each classifier we predict against the test set to get the final performance of the model. The best performing model on TNR metric will be choosen. We use gridsearch for hyperparameter tuning ","9879f6d9":"### Categorical Pipeline","70b0634d":"# K Nearest Neighbors Classifier is best performing model and we select it with hyper parameters:","173cbaad":"* **DataFrame_selector**: this class splits numerical and categorical attributes\n* **CustomLabelBinarizer**: Transforms data into one hot vectors\n* **StandardScaler**: Standardises the numerical data ((x-mean)\/std)","4124934f":"## Random Forest Classifier","9dc92f93":"# Data science metric \nTrue Negative Rate is the metric used to evaluate model performance.\nIt is i given by:\nTNR=True Negative\/(True Negative+False Positive)\nWe choose TNR because we need to find all those student who will not get placed. For this problem statement it is okay to get a few False Negatives i.e. a \"Placed\" student can be allowed classified into \"Not Placed\" but otherwise is dangerous. Also since Negative rate is less in number our model performance should prefer negative class performance. ","e05d28db":"# Accuracy of Model ","46165b8c":"Below are the ratios of \"Placed\" category in train and test sets","1d169bed":"### Test data transformed by our Pipeline","30a26327":"### Feature Union","9a90c975":"From above information we know salary has null values and we know salary depends on whether the person is placed and hence isnt needed.","bee84c77":"# Stakeholders(highlight for which you will be working)\n1. Students\n2. Placement Cell\n3. Training School\n4. College \n5. Recruiters","96645b67":"## Gaussian Naive Bayes"}}