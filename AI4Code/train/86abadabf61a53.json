{"cell_type":{"0edd27b8":"code","faef32eb":"code","46bb66a5":"code","2b961540":"code","d1606836":"code","2e69419c":"code","7b4f4ccf":"code","c0b090a5":"code","564cee97":"code","2eb7e4d4":"code","83efee2a":"code","d4f06f50":"code","0e6a263f":"code","7d7bb2c7":"code","dc9bd204":"code","434920c9":"code","8f78926e":"code","a85eb116":"code","4365bd11":"code","bc3d6daa":"code","85c2d776":"code","558d7760":"code","d861c7b8":"code","d1a2cbd2":"code","37855480":"code","3ae27f6c":"code","b6dda2d4":"code","88b79587":"code","67d8db07":"code","bc8fc4d2":"code","4bed01c8":"code","14742b20":"code","82488a8f":"code","0158f5d1":"code","b9f5fbf0":"code","de8e72d5":"code","0f031546":"code","a2cbd99f":"code","1e7bb544":"code","fe00590e":"code","fa9db446":"code","e918bb39":"code","b1fee3fd":"code","57dc5e27":"code","a4a6e5ea":"code","7e73b592":"markdown","d14dbd57":"markdown","c8739903":"markdown","c5223fd8":"markdown","b94d7766":"markdown","d0f2b697":"markdown","b3b0325a":"markdown","56d96d51":"markdown","a5acea22":"markdown","6b444586":"markdown","544227ef":"markdown","558bb06c":"markdown","ca2cc85a":"markdown","4b5b3a4d":"markdown"},"source":{"0edd27b8":"from fastai.core import *\nimport transformers; transformers.__version__","faef32eb":"KAGGLE_WORKING = Path(\"\/kaggle\/working\")","46bb66a5":"path = Path(\"..\/input\/tweet-sentiment-extraction\/\")\ntrain_df = pd.read_csv(path\/'train.csv')\ntest_df = pd.read_csv(path\/'test.csv')\ntrain_df = train_df.dropna().reset_index(drop=True)","2b961540":"# an example for SQUAD json data format\nsquad_sample = {\n    \"version\": \"v2.0\",\n    \"data\": [\n        {\n            \"title\": \"Beyonc\\u00e9\",\n            \"paragraphs\": [\n                {\n                    \"qas\": [\n                        {\n                            \"question\": \"When did Beyonce start becoming popular?\",\n                            \"id\": \"56be85543aeaaa14008c9063\",\n                            \"answers\": [\n                                {\n                                    \"text\": \"in the late 1990s\",\n                                    \"answer_start\": 269\n                                }\n                            ],\n                            \"is_impossible\": False\n                        }\n                    ],\n                    \"context\": \"Beyonc\\u00e9 Giselle Knowles-Carter (\/bi\\u02d0\\u02c8j\\u0252nse\\u026a\/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \\\"Crazy in Love\\\" and \\\"Baby Boy\\\".\"\n                }\n            ]\n        }\n    ]\n}","d1606836":"def get_answer_start(context, answer):\n    len_a = len(answer)\n    for i, _ in enumerate(context):\n        if context[i:i+len_a] == answer: return i\n    raise Exception(\"No overlapping segment found\")","2e69419c":"def generate_qas_dict(text_id, context, answer, question):\n    qas_dict = {}\n    qas_dict['question'] = question\n    qas_dict['id'] = text_id\n    qas_dict['is_impossible'] = False\n    \n    if answer is None: \n        qas_dict['answers'] = []\n    else: \n        answer_start = get_answer_start(context, answer)\n        qas_dict['answers'] = [{\"text\":answer, \"answer_start\":answer_start}]\n    return qas_dict","7b4f4ccf":"def create_squad_from_df(df):\n    data_dicts = []\n    for _, row in df.iterrows():\n        text_id = row['textID']\n        context = row['text']\n        answer =  row['selected_text'] if 'selected_text' in row else None\n        question = row['sentiment']\n\n        qas_dict = generate_qas_dict(text_id, context, answer, question)\n        data_dict = {\"paragraphs\" : [{\"qas\" : [qas_dict], \"context\":context}]}\n        data_dict['title'] = text_id\n        data_dicts.append(data_dict)\n\n    return {\"version\": \"v2.0\", \"data\": data_dicts}","c0b090a5":"# train_no_neutral_df = train_df[train_df.sentiment != 'neutral'].reset_index(drop=True)\n# test_no_neutral_df = test_df[test_df.sentiment != 'neutral'].reset_index(drop=True)\n# train_no_neutral_df.shape, test_no_neutral_df.shape","564cee97":"from sklearn.model_selection import KFold","2eb7e4d4":"# create 5 fold trn-val splits with only positive\/negative tweets\nos.makedirs(\"squad_data\", exist_ok=True)\nkfold = KFold(5, shuffle=True, random_state=42)\nfold_idxs = list(kfold.split(train_df))\n\nfor i, (trn_idx, val_idx) in enumerate(fold_idxs[:1]):\n    _trn_fold_df = train_df.iloc[trn_idx]\n    _val_fold_df = train_df.iloc[val_idx]\n    train_squad_data = create_squad_from_df(_trn_fold_df)\n    valid_squad_data = create_squad_from_df(_val_fold_df)\n    with open(f\"squad_data\/train_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(train_squad_data))\n    with open(f\"squad_data\/valid_squad_data_{i}.json\", \"w\") as f: f.write(json.dumps(valid_squad_data))","83efee2a":"# # create for test \ntest_squad_data =  create_squad_from_df(test_df)\nwith open(\"squad_data\/test_squad_data.json\", \"w\") as f: f.write(json.dumps(test_squad_data))","d4f06f50":"from fastai.text import *\nfrom transformers import (AutoTokenizer, AutoConfig, AutoModel, AutoModelForQuestionAnswering,\n                         RobertaTokenizer)\nfrom transformers.data.processors.squad import (SquadResult, SquadV1Processor, SquadV2Processor,\n                                                SquadExample, squad_convert_examples_to_features)","0e6a263f":"PRETRAINED_TYPE = 'roberta-base'\n# PRETRAINED_TYPE = 'distilbert-base-uncased'","7d7bb2c7":"PRETRAINED_TOK_PATH = Path(\"\/kaggle\/input\/tse-fastai-squad-bert-pretrained\/roberta-base-tokenizer\/\")","dc9bd204":"shutil.copytree(PRETRAINED_TOK_PATH, KAGGLE_WORKING\/\"tokenizer_dir\")","434920c9":"PRETRAINED_TOK_PATH = KAGGLE_WORKING\/\"tokenizer_dir\"","8f78926e":"shutil.copyfile(str(PRETRAINED_TOK_PATH\/\"tokenizer_config.json\"), str(PRETRAINED_TOK_PATH\/\"config.json\"))","a85eb116":"PRETRAINED_TOK_PATH.ls()","4365bd11":"processor = SquadV2Processor()\ntokenizer = RobertaTokenizer.from_pretrained(str(PRETRAINED_TOK_PATH))","bc3d6daa":"max_seq_length = 192\nmax_query_length = 10\n\ndef get_dataset(examples, is_training):\n    return squad_convert_examples_to_features(\n        examples=examples,\n        tokenizer=tokenizer,\n        doc_stride=200,\n        max_seq_length=max_seq_length,\n        max_query_length=10,\n        is_training=is_training,\n        return_dataset=\"pt\",\n        threads=defaults.cpus,\n    )","85c2d776":"class SQUAD_Dataset(Dataset):\n    def __init__(self, dataset_tensors, examples, features, is_training=True):\n        self.dataset_tensors = dataset_tensors\n        self.examples = examples\n        self.features = features\n        self.is_training = is_training\n        \n        \n    def __getitem__(self, idx):\n        'fastai requires (xb, yb) to return'\n        'AutoModel handles loss computation in forward hence yb will be None'\n        input_ids = self.dataset_tensors[0][idx]\n        attention_mask = self.dataset_tensors[1][idx]\n        token_type_ids = self.dataset_tensors[2][idx]\n        xb = (input_ids, attention_mask, token_type_ids)\n        if self.is_training: \n            start_positions = self.dataset_tensors[3][idx]\n            end_positions = self.dataset_tensors[4][idx]\n            yb = [start_positions, end_positions]\n        else:\n            yb = 0\n        return xb, yb\n    \n    def __len__(self): return len(self.dataset_tensors[0])","558d7760":"PRETRAINED_PATH = Path(\"\/kaggle\/input\/tse-fastai-squad-bert-pretrained\/\")","d861c7b8":"def get_fold_ds(foldnum):\n    data_dir = \"\/kaggle\/working\/squad_data\"\n    train_filename = f\"train_squad_data_{foldnum}.json\"\n    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n    test_filename = \"test_squad_data.json\"\n    \n    # tokenize\n    train_examples = processor.get_train_examples(data_dir, train_filename)\n    valid_examples = processor.get_train_examples(data_dir, valid_filename)\n    test_examples = processor.get_dev_examples(data_dir, test_filename)\n\n    # create tensor dataset\n    train_features, train_dataset = get_dataset(train_examples, True)\n    valid_features, valid_dataset = get_dataset(valid_examples, True)\n    test_features, test_dataset = get_dataset(test_examples, False)\n    \n    # create pytorch dataset\n    train_ds = SQUAD_Dataset(train_dataset.tensors, train_examples, train_features)\n    valid_ds = SQUAD_Dataset(valid_dataset.tensors, valid_examples, valid_features)\n    test_ds = SQUAD_Dataset(test_dataset.tensors, test_examples, test_features, False)\n    \n    return train_ds, valid_ds, test_ds    ","d1a2cbd2":"from transformers import AutoModelForPreTraining, RobertaModel, BertModel","37855480":"# MODEL_TYPE = 'distilbert'\nMODEL_TYPE = 'roberta'","3ae27f6c":"class QAHead(Module): \n    def __init__(self, p=0.5):    \n        self.d0 = nn.Dropout(p)\n        self.l0 = nn.Linear(768, 256)\n        self.d1 = nn.Dropout(p)\n        self.l1 = nn.Linear(256, 2)        \n    def forward(self, x):\n        return self.l1(self.d1(self.l0(self.d0(x))))\n    \nclass TSEModel(Module):\n    def __init__(self, model): \n        self.sequence_model = model\n        self.head = QAHead()\n        \n    def forward(self, *xargs):\n        inp = {}\n        inp[\"input_ids\"] = xargs[0]\n        inp[\"attention_mask\"] = xargs[1]\n        inp[\"token_type_ids\"] = xargs[2]\n        if MODEL_TYPE in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]: del inp[\"token_type_ids\"]\n    \n        sequence_output, _ = self.sequence_model(**inp)\n        start_logits, end_logits = self.head(sequence_output).split(1, dim=-1)\n        return (start_logits.squeeze(-1), end_logits.squeeze(-1))","b6dda2d4":"class CELoss(Module):\n    \"single backward by concatenating both start and logits with correct targets\"\n    def __init__(self): self.loss_fn = nn.CrossEntropyLoss()\n    def forward(self, inputs, start_targets, end_targets):\n        start_logits, end_logits = inputs\n        \n        logits = torch.cat([start_logits, end_logits]).contiguous()\n        \n        targets = torch.cat([start_targets, end_targets]).contiguous()\n        \n        return self.loss_fn(logits, targets)","88b79587":"def model_split_func(m): \n    n = len(m.sequence_model.encoder.layer) - 5\n    return (m.sequence_model.encoder.layer[n], m.head)","67d8db07":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","bc8fc4d2":"def get_best_start_end_idxs(_start_logits, _end_logits):\n    best_logit = -1000\n    best_idxs = None\n    for start_idx, start_logit in enumerate(_start_logits):\n        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n            logit_sum = (start_logit + end_logit).item()\n            if logit_sum > best_logit:\n                best_logit = logit_sum\n                best_idxs = (start_idx, start_idx+end_idx)\n    return best_idxs","4bed01c8":"def get_best_start_end_idxs_v2(_start_logits, _end_logits):\n    start_idx, end_idx = torch.argmax(_start_logits).item(), torch.argmax(_end_logits).item()\n    if start_idx > end_idx: end_idx = start_idx\n    return (start_idx, end_idx)","14742b20":"def get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map):\n    \n    start_offset_id = token_to_orig_map[start_idx] \n    end_offset_id = token_to_orig_map[end_idx]\n    \n    \n    return \"\".join([ct for ct, char_offs in zip(context_text, char_to_word_offset) if \n                                     (char_offs >= start_offset_id) & (char_offs <= end_offset_id)])","82488a8f":"class JaccardScore(Callback):\n    \"Stores predictions and targets to perform calculations on epoch end.\"\n    def __init__(self, valid_ds): \n        self.valid_ds = valid_ds\n        self.token_to_orig_map = [o.token_to_orig_map for o in valid_ds.features]\n        self.context_text = [o.context_text for o in valid_ds.examples]\n        self.answer_text = [o.answer_text for o in valid_ds.examples]\n        self.char_to_word_offset = [o.char_to_word_offset for o in valid_ds.examples]\n\n        self.offset_shift = min(self.token_to_orig_map[0].keys())\n        \n        \n    def on_epoch_begin(self, **kwargs):\n        self.jaccard_scores = []  \n        self.valid_ds_idx = 0\n        \n        \n    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n        \n#         import pdb;pdb.set_trace()\n        \n        input_ids = last_input[0]\n        attention_masks = last_input[1].bool()\n        token_type_ids = last_input[2].bool()\n\n        start_logits, end_logits = last_output\n        \n        # mask select only context part\n        for i in range(len(input_ids)):\n            \n            if MODEL_TYPE == \"roberta\": \n                \n                _input_ids = input_ids[i].masked_select(attention_masks[i])\n                _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n                start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n                start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n\n            \n            context_text = self.context_text[self.valid_ds_idx]\n            char_to_word_offset = self.char_to_word_offset[self.valid_ds_idx]\n            token_to_orig_map = self.token_to_orig_map[self.valid_ds_idx]\n            \n            _answer =  get_answer_by_char_offset(context_text, char_to_word_offset, start_idx, end_idx, token_to_orig_map)\n            _answer_text = self.answer_text[self.valid_ds_idx]\n            \n            score = jaccard(_answer, _answer_text)\n            self.jaccard_scores.append(score)\n\n            self.valid_ds_idx += 1\n            \n    def on_epoch_end(self, last_metrics, **kwargs):        \n        res = np.mean(self.jaccard_scores)\n        return add_metrics(last_metrics, res)","0158f5d1":"from fastai.callbacks import *\n\ndef new_on_train_begin(self, **kwargs:Any)->None:\n    \"Initializes the best value.\"\n    if not hasattr(self, 'best'):\n        self.best = float('inf') if self.operator == np.less else -float('inf')\n\nSaveModelCallback.on_train_begin = new_on_train_begin","b9f5fbf0":"from tqdm import tqdm","de8e72d5":"train_ds, valid_ds, test_ds = get_fold_ds(0)\ndata = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs=128)","0f031546":"config = AutoConfig.from_pretrained(PRETRAINED_PATH\/\"roberta-base-config\")\nmodel = AutoModel.from_config(config)\ntse_model = TSEModel(model)\nlearner = Learner(data, tse_model, loss_func=CELoss(), path=PRETRAINED_PATH, model_dir=f\".\")","a2cbd99f":"token_to_orig_map = [o.token_to_orig_map for o in test_ds.features]\ncontext_text = [o.context_text for o in test_ds.examples]\nchar_to_word_offset = [o.char_to_word_offset for o in test_ds.examples]\noffset_shift = min(token_to_orig_map[0].keys())\ntest_ds_idx = 0 \n\nMODEL_NAME = \"roberta\"\nfinal_answers = []\n\nwith torch.no_grad():        \n    for xb,yb in tqdm(learner.data.test_dl):\n        model0 = learner.load(f'models_fold_0\/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits0, end_logits0 = to_cpu(model0(*xb))\n        start_logits0, end_logits0 = start_logits0.float(), end_logits0.float()\n        \n        model1 = learner.load(f'models_fold_1\/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits1, end_logits1 = to_cpu(model1(*xb))\n        start_logits1, end_logits1 = start_logits1.float(), end_logits1.float()\n        \n        model2 = learner.load(f'models_fold_2\/{MODEL_NAME}-qa-finetune').model.eval()        \n        start_logits2, end_logits2 = to_cpu(model2(*xb))\n        start_logits2, end_logits2 = start_logits2.float(), end_logits2.float()\n        \n        model3 = learner.load(f'models_fold_3\/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits3, end_logits3 = to_cpu(model3(*xb))\n        start_logits3, end_logits3 = start_logits3.float(), end_logits3.float()\n        \n        model4 = learner.load(f'models_fold_4\/{MODEL_NAME}-qa-finetune').model.eval()\n        start_logits4, end_logits4 = to_cpu(model4(*xb))\n        start_logits4, end_logits4 = start_logits4.float(), end_logits4.float()\n        \n        \n        input_ids = to_cpu(xb[0])\n        attention_masks = to_cpu(xb[1].bool())\n        token_type_ids = to_cpu(xb[2].bool())\n        \n        start_logits = (start_logits0 + start_logits1 + start_logits2 + start_logits3 + start_logits4) \/ 5\n        end_logits = (end_logits0 + end_logits1 + end_logits2 + end_logits3 + end_logits4) \/ 5\n        \n        # mask select only context part\n        for i in range(len(input_ids)):\n\n            _input_ids = input_ids[i].masked_select(attention_masks[i])\n            _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n            _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] # ignore first 4 (non context) and last special token:2\n            start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n            start_idx, end_idx = start_idx + offset_shift, end_idx + offset_shift\n\n            _context_text = context_text[test_ds_idx]\n            _char_to_word_offset = char_to_word_offset[test_ds_idx]\n            _token_to_orig_map = token_to_orig_map[test_ds_idx]\n            \n            predicted_answer =  get_answer_by_char_offset(_context_text, _char_to_word_offset, start_idx, end_idx, _token_to_orig_map)\n            final_answers.append(predicted_answer)\n            \n            test_ds_idx += 1","1e7bb544":"test_df['selected_text'] = final_answers","fe00590e":"# predict same text if word count < 3\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if len(o['text']) < 3 else o['selected_text'], 1)","fa9db446":"# keep neutral as it is or not?\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if o['sentiment'] == 'neutral' else o['selected_text'], 1)","e918bb39":"subdf = test_df[['textID', 'selected_text']]","b1fee3fd":"subdf.head()","57dc5e27":"subdf","a4a6e5ea":"## this shouldn't be necessary since evaluation code in Kaggle is fixed\n# def f(selected): return \" \".join(set(selected.lower().split()))\n# subdf.selected_text = subdf.selected_text.map(f)\nsubdf.to_csv(\"submission.csv\", index=False)","7e73b592":"For each epoch we will calculate `JaccardScore` on validation set","d14dbd57":"Here we define parameter group split points for `gradual unfreezing`. Idea is coming from [ULMFIT paper](https:\/\/arxiv.org\/pdf\/1801.06146.pdf).","c8739903":"Hope this is helpful to someone :)","c5223fd8":"### Create KFold Validation\n\nFor training we will be using only positive and negative tweets, as neutral tweets have a score of `~0.97` when submitted as it is. Also, positive and negative tweets are balanced so I am here using a vanilla KFold cross validation approach. Trained models for all folds can be used for further ensembling if needed.","b94d7766":"### Model\n\nHere we have `ModelWrapper` to make model from `transformers` to work with `fastai` 's `Learner` class. Also, loss is computed within the model, for this we will use a `DummyLoss` to work with `Learner.fit()`","d0f2b697":"### fin","b3b0325a":"Here we are initialazing model, splitting parameter groups and putting model callback for mixed precision training. `bs=128` is a good choice for the GPU memory we have at hand.","56d96d51":"### Predict","a5acea22":"We will choose start and end indexes for predictions such that sum of logits is maximum while satisfying `start_idx <= end_idx`","6b444586":"## Huggingface meets Fastai \n\n![](https:\/\/huggingface.co\/landing\/assets\/transformers-docs\/huggingface_logo.svg)\n![](https:\/\/docs.fast.ai\/images\/company_logo.png)","544227ef":"### Submit","558bb06c":"### Data\n\nSince we are training a BERT model we will use bert tokenizer for `bert-base-cased`. You can use `Auto*` classes from `transformers` library to load and train with any model available. For demonstration I am training with `foldnum=0`.","ca2cc85a":"This notebook shows training Bert model from `transformers` library  with `fastai` library interface. By doing so we get to use goodies like `lr_finder()`, `gradual_unfreezing`, `Callbacks`, `to_fp16()` and other customizations if necessary.\n\nSince finetuning requires loading pretrained models from the internet this notebook can't be submitted directly but required models or output of this notebook can be saved as a Kaggle dataset for further submission.","4b5b3a4d":"### SQUAD Q\/A Data Prep\n\nHere we are creating a SQUAD format dataset as we will leverage data prep utilities from `transformers` library. We could use SQUAD V1 or V2 for preparing data, but this dataset doesn't require `is_impossible` as it doesn't have any adversarial questions. Questions are coming from sentiment of the tweets; being either `positive` or `negative`. The idea has been taken from other kernels, so thanks!"}}