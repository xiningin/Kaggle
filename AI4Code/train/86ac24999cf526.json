{"cell_type":{"3de804e4":"code","e7261d85":"code","95cd96d1":"code","5e8feb29":"code","e1235ffb":"code","62a08c4b":"code","40afa279":"code","4a1d88a3":"code","9e917438":"code","46fa0e29":"code","f499171d":"code","481fc358":"code","ca6b4504":"code","7b7bc713":"code","2c4104ac":"code","bd8b7983":"code","6513eccf":"code","c7ca6321":"code","9596c442":"code","10458b2d":"code","f8192fd5":"code","fb92fbaa":"code","1db88077":"markdown","d11b9573":"markdown","196299e6":"markdown","fd0be4a4":"markdown","0f5b0837":"markdown","1cdb7d38":"markdown","e2f08332":"markdown","62b651d7":"markdown","98c44917":"markdown","db9f16dc":"markdown","13cef9b8":"markdown"},"source":{"3de804e4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\ndf = pd.read_csv('..\/input\/wavenet-data\/train_1.csv')\ndf.head()","e7261d85":"df.info()","95cd96d1":"data_start_date = df.columns[1]\ndata_end_date = df.columns[-1]\nprint('Data ranges from %s to %s' % (data_start_date, data_end_date))","5e8feb29":"def plot_random_series(df, n_series):\n    \n    sample = df.sample(n_series, random_state=8)\n    page_labels = sample['Page'].tolist()\n    series_samples = sample.loc[:,data_start_date:data_end_date]\n    \n    plt.figure(figsize=(10,6))\n    \n    for i in range(series_samples.shape[0]):\n        np.log1p(pd.Series(series_samples.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n    \n    plt.title('Randomly Selected Wikipedia Page Daily Views Over Time (Log(views) + 1)')\n    plt.legend(page_labels)\n    \nplot_random_series(df, 6)","e1235ffb":"from datetime import timedelta\n\npred_steps = 60 \npred_length=timedelta(pred_steps)\n\nfirst_day = pd.to_datetime(data_start_date) \nlast_day = pd.to_datetime(data_end_date)\n\nval_pred_start = last_day - pred_length + timedelta(1)\nval_pred_end = last_day\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1) ","62a08c4b":"enc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1) ","40afa279":"print('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)","4a1d88a3":"date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]),\n                          data=[i for i in range(len(df.columns[1:]))])\n\nseries_array = df[df.columns[1:]].values\n\ndef get_time_block_series(series_array, date_to_index, start_date, end_date):\n    \n    inds = date_to_index[start_date:end_date]\n    return series_array[:,inds]\n\ndef transform_series_encode(series_array):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_mean = series_array.mean(axis=1).reshape(-1,1) \n    series_array = series_array - series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array, series_mean\n\ndef transform_series_decode(series_array, encode_series_mean):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_array = series_array - encode_series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array","9e917438":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate\nfrom tensorflow.keras.optimizers import Adam\n\n# convolutional operation parameters\nn_filters = 32 # 32 \nfilter_width = 2\ndilation_rates = [2**i for i in range(8)] * 2 \n\n# define an input history series and pass it through a stack of dilated causal convolution blocks. \nhistory_seq = Input(shape=(None, 1))\nx = history_seq\n\nskips = []\nfor dilation_rate in dilation_rates:\n    \n    # preprocessing - equivalent to time-distributed dense\n    x = Conv1D(16, 1, padding='same', activation='relu')(x) \n    \n    # filter convolution\n    x_f = Conv1D(filters=n_filters,\n                 kernel_size=filter_width, \n                 padding='causal',\n                 dilation_rate=dilation_rate)(x)\n    \n    # gating convolution\n    x_g = Conv1D(filters=n_filters,\n                 kernel_size=filter_width, \n                 padding='causal',\n                 dilation_rate=dilation_rate)(x)\n    \n    # multiply filter and gating branches\n    z = Multiply()([Activation('tanh')(x_f),\n                    Activation('sigmoid')(x_g)])\n    \n    # postprocessing - equivalent to time-distributed dense\n    z = Conv1D(16, 1, padding='same', activation='relu')(z)\n    \n    # residual connection\n    x = Add()([x, z])    \n    \n    # collect skip connections\n    skips.append(z)\n\n# add all skip connection outputs \nout = Activation('relu')(Add()(skips))\n\n# final time-distributed dense layers \nout = Conv1D(128, 1, padding='same')(out)\nout = Activation('relu')(out)\nout = Dropout(.2)(out)\nout = Conv1D(1, 1, padding='same')(out)\n\n# extract the last 60 time steps as the training target\ndef slice(x, seq_length):\n    return x[:,-seq_length:,:]\n\npred_seq_train = Lambda(slice, arguments={'seq_length':60})(out)\n\nmodel = Model(history_seq, pred_seq_train)\nmodel.compile(Adam(), loss='mean_absolute_error')","46fa0e29":"model.summary()","f499171d":"first_n_samples = 120000\nbatch_size = 2**11\nepochs = 10\n\n# sample of series from train_enc_start to train_enc_end  \nencoder_input_data = get_time_block_series(series_array, date_to_index, \n                                           train_enc_start, train_enc_end)[:first_n_samples]\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\n# sample of series from train_pred_start to train_pred_end \ndecoder_target_data = get_time_block_series(series_array, date_to_index, \n                                            train_pred_start, train_pred_end)[:first_n_samples]\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n\n# we append a lagged history of the target series to the input data, \n# so that we can train with teacher forcing\nlagged_target_history = decoder_target_data[:,:-1,:1]\nencoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n\nmodel.compile(Adam(), loss='mean_absolute_error')\nhistory = model.fit(encoder_input_data, decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.2)  ","481fc358":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error Loss')\nplt.title('Loss Over Time')\nplt.legend(['Train','Valid'])","ca6b4504":"def predict_sequence(input_sequence):\n\n    history_sequence = input_sequence.copy()\n    pred_sequence = np.zeros((1,pred_steps,1)) # initialize output (pred_steps time steps)  \n    \n    for i in range(pred_steps):\n        \n        # record next time step prediction (last time step of model output) \n        last_step_pred = model.predict(history_sequence)[0,-1,0]\n        pred_sequence[0,i,0] = last_step_pred\n        \n        # add the next time step prediction to the history sequence\n        history_sequence = np.concatenate([history_sequence, \n                                           last_step_pred.reshape(-1,1,1)], axis=1)\n\n    return pred_sequence","7b7bc713":"encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\ndecoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)","2c4104ac":"def predict_and_plot(encoder_input_data, decoder_target_data, sample_ind, enc_tail_len=50):\n\n    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n    pred_series = predict_sequence(encode_series)\n    \n    encode_series = encode_series.reshape(-1,1)\n    pred_series = pred_series.reshape(-1,1)   \n    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n    \n    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n    x_encode = encode_series_tail.shape[0]\n    \n    plt.figure(figsize=(10,6))   \n    \n    plt.plot(range(1,x_encode+1),encode_series_tail)\n    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n    \n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend(['Encoding Series','Target Series','Predictions'])","bd8b7983":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=16534, enc_tail_len=100)","6513eccf":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=16555, enc_tail_len=100)","c7ca6321":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=4000, enc_tail_len=100)","9596c442":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=68000, enc_tail_len=100)","10458b2d":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=6007, enc_tail_len=100)","f8192fd5":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=70450, enc_tail_len=100)","fb92fbaa":"predict_and_plot(encoder_input_data, decoder_target_data, \n                 sample_ind=16551, enc_tail_len=100)","1db88077":"Generating some plots as below, we can see that our longer time horizon predictions (60 days) are often strong and expressive. Our full-fledged model is able to effectively capture weekly seasonality patterns and long term trends, and does a very nice job adapting to the varying levels of fluctuation in each series.   \n\nStill, we can do even better! We'd benefit from increasing the sample size for training and fine-tuning our hyperparameters, but also by giving the model access to additional relevant information. So far we've only fed the model raw time series data, but it can likely benefit from the inclusion of **exogenous variables** such as the day of the week and the language of the webpage corresponding to each series. To see how these exogenous variables can be incorporated directly into the model **check out the next notebook in this series**. \n\nIf you're interested in digging even deeper into state of the art WaveNet style architectures, I also highly recommend checking out [Sean Vasquez's model](https:\/\/github.com\/sjvasquez\/web-traffic-forecasting) that was designed for this data set. He implements a customized seq2seq WaveNet architecture in tensorflow.    ","d11b9573":"## 3. Building the Model - Architecture\n\nThis convolutional architecture is a full-fledged version of the [WaveNet model](https:\/\/deepmind.com\/blog\/wavenet-generative-model-raw-audio\/), designed as a generative model for audio (in particular, for text-to-speech applications). The wavenet model can be abstracted beyond audio to apply to any time series forecasting problem, providing a nice structure for capturing long-term dependencies without an excessive number of learned weights.\n\nThe core building block of the wavenet model is the **dilated causal convolution layer**, discussed in detail in the [previous notebook of this series](https:\/\/github.com\/JEddy92\/TimeSeries_Seq2Seq\/blob\/master\/notebooks\/TS_Seq2Seq_Conv_Intro.ipynb) as well as the [accompanying blog post](https:\/\/jeddy92.github.io\/JEddy92.github.io\/ts_seq2seq_conv\/). In summary, this style of convolution properly handles temporal flow and allows the receptive field of outputs to increase exponentially as a function of the number of layers. This structure is nicely visualized by the below diagram from the wavenet paper. \n\n![dilatedconv](images\/WaveNet_dilatedconv.png)\n\nThe model also utilizes some other key techniques: **gated activations**, **residual connections**, and **skip connections**. I'll introduce and explain these techniques, then show how to implement our full-fledged WaveNet architecture in keras. The WaveNet paper diagram below details how the model's components fit together block by block into a stack of operations, so we'll use it as a handy reference as we go (note that there are slight discrepancies between the diagram and what we implement, e.g. the original WaveNet has a softmax classification rather than regression output).   \n\n![blocks](images\/WaveNet_residblock.png)\n\n### **Gated Activations**\n\nIn the boxed portion of the architecture diagram, you'll notice that the dilated convolution output splits into two branches that are later recombined via element-wise multiplication. This depicts a *gated activation unit*, where we interpret the *tanh* activation branch as a learned filter and the *sigmoid* activation branch as a learned gate that regulates the information flow from the filter. If this reminds you of the gating mechanisms used in [LSTMs or GRUs](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/) you're on point, as those models use the same style of information gating to control adjustments to their cell states.\n\nIn mathematical notation, this means we map a convolutional block's input $x$ to output $z$ via the below, where $W_f$ and $W_g$ correspond to (learned) dilated causal convolution weights:\n\n$$ z = tanh(W_f * x) \\odot \\sigma(W_g * x) $$\n\nWhy use gated activations instead of the more standard *ReLU* activation? The WaveNet designers found that gated activations saw stronger performance empirically than ReLU activations for audio data, and this outperformance may extend broadly to time series data. Perhaps the [sparsity induced by ReLU activations](http:\/\/proceedings.mlr.press\/v15\/glorot11a.html) is not as well suited to time series forecasting as it is to other problem domains, or gated activations allow for smoother information (gradient) flow over a many-layered WaveNet architecture. However, this choice of activation is certainly not set in stone and I'd be interested to see a results comparison when trying ReLU instead. With that caveat, we'll be sticking with the gated activations in the interest of learning about the full original architecture.            \n\n### **Residual and Skip Connections**\n\nIn traditional neural network architectures, a neuron layer takes direct input only from the layer that precedes it, so early layers influence deeper layers via a heirarchy of intermediate computations. In theory, this heirarchy allows the network to properly build up high-level predictive features off of lower-level\/raw signals. For example, in image classification problems, neural nets start from raw pixel values, find generic geometric and textural patterns, then combine these generic patterns to construct fine-grained representations of the features that identify specific object types.\n\nBut what if lower-level signals are actually immediately useful for prediction, and may be at risk of distortion as they're passed through a complex heirarchy of computations? We could always simplify the heirarchy by using fewer layers and units, but what if we want the best of both worlds: direct, unfiltered low-level signals and nuanced heirarchical representations? One avenue for addressing this problem is provided by **skip connections**, which act to preserve earlier feature layer outputs as the network passes forward signals for final prediction processing. To build intuition for why we would want a mix of feature complexities in our problem domain, consider the wide range of time series drivers - there are strong and direct autoregressive components, moderately more sophisticated trend and seasonality components, and idiosyncratic trajectories that are difficult to spot with the human eye.        \n\nTo leverage skip connections, we can simply store the tensor output of each convolutional block in addition to passing it through further blocks (or choose select blocks to store output from). At the end of the block heirarchy, we then have a collection of feature outputs at *all levels of the heirarchy*, rather than a singular set of maximally complex feature outputs. This collection of outputs is then combined for final processing, typically via concatenation or addition (we'll use the latter).\n\nWith this in mind, return to the WaveNet block diagram above, and notice how for each block in the stack, the post-convolution gated activations pass through to the set of skip connections. This visualizes the tensor output storage and eventual combination just described. Note that the frequency and structure of skip connections is fully customizable and can be chosen experimentally and via domain expertise - as an example of an alternate skip connection structure, check out this convolutional architecture from a [semantic segmentation paper](https:\/\/www.researchgate.net\/publication\/327330378_Semantic_Segmentation_Based_on_Deep_Convolution_Neural_Network).\n\n![CNN_skips](images\/CNN_skips.png)\n\n**Residual connections** are closely related to skip connections; in fact, they can be viewed as specialized, short skips further into the network (often and in our case just one layer). With residual connections, we think of mapping a network block's input to output via $x_{out} = f(x_{in}) + x_{in}$ instead of using the traditional direct mapping $x_{out} = f(x_{in})$, for some function $f$ that corresponds to the model's learned weights. This helps allow for the possibility that the model learns a mapping that acts almost as an identity function, with the input passing through nearly unchanged. In the diagram above, such connections are visualized by the rounded arrows grouped with each pair of convolutions.  \n\nWhy would this be beneficial? Well, the effectiveness of residual connections is still not fully understood, but a compelling explanation is that they facilitate the use of deeper networks by allowing for more direct gradient flow in backpropagation. It's often difficult to efficienctly train the early layers of a deep network due to the length of the backpropagation chain, but residual and skip connections create an easier information highway. Intuitively, perhaps you can think of both as mechanisms for guarding against overcomputation and intermediate signal loss. You can check out the [ResNet paper](https:\/\/arxiv.org\/pdf\/1512.03385.pdf) that originated the residual connection concept for more discussion and empirical results.\n\nThough our architecture will be shallower than the original WaveNet (fewer convolutional blocks), we'll likely still see some benefit from introducing skip and residual connections at every block. Returning to the WaveNet architecture diagram again, you can see how the residual connection allows each block's input to bypass the convolution stage, and then adds that input to the convolution output. A final point to note is that the diagram's *1x1 convolutions* are really just equivalent to (time-distributed) fully connected layers, and serve in post-processing and standardization capacities. Our setup will use layers of this style (with different filter dimensions) for **post\/pre-processing** to facilitate our skip and residual connections, as well as for generating final prediction outputs.           \n\n### **Our Architecture**\n\nWith all of our components now laid out, here's what we'll use:\n\n* 16 dilated causal convolutional blocks\n    * Preprocessing and postprocessing (time distributed) fully connected layers (convolutions with filter width 1): 16 output units\n    * 32 filters of width 2 per block\n    * Exponentially increasing dilation rate with a reset (1, 2, 4, 8, ..., 128, 1, 2, ..., 128) \n    * Gated activations\n    * Residual and skip connections\n* 2 (time distributed) fully connected layers to map sum of skip outputs to final output \n\nWe'll extract the last 60 steps from the output sequence as our predicted output for training. We'll use teacher forcing again during training. Similarly to the previous notebook, we'll have a separate function that runs an inference loop to generate predictions on unseen data, iteratively filling previous predictions into the history sequence (section 4). ","196299e6":"## 5. Generating and Plotting Predictions \n\nNow we have everything we need to generate predictions for encoder (history) \/target series pairs that we didn't train on (note again we're using \"encoder\"\/\"decoder\" terminology to stay consistent with notebook 1 -- here it's more like history\/target). We'll pull out our set of validation encoder\/target series (recall that these are shifted forward in time). Then using a plotting utility function, we can look at the tail end of the encoder series, the true target series, and the predicted target series. This gives us a feel for how our predictions are doing.  ","fd0be4a4":"## 1. Loading and Previewing the Data \n\nFirst thing's first, let's load up the data and get a quick feel for it (reminder that the dataset is available [here](https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/data)). \n\nNote that there are a good number of NaN values in the data that don't disambiguate missing from zero. For the sake of simplicity in this tutorial, we'll naively fill these with 0 later on.","0f5b0837":"## 4. Building the Model - Inference Loop\n\nLike in the previous notebook, we'll generate predictions by running our model from section 3 in a loop, using each iteration to extract the prediction for the time step one beyond our current history then append it to our history sequence. With 60 iterations, this lets us generate predictions for the full interval we've chosen. \n\nRecall that we designed our model to output predictions for 60 time steps at once in order to use teacher forcing for training. So if we start from a history sequence and want to predict the first future time step, we can run the model on the history sequence and take the last time step of the output, which corresponds to one time step beyond the history sequence. ","1cdb7d38":"### Keras Data Formatting\n\nNow that we have the time segment dates, we'll define the functions we need to extract the data in keras friendly format. Here are the steps:\n\n* Pull the time series into an array, save a date_to_index mapping as a utility for referencing into the array \n* Create function to extract specified time interval from all the series \n* Create functions to transform all the series. \n    - Here we smooth out the scale by taking log1p and de-meaning each series using the encoder series mean, then reshape to the **(n_series, n_timesteps, n_features) tensor format** that keras will expect. \n    - Note that if we want to generate true predictions instead of log scale ones, we can easily apply a reverse transformation at prediction time. ","e2f08332":"It's typically a good idea to look at the convergence curve of train\/validation loss.","62b651d7":"With our training architecture defined, we're ready to train the model! This will take quite a while if you're not running fancy hardware (read GPU). We'll leverage the transformer utility functions we defined earlier, and train using mean absolute error loss.\n\nNote that for this full-fledged model, we have more than twice as many total parameters to train as we did with the simpler WaveNet model, explaining the slower training time (along with using more training data). From the loss curve you'll see plotted below, it also seems likely that the model can continue to improve with more than 10 training epochs -- the more complex model probably needs additional time to reach its full potential. That said, from the results plots (see section 5) we can see that this full-fledged model is very capable of handling the 60-day forecast horizon and often can generate very expressive predictions. \n\nThis is only a starting point, and I would encourage you to play around with this architecture to see if you can get even better results! You could try using more data, adjusting the hyperparameters, tuning the learning rate and number of epochs, etc.  ","98c44917":"# High-Dimensional Time Series Forecasting with Convolutional Neural Networks: Full-Fledged WaveNet\n\n**Note**: for a written overview on this topic, check out my two blog posts that walk through the core concepts behind WaveNet - [part 1](https:\/\/jeddy92.github.io\/JEddy92.github.io\/ts_seq2seq_conv\/), [part 2](https:\/\/jeddy92.github.io\/JEddy92.github.io\/ts_seq2seq_conv2\/). \n\nThis notebook expands on the [previous notebook in this series](https:\/\/github.com\/JEddy92\/TimeSeries_Seq2Seq\/blob\/master\/notebooks\/TS_Seq2Seq_Conv_Intro.ipynb), demonstrating in python\/keras code how a **convolutional** sequence-to-sequence neural network modeled after WaveNet can be built for the purpose of high-dimensional time series forecasting. I assume working familiarity with **dilated causal convolutions** (WaveNet's core building block), and recommend referencing the 3rd section of the previous notebook if you need to review the concept.\n\nFor an introduction to neural network forecasting with an LSTM architecture, check out the [first notebook in this series](https:\/\/github.com\/JEddy92\/TimeSeries_Seq2Seq\/blob\/master\/notebooks\/TS_Seq2Seq_Intro.ipynb).   \n\nIn this notebook I'll be using the daily wikipedia web page traffic dataset again, available [here on Kaggle](https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/data). The corresponding competition called for forecasting 60 days into the future, which we'll now mirror in this demonstration of a full-fledged model. Once again we'll use all of the series history available in \"train_1.csv\" for the encoding stage of the model. \n\nOur goal here is to expand on the previous notebook's simple WaveNet implementation, adding additional architecture components from the [original model](https:\/\/arxiv.org\/pdf\/1609.03499.pdf). In particular, each convolutional block of our network will incorporate **gated activations**, **residual connections**, and **skip connections** in addition to the dilated causal convolutions we saw in the previous notebook. I'll explain how these three new mechanisms work in section 3. Feel free to skip ahead to that section if you're comfortable with the data setup and formatting steps (as in the previous notebooks), and want to get right into the neural network.     \n\nHere's a section breakdown of this notebook -- enjoy!\n\n**1. Loading and Previewing the Data**   \n**2. Formatting the Data for Modeling**  \n**3. Building the Model - Training Architecture**  \n**4. Building the Model - Inference Loop**  \n**5. Generating and Plotting Predictions**","db9f16dc":"We can define a function that lets us visualize some random webpage series as below. For the sake of smoothing out the scale of traffic across different series, we apply a log1p transformation before plotting - i.e. take $\\log(1+x)$ for each value $x$ in a series.","13cef9b8":"## 2. Formatting the Data for Modeling \n\nSadly we can't just throw the dataframe we've created into keras and let it work its magic. Instead, we have to set up a few data transformation steps to extract nice numpy arrays that we can pass to keras. But even before doing that, we have to know how to appropriately partition the time series into encoding and prediction intervals for the purposes of training and validation. Note that for our simple convolutional model we won't use an encoder-decoder architecture like in the first notebook, but **we'll keep the \"encoding\" and \"decoding\" (prediction) terminology to be consistent** -- in this case, the encoding interval represents the entire series history that we will use for the network's feature learning, but not output any predictions on. \n\nWe'll use a style of **walk-forward validation**, where our validation set spans the same time-range as our training set, but shifted forward in time (in this case by 60 days). This way, we simulate how our model will perform on unseen data that comes in the future. \n\n[Artur Suilin](https:\/\/github.com\/Arturus\/kaggle-web-traffic\/blob\/master\/how_it_works.md) has created a very nice image that visualizes this validation style and contrasts it with traditional validation. I highly recommend checking out his entire repo, as he's implemented a truly state of the art (and competition winning) seq2seq model on this data set. \n\n![architecture](images\/ArturSuilin_validation.png)\n\n### Train and Validation Series Partioning\n\nWe need to create 4 sub-segments of the data:\n\n    1. Train encoding period\n    2. Train decoding period (train targets, 60 days)\n    3. Validation encoding period\n    4. Validation decoding period (validation targets, 60 days)\n    \nWe'll do this by finding the appropriate start and end dates for each segment. Starting from the end of the data we've loaded, we'll work backwards to get validation and training prediction intervals. Then we'll work forward from the start to get training and validation encoding intervals. "}}