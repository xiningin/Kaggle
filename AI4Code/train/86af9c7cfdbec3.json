{"cell_type":{"9c674337":"code","5e982a22":"code","51fdd5a1":"code","d5a4e347":"code","2cb422e6":"code","288ed96e":"code","23285e96":"code","5961a578":"code","432b330f":"code","8a5a02de":"code","530d726c":"code","6a18e7be":"code","e5cb9248":"code","f38e2063":"code","21aa2aa8":"code","8854e43b":"code","2424c52e":"code","ed3450cf":"code","978efd02":"code","c81bf236":"markdown","54732d6b":"markdown","e2422d5f":"markdown","7df1d9b8":"markdown","386ae88c":"markdown","c5c37242":"markdown","95ba7b3b":"markdown"},"source":{"9c674337":"# import\nimport math\nimport numpy as np\nimport pandas as pd # for manipulating dataset\nfrom sklearn import tree # for decision tree and to plot tree\nimport matplotlib.pyplot as plt","5e982a22":"# implement gini\ndef gini(samples):\n    sample_sum = sum(samples)\n    score = 0\n    probs = []\n    for sample in samples:\n        prob = sample \/ sample_sum\n        # probability of every sample\n        probs.append(prob)\n    \n    for prob in probs:\n        score += (prob * (1 -  prob))\n    return score","51fdd5a1":"# implement entroty (info. gain) function\ndef entropy(samples):\n    sample_sum = sum(samples)\n    score = 0\n    probs = []\n    for sample in samples:\n        prob = sample \/ sample_sum\n        probs.append(prob)\n    \n    for prob in probs:\n        if prob > 0:\n            score += (prob * math.log(prob))\n    return -1 * score","d5a4e347":"# we need weighted gini for node\ndef weighted(p1,p2,n1,n2):\n    # n1 = samples with p1 prob\n    # n2 = samples with p2 prob\n    return (p1 * n1 + p2 * n2) \/ (n1 + n2)","2cb422e6":"# we're going to use titanic datasets\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.head()","288ed96e":"# we'll only use Sex , Pclass and Survived(target column) column\ncol = ['Survived' , 'Pclass' , 'Sex']\ndf = df[col]\ndf.head()","23285e96":"# Sex column contains categorical value male and female so we'll convert male to 1 and female to 0\nmapping = {\n    'male' : 1,\n    'female' : 0\n}\ndf['Sex'] = df.Sex.map(mapping)\ndf.head()","5961a578":"df.Survived.value_counts() # total we have 549 data with 0 label and 342 data with 1 label","432b330f":"# gini of whole data\n# starting gini\ngini([549 , 342]) ","8a5a02de":"# pclass contain only 3 value 1,2,3\ndf.Pclass.unique()","530d726c":"# so now what kinds of condition we can make ?\n\n# let's say we have pclass with : 1 , 2 , 3\n# so we can make split at :         |   |\n#                               (1.5)  (2.5)\n\n# and sex has 2 value : 0 , 1 so we can make split at 0.5\n","6a18e7be":"# if the condition is true\nx = df[df.Pclass <= 1.5]\nprint(f\"total elements with pclass <= 1.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([80,136])}\\n\")\n\n# if the condition is false\nx = df[df.Pclass > 1.5]\nprint(f\"total elements with pclass > 1.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([469,206])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.466 , 0.424 , 216 , 675)}\")","e5cb9248":"# if the condition is true\nx = df[df.Pclass <= 2.5]\nprint(f\"total elements with pclass <= 2.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([177,223])}\\n\")\n\n# if the condition is false\nx = df[df.Pclass > 2.5]\nprint(f\"total elements with pclass > 2.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([372,119])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.493 , 0.367 , 400 , 491)}\")","f38e2063":"# if the condition is true\nx = df[df.Sex <= 0.5]\nprint(f\"total elements with sex <= 0.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([81,233])}\\n\")\n\n# if the condition is false\nx = df[df.Sex > 0.5]\nprint(f\"total elements with sex > 0.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([468,109])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.382 , 0.306 , 314 , 577)}\")","21aa2aa8":"# so sex has minimum weighted gini so it becomes our starting condition\n\n# gini : 0.383 , (81,233)\n# gini : 0.306 , (468,109)\n\n# now lets check for sex <= 0.5 and pclass <= 1.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass <= 1.5)]\nprint(f\"total elements with sex <= 0.5 and pclass <= 1.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([3,91])}\\n\")\n\n# now lets check for sex <= 0.5 and pclass > 1.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass > 1.5)]\nprint(f\"total elements with sex <= 0.5 and pclass > 1.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([78,142])}\\n\")\n\nprint(f\"weighted gini : {weighted(0.061 , 0.457 , 94 , 220)}\")","8854e43b":"# now lets check for sex <= 0.5 and pclass <= 2.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass <= 2.5)]\nprint(f\"total elements with sex <= 0.5 and pclass <= 2.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([9,161])}\\n\")\n\n# now lets check for sex <= 0.5 and pclass > 2.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass > 2.5)]\nprint(f\"total elements with sex <= 0.5 and pclass > 2.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([72,72])}\\n\")\n\nprint(f\"weighted gini : {weighted(0.1 , 0.5 , 170 , 144)}\")","2424c52e":"# here weighted average for pclass<=2.5 is less then pclass<=1.5 we'll select pclass <= 2.5\n# and this process will last untill we reached leaf nodes\n\n# now let's implement whole tree with sklearn\n\nfeatures = ['Sex' , 'Pclass']\n# model intialization\nmodel = tree.DecisionTreeClassifier()\n# fitting model\nmodel.fit(df[features] , df.Survived)\n\nplt.figure(figsize = (20,10))\ntree.plot_tree(model , feature_names = features)\nplt.show()","ed3450cf":"# Problem with Decision Tree  : \n\n# As the number of split increases , complexity of DT will also increase.\n# In general simple DTs will be preferred over complex DTS.\n# main problem with DT is that if tree will become more complex and classify every data point with 100% accuracy then there might arises problem of overfitting\n# In Below image we'll prefer Black line over Green line.\n","978efd02":"# to handle problem of overfitting we'll try to contro the depth of the decision tree , it might decrease training accuracy \n# but it'll surely increase testing accuracy\n# In other words , do splitting until we get 96% or 99% pure class instead of 100% pure class\n\n# in sklearn we can provide max_depth , for e.g.\n\nmodel = tree.DecisionTreeClassifier(max_depth = 7)\n\n# we can figure out optimal max_depth by cross validation or may be using some grid search algorithm","c81bf236":"![](http:\/\/i.ibb.co\/hdK8LCR\/pclass.png)","54732d6b":"# Decision Trees are one of the most powerful [supervised algorithm](https:\/\/en.wikipedia.org\/wiki\/Supervised_learning) that we have ever discovered.\n# Intuition behind decision tree is to divide datasets into smaller datasets based on some feature until we reached down to dataset that can be uniquely classified.\n# As a name suggest , In this algorithm we'll devide datasets based on some kind of decision and we'll create tree structure.\n![](http:\/\/i.ibb.co\/ZTZcjbm\/decision.png)\n# The decision to split at each node is made based on the metric called purity. \n# A node is 100% impure when a node is split evenly 50\/50 and 100% pure when all of its data belongs to a single class.\n# In order to optimize our model we need to reach maximum purity and avoid impurity\n\n# Let's learn about some of the purity metric :\n\n# 1. gini : $G = \\sum_{i=0}^{n-1} p_{i} * (1 - p_{i})$ \n      \n# 2. Entropy (Information gain) : $E = - \\sum_{i=0}^{n-1} p_{i} * log(p_{i})$\n# where $p_{i}$ is the probability of ith class and $n$ is total number of samples\n\n# Let's implement decision Tree based on gini :","e2422d5f":"![](http:\/\/i.ibb.co\/09fqDcB\/over.png)","7df1d9b8":"# This whole notebook is based on @abhishek thakur's [Youtube](https:\/\/youtu.be\/1DMWkIJRivo) video.\n# reference : Blog from [towards data science](https:\/\/towardsdatascience.com\/the-complete-guide-to-decision-trees-28a4e3c7be14)","386ae88c":"# **Decision Tree**","c5c37242":"![](http:\/\/i.ibb.co\/DRHLCHJ\/sex.png)","95ba7b3b":"![](http:\/\/i.ibb.co\/X8WSPWT\/pclass2.png)"}}