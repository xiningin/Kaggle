{"cell_type":{"f15b8134":"code","2c49434f":"code","52872914":"code","8bc634ff":"code","9d53b8ac":"code","db4d208e":"code","d2d0653c":"code","584593e7":"code","259c9fc5":"code","07992019":"code","72271818":"code","e31e1b5f":"code","88ca79c6":"code","4e3b3315":"code","d5703e4e":"code","a21025e0":"code","ad83c3c3":"code","6b215f6c":"code","ccbc56b2":"code","4903c51f":"code","7c4f5d3d":"code","5f0af80d":"code","4b0666f3":"code","61163459":"code","6fd4e029":"code","5f2613ef":"code","9c5bcfd2":"code","523a747a":"code","36e3bd97":"code","6e238a5f":"code","a9109223":"code","3b9d3ac0":"code","3cac17c7":"code","b708ee3d":"code","e4f0af39":"code","7d04139a":"code","cdbd6cf2":"code","19a5e31e":"code","8dedf17a":"code","1d1b291e":"code","30dc8f20":"code","98d11ec8":"markdown","2b41191b":"markdown","8d97f6ae":"markdown","1633ae95":"markdown","1e89dac6":"markdown","da389bf3":"markdown","0933bb5b":"markdown","6f5b48b9":"markdown","56c54228":"markdown","253bfda7":"markdown"},"source":{"f15b8134":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","2c49434f":"train = pd.read_csv('\/kaggle\/input\/datmin-joints-2020\/train_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/datmin-joints-2020\/test_data.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/datmin-joints-2020\/sample_submission.csv')\n","52872914":"# Keanehan nilai dari setiap kolom\nkolom_huruf = [('word-1','a','588184'), ('word-5','4+F2185','11h','a'), ('word-8','a','`16'),('word-9','`','`29'), ('word-10','`10'), ('word-11','\\\\'),('word-13','a','`27'),\n              ('word-14','`3'),('word-17','a','`4'),('word-18','\\\\','a'),('word-19','`155','`363','`361', '`51','`764','`84','994821','536457','`61'),\n              ('word-22','31415669','`21','`11'),('word-24','`2'), ('word-27','`2','['), ('word-28','a'), ('word-30','a'),('word-33','`10','`18','a'),('word-35','`3'),\n              ('word-38','412334','`10'),('word-39','`1'),('word-40','a')]\n\n'''\nDapat diambil kesimpulan jika dikelompokkan, nilai unique dari setiap kesalahan input adalah :\n1. 'a' --> dibikin nan atau nilai tertentu ?\n2. '`' --> Paling banyak, nanti di hilangkan maka itu nilai yang asli\n3. '\\\\' --> sedikit, fix NaN\n4.  '[' --> sedikit, fix NaN\n5. 31415669 --> kasus angka sebanyak ini, dijadikan nan atau nilai tertentu ?\n6. 4+F2185 --> Fix NaN\n\n\n'''\n\n# Membuat dictionary, dimana keynya berupa nomor kolom, dan valuenya adalah nilai kolom_huruf\n# Ini digunakan untuk looping\nkolom_huruf = {1:kolom_huruf[0],5:kolom_huruf[1],8:kolom_huruf[2],9:kolom_huruf[3],10:kolom_huruf[4],11:kolom_huruf[5],13:kolom_huruf[6],14:kolom_huruf[7],17:kolom_huruf[8],\n              18:kolom_huruf[9],19:kolom_huruf[10], 22:kolom_huruf[11], 24:kolom_huruf[12], 27:kolom_huruf[13], 28:kolom_huruf[14], 30:kolom_huruf[15], 33:kolom_huruf[16],\n              35:kolom_huruf[17],38:kolom_huruf[18],39:kolom_huruf[19],40:kolom_huruf[20]}","8bc634ff":"# Membuat fungsi mengganti huruf atau benerin nilai yang inputnya salah\ndef ganti_huruf(kolom,huruf):\n    \n    nomor_row = 0\n    for i in range(train.shape[0]):\n        if train.iloc[i,kolom] == huruf:\n            # Menyimpan nomor row untuk mengetahui row keberapa dimana a itu berada\n            #print('nomor_kolom :',i)\n            nomor_row = i \n \n    # Test apakah row tersebut bener atau gak\n   # print('testing apakah bener:',train.iloc[nomor_row,kolom])\n    \n    # Ganti huruf tersebut menjadi nan atau hilangkan tanda '`'\n    if kolom == 9:\n        if '`' in huruf:\n            train.iloc[nomor_row,kolom] = np.nan\n            \n    elif 'a' in huruf:\n        train.iloc[nomor_row,kolom] = 0\n        \n    elif  '`' in huruf:\n        train.iloc[nomor_row,kolom] = huruf[1:] \n        \n    elif 'h' in huruf:\n        train.iloc[nomor_row,kolom] = huruf[:-1] # untuk nangkep word-5 --> '11h'\n    \n    else:\n        #print(kolom,huruf)\n        train.iloc[nomor_row,kolom] = np.nan\n   # print('Berhasil !!!')","9d53b8ac":"# Looping fungsi ganti_huruf untuk setiap kolom_huruf !\nfor key,value in kolom_huruf.items():\n    \n    # Dibuat error excepition karena nanti akan terjadi IndexError !\n    try :\n        for j in range(len(value)):\n            \n            # key --> kolom, value --> huruf\n            ganti_huruf(key,value[j+1])\n            \n    except:\n        pass","db4d208e":"# Kolom 8 dan 13, entah kenapa tidak berhasil untuk mengganti huruf a diatas ! jadi dilakukan secara terpisah dari looping\nganti_huruf(8,'a')\nganti_huruf(13,'a')\n# Missing Values\ntrain = train.fillna(0)","d2d0653c":"kolom = ['word-1', 'word-2', 'word-3', 'word-4', 'word-5', 'word-6',\n       'word-7', 'word-8', 'word-9', 'word-10', 'word-11', 'word-12',\n       'word-13', 'word-14', 'word-15', 'word-16', 'word-17', 'word-18',\n       'word-19', 'word-20', 'word-21', 'word-22', 'word-23', 'word-24',\n       'word-25', 'word-26', 'word-27', 'word-28', 'word-29', 'word-30',\n       'word-31', 'word-32', 'word-33', 'word-34', 'word-35', 'word-36',\n       'word-37', 'word-38', 'word-39', 'word-40']\n\n# Mengganti tipe data object ke float\nfor i in kolom:\n\n    train[i] = train[i].astype('int')\n    \ntrain.info()","584593e7":"# Tambahan data error ! \ntrain.loc[32, 'word-4'] = 1 # eror karena -1\ntrain.loc[24, 'word-25'] = 9 # error karena -9\ntrain.loc[3220, 'word-36'] = 0 # error karena nilainya sangat besar, contoh : 1295321","259c9fc5":"test = test.fillna(0)\ntrain_ = train.drop(columns = 'Result')\nfull = pd.concat([train_,test])\nfull = full.drop(columns = 'id')\nfull.head()","07992019":"full1 = full.copy()\n\nX_train_full1 = full1.iloc[:3620] \nX_test_full1 = full1.iloc[3620:] \n\nX1 = X_train_full1\ny1 = train.Result\nX1.head()","72271818":"full2 = full.copy()\n\nfrom array import array\ntf = 1\/full2.sum(axis=1)[:,None]\nfull2 = full2*tf\n\nidf = np.log(full2.shape[0]\/ (full2>0).sum())\nfull2 = full2*idf\n\nfull2 = full2.fillna(0)\n\nX_train_full2 = full2.iloc[:3620] \nX_test_full2 = full2.iloc[3620:] \n\nX2 = X_train_full2\ny2 = train.Result\nX2.head()","e31e1b5f":"from numpy import array\nfrom sklearn.decomposition import PCA\nfull3 = full.copy()\n\nbinary = list()\nfor i in range(len(full3)):\n    bi = list()\n    for j in full3.iloc[i,]:\n        if j != 0:\n            bi.append(1)\n        else: bi.append(0)\n    binary.append(bi)\n    \nha = pd.DataFrame(binary, columns = full.columns)\n\nfor i in ha.columns:\n        full3[i+'binary'] = ha[i]\n\nX_train_full3 = full3.iloc[:3620] \nX_test_full3 = full3.iloc[3620:] \n\nX3 = X_train_full3\ny3 = train.Result\n\nX3.head()","88ca79c6":"full4 = full.copy()","4e3b3315":"std_words = list()\nfor j in range(len(full4)): \n    std_words.append(full4.iloc[j].std())\n    \nstd_words = pd.Series(std_words)\n\n\njumlah_kata = list()\nfor i in range(len(full4)):\n    jumlah_kata.append(full4.iloc[i,].sum())\n               \nTotal_Kata = pd.Series(jumlah_kata)\n\nunique_words = list()\njumlah_unique = 0\nfor j in range(len(full4)):\n    for key,i in full4.iloc[j].items():\n        if key != 'Result':\n            if i == 1:\n                jumlah_unique += 1\n    unique_words.append(jumlah_unique)\n    jumlah_unique = 0\n    \nunique_words = pd.Series(unique_words)\n    \nabsence_words = list()\njumlah_absence = 0\nfor j in range(len(full4)):\n    for key,i in full4.iloc[j].items():\n        if key != 'Result':\n            if i == 0:\n                jumlah_absence += 1\n    absence_words.append(jumlah_absence)\n    jumlah_absence = 0\nabsence_words = pd.Series(absence_words)\n","d5703e4e":"from array import array\ntf = 1\/full4.sum(axis=1)[:,None]\nfull4 = full4*tf\n\nidf = np.log(full4.shape[0]\/ (full4>0).sum())\nfull4 = full4*idf\n\nfull4 = full4.fillna(0)","a21025e0":"full4['Kata_Unik'] = unique_words\nfull4['Persentase_Unik_vs_TotalKata'] = (pd.Series(unique_words).multiply(100))\/pd.Series(Total_Kata)\nfull4['Std_Kata'] = std_words\nfull4['Absen_Kata'] = absence_words\nfull4['Total_Kata'] = Total_Kata\nfull4['multiply_18_34'] = full['word-18'].multiply(full['word-34'])\nfull4['multiply_15_33'] = full['word-15'].multiply(full['word-33'])","ad83c3c3":"full4 = full4.fillna(0)","6b215f6c":"X_train_full4 = full4.iloc[:3620] \nX_test_full4 = full4.iloc[3620:] \n\nX4 = X_train_full4\ny4 = train.Result\nX4.head()","ccbc56b2":"full5 = full.copy()\n","4903c51f":"jumlah_kata = list()\nfor i in range(len(full5)):\n    jumlah_kata.append(full5.iloc[i,].sum())\n               \nTotal_Kata = pd.Series(jumlah_kata)\n\nfull5['multiply_18_34'] = full4['word-18'].multiply(full4['word-34'])\nfull5['Total_Kata'] = Total_Kata\nfull5['multiply_15_33'] = full['word-15'].multiply(full['word-33'])","7c4f5d3d":"full5 = full5.fillna(0)\nX_train_full5 = full5.iloc[:3620] \nX_test_full5 = full5.iloc[3620:] \n\nX5 = X_train_full5\ny5 = train.Result\nX5.head()","5f0af80d":"from sklearn.model_selection import StratifiedKFold\n\n# Some useful parameters which will come in handy later on\nntrain = full.iloc[:3620].shape[0]\nntest = full.iloc[3620:].shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 3 # set folds for out-of-fold prediction\nskf = StratifiedKFold(n_splits=NFOLDS, random_state=SEED)","4b0666f3":"# clf --> adalah model yang ingin digunakan\n# x_tr --> adalah tipe dataset yang ingin digunakan, seluruh data_train\n# y_tr --> adalah variabel target\n# x_test --> adalah test set untuk di lakukan prediksi, lalu di submission\ndef get_oof(clf, x_tr, y_tr, x_test, deeplearning = False, deep = False):\n    count = 0\n    \n    # BUAT VECTOR UNTUK MENYIMPAN HASIL CV \n    oof_train = np.zeros((ntrain,))\n    oof_train_ = np.zeros((ntrain,))\n\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    oof_y_valid = np.zeros((ntrain,))\n    \n    # Cross validasi\n    for train_index, test_index in skf.split(x_tr,y_tr):\n        X_train, X_valid = x_tr.iloc[train_index], x_tr.iloc[test_index] \n        y_train, y_valid = y_tr.iloc[train_index], y_tr.iloc[test_index]\n\n        if deeplearning == True:\n            clf.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n            clf.fit(X_train,y_train,batch_size=25,epochs=10,validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n            oof_train[test_index] = clf.predict(X_valid)[:,0]\n            oof_test_skf[count,:] = clf.predict(x_test)[:,0]\n        elif deeplearning == True & deep == True:\n            clf.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n\n            clf.fit(X_train,y_train, batch_size=16, epochs=30,validation_data=(X_valid, y_valid),callbacks=[es, rlr],verbose=1)\n            oof_train[test_index] = clf.predict(X_valid)[:,0]\n            oof_test_skf[count,:] = clf.predict(x_test)[:,0]\n            \n        else:\n            clf.fit(X_train, y_train)\n            oof_train[test_index] = clf.predict_proba(X_valid)[:,1]\n            oof_test_skf[count,:] = clf.predict_proba(x_test)[:,1]\n            oof_train_[test_index] = clf.predict(X_valid)\n            oof_y_valid[test_index] = y_valid\n            \n            if count > 1:\n                oof_y_pred = pd.Series(oof_train_.ravel())\n                tn, fp, fn, tp = confusion_matrix(oof_y_valid, oof_y_pred).ravel()\n                #Metric yang digunakan\n                categorization_accuracy = (tp+tn)\/(tp+tn+fp+fn)\n                print(categorization_accuracy)\n        count+=1\n        \n   \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    # Output adalah matriks yg berisikan nilai prediksi\n    # oof_train --> isinya hasil prediksi dari seluruh cross validation\n    # oof_test --> isinya hasil prediksi data test (x_test)\n    # oof_y_valid --> isinya adalah seluruh target y yang asli saat di cross validation\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), oof_y_valid","61163459":"y = train.Result","6fd4e029":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom catboost import CatBoostClassifier\n\next = ExtraTreesClassifier()\nsvc = SVC(kernel='sigmoid', gamma=1.0, probability=True)\nlgr = LogisticRegression()\nnb = MultinomialNB()\nxgb = XGBClassifier(n_estimators=250,scale_pos_weight =  (y.shape[0]-y.sum()) \/ y.sum())\nlgbm = LGBMClassifier(n_estimators = 250, scale_pos_weight=(y.shape[0]-y.sum()) \/ y.sum())\n\ncat = CatBoostClassifier()\nlda = discriminant_analysis.LinearDiscriminantAnalysis()\nqda = discriminant_analysis.QuadraticDiscriminantAnalysis()\ndct = tree.DecisionTreeClassifier()\nnsvc =svm.NuSVC(probability=True)\nlsvc =  svm.LinearSVC()\nknc = neighbors.KNeighborsClassifier()\nber =  naive_bayes.BernoulliNB()\ngau =  naive_bayes.GaussianNB()\npac = linear_model.PassiveAggressiveClassifier()\nrid =   linear_model.RidgeClassifierCV()\nsgdc =  linear_model.SGDClassifier()\nper = linear_model.Perceptron()\ngaup = gaussian_process.GaussianProcessClassifier()\ngbc = ensemble.GradientBoostingClassifier()\nrf = ensemble.RandomForestClassifier()\nada = ensemble.AdaBoostClassifier()\nbag = ensemble.BaggingClassifier()\n","5f2613ef":"# Mengambil hasil prediksi untuk data_train, data_test per model\n\nprint('catboost score:')\nxgb_oof_train191, xgb_oof_test191,oof_y_valid191 = get_oof(bag,X_train_full1, y, X_test_full1) # Bagging\nxgb_oof_train192, xgb_oof_test192,oof_y_valid192 = get_oof(bag,X_train_full2, y, X_test_full2) # Bagging\nxgb_oof_train193, xgb_oof_test193,oof_y_valid193 = get_oof(bag,X_train_full3, y, X_test_full3) # Bagging\nxgb_oof_train194, xgb_oof_test194,oof_y_valid194 = get_oof(bag,X_train_full4, y, X_test_full4) # Bagging\nxgb_oof_train195, xgb_oof_test195,oof_y_valid195 = get_oof(bag,X_train_full5, y, X_test_full5) # Bagging\n\n\nprint('xgbg score:')\nxgb_oof_train11, xgb_oof_test11,oof_y_valid11 = get_oof(xgb,X_train_full1, y, X_test_full1) # Xtreme Gradient Boost\nxgb_oof_train12, xgb_oof_test12,oof_y_valid12 = get_oof(xgb,X_train_full2, y, X_test_full2) # Xtreme Gradient Boost\nxgb_oof_train13, xgb_oof_test13,oof_y_valid13 = get_oof(xgb,X_train_full3, y, X_test_full3) # Xtreme Gradient Boost\nxgb_oof_train14, xgb_oof_test14,oof_y_valid14 = get_oof(xgb,X_train_full4, y, X_test_full4) # Xtreme Gradient Boost\nxgb_oof_train15, xgb_oof_test15,oof_y_valid15 = get_oof(xgb,X_train_full5, y, X_test_full5) # Xtreme Gradient Boost\n\nprint('lgbm score:')\nxgb_oof_train21, xgb_oof_test21,oof_y_valid21 = get_oof(lgbm,X_train_full1, y, X_test_full1) # LGBM\nxgb_oof_train22, xgb_oof_test22,oof_y_valid22 = get_oof(lgbm,X_train_full2, y, X_test_full2) # LGBM\nxgb_oof_train23, xgb_oof_test23,oof_y_valid23 = get_oof(lgbm,X_train_full3, y, X_test_full3) # LGBM\nxgb_oof_train24, xgb_oof_test24,oof_y_valid24 = get_oof(lgbm,X_train_full4, y, X_test_full4) # LGBM\nxgb_oof_train25, xgb_oof_test25,oof_y_valid25 = get_oof(lgbm,X_train_full5, y, X_test_full5) # LGBM\n\nprint('multinomial score:')\nxgb_oof_train31, xgb_oof_test31,oof_y_valid31 = get_oof(nb,X_train_full1, y, X_test_full1) # MultinomialNB\nxgb_oof_train32, xgb_oof_test32,oof_y_valid32 = get_oof(nb,X_train_full2, y, X_test_full2) # MultinomialNB\n# xgb_oof_train33, xgb_oof_test33,oof_y_valid33 = get_oof(nb,X_train_full3, y, X_test_full3) # MultinomialNB\nxgb_oof_train34, xgb_oof_test34,oof_y_valid34 = get_oof(nb,X_train_full4, y, X_test_full4) # MultinomialNB\nxgb_oof_train35, xgb_oof_test35,oof_y_valid35 = get_oof(nb,X_train_full5, y, X_test_full5) # MultinomialNB\n\nprint('Logistic regression score:')\nxgb_oof_train41, xgb_oof_test41,oof_y_valid41 = get_oof(lgr,X_train_full1, y, X_test_full1) # LogisticRegression\nxgb_oof_train42, xgb_oof_test42,oof_y_valid42 = get_oof(lgr,X_train_full2, y, X_test_full2) # LogisticRegression\nxgb_oof_train43, xgb_oof_test43,oof_y_valid43 = get_oof(lgr,X_train_full3, y, X_test_full3) # LogisticRegression\nxgb_oof_train44, xgb_oof_test44,oof_y_valid44 = get_oof(lgr,X_train_full4, y, X_test_full4) # LogisticRegression\nxgb_oof_train45, xgb_oof_test45,oof_y_valid45 = get_oof(lgr,X_train_full5, y, X_test_full5) # LogisticRegression\n\nprint('svm score:')\nxgb_oof_train51, xgb_oof_test51,oof_y_valid51 = get_oof(svc,X_train_full1, y, X_test_full1) # SVM\nxgb_oof_train52, xgb_oof_test52,oof_y_valid52 = get_oof(svc,X_train_full2, y, X_test_full2) # SVM\nxgb_oof_train53, xgb_oof_test53,oof_y_valid53 = get_oof(svc,X_train_full3, y, X_test_full3) # SVM\nxgb_oof_train54, xgb_oof_test54,oof_y_valid54 = get_oof(svc,X_train_full4, y, X_test_full4) # SVM\nxgb_oof_train55, xgb_oof_test55,oof_y_valid55 = get_oof(svc,X_train_full5, y, X_test_full5) # SVM\n\nprint('extra tree:')\nxgb_oof_train61, xgb_oof_test61,oof_y_valid61 = get_oof(ext,X_train_full1, y, X_test_full1) # ExtraTree\nxgb_oof_train62, xgb_oof_test62,oof_y_valid62 = get_oof(ext,X_train_full2, y, X_test_full2) # ExtraTree\nxgb_oof_train63, xgb_oof_test63,oof_y_valid63 = get_oof(ext,X_train_full3, y, X_test_full3) # ExtraTree\nxgb_oof_train64, xgb_oof_test64,oof_y_valid64 = get_oof(ext,X_train_full4, y, X_test_full4) # ExtraTree\nxgb_oof_train65, xgb_oof_test65,oof_y_valid65 = get_oof(ext,X_train_full5, y, X_test_full5) # ExtraTree\n\nprint('lda score:')\nxgb_oof_train71, xgb_oof_test71,oof_y_valid71 = get_oof(lda,X_train_full1, y, X_test_full1) # LinearDiscriminantAnalysis\nxgb_oof_train72, xgb_oof_test72,oof_y_valid72 = get_oof(lda,X_train_full2, y, X_test_full2) # LinearDiscriminantAnalysis\nxgb_oof_train73, xgb_oof_test73,oof_y_valid73 = get_oof(lda,X_train_full3, y, X_test_full3) # LinearDiscriminantAnalysis\nxgb_oof_train74, xgb_oof_test74,oof_y_valid74 = get_oof(lda,X_train_full4, y, X_test_full4) # LinearDiscriminantAnalysis\nxgb_oof_train75, xgb_oof_test75,oof_y_valid75 = get_oof(lda,X_train_full5, y, X_test_full5) # LinearDiscriminantAnalysis\n\nprint('qda score:')\nxgb_oof_train81, xgb_oof_test81,oof_y_valid81 = get_oof(qda,X_train_full1, y, X_test_full1) # QuadraticDiscriminantAnalysis\nxgb_oof_train82, xgb_oof_test82,oof_y_valid82 = get_oof(qda,X_train_full2, y, X_test_full2) # QuadraticDiscriminantAnalysis\nxgb_oof_train83, xgb_oof_test83,oof_y_valid83 = get_oof(qda,X_train_full3, y, X_test_full3) # QuadraticDiscriminantAnalysis\nxgb_oof_train84, xgb_oof_test84,oof_y_valid84 = get_oof(qda,X_train_full4, y, X_test_full4) # QuadraticDiscriminantAnalysis\nxgb_oof_train85, xgb_oof_test85,oof_y_valid85 = get_oof(qda,X_train_full5, y, X_test_full5) # QuadraticDiscriminantAnalysis\n\nprint('decision tree score:')\nxgb_oof_train91, xgb_oof_test91,oof_y_valid91 = get_oof(dct,X_train_full1, y, X_test_full1) # DecisionTree\nxgb_oof_train92, xgb_oof_test92,oof_y_valid92 = get_oof(dct,X_train_full2, y, X_test_full2) # DecisionTree\nxgb_oof_train93, xgb_oof_test93,oof_y_valid93 = get_oof(dct,X_train_full3, y, X_test_full3) # DecisionTree\nxgb_oof_train94, xgb_oof_test94,oof_y_valid94 = get_oof(dct,X_train_full4, y, X_test_full4) # DecisionTree\nxgb_oof_train95, xgb_oof_test95,oof_y_valid95 = get_oof(dct,X_train_full5, y, X_test_full5) # DecisionTree\n\n\nprint('nusvc score:')\nxgb_oof_train101, xgb_oof_test101,oof_y_valid101 = get_oof(nsvc,X_train_full1, y, X_test_full1) # NuSVC\nxgb_oof_train102, xgb_oof_test102,oof_y_valid102 = get_oof(nsvc,X_train_full2, y, X_test_full2) # NuSVC\nxgb_oof_train103, xgb_oof_test103,oof_y_valid103 = get_oof(nsvc,X_train_full3, y, X_test_full3) # NuSVC\nxgb_oof_train104, xgb_oof_test104,oof_y_valid104 = get_oof(nsvc,X_train_full4, y, X_test_full4) # NuSVC\nxgb_oof_train105, xgb_oof_test105,oof_y_valid105 = get_oof(nsvc,X_train_full5, y, X_test_full5) # NuSVC\n\nprint('kneighbors score:')\nxgb_oof_train111, xgb_oof_test111,oof_y_valid111 = get_oof(knc,X_train_full1, y, X_test_full1) # KNeighborsClassifier\nxgb_oof_train112, xgb_oof_test112,oof_y_valid112 = get_oof(knc,X_train_full2, y, X_test_full2) # KNeighborsClassifier\nxgb_oof_train113, xgb_oof_test113,oof_y_valid113 = get_oof(knc,X_train_full3, y, X_test_full3) # KNeighborsClassifier\nxgb_oof_train114, xgb_oof_test114,oof_y_valid114 = get_oof(knc,X_train_full4, y, X_test_full4) # KNeighborsClassifier\nxgb_oof_train115, xgb_oof_test115,oof_y_valid115 = get_oof(knc,X_train_full5, y, X_test_full5) # KNeighborsClassifier\n\nprint('bernouli score:')\nxgb_oof_train121, xgb_oof_test121,oof_y_valid121 = get_oof(ber,X_train_full1, y, X_test_full1) # BernoulliNB\nxgb_oof_train122, xgb_oof_test122,oof_y_valid122 = get_oof(ber,X_train_full2, y, X_test_full2) # BernoulliNB\nxgb_oof_train123, xgb_oof_test123,oof_y_valid123 = get_oof(ber,X_train_full3, y, X_test_full3) # BernoulliNB\nxgb_oof_train124, xgb_oof_test124,oof_y_valid124 = get_oof(ber,X_train_full4, y, X_test_full4) # BernoulliNB\nxgb_oof_train125, xgb_oof_test125,oof_y_valid125 = get_oof(ber,X_train_full5, y, X_test_full5) # BernoulliNB\n\nprint('gaussian score:')\nxgb_oof_train131, xgb_oof_test131,oof_y_valid131 = get_oof(gau,X_train_full1, y, X_test_full1) # GaussianNB\nxgb_oof_train132, xgb_oof_test132,oof_y_valid132 = get_oof(gau,X_train_full2, y, X_test_full2) # GaussianNB\nxgb_oof_train133, xgb_oof_test133,oof_y_valid133 = get_oof(gau,X_train_full3, y, X_test_full3) # GaussianNB\nxgb_oof_train134, xgb_oof_test134,oof_y_valid134 = get_oof(gau,X_train_full4, y, X_test_full4) # GaussianNB\nxgb_oof_train135, xgb_oof_test135,oof_y_valid135 = get_oof(gau,X_train_full5, y, X_test_full5) # GaussianNB\n\nprint('gausian process score:')\nxgb_oof_train141, xgb_oof_test141,oof_y_valid141 = get_oof(gaup,X_train_full1, y, X_test_full1) # GaussianProcess\nxgb_oof_train142, xgb_oof_test142,oof_y_valid142 = get_oof(gaup,X_train_full2, y, X_test_full2) # GaussianProcess\nxgb_oof_train143, xgb_oof_test143,oof_y_valid143 = get_oof(gaup,X_train_full3, y, X_test_full3) # GaussianProcess\nxgb_oof_train144, xgb_oof_test144,oof_y_valid144 = get_oof(gaup,X_train_full4, y, X_test_full4) # GaussianProcess\nxgb_oof_train145, xgb_oof_test145,oof_y_valid145 = get_oof(gaup,X_train_full5, y, X_test_full5) # GaussianProcess\n\nprint('gradient boosting score:')\nxgb_oof_train151, xgb_oof_test151,oof_y_valid151 = get_oof(gbc,X_train_full1, y, X_test_full1) # GradientBoosting\nxgb_oof_train152, xgb_oof_test152,oof_y_valid152 = get_oof(gbc,X_train_full2, y, X_test_full2) # GradientBoosting\nxgb_oof_train153, xgb_oof_test153,oof_y_valid153 = get_oof(gbc,X_train_full3, y, X_test_full3) # GradientBoosting\nxgb_oof_train154, xgb_oof_test154,oof_y_valid154 = get_oof(gbc,X_train_full4, y, X_test_full4) # GradientBoosting\nxgb_oof_train155, xgb_oof_test155,oof_y_valid155 = get_oof(gbc,X_train_full5, y, X_test_full5) # GradientBoosting\n\nprint('random foreset score:')\nxgb_oof_train161, xgb_oof_test161,oof_y_valid161 = get_oof(rf,X_train_full1, y, X_test_full1) # RandomForest\nxgb_oof_train162, xgb_oof_test162,oof_y_valid162 = get_oof(rf,X_train_full2, y, X_test_full2) # RandomForest\nxgb_oof_train163, xgb_oof_test163,oof_y_valid163 = get_oof(rf,X_train_full3, y, X_test_full3) # RandomForest\nxgb_oof_train164, xgb_oof_test164,oof_y_valid164 = get_oof(rf,X_train_full4, y, X_test_full4) # RandomForest\nxgb_oof_train165, xgb_oof_test165,oof_y_valid165 = get_oof(rf,X_train_full5, y, X_test_full5) # RandomForest\n\nprint('ada boost score:')\nxgb_oof_train171, xgb_oof_test171,oof_y_valid171 = get_oof(ada,X_train_full1, y, X_test_full1) # AdaBoost\nxgb_oof_train172, xgb_oof_test172,oof_y_valid172 = get_oof(ada,X_train_full2, y, X_test_full2) # AdaBoost\nxgb_oof_train173, xgb_oof_test173,oof_y_valid173 = get_oof(ada,X_train_full3, y, X_test_full3) # AdaBoost\nxgb_oof_train174, xgb_oof_test174,oof_y_valid174 = get_oof(ada,X_train_full4, y, X_test_full4) # AdaBoost\nxgb_oof_train175, xgb_oof_test175,oof_y_valid175 = get_oof(ada,X_train_full5, y, X_test_full5) # AdaBoost\n\nprint('baggin score:')\nxgb_oof_train181, xgb_oof_test181,oof_y_valid181 = get_oof(bag,X_train_full1, y, X_test_full1) # Bagging\nxgb_oof_train182, xgb_oof_test182,oof_y_valid182 = get_oof(bag,X_train_full2, y, X_test_full2) # Bagging\nxgb_oof_train183, xgb_oof_test183,oof_y_valid183 = get_oof(bag,X_train_full3, y, X_test_full3) # Bagging\nxgb_oof_train184, xgb_oof_test184,oof_y_valid184 = get_oof(bag,X_train_full4, y, X_test_full4) # Bagging\nxgb_oof_train185, xgb_oof_test185,oof_y_valid185 = get_oof(bag,X_train_full5, y, X_test_full5) # Bagging\n","9c5bcfd2":"# Membuat dataframe dari seluruh hasil prediksi data train\nbase_predictions_train = pd.DataFrame( {\n\n    'Model11': xgb_oof_train11.ravel(),\n    'Model12': xgb_oof_train12.ravel(),\n    'Model13': xgb_oof_train13.ravel(),\n    'Model14': xgb_oof_train14.ravel(),\n    'Model15': xgb_oof_train15.ravel(),\n\n    'Model21': xgb_oof_train21.ravel(),\n    'Model22': xgb_oof_train22.ravel(),\n    'Model23': xgb_oof_train23.ravel(),\n    'Model24': xgb_oof_train24.ravel(),\n    'Model25': xgb_oof_train25.ravel(),\n    \n    'Model31': xgb_oof_train31.ravel(),\n    'Model32': xgb_oof_train32.ravel(),\n#     'Model33': xgb_oof_train33.ravel(),\n    'Model34': xgb_oof_train34.ravel(),\n    'Model35': xgb_oof_train35.ravel(),\n    \n    'Model41': xgb_oof_train41.ravel(),\n    'Model42': xgb_oof_train42.ravel(),\n    'Model43': xgb_oof_train43.ravel(),\n    'Model44': xgb_oof_train44.ravel(),\n    'Model45': xgb_oof_train45.ravel(),\n    \n    \n    'Model51': xgb_oof_train51.ravel(),\n    'Model52': xgb_oof_train52.ravel(),\n    'Model53': xgb_oof_train53.ravel(),\n    'Model54': xgb_oof_train54.ravel(),\n    'Model55': xgb_oof_train55.ravel(),\n    \n    'Model61': xgb_oof_train61.ravel(),\n    'Model62': xgb_oof_train62.ravel(),\n    'Model63': xgb_oof_train63.ravel(),\n    'Model64': xgb_oof_train64.ravel(),\n    'Model65': xgb_oof_train65.ravel(),\n    \n    \n    'Model71': xgb_oof_train71.ravel(),\n    'Model72': xgb_oof_train72.ravel(),\n    'Model73': xgb_oof_train73.ravel(),\n    'Model74': xgb_oof_train74.ravel(),\n    'Model75': xgb_oof_train75.ravel(),\n   \n    'Model81': xgb_oof_train81.ravel(),\n    'Model82': xgb_oof_train82.ravel(),\n    'Model83': xgb_oof_train83.ravel(),\n    'Model84': xgb_oof_train84.ravel(),\n    'Model85': xgb_oof_train85.ravel(),\n    \n    'Model91': xgb_oof_train91.ravel(),\n    'Model92': xgb_oof_train92.ravel(),\n    'Model93': xgb_oof_train93.ravel(),\n    'Model94': xgb_oof_train94.ravel(),\n    'Model95': xgb_oof_train95.ravel(),\n    \n    'Model101': xgb_oof_train101.ravel(),\n    'Model102': xgb_oof_train102.ravel(),\n    'Model103': xgb_oof_train103.ravel(),\n    'Model104': xgb_oof_train104.ravel(),\n    'Model105': xgb_oof_train105.ravel(),\n    \n    \n    'Model111': xgb_oof_train111.ravel(),\n    'Model112': xgb_oof_train112.ravel(),\n    'Model113': xgb_oof_train113.ravel(),\n    'Model114': xgb_oof_train114.ravel(),\n    'Model115': xgb_oof_train115.ravel(),\n    \n    'Model121': xgb_oof_train121.ravel(),\n    'Model122': xgb_oof_train122.ravel(),\n    'Model123': xgb_oof_train123.ravel(),\n    'Model124': xgb_oof_train124.ravel(),\n    'Model125': xgb_oof_train125.ravel(),\n    \n    \n    'Model131': xgb_oof_train131.ravel(),\n    'Model132': xgb_oof_train132.ravel(),\n    'Model133': xgb_oof_train133.ravel(),\n    'Model134': xgb_oof_train134.ravel(),\n    'Model135': xgb_oof_train135.ravel(),\n    \n    \n    'Model141': xgb_oof_train141.ravel(),\n    'Model142': xgb_oof_train142.ravel(),\n    'Model143': xgb_oof_train143.ravel(),\n    'Model144': xgb_oof_train144.ravel(),\n    'Model145': xgb_oof_train145.ravel(),\n    \n    'Model151': xgb_oof_train151.ravel(),\n    'Model152': xgb_oof_train152.ravel(),\n    'Model153': xgb_oof_train153.ravel(),\n    'Model154': xgb_oof_train154.ravel(),\n    'Model155': xgb_oof_train155.ravel(),\n    \n    'Model161': xgb_oof_train161.ravel(),\n    'Model162': xgb_oof_train162.ravel(),\n    'Model163': xgb_oof_train163.ravel(),\n    'Model164': xgb_oof_train164.ravel(),\n    'Model165': xgb_oof_train165.ravel(),\n    \n    'Model171': xgb_oof_train171.ravel(),\n    'Model172': xgb_oof_train172.ravel(),\n    'Model173': xgb_oof_train173.ravel(),\n    'Model174': xgb_oof_train174.ravel(),\n    'Model175': xgb_oof_train175.ravel(),\n    \n    'Model181': xgb_oof_train181.ravel(),\n    'Model182': xgb_oof_train182.ravel(),\n    'Model183': xgb_oof_train183.ravel(),\n    'Model184': xgb_oof_train184.ravel(),\n    'Model185': xgb_oof_train185.ravel(),\n    \n    'Model191': xgb_oof_train191.ravel(),\n    'Model192': xgb_oof_train192.ravel(),\n    'Model193': xgb_oof_train193.ravel(),\n    'Model194': xgb_oof_train194.ravel(),\n    'Model195': xgb_oof_train195.ravel(),\n    })\nbase_predictions_train.head()","523a747a":"# membuat kolom hasil prediksi dari data_train (seluruh model)\nx_train = np.concatenate((  \n    xgb_oof_train11,\n    xgb_oof_train12,\n    xgb_oof_train13,\n    xgb_oof_train14,\n    xgb_oof_train15,\n\n    xgb_oof_train21,\n    xgb_oof_train22,\n    xgb_oof_train23,\n    xgb_oof_train24,\n    xgb_oof_train25,\n    \n    xgb_oof_train31,\n    xgb_oof_train32,\n    xgb_oof_train34,\n    xgb_oof_train35,\n    \n    xgb_oof_train41,\n    xgb_oof_train42,\n    xgb_oof_train43,\n    xgb_oof_train44,\n    xgb_oof_train45,\n    \n    \n    xgb_oof_train51,\n    xgb_oof_train52,\n    xgb_oof_train53,\n    xgb_oof_train54,\n    xgb_oof_train55,\n    \n    xgb_oof_train61,\n    xgb_oof_train62,\n    xgb_oof_train63,\n    xgb_oof_train64,\n    xgb_oof_train65,\n    \n    \n    xgb_oof_train71,\n    xgb_oof_train72,\n    xgb_oof_train73,\n    xgb_oof_train74,\n    xgb_oof_train75,\n   \n    xgb_oof_train81,\n    xgb_oof_train82,\n    xgb_oof_train83,\n    xgb_oof_train84,\n    xgb_oof_train85,\n    \n    xgb_oof_train91,\n    xgb_oof_train92,\n    xgb_oof_train93,\n    xgb_oof_train94,\n    xgb_oof_train95,\n    \n    xgb_oof_train101,\n    xgb_oof_train102,\n    xgb_oof_train103,\n    xgb_oof_train104,\n    xgb_oof_train105,\n    \n    \n    xgb_oof_train111,\n    xgb_oof_train112,\n    xgb_oof_train113,\n    xgb_oof_train114,\n    xgb_oof_train115,\n    \n    xgb_oof_train121,\n    xgb_oof_train122,\n    xgb_oof_train123,\n    xgb_oof_train124,\n    xgb_oof_train125,\n    \n    \n    xgb_oof_train131,\n    xgb_oof_train132,\n    xgb_oof_train133,\n    xgb_oof_train134,\n    xgb_oof_train135,\n    \n    \n    xgb_oof_train141,\n    xgb_oof_train142,\n    xgb_oof_train143,\n    xgb_oof_train144,\n    xgb_oof_train145,\n    \n    xgb_oof_train151,\n    xgb_oof_train152,\n    xgb_oof_train153,\n    xgb_oof_train154,\n    xgb_oof_train155,\n    \n    xgb_oof_train161,\n    xgb_oof_train162,\n    xgb_oof_train163,\n    xgb_oof_train164,\n    xgb_oof_train165,\n    \n    xgb_oof_train171,\n    xgb_oof_train172,\n    xgb_oof_train173,\n    xgb_oof_train174,\n    xgb_oof_train175,\n    \n    xgb_oof_train181,\n    xgb_oof_train182,\n    xgb_oof_train183,\n    xgb_oof_train184,\n    xgb_oof_train185,\n    \n    xgb_oof_train191,\n    xgb_oof_train192,\n    xgb_oof_train193,\n    xgb_oof_train194,\n    xgb_oof_train195), axis=1)\n    \n   \n","36e3bd97":"# membuat kolom hasil prediksi dari data_Test (seluruh model)\nx_test = np.concatenate((xgb_oof_test11,\n    xgb_oof_test12,\n    xgb_oof_test13,\n    xgb_oof_test14,\n    xgb_oof_test15,\n\n    xgb_oof_test21,\n    xgb_oof_test22,\n    xgb_oof_test23,\n    xgb_oof_test24,\n    xgb_oof_test25,\n    \n    xgb_oof_test31,\n    xgb_oof_test32,\n\n    xgb_oof_test34,\n    xgb_oof_test35,\n    \n    xgb_oof_test41,\n    xgb_oof_test42,\n    xgb_oof_test43,\n    xgb_oof_test44,\n    xgb_oof_test45,\n    \n    \n    xgb_oof_test51,\n    xgb_oof_test52,\n    xgb_oof_test53,\n    xgb_oof_test54,\n    xgb_oof_test55,\n    \n    xgb_oof_test61,\n    xgb_oof_test62,\n    xgb_oof_test63,\n    xgb_oof_test64,\n    xgb_oof_test65,\n    \n    \n    xgb_oof_test71,\n    xgb_oof_test72,\n    xgb_oof_test73,\n    xgb_oof_test74,\n    xgb_oof_test75,\n   \n    xgb_oof_test81,\n    xgb_oof_test82,\n    xgb_oof_test83,\n    xgb_oof_test84,\n    xgb_oof_test85,\n    \n    xgb_oof_test91,\n    xgb_oof_test92,\n    xgb_oof_test93,\n    xgb_oof_test94,\n    xgb_oof_test95,\n    \n    xgb_oof_test101,\n    xgb_oof_test102,\n    xgb_oof_test103,\n    xgb_oof_test104,\n    xgb_oof_test105,\n    \n    \n    xgb_oof_test111,\n    xgb_oof_test112,\n    xgb_oof_test113,\n    xgb_oof_test114,\n    xgb_oof_test115,\n    \n    xgb_oof_test121,\n    xgb_oof_test122,\n    xgb_oof_test123,\n    xgb_oof_test124,\n    xgb_oof_test125,\n    \n    \n    xgb_oof_test131,\n    xgb_oof_test132,\n    xgb_oof_test133,\n    xgb_oof_test134,\n    xgb_oof_test135,\n    \n    \n    xgb_oof_test141,\n    xgb_oof_test142,\n    xgb_oof_test143,\n    xgb_oof_test144,\n    xgb_oof_test145,\n    \n    xgb_oof_test151,\n    xgb_oof_test152,\n    xgb_oof_test153,\n    xgb_oof_test154,\n    xgb_oof_test155,\n    \n    xgb_oof_test161,\n    xgb_oof_test162,\n    xgb_oof_test163,\n    xgb_oof_test164,\n    xgb_oof_test165,\n    \n    xgb_oof_test171,\n    xgb_oof_test172,\n    xgb_oof_test173,\n    xgb_oof_test174,\n    xgb_oof_test175,\n    \n    xgb_oof_test181,\n    xgb_oof_test182,\n    xgb_oof_test183,\n    xgb_oof_test184,\n    xgb_oof_test185,\n    \n    xgb_oof_test191,\n    xgb_oof_test192,\n    xgb_oof_test193,\n    xgb_oof_test194,\n    xgb_oof_test195,\n    ), axis=1)","6e238a5f":"# meta_model, model ini digunakan untuk training data hasil prediksi kita --> x_train\ngbm = XGBClassifier( n_estimators= 250,max_depth= 4,min_child_weight= 2,gamma=0.9, subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',\n                    nthread= -1,scale_pos_weight =  (y.shape[0]-y.sum()) \/ y.sum()) # default parameter :learning_rate = 0.02,#gamma=1,\ngbm.fit(x_train, y)\n\n# lalu hasil model tersebut dilakukan prediksi ke x_test\npredictions2 = gbm.predict(x_test)","a9109223":"submit_stacking1= pd.DataFrame(test.iloc[:,0])\nsubmit_stacking1['Result'] = predictions2\nsubmit_stacking1.head()","3b9d3ac0":"submit_stacking1.Result.value_counts()","3cac17c7":"submit_stacking2= pd.DataFrame(test.iloc[:,0])\nsubmit_stacking2['Result'] = predictions2\nsubmit_stacking2.head()","b708ee3d":"submit_stacking2.Result.value_counts()","e4f0af39":"submit_stacking3= pd.DataFrame(test.iloc[:,0])\nsubmit_stacking3['Result'] = predictions2\nsubmit_stacking3.head()","7d04139a":"submit_stacking3.Result.value_counts()","cdbd6cf2":"fi = pd.DataFrame()\nfi['R1'] = submit_stacking1.Result\nfi['R2'] = submit_stacking2.Result\nfi['R3'] = submit_stacking3.Result","19a5e31e":"final = list()\nfrom statistics import mode\nfor i in range(len(fi)):\n    final_bin.append(mode(fi.iloc[i,]))","8dedf17a":"submit = pd.DataFrame(test.iloc[:,0])\nsubmit['Result'] = hasil['final']\nsubmit.head()","1d1b291e":"submit.Result.value_counts()","30dc8f20":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nsubmit.to_csv('final_ensemble_bin.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='final_ensemble_bin.csv')","98d11ec8":"### FITTING","2b41191b":"## Third Dataset","8d97f6ae":"## PREPROCESS","1633ae95":"Lakukan training stacking dari awal sebanyak 3x, lalu voting 3 hasil prediksi dengan mengambil nilai mode per baris","1e89dac6":"## First Dataset","da389bf3":"## Second Dataset","0933bb5b":"## Five Dataset","6f5b48b9":"## Fourth Dataset","56c54228":"#### VOTING","253bfda7":"## NOTE:\nIni bukan hasil yang saya gunakan, tapi kurang lebih metode yang saya gunakan seperti ini.\n<t> Sangat Brute Force :v"}}