{"cell_type":{"b30040f3":"code","53d958e0":"code","6b53a461":"code","4e5b6901":"code","9ef28a0f":"code","1aaf8b63":"code","7d57430c":"code","0e50b145":"code","f8c56b8e":"code","828bb498":"code","50a17a37":"code","1d0dbea8":"code","8c1b4817":"code","60aaae40":"code","560f2ed2":"code","7c0696c7":"code","85e4ccdc":"code","5097802b":"code","3a139e26":"code","fbc38a48":"code","cd567f3d":"code","1c31f6d6":"markdown","6e4bd3e9":"markdown","de9dc1b1":"markdown","f59541a3":"markdown","d3b8dcb9":"markdown","f2f20d88":"markdown","68f7b56b":"markdown"},"source":{"b30040f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","53d958e0":"df_X_train = pd.read_csv(\"..\/input\/X_train.csv\")\ndf_X_test = pd.read_csv(\"..\/input\/X_test.csv\")\ndf_y_train = pd.read_csv(\"..\/input\/y_train.csv\")\n\ndef check_for_nas(df):\n    return sum(df.isnull().sum())\n\nprint(\"Number of na values in X_train: {}\".format(check_for_nas(df_X_train)))\nprint(\"Number of na values in X_test: {}\".format(check_for_nas(df_X_test)))\nprint(\"Number of na values in y_train: {}\".format(check_for_nas(df_y_train)))","6b53a461":"df_y_train['surface'].value_counts()","4e5b6901":"def convert_q_e(list_):\n    '''Convert quaternion to euler angles.\n    list_ has 4 elements in the order mentioned below\n    or_x - X orientation\n    or_y - Y orientation\n    or_z - Z orientation\n    or_w - W orientation\n    Returns a tuple of (roll,pitch,yaw)'''\n    or_x = list_[0]\n    or_y = list_[1] \n    or_z = list_[2] \n    or_w = list_[3] \n    \n    sinr_cosp = 2 * ((or_w * or_x) + (or_y * or_z))\n    cosr_cosp = 1 - 2 * ((or_x * or_x) + (or_y * or_y))\n    roll = np.arctan2(sinr_cosp, cosr_cosp)\n    \n    pitch = None\n    sinp = 2 * ((or_w * or_y) - (or_z * or_x))\n    if abs(sinp) >= 1:\n        pitch = np.copysign(np.pi\/2,sinp)\n    else:\n        pitch = np.arcsin(sinp)\n    \n    siny_cosp = 2 * ((or_w * or_z) + (or_x * or_y))\n    cosy_cosp = 1 - 2 * ((or_y * or_y) + (or_z * or_z))\n    yaw = np.arctan2(siny_cosp, cosy_cosp)\n    \n    return(roll, pitch, yaw)\n\n","9ef28a0f":"# Using apply to convert orientation to euler angles\norientation_col_names = ['orientation_X','orientation_Y','orientation_Z','orientation_W']\ndf_X_train_euler = df_X_train.loc[:,orientation_col_names].apply(convert_q_e,axis=1,result_type='expand')\ndf_X_train_euler.rename(columns={0:'roll',1:'pitch',2:'yaw'},inplace = True)\n\ndf_X_test_euler = df_X_test.loc[:,orientation_col_names].apply(convert_q_e,axis=1,result_type='expand')\ndf_X_test_euler.rename(columns={0:'roll',1:'pitch',2:'yaw'}, inplace=True)\n    \n# df_X_train_processed = df_X_train.drop(orientation_col_names,axis=1)\ndf_X_train_processed = pd.concat([df_X_train,df_X_train_euler], axis = 1)\n# Need to remove row_id and measurement_number as they are redundant\nredundant_cols = ['row_id','measurement_number']\ndf_X_train_processed = df_X_train_processed.drop(redundant_cols,axis=1)\n\n# df_X_test_processed = df_X_test.drop(orientation_col_names,axis=1)\ndf_X_test_processed = pd.concat([df_X_test,df_X_test_euler],axis = 1)\ndf_X_test_processed = df_X_test_processed.drop(redundant_cols, axis = 1)\n   ","1aaf8b63":"df_X_test_processed.head()","7d57430c":"# Function to address 1\nintegration = lambda x: 0.001 * (x.diff().sum())\n\n# Function to address 2\nmean_abs = lambda x:x.abs().mean()\n\n# Function to address 3\nsum_abs = lambda x:x.abs().sum()\n\n# Function to address 4\nmean_diff = lambda x:x.mean()\n\n# Function to address 11\ndef modified_mean_diff(x):\n    res = [0]\n    res.extend(list(np.diff(x)))\n    return np.mean(res)\n","0e50b145":"# Function to create aggregates\ndef create_aggregates(df,function,appendage,cols_affected = None):\n    '''Returns an aggregate dataset based on group_by dataset. Appendage modifies colnames by adding this string to the end of the colname\n    df - Pandas Dataframe Object on which group by is performed\n    cols_affected - list of columns on which aggregation needs to be performed\n    function - function to aggregate groupby df on\n    appendage - string to modify colnames'''\n    if cols_affected:\n        cols_affected.append('series_id')\n        df_sub = df.loc[:,cols_affected].copy()\n        group_by_df = df_sub.groupby(by=['series_id'])\n    else:\n        group_by_df = df.groupby(by=['series_id'])\n    \n    df_ = group_by_df.agg(function).rename(mapper = lambda x:x + \"_\" + appendage, axis = 'columns').copy()\n    return df_\n    ","f8c56b8e":"# Create feature datasets based on bullet points above. Merge them later as a final step\n#Step - 1\n#Train\ndf_X_train_step_1 = create_aggregates(df_X_train_processed,function=integration,appendage='integral')\n#Test\ndf_X_test_step_1 = create_aggregates(df_X_test_processed,function=integration,appendage='integral')\n\n#Step - 2\n#Train\ndf_X_train_step_2 = create_aggregates(df_X_train_processed,function=mean_abs,appendage='mean_abs')\n#Test\ndf_X_test_step_2 = create_aggregates(df_X_test_processed,function=mean_abs,appendage='mean_abs')\n\n#Step - 3\n#Train\ndf_X_train_step_3 = create_aggregates(df_X_train_processed,function=sum_abs,appendage='sum_abs')\n#Test\ndf_X_test_step_3 = create_aggregates(df_X_test_processed,function=sum_abs,appendage='sum_abs')\n\n#Step - 4\n#Train\ndf_X_train_step_4 = create_aggregates(df_X_train_processed,function=mean_diff,appendage='mean_diff')\n#Test\ndf_X_test_step_4 = create_aggregates(df_X_test_processed,function=mean_diff,appendage='mean_diff')\n\n#Step - 5\n#Train\ndf_X_train_step_5 = create_aggregates(df_X_train_processed,function=np.mean,appendage='mean')\n#Test\ndf_X_test_step_5 = create_aggregates(df_X_test_processed,function=np.mean,appendage='mean')\n\n#Step - 6\n#Train\ndf_X_train_step_6 = create_aggregates(df_X_train_processed,function=np.std,appendage='std')\n#Test\ndf_X_test_step_6 = create_aggregates(df_X_test_processed,function=np.std,appendage='std')\n\n#Step - 7 \n#Train\ndf_X_train_step_7 = create_aggregates(df_X_train_processed,function=np.max,appendage='max')\n#Test\ndf_X_test_step_7 = create_aggregates(df_X_test_processed,function=np.max,appendage='max')\n\n#Step - 8\n#Trainm\ndf_X_train_step_8 = create_aggregates(df_X_train_processed,function=np.min,appendage='min')\n#Test\ndf_X_test_step_8 = create_aggregates(df_X_test_processed,function=np.min,appendage='min')\n\n# Step-9\n#Train\ndf_X_train_step_9 = create_aggregates(df_X_train_processed,function=np.ptp,appendage='ptp')\n#Test\ndf_X_test_step_9 = create_aggregates(df_X_test_processed,function=np.ptp,appendage='ptp')\n\n# Step - 10\n#Train\ndf_X_train_step_10 = create_aggregates(df_X_train_processed,function=np.median,appendage='median')\n#Test\ndf_X_test_step_10 = create_aggregates(df_X_test_processed,function=np.median,appendage='median')\n\n# Step - 11\n#Train\ndf_X_train_step_11 = create_aggregates(df_X_train_processed,function=modified_mean_diff,appendage='mod_diff')\n#Test\ndf_X_test_step_11 = create_aggregates(df_X_test_processed,function=modified_mean_diff,appendage='mod_diff')","828bb498":"X_train_master = pd.concat([df_X_train_step_1,df_X_train_step_2,df_X_train_step_3,df_X_train_step_4,df_X_train_step_5,df_X_train_step_6,df_X_train_step_7,df_X_train_step_8,df_X_train_step_9,df_X_train_step_10,df_X_train_step_11],axis=1,join='inner')\nX_test_master = pd.concat([df_X_test_step_1,df_X_test_step_2,df_X_test_step_3,df_X_test_step_4,df_X_test_step_5,df_X_test_step_6,df_X_test_step_7,df_X_test_step_8,df_X_test_step_9,df_X_test_step_10,df_X_test_step_11],axis=1,join='inner')","50a17a37":"X_train_master.head()","1d0dbea8":"# Scale the datasets before proceeding to classification tasks\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nscaler = StandardScaler()\nX_train_master_scaled = scaler.fit_transform(X_train_master)\nX_test_master_scaled = scaler.fit_transform(X_test_master)\n","8c1b4817":"# Y_train is not numeric yet - convert using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_labels_encoded = le.fit_transform(df_y_train['surface'])","60aaae40":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n# param_grid = {'n_estimators':[500],'max_depth':[5],'max_features':['log2']}\n\n# param_grid = {'n_estimators':[500]}\ngdbt = GradientBoostingClassifier()\n\n# gscv = GridSearchCV(estimator=gdbt,param_grid=param_grid,cv=5)\ngscv = GradientBoostingClassifier(n_estimators=500,max_depth=5,max_features='log2')\ngscv.fit(X_train_master_scaled,y_labels_encoded)","560f2ed2":"# print(\"Best score: {}\".format(gscv.best_score_))\n# print(\"Best Classifier: {}\".format(gscv.best_estimator_))","7c0696c7":"import csv\ndef write_to_csv(test_array,classifier,encoder,output_file_name=\"predictions.csv\"):\n    '''Writes a sample csv\n    test_array - numpy array; this needs to be sorted on series_id\n    classifier - result from cross vaidation analysis\n    encoder - label encoder used for labels'''\n    predictions = classifier.predict(test_array)\n    transformed_predictions = list((encoder.inverse_transform(predictions)))\n    series_id = list(range(len(transformed_predictions)))\n    df_ = pd.DataFrame({'series_id':series_id,'surface':transformed_predictions})\n    df_.to_csv(output_file_name,index=False)\n    return df_\n    \n# write_to_csv(X_test_master_scaled,gscv.best_estimator_,le)\nwrite_to_csv(X_test_master_scaled,gscv,le)","85e4ccdc":"# from sklearn.preprocessing import OneHotEncoder\n# surface_labels = np.array(df_y_train['surface']).reshape(-1,1)\n# ohe = OneHotEncoder(handle_unknown='ignore')\n# surface_labels_ohe = ohe.fit_transform(surface_labels).toarray()","5097802b":"# import tensorflow as tf\n\n# x = tf.placeholder(tf.float32,[None,X_train_master.shape[1]])\n\n# W1 = tf.Variable(tf.random.normal([X_train_master.shape[1],256]))\n# b1 = tf.Variable(tf.zeros([1,256]))\n# Z1 = tf.matmul(x,W1) + b1\n# sigma1 = tf.nn.relu(Z1)\n\n# W2 = tf.Variable(tf.random.normal([256,9]))\n# b2 = tf.Variable(tf.zeros([1,9]))\n# Z2 = tf.matmul(sigma1, W2) + b2\n# sigma2 = tf.nn.sigmoid(Z2)\n\n# y = tf.nn.softmax(sigma2)\n# y_ = tf.placeholder(tf.float32,[None,surface_labels_ohe.shape[1]])\n\n\n\n# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n# train_step = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cross_entropy)\n\n\n# init = tf.global_variables_initializer()\n\n# max_iter = 10000\n\n# with tf.Session() as sess:\n#     sess.run(init)\n#     iters = 0\n#     while iters < max_iter:\n# #         for i in range(X_train_master_scaled.shape[0]):\n# #         print(sess.run(W1,feed_dict={x:X_train_master_scaled[i,:].reshape(1,93),y_:surface_labels_ohe[i,:].reshape(1,9)}))\n#         sess.run(train_step,feed_dict={x:X_train_master_scaled.reshape(-1,93),y_:surface_labels_ohe.reshape(-1,9)})\n# #         print(sess.run(cross_entropy,feed_dict={x:X_train_master_scaled.reshape(-1,93),y_:surface_labels_ohe.reshape(-1,9)}))\n#         iters = iters + 1\n    \n#     prediction = tf.argmax(y,1)\n#     predicted_labels = prediction.eval(feed_dict={x: X_train_master_scaled.reshape(-1,93)})\n#     correct_prediction = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(y_,1)),tf.float32)) \/ 3810\n#     print(correct_prediction.eval(feed_dict={x:X_train_master_scaled.reshape(-1,93),y_:surface_labels_ohe.reshape(-1,9)}))\n    \n#     print(prediction.eval(feed_dict={x: X_train_master_scaled.reshape(-1,93)}))\n    \n# y = tf.nn.softmax(tf.matmul(x,W) + b)\n\n# y_ = tf.placeholder(tf.float32,[None,surface_labels_ohe.shape[1]])\n\n# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n# train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n# sess = tf.Session()\n# tf.global_variables_initializer().run(session=sess)\n\n# train_features = X_train_master[10000:]\n# train_labels = surface_labels_ohe[10000:]\n\n# for _ in range(1000):\n#   batch_xs, batch_ys = train_features, train_labels\n#   sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n    \n# sess.close()\n# # correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n","3a139e26":"correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\nprediction = tf.argmax(y,1)\npredicted_labels = prediction.eval(feed_dict={x: train_features})\n# print(prediction.eval(feed_dict={x: train_features}))\n","fbc38a48":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","cd567f3d":"w = tf.Variable([0],dtype=tf.float32)\nx = tf.placeholder(tf.float32,[3,1])\ncost = x[0][0] * w ** 2 + x[1][0] * w + x[2][0]\ncoeff = np.array([[1],[-10],[25]])\nW1 = tf.Variable(tf.random.normal([X_train_master.shape[1],128]))\n\ntrain = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(W1))","1c31f6d6":"## Try a GDBT\/RandomForest\nTrying on a GDBT before moving on to Neural Nets (Assuming Neural Nets are superior for this problem)\n\n","6e4bd3e9":"Replace orientaion_x\/y\/z\/w in the dataframe with euler angles in both training and test sets","de9dc1b1":"Series Ids need to be grouped by and channels that have sensor information from the physical work need to be aggregated. Following ideas for aggregating sensor values need to be tested\n1. angular velocity integration - this gives the angular distance traveled for the group - assume 1 millisecond delta for this calculation\n2. Mean of absolute of linear acceleration - as a proxy for acceleration and braking for the surface\n3. sum of absolute diffs of roll\/pitch\/yaw - Gives the total angle covered in each dimension - proxy to swiftness\n4. Angular_velocity diff means - as a proxy to velocity angular velocity changes, mostly for changes in direction\n5. Means of all columns\n6. Standard Devs of all columns\n7. Max of all columns\n8. Min\n9. Range\n10. Median\n11. Diff of Linear acceleration is a proxy for jerk - traaction and inertia are functions of jerk\n\n\n** In this run - not using cols_affected and letting transformations work on every channel **","f59541a3":"Imbalance is apparent - need to think about this later","d3b8dcb9":"## Softmax regression using tensor-flow\nIdea is to build a neural-net in tensor-flow and perform softmax regression. Therefore, one-hot encoding needs to be performed so that each of the surface types is assigned a bit on a 9-bit vector\n\nconcrete - [1 0 0 0 0 0 0 0 0] <br\/>\nsoft_pvc = [0 1 0 0 0 0 0 0 0] <br\/>\n.\n.\nso on\n\nStart with a 2 - layer 128 -> 9 hidden unit neural network\n","f2f20d88":"Check surface value counts","68f7b56b":"## Feature engineering (Need to add many more features)"}}