{"cell_type":{"24c82141":"code","4ea8a750":"code","d597e625":"code","62397357":"code","9d2dc6af":"code","1430430d":"code","ce366e67":"code","4d2f0542":"code","7d6ba9ee":"code","c745ace4":"code","57275aee":"code","344546d6":"code","489b7abd":"markdown","42df1786":"markdown","e99147d8":"markdown","c121724c":"markdown","4a118048":"markdown","ddec57cb":"markdown","8e022014":"markdown","4c931dfd":"markdown","884bd94b":"markdown"},"source":{"24c82141":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline\n\n#importing train_test_split\nfrom sklearn.model_selection import train_test_split","4ea8a750":"# Import the campaign dataset from Excel (Sheet 0 = Non Responders, Sheet 1 = Responders)\ndiabetes_df = pd.read_csv(\"..\/input\/diabetes.csv\")\ndiabetes_df.head()","d597e625":"#Examine Shape of Dataset\ndiabetes_df.shape","62397357":"#Examine Class Distribution\ndiabetes_df.Outcome.value_counts() \/ len(diabetes_df)","9d2dc6af":"# Create array to store our features and target variable\nX = diabetes_df.drop('Outcome',axis=1).values\ny = diabetes_df['Outcome'].values","1430430d":"# Apply Standard Scaler to our X dataset\nimport sklearn.preprocessing as preproc\nX_scaled = preproc.StandardScaler().fit_transform(X)\nX_scaled","ce366e67":"#Split our data into a train and test set\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=42, stratify=y)","4d2f0542":"# Import KNN Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create K values (1-10) & Create Arrays to store train\/test performance accuracy\nk = np.arange(1,50)\ntrain_accuracy = np.empty(len(k))\ntest_accuracy = np.empty(len(k))\n\nfor i,k in enumerate(k):\n    # Instantiate NN Classifier with K Neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    # Fit KNN model\n    knn.fit(X_train, y_train)\n    \n    # Evaluate train performance \n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    # Evaluate test performance\n    test_accuracy[i] = knn.score(X_test, y_test)","7d6ba9ee":"# Visualize Train\/Test Performance\nk = np.arange(1,50)\nplt.title('k-NN Varying number of neighbors')\nplt.plot(k, test_accuracy, label='Testing Accuracy')\nplt.plot(k, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('# K-Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","c745ace4":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\n\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X_scaled,y)","57275aee":"knn_cv.best_score_","344546d6":"knn_cv.best_params_","489b7abd":"**K-Nearest Neighbors** \n\n1. KNN is based on the principle of how people judge\nyou by observing your peers (neighbors)\n2. Makes predictions by:\na. Averaging - For Regression Tasks\nb. Majority Voting - For Classification Tasks\n\n**K-Nearest Neighbors - Two Important Steps**\n1. Choosing the right distance metric\n(e.g. for something like words, you may want to use cosine similarity because you're more interested\nin the direction of the word rather than the actual size of the values)\n2. Choosing the value of K\n\n**K-Nearest Neighbors - How It Works**\n\nThe algorithm uses feature \"similarity\" to predict new values of any new data\npoint\n1. Determine K (no. of nearest neighbors)\n2. Calculate distance (Euclidean,Manhattan,etc.)\n3. Determine K-minimum distance neighbors\n4. Make Prediction\na. For classification, the new data point is assigned to the majority class\nof the K neighbors.\nb. For regression, the new data point is assigned by the average of the K\nneighbors.","42df1786":"**Visualize the Train\/Test Report**","e99147d8":"**Create Seperate Arrays for IVs and DV**","c121724c":"**Import K-NN Classifier**\n\nLet's apply different values of K to evaluate which value should give us the best prediction performance\n\nWe will be using 50 different values of K (1-50)","4a118048":"**Examinen the Best K Value**","ddec57cb":"**Examine the Best Score**","8e022014":"**Scale the Data**\n\nApply Standard Scaling","4c931dfd":"**Train\/Test Split**","884bd94b":"**Apply GridSearchCV**\n\nIt's hard to visually see which value of K is best for our prediction accuracy.\n\nWe'll apply GridSearchCV where:\n\nFor each value of K, we will apply 5-Fold Cross Validation to it\nSpecifically:\n\nTry different values of K\n\nTrain\/Fit them all seperately\n\nEvaluate each of their performance\n\nSelect the best score"}}