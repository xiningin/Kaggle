{"cell_type":{"17939647":"code","a9fd499f":"code","6b2c8e43":"code","c1909094":"code","a114022f":"code","08dbfd69":"code","ad80f278":"code","357a9e67":"code","ba62a8d6":"code","e1228760":"code","cc81186e":"code","6c27bf75":"code","84beb67c":"code","674ea8e3":"code","8b7f6e45":"code","fa3e0531":"code","5fc5dda0":"code","e3506d90":"code","e29c71f2":"code","af4abe45":"code","cc75a805":"code","7b71416a":"code","e35c6ed0":"code","970126be":"code","17c9403d":"code","92d0be82":"code","a3cf19fa":"code","a7775994":"code","f730079f":"code","978bcc6b":"code","2d0b5ae5":"code","fdfc77e8":"code","916053bc":"code","5edc00bb":"code","d5356879":"code","13d5fee6":"code","25d99d4b":"code","51a4d746":"code","08d35285":"code","ed669151":"code","37c39266":"code","15a16f3d":"code","e78a0ec8":"code","32ca516b":"code","c661d0b4":"code","3df691d9":"code","70f7f4e7":"code","41afad90":"code","5b8b5b39":"code","a716c4ec":"code","d59dc4c7":"code","b3c08e0e":"code","642188aa":"code","a95b1e1c":"code","a97472fc":"code","f282206d":"code","70b10697":"code","20e12374":"code","ec756b50":"code","58c9f0c9":"code","bbdeff20":"code","0f652c8e":"code","2d5ae4c3":"code","df13aeb0":"code","22e628ec":"code","c1b7925b":"code","6b22ca9d":"code","f75e0bc1":"code","8ae7c717":"code","31e00b3e":"code","bb6e82d6":"code","00b8f958":"code","03104777":"code","775ec78f":"code","6bc8fb88":"code","b550c443":"code","f53e62ab":"code","2a003239":"markdown","78e9afb7":"markdown","af55ba93":"markdown","4dba27d6":"markdown","06ca7120":"markdown","78828833":"markdown","ba257133":"markdown","8b18f457":"markdown","c22ec26d":"markdown","bc6d8188":"markdown","c3d64abc":"markdown","8256fd52":"markdown","ceb01d59":"markdown","f070fe58":"markdown","e649c756":"markdown","cb212f62":"markdown","8f85b931":"markdown"},"source":{"17939647":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a9fd499f":"# Importing libraries\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\n# Function in order to flatten the JSON filds\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","6b2c8e43":"%%time\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")","c1909094":"train_df_copy = train_df","a114022f":"train_df_copy.head()","08dbfd69":"train_df_copy.info()","ad80f278":"#sns.countplot(train_df_copy['geoNetwork.city'])\n#plt.rcParams[\"figure.figsize\"] = (25, 10)\n\ntrain_df_copy['trafficSource.adwordsClickInfo.criteriaParameters'].value_counts()\n\n\n","357a9e67":"# Having a look at unique values of geoNetwork.city, geoNetwork.metro, geoNetwork.region, \n# and trafficSource.campaign and trafficSource.adwordsClickInfo.criteriaParameters\n# these fields have more than half missing values\n# Dropping these columns\n#train_df_copy['geoNetwork.city'].describe()\ntrain_df_copy.drop(['trafficSource.adwordsClickInfo.criteriaParameters'],axis=1,inplace=True)\ntest_df.drop(['trafficSource.adwordsClickInfo.criteriaParameters'],axis=1,inplace=True)\n\n","ba62a8d6":"train_df_copy.drop(['fullVisitorId','sessionId',\n                    'visitId'],axis=1,inplace=True)\ntest_df.drop(['sessionId', 'visitId'],axis=1,inplace=True)","e1228760":"train_df_copy['trafficSource.adwordsClickInfo.gclId'].describe()","cc81186e":"train_df_copy.drop(['trafficSource.adwordsClickInfo.gclId'],axis=1,inplace=True)\ntest_df.drop(['trafficSource.adwordsClickInfo.gclId'],axis=1,inplace=True)","6c27bf75":"for col in ['totals.bounces','totals.newVisits','totals.visits','totals.transactionRevenue']:\n    train_df_copy[col].fillna(0,inplace = True)\ntrain_df_copy['trafficSource.isTrueDirect'].fillna(False,inplace=True)    \n\n\nfor col in ['totals.bounces','totals.newVisits','totals.visits']:\n    test_df[col].fillna(0,inplace = True)\ntest_df['trafficSource.isTrueDirect'].fillna(False,inplace=True)","84beb67c":"# Check missing values\ntrain_df_copy.isnull().sum().sort_values()","674ea8e3":"train_df_copy['totals.pageviews'].describe()\ntrain_df_copy['trafficSource.adwordsClickInfo.page'].describe()\n\n# We fill in the missing values with the most frequent occurence of these columns --> 1\ntrain_df_copy['totals.pageviews'].fillna(1, inplace=True)\ntrain_df_copy['trafficSource.adwordsClickInfo.page'].fillna(1, inplace=True)\n\ntest_df['totals.pageviews'].fillna(1, inplace=True)\ntest_df['trafficSource.adwordsClickInfo.page'].fillna(1, inplace=True)\n\n# Converting totals.pageviews and trafficSource.adwordsClickInfo.page from object to integer\ntrain_df_copy['totals.pageviews'] = train_df_copy['totals.pageviews'].astype(int)\ntrain_df_copy['trafficSource.adwordsClickInfo.page'] = train_df_copy['trafficSource.adwordsClickInfo.page'].astype(int)\n\ntest_df['totals.pageviews'] = test_df['totals.pageviews'].astype(int)\ntest_df['trafficSource.adwordsClickInfo.page'] = test_df['trafficSource.adwordsClickInfo.page'].astype(int)\n","8b7f6e45":"#trafficSource.campaignCode    \n# Only one non-missing. Drop the column\ntrain_df_copy.drop('trafficSource.campaignCode', axis = 1, inplace = True)\n\n#test_df.drop('trafficSource.campaignCode', axis = 1, inplace = True)   #Doesnt exist in test data\n","fa3e0531":"train_df_copy['geoNetwork.city'].replace('not available in demo dataset',np.NaN,inplace=True)\ntrain_df_copy['geoNetwork.metro'].replace('not available in demo dataset',np.NaN, inplace=True)\ntrain_df_copy['geoNetwork.region'].replace('not available in demo dataset',np.NaN, inplace=True)\ntrain_df_copy['trafficSource.campaign'].replace('(not set)',np.NaN,inplace=True)\n\ntest_df['geoNetwork.city'].replace('not available in demo dataset',np.NaN,inplace=True)\ntest_df['geoNetwork.metro'].replace('not available in demo dataset',np.NaN, inplace=True)\ntest_df['geoNetwork.region'].replace('not available in demo dataset',np.NaN, inplace=True)\ntest_df['trafficSource.campaign'].replace('(not set)',np.NaN,inplace=True)\n\n#geoNetwork.city, geoNetwork.metro, geoNetwork.region, \n# and trafficSource.campaign","5fc5dda0":"## fillna for the other objects\nfor col in ['trafficSource.keyword',\n            'trafficSource.referralPath',\n            'trafficSource.adwordsClickInfo.adNetworkType',\n            'trafficSource.adwordsClickInfo.isVideoAd',\n            'trafficSource.adwordsClickInfo.slot',\n            'trafficSource.adContent',\n            'geoNetwork.city',\n            'geoNetwork.metro',\n            'geoNetwork.region',\n            'trafficSource.campaign'\n            ]:\n    \n    train_df_copy[col].fillna('unknown', inplace=True)\n    test_df[col].fillna('unknown', inplace=True)\n\n","e3506d90":"# drop constant columns\nconstant_column = [col for col in train_df_copy.columns if train_df_copy[col].nunique() == 1]\n\nprint('drop columns:', constant_column)\ntrain_df_copy.drop(constant_column, axis=1, inplace=True)\n\ntest_df.drop(constant_column, axis=1, inplace=True)","e29c71f2":"train_df_copy.info()","af4abe45":"# totals.bounces, totals.hits, totals.newVisits\ntrain_df_copy['totals.bounces'] = train_df_copy['totals.bounces'].astype(int)\ntrain_df_copy['totals.hits'] = train_df_copy['totals.hits'].astype(int)\ntrain_df_copy['totals.newVisits'] = train_df_copy['totals.newVisits'].astype(int)\n\ntrain_df_copy['trafficSource.adwordsClickInfo.isVideoAd'] = train_df_copy['trafficSource.adwordsClickInfo.isVideoAd'].astype(bool)\n\ntest_df['totals.bounces'] = test_df['totals.bounces'].astype(int)\ntest_df['totals.hits'] = test_df['totals.hits'].astype(int)\ntest_df['totals.newVisits'] = test_df['totals.newVisits'].astype(int)\n\ntest_df['trafficSource.adwordsClickInfo.isVideoAd'] = test_df['trafficSource.adwordsClickInfo.isVideoAd'].astype(bool)","cc75a805":"# Parsing the date. \nformat_str = '%Y%m%d'\ntrain_df_copy['formated_date'] = train_df_copy['date'].apply(lambda x: datetime.strptime(str(x), format_str))\n\ntrain_df_copy['day'] = train_df_copy['formated_date'].apply(lambda x:x.day)\ntrain_df_copy['weekday'] = train_df_copy['formated_date'].apply(lambda x:x.weekday())\n\ntrain_df_copy.drop(['formated_date','date'], axis=1, inplace=True)\n\n##TEST\ntest_df['formated_date'] = test_df['date'].apply(lambda x: datetime.strptime(str(x), format_str))\n\ntest_df['day'] = test_df['formated_date'].apply(lambda x:x.day)\ntest_df['weekday'] = test_df['formated_date'].apply(lambda x:x.weekday())\n\ntest_df.drop(['formated_date','date'], axis=1, inplace=True)\n\n\n","7b71416a":"train_df_copy['day'] = train_df_copy['day'].astype('category')\ntrain_df_copy['weekday'] = train_df_copy['weekday'].astype('category')\n\n##TEST\ntest_df['day'] = test_df['day'].astype('category')\ntest_df['weekday'] = test_df['weekday'].astype('category')","e35c6ed0":"train_df_copy.drop(['visitStartTime'], axis=1, inplace=True)\ntest_df.drop(['visitStartTime'], axis=1, inplace=True)\n","970126be":"train_df_copy.info()","17c9403d":"train_df_copy['totals.transactionRevenue'] = train_df_copy['totals.transactionRevenue'].astype(int)","92d0be82":"def mean_transactionRevenue_plot(feature):\n    ax = train_df_copy.groupby(feature)['totals.transactionRevenue'].mean().plot.bar()\n    ax.set_ylabel('mean transaction revenue')\n\n\n","a3cf19fa":"mean_transactionRevenue_plot('channelGrouping')","a7775994":"mean_transactionRevenue_plot('device.browser')","f730079f":"mean_transactionRevenue_plot('device.deviceCategory')","978bcc6b":"mean_transactionRevenue_plot('device.operatingSystem')","2d0b5ae5":"mean_transactionRevenue_plot('geoNetwork.continent')","fdfc77e8":"mean_transactionRevenue_plot('geoNetwork.country')","916053bc":"mean_transactionRevenue_plot('geoNetwork.subContinent')","5edc00bb":"train_df_copy.groupby('totals.bounces')['totals.transactionRevenue'].mean()","d5356879":"mean_transactionRevenue_plot('totals.bounces')","13d5fee6":"mean_transactionRevenue_plot('totals.hits')","25d99d4b":"mean_transactionRevenue_plot('totals.newVisits')","51a4d746":"mean_transactionRevenue_plot('totals.pageviews')","08d35285":"mean_transactionRevenue_plot('trafficSource.adContent')","ed669151":"train_df_copy['trafficSource.adwordsClickInfo.isVideoAd'].describe()\nmean_transactionRevenue_plot('trafficSource.adwordsClickInfo.isVideoAd')","37c39266":"train_df_copy['trafficSource.adwordsClickInfo.page'].describe()\nmean_transactionRevenue_plot('trafficSource.adwordsClickInfo.page')","15a16f3d":"train_df_copy['trafficSource.adwordsClickInfo.slot'].describe()\nmean_transactionRevenue_plot('trafficSource.adwordsClickInfo.slot')","e78a0ec8":"train_df_copy['trafficSource.isTrueDirect'].describe()\nmean_transactionRevenue_plot('trafficSource.isTrueDirect')","32ca516b":"train_df_copy['trafficSource.keyword'].describe()\ntrain_df_copy.drop('trafficSource.keyword',axis=1,inplace=True)\ntest_df.drop('trafficSource.keyword',axis=1,inplace=True)","c661d0b4":"train_df_copy['trafficSource.medium'].describe()\nmean_transactionRevenue_plot('trafficSource.medium')","3df691d9":"train_df_copy['trafficSource.referralPath'].value_counts()\n#train_df_copy.drop('trafficSource.referralPath',axis=1,inplace=True)\n#test_df.drop('trafficSource.referralPath',axis=1,inplace=True)","70f7f4e7":"train_df_copy['trafficSource.source'].describe()\nmean_transactionRevenue_plot('trafficSource.source')","41afad90":"mean_transactionRevenue_plot('weekday')","5b8b5b39":"mean_transactionRevenue_plot('geoNetwork.continent')","a716c4ec":"mean_transactionRevenue_plot('geoNetwork.country')","d59dc4c7":"train_df_copy.info()","b3c08e0e":"# %% Categorical columns\n# List of categorical columns to recode\ncatCols = ['channelGrouping', 'device.browser','device.deviceCategory',\n           'device.operatingSystem', 'geoNetwork.continent','geoNetwork.country',\n           'geoNetwork.networkDomain',\n           'geoNetwork.subContinent', 'trafficSource.adContent', \n           'trafficSource.adwordsClickInfo.adNetworkType',\n           'trafficSource.adwordsClickInfo.slot',\n           'trafficSource.source','trafficSource.medium',\n           'day','weekday', 'trafficSource.referralPath',\n           'geoNetwork.city', \n           'geoNetwork.metro', \n           'geoNetwork.region', \n           'trafficSource.campaign'\n          ]\n\n# Recode\nfor c in catCols:\n    # Convert column to pd.Categotical\n    train_df_copy[c] = pd.Categorical(train_df_copy[c])\n    test_df[c] = pd.Categorical(test_df[c])\n\n    # Extract the cat.codes and replace the column with these\n    train_df_copy[c] = train_df_copy[c].cat.codes\n    test_df[c] = test_df[c].cat.codes\n\n    # Convert the cat codes to categotical...\n    train_df_copy[c] = pd.Categorical(train_df_copy[c])\n    test_df[c] = pd.Categorical(test_df[c])\n\n\n\n# Generate a logical index of categorical columns to maybe use with LightGBM later\ncatCols = [i for i,v in enumerate(train_df_copy.dtypes) if str(v)=='category']","642188aa":"#train_df_copy.info()","a95b1e1c":"train_df_copy.info()","a97472fc":"#%% Prepare data\ndef prepLGB(data,\n            classCol='',\n            IDCol='',\n            fDrop=[]):\n\n        # Drop class column\n        if classCol != '':\n            labels = data[classCol]\n            fDrop = fDrop + [classCol]\n        else:\n            labels = []\n\n        if IDCol != '':\n            IDs = data[IDCol]\n        else:\n            IDs = []\n\n        if fDrop != []:\n           data = data.drop(fDrop,\n                            axis=1)\n\n        # Create LGB mats\n        lData = lgb.Dataset(data, label=labels,\n                            free_raw_data=False,\n                            feature_name=list(data.columns),\n                            categorical_feature='auto')\n\n        return lData, labels, IDs, data\n","f282206d":"train_df_copy['totals.transactionRevenue'] = np.log1p(train_df_copy['totals.transactionRevenue'])","70b10697":"import lightgbm as lgb\n#reg = lgb.train(params, d_train, 100)\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\nfrom sklearn.metrics import make_scorer, mean_squared_error,roc_auc_score, roc_curve\n\nfrom sklearn.pipeline import Pipeline\nimport numpy as np","20e12374":"test_df.head()","ec756b50":"# Split training data in to training and validation sets.\n# Validation set is used for early stopping.\ntrainData, validData = train_test_split(train_df_copy,\n                                        test_size=0.3)\n\n# Prepare the data sets\ntrainDataL, trainLabels, trainIDs, trainData = prepLGB(trainData,\n                                                 classCol='totals.transactionRevenue',\n                                                 IDCol='',\n                                                 fDrop=[])\n\nvalidDataL, validLabels, validIDs, validData = prepLGB(validData,\n                                                 classCol='totals.transactionRevenue',\n                                                 IDCol='',\n                                                 fDrop=[])\n\ntestDataL, _, _ , testData = prepLGB(test_df,\n                                  classCol='',\n                                     IDCol='fullVisitorId',\n                                     fDrop=[])\n","58c9f0c9":"# Prepare data set using all the training data\nallTrainDataL, allTrainLabels, _ , allTrainData = prepLGB(train_df_copy,\n                                                 classCol='totals.transactionRevenue',\n                                                 IDCol='',\n                                                 fDrop=[])","bbdeff20":"#inspecting the target variable\nsns.kdeplot(np.log(train_df_copy['totals.transactionRevenue']))","0f652c8e":"#y = np.log1p(train_df_copy['totals.transactionRevenue'])\n#X = train_df_copy.drop('totals.transactionRevenue',axis=1)","2d5ae4c3":"#X.info()","df13aeb0":"#https:\/\/www.kaggle.com\/garethjns\/microsoft-lightgbm-with-parameter-tuning-0-823\n#https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html?highlight=fit","22e628ec":"#grouped_test = test_df[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\n#grouped_test.to_csv('submit.csv',index=False)","c1b7925b":"lgbm_model.get_params().keys()\n","6b22ca9d":"import lightgbm as lgb\n#reg = lgb.train(params, d_train, 100)\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\nfrom sklearn.metrics import make_scorer, mean_squared_error,roc_auc_score, roc_curve\n\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n    \n    #Hiper parameters to explore\nparams = {\"learning_rate\":np.arange(0.1,0.2,0.05),\n              \"metric\": ['rmse'],\n              \"num_leaves\": [70,80],\n              \"min_data_in_leaf\": [100,1000],\n              \"max_depth\": [7],\n              'objective': ['regression'],\n              'boosting': ['gbdt'],\n          'bagging_fraction': [0.7,0.8],\n        'bagging_freq': [1,3,5],\n        'feature_fraction': [0.5,0.7],\n        'max_bin': [128, 255],\n          \"min_child_samples\" : [100]\n             \n                  }\n                \n#lgbm_model = lgb.train()\nlgbm_model = lgb.LGBMRegressor(boosting_type= 'gbdt',\n          objective = 'regression',\n          verbose = 5,\n          max_depth = params['max_depth'],\n          learning_rate = params['learning_rate'],\n          metric = ['rmse'],\n                               num_leaves = params['num_leaves'],\n                               min_data_in_leaf = params['min_data_in_leaf'],\n                               bagging_fraction= params['bagging_fraction'],\n        bagging_freq= params['bagging_freq'],\n                               feature_fraction= params['feature_fraction'],\n                               max_bin= params['max_bin'],\n                               min_child_samples = params['min_child_samples']\n                                         \n                              ) #Regressor\n\n# To view the default model params:\n\nlgbm_model.get_params().keys()\n\ngrid = RandomizedSearchCV(lgbm_model, params, cv=5, n_jobs=4, verbose = 8)\ngrid.fit(allTrainData, allTrainLabels)\n\n\n","f75e0bc1":"print(grid.best_params_)\nprint(grid.best_score_)","8ae7c717":"# Using parameters already set above, replace in the best from the grid search\nparams['max_depth'] = grid.best_params_['max_depth']\nparams['learning_rate'] = grid.best_params_['learning_rate']\nparams['num_leaves'] = grid.best_params_['num_leaves']\nparams['min_data_in_leaf'] = grid.best_params_['min_data_in_leaf']\n\nparams['min_child_samples'] = grid.best_params_['min_child_samples']\n\nparams['bagging_fraction'] = grid.best_params_['bagging_fraction']\nparams['bagging_freq'] = grid.best_params_['bagging_freq']\nparams['feature_fraction'] = grid.best_params_['feature_fraction']\nparams['max_bin'] = grid.best_params_['max_bin']\n\n\nprint('Fitting with params: ')\nprint(params)","31e00b3e":"k = 4\n\nfor i in range(0, k):\n    print('Fitting model', k)\n\n    # Prepare the data set for fold\n    trainData, validData = train_test_split(train_df_copy,\n                                            test_size=0.4)\n    trainDataL, trainLabels, trainIDs, trainData = prepLGB(trainData,\n                                                     classCol='totals.transactionRevenue',\n                                                     IDCol='',\n                                                     fDrop=[])\n    validDataL, validLabels, validIDs, validData = prepLGB(validData,\n                                                     classCol='totals.transactionRevenue',\n                                                     IDCol='',\n                                                     fDrop=[])\n    # Train\n    gbm = lgb.train(params,\n                    trainDataL,\n                    100000,\n                    valid_sets=[trainDataL, validDataL],\n                    early_stopping_rounds=50,\n                    verbose_eval=4)\n\n                    #valid_sets=[trainDataL, validDataL],\n","bb6e82d6":"lgb.plot_importance(gbm,height=0.5,figsize=(10,10))\nplt.show()","00b8f958":"testData.head()","03104777":"predsValid = 0\npredsTrain = 0\npredsTest = 0\n\npredsValid += gbm.predict(validData,num_iteration=gbm.best_iteration)\/k\npredsTrain += gbm.predict(trainData,num_iteration=gbm.best_iteration)\/k\npredsTest += gbm.predict(testData.drop('fullVisitorId',axis=1),num_iteration=gbm.best_iteration)\/k\n","775ec78f":"#preds_Test = pd.concat(testData['fullVisitorId'],predsTest,axis=1)\n","6bc8fb88":"predsTest[predsTest<0] = 0\nsub = pd.DataFrame()\nsub[\"PredictedLogRevenue\"] = np.expm1(predsTest)\nsub['fullVisitorId'] = testData['fullVisitorId']\n\nsub = sub.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub[\"PredictedLogRevenue\"] = np.log1p(sub[\"PredictedLogRevenue\"])\nsub.to_csv(\"baseline_lgb1.csv\", index=False)","b550c443":"sub.head(100)","f53e62ab":"sub.head(100)","2a003239":"Investigating which features might have correlations with the transactionRevenue","78e9afb7":"In order to convert all the json fields in the file to a flattened csv format we refer to the code from [this kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook) from[ Julian](https:\/\/www.kaggle.com\/julian3833).","af55ba93":"Now that we have a clean and not-missing dataframe, we start thinking about the date and visitStartTime columns.\n\n**Date**: I only keep the day and the weekday features from the date column because in the test set, we would have different months and different year, so including them in the train set will only make the model overfit\n\n**VisitStartTime**: For now I remove this column because it is all in UTC time (time of the main google server) and it is only useful if we can combine it with the geographical information in order to see this VisitStartTime in local time. ","4dba27d6":"**'pageviews'** and **'trafficSource.adwordsClickInfo.page'**","06ca7120":"We can start training the model","78828833":"LightGBM offers good accuracy with integer-encoded categorical features. LightGBM applies Fisher (1958) to find the optimal split over categories as described here. This often performs better than one-hot encoding.\nUse categorical_feature to specify the categorical features. Refer to the parameter categorical_feature in Parameters.\nCategorical features must be encoded as non-negative integers (int) less than Int32.MaxValue (2147483647). It is best to use a contiguous range of integers started from zero.\nIt is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy.\n\nInstead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. ","ba257133":"Let's **decide** for all the preprocessing (missing value imputation, etc) based on **only the train data** and then apply the preprocessing to the test data. if we concatenate the train and test data and decide, then our decisions will be very optimistic because they are also based on the test data that we are not allowed to see. ","8b18f457":"Converting features that are objects but should be integers","c22ec26d":"Based on the extensive column descriptions on https:\/\/support.google.com\/analytics\/answer\/3437719?hl=en, we can see that for some columns, a NULL value actually means something. Here I will write the list of these features together with their meaning:\n\n**totals.bounces** (Integer):Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null.\n\n**totals.newVisits** (Integer):Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null.\n\n**totals.visits** (Integer): The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session.\n\n**TrafficSource.isTrueDirect** (Boolean):True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL.\n\nSo I am going to keep the NULL values for these fields and recode them.\n\n\n    ","bc6d8188":"My sources for LBGM:\nhttps:\/\/www.kaggle.com\/garethjns\/microsoft-lightgbm-with-parameter-tuning-0-823\nhttps:\/\/www.kaggle.com\/ievgenvp\/lgbm-custom-randomizedsearchcv-lb-283\nhttps:\/\/www.kaggle.com\/vinnsvinay\/introduction-to-boosting-using-lgbm-lb-0-68357\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-ga-customer-revenue\nhttps:\/\/www.kaggle.com\/karkun\/sergey-ivanov-msu-mmp\nhttps:\/\/www.kaggle.com\/ogrellier\/i-have-seen-the-future\nhttps:\/\/www.kaggle.com\/plasticgrammer\/customer-revenue-prediction-playground","c3d64abc":"Trying the simplest model","8256fd52":" \"categorical_feature\"=name: ['channelGrouping','device.browser','device.deviceCategory',\n                                     'device.operatingSystem','geoNetwork.continent',\n                                     'geoNetwork.country','geoNetwork.subContinent',\n                                      'trafficSource.adContent',\n                                      'trafficSource.adwordsClickInfo.adNetworkType',\n                                      'trafficSource.medium', 'trafficSource.source',\n                                      'day', 'weekday'\n                                     ]","ceb01d59":"**Objective of the competition**: In this competition, we\u2019re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. \n\n**Data Description** \nWe are given two datasets; train.csv and test.csv.\n\nEach row in the dataset is one visit to the store. We are predicting the natural log of the sum of all transactions per user.\n\nThe data fields are:\n\n*fullVisitorId*- A unique identifier for each user of the Google Merchandise Store.\n\n*channelGrouping* - The channel via which the user came to the Store.\n\n*date* - The date on which the user visited the Store.\n\n*device* - The specifications for the device used to access the Store. \n\n*geoNetwork* - This section contains information about the geography of the user.\n\n*sessionId* - A unique identifier for this visit to the store.\n\n*socialEngagementType* - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n\n*totals* - This section contains aggregate values across the session.\n\n*trafficSource* - This section contains information about the Traffic Source from which the session originated.\n\n*visitId* - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the \nuser. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n\n*visitNumber* - The session number for this user. If this is the first session, then this is set to 1.\n\n*visitStartTime* - The timestamp (expressed as POSIX time).\n\nP.s: It is important to note that some of the fields are in json format ('device', 'geoNetwork', 'totals', 'trafficSource')\nP.s: There is more information on the columns in this link: https:\/\/support.google.com\/analytics\/answer\/3437719?hl=en\n\n\n","f070fe58":"Investigating other missing values:","e649c756":"Let's now get all Dummy variables","cb212f62":"Remove the highly correlated features:\n\ntotals.newVisits and trafficSource.isTrueDirect --> drop totals.newVisits\n\ntrafficSource.adwordsClickInfo.isVideoAd and trafficSource.adwordsClickInfo.slot  --> drop both\n\ntotals.pageviews and totals.hits --> drop totals.hits\n","8f85b931":"Good EXplanation on LGBM: https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc"}}