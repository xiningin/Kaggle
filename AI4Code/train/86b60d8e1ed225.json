{"cell_type":{"f24eea38":"code","be1f94d7":"code","f493d75c":"code","272cbefe":"code","f65b475f":"code","b7a56841":"code","6c3222d7":"code","c8b7bc26":"code","96de03b1":"code","75f7de9f":"code","b5a808ee":"code","1823ff5a":"code","9c9434b7":"code","58f19f1c":"code","d8ea2536":"code","5611d175":"markdown","6dbee135":"markdown","6b15e7b0":"markdown","7bf92cac":"markdown","d9b31e89":"markdown","f61f0282":"markdown","b8e69fdb":"markdown","97db9a36":"markdown","69e9e1b7":"markdown","dee7b6e9":"markdown","092e0069":"markdown","80c70d04":"markdown","4c922a32":"markdown"},"source":{"f24eea38":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.manifold import Isomap \nnltk.download('vader_lexicon')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.spatial import distance_matrix\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.cluster import KMeans,DBSCAN\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rc('figure',figsize=(17,13))","be1f94d7":"f_data = pd.read_csv('\/kaggle\/input\/notebooks-of-the-week-hidden-gems\/kaggle_hidden_gems.csv')\nf_data = f_data.rename(columns={'review':'text'})\nf_data.head(3)","f493d75c":"#Remove twitter handlers\nf_data.text = f_data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n#remove hashtags\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n\n\n# Remove URLS\nf_data.text = f_data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\nf_data.text = f_data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","272cbefe":"#Extracting Text Sentiments Using the \"VADER\" lexicon\nsid = SIA()\nf_data['sentiments']           = f_data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nf_data['Positive Sentiment']   = f_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nf_data['Neutral Sentiment']    = f_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nf_data['Negative Sentiment']   = f_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nf_data.drop(columns=['sentiments'],inplace=True)\nhg_data = f_data[['date','text','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].copy()\n\n#Number of Words\nhg_data['Number_Of_Words'] = hg_data.text.apply(lambda x:len(x.split(' ')))\n#Average Word Length\nhg_data['Mean_Word_Length'] = hg_data.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )\n\n","f65b475f":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Lyrics',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],label='Negative Sentiment')\nsns.kdeplot(f_data['Neutral Sentiment'] ,label='Neutral Sentiment' ,color='orange' )\nsns.kdeplot(f_data['Positive Sentiment'],label='Positive Sentiment',color='tab:red')\nplt.legend()\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Lyrics',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],cumulative=True,label='Negative Sentiment')\nsns.kdeplot(f_data['Neutral Sentiment'],cumulative=True,label='Neutral Sentiment' ,color='orange' )\nsns.kdeplot(f_data['Positive Sentiment'],cumulative=True ,label='Positive Sentiment',color='tab:red')\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.legend()\nplt.show()","b7a56841":"\nlyr = hg_data.text.copy()\n#lyr = lyr.apply(lambda x:x.replace(',',''))\n\n\nCV = CountVectorizer(stop_words='english',ngram_range=(1,1))\ncv = CV.fit_transform(lyr)\ncv_df = pd.DataFrame(cv.toarray(),columns=CV.vocabulary_,index=hg_data.date)\nDM = pairwise_distances(cv_df,cv_df,metric='manhattan')\nsns.clustermap(DM,cmap='vlag')","6c3222d7":"NUMBER_OF_COMPONENTS = 100\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = lyr\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in STOPWORDS]))\n\nC_vector = cv\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Tweet Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","c8b7bc26":"best_fearures = [[CV.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:NUMBER_OF_COMPONENTS])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 1\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable For X% of Variance<b>\",'<b>Appeared On X Reviews<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","96de03b1":"NUMBER_OF_COMPONENTS = 2\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\nisomap = Isomap(NUMBER_OF_COMPONENTS)\n\ntext_data = lyr\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in STOPWORDS]))\n\nC_vector = cv\n\npc_matrix = isomap.fit_transform(C_vector)\n\ndec_df = hg_data.copy()\ndec_df = dec_df.assign(pc_1 = pc_matrix[:,0],pc_2 = pc_matrix[:,1])\nex.scatter(dec_df,x='pc_1',y='pc_2',title=r'Martins Comments Projected From R^1124 --> R^2')\n","75f7de9f":"dbscan = DBSCAN(min_samples=3,eps=0.8)\ndbscan.fit(dec_df[['pc_1','pc_2']])\ndec_df = dec_df.assign(cluster=dbscan.labels_)\nfig = ex.scatter(dec_df,x='pc_1',y='pc_2',title=r'Martins Comments Clustered in R^2',color='cluster')\nfig.update_layout(hovermode='y')\n","b5a808ee":"wordclouds = []\nplt.figure(figsize=(20,10))\nfor i in range(0,np.max(dec_df.cluster)+1):\n    cluster = dec_df.query(f'cluster=={i}')\n    WC = WordCloud(background_color='white',width=500,height=500).generate(' '.join(cluster.text))\n    wordclouds.append(WC)\n    \nfor i in range(0,np.max(dec_df.cluster)+1):\n    plt.subplot(2,np.max(dec_df.cluster)\/\/2+1,i+1)\n    plt.imshow(wordclouds[i])\n    plt.axis('off')\n    plt.title(f'Most Common Words in Cluster : {i}',fontsize=12)\n    \nplt.tight_layout()","1823ff5a":"dec_df","9c9434b7":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as grid_spec\nfrom sklearn.neighbors import KernelDensity\n\ncountries = [x for x in range(0,11)]\ncolors = ['#0000ff', '#3300cc', '#660099', '#990066', '#cc0033', '#ff0000','#612B1F','#9272CA','#8DB23F','#BB3C76','#645FB3']\n#dec_df[dec_df.cluster >= 0]\ngs = grid_spec.GridSpec(len(countries),1)\nfig = plt.figure(figsize=(16,9))\n\ni = 0\n\nax_objs = []\nfor cluster in range(0,11):\n    x = np.array(dec_df[dec_df.cluster >= 0][dec_df[dec_df.cluster >= 0].cluster == cluster]['Positive Sentiment'])\n    x_d = np.linspace(0,1, 1000)\n\n    kde = KernelDensity(bandwidth=0.03, kernel='gaussian')\n    kde.fit(x[:, None])\n\n    logprob = kde.score_samples(x_d[:, None])\n\n    # creating new axes object\n    ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))\n\n    # plotting the distribution\n    ax_objs[-1].plot(x_d, np.exp(logprob),color=\"#f0f0f0\",lw=1)\n    ax_objs[-1].fill_between(x_d, np.exp(logprob), alpha=1,color=colors[i])\n\n    ax_objs[-1].set_xlim(0,1)\n    #ax_objs[-1].set_ylim(0,2.5)\n\n    # make background transparent\n    rect = ax_objs[-1].patch\n    rect.set_alpha(0)\n\n    # remove borders, axis ticks, and labels\n    ax_objs[-1].set_yticklabels([])\n\n    if i == len(countries)-1:\n        ax_objs[-1].set_xlabel(\"Positive Sentiment\", fontsize=16,fontweight=\"bold\")\n    else:\n        ax_objs[-1].set_xticklabels([])\n\n    spines = [\"top\",\"right\",\"left\",\"bottom\"]\n    for s in spines:\n        ax_objs[-1].spines[s].set_visible(False)\n\n    #adj_country = country.replace(\" \",\"\\n\")\n    ax_objs[-1].text(-0.02,0,f'Cluster: {cluster}',fontweight=\"bold\",fontsize=14,ha=\"right\")\n\n\n    i += 1\n\ngs.update(hspace=-0.6)\n\nfig.text(0.07,0.85,\"Distribution Positive Sentiment in Each Cluster\",fontsize=20)\n\nplt.grid(False)\nplt.tight_layout()\nplt.show()","58f19f1c":"mean_df = hg_data.groupby('date').mean()\nb_date_mean = mean_df.reset_index()\n\n\nfig = go.Figure()\n\nfor column in ['Mean_Word_Length','Number_Of_Words','Positive Sentiment','Negative Sentiment']:\n    fig.add_trace(\n        go.Scatter(\n            x = b_date_mean.date,\n            y = b_date_mean[column],\n            name = column,\n            mode='lines'\n        )\n    )\n    \n\nbtns = []\nfor x,col in enumerate(['Mean_Word_Length','Number_Of_Words','Positive Sentiment','Negative Sentiment']):\n    bol = [False]*12\n    bol[x]=True\n    d = dict(label = col,\n                  method = 'update',\n                  args = [{'visible':bol},\n                          {'title': 'Distribution of [' +col+'] Over Our Timeline',\n                           'showlegend':True}])\n    btns.append(d)\n    \n    \nfig.update_layout(title='How Different Text Attributes Change Over The History of \"Hidden Gems\"',\n    updatemenus=[go.layout.Updatemenu(\n        active=0,\n        showactive=True,\n        buttons=btns\n        )\n    ])\n\nfig.update_xaxes(title_text='Date')\nfig.update_yaxes(title_text='Post Mean Value')\n\nfig.show()","d8ea2536":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=4, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean.date, y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean.date, y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean.date, y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean.date, y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","5611d175":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Preprocessing and Feature Engineering<\/h1>\n","6dbee135":"Observing the ridge plot of the positive sentiment with each cluster, we see that there is no clear patterns and that the positive sentiment in Martins description is not connected to any topic; a possible alternative hypothesis would be that in the case for example if Martin preferred NLP over any other topic, then he would post more NLP related notebooks and make somewhat more positive statements about those notebooks this is in contrast to what we actually see, and that is usually a multimodal distribution in each of our clusters.","6b15e7b0":"Treating the positive sentiment of Martin's descriptions as our signal and decomposing it into three components (trend, seasonal and residual) based on a period value of 4 (each month), we see that indeed there was somewhat a trend on a decline in July of 2020 and hitting is minima at December of 2020 after which an incline is observed, \nI wonder what happened in those months that show such underlying behavior.\n\nIt could be an interesting experiment to predict the positive sentiments of future hidden gems descriptions, but due to the non-stationary nature of the sentiments, this is not possible.\n","7bf92cac":"**The above plot answer the question, how much variation is actually in Martins's descriptions?**\nWe already observed that based on the L1 norm, there are some visible similarities, but how many words are required to explain at least 80% of the variance in Martin descriptions, apparently 100 terms due to an excellent job.","d9b31e89":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraires And Utilities<\/h1>\n","f61f0282":"Knowing that there are descriptions that tend to be similar and understanding that most of the descriptions variance is centered in only 100 words, the next question that naturally arises is what descriptions can be clustered together and the properties of such clusters.\n\nFirst, we take a quick look at a low dimensionality representation of the descriptions. We can clearly see that there some fairly dense clusters with a small number of descriptions, but overall, there is a large spread which supports our hypothesis that the similarity is caused by something minor like similar topics and not due to Martins's writing habits.","b8e69fdb":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","97db9a36":"It's pretty interesting to observe and see the patterns emerging in the word clouds of each cluster's descriptions.\nFor example, cluster 4 is centered around NLP and classifications compared to cluster 0, which deals mainly with analysis and visuals.","69e9e1b7":"**Observation**: looking at the distribution of sentiments in Martins description of each notebook in his hidden gems posts, we see that the distribution of the negative sentiment is centered around zero with a very low standard deviation (can be seen with the very tall and tight bell around zero) along with to very wide distributions for the positive and neutral sentiments.\nWe can learn that Martin usually avoids making negative comments about the \"Gems\" he chooses to post, but at the same time, the deviation in the neutral and positive sentiments may indicate that Martin has a certain pattern of descriptions that tend to be skewed towards some extent.","dee7b6e9":"We use DBSCAN to extract the cluster formed by descriptions that are located close enough together in the reduced dimension.","092e0069":"The above interactive plots let us observe how different attributes regarding Martin's descriptions change over the lifetime of the \"hidden gems\" posts.\nLooking closely at positive sentiment and its deviation through time, it seems there was a slight decline and another incline around December of 2020.\n\n","80c70d04":"**Closing Note**: I personally like to thank Martin for bringing to light quality notebooks every week, helping new members of this amazing community get their hard word noticed, getting feedback and reviews that help them grow in the vast field known as data science.","4c922a32":"**The question** I asked was how similar are Marin's description of the notebooks he posts; after following Martin's hidden gems for a while now, it is clear that he tries to point out the hidden value in each notebook, but how different are his description?\n\nTo **answer** the question, all the descriptions written by martin were vectorized, and the L1 norm was calculated between each pair of reviews.\n\nLooking closely at the clustered heatmap, we can clearly see the reviews that resemble each other and the entirely different ones.\nSuch resemblance between descriptions may be due to the resemblance between some of the ideas in the notebook posted by Martin.\n"}}