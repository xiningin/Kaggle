{"cell_type":{"a24d40f8":"code","f2f9b68c":"code","cd208bc1":"code","f037da6f":"code","605cc8b4":"code","2c011267":"markdown","75e2d267":"markdown","7d2193e7":"markdown","f4e2d7b8":"markdown","3a97acc4":"markdown","6ddaa201":"markdown","346765bb":"markdown","0cf0732b":"markdown"},"source":{"a24d40f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom math import sqrt\nimport seaborn as sns\n\nimport math\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n","f2f9b68c":"data = pd.read_csv('..\/input\/Data\/Stocks\/gs.us.txt', sep=',', header=0).fillna(0)\ndata.head()","cd208bc1":"train_data, test_data = data[0:int(len(data)*0.9)], data[int(len(data)*0.9):]\nplt.figure(figsize=(16,8))\nplt.grid(True)\nplt.xlabel('Dates')\nplt.ylabel('Open Prices')\nplt.plot(data['Open'], 'red', label='Train data')\nplt.plot(test_data['Open'], 'blue', label='Test data')\nplt.legend()\nplt.title('Opening price for Goldman Sachs')\nplt.show()\n\ndef shiftLbyn(arr, n=0):\n    return arr[n::] + arr[:n:]\n\ndef shiftRbyn(arr, n=0):\n    return arr[n:len(arr):] + arr[0:n:]\n#print(test_data['Open'].values.tolist())\n\n#print(test_data['Open'].corr(test_data['Open'],method='pearson'))\n\ntest_lag1=shiftLbyn(test_data['Open'].values.tolist(), 1)\ntest_lag2=shiftLbyn(test_data['Open'].values.tolist(), 2)\ntest_lag3=shiftLbyn(test_data['Open'].values.tolist(), 3)\n\ndf=pd.DataFrame()\ndf['data']=test_data['Open'].values.tolist()\ndf['test_lag1']=test_lag1\ndf['test_lag2']=test_lag2\ndf['test_lag3']=test_lag3\n\nprint(\"Lag 1 correlation: \"+str(df['data'].corr(df['test_lag1'])))\nprint(\"Lag 2 correlation: \"+str(df['data'].corr(df['test_lag2'])))\nprint(\"Lag 3 correlation: \"+str(df['data'].corr(df['test_lag3'])))\n\n\n##plot correlation with lags\ntr=train_data['Open'].values.tolist()\ndff=pd.DataFrame()\ndff['cor']=np.correlate(tr,tr,mode='full')\ndff['cor']=dff['cor'].apply(lambda x:x\/dff['cor'].max())\n\nplt.stem(dff['cor'][int((len(dff['cor'])+1)\/2):])\nplt.xlabel('lags')\nplt.ylabel('Correlation')\nplt.title('Correlation')\nplt.show()\n\n\n\nsns.pairplot(df)","f037da6f":"import time\nstart = time.time()\n\ntrain_arima = train_data['Open']\ntest_arima = test_data['Open']\n#print(test_arima)\nhistory = [x for x in train_arima]\ny = test_arima\n# make first prediction\npredictions = list()\nmodel = ARIMA(history, order=(1,1,0))\nmodel_fit = model.fit(disp=0)\nyhat = model_fit.forecast()[0]\npredictions.append(yhat)\nhistory.append(y.iloc[0])\nstart = time.time()\n# rolling forecasts\nfor i in range(1, len(y)):\n    # predict\n    model = ARIMA(history, order=(1,1,0))\n    model_fit = model.fit(disp=0)\n    yhat = model_fit.forecast()[0]\n    # invert transformed prediction\n    predictions.append(yhat)\n    # observation\n    obs = y.iloc[i]\n    history.append(obs)\n# report performance\nmse = mean_squared_error(y, predictions)\nprint('MSE: '+str(mse))\nmae = mean_absolute_error(y, predictions)\nprint('MAE: '+str(mae))\nrmse = math.sqrt(mean_squared_error(y, predictions))\nprint('RMSE: '+str(rmse))\n#import time\n\nend = time.time()\n\nelapsed = end - start\nprint(\"\\nTime elapsed:\" +str(elapsed)+\" s\")\n#print(\"\\nResult Summary: \")\n#print(model_fit.summary())\n\nplt.plot(y.values,label='test data')\nplt.plot(predictions,label='ARIMA predictions')\nplt.title('ARIMA model')\nplt.legend()\nplt.show()","605cc8b4":"#Kalman filter\n\nimport time\nstart = time.time()\n\ntest_dataa=test_arima.values\n\nA=[[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\nconst=0\nP_init=[[10**-7,0,0,0],[0,10**-7,0,0],[0,0,10**-7,0],[0,0,0,10**-7]]\nR=[[10**-7,0,0,0],[0,10**-7,0,0],[0,0,10**-7,0],[0,0,0,10**-7]]\nQ=[[10**-7,0,0,0],[0,10**-7,0,0],[0,0,10**-7,0],[0,0,0,10**-7]]\nKF=[]\nupdate=[]\nKF.append(test_dataa[0])\nKF.append(test_dataa[1])\nKF.append(test_dataa[2])\nKF.append(test_dataa[4])\nfor i in range(4,len(test_dataa)-4):\n    x_init=[[test_dataa[i-4]],[test_dataa[i-3]],[test_dataa[i-2]],[test_dataa[i-1]]]\n    #prediction\n    #print(i)\n    prediction=np.dot(A,x_init)+const\n    #print(x_min[1])\n    P_min=np.dot(np.dot(A,P_init),A)+Q\n    KF.append(prediction[3].tolist()[0])\n    #measurement update\n    y_min=prediction[3]\n    #print(y_min)\n    P_y_min=P_min+R\n    K_gain=np.dot(P_min,np.linalg.inv(P_y_min))[3][3]\n    #print(K_gain)\n    x_init=prediction-K_gain*(y_min-test_dataa[i])\n    update.append(x_init)\n    #x_init=np.array([])\n    #print(x_init)\n    P_init=P_min-K_gain*P_min\n#print(KF[0:10])   \n#print(test_dataa[0:10])\n#df['KF']=KF\n\nmse = mean_squared_error(KF, test_dataa[0:len(test_dataa)-4])\nprint('MSE: '+str(mse))\nmae = mean_absolute_error(KF, test_dataa[0:len(test_dataa)-4])\nprint('MAE: '+str(mae))\nrmse = math.sqrt(mean_squared_error(KF, test_dataa[0:len(test_dataa)-4]))\nprint('RMSE: '+str(rmse))\n\nend = time.time()\n\nelapsed = end - start\nprint(\"Time elapsed:\" +str(elapsed)+\" s\")\n\nplt.plot(test_dataa,label='test data')\nplt.plot(KF,'green',label='Kalman filter prediction')\nplt.title('Kalman filter')\nplt.legend()\nplt.show()\n\n#print(len(KF))\n#print(len(train_arima))\n\n","2c011267":"Lets load the libraries and packages now.","75e2d267":"## ARIMA vs. KALMAN filtering for modelling stock prices. ","7d2193e7":"We divided the data in the ratio of 90-10 as train:test dataset. The correlation plot shows that the correlation continues till the end of the data set. It intuitively indicated that the first data sample is somehow related to the last data sample as well. \nWe can observe the scatter plots telling the same story as they all are quite linear for all combinations of lags. we do see some outliers in the scatter plot that arise because of the sudden jump in the test data from 160 to 230. The same jump also results in the trench in the histograms above.\n\nAnyway, barring the outliers, the scatter plots and the correlation behavious indicates towards a typical autoregressive process where every sample is dependant on the previous sample as follows:\n\n$X(k)=X(k-1)+n_k$\nwhere $n_k$ represents a normal gaussian process. If the variance of this noise remains constant with time, it is called stationary process and non-stationary otherwise. Generally, the stock price data tends to be non-stationary.\n\nA commonly used method for time series is ARIMA model. This model converts a non-stationary process to stationary process by taking the differences of consecutive samples and training the co-efficients of these differences.\n\n$X(k)=c+\\phi_1(X(k-1)-X(k-2))+\\phi_2(X(k-2)-X(k-3))+..$\nwhere $c$ is a constant and $\\phi_1, \\phi_2....$ are coefficients trained by the ARIMA model. Once these parameters are trained, the test data set is feed forwarded through the ARIMA model for predictions.\n\nOn the other hand, a Kalman filter, being able to work with non-stationary process, tries to imitate the $n_k$ parameter and get the next sample's prediction.\n\n\n\nLets try the ARIMA model first.","f4e2d7b8":"We have seen many kernals using the autoregressive ARIMA model to model the Goldman Sachs Stock price prediction. Although this model work very well, the library access and training of the models take a considerable processing time.\n\nWe have tried two things here:\n1) enhance the accuracy of the model \n2) reduce the processing time\n\nTowards both these objectives, we choose to implement a Kalman filtering algorithm. The reason to implement this lies in the non-stationary nature of the stock prices. The ARIMA algorithm first changes the data to a stationary process by taking differences in consecutive samples as input data. In this process, we expect the ARIMA model to loose accuracy.\nIn addition, the Kalman filter is known to a simpler algorithm and we expect to get some benefits in processing time. \n\nWe shall see here that the Kalman filter performs similar to an ARIMA model with processing time of just $0.08$ seconds!! As the time latency is very important in stock market dealings, this improvement in processing time is a significant contribution.","3a97acc4":"Lets split the stock opening price for Goldman Sachs data in test and training set. We shall see the correlation properties of the data with its previous data. ","6ddaa201":"Alright, so lets compare the performances of the two algorithms.\n\n| Algorithm | RMSE | Processing Time | MSE | MAE |\n| --- | --- | --- |\n| ARIMA | 2.685970779042733| 19.115743398666382 s | 7.214439025871427 | 2.0280254139213065 |\n| Kalman filter | 2.639540096730153| 0.07828688621520996 s | 6.967171922246225 | 1.9818142548596116 |\n||||||\n\n\n\nBrilliant! Although we did not see a major benefit in RMSE performance, we definitely see a benefit in MSE and MAE. But the best part is the procesisng time. Where the ARIMA algorithm takes about 19 seconds to achieve this RMSE, we can achieve the same or even better in just 0.08 seconds with Kalman filter!! The benfit in processing time comes from the dynamic nature of Kalman filter and the fact that it avoids training of massive models. The simple formulation of the Kalman filter allows it to keep track of even a non-stationary process as demonstrated in this project. The caveat (ofcourse) is that the Kalman filter works for prediction of immediate neighboring samples as it requires the lates observations for updating the predictions. Unlike the ARIMA models, it cannot predict samples further down the line in future.\n\nHere I have kept the model in Kalman filter very simple. But you can play around with the matrix \"$A$\" and the constant \"const\" to see if it gives any benefits.","346765bb":"## Try Kalman filtering","0cf0732b":"## ARIMA model"}}