{"cell_type":{"b6197bb5":"code","5ee27bf7":"code","237b2bde":"code","f2015f36":"code","cf782b19":"code","d5a3ac13":"code","753a3d6d":"code","db69166f":"code","90824317":"code","be1180db":"code","31469d49":"code","1ebb3ebb":"code","bd960550":"code","cf58ef50":"code","b7b0ee38":"code","70eeefb1":"code","21acff4f":"code","e766df60":"code","0c812c45":"code","13cc7cbb":"code","97c9df50":"code","9d91ca4e":"code","b8470590":"markdown","219cca60":"markdown","52cdbd26":"markdown","8396bb99":"markdown","00e67b53":"markdown","6906d2ca":"markdown","048e1a51":"markdown","86c075c2":"markdown","e801d4af":"markdown","7d93469a":"markdown","81b6efd7":"markdown","4385e054":"markdown"},"source":{"b6197bb5":"!pip install beautifulsoup4","5ee27bf7":"import pandas as pd\nimport numpy as np\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom keras.models import Sequential, load_model\nfrom keras.layers import LSTM\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport pickle\nimport heapq","237b2bde":"def fetchHTML(url):\n    page = requests.get(url)\n    html = BeautifulSoup(page.content, 'html.parser')\n    return html","f2015f36":"def getSMS(sms_div):\n    malayalam_texts=[]\n    for div in sms_div:\n        malayalam_texts.append(div.find('p').text)\n    return malayalam_texts","cf782b19":"messages=[]\ncategories=['general','love-sms','friendship-sms','sms-jokes','birthday-wishes','christmas-sms']\nfor item in categories:\n    try:\n        soup=fetchHTML('https:\/\/www.prokerala.com\/greetings\/sms\/malayalam\/{}\/'.format(item))\n        last=int(soup.find(\"a\", {\"title\": \"Last Page\"}).text)\n        messages+=getSMS(soup.find_all(\"div\", {\"class\": \"content-wrapper msg-content\"}))\n        for i in range(2,last+1):\n            try:\n                soup=fetchHTML('https:\/\/www.prokerala.com\/greetings\/sms\/malayalam\/{}\/page-{}.html'.format(item,i))\n                messages+=getSMS(soup.find_all(\"div\", {\"class\": \"content-wrapper msg-content\"}))\n            except:\n                pass\n    except:\n        pass","d5a3ac13":"dataset=pd.DataFrame(messages,columns=['SMS'])\ndataset.to_csv('malayalam_dataset.csv',index=False)","753a3d6d":"def data_cleaning(data):\n    data=str(data)\n    cleaned_data=re.sub('[\\u200b0-9a-zA-Z\u00e2\u2764\u26a0\\xa0\\u200d\u201d\u2026]','',data)\n    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    cleaned_data=cleaned_data.translate(translator)\n    return cleaned_data","db69166f":"dataset=pd.read_csv('..\/input\/malayalam-dataset\/malayalam_dataset.csv')\ndataset.count()","90824317":"dataset.iloc[1,0]","be1180db":"data_cleaning(dataset.iloc[1,0])","31469d49":"word_list=[]\nfor sms in dataset.SMS:\n    temp=data_cleaning(sms).split()\n    if temp:\n        word_list+=temp\n        \nword_list[:10] #first 10 words in the list","1ebb3ebb":"unique_words = np.unique(word_list)\nunique_word_index = dict((c, i) for i, c in enumerate(unique_words))","bd960550":"word_length = 5\nprev_words = []\nnext_words = []\nfor i in range(len(word_list) - word_length):\n    prev_words.append(word_list[i:i + word_length])\n    next_words.append(word_list[i + word_length])\nprint(prev_words[0])\nprint(next_words[0])","cf58ef50":"X = np.zeros((len(prev_words), word_length, len(unique_words)), dtype=bool)\nY = np.zeros((len(next_words), len(unique_words)), dtype=bool)\nfor i, each_words in enumerate(prev_words):\n    for j, each_word in enumerate(each_words):\n        X[i, j, unique_word_index[each_word]] = 1\n    Y[i, unique_word_index[next_words[i]]] = 1","b7b0ee38":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(word_length, len(unique_words))))\nmodel.add(Dense(len(unique_words)))\nmodel.add(Activation('softmax'))","70eeefb1":"optimizer = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=5, shuffle=True).history","21acff4f":"model.save('keras_next_word_predictor.h5')\npickle.dump(history, open(\"history.p\", \"wb\"))\nmodel = load_model('keras_next_word_predictor.h5')\nhistory = pickle.load(open(\"history.p\", \"rb\"))","e766df60":"def prepare_input(text):\n    x = np.zeros((1, word_length, len(unique_words)))\n    for t, word in enumerate(text.split()):\n        print(word)\n        x[0, t, unique_word_index[word]] = 1\n    return x","0c812c45":"def sample(preds, top_n=3):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds)\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n\n    return heapq.nlargest(top_n, range(len(preds)), preds.take)","13cc7cbb":"def predict_completions(text, n=3):\n    if text == \"\":\n        return(\"0\")\n    x = prepare_input(text)\n    preds = model.predict(x, verbose=0)[0]\n    next_indices = sample(preds, n)\n    return [unique_words[idx] for idx in next_indices]","97c9df50":"def get_input():\n    text = input(\"\u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 \u0d1f\u0d48\u0d2a\u0d4d\u0d2a\u0d4d \u0d1a\u0d46\u0d2f\u0d41\u0d15>>>\")\n    return predict_completions(text, 5)","9d91ca4e":"predict_completions('\u0d28\u0d3f\u0d28\u0d4d\u0d28\u0d4b\u0d1f\u0d4d \u0d0e\u0d28\u0d3f\u0d15\u0d4d\u0d15\u0d4d \u0d12\u0d30\u0d41 \u0d15\u0d3e\u0d30\u0d4d\u0d2f\u0d02 \u0d2a\u0d31\u0d2f\u0d3e\u0d7b', 5)","b8470590":"I am using a single-layer LSTM model with 128 neurons, a fully connected layer, and a softmax function for activation.","219cca60":"The best possible n words after the prediction from the model is done by sample function.","52cdbd26":"Since we are building a malayalam new word predictor for keypad, I am going to build a datasrt from prokerala website that contains malayalam text messages. ","8396bb99":"Script for input","00e67b53":"For ease of use again, I am saving the data as CSV into my system and will load the CSV hereafter when","6906d2ca":"we will save the trained model and just load it back as needed.","048e1a51":"we create two numpy array X(for storing the features) and Y(for storing the corresponding label(here, next word)). We iterate X and Y if the word is present then the corresponding position is made 1.","86c075c2":"The below function cleans the text, removes manglish SMSs and removes punctuations.","e801d4af":"For prediction, we use the function predict_completions which use the model to predict and return the list of n predicted words.","7d93469a":"['\u0d28\u0d3f\u0d28\u0d4d\u0d28\u0d46', '\u0d0e\u0d28\u0d4d\u0d28\u0d46', '\u0d1e\u0d3e\u0d7b', '\u0d0e\u0d28\u0d3f\u0d15\u0d4d\u0d15\u0d4d', '\u0d0e\u0d28\u0d4d\u0d31\u0d46'] are the predicted words. Not bad for a model that's created on a small dataset. I have restricted the epochs to 5 because the model is overfitting so quickly. The data that I collected from prokerala had majority text in Manglish (after about 100 pages) and removing them resulted in few data left that has seriously affected the model. Also I am  not going deep into optimizing the model since this is just a demo implementation.","81b6efd7":"We define a **word_length** which means that the number of previous words that determines the next word. Also, we create an empty list called prev_words to store a set of five previous words and its corresponding next word in the next_words list.","4385e054":"Now, we need to predict new words using this model. To do that we input the sample as a feature vector. we convert the input string to a single feature vector."}}