{"cell_type":{"4ef4c074":"code","071a21f0":"code","8cdffdbe":"code","55d94a55":"code","434d3b20":"code","007e9ece":"code","69c945e4":"code","3f64430e":"code","9427196e":"code","5e5d0da9":"code","7b4fa40f":"code","704e11c9":"code","73b31f11":"code","54d76956":"code","bd735513":"code","30886fc7":"code","5bd98a5a":"code","337ab9a4":"code","1764c595":"code","7fdb5ce0":"code","bf5ee164":"code","53f471ab":"code","c3d11c93":"code","1eb77d1c":"code","41c605b9":"code","ce24305b":"code","75423610":"code","841f534c":"code","10fbef58":"code","f6fdab14":"code","00f1d935":"code","0a358128":"code","c6457790":"code","9550c172":"code","967185f5":"code","f9577c01":"code","c22dae37":"code","7277fbbd":"code","f75f6ee6":"code","86efbe48":"code","eed48580":"code","26990987":"code","8d8c3e7f":"code","9973197d":"code","240164cc":"code","d6f35c9d":"code","48194207":"code","130af846":"code","ac69c874":"markdown","c7433f66":"markdown","be8bdd98":"markdown","81f3c8a1":"markdown","237ec339":"markdown","c372a25e":"markdown","64e19dcd":"markdown","c53ea6ea":"markdown","1706138c":"markdown","70607fdb":"markdown","28bd698d":"markdown"},"source":{"4ef4c074":"import logging\nimport os\nimport numpy as np\nimport time\nimport pandas as pd\nfrom typing import Optional, Dict, Tuple\nfrom implicit.als import AlternatingLeastSquares\nfrom scipy import sparse\nfrom datetime import timedelta\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.metrics import roc_auc_score\nfrom os.path import join as pjoin\nfrom datetime import datetime\n","071a21f0":"MAILING_DATETIME = datetime(2020, 4, 4)\nRANDOM_STATE = 1\nN_PURCHASES_ROWS = None\nN_ALS_ITERATIONS = 15","8cdffdbe":"SECONDS_IN_DAY = 60 * 60 * 24\ndef drop_column_multi_index_inplace(df):\n    df.columns = ['_'.join(t) for t in df.columns]\n\ndef make_sum_csr(df,index_col,value_col,col_to_sum):\n    print(df[col_to_sum].values.shape)\n    print(df[index_col].values.shape)\n    print(df[value_col].values.shape)\n    coo = sparse.coo_matrix((df[col_to_sum].values,(df[index_col].values,df[value_col].values)))\n    csr = coo.tocsr(copy=False)\n    return csr\n    \n    \ndef make_count_csr(df,index_col,value_col):\n    col_to_sum_name = '__col_to_sum__'\n    df['__col_to_sum__'] = 1\n    csr = make_sum_csr(df,index_col=index_col,value_col=value_col,col_to_sum=col_to_sum_name)\n    df.drop(columns=col_to_sum_name, inplace=True)\n    return csr\n\n\ndef make_latent_feature(df,index_col,value_col,n_factors,n_iterations,sum_col=None):\n    if sum_col is None:\n        csr = make_count_csr(df, index_col=index_col, value_col=value_col)\n    else:\n        csr = make_sum_csr(df,index_col=index_col,value_col=value_col,col_to_sum=sum_col)\n\n    model = AlternatingLeastSquares(\n        factors=n_factors,\n        dtype=np.float32,\n        iterations=n_iterations,\n        regularization=0.1,\n        use_gpu=False,  # True if n_factors >= 32 else False,\n\n    )\n    np.random.seed(RANDOM_STATE)\n    model.fit(csr.T)\n\n    return model.user_factors\n","55d94a55":"INTERVALS = ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'hour']\n\ndef make_client_features(clients):\n    print('Preparing features')\n    min_datetime = clients['first_issue_date'].min()\n    days_from_min_to_issue = (\n            (clients['first_issue_date'] - min_datetime)\n            .dt.total_seconds() \/\n            SECONDS_IN_DAY\n    ).values\n    days_from_min_to_redeem = (\n            (clients['first_redeem_date'] - min_datetime)\n            .dt.total_seconds() \/\n            SECONDS_IN_DAY\n    ).values\n\n    age = clients['age'].values\n    age[age < 0] = -2\n    age[age > 100] = -3\n    \n    print('Combining features')\n    gender = clients['gender'].values\n    features = pd.DataFrame({\n        'client_id': clients['client_id'].values,\n        'gender_M': (gender == 'M').astype(int),\n        'gender_F': (gender == 'F').astype(int),\n        'gender_U': (gender == 'U').astype(int),\n        'age': age,\n        'days_from_min_to_issue': days_from_min_to_issue,\n        'days_from_min_to_redeem': days_from_min_to_redeem,\n        'issue_redeem_delay': days_from_min_to_redeem - days_from_min_to_issue})\n    features = features.fillna(-1)\n    print(f'Client features are created. Shape = {features.shape}')\n    return features","434d3b20":"os.environ['MKL_NUM_THREADS'] = '1'\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\n\nN_FACTORS = {\n    'product_id': 32,\n    'level_1': 2,\n    'level_2': 3,\n    'level_3': 4,\n    'level_4': 5,\n    'segment_id': 4,\n    'brand_id': 10,\n    'vendor_id': 10,\n}\n\nN_ITERATIONS = N_ALS_ITERATIONS\n\n\ndef make_product_features(products,purchases):\n    \n    print('Creating purchases-products matrix')\n    purchases_products = pd.merge(purchases,products,on='product_id')\n    \n    print('Purchases-products matrix is ready')\n\n    del purchases\n    del products\n\n    print('Creating latent features')\n    latent_features = make_latent_features(purchases_products)\n\n    print('Creating usual features')\n    usual_features = make_usual_features(purchases_products)\n\n    print('Combining features')\n    features = pd.merge(latent_features,usual_features,on='client_id')\n\n    print(f'Product features are created. Shape = {features.shape}')\n    return features\n\n\ndef make_usual_features(purchases_products):\n    pp_gb = purchases_products.groupby('client_id')\n    usual_features = pp_gb.agg(\n        {\n            'netto': ['median', 'max', 'sum'],\n            'is_own_trademark': ['sum', 'mean'],\n            'is_alcohol': ['sum', 'mean'],\n            'level_1': ['nunique'],\n            'level_2': ['nunique'],\n            'level_3': ['nunique'],\n            'level_4': ['nunique'],\n            'segment_id': ['nunique'],\n            'brand_id': ['nunique'],\n            'vendor_id': ['nunique']})\n    drop_column_multi_index_inplace(usual_features)\n    usual_features.reset_index(inplace=True)\n\n    return usual_features\n\n\ndef make_latent_features(purchases_products):\n    latent_feature_matrices = []\n    latent_feature_names = []\n    for col, n_factors in N_FACTORS.items():\n        print(f'Creating latent features for {col}')\n\n        counts_subject_by_client = (\n            purchases_products\n            .groupby('client_id')[col]\n            .transform('count')\n        )\n        share_col = f'{col}_share'\n        purchases_products[share_col] = 1 \/ counts_subject_by_client\n\n        latent_feature_matrices.append(\n            make_latent_feature(\n                purchases_products,\n                index_col='client_id',\n                value_col=col,\n                n_factors=n_factors,\n                n_iterations=N_ITERATIONS,\n                sum_col=share_col\n            )\n        )\n\n        purchases_products.drop(columns=share_col, inplace=True)\n\n        latent_feature_names.extend(\n            [f'{col}_f{i+1}' for i in range(n_factors)]\n        )\n\n        print(f'Features for {col} were created')\n\n    # Add features that show how much client likes product in category\n    print(f'Creating latent features for product in 4th category')\n    col = 'product_id'\n    counts_products_by_client_and_category = (\n        purchases_products\n            .groupby(['client_id', 'level_4'])[col]\n            .transform('count')\n    )\n    share_col = f'{col}_share_by_client_and_cat'\n    purchases_products[share_col] = 1 \/ counts_products_by_client_and_category\n\n    n_factors = N_FACTORS[col]\n    latent_feature_matrices.append(\n        make_latent_feature(\n            purchases_products,\n            index_col='client_id',\n            value_col=col,\n            n_factors=n_factors,\n            n_iterations=N_ITERATIONS,\n            sum_col=share_col\n        )\n    )\n\n    purchases_products.drop(columns=share_col, inplace=True)\n\n    latent_feature_names.extend(\n        [f'product_by_cat_f{i+1}' for i in range(n_factors)]\n    )\n\n    print(f'Features for {col} were created')\n\n    latent_features = pd.DataFrame(\n        np.hstack(latent_feature_matrices),\n        columns=latent_feature_names\n    )\n    latent_features.insert(0, 'client_id', np.arange(latent_features.shape[0]))\n\n    return latent_features","007e9ece":"ORDER_COLUMNS = [\n    'transaction_id',\n    'datetime',\n    'regular_points_received',\n    'express_points_received',\n    'regular_points_spent',\n    'express_points_spent',\n    'purchase_sum',\n    'store_id'\n]\n\nFLOAT32_MAX = np.finfo(np.float32).max\nPOINT_TYPES = ('regular', 'express')\nPOINT_EVENT_TYPES = ('spent', 'received')\nWEEK_DAYS = [\n    'Monday',\n    'Tuesday',\n    'Wednesday',\n    'Thursday',\n    'Friday',\n    'Saturday',\n    'Sunday'\n]\nTIME_LABELS = ['Night', 'Morning', 'Afternoon', 'Evening']\n\n\ndef make_purchase_features_for_last_days(purchases,n_days):\n    print(f'Creating purchase features for last {n_days} days...')\n    cutoff = MAILING_DATETIME - timedelta(days=n_days)\n    purchases_last = purchases[purchases['datetime'] >= cutoff]\n    purchase_last_features = make_purchase_features(purchases_last)\n    print(f'Purchase features for last {n_days} days are created')\n    return purchase_last_features\n\n\ndef make_purchase_features(purchases):\n    print('Creating purchase features...')\n    n_clients = purchases['client_id'].nunique()\n\n    print('Creating really purchase features...')\n    purchase_features = make_really_purchase_features(purchases)\n    print('Really purchase features are created')\n\n    print('Creating small product features...')\n    product_features = make_small_product_features(purchases)\n    print('Small product features are created')\n\n    print('Preparing orders table...')\n\n    orders = purchases.reindex(columns=['client_id'] + ORDER_COLUMNS)\n    del purchases\n    orders.drop_duplicates(inplace=True)\n    print(f'Orders table is ready. Orders: {len(orders)}')\n\n    print('Creating order features...')\n    order_features = make_order_features(orders)\n    print('Order features are created')\n\n    print('Creating store features...')\n    store_features = make_store_features(orders)\n    print('Store features are created')\n\n    print('Creating order interval features...')\n    order_interval_features = make_order_interval_features(orders)\n    print('Order interval features are created')\n\n    print('Creating features for orders with express points spent ...')\n    orders_with_express_points_spent_features = \\\n        make_features_for_orders_with_express_points_spent(orders)\n    print('Features for orders with express points spent are created')\n\n\n    features = (\n        purchase_features\n        .merge(order_features, on='client_id')\n        .merge(product_features, on='client_id')\n        .merge(store_features, on='client_id')\n        .merge(order_interval_features, on='client_id')\n        .merge(orders_with_express_points_spent_features, on='client_id')\n    )\n\n    print('Creating ratio time features...')\n    ratio_time_features = make_ratio_time_features(features)\n    print('Ratio time features are created')\n\n    features = features.merge(ratio_time_features, on='client_id')\n\n    assert len(features) == n_clients, \\\n        f'n_clients = {n_clients} but len(features) = {len(features)}'\n\n    features['days_from_last_order_share'] = \\\n        features['days_from_last_order'] \/ features['orders_interval_median']\n\n    features['most_popular_store_share'] = (\n        features['store_transaction_id_count_max'] \/\n        features['transaction_id_count']\n    )\n\n    features['ratio_days_from_last_order_eps_to_median_interval_eps'] = (\n        features['days_from_last_express_points_spent'] \/\n        features['orders_interval_median_eps']\n    )\n\n    features['ratio_mean_purchase_sum_eps_to_mean_purchase_sum'] = (\n        features['median_purchase_sum_eps'] \/\n        features['purchase_sum_median']\n    )\n\n    print(f'Purchase features are created. Shape = {features.shape}')\n    return features\n\n\ndef make_really_purchase_features(purchases):\n    simple_purchases = purchases.reindex(\n        columns=['client_id', 'product_id', 'trn_sum_from_iss']\n    )\n    prices_bounds = [0, 98, 195, 490, 950, 1900, 4400, FLOAT32_MAX]\n    agg_dict = {}\n    for i, lower_bound in enumerate(prices_bounds[:-1]):\n        upper_bound = prices_bounds[i + 1]\n        name = f'price_from_{lower_bound}'\n        simple_purchases[name] = (\n            (simple_purchases['trn_sum_from_iss'] >= lower_bound) &\n            (simple_purchases['trn_sum_from_iss'] < upper_bound)\n        ).astype(int)\n        agg_dict[name] = ['sum', 'mean']\n\n    agg_dict.update(\n        {\n            'trn_sum_from_iss': ['median'],  # median product price\n            'product_id': ['count', 'nunique']\n        }\n    )\n    simple_features = simple_purchases.groupby('client_id').agg(agg_dict)\n    drop_column_multi_index_inplace(simple_features)\n    simple_features.reset_index(inplace=True)\n\n    p_gb = purchases.groupby(['client_id', 'transaction_id'])\n    purchase_agg = p_gb.agg(\n        {\n            'product_id': ['count'],\n            'product_quantity': ['max']\n        }\n    )\n    drop_column_multi_index_inplace(purchase_agg)\n    purchase_agg.reset_index(inplace=True)\n    o_gb = purchase_agg.groupby('client_id')\n    complex_features = o_gb.agg(\n        {\n            # mean products in order\n            'product_id_count': ['mean', 'median'],\n            # mean max number of one product\n            'product_quantity_max': ['mean', 'median']\n        }\n    )\n    drop_column_multi_index_inplace(complex_features)\n    complex_features.reset_index(inplace=True)\n    features = pd.merge(\n        simple_features,\n        complex_features,\n        on='client_id'\n    )\n\n\n    return features\n\n\ndef make_order_features(orders):\n    orders = orders.copy()\n\n    o_gb = orders.groupby('client_id')\n\n    agg_dict = {\n            'transaction_id': ['count'],  # number of orders\n            'regular_points_received': ['sum', 'max', 'median'],\n            'express_points_received': ['sum', 'max', 'median'],\n            'regular_points_spent': ['sum', 'min', 'median'],\n            'express_points_spent': ['sum', 'min', 'median'],\n            'purchase_sum': ['sum', 'max', 'median'],\n            'store_id': ['nunique'],  # number of unique stores\n            'datetime': ['max']  # datetime of last order\n        }\n\n    # is regular\/express points spent\/received\n    for points_type in POINT_TYPES:\n        for event_type in POINT_EVENT_TYPES:\n            col_name = f'{points_type}_points_{event_type}'\n            new_col_name = f'is_{points_type}_points_{event_type}'\n            orders[new_col_name] = (orders[col_name] != 0).astype(int)\n            agg_dict[new_col_name] = ['sum']\n\n    features = o_gb.agg(agg_dict)\n    drop_column_multi_index_inplace(features)\n    features.reset_index(inplace=True)\n\n    features['days_from_last_order'] = (\n        MAILING_DATETIME - features['datetime_max']\n    ).dt.total_seconds() \/\/ SECONDS_IN_DAY\n    features.drop(columns=['datetime_max'], inplace=True)\n\n    # proportion of regular\/express points spent to all transactions\n    for points_type in POINT_TYPES:\n        for event_type in POINT_EVENT_TYPES:\n            col_name = f'is_{points_type}_points_{event_type}_sum'\n            new_col_name = f'proportion_count_{points_type}_points_{event_type}'\n            features[new_col_name] = (\n                    features[col_name] \/ features['transaction_id_count']\n            )\n\n    express_col = f'is_express_points_spent_sum'\n    regular_col = f'is_regular_points_spent_sum'\n    new_col_name = f'ratio_count_express_to_regular_points_spent'\n    features[new_col_name] = (\n            features[express_col] \/ features[regular_col]\n    ).replace(np.inf, FLOAT32_MAX)\n\n    for points_type in POINT_TYPES:\n        spent_col = f'is_{points_type}_points_spent_sum'\n        received_col = f'is_{points_type}_points_received_sum'\n        new_col_name = f'ratio_count_{points_type}_points_spent_to_received'\n        features[new_col_name] = (\n                features[spent_col] \/ features[received_col]\n        ).replace(np.inf, 1000)\n\n\n    for points_type in POINT_TYPES:\n        spent_col = f'{points_type}_points_spent_sum'\n        orders_sum_col = f'purchase_sum_sum'\n        new_col_name = f'ratio_sum_{points_type}_points_spent_to_purchases_sum'\n        features[new_col_name] = features[spent_col] \/ features[orders_sum_col]\n\n    new_col_name = f'ratio_sum_express_points_spent_to_sum_regular_points_spent'\n    regular_col = f'regular_points_spent_sum'\n    express_col = f'express_points_spent_sum'\n    features[new_col_name] = features[express_col] \/ features[regular_col]\n\n    return features\n\n\ndef make_features_for_orders_with_express_points_spent(orders):\n\n    orders_with_eps = orders.loc[orders['express_points_spent'] != 0]\n\n    o_gb = orders_with_eps.groupby(['client_id'])\n    features = o_gb.agg(\n        {\n            'purchase_sum': ['median'],\n            'datetime': ['max']\n        }\n    )\n    drop_column_multi_index_inplace(features)\n    features.reset_index(inplace=True)\n    features['days_from_last_express_points_spent'] = (\n            MAILING_DATETIME - features['datetime_max']\n    ).dt.days\n    features.drop(columns=['datetime_max'], inplace=True)\n    features.rename(\n        columns={\n            'purchase_sum_median': 'median_purchase_sum_eps'\n        },\n        inplace=True)\n\n    order_int_features = make_order_interval_features(orders_with_eps)\n    renamings = {\n        col: f'{col}_eps'\n        for col in order_int_features\n        if col != 'client_id'\n    }\n    order_int_features.rename(columns=renamings, inplace=True)\n\n    features = pd.merge(\n        features,\n        order_int_features,\n        on='client_id')\n\n    features = features.merge(\n        pd.Series(orders['client_id'].unique(), name='client_id'),\n        how='right')\n\n    return features\n\n\ndef make_time_features(orders):\n    # np.unique returns sorted array\n    client_ids = np.unique(orders['client_id'].values)\n\n    orders['weekday'] = np.array(WEEK_DAYS)[\n        orders['datetime'].dt.dayofweek.values\n    ]\n\n    time_bins = [-1, 6, 11, 18, 24]\n\n    orders['part_of_day'] = pd.cut(\n        orders['datetime'].dt.hour,\n        bins=time_bins,\n        labels=TIME_LABELS\n    ).astype(str)\n\n    time_part_encoder = LabelEncoder()\n    orders['part_of_day'] = time_part_encoder.fit_transform(orders['part_of_day'])\n\n    time_part_columns_name = time_part_encoder.inverse_transform(\n        np.arange(len(time_part_encoder.classes_))\n    )\n\n    time_part_cols = make_count_csr(orders,index_col='client_id',value_col='part_of_day')[client_ids, :]  # drop empty rows\n\n    time_part_cols = pd.DataFrame(\n        time_part_cols.toarray(),\n        columns=time_part_columns_name)\n    time_part_cols['client_id'] = client_ids\n\n    weekday_encoder = LabelEncoder()\n    orders['weekday'] = weekday_encoder.fit_transform(orders['weekday'])\n\n    weekday_column_names = weekday_encoder.inverse_transform(\n        np.arange(len(weekday_encoder.classes_))\n    )\n    weekday_cols = make_count_csr(\n        orders,\n        index_col='client_id',\n        value_col='weekday')[client_ids, :]  # drop empty rows\n    weekday_cols = pd.DataFrame(\n        weekday_cols.toarray(),\n        columns=weekday_column_names)\n    weekday_cols['client_id'] = client_ids\n\n    time_part_features = pd.merge(\n        left=time_part_cols,\n        right=weekday_cols,\n        on='client_id')\n    time_part_features.columns = [\n        f'{col}_orders_count' if col != 'client_id' else col\n        for col in time_part_features.columns\n    ]\n\n    return time_part_features\n\n\ndef make_small_product_features(purchases):\n    cl_pr_gb = purchases.groupby(['client_id', 'product_id'])\n    product_agg = cl_pr_gb.agg({\n        'product_quantity': ['sum']})\n\n    drop_column_multi_index_inplace(product_agg)\n    product_agg.reset_index(inplace=True)\n\n    cl_gb = product_agg.groupby(['client_id'])\n    features = cl_gb.agg({'product_quantity_sum': ['max']})\n\n    drop_column_multi_index_inplace(features)\n    features.reset_index(inplace=True)\n\n    return features\n\n\ndef make_store_features(orders):\n    cl_st_gb = orders.groupby(['client_id', 'store_id'])\n    store_agg = cl_st_gb.agg({\n        'transaction_id': ['count']})\n\n    drop_column_multi_index_inplace(store_agg)\n    store_agg.reset_index(inplace=True)\n\n    cl_gb = store_agg.groupby(['client_id'])\n    simple_features = cl_gb.agg(\n        {\n            'transaction_id_count': ['max', 'mean', 'median']\n        }\n    )\n\n    drop_column_multi_index_inplace(simple_features)\n    simple_features.reset_index(inplace=True)\n    simple_features.columns = (\n        ['client_id'] +\n        [\n            f'store_{col}'\n            for col in simple_features.columns[1:]\n        ]\n    )\n\n    latent_features = make_latent_store_features(orders)\n\n    features = pd.merge(\n        simple_features,\n        latent_features,\n        on='client_id'\n    )\n\n    return features\n\n\n\n\ndef make_latent_store_features(orders):\n    n_factors = 8\n    latent_feature_names = [f'store_id_f{i + 1}' for i in range(n_factors)]\n\n    latent_feature_matrix = make_latent_feature(\n        orders,\n        index_col='client_id',\n        value_col='store_id',\n        n_factors=n_factors,\n        n_iterations=N_ALS_ITERATIONS)\n\n    latent_features = pd.DataFrame(\n        latent_feature_matrix,\n        columns=latent_feature_names\n    )\n    latent_features.insert(0, 'client_id', np.arange(latent_features.shape[0]))\n\n    return latent_features\n\n\ndef make_order_interval_features(orders):\n    orders = orders.sort_values(['client_id', 'datetime'])\n\n    last_order_client = orders['client_id'].shift(1)\n    is_same_client = last_order_client == orders['client_id']\n    orders['last_order_datetime'] = orders['datetime'].shift(1)\n\n    orders['orders_interval'] = np.nan\n    orders.loc[is_same_client, 'orders_interval'] = (\n        orders.loc[is_same_client, 'datetime'] -\n        orders.loc[is_same_client, 'last_order_datetime']\n    ).dt.total_seconds() \/ SECONDS_IN_DAY\n\n    cl_gb = orders.groupby('client_id', sort=False)\n    features = cl_gb.agg(\n        {\n            'orders_interval': [\n                'mean',  # mean interval between orders\n                'median',\n                'std',  # constancy of orders\n                'min',\n                'max',\n                'last',  # interval between last 2 orders\n            ]\n        }\n    )\n    drop_column_multi_index_inplace(features)\n    features.reset_index(inplace=True)\n    features.fillna(-3, inplace=True)\n\n    return features\n\n\ndef make_ratio_time_features(features):\n    time_labels = TIME_LABELS + WEEK_DAYS\n    columns = [f'{col}_orders_count' for col in time_labels]\n    share_columns = [f'{col}_share' for col in columns]\n\n    time_features = features.reindex(columns=columns).values\n    orders_count = features['transaction_id_count'].values\n\n    share_time_features = time_features \/ orders_count.reshape(-1, 1)\n\n    share_time_features = pd.DataFrame(\n        share_time_features,\n        columns=share_columns\n    )\n    share_time_features['client_id'] = features['client_id']\n\n    return share_time_features","69c945e4":"def make_z(treatment, target):\n    y = target\n    w = treatment\n    z = y * w + (1 - y) * (1 - w)\n    return z\n\n\ndef calc_uplift(prediction):\n    uplift = 2 * prediction - 1\n    return uplift\n\n\ndef get_feature_importances(est, columns):\n    return pd.DataFrame({\n        'column': columns,\n        'importance': est.feature_importances_\n    }).sort_values('importance', ascending=False)","3f64430e":"def uplift_fit(model, X_train, treatment_train, target_train):\n    z = make_z(treatment_train, target_train)\n    model = clone(model)\n    model.fit(X_train, z)\n    return model\n\ndef uplift_predict(model, X_test, z=True):\n    predict_z = model.predict_proba(X_test)[:, 1]\n    uplift = calc_uplift(predict_z)\n    if z: return predict_z\n    else: return uplift","9427196e":"def score_uplift(prediction,treatment,target,rate = 0.3):\n    \"\"\"\n    \u041f\u043e\u0434\u0441\u0447\u0435\u0442 Uplift Score\n    \"\"\"\n    order = np.argsort(-prediction)\n    treatment_n = int((treatment == 1).sum() * rate)\n    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n    control_n = int((treatment == 0).sum() * rate)\n    control_p = target[order][treatment[order] == 0][:control_n].mean()\n    score = treatment_p - control_p\n    return score\n\n\ndef score_roc_auc(prediction,treatment,target):\n    y_true = make_z(treatment, target)\n    score = roc_auc_score(y_true, prediction)\n    return score\n\n\ndef uplift_metrics(prediction,treatment,target,rate_for_uplift = 0.3):\n    scores = {\n        'roc_auc': score_roc_auc(prediction, treatment, target),\n        'uplift': score_uplift(prediction, treatment, target, rate_for_uplift)\n    }\n    return scores","5e5d0da9":"def load_clients():\n    return pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/clients2.csv',\n                       parse_dates=['first_issue_date', 'first_redeem_date'])\n\ndef prepare_clients():\n    print('Preparing clients...')\n    clients = load_clients()\n    client_encoder = LabelEncoder()\n    clients['client_id'] = client_encoder.fit_transform(clients['client_id'])\n    print('Clients are ready')\n    return clients, client_encoder\n\n\ndef load_products():\n    return pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/products.csv')\n\n\ndef prepare_products():\n    print('Preparing products...')\n    products = load_products()\n    product_encoder = LabelEncoder()\n    products['product_id'] = product_encoder. \\\n        fit_transform(products['product_id'])\n\n    products.fillna(-1, inplace=True)\n\n    for col in [\n        'level_1', 'level_2', 'level_3', 'level_4',\n        'segment_id', 'brand_id', 'vendor_id'\n    ]:\n        products[col] = LabelEncoder().fit_transform(products[col].astype(str))\n    print('Products are ready')\n    return products, product_encoder\n\n\ndef load_purchases():\n    print('Loading purchases...')\n    purchases_train = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/train_purch\/train_purch.csv',\n        nrows=N_PURCHASES_ROWS)\n    purchases_test = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/test_purch\/test_purch.csv',\n        nrows=N_PURCHASES_ROWS)\n    purchases = pd.concat([purchases_train, purchases_test])\n    print('Purchases are loaded')\n    return purchases\n\n\ndef prepare_purchases(client_encoder,product_encoder):\n    print('Preparing purchases...')\n    purchases = load_purchases()\n\n    print('Handling n\/a values...')\n    purchases.dropna(\n        subset=['client_id', 'product_id'],\n        how='any',\n        inplace=True\n    )\n    purchases.fillna(-1, inplace=True)\n\n    print('Label encoding...')\n    purchases['client_id'] = client_encoder.transform(purchases['client_id'])\n    purchases['product_id'] = product_encoder.transform(purchases['product_id'])\n    for col in ['transaction_id', 'store_id']:\n        purchases[col] = LabelEncoder(). \\\n            fit_transform(purchases[col].astype(str))\n\n    print('Date and time conversion...')\n    purchases['datetime'] = pd.to_datetime(\n        purchases['transaction_datetime'],\n        format='%Y-%m-%d %H:%M:%S'\n    )\n    purchases.drop(columns=['transaction_datetime'], inplace=True)\n\n    print('Purchases are ready')\n    return purchases\n\n\ndef load_train():\n    return pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/train.csv',\n        index_col='client_id')\n\n\ndef load_test():\n    return pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/test.csv',\n        index_col='client_id')","7b4fa40f":"import logging\nimport pickle\nfrom datetime import timedelta\nfrom os.path import join as pjoin\n\nimport pandas as pd\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\n\nlog_format = '[%(asctime)s] %(name)-25s %(levelname)-8s %(message)s'\nlogging.basicConfig(\n    format=log_format,\n    level=logging.INFO\n)","704e11c9":"def prepare_features():\n    print('Loading data...')\n    clients, client_encoder = prepare_clients()\n    products, product_encoder = prepare_products()\n    purchases = prepare_purchases(client_encoder, product_encoder)\n    del product_encoder\n    print('Data is loaded')\n\n    print('Preparing features...')\n    purchase_features = make_purchase_features(purchases)\n\n    purchases_ids = purchases.reindex(columns=['client_id', 'product_id'])\n    del purchases\n    product_features = make_product_features(products, purchases_ids)\n    del purchases_ids\n\n    client_features = make_client_features(clients)\n\n    print('Combining features...')\n    features = (\n        client_features\n            .merge(purchase_features, on='client_id', how='left')\n           \n            .merge(product_features, on='client_id', how='left')\n    )\n    del client_features\n    del purchase_features\n    del product_features\n\n    features.fillna(-2, inplace=True)\n\n    features['client_id'] = client_encoder.inverse_transform(features['client_id'])\n    del client_encoder\n\n    print('Features are ready')\n\n    return features\n\n\ndef save_submission(indices_test, test_pred, filename):\n    df_submission = pd.DataFrame({'pred': test_pred}, index=indices_test)\n    df_submission.to_csv(filename)","73b31f11":"features = prepare_features()","54d76956":"print('Saving features...')\nwith open('features.pkl', 'wb') as f:\n    pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\nprint('Features are saved')","bd735513":"print('Loading features...')\nwith open('features.pkl', 'rb') as f:\n    features: pd.DataFrame = pickle.load(f)\nprint('Features are loaded')\n\nprint(f'Features shape: {features.shape}')\n\nprint('Preparing data sets...')\nfeatures.set_index('client_id', inplace=True)","30886fc7":"train = load_train()\ntest = load_test()\nindices_train = train.index\nindices_test = test.index\n\nX_train = features.loc[indices_train]\ntreatment_train = train.loc[indices_train, 'treatment_flg'].values\ntarget_train = train.loc[indices_train, 'target'].values\n\nX_test = features.loc[indices_test]\n\nindices_learn, indices_valid = train_test_split(train.index,test_size=0.3,random_state=RANDOM_STATE + 1)\n\nX_learn = features.loc[indices_learn]\ntreatment_learn = train.loc[indices_learn, 'treatment_flg'].values\ntarget_learn = train.loc[indices_learn, 'target'].values\n\nX_valid = features.loc[indices_valid]\ntreatment_valid = train.loc[indices_valid, 'treatment_flg'].values\ntarget_valid = train.loc[indices_valid, 'target'].values\nprint('Data sets prepared')","5bd98a5a":"X_train","337ab9a4":"clf_ = LGBMClassifier(\n        boosting_type='rf',\n        n_estimators=15000,\n        num_leaves=40,\n        max_depth=3,\n        max_bin=110,\n        # reg_lambda=1,\n        learning_rate=0.001,\n        random_state=RANDOM_STATE,\n        n_jobs=-1,\n        bagging_freq=1,\n        bagging_fraction=0.5,\n        importance_type='split',\n        is_unbalance=True,\n        min_child_samples=20,\n        min_child_weight=0.001,\n        min_split_gain=0.0,\n        objective='binary',\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        silent=True,\n        subsample=1.0,\n        subsample_for_bin=200000,\n        subsample_freq=0\n    )","1764c595":"print('Build model for learn data set...')\nclf = uplift_fit(clf_, X_learn, treatment_learn, target_learn)\nprint('Model is ready')\nlearn_pred = uplift_predict(clf, X_learn)\nlearn_scores = uplift_metrics(learn_pred, treatment_learn, target_learn)\nprint(f'Learn scores: {learn_scores}')\nvalid_pred = uplift_predict(clf, X_valid)\nvalid_scores = uplift_metrics(valid_pred, treatment_valid, target_valid)\nprint(f'Valid scores: {valid_scores}')","7fdb5ce0":"test_pred = uplift_predict(clf, X_test, z = True)\nprint('Saving submission...')\nsave_submission(indices_test,test_pred,'submission_without_my.csv')\nprint('Submission is ready')","bf5ee164":"test_pred","53f471ab":"df_train = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/train.csv', index_col='client_id')\ndf_clients = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/clients2.csv', index_col='client_id')\ndf_test = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/test.csv', index_col='client_id')\ndf_products = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/data\/products.csv')\ndf_pursh = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/train_purch\/train_purch.csv')\ndf_pursh_test = pd.read_csv('\/kaggle\/input\/x5-uplift-valid\/test_purch\/test_purch.csv')","c3d11c93":"indices_train = df_train.index\ndf_clients_train = df_clients.loc[indices_train]\ndf_train_all = df_clients_train.merge(df_train, right_index=True, left_index=True)\ndf_train_all.shape\nindices_test = df_test.index\ndf_clients_test = df_clients.loc[indices_test]\ndf_test_all = df_clients_test.merge(df_test, right_index=True, left_index=True)\ndf_test_all.shape","1eb77d1c":"df_test_all = df_test_all.drop(['client_id.1'], axis = 1)\ndf_train_all = df_train_all.drop(['client_id.1'], axis = 1)","41c605b9":"#\u041c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0430\u043f\u043b\u0438\u0444\u0442\u0430\ndef uplift_score(data):\n    return data[data.treatment_flg == 1].target.mean() - data[data.treatment_flg == 0].target.mean()","ce24305b":"max_uplf = 0\n#\u043d\u0430\u0439\u0434\u0435\u043c \u0432\u043e\u0437\u0440\u0430\u0441\u0442, \u0434\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0438 \u043f\u043e\u0441\u043b\u0435 \u0430\u043f\u043b\u0438\u0444\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f: \nfor i in range(18,60, 1):\n    if max_uplf < (uplift_score(df_train_all[df_train_all.age>i]) - uplift_score(df_train_all[df_train_all.age<=i])):\n        max_uplf = (uplift_score(df_train_all[df_train_all.age>i]) - uplift_score(df_train_all[df_train_all.age<=i]))\n        print(i)","75423610":"#\u041d\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a - \u0432\u043e\u0437\u0440\u0430\u0441\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 53 \u0438\u043b\u0438 \u043c\u0435\u043d\u044c\u0448\u0435 53 (\u0432\u043a\u043b\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e)\ndf_train_all['age_53'] = df_train_all.age.apply(lambda x: 1 if (x > 53) else 0)\ndf_test_all['age_53'] = df_test_all.age.apply(lambda x: 1 if (x > 53) else 0)","841f534c":"most_pop_product = df_pursh['product_id'].value_counts(normalize=True).index[0]\nmost_pop_product","10fbef58":"train_with_most = set(df_pursh[df_pursh.product_id==most_pop_product].client_id)\ntrain_without_most = set(indices_train) - train_with_most\n\ntest_with_most = set(df_pursh_test[df_pursh_test.product_id==most_pop_product].client_id)\ntest_without_most = set(indices_test) - test_with_most","f6fdab14":"uplift_score(df_train.loc[list(train_with_most)]) - uplift_score(df_train.loc[list(train_without_most)])","00f1d935":"df_train.loc[list(train_with_most)]\ndf_most_popular = pd.concat([df_train.loc[list(train_with_most)],df_train.loc[list(train_without_most)]]).drop(['target', 'treatment_flg'],\n                                                                                                        axis = 1)\ndf_most_popular['most_popular'] = 0\ndf_most_popular['most_popular'][:df_train.loc[list(train_with_most)].shape[0]] = 1 \n\n\n\ndf_most_popular_test = pd.concat([df_test.loc[list(test_with_most)],df_test.loc[list(test_without_most)]])\ndf_most_popular_test['most_popular'] = 0\ndf_most_popular_test['most_popular'][:df_test.loc[list(test_with_most)].shape[0]] = 1 ","0a358128":"df_train_all = df_train_all.merge(df_most_popular, right_index=True, left_index=True)\ndf_test_all = df_test_all.merge(df_most_popular_test, right_index=True, left_index=True)","c6457790":"gr = df_pursh.groupby(['client_id', 'transaction_id'])['purchase_sum'].mean()  #\u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u0438\u0435 \u0432 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f\u0445, \u043e\u0434\u043d\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f = \u043e\u0434\u043d\u0430 \u043f\u043e\u043a\u0443\u043f\u043a\u0430\npr = gr.reset_index().groupby(['client_id'])['purchase_sum'].sum()\nmedian_sum = pr.loc[indices_train].median()","9550c172":"gr_test = df_pursh_test.groupby(['client_id', 'transaction_id'])['purchase_sum'].mean()  #\u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u0438\u0435 \u0432 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f\u0445, \u043e\u0434\u043d\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f = \u043e\u0434\u043d\u0430 \u043f\u043e\u043a\u0443\u043f\u043a\u0430\npr_test = gr_test.reset_index().groupby(['client_id'])['purchase_sum'].sum()","967185f5":"df_train_all = df_train_all.merge(pr, right_index=True, left_index=True)\ndf_test_all = df_test_all.merge(pr_test, right_index=True, left_index=True)","f9577c01":"df_train_all['better_median_purchase'] = df_train_all.purchase_sum.apply(lambda x: 1 if (x > median_sum) else 0)\ndf_test_all['better_median_purchase'] = df_test_all.purchase_sum.apply(lambda x: 1 if (x > median_sum) else 0)","c22dae37":"uplift_score(df_train_all[df_train_all.better_median_purchase == 1])","7277fbbd":"uplift_score(df_train_all[df_train_all.better_median_purchase == 0]) #\u0441\u043d\u043e\u0432\u0430 \u0441\u0438\u043b\u044c\u043d\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f","f75f6ee6":"owner_products = df_products[df_products['is_own_trademark']==1].product_id","86efbe48":"with_owner = set(list(df_pursh[df_pursh['product_id'].isin(owner_products)]['client_id']))\n\nwith_owner_test = set(list(df_pursh_test[df_pursh_test['product_id'].isin(owner_products)]['client_id']))","eed48580":"df_train_all['with_owner'] = 0\ndf_train_all.loc[with_owner]['with_owner']  = 1\n\ndf_test_all['with_owner'] = 0\ndf_test_all.loc[with_owner_test]['with_owner']  = 1","26990987":"uplift_score(df_train_all[df_train_all.with_owner == 0])  #\u0442\u0435, \u043a\u0442\u043e \u043d\u0435 \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438, \u0443 \u043d\u0438\u0445 \u0430\u043f\u043b\u0438\u0444\u0442 \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0432\u044b\u0448\u0435","8d8c3e7f":"df_new_features = df_train_all[['with_owner', 'better_median_purchase', 'most_popular', 'age_53', 'purchase_sum']]","9973197d":"df_new_features_test = df_test_all[['with_owner', 'better_median_purchase', 'most_popular', 'age_53', 'purchase_sum']]","240164cc":"X_learn_with_my = X_learn.merge(df_new_features.loc[indices_learn], right_index=True, left_index=True)\nX_valid_with_my = X_valid.merge(df_new_features.loc[indices_valid], right_index=True, left_index=True)\nX_test_with_my = X_test.merge(df_new_features_test, right_index=True, left_index=True)","d6f35c9d":"clf_ = LGBMClassifier(\n        boosting_type='rf',\n        n_estimators=15000,\n        num_leaves=40,\n        max_depth=3,\n        max_bin=110,\n        # reg_lambda=1,\n        learning_rate=0.001,\n        random_state=RANDOM_STATE,\n        n_jobs=-1,\n        bagging_freq=1,\n        bagging_fraction=0.5,\n        importance_type='split',\n        is_unbalance=True,\n        min_child_samples=20,\n        min_child_weight=0.001,\n        min_split_gain=0.0,\n        objective='binary',\n        reg_alpha=0.0,\n        reg_lambda=0.0,\n        silent=True,\n        subsample=1.0,\n        subsample_for_bin=200000,\n        subsample_freq=0\n    )","48194207":"print('Build model for learn data set...')\nclf = uplift_fit(clf_, X_learn_with_my, treatment_learn, target_learn)\nprint('Model is ready')\nlearn_pred = uplift_predict(clf, X_learn_with_my)\nlearn_scores = uplift_metrics(learn_pred, treatment_learn, target_learn)\nprint(f'Learn scores: {learn_scores}')\nvalid_pred = uplift_predict(clf, X_valid_with_my)\nvalid_scores = uplift_metrics(valid_pred, treatment_valid, target_valid)\nprint(f'Valid scores: {valid_scores}')","130af846":"test_pred_with_my = uplift_predict(clf, X_test_with_my, z = True)\nsave_submission(indices_test,test_pred_with_my,'submission_with_my.csv')","ac69c874":"\n\u041d\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a - \u0441\u0443\u043c\u043c\u0430 \u0432\u0441\u0435\u0445 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u043e\u043b\u044c\u0448\u0435 \u043c\u0435\u0434\u0438\u0430\u043d\u043d\u043e\u0433\u043e \u0438\u043b\u0438 \u043c\u0435\u043d\u044c\u0448\u0435","c7433f66":"\n\u0421\u043e\u0435\u0434\u0438\u043d\u0438\u043c df_clients c df_train \u0438 df_test","be8bdd98":"\n\u041d\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a - \u0444\u0430\u043a\u0442 \u043f\u043e\u043a\u0443\u043f\u043a\u0438 \u0444\u0438\u0440\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430","81f3c8a1":"\n\u041d\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a - \u0444\u0430\u043a\u0442 \u043f\u043e\u043a\u0443\u043f\u043a\u0438 \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430","237ec339":"**\u041b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 (0.49) \u0440\u0430\u043d\u043d\u0435\u0435 \u0431\u044b\u043b \u043d\u0430\u043c\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0430\u0448\u0438\u0445 10 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 Catboost**","c372a25e":"**\u0411\u0435\u0437 \u0441\u0432\u043e\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:**","64e19dcd":"\n\u041d\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a - \u0441\u0443\u043c\u043c\u0430 \u0432\u0441\u0435\u0445 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f","c53ea6ea":"\n'gender' - \u0443\u0436\u0435 \u0435\u0441\u0442\u044c \u0432 features","1706138c":"**\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c**","70607fdb":"'client_id.1' - \u0441\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a, \u0432\u044b\u043a\u0438\u043d\u0435\u043c","28bd698d":"**\u0431\u044b\u043b\u043e - 0.04391, \u0441\u0442\u0430\u043b\u043e \u043f\u043e\u0441\u043b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043c\u043e\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 - 0.04394**"}}