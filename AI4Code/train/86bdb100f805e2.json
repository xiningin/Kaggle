{"cell_type":{"175531b8":"code","d54069c2":"code","d870a651":"code","df3b6873":"code","2d965345":"code","06b525ed":"code","b3920024":"code","63143ecf":"code","76fc0663":"code","fdf63584":"code","c61fb0af":"code","21d114c4":"code","66224dcb":"code","98f33701":"code","a63072bf":"code","c15d86fc":"code","ff4444de":"code","05d95824":"code","f8f1bb18":"code","e569c3cd":"code","0360941f":"code","c7caead4":"code","ea11137f":"code","aff3cf14":"code","b08375ee":"code","58cef3ef":"code","7fb54581":"code","034a1c4e":"code","ecefbb5d":"code","ce6a6fdb":"code","a21eff5d":"code","9604cd7e":"code","ee704a78":"code","eb144e8c":"code","74a0ac36":"code","8ec6a1e6":"code","f0c52dca":"code","618daea4":"code","aecb6f94":"code","3797eaae":"code","360c9f69":"code","367eb616":"code","404fc07b":"code","6fa98277":"code","18964ffd":"code","7b6b21e3":"code","e2dec30f":"code","f3c3397d":"code","dbfd89df":"code","d06e0a0c":"code","0d62fbf0":"code","029b8770":"code","d855e837":"code","39be291e":"code","96d3644e":"code","5f7ccf8d":"code","75ebb45f":"code","8b823bea":"code","ddac60b6":"code","98bc7c9b":"code","9391247e":"code","0c1baae5":"code","13143fdd":"code","fbe5f3e8":"code","0eaebf8d":"code","f3c9fb65":"markdown","43834f4d":"markdown","42ddc80f":"markdown","f315346c":"markdown","e03cb621":"markdown","513a1208":"markdown","38db4564":"markdown","a974bff2":"markdown","eb1e4049":"markdown","5a400636":"markdown","8e3ad52b":"markdown","3c102563":"markdown"},"source":{"175531b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d54069c2":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","d870a651":"train.head()","df3b6873":"# Number of unique classes in each object column\ntrain_transaction.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","2d965345":"train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","06b525ed":"print('  {:.4f}% of Transactions that are fraud in train '.format(train_transaction['isFraud'].mean() * 100))","b3920024":"train_transaction.groupby('isFraud').count() .plot(kind='barh',\n          title='Distribution of Target in Train',\n          figsize=(15, 3),legend=None)\nplt.show()","63143ecf":"train_transaction['TransactionAmt'] .apply(np.log).plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()","76fc0663":"train_transaction.groupby('ProductCD').count().sort_index().plot(kind='barh',\n          figsize=(15, 3),legend=None,\n         title='Count of Observations by ProductCD')\nplt.show()","fdf63584":"train_transaction.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD')\nplt.show()","c61fb0af":"card_cols = [c for c in train_transaction.columns if 'card' in c]\ntrain_transaction[card_cols].head()","21d114c4":"color_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\ncolor_idx = 0\nfor c in card_cols:\n    if train_transaction[c].dtype in ['float64','int64']:\n        train_transaction[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()","66224dcb":"train_transaction_fr = train_transaction.loc[train_transaction['isFraud'] == 1]\ntrain_transaction_nofr = train_transaction.loc[train_transaction['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ntrain_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ntrain_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ntrain_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ntrain_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()","98f33701":"m_cols = [c for c in train_transaction if c[0] == 'M']\ntrain_transaction[m_cols].head()","a63072bf":"(train_transaction[m_cols] == 'T').sum().plot(kind='bar',\n                                              title='Count of T by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[3])\nplt.show()\n(train_transaction[m_cols] == 'F').sum().plot(kind='bar',\n                                              title='Count of F by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[4])\nplt.show()\n(train_transaction[m_cols].isna()).sum().plot(kind='bar',\n                                              title='Count of NaN by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[0])\nplt.show()","c15d86fc":"import gc\n\ndel train_transaction, train_identity\ndel test_transaction, test_identity\ngc.collect()","ff4444de":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","05d95824":"%%time\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","f8f1bb18":"isna = train.isna().sum(axis=1)\nisna_test = test.isna().sum(axis=1)","e569c3cd":"plt.hist(isna, normed=True, bins=30, alpha=0.4, label='train')\nplt.hist(isna_test, normed=True, bins=30, alpha=0.4, label='test')\nplt.xlabel('Number of features which are NaNs')\nplt.legend()","0360941f":"training_missing = train.isna().sum(axis=0) \/ train.shape[0] \ntest_missing = test.isna().sum(axis=0) \/ test.shape[0] ","c7caead4":"change = (training_missing \/ test_missing).sort_values(ascending=False)\nchange = change[change<1e6] # remove the divide by zero errors","ea11137f":"change","aff3cf14":"fig, axs = plt.subplots(ncols=2)\n\ntrain_vals = train[\"D15\"].fillna(-999)\ntest_vals = test[test[\"TransactionDT\"]>2.5e7][\"D15\"].fillna(-999) # values following the shift\n\n\naxs[0].hist(train_vals, alpha=0.5, normed=True, bins=25)\n    \naxs[1].hist(test_vals, alpha=0.5, normed=True, bins=25)\n\n\nfig.set_size_inches(7,3)\nplt.tight_layout()","b08375ee":"isna_df = pd.DataFrame({'missing_count':isna,'isFraud':train['isFraud']})","58cef3ef":"plt.plot(isna_df.groupby('missing_count').mean(), 'k.')\nplt.ylabel('Fraction of fradulent transactions')\nplt.xlabel('Number of missing variables')\nplt.axhline(0)","7fb54581":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start\/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","034a1c4e":"vals = plt.hist(train['TransactionDT'] \/ (3600*24), bins=1800)\nplt.xlim(70, 78)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,1000)","ecefbb5d":"train['weekday'] = make_day_feature(train, offset=0.58)\nplt.plot(train.groupby('weekday').mean()['isFraud'])\n\nplt.ylim(0, 0.04)\nplt.xlabel('Encoded day')\nplt.ylabel('Fraction of fraudulent transactions')\ntrain['hours'] = make_hour_feature(train)\nplt.plot(train.groupby('hours').mean()['isFraud'], color='k')\nax = plt.gca()\nax2 = ax.twinx()\n_ = ax2.hist(train['hours'], alpha=0.3, bins=24)\nax.set_xlabel('Encoded hour')\nax.set_ylabel('Fraction of fraudulent transactions')\n\nax2.set_ylabel('Number of transactions')","ce6a6fdb":"train['hours'] = make_hour_feature(train)\nplt.plot(train.groupby('hours').mean()['isFraud'], color='k')\nax = plt.gca()\nax2 = ax.twinx()\n_ = ax2.hist(train['hours'], alpha=0.3, bins=24)\nax.set_xlabel('Encoded hour')\nax.set_ylabel('Fraction of fraudulent transactions')\n\nax2.set_ylabel('Number of transactions')","a21eff5d":"train.info()","9604cd7e":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id',\"weekday\",\"hours\"]","ee704a78":"cols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionDT')","eb144e8c":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","74a0ac36":"columns_a = ['TransactionAmt', 'id_02', 'D15']\ncolumns_b = ['card1', 'card4', 'addr1']\n\nfor col_a in columns_a:\n    for col_b in columns_b:\n        for df in [train, test]:\n            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('mean')\n            df[f'{col_a}_to_std_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('std')","8ec6a1e6":"# New feature - log of transaction amount.\ntrain['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n","f0c52dca":"from sklearn.preprocessing import LabelEncoder\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n\n# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n\n# Encoding - count encoding separately for train and test\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","618daea4":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","aecb6f94":"train['P_emaildomain']","3797eaae":"%%time\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","360c9f69":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","367eb616":"from sklearn.model_selection import KFold\nimport lightgbm as lgb","404fc07b":"params = {}\nparams['learning_rate']= 0.003\nparams['boosting_type']='gbdt'\nparams['objective']='binary'\nparams['metric']= 'auc'\nparams['sub_feature']=0.5\nparams['num_leaves']= 10\nparams['min_data']=50\nparams['max_depth']=10","6fa98277":"X.head()","18964ffd":"%%time\n\nNFOLDS = 2\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 100, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","7b6b21e3":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","e2dec30f":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')","f3c3397d":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nimport gc\n\ndel train_transaction, train_identity\ndel test_transaction, test_identity\ngc.collect()","dbfd89df":"def fill_pairs(train, test, pairs):\n    for pair in pairs:\n\n        unique_train = []\n        unique_test = []\n\n        print(f'Pair: {pair}')\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n\n        for value in train[pair[0]].unique():\n            unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n\n        for value in test[pair[0]].unique():\n            unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n\n        pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n        pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n        \n        print('Filling train...')\n\n        for value in pair_values_train[pair_values_train == 1].index:\n            train.loc[train[pair[0]] == value, pair[1]] = train.loc[train[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print('Filling test...')\n\n        for value in pair_values_test[pair_values_test == 1].index:\n            test.loc[test[pair[0]] == value, pair[1]] = test.loc[test[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n        \n    return train, test","d06e0a0c":"card_features = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\ntrain[card_features].head()\npd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","0d62fbf0":"pairs = [('card1', 'card2'), ('card1', 'card3')]\n\ntrain, test = fill_pairs(train, test, pairs)","029b8770":"pd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","d855e837":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nX = train.loc[:, train.columns != 'isFraud']\ny = train.iloc[:, train.columns == 'isFraud']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","39be291e":"d_train = lgb.Dataset(X_train, label= y_train)","96d3644e":"params = {}\nparams['learning_rate']= 0.003\nparams['boosting_type']='gbdt'\nparams['objective']='binary'\nparams['metric']='auc'\nparams['sub_feature']=0.5\nparams['num_leaves']= 10\nparams['min_data']=50\nparams['max_depth']=10","5f7ccf8d":"from sklearn import preprocessing\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values)) ","75ebb45f":"clf = lgb.train(params, d_train, 100)","8b823bea":"clf = lgb.train(params, d_train, 100)\ny_pred = clf.predict(X_test)","ddac60b6":"#convert into binary values\n\nfor i in range(0,len(X_test.index)):\n    if (y_pred[i] >= 0.04):\n        y_pred[i] = 1\n    else:\n        y_pred[i] =0\nlen(y_pred)   ","98bc7c9b":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_pred, y_test)\naccuracy","9391247e":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","0c1baae5":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","13143fdd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),X_train.columns)), columns=['Value','Feature'])","fbe5f3e8":"feature_imp = feature_imp.sort_values(by=\"Value\", ascending=False)","0eaebf8d":"plt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.iloc[0:30,:])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","f3c9fb65":"Clearly, there are certain numbers of missing data which correlates with a an increased chance of the transaction being fradulent.","43834f4d":"**Missing Values Analysis**","42ddc80f":"This shows the distribution of the number of missing values for the training and test sets.","f315346c":"**Create a day Features**","e03cb621":"**M1-M9**","513a1208":"\n1. ProductCD C has the most fraud with >11%\n","38db4564":"**Does more missing data increase the chance of a Fradulent transaction**","a974bff2":"Ref Kernels\n* https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm\n* https:\/\/www.kaggle.com\/grazder\/filling-card-nans\n* https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n* https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\n","eb1e4049":"**Card**","5a400636":"*Looking at the distribution of values for a feature which changes significantly.*\n> We will look at D15.","8e3ad52b":"**Baseline Model With Fill NaNs in amount category with most frequent value **","3c102563":"**Baseline Model With Features Engineering**"}}