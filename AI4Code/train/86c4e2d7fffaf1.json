{"cell_type":{"6c8bf441":"code","0bfc471c":"code","2f9abe50":"code","80b27e57":"code","30849517":"code","710658ae":"code","9f62b04d":"code","9eb2ddfa":"code","1992e28a":"code","53e50b85":"code","f0f2e76f":"code","cd5988f4":"code","364ca8af":"code","1a5bce38":"code","6935d37c":"code","ba5f91b9":"code","3cd4bc43":"code","a397f644":"code","534dfe87":"code","206c00fc":"code","e76ca920":"code","c96c3649":"code","653746fa":"markdown","f7218767":"markdown","2cbd2ef9":"markdown","1248f6a9":"markdown","b39817d6":"markdown","8bcf7174":"markdown","7bcd5412":"markdown","29d2dcdc":"markdown","5b49bd72":"markdown","9894a85f":"markdown","ab62a343":"markdown","7e89ff7b":"markdown","67713b72":"markdown","e5b69737":"markdown","d2a58a03":"markdown","6ad2c109":"markdown","8cf1faca":"markdown","ec347a6d":"markdown","51806430":"markdown","b594d8dd":"markdown","8a74ff74":"markdown","326d982a":"markdown","de1d91a0":"markdown","9c956904":"markdown","141b7b27":"markdown","6aedce08":"markdown","1dd2d436":"markdown","0821033d":"markdown","1271669b":"markdown","3a5df7bb":"markdown","5a1fb346":"markdown","a348e39c":"markdown","952e4333":"markdown","29fd9394":"markdown","dfc1a305":"markdown","d249e52b":"markdown","42d84364":"markdown","caecd9e4":"markdown","4d1b7f5c":"markdown","75ed9d2d":"markdown","3e9ad2d5":"markdown","c4e5d331":"markdown","e6b7c2db":"markdown","f49cdfa1":"markdown","26ac8022":"markdown","abb45087":"markdown","2623e57f":"markdown","b2700573":"markdown","52fdd899":"markdown","52239c34":"markdown","c8315013":"markdown"},"source":{"6c8bf441":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom numpy import nan\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport math\n\n\nsampleSubmissionData = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nsalesTrainValidationData = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsellPricesData = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncalendarData = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')","0bfc471c":"def getWeekdaysInRange(start, end, day):\n    dates = []\n    for i in range (start,end):\n        if calendarData.loc[i].weekday == day:\n            dates.append(calendarData.loc[i].date)\n    return dates\ndef getEventDaysInRange(start, end):\n    dates = []\n    for i in range (start,end):\n        if type(calendarData.loc[i].event_name_1) == str or type(calendarData.loc[i].event_name_2) == str:\n            dates.append(calendarData.loc[i].date)\n    return dates\ndef getSnapDaysInRange(start, end, state):\n    dates = []\n    for i in range (start,end):\n        if (state == 'CA' and calendarData.loc[i].snap_CA or \n            state == 'TX' and calendarData.loc[i].snap_TX or \n            state == 'WI' and calendarData.loc[i].snap_WI):\n            dates.append(calendarData.loc[i].date)\n    return dates","2f9abe50":"salesTrainValidationData.head()","80b27e57":"sellPricesData.head()","30849517":"calendarData.head()","710658ae":"storeCount = salesTrainValidationData.groupby('state_id')['store_id'].nunique().reset_index()\nstoreCount.columns = ['State', '# of Stores']\nstoreCount","9f62b04d":"totalSalesPerDay = salesTrainValidationData.copy().loc[:,\"d_1\":\"d_1913\"].sum().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1:1913].loc[:,'date'].values)\ntotalSalesPerDay = totalSalesPerDay.set_index(pd.to_datetime(dates))\ntotalSalesPerDay.plot(title=\"Total sales per day\", figsize=(30,8), legend=None)\nplt.style.use('bmh')\nplt.xlabel('')\nplt.ylabel('Sales')\nplt.show()","9eb2ddfa":"sales6Months = salesTrainValidationData.copy().loc[:,\"d_1733\":\"d_1913\"].sum().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nsales6Months = sales6Months.set_index(pd.to_datetime(dates))\n\nsales1Year = salesTrainValidationData.copy().loc[:,\"d_1548\":\"d_1913\"].sum().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1548:1913].loc[:,'date'].values)\nsales1Year = sales1Year.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(3,1,figsize=(24,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(sales6Months, zorder=2)\nax[0].set_title('Total sales per day for the last 6 months with weekends')\nax[0].set_ylabel('Sales')\n\nfor date in getWeekdaysInRange(1733, 1913, 'Friday'):\n    ax[0].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n    \nfor date in getWeekdaysInRange(1733, 1913, 'Saturday'):\n    ax[0].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n    \nfor date in getWeekdaysInRange(1733, 1913, 'Sunday'):\n    ax[0].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n\nax[1].plot(sales1Year, zorder=2)\nax[1].set_title('Total sales per day for the last year with events')\nax[1].set_ylabel('Sales')\n\nfor date in getEventDaysInRange(1548, 1913):\n    ax[1].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=8)\n\nax[2].plot(sales1Year, zorder=2)\nax[2].set_title('Total sales per day for the last year with weekends and events')\nax[2].set_ylabel('Sales')\n\nfor date in getWeekdaysInRange(1548, 1913, 'Friday'):\n    ax[2].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n    \nfor date in getWeekdaysInRange(1548, 1913, 'Saturday'):\n    ax[2].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n    \nfor date in getWeekdaysInRange(1548, 1913, 'Sunday'):\n    ax[2].axvline(pd.Timestamp(date), color='#d5dee0', zorder=1, linewidth=8)\n\nfor date in getEventDaysInRange(1548, 1913):\n    ax[2].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=8)\n\nplt.show()","1992e28a":"totalSalesPerDayFirstWinter = salesTrainValidationData.copy().loc[:,\"d_1768\":\"d_1798\"].sum().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1768:1798].loc[:,'date'].values)\ntotalSalesPerDayFirstWinter = totalSalesPerDayFirstWinter.set_index(pd.to_datetime(dates))\n\ntotalSalesPerDaySecondWinter = salesTrainValidationData.copy().loc[:,\"d_1403\":\"d_1433\"].sum().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1403:1433].loc[:,'date'].values)\ntotalSalesPerDaySecondWinter = totalSalesPerDaySecondWinter.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(2,1,figsize=(20,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(totalSalesPerDayFirstWinter)\nax[0].set_title('Winter sales 2015')\nax[0].axvline(pd.Timestamp(\"2015-12-25\"), color='#cfb4b4', zorder=1, linewidth=3)\n\nax[1].plot(totalSalesPerDaySecondWinter)\nax[1].set_title('Winter sales 2014')\nax[1].axvline(pd.Timestamp(\"2014-12-25\"), color='#cfb4b4', zorder=1, linewidth=3)\n\nplt.show()","53e50b85":"totalSalesPerDayPast6Months = salesTrainValidationData.copy().groupby('state_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntotalSalesPerDayPast6Months = totalSalesPerDayPast6Months.set_index(pd.to_datetime(dates))\n\ntotalSalesPerDayPast6Months.plot(title=\"Total sales by states\", figsize=(30,8))\nplt.style.use('bmh')\nplt.show()","f0f2e76f":"dates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\n\ntotalSalesPerDayPast6MonthsCA = salesTrainValidationData.copy().groupby('store_id').sum().loc[\"CA_1\":\"CA_4\", \"d_1733\":\"d_1913\"].transpose()\ntotalSalesPerDayPast6MonthsCA = totalSalesPerDayPast6MonthsCA.set_index(pd.to_datetime(dates))\ntotalSalesPerDayPast6MonthsCA.transpose()\n\ntotalSalesPerDayPast6MonthsTX = salesTrainValidationData.copy().groupby('store_id').sum().loc[\"TX_1\":\"TX_3\", \"d_1733\":\"d_1913\"].transpose()\ntotalSalesPerDayPast6MonthsTX = totalSalesPerDayPast6MonthsTX.set_index(pd.to_datetime(dates))\ntotalSalesPerDayPast6MonthsTX.transpose()\n\ntotalSalesPerDayPast6MonthsWI = salesTrainValidationData.copy().groupby('store_id').sum().loc[\"WI_1\":\"WI_3\", \"d_1733\":\"d_1913\"].transpose()\ntotalSalesPerDayPast6MonthsWI = totalSalesPerDayPast6MonthsWI.set_index(pd.to_datetime(dates))\ntotalSalesPerDayPast6MonthsWI.transpose()\n\nf,ax = plt.subplots(3,1,figsize=(20,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(totalSalesPerDayPast6MonthsCA)\nax[0].set_title('Sales by store in CA')\nax[0].legend(('CA_1', 'CA_2', 'CA_3', 'CA_4'), loc='upper left')\n\nax[1].plot(totalSalesPerDayPast6MonthsTX)\nax[1].set_title('Sales by store in TX')\nax[1].legend(('TX_1', 'TX_2', 'TX_3'), loc='upper left')\n\nax[2].plot(totalSalesPerDayPast6MonthsWI)\nax[2].set_title('Sales by store in WI')\nax[2].legend(('WI_1', 'WI_2', 'WI_3'), loc='upper left')\nplt.show()","cd5988f4":"totalSalesPerDayPast6Months = salesTrainValidationData.copy().groupby('cat_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntotalSalesPerDayPast6Months = totalSalesPerDayPast6Months.set_index(pd.to_datetime(dates))\n\ntotalSalesPerDayPast6Months.plot(title=\"Total sales by categories across all stores\", figsize=(30,8))\nplt.style.use('bmh')\nplt.show()","364ca8af":"totalSalesPerDayPast6Months = salesTrainValidationData.copy().groupby(by=['state_id','cat_id'], axis=0).sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntotalSalesPerDayPast6Months = totalSalesPerDayPast6Months.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(3,1,figsize=(20,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(totalSalesPerDayPast6Months['CA'])\nax[0].set_title('Sales by category in CA')\nax[0].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\n\nax[1].plot(totalSalesPerDayPast6Months['TX'])\nax[1].set_title('Sales by category in TX')\nax[1].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\n\nax[2].plot(totalSalesPerDayPast6Months['WI'])\nax[2].set_title('Sales by category in WI')\nax[2].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\nplt.show()","1a5bce38":"totalSalesPerDayPast6Months = salesTrainValidationData.copy().groupby('dept_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntotalSalesPerDayPast6Months = totalSalesPerDayPast6Months.set_index(pd.to_datetime(dates))\n\ntotalSalesPerDayPast6Months.plot(title=\"Total sales by subcategories across all stores\", figsize=(30,8))\nplt.style.use('bmh')\nplt.legend(loc='upper left')\nplt.show()","6935d37c":"caSales = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['CA_1', 'CA_2', 'CA_3', 'CA_4'])]\ncaSales = caSales.groupby('dept_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ncaSales = caSales.set_index(pd.to_datetime(dates))\n\ntxSales = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['TX_1', 'TX_2', 'TX_3'])]\ntxSales = txSales.groupby('dept_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntxSales = txSales.set_index(pd.to_datetime(dates))\n\nwiSales = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['WI_1', 'WI_2', 'WI_3'])]\nwiSales = wiSales.groupby('dept_id').sum().loc[:,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nwiSales = wiSales.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(3,1,figsize=(24,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(caSales)\nax[0].set_title('Total sales of subcategories for CA')\nax[0].legend(('FOODS_1', 'FOODS_2', 'FOODS_3', 'HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2'), loc='upper left')\nax[0].set_ylabel('Sales')\n\nax[1].plot(txSales)\nax[1].set_title('Total sales of subcategories for TX')\nax[1].legend(('FOODS_1', 'FOODS_2', 'FOODS_3', 'HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2'), loc='upper left')\nax[1].set_ylabel('Sales')\n\nax[2].plot(wiSales)\nax[2].set_title('Total sales of subcategories for WI')\nax[2].legend(('FOODS_1', 'FOODS_2', 'FOODS_3', 'HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2'), loc='upper left')\nax[2].set_ylabel('Sales')\n\nplt.show()","ba5f91b9":"foods = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['CA_1', 'CA_2', 'CA_3', 'CA_4'])]\nfoods = foods.groupby('dept_id').sum().loc[\"FOODS_1\":\"FOODS_3\":,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nfoods = foods.set_index(pd.to_datetime(dates))\n\nhobbies = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['CA_1', 'CA_2', 'CA_3', 'CA_4'])]\nhobbies = hobbies.groupby('dept_id').sum().loc[\"HOBBIES_1\":\"HOBBIES_2\":,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nhobbies = hobbies.set_index(pd.to_datetime(dates))\n\nhousehold = salesTrainValidationData.copy().loc[salesTrainValidationData['store_id'].isin(['CA_1', 'CA_2', 'CA_3', 'CA_4'])]\nhousehold = household.groupby('dept_id').sum().loc[\"HOUSEHOLD_1\":\"HOUSEHOLD_2\":,\"d_1733\":\"d_1913\"].transpose()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nhousehold = household.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(3,1,figsize=(24,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(foods)\nax[0].set_title('Overall sales for food subcategories in CA with snap days')\nax[0].legend(('FOODS_1', 'FOODS_2', 'FOODS_3'), loc='upper left')\n\nfor date in getSnapDaysInRange(1733, 1913, 'CA'):\n    ax[0].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nax[1].plot(hobbies)\nax[1].set_title('Overall sales for hobby subcategories in CA with snap days')\nax[1].legend(('HOBBIES_1', 'HOBBIES_2'), loc='upper left')\n\nfor date in getSnapDaysInRange(1733, 1913, 'CA'):\n    ax[1].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nax[2].plot(household)\nax[2].set_title('Overall sales for household subcategories in CA with snap days')\nax[2].legend(('HOUSEHOLD_1', 'HOUSEHOLD_2'), loc='upper left')\n\nfor date in getSnapDaysInRange(1733, 1913, 'CA'):\n    ax[2].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nplt.show()","3cd4bc43":"caSales = salesTrainValidationData.copy().groupby('store_id').sum().loc['CA_1':'CA_4',\"d_1733\":\"d_1913\"].sum().transpose().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ncaSales = caSales.set_index(pd.to_datetime(dates))\n\ntxSales = salesTrainValidationData.copy().groupby('store_id').sum().loc['TX_1':'TX_3',\"d_1733\":\"d_1913\"].sum().transpose().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\ntxSales = txSales.set_index(pd.to_datetime(dates))\n\nwiSales = salesTrainValidationData.copy().groupby('store_id').sum().loc['WI_1':'WI_3',\"d_1733\":\"d_1913\"].sum().transpose().to_frame()\ndates = pd.to_datetime(calendarData.copy().loc[1733:1913].loc[:,'date'].values)\nwiSales = wiSales.set_index(pd.to_datetime(dates))\n\nf,ax = plt.subplots(3,1,figsize=(24,8))\nf.tight_layout(pad=3.0)\nplt.style.use('bmh')\n\nax[0].plot(caSales, zorder=2)\nax[0].set_title('Total sales for CA with snap days')\n\nfor date in getSnapDaysInRange(1733, 1913, 'CA'):\n    ax[0].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nax[1].plot(txSales, zorder=2)\nax[1].set_title('Total sales for TX with snap days')\n\nfor date in getSnapDaysInRange(1733, 1913, 'TX'):\n    ax[1].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nax[2].plot(wiSales, zorder=2)\nax[2].set_title('Total sales for WI with snap days')\n\nfor date in getSnapDaysInRange(1733, 1913, 'WI'):\n    ax[2].axvline(pd.Timestamp(date), color='#cfb4b4', zorder=1, linewidth=3)\n\nplt.show()","a397f644":"def calculateSlope(series, n, sumX):\n    def a():\n        a = 0\n        for i in range (0, n):\n            a += (i + 1) * series.iloc[i]\n        return a * n\n    def b():\n        sumY = 0\n        for i in range (0, n):\n            sumY += i + 1\n        return series.sum() * sumY\n    def c():\n        c = 0\n        for i in range (0, n):\n            c += (i + 1)**2\n        return c * n\n    def d():\n        return sumX**2\n    return (a() - b()) \/ (c() - d())\n\ndef calculateYIntercept(series, n, m, sumX):\n    def e():\n        return series.sum()\n    def f():\n        return m * sumX\n    return (e() - f()) \/ n\n\ndef calculateTrend(series, count):\n    n = len(series.index)\n\n    sumX = 0\n    for i in range (0, n):\n        sumX += i + 1\n\n    m = calculateSlope(series, n, sumX)\n    yi = calculateYIntercept(series, n, m, sumX)\n\n    trend = []\n    for i in range(n + 1, n + 1 + count):\n        trend.append(m * i + yi)\n\n    return pd.Series(trend)","534dfe87":"train = salesTrainValidationData.copy().loc[:,'d_1':'d_1885'].sum()\nvalidation = salesTrainValidationData.copy().loc[:,'d_1886':'d_1913'].sum()\ntrend = calculateTrend(train, 28)\npredicted = trend.to_frame().set_index(validation.index.values).transpose().iloc[0]\nrmse = math.sqrt(metrics.mean_squared_error(validation, predicted))\n\ncompare = pd.DataFrame({\n    'Actual': validation,\n    'Predicted': predicted\n})\n\ndates = pd.to_datetime(calendarData.copy().loc[1886:1913].loc[:,'date'].values)\ncompare = compare.set_index(pd.to_datetime(dates))\ncompare\ncompare.plot(title='Actual total sales with trendline prediction RMSE (' + str(rmse) + ')', figsize=(24,4))\nplt.style.use('bmh')\nplt.xlabel('')\nplt.ylabel('Sales')\nplt.show()\n","206c00fc":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nvalidation = salesTrainValidationData.copy().loc[:,'d_1886':'d_1913'].sum()\nendog = salesTrainValidationData.copy().loc[:,'d_1':'d_1885'].sum().reset_index().drop(columns=['index'])\nmodel = SARIMAX(endog, order=(4, 1, 1), seasonal_order=(4, 1, 1, 7))\nfit = model.fit()\nyhat = fit.forecast(28)\nrmse = math.sqrt(metrics.mean_squared_error(validation, yhat.values))\n\ncompare = pd.DataFrame({\n    'Actual': validation,\n    'Predicted': yhat.values\n})\n\ndates = pd.to_datetime(calendarData.copy().loc[1886:1913].loc[:,'date'].values)\ncompare = compare.set_index(pd.to_datetime(dates))\ncompare.plot(title='Actual total sales with SARIMAX prediction RMSE (' + str(rmse) + ')', figsize=(24,4))\nplt.style.use('bmh')\nplt.xlabel('')\nplt.ylabel('Sales')\nplt.show()","e76ca920":"salesTrain = salesTrainValidationData.copy().loc[:,\"d_1\":\"d_1885\"].sum().to_frame()\nsalesValidation = salesTrainValidationData.copy().loc[:,\"d_1886\":\"d_1913\"].sum().to_frame()\nsalesTrain.columns = ['dailySales']\nsalesValidation.columns = ['dailySales']\ndatesTrain = calendarData[0:1885].copy()\ndatesValidation = calendarData[1885:1913].copy()\ndatesTrain = datesTrain[['weekday','month', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\ndatesValidation = datesValidation[['weekday','month', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\ndatesTrain['dailySales'] = salesTrain['dailySales'].values\ndatesValidation['dailySales'] = salesValidation['dailySales'].values\nfeaturesTrain = pd.get_dummies(datesTrain)\nfeaturesValidation = pd.get_dummies(datesValidation)\n\nfor c in featuresTrain.columns:\n    if c not in featuresValidation.columns:\n        featuresValidation[c] = c\n        featuresValidation[c] = 0\n\nlabelsTrain = np.array(featuresTrain['dailySales'])\nfeaturesTrain = featuresTrain.drop('dailySales', axis=1)\nlabelsValidation = np.array(featuresValidation['dailySales'])\nfeaturesValidation = featuresValidation.drop('dailySales', axis=1)\nfeaturesTrain = np.array(featuresTrain)\nfeaturesValidation = np.array(featuresValidation)\n\nrf = RandomForestRegressor(n_estimators=100, random_state=1)\nrf.fit(featuresTrain, labelsTrain)\npredictions = rf.predict(featuresValidation)\nrmse = math.sqrt(metrics.mean_squared_error(labelsValidation, predictions))\n\ncompare = pd.DataFrame({\n    'Actual': labelsValidation,\n    'Predicted': predictions\n})\n\ndates = pd.to_datetime(calendarData.copy().loc[1886:1913].loc[:,'date'].values)\ncompare = compare.set_index(pd.to_datetime(dates))\ncompare\ncompare.plot(title='Actual total sales last 28 days of data with RMSE (' + str(rmse) + ')', figsize=(24,4))\nplt.style.use('bmh')\nplt.xlabel('')\nplt.ylabel('Sales')\nplt.show()","c96c3649":"salesTrain = salesTrainValidationData.copy().loc[:,\"d_1\":\"d_1285\"].sum().to_frame()\nsalesValidation = salesTrainValidationData.copy().loc[:,\"d_1286\":\"d_1913\"].sum().to_frame()\nsalesTrain.columns = ['dailySales']\nsalesValidation.columns = ['dailySales']\ndatesTrain = calendarData[0:1285].copy()\ndatesValidation = calendarData[1285:1913].copy()\ndatesTrain = datesTrain[['weekday','month', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\ndatesValidation = datesValidation[['weekday','month', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'wm_yr_wk', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\ndatesTrain['dailySales'] = salesTrain['dailySales'].values\ndatesValidation['dailySales'] = salesValidation['dailySales'].values\nfeaturesTrain = pd.get_dummies(datesTrain)\nfeaturesValidation = pd.get_dummies(datesValidation)\n\nfor c in featuresTrain.columns:\n    if c not in featuresValidation.columns:\n        featuresValidation[c] = c\n        featuresValidation[c] = 0\n\nlabelsTrain = np.array(featuresTrain['dailySales'])\nfeaturesTrain = featuresTrain.drop('dailySales', axis=1)\nlabelsValidation = np.array(featuresValidation['dailySales'])\nfeaturesValidation = featuresValidation.drop('dailySales', axis=1)\nfeaturesTrain = np.array(featuresTrain)\nfeaturesValidation = np.array(featuresValidation)\n\nrf = RandomForestRegressor(n_estimators=100, random_state=1)\nrf.fit(featuresTrain, labelsTrain)\npredictions = rf.predict(featuresValidation)\nrmse = math.sqrt(metrics.mean_squared_error(labelsValidation, predictions))\n\ncompare = pd.DataFrame({\n    'Actual': labelsValidation,\n    'Predicted': predictions\n})\n\ndates = pd.to_datetime(calendarData.copy().loc[1286:1913].loc[:,'date'].values)\ncompare = compare.set_index(pd.to_datetime(dates))\ncompare\ncompare.plot(title='Actual total sales last 2 years of data with RMSE (' + str(rmse) + ')', figsize=(24,4))\nplt.style.use('bmh')\nplt.xlabel('')\nplt.ylabel('Sales')\nplt.show()","653746fa":"SARIMA is an extension of ARIMA that includes seasonality, which could be a good fit for the total sales data, due to the weekly fluctuations. As with the baseline the total sales for the whole period except for the last 28 days were used as training data, and the last 28 days to validate the fit of the model. An important parameter in our case is the m parameter which describes the number of time steps for a single seasonal period. This parameter was set to 7 to match the weekly fluctuations. The model got a score of around 2792 which is much better than the baseline.","f7218767":"> # SARIMAX","2cbd2ef9":"> # Random Forest","1248f6a9":"> # Comparison of categories","b39817d6":"**Sales Train Data**","8bcf7174":"Then we decided that it would be interesting to see if there was anything to learn from exploring the various categories subcategories which we did in the following graph.","7bcd5412":"***We found***\n* States are fairly average, taking into account that CA has 4 stores instead of 3","29d2dcdc":"***We found***\n* SNAP days does not seem to a major impact on sales\n* It could seem that the regular weekdays are slightly higher than usual when there's SNAP days","5b49bd72":"# Models","9894a85f":"***We found***\n* Of subcategories, FOODS_3 is the most popular by far, and could be what is causing the food category to outpeform hobbies and household by such a big margin","ab62a343":"We noticed that in all of our previous graphs, that sometime in december the total sales would consistently drop to 0. We suspected that these drops was caused by the store being closed during the holidays and therefore we developed the following graph. Since it's only one day it won't have the greatest impact on our predictions, and should most likely not be accounted for in the data set since its a clear outlier. ","7e89ff7b":"***We found***\n* Weekends have a significant effect on sales\n* Sales rises quickly from friday and starts to fall down on monday, while still having significant higher sales compared to regular weekdays\n* Most events do not seem to have a significant impact on sales\n* Few events seem to increase sales, but is inconclusive\n* The events are often located during weekend spikes, which makes it hard to tell if the event had a big impact\n* One or two events have a negative impact on sales\n\nThis graph concludes our initial EDA in which we learned a great deal about data. The information we obtained from here was valuable later on. ","67713b72":"At the start of the project we conducted an exploratory data analysis. This was done first and foremost to learn as much as we could about the data as we didn't have that much prior knowledge about the datasets. The EDA also helps us uncover predicatble price fluctuations in the data. These can be caused by things such as varios events, foodstamps, seansons and more. As there was enormous amounts of data to examine and consider, we decided to limit our EDA to areas of the data we thought would give us the most value in terms of making accuracte predictions. The EDA is split up into th the following categories;\n\n* Data structure\n* Investgating sales fluctuations\n* The effects of sales during Christmas\n* Comparison of stores\n* Comparison of categories and sub-categories\n* Investigating SNAPs impact on total sales\n\n","e5b69737":"For features, we selected:\n* `weekday, month, year, snap_CA, snap_TX, snap_WI, wm_yr_wk and events` from `calendar.csv` which tells what day of the year the sales are reffering to, which states had snap on the specific day aswell as any potential events.\n* `d_1, d_2, d_3 ... d_1913` from `sales_train_validation.csv` which we join with the date information.\n\nWe then trained the model on all the available days except the last 28, which we used for validation, seeing as the competition will be based on how well the predictions are for the future 28 days.\n\nAs seen on the picture above, comparing the model to the actual sales, the random forest model shows clear use of the weekly swings in sales, and we can conclude its impacted by the SNAP and event days since each weekend is predicted to be different, which is a clear signature that the other variables had an effect.","d2a58a03":"After our initial investigation into sales for each state, we wanted more information about each area. Therefore we developed the following graph in which we obtained more information about the invidivual stores in the states. For these graphs, we also used the data for the last 6 months.","6ad2c109":"We also explored the `sales_train.csv` file by calling the `.head()` method. This reveals that this dataframe also contains information about each product. The `wm_year_wk` column tells us which week we're looking at for a given product and `sell_price` tells us the averae sales price for the product in question week by week. In otherwords this file describes the average sell price for each product for each week of the 5 year period, where the other table listed how many of each product was sold each day. ","8cf1faca":"***We found***\n* All wallmart stores have 0 sales christmas day, most likely because they are closed","ec347a6d":"For the models, we decided to do it on a total sales per day basis, instead of the competitions format of predicting on each item. This was done to reduce the complexity of the models since we mostly wanted to see how the different models holds up against each other.","51806430":"Afterwards we again checked if there was any noticeable discrepancy between subcategories between the different states from the data.","b594d8dd":"***We found***\n* Theres an overall upwards trend\n* Seasonal trends (sales go slightly down in the later months of each year)\n* All shops have have a day at the end of the year where they sell 0 items\n* Constant fluctuations throughout indicating days with low sales followed by days with high sales","8a74ff74":"> # Initial EDA","326d982a":"This random forest is using the same type of data as the one with a 28-day prediction. After having made the first one, we wanted to see how well we could predict sales over a longer period, this being 2 years, to see how well it would hold up.\n\nAs can be seen on the graph, the predictions gets worse the further ahead we are predicting, which is an indicator that the random forest model is not taking the overall upwards trend of wallmart sales into account.\n\nBesides this, we feel it holds up quite well for the first year or so of predicting, seeing as the training data is substantially smaller in this model since the prediction part of the data split is bigger.","de1d91a0":"*We found*\n* There are 10 stores across 3 states","9c956904":"After the `.csv` files was loaded and the initial setup was completed we decided to investigate the contents of the files briefly by calling the `.head()` method on the files. We already had a faint idea what the files would contain, as we had looked at the [guildelines](https:\/\/mofc.unic.ac.cy\/m5-competition\/) outlying the scope and frame of the project. As previously mentioned we worked with three different data sets which are;\n\n* `sales_train.csv`\n* `sell_prices.csv`\n* `calender.csv`\n","141b7b27":"***We found***\n* All states seem to be following the pattern of food being the topselling category, followed by hobbies and household","6aedce08":"A baseline is a simple prediction that can be used to evaluate whether or not the predictions from a more advanced model is good. For a baseline we have calculated the trend of total sales for the whole period except for the last 28 days, which will be used to score the prediction. When compared with the actual values for the last 28 days, the baseline got a RMSE score of 7205.","1dd2d436":"***We found***\n* The pattern from before remains true across all states","0821033d":"## Data Structure","1271669b":"> # Investigating sale fluctuations","3a5df7bb":"***We found***\n* It remains hard to tell if SNAP has a major impact on sales\n* The pattern of when SNAP days appear can seem to have an impact on the weekend fluctuations, leading to a more stable pattern when the SNAP days are spread out","5a1fb346":"Lastly we wanted to see if the individual states were doing SNAP differently and how this would affect the data. ","a348e39c":"To learn more about the data we decided it would be relevant to look at the total volume of sales from all stores in all states over the entire period. We were hoping that this would uncover any obvious points of interest in the data. We constructed the following graph to visually represent the total sales over the 5 year period.","952e4333":"> # Comparison of stores","29fd9394":"> # Investigating SNAPs impact on total sales","dfc1a305":"We wanted to explore if the different states were equal in terms of sales or if some states were outperforming others. In addition to this, we also just wanted to see if there was something interested to learn from developing and studying this graph. Inorder to have a manageable dataset we selected the most recent 6 months of the data to study.","d249e52b":"We explored the `sales_train.csv` file by calling the `.head()` method. We found that the dataframe contains a list of all the products available in the stores. We also found that the dataframe has columns describing which category it belongs to, which store it's from in which state and how many of each given product was sold each day during the 5 year period.","42d84364":"Next we wanted to investigate the categories from the dataset. We did this in hopes that it would have some clues we could use during our prediction. First we made a graph which shows the total distribution of sales over all categories from all states\/stores.","caecd9e4":"> # The effects of sales during christmas","4d1b7f5c":"To end our EDA, we wanted to investigate if SNAP and SNAP days has a noticeable effect on sales. If they do, we wanted to know how much they impacted sales and how we could use this information in our predictions. To investigate this we developed the following graphs.","75ed9d2d":"Next we wanted to further explore why the previous graph was so fluctuating so much. We suspected that these rapid fluctuations was due to weekends and events. Therefore we decided to develop the following graph to see if weekends and events really was the cause of the constant spiking. We felt that since the fluctuations were so constant, that a snapshot of 6 months of the data would be easier to work with while investigtaing the sales fluctuations. This pattern of only looking a 6 months at a time will repeat from here on out. Looking at too big a dataset doesnt make sense and it makes the data easier to handle. Looking at a graph that describes a 5 years period is too big and umanageable, which makes it hard to get any usefull visual representation from them.","3e9ad2d5":"Afterwards we wanted to further investigate if there was a descrepancy between sales in the various categories in different states. However based on the previous graph we were fully expecting foods to be ahead followed by household and hobby categories.","c4e5d331":"> # Baseline\n","e6b7c2db":"***We found***\n* CA_3 is outpeforming the other stores in CA by a considerable amount, while CA_4 is underperforming\n* TX and WI stores are all performing at about the same amount\n* The differences in stores performing could be caused by outside forces such as nearby population and location","f49cdfa1":"***We found***\n* Foods are by far the topselling category\n* Hobbies are selling the least, slightly behind household","26ac8022":"Lastly we also explored the `calender.csv` file by calling the .head() method. This revelas a dataframe with several interesting columns. The dataframe has the same `wm_yr_wk` column as the `sales_price.csv` file which again shows us which week each specific day belongs to. The dataframe also gives us information about events through the `event_name_1` and `event_type_1`. There are also columns called `event_name2` and `event_type2`, which we learned was only there because you cannot have lists in a dataframe. This ultimately means that the only reason theres duplicate event name and type columns is incase there are multiple events during the same day. Lastly, the dataframe also gives us information about which days are and aren't SNAP days through the `snap_CA`,`snap_TX` and `snap_WI` columns. SNAP days are days in which customers eligible for the [SNAP](https:\/\/www.fns.usda.gov\/snap\/eligible-food-items) program are eligible to spend the stamps in a given store.","abb45087":"This is our project, which was developed as a semesterproject in the subject AI during our second semester of the softwaredevelopment bachelor. \nOur project contains an exploratory data analysis (EDA) over specific parts of the data, which helped us gain insight into the workings of the datasets. Furthermore the goal of the EDA was to uncover potential variables which could potentially alter the predictions we made based on the dataset. After the completion of the EDA, we used what we learned about the data to make predictions about the sales trends by using various predictions models such as SARIMAX.\n\nSo what are we trying to predict? The goal of the challenge is to predict sales data for a period of 28 days for Wallmart. This can be achieved since we've been given three different data sets, which describes the sales trends for the past 5 years. The data consists of 3049 individual products from the three different categories foods, household and hobbies. The products comes from 10 different stores across 3 different states. The data contains sales data for all these product for a period of 1942 days, which rougly adds up to 5 years.\n\nLet's take a closer look at the datasets we've been handed;\n\n* `sales_train.csv` This is our main training data. It contains columns over the days in the dataset and the amount of sales made of a given product for each specific day. This dataset also gives us other various information about each specific product such as `state_id`, `store_id`, `department_id`, `category_id` and `item_id`.\n* `sell_prices.csv` Contains a list of the stores and item ID's together with the sales prices of the item as a weekly average.\n* `calender.csv` This dataset contains a list of all the dates we're working with. This dataset also contains information about events such as the superbowl, christmas etc. which predictably alters the sales trend for certain periods. This helps us make better educated guesses as to why the data behaves as it does, and it also gives us information about predictable fluctuations in sales trends. The calendar dataset also gives us information about [SNAP](https:\/\/www.fns.usda.gov\/snap\/eligible-food-items) and SNAP days. This is relevant as days where the stamps are eligible for use in the stores, potentially can alter sales trends. This will be researched in greater length during the EDA. \n\n","2623e57f":"The first thing we did was to find out exactly how many stores we were working with. We already knew the answer from studying the guidelines, but we mostly wanted to test the data.","b2700573":"**Calendar Data**","52fdd899":"**Sell Prices Data**","52239c34":"# Introduction","c8315013":"# Exploratory Data Analysis (EDA)"}}