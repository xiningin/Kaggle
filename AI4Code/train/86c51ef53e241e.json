{"cell_type":{"b59f33d1":"code","f43d9b83":"code","3cfae6cf":"code","099d3b52":"code","ee696789":"code","780259a7":"code","5508cc6f":"code","48e31f11":"code","2274cdc4":"code","9cb21034":"code","5e3d92db":"code","c0564757":"code","08ea154a":"code","d9fc18f8":"code","d9b28852":"code","d6764b8a":"code","2f857fbb":"code","4fa73547":"code","720b56e9":"code","c2510076":"code","aedbc46b":"code","6dc2d6eb":"code","2d1a2fdb":"code","5b935ba5":"code","0a9bb506":"code","dbbf09f8":"code","19fbc3db":"code","b122ca69":"code","419e2977":"code","811a600b":"code","0d27a8ca":"code","c50dcb0e":"markdown","ed262451":"markdown","14a376f7":"markdown","039720f4":"markdown","fc8af53c":"markdown","339c9529":"markdown","434a537d":"markdown","1d193c91":"markdown","97c2c15d":"markdown","a0683b5a":"markdown","0870767b":"markdown","f60a677c":"markdown","0798d871":"markdown","98803e1b":"markdown","02735dc3":"markdown","255d8216":"markdown","b9246bf0":"markdown","70ae2e34":"markdown","9d41ac1c":"markdown","3e5bfbde":"markdown","3033e6f4":"markdown","621d79e3":"markdown","4910beee":"markdown"},"source":{"b59f33d1":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport itertools\nfrom sklearn.preprocessing import LabelEncoder\n\n\nsns.set()\n\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntrain['date'] = pd.to_datetime(train['date'], format = '%d.%m.%Y', infer_datetime_format = True)\n\ncats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsub = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","f43d9b83":"f, axes = plt.subplots(2, 2, figsize = (17,9))\ng0 = sns.distplot(train.item_price, kde = False, ax = axes[0,0])\ng1 = sns.boxplot(train.item_price, ax = axes[0,1])\ng2 = sns.distplot(train.item_price[train.item_price.between(0,10000)], bins = 100, kde = False, ax = axes[1,0])\ng3 = sns.boxplot(train.item_price[train.item_price.between(0,10000)], ax = axes[1,1], fliersize = 2)\n\nf.suptitle('Item Price - Distribution and Boxplot', fontsize = 16, fontweight='bold')\ng0.set_xlabel('Item Price', size = 14)\ng1.set_xlabel('Item Price', size = 14)\ng2.set_xlabel('Item Price', size = 14)\ng3.set_xlabel('Item Price', size = 14);","3cfae6cf":"f, axes = plt.subplots(2, 2, figsize = (17,9))\ng0 = sns.distplot(train.item_cnt_day, kde = False, ax = axes[0,0])\ng1 = sns.boxplot(train.item_cnt_day, ax = axes[0,1])\ng2 = sns.distplot(train.item_cnt_day[train.item_cnt_day.between(0,100)], bins = 100, kde = False, ax = axes[1,0])\ng3 = sns.boxplot(train.item_cnt_day[train.item_cnt_day.between(0,100)], ax = axes[1,1], fliersize = 2)\n\nf.suptitle('Item Count per Day - Distribution and Boxplot', fontsize=16, fontweight='bold')\ng0.set_xlabel('Item Count per Day', size = 14)\ng1.set_xlabel('Item Count per Day', size = 14)\ng2.set_xlabel('Item Count per Day', size = 14)\ng3.set_xlabel('Item Count per Day', size = 14);","099d3b52":"f, axes = plt.subplots(1, 1, figsize = (17,5))\ng = sns.barplot(x = 'shop_id', y = 'item_cnt_day', data = train,\n                estimator = lambda x: np.sum(x) \/ np.sum(train.item_cnt_day) * 100,\n                order = train.groupby('shop_id').sum()['item_cnt_day'].sort_values(ascending = False).index,\n                color = 'orange', ci = None)\ng.set_xlabel('Shop ID', size = 14)\ng.set_ylabel('Sold Items (%)', size = 14);","ee696789":"train_month_year = train.groupby(['date_block_num']).sum().reset_index().drop(columns = ['item_price', 'shop_id', 'item_id'])\n\nf, axes = plt.subplots(1, 1, figsize = (17,5))\ng = sns.lineplot(x = 'date_block_num', y = 'item_cnt_day', data = train_month_year)\ng.set_xlabel('Month', size = 14)\ng.set_ylabel('Item Count Month', size = 14)\ng.set_xlim(0, 33);","780259a7":"sales_by_item_id = train.pivot_table(index = ['item_id'], values = ['item_cnt_day'], columns = 'date_block_num',\n                                     aggfunc = np.sum, fill_value = 0).reset_index()\nsales_by_item_id.columns = ['item_id'] + list(sales_by_item_id.columns.droplevel()[1:].map(str))\n\noutdated_items_3mo = sales_by_item_id[sales_by_item_id.loc[:,'31':].sum(axis=1)==0]\noutdated_items_6mo = sales_by_item_id[sales_by_item_id.loc[:,'28':].sum(axis=1)==0]\n\nprint('Outdated items (3 months):', 100.0*outdated_items_3mo.shape[0]\/sales_by_item_id.shape[0])\nprint('Outdated items (6 months):', 100.0*outdated_items_6mo.shape[0]\/sales_by_item_id.shape[0])","5508cc6f":"sales_by_shop_id = train.pivot_table(index = ['shop_id'], values = ['item_cnt_day'], columns = 'date_block_num',\n                                     aggfunc = np.sum, fill_value = 0).reset_index()\nsales_by_shop_id.columns = ['shop_id'] + list(sales_by_shop_id.columns.droplevel()[1:].map(str))\n\noutdated_shop_3mo = sales_by_shop_id[sales_by_shop_id.loc[:,'31':].sum(axis=1)==0]\noutdated_shop_6mo = sales_by_shop_id[sales_by_shop_id.loc[:,'28':].sum(axis=1)==0]\n\nprint('Outdated shops (3 months):', 100.0*outdated_shop_3mo.shape[0]\/sales_by_shop_id.shape[0])\nprint('Outdated shops (6 months):', 100.0*outdated_shop_6mo.shape[0]\/sales_by_shop_id.shape[0])","48e31f11":"train_featured = train.copy()\ntest_featured = test.copy()\n\ntrain_featured.loc[train_featured.shop_id == 11, 'shop_id'] = 10\ntest_featured.loc[test_featured.shop_id == 11, 'shop_id'] = 10\n\ntrain_featured.loc[train_featured.shop_id == 1, 'shop_id'] = 58\ntest_featured.loc[test_featured.shop_id == 1, 'shop_id'] = 58\n\ntrain_featured.loc[train_featured.shop_id == 0, 'shop_id'] = 57\ntest_featured.loc[test_featured.shop_id == 0, 'shop_id'] = 57\n\ntrain_featured.loc[train_featured.shop_id == 40, 'shop_id'] = 39\ntest_featured.loc[test_featured.shop_id == 40, 'shop_id'] = 39\n\nshops_featured = shops.copy()\nshops_featured['shop_name'] = shops_featured['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops_featured['shop_city'] = shops_featured['shop_name'].str.partition(' ')[0]\nshops_featured['shop_type'] = shops_featured['shop_name'].apply(lambda x: '\u043c\u0442\u0440\u0446' if '\u043c\u0442\u0440\u0446' in x else '\u0442\u0440\u0446' if '\u0442\u0440\u0446' in x else '\u0442\u0440\u043a' if '\u0442\u0440\u043a' in x else '\u0442\u0446' if '\u0442\u0446' in x else '\u0442\u043a' if '\u0442\u043a' in x else 'NO_DATA')\n\nshops_featured['shop_city'] = LabelEncoder().fit_transform(shops_featured['shop_city'])\nshops_featured['shop_type'] = LabelEncoder().fit_transform(shops_featured['shop_type'])\n\nshops_featured.drop(columns = 'shop_name', inplace = True)\nshops_featured.head()","2274cdc4":"cats_featured = cats.copy()\ncats_featured['split'] = cats_featured['item_category_name'].str.split('-')\ncats_featured['category_type'] = cats_featured['split'].map(lambda x: x[0].strip())\ncats_featured['category_subtype'] = cats_featured['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n\ncats_featured['category_type'] = LabelEncoder().fit_transform(cats_featured['category_type'])\ncats_featured['category_subtype'] = LabelEncoder().fit_transform(cats_featured['category_subtype'])\n\n\ncats_featured = cats_featured[['item_category_id','category_type', 'category_subtype']]\ncats_featured.head()","9cb21034":"items_featured = items.copy()\nitems_featured.drop(columns = ['item_name'], inplace = True)\nitems_featured.head()","5e3d92db":"# # Creating base dataframe with all the possible permutations between shops and items present on the test set\n# import itertools\n\n# ts = time.time()\n\n# date_block_nums = np.array(range(35))\n# shops_test = test.shop_id.unique()\n# items_test = test.item_id.unique()\n# base = pd.DataFrame(itertools.product(date_block_nums, shops_test, items_test),\n#                     columns = ['date_block_num','shop_id','item_id'])\n\n# time.time() - ts","c0564757":"from itertools import product\n\nts = time.time()\n\nbase = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_featured[train_featured.date_block_num==i]\n    base.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype = 'int16'))\n    \nbase = pd.DataFrame(np.vstack(base), columns=cols)\nbase.sort_values(cols,inplace = True)\n\ntime.time() - ts","08ea154a":"#Creating Revenue Feature\ntrain_featured['revenue'] = train_featured['item_price'] *  train_featured['item_cnt_day']\n\n# Eliminating \"item_cnt_day\" and \"item_price\" outliers\ntrain_featured = train_featured[(train_featured.item_cnt_day.between(0,1000)) & (train_featured.item_price.between(0,100000))]\n\n# Aggregating by month\ntrain_featured_month = (train_featured[['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']]\n                        .groupby(['date_block_num','shop_id','item_id'])\n                        .sum())\ntrain_featured_month.reset_index(inplace = True)\ntrain_featured_month['item_cnt_month'] = train_featured_month['item_cnt_day'].clip(0,20).fillna(0)\ntrain_featured_month.drop(columns = 'item_cnt_day', inplace = True)\n\n# Preparating test set\ntest_featured['date_block_num'] = 34\ntest_featured = test_featured[['date_block_num', 'shop_id', 'item_id']]\n\n# Concatenating train and test set\nbase = pd.concat([base, test_featured])\n\n# Concatenating shops and categories information\ndf_raw = pd.merge(base, train_featured_month, on = ['date_block_num', 'shop_id', 'item_id'], how = 'left').fillna(0)\ndf_raw = pd.merge(df_raw, shops_featured, on = ['shop_id'], how='left')\ndf_raw = pd.merge(df_raw, items_featured, on = ['item_id'], how='left')\ndf_raw = pd.merge(df_raw, cats_featured, on = ['item_category_id'], how='left')\n\n# Adding month and year feature\ndf_raw['year'] = df_raw['date_block_num'].apply(lambda x: ((x\/\/12) + 2013))\ndf_raw['month'] = df_raw['date_block_num'].apply(lambda x: (x % 12) + 1)\n\ndf_raw.head()","d9fc18f8":"df_raw_1 = df_raw.copy()\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num', 'shop_id', 'item_id', col + '_lag_' + str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on = ['date_block_num', 'shop_id', 'item_id'], how = 'left')\n    return df\n\ndf_raw_1 = lag_feature(df_raw_1, [1, 2, 3, 4, 5, 6], 'item_cnt_month')\n\ndataset = df_raw_1.copy()","d9b28852":"ts = time.time()\n\n# Add total average price per item feature\ngroup = train_featured.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace = True)\ndataset = pd.merge(dataset, group, on = ['item_id'], how = 'left')\n\n# Add monthly average price per item feature\ngroup = train_featured.groupby(['date_block_num', 'item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace = True)\ndataset = pd.merge(dataset, group, on = ['date_block_num', 'item_id'], how = 'left')\n\nlags = [1, 2, 3, 4, 5, 6]\n\ndef lag_feature_2(df, lags, col):\n    tmp = df[['item_avg_item_price','date_block_num', 'shop_id', 'item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['item_avg_item_price', 'date_block_num', 'shop_id', 'item_id', col + '_lag_' + str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on = ['item_avg_item_price', 'date_block_num', 'shop_id', 'item_id'], how = 'left')\n    return df\n\ndataset = lag_feature_2(dataset, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    dataset['delta_price_lag_' + str(i)] = \\\n        (dataset['date_item_avg_item_price_lag_' + str(i)] - dataset['item_avg_item_price']) \/ dataset['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_' + str(i)]:\n            return row['delta_price_lag_' + str(i)]\n    return 0\n    \ndataset['delta_price_lag'] = dataset.apply(select_trend, axis=1)\ndataset['delta_price_lag'] = dataset['delta_price_lag'].astype(np.float16)\ndataset['delta_price_lag'].fillna(0, inplace = True)\n\nfeatures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    features_to_drop += ['date_item_avg_item_price_lag_' + str(i)]\n    features_to_drop += ['delta_price_lag_' + str(i)]\n\ndataset.drop(features_to_drop, axis = 1, inplace = True)\n\ntime.time() - ts","d6764b8a":"# ts = time.time()\n# cache = {}\n# dataset['item_shop_last_sale'] = -1\n# dataset['item_shop_last_sale'] = dataset['item_shop_last_sale'].astype(np.int8)\n# for idx, row in dataset.iterrows():    \n#     key = str(row.item_id)+' '+str(row.shop_id)\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         dataset.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n#         cache[key] = row.date_block_num         \n\n# ####################################\n\n# cache = {}\n# dataset['item_last_sale'] = -1\n# dataset['item_last_sale'] = dataset['item_last_sale'].astype(np.int8)\n# for idx, row in dataset.iterrows():    \n#     key = row.item_id\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         if row.date_block_num>last_date_block_num:\n#             dataset.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n#             cache[key] = row.date_block_num\n\n# ####################################\n\n# dataset['item_shop_first_sale'] = dataset['date_block_num'] - dataset.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n# dataset['item_first_sale'] = dataset['date_block_num'] - dataset.groupby('item_id')['date_block_num'].transform('min')\n# time.time() - ts","2f857fbb":"# features_to_encode = ['item_id',\n#                       'shop_id', 'shop_city', 'shop_type',\n#                       'item_category_id', 'category_type', 'category_subtype',\n#                       'year', 'month']\n\n# for feature_i in features_to_encode:\n#     gp_feature_mean = dataset[dataset.date_block_num < 34].groupby([feature_i]).agg({'item_cnt_month': ['mean']})\n#     gp_feature_mean.columns = [feature_i + '_mean']\n#     gp_feature_mean.reset_index(inplace = True)\n\n#     dataset = pd.merge(dataset, gp_feature_mean, on = [feature_i], how = 'left')","4fa73547":"list_features_to_encode = [['date_block_num'],\n                           ['date_block_num', 'item_id'],\n                           ['date_block_num', 'shop_id'],\n                           ['date_block_num', 'item_category_id'],\n                           ['date_block_num', 'shop_id', 'item_category_id']]\n\nfor list_feature_i in list_features_to_encode:\n    print(list_feature_i)\n    gp_feature_mean = dataset.groupby(list_feature_i).agg({'item_cnt_month': ['mean']})\n    column_name = '_'.join(list_feature_i) + '_mean'\n    gp_feature_mean.columns = [column_name]\n    gp_feature_mean.reset_index(inplace = True)\n\n    dataset = pd.merge(dataset, gp_feature_mean, on = list_feature_i, how = 'left')\n    dataset = lag_feature(dataset, [1,2], column_name)\n    dataset.drop([column_name], axis = 1, inplace=True)","720b56e9":"for shop_id in dataset['shop_id'].unique():\n    for column in dataset.columns:\n        shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n        \n        dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median","c2510076":"del train\ndel cats\ndel items\ndel sub\ndel shops\ndel test\ndel train_featured\ndel cats_featured\ndel items_featured\ndel shops_featured\ndel test_featured\ndel df_raw\ndel df_raw_1\ndel base","aedbc46b":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndataset = downcast_dtypes(dataset)","6dc2d6eb":"dataset.to_pickle('final_dataset.pkl')","2d1a2fdb":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost\nimport pickle\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","5b935ba5":"dataset = pd.read_pickle('\/kaggle\/working\/final_dataset.pkl')\n\ntrain_set = dataset[dataset.date_block_num.between(12,32)]\nvalidation_set = dataset[dataset.date_block_num == 33]\ntest_set = dataset[dataset.date_block_num == 34]\n\nX_train = train_set.drop(columns = ['item_cnt_month'])\nY_train = train_set['item_cnt_month']\n\nX_validation = validation_set.drop(columns = ['item_cnt_month'])\nY_validation = validation_set['item_cnt_month']\n\nX_test = test_set.drop(columns = ['item_cnt_month'])\nY_test = test_set['item_cnt_month']","0a9bb506":"del dataset\ndel train_set\ndel validation_set\ndel test_set","dbbf09f8":"dtrain = xgb.DMatrix(X_train, label = Y_train)\ndvalidation = xgb.DMatrix(X_validation, label = Y_validation)\ndtest = xgb.DMatrix(X_test, label = Y_test)\n\nwatchlist = [(dtrain, 'train'), (dvalidation, 'validation')]\n\nparams = {'objective': 'reg:squarederror',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'rmse',\n          'eta': 0.1,\n          'max_depth': 4,\n          'random_state': 0}\n\nmodel_xgb = xgb.train(params, dtrain, 1000, watchlist, early_stopping_rounds = 10)\npickle.dump(model_xgb, open(\"model_xgb.pickle.dat\", \"wb\"))","19fbc3db":"model_cat = CatBoostRegressor(\n    task_type = 'GPU',\n    iterations = 1000,\n    random_seed = 0,\n    learning_rate = 0.1,\n    od_type = 'Iter',\n    od_wait = 10,\n    eval_metric = 'RMSE'\n)\n\nmodel_cat.fit(\n    X_train, Y_train,\n    eval_set = (X_validation, Y_validation),\n    logging_level = 'Verbose',\n    plot = False\n)\n\npickle.dump(model_cat, open(\"model_cat.pickle.dat\", \"wb\"))","b122ca69":"params = {\n    \"objective\" : \"regression\",\n    #\"device_type\": \"gpu\",\n    \"metric\" : \"rmse\",\n    \"learning_rate\": 0.01,\n    \"seed\": 0\n}\n\nlgtrain = lgb.Dataset(X_train, label = Y_train)\nlgval = lgb.Dataset(X_validation, label = Y_validation)\nlgtest = lgb.Dataset(X_test, label = Y_test)\nevals_result = {}\nmodel_light = lgb.train(params, lgtrain, 1000, \n                  valid_sets = [lgtrain, lgval], \n                  early_stopping_rounds = 20, \n                  evals_result = evals_result)\n\npickle.dump(model_light, open(\"model_light.pickle.dat\", \"wb\"))","419e2977":"# Importing Predictions\nmodel_xgb = pickle.load(open(\"\/kaggle\/working\/model_xgb.pickle.dat\", \"rb\"))\nmodel_cat = pickle.load(open(\"\/kaggle\/working\/model_cat.pickle.dat\", \"rb\"))\nmodel_light = pickle.load(open(\"\/kaggle\/working\/model_light.pickle.dat\", \"rb\"))\n\n# XGBoost Predictions\ny_pred_train_xgb = model_xgb.predict(dtrain).clip(0,20)\ny_pred_validation_xgb = model_xgb.predict(dvalidation).clip(0,20)\ny_pred_test_xgb = model_xgb.predict(dtest).clip(0,20)\n\n# Catboost Predictions\ny_pred_train_cat = model_cat.predict(X_train).clip(0,20)\ny_pred_validation_cat = model_cat.predict(X_validation).clip(0,20)\ny_pred_test_cat = model_cat.predict(X_test).clip(0,20)\n\n# LightGBM Predictions\ny_pred_train_light = model_light.predict(X_train).clip(0,20)\ny_pred_validation_light = model_light.predict(X_validation).clip(0,20)\ny_pred_test_light = model_light.predict(X_test).clip(0,20)\n\n# Mounting train dataframe for stacking\ndf_train = pd.concat([pd.DataFrame(y_pred_train_xgb), pd.DataFrame(y_pred_train_cat), pd.DataFrame(y_pred_train_light), Y_train.reset_index()['item_cnt_month']], axis = 1)\ndf_train.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']\n\n# Mounting vaidation dataframe for stacking\ndf_validation = pd.concat([pd.DataFrame(y_pred_validation_xgb), pd.DataFrame(y_pred_validation_cat), pd.DataFrame(y_pred_validation_light), Y_validation.reset_index()['item_cnt_month']], axis = 1)\ndf_validation.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']\n\n# Mounting test dataframe for stacking\ndf_test = pd.concat([pd.DataFrame(y_pred_test_xgb), pd.DataFrame(y_pred_test_cat), pd.DataFrame(y_pred_test_light), Y_test.reset_index()['item_cnt_month']], axis = 1)\ndf_test.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']","811a600b":"meta_model = LinearRegression(n_jobs = -1)\n\ndf_train_X = df_train.drop(columns = 'Y')\ndf_train_Y = df_train['Y']\n\ndf_validation_X = df_validation.drop(columns = 'Y')\ndf_validation_Y = df_validation['Y']\n\ndf_test_X = df_test.drop(columns = 'Y')\ndf_test_Y = df_test['Y']\n\nmeta_model.fit(df_train_X, df_train_Y)\nfinal_pred_train = meta_model.predict(df_train_X).clip(0,20)\nfinal_pred_validation = meta_model.predict(df_validation_X).clip(0,20)\nfinal_pred_test = meta_model.predict(df_test_X).clip(0,20)\n\nprint('Stacking | RMSE on Train Set:', np.sqrt(mean_squared_error(final_pred_train, df_train_Y)))\nprint('Stacking | RMSE on Validation Set:', np.sqrt(mean_squared_error(final_pred_validation, df_validation_Y)))\nprint(' XGBoost | RMSE on Validation Set:', ((df_validation.Y - df_validation.XGBoost)**2).mean()**0.5)\nprint('Catboost | RMSE on Validation Set:', ((df_validation.Y - df_validation.Catboost)**2).mean()**0.5)\nprint('LightGBM | RMSE on Validation Set:', ((df_validation.Y - df_validation.LightGBM)**2).mean()**0.5)","0d27a8ca":"# Saving Predictions\ndef submit_df(df_raw):\n    df_out = df_raw.reset_index()\n    df_out.columns = ['ID', 'item_cnt_month']\n    return df_out\n\nsubmit_df(pd.DataFrame(y_pred_test_xgb)).to_csv('y_pred_test_xgb.csv', index = False)\nsubmit_df(pd.DataFrame(y_pred_test_cat)).to_csv('y_pred_test_cat.csv', index = False)\nsubmit_df(pd.DataFrame(y_pred_test_light)).to_csv('y_pred_test_light.csv', index = False)\nsubmit_df(pd.DataFrame(final_pred_test)).to_csv('y_pred_test_stack.csv', index = False)","c50dcb0e":"#### Trend Features","ed262451":"#### Saving Dataset","14a376f7":"#### Outliers","039720f4":"### 4.3) LightGBM","fc8af53c":"#### Outdated items","339c9529":"### 2) Feature Engineering","434a537d":"#### Outdated shops","1d193c91":"#### Organizing and Extracting Features - Categories","97c2c15d":"#### Organizing and Extracting Features - Items","a0683b5a":"### 4.2) Catboost","0870767b":"### 4) Model Training\n\n- **XGBoost**\n\n- **CatBoost**\n\n- **LightGBM**\n\n- **Model Stacking**","f60a677c":"#### Months since the last sale for each shop\/item pair and for item","0798d871":"#### Organizing dataset","98803e1b":"#### Filling Missing Values","02735dc3":"#### Organizing and Extracting Features - Shops","255d8216":"#### Lag Features","b9246bf0":"### 4.4) Model Stacking and Predictions","70ae2e34":"#### Mean Encoding","9d41ac1c":"#### Time Series Visualization","3e5bfbde":"#### Sold Items x Shops","3033e6f4":"## Predict Future Sales\n\nThese notebook was based on three kernels, mostly on the first one. Thank very much [**dlarionov**](https:\/\/www.kaggle.com\/dlarionov\/), [**dimitreoliveira**](https:\/\/www.kaggle.com\/dimitreoliveira\/), and [**kyakovlev**](https:\/\/www.kaggle.com\/kyakovlev\/)!\n\n- https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n- https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda\n- https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\n\n### 1) Exploratory Data Analysis\n\n#### Importing Packages and Datasets","621d79e3":"### 3) Train\/Test Split\n\n- **`Train set`**: months 0 - 32\n\n- **`Validation set`**: month 33\n\n- **`Test set`**: month 34","4910beee":"### 4.1) XGBoost"}}