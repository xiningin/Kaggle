{"cell_type":{"afc925bf":"code","f7bef9bc":"code","c8cb6dad":"code","b8b38e16":"code","e24563ba":"code","9bb56ade":"code","baa29e09":"code","e7c1e43a":"code","b42258ec":"code","38bd7853":"code","30ad18a5":"code","80c2612f":"code","5db61ce4":"code","a6babe07":"code","af6d3807":"code","8a6bf84a":"code","8d36ca26":"code","d4ff8a01":"code","f390e6b8":"code","735de092":"code","74c42f6b":"code","d5365f4b":"code","e943212c":"code","0867df49":"code","3c01f27a":"code","5c3da32a":"code","a8f76c06":"code","911499bf":"code","2b8ad768":"code","3bde5ea6":"code","b1571111":"code","759d5dc8":"code","cdd318a0":"code","6f5afc4f":"code","5d8111a9":"code","be42a748":"code","72c39af0":"code","8b489695":"code","51ec2c77":"code","b25c539b":"code","8c55796d":"code","bd8a5f3e":"code","0c3ba0c5":"code","de47b50b":"code","6ec1c33b":"code","f932de38":"code","b623b35c":"code","ec7aae63":"code","de49a87f":"code","01afc5a8":"code","f4a8255b":"code","11e4c57c":"code","419b8ad2":"code","ab03d98d":"code","64371a6e":"code","09de1c6d":"code","8e674936":"code","4359e897":"code","657b4c2d":"code","40eb4675":"code","fcf5b4ed":"code","d3a624d4":"code","73c212b9":"code","f42d37a6":"code","f5d8b03c":"code","5efa0378":"markdown","1dd4cbe2":"markdown","fcc43e81":"markdown","f255bd91":"markdown","58eb9c36":"markdown","1d521df4":"markdown","65e455bb":"markdown","10db616a":"markdown","20ae232f":"markdown","8f1af743":"markdown","14492bb3":"markdown","e127fe99":"markdown","92f113aa":"markdown","eef4df9b":"markdown","dbd029d2":"markdown","fe6be9b7":"markdown","8bd8def6":"markdown","be3757b8":"markdown","7f3d8337":"markdown","c6e9277c":"markdown","99ecafd5":"markdown","be3cf504":"markdown","d7fe2b89":"markdown"},"source":{"afc925bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7bef9bc":"!pip3 install --user --upgrade plotly","c8cb6dad":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n%matplotlib inline","b8b38e16":"train_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_set","e24563ba":"train_set.columns = map(str.lower, train_set.columns)\ntrain_set","9bb56ade":"train_set.info()","baa29e09":"train_set.describe()","e7c1e43a":"import pandas_profiling as pp","b42258ec":"pp.ProfileReport(train_set)","38bd7853":"train_set.head()","30ad18a5":"train_set[train_set.fare<1].info()","80c2612f":"train_set.groupby('survived').count().passengerid","5db61ce4":"fig = px.pie(train_set.groupby(\"survived\").count(), values='passengerid', names=[\"not survived\", \"survived\"], title='Percentage of Survivers', \n             color_discrete_sequence=px.colors.qualitative.Set2, hole=.3)\nfig.show()","a6babe07":"train_set.groupby(['survived', 'sex'])['passengerid'].count()","af6d3807":"fig = px.sunburst(train_set, path=['sex', 'survived'], values='passengerid',color_discrete_sequence=px.colors.qualitative.Pastel2)\nfig.show()","8a6bf84a":"def titles(df):\n    \n    df['title'] = df['name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    \n    return df\n","8d36ca26":"titles(train_set)\n","d4ff8a01":"normalized_titles = {\"Capt\":\"o\",\n                     \"Col\":\"o\",\n                     \"Major\":\"o\",\n                     \"Jonkheer\":\"r\",\n                     \"Don\":\"r\",\n                     \"Sir\" :\"r\",\n                     \"Dr\":\"o\",\n                     \"Rev\":\"o\",\n                     \"the Countess\":\"r\",\n                     \"Dona\":\"r\",\n                     \"Mme\":\"Mrs\",\n                     \"Mlle\":\"Miss\",\n                     \"Ms\":\"Mrs\",\n                     \"Mr\" :\"Mr\",\n                     \"Mrs\" :\"Mrs\",\n                     \"Miss\":\"Miss\",\n                     \"Master\":\"Master\",\n                     \"Lady\":\"Mrs\"}\n\ndef normalize_titles(df):\n  \n    df['normalized_title'] = df['name'].apply(lambda name: name.split(',')[1].split('.')[0].strip()).map(normalized_titles)\n    \n    return df","f390e6b8":"normalize_titles(train_set)\n","735de092":"def fill_age(df):\n    title_list = []\n    df['n_age'] = pd.DataFrame(np.zeros((len(train_set), 1)))\n    \n    \n    for i in df.title.unique():\n        title_list.append(i)\n    \n        \n    for x in title_list:\n        title_with_ages = df[df.title == x]['age']\n        mean = title_with_ages.mean()\n        \n        df[x] = title_with_ages.fillna(round(mean,0))\n        \n        df[x]=df[x].fillna(0)\n        \n        df['n_age'] += df[x].values\n        \n        del df[x]\n    \n        \n    return df","74c42f6b":"fill_age(train_set)\n","d5365f4b":"def age_grouping(df):\n    \n    df['Infants'] = df['n_age'].apply(lambda x: 1 if x < 1 else 0)\n    df['Child'] = df['n_age'].apply(lambda x: 1 if x < 18 else 0)\n    df['Adult'] = df['n_age'].apply(lambda x: 1 if x >= 18 and x < 65 else 0)\n    df['Elderly'] = df['n_age'].apply(lambda x: 1 if x >= 65 else 0)\n    \n    return df","e943212c":"age_grouping(train_set)\n","0867df49":"train_set['cabin'].isnull().value_counts()\n","3c01f27a":"def cabin_type(df):\n    df['cabin_type'] = df['cabin'].astype(str).str[0]\n    return df","5c3da32a":"cabin_type(train_set)","a8f76c06":"fare_group = train_set.groupby('cabin_type').mean('fare').reset_index()[['cabin_type','fare']]\n\nfig = px.bar(fare_group, x='cabin_type', y='fare')\nfig.show()","911499bf":"cabin_type_group = train_set.groupby(['cabin_type', 'survived']).count().reset_index()[['survived','cabin_type','passengerid']]\ncabin_type_group","2b8ad768":"cabin_type_group[['survived']] = cabin_type_group[['survived']].astype(str)\nfig = px.bar(cabin_type_group, x='cabin_type', y='passengerid', color='survived')\nfig.show()","3bde5ea6":"train_set.groupby('embarked').count()","b1571111":"def family(df):\n    \n    df[\"family\"] = df[\"sibsp\"] + df[\"parch\"]\n    df[\"family\"] = df[\"family\"].apply(lambda x: 1 if x>0 else 0)\n      \n    return df","759d5dc8":"family(train_set)\n","cdd318a0":"train_set[train_set.cabin_type == 'n']","6f5afc4f":"def cabin_filling(df):\n    \n    grouping_fare = df.groupby('cabin_type').mean('fare').reset_index()[['cabin_type','fare']]\n    \n    a_mean = grouping_fare[grouping_fare.cabin_type == 'A']['fare'].tolist()[0]\n    b_mean = grouping_fare[grouping_fare.cabin_type == 'B']['fare'].tolist()[0]\n    c_mean = grouping_fare[grouping_fare.cabin_type == 'C']['fare'].tolist()[0]\n    d_mean = grouping_fare[grouping_fare.cabin_type == 'D']['fare'].tolist()[0]\n    e_mean = grouping_fare[grouping_fare.cabin_type == 'E']['fare'].tolist()[0]\n    f_mean = grouping_fare[grouping_fare.cabin_type == 'F']['fare'].tolist()[0]\n    g_mean = grouping_fare[grouping_fare.cabin_type == 'G']['fare'].tolist()[0]    \n\n    x = {\n        'A': a_mean,\n        'B': b_mean, \n        'C': c_mean, \n        'D': d_mean, \n        'E': e_mean, \n        'F': f_mean,\n        'G': g_mean\n    }\n    \n    sorted_x = sorted(x.items(), key=lambda kv: kv[1])\n\n    def findCabinType(itemFare):\n        for cabinType, cabinTypeMeanFare in sorted_x:\n            if itemFare <= cabinTypeMeanFare:\n                return cabinType\n\n        return 'B'\n        \n    df['cabin_type2'] = df[df.cabin_type == 'n']['fare'].apply(findCabinType)\n    df['cabin_type2'].fillna(df['cabin_type'], inplace=True)\n    df['cabin_type'] = df['cabin_type2']\n    del df['cabin_type2']\n    \n    return df\n\ncabin_filling(train_set)","5d8111a9":"# train_set = cabin_filling(train_set)\n# train_set[train_set.cabin_type == 'n']","be42a748":"def get_dummies(df):\n    \n    #for embarked column\n    \n    embarked = pd.get_dummies(df.embarked, prefix = 'Emb')\n    df = pd.concat([df,embarked],axis = 1)\n    df = df.drop(['embarked'],axis = 1)\n    \n    #for sex column\n    \n    sex = pd.get_dummies(df.sex, prefix = 'sex')\n    df = pd.concat([df,sex],axis = 1)\n    df = df.drop(['sex', 'sex_female'],axis = 1)\n    \n    #for normalized_title column \n    \n    normalized_title = pd.get_dummies(df.normalized_title, prefix = 'title')\n    df = pd.concat([df,normalized_title],axis = 1)\n    df = df.drop(['normalized_title'],axis = 1)\n    \n    #for cabin_type column\n    \n    cabin_type = pd.get_dummies(df.cabin_type, prefix = 'cabin_type')\n    df = pd.concat([df,cabin_type],axis = 1)\n    df = df.drop(['cabin_type'],axis = 1)\n    \n    return df","72c39af0":"get_dummies(train_set)","8b489695":"train_set.columns","51ec2c77":"def transform(df):\n    df= titles(df)\n    df=normalize_titles(df)\n    df=fill_age(df)\n    df=age_grouping(df)\n    df= cabin_type(df)\n    df = family(df)\n    df=cabin_filling(df)\n    df= get_dummies(df)\n    \n    df = df.drop(['name','age', 'ticket', 'cabin', 'title'], axis=1)\n    \n    \n    return df","b25c539b":"train_set = transform(train_set)\n# train_set = train_set.drop('cabin_type_T', axis=1)\n# train_set","8c55796d":"train_set = train_set.drop('cabin_type_T', axis=1)\ntrain_set","bd8a5f3e":"train_set.info()","0c3ba0c5":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_set.drop(columns=[\"survived\"]), train_set[\"survived\"], random_state = 42)  ","de47b50b":"# from sklearn.preprocessing import StandardScaler\n\n# def scale_test(x_train, x_test = None):\n#     scaler = StandardScaler()\n#     scaler.fit(x_train)\n#     if x_test is None:\n#         return pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n#     return pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)","6ec1c33b":"# x_train = scale_test(x_train)\n# x_test = scale_test(x_test)","f932de38":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef classification_metrics(y_test, predict):\n    print(\"Confusion Matrix:\\n\")\n    print(confusion_matrix(y_test, predict))\n    print(\"\\nAccuracy: \", accuracy_score(y_test, predict))\n    print(\"\\nClassification report:\\n\")\n    print(classification_report(y_test, predict))","b623b35c":"from sklearn.linear_model import RidgeClassifier\n\nrcC = RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n                max_iter=None, normalize=False, random_state=None,\n                solver='auto', tol=0.001)\nrcC.fit(x_train, y_train)\npredict = pd.DataFrame(data=rcC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","ec7aae63":"from sklearn.linear_model import LogisticRegression\n\nlrC = LogisticRegression(max_iter=10000, random_state=0)\nlrC.fit(x_train, y_train)\npredict = pd.DataFrame(data=lrC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","de49a87f":"from sklearn.svm import SVC\n\nsvC = SVC(random_state=0)\nsvC.fit(x_train, y_train)\npredict = pd.DataFrame(data=svC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","01afc5a8":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(x_train, y_train)\npredict = pd.DataFrame(data=dtc.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","f4a8255b":"from sklearn.ensemble import ExtraTreesClassifier\netC = ExtraTreesClassifier(random_state=0, n_estimators=10)\n\netC.fit(x_train, y_train)\npredict = pd.DataFrame(data=etC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","11e4c57c":"from sklearn.ensemble import RandomForestClassifier\nrfC = RandomForestClassifier(criterion='entropy', max_depth= 8, max_leaf_nodes=30, min_samples_leaf=30, n_estimators= 600, random_state=0)\n\nrfC.fit(x_train, y_train)\npredict = pd.DataFrame(data=rfC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","419b8ad2":"from sklearn.ensemble import AdaBoostClassifier\n\nabC = AdaBoostClassifier(random_state=0, n_estimators=10, learning_rate=0.9)\nabC.fit(x_train, y_train)\npredict = pd.DataFrame(data=abC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","ab03d98d":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbC = GradientBoostingClassifier(random_state=0, n_estimators=10)\ngbC.fit(x_train, y_train)\npredict = pd.DataFrame(data=gbC.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","64371a6e":"from xgboost import XGBClassifier\n\nxgB = XGBClassifier(random_state=0)\nxgB.fit(x_train, y_train)\npredict = pd.DataFrame(data=xgB.predict(x_test), index = x_test.index)\nclassification_metrics(y_test, predict)","09de1c6d":"testData = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntestData.columns = map(str.lower, testData.columns)\n\ntestData","8e674936":"testData = transform(testData)\ntestData\n","4359e897":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n\nclfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \nparams = []\nfor name, model  in clfs:\n    #kfold = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, x_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)    \n    params.append(model.get_params)\n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)","657b4c2d":"groundTruth = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col= \"PassengerId\")\ngroundTruth","40eb4675":"testData = testData.fillna(testData.fare.mean())\ntestData.info()","fcf5b4ed":"testData","d3a624d4":"xgB = XGBClassifier(random_state=0, n_estimators=7)\nxgB.fit(x_train, y_train)\npredict = pd.DataFrame(data=xgB.predict(testData), index = testData.index)\nxgb_accuracy = accuracy_score(groundTruth, predict)\nprint('xgb_accuracy:', xgb_accuracy)\n\n\ngbC = GradientBoostingClassifier(random_state=0, n_estimators=8)\ngbC.fit(x_train, y_train)\npredict = pd.DataFrame(data=gbC.predict(testData), index = testData.index)\ngbc_accuracy = accuracy_score(groundTruth, predict)\nprint('gbc_accuracy:', gbc_accuracy)\n\n\nabC = AdaBoostClassifier(random_state=0, n_estimators=6, learning_rate=0.9)\nabC.fit(x_train, y_train)\npredict = pd.DataFrame(data=abC.predict(testData), index = testData.index)\nadaBoost_accuracy = accuracy_score(groundTruth, predict)\nprint('adaBoost_accuracy:', adaBoost_accuracy)\n\n    \nrfC = RandomForestClassifier(criterion='entropy', max_depth= 8, max_leaf_nodes=30, min_samples_leaf=30, n_estimators= 600, random_state=0, max_features='log2')\nrfC.fit(x_train, y_train)\npredict = pd.DataFrame(data=rfC.predict(testData), index = testData.index)\nrfC_accuracy = accuracy_score(groundTruth, predict)\nprint('rfC_accuracy:', rfC_accuracy)\n\n\netC = ExtraTreesClassifier(random_state=0, n_estimators=10)\netC.fit(x_train, y_train)\npredict = pd.DataFrame(data=etC.predict(testData), index = testData.index)\netC_accuracy = accuracy_score(groundTruth, predict)\nprint('etC_accuracy:', etC_accuracy)\n\n\nrcC = RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n                    max_iter=None, normalize=False, random_state=None,\n                    solver='auto', tol=0.001)\n\nrcC.fit(x_train, y_train)\npredict = pd.DataFrame(data=rcC.predict(testData), index = testData.index)\nrcC_accuracy = accuracy_score(groundTruth, predict)\nprint('rcC_accuracy:', rcC_accuracy)\n\nlrC = LogisticRegression(max_iter=45, random_state=0)\nlrC.fit(x_train, y_train)\npredict = pd.DataFrame(data=lrC.predict(testData), index = testData.index)\nlrC_accuracy = accuracy_score(groundTruth, predict)\nprint('lrC_accuracy:', lrC_accuracy)\n\nsvC = SVC(random_state=0)\nsvC.fit(x_train, y_train)\npredict = pd.DataFrame(data=svC.predict(testData), index = testData.index)\nsvC_accuracy = accuracy_score(groundTruth, predict)\nprint('svC_accuracy:', svC_accuracy)","73c212b9":"testData","f42d37a6":"sub = pd.DataFrame()\nsub['Passengerid'] = testData['passengerid']\nsub['Survived'] = rfC.predict(testData)\nsub.to_csv('submission_rfc5.csv',index=False)\n","f5d8b03c":"sub","5efa0378":"### SVM","1dd4cbe2":"### Embarked","fcc43e81":"### Decision Tree","f255bd91":"### Age","58eb9c36":"### ExtraTreesClassifier","1d521df4":"### Deleting text and unnecessary columns","65e455bb":"### AdaBoost","10db616a":"## Reading Data","20ae232f":"### Clasification Metrics","8f1af743":"### Cabin","14492bb3":"### Ridge Classifier","e127fe99":"### Logistic Regression","92f113aa":"## Import Libraries","eef4df9b":"### Random Forest","dbd029d2":"## Survival Info in General","fe6be9b7":"### XGBoost","8bd8def6":"## Get Dummies","be3757b8":"# Exploratory Data Analysis","7f3d8337":"## Null Check","c6e9277c":"# Modelling","99ecafd5":"## Feature Engineering","be3cf504":"### Gradient Boosting","d7fe2b89":"### Family or Not"}}