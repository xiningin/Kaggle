{"cell_type":{"dcfd7e3c":"code","01081806":"code","ce479401":"code","836a6775":"code","ccb70055":"code","2ad34ea5":"code","2978fdca":"code","d2f739cb":"code","bab0e7be":"code","d191a30e":"code","5eeed175":"code","c7fc705b":"code","6679d30e":"code","a2b989c5":"code","8ada965d":"code","d2f817f6":"code","c29fbfe7":"code","ea7320f6":"code","322f600d":"code","6f7943bb":"code","cda2797b":"code","853ee9e9":"code","f0e40b6a":"code","b9e20c12":"code","ae174c49":"code","db73587c":"code","a629cf5e":"code","2caf5f45":"code","47cf6b4f":"code","d0449e95":"code","1689ef0e":"code","f4628c42":"code","869e3e9b":"code","1f11e2ee":"code","d497c9ee":"code","170f4459":"code","bc968ad1":"code","426a517b":"code","0bb88385":"code","03de09b3":"code","6fd76d77":"code","c697c50c":"code","1f860a0c":"code","4d61b7f8":"code","2c0d118e":"code","6d7e3d55":"code","8c18570a":"code","0074b50b":"code","b23eb50a":"code","116e20f2":"code","7feaa1d8":"code","e8720bd5":"code","7d50acd4":"code","49142df5":"code","2303b870":"code","8290bb2c":"code","cd6860c6":"code","a48012e0":"code","e401b0d0":"code","b2673516":"code","26d35af5":"code","42b1b48a":"code","f351011a":"code","0cf9d7b4":"code","5ee2fd05":"code","cb540544":"code","54f0d6ab":"code","4ebbd23b":"code","27a2faa9":"code","e2396759":"code","9805ea2a":"code","64b91123":"code","7bc70e8e":"code","fd579c77":"code","240fa552":"code","cf0b3bab":"code","18f16ae9":"code","a3751ffc":"code","3ff8fbc1":"code","d94a433d":"code","fd0e21b8":"code","4a4255b0":"code","eb8d8a07":"code","47d70a3d":"code","8e0bd539":"code","74b3ab31":"code","b1276bc9":"code","05242524":"code","65a3fe7c":"code","11d3e5cb":"code","01946cde":"code","c4a017ce":"code","4efa6392":"code","1288fd38":"code","869461cb":"code","cd80b59a":"code","9dd974a9":"code","80005764":"code","df7a6c10":"code","c2fe3bfc":"code","2d282916":"code","20eae83b":"markdown","7ba19432":"markdown","31dbc574":"markdown","953dcbde":"markdown","0fd7c870":"markdown","656e07dc":"markdown","79b1dd6d":"markdown","83d8a07e":"markdown","90599b67":"markdown","54db8864":"markdown","8f49de28":"markdown","bea52c67":"markdown","db314fd4":"markdown","206769fc":"markdown","f1b62a19":"markdown","744fbb4e":"markdown","18adecfd":"markdown","5afe454f":"markdown","912f3842":"markdown","32feb334":"markdown","d68cebe6":"markdown","d0d1022b":"markdown","532d4623":"markdown","f7a0c3d6":"markdown","ccbbdc3d":"markdown","d4d1e559":"markdown","ab1bf182":"markdown","5a91ea59":"markdown","44b15de0":"markdown","b6975462":"markdown","62217b02":"markdown","d591d1dd":"markdown","13bebe39":"markdown","bd58b10c":"markdown","4f7b0141":"markdown","607e489f":"markdown","a68e6e02":"markdown","69b5d021":"markdown","326ff473":"markdown","91f8ba13":"markdown","f90cad6d":"markdown","ed77c4db":"markdown","ebd65a43":"markdown","23d4595e":"markdown","eb09c65c":"markdown"},"source":{"dcfd7e3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01081806":"# Import necessary library\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport random\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats.distributions import norm\nfrom sklearn.neighbors import KernelDensity\nfrom scipy.stats import gaussian_kde\nfrom statsmodels.nonparametric.kde import KDEUnivariate\nfrom statsmodels.nonparametric.kernel_density import KDEMultivariate\nimport scipy as sp","ce479401":"def my_kde(data, width=1, gridsize=100, normalized=True, bounds=None):\n    \"\"\"\n    Compute the gaussian KDE from the given sample.\n\n    Args:\n        data (array or list): sample of values\n        width (float): width of the normal functions\n        gridsize (int): number of grid points on which the kde is computed\n        normalized (bool): if True the KDE is normalized (default)\n        bounds (tuple): min and max value of the kde\n\n    Returns:\n        The grid and the KDE\n    \"\"\"\n    # boundaries\n    if bounds:\n        xmin, xmax = bounds\n    else:\n        xmin = min(data) - 3 * width\n        xmax = max(data) + 3 * width\n\n    # grid points\n    x = np.linspace(xmin, xmax, gridsize)\n\n    # compute kde\n    kde = np.zeros(gridsize)\n    for val in data:\n        kde += norm.pdf(x, loc=val, scale=width)\n\n    # normalized the KDE\n    if normalized:\n        kde \/= sp.integrate.simps(kde, x)\n\n    return x, kde","836a6775":"def kde_sklearn(x, x_grid, bandwidth=0.1, **kwargs):\n    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(x[:, np.newaxis])\n    # score_samples() returns the log-likelihood of the samples\n    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n    return np.exp(log_pdf)\n\nfrom sklearn.neighbors import KernelDensity\n\ndef kde2D(x, y, bandwidth, xbins=100j, ybins=100j, **kwargs): \n    \"\"\"Build 2D kernel density estimate (KDE).\"\"\"\n\n    # create grid of sample locations (default: 100x100)\n    xx, yy = np.mgrid[x.min():x.max():xbins, \n                      y.min():y.max():ybins]\n\n    xy_sample = np.vstack([yy.ravel(), xx.ravel()]).T\n    xy_train  = np.vstack([y, x]).T\n\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(xy_train)\n\n    # score_samples() returns the log-likelihood of the samples\n    z = np.exp(kde_skl.score_samples(xy_sample))\n    return xx, yy, np.reshape(z, xx.shape)","ccb70055":"np.random.seed(42)\nmu1, sigma1 = 5, 1 # mean and standard deviation\ns1 = np.random.normal(mu1, sigma1, 1000)\ns1","2ad34ea5":"# Verify the mean and the variance:\nabs(mu1 - np.mean(s1))","2978fdca":"abs(sigma1 - np.std(s1, ddof=1))","d2f739cb":"# Display the histogram of the samples, along with the probability density function:\nimport matplotlib.pyplot as plt\n\ncount, bins, ignored = plt.hist(s1, 30, density=True)\n\nplt.plot(bins, 1\/(sigma1 * np.sqrt(2 * np.pi)) *\n\n               np.exp( - (bins - mu1)**2 \/ (2 * sigma1**2) ),\n\n         linewidth=2, color='r')\n\nplt.show()","bab0e7be":"# first lets try with scipy library\nfrom sklearn.neighbors import KernelDensity\nx_d = np.linspace(0, 10, 1000)\n# instantiate and fit the KDE model\nkde = KernelDensity(bandwidth=1, kernel='gaussian')\nkde.fit(s1[:, None])\n\n# score_samples returns the log of the probability density\nlogprob = kde.score_samples(x_d[:, None])\n\nplt.fill_between(x_d, np.exp(logprob), alpha=0.7)\nplt.plot(s1, np.full_like(s1, -0.01), '|k', markeredgewidth=1)\nplt.ylim(-0.02, 0.3)","d191a30e":"# our functions with the computed bandwidth\nx, kde = my_kde(s1, width=1, gridsize=200)\n# plot\nfig, ax = plt.subplots()\nax.plot(x, kde, label=\"by hands\", linewidth=3, alpha=0.5)\nax.hist(s1, 30, fc='gray', histtype='stepfilled', alpha=0.7, density = True)\nax.set_xlim(0, 10)\nax.legend(loc='upper left')","5eeed175":"fig, ax = plt.subplots()\nfor bandwidth in [0.1,1, 5, 10]:\n    ax.plot(x_d, kde_sklearn(s1, x_d, bandwidth=bandwidth),\n            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\nax.hist(s1, 30, fc='gray', histtype='stepfilled', alpha=0.7, density = True)\nax.set_xlim(0, 10)\nax.legend(loc='upper left')","c7fc705b":"# Selecting the bandwidth via cross-validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import LeaveOneOut\n\nbandwidths = 10 ** np.linspace(-1, 10, 100)\ngrid = GridSearchCV(KernelDensity(kernel='gaussian'),\n                    {'bandwidth': bandwidths},\n                    cv=LeaveOneOut().get_n_splits(s1))\ngrid.fit(s1[:, None]);","6679d30e":"grid.best_params_","a2b989c5":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(KernelDensity(),\n                    {'bandwidth': np.linspace(-1, 10, 100)},\n                    cv=20) # 20-fold cross-validation\ngrid.fit(s1[:, None])\nprint (grid.best_params_)","8ada965d":"kde = grid.best_estimator_\npdf = np.exp(kde.score_samples(x_d[:, None]))\n\nfig, ax = plt.subplots()\nax.plot(x_d, pdf, linewidth=3, alpha=0.5, label='bw=%.2f' % kde.bandwidth)\nax.hist(s1, 30, fc='green', histtype='stepfilled', alpha=0.7, density = True)\nax.legend(loc='upper left')\nax.set_xlim(0, 10);","d2f817f6":"np.random.seed(42)\nmu1, sigma1 = 5, 1 # mean and standard deviation\ns1 = np.random.normal(mu1, sigma1, 1000)\n# Display the histogram of the samples, along with the probability density function:\nimport matplotlib.pyplot as plt\n\ncount, bins, ignored = plt.hist(s1, 30, density=True)\n\nplt.plot(bins, 1\/(sigma1 * np.sqrt(2 * np.pi)) *\n\n               np.exp( - (bins - mu1)**2 \/ (2 * sigma1**2) ),\n\n         linewidth=2, color='r')\n\nplt.show()","c29fbfe7":"fig, ax = plt.subplots()\nfor bandwidth in [0.1,1, 5, 10]:\n    ax.plot(x_d, kde_sklearn(s1, x_d, bandwidth=bandwidth),\n            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\nax.hist(s1, 30, fc='gray', histtype='stepfilled', alpha=0.7, density = True)\nax.set_xlim(0, 10)\nax.legend(loc='upper left')","ea7320f6":"kde = grid.best_estimator_\npdf = np.exp(kde.score_samples(x_d[:, None]))\n\nfig, ax = plt.subplots()\nax.plot(x_d, pdf, linewidth=3, alpha=0.5, label='bw=%.2f' % kde.bandwidth)\nax.hist(s1, 30, fc='green', histtype='stepfilled', alpha=0.7, density = True)\nax.legend(loc='upper left')\nax.set_xlim(0, 10);","322f600d":"np.random.seed(42)\nmu2, sigma2 = 0, 0.2 # mean and standard deviation\ns2 = np.random.normal(mu2, sigma2, 1000)\n# Display the histogram of the samples, along with the probability density function:\nimport matplotlib.pyplot as plt\n\ncount, bins, ignored = plt.hist(s2, 30, density=True)\n\nplt.plot(bins, 1\/(sigma2 * np.sqrt(2 * np.pi)) *\n\n               np.exp( - (bins - mu2)**2 \/ (2 * sigma2**2) ),\n\n         linewidth=2, color='r')\n\nplt.show()","6f7943bb":"x_d2 = np.linspace(-10, 10, 1000)","cda2797b":"fig, ax = plt.subplots()\nfor bandwidth in [0.1,1, 5, 10]:\n    ax.plot(x_d2, kde_sklearn(s2, x_d2, bandwidth=bandwidth),\n            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\nax.hist(s2, 30, fc='gray', histtype='stepfilled', alpha=0.7, density = True)\nax.set_xlim(-1, 2)\nax.legend(loc='upper right')","853ee9e9":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(KernelDensity(),\n                    {'bandwidth': np.linspace(-1, 10, 100)},\n                    cv=20) # 20-fold cross-validation\ngrid.fit(s2[:, None])\nprint (grid.best_params_)","f0e40b6a":"kde = grid.best_estimator_\npdf = np.exp(kde.score_samples(x_d2[:, None]))\n\nfig, ax = plt.subplots()\nax.plot(x_d2, pdf, linewidth=3, alpha=0.5, label='bw=%.2f' % kde.bandwidth)\nax.hist(s2, 30, fc='green', histtype='stepfilled', alpha=0.7, density = True)\nax.legend(loc='upper left')\nax.set_xlim(-1, 2);","b9e20c12":"# lets generate 1st set of data \nmean1 = [1, 0]\ncov1= [[0.9, 0.40], [0.40, 0.90]] \n# Plot the data pattern\nnp.random.seed(42)\nx1, y1 = np.random.multivariate_normal(mean1, cov1, 500).T\nplt.plot(x1, y1, 'x',c=\"orange\")\nplt.title(\"label=2-D Gaussian random data with N1 = 500\")\nplt.axis('equal')\nplt.show()","ae174c49":"x1","db73587c":"import pandas as pd\ndf1= pd.DataFrame({\"x\":x1, \"y\":y1})\ndf1['class'] = 1\nprint(df1)","a629cf5e":"# lets generate 2nd set of data \nmean2 = [0, 2.5]\ncov2= [[0.9, 0.40], [0.40, 0.90]] \n# Plot the data pattern\nnp.random.seed(42)\nx2, y2 = np.random.multivariate_normal(mean2, cov2, 500).T\nplt.plot(x2, y2, 'x',c=\"orange\")\nplt.title(\"label=2-D Gaussian random data with N1 = 500\")\nplt.axis('equal')\nplt.show()","2caf5f45":"df2= pd.DataFrame({\"x\":x2, \"y\":y2})\ndf2['class'] = 2\nprint(df2)","47cf6b4f":"# To generate X commbine df1,df2 together\nX_data = pd.concat([df1, df2], axis=0)\n# Shuffle the data frame\nX_data = X_data.sample(frac = 1) \n# check the shape to get confrmation of row joining\nX_data","d0449e95":"# Now take x and y value only for clustering\nx3=X_data['x'].values \nprint(x3)\n","1689ef0e":"x3.shape","f4628c42":"y3=X_data['y'].values \nprint(y3)","869e3e9b":"y3.shape","1f11e2ee":"#For bandwidth=0.10\nx, y = x3 + y3, x3 - y3\n\nxx, yy, zz = kde2D(x, y,0.1)\nplt.pcolormesh(xx, yy, zz)\nplt.scatter(x, y, s=2, facecolor='white')\nplt.title('2D Gaussian Kernel density estimation')\nplt.xlabel(\"X3\")\nplt.ylabel(\"Y3\")","d497c9ee":"#For bandwidth=1.0\nx, y = x3 + y3, x3 - y3\n\nxx, yy, zz = kde2D(x, y,1)\nplt.pcolormesh(xx, yy, zz)\nplt.scatter(x, y, s=2, facecolor='white')\nplt.title('2D Gaussian Kernel density estimation')\nplt.xlabel(\"X3\")\nplt.ylabel(\"Y3\")","170f4459":"#For bandwidth=5\nx, y = x3 + y3, x3 - y3\n\nxx, yy, zz = kde2D(x, y,5)\nplt.pcolormesh(xx, yy, zz)\nplt.scatter(x, y, s=2, facecolor='white')\nplt.title('2D Gaussian Kernel density estimation')\nplt.xlabel(\"X3\")\nplt.ylabel(\"Y3\")","bc968ad1":"#For bandwidth=10\nx, y = x3 + y3, x3 - y3\n\nxx, yy, zz = kde2D(x, y,10)\nplt.pcolormesh(xx, yy, zz)\nplt.scatter(x, y, s=2, facecolor='white')\nplt.title('2D Gaussian Kernel density estimation')\nplt.xlabel(\"X1\")\nplt.ylabel(\"Y1\")","426a517b":"import numpy as np\n \ndef PCA(X , num_components):\n     \n    #Step-1\n    X_meaned = X - np.mean(X , axis = 0)\n     \n    #Step-2\n    cov_mat = np.cov(X_meaned , rowvar = False)\n     \n    #Step-3\n    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n     \n    #Step-4\n    sorted_index = np.argsort(eigen_values)[::-1]\n    sorted_eigenvalue = eigen_values[sorted_index]\n    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n     \n    #Step-5\n    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n     \n    #Step-6\n    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n     \n    return X_reduced","0bb88385":"# Imports\nimport torch\nimport torchvision\nfrom torchvision.datasets import MNIST\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","03de09b3":"train = pd.read_csv(\"\/kaggle\/input\/mnist-data\/mnist_train.csv\")\ntrain","6fd76d77":"# extract label\ntrain_label = train['label']\ntrain_label.head()","c697c50c":"# Drop the label\ntrain.ld = train.drop(\"label\", axis=1)\ntrain.ld.head()","1f860a0c":"#check the shape\nprint(train.ld.shape)\nprint(train_label.shape)","4d61b7f8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","2c0d118e":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 100\n\ngrid_data = train.ld.iloc[idx].to_numpy().reshape(28,28) # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(train_label[idx])","6d7e3d55":"# get PCA for MNIST training data\npca_train=PCA(train.ld , 16)\npca_train","8c18570a":"print(pca_train.shape)","0074b50b":"pca_dataframe=pd.DataFrame(pca_train, columns=[\"pc1\", \"pc2\",\"pc3\", \"pc4\",\"pc5\", \"pc6\",\"pc7\", \"pc8\",\"pc9\", \"pc10\",\"pc11\", \"pc12\",\"pc13\", \"pc14\",\"pc15\", \"pc16\"])\nprint (pca_dataframe)","b23eb50a":"pca_train_with_label=pd.merge(pca_dataframe,train_label, left_index=True, right_index=True)\npca_train_with_label","116e20f2":"pca_train_with_label.shape","7feaa1d8":"test= pd.read_csv(\"\/kaggle\/input\/mnist-data\/mnist_test.csv\")\ntest","e8720bd5":"# extract label\ntest_label = test['label']\ntest_label.head()","7d50acd4":"# Drop the label\ntest.ld = test.drop(\"label\", axis=1)\ntest.ld.head()","49142df5":"#check the shape\nprint(test.shape)","2303b870":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nfrom time import time\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nimport math","8290bb2c":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()\n","cd6860c6":"n_components = 16\nt0 = time()\npca = PCA(n_components=n_components, svd_solver='randomized',\n          whiten=True).fit(train.ld)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nX_train_pca = pca.transform(train.ld)","a48012e0":"X_train_pca.shape","e401b0d0":"X_train_pca","b2673516":"pca.components_","26d35af5":"pca.components_.shape","42b1b48a":"X_train_pca ","f351011a":"plt.hist(pca.explained_variance_ratio_, bins=n_components, log=True)\npca.explained_variance_ratio_.sum()","0cf9d7b4":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 16\npca_data = pca.fit_transform(train.ld)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","5ee2fd05":"# attaching the label for each 16-d data point \npca_data = np.vstack((pca_data.T, train_label)).T\npca_data ","cb540544":"pca_data.shape","54f0d6ab":"# ploting the projected train data points with seaborn\nimport seaborn as sn\n\nsn.FacetGrid(pca_train_with_label, hue=\"label\", size=6).map(plt.scatter, 'pc1', 'pc2').add_legend()\nplt.show()","4ebbd23b":"# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"pc1\", \"pc2\",\"pc3\", \"pc4\",\"pc5\", \"pc6\",\"pc7\", \"pc8\",\"pc9\", \"pc10\",\"pc11\", \"pc12\",\"pc13\", \"pc14\",\"pc15\", \"pc16\", \"label\"))","27a2faa9":"\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, 'pc1', 'pc2').add_legend()\nplt.show()","e2396759":"# Plots the image represented by a row\ndef plot_number(row, w=28, h=28, labels=True):\n    if labels:\n        # the first column contains the label\n        label = row[0]\n        # The rest of columns are pixels\n        pixels = row[1:]\n    else:\n        label = ''\n        # The rest of columns are pixels\n        pixels = row[0:]\n    \n#    print(row.shape, pixels.shape)\n        \n\n    # Make those columns into a array of 8-bits pixels\n    # This array will be of 1D with length 784\n    # The pixel intensity values are integers from 0 to 255\n    pixels = 255-np.array(pixels, dtype='uint8')\n\n    # Reshape the array into 28 x 28 array (2-dimensional array)\n    pixels = pixels.reshape((w, h))\n\n    # Plot\n    if labels:\n        plt.title('Label is {label}'.format(label=label))\n    plt.imshow(pixels, cmap='gray')\n\n# Plots a whole slice of pictures\ndef plot_slice(rows, size_w=28, size_h=28, labels=True):\n    num = rows.shape[0]\n    w = 4\n    h = math.ceil(num \/ w)\n    fig, plots = plt.subplots(h, w)\n    fig.tight_layout()\n\n    for n in range(0, num):\n        s = plt.subplot(h, w, n+1)\n        s.set_xticks(())\n        s.set_yticks(())\n        plot_number(rows.iloc[n], size_w, size_h, labels)\n    plt.show()","9805ea2a":"train.shape","64b91123":"# Plot first 12 orginal image\nplot_slice(train[0:12])","7bc70e8e":"## Plot first 10 PCA image from sckit learn","fd579c77":"X_train_pca.shape","240fa552":"approximation0 = pca.inverse_transform(X_train_pca)\napproximation0","cf0b3bab":"plt.figure(figsize=(16,8));\n#scale=np.abs(approximation0).max()\n# Logistic model weights plot\nfor i in range(10):\n    model_plot=plt.subplot(2, 5, i+1)\n    model_plot.imshow(approximation0[i].reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',clim=(0, 255))\n","18f16ae9":"pca = PCA(.95)\nlower_dimensional_data = pca.fit_transform(train.ld)\npca.n_components_","a3751ffc":"n_components = 30\nt0 = time()\npca = PCA(n_components=n_components, svd_solver='randomized',\n          whiten=True).fit(train.ld)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nX_train_pca = pca.transform(train.ld)","3ff8fbc1":"plt.hist(pca.explained_variance_ratio_, bins=n_components, log=True)\npca.explained_variance_ratio_.sum()","d94a433d":"approximation = pca.inverse_transform(X_train_pca)\napproximation","fd0e21b8":"train_ld_a=np.array(train.ld)\ntrain_ld_a","4a4255b0":"plt.figure(figsize=(8,4));\n# Original Image\nplt.subplot(1, 2, 1);\nplt.imshow(train_ld_a[1].reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('784 components', fontsize = 14)\nplt.title('Original Image', fontsize = 20);\n\n\n# 30 principal components\nplt.subplot(1, 2, 2);\nplt.imshow(approximation[1].reshape(28, 28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('30 components', fontsize = 14)\nplt.title('73% of Explained Variance', fontsize = 20);\n","eb8d8a07":"# if n_components is not set all components are kept (784 in this case)\npca = PCA()\npca.fit(train.ld)\n# Summing explained variance\ntot = sum(pca.explained_variance_)\ntot\nvar_exp = [(i\/tot)*100 for i in sorted(pca.explained_variance_, reverse=True)] \nprint(var_exp[0:5])\ntot = sum(pca.explained_variance_)\ntot\nvar_exp = [(i\/tot)*100 for i in sorted(pca.explained_variance_, reverse=True)] \nprint(var_exp[0:5])\n# Cumulative explained variance\ncum_var_exp = np.cumsum(var_exp)","47d70a3d":"# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \nplt.figure(figsize=(10, 5))\nplt.step(range(1, 785), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.title('Cumulative Explained Variance as a Function of the Number of Components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\nplt.axhline(y = 90, color='c', linestyle='--', label = '90% Explained Variance')\nplt.axhline(y = 85, color='r', linestyle='--', label = '85% Explained Variance')\nplt.axhline(y = 73, color='g', linestyle='--', label = '73% Explained Variance')\nplt.legend(loc='best')\nplt.show()","8e0bd539":"import time\nimport numpy as np\n\nfrom sklearn.datasets import fetch_openml # MNIST data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state","74b3ab31":"# apply logistic regressor with 'sag' solver, epoches=10, C is the inverse regularization strength\n\nclf = LogisticRegression(C=1e5,\n                         multi_class='multinomial',\n                         penalty='l2', solver='sag', tol=0.1,max_iter=10)\n# fit data\nclf.fit(train.ld, train_label)\n# percentage of nonzero weights\nsparsity = np.mean(clf.coef_ == 0) * 100\n# compute accuracy\nscore = clf.score(test.ld, test_label)\n\n#display run time\nrun_time = time.time() - t0\nprint('Example run in %.3f s' % run_time)\n\nprint(\"Sparsity with L2 penalty: %.2f%%\" % sparsity)\nprint(\"Test score with L2 penalty: %.4f\" % score)","b1276bc9":"weight_orginal=clf.coef_\nweight_orginal","05242524":"from time import time\nn_components = 30\n#t0 = time()\npca = PCA(n_components=n_components, svd_solver='randomized',\n          whiten=True).fit(train.ld)\n#print(\"done in %0.3fs\" % (time() - t0))\n\nX_train_pca = pca.transform(train.ld)","65a3fe7c":"X_train_pca =pd.DataFrame(X_train_pca, columns=[\"pc1\", \"pc2\",\"pc3\", \"pc4\",\"pc5\", \"pc6\",\"pc7\", \"pc8\",\"pc9\", \"pc10\",\"pc11\", \"pc12\",\"pc13\", \"pc14\",\"pc15\", \"pc16\",\"pc17\", \"pc18\",\"pc19\", \"pc20\",\"pc21\", \"pc22\",\"pc23\", \"pc24\",\"pc25\", \"pc26\",\"pc27\", \"pc28\",\"pc29\", \"pc30\"])\nX_train_pca","11d3e5cb":"print(X_train_pca.shape)\nprint(train_label.shape)","01946cde":"X_test_pca = pca.transform(test.ld)","c4a017ce":"X_test_pca ","4efa6392":"X_test_pca =pd.DataFrame(X_test_pca, columns=[\"pc1\", \"pc2\",\"pc3\", \"pc4\",\"pc5\", \"pc6\",\"pc7\", \"pc8\",\"pc9\", \"pc10\",\"pc11\", \"pc12\",\"pc13\", \"pc14\",\"pc15\", \"pc16\",\"pc17\", \"pc18\",\"pc19\", \"pc20\",\"pc21\", \"pc22\",\"pc23\", \"pc24\",\"pc25\", \"pc26\",\"pc27\", \"pc28\",\"pc29\", \"pc30\"])\nX_test_pca","1288fd38":"print(X_test_pca.shape)\nprint(test_label.shape)","869461cb":"# apply logistic regressor with 'sag' solver, epoches=10, C is the inverse regularization strength\nimport time\nfrom time import time\nt0 = time()\nclf = LogisticRegression(C=1e5,\n                         multi_class='multinomial',\n                         penalty='l2', solver='sag', tol=0.1,max_iter=10)\n# fit data\nclf.fit(X_train_pca, train_label)\n\n# percentage of nonzero weights\nsparsity = np.mean(clf.coef_ == 0) * 100\n# compute accuracy\nscore = clf.score(X_test_pca, test_label)\n\n#display run time\n#run_time = time.time() - t0\n#print('Example run in %.3f s' % run_time)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Sparsity with L2 penalty: %.2f%%\" % sparsity)\nprint(\"Test score with L2 penalty: %.4f\" % score)","cd80b59a":"# Plot first 12 orginal image\nplot_slice(train[0:12])","9dd974a9":"# Lets call weight of our previous Logistic regression model\nweight_orginal","80005764":"weight_orginal.shape","df7a6c10":"weight_orginal_frame=pd.DataFrame(weight_orginal)\nweight_orginal_frame","c2fe3bfc":"plt.figure(figsize=(16,8));\nscale=np.abs(weight_orginal).max()\n# Logistic model weights plot\nfor i in range(10):\n    model_plot=plt.subplot(2, 5, i+1)\n    model_plot.imshow(weight_orginal[i].reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',vmin=-scale,vmax=scale)\n    \n    model_plot.set_xlabel(\"label % i\"%i)","2d282916":"plt.figure(figsize=(16,8));\n#scale=np.abs(approximation0).max()\n# Logistic model weights plot\nfor i in range(10):\n    model_plot=plt.subplot(2, 5, i+1)\n    model_plot.imshow(approximation0[i].reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',clim=(0, 255))","20eae83b":"##2-D Gaussian random data with N1 = 500","7ba19432":"# 1. (20pts) Write a function [p, x] = mykde(X,h) that performs kernel density estimation on data X with bandwidth h. It should return the estimated density p(x) and its domain x where you estimated the p(x) for X in 1-D and 2-D.","31dbc574":"# What do Neural Networks learn? ","953dcbde":"# Our Own KDE","0fd7c870":"# Make the data frame","656e07dc":"### Print the result with best brandwodth","79b1dd6d":"### 2. (20pts) Generate 2 sets of 2-D Gaussian random data with N1 = 500 and N2 = 500 using the following parameters: \u00b51 = [1; 0]; \u00b52 = [0; 2:5]; \u03a31 = 0.9:0.4 :0.9: :0.4\u0015 ; \u03a32 = \u00140 0: :9 0 4 0: :4 9\u0015 : (1) Test your function mykde on this data with h = {0.1; 1; 5; 10}. In your report, report figures of estimated densities (either 3D or 2D plot with color representing the density).","83d8a07e":"# Run Logistic regression with PCA data","90599b67":"# 3. (10pts) Visualize the data using the first 2 PC, i.e., plot each sample as scatter plot by projecting them onto a 2D spaces whose each axis is the PC. Do they look good or not so good? Explain.","54db8864":"# so, with 30PCs we will get 73.19% variance in our data","8f49de28":"# 1. (10pts) Write your own function [PC] = myPCA(X, k) that returns k principle components (PCs) on data X.","bea52c67":"# Plot density for different bandwidth","db314fd4":"## It looks that  the image with weight only from orginal image model is obscure while PCA image have much more clear and understandable shape with clear letter.","206769fc":"##### It makes sense as the weight tries to represent number from 0 to 9. The above figure almost clear about the number from 0 to 9","f1b62a19":"# Plot first 12 orginal image","744fbb4e":"# Now lets see the test data","18adecfd":"# 4. (10pts) Visualize 10 PC as images. That is, reshape each PC as a 28x28 image and show it as a 2D image. Any interesting shapes?","5afe454f":"### Neural networks are generating a lot of excitement, as they are quickly proving to be a promising and practical form of machine intelligence. Neural Network or logistic regression, all kinds of these model learn about the weight optimization.Neural networks are composed of layers of computational units (neurons), with connections among the neurons in different layers. These networks transform data \u2013 like the pixels in an image \u2013 until they can classify it as an output, such as naming an object in an image. Each neuron in a network transforms data using a series of computations: a neuron multiplies an initial value by some weight, sums results with other values coming into the same neuron, adjusts the resulting number by the neuron\u2019s bias, and then normalizes the output with an activation function. The bias is a neuron-specific number that adjusts the neuron\u2019s value once all the connections are processed, and the activation function ensures values that are passed on lie within a tunable, expected range. This process is repeated until the final output layer can provide scores or predictions related to the classification task at hand","912f3842":"# train a classifier for 10 epochs using transformed data with 30 PCs","32feb334":"# 5. (10pts) Let\u2019s try out classification together with the PCA. For this question, you may use built-in functions or libraries. Use Logistic Regression with 10 output nodes with softmax function as the classifier. First, train a classifier using the raw images for 10 epochs and measure the training time. Then, train a classifier for 10 epochs using transformed data with 30 PCs, i.e., project each image to a 30-D space using the 30 PCs and train the classifier, and measure the training time. How much improvement in the training time? How are their performances on the testing set? Explain your thoughts.","d68cebe6":"# References\n01. https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.13-kernel-density-estimation.html\n02. https:\/\/jakevdp.github.io\/blog\/2013\/12\/01\/kernel-density-estimation\/\n03. https:\/\/jduncstats.com\/post\/2019-03-16_kde-scratch\/\n04. https:\/\/www.askpython.com\/python\/examples\/principal-component-analysis\n05. https:\/\/www.kaggle.com\/ddmngml\/pca-and-svm-on-mnist-dataset\n06. https:\/\/mylearningsinaiml.wordpress.com\/2018\/09\/10\/pca-visualization-mnist-data\/\n07. https:\/\/github.com\/mGalarnyk\/Python_Tutorials\/blob\/master\/Sklearn\/PCA\/PCA_Image_Reconstruction_and_such.ipynb\n08. http:\/\/physics.bu.edu\/~pankajm\/ML-Notebooks\/HTML\/NB7_CVII-logreg_mnist.html\n09. https:\/\/www.kdnuggets.com\/2015\/12\/how-do-neural-networks-learn.html\n10. https:\/\/gsalvatovallverdu.gitlab.io\/python\/kernel_density_estimation\/","d0d1022b":"## another Gaussian random data with \u00b52 = 0 and \u03c32 = 0.2. ","532d4623":"## Lets compare above figure with some randome number PCA image\n","f7a0c3d6":"# Showing Graph of Explained Variance vs Number of Principal Components","ccbbdc3d":"# So with 30 PCA, our accuracy is 89.01% while with orginal data it was 92.39% which is very close with PCA results. Without PCA the run time is 27.27 s with 784 components but after taking 30PCA the run time reduce to 0.727s which indicates that the algorithm works 37 times faster than model with orginal image data","d4d1e559":"## Generate N = 1000 Gaussian random data with \u00b51 = 5 and \u03c31 = 1. Test your function mykde onthis data with h = {0.1; 1; 5; 10}. In your report, report the histogram of X along with the figures of estimated densities.","ab1bf182":"# 2.(10pts) Run your PCA on MNIST dataset (you used this data for HW2). Taking too long time? You can use the tricks explained in the lecture notes.","5a91ea59":"# ABSTRACT\n###### Clustering data is important to identify the similar characteristics within the data.  Kerneldensity estimation (KDE) plays important role for this clustering. Another important thing isprinciple component analysis (PCA) which reduce the dimension from large dimension data.In the first phase, this study uses ID and 2D KDE function to explore the kernel density underconsideration of different bandwidth.In 2nd phase, this study explored the PCA techniqueson MNIST data to understand the PCA effects on logistic regression results and run time.The results found that KDE performs best with bandwidth 0.333 and 0.111 for 1D randomgenerated data but for 2D random generated data bandwidth 1 clearly differentiate cluster1 and cluster 2 data than others considered bandwidth (0.1, 5, 10). The logistic regressionwith original image (784 component) shows 92.05 percent accuracy and run time 27.72seconds while with 30 PCAs accuracy is 89.23 and run time 37 times faster(0727 s). 30 PCAsexplained 73 percent variation of our MNIST data. We considered 10 epochs for both casesand visualization of image from original and PCA is understandable and clear, which indicatesthat reducing the dimension for modeling on MNIST data does not affect the results but itaccelerate the analysis process.  Overall,KDE, PCA and Logistic regression provide greaterscope for data characterization and faster classification.","44b15de0":"# Problem 2\n(Dimension Reduction 60pts)\n1. (10pts) Write your own function [PC] = myPCA(X, k) that returns k principle components (PCs) on\ndata X.\n2. (10pts) Run your PCA on MNIST dataset (you used this data for HW2). Taking too long time? You\ncan use the tricks explained in the lecture notes.\n3. (10pts) Visualize the data using the first 2 PC, i.e., plot each sample as scatter plot by projecting\nthem onto a 2D spaces whose each axis is the PC. Do they look good or not so good? Explain.\n4. (10pts) Visualize 10 PC as images. That is, reshape each PC as a 28x28 image and show it as a 2D\nimage. Any interesting shapes?\n5. (10pts) Let\u2019s try out classification together with the PCA. For this question, you may use built-in\nfunctions or libraries. Use Logistic Regression with 10 output nodes with softmax function as the\nclassifier. First, train a classifier using the raw images for 10 epochs and measure the training time.\nThen, train a classifier for 10 epochs using transformed data with 30 PCs, i.e., project each image to\na 30-D space using the 30 PCs and train the classifier, and measure the training time. How much\nimprovement in the training time? How are their performances on the testing set? Explain your\nthoughts.\n6. (10pts) What do Neural Networks learn? Let\u2019s take a look at the pre-trained Logistic Regression\nmodel using the raw data. Each output node has 784 weights associated with it. Reshape the weights\nto an 28x28 image; do this for the all output nodes and visualize each of them. Any interesting shapes?\nWhat do they represent? How do they differ from the features learned from PCA (from Q4)? Explain\nyour thoughts.","b6975462":"# 6. (10pts) What do Neural Networks learn? Let\u2019s take a look at the pre-trained Logistic Regression model using the raw data. Each output node has 784 weights associated with it. Reshape the weights to an 28x28 image; do this for the all output nodes and visualize each of them. Any interesting shapes? What do they represent? How do they differ from the features learned from PCA (from Q4)? Explain your thoughts.","62217b02":"# The bottom green is our study target in this assignment with 73% variance with 30 PCs","d591d1dd":"# Lets run Logistic regression with orginal data","13bebe39":"# Bandwidth selection","bd58b10c":"## Generate N = 1000 1-D Gaussian random data with \u00b51 = 5 and \u03c31 = 1","4f7b0141":"# It says, we need to reduce the dimension from 784 to 154 to get 95% varience,, previously we considered 65% variance with 16 components.If we cansider 30 pcs, lets see the variance","607e489f":"# Bring label back","a68e6e02":"# PCA using Scikit-Learn ","69b5d021":"# By our own KDE function","326ff473":"# Using sckit Learn","91f8ba13":"![image.png](attachment:image.png)","f90cad6d":"# 16 components collect 59% of variance","ed77c4db":"# Generate N = 1000 1-D Gaussian random data with \u00b51 = 5 and \u03c31 = 1 and another Gaussian random data with \u00b52 = 0 and \u03c32 = 0:2. Test your function mykde on this data with h ={0.1; 1; 5; 10}. In your report, report the histogram of X along with the figures of estimated densities","ebd65a43":"# Comments: Yes, the PCA image is understandable and clear in visualization.","23d4595e":"# PCA using Scikit-Learn :","eb09c65c":"##### Kernel density estimation (KDE) is in some senses an algorithm which takes the mixture-of-Gaussians idea to its logical extreme: it uses a mixture consisting of one Gaussian component per point, resulting in an essentially non-parametric estimator of density. density estimator is an algorithm which seeks to model the probability distribution that generated a dataset. For one dimensional data, we probably already familiar with one simple density estimator: the histogram. A histogram divides the data into discrete bins, counts the number of points that fall in each bin, and then visualizes the results in an intuitive manner. "}}