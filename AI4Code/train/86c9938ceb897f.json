{"cell_type":{"70b39453":"code","d4f78255":"code","7340f4f9":"code","2d119d8e":"code","1dd149e2":"code","5351f87a":"code","b92139d0":"code","5b9153c0":"code","5bf8943e":"code","dc81c80a":"code","7bc9b5cc":"code","0049ce17":"code","ef1af73f":"code","905abb4d":"code","4ac7aa8d":"code","e4a01047":"code","6bd8a281":"code","be1b31d1":"code","8d6e81bd":"code","d49d1616":"code","9bd10296":"code","c6312df2":"code","718becc3":"code","c49333c4":"code","f855a25f":"code","e5f5f27d":"code","5da12050":"code","5c195d9b":"code","e23151da":"code","66327119":"code","b7eb3481":"code","2ba839f9":"code","352e7b8f":"code","122436b4":"code","4b6beb3f":"code","278583a6":"code","d1ec6a4b":"code","b8513e9c":"code","61f766e0":"code","3b8023d1":"code","526843b7":"code","fbbf0335":"code","a259b6f1":"code","58cae984":"code","96dfb645":"code","7a23a8ac":"code","2409f1ce":"code","10a9f529":"code","947244c5":"code","e3381d89":"code","d2530bab":"code","411f08c0":"code","59cfe70e":"code","d3e5f1f6":"code","e50738fe":"code","55b448bf":"code","0c2b58ca":"code","1c0f300f":"code","f08fec8e":"code","9d3d242d":"code","9286f2db":"code","c5a5dd6d":"code","189e711d":"code","fc5561b4":"code","380104bc":"code","12697265":"code","9e78dd47":"code","681df53f":"code","92302465":"code","aa2ab3e0":"code","c1fe621b":"code","78ab4061":"code","8a5ad58b":"code","48afd622":"markdown","1e497412":"markdown","9400103d":"markdown","5b4cf4f5":"markdown","b4bbed96":"markdown","69800a85":"markdown","3305108e":"markdown","ec3d5e0a":"markdown","a0f843be":"markdown","792ce004":"markdown","0cbb76de":"markdown","e2d7eb30":"markdown","8d981be6":"markdown","93d4a2b9":"markdown","f8778f0d":"markdown","e40a606b":"markdown","a3840b85":"markdown","996fd2dd":"markdown","efa3ef05":"markdown","69df49f7":"markdown","e5f081c2":"markdown","76c96726":"markdown","0190a74a":"markdown","688b16ac":"markdown","78f35122":"markdown","0d0e99cc":"markdown","1bb8ac9b":"markdown","74615a4e":"markdown","46aa2920":"markdown","b6732dfb":"markdown","c7aa37e9":"markdown","00ccb1f5":"markdown","d812b56a":"markdown","b5e5608c":"markdown","ef592c18":"markdown","e2c2a822":"markdown","1d571b3f":"markdown"},"source":{"70b39453":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d4f78255":"df = pd.read_csv(\"..\/input\/inmueblesanalisis\/ar_properties.csv\")","7340f4f9":"df.isnull().sum()","2d119d8e":"df.describe()","1dd149e2":"df[[\"l1\",\"l2\",\"l3\",\"l4\"]].isnull().sum()","5351f87a":"\ncomplete_prices_df = df.iloc[(df[[\"currency\",\"price\"]].dropna()).index,:]\ncomplete_prices_df","b92139d0":"#re-define indices\ncomplete_prices_df = complete_prices_df.reset_index(drop=True)","5b9153c0":"categorical_cols = (complete_prices_df.dtypes[complete_prices_df.dtypes==\"object\"]).index\nnumerical_cols = (complete_prices_df.dtypes[complete_prices_df.dtypes==\"float64\"]).index","5bf8943e":"categorical_cols = pd.Series(categorical_cols)\ncategorical_cols","dc81c80a":"numerical_cols = (numerical_cols[numerical_cols != \"l6\"])\nnumerical_cols = pd.Series(numerical_cols)\nnumerical_cols","7bc9b5cc":"numerical_cols = pd.Series(numerical_cols).append(pd.Series([\"currency\"]))","0049ce17":"complete_prices_df[\"operation_type\"].value_counts()","ef1af73f":"dolar_only = complete_prices_df[(complete_prices_df[\"currency\"]==\"USD\") & (complete_prices_df[\"operation_type\"]==\"Venta\")].reset_index(drop=True)\n","905abb4d":"lean_df1 = dolar_only[numerical_cols].dropna().reset_index(drop=True)","4ac7aa8d":"lean_df1","e4a01047":"X = lean_df1.drop(axis=1,columns=[\"currency\",\"price\"])\ny = lean_df1[\"price\"]","6bd8a281":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ntrain_X_numerical, val_X_numerical, train_y_numerical, val_y_numerical = train_test_split(X,y,random_state=9)\n","be1b31d1":"#model training\ngbr_numerical_notlabeled = GradientBoostingRegressor(random_state=1).fit(train_X_numerical,train_y_numerical)","8d6e81bd":"from sklearn.metrics import mean_absolute_error\nprediction1 = gbr_numerical_notlabeled.predict(val_X_numerical)\nmean_absolute_error(val_y_numerical,prediction1)","d49d1616":"dolar_only_features_to_impute = dolar_only[numerical_cols].drop(axis=1,columns=[\"currency\",\"price\"])\n#so we use now the dataframe with all the rows with no missing price nor currency, we will impute it ONLY ON THE FEATURES. \n#There\u00b4s no apparent reason to impute on the target in this case. There\u00b4s a very big variance between prices and its other features.","9bd10296":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer().fit(dolar_only_features_to_impute)\nnew_X = pd.DataFrame(my_imputer.transform(dolar_only_features_to_impute),columns = [dolar_only_features_to_impute.columns])","c6312df2":"len(new_X.index)==len(dolar_only[\"price\"].index) #to make sure we still have the same number of rows in target and features","718becc3":"new_y = dolar_only[\"price\"]","c49333c4":"tr_X,val_X,tr_y,val_y = train_test_split(new_X,new_y,random_state=3)","f855a25f":"from sklearn.ensemble import RandomForestRegressor","e5f5f27d":"\nfst_rfr = RandomForestRegressor(random_state=1).fit(tr_X,tr_y)\n","5da12050":"preds_fst_rfr = fst_rfr.predict(val_X)\nmean_absolute_error(val_y,preds_fst_rfr)","5c195d9b":"snd_gbr = GradientBoostingRegressor(random_state=1).fit(tr_X,tr_y)\npreds_snd_gbr = snd_gbr.predict(val_X)\nmean_absolute_error(val_y,preds_snd_gbr)","e23151da":"dolar_only_features_to_impute.isnull().sum().sort_values()","66327119":"dolar_only_features_to_impute.columns","b7eb3481":"print((dolar_only_features_to_impute[\"rooms\"]>6).mean())\nprint((dolar_only_features_to_impute[\"bathrooms\"]>3).mean())\nprint((dolar_only_features_to_impute[\"bedrooms\"]>3).mean())\n#los valores con cuartos, ba\u00f1os y habitaciones con numeros raramente grandes son insignificantes.\n#vale la pena imputar sus valores para no perder datos de otras columnas","2ba839f9":"df.describe()","352e7b8f":"print(df[\"l2\"].value_counts())","122436b4":"df_ar = df[df[\"l1\"]==\"Argentina\"]\ndf_ar_gbaNor = df_ar[df_ar[\"l2\"]==\"Bs.As. G.B.A. Zona Norte\"]\nprint(df_ar_gbaNor[\"l3\"].isnull().sum())\ndf_ar_gbaNor[\"l3\"].value_counts()\ndf_ar_gbaNor_sell = df_ar_gbaNor[df_ar_gbaNor[\"operation_type\"]==\"Venta\"]","4b6beb3f":"df_ar_gbaNor_sell_in_dollars = df_ar_gbaNor_sell[df_ar_gbaNor_sell[\"currency\"]==\"USD\"]\ndf_ar_gbaNor_sell_in_dollars","278583a6":"#here two variables from the untidy section are taken, they are just the names of the columns that are numeric and the columns that are categorical\nprint(categorical_cols)\nnumerical_cols","d1ec6a4b":"df_ar_gbaNor_sell_in_dollars[categorical_cols[5:10]]","b8513e9c":"df_ar_gbaNor_sell_in_dollars[\"l5\"].value_counts()","61f766e0":"print(df_ar_gbaNor_sell_in_dollars[\"l4\"].isnull().sum()\/df_ar_gbaNor_sell_in_dollars.shape[0])\nprint(df_ar_gbaNor_sell_in_dollars[\"l3\"].isnull().sum()\/df_ar_gbaNor_sell_in_dollars.shape[0])\n#32% of the values in l4 (neighborhood) are missing, meanwhile 2% of the l3 (district) column are missing \n#we will consider imputation for these locations","3b8023d1":"relevant_categorical_cols = categorical_cols.iloc[[4,7,8,14]]","526843b7":"relevant_categorical_cols","fbbf0335":"relevant_numerical_cols = numerical_cols[0:8].drop([3,4])\nrelevant_numerical_cols","a259b6f1":"relevant_cols = relevant_categorical_cols.append(relevant_numerical_cols)\nrelevant_cols","58cae984":"df_ar_gbaNor_sell_in_dollars[relevant_cols].isnull().sum().div(df_ar_gbaNor_sell_in_dollars.shape[0])*100","96dfb645":"df_ar_gbaNor_sell_in_dollars_loc_not_missing = df_ar_gbaNor_sell_in_dollars[df_ar_gbaNor_sell_in_dollars[\"lat\"].notnull() & df_ar_gbaNor_sell_in_dollars[\"lon\"].notnull() & df_ar_gbaNor_sell_in_dollars[\"l3\"].notnull()]\ndf_ar_gbaNor_sell_in_dollars_loc_not_missing[relevant_cols].isnull().sum().div(df_ar_gbaNor_sell_in_dollars_loc_not_missing.shape[0])*100","7a23a8ac":"relevant_cols_no_l4 = relevant_cols.drop([8,4,5])\n","2409f1ce":"relevant_cols_no_l4.drop(index=7)","10a9f529":"df_ar_gbaNor_sell_in_dollars_loc_not_missing[relevant_cols_no_l4].isnull().sum().div(df_ar_gbaNor_sell_in_dollars_loc_not_missing.shape[0])","947244c5":"relevant_cols_no_l4","e3381d89":"X = df_ar_gbaNor_sell_in_dollars_loc_not_missing[relevant_cols_no_l4.drop(index=7)]\ny = df_ar_gbaNor_sell_in_dollars_loc_not_missing[\"price\"][X.index]","d2530bab":"X = X.reset_index(drop=True)\ny = y.reset_index(drop=True)","411f08c0":"X[\"surface_covered\"].describe()","59cfe70e":"X.columns","d3e5f1f6":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer()\nimputed_cols = imputer.fit_transform(X[[\"rooms\",\"surface_covered\"]])","e50738fe":"X_imputed_columns_df = pd.DataFrame(imputed_cols,columns=[\"rooms\",\"surface_covered\"])\nX_imputed = X","55b448bf":"X_imputed[[\"rooms\",\"surface_covered\"]] = X_imputed_columns_df","0c2b58ca":"X_imputed","1c0f300f":"from sklearn.model_selection import train_test_split\n\ntrain_X_imputed, test_X_imputed, train_y, test_y = train_test_split(X_imputed,y, test_size = 0.15,random_state=19)","f08fec8e":"from sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nproperty_type_train_oh_encoded = pd.DataFrame(OH_encoder.fit_transform(train_X_imputed[[\"property_type\"]]))\nproperty_type_test_oh_encoded = pd.DataFrame(OH_encoder.transform(test_X_imputed[[\"property_type\"]]))\n\nproperty_type_train_oh_encoded.index = train_X_imputed.index\nproperty_type_test_oh_encoded.index= test_X_imputed.index\n\n","9d3d242d":"property_type_train_oh_encoded","9286f2db":"train_X_imputed_no_property_type = train_X_imputed.drop([\"property_type\"],axis=\"columns\")\ntest_X_imputed_no_property_type = test_X_imputed.drop([\"property_type\"],axis=\"columns\")\n\ntrain_X_imputed_no_property_type","c5a5dd6d":"labeled_train_X_imputed = pd.concat([train_X_imputed_no_property_type,property_type_train_oh_encoded],axis=1)\nlabeled_train_X_imputed","189e711d":"labeled_train_X_imputed","fc5561b4":"labeled_test_X_imputed = pd.concat([test_X_imputed_no_property_type,property_type_test_oh_encoded],axis=1)","380104bc":"from sklearn.ensemble import RandomForestRegressor\n\nfst_random_forest = RandomForestRegressor(random_state=4).fit(labeled_train_X_imputed,train_y)","12697265":"fst_preds = fst_random_forest.predict(labeled_test_X_imputed)","9e78dd47":"from sklearn.metrics import mean_absolute_error\nprint(\"The error percentage is \" + str(mean_absolute_error(test_y,fst_preds)*100\/test_y.mean())+\"%\")\nmean_absolute_error(test_y,fst_preds)","681df53f":"from sklearn.ensemble import GradientBoostingRegressor\n\nsnd_gradient_booster = GradientBoostingRegressor().fit(labeled_train_X_imputed,train_y)","92302465":"snd_preds = snd_gradient_booster.predict(labeled_test_X_imputed)\nprint(\"The error percentage is \" + str(mean_absolute_error(test_y,snd_preds)*100\/test_y.mean()) + \"%\")\nmean_absolute_error(test_y,snd_preds)","aa2ab3e0":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm=PermutationImportance(fst_random_forest,random_state=1).fit(labeled_test_X_imputed,test_y)\n","c1fe621b":"colnames = labeled_test_X_imputed.columns.tolist()\ncolnames\nfor k in range(len(colnames)):\n    colnames[k] = str(colnames[k])\ncolnames","78ab4061":"eli5.show_weights(perm , feature_names = colnames )","8a5ad58b":"from sklearn.metrics import mean_absolute_error\nfor k in [10,1000,100000,10000000]:\n    random_forest = RandomForestRegressor(random_state=4,max_leaf_nodes = k).fit(labeled_train_X_imputed,train_y)\n    predictions = random_forest.predict(labeled_test_X_imputed)\n    mae = mean_absolute_error(test_y,predictions)\n    print(\"error: \" + str(mae) + \"; leafnodes: \" + str(k) + \" \\n \")\n    \n","48afd622":"### 1.3. Handling missing values","1e497412":"### 1. Objectives: <a id=\"s1\"><\/a>\n    In this work, the aim is mainly to construct a relatively accurate price predictor in a determined city or district. Northern Great Buenos Aires was chosen because of its high real estate activity and its numerous neighbourhoods. \n    The report aims to adress one single problem: sellers don\u00b4t often know which price to give initially to its properties. There are multiple factors that influence the price that a potential purchaser may be willing to pay for it and therefore real estate agencies and owners would benefit greatly from having such a tool, which prevents them from: losing time by avoiding an excessively high initial price, or losing money for an excessively low initial price. A high experienced property seller may understand the market and prices well, but a model trained with thousands of cases might have a much more accurate  prediction.\n\n    At the end of it, we will make an attempt to generalize this methods to make them appliable to other regions.  \n### Analytic approach:\n    The data was extracted from Properati\u00b4s page. The first step will be a short exploration. The data preparation phase will be meticulous and rigurous: we want to cleanse the data as much as we can and still get the benefits from cases with some missing information. First we will get the data and filter it by geographic location, then by the type of operation (rent prices are not part of the scope), and then by currency (considering that the argentinian peso has devaluated several times in the past, only properties published in dollars will be analyzed). After this, the resulting dataFrame will be explored to perform a feature selection. We will probably start by using only features with numeric values or binary categorized values, then we will perform some changes on them and adress issues related with missing values. After this, a first model training will be selected and trained with the data for posterior error measurement and feature importance measurement,","9400103d":"### 0.2.1. GradientBoostingRegressor <a id=\"s0p2p1\">","5b4cf4f5":"It seems like latitude and longitude are important features to the model. \nStill, a new set of feature combinations will be tested.","b4bbed96":"# 6. Splitting data and labeling categorical columns<a id=\"s6\"><\/a>","69800a85":"I consider that this is more than enough for now with this notebook. It is already way too extensive.\nThere are lots of useful insights to get from the notebook:\n1. Random forests are useful for this kind of datasets.\n2. Relevant columns to train with this huge dataset.\n3. How to select the appropiate locations.\n4. How NOT to adress this sorts of datasets (from the untidy section).\n5. A model that predicts property prices with an 80% accuracy whithin the northern Great Buenos Aires.\n\nFollowing project: \n<a href=\"https:\/\/www.kaggle.com\/msorondo\/property-price-predictions-buenos-aires-city\">Property price predictions: Buenos Aires City<\/a>","3305108e":"It makes sense to me to impute numbers with little variance like bathrooms. Still it i\u00b4ll just drop lat and lon missing values and impute the rest","ec3d5e0a":"Feature importance measurement","a0f843be":"# 5. Handling missing values <a id=\"s5\"><\/a>","792ce004":"Gradient booster","0cbb76de":"Now the first feature dataFrame will be created in order to perform imputation on it.","e2d7eb30":"Random forest","8d981be6":"> 60 thousand dollars of mean error is A LOT. There are a series of problems: We are working with aproximately 160000 rows from a total of almost 1 million rows. We are also missing lots of data from categorical columns. What I suggest is the following: to get back the columns with missing values and to perform an imputation on it. And then to proceed to measure the importance of the columns in order to see which features actually contribute to accuracy. This will let us drop useless numerical features (if there are), and thus mitigating the effects of missing data on the prediction.","93d4a2b9":"Leaf Node optimization<a id=\"leafnodeoptimization\"><\/a>","f8778f0d":"## 2. Training and testing some models <a id=\"s0p2\"><\/a>","e40a606b":"There are lots of missing values at l5 column (columna l5). We\u00b4ll probably use up to l4.","a3840b85":"This model seems to perform best with no limit in the leaf nodes.\n","996fd2dd":"# 1. Tidyness <a id=\"s1\"><\/a>","efa3ef05":"### Let\u00b4s use a gradient booster regressor and a random forest regressor to see how they compare and select one.  ","69df49f7":"After consulting to other data scientists, some suggested to try a new feature selection that excludes lat and lon columns (whithin other little changes). So I\u00b4ll first weight how much do the lat and lon columns help to perform prediction.","e5f081c2":"We\u00b4ll keep the random forest. It outperformed the gradient booster by a substantial amount of error.","76c96726":"There is very little overlap between the rows with missing values and this makes it a bit difficult to work. That\u00b4s why we\u00b4ll drop rows with missing latitude and longitude. This two will not be imputed because they\u00b4re numerical but still they don\u00b4t measure position and not quantity. l3 missing values are so little that they can be dropped with no significant impact. Since there will be enough presition from lat and lon, we can also drop the entire l4 column (it gives little and low quality information that is very similar to l3).","0190a74a":"TABLE OF CONTENTS\n1. [Objectives and methodology description](#s1)\n2. [Data Exploration](#s2)\n3. [Data filtering](#s3)\n4. [(first)Feature selection](#s4)\n5. [Handling missing values](#s5)  \n    [Imputation](#imputation)  \n6. [Labeling categorical values](#s6)  \n7. [First model tests](#s7)  \n    7.1. Random Forest Regressor  \n    7.2. Gradient boosting regressor  \n    7.3. Feature importance\n    7.4. Leaf Node optimization\n8. [Feature reorganization](#s8)\n# [Checkpoint](#checkpoint)","688b16ac":"# 7. First model tests","78f35122":"Awful.","0d0e99cc":"# 0. Data importation and preparation <a id=\"s0p1\"><\/a>","1bb8ac9b":"More awful.\nSo, maybe imputation isn\u00b4t as good as expected. My suspicion is that the main problem comes with imputating on the latitude and longitude. Let\u00b4s look again at the missing values per column.","74615a4e":"# 2. Data Exploration","46aa2920":"## Property price predictions with regressive machine learning models","b6732dfb":"## Imputation <a id=\"imputation\"><\/a>","c7aa37e9":"### First feature selection","00ccb1f5":"# 3. Data filtering<a id=\"s3\"><\/a>","d812b56a":"Add currency category and label as numbers","b5e5608c":"*TABLA DE CONTENIDOS*\n## 0. [Untidyness]: Preliminary work.(*)\n0.1. [Data importation and preparation](#s0p1)  \n    0.2. [Training and Testing some models](#s0p2)     \n   0.2.1. [Gradient boosting regressor](#s0p2p1)        \n   0.2.2. <font color=\"red\">...<\/font>[BreakPoint](#checkPoint)    \n\n# 1. [Tidyness](#s1)\n[Flow Chart](#flowchart)\n\n(*): I am not fond of this part of the document. It is untidy and poorly modularized. Still, I am not going to delete it since there are a couple of valuable insights that are used in section 1. There is no need at all to look at it to understand the document, the names of the variables used from it tells its characteristics. The only useful insight about this section is that a poor planning takes to disastrous results.","ef592c18":" ## 8. Conclusions and takeaways for future projects <a id=\"s8\"><\/a>","e2c2a822":"## 4. First feature selection <a id=\"s4\">","1d571b3f":"<font color=\"red\">Breakpoint<\/font><a id=\"checkPoint\"><\/a>"}}