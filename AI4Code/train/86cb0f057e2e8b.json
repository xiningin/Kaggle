{"cell_type":{"325dbdeb":"code","adcae051":"code","16ecfa4d":"code","00648879":"code","a6b01f02":"code","66fc112f":"code","996e3d8a":"code","bb6681f8":"code","90070ecd":"code","682e86b4":"code","7b65b475":"code","87ca22c2":"code","6dd3a16e":"code","dc26678b":"code","a7b138f0":"code","0a5265a9":"code","e46b18b5":"code","965d4e86":"code","894ffc09":"code","407726da":"code","1b7b8912":"code","97313ae8":"code","b263fb84":"code","6609e309":"code","8b808a77":"code","b0d673ad":"code","4ff33881":"code","a45093e1":"code","6b8eea56":"code","ee578a31":"code","c98e13a8":"code","267f9959":"code","a236d670":"code","1f8bc6aa":"code","fe51052b":"code","771e1c6f":"code","03831147":"code","d48ca5d6":"code","c05e4d66":"code","19e99dea":"code","19f2d329":"code","e188256f":"code","a01beb8e":"code","6c91f530":"code","c425dc1f":"code","ab48567f":"markdown","10930108":"markdown","ab094030":"markdown","78f4c30c":"markdown","2e551c7a":"markdown","6f6b8d57":"markdown","8acc44ab":"markdown","79b913b0":"markdown","5d8c69e9":"markdown","14aab456":"markdown","e6bb3054":"markdown","6d96a900":"markdown","38281283":"markdown","46da5e46":"markdown","2ee8fa61":"markdown","51e45a53":"markdown","5e388257":"markdown","f8c84f54":"markdown"},"source":{"325dbdeb":"#Let's import the usual suspects\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","adcae051":"#Importing the dataset\ntrain = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv')\ndata = train.append(test, sort = False)\ndata.head()","16ecfa4d":"data.info()","00648879":"data.describe()","a6b01f02":"#Histogram for count\nsns.set_style('darkgrid')\nsns.distplot(train['count'], bins = 100, color = 'green')\nplt.show()","66fc112f":"#Q-Q Plot\nfrom scipy import stats\nplt = stats.probplot(train['count'], plot=sns.mpl.pyplot)","996e3d8a":"#Boxplot for count\nimport matplotlib.pyplot as plt\nsns.boxplot(x = 'count', data = train, color = 'mediumpurple')\nplt.show()","bb6681f8":"#Calculating the number of outliers\nQ1 = train['count'].quantile(0.25)\nQ3 = train['count'].quantile(0.75)\nIQR = Q3 - Q1\noutliers = train[(train['count'] < (Q1 - 1.5 * IQR)) | (train['count'] > (Q3 + 1.5 * IQR))]\nprint((len(outliers)\/len(data))*100)","90070ecd":"#Data without the outliers in count\ndata = data[~data.isin(outliers)]\ndata = data[data['datetime'].notnull()]","682e86b4":"sns.barplot(x = 'season', y = 'count', data = train, estimator = np.average, palette='coolwarm')\nplt.ylabel('Average Count')\nplt.show()","7b65b475":"sns.barplot(x = 'holiday', y = 'count', data = train, estimator = np.average, palette='deep')\nplt.ylabel('Average Count')\nplt.show()","87ca22c2":"sns.barplot(x = 'workingday', y = 'count', data = train, estimator = np.average, palette='colorblind')\nplt.ylabel('Average Count')\nplt.show()","6dd3a16e":"sns.barplot(x = 'weather', y = 'count', data = train, estimator = np.average, palette='deep')\nplt.ylabel('Average Count')\nplt.show() ","dc26678b":"plt.figure(figsize = (10,7))\ntc = train.corr()\nsns.heatmap(tc, annot = True, cmap = 'coolwarm', linecolor = 'white', linewidths=0.1)","a7b138f0":"#Convert to integer variables\ncolumns=['season', 'holiday', 'workingday', 'weather']\nfor i in columns:\n    data[i] = data[i].apply(lambda x : int(x))","0a5265a9":"#Convert string to datatime and create Hour, Month and Day of week\ndata['datetime'] = pd.to_datetime(data['datetime'])\ndata['Hour'] = data['datetime'].apply(lambda x:x.hour)\ndata['Month'] = data['datetime'].apply(lambda x:x.month)\ndata['Day of Week'] = data['datetime'].apply(lambda x:x.dayofweek)","e46b18b5":"plt.figure(figsize = (8,4))\nsns.lineplot(x = 'Month', y = 'count', data = data, estimator = np.average, hue = 'weather', palette = 'coolwarm')\nplt.ylabel('Average Count')\nplt.show()","965d4e86":"data[data['weather'] == 4]","894ffc09":"fig, axes = plt.subplots(ncols = 2, figsize = (15,5), sharey = True)\nsns.pointplot(x = 'Hour', y = 'count', data = data, estimator = np.average, hue = 'workingday', ax = axes[0], palette = 'muted')\nsns.pointplot(x = 'Hour', y = 'count', data = data, estimator = np.average, hue = 'holiday', ax = axes[1], palette = 'muted')\nax = [0,1]\nfor i in ax:\n    axes[i].set(ylabel='Average Count')","407726da":"plt.figure(figsize = (10,4))\nsns.pointplot(x = 'Hour', y = 'count', data = data, estimator=np.average, hue = 'Day of Week', palette='coolwarm')","1b7b8912":"sns.jointplot(x = 'atemp', y = 'count', data = data, kind = 'kde', cmap = 'plasma')\nplt.show()","97313ae8":"plt.figure(figsize = (8,4))\nsns.pointplot(x = 'Hour', y = 'casual', data = data, estimator = np.average, color = 'blue')\nsns.pointplot(x = 'Hour', y = 'registered', data = data, estimator = np.average, color = 'red')\nplt.ylabel('Registered')\nplt.show()","b263fb84":"#Histogram for Windspeed\nsns.set_style('darkgrid')\nsns.distplot(data['windspeed'], bins = 100, color = 'purple') #Windspeed cannot be 0.\nplt.show()","6609e309":"#Replacing 0s in windspeed with the mean value grouped by season\ndata['windspeed'] = data['windspeed'].replace(0, np.nan)\ndata['windspeed'] = data['windspeed'].fillna(data.groupby('weather')['season'].transform('mean'))\nsns.distplot(data['windspeed'], bins = 100, color = 'red')\nplt.show()","8b808a77":"#Encoding cyclical features\ndata['Month_sin'] = data['Month'].apply(lambda x: np.sin((2*np.pi*x)\/12))\ndata['Month_cos'] = data['Month'].apply(lambda x: np.cos((2*np.pi*x)\/12))\ndata['Hour_sin'] = data['Hour'].apply(lambda x: np.sin((2*np.pi*(x+1))\/24))\ndata['Hour_cos'] = data['Hour'].apply(lambda x: np.cos((2*np.pi*(x+1))\/24))\ndata['DayOfWeek_sin'] = data['Day of Week'].apply(lambda x: np.sin((2*np.pi*(x+1))\/7))\ndata['DayOfWeek_cos'] = data['Day of Week'].apply(lambda x: np.cos((2*np.pi*(x+1))\/7))","b0d673ad":"#trainsforming target variable using log transformation\ndata['count'] = np.log(data['count'])","4ff33881":"#Converting Categorical to numerical - Removing Co-Linearity\ndata_ = pd.get_dummies(data=data, columns=['season', 'holiday', 'workingday', 'weather'])\ntrain_ = data_[pd.notnull(data_['count'])].sort_values(by=[\"datetime\"])\ntest_ = data_[~pd.notnull(data_['count'])].sort_values(by=[\"datetime\"])","a45093e1":"#Standardizing numerical variables\nfrom sklearn.preprocessing import StandardScaler\ncols = ['temp','atemp','humidity', 'windspeed', 'Month_sin', 'Month_cos', 'Hour_sin', 'Hour_cos', 'DayOfWeek_sin','DayOfWeek_cos']\nfeatures = data[cols]\n\n#Standard Scaler\nscaler = StandardScaler().fit(features.values)\ndata[cols] = scaler.transform(features.values)","6b8eea56":"#Predictor columns names\ncols = ['temp','atemp','humidity', 'windspeed', 'Month_sin', 'Month_cos', 'Hour_sin', 'Hour_cos', 'DayOfWeek_sin','DayOfWeek_cos', 'season_1','season_2', 'season_3',\n        'holiday_0', 'workingday_0', 'weather_1', 'weather_2', 'weather_3']","ee578a31":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","c98e13a8":"#train test split\nX = train_[cols]\ny = train_['count']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","267f9959":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(lm.intercept_)","a236d670":"plt.figure(figsize = (18,4))\ncoeff = pd.DataFrame(lm.coef_, index = X.columns, columns = ['Coefficient'])\nsns.barplot(x = coeff.index, y = 'Coefficient', data = coeff, color = 'red')","1f8bc6aa":"plt.figure(figsize = (8,4))\npred = lm.predict(X_test)\nsns.scatterplot(x = y_test, y = pred)\nplt.xlabel('Count')\nplt.ylabel('Predictions')\nplt.show()","fe51052b":"sns.distplot((y_test-pred),bins=100, color = 'gray')\nplt.show()","771e1c6f":"print('RMSLE:', np.sqrt(metrics.mean_squared_log_error(np.exp(y_test), np.exp(pred))))","03831147":"from sklearn.linear_model import Ridge\n#Assiging different sets of alpha values to explore which can be the best fit for the model. \ntemp_msle = {}\nfor i in np.linspace(0, 40, 20):\n    ridge = Ridge(alpha= i, normalize=True)\n    #fit the model. \n    ridge.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    pred = ridge.predict(X_test)\n\n    msle = np.sqrt(metrics.mean_squared_log_error(np.exp(y_test), np.exp(pred)))\n    temp_msle[i] = msle","d48ca5d6":"temp_msle","c05e4d66":"from sklearn.linear_model import Lasso\n## Assiging different sets of alpha values to explore which can be the best fit for the model. \ntemp_msle = {}\nfor i in np.logspace(-10, -1, 20):\n    ## Assigin each model. \n    lasso = Lasso(alpha= i, normalize=True, tol = 0.1)\n    ## fit the model. \n    lasso.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    pred = lasso.predict(X_test)\n\n    msle = np.sqrt(metrics.mean_squared_log_error(np.exp(y_test), np.exp(pred)))\n    temp_msle[i] = msle","19e99dea":"temp_msle","19f2d329":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 500)\nrfr.fit(X_train, y_train)","e188256f":"plt.figure(figsize = (8,4))\npred = rfr.predict(X_test)\nsns.scatterplot(x = y_test, y = pred)\nplt.xlabel('Count')\nplt.ylabel('Predictions')\nplt.show()","a01beb8e":"sns.distplot((y_test-pred),bins=100, color = 'gray')","6c91f530":"#RMSLE\nprint('RMSLE:', np.sqrt(metrics.mean_squared_log_error(np.exp(y_test), np.exp(pred))))","c425dc1f":"#submission\nnew = test_[cols]\npred = rfr.predict(new)\nsubmission = pd.DataFrame({'datetime':test['datetime'],'count':np.exp(pred)})\nsubmission['count'] = submission['count'].astype(int)\nsubmission.to_csv('submission.csv',index=False)","ab48567f":"**Exploratory Data Analysis**","10930108":"**Random Forest**","ab094030":"As the target variable is a highly skewed data, we will try to transform this data using either log, square-root or box-cox transformation. After trying out all three, log square gives the best result. Also as the evaluation metric is RMSLE, using log would help as it would allow to less penalize the large difference in final variable values.","78f4c30c":"1.72% of the target values are above Q3 + 1.5IQR. Let's get rid of this.","2e551c7a":"The variability between the actual values and the predicted values is higher.","6f6b8d57":"**Linear Regression**","8acc44ab":"There is no line plot for weather = 4, because there is only three data point for weather = 4","79b913b0":"Types of variables:\n* Categorical - Season, Holiday, Working day, Weather\n* Timeseries - Datetime\n* Numerical - Temp, aTemp, Humidity, Windspeed, Casual, Registered, Count","5d8c69e9":"**Ridge Regression**","14aab456":"The residual distribution is normal.","e6bb3054":"**Feature Engineering**","6d96a900":"* During working days there is a high demand around the 7th hour and 17th hour. There is a lower demand during 0 to 5th hour and 10 to 14th hour.\n* During non workin days there is a high demand during 10 to 14th hour. There is a lower demand around the 7th hour.","38281283":"Count is hightly correlated with Casual and Registered. It's because Count is derived from Casual and Registered. We'll have to omit these variables. Temp and atemp are highly correlated.","46da5e46":"The variability between the actual values and the predicted values is lesser than the linear regression.","2ee8fa61":"These three charts above can tell us a lot about our target variable.\n\n* Our target variable, count is not normally distributed.\n* Our target variable is right-skewed.\n* There are multiple outliers in the variable.","51e45a53":"![](http:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3948\/media\/bikes.png)\n\nI am super excited to share my first kernel with the Kaggle community. This kernel is for all the aspiring data scientists who wants to learn and review their knowledge. As I go on in this journey and learn new topics, I will incorporate them with each new updates. Going back to the topics of this kernel, I will do visualizations to explain the data, and machine learning algorithms to forecast bike rental demand  in the Capital Bikeshare program in Washington, D.C.","5e388257":"Clearly, weekend and weekdays follows a different pattern.","f8c84f54":"**Lasso Regression**"}}