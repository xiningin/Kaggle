{"cell_type":{"39538102":"code","d979bbdf":"code","dc973996":"code","60718c19":"code","31a22fc4":"markdown","b252a704":"markdown","0310065b":"markdown","1d6a802a":"markdown","58b82923":"markdown"},"source":{"39538102":"import numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nplt.rcParams[\"figure.figsize\"] = [16,9]","d979bbdf":"_dir = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\n\n# Load\ntrain_df = pd.read_csv(_dir + 'train.csv', index_col=0)\ntrain_df = train_df.assign(dataset = 'train')\n\ntest_df = pd.read_csv(_dir + 'test.csv', index_col=0)\ntest_df = test_df.assign(dataset = 'test')\n\ndf = pd.concat([train_df, test_df], axis=0)\n\n# Get dictionary of types\ndict_types = {c[0]: c[1] for c in zip(df.dtypes.index, df.dtypes)}\n\n# Transformation\ndf.SalePrice = np.log(df.SalePrice)\n\n# Remove outliers\nremove_outliers_cols = ['LotArea', '1stFlrSF', 'WoodDeckSF', 'OpenPorchSF', \n                        'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal']\n\nz_score_threshold = 4.5\n\ndf = df.assign(outlier=False)\n\nfor c in remove_outliers_cols:\n    df.loc[abs(scipy.stats.zscore(df[c])) > z_score_threshold, 'outlier'] = True\n\n# Scaling\nscale_cols = [c for c in df.dtypes[df.dtypes == float].keys() if c not in ['Id', 'SalePrice']]\n\nscaler = StandardScaler()\n\nfor c in scale_cols:\n    df[c] = scaler.fit_transform(df[[c]])\n\n# Imputation\nt_df = (df.isnull().sum()\/df.shape[0]).to_frame()\nt_df = t_df[t_df.iloc[:, 0] !=0]\nt_df = t_df.assign(action='None')\nt_df.loc[t_df.iloc[:, 0] >= 0.50, 'action'] = 'drop_col'\nt_df.loc[t_df.iloc[:, 1] == 'None', 'action'] = 'med_mod'\n\n# Dropping when too many missing values\ncols_to_drop = [c for c in t_df[t_df.action == 'drop_col'].index if c not in ['SalePrice', 'dataset']]\ncols_to_replace_with_med_or_mod = t_df[t_df.action == 'med_mod'].index\n\ndf = df.drop(cols_to_drop, axis=1)\n\nfor c in cols_to_replace_with_med_or_mod:\n    if dict_types[c] in [int, float]:\n        df.loc[df[c].isnull(), c] = df[c].median()\n    elif dict_types[c] == object:\n        df.loc[df[c].isnull(), c] = df[c].mode().values[0]\n\nobj_cols = [c for c in df.dtypes[df.dtypes == object].keys() if c not in ['dataset']]\n\n# Categorical encoding\n# df = pd.get_dummies(df, columns=obj_cols, drop_first=True)\n\n# Mean Encoding\nfor c in obj_cols:\n    means = df.iloc[:train_df.shape[0]].groupby(c).SalePrice.mean()\n    df[c + '_mean_target'] = df[c].map(means)\n\ndf = df.drop(obj_cols, axis=1)","dc973996":"X = df[df.dataset == 'train'].drop(['SalePrice', 'dataset'], axis=1)\ny = df[df.dataset == 'train']['SalePrice']\n\nrandom_state = 40\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\nmodel = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = np.exp(model.predict(X_test)) #.clip(min(np.exp(df.SalePrice)), float('inf'))\nmean_squared_error(y_test, np.log(y_pred))**(0.5)","60718c19":"# Training on full dataset\nmodel.fit(X, y)\n\n# Predict on test set\ny_sub = np.exp(model.predict(df[df.dataset == 'test'].drop(['SalePrice', 'dataset'], axis=1))) #.clip(min(np.exp(df.SalePrice)), float('inf'))\n\n# Submission\ntest_df.assign(SalePrice = y_sub)[['SalePrice']].to_csv('submission.csv')","31a22fc4":"# Cross Validation","b252a704":"## Data cleaning and transformation","0310065b":"## Submission","1d6a802a":"## Import Libraries","58b82923":"This notebook contains very simple but efficient model - a linear regression - in order to predict the house prices.\n\nThe main steps are:\n- Log transform of the sale price\n- Outlier removal\n- Standardization\n- Basic imputation of missing values\n- Mean encoding\n- Cross validation\n- Fitting and submission\n    \nIn the given context, this notebook shows the importance of the feature engineering part. In fact, you can reach a pretty decent score without focusing too much on the model type and the model tuning."}}