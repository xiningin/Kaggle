{"cell_type":{"7189a306":"code","add29d0a":"code","87476c65":"code","b24a2d30":"code","c1f348bb":"code","a8394393":"code","b8be7fc9":"code","b596a839":"code","cc0d1aab":"code","8457038b":"code","2049593e":"code","1f72398a":"code","346d0b1a":"code","5c09b6ac":"code","a337cd6b":"code","cbb37201":"code","f482c9f0":"code","0fd094fb":"markdown","6e6f626b":"markdown","53920e23":"markdown","f7719b8d":"markdown","4bf3dbf0":"markdown","519495d2":"markdown","48524443":"markdown","a2f36221":"markdown","bb6e023e":"markdown"},"source":{"7189a306":"import numpy as np","add29d0a":"np.random.seed(463) # for reproducibility\n\nn = 500\nR = 4\nx1 = 2* R * np.random.rand(n, 1) - R\nx2 = 2* R * np.random.rand(n, 1) - R\n\nX = np.concatenate((x1,x2), axis = 1)","87476c65":"B0 = -1.0\nB1 = 0.5\nB2 = 0.2\nB3 = -0.1\nnoiseLevel = 1.0\ny_hat = B0 + B1 * x1 + B2 * x1**2 + B3 * x1**3 + noiseLevel * np.random.randn(n, 1)\n\nlabel = (y_hat > x2)\ny = label.astype(int).ravel()","b24a2d30":"xr = np.arange(-R, R, 0.1)\nyr = B0 + B1 * xr + B2 * xr**2 + B3 * xr**3","c1f348bb":"x1a = x1[label]\nx2a = x2[label]\n\nx1b = x1[~label]\nx2b = x2[~label]","a8394393":"import matplotlib.pyplot as plt\n%matplotlib inline","b8be7fc9":"fig = plt.figure(figsize=(12,8))\nplt.scatter(x1a, x2a, color = 'green', marker = '.')\nplt.scatter(x1b, x2b, color = 'red', marker = '.')\nplt.plot(xr,yr)\nplt.ylim([np.min(x2), np.max(x2)])\nplt.title(\"Data points\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.grid()","b596a839":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics","cc0d1aab":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)","8457038b":"# try K=1 through K=99 and record testing accuracy\nk_range = range(1, 99, 2)\n\n# We can create Python dictionary using [] or dict()\ntrain_scores = []\ntest_scores = []\n\n# We use a loop through the range 1 to 26\n# We append the scores in the dictionary\nbestK = 1\nbestAccuracy = 0\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n\n    y_pred = knn.predict(X_train)\n    train_scores.append(metrics.accuracy_score(y_train, y_pred))\n\n    y_pred = knn.predict(X_test)\n    testAccuracy = metrics.accuracy_score(y_test, y_pred)\n    test_scores.append(testAccuracy)\n    \n    if (testAccuracy >= bestAccuracy):\n        bestAccuracy = testAccuracy\n        bestK = k","2049593e":"plt.figure(figsize=(12,8))\nplt.plot(k_range, train_scores)\nplt.plot(k_range, test_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Accuracy')\nplt.legend([\"Train accuracy\", \"Test accuracy\"], loc =\"upper right\")\nplt.grid()","1f72398a":"print('Best K = ', bestK)    \n\nknn = KNeighborsClassifier(n_neighbors=bestK)","346d0b1a":"knn.fit(X, y)","5c09b6ac":"y_pred = knn.predict(X_train)\nprint('Train performance: ', metrics.accuracy_score(y_train, y_pred))","a337cd6b":"y_pred = knn.predict(X_test)\nprint('Test performance: ', metrics.accuracy_score(y_test, y_pred))","cbb37201":"# create points on a grid \nxx, yy = np.meshgrid(xr, xr)\n\nxx = xx.flatten()\nyy = yy.flatten()\n\nX_grid = np.vstack([xx, yy]).T\n\n# do classification for grid points\ny_pred = knn.predict(X_grid)","f482c9f0":"fig = plt.figure(figsize=(12,8))\nplt.scatter(xx[y_pred==0], yy[y_pred==0], 3, color='green', marker='.')\nplt.scatter(xx[y_pred==1], yy[y_pred==1], 3, color='red', marker='.')\nplt.plot(xr, yr)\nplt.ylim([np.min(x2), np.max(x2)])\nplt.title(\"Data points\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.grid()","0fd094fb":"For  \ud835\udc58=1  training error is zero (maximum accuracy is obtained). However, test error can be minimum for another  \ud835\udc58 . So, we aim for  \ud835\udc58  which gives the best performance on test data (while training is done on train set). This way, best parameter ( \ud835\udc58 ) can be chosen considering bias-variance tradeoff.","6e6f626b":"Draw decision surface","53920e23":"Train the model with X and y (not X_train and y_train)","f7719b8d":"Show the test performance","4bf3dbf0":"**Let's generate some data**","519495d2":"Instantiate the model with the best known parameters","48524443":"**Split X and y into training and testing sets**","a2f36221":"**Plot the relationship between K and testing accuracy**","bb6e023e":"We can change B0, B1, B2, B3, and noiseLevel to create data with different characteristics."}}