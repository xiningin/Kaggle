{"cell_type":{"12c0fa89":"code","6da0a28a":"code","ae8c3fde":"code","e08b003f":"code","b977285c":"code","f26fcf7f":"code","cc7f9dd7":"code","938ef012":"code","e8d7b9bd":"code","fdd9641a":"code","e245b36c":"code","e486c274":"code","0aa4d90e":"code","34b6b89f":"code","b1cc9734":"code","69d53b61":"code","8666bb65":"code","42b5617c":"code","d4075f57":"code","f92bfefd":"code","692aa8fc":"code","7ce5cd6a":"code","5b079084":"code","526e2525":"code","8eb2737a":"code","cfaa7a32":"code","110c057e":"code","e3bd94ca":"code","ef2b8f75":"code","d6cecdef":"code","9784407b":"code","a76bb8f2":"code","c484330e":"code","1e466702":"code","d7ef2163":"code","cf643f9e":"code","c3376886":"code","3f51a7da":"code","39d588c8":"code","18ad41ca":"code","e6d7da96":"code","7f36eff7":"code","4fb9a97b":"code","00382f57":"markdown","f4851587":"markdown","4c9a6aea":"markdown","5c6e52cc":"markdown","883d320c":"markdown","ff5302ae":"markdown","6884080f":"markdown","fe100247":"markdown","c0afea3a":"markdown","218c8351":"markdown","96bbf432":"markdown","65bb8bd1":"markdown"},"source":{"12c0fa89":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","6da0a28a":"print(torch.__version__)","ae8c3fde":"# for reproducibility\ntorch.manual_seed(0)","e08b003f":"!ls \/","b977285c":"!ls \/tmp","f26fcf7f":"!mkdir \/tmp\/Xray_train_data","cc7f9dd7":"!cp -R \"..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/COVID-19\" \"\/tmp\/Xray_train_data\"","938ef012":"!cp -R \"..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/NORMAL\" \"\/tmp\/Xray_train_data\"","e8d7b9bd":"!ls \"\/tmp\/Xray_train_data\"","fdd9641a":"!ls -1 \"\/tmp\/Xray_train_data\/COVID-19\"| wc -l ","e245b36c":"!ls -1 \"\/tmp\/Xray_train_data\/NORMAL\"| wc -l ","e486c274":"# Deleting extra image\n!find \"\/tmp\/Xray_train_data\/NORMAL\" -type f -print0 | sort -zR | tail -zn +220 | xargs -0 rm","0aa4d90e":"!ls -1 \"\/tmp\/Xray_train_data\/NORMAL\"| wc -l ","34b6b89f":"from torchvision.datasets import ImageFolder\nfrom torchvision import transforms","b1cc9734":"# Defining transform to resize 1024x1024 to 128x128\n# To change to Tensor\ntransform=transforms.Compose([\n                              transforms.Resize([64,64]),\n                              transforms.ToTensor()\n])","69d53b61":"train_val_path=\"\/tmp\/Xray_train_data\"","8666bb65":"dataset=ImageFolder(train_val_path,transform=transform)","42b5617c":"len(dataset)","d4075f57":"# Checking For Samples\nimg0,label0=dataset[10]\nprint(img0.shape,label0)\nimg1,label1=dataset[300]\nprint(img1.shape,label1)\nprint(\"*\"*60)\nprint(dataset.classes)#list out all the classes","f92bfefd":"def show(img,label):\n  print(\"label-->\",dataset.classes[label])\n  plt.imshow(img.permute(1,2,0))\n\nshow(*dataset[144])\n# 0-->Covid-19;1-->Normal","692aa8fc":"# Splitting the data into train and validation set\ndef split_train_val(tot_img,val_percentage=0.2,rnd=23):\n  # Here indices are randomly permuted \n  number_of_val=int(tot_img*val_percentage)\n  np.random.seed(rnd)\n  indexs=np.random.permutation(tot_img)\n  return indexs[number_of_val:],indexs[:number_of_val]\n\nrandomness=12\nval_per=0.5\ntrain_indices,validation_indices=split_train_val(len(dataset),val_per,randomness)\nprint(validation_indices[:5])","7ce5cd6a":"from torch.utils.data.sampler import SubsetRandomSampler #samples randomly from given indices\nfrom torch.utils.data.dataloader import DataLoader # loads the data from sampler","5b079084":"# Subset random sampler takes the indices to pick the data\n# dataloader loads with the main dataset, with batch size and the sampler object\nbatch_size=16\n# Training Part\ntrain_sampler=SubsetRandomSampler(train_indices)\ntrain_ds=DataLoader(dataset,batch_size,sampler=train_sampler)\n\n# Validation Part\nval_sampler=SubsetRandomSampler(validation_indices)\nval_ds=DataLoader(dataset,batch_size,sampler=val_sampler)","526e2525":"import torch.nn as nn\nimport torch.nn.functional as F","8eb2737a":"# 1st layer of Conv2d\n# 1st Argument is number of color channel for RGB=3, for BW=1\n# 2nd Argument if number of filters, 3rd is filter size\n# how to calculate its output directly to Linear Unit\n# image=3x64x64-->64-3(filter_size)+1=62. So, output is 62x62xnumber of filter\n# then 62x62xnum_of_filter-->maxpool(2,2)-->62\/2=31-->8*31*31\n# for same layer number of filter of output is input of new channel\n# for conv to linear layer the above calculation is reqd.\n\n\"\"\"\n# Use nn.Sequential for implementation\n# Though I myself doesnt like it.\nmodel=nn.Sequential(\n        nn.Conv2d(3,8,3), \n        nn.ReLU(),\n        nn.MaxPool2d(2,2),\n        nn.Flatten(start_dim=1), #.view(-1,)\n        nn.Linear(8*31*31,2)\n\n)\n\"\"\"\n# Recommended to use Object Oriented Neural Network\n# pytorch nn library provide with 2 component on abstarct level.\n# (i) is transformation i.e code (ii) Collection of weight - data\n# class Module base class for all nn module\n# Every neural network inherits from nn.Module class\nclass ConvNet(nn.Module):\n  def __init__(self):\n    # super here used access method of parent class\n    # dont worry much just boiler plate\n    super(ConvNet,self).__init__()\n    # In conv layer in_channels== input; out_channels=output; kernel_size=filter size\n    self.conv1=nn.Conv2d(in_channels=3,out_channels=8,kernel_size=3)\n    # Linear layer in_features is input, how 8*31*31 came is explained in above comment\n    # out_features= output\n    self.fc1=nn.Linear(in_features=8*31*31,out_features=32)\n    self.out=nn.Linear(in_features=32,out_features=2)\n\n  def forward(self,l):\n    # this method implements forward propagation\n    # So, layers are structured as such\n\n    # 1 Conv layer\n    # may be thinking self.conv1 is an layer object instance how can we call as if it a function\n    # Checkout python documents __call__ this special method is used, so that instances behaves like function\n    # __call__ this special method invokes anytime the object instance is called. This interacts with forward method.\n    l=self.conv1(l)\n    l=F.relu(l)\n    l=F.max_pool2d(l,kernel_size=2)\n\n    # linear and final layer\n    # -1 indicates, give any number of batch size\n    l=l.reshape(-1,8*31*31)\n    l=self.fc1(l)\n    l=self.out(l)\n\n    return l","cfaa7a32":"\nmodel=ConvNet()","110c057e":"# If gpu present then use it or else use cpu\n# if gpu not present dont run this cell\ndef default_device():\n    \n    if torch.cuda.is_available():\n        return torch.device(\"cuda:0\")\n    else:\n        return torch.device(\"cpu\")\n\ndevice=default_device()","e3bd94ca":"# Loading model on GPU\nmodel.to(device)","ef2b8f75":"# Define loss and optimizer\nimport torch.optim as optim\nloss_type = nn.CrossEntropyLoss()\n# Adam optimizer is the combination of momentum with RMSprop and is more powerful\noptimizer = optim.Adam(model.parameters(), lr=0.0005)","d6cecdef":"loss_val=[]\nfor epoch in range(12):  \n# loop over the dataset multiple times\n    print(\"Epoch count-->\",epoch)\n    running_loss=0.0\n    for i, data in enumerate(train_ds):\n        \n        inputs, labels = data\n        # Loading inputs,labels on GPU\n        inputs,labels=inputs.to(device),labels.to(device)\n        #inputs,labels=inputs,labels\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Passing input into the model\n        outputs = model(inputs)\n        \n        # Caculating loss with crossentropy\n        loss = loss_type(outputs, labels)\n        \n        # calculates the gradient \n        loss.backward()\n        \n        # update the weights\n        optimizer.step()\n        \n        running_loss=running_loss+loss.item()* inputs.size(0)\n        \n    loss_val.append(running_loss \/ len(train_ds))\n        \n    print(running_loss)\n        \nplt.plot(loss_val,label=\"loss\")\nplt.legend()\nprint('Finished Training')","9784407b":"right = 0\ntotal = 0\n\nwith torch.no_grad():\n# Switching off the gradient part, so that backpropagation doesnt take place\n    for data in val_ds:\n        images, labels = data\n        #images,labels=images,labels\n        inputs,labels=inputs.to(device),labels.to(device)\n        outputs = model(images)\n        \n        _, predicted = torch.max(outputs,dim=1)\n        total += labels.size(0)\n        # Caculating number of right prediction\n        right += (predicted == labels).sum()\n        \n        \n\nprint('Accuracy on the validation images: %d %%' % (\n    100 * right \/ total))","a76bb8f2":"!mkdir \/tmp\/Xray_test_data","c484330e":"!cp -R \"..\/input\/chest-xray-for-covid19-detection\/Dataset\/Train\/Covid\" \"\/tmp\/Xray_test_data\"","1e466702":"!cp -R \"..\/input\/chest-xray-for-covid19-detection\/Dataset\/Train\/Normal\" \"\/tmp\/Xray_test_data\"","d7ef2163":"!ls \"\/tmp\/Xray_test_data\"","cf643f9e":"!ls -1 \"\/tmp\/Xray_test_data\/Covid\"| wc -l ","c3376886":"!ls -1 \"\/tmp\/Xray_test_data\/Normal\"| wc -l ","3f51a7da":"test_path = \"\/tmp\/Xray_test_data\"","39d588c8":"transform=transforms.Compose([\n                              transforms.Resize([64,64]),\n                              transforms.ToTensor()\n])\ntest_dataset=ImageFolder(test_path,transform=transform)","18ad41ca":"len(test_dataset)","e6d7da96":"# Checking For Samples\nimg0,label0=test_dataset[0]\nprint(img0.shape,label0)\nimg1,label1=test_dataset[150]\nprint(img1.shape,label1)\nprint(\"*\"*60)\nprint(test_dataset.classes)#list out all the classes","7f36eff7":"batch_size=32\n# Training Part\ntest_ds=DataLoader(test_dataset,batch_size)","4fb9a97b":"right_test = 0\ntotal_test = 0\nwith torch.no_grad():\n    for data in test_ds:\n        images, labels_test = data\n        images,labels_test=images.to(device),labels_test.to(device)\n        #images,labels_test=images,labels_test\n        outputs_test = model(images)\n        _, predicted_test = torch.max(outputs_test, 1)\n        total_test += labels_test.size(0)\n        right_test += (predicted_test == labels_test).sum()\n\nprint('Accuracy on the Test images: %d %%' % (\n    100 * right_test \/ total_test))","00382f57":"## **Applying CNN**","f4851587":"## **Starting Actual work**","4c9a6aea":"### To deal with imbalanced dataset, I just took a lazy way out.","5c6e52cc":"## If helped, do give an upvote. It means a lot.","883d320c":"## **Importing Test data and testing on it**\n#### This is completely from different source from train,val data","ff5302ae":"### If you are forking this notebook, and do comment, if its using GPU.\n### In my kaggle kernel it isnt","6884080f":" ## Importing kaggle Data Train_Val Set","fe100247":"### Splitting Validation Set from Training Set","c0afea3a":"#### Version 3 doesnot have any significance update. Just updated some folder issue.\n#### Thanks to @sarques","218c8351":"### Use \/tmp to create temporary folder","96bbf432":"### Directing to Train Folder\n##### Works similiar as ImageData Generator of Keras","65bb8bd1":"# **Using pytorch detecting Covid-19 Infected Lungs from Normal Lungs with Chest X-Ray**"}}