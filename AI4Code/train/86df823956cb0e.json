{"cell_type":{"94054080":"code","a0ba10f7":"code","1b53357e":"code","7a8aeaad":"code","3b55b257":"code","5e8bae33":"code","60fb4925":"code","94116583":"code","59773892":"code","16d8f6ec":"code","ec3f16b7":"code","f4e7d95b":"code","0ef91f61":"code","1e733679":"code","3ca0ee48":"code","cff54312":"code","6325ec3e":"code","7b796da7":"code","b86803fb":"code","56252aac":"code","c97e12bf":"code","b41cbfb5":"code","5696330e":"code","282849e1":"code","f89b39bd":"code","5f415517":"code","612cafed":"code","6d78b845":"code","a708b6cb":"code","b40307b6":"code","bff7851b":"code","a4b1b44f":"code","f01f8efd":"code","0f410023":"code","8587bba7":"code","ac3b548a":"code","f7aef3c9":"code","ca277056":"code","785febfc":"code","cef83183":"code","c0406941":"code","275f192a":"code","8c1ad1aa":"code","7d36b61a":"code","ac10aeb2":"code","0c42e11f":"code","858fe4d0":"code","253e9cb4":"code","1919f207":"code","7f695d7f":"code","99368336":"code","352f68a5":"code","1f6ff7b1":"code","78ebaccd":"code","f7439f43":"code","0da63a9a":"code","322b2bc0":"code","83bd09cd":"code","a7161163":"code","3a0e36d7":"code","61dbc400":"code","81f5ed91":"code","8bb2ce3e":"markdown","71c5a9f0":"markdown","a6857d03":"markdown","40045a95":"markdown","cfca2ed6":"markdown","46c78f13":"markdown","34b07d6f":"markdown","b2185889":"markdown","8ec5f4ec":"markdown","00030d51":"markdown","d342df8b":"markdown","af63c850":"markdown","bd76a059":"markdown","c80116d9":"markdown","cfc4255f":"markdown","f8723204":"markdown","500cb6d1":"markdown","f96d535b":"markdown","2b2b12da":"markdown","bd90d773":"markdown","da76319b":"markdown","f5879860":"markdown","fd3c577b":"markdown","362477f7":"markdown","4211b28d":"markdown","a6ecdc63":"markdown","3718513e":"markdown","dd27e0c6":"markdown","ecb0bf35":"markdown","69c4b68c":"markdown","d172743a":"markdown","b229725e":"markdown","fafd55cc":"markdown","6d6db8e6":"markdown","2f0a07dd":"markdown","9a881368":"markdown","790a32e7":"markdown","7caebe30":"markdown","e4cef38c":"markdown","41dbfcf9":"markdown","e4a4d6ee":"markdown","3f4b7398":"markdown","8611ea26":"markdown","35c2ba98":"markdown","337cc473":"markdown","3e466923":"markdown","21a2e20f":"markdown","40139928":"markdown","ce8ad012":"markdown","2bcffd0b":"markdown","8f7a8502":"markdown","742c840e":"markdown","e8150ef4":"markdown","b517c92a":"markdown","1551935f":"markdown","3094b217":"markdown","c712e25a":"markdown","54d01ca2":"markdown","a3931a9f":"markdown","696914c6":"markdown","7c2d96e0":"markdown","23476eeb":"markdown","95cc5327":"markdown","296e1441":"markdown","63c34b9b":"markdown"},"source":{"94054080":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\npd.set_option('display.max_columns', 100)","a0ba10f7":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","1b53357e":"train.head()","7a8aeaad":"train.info()","3b55b257":"train.tail()","5e8bae33":"train.shape","60fb4925":"# test head\ntest.head()","94116583":"train.drop_duplicates()\ntrain = train.drop(['id'], axis = 1)\ntrain.shape","59773892":"test.drop_duplicates()\ntest_id = test['id']\ntest = test.drop(['id'], axis = 1)\ntest.shape","16d8f6ec":"test.head()","ec3f16b7":"colunas = train.columns.tolist()\ncolunas_reg = [col for col in colunas if 'reg' in col]\ncolunas_cat = [col for col in colunas if 'cat' in col]\ncolunas_bin = [col for col in colunas if 'bin' in col]\ncolunas_car = [col for col in colunas if 'car' in col and 'cat' not in col]\ncolunas_calc = [col for col in colunas if 'calc' in col]\nprint(colunas_cat)","f4e7d95b":"train.loc[:,colunas_reg].describe()","0ef91f61":"train.loc[:, colunas_car].describe()","1e733679":"id_0 = train[train.target == 0].index\nid_1 = train[train.target == 1].index\npropor\u00e7\u00e3o = id_1.shape[0]\/train.shape[0]\nprint('Qual a probabilidade a priori de um segurado sinistrar: {}'. format(propor\u00e7\u00e3o))","3ca0ee48":"var_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        var_missing.append(f)\n        missings_perc = missings\/train.shape[0]\n        \n        print('Vari\u00e1vel {} tem {} exemplos ({:.2%}) com valores omissos'.format(f, missings, missings_perc))\n        \nprint('No total, existem {} vari\u00e1veis com valores omissos'.format(len(var_missing)))","cff54312":"# Excluindo vari\u00e1veis com muitos dados omissos\nvariaveis_excluir = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(variaveis_excluir, inplace=True, axis=1)\ntrain.drop(colunas_calc, inplace = True, axis = 1)\n# Imputando com a m\u00e9dia e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = media_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = media_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = moda_imp.fit_transform(train[['ps_car_11']]).ravel()","6325ec3e":"# excluir a vari\u00e1vel 'ps_car_03_cat' e 'ps_car_05_cat'\ncolunas_cat.remove('ps_car_03_cat')\ncolunas_cat.remove('ps_car_05_cat')\nprint(colunas_cat)","7b796da7":"for i in colunas_cat:\n    plt.figure()\n    fig, ax = plt.subplots(figsize = (20,10))\n    sns.barplot(ax = ax, x = i, y = 'target', data = train)\n    plt.ylabel('% target', fontsize = 18)\n    plt.xlabel(i, fontsize = 18)\n    plt.tick_params(axis = 'both', which= 'major', labelsize = 18)\n    plt.show()","b86803fb":"continuas = [colunas_reg, colunas_car]\ndef correl(t):\n    correlacao = train[t].corr()\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlacao, cmap = cmap, vmax = 1.0, center = 0, fmt = '.2f',\n           square = True, linewidths = .5, annot = True, cbar_kws ={\"shrink\": .75})\n    plt.show();\n    \n# Vari\u00e1veis reg\nfor j in continuas:\n    print('Heat Map de correla\u00e7\u00f5es para as vari\u00e1veis {}' .format(j))\n    correl(j)","56252aac":"for i in train.columns:\n    if train[i].dtype == 'int64' and i != 'target':\n        train[i] = train[i].astype('category')","c97e12bf":"# Checando\ntrain.info()","b41cbfb5":"# fun\u00e7\u00e3o get_dummies transforma as categorias em vari\u00e1veis bin\u00e1rias\ntrain = pd.get_dummies(train)\ntrain.head()","5696330e":"# checando a dimens\u00e3o\ntrain.shape","282849e1":"X = train.drop([\"target\"], axis = 1)\ny = train[\"target\"]","f89b39bd":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)","5f415517":"scaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)","612cafed":"def gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() \/ totalLosses\n\n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n\n\ndef gini_normalized(actual, pred):\n    return gini(actual, pred) \/ gini(actual, actual)","6d78b845":"# Testando com a regress\u00e3o log\u00edstica com penaliza\u00e7\u00e3o L2\nlr = LogisticRegression(penalty='l2', random_state=1)\nlr.fit(X_train, y_train)\nprob = lr.predict_proba(X_test)[:,1]\nprint(\"\u00cdndice de Gini normalizado para a Regress\u00e3o Log\u00edstica: \",gini_normalized(y_test, prob))","a708b6cb":"# Random Forest com 20 \u00e1rvores\nrf = RandomForestClassifier(n_estimators = 20, max_depth = 4, random_state = 1, max_features = 20)\nrf.fit(X_train, y_train)\npredictions_prob = rf.predict_proba(X_test)[:,1]\nprint(\"\u00cdndice de Gini normalizado para o Random Forest: \", gini_normalized(y_test, predictions_prob))","b40307b6":"# taxa de aprendizagem = 0.05\nxgbm = XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\nxgbm.fit(X_train, y_train)\nprob_xgb = xgbm.predict_proba(X_test)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"\u00cdndice de Gini normalizado para o XGBoost com learning_rate = 0.05: \", gini_normalized(y_test, prob_xgb))\nprint(\"--------------------------------------------------------------------------------------------\")","bff7851b":"# Optamos por tentar submeter o modelo com melhor score nos dados X_test - XGBoost com . \n# Testaremos nos dados completo de treinamento.\nprob_xgb_y = xgbm.predict_proba(X)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"\u00cdndice de Gini normalizado para o XGBoost com learning_rate = 0.05 dados treino: \", gini_normalized(y, prob_xgb_y))\nprint(\"--------------------------------------------------------------------------------------------\")","a4b1b44f":"# Excluindo vari\u00e1veis com muitos dados omissos\nvariaveis_excluir = ['ps_car_03_cat', 'ps_car_05_cat']\ntest.drop(variaveis_excluir, inplace=True, axis=1)\ntest.drop(colunas_calc, inplace = True, axis = 1)\n# Imputando com a m\u00e9dia e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntest['ps_reg_03'] = media_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_14'] = media_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = moda_imp.fit_transform(test[['ps_car_11']]).ravel()\n\n# categorizando \nfor i in test.columns:\n    if test[i].dtype == 'int64' and i != 'target':\n        test[i] = test[i].astype('category')\n        \n# fun\u00e7\u00e3o get_dummies transforma as categorias em vari\u00e1veis bin\u00e1rias\ntest = pd.get_dummies(test)","f01f8efd":"# Normaliza\u00e7\u00e3o\nscaler = MinMaxScaler()\nscaler.fit(test)\ntest = scaler.transform(test)","0f410023":"# Aplicando no modelo XGBoost\ny_test = xgbm.predict_proba(test)[:,1]\n\n# Em results_df est\u00e1 a base de teste escorada, a coluna target possui as probabilidades\nresults_df = pd.DataFrame(data={'id':test_id, 'target':y_test})\nprint(results_df)\nresults_df.to_csv('submiss\u00e3o_1.csv', index=False)","8587bba7":"# Testando nos dados sem exclus\u00e3o de vari\u00e1veis\n#train = pd.read_csv('..\/input\/train.csv')\n#X = train.iloc[:,2:]\n#y = train.iloc[:,1:2]\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"]\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(\n#    X, y, stratify=y, random_state=0)\n# Normaliza\u00e7\u00e3o Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)\n# Modelos\n# Regress\u00e3o Log\u00edstica\n# Testando com a regress\u00e3o log\u00edstica com penaliza\u00e7\u00e3o L2\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(X_train, y_train)\n#prob_lr = lr.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para a Regress\u00e3o Log\u00edstica: \",gini_normalized(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n# Random Forest com 200 \u00e1rvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 4, random_state = 1, max_features = 15)\n#rf.fit(X_train, y_train)\n#prob_rf = rf.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o Random Forest: \", gini_normalized(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n# XGBoost \n#xgbm = XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(X_train, y_train)\n#prob_xgb = xgbm.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o XGBoost: \", gini_normalized(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n# LightGBM\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(X_train, y_train)\n#prob_lgb = lgb.predict_proba(X_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o LightGBM: \", gini_normalized(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n","ac3b548a":"import pandas as pd\nfrom pandas import read_csv, DataFrame\nimport numpy as np\nfrom numpy.random import seed\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom matplotlib import pyplot as plt","f7aef3c9":"# Carregamento das bases de treinamento e teste em dataframes\ntrain = pd.read_csv('..\/input\/train.csv')\n\nprint(train.shape)\n\n# X armazena dos dados em um dataframe\nX = train.iloc[:,2:]\n# y armazena os labels em um dataframe\ny = train.iloc[:,1:2]\n\n# target_names armazena os valores distintos dos labels\ntarget_names = train['target'].unique()\n\n# Normaliza os dados de treinamento\nscaler = MinMaxScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nprint(X_scaled)","ca277056":"X.head()","785febfc":"X.columns","cef83183":"print(\"N\u00famero de Colunas: \", X.shape[1])","c0406941":"y.head()","275f192a":"# Cria\u00e7\u00e3o do AutoEncoder com 3 neur\u00f4nios na camada escondida usando Keras.\n#input_dim = X_scaled.shape[1]\n\n# Defini\u00e7\u00e3o do n\u00famero de vari\u00e1veis resultantes do Encoder\n#encoding_dim = 10\n\n#input_data = Input(shape=(input_dim,))\n\n# Configura\u00e7\u00f5es do Encoder\n#encoded = Dense(encoding_dim, activation='linear')(input_data)\n#encoded = Dense(encoding_dim, activation='sgmoid')(input_data)\n#encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_data)\n\n#encoded1 = Dense(20, activation = 'relu')(input_data)\n#encoded2 = Dense(10, activation = 'relu')(encoded1)\n#encoded3 = Dense(5, activation = 'relu')(encoded2)\n#encoded4 = Dense(encoding_dim, activation = 'relu')(encoded3)\n\n# Configura\u00e7\u00f5es do Decoder\n#decoded = Dense(input_dim, activation='linear')(encoded)\n#decoded = Dense(input_dim, activation='sgmoid')(encoded)\n\n#decoded1 = Dense(5, activation = 'relu')(encoded4)\n#decoded2 = Dense(10, activation = 'relu')(decoded1)\n#decoded3 = Dense(20, activation = 'relu')(decoded2)\n#decoded4 = Dense(input_dim, activation = 'sigmoid')(decoded3)\n\n# Combinando o Encoder e o Decoder em um modelo AutoEncoder\n#autoencoder = Model(input_data, decoded4)\n#autoencoder.compile(optimizer='adam', loss='mse')\n#print(autoencoder.summary())\n# Treinamento de fato - Defini\u00e7\u00e3o de alguns par\u00e2metros como n\u00famero de \u00e9pocas, batch size, por exemplo.\n#history = autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=256, shuffle=True, validation_split=0.1, verbose = 1)\n\n#plot our loss \n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('Model Train vs Validation Loss')\n#plt.ylabel('Loss')\n#plt.xlabel('Epoch')\n#plt.legend(['Train', 'Validation'], loc='upper right')\n#plt.show()","8c1ad1aa":"#test = pd.read_csv('..\/input\/test.csv')\n\n#print(test.shape)\n\n# X armazena dos dados em um dataframe\n#X = test.iloc[:,1:]\n\n# Normaliza os dados de treinamento\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X_scaled = scaler.transform(X)\n\n# Utilizar o Encoder para codificar os dados de entrada\n#encoder = Model(input_data, encoded4)\n#encoded_data = encoder.predict(X_scaled)\n\n#print(encoded_data)","7d36b61a":"# Carregamento das bases de treinamento e teste em dataframes\n#train = pd.read_csv('..\/input\/train.csv')\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"]\n\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\n# Normaliza\u00e7\u00e3o Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)\n\n# aplicando autoencoder nos dados de treinamento e teste\n#encoder = Model(input_data, encoded4)\n#encoded_data_train = encoder.predict(X_train)\n#encoder = Model(input_data, encoded4)\n#encoded_data_test = encoder.predict(X_test)\n\n# Modelos\n# Regress\u00e3o Log\u00edstica\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(encoded_data_train, y_train)\n#prob_lr = lr.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para a Regress\u00e3o Log\u00edstica com autoencoder: \",gini_normalized(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# Random Forest com 200 \u00e1rvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 5, random_state = 1, max_features = 7)\n#rf.fit(encoded_data_train, y_train)\n#prob_rf = rf.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o Random Forest com autoencoder: \", gini_normalized(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# XGBoost \n#import xgboost as xgb\n#xgbm = xgb.XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(encoded_data_train, y_train)\n#prob_xgb = xgbm.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o XGBoost com autoencoder: \", gini_normalized(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# LightGBM\n#from lightgbm import LGBMClassifier\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(encoded_data_train, y_train)\n#prob_lgb = lgb.predict_proba(encoded_data_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o LightGBM com autoencoder: \", gini_normalized(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","ac10aeb2":"#import warnings\n#warnings.filterwarnings('ignore')\n#import numpy as np\n#import pandas as pd\n#from datetime import datetime\n#from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n#from sklearn.metrics import roc_auc_score\n#from sklearn.model_selection import StratifiedKFold\n#from sklearn.model_selection import train_test_split\n#from xgboost import XGBClassifier","0c42e11f":"#def timer(start_time=None):\n#    if not start_time:\n#        start_time = datetime.now()\n#        return start_time\n#    elif start_time:\n#        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#        tmin, tsec = divmod(temp_sec, 60)\n#        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n\n\n# Carregamento das bases de treinamento e teste em dataframes\n#train = pd.read_csv('..\/input\/train.csv', dtype={'id': np.int32, 'target': np.int8})\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#Y = train[\"target\"].values\n\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, random_state=0)\n\n######################################\n\n#train_df = pd.read_csv('..\/input\/train.csv', dtype={'id': np.int32, 'target': np.int8})\n#Y = train_df['target'].values\n#X = train_df.drop(['target', 'id'], axis=1)\n#test_df = pd.read_csv('..\/input\/test.csv', dtype={'id': np.int32})\n#test = test_df.drop(['id'], axis=1)","858fe4d0":"# Grid de Par\u00e2metros para o XGBoost\n#params = {\n    #    'min_child_weight': [1, 5, 10],\n   #     'gamma': [0.5, 1, 1.5, 2, 5],\n  #      'subsample': [0.6, 0.8, 1.0],\n #       'colsample_bytree': [0.6, 0.8, 1.0],\n#        'max_depth': [3, 4, 5]}","253e9cb4":"#xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic', silent=True, nthread=1)","1919f207":"# N\u00famero de folds para Cross-Validation\n#folds = 2\n\n# N\u00famero de combina\u00e7\u00f5es a serem feitos no Grid Search. No Total podem ser feitas 3x5x3x3x3 = 405 combina\u00e7\u00f5es. Quantos mais combina\u00e7\u00f5es, mais tempo leva.\n#param_comb = 1\n\n# Configura\u00e7\u00e3o dos folds estratificados para o Cross-Validation\n#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# Configura\u00e7\u00e3o do Grid Search\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001 )\n#grid = GridSearchCV(estimator=xgb, param_grid=params ,scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3)\n\n# Execu\u00e7\u00e3o do Treinamento com Grid Search\n#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n#random_search.fit(X, Y)\n#grid.fit(X, Y)\n#timer(start_time) # timing ends here for \"start_time\" variable","7f695d7f":"#print('\\n All results:')\n#print(random_search.cv_results_)\n#print('\\n Best estimator:')\n#print(random_search.best_estimator_)\n#print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n#print(random_search.best_score_ * 2 - 1)\n#print('\\n Best hyperparameters:')\n#print(random_search.best_params_)\n#results = pd.DataFrame(random_search.cv_results_)\n#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)\n\n# print('\\n All results:')\n# print(grid.cv_results_)\n# print('\\n Best estimator:')\n# print(grid.best_estimator_)\n# print('\\n Best score:')\n# print(grid.best_score_ * 2 - 1)\n# print('\\n Best parameters:')\n# print(grid.best_params_)\n# results = pd.DataFrame(grid.cv_results_)\n# results.to_csv('xgb-grid-search-results-01.csv', index=False)\n\n# y_test = grid.best_estimator_.predict_proba(test)\n# results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n# results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)","99368336":"# predict_proba j\u00e1 utiliza o modelo com os melhores hyperpar\u00e2metros para realizar o predict da base de teste.\n#y_test = random_search.predict_proba(test)\n\n# Em results_df est\u00e1 a base de teste escorada, a coluna target possui as probabilidades\n#results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n#print(results_df)\n#results_df.to_csv('submission-random-grid-search-xgb-porto-01.csv', index=False)","352f68a5":"# Carregando Bibliotecas\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\npd.set_option('display.max_columns', 100)\n\n","1f6ff7b1":"# Computing gini coefficient ( Coursey Kaggle)\n# from CPMP's kernel https:\/\/www.kaggle.com\/cpmpml\/extremely-fast-gini-computation @jit\n#def eval_gini(y_true, y_prob):\n#    y_true = np.asarray(y_true)\n#    y_true = y_true[np.argsort(y_prob)]\n#    ntrue = 0\n#    gini = 0\n#    delta = 0\n#    n = len(y_true)\n#    for i in range(n-1, -1, -1):\n#        y_i = y_true[i]\n#        ntrue += y_i\n#        gini += y_i * delta\n#        delta += 1 - y_i\n#    gini = 1 - 2 * gini \/ (ntrue * (n - ntrue))\n#    return gini","78ebaccd":"# IMPORTA\u00c7\u00c3O DOS DADOS\n#train = pd.read_csv('..\/input\/train.csv')\n#test = pd.read_csv('..\/input\/test.csv')","f7439f43":"# Separando X e y\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"].values","0da63a9a":"# Excluindo vari\u00e1veis com 'calc'\n#colunas = X.columns.tolist()\n#colunas_cat = [col for col in colunas if 'cat' in col]\n#colunas_calc = [col for col in colunas if 'calc' in col]\n\n#variaveis_excluir = colunas_calc\n#X.drop(variaveis_excluir, inplace=True, axis=1)","322b2bc0":"# One-hot nas categ\u00f3ricas\n#X = pd.get_dummies(X)\n#X.head()","83bd09cd":"# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(\n#    X, y, stratify=y, random_state=0)\n# Normaliza\u00e7\u00e3o Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)","a7161163":"# Testar os modelos\n# Modelos\n# Regress\u00e3o Log\u00edstica\n# Testando com a regress\u00e3o log\u00edstica com penaliza\u00e7\u00e3o L2\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(X_train, y_train)\n#prob_lr = lr.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para a Regress\u00e3o Log\u00edstica: \",eval_gini(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n# Random Forest com 200 \u00e1rvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 4, random_state = 1, max_features = 15)\n#rf.fit(X_train, y_train)\n#prob_rf = rf.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o Random Forest: \", eval_gini(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n# XGBoost \n#xgbm = XGBClassifier(max_depth=4, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(X_train, y_train)\n#prob_xgb = xgbm.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o XGBoost: \", eval_gini(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n# LightGBM\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(X_train, y_train)\n#prob_lgb = lgb.predict_proba(X_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o LightGBM: \", eval_gini(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","3a0e36d7":"## Testando na base de treinamento completa\n# Normaliza\u00e7\u00e3o Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X = scaler.transform(X)","61dbc400":"# base completa\n# Modelos\n#prob_lr = lr.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para a Regress\u00e3o Log\u00edstica: \",eval_gini(y, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_rf = rf.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o Random Forest: \", eval_gini(y, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_xgb = xgbm.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o XGBoost: \", eval_gini(y, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_lgb = lgb.predict_proba(X)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"\u00cdndice de Gini normalizado para o LightGBM: \", eval_gini(y, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","81f5ed91":"# Criando o arquivo para submiss\u00e3o\n#test_df = pd.read_csv('..\/input\/test.csv', dtype={'id': np.int32})\n#test = test_df.drop(['id'], axis=1)\n#test.drop(variaveis_excluir, inplace=True, axis=1)\n#test = pd.get_dummies(test)\n#scaler = MinMaxScaler()\n#scaler.fit(test)\n#test = scaler.transform(test)\n#y_test = xgbm.predict_proba(test)[:,1]\n\n# Em results_df est\u00e1 a base de teste escorada, a coluna target possui as probabilidades\n#results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test})\n#print(results_df)\n#results_df.to_csv('submission-random-grid-search-xgb-01.csv', index=False)","8bb2ce3e":"Utilizamos a implementa\u00e7\u00e3o do coeficiente de gini normalizado em python obtido neste t\u00f3pico: http:\/\/www.kaggle.com\/c\/ClaimPredictionChallenge\/discussion\/703","71c5a9f0":"Algumas informa\u00e7\u00f5es sobre os dados:\n\n* as vari\u00e1veis com o sufixo **\"bin\"** s\u00e3o bin\u00e1rias; e **\"cat\"** para indicar vari\u00e1veis categ\u00f3ricas.\n* Os dados omissos **missing data** foram codificados com o valor **-1** . \n* A vari\u00e1vel de interesse, **target** significa se o segurado incorreu em sinistou ou n\u00e3o. \n\nPortanto, o objetivo deste prejeto \u00e9 o de construir um algoritmo de classifica\u00e7\u00e3o com o objetivo de prever se o segurado ir\u00e1 sinistrar. \n","a6857d03":"- **ps_car_03_cat and ps_car_05_cat** t\u00eam uma elevada quantidade de dados omissos - optamos pela remo\u00e7\u00e3o dessas vari\u00e1veis.\n- Para a vari\u00e1vel **ps_reg_03** e **ps_car_14**, aplicaremos a imputa\u00e7\u00e3o da m\u00e9dia.\n- Enquanto que, para **ps_car_11**, imputaremos a moda (valor mais frequente).\n- Para as demais vari\u00e1veis categ\u00f3ricas, optamos por deixar o missing como uma caracter\u00edstica. ","40045a95":"Verificamos que os dados omissos (missing) s\u00e3o representativos. Dessa forma, optamos por deix\u00e1-los como uma categoria adicional, pois o segurado que n\u00e3o apresentou informa\u00e7\u00e3o apresenta maior probabilidade de sinistrar (acidentar).","cfca2ed6":"A partir desses dados de treinamento, vemos:\n* vari\u00e1veis bin\u00e1rias\n* vari\u00e1veis categ\u00f3ricas codigicadas como valores inteiros\n* vari\u00e1veis restantes como valores reais (float) ou inteiros\n* -1 representando os valores faltantes\n* a vari\u00e1vel **target** e o **id**","46c78f13":"**Criamos as vari\u00e1veis bin\u00e1rias. ","34b07d6f":"#### vari\u00e1veis 'car'","b2185889":"O conjunto de dados de teste tamb\u00e9m possui a vari\u00e1vel **id** que representa a identifica\u00e7\u00e3o do segurado. \nVamos exclu\u00ed-la de ambos os conjuntos de dados (treinamento e teste).","8ec5f4ec":"Visualizando a quantidade de linhas e colunas dos dados de treinamento.","00030d51":"### Coeficiente de Gini Normalizado","d342df8b":"# Autoencoder","af63c850":"## Carregando bibliotecas","bd76a059":"### XGBoost ","c80116d9":"### Qual a propor\u00e7\u00e3o de segurados com sinistros (priori).","cfc4255f":"#### Divindindo os dados em treinamento e teste\nVamos dividir os dados em treinamento e teste para realizar a avalia\u00e7\u00e3o por meio da valida\u00e7\u00e3o cruzada com o intuito de melhorar a capacidade de generaliza\u00e7\u00e3o dos classificadores. \nUsamos a fun\u00e7\u00e3o `train_test_split` do sklearn com a amostragem estratificada para manter a propor\u00e7\u00e3o da vari\u00e1vel target. ","f8723204":"Testamos tamb\u00e9m o Random Forest com 200 \u00e1rvores, 15 atributos e com profundidade m\u00e1xima de 4 n\u00f3s.","500cb6d1":"## TESTE - FINAL","f96d535b":"Percebe-se claramente que os modelos apresentaram maiores valores para o coeficiente de gini normalizado, isto \u00e9, o tratamento dos dados (exclus\u00e3o de vari\u00e1veis e imputa\u00e7\u00e3o de m\u00e9dia e moda) n\u00e3o ajudou. \nDessa forma, tentaremos rodar um autoencoder que serve para reduzir a dimensionalidade dos atributos. ","2b2b12da":"### Vari\u00e1veis cont\u00ednuas - Correla\u00e7\u00e3o","bd90d773":"Testamos o XGBoost com 100 estima\u00e7\u00f5es, profundidade m\u00e1xima de 5, e taxa de aprendizagem de 0.05.","da76319b":"<a class=\"anchor\" id=\"pre_process\"><\/a>","f5879860":"<a class=\"anchor\" id=\"autoencoder\"><\/a>","fd3c577b":"<a class=\"anchor\" id=\"eda\"><\/a>","362477f7":"## An\u00e1lise explorat\u00f3ria de dados","4211b28d":"## Modelos","a6ecdc63":"### Random Forest","3718513e":"#### Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias","dd27e0c6":"Verificamos correla\u00e7\u00e3o forte para as vari\u00e1veis:\n- ps_reg_02 e ps_reg_03 (0,7)\n- ps_car_12 e ps_car_13 (0,67)\n- ps_car_12 e ps_car_14 (0,58)\n- ps_car_13 and ps_car_15 (0,67)","ecb0bf35":"### Categ\u00f3ricas\nVamos criar gr\u00e1ficos de barras para as vari\u00e1veis categ\u00f3ricas e analisar como est\u00e1 a distribui\u00e7\u00e3o e os dados omissos.","69c4b68c":"#### Codifica\u00e7\u00e3o das vari\u00e1veis 'int64' como 'category' \nFizemos a codifica\u00e7\u00e3o em `category` para realizar o procedimento de one-hot-encoding das vari\u00e1veis categ\u00f3ricas.","d172743a":"Vamos separar a vari\u00e1vel **target** dos atributos, criando uma matriz X e um vetor Y.","b229725e":"Falta a vari\u00e1vel **target**. <br>\nTudo bem, vamos analisar a estrutura dos dados com a fun\u00e7\u00e3o `info()`.","fafd55cc":"## Introdu\u00e7\u00e3o","6d6db8e6":"### Regress\u00e3o log\u00edstica","2f0a07dd":"Temos 59 colunas e 595212 linhas. <br>\nVamos analisar abaixo se h\u00e1 duplica\u00e7\u00e3o de linhas.\n\n","9a881368":"<a class=\"anchor\" id=\"xgboost_grid\"><\/a>\n# XGBoost com Grid Search","790a32e7":"#### Criando vetores com os nome das vari\u00e1veis pelos grupos (reg, bin, car ...)","7caebe30":"### Testando com dados brutos","e4cef38c":"Primeiro teste da regress\u00e3o log\u00edstica com a penaliza\u00e7\u00e3o L2. \nUsamos o pipeline para fazer a normaliza\u00e7\u00e3o min-max.","41dbfcf9":"Apenas ps_car_12 e ps_car_15 possuem dados faltantes\nVamos aplicar a normaliza\u00e7\u00e3o **min-max** para padronizar a escala. ","e4a4d6ee":"5 primeiras linhas dos dados de treinamento","3f4b7398":"N\u00e3o h\u00e1 linhas em duplicidade.\nPrecisamos verificar a estrutura da tabela dos dados de teste para ver se est\u00e1 semelhante.","8611ea26":"### Realizando one-hot-encoding nas vari\u00e1veis categ\u00f3ricas\nPara as vari\u00e1veis que possuem mais de 2 categorias, realizamos o processo de one-hot-encoding que \u00e9 a cria\u00e7\u00e3o de atributos (vari\u00e1veis) para cada categoria da vari\u00e1vel. Essas vari\u00e1veis ser\u00e3o bin\u00e1rias, assumindo o valor 1 quando da presen\u00e7a da categoria, e 0, na aus\u00eancia.","35c2ba98":"## Loading data","337cc473":"<a class=\"anchor\" id=\"modelos\"><\/a>","3e466923":"#### Utiliza\u00e7\u00e3o do Encoder gerado para realizar a compress\u00e3o e reduzir a dimens\u00e3o da base de treinamento","21a2e20f":"<a class=\"anchor\" id=\"modelosauto\"><\/a>","40139928":"#### Normaliza\u00e7\u00e3o min-max ","ce8ad012":"## Conhecendo os dados","2bcffd0b":"## Avalia\u00e7\u00e3o dos Modelos ap\u00f3s o Autoencoder","8f7a8502":"## Estat\u00edstica descritiva","742c840e":"O m\u00e9todo `describe` apresenta as estat\u00edsticas descritivas para todas as colunas do data frame. \nContudo, s\u00f3 far\u00e1 sentido aplic\u00e1-lo nas vari\u00e1veis cont\u00ednuas, isto \u00e9, naquelas representadas pelo conjunto dos valores reais. \nPara analisar as vari\u00e1veis categ\u00f3ricas, utilizaremos os gr\u00e1ficos na an\u00e1lise explorat\u00f3ria. ","e8150ef4":"## Pr\u00e9-processamento","b517c92a":"<a class=\"anchor\" id=\"conhecendo_dados\"><\/a>","1551935f":"<a id='carregando_bibliotecas'><\/a>","3094b217":"### Checando os valores omissos (*missing*)\nOs valores omissos foram representados pelo valor -1.","c712e25a":"5 \u00faltimas linhas dos dados de treinamento","54d01ca2":"Apenas a vari\u00e1vel **reg_03** possui missing. <br>\nPara solucionar esse problema, vamos usar um m\u00e9todo de imputar a mediana onde existe o valor **-1**. <br>\nAp\u00f3s, faremos a normaliza\u00e7\u00e3o **min-max**  em **reg_02** e **reg_03** para reduzir a escala, fixando-as no intervalor [0,1].","a3931a9f":"Percebemos que falta uma coluna no conjunto de teste. Qual seria?","696914c6":"#### Carregamento e normaliza\u00e7\u00e3o da base de treinamento","7c2d96e0":"Aplicar esses procedimentos aos dados de teste","23476eeb":"#### Configura\u00e7\u00e3o e execu\u00e7\u00e3o do treinamento do Autoencoder","95cc5327":"<a class=\"anchor\" id=\"estat_descritiva\"><\/a>","296e1441":"Este trabalho foi realizado por Filipe C. L. Duarte (fcld), H\u00e9lio Gon\u00e7alves de Souza Junior (hgsj) e Matheus de Farias Cavalcanti Santos (mfcs) como parte da avalia\u00e7\u00e3o da disciplina de Aprendizagem de M\u00e1quina da P\u00f3s-Gradua\u00e7\u00e3o em Ci\u00eancias da Computa\u00e7\u00e3o da UFPE no primeiro semestre de 2019. \n\n\n1. [Carregando bibliotecas](#carregando_bibliotecas)\n2. [Conhecendo os dados](#conhecendo_dados)\n3. [Estat\u00edstica descritiva](#estat_descritiva)\n4. [Pr\u00e9-processamento](#pre_process)\n5. [An\u00e1lise explorat\u00f3ria de dados](#eda)\n7. [Modelos](#modelos)\n8. [Autoencoder](#autoencoder)\n9. [Modelos ap\u00f3s redu\u00e7\u00e3o da dimensionalidade](#modelosauto)\n10. [XGBoost com Grid Search](#xgboost_grid)","63c34b9b":"Verificamos que os modelos n\u00e3o produziram bons resultados em compara\u00e7\u00e3o com o score da competi\u00e7\u00e3o.\nPortanto, vamos tentar os modelos modelos com os dados sem pr\u00e9-processamento, apenas com a normaliza\u00e7\u00e3o min-max. \nOptamos por essa abordagem em raz\u00e3o de que os competidores que alcan\u00e7aram bons resultados n\u00e3o exclu\u00edram vari\u00e1veis, e n\u00e3o realizaram a exclus\u00e3o de dados missing. "}}