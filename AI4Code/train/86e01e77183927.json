{"cell_type":{"e9317590":"code","6732b171":"code","561e6656":"code","be20edca":"code","a570ff63":"code","f137c9a0":"code","fccc668e":"code","801a7513":"code","3fb15a9e":"code","d683d4e0":"code","4d8df606":"code","92721f61":"markdown","24b90c04":"markdown","f86fe7cd":"markdown","e557d921":"markdown","a2d6d912":"markdown"},"source":{"e9317590":"import os\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torchvision\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport xml.etree.ElementTree as ET\n\nimages_dir = '\/kaggle\/input\/ship-detection\/images\/'\nannotations_dir = '\/kaggle\/input\/ship-detection\/annotations\/'\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","6732b171":"sample_id = 44\n\nsample_image_path = f'\/kaggle\/input\/ship-detection\/images\/boat{sample_id}.png'\nsample_annot_path = f'\/kaggle\/input\/ship-detection\/annotations\/boat{sample_id}.xml'","561e6656":"sample_image = Image.open(sample_image_path)\nsample_image","be20edca":"with open(sample_annot_path) as annot_file:\n    print(''.join(annot_file.readlines()))","a570ff63":"tree = ET.parse(sample_annot_path)\nroot = tree.getroot()\n\nsample_annotations = []\n\nfor neighbor in root.iter('bndbox'):\n    xmin = int(neighbor.find('xmin').text)\n    ymin = int(neighbor.find('ymin').text)\n    xmax = int(neighbor.find('xmax').text)\n    ymax = int(neighbor.find('ymax').text)\n    \n    sample_annotations.append([xmin, ymin, xmax, ymax])\n    \nprint('Ground-truth annotations:', sample_annotations)","f137c9a0":"sample_image_annotated = sample_image.copy()\n\nimg_bbox = ImageDraw.Draw(sample_image_annotated)\n\nfor bbox in sample_annotations:\n    img_bbox.rectangle(bbox, outline=\"white\") \n    \nsample_image_annotated","fccc668e":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n    pretrained=True, progress=True, num_classes=91, pretrained_backbone=True)\n\nmodel","801a7513":"model.eval()\n\nnp_sample_image = np.array(sample_image.convert(\"RGB\"))\n\ntransformed_img = torchvision.transforms.transforms.ToTensor()(\n        torchvision.transforms.ToPILImage()(np_sample_image))\n\nresult = model([transformed_img])\n\nresult","3fb15a9e":"COCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack', 'umbrella', 'N\/A', 'N\/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N\/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table',\n    'N\/A', 'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']","d683d4e0":"boat_id = 9\nboat_boxes = [x.detach().numpy().tolist() for i, x in enumerate(result[0]['boxes']) if result[0]['labels'][i] == boat_id]\nboat_boxes","4d8df606":"sample_image_annotated = sample_image.copy()\n\nimg_bbox = ImageDraw.Draw(sample_image_annotated)\n\nfor bbox in sample_annotations:\n    img_bbox.rectangle(bbox, outline=\"white\") \n\nfor bbox in boat_boxes:\n    x1, x2, x3, x4 = map(int, bbox)\n    print(x1, x2, x3, x4)\n    img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\") \n\nsample_image_annotated","92721f61":"### Draw Image With Annotations","24b90c04":"### Download Faster R-CNN","f86fe7cd":"### Draw Predictions","e557d921":"This notebook shows how to read Ship Detection data (image, annotations) and run pretrained Faster R-CNN (from torchvision.models) to get predictions. I used fully pretrained FRCNN (with all COCO classes), so it unfortunately detects bunch of different object classes and tends to confuse ships with clocks or birds, for some reason. In another notebook, I'll train that model on Ship Detection dataset to only detect ship bounding-boxes.","a2d6d912":"### Sample Image\n\nChoosing one sample image (and annotation file) to display and predict."}}