{"cell_type":{"e540f90d":"code","2ebc6b99":"code","e9812d26":"code","b5c99bb9":"code","5ada3379":"code","3e673b93":"code","b974974f":"code","0a00b56e":"code","75e62c99":"code","a46dca49":"code","bcdd5c82":"code","c6250414":"code","265941da":"markdown"},"source":{"e540f90d":"import numpy as np\nimport pandas as pd\nimport warnings\nimport sklearn","2ebc6b99":"target=\"TARGET\"\nsubmission_id_col=\"SK_ID_CURR\"\n\nseed_split=1 \ntest_size=1\/3\nseed_train=100\n\ndf_kaggle_train=pd.read_csv(\"..\/input\/application_train.csv\")\ndf_kaggle_test=pd.read_csv(\"..\/input\/application_test.csv\")\n","e9812d26":"from sklearn.model_selection import train_test_split\n\n# Split X,y\ny= df_kaggle_train[target].values\ndf_kaggle_train.drop(columns=target,inplace=True)\n\n# Split kaggle train, reserve internal hold out test set\nX_train, X_test, y_train,y_test = train_test_split(df_kaggle_train,y, test_size=test_size, random_state=seed_split,stratify =y)","b5c99bb9":"from sklearn.base import TransformerMixin\n\n# Note: sklearn .20 has now SimpleImputer works for categorical also, workaround\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self, default_value=\"NA\"):\n        self.default_value = default_value\n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return pd.DataFrame(X).fillna(self.default_value)","5ada3379":"import sklearn.preprocessing as pp\nfrom sklearn_pandas import DataFrameMapper\n\n# Some workarounds for sklearn 19.1 (.20 use OneHot, SimpleImputer)\nnums=[ ([c],pp.Imputer()) for c in X_train.select_dtypes(include=[np.number])]\ncats=[ ([c],[DataFrameImputer(default_value=\"NA\"), pp.LabelEncoder()]) for c in X_train.select_dtypes(include=[\"object\"])]\nmapper=DataFrameMapper(nums+cats)","3e673b93":"from sklearn.pipeline import Pipeline\nfrom lightgbm import LGBMClassifier\n\npipeline=Pipeline([('featurize', mapper),(\"clf\",LGBMClassifier(random_state=seed_train))])\n","b974974f":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    pipeline.fit(X_train,y_train)\n    \n    y_pred_train=pipeline.predict_proba(X_train)[:,1]\n    y_pred_test=pipeline.predict_proba(X_test)[:,1]","0a00b56e":"from sklearn.metrics import roc_auc_score\n\nprint(\"train score\",roc_auc_score(y_score=y_pred_train,y_true=y_train))\nprint(\"test score\",roc_auc_score(y_score=y_pred_test,y_true=y_test))","75e62c99":"# Full fit\nfull_pipeline=sklearn.clone(pipeline)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    full_pipeline.fit(df_kaggle_train,y)\n    y_pred_submission=full_pipeline.predict_proba(df_kaggle_test)[:,1]","a46dca49":"# Prepare submission\ndf_submission=pd.DataFrame({submission_id_col:df_kaggle_test[submission_id_col],target:y_pred_submission})\ndf_submission.head()","bcdd5c82":"# Check predictions \ndf_submission[target].hist()\nprint(\"y mean:\",np.mean(y))\nprint(\"y submission mean:\",df_submission[target].mean())","c6250414":"df_submission.to_csv(f\"submission.csv\",index=False)\nprint(\"Done!\")","265941da":"# Minimal pipeline with LightGBM (~.744 AUC on Public Leaderboard)\n\n- Still trying to make it shorter. Please do let me know if you have any suggestions\/improvements! \n- No EDA, no custom\/manual feature engineering (aside from generic numeric\/categoricals handling in pipeline)\n- (pipeline goal is to be as generic as possible)\n- some warnings with LabelEnconder I coudnt fix yet, checking.\n- note: sklearn .20 has some  solutions for OneHot\/Categorical Imputation, anyone knows ow to update the package Kaggle kernels?"}}