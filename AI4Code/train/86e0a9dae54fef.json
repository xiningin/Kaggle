{"cell_type":{"1fbeff65":"code","13b85496":"code","27990fdd":"code","4f84450b":"code","34b2a9fa":"code","cbc75d2d":"code","0d9779b7":"code","b8c170a5":"code","b9505a36":"code","847f9582":"code","2c757655":"code","b12be090":"code","a3b12aea":"code","f214b491":"code","a9658a69":"code","48d8dd4d":"code","dbebc6d7":"code","f7c3d12a":"code","1408038a":"code","63f6fcc3":"code","fb18ba5a":"code","dfa4a07c":"code","9152d7a8":"code","4b865c1b":"code","1917c945":"code","838c9efd":"code","ac3c557b":"code","cb62bb0a":"code","dcb31b80":"code","0309822a":"code","d6298717":"code","08b3a41c":"code","ad7940b7":"code","95686feb":"code","832152e6":"code","8d38c3de":"code","9a298d28":"code","96dbf495":"code","9d951258":"code","167f8bab":"code","bd122e45":"code","3eb4af73":"code","a3192b95":"code","fc220100":"code","04993513":"code","7a80862b":"code","c9a73647":"code","a7b2c0a9":"code","6c8c5f64":"code","5481bab0":"code","2b6ec5b3":"code","c62f484a":"code","d4b9de06":"markdown","15a4f171":"markdown","c462cfd2":"markdown","23179f51":"markdown","59ec39d2":"markdown","b0be1cbc":"markdown","4947d614":"markdown","b2a17f35":"markdown","07c7d192":"markdown","cd992cf7":"markdown","cbb90c65":"markdown","fec78ce3":"markdown","5eeaeae3":"markdown","40b9e5cf":"markdown","03b6b17f":"markdown","7e97eef2":"markdown","b690b049":"markdown","29714064":"markdown","059c8b7a":"markdown","0253b8b1":"markdown","17b752bf":"markdown","1fda68f7":"markdown","da4353b7":"markdown","c90d38de":"markdown","d1073753":"markdown","5a9920c7":"markdown","6311feb6":"markdown","ae05ef06":"markdown","b66b18cf":"markdown","9e7b4631":"markdown","7e937446":"markdown","2813bcca":"markdown","945e1845":"markdown","244dd834":"markdown","067ed1db":"markdown","8d55222a":"markdown","094d6d86":"markdown","a2307e27":"markdown","f3e4fbc1":"markdown","3ea79626":"markdown","d699f23a":"markdown","a9404dbb":"markdown","38e90467":"markdown","a421ea41":"markdown","7d0f3c58":"markdown","87799756":"markdown","d882e17f":"markdown","4bf5a426":"markdown","1128d6b2":"markdown","1783dca5":"markdown"},"source":{"1fbeff65":"import numpy as np #linear algebra\nimport pandas as pd # data processing I\/O CSV files\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud\n\nfrom collections import Counter\n\n#machine learning library\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, RandomizedSearchCV, train_test_split\n\n#filter out warnings\nimport warnings\nwarnings.filterwarnings('ignore')","13b85496":"prob = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/train\/problem_data.csv')\ntrain = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/train\/train_submissions.csv')\nuser = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/train\/user_data.csv')\ntest = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/test_submissions_NeDLEvX.csv')\nsub = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/sample_submission_SCGtj9F.csv')","27990fdd":"train.head()","4f84450b":"prob.head()","34b2a9fa":"user.head()","cbc75d2d":"test.head()","0d9779b7":"train = train.merge(prob, on='problem_id')\ntrain = train.merge(user, on='user_id')","b8c170a5":"train.head()","b9505a36":"#test_ids = test.pop('ID') # save test IDs for submission\ntest = test.merge(prob, on='problem_id') # merge test data with user and prob data\ntest = test.merge(user, on='user_id')\ntest_ids = test.pop('ID') # save test IDs for submission","847f9582":"print(train.info())\nprint('='*50)\nprint(test.info())","2c757655":"print(train.isna().sum())","b12be090":"plt.figure(figsize=(10,5))\nsns.set(font_scale=1.2)\nsns.countplot(y='attempts_range', data=train).set_title('Distribution of attempts_range ')\nplt.show()","a3b12aea":"plt.figure(figsize=(12,7))\nsns.set(font_scale=1.2)\nsns.countplot(y='level_type', data=train, order=train.level_type.value_counts().index).set_title('Problem levels')\nplt.show()","f214b491":"fig, axes = plt.subplots(4,2)\nfig.set_size_inches(20,27)\nsns.countplot(train['points'][train.level_type=='A'], ax=axes[0][0]).set_title('Points distribution for \"A\" level problem')\nsns.countplot(train['points'][train.level_type=='B'], ax=axes[0][1]).set_title('Points distribution for \"B\" level problem')\nsns.countplot(train['points'][train.level_type=='C'], ax=axes[1][0]).set_title('Points distribution for \"C\" level problem')\nsns.countplot(train['points'][train.level_type=='D'], ax=axes[1][1]).set_title('Points distribution for \"D\" level problem')\nsns.countplot(train['points'][train.level_type=='E'], ax=axes[2][0]).set_title('Points distribution for \"E\" level problem')\nsns.countplot(train['points'][train.level_type=='F'], ax=axes[2][1]).set_title('Points distribution for \"F\" level problem')\nsns.countplot(train['points'][train.level_type=='G'], ax=axes[3][0]).set_title('Points distribution for \"G\" level problem')\nsns.countplot(train['points'][train.level_type=='H'], ax=axes[3][1]).set_title('Points distribution for \"H\" level problem')\nplt.tight_layout()","a9658a69":"tags = train.tags.str.split(',')\ntags = tags.dropna()\ntext = tags.explode().to_list()\nwc = WordCloud(width=1500, height=1000).generate(' '.join(text))\nplt.figure(figsize=(15,10))\nplt.axis('off')\nplt.imshow(wc)\nplt.show()","48d8dd4d":"tag_counts = Counter(text)\ntag_counts = dict(tag_counts.most_common(10))\ntag_counts = pd.DataFrame(tag_counts.items(), columns = ['tag', 'count'])\nplt.figure(figsize=(12,6))\nsns.barplot(x='count', y='tag', data=tag_counts).set_title('Most Common Tags')\nplt.show()","dbebc6d7":"plt.figure(figsize=(12,7))\nsns.scatterplot(x='problem_solved', y='max_rating', hue='rank', data=train, s=85)\nplt.show()","f7c3d12a":"top10countries = train.country.value_counts().head(10)\nplt.figure(figsize=(12,6))\nsns.barplot(x=top10countries.index, y=top10countries.values)\nplt.xticks(rotation=45)\nplt.show()","1408038a":"expert = train[['country','rank','level_type','attempts_range']][train['rank']=='expert']\nexp = expert['country'].value_counts().head(10)\nexp.plot.pie(y=exp.values, figsize=(10,10), autopct='%.1f')\nplt.show()","63f6fcc3":"advanced = train[['country','rank','level_type','attempts_range']][train['rank']=='advanced']\nadv = advanced['country'].value_counts().head(10)\nadv.plot.pie(y=adv.values, figsize=(10,10), autopct='%.1f')\nplt.show()","fb18ba5a":"intermediate = train[['country','rank','level_type','attempts_range']][train['rank']=='intermediate']\ninter = intermediate['country'].value_counts().head(10)\ninter.plot.pie(y=inter.values, figsize=(10,10), autopct='%.1f')\nplt.show()","dfa4a07c":"beginner = train[['country','rank','level_type','attempts_range']][train['rank']=='beginner']\nbeg = beginner['country'].value_counts().head(10)\nbeg.plot.pie(y=beg.values, figsize=(10,10), autopct='%.1f')\nplt.show()","9152d7a8":"rank_prob_level = pd.DataFrame()\nrank_prob_level['beginner'] = beginner.level_type.value_counts().sort_index()\nrank_prob_level['intermediate'] = intermediate.level_type.value_counts().sort_index()\nrank_prob_level['advanced'] = advanced.level_type.value_counts().sort_index()\nrank_prob_level['expert'] = expert.level_type.value_counts().sort_index()\n\nrank_prob_level = rank_prob_level.reset_index().rename(columns={'index':'level_type'})\n\nsns.set(rc={'figure.figsize':(14,8)})\nrank_prob_level.set_index('level_type').T.plot(kind='bar', stacked=True)\nplt.show()","4b865c1b":"attempts_range = pd.DataFrame()\nattempts_range['beginner'] = beginner.attempts_range.value_counts().sort_index()\nattempts_range['intermediate'] = intermediate.attempts_range.value_counts().sort_index()\nattempts_range['advanced'] = advanced.attempts_range.value_counts().sort_index()\nattempts_range['expert'] = expert.attempts_range.value_counts().sort_index()\n\nattempts_range = attempts_range.reset_index().rename(columns={'index':'attempts_range'})\n\nsns.set(rc={'figure.figsize':(14,8)})\nattempts_range.set_index('attempts_range').T.plot(kind='bar', stacked=True)\nplt.show()","1917c945":"prob_lvl_attempts = train[['level_type', 'attempts_range']]\nprob_lvl_attempts = pd.DataFrame(prob_lvl_attempts.groupby(['level_type','attempts_range'])['attempts_range'].agg('count'))\nprob_lvl_attempts = prob_lvl_attempts.rename(columns={'attempts_range':'num_attempts_range'})\nprob_lvl_attempts = prob_lvl_attempts.reset_index()\n\ndata = prob_lvl_attempts.set_index(['level_type', 'attempts_range'])\nsns.set(rc={'figure.figsize':(16,7)})\ndata.unstack().plot(kind='bar', stacked=True)\nplt.show()","838c9efd":"train.head()","ac3c557b":"train = pd.concat([train.drop('tags', axis=1), train.tags.str.get_dummies(sep=',').add_prefix('tag_')],1)\n\n#one-hot encode test data\ntest = pd.concat([test.drop('tags', axis=1), test.tags.str.get_dummies(sep=',').add_prefix('tag_')],1)","cb62bb0a":"print(f'Percent of NaN in train level_type: {(train.level_type.isna().sum()\/train.shape[0]*100):.2f}%')\nprint(f'Percent of NaN in test level_type: {(test.level_type.isna().sum()\/test.shape[0]*100):.2f}%')","dcb31b80":"train['level_type'] = train['level_type'].fillna('A')\ntest['level_type'] = test['level_type'].fillna('A')","0309822a":"# now LabelEncoder can be applied on level_type\nle = LabelEncoder()\ntrain['level_type'] = le.fit_transform(train['level_type'])\ntest['level_type'] = le.transform(test['level_type'])","d6298717":"le = LabelEncoder()\ntrain['rank'] = le.fit_transform(train['rank'])\ntest['rank'] = le.transform(test['rank'])","08b3a41c":"train['user_id'] = train['user_id'].str[5:].astype(int)\ntest['user_id'] = test['user_id'].str[5:].astype(int)\n\ntrain['problem_id'] = train['problem_id'].str[5:].astype(int)\ntest['problem_id'] = test['problem_id'].str[5:].astype(int)","ad7940b7":"print(f'Percent of NaN in train country: {(train.country.isna().sum()\/train.shape[0]*100):.2f}%')\nprint(f'Percent of NaN in test country: {(test.country.isna().sum()\/test.shape[0]*100):.2f}%')","95686feb":"imp = IterativeImputer()  #instantiate IterativeImputer\n#transform train set\ntrain_country = train.pop('country')\ntrain_cols = train.columns\ntrain = pd.DataFrame(imp.fit_transform(train), columns = train_cols) #fit imputer & transform data\ntrain = train.astype(int)\ntrain['country'] = train_country\n\n#transform test set\ntest_country = test.pop('country')\ntest_cols = test.columns\ntest = pd.DataFrame(imp.fit_transform(test), columns = test_cols)\ntest = test.astype(int)\ntest['country'] = test_country","832152e6":"#Encode country names to numeric values\nle = LabelEncoder()\nle_tr = le.fit(train['country'].dropna())\ncountry_map = dict(zip(le.classes_, range(len(le.classes_)))) # dictionary maps country to numeric code\ntrain['country'] = train['country'].map(country_map)\ntest['country'] = test['country'].map(country_map)","8d38c3de":"#Once again use imputer to impute missing values in the final column\nimp = IterativeImputer()\ntrain_cols = train.columns\ntrain = pd.DataFrame(imp.fit_transform(train), columns = train_cols)\ntrain = train.astype(int)\n\ntest_cols = test.columns\ntest = pd.DataFrame(imp.fit_transform(test), columns = test_cols)\ntest = test.astype(int)","9a298d28":"print(train.isna().sum().any())\nprint(test.isna().sum().any())","96dbf495":"X_train = train.drop('attempts_range', axis=1)\ny_train = train['attempts_range']","9d951258":"print(f'X_train.shape: {X_train.shape}')\nprint(f'y_train.shape: {y_train.shape}')\nprint(f'test.shape: {test.shape}')","167f8bab":"forest = RandomForestClassifier()\nrf = cross_val_score(forest, X_train, y_train, scoring = 'f1_weighted', cv=4)\nprint(rf)\nprint('')\nprint(np.mean(rf))","bd122e45":"forest.fit(X_train, y_train)\nfeat_imp = pd.DataFrame(forest.feature_importances_, index=X_train.columns)\nfeat_imp = feat_imp.sort_values(by=0, ascending=False)\n\n#plot feature importances\nplt.figure(figsize=(12,15))\nsns.barplot(x=feat_imp[0], y=feat_imp.index)\nplt.tight_layout()","3eb4af73":"x_tr, x_te, y_tr, y_te = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=1)","a3192b95":"# set parameter range for randomized search\nparam_distributions = dict(\n    n_estimators = [100, 200, 500],\n    max_depth = [4, 8, 12, None],\n    min_samples_split = [2, 4, 6],\n    max_features = ['sqrt', 'log2'],\n    max_samples = [0.6, 0.8, 1.0]\n    \n)\n\nrf2 = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_distributions, scoring = 'f1_weighted', cv=4)\nrf2.fit(x_tr, y_tr) # fit the best model on train set","fc220100":"rf_pred_y_tr = rf2.predict(x_tr) # get model predictions of train set\nrf_pred_y_te = rf2.predict(x_te) # get model predictions of test set\n\n#Print the F1 score for train and test sets\nprint(f\"train f1-score: {f1_score(y_tr, rf_pred_y_tr, average= 'weighted')}\") \nprint(f\"test f1-score: {f1_score(y_te, rf_pred_y_te, average= 'weighted')}\")","04993513":"print(rf2.best_params_)","7a80862b":"xgb = XGBClassifier(max_depth=16, eta=0.12) #max_depth and eta were set after hyperparamter tuning \nxgb.fit(x_tr, y_tr) #fit model on train set\nxgb_pred_y_te = xgb.predict(x_te) #predict on test set\nxgb_pred_y_tr = xgb.predict(x_tr) #predict on train set\nprint(f\"train f1-score: {f1_score(y_tr, xgb_pred_y_tr, average= 'weighted')}\")\nprint(f\"test f1-score: {f1_score(y_te, xgb_pred_y_te, average= 'weighted')}\")","c9a73647":"estimators = [('xgb1', XGBClassifier(max_depth=16, eta=0.12)), \n               ('rf', RandomForestClassifier(n_estimators=200, min_samples_split=4, max_samples=0.8, min_impurity_split=0.2)),\n             ]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=XGBClassifier())\nclf.fit(x_tr, y_tr)","a7b2c0a9":"clf_pred_y_tr = clf.predict(x_tr)\nclf_pred_y_te = clf.predict(x_te)","6c8c5f64":"print(f\"train f1-score: {f1_score(y_tr, clf_pred_y_tr, average= 'weighted')}\") \nprint(f\"test f1-score: {f1_score(y_te, clf_pred_y_te, average= 'weighted')}\")","5481bab0":"# Fit the selected model on the entire train set\nxgb.fit(X_train, y_train)","2b6ec5b3":"y_test_pred = xgb.predict(test)","c62f484a":"#create a new dataframe and populate test ids & predictions in it\nmy_sub = pd.DataFrame()\nmy_sub['ID'] = test_ids\nmy_sub['attempts_range'] = y_test_pred\n#load test file\ntest1 = pd.read_csv('..\/input\/avrecommendation-system\/AV-Recommendation Engine\/test_submissions_NeDLEvX.csv')\n#align test ids in the same order as they are in the test file\ntest1 = test1.merge(my_sub, on='ID').drop(['user_id', 'problem_id'],axis=1)\n#generate CSV file\ntest1.to_csv('Recommendation Engine Submission.csv', index=False)","d4b9de06":"**Problem level and users**","15a4f171":"There is a further improvement in F1 score on test set although it is not much. ","c462cfd2":"# Model Selection \n\nAmong all the models tested, the third model did a relativelely better job than the other three models. Hence I would select it as the final model.","23179f51":"**Attempts_range and Users**","59ec39d2":"One can see a strong correlation between rank and max_rating. It is usually the case that skilled programmers get higher ratings and usually tend to solve more problems than entry level programmers, this correlation can be seen in the figure.","b0be1cbc":"**Rank**\n\nLabel encode rank in train and test set","4947d614":"The data is present in three tables - train, user, prob. Merge the data and have it in one table for analysis.","b2a17f35":"Country is missing for almost one-fourth of train as well as test data. This issue can be fixed in the following ways:\n\ni. **Drop rows with missing data:** This approach will discard 25% of available data. This would lead to loss of important information which may otherwise be used for predicting the target. Obviosuly this should be avoided if possible.\n\nii. **Fill missing values with aggregate values such as mode:** This approach most likely will lead to better results than the first approach however the variable's predictive capacity would get compromised if significant number of artificial entries are added.\n\niii. **Impute missing values from available values in the table:** This approach works better than the first two approaches in many situations even though it may not yield the best results as we are imputing the missing values which may differ from the actual values. ","07c7d192":"Top 10 countries among advanced participants","cd992cf7":"# Modeling","cbb90c65":"# Split data into train and test sets","fec78ce3":"Beginners submitted more problems at the entry level. Levels A, B and C comprise the biggest chunk of all problems solved by beginners. More skilled users submitted a significant number of higher level problems. Advanced and expert users have almost equal share of problems at all the levels present in the group.","5eeaeae3":"# 4. Stacking of XGBoost and RandomForestClassifier\n\nSometimes, stacked classifiers do a significantly better job than single classifiers. I will stack a XGBClassifier and RandomForestClassifier with tuned parameters and use another XGBClassifier with default params as final estimator.","40b9e5cf":"# This notebook contains:\n\n* EDA\n* Feature Engineering\n* Modeling\n* Submission","03b6b17f":"**Plot Feature imortances**","7e97eef2":"# 2. RandomizedSearchCV on RandomForestClassifier\n\nI will use RandomizedSearchCV on RandomForestClassifier to see if it results in performance improvement over the default classifier.","b690b049":"Both train and test datasets have null values. These have to be fixed since ML models will not accept NaN values. ","29714064":"There is an improvement in the second model's performance on the test set. However, there is an issue of overfitting. Let's see what are the best parameters.","059c8b7a":"Even though overall users from India are the maximum at about 30K, maximum experts are from Russia follwed by China and Ukraine. ","0253b8b1":"# Generate Submission File","17b752bf":"**Tags**","1fda68f7":"**Users from top 10 countries**","da4353b7":"Top 10 countries among beginners","c90d38de":"**Distribution of target: attempts_range **","d1073753":"**Check for null values**","5a9920c7":"**User id and Problem id**\n\nUser id and problem id have prefix - user_ and prob_ before the id number. Remove the prefix and only keep numerical id.","6311feb6":"# Loading the data","ae05ef06":"One way to find out if there is a correlation between level_type and difficulty is by checking the points assigned for problems at the various levels since challenging problems fetch greater points.\n\nLevel_type & Points","b66b18cf":"# Exploratory Data Analysis","9e7b4631":"As the missing values are only a tiny proportion of data. I will fill the blanks with the mode value.","7e937446":"# 3. XGBoost Classifier","2813bcca":"Majority of the advanced and expert users are from Russia and China whereas a vast majority of participants at beginner and intermediate levels are from Bangladesh and India.","945e1845":"# Import necesary libraries","244dd834":"**Country**","067ed1db":"In this case, stacking the classifiers did not lead to an improvement on test performance.","8d55222a":"**Tags column**\n\nTags column contains problem tags which give a sense of the type of problem and therefore hold useful information for a model. Some rows contain multiple tags. It is a good idea to create separate columns for each tag and apply binary coding to indicate its presence or absence in a problem. One hot encoding tags for both train and test data.","094d6d86":"**Problem level:** There are various levels of problems in this data. They are encoded into letters A to N and specified in level_type column. The problem levels probably indicate the difficulty level. ","a2307e27":"**Problem level and Attempts_range**","f3e4fbc1":"Check if there are any null values in train and test sets","3ea79626":"All the data is present in one table - train. Now it can be used for analysis.","d699f23a":"**Make predictions on test data**","a9404dbb":"**Level type**\n\nLevel type must be converted to numeric encoding from letter encoding. However there are sum NA values which have to be dealth with before applying encoder.","38e90467":"# Recommendation Engine\n\nIn this challenge we are required to build a model to predict the number of attempts taken  by participants for a successful submission to online programming challenges. Data of programmers and questions they solved previously were given along with the time they took to solve the questions. \n\nInput data contains three files:\n\n**train_submissions.csv** - This contains 1,55,295 submissions which are selected randomly from 2,21,850 submissions. It contains 3 columns (\u2018user_id\u2019, \u2018problem_id\u2019, \u2018attempts_range\u2019). The variable \u2018attempts_range\u2019 denotes the range of attempts that the users made to get the solution accepted which is our target. It takes values between 1 and 6.          \n\n**user_data.csv** - This file contains data of users. It contains the following features :-\n* user_id - unique ID assigned to each user\n* submission_count - total number of user submissions\n* problem_solved - total number of accepted user submissions\n* contribution - user contribution to the judge\n* country - location of user\n* follower_count - amount of users who have this user in followers\n* last_online_time_seconds - time when user was last seen online\n* max_rating - maximum rating of user\n* rating - rating of user\n* rank - can be one of \u2018beginner\u2019 ,\u2019intermediate\u2019 , \u2018advanced\u2019, \u2018expert\u2019\n* registration_time_seconds - time when user was registered\n \n\n**problem_data.csv** - This file contains data of the problems. It contains the following features :-\n* problem_id - unique ID assigned to each problem\n* level_id - the difficulty level of the problem between \u2018A\u2019 to \u2018N\u2019\n* points - amount of points for the problem\n* tags - problem tag(s) like greedy, graphs, DFS etc.\n \nTest data has one file:\n\n**test_submissions.csv** - This contains the remaining 66,555 submissions from total 2,21,850 submissions. Contains 1 column (ID). The \u2018attempts_range\u2019 column which is to be predicted.\n\n","a421ea41":"Ploblems solved by users at various levels","7d0f3c58":"# 1. Random Forest Classifier with default params\n\nFirst, I would fit a RandomForestClassifier with default parameters and look at the feature importances. As the competition uses Weighted F1 score, I will pass the same scoring metrics in cross validation with 4 splits. ","87799756":"Sklearn provides IterativeImputer, a multivariate imputer that estimates missing features from all the other features. It works only with numeric data. Therefore string values in 'country' have to be converted to numeric values before applying imputer on the data.\n\nI will do the following to impute missing countries:\n\n* Impute the missing 'points' after dropping country\n* Fit LabelEncoder on countries in train set\n* Create a dictionary that maps countries and numeric codes\n* Map the countries using the dictionary\n* Apply IterativeImputer on train and test sets to impute missing countries. ","d882e17f":"Top 10 countries among experts","4bf5a426":"Top 10 countries among intermediate level users","1128d6b2":"We can see a consistent increase in points from level A to G. So difficulty level increases from A to H. ","1783dca5":"# Feature Engineering"}}