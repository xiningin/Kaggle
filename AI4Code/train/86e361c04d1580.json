{"cell_type":{"c9449284":"code","939bfa56":"code","b13b2abb":"code","be3bdf3a":"code","0b44c326":"code","77f95acc":"code","60cc9a27":"code","728e880b":"code","35d50e79":"code","f59631c8":"code","5350ceb5":"code","ab537e89":"code","68252291":"code","de103fe5":"code","35e9ee69":"code","36090973":"code","c09813f7":"code","488ff378":"code","a3426ab0":"code","d3d3bfa0":"code","6f92d539":"markdown"},"source":{"c9449284":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport glob\nimport json\n\n#sys.path.insert(0, \"..\/\")\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13'\n\njson_filenames = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nprint(len(json_filenames))\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","939bfa56":"all_articles_df = pd.DataFrame(columns=[\"source\", \"title\", \"doc_id\",  \"abstract\", \"text_body\"])\n\n#all_articles_df = pd.DataFrame.from_dict(all_articles_df)","b13b2abb":"def return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [data['abstract'][x]['text'] for x in range(len(data['abstract']) - 1)]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. For some reason I am getting an index error\n            # In one of the Json files, so rather than have it wrapped in a lovely list\n            # comprehension I've had to use a for loop like a neanderthal. \n            \n            # Needless to say this bug will be revisited and conquered. \n            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n\nall_articles_df = return_corona_df(json_filenames, all_articles_df, 'b')\nall_articles_df_out = all_articles_df.to_csv('kaggle_covid-19_open_csv_format.csv')","be3bdf3a":"all_articles_df.head()","0b44c326":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n","77f95acc":"titles = all_articles_df['title']\ntitles.fillna(\"\",inplace=True)\n","60cc9a27":"titles.iloc[:5]","728e880b":"# Fit model","35d50e79":"tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\nX_tfidf = tfidf.fit_transform(titles)\ntfidf_feature_names = tfidf.get_feature_names()\n\nvectorizer = CountVectorizer(stop_words='english', max_features=1000)\nX_tf = vectorizer.fit_transform(titles)\ntf_feature_names = vectorizer.get_feature_names()","f59631c8":"tfidf_feature_names[500:510]","5350ceb5":"clustered = KMeans(n_clusters=6, random_state=0).fit_predict(X_tfidf)\n\nall_articles_df['cluster_abstract']=clustered\n\ngrouped=all_articles_df.groupby('cluster_abstract')","ab537e89":"grouped.count()","68252291":"import pylab as plt\nfrom numpy import arange\nplt.figure()\nfor i in arange(500):\n    plt.plot(i,len(titles.iloc[i]), 'ro')\nplt.show()","de103fe5":"n_topics = 15\n\n# Run NMF\nnmf = NMF(n_components=n_topics).fit(X_tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=n_topics).fit(X_tf)","35e9ee69":"for j in arange(10):\n    print(\"==============\")\n    for i in nmf.components_[j].argsort()[:-10:-1]:\n        print(tfidf_feature_names[i])","36090973":"#extract topics\ndef display_topics(model, feature_names, no_top_words):\n    topics=[]\n    for topic_idx, topic in enumerate(model.components_):\n        #rint (\"Topic %d:\" % (topic_idx))\n        topic_words=\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n        #rint(topic_words)\n        topics.append(topic_words)\n    return topics\n\nno_top_words = 5\n#rint(\"NMF: \")\ntopics_nmf=display_topics(nmf, tfidf_feature_names, no_top_words)\n#rint(\"\\nLDA: \")\ntopics_lda=display_topics(lda, tf_feature_names, no_top_words)\n\n#rint(topics_nmf)\n#rint(topics_lda)\n\npred_lda=lda.transform(X_tf)\npred_nmf=nmf.transform(X_tfidf)\n\nres_lda=[topics_lda[np.argmax(r)] for r in pred_lda]\nres_nmf=[topics_nmf[np.argmax(r)] for r in pred_nmf]","c09813f7":"all_articles_df['topic_lda']=res_lda\nall_articles_df['topic_nmf']=res_nmf","488ff378":"all_articles_df.head()","a3426ab0":"grouped=all_articles_df.groupby('topic_nmf')","d3d3bfa0":"grouped.count()","6f92d539":"** Factorization **"}}